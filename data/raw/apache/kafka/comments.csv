id,pr_number,user,created_at,body
297865351,2929,lindong28,2017-04-27T23:19:46Z,@jjkoshy @onurkaraman Can you review this patch? The patch has been rebased onto trunk. Both integration test and the newly-added system test has passed. Thanks.
297871665,2929,asfbot,2017-04-28T00:02:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3240/
Test FAILed (JDK 8 and Scala 2.12).
"
297872143,2929,asfbot,2017-04-28T00:05:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3249/
Test PASSed (JDK 8 and Scala 2.11).
"
297872198,2929,asfbot,2017-04-28T00:06:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3244/
Test PASSed (JDK 7 and Scala 2.10).
"
297915554,2929,asfbot,2017-04-28T06:08:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3259/
Test PASSed (JDK 7 and Scala 2.10).
"
297915927,2929,asfbot,2017-04-28T06:11:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3264/
Test PASSed (JDK 8 and Scala 2.11).
"
297917836,2929,asfbot,2017-04-28T06:25:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3255/
Test PASSed (JDK 8 and Scala 2.12).
"
298066652,2929,asfbot,2017-04-28T18:02:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3287/
Test PASSed (JDK 8 and Scala 2.11).
"
298067201,2929,asfbot,2017-04-28T18:05:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3282/
Test PASSed (JDK 7 and Scala 2.10).
"
298070725,2929,asfbot,2017-04-28T18:20:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3278/
Test PASSed (JDK 8 and Scala 2.12).
"
298077449,2929,lindong28,2017-04-28T18:47:47Z,@onurkaraman @jjkoshy I have gone through the code again after rebasing the patch and it should be ready for review.
298088455,2929,asfbot,2017-04-28T19:38:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3283/
Test FAILed (JDK 7 and Scala 2.10).
"
298088919,2929,asfbot,2017-04-28T19:40:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3288/
Test PASSed (JDK 8 and Scala 2.11).
"
298096134,2929,asfbot,2017-04-28T20:15:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3279/
Test PASSed (JDK 8 and Scala 2.12).
"
306559832,2929,junrao,2017-06-06T17:31:21Z,"@lindong28 : Thanks for the patch. Haven't looked at it in details. Just a couple of quick comments. (1) Once a disk is marked offline, it might be useful for the admin to be able to fix the bad disk (e.g., remounting) while the broker is online. This reduces the time that a broker has to be down while waiting for a disk to be fixed. If we do support this, it would be useful to test this out a bit. (2) According to http://www.mapdb.org/blog/mmap_files_alloc_and_jvm_crash/, it seems that an I/O exception in mmap buffer crashes the JVM. This will affect the failure detection in JBOD. Not sure how much we can do to improve it in the short term. At the minimum, it would be useful to do some testing to see how much this impacts us and document the impact."
306576951,2929,asfbot,2017-06-06T18:32:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4956/
Test PASSed (JDK 8 and Scala 2.12).
"
306579030,2929,asfbot,2017-06-06T18:39:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4972/
Test PASSed (JDK 7 and Scala 2.11).
"
306989136,2929,lindong28,2017-06-08T03:35:19Z,"@becketqin @junrao Thanks for helping with the review! I wanted to rebase earlier but got delayed due to the busy oncall week. I should be able to rebase/test the patch and review it myself again by this Friday. Hope we can get this reviewed and committed soon instead of another major rebase :)

@junrao Thanks for the suggestion. It is definitely useful for the admin to be able to fix the bad disk while the broker is running. I think it requires a new KIP in addition to KIP-112 and KIP-113 because it takes new tool script and notification event which is not currently included in those two KIPs. Is it OK for me work on this as a new KIP after KIP-112 and KIP-113 is completed?

The handling of I/O exception in mmap seems similar to the sixth future work mentioned in KIP-112 wiki, i.e. handling various failure scenario case-by-case. KIP-112 currently only handles disk failure that can be caught in the form of IOException instead of JVM crash. I will think about how to handle this I/O exception and let you know my answer.
"
306992238,2929,asfbot,2017-06-08T04:03:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5043/
Test FAILed (JDK 8 and Scala 2.12).
"
306992430,2929,asfbot,2017-06-08T04:04:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5059/
Test FAILed (JDK 7 and Scala 2.11).
"
306997336,2929,asfbot,2017-06-08T04:49:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5045/
Test FAILed (JDK 8 and Scala 2.12).
"
306997776,2929,asfbot,2017-06-08T04:52:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5061/
Test FAILed (JDK 7 and Scala 2.11).
"
307003408,2929,lindong28,2017-06-08T05:35:28Z,"@becketqin The patch is ready for review. I have rebased it onto the latest trunk, addressed comments, passed integration tests and gone over the patch myself. Thanks!"
307008569,2929,asfbot,2017-06-08T06:11:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5063/
Test PASSed (JDK 7 and Scala 2.11).
"
307008587,2929,asfbot,2017-06-08T06:11:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5047/
Test PASSed (JDK 8 and Scala 2.12).
"
307721593,2929,asfbot,2017-06-12T08:21:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5146/
Test PASSed (JDK 8 and Scala 2.12).
"
307723211,2929,asfbot,2017-06-12T08:28:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5162/
Test FAILed (JDK 7 and Scala 2.11).
"
307735488,2929,asfbot,2017-06-12T09:20:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5148/
Test FAILed (JDK 8 and Scala 2.12).
"
307752604,2929,asfbot,2017-06-12T10:35:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5150/
Test PASSed (JDK 8 and Scala 2.12).
"
307758355,2929,asfbot,2017-06-12T11:02:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5166/
Test PASSed (JDK 7 and Scala 2.11).
"
307870474,2929,asfbot,2017-06-12T18:06:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5165/
Test FAILed (JDK 8 and Scala 2.12).
"
307870489,2929,asfbot,2017-06-12T18:06:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5181/
Test FAILed (JDK 7 and Scala 2.11).
"
307888980,2929,asfbot,2017-06-12T19:06:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5182/
Test PASSed (JDK 7 and Scala 2.11).
"
307891134,2929,asfbot,2017-06-12T19:14:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5166/
Test PASSed (JDK 8 and Scala 2.12).
"
308255846,2929,asfbot,2017-06-13T21:36:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5232/
Test PASSed (JDK 8 and Scala 2.12).
"
308256697,2929,asfbot,2017-06-13T21:39:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5250/
Test PASSed (JDK 7 and Scala 2.11).
"
309190364,2929,asfgit,2017-06-17T03:33:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5433/
Test FAILed (JDK 7 and Scala 2.11).
"
309191562,2929,asfgit,2017-06-17T04:01:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5418/
Test PASSed (JDK 8 and Scala 2.12).
"
309918401,2929,junrao,2017-06-20T23:17:51Z,"""t is definitely useful for the admin to be able to fix the bad disk while the broker is running. I think it requires a new KIP in addition to KIP-112 and KIP-113 because it takes new tool script and notification event which is not currently included in those two KIPs. Is it OK for me work on this as a new KIP after KIP-112 and KIP-113 is completed?""

@lindong28 : For the above, I wasn't referring to the case when the admin fixes the disk and then notifies the broker about the fix online. The case that I was referring to is that when a disk is marked offline, if the admin can fix the disk while the broker is still online, then the admin can just restart the broker quickly after the disk is fixed. The downtime window for that broker is just the restart time. If the admin has to first shut down the broker and then fix the disk, the time for fixing the disk is part of the downtime window. So, my question is that is there any issue that prevents the admin from fixing the offline disk while the broker is up (e.g., the mmap-ed files on the bad disk)."
309928827,2929,asfgit,2017-06-21T00:30:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5520/
Test FAILed (JDK 8 and Scala 2.12).
"
309929077,2929,asfgit,2017-06-21T00:32:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5534/
Test PASSed (JDK 7 and Scala 2.11).
"
309958368,2929,lindong28,2017-06-21T04:16:53Z,"@junrao Thanks for the explanation. I think I understand your point now. My understanding is that you are concerned with the possibility of broker crash with single disk failure due to mapped files issue.

Because I don't have multiple disk devices on my desktop to test mount/unmount of log directories while broker is running, I will test it in dev test cluster and I am waiting for SRE to mount/unmount log directory for me since they have the sudo access. I will let you know the result tomorrow.

I have also gone through the blog of mapped files you provided. Most of the issue (e.g. limited number of mmap handlers) are the same between RAID-10 broker and JBOD broker. Since we are not having them now, I assume we won't have them with JBOD in the near future. The only issue that is more likely to happen with JBOD is that JVM may crash when disk gets full. It is more likely to happen with JBOD because the single JBOD log directory will have smaller size than the RAID-10 log directory.

We currently use mmap files for reading/writing to index files. The solution suggested by the blog is to write to index files using `FileChannel` instead of mmap files. It is mentioned that performance is minimal if large blocks are used. However, it is not clear whether there will be significant performance hit for index file write operation since the block size for index file write is small.

Suppose we can not use `FileChannel` to write to index files due to performance reason, then I think the solution is for Kafka administrator to properly monitor the usage of log directories on brokers and rebalance usage across log directories before any log directory gets full. For example rebalance can be triggered if usage exceeds 60%. In the rare case that there is extreme traffic spike that exhausts the space of a log directory, it seems OK to just let this broker fail. It will reduce availability of the broker in any unnecessary manner but it seems OK if this happens rarely.

To summarize, I will test how broker handles umount of log directory while it is running. And KIP-113 provides a reasonable solution to deal with VM crash due to full disk. Does this sound reasonable?
"
310195927,2929,lindong28,2017-06-21T20:30:00Z,@junrao I have updated the code (See [here](https://github.com/apache/kafka/pull/2929/commits/143f76606b89d2b76aa30775996037f1723d547b)) and verified that the all files handlers in the offline log directory will be removed and we can unmount the disk while the broker is still running after the corresponding log directory goes offline.
310212345,2929,asfgit,2017-06-21T21:36:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5566/
Test FAILed (JDK 8 and Scala 2.12).
"
310218187,2929,asfgit,2017-06-21T22:03:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5580/
Test PASSed (JDK 7 and Scala 2.11).
"
310551086,2929,asfgit,2017-06-23T02:08:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5635/
Test FAILed (JDK 8 and Scala 2.12).
"
310551100,2929,asfgit,2017-06-23T02:08:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5649/
Test FAILed (JDK 7 and Scala 2.11).
"
310559209,2929,asfgit,2017-06-23T03:10:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5650/
Test PASSed (JDK 7 and Scala 2.11).
"
310562386,2929,asfgit,2017-06-23T03:34:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5636/
Test PASSed (JDK 8 and Scala 2.12).
"
310785873,2929,becketqin,2017-06-23T22:22:44Z,"Talked to @lindong28 offline. I think there might be a cleaner way to handle the disk exceptions. It looks that what we want to do when disk exception happens is the following:
1. Notify controller there is a disk failure and mark the disk as failed.
2. return an error to whoever sent the request if the failure was triggered by a request.

I think we can have a util method to do (1) immediately when the disk IO occurs, and bubble up a  KafkaStorageException which is a retriable exception. When the clients received this exception, it will act accordingly. The benefit of doing this is that we don't need to care about the what exact operation was in progress when the disk IO exception occurs. The leader will always be moved to somewhere else and the related operation will be retried if needed."
310809552,2929,lindong28,2017-06-24T03:01:45Z,"After further discussion with @becketqin, we agree not to handle IOException in asyc read for FetchRequest. I will add `LogManager` to the constructor of `LogCleaner` and `LogCleanerManager` so that they can call `logManager.handleLogDirectoryFailure(...)` when there is `IOException`.

In addition, `Partition.getOrCreateReplica(...)` will check whether the log of an existing local Replica is actually offline and throw `KafkaStorageException` if it is offline. We need this extra check because with the change in LogCleaner, it is possible for replicas to be offline in `LogManager` while they are still in the cache `ReplicaManager.allPartitions`. This guarantees that if `LogCleaner` encounters `IOException`, controller will be notified via zookeeper path and `LeaderAndIsrResponse` will tell controller the offline replicas."
310817415,2929,lindong28,2017-06-24T06:08:40Z,"@becketqin Regarding your suggestion to go through usages of all methods in `FileRecords`. There are currently many IO related operations in Kafka whose IOException is either explicitly swallowed or not explicitly handled. I am not sure we should find out all of them and mark the corresponding log directory and replicas as offline in this patch. 

One reason is that this maybe over engineering and even reduce the availability of Kafka in an unnecessary manner. Given that we are not explicitly shutting down broker after catching those IOException, either most log directory failure will already be caught and trigger `Exit.halt(1)`, or Kafka broker may be working just fine with its current way of handling those IOException. Thus it may not do us much benefit to handle them in a different way. 

It is also about the efficiency and the priority of work. I agree with you that there exists IOException that we can handle in a better way to improve the quality of JBOD support. But this task seems independent to the features added in this patch. The main goal of this patch is to change the way we handle log directory failure (i.e. those that currently trigger `halt()`) and it has covered most log directory failure (i.e. those that trigger IOException during ProduceRequest, FetchRequest, checkpoint read/write and LogCleaner read/write). On the other hand, what you asked for is what other IOException should be treated as log directory failure. I am wondering if we can do that separately in a follow-up patch after KIP-112 and KIP-113 are implemented. This may help speedup the overall implementation efficiency and reduce the need for rebase by splitting a large patch into smaller ones. Also, by finishing KIP-112 and KIP-113 first, we can deploy the JBOD feature sooner and the experience from its deployment can help us determine whether we need to handle some exception differently."
310824773,2929,asfgit,2017-06-24T08:24:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5685/
Test PASSed (JDK 7 and Scala 2.11).
"
310825987,2929,asfgit,2017-06-24T08:50:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5671/
Test PASSed (JDK 8 and Scala 2.12).
"
311121325,2929,junrao,2017-06-26T17:07:00Z,"@lindong28 : I will try to make a pass of the patch later this week as well. About the disk failure detection, we can probably start with something simple. Currently, we only fail a broker on IOException during writes. We can probably just treat that as disk failure to start with. It would be useful to make the failure detection part a bit more general so that we can incorporate more sophisticated failure detection in the future (e.g., consistent long I/Os, on-disk CRC failure, etc). "
311122631,2929,lindong28,2017-06-26T17:12:03Z,"@junrao It would be great to have your review! Thanks! Yeah I also would like to start with something simple. Becket and I have agreed to limit the scope of disk failure in this patch to mainly those that currently cause `Exit.halt(1)` as of the current Kafka implementation.

And yes, we need to expand it later to make the failure detection more general. For example, detection of long I/Os is included as the sixth future work in the KIP-112."
311594553,2929,lindong28,2017-06-28T08:36:18Z,"@becketqin Thanks much for your review! I have updated the patch to address all the comments. I have added one integration test to simulate the log directory failure and verify that producer will receive NotLeaderForPartitionException. Note that producer request does not necessarily trigger IOException immediately after log directory failure because log append operation uses mmap and its write operation to disk is delayed. On the other hand, our write operation to checkpoint file can see IOException immediately because it uses FileChannel. And if the IOException is triggered when broker writes to checkpoint file, ProduceResponse will show NotLeaderForPartitionException instead of KafkaStorageException.

I will try to add another test to verify that producer can send message after retry."
311597230,2929,lindong28,2017-06-28T08:48:08Z,"@junrao It seems that @becketqin has finished reviewing the patch and I think the KIP-112 implementation is in good shape. On the other hand, I am in the process of writing KIP-113 and it should be ready for review this week. I am not sure if you have time to review both KIP-112 and KIP-113. KIP-113 is like going to be more complicated than KIP-112 and needs closer look by senior committers. I am wondering if you can help review KIP-113 if you have time for only one KIP. Thanks!"
311607736,2929,asfgit,2017-06-28T09:31:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5748/
Test PASSed (JDK 8 and Scala 2.12).
"
311608996,2929,asfgit,2017-06-28T09:36:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5762/
Test PASSed (JDK 7 and Scala 2.11).
"
311746493,2929,asfgit,2017-06-28T18:27:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5774/
Test FAILed (JDK 7 and Scala 2.11).
"
311751978,2929,asfgit,2017-06-28T18:47:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5776/
Test FAILed (JDK 7 and Scala 2.11).
"
311753884,2929,asfgit,2017-06-28T18:54:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5762/
Test FAILed (JDK 8 and Scala 2.12).
"
311794758,2929,asfgit,2017-06-28T21:17:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5765/
Test FAILed (JDK 8 and Scala 2.12).
"
311797592,2929,asfgit,2017-06-28T21:29:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5779/
Test FAILed (JDK 7 and Scala 2.11).
"
311807898,2929,asfgit,2017-06-28T22:16:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5771/
Test FAILed (JDK 8 and Scala 2.12).
"
311815675,2929,asfgit,2017-06-28T23:01:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5785/
Test PASSed (JDK 7 and Scala 2.11).
"
311820470,2929,lindong28,2017-06-28T23:31:46Z,"@becketqin I have updated the newly-added test and it verifies that the producer will see NotLeaderForPartitionException when there is log directory failure and succeed after it retries. I have also rebased the patch onto trunk. All tests have passed except for the one test in Kafka connector, which I don't think is caused by this patch."
311823840,2929,junrao,2017-06-28T23:53:38Z,@lindong28 : I am taking a look a the patch now. Should be done by tomorrow.
311824795,2929,lindong28,2017-06-29T00:00:41Z,@junrao Thanks much!
312411970,2929,lindong28,2017-07-01T05:28:57Z,"@junrao Thanks so much for detailed review. I have addressed most of the comments in the updated patch. The main remaining issues are 1) whether we should invoke `ReplicaManager.handleLogDirFailure()` in `LogManager.handleLogDirFailure()` and 2) whether/how we can handle `IOException` in the methods of `TransactionStateManager`. I have provided explanation under the corresponding comments.

BTW, there is one comment regarding a line in `Partition.scala` that can only be replied in https://github.com/apache/kafka/pull/2929/files but not in this page."
312424767,2929,asfgit,2017-07-01T10:41:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5860/
Test FAILed (JDK 7 and Scala 2.11).
"
312431812,2929,asfgit,2017-07-01T13:19:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5845/
Test FAILed (JDK 8 and Scala 2.12).
"
313316793,2929,asfgit,2017-07-06T07:19:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5912/
Test FAILed (JDK 7 and Scala 2.11).
"
313317046,2929,asfgit,2017-07-06T07:21:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5897/
Test FAILed (JDK 8 and Scala 2.12).
"
313317734,2929,lindong28,2017-07-06T07:24:55Z,Thanks much for the comment @junrao.
313363895,2929,asfgit,2017-07-06T10:54:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5917/
Test FAILed (JDK 7 and Scala 2.11).
"
313364502,2929,asfgit,2017-07-06T10:57:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5902/
Test FAILed (JDK 8 and Scala 2.12).
"
313542970,2929,asfgit,2017-07-06T23:05:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5932/
Test FAILed (JDK 7 and Scala 2.11).
"
313543265,2929,asfgit,2017-07-06T23:07:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5917/
Test FAILed (JDK 8 and Scala 2.12).
"
313614199,2929,asfgit,2017-07-07T07:56:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5934/
Test FAILed (JDK 8 and Scala 2.12).
"
313615888,2929,asfgit,2017-07-07T08:04:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5935/
Test FAILed (JDK 8 and Scala 2.12).
"
313626372,2929,asfgit,2017-07-07T08:54:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5954/
Test FAILed (JDK 7 and Scala 2.11).
"
313652768,2929,asfgit,2017-07-07T11:04:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5950/
Test FAILed (JDK 7 and Scala 2.11).
"
313662219,2929,asfgit,2017-07-07T11:58:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5940/
Test FAILed (JDK 8 and Scala 2.12).
"
313662223,2929,asfgit,2017-07-07T11:58:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5955/
Test FAILed (JDK 7 and Scala 2.11).
"
313762445,2929,asfgit,2017-07-07T18:44:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5968/
Test FAILed (JDK 8 and Scala 2.12).
"
313789201,2929,asfgit,2017-07-07T20:47:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5987/
Test PASSed (JDK 7 and Scala 2.11).
"
313795709,2929,asfgit,2017-07-07T21:19:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5972/
Test PASSed (JDK 8 and Scala 2.12).
"
313796522,2929,lindong28,2017-07-07T21:23:32Z,"@junrao Finally I was able to fix the test failure by closing the newly added LogDirFailureHandler thread, removing newly-added when the ReplicaManager shutdown, and optimizing the `LogManager.liveLogDirs` not to instantiate new array when there is no offline log dir. These are only causing test failure in Github (not on my machines) probably because the virtual machine used in Github is slower with less memory.

The patch is fully ready for review again :)"
313807695,2929,asfgit,2017-07-07T22:28:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5977/
Test FAILed (JDK 8 and Scala 2.12).
"
313807844,2929,asfgit,2017-07-07T22:29:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5992/
Test FAILed (JDK 7 and Scala 2.11).
"
313809579,2929,asfgit,2017-07-07T22:41:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5978/
Test FAILed (JDK 8 and Scala 2.12).
"
313809661,2929,asfgit,2017-07-07T22:41:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5993/
Test FAILed (JDK 7 and Scala 2.11).
"
313811677,2929,asfgit,2017-07-07T22:56:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5979/
Test FAILed (JDK 8 and Scala 2.12).
"
313811830,2929,asfgit,2017-07-07T22:57:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5994/
Test FAILed (JDK 7 and Scala 2.11).
"
313821021,2929,asfgit,2017-07-08T00:24:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5997/
Test PASSed (JDK 7 and Scala 2.11).
"
313822907,2929,asfgit,2017-07-08T00:50:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5982/
Test PASSed (JDK 8 and Scala 2.12).
"
314349799,2929,asfgit,2017-07-11T06:40:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6024/
Test PASSed (JDK 7 and Scala 2.11).
"
314354662,2929,asfgit,2017-07-11T07:06:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6009/
Test PASSed (JDK 8 and Scala 2.12).
"
314612960,2929,lindong28,2017-07-12T00:51:40Z,"@junrao Excuse me.. I am wondering if there is any further issue to be addressed with the patch. Or if there is no major issue and if you are busy, is it OK for Becket to give it a final pass and merge it?"
314808010,2929,junrao,2017-07-12T15:35:26Z,@lindong28 : I started looking at your latest patch. I will finish my review today.
314838404,2929,lindong28,2017-07-12T17:22:20Z,@junrao : Thanks much for your time! I also reviewed the entire patch again and found 3 minor issues. I will address all of them after your review.
315308226,2929,lindong28,2017-07-14T08:54:46Z,"@junrao Thanks so much for taking so much time to review this patch! I thought the patch is ready but clearly there was space for improvement. I have updated the patch to address all comments. I have gone over the patch carefully with the hope that I don't miss anything this time.

Besides minor changes such as the method comment and removal of unused import, here are the bigger changes that were made:

- Simplified the way we handle KafkaStorageException such that we no longer need to invoke `maybeAddLogFailureEvent()` if `KafkaStorageException` is caught.
- Added a final static variable `ReplicaManager.OfflinePartition` and use it to make sure that KafkaStorageException is consistently returned when client attempts to access an offline replica. This is verified with the updated test code.
- Replaced all usage of `kafka.common.KafkaStorageException` with `org.apache.kafka.common.errors.KafkaStorageException` so that we can remove `kafka.common.KafkaStorageException` going forward.
- Added metric `OfflineReplicaCount` which counts the number of offline local replicas on a broker.
- Added LogDirFailureChannel to Log and LogSegment and invoke `maybeAddLogFailureEvent()` right after the IOException is caught.
- Renamed the new exception to UnknownErrorCodeException"
315320776,2929,asfgit,2017-07-14T09:55:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6053/
Test FAILed (JDK 8 and Scala 2.12).
"
315358427,2929,asfgit,2017-07-14T13:23:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6070/
Test FAILed (JDK 7 and Scala 2.11).
"
315358435,2929,asfgit,2017-07-14T13:23:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6055/
Test FAILed (JDK 8 and Scala 2.12).
"
315370445,2929,asfgit,2017-07-14T14:15:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6059/
Test FAILed (JDK 8 and Scala 2.12).
"
315370498,2929,asfgit,2017-07-14T14:15:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6074/
Test FAILed (JDK 7 and Scala 2.11).
"
315454432,2929,asfgit,2017-07-14T20:04:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6068/
Test FAILed (JDK 8 and Scala 2.12).
"
315467270,2929,asfgit,2017-07-14T21:03:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6088/
Test PASSed (JDK 7 and Scala 2.11).
"
315472417,2929,asfgit,2017-07-14T21:29:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6073/
Test PASSed (JDK 8 and Scala 2.12).
"
315479566,2929,asfgit,2017-07-14T22:07:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6083/
Test FAILed (JDK 7 and Scala 2.11).
"
315482697,2929,asfgit,2017-07-14T22:28:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6091/
Test PASSed (JDK 7 and Scala 2.11).
"
315485737,2929,asfgit,2017-07-14T22:50:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6076/
Test PASSed (JDK 8 and Scala 2.12).
"
315517505,2929,lindong28,2017-07-15T07:52:13Z,"@junrao This afternoon I reviewed the patch myself by going over the entire patch line by line. I improved the comments in a few places, removed unnecessary imports and reverted a few changes that have become unnecessary now. I sincerely think that I should have fixed these before your previous review.. 

For ease of review, I have squashed the commits before your previous review into one commit ([link](https://github.com/apache/kafka/pull/2929/commits/c34eef1d09a120a9ec104279a04824f80f3b1c04)). The second commit ([link](https://github.com/apache/kafka/pull/2929/commits/2ab2b89d230995efc488fb7e42f7a13946f5c5ff)) includes the change that were made to address your previous comments from the previous review. The summary of this change can be found in the earlier comment I posted yesterday. The 3rd commit ([link](https://github.com/apache/kafka/pull/2929/commits/811353e51a4b73dcb316073e55f15979bf005395)
) improved comment and reverted unnecessary changes.

Some major changes where made based on that previous two rounds of review from you, e.g. introduction of the LogDirFailureChannel and the consistency of KafkaStorageException in response. I am 95% confident now that the patch won't need similar major change anymore in its current state. I will give this patch another two rounds of end-to-end review, make sure that there is indeed no further improvement I can figure out by myself, before giving it to you for further review.

Thanks again for your time!
"
315519845,2929,asfgit,2017-07-15T08:40:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6093/
Test PASSed (JDK 7 and Scala 2.11).
"
315520940,2929,asfgit,2017-07-15T09:02:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6078/
Test PASSed (JDK 8 and Scala 2.12).
"
315590692,2929,lindong28,2017-07-16T07:14:33Z,"@junrao I gave the patch another round of review, made some minor improvements (e.g. comment) and fixed system test failure. The only notable change is that `KafkaStorageException` and `UnknownCodeException` now extends `InvalidMetadataException`. This is needed so that producer can update metadata if the leader replica is now offline.

These changes are in [this](https://github.com/apache/kafka/pull/2929/commits/d9c50554a8461c0e00b92e88184871953879b005) commit. I will give the patch the third review tomorrow."
315649448,2929,asfgit,2017-07-17T00:34:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6101/
Test PASSed (JDK 7 and Scala 2.11).
"
315650806,2929,asfgit,2017-07-17T00:55:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6086/
Test PASSed (JDK 8 and Scala 2.12).
"
315707331,2929,lindong28,2017-07-17T09:30:01Z,"@junrao I have carefully reviewed the patch three times and made all the improvements I can think of. All system tests and integration tests have passed. Would you have time to review the patch sometime this week? Thank you!

There is only one improvement that I haven't made in this patch. Currently the producer will still send ProduceRequest with version 3 if the message magic version is 2. This is because the newly-added produce version 4 is incompatible with 0.11 broker. The downside of this is that server will translate KafkaStorageException to NotLeaderForPartitionException. But this should not cause any problem to the functionality of the producer. We can fix this in a followup patch if needed."
315720721,2929,asfgit,2017-07-17T10:37:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6094/
Test PASSed (JDK 8 and Scala 2.12).
"
315982647,2929,lindong28,2017-07-18T07:34:12Z,@junrao Thanks so much for the quick and detailed review! I have addressed most of the comments. Please see my reply above.
315983822,2929,asfgit,2017-07-18T07:39:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6141/
Test FAILed (JDK 7 and Scala 2.11).
"
315985303,2929,asfgit,2017-07-18T07:46:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6126/
Test FAILed (JDK 8 and Scala 2.12).
"
316247280,2929,lindong28,2017-07-19T01:40:30Z,@junrao I have updated the patch and addressed all the comments. All tests have passed. I have reviewed the latest commit and it looks OK to me. I am reviewing the entire patch with all the commits.
316256221,2929,asfgit,2017-07-19T02:41:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6157/
Test PASSed (JDK 7 and Scala 2.11).
"
316258064,2929,asfgit,2017-07-19T02:54:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6141/
Test PASSed (JDK 8 and Scala 2.12).
"
316276230,2929,lindong28,2017-07-19T05:23:52Z,"@junrao Thanks much for all the help! I have rebased the patch onto trunk head and finished reviewing the patch. The following changes were made in the latest commit.

- Catch IOException before it is propagated to ReplicaManager
- Removed a few other IOException handling now that the IOException will be caught and thrown as KafkaStorageException in Log's methods.
- Halt broker on log directory failure if inter.broker.protocol < 0.11.1
- Removed UnknownErrorCodeException
- Updated upgrade.html
"
316284313,2929,asfgit,2017-07-19T06:21:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6159/
Test PASSed (JDK 7 and Scala 2.11).
"
316286526,2929,asfgit,2017-07-19T06:33:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6143/
Test PASSed (JDK 8 and Scala 2.12).
"
316905644,2929,asfgit,2017-07-21T05:19:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6238/
Test FAILed (JDK 7 and Scala 2.11).
"
316907147,2929,asfgit,2017-07-21T05:30:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6222/
Test FAILed (JDK 8 and Scala 2.12).
"
316925510,2929,asfgit,2017-07-21T07:22:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6243/
Test PASSed (JDK 7 and Scala 2.11).
"
316926503,2929,lindong28,2017-07-21T07:27:48Z,"@junrao Thanks much for the quick help! I have rebased the patch onto trunk head and addressed all the comments. In particular, I have further moved IOException from LogSegment to those methods in Log and updated the comments as suggested.

I have reviewed the entire patch again end-to-end and fixed everything to my best knowledge. It should be ready for your review again. Thanks!


"
316929195,2929,asfgit,2017-07-21T07:41:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6227/
Test PASSed (JDK 8 and Scala 2.12).
"
317064344,2929,lindong28,2017-07-21T17:35:06Z,"@junrao Thank you so much for taking so much time to review the patch. I definitely think the current patch is much more consistent in style and cleaner than it was before. Initially I was focusing only on the functionality. I will focus more on the code style and consistency next time, e.g. in KIP-113.

@becketqin I have addressed Jun's comments and rebased patch onto trunk head. All tests have passed. I will also review the patch again myself. Can you take a look at the patch? Thanks!"
317079133,2929,asfgit,2017-07-21T18:34:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6258/
Test PASSed (JDK 7 and Scala 2.11).
"
317083730,2929,asfgit,2017-07-21T18:52:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6242/
Test PASSed (JDK 8 and Scala 2.12).
"
317103017,2929,lindong28,2017-07-21T20:20:20Z,@becketqin I have finished reviewing the patch and made minor changes in the latest commit. I verified that the patch has passed all ducktape system tests.
317115352,2929,asfgit,2017-07-21T21:20:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6263/
Test PASSed (JDK 7 and Scala 2.11).
"
317117193,2929,asfgit,2017-07-21T21:30:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6247/
Test PASSed (JDK 8 and Scala 2.12).
"
317153705,2929,asfgit,2017-07-22T04:25:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6280/
Test PASSed (JDK 7 and Scala 2.11).
"
317154288,2929,asfgit,2017-07-22T04:35:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6264/
Test PASSed (JDK 8 and Scala 2.12).
"
317157179,2929,lindong28,2017-07-22T05:32:34Z,@becketqin Thanks much for your help! I have updated the patch to address all comments except those that can be done in the followup patch. And the patch has been rebased onto trunk head.
317160135,2929,asfgit,2017-07-22T06:30:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6282/
Test PASSed (JDK 7 and Scala 2.11).
"
317161568,2929,asfgit,2017-07-22T06:57:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6266/
Test PASSed (JDK 8 and Scala 2.12).
"
656589433,9001,dajac,2020-07-10T09:48:19Z,@kowshik I just noticed that you haven't updated the code which creates the `ApiVersionsResponse` in `SaslServerAuthenticator`. Is it intentional or something left to be done?
656902905,9001,kowshik,2020-07-10T21:37:40Z,"@dajac Thank you for taking a look! IIUC you are referring to these lines:

https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/security/authenticator/SaslServerAuthenticator.java#L550-L553

My requirement is that under the hood of the newly added API: `org.apache.kafka.clients.Admin#describeFeatures`, the `ApiVersionsResponse` returned to the `AdminClient` needs to contain the features information. Note that this new API issues an explicit `ApiVersionsRequest` under the hood. In such a case do you think I should populate the features information in the above lines in `SaslServerAuthenticator` too? I'm trying to understand where would this come into play (sorry I know little to nothing about `SaslServerAuthenticator`)."
659752993,9001,abbccdda,2020-07-17T00:27:00Z,retest this
659793027,9001,abbccdda,2020-07-17T02:19:38Z,retest this
659793614,9001,abbccdda,2020-07-17T02:21:09Z,retest this please
660241065,9001,abbccdda,2020-07-17T17:27:39Z,retest this please
660531259,9001,abbccdda,2020-07-18T19:34:14Z,"retest this please

"
665189199,9001,abbccdda,2020-07-28T18:01:36Z,retest this please
665213772,9001,abbccdda,2020-07-28T18:48:24Z,retest this please
665327603,9001,abbccdda,2020-07-28T22:31:44Z,retest this please
700585584,9001,kowshik,2020-09-29T09:31:43Z,"@junrao Thanks a lot for the review! I've addressed the comments in the recent commit: 06d8b47131f168db88e4f7d5bda3dd025ba9a2a2. I've provided a response to all of your comments. There are few I couldn't address, and 1-2 comments I'll address in the near future."
702023940,9001,kowshik,2020-10-01T09:52:21Z,"> Just looked through the change for firstActiveVersion, which makes sense. The only question I have is that if I'm doing a bold release by removing the deprecated code completely, such that previously I have version range [1, 6] and now the code only supports [4, 6] for a pair [min, max], would my deployment be successful automatically?

@abbccdda Thanks for the review! I've addressed the comments from your most recent pass in a7f4860f5f8bb87cfb01452e208ff8f4e45bcd8b. To answer your question, the deployment will fail if the feature was finalized at say `[minVersionLevel=1, maxVersionLevel=6]` previously, but the new broker only supports version range: `[minVersion=4, maxVersion=6]`. This is where `firstActiveVersion` becomes useful. By bumping it up during a release (instead of the supported feature's `minVersion`), we are able to get past this situation. When `firstActiveVersion` is advanced in the code, and the cluster is deployed, the controller (and all brokers) know that the advancement acts a request to the controller to act upon the feature deprecation (by writing the advanced value to `FeatureZNode`). So, in this case we would release the broker with the supported feature version range: `[minVersion=1, firstActiveVersion=4, maxVersion=6]`, and the deployment wouldn't fail.

"
702026788,9001,kowshik,2020-10-01T09:57:55Z,@junrao Thanks for the review! I've addressed the comments in c31d6b5245c635e659ff0f203bd08bc015a15ffb.
702621705,9001,kowshik,2020-10-02T09:21:02Z,@junrao Thanks for the review comments! I have done the change proposed in https://github.com/apache/kafka/pull/9001#discussion_r498574911 in the most recent commit: 4218f95904989028a469930d0c266362bf173ece . Please have a look.
703048608,9001,kowshik,2020-10-03T05:14:21Z,"@junrao Thanks for the review! I've addressed the comments, the PR is ready for another pass. I've also fixed the compilation errors."
703873486,9001,kowshik,2020-10-05T20:34:42Z,@junrao Thanks for the review! I've addressed the latest comments in e55358fd1a00f12ef98fc4d2d649a297ddf146da . The PR is ready for another pass.
704393633,9001,junrao,2020-10-06T16:21:21Z,@abbccdda : Any more comments from you?
704596581,9001,kowshik,2020-10-06T22:59:16Z,"@junrao The test failure in `MirrorConnectorsIntegrationTest.testReplication` does not seem related. I have rebased the PR now against latest AK trunk, I'd like to see if the failure happens again."
704786154,9001,kowshik,2020-10-07T08:40:13Z,"The test failures in the latest CI runs do not seem related to this PR:
 * JDK 8: `org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector`
 * JDK 11: `org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta`

The test that failed previously under JDK 15 has passed in the latest CI run: `MirrorConnectorsIntegrationTest.testReplication`."
705068372,9001,junrao,2020-10-07T16:59:34Z,"@kowshik : There are 27 system test failures with this PR. http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-10-07--001.1602079476--kowshik--kip584_features_write_path--e1c79cee2/report.html

Are they existing test failures compared with http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2020-10-07--001.1602079305--apache--trunk--af27c2ddf/report.html ?"
705073059,9001,kowshik,2020-10-07T17:07:49Z,"@junrao Thanks for the links! I had a look at the links. I found similar stats in both links, with exactly 27 test failures in both links. I compared the individual test failures and found that they have all failed on the same tests. Would that mean we are OK to merge this PR (since it doesn't seem to introduce a new failure)?"
705079102,9001,junrao,2020-10-07T17:18:59Z,"@kowshik : Thanks for following up. I will merged this PR as it is since the system test failures are not new. 

Also, in Scala, we prefer sealed traits over Enumeration since the former gives you exhaustiveness checking. With Scala enums, you don't get a warning if you add a new value that is not handled in a given pattern match. Maybe you can address that in your followup PR."
705378300,9001,chia7712,2020-10-08T07:10:05Z,@kowshik sorry for bringing trivial comments after this is merged. I just noticed those nits in testing new APIs in 2.7.0.
705419934,9001,kowshik,2020-10-08T08:35:51Z,"@chia7712 No worries, thanks for the suggestions! I have opened a separate PR addressing your comments. Would you be able to please review it? https://github.com/apache/kafka/pull/9393"
276579538,2476,asfbot,2017-02-01T06:01:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1398/
Test PASSed (JDK 8 and Scala 2.11).
"
276580352,2476,asfbot,2017-02-01T06:08:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1395/
Test PASSed (JDK 7 and Scala 2.10).
"
276590197,2476,asfbot,2017-02-01T07:26:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1395/
Test FAILed (JDK 8 and Scala 2.12).
"
276871714,2476,lindong28,2017-02-02T05:26:00Z,@ijuma @ewencp @jjkoshy @radai-rosenblatt I have manually tested it successfully using the `./bin/kafka-purge-data.sh` in the patch. I am going to add unit test and probably ducktape integration test as well. But the core code should be ready for review. Would you have time to review the patch?
276875007,2476,asfbot,2017-02-02T05:55:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1422/
Test PASSed (JDK 8 and Scala 2.11).
"
276875106,2476,asfbot,2017-02-02T05:56:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1419/
Test PASSed (JDK 7 and Scala 2.10).
"
276879992,2476,asfbot,2017-02-02T06:34:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1419/
Test PASSed (JDK 8 and Scala 2.12).
"
277101595,2476,ijuma,2017-02-02T22:13:55Z,"@lindong28, thanks for the PR. I probably won't have time to review before next week. cc @junrao as well since he reviewed the KIP."
279091164,2476,asfbot,2017-02-10T23:07:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1639/
Test FAILed (JDK 8 and Scala 2.11).
"
279097550,2476,asfbot,2017-02-10T23:48:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1640/
Test FAILed (JDK 8 and Scala 2.11).
"
279102471,2476,asfbot,2017-02-11T00:24:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1637/
Test FAILed (JDK 8 and Scala 2.12).
"
279108718,2476,asfbot,2017-02-11T01:18:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1637/
Test FAILed (JDK 7 and Scala 2.10).
"
279289301,2476,asfbot,2017-02-13T03:56:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1656/
Test FAILed (JDK 8 and Scala 2.11).
"
279289445,2476,lindong28,2017-02-13T03:58:27Z,@jjkoshy @junrao @becketqin @ijuma @radai-rosenblatt I have added tests and the patch is fully ready for review. Would you have time to review this patch?
279293687,2476,asfbot,2017-02-13T04:46:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1653/
Test PASSed (JDK 7 and Scala 2.10).
"
279297222,2476,asfbot,2017-02-13T05:24:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1653/
Test PASSed (JDK 8 and Scala 2.12).
"
279343352,2476,becketqin,2017-02-13T10:06:50Z,@lindong28 Thanks for the patch. It seems the patch has conflicts. Could you rebase?
279471864,2476,lindong28,2017-02-13T18:04:15Z,@becketqin I thought it will take 1+ week for the patch to be reviewed and there will be conflict again anyway. Thus I was going to rebase it after first round of review. What is our general guideline for rebasing big patches? I can certainly rebase it now if you think it is useful.
279524819,2476,lindong28,2017-02-13T21:16:17Z,@becketqin All conflicts have been resolved and all tests are passed. Thanks!
279525744,2476,asfbot,2017-02-13T21:19:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1664/
Test FAILed (JDK 7 and Scala 2.10).
"
279537509,2476,asfbot,2017-02-13T22:03:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1667/
Test PASSed (JDK 8 and Scala 2.11).
"
279537872,2476,asfbot,2017-02-13T22:04:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1664/
Test PASSed (JDK 8 and Scala 2.12).
"
279567997,2476,becketqin,2017-02-14T00:24:45Z,"@lindong28 Thanks for updating the patch. I'll take a look. Usually if there are multiple big patches in parallel, the committers who are reviewing the code would hold back some of the patches to avoid unnecessary rebase."
281270870,2476,lindong28,2017-02-21T07:59:37Z,@becketqin Thanks so much for taking time to review the patch! Can you check if the updated patch has addressed your comments?
281288418,2476,asfbot,2017-02-21T09:22:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1771/
Test FAILed (JDK 7 and Scala 2.10).
"
281298276,2476,asfbot,2017-02-21T10:02:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1774/
Test FAILed (JDK 8 and Scala 2.11).
"
281306318,2476,asfbot,2017-02-21T10:37:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1772/
Test FAILed (JDK 8 and Scala 2.12).
"
281421088,2476,asfbot,2017-02-21T17:47:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1783/
Test FAILed (JDK 8 and Scala 2.12).
"
281421124,2476,asfbot,2017-02-21T17:47:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1785/
Test FAILed (JDK 8 and Scala 2.11).
"
281421175,2476,asfbot,2017-02-21T17:47:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1782/
Test FAILed (JDK 7 and Scala 2.10).
"
281443750,2476,asfbot,2017-02-21T19:02:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1786/
Test FAILed (JDK 8 and Scala 2.11).
"
281443787,2476,asfbot,2017-02-21T19:02:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1784/
Test FAILed (JDK 8 and Scala 2.12).
"
281443869,2476,asfbot,2017-02-21T19:02:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1783/
Test FAILed (JDK 7 and Scala 2.10).
"
281471749,2476,asfbot,2017-02-21T20:34:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1785/
Test FAILed (JDK 7 and Scala 2.10).
"
281480052,2476,asfbot,2017-02-21T21:05:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1788/
Test FAILed (JDK 8 and Scala 2.11).
"
281495600,2476,asfbot,2017-02-21T22:00:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1786/
Test FAILed (JDK 8 and Scala 2.12).
"
281590614,2476,asfbot,2017-02-22T07:27:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1793/
Test PASSed (JDK 7 and Scala 2.10).
"
281590832,2476,asfbot,2017-02-22T07:28:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1794/
Test PASSed (JDK 8 and Scala 2.12).
"
281590833,2476,asfbot,2017-02-22T07:28:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1796/
Test PASSed (JDK 8 and Scala 2.11).
"
281594223,2476,lindong28,2017-02-22T07:48:30Z,@becketqin I have addressed all your comments and all tests have passed. Could you take another look? Thanks!
282936279,2476,becketqin,2017-02-28T04:05:51Z,@junrao @jjkoshy Do you have time to take a look? Given that we probably have a few big patches from KIP-98 (and potentially KIP-82 and KIP-112). We probably need to synchronize on them to avoid unnecessary rebase. It might be better to get this patch in first. What do you think? Thanks.
283208316,2476,asfbot,2017-03-01T00:43:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1902/
Test FAILed (JDK 8 and Scala 2.12).
"
283208367,2476,asfbot,2017-03-01T00:43:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1904/
Test FAILed (JDK 8 and Scala 2.11).
"
283208410,2476,asfbot,2017-03-01T00:43:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1901/
Test FAILed (JDK 7 and Scala 2.10).
"
283224891,2476,asfbot,2017-03-01T02:20:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1912/
Test FAILed (JDK 8 and Scala 2.11).
"
283225152,2476,asfbot,2017-03-01T02:21:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1909/
Test FAILed (JDK 7 and Scala 2.10).
"
283233253,2476,asfbot,2017-03-01T03:17:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1910/
Test FAILed (JDK 8 and Scala 2.12).
"
283233279,2476,asfbot,2017-03-01T03:18:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1915/
Test FAILed (JDK 8 and Scala 2.11).
"
283234912,2476,asfbot,2017-03-01T03:30:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1912/
Test FAILed (JDK 7 and Scala 2.10).
"
283235029,2476,asfbot,2017-03-01T03:31:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1913/
Test FAILed (JDK 8 and Scala 2.12).
"
283340571,2476,ijuma,2017-03-01T13:33:25Z,"Also, it would be good to merge trunk into this branch. I know this can be annoying, but the request/response code has changed enough that some parts of the review can't be done properly without that."
283426390,2476,lindong28,2017-03-01T18:28:24Z,"@ijuma Thanks for reviewing the patch. This KIP depends on the KAFKA-4820 and I will rebase the patch after KAFKA-4820 is committed. And I will also explain the PurgeDataCommand in the mailing thread and the KIP wiki.

Initially we considered to use purge, truncate or delete in the method name. I don't have a strong preference among them. I chose purge because Joel/Becket prefers it and no one else raises concerns with it in the open source discussion. I think one reason is to distinguish it from `deleteTopic` method. Now that we have `data` in the name, I agree that it may be better to use `deleteDataBefore`. Let me propose it in the open source mailing list.

I think the purpose of including `data` in the method name is to distinguish it from `deleteTopic` method name. The reason of not having `data` in the API name PurgeRequest is because `ProduceRequest` or `FetchRequest` doesn't include `data` in the name.

The reason of including `before` in the method name is because we may want to have a method to delete data after a given offset in the future."
283438149,2476,ijuma,2017-03-01T19:09:53Z,Thanks for explaining the reasoning for the name choices. Let me think about it and see how it fits with the other proposed AdminClient methods.
284215260,2476,asfbot,2017-03-05T09:11:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2007/
Test PASSed (JDK 8 and Scala 2.11).
"
284215306,2476,asfbot,2017-03-05T09:11:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2004/
Test PASSed (JDK 7 and Scala 2.10).
"
284215633,2476,lindong28,2017-03-05T09:17:15Z,@ijuma @becketqin I have addressed all the comments and rebased the patch onto trunk. Do you have time to review the latest patch?
284216926,2476,asfbot,2017-03-05T09:46:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2005/
Test FAILed (JDK 8 and Scala 2.12).
"
284318420,2476,asfbot,2017-03-06T07:06:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2024/
Test PASSed (JDK 8 and Scala 2.11).
"
284318543,2476,asfbot,2017-03-06T07:07:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2021/
Test PASSed (JDK 7 and Scala 2.10).
"
284323091,2476,asfbot,2017-03-06T07:38:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2022/
Test FAILed (JDK 8 and Scala 2.12).
"
284937211,2476,asfbot,2017-03-08T03:36:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2066/
Test FAILed (JDK 8 and Scala 2.11).
"
284937428,2476,asfbot,2017-03-08T03:37:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2063/
Test PASSed (JDK 7 and Scala 2.10).
"
284942442,2476,asfbot,2017-03-08T04:17:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2064/
Test FAILed (JDK 8 and Scala 2.12).
"
284963025,2476,lindong28,2017-03-08T06:56:21Z,@junrao Thanks much for your review. I have updated the patch to address most of the comments. Can you see if my reply to your comments make sense?
284973750,2476,asfbot,2017-03-08T08:04:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2071/
Test FAILed (JDK 8 and Scala 2.11).
"
284973903,2476,asfbot,2017-03-08T08:05:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2068/
Test PASSed (JDK 7 and Scala 2.10).
"
284974463,2476,asfbot,2017-03-08T08:08:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2069/
Test PASSed (JDK 8 and Scala 2.12).
"
285200046,2476,asfbot,2017-03-08T23:08:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2085/
Test FAILed (JDK 7 and Scala 2.10).
"
285200251,2476,asfbot,2017-03-08T23:09:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2088/
Test FAILed (JDK 8 and Scala 2.11).
"
285200301,2476,asfbot,2017-03-08T23:09:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2086/
Test FAILed (JDK 8 and Scala 2.12).
"
285213163,2476,asfbot,2017-03-09T00:16:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2091/
Test PASSed (JDK 8 and Scala 2.12).
"
285216306,2476,asfbot,2017-03-09T00:34:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2090/
Test FAILed (JDK 7 and Scala 2.10).
"
285220538,2476,asfbot,2017-03-09T00:58:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2093/
Test PASSed (JDK 8 and Scala 2.11).
"
285518268,2476,lindong28,2017-03-09T23:29:55Z,@ijuma @becketqin @junrao Do you have time to give the patch another pass? I have gong through the patch and all the reviews and the patch seems good now.
285582265,2476,junrao,2017-03-10T05:31:15Z,@lindong28 : Have you run system tests on this PR?
285972512,2476,lindong28,2017-03-12T20:13:07Z,@junrao Sorry for late reply. I didn't run Ducktape-based system tests previously. I have been trying to run `tests/kafkatest` since your comment. But it seems that some tests will fail even on Kafka trunk without my patch. I am doing more detailed analysis to see if my patch is the cause of any test failure.
286312386,2476,lindong28,2017-03-14T03:34:02Z,@junrao Some tests fail consistently even without my patch if I run ducktape-based system test on my desktop. I am not sure if it is because the setup on my desktop. Becket just told me that it may be more reliable to run system test via https://jenkins.confluent.io/job/system-test-kafka-branch-builder/. But all recent system tests have failed due to error `ImportError: No module named setuptools` since last Friday. Do you know who can look into this problem and fix the Jenkins setup?
286471347,2476,junrao,2017-03-14T16:09:04Z,@lindong28 : Sorry for the inconvenience. We just fixed an issue in the branch builder. Could you try it again?
286545382,2476,lindong28,2017-03-14T20:13:53Z,@junrao Thanks much for the quick help! Please ignore my previous comment.. I thought my test has succeeded but I was looking at the wrong test result. It should take another 3 hours for my test to finish.
286556930,2476,asfbot,2017-03-14T20:55:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2173/
Test PASSed (JDK 7 and Scala 2.10).
"
286560968,2476,lindong28,2017-03-14T21:07:54Z,"@junrao I noticed that in a test I started yesterday using `system-test-kafka-branch-builder-2` (instead of `system-test-kafka-branch-builder`), my branch has indeed passed all tests. See https://jenkins.confluent.io/job/system-test-kafka-branch-builder-2/201/console.

The patch has been rebased onto trunk without any conflict and `nextOffsetFromLog()` has been made private. Do we need further review or test? Is there anything else I can do?

Thanks for help!"
286567782,2476,asfbot,2017-03-14T21:33:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2174/
Test PASSed (JDK 8 and Scala 2.12).
"
286567825,2476,asfbot,2017-03-14T21:33:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2176/
Test FAILed (JDK 8 and Scala 2.11).
"
286928771,2476,lindong28,2017-03-16T01:15:34Z,"@ijuma @junrao @becketqin I have renamed `PurgeRequest` to `DeleteRecordsRequest`, `PurgeResonse` to `DeleteRecordsResponse`, and `purgeDataBefore()` to `deleteRecordsBefore()`. 

For simplicity and consistency of internal code, I am still using `purge` to refer to `deleteRecords` when `records` is not explicitly specified as the operation target. For example, I use `LogPurgeResult` instead of `LogDeleteResult` or `LogDeleteRecordsResult`. And `delayedPurgePurgatory` instead of `delayedDeletePurgatory` or `delayedDeleteRecordsPurgatory`. This should address Jun's goal of differentiating between removing whole log vs removing portion of the log since we only use `delete` when `records` is specified.

Can you review this patch again? Thanks!
"
286934887,2476,asfbot,2017-03-16T01:56:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2208/
Test PASSed (JDK 7 and Scala 2.10).
"
286935115,2476,asfbot,2017-03-16T01:57:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2211/
Test PASSed (JDK 8 and Scala 2.11).
"
286940468,2476,asfbot,2017-03-16T02:33:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2209/
Test PASSed (JDK 8 and Scala 2.12).
"
287198169,2476,asfbot,2017-03-16T21:34:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2237/
Test PASSed (JDK 8 and Scala 2.11).
"
287198230,2476,asfbot,2017-03-16T21:35:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2234/
Test PASSed (JDK 7 and Scala 2.10).
"
287206306,2476,asfbot,2017-03-16T22:09:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2235/
Test FAILed (JDK 8 and Scala 2.12).
"
287282133,2476,asfbot,2017-03-17T07:02:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2241/
Test PASSed (JDK 8 and Scala 2.12).
"
287282233,2476,asfbot,2017-03-17T07:03:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2244/
Test PASSed (JDK 8 and Scala 2.11).
"
287282565,2476,asfbot,2017-03-17T07:05:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2241/
Test PASSed (JDK 7 and Scala 2.10).
"
287506966,2476,becketqin,2017-03-18T01:28:42Z,Thanks for the patch. LGTM except one pending comments. @junrao Do you have time to take another look? Thanks.
287512495,2476,asfbot,2017-03-18T03:14:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2256/
Test PASSed (JDK 7 and Scala 2.10).
"
287512616,2476,asfbot,2017-03-18T03:17:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2259/
Test PASSed (JDK 8 and Scala 2.11).
"
287513952,2476,asfbot,2017-03-18T03:48:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2256/
Test PASSed (JDK 8 and Scala 2.12).
"
287695596,2476,lindong28,2017-03-20T08:02:37Z,@junrao Thanks for the comment! I have addressed most of them. Can you check if my reply make sense?
287701236,2476,asfbot,2017-03-20T08:42:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2271/
Test FAILed (JDK 8 and Scala 2.11).
"
287701287,2476,asfbot,2017-03-20T08:42:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2268/
Test FAILed (JDK 7 and Scala 2.10).
"
287707047,2476,asfbot,2017-03-20T09:13:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2268/
Test FAILed (JDK 8 and Scala 2.12).
"
287843203,2476,asfbot,2017-03-20T17:53:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2281/
Test FAILed (JDK 8 and Scala 2.12).
"
287843916,2476,asfbot,2017-03-20T17:55:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2281/
Test FAILed (JDK 7 and Scala 2.10).
"
287845169,2476,asfbot,2017-03-20T17:59:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2284/
Test FAILed (JDK 8 and Scala 2.11).
"
287913011,2476,asfbot,2017-03-20T22:06:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2288/
Test PASSed (JDK 7 and Scala 2.10).
"
287913261,2476,asfbot,2017-03-20T22:07:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2291/
Test PASSed (JDK 8 and Scala 2.11).
"
287913462,2476,asfbot,2017-03-20T22:08:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2288/
Test PASSed (JDK 8 and Scala 2.12).
"
287960774,2476,asfbot,2017-03-21T02:47:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2294/
Test PASSed (JDK 8 and Scala 2.11).
"
287962323,2476,asfbot,2017-03-21T02:57:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2291/
Test PASSed (JDK 7 and Scala 2.10).
"
287977859,2476,asfbot,2017-03-21T05:03:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2291/
Test FAILed (JDK 8 and Scala 2.12).
"
288245432,2476,asfbot,2017-03-21T22:58:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2310/
Test PASSed (JDK 7 and Scala 2.10).
"
288254818,2476,asfbot,2017-03-21T23:47:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2310/
Test PASSed (JDK 8 and Scala 2.12).
"
288268388,2476,asfbot,2017-03-22T01:12:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2314/
Test FAILed (JDK 8 and Scala 2.11).
"
288525644,2476,lindong28,2017-03-22T20:17:33Z,Test with JDK 8 and Scala 2.11 failed because `KafkaStreamsTest.testCannotStartOnceClosed` took more 2+ hours before it is killed. `./gradlew -PscalaVersion=2.11 streams:test --tests org.apache.kafka.streams.KafkaStreamsTest` works on my machine. Thus the failure is probably not caused by this patch. All system tests have passed. See https://jenkins.confluent.io/job/system-test-kafka-branch-builder-2/210.
288540026,2476,asfbot,2017-03-22T21:07:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2333/
Test FAILed (JDK 8 and Scala 2.12).
"
288543041,2476,asfbot,2017-03-22T21:19:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2333/
Test PASSed (JDK 7 and Scala 2.10).
"
288549149,2476,asfbot,2017-03-22T21:42:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2337/
Test FAILed (JDK 8 and Scala 2.11).
"
288558944,2476,asfbot,2017-03-22T22:25:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2337/
Test PASSed (JDK 7 and Scala 2.10).
"
288559739,2476,asfbot,2017-03-22T22:29:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2337/
Test PASSed (JDK 8 and Scala 2.12).
"
288560216,2476,asfbot,2017-03-22T22:31:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2341/
Test PASSed (JDK 8 and Scala 2.11).
"
288563254,2476,lindong28,2017-03-22T22:46:29Z,"@junrao: All comments are addressed. All integration tests and system tests have passed. And @becketqin has reviewed the latest patch. Since we are so close to finish review for this patch, would you have time to review the patch again so that we can merge it before other major change in trunk?"
288900741,2476,asfbot,2017-03-24T00:34:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2360/
Test FAILed (JDK 7 and Scala 2.10).
"
288901392,2476,asfbot,2017-03-24T00:38:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2364/
Test PASSed (JDK 8 and Scala 2.11).
"
288905108,2476,asfbot,2017-03-24T01:06:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2360/
Test FAILed (JDK 8 and Scala 2.12).
"
288922946,2476,asfbot,2017-03-24T03:20:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2366/
Test PASSed (JDK 8 and Scala 2.11).
"
288923076,2476,asfbot,2017-03-24T03:21:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2362/
Test PASSed (JDK 7 and Scala 2.10).
"
288926876,2476,asfbot,2017-03-24T03:53:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2362/
Test FAILed (JDK 8 and Scala 2.12).
"
288931216,2476,lindong28,2017-03-24T04:34:15Z,"@junrao All comments have been addressed as suggested. All tests have passed except `org.apache.kafka.streams.integration.KStreamAggregationDedupIntegrationTest.shouldReduce` with Scala 2.12, which I don't think is caused by this patch. Thanks for all your time to review the patch!"
289166060,2476,asfbot,2017-03-24T23:23:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2372/
Test FAILed (JDK 8 and Scala 2.12).
"
289166451,2476,asfbot,2017-03-24T23:25:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2376/
Test PASSed (JDK 8 and Scala 2.11).
"
289166936,2476,asfbot,2017-03-24T23:29:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2372/
Test PASSed (JDK 7 and Scala 2.10).
"
289188244,2476,lindong28,2017-03-25T04:32:33Z,"@junrao Thanks for the review. Yeah it took me quite a while to rebase the patch. I just finished rebase and the integration tests have passed. I will run system tests as well. Because it is very time consuming to rebase each single commit. I end up squash the all commits into one commit onto trunk.

Can you see if my reply to your most recent comment make sense?"
289189910,2476,asfbot,2017-03-25T05:17:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2377/
Test PASSed (JDK 7 and Scala 2.10).
"
289190036,2476,asfbot,2017-03-25T05:20:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2381/
Test FAILed (JDK 8 and Scala 2.11).
"
289191287,2476,asfbot,2017-03-25T05:54:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2377/
Test PASSed (JDK 8 and Scala 2.12).
"
289324325,2476,asfbot,2017-03-26T23:10:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2394/
Test FAILed (JDK 8 and Scala 2.12).
"
289324328,2476,asfbot,2017-03-26T23:10:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2398/
Test FAILed (JDK 8 and Scala 2.11).
"
289324343,2476,asfbot,2017-03-26T23:11:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2394/
Test FAILed (JDK 7 and Scala 2.10).
"
289325288,2476,asfbot,2017-03-26T23:27:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2399/
Test FAILed (JDK 8 and Scala 2.11).
"
289325301,2476,asfbot,2017-03-26T23:27:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2395/
Test FAILed (JDK 8 and Scala 2.12).
"
289325347,2476,asfbot,2017-03-26T23:28:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2395/
Test FAILed (JDK 7 and Scala 2.10).
"
289328563,2476,asfbot,2017-03-27T00:22:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2396/
Test FAILed (JDK 8 and Scala 2.12).
"
289328564,2476,asfbot,2017-03-27T00:22:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2400/
Test FAILed (JDK 8 and Scala 2.11).
"
289328588,2476,asfbot,2017-03-27T00:23:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2396/
Test FAILed (JDK 7 and Scala 2.10).
"
289328896,2476,asfbot,2017-03-27T00:27:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2397/
Test FAILed (JDK 7 and Scala 2.10).
"
289328902,2476,asfbot,2017-03-27T00:27:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2397/
Test FAILed (JDK 8 and Scala 2.12).
"
289328917,2476,asfbot,2017-03-27T00:28:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2401/
Test FAILed (JDK 8 and Scala 2.11).
"
289332818,2476,asfbot,2017-03-27T01:20:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2398/
Test PASSed (JDK 7 and Scala 2.10).
"
289336469,2476,asfbot,2017-03-27T02:00:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2398/
Test PASSed (JDK 8 and Scala 2.12).
"
289336490,2476,asfbot,2017-03-27T02:00:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2402/
Test PASSed (JDK 8 and Scala 2.11).
"
289336913,2476,lindong28,2017-03-27T02:05:10Z,@junrao @becketqin The patch has been rebased again on Ismael's most recent commit. All integration and system tests have passed. Can you review the latest patch? Thanks!
289613754,2476,lindong28,2017-03-27T23:13:06Z,"@junrao As of current patch the logStartOffset of compacted topic is always 0. It seems reasonable because if consumer seek to offset 0 of a compacted topic, they will still be able to consume (instead of receiving OffsetOutofRangeException) even if the offset of the first message is larger than 0. And it is not straightforward to retrieve the offset of the first message in the segment. According to Java doc, `RecordBatch.baseOffset()` will `return the first offset of the original record batch for magic version prior to 2`, which means the baseOffset of the first batch of the first log segment doesn't necessarily tell us the offset of the first message in the log. Does this sound reasonable?"
289622198,2476,asfbot,2017-03-28T00:04:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2432/
Test PASSed (JDK 8 and Scala 2.12).
"
289622215,2476,asfbot,2017-03-28T00:04:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2432/
Test FAILed (JDK 7 and Scala 2.10).
"
289623760,2476,asfbot,2017-03-28T00:15:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2436/
Test PASSed (JDK 8 and Scala 2.11).
"
289629870,2476,asfbot,2017-03-28T00:54:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2433/
Test FAILed (JDK 8 and Scala 2.12).
"
289631618,2476,asfbot,2017-03-28T01:05:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2433/
Test PASSed (JDK 7 and Scala 2.10).
"
289636897,2476,asfbot,2017-03-28T01:40:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2437/
Test PASSed (JDK 8 and Scala 2.11).
"
289637692,2476,asfbot,2017-03-28T01:45:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2440/
Test PASSed (JDK 8 and Scala 2.11).
"
289638194,2476,asfbot,2017-03-28T01:48:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2436/
Test PASSed (JDK 7 and Scala 2.10).
"
289638552,2476,asfbot,2017-03-28T01:51:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2436/
Test PASSed (JDK 8 and Scala 2.12).
"
289655962,2476,junrao,2017-03-28T03:56:11Z,"It seems that right now, for a compacted topic, the base offset of the first segment is always 0. So, the patch is fine. "
289656024,2476,junrao,2017-03-28T03:56:48Z,@lindong28 : Thanks for the patch. LGTM. @becketqin : Do you want to make another pass and then merge?
289671232,2476,asfbot,2017-03-28T05:56:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2443/
Test PASSed (JDK 8 and Scala 2.11).
"
289671312,2476,asfbot,2017-03-28T05:57:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2439/
Test PASSed (JDK 7 and Scala 2.10).
"
289675529,2476,asfbot,2017-03-28T06:25:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2439/
Test FAILed (JDK 8 and Scala 2.12).
"
289698691,2476,asfbot,2017-03-28T08:24:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2444/
Test FAILed (JDK 8 and Scala 2.11).
"
289725612,2476,asfbot,2017-03-28T10:14:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2440/
Test FAILed (JDK 8 and Scala 2.12).
"
289725648,2476,asfbot,2017-03-28T10:15:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2440/
Test FAILed (JDK 7 and Scala 2.10).
"
289775450,2476,asfbot,2017-03-28T13:49:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2456/
Test PASSed (JDK 7 and Scala 2.10).
"
289776010,2476,asfbot,2017-03-28T13:51:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2456/
Test PASSed (JDK 8 and Scala 2.12).
"
289788471,2476,asfbot,2017-03-28T14:29:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2460/
Test FAILed (JDK 8 and Scala 2.11).
"
289827754,2476,lindong28,2017-03-28T16:32:15Z,@junrao @becketqin All integration tests have passed except `org.apache.kafka.connect.runtime.WorkerTest.testAddRemoveTask` with Scala 2.11. I don't think this is caused by this patch because this test passed all other recent tests. Thanks so much for taking time to review this patch!
289838515,2476,becketqin,2017-03-28T17:08:36Z,"@lindong28 Thanks for updating the patch. Merged to trunk.
@junrao @ijuma Thanks for the review!"
283204521,2614,hachikuji,2017-03-01T00:21:50Z,"cc @ijuma @junrao 

A couple initial points of discussion:

1. Message class hierarchy. The `LogEntry` interface now represents the record set and `LogRecord` represents the record. I have left `Records` with its current name which is a bit weird when you need to write things like `records.entries.records`. I have also been thinking of changing `LogEntry` to `LogRecordSet`, which would make the `Records` name a bit more intuitive. Previously I considered changing `Records` to simply `Log`, though that's a little annoying because of the server class with the same name.
2. The new magic version is implemented in `EosLogEntry` and `EosLogRecord`. This name should be changed of course. Any suggestions? The old magic version is implemented in `OldLogEntry`, so we could call it `NewLogEntry`?
"
283206452,2614,asfbot,2017-03-01T00:32:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1902/
Test PASSed (JDK 8 and Scala 2.11).
"
283206597,2614,asfbot,2017-03-01T00:32:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1899/
Test PASSed (JDK 7 and Scala 2.10).
"
283206965,2614,asfbot,2017-03-01T00:35:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1900/
Test PASSed (JDK 8 and Scala 2.12).
"
283211158,2614,asfbot,2017-03-01T00:58:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1904/
Test PASSed (JDK 7 and Scala 2.10).
"
283211533,2614,asfbot,2017-03-01T01:00:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1905/
Test PASSed (JDK 8 and Scala 2.12).
"
283211631,2614,asfbot,2017-03-01T01:01:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1907/
Test FAILed (JDK 8 and Scala 2.11).
"
283401129,2614,hachikuji,2017-03-01T17:00:01Z,"FYI: System tests run here: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2017-03-01--001.1488358155--confluentinc--exactly-once-message-format--c2a14f8/report.html. The one failure looks like a known problem, but I haven't investigated."
283982801,2614,ijuma,2017-03-03T15:24:57Z,"@hachikuji, good questions!

1. I don't like the `Set` suffix because order is relevant, so if we wanted to go that way, I think it should be `LogRecordsList`. Which is kind of just `LogRecords`, which is confusing given that it's part of `Records`. Is record set just a record batch?
2. Instead of `NewLogEntry`, should it be `DefaultLogEntry`? And `OldLogEntry` could be `LegacyLogEntry` or something like that."
284025203,2614,hachikuji,2017-03-03T18:00:04Z,"@ijuma Thanks for the suggestions. 

1. It is a batch. The producer already uses `RecordBatch`, but maybe we could commandeer that name? I'm not too fond of `LogRecordsList`. It's a bit tough to come up with something reasonable given the name of `Records`. I'd rename that personally to something which conveyed the fact that it was a sequence of bytes of the log (but of course I already tried that and failed).

2. `DefaultLogEntry` sounds good to me. No strong preference between `OldLogEntry` and `LegacyLogEntry`. But perhaps ""legacy"" is a bit more suggestive of its use.

The other thing I wanted to mention is that I've left around the old `Record` class more or less as it currently is. This is why I needed to introduce the `LogRecord` interface. This seemed fine given use of the ""log"" prefix in `LogEntry`, but I've considered several times moving the current `Record` into `OldLogEntry`, and then using `Record` in place of `LogRecord`. What do you think?"
284029702,2614,ijuma,2017-03-03T18:18:21Z,"I'm not fond of `LogRecordsList` either, in case it was not clear from my message. :) I was thinking that maybe we could rename the existing `RecordBatch` to `ProducerBatch` or something.

Yes, I prefer `Legacy` a bit for the reason you state.

I think it makes sense to rename `LogRecord` to `Record`. The current Record could then either be moved to `LegacyLogEntry` (already using this name ;)) or it could just be `LegacyRecord`."
284031096,2614,asfbot,2017-03-03T18:24:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1981/
Test PASSed (JDK 8 and Scala 2.11).
"
284031945,2614,hachikuji,2017-03-03T18:27:27Z,"So if we follow those suggestions, then we have the following hierarchy:
```
Records -> [RecordBatch]
RecordBatch -> [Record]
```
That seems reasonable to me. The `RecordBatch` -> `ProducerBatch` renaming will probably cause some confusion, but people will get over it."
284033990,2614,ijuma,2017-03-03T18:35:46Z,"@junrao, what do you think?"
284039410,2614,asfbot,2017-03-03T18:56:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1979/
Test FAILed (JDK 8 and Scala 2.12).
"
284064563,2614,asfbot,2017-03-03T20:40:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1978/
Test FAILed (JDK 7 and Scala 2.10).
"
284537436,2614,junrao,2017-03-06T21:24:23Z,"A few other high level comments.
1. In the EOS message format, it's probably worth considering including the message count in the message format. This has the benefit that the consumer can allocate the right size of the array to store those messages and is also consistent with how we represent an array in other places.

2. About EosLogEntry and OldLogEntry. Perhaps we can name them based on the magic. So, it would be V2LogEntry, V1LogEntry and V0LogEntry.
"
284557074,2614,ijuma,2017-03-06T22:36:50Z,"@junrao, about including the message format version as a prefix or suffix. I considered that, but the issue is that more than one version is supported by each class. Unless we want to move away from that, it seems a bit awkward."
285238048,2614,asfbot,2017-03-09T02:42:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2099/
Test PASSed (JDK 7 and Scala 2.10).
"
285239097,2614,asfbot,2017-03-09T02:48:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2102/
Test PASSed (JDK 8 and Scala 2.11).
"
285239215,2614,asfbot,2017-03-09T02:49:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2100/
Test PASSed (JDK 8 and Scala 2.12).
"
285763374,2614,asfbot,2017-03-10T19:33:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2115/
Test FAILed (JDK 7 and Scala 2.10).
"
285773511,2614,asfbot,2017-03-10T20:16:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2116/
Test PASSed (JDK 8 and Scala 2.12).
"
285802765,2614,asfbot,2017-03-10T22:29:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2118/
Test FAILed (JDK 8 and Scala 2.11).
"
285830649,2614,asfbot,2017-03-11T01:53:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2125/
Test PASSed (JDK 8 and Scala 2.11).
"
285830670,2614,asfbot,2017-03-11T01:54:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2122/
Test PASSed (JDK 7 and Scala 2.10).
"
285834702,2614,asfbot,2017-03-11T02:24:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2123/
Test FAILed (JDK 8 and Scala 2.12).
"
285983415,2614,junrao,2017-03-12T22:44:20Z,"@hachikuji : Currently, the broker supports a DebuggingConsumerId mode for the fetch request. Should we extend that so that the consumer can read the control message as well? Should we also have some kind of DebuggingMessageFormatter so that ConsoleConsumer can show all the newly introduced fields in the new message format (e.g., pid, epoch, etc) for debugging purpose? Both can be done in a followup patch if needed. "
286201099,2614,asfbot,2017-03-13T18:32:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2147/
Test PASSed (JDK 8 and Scala 2.12).
"
286201513,2614,asfbot,2017-03-13T18:33:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2149/
Test PASSed (JDK 8 and Scala 2.11).
"
286202887,2614,asfbot,2017-03-13T18:38:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2146/
Test PASSed (JDK 7 and Scala 2.10).
"
286206535,2614,asfbot,2017-03-13T18:50:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2151/
Test FAILed (JDK 8 and Scala 2.11).
"
286206839,2614,asfbot,2017-03-13T18:51:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2149/
Test FAILed (JDK 8 and Scala 2.12).
"
286207172,2614,asfbot,2017-03-13T18:52:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2148/
Test FAILed (JDK 7 and Scala 2.10).
"
286211638,2614,asfbot,2017-03-13T19:08:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2150/
Test FAILed (JDK 8 and Scala 2.12).
"
286211650,2614,asfbot,2017-03-13T19:08:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2152/
Test FAILed (JDK 8 and Scala 2.11).
"
286211655,2614,asfbot,2017-03-13T19:08:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2149/
Test FAILed (JDK 7 and Scala 2.10).
"
286218660,2614,asfbot,2017-03-13T19:34:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2151/
Test FAILed (JDK 8 and Scala 2.12).
"
286218693,2614,asfbot,2017-03-13T19:35:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2150/
Test FAILed (JDK 7 and Scala 2.10).
"
286218696,2614,asfbot,2017-03-13T19:35:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2153/
Test FAILed (JDK 8 and Scala 2.11).
"
286223327,2614,asfbot,2017-03-13T19:51:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2151/
Test FAILed (JDK 7 and Scala 2.10).
"
286223373,2614,asfbot,2017-03-13T19:52:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2152/
Test FAILed (JDK 8 and Scala 2.12).
"
286223377,2614,asfbot,2017-03-13T19:52:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2154/
Test FAILed (JDK 8 and Scala 2.11).
"
286261624,2614,asfbot,2017-03-13T22:19:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2156/
Test PASSed (JDK 8 and Scala 2.11).
"
286261907,2614,asfbot,2017-03-13T22:20:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2153/
Test PASSed (JDK 7 and Scala 2.10).
"
286268196,2614,asfbot,2017-03-13T22:50:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2154/
Test FAILed (JDK 8 and Scala 2.12).
"
286282817,2614,asfbot,2017-03-14T00:11:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2161/
Test PASSed (JDK 8 and Scala 2.11).
"
286282970,2614,asfbot,2017-03-14T00:12:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2158/
Test PASSed (JDK 7 and Scala 2.10).
"
286287973,2614,asfbot,2017-03-14T00:44:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2159/
Test FAILed (JDK 8 and Scala 2.12).
"
286305325,2614,asfbot,2017-03-14T02:42:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2163/
Test PASSed (JDK 8 and Scala 2.11).
"
286305405,2614,asfbot,2017-03-14T02:42:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2160/
Test PASSed (JDK 7 and Scala 2.10).
"
286310127,2614,asfbot,2017-03-14T03:16:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2161/
Test FAILed (JDK 8 and Scala 2.12).
"
286533462,2614,asfbot,2017-03-14T19:27:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2170/
Test PASSed (JDK 7 and Scala 2.10).
"
286542161,2614,asfbot,2017-03-14T20:01:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2173/
Test FAILed (JDK 8 and Scala 2.11).
"
286542213,2614,asfbot,2017-03-14T20:01:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2171/
Test FAILed (JDK 8 and Scala 2.12).
"
286587766,2614,asfbot,2017-03-14T22:57:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2184/
Test PASSed (JDK 8 and Scala 2.12).
"
286588258,2614,asfbot,2017-03-14T23:00:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2183/
Test PASSed (JDK 7 and Scala 2.10).
"
286595131,2614,asfbot,2017-03-14T23:33:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2186/
Test FAILed (JDK 8 and Scala 2.11).
"
286815104,2614,junrao,2017-03-15T17:19:08Z,"@hachikuji : Also, it would be great if you could do some basic perf test to make sure there is no noticeable performance degradation. "
286856183,2614,asfbot,2017-03-15T19:41:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2202/
Test PASSed (JDK 8 and Scala 2.11).
"
286859897,2614,asfbot,2017-03-15T19:55:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2199/
Test PASSed (JDK 7 and Scala 2.10).
"
286864642,2614,asfbot,2017-03-15T20:13:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2200/
Test PASSed (JDK 8 and Scala 2.12).
"
286933208,2614,asfbot,2017-03-16T01:45:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2209/
Test PASSed (JDK 8 and Scala 2.11).
"
286933417,2614,asfbot,2017-03-16T01:46:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2206/
Test PASSed (JDK 7 and Scala 2.10).
"
286937576,2614,asfbot,2017-03-16T02:13:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2207/
Test FAILed (JDK 8 and Scala 2.12).
"
286974558,2614,asfbot,2017-03-16T07:05:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2213/
Test PASSed (JDK 8 and Scala 2.11).
"
286974613,2614,asfbot,2017-03-16T07:05:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2211/
Test PASSed (JDK 8 and Scala 2.12).
"
286974769,2614,asfbot,2017-03-16T07:06:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2210/
Test PASSed (JDK 7 and Scala 2.10).
"
287116360,2614,asfbot,2017-03-16T16:38:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2225/
Test FAILed (JDK 7 and Scala 2.10).
"
287129465,2614,asfbot,2017-03-16T17:20:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2226/
Test PASSed (JDK 8 and Scala 2.12).
"
287131707,2614,asfbot,2017-03-16T17:27:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2228/
Test PASSed (JDK 8 and Scala 2.11).
"
287135226,2614,asfbot,2017-03-16T17:39:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2232/
Test PASSed (JDK 8 and Scala 2.11).
"
287136445,2614,asfbot,2017-03-16T17:43:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2230/
Test PASSed (JDK 8 and Scala 2.12).
"
287137238,2614,asfbot,2017-03-16T17:45:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2229/
Test PASSed (JDK 7 and Scala 2.10).
"
287183239,2614,asfbot,2017-03-16T20:35:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2236/
Test PASSed (JDK 8 and Scala 2.11).
"
287183656,2614,asfbot,2017-03-16T20:37:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2233/
Test PASSed (JDK 7 and Scala 2.10).
"
287190715,2614,asfbot,2017-03-16T21:05:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2234/
Test FAILed (JDK 8 and Scala 2.12).
"
287417341,2614,asfbot,2017-03-17T17:20:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2254/
Test PASSed (JDK 8 and Scala 2.11).
"
287417750,2614,asfbot,2017-03-17T17:21:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2251/
Test PASSed (JDK 7 and Scala 2.10).
"
287426693,2614,asfbot,2017-03-17T17:54:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2251/
Test PASSed (JDK 8 and Scala 2.12).
"
287889810,2614,asfbot,2017-03-20T20:37:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2282/
Test PASSed (JDK 7 and Scala 2.10).
"
287891203,2614,asfbot,2017-03-20T20:42:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2285/
Test PASSed (JDK 8 and Scala 2.11).
"
287898512,2614,asfbot,2017-03-20T21:08:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2282/
Test PASSed (JDK 8 and Scala 2.12).
"
287906875,2614,asfbot,2017-03-20T21:40:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2285/
Test PASSed (JDK 7 and Scala 2.10).
"
287907752,2614,asfbot,2017-03-20T21:44:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2288/
Test PASSed (JDK 8 and Scala 2.11).
"
287916230,2614,asfbot,2017-03-20T22:20:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2285/
Test FAILed (JDK 8 and Scala 2.12).
"
287951372,2614,asfbot,2017-03-21T01:42:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2292/
Test PASSed (JDK 8 and Scala 2.11).
"
287951529,2614,asfbot,2017-03-21T01:43:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2289/
Test PASSed (JDK 7 and Scala 2.10).
"
287956340,2614,asfbot,2017-03-21T02:17:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2289/
Test PASSed (JDK 8 and Scala 2.12).
"
287983351,2614,asfbot,2017-03-21T05:48:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2292/
Test PASSed (JDK 7 and Scala 2.10).
"
287983610,2614,asfbot,2017-03-21T05:50:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2295/
Test PASSed (JDK 8 and Scala 2.11).
"
287988525,2614,asfbot,2017-03-21T06:28:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2292/
Test PASSed (JDK 8 and Scala 2.12).
"
288264386,2614,asfbot,2017-03-22T00:45:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2316/
Test FAILed (JDK 8 and Scala 2.11).
"
288265077,2614,asfbot,2017-03-22T00:50:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2312/
Test PASSed (JDK 7 and Scala 2.10).
"
288269464,2614,asfbot,2017-03-22T01:19:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2312/
Test PASSed (JDK 8 and Scala 2.12).
"
288311323,2614,asfbot,2017-03-22T06:36:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2317/
Test FAILed (JDK 7 and Scala 2.10).
"
288311356,2614,asfbot,2017-03-22T06:36:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2317/
Test PASSed (JDK 8 and Scala 2.12).
"
288316018,2614,asfbot,2017-03-22T07:07:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2321/
Test PASSed (JDK 8 and Scala 2.11).
"
288515135,2614,asfbot,2017-03-22T19:38:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2330/
Test PASSed (JDK 7 and Scala 2.10).
"
288515538,2614,asfbot,2017-03-22T19:40:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2330/
Test PASSed (JDK 8 and Scala 2.12).
"
288551756,2614,asfbot,2017-03-22T21:53:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2334/
Test FAILed (JDK 8 and Scala 2.11).
"
288795040,2614,asfbot,2017-03-23T17:21:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2350/
Test PASSed (JDK 8 and Scala 2.12).
"
288796665,2614,asfbot,2017-03-23T17:25:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2354/
Test PASSed (JDK 8 and Scala 2.11).
"
288797754,2614,asfbot,2017-03-23T17:28:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2350/
Test PASSed (JDK 7 and Scala 2.10).
"
288843283,2614,asfbot,2017-03-23T20:02:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2358/
Test PASSed (JDK 8 and Scala 2.11).
"
288844700,2614,asfbot,2017-03-23T20:08:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2354/
Test PASSed (JDK 7 and Scala 2.10).
"
288845391,2614,asfbot,2017-03-23T20:10:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2354/
Test PASSed (JDK 8 and Scala 2.12).
"
288861793,2614,asfbot,2017-03-23T21:13:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2356/
Test PASSed (JDK 7 and Scala 2.10).
"
288863357,2614,asfbot,2017-03-23T21:18:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2360/
Test PASSed (JDK 8 and Scala 2.11).
"
288869923,2614,asfbot,2017-03-23T21:44:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2356/
Test PASSed (JDK 8 and Scala 2.12).
"
289112773,2614,hachikuji,2017-03-24T18:50:07Z,"Ran system core and client system tests here: http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2017-03-23--001.1490326564--confluentinc--exactly-once-message-format--2e7cf82/. 

The failing zookeeper upgrade test seems to be transient. Here's a passing run: http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2017-03-24--001.1490323181--confluentinc--exactly-once-message-format--2e7cf82/. I investigated the failure in one case and it seems to be unrelated (apparently caused the overridden min isr setting of 2 and the fact that the test does not actually ensure that 2 brokers are always up)."
289124423,2614,apurvam,2017-03-24T19:39:54Z,LGTM!
303602595,3131,asfbot,2017-05-24T02:45:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4330/
Test PASSed (JDK 7 and Scala 2.11).
"
303605906,3131,asfbot,2017-05-24T03:11:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4317/
Test PASSed (JDK 8 and Scala 2.12).
"
303627181,3131,asfbot,2017-05-24T06:09:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4334/
Test PASSed (JDK 7 and Scala 2.11).
"
303631575,3131,asfbot,2017-05-24T06:37:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4321/
Test PASSed (JDK 8 and Scala 2.12).
"
303636823,3131,asfbot,2017-05-24T07:05:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4326/
Test PASSed (JDK 8 and Scala 2.12).
"
303643180,3131,asfbot,2017-05-24T07:36:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4339/
Test FAILed (JDK 7 and Scala 2.11).
"
303791182,3131,asfbot,2017-05-24T17:15:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4354/
Test PASSed (JDK 7 and Scala 2.11).
"
303798335,3131,asfbot,2017-05-24T17:41:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4340/
Test PASSed (JDK 8 and Scala 2.12).
"
303843735,3131,asfbot,2017-05-24T20:34:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4360/
Test PASSed (JDK 7 and Scala 2.11).
"
303846246,3131,asfbot,2017-05-24T20:44:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4346/
Test PASSed (JDK 8 and Scala 2.12).
"
303880657,3131,asfbot,2017-05-24T23:26:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4365/
Test PASSed (JDK 7 and Scala 2.11).
"
303885256,3131,asfbot,2017-05-24T23:58:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4351/
Test PASSed (JDK 8 and Scala 2.12).
"
303909746,3131,asfbot,2017-05-25T03:03:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4372/
Test FAILed (JDK 7 and Scala 2.11).
"
303912572,3131,asfbot,2017-05-25T03:27:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4358/
Test PASSed (JDK 8 and Scala 2.12).
"
303922819,3131,asfbot,2017-05-25T05:00:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4374/
Test PASSed (JDK 7 and Scala 2.11).
"
303926156,3131,asfbot,2017-05-25T05:27:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4360/
Test PASSed (JDK 8 and Scala 2.12).
"
303954041,3131,asfbot,2017-05-25T08:28:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4376/
Test PASSed (JDK 7 and Scala 2.11).
"
303960056,3131,asfbot,2017-05-25T08:59:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4362/
Test PASSed (JDK 8 and Scala 2.12).
"
303987005,3131,asfbot,2017-05-25T11:21:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4383/
Test PASSed (JDK 7 and Scala 2.11).
"
303991839,3131,asfbot,2017-05-25T11:52:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4369/
Test PASSed (JDK 8 and Scala 2.12).
"
304094679,3131,asfbot,2017-05-25T19:00:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4402/
Test PASSed (JDK 7 and Scala 2.11).
"
304100643,3131,asfbot,2017-05-25T19:25:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4388/
Test FAILed (JDK 8 and Scala 2.12).
"
304120761,3131,asfbot,2017-05-25T20:51:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4393/
Test FAILed (JDK 8 and Scala 2.12).
"
304124556,3131,asfbot,2017-05-25T21:06:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4407/
Test PASSed (JDK 7 and Scala 2.11).
"
304158688,3131,asfbot,2017-05-26T00:22:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4410/
Test PASSed (JDK 7 and Scala 2.11).
"
304162507,3131,asfbot,2017-05-26T00:53:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4396/
Test PASSed (JDK 8 and Scala 2.12).
"
304241199,3131,asfbot,2017-05-26T09:53:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4431/
Test PASSed (JDK 7 and Scala 2.11).
"
304246374,3131,asfbot,2017-05-26T10:19:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4417/
Test PASSed (JDK 8 and Scala 2.12).
"
304348340,3131,asfbot,2017-05-26T17:54:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4435/
Test PASSed (JDK 8 and Scala 2.12).
"
304349586,3131,asfbot,2017-05-26T17:59:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4449/
Test PASSed (JDK 7 and Scala 2.11).
"
304408970,3131,asfbot,2017-05-26T23:36:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4447/
Test FAILed (JDK 8 and Scala 2.12).
"
304411214,3131,asfbot,2017-05-26T23:59:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4461/
Test PASSed (JDK 7 and Scala 2.11).
"
304443338,3131,asfbot,2017-05-27T10:23:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4480/
Test PASSed (JDK 7 and Scala 2.11).
"
304444410,3131,asfbot,2017-05-27T10:47:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4466/
Test PASSed (JDK 8 and Scala 2.12).
"
304531091,3131,asfbot,2017-05-28T18:17:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4497/
Test PASSed (JDK 8 and Scala 2.12).
"
304531625,3131,asfbot,2017-05-28T18:27:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4512/
Test PASSed (JDK 7 and Scala 2.11).
"
304745849,3131,asfbot,2017-05-30T00:00:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4518/
Test PASSed (JDK 8 and Scala 2.12).
"
304748269,3131,asfbot,2017-05-30T00:32:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4533/
Test FAILed (JDK 7 and Scala 2.11).
"
304790887,3131,asfbot,2017-05-30T06:56:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4536/
Test PASSed (JDK 7 and Scala 2.11).
"
304805491,3131,asfbot,2017-05-30T08:07:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4522/
Test FAILed (JDK 8 and Scala 2.12).
"
304806544,3131,asfbot,2017-05-30T08:12:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4537/
Test PASSed (JDK 7 and Scala 2.11).
"
304923088,3131,asfbot,2017-05-30T15:52:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4535/
Test PASSed (JDK 8 and Scala 2.12).
"
304925020,3131,asfbot,2017-05-30T15:58:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4550/
Test PASSed (JDK 7 and Scala 2.11).
"
305005099,3131,asfbot,2017-05-30T20:54:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4566/
Test PASSed (JDK 7 and Scala 2.11).
"
305012733,3131,asfbot,2017-05-30T21:24:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4551/
Test PASSed (JDK 8 and Scala 2.12).
"
305019065,3131,asfbot,2017-05-30T21:50:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4572/
Test PASSed (JDK 7 and Scala 2.11).
"
305019440,3131,asfbot,2017-05-30T21:52:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4557/
Test PASSed (JDK 8 and Scala 2.12).
"
305033228,3131,asfbot,2017-05-30T23:03:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4579/
Test FAILed (JDK 7 and Scala 2.11).
"
305033513,3131,asfbot,2017-05-30T23:04:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4564/
Test FAILed (JDK 8 and Scala 2.12).
"
305045419,3131,asfbot,2017-05-31T00:18:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4579/
Test FAILed (JDK 8 and Scala 2.12).
"
305055595,3131,asfbot,2017-05-31T01:26:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4594/
Test PASSed (JDK 7 and Scala 2.11).
"
305061347,3131,asfbot,2017-05-31T02:05:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4588/
Test FAILed (JDK 8 and Scala 2.12).
"
305069000,3131,asfbot,2017-05-31T03:03:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4589/
Test PASSed (JDK 8 and Scala 2.12).
"
305069720,3131,asfbot,2017-05-31T03:09:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4604/
Test PASSed (JDK 7 and Scala 2.11).
"
305082113,3131,asfbot,2017-05-31T04:50:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4602/
Test FAILed (JDK 8 and Scala 2.12).
"
305087306,3131,asfbot,2017-05-31T05:29:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4617/
Test PASSed (JDK 7 and Scala 2.11).
"
305109177,3131,asfbot,2017-05-31T07:31:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4609/
Test PASSed (JDK 8 and Scala 2.12).
"
305111013,3131,asfbot,2017-05-31T07:40:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4624/
Test FAILed (JDK 7 and Scala 2.11).
"
305111061,3131,asfbot,2017-05-31T07:40:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4613/
Test PASSed (JDK 8 and Scala 2.12).
"
305111761,3131,asfbot,2017-05-31T07:43:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4628/
Test PASSed (JDK 7 and Scala 2.11).
"
305127346,3131,asfbot,2017-05-31T08:50:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4631/
Test PASSed (JDK 7 and Scala 2.11).
"
305128331,3131,asfbot,2017-05-31T08:54:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4616/
Test PASSed (JDK 8 and Scala 2.12).
"
305149759,3131,asfbot,2017-05-31T10:23:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4621/
Test FAILed (JDK 8 and Scala 2.12).
"
305152166,3131,asfbot,2017-05-31T10:34:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4636/
Test PASSed (JDK 7 and Scala 2.11).
"
305237184,3131,asfbot,2017-05-31T16:11:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4645/
Test PASSed (JDK 7 and Scala 2.11).
"
305246466,3131,asfbot,2017-05-31T16:43:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4630/
Test PASSed (JDK 8 and Scala 2.12).
"
305268613,3131,asfbot,2017-05-31T18:01:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4635/
Test PASSed (JDK 8 and Scala 2.12).
"
305275088,3131,asfbot,2017-05-31T18:25:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4650/
Test PASSed (JDK 7 and Scala 2.11).
"
305353392,3131,asfbot,2017-06-01T00:14:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4690/
Test FAILed (JDK 7 and Scala 2.11).
"
305359783,3131,asfbot,2017-06-01T01:01:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4674/
Test PASSed (JDK 8 and Scala 2.12).
"
305380027,3131,asfbot,2017-06-01T03:34:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4698/
Test PASSed (JDK 7 and Scala 2.11).
"
305383901,3131,asfbot,2017-06-01T04:09:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4682/
Test PASSed (JDK 8 and Scala 2.12).
"
305394053,3131,asfbot,2017-06-01T05:38:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4686/
Test PASSed (JDK 8 and Scala 2.12).
"
305398346,3131,asfbot,2017-06-01T06:11:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4702/
Test PASSed (JDK 7 and Scala 2.11).
"
305403872,3131,asfbot,2017-06-01T06:46:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4691/
Test PASSed (JDK 8 and Scala 2.12).
"
305409183,3131,asfbot,2017-06-01T07:14:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4707/
Test PASSed (JDK 7 and Scala 2.11).
"
305449415,3131,asfbot,2017-06-01T10:06:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4699/
Test PASSed (JDK 8 and Scala 2.12).
"
305449451,3131,asfbot,2017-06-01T10:06:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4715/
Test PASSed (JDK 7 and Scala 2.11).
"
305534733,3131,asfbot,2017-06-01T15:45:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4711/
Test PASSed (JDK 8 and Scala 2.12).
"
305536491,3131,asfbot,2017-06-01T15:50:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4727/
Test PASSed (JDK 7 and Scala 2.11).
"
305578610,3131,asfbot,2017-06-01T18:25:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4719/
Test PASSed (JDK 8 and Scala 2.12).
"
305580447,3131,asfbot,2017-06-01T18:31:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4735/
Test PASSed (JDK 7 and Scala 2.11).
"
305631686,3131,asfbot,2017-06-01T21:54:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4754/
Test PASSed (JDK 7 and Scala 2.11).
"
305636993,3131,asfbot,2017-06-01T22:21:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4738/
Test FAILed (JDK 8 and Scala 2.12).
"
305639662,3131,asfbot,2017-06-01T22:36:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4746/
Test FAILed (JDK 8 and Scala 2.12).
"
305649757,3131,asfbot,2017-06-01T23:39:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4762/
Test PASSed (JDK 7 and Scala 2.11).
"
305671922,3131,asfbot,2017-06-02T02:24:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4763/
Test PASSed (JDK 8 and Scala 2.12).
"
305672000,3131,asfbot,2017-06-02T02:25:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4779/
Test PASSed (JDK 7 and Scala 2.11).
"
305679361,3131,asfbot,2017-06-02T03:24:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4769/
Test PASSed (JDK 8 and Scala 2.12).
"
305681590,3131,asfbot,2017-06-02T03:44:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4785/
Test PASSed (JDK 7 and Scala 2.11).
"
305714451,3131,asfbot,2017-06-02T07:42:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4795/
Test PASSed (JDK 7 and Scala 2.11).
"
305720234,3131,asfbot,2017-06-02T08:10:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4783/
Test FAILed (JDK 8 and Scala 2.12).
"
305753264,3131,asfbot,2017-06-02T10:44:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4786/
Test PASSed (JDK 8 and Scala 2.12).
"
305754406,3131,asfbot,2017-06-02T10:50:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4802/
Test PASSed (JDK 7 and Scala 2.11).
"
305771449,3131,asfbot,2017-06-02T12:25:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4790/
Test FAILed (JDK 8 and Scala 2.12).
"
305772311,3131,asfbot,2017-06-02T12:29:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4806/
Test PASSed (JDK 7 and Scala 2.11).
"
305869752,3131,asfbot,2017-06-02T18:11:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4803/
Test FAILed (JDK 8 and Scala 2.12).
"
305869858,3131,asfbot,2017-06-02T18:11:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4819/
Test PASSed (JDK 7 and Scala 2.11).
"
305911060,3131,asfbot,2017-06-02T21:15:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4814/
Test FAILed (JDK 8 and Scala 2.12).
"
305922690,3131,asfbot,2017-06-02T22:23:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4817/
Test PASSed (JDK 8 and Scala 2.12).
"
305925037,3131,asfbot,2017-06-02T22:40:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4833/
Test PASSed (JDK 7 and Scala 2.11).
"
305935959,3131,asfbot,2017-06-03T00:15:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4821/
Test PASSed (JDK 8 and Scala 2.12).
"
305936546,3131,asfbot,2017-06-03T00:22:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4836/
Test PASSed (JDK 7 and Scala 2.11).
"
305947076,3131,asfbot,2017-06-03T03:15:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4847/
Test PASSed (JDK 7 and Scala 2.11).
"
305948364,3131,asfbot,2017-06-03T03:41:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4831/
Test FAILed (JDK 8 and Scala 2.12).
"
305955690,3131,asfbot,2017-06-03T06:36:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4855/
Test PASSed (JDK 7 and Scala 2.11).
"
305957360,3131,asfbot,2017-06-03T07:11:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4839/
Test PASSed (JDK 8 and Scala 2.12).
"
305967316,3131,asfbot,2017-06-03T10:46:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4869/
Test PASSed (JDK 7 and Scala 2.11).
"
305968501,3131,asfbot,2017-06-03T11:14:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4853/
Test PASSed (JDK 8 and Scala 2.12).
"
306047235,3131,asfbot,2017-06-04T15:31:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4885/
Test PASSed (JDK 7 and Scala 2.11).
"
306049115,3131,asfbot,2017-06-04T16:03:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4869/
Test PASSed (JDK 8 and Scala 2.12).
"
306054070,3131,asfbot,2017-06-04T17:24:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4890/
Test PASSed (JDK 7 and Scala 2.11).
"
306059249,3131,asfbot,2017-06-04T18:51:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4891/
Test PASSed (JDK 7 and Scala 2.11).
"
306061101,3131,asfbot,2017-06-04T19:21:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4875/
Test PASSed (JDK 8 and Scala 2.12).
"
306080063,3131,asfbot,2017-06-05T00:59:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4901/
Test PASSed (JDK 7 and Scala 2.11).
"
306080956,3131,asfbot,2017-06-05T01:10:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4885/
Test PASSed (JDK 8 and Scala 2.12).
"
306115383,3131,asfbot,2017-06-05T06:42:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4907/
Test PASSed (JDK 7 and Scala 2.11).
"
306120483,3131,asfbot,2017-06-05T07:16:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4891/
Test PASSed (JDK 8 and Scala 2.12).
"
306197703,3131,asfbot,2017-06-05T14:12:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4901/
Test PASSed (JDK 8 and Scala 2.12).
"
306197782,3131,asfbot,2017-06-05T14:12:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4917/
Test PASSed (JDK 7 and Scala 2.11).
"
306276321,3131,asfbot,2017-06-05T19:16:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4929/
Test FAILed (JDK 7 and Scala 2.11).
"
306283888,3131,asfbot,2017-06-05T19:47:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4913/
Test PASSed (JDK 8 and Scala 2.12).
"
306366411,3131,asfbot,2017-06-06T02:46:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4944/
Test FAILed (JDK 7 and Scala 2.11).
"
306373265,3131,asfbot,2017-06-06T03:41:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4928/
Test FAILed (JDK 8 and Scala 2.12).
"
306494628,3131,asfbot,2017-06-06T13:58:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4945/
Test PASSed (JDK 8 and Scala 2.12).
"
306496276,3131,asfbot,2017-06-06T14:04:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4961/
Test PASSed (JDK 7 and Scala 2.11).
"
306570065,3131,asfbot,2017-06-06T18:07:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4968/
Test FAILed (JDK 7 and Scala 2.11).
"
306574680,3131,asfbot,2017-06-06T18:24:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4955/
Test PASSed (JDK 8 and Scala 2.12).
"
306633939,3131,asfbot,2017-06-06T22:20:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4965/
Test PASSed (JDK 8 and Scala 2.12).
"
306636335,3131,asfbot,2017-06-06T22:32:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4981/
Test PASSed (JDK 7 and Scala 2.11).
"
306662976,3131,asfbot,2017-06-07T01:51:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4989/
Test PASSed (JDK 8 and Scala 2.12).
"
306663464,3131,asfbot,2017-06-07T01:54:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5005/
Test PASSed (JDK 7 and Scala 2.11).
"
306795944,3131,asfbot,2017-06-07T13:33:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5023/
Test FAILed (JDK 7 and Scala 2.11).
"
306797735,3131,asfbot,2017-06-07T13:40:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5007/
Test PASSed (JDK 8 and Scala 2.12).
"
306862437,3131,asfbot,2017-06-07T17:12:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5015/
Test PASSed (JDK 8 and Scala 2.12).
"
306884478,3131,asfbot,2017-06-07T18:30:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5020/
Test PASSed (JDK 8 and Scala 2.12).
"
306885973,3131,asfbot,2017-06-07T18:35:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5036/
Test PASSed (JDK 7 and Scala 2.11).
"
306943060,3131,asfbot,2017-06-07T22:27:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5043/
Test PASSed (JDK 7 and Scala 2.11).
"
306943524,3131,asfbot,2017-06-07T22:29:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5027/
Test FAILed (JDK 8 and Scala 2.12).
"
306977070,3131,asfbot,2017-06-08T02:04:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5049/
Test PASSed (JDK 7 and Scala 2.11).
"
306978582,3131,asfbot,2017-06-08T02:15:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5033/
Test PASSed (JDK 8 and Scala 2.12).
"
306988176,3131,asfbot,2017-06-08T03:27:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5055/
Test FAILed (JDK 7 and Scala 2.11).
"
306990183,3131,asfbot,2017-06-08T03:44:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5039/
Test PASSed (JDK 8 and Scala 2.12).
"
306999700,3131,asfbot,2017-06-08T05:07:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5060/
Test PASSed (JDK 7 and Scala 2.11).
"
307000746,3131,asfbot,2017-06-08T05:16:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5044/
Test PASSed (JDK 8 and Scala 2.12).
"
307179359,3131,asfbot,2017-06-08T17:54:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5071/
Test FAILed (JDK 7 and Scala 2.11).
"
307180047,3131,asfbot,2017-06-08T17:57:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5055/
Test PASSed (JDK 8 and Scala 2.12).
"
307203007,3131,asfbot,2017-06-08T19:26:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5077/
Test FAILed (JDK 7 and Scala 2.11).
"
307212859,3131,asfbot,2017-06-08T20:07:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5061/
Test PASSed (JDK 8 and Scala 2.12).
"
307234607,3131,asfbot,2017-06-08T21:37:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5063/
Test FAILed (JDK 8 and Scala 2.12).
"
307256276,3131,asfbot,2017-06-08T23:38:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5067/
Test PASSed (JDK 8 and Scala 2.12).
"
307256938,3131,asfbot,2017-06-08T23:42:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5083/
Test PASSed (JDK 7 and Scala 2.11).
"
307280983,3131,asfbot,2017-06-09T02:48:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5093/
Test PASSed (JDK 7 and Scala 2.11).
"
307281949,3131,asfbot,2017-06-09T02:56:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5077/
Test PASSed (JDK 8 and Scala 2.12).
"
307485656,3131,asfbot,2017-06-09T19:59:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5111/
Test FAILed (JDK 7 and Scala 2.11).
"
307489263,3131,asfbot,2017-06-09T20:16:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5095/
Test PASSed (JDK 8 and Scala 2.12).
"
307522216,3131,asfbot,2017-06-09T23:15:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5106/
Test FAILed (JDK 8 and Scala 2.12).
"
307526512,3131,asfbot,2017-06-09T23:55:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5122/
Test FAILed (JDK 7 and Scala 2.11).
"
307532003,3131,asfbot,2017-06-10T01:05:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5128/
Test FAILed (JDK 7 and Scala 2.11).
"
307532357,3131,asfbot,2017-06-10T01:11:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5112/
Test PASSed (JDK 8 and Scala 2.12).
"
307693494,3131,asfbot,2017-06-12T05:32:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5160/
Test PASSed (JDK 7 and Scala 2.11).
"
307695600,3131,asfbot,2017-06-12T05:52:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5144/
Test PASSed (JDK 8 and Scala 2.12).
"
307847705,3131,asfbot,2017-06-12T16:46:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5179/
Test PASSed (JDK 7 and Scala 2.11).
"
307848368,3131,asfbot,2017-06-12T16:49:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5163/
Test PASSed (JDK 8 and Scala 2.12).
"
307922938,3131,asfbot,2017-06-12T20:51:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5172/
Test PASSed (JDK 8 and Scala 2.12).
"
307928274,3131,asfbot,2017-06-12T21:03:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5188/
Test PASSed (JDK 7 and Scala 2.11).
"
307972150,3131,asfbot,2017-06-13T00:29:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5192/
Test PASSed (JDK 8 and Scala 2.12).
"
307972689,3131,asfbot,2017-06-13T00:32:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5208/
Test PASSed (JDK 7 and Scala 2.11).
"
307988611,3131,asfbot,2017-06-13T02:29:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5216/
Test PASSed (JDK 7 and Scala 2.11).
"
307988649,3131,asfbot,2017-06-13T02:29:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5200/
Test PASSed (JDK 8 and Scala 2.12).
"
308116503,3131,asfbot,2017-06-13T13:31:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5226/
Test PASSed (JDK 7 and Scala 2.11).
"
308117020,3131,asfbot,2017-06-13T13:32:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5210/
Test PASSed (JDK 8 and Scala 2.12).
"
308165994,3131,asfbot,2017-06-13T16:03:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5231/
Test PASSed (JDK 7 and Scala 2.11).
"
308166639,3131,asfbot,2017-06-13T16:05:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5215/
Test PASSed (JDK 8 and Scala 2.12).
"
308214074,3131,asfbot,2017-06-13T18:55:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5220/
Test FAILed (JDK 8 and Scala 2.12).
"
308221578,3131,asfbot,2017-06-13T19:23:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5224/
Test FAILed (JDK 8 and Scala 2.12).
"
308223661,3131,asfbot,2017-06-13T19:31:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5240/
Test PASSed (JDK 7 and Scala 2.11).
"
308225763,3131,asfbot,2017-06-13T19:39:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5236/
Test PASSed (JDK 7 and Scala 2.11).
"
308289459,3131,asfbot,2017-06-14T00:58:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5258/
Test PASSed (JDK 8 and Scala 2.12).
"
308290492,3131,asfbot,2017-06-14T01:06:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5273/
Test PASSed (JDK 7 and Scala 2.11).
"
308303745,3131,asfbot,2017-06-14T02:43:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5267/
Test PASSed (JDK 8 and Scala 2.12).
"
308303946,3131,asfbot,2017-06-14T02:45:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5283/
Test PASSed (JDK 7 and Scala 2.11).
"
308317701,3131,asfbot,2017-06-14T04:38:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5285/
Test PASSed (JDK 7 and Scala 2.11).
"
308323080,3131,asfbot,2017-06-14T05:25:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5269/
Test PASSed (JDK 8 and Scala 2.12).
"
308407244,3131,asfbot,2017-06-14T11:48:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5291/
Test PASSed (JDK 7 and Scala 2.11).
"
308407505,3131,asfbot,2017-06-14T11:49:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5275/
Test PASSed (JDK 8 and Scala 2.12).
"
308510527,3131,asfbot,2017-06-14T17:58:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5296/
Test PASSed (JDK 8 and Scala 2.12).
"
308513472,3131,asfbot,2017-06-14T18:09:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5312/
Test PASSed (JDK 7 and Scala 2.11).
"
308585423,3131,asfbot,2017-06-14T23:21:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5313/
Test PASSed (JDK 8 and Scala 2.12).
"
308590350,3131,asfbot,2017-06-14T23:55:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5329/
Test FAILed (JDK 7 and Scala 2.11).
"
308593822,3131,asfbot,2017-06-15T00:21:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5318/
Test PASSed (JDK 8 and Scala 2.12).
"
308599941,3131,asfbot,2017-06-15T01:06:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5334/
Test PASSed (JDK 7 and Scala 2.11).
"
308607464,3131,asfbot,2017-06-15T02:02:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5326/
Test PASSed (JDK 8 and Scala 2.12).
"
308607655,3131,asfbot,2017-06-15T02:04:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5342/
Test PASSed (JDK 7 and Scala 2.11).
"
308838541,3131,asfgit,2017-06-15T19:08:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5346/
Test PASSed (JDK 8 and Scala 2.12).
"
308839378,3131,asfgit,2017-06-15T19:12:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5361/
Test PASSed (JDK 7 and Scala 2.11).
"
308885344,3131,asfgit,2017-06-15T22:42:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5373/
Test PASSed (JDK 7 and Scala 2.11).
"
308890519,3131,asfgit,2017-06-15T23:15:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5358/
Test PASSed (JDK 8 and Scala 2.12).
"
308906740,3131,asfgit,2017-06-16T01:17:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5378/
Test PASSed (JDK 7 and Scala 2.11).
"
308907839,3131,asfgit,2017-06-16T01:26:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5363/
Test PASSed (JDK 8 and Scala 2.12).
"
308997035,3131,ijuma,2017-06-16T11:00:47Z,Please close this PR and use the mailing list for questions.
309044431,3131,asfgit,2017-06-16T14:41:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5400/
Test PASSed (JDK 7 and Scala 2.11).
"
309051841,3131,asfgit,2017-06-16T15:08:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5385/
Test PASSed (JDK 8 and Scala 2.12).
"
309077407,3131,asfgit,2017-06-16T16:51:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5405/
Test PASSed (JDK 7 and Scala 2.11).
"
309085053,3131,asfgit,2017-06-16T17:23:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5390/
Test PASSed (JDK 8 and Scala 2.12).
"
309136438,3131,asfgit,2017-06-16T21:17:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5416/
Test PASSed (JDK 7 and Scala 2.11).
"
309138538,3131,asfgit,2017-06-16T21:29:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5401/
Test PASSed (JDK 8 and Scala 2.12).
"
309170680,3131,asfgit,2017-06-17T00:08:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5422/
Test PASSed (JDK 7 and Scala 2.11).
"
309171385,3131,asfgit,2017-06-17T00:16:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5407/
Test PASSed (JDK 8 and Scala 2.12).
"
309195817,3131,asfgit,2017-06-17T05:57:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5435/
Test PASSed (JDK 7 and Scala 2.11).
"
309196895,3131,asfgit,2017-06-17T06:28:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5420/
Test PASSed (JDK 8 and Scala 2.12).
"
309202852,3131,asfgit,2017-06-17T08:54:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5448/
Test PASSed (JDK 7 and Scala 2.11).
"
309204200,3131,asfgit,2017-06-17T09:24:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5433/
Test PASSed (JDK 8 and Scala 2.12).
"
309213214,3131,asfgit,2017-06-17T12:48:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5439/
Test PASSed (JDK 8 and Scala 2.12).
"
309213845,3131,asfgit,2017-06-17T13:00:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5454/
Test PASSed (JDK 7 and Scala 2.11).
"
309217015,3131,asfgit,2017-06-17T14:03:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5456/
Test PASSed (JDK 7 and Scala 2.11).
"
309217673,3131,asfgit,2017-06-17T14:16:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5441/
Test FAILed (JDK 8 and Scala 2.12).
"
309231505,3131,asfgit,2017-06-17T18:19:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5457/
Test PASSed (JDK 7 and Scala 2.11).
"
309232576,3131,asfgit,2017-06-17T18:39:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5442/
Test PASSed (JDK 8 and Scala 2.12).
"
309246229,3131,asfgit,2017-06-17T23:20:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5445/
Test PASSed (JDK 8 and Scala 2.12).
"
309246901,3131,asfgit,2017-06-17T23:39:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5460/
Test PASSed (JDK 7 and Scala 2.11).
"
309645253,3131,asfgit,2017-06-20T05:00:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5492/
Test PASSed (JDK 7 and Scala 2.11).
"
309648470,3131,asfgit,2017-06-20T05:26:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5477/
Test PASSed (JDK 8 and Scala 2.12).
"
309838167,3131,asfgit,2017-06-20T17:56:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5494/
Test PASSed (JDK 8 and Scala 2.12).
"
309839788,3131,asfgit,2017-06-20T18:02:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5509/
Test PASSed (JDK 7 and Scala 2.11).
"
309880242,3131,asfgit,2017-06-20T20:23:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5516/
Test PASSed (JDK 7 and Scala 2.11).
"
309885321,3131,asfgit,2017-06-20T20:42:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5501/
Test PASSed (JDK 8 and Scala 2.12).
"
309909209,3131,asfgit,2017-06-20T22:24:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5526/
Test PASSed (JDK 7 and Scala 2.11).
"
309909693,3131,asfgit,2017-06-20T22:27:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5512/
Test PASSed (JDK 8 and Scala 2.12).
"
309928511,3131,asfgit,2017-06-21T00:28:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5533/
Test PASSed (JDK 7 and Scala 2.11).
"
309931956,3131,asfgit,2017-06-21T00:54:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5519/
Test PASSed (JDK 8 and Scala 2.12).
"
309941067,3131,asfgit,2017-06-21T02:00:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5543/
Test PASSed (JDK 7 and Scala 2.11).
"
309944513,3131,asfgit,2017-06-21T02:25:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5529/
Test PASSed (JDK 8 and Scala 2.12).
"
310176830,3131,asfgit,2017-06-21T19:11:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5568/
Test PASSed (JDK 7 and Scala 2.11).
"
310183774,3131,asfgit,2017-06-21T19:40:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5554/
Test PASSed (JDK 8 and Scala 2.12).
"
310228086,3131,asfgit,2017-06-21T22:57:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5576/
Test PASSed (JDK 8 and Scala 2.12).
"
310229066,3131,asfgit,2017-06-21T23:03:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5590/
Test PASSed (JDK 7 and Scala 2.11).
"
310251331,3131,asfgit,2017-06-22T01:37:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5601/
Test PASSed (JDK 7 and Scala 2.11).
"
310254024,3131,asfgit,2017-06-22T01:57:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5587/
Test PASSed (JDK 8 and Scala 2.12).
"
310318602,3131,asfgit,2017-06-22T08:52:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5607/
Test PASSed (JDK 7 and Scala 2.11).
"
310319946,3131,asfgit,2017-06-22T08:57:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5593/
Test PASSed (JDK 8 and Scala 2.12).
"
310395271,3131,asfgit,2017-06-22T14:22:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5605/
Test PASSed (JDK 8 and Scala 2.12).
"
310401575,3131,asfgit,2017-06-22T14:43:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5619/
Test PASSed (JDK 7 and Scala 2.11).
"
310442593,3131,asfgit,2017-06-22T17:07:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5625/
Test PASSed (JDK 7 and Scala 2.11).
"
310443047,3131,asfgit,2017-06-22T17:09:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5611/
Test PASSed (JDK 8 and Scala 2.12).
"
310500851,3131,asfgit,2017-06-22T20:58:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5632/
Test PASSed (JDK 7 and Scala 2.11).
"
310501870,3131,asfgit,2017-06-22T21:03:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5618/
Test PASSed (JDK 8 and Scala 2.12).
"
310528559,3131,asfgit,2017-06-22T23:18:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5642/
Test PASSed (JDK 7 and Scala 2.11).
"
310529303,3131,asfgit,2017-06-22T23:24:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5628/
Test PASSed (JDK 8 and Scala 2.12).
"
310703710,3131,asfgit,2017-06-23T15:55:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5651/
Test PASSed (JDK 8 and Scala 2.12).
"
310705823,3131,asfgit,2017-06-23T16:03:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5665/
Test PASSed (JDK 7 and Scala 2.11).
"
310754172,3131,asfgit,2017-06-23T19:32:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5658/
Test PASSed (JDK 8 and Scala 2.12).
"
310758955,3131,asfgit,2017-06-23T19:57:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5672/
Test FAILed (JDK 7 and Scala 2.11).
"
310806712,3131,asfgit,2017-06-24T02:04:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5680/
Test PASSed (JDK 7 and Scala 2.11).
"
310807815,3131,asfgit,2017-06-24T02:27:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5666/
Test PASSed (JDK 8 and Scala 2.12).
"
310824549,3131,asfgit,2017-06-24T08:19:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5684/
Test PASSed (JDK 7 and Scala 2.11).
"
310825140,3131,asfgit,2017-06-24T08:32:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5670/
Test FAILed (JDK 8 and Scala 2.12).
"
310916461,3131,asfgit,2017-06-25T17:35:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5692/
Test PASSed (JDK 7 and Scala 2.11).
"
310917587,3131,asfgit,2017-06-25T17:55:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5678/
Test PASSed (JDK 8 and Scala 2.12).
"
311223867,3131,asfgit,2017-06-27T01:07:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5726/
Test PASSed (JDK 7 and Scala 2.11).
"
311227089,3131,asfgit,2017-06-27T01:30:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5712/
Test PASSed (JDK 8 and Scala 2.12).
"
311389813,3131,asfgit,2017-06-27T15:12:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5728/
Test PASSed (JDK 8 and Scala 2.12).
"
311391878,3131,asfgit,2017-06-27T15:19:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5742/
Test PASSed (JDK 7 and Scala 2.11).
"
311536629,3131,asfgit,2017-06-28T02:08:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5759/
Test FAILed (JDK 7 and Scala 2.11).
"
311537187,3131,asfgit,2017-06-28T02:12:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5745/
Test PASSed (JDK 8 and Scala 2.12).
"
311763999,3131,asfgit,2017-06-28T19:33:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5761/
Test PASSed (JDK 8 and Scala 2.12).
"
311766700,3131,asfgit,2017-06-28T19:44:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5775/
Test PASSed (JDK 7 and Scala 2.11).
"
311783676,3131,asfgit,2017-06-28T20:43:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5763/
Test PASSed (JDK 8 and Scala 2.12).
"
311791544,3131,asfgit,2017-06-28T21:09:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5777/
Test PASSed (JDK 7 and Scala 2.11).
"
312044718,3131,asfgit,2017-06-29T17:51:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5804/
Test PASSed (JDK 7 and Scala 2.11).
"
312046242,3131,asfgit,2017-06-29T17:56:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5790/
Test PASSed (JDK 8 and Scala 2.12).
"
312113587,3131,asfgit,2017-06-29T21:28:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5812/
Test PASSed (JDK 7 and Scala 2.11).
"
312119156,3131,asfgit,2017-06-29T21:53:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5798/
Test PASSed (JDK 8 and Scala 2.12).
"
312189415,3131,asfgit,2017-06-30T06:47:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5833/
Test PASSed (JDK 7 and Scala 2.11).
"
312194538,3131,asfgit,2017-06-30T07:18:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5819/
Test PASSed (JDK 8 and Scala 2.12).
"
312215825,3131,asfgit,2017-06-30T09:03:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5824/
Test PASSed (JDK 8 and Scala 2.12).
"
312219097,3131,asfgit,2017-06-30T09:18:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5838/
Test FAILed (JDK 7 and Scala 2.11).
"
312416128,3131,asfgit,2017-07-01T07:16:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5858/
Test PASSed (JDK 7 and Scala 2.11).
"
312418273,3131,asfgit,2017-07-01T08:08:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5842/
Test PASSed (JDK 8 and Scala 2.12).
"
313290202,3131,asfgit,2017-07-06T04:12:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5896/
Test FAILed (JDK 8 and Scala 2.12).
"
313291087,3131,asfgit,2017-07-06T04:19:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5911/
Test PASSed (JDK 7 and Scala 2.11).
"
313356114,3131,asfgit,2017-07-06T10:15:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5922/
Test PASSed (JDK 7 and Scala 2.11).
"
313357736,3131,asfgit,2017-07-06T10:22:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5907/
Test PASSed (JDK 8 and Scala 2.12).
"
313637261,3131,asfgit,2017-07-07T09:43:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5953/
Test FAILed (JDK 7 and Scala 2.11).
"
313643243,3131,asfgit,2017-07-07T10:12:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5938/
Test PASSed (JDK 8 and Scala 2.12).
"
313680367,3131,asfgit,2017-07-07T13:27:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5961/
Test FAILed (JDK 7 and Scala 2.11).
"
313686281,3131,asfgit,2017-07-07T13:51:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5946/
Test PASSed (JDK 8 and Scala 2.12).
"
313708942,3131,asfgit,2017-07-07T15:08:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5969/
Test FAILed (JDK 7 and Scala 2.11).
"
313713798,3131,asfgit,2017-07-07T15:26:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5954/
Test PASSed (JDK 8 and Scala 2.12).
"
313762129,3131,asfgit,2017-07-07T18:43:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5981/
Test FAILed (JDK 7 and Scala 2.11).
"
313773908,3131,asfgit,2017-07-07T19:36:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5966/
Test PASSed (JDK 8 and Scala 2.12).
"
313980603,3131,asfgit,2017-07-10T01:33:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6004/
Test PASSed (JDK 7 and Scala 2.11).
"
313983637,3131,asfgit,2017-07-10T02:02:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5989/
Test PASSed (JDK 8 and Scala 2.12).
"
314189084,3131,asfgit,2017-07-10T18:12:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6001/
Test PASSed (JDK 8 and Scala 2.12).
"
314189742,3131,asfgit,2017-07-10T18:14:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6016/
Test PASSed (JDK 7 and Scala 2.11).
"
314307941,3131,asfgit,2017-07-11T02:49:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6021/
Test PASSed (JDK 7 and Scala 2.11).
"
314310802,3131,asfgit,2017-07-11T03:11:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6006/
Test PASSed (JDK 8 and Scala 2.12).
"
314899709,3131,asfgit,2017-07-12T21:17:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6034/
Test PASSed (JDK 8 and Scala 2.12).
"
314902490,3131,asfgit,2017-07-12T21:28:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6049/
Test PASSed (JDK 7 and Scala 2.11).
"
314946635,3131,asfgit,2017-07-13T01:49:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6038/
Test PASSed (JDK 8 and Scala 2.12).
"
314949897,3131,asfgit,2017-07-13T02:11:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6053/
Test PASSed (JDK 7 and Scala 2.11).
"
315463525,3131,asfgit,2017-07-14T20:46:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6075/
Test FAILed (JDK 8 and Scala 2.12).
"
315473267,3131,asfgit,2017-07-14T21:33:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6090/
Test PASSed (JDK 7 and Scala 2.11).
"
315699349,3131,asfgit,2017-07-17T08:52:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6108/
Test FAILed (JDK 7 and Scala 2.11).
"
315699350,3131,asfgit,2017-07-17T08:52:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6093/
Test FAILed (JDK 8 and Scala 2.12).
"
315863878,3131,asfgit,2017-07-17T19:53:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6128/
Test PASSed (JDK 7 and Scala 2.11).
"
315872281,3131,asfgit,2017-07-17T20:25:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6113/
Test PASSed (JDK 8 and Scala 2.12).
"
315955557,3131,asfgit,2017-07-18T04:34:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6119/
Test FAILed (JDK 8 and Scala 2.12).
"
315955680,3131,asfgit,2017-07-18T04:35:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6134/
Test FAILed (JDK 7 and Scala 2.11).
"
316404995,3131,asfgit,2017-07-19T14:27:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6167/
Test PASSed (JDK 7 and Scala 2.11).
"
316411781,3131,asfgit,2017-07-19T14:49:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6151/
Test PASSed (JDK 8 and Scala 2.12).
"
316602484,3131,asfgit,2017-07-20T05:41:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6185/
Test PASSed (JDK 7 and Scala 2.11).
"
316604494,3131,asfgit,2017-07-20T05:56:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6169/
Test PASSed (JDK 8 and Scala 2.12).
"
316676695,3131,asfgit,2017-07-20T11:33:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6193/
Test FAILed (JDK 7 and Scala 2.11).
"
316691369,3131,asfgit,2017-07-20T12:44:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6197/
Test PASSed (JDK 7 and Scala 2.11).
"
316691784,3131,asfgit,2017-07-20T12:46:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6181/
Test PASSed (JDK 8 and Scala 2.12).
"
316769888,3131,asfgit,2017-07-20T17:10:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6214/
Test PASSed (JDK 7 and Scala 2.11).
"
316775526,3131,asfgit,2017-07-20T17:32:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6198/
Test PASSed (JDK 8 and Scala 2.12).
"
316796076,3131,asfgit,2017-07-20T18:50:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6217/
Test PASSed (JDK 7 and Scala 2.11).
"
316798910,3131,asfgit,2017-07-20T19:00:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6201/
Test PASSed (JDK 8 and Scala 2.12).
"
316830900,3131,asfgit,2017-07-20T21:10:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6224/
Test PASSed (JDK 7 and Scala 2.11).
"
316836874,3131,asfgit,2017-07-20T21:35:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6208/
Test PASSed (JDK 8 and Scala 2.12).
"
316895115,3131,asfgit,2017-07-21T03:50:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6234/
Test PASSed (JDK 7 and Scala 2.11).
"
316898366,3131,asfgit,2017-07-21T04:17:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6218/
Test PASSed (JDK 8 and Scala 2.12).
"
316909787,3131,asfgit,2017-07-21T05:50:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6239/
Test PASSed (JDK 7 and Scala 2.11).
"
316911735,3131,asfgit,2017-07-21T06:03:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6223/
Test PASSed (JDK 8 and Scala 2.12).
"
317004073,3131,asfgit,2017-07-21T13:42:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6239/
Test PASSed (JDK 8 and Scala 2.12).
"
317009812,3131,asfgit,2017-07-21T14:04:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6255/
Test FAILed (JDK 7 and Scala 2.11).
"
317054127,3131,asfgit,2017-07-21T16:53:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6257/
Test PASSed (JDK 7 and Scala 2.11).
"
317058029,3131,asfgit,2017-07-21T17:08:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6241/
Test PASSed (JDK 8 and Scala 2.12).
"
317140506,3131,asfgit,2017-07-22T00:26:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6257/
Test PASSed (JDK 8 and Scala 2.12).
"
317141581,3131,asfgit,2017-07-22T00:41:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6273/
Test PASSed (JDK 7 and Scala 2.11).
"
317150453,3131,asfgit,2017-07-22T03:18:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6278/
Test PASSed (JDK 7 and Scala 2.11).
"
317151196,3131,asfgit,2017-07-22T03:37:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6262/
Test PASSed (JDK 8 and Scala 2.12).
"
317174515,3131,asfgit,2017-07-22T10:35:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6284/
Test PASSed (JDK 7 and Scala 2.11).
"
317175663,3131,asfgit,2017-07-22T10:57:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6268/
Test PASSed (JDK 8 and Scala 2.12).
"
318122354,3131,asfgit,2017-07-26T17:18:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6359/
Test PASSed (JDK 7 and Scala 2.11).
"
318125523,3131,asfgit,2017-07-26T17:30:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6344/
Test FAILed (JDK 8 and Scala 2.12).
"
318180442,3131,asfgit,2017-07-26T20:57:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6368/
Test PASSed (JDK 7 and Scala 2.11).
"
318187096,3131,asfgit,2017-07-26T21:24:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6353/
Test PASSed (JDK 8 and Scala 2.12).
"
318277051,3131,asfgit,2017-07-27T07:01:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6380/
Test PASSed (JDK 7 and Scala 2.11).
"
318279932,3131,asfgit,2017-07-27T07:16:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6365/
Test PASSed (JDK 8 and Scala 2.12).
"
318490415,3131,asfgit,2017-07-27T21:22:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6392/
Test PASSed (JDK 7 and Scala 2.11).
"
318495451,3131,asfgit,2017-07-27T21:45:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6377/
Test PASSed (JDK 8 and Scala 2.12).
"
318601389,3131,asfgit,2017-07-28T09:06:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6408/
Test PASSed (JDK 7 and Scala 2.11).
"
318606227,3131,asfgit,2017-07-28T09:28:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6393/
Test PASSed (JDK 8 and Scala 2.12).
"
318786012,3131,asfgit,2017-07-28T23:46:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6423/
Test PASSed (JDK 7 and Scala 2.11).
"
318794126,3131,asfgit,2017-07-29T01:24:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6408/
Test FAILed (JDK 8 and Scala 2.12).
"
319434917,3131,asfgit,2017-08-01T17:10:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6471/
Test PASSed (JDK 7 and Scala 2.11).
"
319441568,3131,asfgit,2017-08-01T17:34:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6456/
Test PASSed (JDK 8 and Scala 2.12).
"
319519308,3131,asfgit,2017-08-01T22:51:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6480/
Test PASSed (JDK 7 and Scala 2.11).
"
319523740,3131,asfgit,2017-08-01T23:19:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6465/
Test PASSed (JDK 8 and Scala 2.12).
"
320105168,3131,asfgit,2017-08-03T22:18:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6521/
Test PASSed (JDK 7 and Scala 2.11).
"
320115974,3131,asfgit,2017-08-03T23:24:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6524/
Test PASSed (JDK 7 and Scala 2.11).
"
320230643,3131,asfgit,2017-08-04T11:56:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6545/
Test PASSed (JDK 8 and Scala 2.12).
"
320230710,3131,asfgit,2017-08-04T11:56:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6560/
Test PASSed (JDK 7 and Scala 2.11).
"
320304104,3131,asfgit,2017-08-04T17:16:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6578/
Test PASSed (JDK 7 and Scala 2.11).
"
320309814,3131,asfgit,2017-08-04T17:40:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6563/
Test PASSed (JDK 8 and Scala 2.12).
"
320579436,3131,asfgit,2017-08-07T06:26:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6602/
Test PASSed (JDK 7 and Scala 2.11).
"
320587134,3131,asfgit,2017-08-07T07:13:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6587/
Test PASSed (JDK 8 and Scala 2.12).
"
320891392,3131,asfgit,2017-08-08T08:42:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6621/
Test FAILed (JDK 8 and Scala 2.12).
"
320908484,3131,asfgit,2017-08-08T09:51:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6636/
Test FAILed (JDK 7 and Scala 2.11).
"
321148398,3131,asfgit,2017-08-09T04:18:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6659/
Test PASSed (JDK 7 and Scala 2.11).
"
321151097,3131,asfgit,2017-08-09T04:42:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6644/
Test PASSed (JDK 8 and Scala 2.12).
"
321365402,3131,asfgit,2017-08-09T20:01:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6671/
Test PASSed (JDK 7 and Scala 2.11).
"
321365596,3131,asfgit,2017-08-09T20:02:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6658/
Test FAILed (JDK 8 and Scala 2.12).
"
321528208,3131,asfgit,2017-08-10T11:42:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6685/
Test PASSed (JDK 7 and Scala 2.11).
"
321532227,3131,asfgit,2017-08-10T12:04:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6670/
Test PASSed (JDK 8 and Scala 2.12).
"
321861163,3131,asfgit,2017-08-11T16:39:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6710/
Test PASSed (JDK 7 and Scala 2.11).
"
321865232,3131,asfgit,2017-08-11T16:57:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6695/
Test PASSed (JDK 8 and Scala 2.12).
"
322076337,3131,asfgit,2017-08-14T00:10:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6731/
Test PASSed (JDK 7 and Scala 2.11).
"
322077606,3131,asfgit,2017-08-14T00:33:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6716/
Test PASSed (JDK 8 and Scala 2.12).
"
322220153,3131,asfgit,2017-08-14T15:21:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6728/
Test PASSed (JDK 8 and Scala 2.12).
"
322237165,3131,asfgit,2017-08-14T16:22:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6744/
Test PASSed (JDK 7 and Scala 2.11).
"
322501179,3131,asfgit,2017-08-15T15:27:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6781/
Test FAILed (JDK 7 and Scala 2.11).
"
322502268,3131,asfgit,2017-08-15T15:31:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6766/
Test PASSed (JDK 8 and Scala 2.12).
"
322546855,3131,asfgit,2017-08-15T18:20:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6779/
Test FAILed (JDK 8 and Scala 2.12).
"
322562469,3131,asfgit,2017-08-15T19:20:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6793/
Test PASSed (JDK 7 and Scala 2.11).
"
322636978,3131,asfgit,2017-08-16T01:32:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6809/
Test PASSed (JDK 7 and Scala 2.11).
"
322640924,3131,asfgit,2017-08-16T01:54:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6795/
Test PASSed (JDK 8 and Scala 2.12).
"
322738697,3131,asfgit,2017-08-16T11:12:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6814/
Test PASSed (JDK 7 and Scala 2.11).
"
322743151,3131,asfgit,2017-08-16T11:36:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6800/
Test PASSed (JDK 8 and Scala 2.12).
"
322768922,3131,asfgit,2017-08-16T13:21:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6815/
Test PASSed (JDK 7 and Scala 2.11).
"
322776097,3131,asfgit,2017-08-16T13:46:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6801/
Test PASSed (JDK 8 and Scala 2.12).
"
322919652,3131,asfgit,2017-08-16T22:42:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6829/
Test PASSed (JDK 7 and Scala 2.11).
"
322923844,3131,asfgit,2017-08-16T23:07:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6815/
Test PASSed (JDK 8 and Scala 2.12).
"
342992746,3131,asfgit,2017-11-08T23:11:03Z,"FAILURE
 No test results found.
--none--"
343004359,3131,asfgit,2017-11-09T00:10:22Z,"SUCCESS 
 7148 tests run, 5 skipped, 0 failed.
--none--"
343009705,3131,asfgit,2017-11-09T00:40:13Z,"SUCCESS 
 7148 tests run, 5 skipped, 0 failed.
--none--"
343326735,3131,asfgit,2017-11-09T23:33:04Z,"FAILURE
 No test results found.
--none--"
343338885,3131,asfgit,2017-11-10T00:40:13Z,"SUCCESS 
 7149 tests run, 5 skipped, 0 failed.
--none--"
343343569,3131,asfgit,2017-11-10T01:07:58Z,"SUCCESS 
 7149 tests run, 5 skipped, 0 failed.
--none--"
343559889,3131,asfgit,2017-11-10T19:10:05Z,"FAILURE
 No test results found.
--none--"
343577383,3131,asfgit,2017-11-10T20:27:29Z,"SUCCESS 
 7149 tests run, 5 skipped, 0 failed.
--none--"
343578224,3131,asfgit,2017-11-10T20:31:23Z,"SUCCESS 
 7149 tests run, 5 skipped, 0 failed.
--none--"
344755106,3131,asfgit,2017-11-15T22:46:57Z,"FAILURE
 No test results found.
--none--"
344766679,3131,asfgit,2017-11-15T23:42:47Z,"FAILURE
 7149 tests run, 5 skipped, 2 failed.
--none--"
344768768,3131,asfgit,2017-11-15T23:53:45Z,"FAILURE
 7149 tests run, 5 skipped, 2 failed.
--none--"
344888050,3131,asfgit,2017-11-16T10:55:23Z,"FAILURE
 No test results found.
--none--"
344901719,3131,asfgit,2017-11-16T11:56:12Z,"FAILURE
 7149 tests run, 5 skipped, 2 failed.
--none--"
344906122,3131,asfgit,2017-11-16T12:16:46Z,"FAILURE
 7149 tests run, 5 skipped, 1 failed.
--none--"
345354879,3131,asfgit,2017-11-17T20:17:39Z,"FAILURE
 No test results found.
--none--"
345365938,3131,asfgit,2017-11-17T21:09:31Z,"FAILURE
 7149 tests run, 5 skipped, 2 failed.
--none--"
345376368,3131,asfgit,2017-11-17T21:55:41Z,"FAILURE
 7149 tests run, 5 skipped, 3 failed.
--none--"
330710008,3874,junrao,2017-09-20T00:22:38Z,"Also, in the request protocol and objects, we have ALTER_REPLICA_DIR_REQUEST and DESCRIBE_LOG_DIRS_REQUEST. Could we make them consistent?"
330768947,3874,lindong28,2017-09-20T07:29:33Z,"@junrao Thanks much for reviewing the patch! I have answered all comments and addressed most of them.

I think it is reasonable to rename ALTER_REPLICA_DIR_REQUEST to ALTER_REPLICA_DIRS_REQUEST. Given that it is merely a refactor and it touches many files, should we do this in a follow up patch before or after this patch is merged? I am not sure if you prefer to see this refactor in this pull request because it may make it harder to review this patch."
330771247,3874,lindong28,2017-09-20T07:40:34Z,@junrao I have update the patch to help discussion. I will fix test failures tomorrow.
330891633,3874,ijuma,2017-09-20T15:37:45Z,We can probably do the request naming fixes in a follow-up. I also raised some issues regarding the naming of the AdminClient methods. Maybe we can consider fixing those in the same follow-up.
330891767,3874,ijuma,2017-09-20T15:38:10Z,"@lindong28, there is a merge conflict and 6 test failures."
331022962,3874,ijuma,2017-09-21T01:13:16Z,"@lindong28, let me know when is a good time to start a system tests run."
331024201,3874,lindong28,2017-09-21T01:22:36Z,"@ijuma Sure. I am trying my best to address commands and add test. Can you tell me the deadline for getting this patch committed and pass system tests if we were to include it in 1.0 release? BTW, I am flying to Shanghai and will be on vacation 9/21 - 10/16. My response maybe a bit delayed due to the flight and various errand. Admittedly it makes it harder for me to focus on the patch and the test. I will do best effort."
331086912,3874,lindong28,2017-09-21T08:20:48Z,"@ijuma I have rebased the path onto trunk head and fixed most tests. Currently I disabled 5 tests in ReplicaFetcherThreadTest because these tests replied on the previous internal implementation of ReplicaFetcherThread.maybeTruncate() which is changed in my patch. I tried to fix the test for 1 hour but couldn't probably due to my lack of experience with easymock library.

Although all tests can pass, it seems that there is file descriptor leak because my mac explains about `Too many open files in system`. I will try to fix it tomorrow. I don't have a good idea where it comes from..

I think you can run the system test with my patch. It is probably a good idea to also run the system test without this patch.
"
331087122,3874,lindong28,2017-09-21T08:21:46Z,"FYI, there is one remaining issue related to how we rename directories to replace current log  with the future log. This may also take some time to discuss and implement."
331212055,3874,ijuma,2017-09-21T16:34:18Z,I started the system tests a few hours ago. I will check and post the status soon.
331243188,3874,lindong28,2017-09-21T18:32:08Z,"@ijuma I have fixed all tests in ReplicaFetcherThreadTest and rebased patch onto latest trunk. ""Too many open files in system"" seems to stop showing up when I run tests on Mac laptop after rebasing the patch.

The only remaining issue is how to promote future replica to be the current replica. I am awaiting Jun's reply."
333150135,3874,ijuma,2017-09-29T14:58:44Z,"@lindong28 @junrao There was a discussion about potentially tweaking the name of the AdminClient and request/response for `alterReplicaDir` and `describeLogDirs`. The AdminClient API would have to be changed before the code freeze next Wednesday. What is the current thinking? Two points from me:

1. As I raised previously, `alterReplicaDir` doesn't follow the naming convention of other AdminClient methods that are `plural`. I think we should definitely fix this one.
2. The lack of symmetry between `alterReplicaDir` and `describeLogDirs` is not ideal. Could we not simply call the latter `describeReplicaDirs` as well?

This would have to be a separate PR, but I just raised it here since there was an ongoing discussion about it. Happy to take it to a separate PR or JIRA."
333152696,3874,lindong28,2017-09-29T15:08:09Z,"@ijuma Regarding 1), I think it is a good idea to rename `alterReplicaDir` to `alterReplicaDirs`, and similarly rename the corresponding request and response. I will submit a patch to do it.

Regarding 2) , describeLogDirs has this name because it returns the information per log directory. There is already an existing AdminClient API `describeReplicaLogDir` which returns the log directory information per replica. So I am not sure we need to have symmetry between alterReplicaDir and describeLogDirs.

"
333157758,3874,ijuma,2017-09-29T15:26:26Z,"Sounds good @lindong28, I had missed the other method."
333158559,3874,lindong28,2017-09-29T15:29:11Z,"@ijuma Talking about describeReplicaLogDir, maybe we should rename this to describeReplicaLogDirs as well?"
333159096,3874,ijuma,2017-09-29T15:30:59Z,"Yes. Also, shouldn't `describeReplicaLogDir` be `describeReplicaDirs`? I thought that this one was supposed to be the dual of `alterReplicaDirs`?"
333160232,3874,lindong28,2017-09-29T15:35:21Z,"@ijuma It is describeReplicaLogDir because we return the log directory of the partition in the response, rather than the directory of the partition. The directory of the partition is essentially path_to_log_dir/topi-partition.

Previously I used alterReplicaDirs because alterReplicaLogDirs seems a bit tedious. Technically speaking we are changing both the log directory and the directory of the partition. So it seems fine to say alterReplicaDirs. Do you think we should rename it to alterReplicaLogDirs?"
333161547,3874,ijuma,2017-09-29T15:40:19Z,"Oh, I see. Yes, it does seem to me that it would be clearer if they both talked about `LogDirs`. It may make sense to hear if @junrao  has any thoughts before doing the renaming as it's a bit tedious to change it a second time."
333276886,3874,lindong28,2017-09-30T02:42:50Z,@junrao All issues have been addressed as discussed. I have made a pass through the patch and made minor improvements. I will pass through the patch another 2 times and let you know.
333287139,3874,lindong28,2017-09-30T06:15:50Z,"@junrao Also, thanks so much for taking time to review the patch! Your comments are really helpful."
333711087,3874,lindong28,2017-10-03T01:17:58Z,"@junrao Yes. I think we should commit https://github.com/apache/kafka/pull/3820 first. I am not sure about the details of the 1.0 release plan. My understanding is that we should focus first on the stability for 1.0 release at this time. Thus we should commit bug fixes such as https://github.com/apache/kafka/pull/3820 before 1.0 code freeze. This patch can be committed later after we have finished all necessary works needed for 1.0 release.

I will work on your comments. Thanks!"
333935484,3874,lindong28,2017-10-03T18:27:16Z,"@junrao @ijuma Thanks for your detailed comments! I have rebased patch onto trunk, renamed methods following KAFKA-5995, reviewed patch myself end-to-end, and replied to all comments. I think the only remaining issue is whether future replica should be truncated using leader epoch after the current replica is truncated.
"
335013622,3874,lindong28,2017-10-08T15:19:12Z,@junrao I have rebased patch onto trunk and replied or addressed the latest comments. I will review the patch end-to-end after we have agreed on the solution to the latest comments. Thanks!
335988493,3874,lindong28,2017-10-12T00:47:48Z,@junrao Let me review the patch one more time before committing this patch. Thanks!
336215355,3874,junrao,2017-10-12T17:52:21Z,"@lindong28 : There are a few system test failures on your branch. http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2017-10-11--001.1507803882--lindong28--KAFKA-5163--4c4a26f/report.html

Most of them seem to be related to the old consumer, which is potentially affected by the AbstractFetcherThread change in this patch."
336464525,3874,lindong28,2017-10-13T14:13:31Z,"@junrao There are 6 system test failure in the link you provided. I run system test locally for 3 of the 6 tests and all of these tests succeeded. Then I downloaded the log for these three tests (e.g. `kafkatest.sanity_checks.test_verifiable_producer`). For all these three tests, verifiable_producer.stdout shows the following exception:

```
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.kafka.clients.producer.ProducerRecord.<init>(Ljava/lang/String;Ljava/lang/Integer;Ljava/lang/Long;Ljava/lang/Object;Ljava/lang/Object;)V
        at org.apache.kafka.tools.VerifiableProducer.send(VerifiableProducer.java:232)
        at org.apache.kafka.tools.VerifiableProducer.run(VerifiableProducer.java:462)
        at org.apache.kafka.tools.VerifiableProducer.main(VerifiableProducer.java:500)
``` 

I am not sure how this exception could be related to this patch since this patch does not touch the constructor of ProducerRecord. Could you re-run the test, maybe locally, and see if the failure can be reproduced? Could you tell me which git hash was used in the system test?

BTW, I made one more commit to further improve the patch after I reviewed the patch end-to-end.
"
336473772,3874,ijuma,2017-10-13T14:45:04Z,You should rebase against trunk to fix the `NoSuchMethodError` issue.
336475395,3874,lindong28,2017-10-13T14:50:56Z,@ijuma Sure. I have rebased the patch onto trunk. Is this `NoSuchMethodError` a known issue which has been recently fixed in the trunk?
336476902,3874,ijuma,2017-10-13T14:56:09Z,Yes https://github.com/apache/kafka/commit/bdf8e211ecb50a24071390d493cf1c1732f7346d
336480112,3874,lindong28,2017-10-13T15:06:53Z,"@ijuma @junrao Thanks for the information. This explains the test failure and it is very likely that system test can succeed now that I rebased the patch onto trunk. Could you re-run the system test for this patch?

I have finished reviewing the patch myself and made all the improvements I can think of. Can you see if this patch is good to go? Thanks!!"
337089038,3874,lindong28,2017-10-17T01:19:22Z,@junrao Thanks for all the review! I have addressed all comments and rebased the patch onto trunk. I will go over the patch again. Do we need to re-run system test for this patch?
337093789,3874,junrao,2017-10-17T01:52:57Z,"@lindong28 : Thanks for the latest patch. LGTM. I kicked off another run of system tests. Once the tests finish, I can merge the patch."
337138220,3874,lindong28,2017-10-17T07:06:46Z,@junrao Thanks much! I have reviewed the patch again and slightly improved the error message.
337272700,3874,junrao,2017-10-17T15:36:14Z,@lindong28 : Thanks a lot for working on the PR. Merged to trunk.
337406756,3874,lindong28,2017-10-17T23:23:12Z,@junrao This is great. Thank you so much for all your time and review!!
208608858,1215,becketqin,2016-04-11T23:14:18Z,"It seems this conflicts with KAFKA-3510. I'll do the rebase.
"
208640666,1215,becketqin,2016-04-12T00:34:13Z,"cc @junrao. Will you be able to help review this patch? Thanks!
"
214130026,1215,junrao,2016-04-25T04:45:24Z,"Also, could you patch the DumpLogSegment tool to support the time index?
"
214479103,1215,ijuma,2016-04-25T18:55:55Z,"@becketqin, it would be nice to have some numbers for how long it takes to do search by timestamp before and after the change on a cluster with a reasonable amount of data.
"
214572966,1215,becketqin,2016-04-26T00:38:21Z,"@ijuma Is there a specific use case where the performance of searching by timestamp matters much? Supposedly searching by timestamp should be a relatively rare operation. With LogAppendTime, the time will be similar to offset search. With CreateTime, the time cost largely depends on how the data look like. i.e. if we have to scan tons of logs, it is going to be slow, otherwise it is fast. One thing for sure is that it is going to be slower than before which did not do much except for looking at the last modification time of segment files.
"
215312071,1215,ijuma,2016-04-28T05:13:32Z,"@becketqin As a general rule, we should be very thorough with regards to performance testing whenever we introduce a new disk structure (like an index). I gave one example, but we should really also verify the impact on normal operations as well. Even if we believe that the impact should be minimal, we cannot be sure until we have tested it.
"
215997683,1215,becketqin,2016-04-30T21:56:11Z,"@Ishiihara Thanks for the review.

@ijuma I see, yes, I agree that we should understand the performance impact of the changes we make in general. I was just curious if you care about the search by timestamp specifically.

@junrao I updated the patch to let the index point to the offsets of shallow messages instead of the next offsets. Would you be able to take another look? I am not sure if we are still able to make it into 0.10.0 given 0.10.0 should have cut off on yesterday.
"
223086486,1215,becketqin,2016-06-01T18:41:18Z,"@junrao I addressed your previous comments. Would you take another look?
"
224696267,1215,junrao,2016-06-08T19:09:38Z,"@becketqin : Thanks for updating the patch. As the patch is getting closer to be ready, could you run your branch on the system tests? Also, could you do some performance testing like you did in KIP-31/32 to see if there is any noticeable performance degradation?
"
225242928,1215,becketqin,2016-06-10T17:20:46Z,"@junrao Thanks a lot for the review. I have updated the patch. The only pending thing is whether we want to allow the empty time index or not. I replied to your comment. Please let me know if you think the benefit is good to have or not.

I have sent email to the mailing list regarding the time based log rolling behavior change to see if people have any concern.

I will run the performance test as we did for KIP-31/32 and update the result.
"
227025455,1215,becketqin,2016-06-19T22:54:48Z,"@junrao Thanks a lot for the review. I just updated the patch to address your comments. I will create another KIP to discuss the new search for timestamp semantic.
"
227599503,1215,junrao,2016-06-21T23:11:20Z,"@becketqin : Thanks for the latest patch. I made another pass and left some comments. Most of them are minor though. Once you have addressed those, it would be good if you could run through our system tests and post some performance testing results on the jira. Ideally, we probably want to load some sizable amount of data on at least 2K - 4K partitions in a broker and see if there is any performance degradation.
"
228078724,1215,becketqin,2016-06-23T15:02:22Z,"@junrao Thanks a lot for the patient and careful review. I am on vacation in China from Jun 22 to Jul 11. The network becomes a little flaky. Could you help kick off a system test? I will see if I can go to our newly opened Shanghai office and use the VPN there to run the performance test on our test cluster. Thanks again.
"
231480822,1215,junrao,2016-07-08T21:46:21Z,"Also, in DumpLogSegments, could we dump the timestamp in the .log file too?
"
231481580,1215,junrao,2016-07-08T21:50:36Z,"@becketqin : I made another pass and left a few more comments. It would be good if you can run some performance tests to see if there is any degradation. For instance, with this PR, we will need to read the last entry of each time index during broker startup. Not sure how much impact this will have on starting time. So, it will be good to test this out.
"
232181730,1215,becketqin,2016-07-12T21:07:42Z,"@junrao Thanks for the review and sorry for the late response. I just returned from China yesterday. I have updated the patch to address your comments. Because we are in the middle of data center cutting over, it may take some time to setup the new dev test environment. I will do the performance test this week.
"
232449796,1215,junrao,2016-07-13T18:45:59Z,"Also, I left a comment in the jira about making the output of DumpLogSegments clearer. Could you take a look?
"
232879331,1215,becketqin,2016-07-15T07:07:47Z,"@junrao I just updated the patch to address the latest comments. One thing worth mentioning is that dumping the active time index may be slow because time index requires looking up the offset index. The offset index of the active segment is always the max index size. This will cause the dump log segment tool to create an offset index with a lot of empty index entries in the end so the binary search in this case does not quite work. Also because the active segment is still growing, dumping it may have some race condition and result in false alarm. So I suppose it is fine to assume that people won't dump the active log segment.
"
233226827,1215,junrao,2016-07-18T04:28:00Z,"@becketqin : Thanks for the latest patch. It looks good to me. Do you have any updates on performance tests and the system tests?
"
233382135,1215,becketqin,2016-07-18T16:29:54Z,"@junrao Thanks for the review. I have tested the produce and consume performance with the same script we used for KIP-31/32. The producer and consumer performance are the same. 

Our new dev clusters are ready for use now, but I am still waiting for the mirror maker instances, which should be setup by today. I will run the performance test on the real data and test the segment loading and recovery time with more partitions after that.
"
233383485,1215,becketqin,2016-07-18T16:34:48Z,"For system test, I was trying to use the confluent jenkins server before. But it seems that it does not support building against an arbitrary branch anymore. I am setting up a Hudson job to run the system test internally at this point. But that may take some time as I am not familiar with that.
"
233386240,1215,ijuma,2016-07-18T16:44:46Z,"@becketqin, you can use https://jenkins.confluent.io/job/system-test-kafka-branch-builder/
"
233400013,1215,becketqin,2016-07-18T17:34:12Z,"@ijuma Got it. Thanks.
"
234819292,1215,becketqin,2016-07-25T01:49:03Z,"@junrao Sorry for the late response. I have run the performance test. On a broker with ~15500 log segment files. The log segments loading time is about 6 seconds without this patch. On another broker with 17700 log segments files, the log loading time was 35 seconds with this patch. The results varied a little from run to run, but it is 4x - 6x slower when we need to load the timestamp from the log segment. For a large Kafka cluster, this might be an issue. One optimization may be load the log maximum timestamp lazily. I'll update the new patch after testing that.

I did not see any throughput issue with this patch. So the impact is only at startup time. 
"
235023979,1215,junrao,2016-07-25T17:32:07Z,"@becketqin : Thanks for the results. So the 4x-6x overhead mostly comes from reading the last entry of the time index?

You also did producer/consumer perf test on a large number of topic/partitions and saw no degradation in throughput?
"
235178030,1215,becketqin,2016-07-26T06:50:43Z,"@junrao I made some further test. The previous 4x - 6x slow down may not be very accurate. I noticed a bug in the code that may cause unnecessary rebuild of the indexes if a time index is empty.

After fixing the bug, on a broker with ~20000 log segment files, I got the following results:
1. log loading time: ~13 seconds without time index. 27-28 seconds with time index.:
2. I tested a patch that skips sanity check and loads the max timestamp lazily so we don't read the last entry at starting time. The time reduced by 1-2 seconds but not much.

So it looks that the slow down was mainly caused by the system calls that create the memory mapped files. Because we are creating 2x many memory mapped files, it takes about 2x of the time.

I tested the throughput with part of the production data. The brokers handled fine. And the produce request total time and local time looks similar. I will try to push more traffic and see if that makes any difference.
"
235442848,1215,junrao,2016-07-27T00:01:35Z,"@becketqin : Thanks for the update. Is memory-mapping the index really that expense? Could you do some micro-benchmark to test this out?

Also, have you run the system tests?
"
235443110,1215,becketqin,2016-07-27T00:03:12Z,"@junrao Yes, I have run system test and it passed. Sure, I can run the micro benchmark on mmap.
"
235443205,1215,becketqin,2016-07-27T00:03:48Z,"@junrao the link to system test run. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/483/parameters/
"
235774517,1215,becketqin,2016-07-28T01:49:29Z,"@junrao I did some investigation on the segment loading and here are the findings:
1. mmap() and resize() takes much time during reloading.
2. mmap() speed dramatically slows down after 20000 files.
3. resize() is also much slower when timeindex files are added.

The summary can be found here:
https://docs.google.com/spreadsheets/d/14aahuei8dXALtwniyWDpNrsNtEuYc1-L5mPwRhDnWuQ/edit#gid=550436079

In the graph each bucket contains 250 mmap() or resize() call. The time is the summary of the time taken by all the calls in that bucket. For example, the first 250 mmap() calls during log loading goes to the first bucket, the next 250 mmap() calls goes to the second bucket, and so on.

I am not sure what is the right solution here. Maybe there are some OS level tweaks to avoid the deterioration of the mmap() time and resize() time, but I haven't looked into that yet.
"
236021027,1215,junrao,2016-07-28T20:50:27Z,"@becketqin : Thanks for the results. The results in ""log loading time distributing"" shows that loading the log with timestamp index takes 15 times longer than without (399 secs vs 21 secs). Is that correct? If the overhead is in mmap(), with timestamp index should be at most twice as slow. Does this suggest there is sth in the timestamp index that is much more expensive than the offset index?
"
236038932,1215,becketqin,2016-07-28T22:02:43Z,"@junrao Sorry I did not explain it clearly. In the google sheet there are also two sheets showing that the time taken by mmap() is related to how many files has already been memory mapped. After we have mapped ~20000 files, the time taken for further mmap() calls increased dramatically.

The reason we see a 15 times larger loading time is the following. Currently I have about 22000 log segments in a broker. So that results in 22000 offset index files. It took 21 seconds to load. When we do mmap for those files, it is roughly at the tipping point where the mmap() becomes taking much longer. 

Now we added the time index file so the number of mmap() call is doubled. Even though the first 22000 mmap() calls took the same time as before, which is 21 seconds, the next 22000 mmap() calls took much longer. It does not matter whether the mmap() calls are for offset index file or time index file. Because after adding the time index, presumably the first 22000 mmap() calls are half/half for each type of index files.

I am not sure if this is OS specific and only an issue for our environment or it is a general issue.
"
236065457,1215,junrao,2016-07-29T00:40:50Z,"@becketqin : What happens if you do an experiment with 44K segments and turn off the time index? Does that also take 15 times longer? Were the mmap results based on a micro benchmark or did you extract that from the segment loading test?
"
236270191,1215,becketqin,2016-07-29T19:19:09Z,"@junrao I ran test with 8500 log segments with time index files. They are loaded in less than 3.5 seconds. And it took 1.5 second without time index files. Not sure if that is equivalent to testing files with 44K, but apparently the time does not grow linearly with the number of log segments. 

The mmap results are extracted from the segment loading test. Micro benchmark which repeatedly maps a single file does not reveal this scalability issue.
"
236301416,1215,junrao,2016-07-29T21:43:44Z,"@becketqin : Thanks for the new results. It feels a bit weird that the turning point is at exactly 22K segments. Also, in the ""time to resize"" sheet, the results seem to suggest that resizing at 10 batches with time index is 6 times more expensive than resizing at 10 batches w/o time index. This seems to be at the pointe where the number of loaded segments is still well below 22K?

Is it possible to do an experiment with 44K segments and turn off the time index? This will tell us more about whether the overhead comes from scalability related to mmap() or something else in the time index.
"
236312009,1215,becketqin,2016-07-29T22:44:13Z,"@junrao I refined the profiling code a little, I can stably reproduce the mmap() time issue. But the resize time seems to be consistent now. I'll do an experiment on a broker with ~50K segments without time index and see what happens there.
"
237590174,1215,junrao,2016-08-04T15:31:50Z,"@becketqin : Any new updates on the experiment?
"
239509491,1215,junrao,2016-08-12T17:31:27Z,"@becketqin : Are you still working on this? There are other KIPs such as KIP-58 that will be depending on this patch. So, we probably don't want this patch to sit for too long. We are pretty close to committing this patch.
"
239598105,1215,becketqin,2016-08-13T02:56:55Z,"@junrao Sorry, I just saw your ping... Yes, I am still working on this but was distracted by something else before. I will work on this this weekend and get it done. 

I tried with 50K segments, and it took about 11 seconds to load all the segments without time index, and about 30 seconds with the time index.

I ran a bunch of tests on different machines. It seems that the machine I originally ran the tests on has some disk issue. A stably reproduceable result was that loading the segments took 3-4x longer with the time index. And what interesting is that for some of the logs with small number of log segments (e,g 100) I saw the loading time is significantly longer than other logs with much more segments (e.g. 3000). This happened both w/ and w/o time index. I am trying to see what caused this. 

I'll update the findings this weekend.
"
239683336,1215,junrao,2016-08-14T16:41:56Z,"@becketqin : Thanks for the update. Is the loading time affected by # of log segments, but not by the size of log segments? It seems that loading the time index is still more expensive than the offset index? Looking at the code, we could make a couple of simple improvements.
1. In the constructor of LogSegment, we call timeIndex.lastEntry twice unnecessarily.
   /\* The maximum timestamp we see so far */
   @volatile private var maxTimestampSoFar = timeIndex.lastEntry.timestamp
   @volatile private var offsetOfMaxTimestamp = timeIndex.lastEntry.offset
2. In TimeIndex.sanityCheck(): we also call lastEntry twice unnecessarily.

Not sure how much difference they will make. Does profiling reveal which part of loading time index is expensive? If it's in calculating maxTimestamp and offsetOfMaxTimestamp, it may be possible to do the calculation lazily. This probably will make the code more complicated. So, probably only worth doing if there is significant performance gain.
"
239713049,1215,becketqin,2016-08-15T01:34:37Z,"@junrao I have been running tests this afternoon. 
Actually the previous results I got have included the changes you mentioned above. In addition, in the sanity check one difference for time index is that we also check that the last indexed timestamp is greater than the first indexed timestamp. The requires reading both the first and last entry. I also commented out that check.

The broker that used to have > 40000 segments has only 12000 segments now due to the retention. So I was not able to run the tests on the 40000 partitions. I started to populate data to the cluster again. Hopefully I will have >40000 segments by tomorrow. 

One promising experiment I did this afternoon was changing the number of log recovery threads. This seems reduce the mmap time effectively. But since I do not have a 40000-segments broker this afternoon, it is hard to say if that is going to help in a large broker. I'll try this out tomorrow.

Back to the tests I ran earlier on a 40000 segments broker and this afternoon, the profiling I did was the following:
1. the time used to load each partition 
2. the time used for mmap function specifically to see if there is long mmap (>100 ms)
3. the time used in sanity check.

The profiling shows time spent on (1) is not completely related to the # of segments in the partition. For example, a partition with 1 segment may takes 35 ms to load while another partition with 12 segments only takes 4 ms to load. Even for the two partitions of the same topic that have the exact same number of segments, the time can differ by 10x from 5 ms to 50 ms. **This happens both with and without time index.**

When I look at the partitions that takes longer to load (e.g. 2 seconds w/o time index and 6 seconds w/ time index), those segments tend to be loaded later in the segment loading process.

As for time spent on (2), I did not see any long mmap when the number of segments is less than 12000, whether with or without time index. But after the # of segments is greater than 20000, some long mmap() are witnessed. Again, those long mmap call only occur on the log segments that are loaded later in the segment loading process. The loading time was pretty linear when number of segments is 12000. It took about 2 seconds to load the segments without time index, and took about 4 seconds with time index.

Regarding the time spent on (3), when we have to read both the first timestamp and last timestamp in the time index, the sanity check time significantly grew, but still short compared with the time spent on loading the files.

Based on the test I have run, it seems clear that
1. The biggest part of segment loading time is mmap(), no matter w/ or w/o time index.
2. It does not seem that loading time index is more expensive than loading offset index.
"
239843860,1215,junrao,2016-08-15T15:59:22Z,"@becketqin : Thanks for the updates. 
1. What file system were you using in your testing?
2. Earlier you mentioned that loading 50K segments w/o time index took 11 seconds. How does that compare with say loading 25K segments with time index? If loading time index is no more expensive than loading the offset index, you would expect the latter to take no longer than the former.
"
239872879,1215,becketqin,2016-08-15T17:45:07Z,"@junrao The file system type is xfs. Currently I have ~19000 segments on the broker. Using 10 log recovery threads, it takes about 3 seconds to load without time index and about 10 seconds with time index. If I reduce the log recovery threads number to 5, the loading time becomes 6 seconds with time index.

So it looks that the number of log recovery threads was causing the slow log segments loading. I will test this on a 25K segments cluster with time index as well after the broker reaches there.
"
239929404,1215,junrao,2016-08-15T21:07:01Z,"@becketqin : Thanks for the info. So using more log recovery threads slows down the loading with time index?

Also, about mmap() becoming more expensive with more log segments. @enothereska mentioned to me that metadata lookups in file systems get more expensive with more open file handles since the lookups are based on an implementation of red-black tree. That's mostly CPU overhead. In your tests, do you notice any I/O activity when mmap() becomes more expensive?
"
240002314,1215,becketqin,2016-08-16T05:07:29Z,"@junrao I did not check I/O activities when mmap() becomes more expensive, it is kind of hard to check because the segments loading only takes a few seconds.

I tested log loading on 25K segments with time index using 5 loading threads. It takes about 13 seconds, which is comparable to the 11 seconds on a 50K segments cluster without time index. I will try to test it on a 40K cluster tomorrow.

If the test results look good, I will update the patch with a few bug fixes and rebase on trunk as well.
"
240139087,1215,junrao,2016-08-16T15:29:29Z,"@becketqin : Thanks for the update. In your new patch, could you include a summary of the performance impact (and perhaps the ideal number of log recovery threads) in the upgrade doc?
"
240276231,1215,junrao,2016-08-17T00:01:54Z,"@becketqin : I am trying to hold off https://github.com/apache/kafka/pull/1494 and https://github.com/apache/kafka/pull/1742 to avoid the rebasing overhead of this patch. Do you think you could provide your latest patch in the next day or two? 
"
240319219,1215,becketqin,2016-08-17T05:48:16Z,"@junrao I have all the code ready now. I am still trying to find the reasonable number of recover threads. It seems there is system cache effect, i.e. if I do a start-shutdown-start sequence, the second start would be much faster than the first one because some of the disk read may come from the system cache. To avoid this effect, I need to wait for a while between each broker startup. I will upload the patch latest by Thursday. Is that OK?
"
240436736,1215,junrao,2016-08-17T14:51:07Z,"@becketqin : Thursday is fine. Not sure if this helps, but you can empty dentries/inodes through a command (http://www.tecmint.com/clear-ram-memory-cache-buffer-and-swap-space-on-linux/).
"
240630032,1215,becketqin,2016-08-18T05:50:28Z,"@junrao Thanks for the dentries/inode cleaning command. I just saw them and haven't tried that yet. Currently I have a script reboot a broker every hour and it seems stable enough to give some results.
Some updates on the number of log recovery thread tests. For a cluster with about 40K segments, with one log recovery thread, it takes 14 - 17 seconds to load the logs, and about 20 seconds with 2 threads, > 30 seconds with 5 threads. So it seems that having a single thread loading segments is the best setting.

I have updated the patch after rebasing on trunk. I also squashed some previous commits. 
"
240802214,1215,junrao,2016-08-18T17:50:47Z,"@becketqin : Thanks for the update. Are your latest numbers with time index? What about the numbers w/o time index? Also, how many log dirs are there since the number of recovery thread is per dir?
"
240815948,1215,becketqin,2016-08-18T18:38:20Z,"@junrao The latest numbers are with time index. I see log loading time to be 21 - 23 seconds on a 46K segments with time index. It takes about 10 seconds to load the segments without time index. So the loading time with time index is about 2x of without time index. This is also comparable to the 11 seconds loading time without time index on a 50K segments broker. 

I have ~8500 directories (partitions) on the broker. Some of the partitions are pretty big (> 2500 segments).
"
240838240,1215,junrao,2016-08-18T19:58:31Z,"@becketqin : Thanks. Could you confirm the number of log dirs you have and the number of disks per dir? Intuitively, if there are multiple disks per dir, it seems using more than one 1 recovery thread should lead to better performance.
"
240859249,1215,becketqin,2016-08-18T21:15:27Z,"@junrao We only have one log directory on a RAID 10 containing 10 disk. So in our case, multiple thread might not help. In a JBOD environment, I agree that multiple threads would probably help.
"
240870837,1215,junrao,2016-08-18T22:01:14Z,"@becketqin : Interesting, in theory, using more than 1 thread should still help since those threads can drive the I/Os on different disks in parallel. 
"
240883073,1215,becketqin,2016-08-18T23:04:45Z,"@junrao I am not sure, one thing I noticed is that when there is one thread, the log loading time of each partition is pretty linear to the number of log segments. But if multiple threads are used, the linearity goes away.

For example, the following logs are from running the code using 10 log recovery threads without time index (`Old_Mark` indicates no time index, and the same random Old_Mark means they are from the same test run).

For TOPIC1, it only took ~400 ms to load about 2000 log segments, at about the same time (300 ms earlier) TOPIC2 takes 22 ms to load ~70 log segments. However, later on the same run (3 seconds later), for some other partitions of TOPIC2 that has about the same number of segments, it takes much longer (~400 ms) to load. This was the reason that I suspected that the later mmap() calls are more expensive, which seems also proven from profiling by measuring the time on mmap calls in different buckets.

I am not sure why this happens, but I didn't see such issue when there is one log loading thread.

```
2016/08/16 20:57:28.793 INFO [Log] [pool-9-thread-3] [kafka-server] [] (22 ms) Completed load of log TOPIC2-7 with 74 log segments and log end offset 1835960333 (Old_Mark 5303310516390717976)
2016/08/16 20:57:28.794 INFO [Log] [pool-9-thread-1] [kafka-server] [] (22 ms) Completed load of log TOPIC2-4 with 84 log segments and log end offset 1829077019 (Old_Mark 5303310516390717976)
2016/08/16 20:57:28.795 INFO [Log] [pool-9-thread-2] [kafka-server] [] (23 ms) Completed load of log TOPIC2-8 with 68 log segments and log end offset 1831459833 (Old_Mark 5303310516390717976)
2016/08/16 20:57:28.795 INFO [Log] [pool-9-thread-4] [kafka-server] [] (24 ms) Completed load of log TOPIC2-2 with 79 log segments and log end offset 1836145525 (Old_Mark 5303310516390717976)
......
2016/08/16 20:57:29.133 INFO [Log] [pool-9-thread-1] [kafka-server] [] (337 ms) Completed load of log TOPIC1-2 with 1973 log segments and log end offset 7832957736 (Old_Mark 5303310516390717976)
2016/08/16 20:57:29.177 INFO [Log] [pool-9-thread-2] [kafka-server] [] (380 ms) Completed load of log TOPIC1-9 with 2020 log segments and log end offset 7871065990 (Old_Mark 5303310516390717976)
2016/08/16 20:57:29.203 INFO [Log] [pool-9-thread-5] [kafka-server] [] (407 ms) Completed load of log TOPIC1-7 with 2076 log segments and log end offset 7839205001 (Old_Mark 5303310516390717976)
2016/08/16 20:57:29.245 INFO [Log] [pool-9-thread-3] [kafka-server] [] (447 ms) Completed load of log TOPIC1-0 with 2078 log segments and log end offset 7852775406 (Old_Mark 5303310516390717976)
...
2016/08/16 20:57:32.100 INFO [Log] [pool-9-thread-1] [kafka-server] [] (387 ms) Completed load of log TOPIC2-112 with 77 log segments and log end offset 371172232 (Old_Mark 5303310516390717976)
2016/08/16 20:57:32.100 INFO [Log] [pool-9-thread-2] [kafka-server] [] (383 ms) Completed load of log TOPIC2-122 with 68 log segments and log end offset 373822294 (Old_Mark 5303310516390717976)
2016/08/16 20:57:32.110 INFO [Log] [pool-9-thread-4] [kafka-server] [] (393 ms) Completed load of log TOPIC2-27 with 81 log segments and log end offset 375406677 (Old_Mark 5303310516390717976)
2016/08/16 20:57:32.124 INFO [Log] [pool-9-thread-3] [kafka-server] [] (406 ms) Completed load of log TOPIC2-17 with 72 log segments and log end offset 371089327 (Old_Mark 5303310516390717976)
```
"
240895715,1215,junrao,2016-08-19T00:22:28Z,"@becketqin : Thanks for the latest patch. I made another pass and only had a few minor comments. Once those are addressed, I can merge in the patch.
"
240931044,1215,becketqin,2016-08-19T05:25:29Z,"@junrao Thanks a lot for all the patient reviews. I updated the patch to address you latest comments and added a few unit tests. Thanks again.
"
241075286,1215,junrao,2016-08-19T17:05:05Z,"Thanks for the patch. LGTM
"
241077444,1215,junrao,2016-08-19T17:13:31Z,"@becketqin : Thanks a lot for working on this diligently. A couple of followups.
1. I left a couple of minor comments after merging. Could you address them in a followup jira?
2. Do you plan to work on a followup KIP to add a new seekToTimestamp() api to the consumer and a new request for getting offsets based on timestamp?
"
241471040,1215,becketqin,2016-08-22T16:33:50Z,"@junrao Thanks a lot for the patient review. I will submit a follow up PR soon. And yes, I will start working on the KIP to add seekToTimestamp() API now and post the wiki this week.
"
241557337,1215,ijuma,2016-08-22T21:31:19Z,"@becketqin, it would be good to update the Kafka documentation to mention the time index. Maybe an easy way is to search it for offset index and see if it makes sense to mention the time index in those cases as well. Would you be willing to file a JIRA and take this on as well?
"
418101464,5527,Kaiserchen,2018-09-03T12:30:21Z,"We should just remove all the `final` keywords, I don't think they add any benefit?"
418140255,5527,bellemare,2018-09-03T15:04:25Z,"I had to add all the final keywords to pass the linting check - IIRC, my first run had dozens of linting errors preventing compilation. "
470747449,5527,mjsax,2019-03-07T23:47:51Z,@bellemare What is your JIRA ID? Would like to assign the ticket to you.
470940088,5527,bellemare,2019-03-08T14:07:49Z,@mjsax JIRA ID is abellemare
481338093,5527,sachabest,2019-04-09T17:05:52Z,"Hi, sorry to intrude on a potentially stale PR, but is this functionality still in development? Would be extraordinarily useful for joining two changelog-like entities. "
481350147,5527,pgwhalen,2019-04-09T17:23:26Z,"I sure hope so, my team is looking forward to it as well!  Given that the KIP was accepted a few weeks ago, I think it's safe to say it will make it in fairly soon.  I would definitely pick up development if @bellemare can't continue. "
481367621,5527,bellemare,2019-04-09T18:06:47Z,Hey folks - I'm still trying to get the code put together and finalize some of the changes that were outlined in the KIP. Stay tuned! 
482606869,5527,bellemare,2019-04-12T14:59:37Z,"Hi All - I'm at a point where I need some feedback on a couple of things:
1) The organization of the code
2) The organization of the graph nodes in KTableImpl
3) Insights into why I am getting NullPointerException in KTableKTableForeignKeyInnerJoinMultiIntegrationTest (though not consistently). I believe this is a misunderstanding on my part as to how partitions are co-partitioned, but there may be more to it that I am missing. Basically, it seems that depending on how the tasks and partitions are assigned, we either get a `java.lang.NullPointerException: Task was unexpectedly missing for partition table1-1` or it we don't.
**This must be resolved if we wish to have flexible partition counts for joining, ie: FK join a topic with 3 partitions and a topic with 7 partitions**

4) Anything else.

Feedback is very much appreciated, as this is the first PR I've put up against Kafka and I'm sure I've violated a number of things."
483831611,5527,vvcephei,2019-04-16T20:31:09Z,"Hi @bellemare ,

Thanks for your PR! I'll review this as soon as I get the chance, and pay particular attention to the points you called out.

-John"
491508007,5527,bellemare,2019-05-11T12:43:54Z,"@vvcephei Hi John - thanks for the feedback so far! I haven't had time to attend to this due to some recent personal matters, but I should be able to take a crack at it this upcoming week. I think that (in my mind) the discussion about the topic names has been resolved, so I don't think there are any impediments other than me getting this cleaned up and then rebased to trunk."
492267940,5527,bellemare,2019-05-14T14:43:41Z,"@vvcephei - Completed all your feedback so far John. Thanks so much.
I also updated the Wiki page to more clearly show the underlying implementation in the diagram. I renamed some of the processors and hopefully added more clarity with the comments and other work. 

Currently, I do not know enough about the underlying mechanisms to get variable-partition counts working (ie: join a KTable with 7 partitions with that of 11 partitions). This is explained above in the April 12th post on KTableKTableForeignKeyInnerJoinMultiIntegrationTest. Multi-partition support could be added in a later revision if we wish to get this in for 2.3.

I will rebase this to trunk and commit that too shortly."
492710161,5527,bellemare,2019-05-15T15:44:35Z,"Rebasing to trunk has been considerably longer than I planned. Dealing with the new timestamped data stores has been a bit of a nightmare. Additionally, data which used to be present in the KTableImpl class is no longer available. In the KTableImpl constructor,storebuilder and isQueryable have been replaced by materialized.queryableStoreName(), which means that I do not have the ability to attach my resolver to the original, ""this"" materialized instance in the case where a queryable name is not set. I will look at ways to resolve this, but I do not anticipate being done before 2.3. I have spent considerable time on it in the past day and it's looking like much more is required."
492729592,5527,vvcephei,2019-05-15T16:36:19Z,"Hey @bellemare ,

Thanks for the update, and for the rebase work. Yes, the new timestamped stores changed a *lot* of implementation classes. It's a bummer that it happened to get merged after you forked. I agree, it's unlikely that this be able to get merged by Friday (the feature freeze for 2.3).

But no worries, it just means that we'll have more time to review it, write lots of tests, system tests, work on docs and blogs, etc., before it does get released, which decreases the overall risk of such a big feature.

Once you finish up the rebase, I'll take another pass. I didn't make it all the way through last time, and wound up just commenting on the little things I noticed along the way.

-John"
493129698,5527,bellemare,2019-05-16T16:02:58Z,"There's an issue with the changing of the KTableImpl API 

2.3-trunk:
```
public KTableImpl(final String name,
                  final Serde<K> keySerde,
                  final Serde<V> valSerde,
                  final Set<String> sourceNodes,
                  final String queryableStoreName,
                  final ProcessorSupplier<?, ?> processorSupplier,
                  final StreamsGraphNode streamsGraphNode,
                  final InternalStreamsBuilder builder) {
    super(name, keySerde, valSerde, sourceNodes, streamsGraphNode, builder);
    this.processorSupplier = processorSupplier;
    this.queryableStoreName = queryableStoreName;
}
```
2.0-trunk (what I had rebased it to last)
```
public KTableImpl(final String name,
                  final Serde<K> keySerde,
                  final Serde<V> valSerde,
                  final Set<String> sourceNodes,
                  final String queryableStoreName,
                  final boolean isQueryable,
                  final ProcessorSupplier<?, ?> processorSupplier,
                  final StreamsGraphNode streamsGraphNode,
                  final InternalStreamsBuilder builder) {
    super(name, keySerde, valSerde, sourceNodes, streamsGraphNode, builder);
    this.processorSupplier = processorSupplier;
    this.queryableStoreName = queryableStoreName;
    this.isQueryable = isQueryable;
}
```
`isQueryable` and `queryableStoreName` in 2.0 have been replaced by just `queryableStoreName` in 2.3, with a null value intending to mean that it is not queryable.

The problem is that the oneToMany joiner needs to query the underlying statestore previously represented by `this.queryableStoreName` during resolution of the hash code values. Previously, queryableStoreName was populated even if the underlying state store was anonymous (ie: not passed in as a Materialized parameter). Now, however, if the user does not materialize a state store, the anonymous one is NOT passed in as `this.queryableStoreName`. I think that it was only my code which would be affected by this unfortunately. I think I will need to change it back to how it was in 2.0 unless someone has any other ideas about how to get the underlying queryableStoreName when it is anonymous. "
495390035,5527,mjsax,2019-05-23T21:22:34Z,"With KIP-258 being merged, I think this PR should switch to use `TimestampedKeyValueStores`, too. I try to do a full review soon, now that feature freeze deadline is over I have a little more head room :)"
496669680,5527,adaniline-traderev,2019-05-28T20:08:47Z,"Hi, I was using this PR before successfully, but I get an exception now:
```
Exception in thread ""main"" java.lang.NullPointerException: state store name must not be null
	at java.util.Objects.requireNonNull(Objects.java:228)
	at org.apache.kafka.streams.processor.internals.InternalTopologyBuilder.connectProcessorAndStateStores(InternalTopologyBuilder.java:596)
	at org.apache.kafka.streams.kstream.internals.graph.TableProcessorNode.writeToTopology(TableProcessorNode.java:78)
	at org.apache.kafka.streams.kstream.internals.InternalStreamsBuilder.buildAndOptimizeTopology(InternalStreamsBuilder.java:289)
	at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:556)
	at org.apache.kafka.streams.StreamsBuilder.build(StreamsBuilder.java:545)
```
This happens on TableProcessorNode(""KTABLE-SOURCE-output0000000046""), which is a child of KTableKTableForeignKeyJoinResulutionNode(""KTABLE-SOURCE-resolver0000000045"")

I think what broke it is that this time the join is done after filter (to emulate a left join):
```
tableA.filter(predicate).join(tableB, fkMapper, valueJoiner, materialized)
```

@bellemare - I see ""Is this right?"" comment in the code (org.apache.kafka.streams.kstream.internals.KTableImpl#doJoinOnForeignKey) - this.queryableStoreName is null, but it is asserted not to be null later.
```
        final TableProcessorNode outputTableNode = new TableProcessorNode<>(
                outputProcessorParameters.processorName(),
                outputProcessorParameters,
                storeBuilder,
                new String[]{this.queryableStoreName}  //Is this right? The TableProcessorNode will try to do something with this???
        );
```"
496750446,5527,bellemare,2019-05-29T01:36:58Z,"@adaniline-traderev 
This is a consequence of the changes made in https://github.com/apache/kafka/commit/c0353d8ddce88bac6fc04f85dd40cb95b8ca5cf9 by @guozhangwang (of which I need his guidance on for how best to remedy this issue). 

The workaround is to manually materialize the KTables you are going to use this join function on. For instance, instead of:
```
tableA = builder.table(""inputTopic"", Consumed.with(...));
```
use:
```
tableA = builder.table(""inputTopic"", Consumed.with(...), Materialized.as(""user-TableA"")... );
```
and then this should work (because you provided a queryableStoreName):
```
tableA.filter(predicate).join(tableB, fkMapper, valueJoiner, materialized)
```

Prior to this change, the queryableStoreName was either the user-provided name Materialized.as(""StoreName"") OR the internal name, if no materialized name was provided. From the change notes itself: `KTableImpl's queryableStoreName and isQueryable are merged into queryableStoreName only, and if it is null it means not queryable. As long as it is not null, it should be queryable (i.e. internally generated names will not be used any more).`

The consequence of this is that there is now no way (as far as I can tell) for the KTableImpl to access internally generated state stores, which effectively kills this ticket. Previously I was able to access both user-materialized KTable state stores as well as internal ones. Now I can only access user-materialized ones, which is a non-starter for getting this committed. The only way forward that I can see at the moment (due to my own limited understanding of the code) is to revert the KTableImpl changes that @guozhangwang implemented. To do so would be non-trivial, and I would like his guidance to do so. Until we get past this detail, this JIRA is blocked.

"
497091665,5527,adaniline-traderev,2019-05-29T20:13:01Z,"@bellemare , thank you, with your workaround I was able to progress further. I have another problem now - fk join calls serializers for both tableA and tableB records with the same topic name, causing compatibility error from the schema registry.
The first call is for tableB value, triggered from
https://github.com/apache/kafka/blob/0d6d92e4a137ac06f3f6eb28495a38714a5734e1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/ForeignKeySingleLookupProcessorSupplier.java#L84-L90
The second call is for tableA value, triggered from 
https://github.com/apache/kafka/blob/0d6d92e4a137ac06f3f6eb28495a38714a5734e1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplier.java#L86-L89
The topic name is ""applicationid-steam-KTABLE-JOINOTHER-KTABLE-REPARTITION-0000000041-value""
Is it right to pass topic name in the wrapper serde here?
https://github.com/apache/kafka/blob/0d6d92e4a137ac06f3f6eb28495a38714a5734e1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerde.java#L53-L56"
497104592,5527,bellemare,2019-05-29T20:52:23Z,"@adaniline-traderev This is where it gets tricky. There was a discussion previously in this review (and spilling over into the Confluent schema registry git repo) over whether considerations for the schema registry need to be taken into account or not. My intention was to have this resolved so that it would be favourable for the Schema Registry SerDes (since I too use them) but I have not gotten far enough with testing to hit the error you encountered.

I believe a quick and dirty workaround, if you don't care about the registering dummy schemas, is to change the following:
https://github.com/apache/kafka/blob/0d6d92e4a137ac06f3f6eb28495a38714a5734e1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplier.java#L89
I believe if you create a randomly generated integer during the creation of this class, you can add it to the end of the topic name in L89. It should remedy the issue of registration. If it does not... I suggest just mucking around with the context.topic() names in the various processors and serdes until it works. It's not great, but since I don't have this running in front of me it's the best I can offer right now (and is still definitely a hack).

The problem with the confluent schema registry is that it registers even null-topics, so unless you pass in dummy topics or are absolutely sure your topology isn't registering two schemas on the same internal topic (which it shouldn't even be doing in the first place...), you're kinda screwed. The next step in KTables should be separating the Consumed.with() serdes from the Materialized.with() serdes, so that we can use the confluent ones only for consuming the topic but not at all within the internalized topology.

"
497406670,5527,guozhangwang,2019-05-30T17:09:48Z,"@bellemare I left a comment under your question above, but since I've not done a full pass over it I'm actually not sure if my proposal would work for you (briefly looked into `SubscriptionResolverJoinProcessorSupplier` but still not clear the exact line where it would NPE)"
497731250,5527,bellemare,2019-05-31T14:35:06Z,"@adaniline-traderev I threw in a fix for the schema registry. The main issue is that the valueSerializer here does not have access to the correct topic name.  https://github.com/apache/kafka/blob/0d6d92e4a137ac06f3f6eb28495a38714a5734e1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplier.java#L86-L89 

I have made a dummy topic name, since I do not think we can actually access the true ""TableA"" topic name without a bunch of hacky work. If anyone strongly objects to this they can come up with a better suggestion."
497766138,5527,adaniline-traderev,2019-05-31T16:05:01Z,"@bellemare Thanks, I had to solve it myself by creating a wrapper serde that always passes the correct topic name"
497778195,5527,bellemare,2019-05-31T16:39:43Z,"@adaniline-traderev 
Can you elaborate? My impression here is that the serializers I have created in this ticket are always receiving the correct topic, since they're simply using the existing streams framework when they get forwarded.

If I am doing something wrong in this PR, please point it out. "
497794730,5527,adaniline-traderev,2019-05-31T17:29:12Z,"@bellemare , sorry, I meant that I did this before your change as a workaround - have not had time to test your change yet.
So, to clarify, with the latest changes I can:
- remove materialized from tables if I don't need them (except for fk join method)
- use avro serdes without a need for a fixed topic name wrapper
I will do this changes and will let you know
Thank you"
497802451,5527,bellemare,2019-05-31T17:52:16Z,"Okay sounds good. I appreciate all the feedback you have given, so if you think I'm doing something incorrectly I definitely want to hear about it :)"
498871200,5527,bellemare,2019-06-04T22:48:09Z,"Hi all, to those still paying attention to this PR.
I have done some major work on this over the past couple of days. I suspect I am about 95% complete on it now, but again, I still need feedback. Still need to incorporate KIP-307, but currently blocked on waiting for https://github.com/apache/kafka/pull/6412 to be approved.

- Added a large number of tests in line with ""stateStore.range(...)"" tests (and found a few bugs while doing so...)
- Unit tests for the various serdes
- prefixScan appears to be working with all necessary stores, including timestamped.
- Works with Timetamped tables

Things to consider:
1) The Murmur3 code copied from hive has a number of `fallthrough` warnings. We either need to suppress the warnings or alter the code. Currently I have the warnings suppressed.
2) Currently needs the same partition count for both `this` and `other` table, as I have not been able to dig into the task/partition assignment logic to get it working with variable partitions. This could be done at a later date.
3) Folder structure - all in foreignKeyJoin - I suspect I should not do that.

"
499291988,5527,guozhangwang,2019-06-05T23:25:04Z,"@bellemare Thanks for the great summary about the progress, I think the rest of KIP-307 should be done soon and pinging @vvcephei and @mjsax to continue reviewing on this PR."
499586049,5527,bellemare,2019-06-06T17:14:03Z,"Thanks @guozhangwang 
@adaniline-traderev  and I have been working on a specific bug (introduced by this PR, not existing) that he discovered around some incorrect casting between Timestamped and NonTimestamped stores. I hope to resolve that today or tomorrow, but it shouldn't affect 99% of the PR."
499632932,5527,bellemare,2019-06-06T19:23:24Z,"Okay I think I isolated the issue and submitted a fix. The tests that were failing should now be passing. In the meantime I'm running tests locally to validate. @adaniline-traderev, let me know if this works for you, your feedback has been very helpful."
499928589,5527,bellemare,2019-06-07T15:24:16Z,"Anyone know how to handle licensing for tests?

```
> Task :rat FAILED
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/clients/src/main/java/org/apache/kafka/common/utils/Murmur3.java
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapper.java
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerde.java
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapper.java
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerde.java
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/CombinedKeySerdeTest.java
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerdeTest.java
Unknown license: /Users/adambellemare/Documents/Wishabi/fork/kafka/streams/src/test/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionWrapperSerdeTest.java
```"
499935136,5527,guozhangwang,2019-06-07T15:40:58Z,"@bellemare For any added files (both src and test) you'd need to add Apache license as the javadoc at the top, you can take a look at any existing files. Basically it looks like:

```
/*
 * Licensed to the Apache Software Foundation (ASF) under one or more
 * contributor license agreements. See the NOTICE file distributed with
 * this work for additional information regarding copyright ownership.
 * The ASF licenses this file to You under the Apache License, Version 2.0
 * (the ""License""); you may not use this file except in compliance with
 * the License. You may obtain a copy of the License at
 *
 *    http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an ""AS IS"" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
```"
499996967,5527,bellemare,2019-06-07T18:45:17Z,@guozhangwang - Of course! Obvious in hindsight. Thank you. 
500947033,5527,bellemare,2019-06-11T17:35:43Z,"Alright, well there's not much more for me to do here without more feedback from reviewers and the completion of https://github.com/apache/kafka/pull/6412. 

"
505013615,5527,bellemare,2019-06-24T13:38:23Z,@vvcephei Thanks John! I'll address them all over the next few days. 
505137962,5527,joeslice,2019-06-24T19:02:23Z,"Thanks @bellemare for your hard work on this feature.  It's going to be super valuable to our team!!  In my integration of your branch I noticed something that seems like a bug, or at least warrants some further discussion:

I have a topology with two ""input"" `KTables` (`a` and `b`) and one output (`c`).  Simply put:
* this topology expects stream `a` to be messages like (`key`, `x1`), (`key`, `x2`), etc...
* it expects stream `b` to be messages like (`x1`, `v1`), (`x2`, `v2`), etc.
* on `c` it emits the key from `a` and the value from `b` (where the key on `b` is the value referenced in the message value from `a`).

Some behavior that works as expected:
* when a message in stream `a` references a known (or later known) message key in stream `b`, the resulting stream emits the key in stream `a` and the value from stream `b`.  Success!
* when a message (on either side) has its value updated, the join's new result is emitted.  Success!

What seems worth discussing / incorrect:
* if a message value from stream `a` is updated to reference a non-existent key on stream `b`, nothing is emitted.  This means that ""old"" results are still considered correct in downstream consumers.

For example if I send a message on topic `a`: (`k`, `1`) and a message on topic `b`: (`1`, `one`), it correctly emits on `c`: (`k`, `one`).  If I then send an update on `b`: (`1`, `ONE`) then a message on `c` is emitted: (`k`, `ONE`).  If I then send an update on `a`: (`k`, `2`), but no message with key `2` has ever been seen on `b`, nothing is emitted.

On one hand this seems correct: the results of an inner join should never have a `null` value for either side of the join.  On the other hand this seems incorrect: the resulting stream is not updated to reflect the latest value `null` value/tombstone for key `k`.

I suggest that the output stream should contain tombstones to indicate that the key extracted from `keyExtractor` no longer matches a value on the ""right"" side of the join.

What do you think?

/CC @vvcephei 

For what it's worth, the topology I am using for testing:

```scala
import java.time.Duration
import java.util.{Properties, UUID}

import org.apache.kafka.common.serialization.Serde
import org.apache.kafka.streams.scala.kstream.{Consumed, KTable, Materialized, Produced}
import org.apache.kafka.streams.scala.{ByteArrayKeyValueStore, Serdes, StreamsBuilder}
import org.apache.kafka.streams.{KafkaStreams, StreamsConfig}

object InnerFKJoinRule {
  implicit val stringSerde: Serde[String] = Serdes.String
  implicit val consumed: Consumed[String, String] = Consumed.`with`(Serdes.String, Serdes.String)
  implicit val produced: Produced[String, String] = Produced.`with`(Serdes.String, Serdes.String)

  val materialized: Materialized[String, String, ByteArrayKeyValueStore] =
    Materialized.as[String, String, ByteArrayKeyValueStore](UUID.randomUUID().toString.substring(24))

  def main(args: Array[String]): Unit = {
    val builder = new StreamsBuilder()
    val a: KTable[String, String] = builder.table(""a"")
    val b: KTable[String, String] = builder.table(""b"")

    // ax -> bx; bx -> X
    // ay -> by; by -> ?

    a.joinOnForeignKeyAlpha(b,
      aValue => aValue,
      (aValue: String, bValue: String) => bValue,
      materialized
    ).toStream
      .map((aKey: String, bValue: String) => (aKey, bValue))
      .to(""c"")

    val topology = builder.build()

    val kafkaStreams = new KafkaStreams(topology, config)
    kafkaStreams.cleanUp()
    kafkaStreams.start()

    sys.ShutdownHookThread {
      kafkaStreams.close(Duration.ofSeconds(5))
    }
  }

  val config: Properties = {
    val p = new Properties()
    p.put(StreamsConfig.APPLICATION_ID_CONFIG, ""foreign-key-join-test"")
    p.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""localhost:9092"")
    p.put(StreamsConfig.COMMIT_INTERVAL_MS_CONFIG, ""2000"")
    p.put(StreamsConfig.CACHE_MAX_BYTES_BUFFERING_CONFIG, ""1000000"")
    p.put(StreamsConfig.NUM_STREAM_THREADS_CONFIG, ""2"")
    p
  }
}
```"
505147263,5527,bellemare,2019-06-24T19:29:29Z,"@joeslice Thanks for the excellent discovery! I believe that this is indeed a bug, and I think it's because of the decisions made around propagating the `null` events on a foreign key change. I will take a look at this more closely, because what you have written about expecting a tombstone output makes sense to me. Let me consider this."
505517987,5527,bellemare,2019-06-25T16:20:59Z,"@vvcephei Hi John
I pushed a fix to the prefixScan, along with some tests illustrating how they work. Things SEEM to be working correctly now, but any feedback on this specific set of changes is well welcomed.

@joeslice I have a solution in mind for the bug you discovered, I'll try to get to that in the next day or two."
505859127,5527,bellemare,2019-06-26T12:45:40Z,"Hi @joeslice 
I have updated the KIP to reflect what I think can be done, and it is working in my dev branch. The main issue is that we do not maintain sufficient state to know that a delete was propagated out to downstream consumers on a previous change. A chain of changes with no available FK on the RHS would emit a (K, null) for each change. This is much more akin to a LEFT join, but the big difference is that we wouldn't execute the join logic if INNER is selected.
Anyways, check out the table I made in https://cwiki.apache.org/confluence/display/KAFKA/KIP-213+Support+non-key+joining+in+KTable. I'm going to continue working on the changes I have in mind, and more importantly, build out a number of test scenarios so I can get better coverage.

"
505936671,5527,vvcephei,2019-06-26T15:55:20Z,"Hey @bellemare and @joeslice,

I started a new follow-up discussion thread in the mailing list to evaluate the proposed design modification.

Thanks,
-John"
506104616,5527,bellemare,2019-06-27T01:39:50Z,"@joeslice  - I just pushed a commit that should fix your issue. It also adds left joins since we were 99% of the way there anyways. Please let me know if it's meeting your expectations, because feedback like yours has been extremely valuable in working out the kinks, and I thank you for that.
"
506493558,5527,joeslice,2019-06-27T20:13:52Z,"@bellemare I'm pleased to report that this is working perfectly in my project's test cases.  I have replaced a significant portion of my internals to prove this works in our test cases.  So that's great!

I did notice some `Closing N open iterators` upon shutdown after processing ~1/2 million records, and can't confirm whether it's expected or even troubling.  I thought I would pass it on in case we are ""leaking"" iterators in an unexpected way.

Thanks again for your hard work on this feature!


```
11:00:21.202 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45203 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:21.213 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 44893 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:35.321 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45364 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:35.332 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45517 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:40.339 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45134 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:40.360 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45118 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:44.733 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45184 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:45.046 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45340 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:48.743 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45575 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:48.756 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45279 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:52.533 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45032 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:52.984 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45359 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:56.177 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45457 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:00:56.658 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45230 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:01:00.143 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-1] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45205 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
11:01:00.455 [.....-using-kstreams-snapshot-9a161935-9f25-47bd-a5a5-b0f6cea2ad1c-StreamThread-2] WARN  o.a.k.s.state.internals.RocksDBStore - Closing 45440 open iterators for store STATE-STORE--thisSSName-STATE-STORE-0000000010
```"
506747200,5527,bellemare,2019-06-28T14:07:41Z,@joeslice  - A classic programming blunder - I definitely forgot to close the iterator after the prefix scan. I am adding that now.
506795729,5527,bellemare,2019-06-28T16:28:24Z,Did a rebase to trunk and force pushed. Looking to add Named and incorporate more of John's feedback.
506810499,5527,bellemare,2019-06-28T17:13:50Z,"One issue with using Named... there are 5 Nodes to name, 2 topics and 1 internal materialized store. 
I don't think it's appropriate to use 8 Named elements, but I could see about using it as a common prefix for each of those. "
506820978,5527,bellemare,2019-06-28T17:47:03Z,"sorry... correction - 5 processing nodes, 2 sink nodes, 2 source nodes, 2 topics and 1 materialized internal state store."
507096788,5527,bellemare,2019-07-01T02:44:33Z,"@vvcephei - Added a bunch of updates.
@mjsax and @guozhangwang - perhaps you, along with John, can recommend where ""prefixScan"" should live in the web of KeyValueStore interfaces. 

All I really need it is:
1) Support in RocksDB, Caching and Logging
2) Must be able to access both `prefixScan` and `get`

I tried previously to find a clean way to include it, but could not find a way without effectively adding it to the base KeyValueStore interface. I ended up settling on trying to get it to parity with `range` since they're very similar.
Thanks."
507786579,5527,bellemare,2019-07-02T18:08:22Z,"Hi Folks

I'm going to try to keep this moving along, as I've just hit the 1-year birthday of working on this. We have it working internally and it would be good to get it in to 2.4, but I need more eyes on it to get it over the barrier, otherwise I fear it will languish in PR-purgatory forever.


Outstanding Issues (as raised by John):
1) Where in the interface definitions should PrefixScanKeyValueStore.prefixScan() live? 
  - Not every type will have a prefix / won't make sense.
  - Many state stores need the data definition (Caching, Logging, RocksDB, TimestampedRocksDB) but many do not `need` it per se, for the purpose of this PR.

=> I don't have a good take on this. I want to avoid completely rehauling the KeyValueStore interfaces, but I also want to be able to use the caching and logging stores, as well as RocksDB to do the scan. Everything else is optional to me.

2) The implicit dependencies between CombinedKey, its serialization format, and prefixScan operation.
PrefixScan operates on the byte serialization of CombinedKey, and the byte serialization is important for the cases where we just have a prefix (foreign/RHS update).

=> I don't see this one as much of a problem, as it seems obvious that byte-level prefix scanning should rely on the serialized format. While it could possibly be simplified, I don't have any immediate ideas.


Thanks again for any help...
Adam"
513779794,5527,bellemare,2019-07-22T12:56:52Z,"@guozhangwang, @vvcephei, @mjsax - Do any of you have any thoughts on point #1 and #2 above? Currently everything works in the PR, but I suspect the API for #1 needs some more thought, as does the inter-dependencies of the serialization for #2. Pinging you because it's been 20 days with inactivity."
514062726,5527,tedchangtr,2019-07-23T05:45:02Z,I also wanted to say that along with @pgwhalen and @sachabest we are eagerly looking forward to this functionality as it solves many common uses cases we have. 
514693865,5527,vvcephei,2019-07-24T16:00:26Z,"Hey @bellemare , Sorry (again) for the delay (again). I'm going to check out your branch and see what I can come up with in response to the issues.

-John"
516523887,5527,vvcephei,2019-07-30T17:50:58Z,"Hey @bellemare ,

Like I mentioned before, I needed to check out the code and hack on it to address your questions... It was simpler to send my thoughts in PR form, so I created https://github.com/bellemare/kafka/pull/1 .

It looks like we made some concurrent modifications, but the point was just to express the ideas anyway. WDYT?

By the way, after getting my hands dirty, I have to give you mad props on this PR. You had to work though a bunch of the most complex inner workings of Streams to pull this off. Nice work!

-John"
517427553,5527,bellemare,2019-08-01T19:36:07Z,"@vvcephei I put your changes into a single commit and merged them in. Currently the build + tests seem to pass fine! Thanks so much for your help, it is extremely appreciated.

I think the major stumbling blocks I had were succinctly resolved by John's changes, so I believe the next step is to ensure that proper test coverage exists. I will take a look at that in short order, but just wanted to get his work in so that others can look at it. The bulk of it is summarized in his own descriptions here  (  https://github.com/vvcephei/kafka/tree/john-5527  ), but boils down to:
1) Removed the need for prefixScan
2) Use a basic Byte store with Range scan to access elements (this is part of where we need to ensure it's fully working as expected)
3) Centralize the specificity of prefixes and byte ordering into a single CombinedKeySchema class. 

Thanks again John!

"
519149386,5527,bellemare,2019-08-07T15:31:23Z,"So there is (theoretically) a bit of a problem. Currently our CombinedKeySchema is represented as follows:
`{Integer.BYTES foreignKeyLength}{foreignKeySerialized}{Optional-primaryKeySerialized}`

`store.range(start,end)` will not work when the prefix is the maximum value of a particular byte array. For example:

```
Bytes start = 0xFF\0xFF
Bytes end = Bytes.increment(start) //This is 0x01\0x00\0x00
store.range(start,end)  //will return nothing, because 0x01\0x00\0x00 is smaller than 0xFF\0xFF according to the default Byte comparator. 
```

NamedCache currently still has a fragment of code to remove from the previous prefixScan implementation, but it illustrates the issue perfectly: (https://github.com/apache/kafka/blob/953da3ae6314520e68c8f0b64549ba8e274db6e7/streams/src/main/java/org/apache/kafka/streams/state/internals/NamedCache.java#L278)

**The range iterator:**
``` 
  synchronized Iterator<Map.Entry<Bytes, LRUNode>> subMapIterator(final Bytes from, final Bytes to) {
        return cache.subMap(from, true, to, true).entrySet().iterator();
    }
```

**The prefix iterator.** Note that we use tailMap when the prefix increment would cause a wrap-around:
```  
synchronized Iterator<Map.Entry<Bytes, LRUNode>> subMapPrefixIterator(final Bytes prefix) {
        final Bytes prefixEnd = Bytes.increment(prefix);
        final Comparator<? super Bytes> comparator = cache.comparator();

        //We currently don't set a comparator for the map in this class, so the comparator will always be null.
        final int result = comparator == null ? prefix.compareTo(prefixEnd) : comparator.compare(prefix, prefixEnd);

        final NavigableMap<Bytes, LRUNode> subMapResults;
        if (result > 0) {
            //Prefix increment would cause a wrap-around. Get the submap from toKey to the end of the map
            subMapResults = cache.tailMap(prefix, true);
        } else {
            subMapResults = cache.subMap(prefix, true, prefixEnd, false);
        }
```

Options to fix this:
1) ignore it - we likely never hit Integer.Bytes for foreignKey length (that is, after all, 2,147,483,647 bytes for a signed integer... a bit bigger than we would expect to see in a Kafka event). It is also worth pointing out that this would require the event coming through a repartition topic, and I don't think 2GB+ keys are reasonably likely to be used by anyone. 

2) Custom implementation of Bytes specifying a custom comparator.
  I am leery about this one because technically the current Byte comparison is correct, and we would be making it incorrect simply to work around the range wrap-around corner case.

3) Add an extra byte to the start of CombinedKeySchema just for the wrap-around corner-case:
//{Empty Byte preventing wrap-around issues}{Integer.BYTES foreignKeyLength}{foreignKeySerialized}{Optional-primaryKeySerialized}
In this way, our previous example will now return the correct results, because end > start according to the default Bytes comparator.
```
Bytes start = 0x00\0xFF\0xFF
Bytes end = Bytes.increment(start) //This is 0x01\0x00\0x00
store.range(start,end) //returns correctly because of proper Bytes comparator.
```

My inclination is to somehow use option 1 while acknowledging that it could possibly be an issue. After all, the current key size limit is arbitrary as is (why not use Long?). I suspect that adding the byte would be technically correct but simply a waste of space. I do not think anyone would ever use a 2GB key, but I think we still need to guard against it.



"
519198629,5527,vvcephei,2019-08-07T17:40:34Z,"Nice catch, @bellemare ,

I also think option 1 is ok.

Actually, I _think_ this is not a problem. The key length is actually not arbitrary, since all keys must fit into byte arrays, and arrays in java cannot be larger than ""max int"", aka `0x7fff ffff` Thus, we already have room to wrap around, with the first byte becoming `0x8f`.

Does that add up, or is it just wishful thinking on my part?

-John"
519198864,5527,vvcephei,2019-08-07T17:41:18Z,"Retest this, please."
519223034,5527,vvcephei,2019-08-07T18:47:46Z,The builds are failing because of some style violation. You can debug it with `./gradlew :streams:test-utils:checkstyleMain`.
519223220,5527,vvcephei,2019-08-07T18:48:20Z,"Oh, found it:
```
> Task :streams:test-utils:checkstyleMain FAILED
[ant:checkstyle] [ERROR] /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/test-utils/src/main/java/org/apache/kafka/streams/internals/KeyValueStoreFacade.java:23:8: Unused import - org.apache.kafka.streams.state.KeyValueIterator. [UnusedImports]
[ant:checkstyle] [ERROR] /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/streams/test-utils/src/main/java/org/apache/kafka/streams/internals/KeyValueStoreFacade.java:27:8: Unused import - org.apache.kafka.streams.state.internals.KeyValueIteratorFacade. [UnusedImports]
```"
519512086,5527,bellemare,2019-08-08T13:13:10Z,"@vvcephei Ah, correct! 7FFF FFFF should indeed be the maximum. I have just been testing at the byte level and had erroneously thought that FFFF FFFF was the max. Since that's the case, I don't think that there's any issue with the usage of range as we have it now. "
520025675,5527,vvcephei,2019-08-09T18:48:55Z,"Thanks for double-checking, @bellemare .

Maybe we can get some other reviews at this point?

Also, the test failures were unrelated.

Retest this, please."
520162968,5527,bellemare,2019-08-10T16:38:31Z,"@vvcephei by retest, do you mean rerun the Scala 2.13 tests? I can't figure out how to do that without submitting a dummy commit..."
520532972,5527,vvcephei,2019-08-12T18:06:34Z,"oh, sorry, @bellemare . if you say that sentence in a comment, it triggers the build to re-run. In other words, that last sentence was directed at Jenkins, no you ;)

After that re-test, two of the builds are passing, so I think we don't need to run them again until after the next cycle of reviews. (just guessing there'll be more commits later). I just wanted to see some green checkmarks."
520847928,5527,bellemare,2019-08-13T14:02:39Z,"@vvcephei Thanks! I was a bit confused... I think it's the politeness of which the restesting was asked for that made me think it was directed at me... anyways, TIL!"
524442847,5527,vvcephei,2019-08-23T19:57:32Z,"Hey @bellemare ,

I was just looking over the interface in KTable. Can you correct the Javadoc so that the `return` field is filled out? Also, one of the `named` params is missing a description, and two of the overloads have no javadoc at all.

Thanks!
-John"
524500288,5527,vvcephei,2019-08-24T00:39:31Z,"Hey again, @bellemare ,

I was talking to @guozhangwang about reviewing this PR, and he had a good suggestion... Can you send a separate PR just adding the Murmur3 hash class and test (and guava dependency)?

This would let us discuss the new depenency in isolation, as well as consider adopting the new hash algorithm elsewhere sooner. And as soon as it gets merged, this PR will automatically get almost 1,000 lines of code shorter.

WDYT?
-John"
525309148,5527,bellemare,2019-08-27T13:47:05Z,"@vvcephei Okay, will do."
525315925,5527,bellemare,2019-08-27T14:03:15Z,@vvcephei I assume you mean I should make a new PR and then rebase this one off of the new one? I think I may have to butcher this one up a bit since I don't think I committed the Murmur3 hash code neatly in one commit.
526226069,5527,bellemare,2019-08-29T15:01:16Z,@vvcephei @guozhangwang - The Murmur3 PR - https://github.com/apache/kafka/pull/7271
530440677,5527,bbejeck,2019-09-11T15:44:24Z,"@bellemare one additional thought came to mind.  We should include a test case where we have the `StreamsConfig` set to `OPTIMIZE` (and use overloaded build method `StreamBuilder.build(props)` ) to make sure everything still works as designed.  I think it should, but it will good to confirm."
530527737,5527,bellemare,2019-09-11T19:22:27Z,"@bbejeck - Would it be sufficient to put it in one of the integration tests? It seems to all work when I include the following:
```streamsConfig.put(StreamsConfig.TOPOLOGY_OPTIMIZATION, StreamsConfig.OPTIMIZE);```"
530592010,5527,bbejeck,2019-09-11T22:33:44Z,"@bellemare 

>@bbejeck - Would it be sufficient to put it in one of the integration tests?

Yeah, that sounds fine. Maybe the `KTableKTableForeignKeyJoinIntegrationTest` since it has 8 tests in it?"
532466041,5527,vvcephei,2019-09-18T00:28:24Z,"The job output was lost to the sands of time.

Retest this, please."
533170339,5527,bellemare,2019-09-19T14:56:49Z,"Okay, I added code to the StreamsResetter, but I can't find a good place to test for the correct deletion of the topics during a full app reset. StreamsResetterTest doesn't seem to provide for that. Any suggestions?"
533212559,5527,bbejeck,2019-09-19T16:38:00Z,">Okay, I added code to the StreamsResetter, but I can't find a good place to test for the correct deletion of the topics during a full app reset. StreamsResetterTest doesn't seem to provide for that. Any suggestions?

Off the top of my head maybe add the `StreamsResetter` to one of the integration tests? I'm thinking something like

1. process some records, confirm the output
2. reset the app confirm topics deleted
3. re-run the same records and confirm all processed again

WDYT?
\cc @vvcephei @mjsax "
533217402,5527,vvcephei,2019-09-19T16:51:03Z,"I think it's worth adding the test right now, though, given our recent experience with repeated the multiple operational regressions after implementing Suppress.

See the `org.apache.kafka.streams.integration.AbstractResetIntegrationTest` for testing resets."
533355567,5527,vvcephei,2019-09-20T00:21:31Z,"@bellemare , now that #7271 has been merged, you're going to have to rebase (since the location of Murmur3 is different now). We should see all the Murmur3 code and tests disappear from this diff."
533356431,5527,bbejeck,2019-09-20T00:26:13Z,"Java 11 builds timed out.

Java 8 failed with all tests from `org.apache.kafka.streams.integration.KTableKTableForeignKeyInnerJoinMultiIntegrationTest` and `org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinIntegrationTest` but running locally all test pass for me.

@bellemare can you rebase this PR?  NM just saw the comment from @vvcephei "
536039458,5527,thebearmayor,2019-09-27T17:58:48Z,"Hi, what is the expected output with a leftJoin when an event is published to the LHS with null foreign key (assume no other events have been published). I am observing no output, which was unexpected for a leftJoin. My apologies if this is described in the KIP, I couldn't find this particular situation called out."
536145694,5527,vvcephei,2019-09-28T02:38:54Z,"Hey @thebearmayor , you mean the key itself is `null`?

Normally, this would be considered invalid data (in any join), and Streams would just drop the record with no output (which you've observed). Although, there should be a warning log, if memory serves.

I don't think this was discussed in the KIP discussion, so we might need to amend the KIP to handle this case. As I understand it, in many relational DBs, foreign key references are allowed to be null because the reference itself is understood to be an implication. The presence of a foreign key reference implies that the key is a primary key on the foreign table, but a null reference is the _absence_ of a foreign key, so it doesn't imply anything. In this situation, the DB implementation is free to define the result however it likes.

So, I guess that we can go for consistency and say that null foreign key references work the same way as null equi-join (primary) keys and just drop the record with a warning. On the other hand, the situation is slightly different, and if it's useful to emit a `(lhs, null)` result, we could try to make it work.

However, the implementation would be quite difficult, maybe impossible. The reason for this is that to do the join, we need to send a ""subscription"" to the rhs primary key (ie the foreign key). When the foreign key is `null`, there is no where we can send the subscription, and we can't even pick a sentinel value, since we don't control the type of the foreign key. Some quick details: the (caller-provided) serdes would have to handle null keys, which they normally do not, probably resulting in NPEs, and they (maybe caller-provided) topic partitioner would have to handle null keys also, which it probably does not.

In light of this reflection, I guess I'm leaning toward just documenting the behavior you've observed. Although it might be a bit of dissonance if you're used to working with RDBs that allow it, at the end of the day, it would be less headache for you overall not to have to cope with `null` in all these unexpected places. Hopefully, you could accomplish the same objective by mapping the `null` reference to some non-null, but out-of-domain sentinel value that's guaranteed not to be the primary key of an actual RHS record.

How does this all sound to you?

Thanks for bringing it up,
-John"
536186044,5527,thebearmayor,2019-09-28T12:40:47Z,"Thanks, John.

Specifically, I meant the result of foreignKeyExtractor is `null`. In a normal case that could be because the foreign key reference is missing or the value is null, or maybe you're doing something weird in the extractor which is giving a null.

I think a sentinel value will work fine. My goal is something like ""enhance the left side with data from the right side"", but the left side is the important part. So I don't want to drop records that have invalid or missing references.

Thanks for your help! I do think it would be good to document this, because it surprised me in comparison to a left equi join."
537581324,5527,bellemare,2019-10-02T16:47:47Z,"@thebearmayor Yep, it looks like the issue is as you described. The thing is that it's hard to do, as outlined by John. I think you would have to use your own sentinel value to get an adequate output, and I think I will have to document this into the code."
537582199,5527,bellemare,2019-10-02T16:49:48Z,"I'm going to merge all of the current commits into a single patch and apply it before rebasing to trunk. There are so many commits right now that rebasing takes a significantly long time and is error prone - so I will force push a new, single commit, rebase to trunk, fix up the Murmur3 hash and add the comments regarding left-hand joins with null foreign keys. If anyone objects or has a better idea as to how to handle this, let me know, as this is my first big multi-month(/year) commit where rebasing is painful."
537599188,5527,vvcephei,2019-10-02T17:31:55Z,"Hey @bellemare , I started working on this just now, since we didn't hear back from you for a while. I wound up just merging trunk into your branch again, it wasn't too bad.

I'd recommend not rebasing or squashing. Since there's a merge commit in the history of this branch, it's just going to make you sad.

Once alternative (which I'd also not recommend) is checking out a clean branch from trunk and using diff/patch to just apply the diff between this branch and trunk. This is risky, as you could easily lose some important recent change in trunk, and it would be hard to notice or track down later.

On the other hand, if you just merge trunk into this branch, you can resolve the merge conflicts (there are only two), and then the Github (squash+merge) button should take care of the rest."
537601076,5527,bellemare,2019-10-02T17:36:46Z,"@vvcephei - Ah thanks! Sorry, I was away on vacation for ~9 days and had some other things come up - Did you want to carry it across the line or did you want me to keep at it? "
537605591,5527,bellemare,2019-10-02T17:48:11Z,"Ah I updated it all anyways. Let me know if that comment is sufficient, or if there needs to be more clarity around it."
537606172,5527,vvcephei,2019-10-02T17:49:36Z,"Sounds good! Can you also add this to the `build.gradle` (in the `:streams` project):

```
  test {
    // The suites are for running sets of tests in IDEs.
    // Gradle will run each test class, so we exclude the suites to avoid redundantly running the tests twice.
    exclude '**/*Suite.class'
  }
```

I noticed it when I was running the tests locally"
537608972,5527,bellemare,2019-10-02T17:56:40Z,@vvcephei  How's that?
537610839,5527,vvcephei,2019-10-02T18:01:20Z,"Since Jenkins has been a bit twitchy recently, I'll just make a note that `./gradlew clean :streams:test` has just passed for me."
537694233,5527,vvcephei,2019-10-02T21:43:16Z,"Hmm. I just checked the Jenkins queues, and I don't see this PR in there.

Retest this, please."
537732545,5527,mjsax,2019-10-03T00:15:35Z,"Checkstyle error:
```
[ant:checkstyle] [ERROR] /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.13@2/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java:30:8: Unused import - org.apache.kafka.streams.StreamsConfig. [UnusedImports]
```"
537754256,5527,vvcephei,2019-10-03T02:04:15Z,"Huh. The StreamsPartitionAssignor isn't part of this PR, and that import isn't there in this PR's branch. Maybe someone merged a broken commit to trunk, and then the Jenkins PR builder sucked it in here?

Retest this, please."
537754397,5527,vvcephei,2019-10-03T02:04:56Z,"Oh, yep, it was trunk:

```
commit 11ab6e7d8fbdf213a22b7fa5dff04f374f55a1d2
Author: Guozhang Wang <wangguoz@gmail.com>
Date:   Wed Oct 2 17:05:24 2019 -0700

    HOTFIX: remove unsued StreamsConfig from StreamsPartitionAssignor
```"
537974203,5527,bellemare,2019-10-03T14:37:49Z,"Retest this, please."
537983099,5527,bellemare,2019-10-03T14:58:00Z,"The prior builds had all failed on one test or another. One had failed on 5 integration tests, the other on 1 other integration test, and I forget about the third. It looks like it is flakiness and unrelated to the code changes, but best to run it again."
537995486,5527,vvcephei,2019-10-03T15:27:10Z,"Thanks, @bellemare , yeah, it would be nice to see Jenkins give us at least 1 green build."
538084227,5527,vvcephei,2019-10-03T19:10:47Z,"The java 8 build had just one failure, which I think is ok:
```
org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest > testAddAndRemoveWorker FAILED
```

Likewise with the java 11+scala 2.13 build:
```
03:35:23.161 org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest > testAddAndRemoveWorker STARTED
03:37:02.849 org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest.testAddAndRemoveWorker failed, log available in /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.13/connect/runtime/build/reports/testOutput/org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest.testAddAndRemoveWorker.test.stdout
03:37:02.849 
03:37:02.849 org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest > testAddAndRemoveWorker FAILED
03:37:02.849     org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not execute PUT request
03:37:02.849         at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.configureConnector(EmbeddedConnectCluster.java:281)
03:37:02.849         at org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest.testAddAndRemoveWorker(ConnectWorkerIntegrationTest.java:117)
```

And the java 11+scala 2.12 build:
```
03:37:47.420 org.apache.kafka.connect.integration.ExampleConnectIntegrationTest > testSourceConnector STARTED
03:39:26.620 org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector failed, log available in /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.12/connect/runtime/build/reports/testOutput/org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector.test.stdout
03:39:26.620 
03:39:26.620 org.apache.kafka.connect.integration.ExampleConnectIntegrationTest > testSourceConnector FAILED
03:39:26.620     org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not execute PUT request
03:39:26.621         at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.configureConnector(EmbeddedConnectCluster.java:281)
03:39:26.621         at org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector(ExampleConnectIntegrationTest.java:177)
```"
538085084,5527,bellemare,2019-10-03T19:13:17Z,"(JDK 11 & Scala 2.13) and (JDK 8, Scala 2.11) both failed on the same test, though it appears totally unrelated:

```
15:05:30 org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest > testAddAndRemoveWorker STARTED
15:07:10 org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest.testAddAndRemoveWorker failed, log available in /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.13/connect/runtime/build/reports/testOutput/org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest.testAddAndRemoveWorker.test.stdout
15:07:10 
15:07:10 org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest > testAddAndRemoveWorker FAILED
15:07:10     org.apache.kafka.connect.runtime.rest.errors.ConnectRestException: Could not execute PUT request
15:07:10         at org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster.configureConnector(EmbeddedConnectCluster.java:281)
15:07:10         at 
```"
538086489,5527,vvcephei,2019-10-03T19:17:25Z,"Hmm, It's almost like we're all just sitting around watching these builds...

I agree, I don't think that failure is related, and I also don't think it's worthwhile to keep running the tests to see if that broken test passes next time."
538108501,5527,vvcephei,2019-10-03T20:10:41Z,"Note, the failing test is known to be broken: https://issues.apache.org/jira/browse/KAFKA-8690"
538160544,5527,bbejeck,2019-10-03T22:57:13Z,"Test failures unrelated and local build passes, so merging this."
538161279,5527,bbejeck,2019-10-03T23:00:14Z,Merged #5527 into trunk.
538161381,5527,bbejeck,2019-10-03T23:00:36Z,Thanks @bellemare for the hard work and perseverance to get this done!
538182923,5527,vvcephei,2019-10-04T00:43:45Z,"Yeah, congratulations, @bellemare for this awesome contribution!"
538184423,5527,pgwhalen,2019-10-04T00:51:56Z,"Kudos! Ive been following this feature for a year and am extremely excited to see it make it in. 

> On Oct 3, 2019, at 8:44 PM, John Roesler <notifications@github.com> wrote:
> 
> Yeah, congratulations, @bellemare for this awesome contribution!
> 
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub, or mute the thread.
"
538189810,5527,guozhangwang,2019-10-04T01:18:23Z,"Congratulations @bellemare , it's been a long KIP and journey :)"
538226162,5527,mjsax,2019-10-04T04:26:02Z,"Congratulations @bellemare!!! And thanks for all the hard work!!!

Also thanks to @Kaiserchen for his initial proposal and the many hours you invested in this KIP!

Check out https://twitter.com/kafkastreams/status/1179974460167745536"
538396002,5527,bellemare,2019-10-04T13:26:53Z,"Thanks for all the help everyone, especially from @vvcephei. I couldn't have done it without you all. And thanks to Jan too for getting it started so long ago."
538471438,5527,mjsax,2019-10-04T16:35:51Z,"@bellemare Do we need to update the docs for the new feature? We should at least mention in the upgrade guide.

Would you like to update the wiki, too: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Streams+Join+Semantics"
538481205,5527,guozhangwang,2019-10-04T17:04:56Z,"Besides the upgrade guide, I think we can also update the developer-guide on dsl section: https://kafka.apache.org/23/documentation/streams/developer-guide/dsl-api.html"
539648633,5527,bellemare,2019-10-08T18:43:37Z,"@mjsax @guozhangwang I'll take a look at updating them.

What/where is the upgrade guide?"
539843057,5527,mjsax,2019-10-09T05:46:24Z,"I would add maybe one bullet point to https://github.com/apache/kafka/blob/trunk/docs/upgrade.html (as ""notable changes"") and also list it in https://github.com/apache/kafka/blob/trunk/docs/streams/upgrade-guide.html

Not sure if both files have already a section for `2.4` release -- if not, just add one :)"
541166957,5527,thebearmayor,2019-10-11T18:07:20Z,"@vvcephei @bellemare Do you know of any cases that would lead to duplicate updates being emitted by these joins?

We first noticed this because we saw ""Detected out-of-order KTable update"" messages for topics emitted from some of these left joins. Looking at the updates, they were identical except for the timestamps. But it happens intermittently -- and not consistently even with the same input data. We switched to inner joins, and still saw them. Sometimes they updates had the same timestamp, but when they had different timestamps, one would have the timestamp from the corresponding left-table update, and the other would have the timestamp from the right-table update.

So far I don't think this is causing bad data or any real problems. Just wanted to check if you had seen it before I keep digging.

Thanks!"
541769834,5527,guozhangwang,2019-10-14T16:08:30Z,@thebearmayor I think this is fine: as we've discussed in KIP duplicates may happen for the same join-results but they will happen intermittently as you've observed. Since it is for a KTable changelog streams overwriting multiple times would not cause correctness issues. @vvcephei and @bellemare could chime in with more insights.
541770417,5527,guozhangwang,2019-10-14T16:09:34Z,"That also makes me thinking, should we make 

```
LOG.warn(""Detected out-of-order KTable update for {} at offset {}, partition {}."",
```

an info level entry than warn to not cause unnecessary panic since it would not cause correctness issues anyways?"
541961886,5527,mjsax,2019-10-14T22:39:38Z,"Interesting though -- mid to long term, I think we need to allow better handling of out-of-order data for `KTables` anyway. The main purpose to the log message was for the `builder.table()` operator -- it required data be ordered because it applied updates in offset-order, not timestamp order. Hence, upstream the single-writer principle should be applied -- the warning makes sense for that case IMHO as it indicated a potential upstream problem.

Maybe we should make the warning more flexible and only turn it on for `builder.table()` operator but disable it if we use the same processor elsewhere?"
541973602,5527,thebearmayor,2019-10-14T23:33:02Z,"Hi, @mjsax, just to be clear I am getting the warning from a `builder.table()` operator, which is reading from a topic which is written to by one of these joins. The join operator itself does not issue this warning."
542099166,5527,mjsax,2019-10-15T08:25:49Z,"Thanks for the clarification.

Maybe we introduce more out-of-order records due to the round-trip via two repartition topics than we anticipated... But I am not 100% sure why -- each update to the left-hand side would send one message to the right hand side and should receive zero or one respond messages. An update to the right hand side could send multiple message to the left hand side, however maximum one per key. -- If we compute the join result timestamp as maximum of both input timestamps, I don't see atm why we would introduce much out-of-order data. \cc @bellemare @vvcephei Thoughts?"
542302479,5527,bellemare,2019-10-15T16:41:13Z,"@thebearmayor Do you have any other information on how common it is, or steps to reproduce? 

@mjsax I don't have any ideas off the top of my head, but I will take a look at the code again with this in mind..."
542481324,5527,thebearmayor,2019-10-16T02:22:26Z,"@bellemare I'll tell you everything I can think of, most of which won't be relevant. I didn't mean to take up more of your time. I'll be offline until next week, and I mean to dig into it more then.

I don't have any way yet to reproduce this in development. I've only seen it running in ec2 with 3 instances against large topics, with 16 partitions. I assume there is some timing component to the issue.

- We're not using the absolute latest version of this code. I think we built your branch when it was ""feature complete"" sometime around mid-August.
- We're joining two topics with hundreds of millions of messages each. They're fairly old and compacted, so I don't think there are any records with duplicate keys.
- The join itself is simple -- we just keep the right-side value and discard the left-side.
- It's not very common -- I would say duplicate records occur a few thousand times in a few million input records. Of those, far fewer have out-of-order timestamps."
629872893,8680,kowshik,2020-05-17T22:49:00Z,"Hi @junrao and @abbccdda,

This PR is ready for code review.
Please have a look and do let me know your thoughts.

cc @cmccabe and @hachikuji 

Thank you!"
641549841,8680,junrao,2020-06-09T20:25:00Z,ok to test
641561083,8680,junrao,2020-06-09T20:48:13Z,"It seems there are some checkstyle failures.

```
> Task :clients:checkstyleTest FAILED
20:31:38 [ant:checkstyle] [ERROR] /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk11-scala2.13@2/clients/src/test/java/org/apache/kafka/clients/producer/internals/SenderTest.java:19:8: Unused import - java.util.Optional. [UnusedImports]
```"
641765964,8680,kowshik,2020-06-10T06:47:29Z,@junrao Thanks for the review! I have fixed the checkstyle issue now in 74ff66f.
641766088,8680,kowshik,2020-06-10T06:47:40Z,ok to test
642155141,8680,hachikuji,2020-06-10T17:34:20Z,ok to test
642155244,8680,hachikuji,2020-06-10T17:34:35Z,retest this please
642155469,8680,hachikuji,2020-06-10T17:35:00Z,retest this please
642155913,8680,hachikuji,2020-06-10T17:35:51Z,retest this please
642180658,8680,junrao,2020-06-10T18:22:59Z,test this please
642180814,8680,junrao,2020-06-10T18:23:18Z,retest this please
642270801,8680,junrao,2020-06-10T21:13:36Z,retest this please
642750609,8680,junrao,2020-06-11T15:42:16Z,retest this please
642751664,8680,junrao,2020-06-11T15:42:55Z,test this please
642855293,8680,junrao,2020-06-11T18:26:49Z,The test failures seem unrelated. Merging this to trunk.
1728441122,14406,kirktrue,2023-09-20T21:19:06Z,"@philipnee Can you add the `ctr` label, please  "
1773483995,14406,kirktrue,2023-10-20T22:53:03Z,FYI: I rebased against `trunk` to remove the conflicts.
1776108652,14406,kirktrue,2023-10-23T22:16:56Z,"> Are the 30 test failures related?

No, I do not believe they are.

There are 8 ""new"" failures, of which two are the same test with different parameters. Here are the tests and any Jiras for flakiness:

* `kafka.server.DescribeClusterRequestTest.testDescribeClusterRequestExcludingClusterAuthorizedOperations`: KAFKA-15419
* `kafka.server.DynamicBrokerReconfigurationTest.testLogCleanerConfig`: KAFKA-7966, though the Jira is resolved 
* `o.a.k.common.network.SslVersionsTransportLayerTest.testTlsDefaults`: KAFKA-9714, marked as ""Critical"" but open for 3 1/2 years
* `o.a.k.connect.integration.ConnectorRestartApiIntegrationTest.testMultiWorkerRestartOnlyConnector`: KAFKA-15675, just filed (by me) with some pointers to a recent flakiness rate of 9%
* `o.a.k.streams.integration.StandbyTaskEOSMultiRebalanceIntegrationTest.shouldHonorEOSWhenUsingCachingAndStandbyReplicas`: no Jira. Only around 2% flaky
* `o.a.k.tiered.storage.integration.TransactionsWithTieredStoreTest.testSendOffsetsWithGroupId`: KAFKA-8003, though the Jira is filed against a different test, but test failures in `testSendOffsetsWithGroupId` are mentioned as being related
* `o.a.k.trogdor.coordinator.CoordinatorTest.testTaskRequestWithOldStartMsGetsUpdated`: KAFKA-8115, which is another old, 4.5 year old ""Critical"" issue 

There are 22 ""existing"" failures, which are all related to the `verifyNoUnexpectedThreads` check. `kafka.server.DynamicBrokerReconfigurationTest` is reporting that there are threads when it attempts to tear down the test harness. The remaining 21 failures all report the same unexpected threads in their check during test harness setup. There are other recent, unrelated pull requests that have experienced similar issues. In the other cases, I haven't seen that it's the `DynamicBrokerReconfigurationTest` that is the cause, though."
1776110492,14406,kirktrue,2023-10-23T22:18:20Z,Closing and reopening to trigger another Jenkins test run.
1776377391,14406,kirktrue,2023-10-24T02:05:31Z,"Ugh. One of the builds failed with:

```
ERROR: Error fetching remote repo 'origin'
hudson.plugins.git.GitException: Failed to fetch from https://github.com/apache/kafka.git
	at hudson.plugins.git.GitSCM.fetchFrom(GitSCM.java:1003)
	at hudson.plugins.git.GitSCM.retrieveChanges(GitSCM.java:1245)
	at hudson.plugins.git.GitSCM.checkout(GitSCM.java:1309)
	at org.jenkinsci.plugins.workflow.steps.scm.SCMStep.checkout(SCMStep.java:129)
	at org.jenkinsci.plugins.workflow.steps.scm.SCMStep$StepExecutionImpl.run(SCMStep.java:97)
	at org.jenkinsci.plugins.workflow.steps.scm.SCMStep$StepExecutionImpl.run(SCMStep.java:84)
	at org.jenkinsci.plugins.workflow.steps.SynchronousNonBlockingStepExecution.lambda$start$0(SynchronousNonBlockingStepExecution.java:47)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:829)
```

No integration test failures due to threads, though  "
1776378053,14406,kirktrue,2023-10-24T02:05:48Z,Closing and reopening to restart test.
1777454027,14406,kirktrue,2023-10-24T15:13:43Z,Closing and reopening to restart test. Jenkins doesn't seem happy lately.
1777855712,14406,junrao,2023-10-24T19:04:42Z,@kirktrue : The build for JDK 21 and Scala 2.13 failed this time. Did it succeed before?
1777892441,14406,kirktrue,2023-10-24T19:28:41Z,"@junraoyes. Here's a brief history for JDK 21, starting with the most recent build (build 74):

* [build 74](https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-14406/74/pipeline/): couldn't clone repo
* [build 73](https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-14406/73/pipeline/): tests ran, but Jenkins timed out
* [build 72](https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-14406/72/pipeline/): tests ran
* [build 71](https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-14406/71/pipeline/): tests ran
* [build 70](https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-14406/70/pipeline/): tests ran, but Jenkins shut down in the middle 

These same intra-Jenkins communication, `git` cloning, unexpected threads, and flaky tests affect all of the JDKs at random times "
956456320,11390,kowshik,2021-11-01T18:01:21Z,@satishd It'd be helpful if you could please update the PR description explaining the scope of the draft PR (in its current form) and what's remaining to be done.
971736892,11390,satishd,2021-11-17T16:18:40Z,@junrao @kowshik This PR is ready for review. Please take a look.
992617350,11390,satishd,2021-12-13T15:55:04Z,"Thanks @junrao for the review. Please find inline replies, addressed most of them with latest commits."
998818025,11390,satishd,2021-12-21T14:20:51Z,"@junrao : Thanks for the review. Please find inline replies, updated the PR addressing them with the latest commit."
1015392509,11390,satishd,2022-01-18T13:04:39Z,"@junrao Thanks for the review. Please find inline replies, updated the PR addressing them with the latest commits."
1019966726,11390,satishd,2022-01-24T10:53:11Z,"@junrao : Thanks for the review. Addressed your comments with the latest commits. 

Rebased with the trunk to resolve the conflicts."
1330143560,11390,satishd,2022-11-29T06:21:51Z,"Thanks @showuon for your latest comments. Addressed them inline, let me know if you have any comments. "
1330147549,11390,satishd,2022-11-29T06:28:15Z,"Thanks @divijvaidya for your comments, addressed them with inline comments."
1330147705,11390,satishd,2022-11-29T06:28:27Z,"Thanks @junrao for your comments. Addressed most of them, working on couple of the remaining comments to update the javadoc."
1340522323,11390,showuon,2022-12-07T07:38:07Z,"@junrao @kowshik @ccding @divijvaidya , do you want to have another review? Since branch 3.4 has created, and this PR blocks some following tiered storage feature development (ex: copying segments to tiered storage, retention checks to clean local and remote log segments), we might need to consider to merge it first and have follow-up PRs for complicated changes. WDYT? Thanks."
1341457933,11390,junrao,2022-12-07T19:09:21Z,@showuon : I plan to take another look at the PR in the next few days.
1348935454,11390,satishd,2022-12-13T16:14:51Z,"Thanks @junrao for your review, addressed them with inline replies. I updated the PR with latest commits addressing the review comments. "
1348940728,11390,satishd,2022-12-13T16:15:58Z,@junrao : Filed https://issues.apache.org/jira/browse/KAFKA-14467 as you suggested.
1354541220,11390,satishd,2022-12-16T10:45:06Z,Thanks @junrao for your updated review. Addressed them with inline comments and updated with the latest commits.
1356256797,11390,satishd,2022-12-17T13:12:53Z,"Thanks @junrao for the review, addressed it with the latest commit. 

There are a few tests that are failed but they do not seem to be related to this PR. "
1356390240,11390,ijuma,2022-12-17T19:00:58Z,"Hey @satishd, I wanted to let you know about KAFKA-14470 as I think it affects some of the future KIP-405 PRs. Can we align these efforts so that we can get to the desired end state faster? For example, once the PRs that have been submitted are merged, we can move `RemoteIndexCache` to the `storage` module too."
471557966,6363,kkonstantine,2019-03-11T14:20:39Z,"@ewencp @hachikuji @rhauch @mumrah the PR is ready for review. The following items are expected to be addressed along with your comments:

* The code on task assignment is currently written to be readable and evaluate correctness. I'd like to add micro-benchmarks and I expect it'll be optimized. 
* Unit tests for `DistributedHerder` and `WorkerCoordinatorIncrementalTest` will be expanded to guard against changes in the new protocol more extensively. 
* More logging will be added appropriately and some integration with metrics will be considered. 
* A few more integration tests will be added. 

Some files contain changes that are introduced by other outstanding PRs. If still present here, please skip or review the changes in their respective PRs. 

Really looking forward to your comments! Thanks! "
484717843,6363,rayokota,2019-04-18T23:04:57Z,"@kkonstantine , thanks for responding to my feedback!  I just had one remaining comment.  Looking great!"
493287766,6363,kkonstantine,2019-05-17T01:41:14Z,"Thanks @rhauch @mumrah @rayokota @ryannedolan and @ewencp for all the insightful and useful comments! I believe I've addressed everything, except a few cleanup/refactoring suggestions that deemed high risk at this point and will be addressed in a follow up PR after this feature is merged. 

Soak testing has been also performed and has confirmed correct execution for several days. More extensive testing and performance benchmarking will follow up in the next few days. 

I'll be glad if we can get this in. Thanks!
"
554337624,6363,findnanda,2019-11-15T12:15:43Z,"Hi Can you please suggest in which kafka version is this issue fixed as I am still seeing this problem every time i add a new connector, all the connector gets restarted?"
554407084,6363,rhauch,2019-11-15T15:35:16Z,"@findnanda, the Jira issue (https://issues.apache.org/jira/browse/KAFKA-5505) shows that this was merged and completed in AK 2.3.0. 

If you're using AK 2.3.0 or later and still having problems, please create a new Jira issue and provide the Connect worker configs and a lot more detail about a procedure to replicate the problem. Thanks!"
417397352,5582,rondagostino,2018-08-30T17:15:54Z,"Beginning to add tests, and I need to rebase to get `KAFKA-7324: NPE due to lack of SASLExtensions in SASL/OAUTHBEARER (#5552)`
"
417676910,5582,rondagostino,2018-08-31T14:12:36Z,Rebasing again to resolve conflicts with `KAFKA-6950: Delay response to failed client authentication to prevent potential DoS issues (KIP-306)`
423861956,5582,rondagostino,2018-09-24T01:13:41Z,@rajinisivaram I rebased and pushed a squashed commit that implements your approach except it triggers a re-auth upon a write as described by the KIP rather than upon a read (which is what your prototype used).  I think triggering upon a write is correct.  I'll add metrics and tests soon.  Hopefully we will get the 3rd binding up-vote in time for the 2.1.0 deadline today.  Thanks again for that prototype -- I could not have gotten there myself in a reasonable amount of time because the low-level network code is pretty tough to pick up.
424499373,5582,rondagostino,2018-09-25T20:58:44Z,"@rajinisivaram This PR now has everything except:
- implementation and tests for client-side latency metrics
- implementation and tests for server-side connection kill metrics
- a test to see if server-side kill works

I added re-authentication to the `NioEchoServer` and I test for it with bearer tokens and delegation tokens within `SaslAuthenticatorTest`.  I added re-authentication every 50 ms for all end-to-end scala authorization integration tests as well.

I will work on the remaining 3 items on Wednesday.  Can you review what is here already, or would you prefer to wait?

Ron
"
424644619,5582,rajinisivaram,2018-09-26T09:14:38Z,@rondagostino I will review the PR tonight or tomorrow. There is a failing test iin the PR builds (`org.apache.kafka.common.security.authenticator.SaslAuthenticatorTest.testTokenReauthenticationOverSaslScram[reauthenticate=true]`) - can you check if that is a test issue or a code issue? I will start a system test run once you get a chance to look at that.
424707930,5582,rondagostino,2018-09-26T13:06:39Z,@rajinisivaram It is a test issue.  I cannot reproduce the error locally -- it passes on my laptop.  I pushed a commit to sleep a bit longer in the hope that it will fix the issue during the build.
424718026,5582,rajinisivaram,2018-09-26T13:37:30Z,"@rondagostino Thanks, I have started a system test run on your branch: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1979/. Will review the PR later tonight."
424802468,5582,rondagostino,2018-09-26T17:28:41Z,@rajinisivaram Test still fails here but cannot reproduce the issue locally.  Pushed another commit to try to fix it...
424950015,5582,rondagostino,2018-09-27T03:52:19Z,"@rajinisivaram Pushed a commit that I think fixes/resolves most of the issues you raised.  It's getting late here so I couldn't get to all of them, but I think we are in a better place at this point."
425078873,5582,rondagostino,2018-09-27T12:52:14Z,"@rajinisivaram The test failures are due to the tests being flaky rather than a coding problem in the feature implementation.  I may have been able to solve the delegation token-related test that was failing (it didn't fail in these runs) by increasing the session lifetime in that test (from 50 ms to 500 ms). 
 I will try to fix these flaky tests by increasing the associated session lifetimes as well.  I think what may be going on is the tests are taking a long time to run and by the time they send data the session expiration time has arrived and the connection is being killed.  It has also occurred to me that setting the session lifetime to something less than 1000 ms is probably not a good idea in general -- it may invite this type of flakiness due to unexpected delays, for example -- and maybe we should change the units of the configuration value to seconds instead of milliseconds to make sure it is never less than 1000 ms.  Thoughts?  Commit pushed with larger lifetimes, it was clean on my laptop."
425097942,5582,rondagostino,2018-09-27T13:44:25Z,@rajinisivaram I just pushed a commit to add an ExpiredSessionsKilledCount metric.  The KIP called for expired-connections-killed-total as the metric name but ExpiredSessionsKilledCount is consistent with the existing metric RequestHandlerAvgIdlePercent.  Thoughts?
425170620,5582,rajinisivaram,2018-09-27T17:10:04Z,"@rondagostino Sorry, I haven't had any time today to look into this PR. Given the complexity of the PR and how close it is to feature freeze, it would be very helpful if we could get another reviewer. I will try and take another look tomorrow, but not sure if we will have enough time to get this merged without help from one more reviewer. If it didn't meet feature freeze deadline, would it work for you if it was committed only to trunk and not the 2.1.0 branch?"
425176688,5582,rondagostino,2018-09-27T17:28:45Z,"@rajinisivaram We would really like this in 2.1.0 if at all possible.  Is there anything I can do to help?  Maybe find another reviewer, perhaps someone else who has voted for the KIP?"
425179089,5582,rajinisivaram,2018-09-27T17:36:17Z,"@rondagostino Yes, getting another reviewer would definitely speed up the process."
425180086,5582,rondagostino,2018-09-27T17:39:23Z,"@rajinisivaram There are a couple of changes that are not necessary as part of this PR -- more like cleanup of things I discovered while working on the alternate implementations that we considered and rejected.  I could remove those changes and submit them as minor tickets post-2.1.0 if it helps to minimize the surface area requiring review.  `ExpiringCredentialRefreshingLogin` is one, for example, and `OAuthBearerSaslClientTest` is another."
425261409,5582,rondagostino,2018-09-27T22:21:55Z,@rajinisivaram I discovered that many of the failing tests were due to the fact that GSSAPI was not re-authenticating because it uses the CLIENT_COMPLETE state on the client and I wasn't handling that possibility correctly.   This meant its connections were continually being killed by the server.  This is now fixed.  I also moved the expiration check from KafkaRequestHandler to SocketServer so that the check is done more upstream and therefore with as little delay as possible; I think that delay sometimes has an impact when we run with really tight expiration times on the order of milliseconds.  I'll be curious to see how the tests look now.
425285024,5582,rondagostino,2018-09-28T00:43:35Z,"@rajinisivaram Both builds passed.  I'm going to work on adding the last metrics, and if I get that done tonight then I believe this will be complete."
425312612,5582,rondagostino,2018-09-28T03:48:56Z,"@rajinisivaram I was able to add the additional metrics.  This PR now has all functionality, but I could not figure out how to test server-side expired connection kill or its metric since I have no way of running an older client.  I know it works because it was the cause of the test failures in `SaslGssapiSslEndToEndAuthorizationTest` -- I just don't have a way to test it in an automated fashion because there is no way to disable re-authentication at the client side.  I probably have to add metric documentation as well.  But I think that's it.  I asked someone to review, but this section of the product is pretty difficult to understand without experience, so I'm not optimistic about it.  Any chance you can review this on Friday?  Al tests passed last run, and I hope the same will be true of this one."
425365864,5582,rajinisivaram,2018-09-28T08:41:45Z,"@rondagostino In terms of testing, I think system test may be the way to go for testing with older clients since we already have other system tests for testing with older clients. If this goes in by Monday, you have until code freeze two weeks later to add the system test to 2.1.0."
425408404,5582,rondagostino,2018-09-28T11:37:13Z,@rajinisivaram Ok.  Last set of builds was clean again.  I'm confident this is working as expected and is able to be merged assuming a review cycle can be completed.
425438808,5582,rondagostino,2018-09-28T13:40:12Z,"@rajinisivaram Metric documentation is added, this is now done except for any additional PR review required.  All conversations from above are resolved/fixed.  I've contacted 5 people (4 of whom up-voted the KIP) in the hope that at least one will be able to review."
425588102,5582,rondagostino,2018-09-28T22:53:47Z,"@rajinisivaram I reverted 9 files and eliminated 500 changed lines from this PR.  I will submit minor tickets for these changes separately.  These changes perform cleanup of code that was discovered while implementing rejected alternatives.  These changes are minor and can be submitted separately at a later date.  They have no impact on functionality or performance and are simply reducing technical debt. Keeping these separate reduces the surface area of this PR.
"
426148561,5582,rondagostino,2018-10-02T04:53:16Z,"@rajinisivaram I added functionality to prevent changing the principal or the SASL mechanism during re-authentication and added tests for both cases.  I defined a 1-second minimum before you can re-authenticate a second time (see comment above) to prevent the rogue/buggy client from re-authenticating over and over again (the connection will be closed if the 1-second timeframe is violated, and then there will be the new DDoS delay as well, so I think this covers it).  I will address client interoperability with system tests (I assume it will be easier as you stated, though I am unfamiliar with the system test suite at the moment).  Assuming everything looks good, any chance of this being merged and included in 2.1.0 even though it is the morning after?"
426184322,5582,rajinisivaram,2018-10-02T08:00:59Z,@rondagostino I can do another review today to make sure it is ready to merge. Can you check with the 2.1.0 release manager @lindong28 on whether it can still go into 2.1.0? The alternative is to merge to trunk only after the branch is cut.
426244843,5582,rondagostino,2018-10-02T11:51:38Z,@lindong28 See comment from @rajinisivaram above -- any chance this can get into the 2.1.0 release if it is merged today?
426246111,5582,lindong28,2018-10-02T11:56:32Z,"@rondagostino @rajinisivaram Do you think we can merge this PR by end of Wednesday? Since this PR is almost ready to be merged, it is reasonable to wait for a day or two to include it in 2.1.0 release."
426253681,5582,rondagostino,2018-10-02T12:24:35Z,"@rajinisivaram the test failures seem to be timing-based.  I can't reproduce any of the issues locally, and the JDK 8 and 10 results are widely divergent as well.  I'm going to revert the part of the checks for re-authentications in the EndToEnd tests -- I'll only check to make sure nothing has been killed and no re-authentications have failed and allow zero re-authentications instead of demanding that at least one occurs.  I'll also set the max session re-auth to something greater than 1 second since that is the cutoff for repeated re-authentication rejection and I do see a re-authentication failure in the JDK 8 build (probably in the JDK 10 build as well since there are so many more failures there, but I haven't looked hard enough to find one).  Let's see if the next builds are clean again."
426260661,5582,rajinisivaram,2018-10-02T12:49:14Z,"@rondagostino @lindong28 I will do another review today and see where we are. We also need a good system test run on this branch. Unfortunately the last one I started didn't work, so started another one: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1985/. 

@rondagostino Since we don't have any interop tests yet, it will be good if you can run some manual tests with different versions of broker/clients."
426294584,5582,rondagostino,2018-10-02T14:27:00Z,@rajinisivaram I am unable to see the system test at the link https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1985/.  Do I need some kind of permission in the Jenkins instance that I do not have?  I get HTTP 404 Not Found.  I will work on manually running some interop tests.
426304117,5582,rajinisivaram,2018-10-02T14:52:16Z,"@rondagostino Sorry, not sure about access since it is a Confluent Jenkins. I will update with the system test results when the test run completes."
426329033,5582,rajinisivaram,2018-10-02T15:56:48Z,@rondagostino We need to get to the bottom of these test failures today if we want to merge tomorrow.
426358785,5582,rondagostino,2018-10-02T17:22:10Z,"@rajinisivaram Agreed.  The tests failures in the JDK 8 build appeared when I ""fixed"" the ExpiredConnectionsKilledCount metric.  I'm going to comment out that metric to see if the failures go away.  Unfortunately this is a long turnaround cycle -- a couple of hours -- but it's the best I can do given that I can't reproduce the issue locally under JDK 1.8."
426396730,5582,rondagostino,2018-10-02T19:15:54Z,"@rajinisivaram Still seeing this error on JDK 8 build:

> kafka.api.GroupEndToEndAuthorizationTest > testNoConsumeWithoutDescribeAclViaSubscribe FAILED
> java.lang.Exception: Unexpected exception, expected<org.apache.kafka.common.errors.TopicAuthorizationException> but was<org.apache.kafka.common.errors.SaslAuthenticationException>
> Caused by:
> org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed due to invalid credentials with SASL mechanism SCRAM-SHA-256

This is the same type of problem that occurred before.  It isn't happening as frequently without the metric there, but it is still happening.  I pushed a commit to add more debug information to the error message.  Will have to wait for these builds to finish and then wait another 1.5 hours for a failure to occur in the next build."
426462405,5582,rondagostino,2018-10-02T23:20:50Z,"@rajinisivaram @lindong28 I agree it would be unwise to include this in the 2.1.0 release.  Thank you both for your willingness to entertain the possibility over the past days.  The point that convinces me is this one:
> we have to ensure that there are no paths where existing authentication has changed in any way. Since we dont have tests that cover every possible scenario, I am not sure how we can verify that.

I am actively working on the AuthenticationException issue, and while it is difficult to run experiments where the turnaround time is on the order of hours, I am hopeful that I can figure out what is going on there.

Let's shoot for merging this to trunk when the above two issues are resolved.  I don't expect to be able to write exhaustive tests to guarantee that authentication is not impacted by re-authentication, but I will try to re-organize the code so that the diff makes it much easier to confirm by inspection that there is no impact.  I did not keep this in mind as I worked, so I think I can accomplish the same things with more clearly fenced-in changes.  I think that -- at a minimum -- will be helpful.

Thanks again for your willingness to give the PR a chance for 2.1.0, and Rajini thank you especially for the technical advice on the implementation and the reviews.  Hopefully I can get this into a trunk-mergeable state soon."
426643419,5582,rajinisivaram,2018-10-03T13:46:38Z,The system test run from this branch from yesterday passed all tests except the nine variations of QuotaTest which are also failing from trunk.
426653511,5582,rondagostino,2018-10-03T14:14:46Z,"@rajinisivaram I am now unable to reproduce the errors.  I've run 3 separate builds -- some were clean, and while others had errors they seem to be unrelated.  I can't fix something that doesn't show up!"
426656500,5582,rondagostino,2018-10-03T14:23:19Z,"I will respond to review comments incrementally so that I kick off a bunch of builds.  One of them is bound to exhibit the problem at some point, and then maybe I'll get useful information from the additional detail I put in the error message."
426986850,5582,rondagostino,2018-10-04T11:39:56Z,"@rajinisivaram Still unable to reproduce the problem, both JDK 8 and 10 builds are totally clean at this point.  Today I'm going to try to re-work the `SaslClientAuthenticator` and `SaslServerAuthenticator` code so that we can more easily say via inspection that the existing authentication flow is unchanged.  Hopefully that will be useful and then the builds will either remain clean or the `AuthenticationException` problem will arise (and I will get additional information via the exception message, which I augmented).  Then we can decide whether to commit this to trunk or not."
426999888,5582,rajinisivaram,2018-10-04T12:28:41Z,"@rondagostino Thanks for making the updates. FYI if you want to rerun tests without a commit, you can just add a comment `retest this please`."
427166084,5582,rondagostino,2018-10-04T20:55:22Z,retest this please
427260765,5582,rondagostino,2018-10-05T06:33:47Z,"@rajinisivaram I reorganized `SaslClientAuthenticator` and `SaslServerAuthenticator` to make it much easier to tell by inspection that authentication is unaffected by the re-authentication code.  I am unable to reproduce the `AuthenticationException` problem we were seeing the other day -- the issue has not reappeared across several builds.  The reorganized code and a successful system test should give us high confidence that authentication is unaffected and the issue is likely something that happens rarely -- maybe due to some kind of race condition that is possible only during specific re-authentication states.  That's conjecture on my part, but I don't know what else I can do if the issue doesn't reappear.  I spent the entire day staring at the code and reorganizing it, and nothing jumped out at me.

Please review the latest code and let me know your thoughts on how you wish to proceed with respect to potentially merging this to trunk."
427291312,5582,rajinisivaram,2018-10-05T08:46:16Z,"@rondagostino Thanks for the updates. I will do another round of review early next week. Since there have been no more failures, I think we can commit to trunk. Since we are early in the release cycle for the next release, we will have plenty of builds before that and if we do run into that exception again, hopefully the additional debug you added will pinpoint the cause."
427367528,5582,rondagostino,2018-10-05T13:33:36Z,"@rajinisivaram Thanks.  I'll look forward to addressing any review comments.  FYI, I just updated the KIP to refer to 2.2.0 instead of 2.1.0 and the current metric names (some of which changed over the course of this PR).  I also just pushed a commit updating the `ops.html` document to refer to 2.2.0 instead of 2.1.0."
429698198,5582,rondagostino,2018-10-15T03:32:48Z,"@rajinisivaram Everything is finished except re-doing `SaslAuthenticatorTest` to make it less wasteful (i.e. don't just blindly re-run all tests in ""re-authenticate"" mode) and performing a rebase.  I found rebase without squash to be difficult given that I had a bunch of OAuth technical debt cleanup changes in here along the way that I backed out.  I have a squash rebase ready to push that runs the test suite cleanly; I'll push that sometime Monday EDT, then assuming that runs well on the build farm all that will remain will be `SaslAuthenticatorTest`."
430094679,5582,rondagostino,2018-10-16T04:12:22Z,"@rajinisivaram I believe this is all set, all review comments are addressed.  Note that there were lots of errors on the previous JDK 11 build (https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/97/) but I looked at some of the other PR builds and the same thing happens elsewhere at times (e.g. https://builds.apache.org/job/kafka-pr-jdk11-scala2.12/99/).  The JDK 8 build was clean.  So I suspect this is fine.  **I have yet to see a recurrence of the `AuthenticationException` problem**."
430175809,5582,rajinisivaram,2018-10-16T09:53:25Z,@rondagostino Thanks for the updates. Agree that the test failures are unrelated to this PR. I will do another review tomorrow and hopefully we can merge this week.
430468029,5582,rondagostino,2018-10-17T02:32:35Z,Rebasing to resolve conflict with `clients/src/test/java/org/apache/kafka/common/network/NetworkTestUtils.java`
430472303,5582,rondagostino,2018-10-17T02:58:54Z,"@mjsax Looks like `trunk` build is broken due to https://github.com/apache/kafka/commit/2646781d3265c8269f35d39c199dfddf14d3c2a8.  When I rebased onto trunk for this PR I am getting this compile error locally and I assume it will also happen here:

```
streams/src/main/java/org/apache/kafka/streams/kstream/internals/TimeWindowedKStreamImpl.java:198: error: cannot find symbol
                    windows.segmentInterval()
                           ^
  symbol:   method segmentInterval()
  location: variable windows of type Windows<W>
  where W is a type-variable:
    W extends Window declared in class TimeWindowedKStreamImpl
```

I think `org.apache.kafka.streams.kstream.WindowsTest` has a similar compile error.
"
430474995,5582,rondagostino,2018-10-17T03:15:29Z,"Ok, I see https://github.com/apache/kafka/pull/5806 is where this will be fixed."
431457065,5582,rondagostino,2018-10-19T18:32:52Z,@rajinisivaram Any progress on another review?  Would like to merge ASAP so it gets as many builds and test runs as possible prior to the next release.
431789440,5582,rajinisivaram,2018-10-22T09:49:32Z,"@rondagostino Sorry, haven't had time to review yet, will try and do it today or tomorrow."
432309421,5582,rondagostino,2018-10-23T16:00:31Z,@rajinisivaram Latest comments are addressed/pushed.  The one issue that might need more attention is the `REAUTH_BAD_MECHANISM` state in `SaslServerAuthenticator`.  See my comment above.
433043426,5582,omkreddy,2018-10-25T13:08:05Z,@rondagostino my recent merge #5684 created some conflicts with this PR. Please rebase the PR. 
433181272,5582,rondagostino,2018-10-25T19:46:16Z,"@rajinisivaram Made two suggested changes, both builds were clean.  Rebasing onto latest trunk now to resolve conflicts."
433383564,5582,rondagostino,2018-10-26T11:56:58Z,"@rajinisivaram After rebase JDK8 build was clean, JDK 11 build has 2 failures but they seem unrelated/transient issues."
433390920,5582,rajinisivaram,2018-10-26T12:28:08Z,"@rondagostino Thanks for the updates. I think we are good to go. I will take a look later today and merge to trunk (if there are any other changes required, we can do them in follow-on PRs)."
627487350,8657,chia7712,2020-05-12T17:34:11Z,@rajinisivaram @junrao @windkit please take a look :)
637668386,8657,chia7712,2020-06-02T16:34:08Z,@junrao Could you take a look? 
643890644,8657,chia7712,2020-06-15T04:06:29Z,"> Another way that doesn't require checking lock.isHeldByCurrentThread is the following. But your approach seems simpler.

So... could we keep it simpler?"
643933638,8657,chia7712,2020-06-15T06:37:38Z,"```
kafka.admin.ReassignPartitionsUnitTest > testModifyBrokerThrottles FAILED
```

the flaky is traced by #8853"
644521032,8657,chia7712,2020-06-16T04:16:09Z,"> It will be helpful if you could preserve the commit history in future updates to the PR since that makes it easier to identify the delta changes.

my bad :(

I'll keep that in mind"
646487562,8657,chia7712,2020-06-19T07:34:56Z,"@junrao thanks for all your reviews. 

> I triggered a system test on this PR. https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3970/

How to see the result from this url? "
646685855,8657,junrao,2020-06-19T15:03:35Z,"@chia7712 : The system test results can be found in http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2020-06-18--001.1592550132--chia7712--fix_8334_avoid_deadlock--142a6c4c0/ 

If you click on report.html, it shows there were 76 failures. Could you take a look and see if it's related to this PR? I will trigger another run just to see if any of those failures were transient."
646718970,8657,chia7712,2020-06-19T16:08:47Z,"> Could you take a look and see if it's related to this PR? I will trigger another run just to see if any of those failures were transient.

I compare the failed tests to http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2020-06-16--001.1592310680--confluentinc--master--d07ee594d/report.html (the link is attached to https://issues.apache.org/jira/browse/KAFKA-10180). There are two failed tests happens only on this PR.

1. ConsumeBenchTest
```
Module: kafkatest.tests.core.consume_bench_test
Class:  ConsumeBenchTest
Method: test_multiple_consumers_random_group_topics
```
2. StreamsOptimizedTest
```
Module: kafkatest.tests.streams.streams_optimized_test
Class:  StreamsOptimizedTest
Method: test_upgrade_optimized_topology
```

wait for the new run."
647020292,8657,junrao,2020-06-20T16:55:57Z,This is a second run of the system tests. http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2020-06-19--001.1592614513--chia7712--fix_8334_avoid_deadlock--142a6c4c0/
647233460,8657,chia7712,2020-06-22T02:32:10Z,"> This is a second run of the system tests. http://testing.confluent.io/confluent-kafka-branch-builder-system-test-results/?prefix=2020-06-19--001.1592614513--chia7712--fix_8334_avoid_deadlock--142a6c4c0/

```ConsumeBenchTest``` pass on second run.


```StreamsOptimizedTest``` still fail and the error message is 
```
Exception in thread ""StreamsOptimizedTest-239c8c2e-5338-4a4c-a009-729ca21e1673-StreamThread-1"" java.lang.IllegalStateException: Tried to lookup lag for unknown task 2_0
	at org.apache.kafka.streams.processor.internals.assignment.ClientState.lagFor(ClientState.java:306)
	at java.util.Comparator.lambda$comparingLong$6043328a$1(Comparator.java:511)
	at java.util.Comparator.lambda$thenComparing$36697e65$1(Comparator.java:216)
	at java.util.TreeMap.put(TreeMap.java:552)
	at java.util.TreeSet.add(TreeSet.java:255)
	at java.util.AbstractCollection.addAll(AbstractCollection.java:344)
	at java.util.TreeSet.addAll(TreeSet.java:312)
	at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.getPreviousTasksByLag(StreamsPartitionAssignor.java:1250)
	at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assignTasksToThreads(StreamsPartitionAssignor.java:1164)
	at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.computeNewAssignment(StreamsPartitionAssignor.java:920)
	at org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor.assign(StreamsPartitionAssignor.java:391)
	at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:583)
```

need to dig in it :(

@junrao Could you trigger a system test for trunk branch ? the related code have been updated recently (0f68dc7a640b26a8edea154ea4ea2b6d93b5104b)

I run system test ```streams_optimized_test``` on my local for trunk branch and it always fails :( "
647352301,8657,chia7712,2020-06-22T07:57:28Z,I have filed https://issues.apache.org/jira/browse/KAFKA-10191 to trace ```StreamsOptimizedTest```
647688705,8657,junrao,2020-06-22T18:07:36Z,It seems we just fixed a bunch of client compatibility related failures in https://github.com/apache/kafka/pull/8841. Another 18 test failures were due to TLSv1.3 and are tracked in https://issues.apache.org/jira/browse/KAFKA-10180. I started another run of system tests.
648249808,8657,junrao,2020-06-23T15:47:28Z,There were still lots of client compatibility related failures http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-06-22--001.1592885123--chia7712--fix_8334_avoid_deadlock--142a6c4c0/report.html . Not sure why since those tests were passing in http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-06-16--001.1592335112--mumrah--KAFKA-10123--63c1b14a4/report.html
648254407,8657,junrao,2020-06-23T15:54:48Z,@chia7712 : I think the client compatibility test failures are probably because you haven't rebased the PR. #8841 was fixed 6 days ago. Could you rebase your PR?
648254987,8657,chia7712,2020-06-23T15:55:44Z,"> Could you rebase your PR?

done"
648259565,8657,junrao,2020-06-23T16:03:31Z,Thanks. Triggering another round of system tests.
649615869,8657,junrao,2020-06-25T15:17:27Z,"Latest test results with 31 failures. http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-06-24--001.1593050486--chia7712--fix_8334_avoid_deadlock--4cd68e7a9/report.html

I will trigger a system tests for trunk."
649635872,8657,chia7712,2020-06-25T15:46:00Z,"> I will trigger a system tests for trunk.

@junrao big thanks !"
650240109,8657,junrao,2020-06-26T15:25:37Z,"For comparison, 29 test failures in trunk http://testing.confluent.io/confluent-kafka-system-test-results/?prefix=2020-06-25--001.1593132363--apache--trunk--3348fc49d/"
650503360,8657,chia7712,2020-06-27T06:13:16Z,"> For comparison, 29 test failures in trunk http://testing.confluent.io/confluent-kafka-system-test-results/?prefix=2020-06-25--001.1593132363--apache--trunk--3348fc49d/

It seems to me there are many flaky in system tests and I need more time to dig in them (TLSv1.3 is traced by https://issues.apache.org/jira/browse/KAFKA-10180)

**Following failed tests happens with this PR**
```
Module: kafkatest.tests.streams.streams_upgrade_test
Class:  StreamsUpgradeTest
Method: test_simple_upgrade_downgrade
Arguments:
{
  ""from_version"": ""0.10.1.1"",
  ""to_version"": ""0.10.2.2""
}
```

```
Module: kafkatest.tests.streams.streams_cooperative_rebalance_upgrade_test
Class:  StreamsCooperativeRebalanceUpgradeTest
Method: test_upgrade_to_cooperative_rebalance
Arguments:
{
  ""upgrade_from_version"": ""1.0.2""
}
```

```
Module: kafkatest.tests.core.round_trip_fault_test
Class:  RoundTripFaultTest
Method: test_round_trip_workload_with_broker_partition
```
```
Module: kafkatest.tests.connect.connect_distributed_test
Class:  ConnectDistributedTest
Method: test_bounce
Arguments:
{
  ""clean"": true,
  ""connect_protocol"": ""compatible""
}
```

```
Module: kafkatest.tests.core.group_mode_transactions_test
Class:  GroupModeTransactionsTest
Method: test_transactions
Arguments:
{
  ""bounce_target"": ""clients"",
  ""failure_mode"": ""clean_bounce""
}
```

```
Module: kafkatest.tests.core.replica_scale_test
Class:  ReplicaScaleTest
Method: test_produce_consume
Arguments:
{
  ""partition_count"": 34,
  ""replication_factor"": 3,
  ""topic_count"": 500
}
```
```
Module: kafkatest.tests.core.zookeeper_security_upgrade_test
Class:  ZooKeeperSecurityUpgradeTest
Method: test_zk_security_upgrade
Arguments:
{
  ""security_protocol"": ""SASL_PLAINTEXT""
}
```
```
Module: kafkatest.tests.streams.streams_eos_test
Class:  StreamsEosTest
Method: test_failure_and_recovery
Arguments:
{
  ""processing_guarantee"": ""exactly_once""
}
```

---
**Following failed tests happens without this PR**
```
Module: kafkatest.tests.streams.streams_upgrade_test
Class:  StreamsUpgradeTest
Method: test_metadata_upgrade
Arguments:
{
  ""from_version"": ""0.10.0.1"",
  ""to_version"": ""0.11.0.3""
}
```
```
Module: kafkatest.tests.streams.streams_upgrade_test
Class:  StreamsUpgradeTest
Method: test_metadata_upgrade
Arguments:
{
  ""from_version"": ""0.11.0.3"",
  ""to_version"": ""2.4.1""
}
```
```
Module: kafkatest.tests.core.transactions_test
Class:  TransactionsTest
Method: test_transactions
Arguments:
{
  ""bounce_target"": ""brokers"",
  ""check_order"": true,
  ""failure_mode"": ""clean_bounce"",
  ""use_group_metadata"": false
}
```
```
Module: kafkatest.tests.core.transactions_test
Class:  TransactionsTest
Method: test_transactions
Arguments:
{
  ""bounce_target"": ""clients"",
  ""check_order"": false,
  ""failure_mode"": ""hard_bounce"",
  ""use_group_metadata"": true
}
```
```
Module: kafkatest.tests.core.security_rolling_upgrade_test
Class:  TestSecurityRollingUpgrade
Method: test_rolling_upgrade_phase_two
Arguments:
{
  ""broker_protocol"": ""SASL_SSL"",
  ""client_protocol"": ""SASL_SSL""
}
```
```
Module: kafkatest.tests.client.consumer_test
Class:  OffsetValidationTest
Method: test_consumer_failure
Arguments:
{
  ""clean_shutdown"": true,
  ""enable_autocommit"": true
}
```
```
Module: kafkatest.tests.core.upgrade_test
Class:  TestUpgrade
Method: test_upgrade
Arguments:
{
  ""compression_types"": [
    ""snappy""
  ],
  ""from_kafka_version"": ""0.10.0.1"",
  ""to_message_format_version"": null
}
```"
650583070,8657,junrao,2020-06-27T16:31:50Z,@chia7712 I just merged the PR for KAFKA-10180. Perhaps you can rebase again.
650586275,8657,chia7712,2020-06-27T17:01:31Z,"> Perhaps you can rebase again.

done"
650786185,8657,junrao,2020-06-28T16:02:44Z,"Latest system test results. Down to 15 failures. http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-06-28--001.1593338982--chia7712--fix_8334_avoid_deadlock--c69ab94d3/report.html

Will do another trunk run for comparison."
650923843,8657,chia7712,2020-06-29T05:50:05Z,"1. connect_rest_test.py is traced by #8944
2. zookeeper_tls_test.py is traced by #8949"
651902726,8657,junrao,2020-06-30T16:24:40Z,14 system test failures in trunk. http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2020-06-29--001.1593477077--apache--trunk--52d9d6651/report.html
651941352,8657,chia7712,2020-06-30T17:38:11Z,"There are three failed tests which take place with this PR.

1. core/transactions_test
1. core/downgrade_test
1. core/upgrade_test

will dig in them :)"
653224351,8657,chia7712,2020-07-02T21:17:33Z,"> core/transactions_test
core/downgrade_test
core/upgrade_test

those are flaky on my local as well. @junrao Should I fix all flaky before merging this PR? "
653227128,8657,junrao,2020-07-02T21:24:53Z,"@chia7712 : Thanks for the investigation. You don't need to fix all those flaky tests. It would be helpful if you could file separate jiras to track them, if not already.

Overall, do you think there is no new system test failure introduced by this PR?"
653232232,8657,chia7712,2020-07-02T21:41:37Z,"> Overall, do you think there is no new system test failure introduced by this PR?

I did not observe obvious relation between flaky and this PR. I have filed 4 PRs to fix flaky system tests. Maybe we can run system tests again after those PRs are merged. Hope there is good luck to me :)"
653953935,8657,junrao,2020-07-05T23:39:49Z,@chia7712 : Thanks. Are the unit test failures also due to flaky tests?
653956055,8657,chia7712,2020-07-05T23:58:41Z,"> Are the unit test failures also due to flaky tests?

I think so. However, Im trying to resolve all of them so as to make this PR more safe :)

#8981 #8974 #8913 are pending for reviewing. 

The remaining failed tests on this PR are downgrade_test and upgrade_test. I will check them carefully tomorrow.

"
656365594,8657,junrao,2020-07-09T21:43:11Z,@chia7712 : All 3 PRs you fixed above have been merged. Do you want to rebase again so that I can run system tests one more time?
656382338,8657,chia7712,2020-07-09T22:31:33Z,"> Do you want to rebase again so that I can run system tests one more time?

sure!"
657739902,8657,chia7712,2020-07-13T19:11:12Z,@junrao could you trigger system tests again?
658027392,8657,chia7712,2020-07-14T07:47:55Z,"```
org.apache.kafka.streams.integration.EosIntegrationTest.shouldNotViolateEosIfOneTaskFailsWithState[exactly_once]
```

It is traced by https://issues.apache.org/jira/browse/KAFKA-9831"
658281873,8657,junrao,2020-07-14T16:31:22Z,"@chia7712 : Here is the latest system test result. http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-07-14--001.1594711494--chia7712--fix_8334_avoid_deadlock--9ebe39447/report.html 

The number of failures went up to 17."
658294532,8657,chia7712,2020-07-14T16:55:42Z,"> The number of failures went up to 17.

ok. Let me dig in them again :)

there are 3 failed tests related to ```security_rolling_upgrade_test``` and I have filed #9021 to fix them.
 
```transactions_test.py``` is traced by #9026 and the approach of #9026 works for ```group_mode_transactions_test.py``` I think.

```streams_standby_replica_test``` -> https://issues.apache.org/jira/browse/KAFKA-10287

```client_compatibility_features_test.py``` -> https://issues.apache.org/jira/browse/KAFKA-10288

```connect_distributed_test.py``` -> https://issues.apache.org/jira/browse/KAFKA-10289

```compatibility_test_new_broker_test.py``` -> https://issues.apache.org/jira/browse/KAFKA-10290

```log4j_appender_test.py``` -> https://issues.apache.org/jira/browse/KAFKA-10291

```streams_broker_bounce_test.py```  -> https://issues.apache.org/jira/browse/KAFKA-10292

```streams/streams_eos_test.py``` -> https://issues.apache.org/jira/browse/KAFKA-10293
"
661862318,8657,chia7712,2020-07-21T13:29:35Z,"@junrao Could you run system tests on both trunk and this PR?

the tests which fail frequently on my local are shown below.

1. transactions_test.py #9026
1. group_mode_transactions_test.py #9026
1. security_rolling_upgrade_test.py #9021
1. streams_standby_replica_test.py (I'm digging it)

Also, I have filed tickets for other failed system tests."
662642708,8657,junrao,2020-07-22T19:15:47Z,@chia7712 : I just merged #9026. Could you rebase again? I will run the system tests after that. Thanks. 
662810186,8657,chia7712,2020-07-23T04:14:53Z,"> Could you rebase again? I will run the system tests after that. Thanks.

done. the known flaky ```group_mode_transactions_test.py``` is traced by #9059"
663109443,8657,junrao,2020-07-23T16:37:46Z,"@chia7712: Only 6 test failures in the latest run with your PR. http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-07-23--001.1595503536--chia7712--fix_8334_avoid_deadlock--3462b0008/report.html

I will do another run on trunk for comparison."
663597341,8657,junrao,2020-07-24T15:32:27Z,3 system test failures with trunk. http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2020-07-23--001.1595551051--apache--trunk--0b181fdde/report.html
663618045,8657,chia7712,2020-07-24T16:17:50Z,"@junrao thanks for the reports.

Unfortunately, the failed tests are totally different in both results :(

This PR has been rebased so the fix for ```group_mode_transactions_test``` is included.

```streams_standby_replica_test``` will get fixed by #9066.

I will test the others on my local later."
663818439,8657,chia7712,2020-07-25T06:42:16Z,"@junrao 

I have rebased this PR to include fix of ```group_mode_transactions_test```. Could you run system tests again? Except for ```streams_eos_test```, ```streams_standby_replica_test``` and transaction tests, other tests work well on my local."
664652669,8657,junrao,2020-07-27T21:40:27Z,"@chia7712 : Thanks for the diligence. Running the system tests again.

@ijuma @hachikuji @rajinisivaram : There are two proposed ways of solving this issue.

1. Async completion of delayed operations in a separate thread pool (https://github.com/hachikuji/kafka/commit/3bee4acc442f09e95d60bbb7ce9eb246310dcb63). 

2. This PR where we let the caller of ReplicaManager.appendRecords() to complete returned delayed operations without holding a lock.

Approach 1 makes the API a bit cleaner. However, it introduces yet another thread pool used in the common path.This thread pool needs to be configured and monitored properly, which adds complexity. Since the deadlock issue is isolated in GroupCoordinator, I am not sure if it's worthwhile to add another thread pool just to avoid the deadlock. So, even though approach 2 adds a bit complexity in the API, it's probably still the better solution at this stage. Please let me know your thoughts."
665146517,8657,junrao,2020-07-28T16:38:05Z,2 system test failures in the latest PR. http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-07-27--001.1595898508--chia7712--fix_8334_avoid_deadlock--51cc7ae29/report.html
665149860,8657,chia7712,2020-07-28T16:44:19Z,"```streams_standby_replica_test```  -> https://issues.apache.org/jira/browse/KAFKA-10287

I will take a look at ```streams_broker_bounce_test``` (https://issues.apache.org/jira/browse/KAFKA-10292)"
666398632,8657,chia7712,2020-07-30T14:26:32Z,"> 2 system test failures in the latest PR

those 2 failed tests are flaky on my local and there are issue/PR related to them.

@junrao @ijuma @hachikuji @rajinisivaram please take a look."
666403581,8657,ijuma,2020-07-30T14:33:18Z,"@junrao Thanks for the summary. With regards to the thread pool option, would this be used for completion of delayed operations for the group coordinator only? When it comes to tuning and monitoring, you're thinking we'd have to introduce a config for the number of threads and an `idle` metric?"
666753554,8657,junrao,2020-07-30T22:45:13Z,"@ijuma : I think the proposal is to complete all delayed operations in a separate thread pool. My concerns for that approach are the following: (1) Configuration: How many threads should be used? Should that be dynamically configurable? Should the queue to this thread pool be unbounded or fixed size? If it's the latter, how do we configure the queue size? (2) Monitoring: How do we monitor the utilization of the new thread pool and the potential back pressure it creates (if the queue is fixed size)? (3) Quota: Do we need to account for the resource utilization of this new thread pool per client/user for quota purpose? (4) Debugging: If there is a latency issue, how do we know whether this is due to delays in the new thread pool? 

So, overall, I feel that we should be careful with adding another thread pool used in the common code path since it adds other complexity. Given that this issue is isolated in GroupCoordinator, I am not sure adding a thread pool is an overall simpler solution.

@chia7712 : I saw that you consolidated the commit history. Were there significant changes since the last review?"
666882466,8657,chia7712,2020-07-31T02:46:07Z,"> Were there significant changes since the last review?

I rebased code base to include https://github.com/apache/kafka/commit/96e0719e421a91b5fa289dc0d23276977655633e which fixes flaky ```streams_standby_replica_test```.

retest this please"
667233872,8657,junrao,2020-07-31T17:09:09Z,"@chia7712 : So, it's just a rebase and there is no change in your PR?"
667442912,8657,chia7712,2020-08-01T01:01:39Z,"> So, it's just a rebase and there is no change in your PR?

The last change to this PR is to rename a class (according to @ijumas comment)"
669330984,8657,junrao,2020-08-05T17:39:14Z,"@chia7712 : The following is my thought after thinking about this a bit more. The changes that we made in DelayedJoin is complicated and it still doesn't completely solve the deadlock issue. Adding more complexity there to solve the issue is probably not ideal. As a compromise, I was thinking another approach. We could pass down a flag to ReplicaManager.appendRecords() to complete the delayed requests in a separate thread there. Only GroupCoordinator will set this flag. So, the background completeness check is limited to the offset_commit topic. Since this topic is low volume, a single thread is likely enough to handle the load. So, we don't have to make the number of thread configurable and don't have to worry about this thread being overwhelmed. The benefit of this approach is that the code is probably easier to understand since we could keep most existing logic in GroupCoordinator unchanged. What do you think?"
669647781,8657,chia7712,2020-08-06T02:35:20Z,"@junrao I also don't want to include more mechanisms to complicate this story. It seems to me we can do a litter factor for ```DelayedOperation``` to resolve this issue. ```forceComplete``` is thread-safe already so it does not need lock. It means ```forceComplete``` can be decoupled from ```tryComplete```.  With this change, ```forceComplete ``` is involved by ```maybeTryComplete``` and it is called after releasing lock. for example:
```scala
private[server] def maybeTryComplete(): Boolean = {
  boolean timeToComplete = try lock.lock tryComplete() finally lock.unlock
  timeToComplete && forceComplete()
}
```

Also, ```tryComplete``` can be renamed to ```canComplete```. the method name is consistent to its actual behavior.

The benefit from this change are shown below. 
1. simplify behavior of ```tryComplete```. the subclasses don't need to call ```forceComplete``` always
1. lower possibility of conflicting lock"
670113688,8657,junrao,2020-08-06T18:38:05Z,"@chia7712 : If we could solve the issue by simplifying DelayedOperation, it would be ideal. I am not sure how your proposal avoids the above potential deadlock. Could you provide a bit more detail? Thanks."
670293057,8657,chia7712,2020-08-07T02:55:59Z,"> Could you provide a bit more detail?

The new deadlock you mentioned is caused by PR. This PR introduces extra lock (```ReentrantLock```) to ```DelayedJoin``` (By contrast, previous ```DelayedJoin``` has a group lock only). The thread has to get two locks (group lock and ReentrantLock) to complete ```DelayedJoin``` and so deadlock happens when two locks are held by different thread. Hence, my approach is to remove the extra lock introduced by this PR and so deadlock will be gone.

The changes to ```DelayedJoin``` in this PR is to resolve deadlock happening on ```DelayedJoin#onComplete```. ```onComplete``` has to get other group lock to resolve delayed request but it is executed within a group lock. In order to resolve deadlock, this PR tries to move ```onComplete``` out of group lock. However, after thinking about this a bit more, why ```onComplete``` is executed within a lock? It is thread-safe already and it is not always executed within a lock (if it is executed on timeout). Hence, the implementation of my approach is to refactor ```DelayedOperation``` to move ```onComplete```  out of unnecessary lock. The refactor includes following changes.

1. ```forceComplete``` should be executed by ```DelayedOperation``` other than subclasss
1. ```forceComplete``` is executed after releasing lock
```scala
private[server] def maybeTryComplete(): Boolean = {
  lock.lock
  val timeToComplete = try tryComplete() finally lock.unlock
  timeToComplete && forceComplete()
}
```
3. ```forceComplete``` is renamed to ```canComplete```
"
671520491,8657,junrao,2020-08-10T18:38:10Z,"@chia7712 : I agree mostly with your assessment. For most delayed operations, the checking for the completeness of the operation and the calling of onComplete() don't have to be protected under the same lock.

The only one that I am not quite sure is DelayedJoin. Currently, DelayedJoin.tryCompleteJoin() checks if all members have joined and DelayedJoin.onComplete() modifies the state of the group. Both operations are done under the same group lock. If we relax the lock, it seems that the condition ""all members have joined"" may no longer be true when we get to DelayedJoin.onComplete() even though that condition was true during the DelayedJoin.tryCompleteJoin() check. It's not clear what we should do in that case."
671749755,8657,chia7712,2020-08-11T06:13:06Z,"> Both operations are done under the same group lock. If we relax the lock, it seems that the condition ""all members have joined"" may no longer be true when we get to DelayedJoin.onComplete() even though that condition was true during the DelayedJoin.tryCompleteJoin() check. It's not clear what we should do in that case.

Your feedback always makes sense.  

It seems to me the approach has to address following issue.

1. avoid conflicting (multiples) locks
1. small change (don't introduce complicated mechanism)
1. keep behavior compatibility (```hasAllMembersJoined``` and ```onCompleteJoin``` should be included in same lock)

I'd like to add an new method (default implementation is empty body) to ```DelayedOperation```. The new method is almost  same to ```onComplete``` except for that it is executed without locking. Currently, only ```DelayedJoin``` has to use it.

```scala

  def cleanup(): Unit = {}

  private[server] def maybeTryComplete(): Boolean = {
    lock.lock()
    if (try tryComplete() finally lock.unlock()) {
      cleanup()
      true
    } else false  
  }

  override def run(): Unit = {
    if (forceComplete()) {
      cleanup()
      onExpiration()
    }
  }
```
"
679256967,8657,junrao,2020-08-24T17:16:42Z,"@chia7712 : Sorry for the late response. I just realized there seems to be another issue in addition to the above one that I mentioned. The second issue is that we hold a group lock while calling `joinPurgatory.tryCompleteElseWatch`. In this call, it's possible that DelayedJoin.onComplete() will be called. In that case, since the caller holds the group lock, we won't be completing partitionsToComplete in completeDelayedRequests()."
679855792,8657,chia7712,2020-08-25T07:30:26Z,"> The second issue is that we hold a group lock while calling joinPurgatory.tryCompleteElseWatch. In this call, it's possible that DelayedJoin.onComplete() will be called. In that case, since the caller holds the group lock, we won't be completing partitionsToComplete in completeDelayedRequests().

@junrao I go through group lock again and it is almost used anywhere :( 

I'm worry about deadlock caused by current approach so I'd like to address @hachikuji refactor and your approach (separate thread). The following changes are included.

1. ```Partition``` does not complete delayed operations. Instead, it adds those delayed operations to a queue. The callers ought to call the new method ```Partition.completeDelayedActions``` to complete those delayed operations in proper place (to avoid conflicting locking).
1. apple above new method to all callers who need to complete delay operations.
1. ```GroupCoordinator.onCompleteJoin``` is called by ```DelayedJoin``` only but it always held a group lock. To resolve it, we complete delayed requests in a separate thread. 
1. keep using ```tryLock```. The known conflicting locks should be resolved by above changes. Using ```tryLock``` can protect us from deadlock which we have not noticed :)

@junrao WDYT?"
681109748,8657,junrao,2020-08-26T20:35:03Z,"@chia7712 : Thanks for the reply. I like your overall idea and I think it can be used to solve the problem completely in a simpler way.

1. Instead of at `Partition`, we collect all pending delayed check operations in a queue in ReplicaManager. All callers to ReplicaManager.appendRecords() are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without holding any conflicting locks.

2. Most callers to ReplicaManager.appendRecords() are from KafkaApis. We can just add the logic to check the ReplicaManager queue at the end of KafkaApis.handle(), at which point, no conflicting locks will be held.

3. Another potentially caller is the expiration thread in a purgatory. SystemTimer always runs the expiration logic in a separate thread and DelayedOperation.onExpiration() is always called without holding any conflicting lock. So, for those delayed operations using ReplicaManager.appendRecords(), we can pass down a flag to DelayedOperation so that at the end of onExpiration, we check the ReplicaManager queue if the flag is set.

4. We keep `DelayedJoin` as it is, still passing in the group lock to DelayedOperation to avoid deadlocks due to two levels of locking.

5. We can still get rid of the `tryLock` logic in DelayedOperation for simplification since there is no opportunity for deadlock.

What do you think?"
681318790,8657,chia7712,2020-08-27T03:16:04Z,"> We keep DelayedJoin as it is, still passing in the group lock to DelayedOperation to avoid deadlocks due to two levels of locking.

Just double check. We use a separate thread to handle ```groupManager.storeGroup``` and ```joinPurgatory.tryCompleteElseWatch``` in ```GroupCoordinator.onCompleteJoin```, right? ```GroupCoordinator.onCompleteJoin``` is called by ```DelayedJoin.onComplete``` only and it is possible to hold a group lock already."
681361030,8657,junrao,2020-08-27T04:04:57Z,"The following is my understand. The current PR introduces a new deadlock through the following path.

path 1
hold group lock -> joinPurgatory.tryCompleteElseWatch(delayedJoin) -> watchForOperation (now delayedJoin visible through other threads) -> operation.maybeTryComplete() -> hold delayedJoin.lock

path 2
delayedJoin.maybeTryComplete -> hold hold delayedJoin.lock -> tryComplete() -> hold group lock

The existing code doesn't have this deadlock since (1) delayedJoin.lock is the same as the group lock held in the caller and (2) a delayed join operation is registered under the group key (so each time we check completeness for a group key, only one delayed join operation will be affected). By switching back to this code, we avoid the new deadlock.

The existing code has a different deadlock issue that groupManager.storeGroup() in GroupCoordinator.onCompleteJoin may need to complete to other delayed operations and potentially hold a different group lock while already holding a group lock. This issue will be resolved if we complete those delayed operations due to groupManager.storeGroup() elsewhere without holding any locks.
"
681375395,8657,chia7712,2020-08-27T04:14:55Z,"> This issue will be resolved if we complete those delayed operations due to groupManager.storeGroup() elsewhere without holding any locks.

So ```DelayedJoin``` DOES NOT complete any delayed requests in this path anymore and we expect that someone who don't hold lock should complete them?"
681708424,8657,chia7712,2020-08-27T07:41:55Z,"@junrao I have submitted  a draft patch. As this new approach is totally different from previous code, I delete previous commits in order to clean git history."
682879578,8657,chia7712,2020-08-28T16:50:30Z,"@junrao Thanks for all suggestions!

Is Jenkins on vacation? Could you trigger a system test?"
682991870,8657,ijuma,2020-08-28T17:54:39Z,@chia7712 See KAFKA-10444 with regards to Jenkins.
683052000,8657,junrao,2020-08-28T18:30:49Z,@ijuma @hachikuji @rajinisivaram : Do you want to take another look at the latest solution from Chia-Ping? It (1) solves the known issues completely; (2) doesn't require new threads; (3) adds minimal changes to existing code; (4) simplifies existing code by removing the tryLock logic.
683075891,8657,ijuma,2020-08-28T18:53:25Z,"This looks promising. One question, do we want every request to drain this ReplicaManager queue or only the callers of `appendRecords`? I think this answer affects the design a bit.

If it's meant to be called by every request, then maybe we should have the delayed actions in a separate class instead of ReplicaManager. Other classes could, in theory, add their own delayed actions to this queue too.

If it's meant to be called after calling `appendRecords`, then it may be cleaner to add the call within the method that calls `appendRecords` (with maybe a helper method in `KafkaApis` to make it less error prone)."
683284825,8657,chia7712,2020-08-29T12:36:45Z,"> If it's meant to be called by every request, then maybe we should have the delayed actions in a separate class instead of ReplicaManager. Other classes could, in theory, add their own delayed actions to this queue too.

I preferred this idea as it prevent us from missing any action.
"
683400677,8657,chia7712,2020-08-30T09:59:37Z,"> Could you do some perf tests so that we know the performance of fetch requests doesn't change noticeably?

@junrao Are there any suggested official benchmark tools?"
683434648,8657,chia7712,2020-08-30T15:33:32Z,"> Could you do some perf tests so that we know the performance of fetch requests doesn't change noticeably?

the result of ```Benchmark.test_producer_and_consumer``` is attached below. It seems the patch gets better throughput :)

I will run more performance tests tomorrow.

**BEFORE**
```
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none
status:     PASS
run time:   1 minute 8.041 seconds
{""consumer"": {""records_per_sec"": 570060.4264, ""mb_per_sec"": 54.3652}, ""producer"": {""records_per_sec"": 611583.389395, ""mb_per_sec"": 58.33}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   58.549 seconds
{""consumer"": {""records_per_sec"": 1331203.4079, ""mb_per_sec"": 126.9535}, ""producer"": {""records_per_sec"": 1257387.149503, ""mb_per_sec"": 119.91}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none
status:     PASS
run time:   1 minute 7.757 seconds
{""consumer"": {""records_per_sec"": 577133.9528, ""mb_per_sec"": 55.0398}, ""producer"": {""records_per_sec"": 582106.059724, ""mb_per_sec"": 55.51}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   59.287 seconds
{""consumer"": {""records_per_sec"": 1421868.335, ""mb_per_sec"": 135.5999}, ""producer"": {""records_per_sec"": 1332267.519318, ""mb_per_sec"": 127.05}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=none
status:     PASS
run time:   1 minute 1.819 seconds
{""consumer"": {""records_per_sec"": 987361.7694, ""mb_per_sec"": 94.1622}, ""producer"": {""records_per_sec"": 942862.530643, ""mb_per_sec"": 89.92}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=snappy
status:     PASS
run time:   55.877 seconds
{""consumer"": {""records_per_sec"": 1479508.8031, ""mb_per_sec"": 141.097}, ""producer"": {""records_per_sec"": 1339046.598822, ""mb_per_sec"": 127.7}}
--------------------------------------------------------------------------------
```


**AFTER**
```
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=none
status:     PASS
run time:   1 minute 9.619 seconds
{""consumer"": {""records_per_sec"": 615422.4875, ""mb_per_sec"": 58.6913}, ""producer"": {""records_per_sec"": 640779.187492, ""mb_per_sec"": 61.11}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.2.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   57.667 seconds
{""consumer"": {""records_per_sec"": 1444043.3213, ""mb_per_sec"": 137.7147}, ""producer"": {""records_per_sec"": 1398014.818957, ""mb_per_sec"": 133.33}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=none
status:     PASS
run time:   1 minute 6.802 seconds
{""consumer"": {""records_per_sec"": 619770.6848, ""mb_per_sec"": 59.1059}, ""producer"": {""records_per_sec"": 646203.55412, ""mb_per_sec"": 61.63}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy
status:     PASS
run time:   1 minute 0.790 seconds
{""consumer"": {""records_per_sec"": 1484119.9169, ""mb_per_sec"": 141.5367}, ""producer"": {""records_per_sec"": 1403902.849923, ""mb_per_sec"": 133.89}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=none
status:     PASS
run time:   1 minute 0.074 seconds
{""consumer"": {""records_per_sec"": 916422.2874, ""mb_per_sec"": 87.3968}, ""producer"": {""records_per_sec"": 941796.948578, ""mb_per_sec"": 89.82}}
--------------------------------------------------------------------------------
test_id:    kafkatest.benchmarks.core.benchmark_test.Benchmark.test_producer_and_consumer.security_protocol=PLAINTEXT.compression_type=snappy
status:     PASS
run time:   57.799 seconds
{""consumer"": {""records_per_sec"": 1414027.1493, ""mb_per_sec"": 134.8521}, ""producer"": {""records_per_sec"": 1330671.989355, ""mb_per_sec"": 126.9}}
--------------------------------------------------------------------------------
```"
683602732,8657,chia7712,2020-08-31T07:03:55Z,"@junrao the result of ```benchmark_test.py``` is attached (see description)

The main regression ({""records_per_sec"": 3653635.3672, ""mb_per_sec"": 348.4378} -> {""records_per_sec"": 2992220.2274, ""mb_per_sec"": 285.3604}) happens in case ```test_id: kafkatest.benchmarks.core.benchmark_test.Benchmark.test_consumer_throughput.interbroker_security_protocol=PLAINTEXT.tls_version=TLSv1.3.security_protocol=SSL.compression_type=snappy```. 

I re-run the case 5 times and it seems the throughput of that case is not stable.

**BEFORE**

1. {""records_per_sec"": 3653635.3672, ""mb_per_sec"": 348.4378} 
1. {""records_per_sec"": 3812428.517, ""mb_per_sec"": 363.5815}
1. {""records_per_sec"": 3012048.1928, ""mb_per_sec"": 287.2513}
1. {""records_per_sec"": 3182686.1871, ""mb_per_sec"": 303.5246}
1. {""records_per_sec"": 2997601.9185, ""mb_per_sec"": 285.8736}

**AFTER**

1. {""records_per_sec"": 2992220.2274, ""mb_per_sec"": 285.3604}
1. {""records_per_sec"": 3698224.8521, ""mb_per_sec"": 352.6902}
1. {""records_per_sec"": 2977076.5109, ""mb_per_sec"": 283.9161}
1. {""records_per_sec"": 3676470.5882, ""mb_per_sec"": 350.6156}
1. {""records_per_sec"": 3681885.1252, ""mb_per_sec"": 351.1319}
"
683859366,8657,junrao,2020-08-31T15:41:57Z,"@chia7712 : Thanks for the performance results. It seems that the average across multiple runs doesn't change much?

Also, 1 failure in the latest system test run.
http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2020-08-30--001.1598849124--chia7712--fix_8334_avoid_deadlock--960f19b29/report.html
"
683863356,8657,chia7712,2020-08-31T15:48:18Z,"> It seems that the average across multiple runs doesn't change much?

yep. I didn't observe obvious regression caused by this patch.

> Also, 1 failure in the latest system test run.

```kafkatest.tests.connect.connect_distributed_test``` was flaky (see https://issues.apache.org/jira/browse/KAFKA-10289)"
685831526,8657,junrao,2020-09-02T15:58:01Z,"@chia7712 It seems there are some compilation errors in jenkins?

https://ci-builds.apache.org/job/Kafka/job/kafka-pr/view/change-requests/job/PR-8657/1/consoleFull

`
00:31:51  [Error] /home/jenkins/jenkins-agent/workspace/Kafka_kafka-pr_PR-8657/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala:136: value stringSerde is not a member of object org.apache.kafka.streams.scala.Serdes
`"
685833800,8657,chia7712,2020-09-02T16:01:44Z,"> 00:31:51 [Error] /home/jenkins/jenkins-agent/workspace/Kafka_kafka-pr_PR-8657/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala:136: value stringSerde is not a member of object org.apache.kafka.streams.scala.Serdes

this error is caused by #8955 and #9245 is going to fix it"
685837334,8657,chia7712,2020-09-02T16:07:31Z,```kafka.network.DynamicConnectionQuotaTest.testDynamicListenerConnectionCreationRateQuota``` pass on my local. retest this please
685838009,8657,chia7712,2020-09-02T16:08:42Z,Retest this please
688481474,8657,chia7712,2020-09-07T19:19:30Z,"@junrao Thanks for all reviews again  

> Do you plan to remove some of the unused methods in DelayedOperations in Partition?

my bad. I forgot this request :(

Expect for ```checkAndCompleteFetch```, the other unused methods (in production scope) are removed by this PR.

> Currently, when calling checkAndComplete() for the produce/fetch/deleteRecords purgatory, we still hold replicaStateChangeLock. This doesn't seem to cause any deadlock for now. In the future, we can potentially improve this by calling checkAndComplete() outside of the replicaStateChangeLock by passing leader epoch into those delayed operations and checking if leader epoch has changed in tryComplete().

It seems we can remove ```delayedOperations``` from```Partition```. That is similar to this PR and ```Partition``` SHOULD NOT complete delayed request anymore. I can take over this in separate PR :)"
689241869,8657,junrao,2020-09-09T01:29:16Z,@ijuma @hachikuji @rajinisivaram : I think this PR is ready to be merged. Any further comments from you?
689256048,8657,chia7712,2020-09-09T02:17:30Z,"```
Build / JDK 15 / kafka.network.ConnectionQuotasTest.testNoConnectionLimitsByDefault
Build / JDK 11 / kafka.network.DynamicConnectionQuotaTest.testDynamicListenerConnectionCreationRateQuota
Build / JDK 11 / org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta[true]
```
```
Module: kafkatest.tests.connect.connect_distributed_test
Class:  ConnectDistributedTest
Method: test_bounce
Arguments:
{
  ""clean"": true,
  ""connect_protocol"": ""sessioned""
}
```

On my local, they are flaky on trunk branch."
689838971,8657,junrao,2020-09-09T21:46:20Z,@chia7712 : Thanks a lot for staying on this tricky issue and finding a simpler solution!
689921735,8657,chia7712,2020-09-10T01:46:16Z,"> Thanks a lot for staying on this tricky issue and finding a simpler solution!

thanks for all suggestions. I benefit a lot from it."
267497331,2264,asfbot,2016-12-16T01:53:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/183/
Test FAILed (JDK 8 and Scala 2.11).
"
267497361,2264,asfbot,2016-12-16T01:53:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/181/
Test FAILed (JDK 7 and Scala 2.10).
"
267497429,2264,asfbot,2016-12-16T01:54:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/182/
Test FAILed (JDK 8 and Scala 2.12).
"
267664594,2264,asfbot,2016-12-16T18:36:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/196/
Test FAILed (JDK 8 and Scala 2.11).
"
267664621,2264,asfbot,2016-12-16T18:36:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/195/
Test FAILed (JDK 8 and Scala 2.12).
"
267664696,2264,asfbot,2016-12-16T18:37:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/194/
Test FAILed (JDK 7 and Scala 2.10).
"
267706951,2264,asfbot,2016-12-16T22:03:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/199/
Test FAILed (JDK 8 and Scala 2.11).
"
267707034,2264,asfbot,2016-12-16T22:04:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/197/
Test FAILed (JDK 7 and Scala 2.10).
"
267707070,2264,asfbot,2016-12-16T22:04:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/198/
Test FAILed (JDK 8 and Scala 2.12).
"
267731925,2264,cmccabe,2016-12-17T01:06:34Z,"Previously, there were many places in the code that were creating AbstractRequest objects directly and passing them to the NetworkClient. Unfortunately, AbstractRequest objects are version-specific.  For example, a ListOffsetRequest object of version 0 will have different fields inside its Struct than a ListOffsetRequest object of version 1.  As soon as the message constructor is invoked, the contained Structs are formatted in the version-specific format.

This change adds the AbstractRequest#Builder types.  Client code in the Fetcher and Producer, and other places, can create these objects and pass them to the NetworkClient.  The NetworkClient can then decide what version of each request  to use based on the ApiVersionsRequest data.  When the version that needs to be used is too old to support a necessary feature, an error is returned to the     upper layer, which can then decide what to do."
270150362,2264,ijuma,2017-01-03T16:08:53Z,"@cmccabe, can you please rebase this branch? Thanks!"
270250551,2264,cmccabe,2017-01-03T23:01:26Z,Rebased on trunk and fixed some unit tests
270256325,2264,asfbot,2017-01-03T23:35:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/458/
Test FAILed (JDK 8 and Scala 2.11).
"
270256676,2264,asfbot,2017-01-03T23:38:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/456/
Test FAILed (JDK 7 and Scala 2.10).
"
270266703,2264,asfbot,2017-01-04T00:42:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/457/
Test FAILed (JDK 8 and Scala 2.12).
"
270380394,2264,ijuma,2017-01-04T14:18:19Z,"Sorry, can you please rebase again?"
270446437,2264,cmccabe,2017-01-04T18:26:06Z,Rebased
270447481,2264,cmccabe,2017-01-04T18:30:09Z,"Also removed a few places where this change duplicated KAFKA-4548, and reverted a change to the test log4j.properties file"
270457220,2264,asfbot,2017-01-04T19:05:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/493/
Test FAILed (JDK 8 and Scala 2.11).
"
270457261,2264,asfbot,2017-01-04T19:05:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/492/
Test FAILed (JDK 8 and Scala 2.12).
"
270458416,2264,asfbot,2017-01-04T19:10:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/491/
Test FAILed (JDK 7 and Scala 2.10).
"
270510560,2264,cmccabe,2017-01-04T22:51:34Z,"Fix AuthorizerIntegrationTest, FetchRequestTest, SaslApiVersionsRequestTest, PlaintextConsumerTest.

Fixed a bug where Fetcher#beginningOffsets and Fetcher#endOffsets incorrectly required a broker with the new ListOffsetsRequest RPC.  Will add this to CompatibilityTest."
270518816,2264,asfbot,2017-01-04T23:34:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/501/
Test FAILed (JDK 8 and Scala 2.12).
"
270526384,2264,asfbot,2017-01-05T00:20:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/500/
Test FAILed (JDK 7 and Scala 2.10).
"
270530276,2264,asfbot,2017-01-05T00:46:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/502/
Test FAILed (JDK 8 and Scala 2.11).
"
270534073,2264,asfbot,2017-01-05T01:06:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/506/
Test PASSed (JDK 8 and Scala 2.11).
"
270546749,2264,asfbot,2017-01-05T02:21:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/504/
Test PASSed (JDK 7 and Scala 2.10).
"
270554759,2264,asfbot,2017-01-05T03:25:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/505/
Test FAILed (JDK 8 and Scala 2.12).
"
270775426,2264,asfbot,2017-01-05T22:24:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/545/
Test PASSed (JDK 8 and Scala 2.12).
"
270775461,2264,asfbot,2017-01-05T22:24:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/546/
Test PASSed (JDK 8 and Scala 2.11).
"
270777714,2264,asfbot,2017-01-05T22:34:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/544/
Test PASSed (JDK 7 and Scala 2.10).
"
271011285,2264,asfbot,2017-01-06T21:24:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/582/
Test PASSed (JDK 8 and Scala 2.12).
"
271011796,2264,asfbot,2017-01-06T21:26:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/583/
Test PASSed (JDK 8 and Scala 2.11).
"
271030272,2264,asfbot,2017-01-06T22:41:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/581/
Test PASSed (JDK 7 and Scala 2.10).
"
271048383,2264,asfbot,2017-01-07T00:49:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/595/
Test FAILed (JDK 8 and Scala 2.12).
"
271048409,2264,asfbot,2017-01-07T00:49:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/594/
Test FAILed (JDK 7 and Scala 2.10).
"
271048422,2264,asfbot,2017-01-07T00:49:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/596/
Test FAILed (JDK 8 and Scala 2.11).
"
271129057,2264,asfbot,2017-01-08T04:28:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/604/
Test PASSed (JDK 8 and Scala 2.12).
"
271129329,2264,asfbot,2017-01-08T04:36:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/603/
Test PASSed (JDK 7 and Scala 2.10).
"
271131576,2264,asfbot,2017-01-08T05:43:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/605/
Test PASSed (JDK 8 and Scala 2.11).
"
271399594,2264,asfbot,2017-01-09T20:34:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/644/
Test FAILed (JDK 8 and Scala 2.12).
"
271399616,2264,asfbot,2017-01-09T20:34:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/645/
Test FAILed (JDK 8 and Scala 2.11).
"
271399709,2264,asfbot,2017-01-09T20:34:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/643/
Test FAILed (JDK 7 and Scala 2.10).
"
271399925,2264,cmccabe,2017-01-09T20:35:42Z,"Good point about KIP-74 compatibility handling.  it sounds like we will have to reproduce the pre-KIP-74 behavior of ""getting stuck"" when messages are too big rather than the new behavior of returning at least one message.  I'll add back in that code shortly and work on a compatibility test for it.  The other comments should be addressed now in the latest patch."
271410230,2264,asfbot,2017-01-09T21:16:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/646/
Test PASSed (JDK 8 and Scala 2.11).
"
271410279,2264,asfbot,2017-01-09T21:17:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/645/
Test PASSed (JDK 8 and Scala 2.12).
"
271430360,2264,asfbot,2017-01-09T22:37:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/644/
Test PASSed (JDK 7 and Scala 2.10).
"
271673998,2264,asfbot,2017-01-10T19:32:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/687/
Test FAILed (JDK 8 and Scala 2.12).
"
271674080,2264,asfbot,2017-01-10T19:32:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/689/
Test FAILed (JDK 8 and Scala 2.11).
"
271674087,2264,asfbot,2017-01-10T19:32:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/687/
Test FAILed (JDK 7 and Scala 2.10).
"
271674875,2264,cmccabe,2017-01-10T19:35:27Z,"I posted an update with a lot of fixes.  I still need to address:
* Brokers should not send ApiVersionsRequest
* Implement pre-KIP-74 message-too-big behavior
* More centralized handling of versionMismatch exception
* some other stuff"
271675937,2264,asfbot,2017-01-10T19:39:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/691/
Test FAILed (JDK 8 and Scala 2.11).
"
271675962,2264,asfbot,2017-01-10T19:39:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/689/
Test FAILed (JDK 8 and Scala 2.12).
"
271676111,2264,asfbot,2017-01-10T19:39:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/689/
Test FAILed (JDK 7 and Scala 2.10).
"
271680561,2264,hachikuji,2017-01-10T19:56:30Z,"@ijuma I think your builder suggestion seems worth investigating.. If we continue with the model of delayed version determination, I was considering whether we could decouple the `Struct` representation from the request objects. Haven't really thought this through, but I was thinking something like this:

```java
public abstract class AbstractRequest {
  abstract Struct toStruct(short version);
}
``` 

That way the request objects remain immutable. I think you also had a suggestion which allowed us to construct the right version up-front."
271720119,2264,cmccabe,2017-01-10T22:35:31Z,"* Brokers no longer send ApiVersionsRequest, to allow inter-broker communication with versions before 0.10
* Addressed some style comments"
271720565,2264,asfbot,2017-01-10T22:37:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/702/
Test FAILed (JDK 8 and Scala 2.12).
"
271720662,2264,asfbot,2017-01-10T22:37:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/704/
Test FAILed (JDK 8 and Scala 2.11).
"
271720861,2264,asfbot,2017-01-10T22:38:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/702/
Test FAILed (JDK 7 and Scala 2.10).
"
271724878,2264,cmccabe,2017-01-10T22:56:48Z,"@hachikuji, @ijuma : unfortunately github doesn't let me post comments on the thread you have going about refactoring AbstractRequest.  So I'll comment here.

There are really two questions here:
1. Should the AbstractRequest objects represent serialized data or unserialized data?
2. Can we know the version of the request that we need before we send it to NetworkClient

For #1, currently, AbstractRequest objects represent serialized data, so they inherently have a version and a binary format.  A lot of the methods in AbstractRequest focus on serialization.. for example, AbstractRequest#getRequest deserializes, AbstractRequestResponse#sizeOf gets the size of the serialized data, etc.  You have to change the whole class hierarchy to modify this assumption.  Probably if you do that you would want to use a different name, such as Message, to represent the fact that the new class hierarchy you are creating has nothing to do with serialized data.  This might be worth doing.  It would be a very big change.

For #2, there are methods in NetworkClient such as leastLoadedNode that send the request to a node of the NetworkClient's own choosing.  This doesn't really work unless the NetworkClient is in charge of picking a version.  There also seems to be a bit of a layering violation here if the users of NC need to start concerning themselves with questions like has the peer they're talking to restarted or dropped its TCP session since we last learned about its version?

#1 might make sense as part of a general refactor of the RPC class hierarchies.  Getting rid of the Request class altogether might be a good idea since it invites confusion with ClientRequest.  I'm not sure I see a big win from #2 or a way to do it (but perhaps I am missing something)."
271737514,2264,asfbot,2017-01-11T00:03:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/708/
Test FAILed (JDK 8 and Scala 2.12).
"
271737617,2264,asfbot,2017-01-11T00:04:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/708/
Test FAILed (JDK 7 and Scala 2.10).
"
271738011,2264,asfbot,2017-01-11T00:06:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/710/
Test FAILed (JDK 8 and Scala 2.11).
"
271747190,2264,asfbot,2017-01-11T01:01:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/709/
Test FAILed (JDK 8 and Scala 2.12).
"
271748221,2264,asfbot,2017-01-11T01:08:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/709/
Test FAILed (JDK 7 and Scala 2.10).
"
271756952,2264,asfbot,2017-01-11T02:02:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/711/
Test FAILed (JDK 8 and Scala 2.11).
"
271759218,2264,asfbot,2017-01-11T02:18:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/714/
Test FAILed (JDK 8 and Scala 2.12).
"
271759289,2264,asfbot,2017-01-11T02:19:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/716/
Test PASSed (JDK 8 and Scala 2.11).
"
271764967,2264,asfbot,2017-01-11T03:00:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/714/
Test FAILed (JDK 7 and Scala 2.10).
"
271765728,2264,ijuma,2017-01-11T03:06:31Z,I submitted a PR with some clean-ups: https://github.com/cmccabe/kafka/pull/1
271775275,2264,cmccabe,2017-01-11T04:30:18Z,@ijuma: Thanks for the cleanup pull request.  Looks good... I merged it into this branch.
271779259,2264,asfbot,2017-01-11T05:06:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/718/
Test PASSed (JDK 8 and Scala 2.12).
"
271779325,2264,asfbot,2017-01-11T05:07:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/720/
Test PASSed (JDK 8 and Scala 2.11).
"
271789607,2264,asfbot,2017-01-11T06:30:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/718/
Test PASSed (JDK 7 and Scala 2.10).
"
271867738,2264,ijuma,2017-01-11T13:26:33Z,"There are a number of failures in the system tests run. I started another one to double-check that it's not an environmental issue. In the meantime, I submitted another PR with clean-ups:

https://github.com/cmccabe/kafka/pull/2

I think we can merge after that (assuming the system tests pass) and do additional work in follow-up PRs."
271931367,2264,asfbot,2017-01-11T17:18:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/742/
Test FAILed (JDK 8 and Scala 2.11).
"
271931438,2264,asfbot,2017-01-11T17:18:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/740/
Test FAILed (JDK 8 and Scala 2.12).
"
271931603,2264,asfbot,2017-01-11T17:19:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/740/
Test FAILed (JDK 7 and Scala 2.10).
"
271966420,2264,asfbot,2017-01-11T19:18:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/751/
Test PASSed (JDK 8 and Scala 2.11).
"
271966550,2264,asfbot,2017-01-11T19:18:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/749/
Test PASSed (JDK 8 and Scala 2.12).
"
272002264,2264,asfbot,2017-01-11T21:36:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/749/
Test FAILed (JDK 7 and Scala 2.10).
"
1728555687,14182,rreddy-22,2023-09-20T23:34:54Z,Created new PR https://github.com/apache/kafka/pull/14416/files
173692160,764,apovzner,2016-01-21T20:02:06Z,"@becketqin would you add the remaining KIP-31 and KIP-32 work to this patch (client side work and timestamp in produce response)? Or that would be a different patch?
"
173710219,764,becketqin,2016-01-21T21:16:32Z,"@apovzner This patch contains all the features in KIP-31 and KIP-32. The rest of the work is probably adding integration test. I have already added some unit tests but we can also add more if needed.
"
173749672,764,apovzner,2016-01-21T23:31:50Z,"@becketqin My question about remaining KIP-31 and KIP-32 work was based on outdated info -- I did not refresh my window and did not see that you added client-side implementation + returning timestamp in produce response. I see now that you also updated the PR description, thanks!
"
174748282,764,guozhangwang,2016-01-26T00:39:19Z,"@becketqin Here are some high-level thoughts about the protocol:
- Basically we want the consumer to return the timestamp of the type specified by that topic even for compressed message set, but without the additional information the consumer would not know if LogAppendTime or LogCreationTime is used. And as @apovzner mentioned by just setting the wrapper message as the max value of all inner message timestamps and letting consumer check if wrapper timestamp is the max value does not perfectly work since 1) it requires the consumers to always decompress the whole message before returning any to the user, hence restricting buffer memory management we wanted to add in the future, 2) there is a corner case that if LogAppendTime is used and broker overrides the wrapper timestamp, it happens to be the same as the max of inner timestamps.

I think we can add this information into the attribute field of the message, which currently only used 2 bits for four different compression types; instead we can make it a mask manner where the first 3 (or if we want to be safer, use 4) bits are preserved for indicating compression codec, leaves us a total of 8 (or 16) supported compression types, and use the forth (fifth) bit for indicating if the wrapper timestamp (for LogAppendTime, hence it is overridden) or the inner timestamp (for LogCreationTime) should be used to set the consumer record's timestamp.

And with this neither producer nor consumer needs to learn about this per-topic config from metadata responses, which makes the client change simpler, and other languages' adoption easier.
- I am curious if the ducktape integration tests will be added in another PR?
"
174759962,764,becketqin,2016-01-26T01:32:12Z,"@guozhangwang Using attribute field is a good approach. It also lets consumers know the timestamp type. To make sure I understand your suggestion correctly:
1. The producer simply send message assuming the broker is using CreateTime. i.e. both attributes and timestamp will be using CreateTime.
2. If log append time is used, the broker only overrides the outer message's Attribute field and Timestmap field to use LogAppendTime
3. When consumer sees the message, it checks both magic as well as attribute field to see which timestamp is used (if magic > 0), and then decide whether it will override the inner message's timestamp or not.

Another thing is that we still need to decompress the entire compressed message, because of the reason I mentioned in one of the comments. Given the stream compression used by producer, we will not have a actual ""relative offset compared with last message"" until we close the batch. Instead, we only have the ""relative offset compared with the first message"" when we write a message into a batch. Because the outer message only has the absolute offset of the last message, in order to have the absolute offset of an inner message, we have to decompress the entire compressed message to find out the ""relative offset compared with the last message"", then compute the absolute offset.

I feel this is fine for new consumer because we are delivering messages in batch to use anyways.
"
174762898,764,becketqin,2016-01-26T01:41:50Z,"BTW, currently the CompressionCodecMask is set to 0x7, so it is 3 bits. Changing that to 4 bits is backward compatible so that should be fine.
"
174773567,764,guozhangwang,2016-01-26T01:59:22Z,"@becketqin yes your understanding is correct.

I was initially thinking about possibilities of NOT decompressing the whole message when we add the memory management feature in the future so that we can choose to buffer less decompressed messages. But it seems not possible now, which maybe still fine for us so let's forget about it.
"
175792263,764,becketqin,2016-01-27T18:55:42Z,"@apovzner @guozhangwang I updated the patch with Guozhang's proposal. I will add integration test in a separate PR. 
The intended tests are:
1. Change timestamp type on the fly.
2. Test message format version upgrade
I actually want to do end to end test using different version of producers and consumers. But not sure if it is possible the current integration test because that requires different clients jars.
"
175972676,764,apovzner,2016-01-28T04:47:00Z,"@becketqin You can do end-to-end compatibility testing with system tests. Take a look at compatibility_test.py. It currently tests 0.9.X java producer against 0.8.X brokers and 0.8.X consumer against an 0.9.X brokers. They both succeed on expected failure. You can add couple of more system tests to that to test newer brokers with older producers and/or consumers. Note that you would need to update vagrant/base.sh to get Kafka release 0.9.0.0.
"
175982253,764,apovzner,2016-01-28T05:08:59Z,"@becketqin Maybe I missed it, but I don't see where producer assigns timestamps if the user does not specify the timestamp in ProducerRecord. The code was there before, but maybe it got accidentally removed with recent changes?
"
175992639,764,becketqin,2016-01-28T05:49:37Z,"@apovzner Thanks for the direction on compatibility test. Extracting timestamp type out makes sense, given we already did that for CompressionType. I will change server side as well.
"
176356903,764,apovzner,2016-01-28T19:27:13Z,"@becketqin  I reviewed the KIP-32 part of the patch (did not go in detail about KIP-31 related changes). Using timestamp type in attributes made producer/consumer code much cleaner! I made minor comments. Otherwise looks good to me. 
"
176481781,764,becketqin,2016-01-28T23:37:32Z,"@apovzner Thanks for the review. @guozhangwang @junrao Will you help take a look at the patch? Thanks.
"
176513163,764,becketqin,2016-01-29T01:39:13Z,"The test failure is intermittent and is not related to this change.
"
176885686,764,becketqin,2016-01-29T17:52:43Z,"@junrao. Thanks for the review. I addressed your previous comments. Would you take another look? Thanks.
"
178268615,764,apovzner,2016-02-02T00:10:48Z,"@becketqin Correct me if I am wrong, but it looks like we don't have integration test for the case when the broker rejects messages because the timestamp is outside of configured time difference.
I think it would be easier to track the additional tests we need to do (beyond this PR) if we created separate KAFKA JIRAs for them. Could you please create JIRAs?
"
178321814,764,becketqin,2016-02-02T02:32:25Z,"@apovzner Good catch. I am going to add the rejected message unit integration test. This actually exposed a separate bug on the server side that causing it not returning the correct error code. I create KAFKA-3189 for that bug. Will update unit integration test after that bug is fixed.
"
178911759,764,becketqin,2016-02-03T00:31:04Z,"Thanks a lot for the review @junrao .I updated the patch to address your comments.
Some explanation on the changes:
1. The broker now return InvalidMessageException when producer does not set the timestamp type to CreateTime. Previously it just overrides it.
2. It seems difficult to completely avoid unnecessary message magic value check when send fetch response back. I chose the way which I think has the least impact. But it might still break if people roll back message.format.version.
3. I haven't add the configuration sanity check for message.format.version yet. I will do that after we decide on whether it should be a topic level configuration and whether we want to use magic value or ApiVersions.
"
178974585,764,junrao,2016-02-03T03:00:56Z,"@becketqin : Thanks for the patch. Do you plan add some integration tests to cover backward compatibility? We can do that in a separate jira.
"
179291438,764,junrao,2016-02-03T15:24:29Z,"Also, could you update the doc on upgrade?
"
180561754,764,becketqin,2016-02-05T21:27:35Z,"@junrao Thanks for the review. Your comments are addressed. 
I have created separate tickets for integration test. I am working on the upgrade doc now and will update it later.
"
181683922,764,becketqin,2016-02-09T03:02:02Z,"@ijuma I will start a poll on the mailing list as Jun suggested.

@junrao I updated the patch with upgrade doc. Would you take a look? Thanks.
The test failure is irrelevant. I will open a ticket for that as it fails from time to time.
"
183538336,764,junrao,2016-02-13T00:10:08Z,"Hi, Jiangjie,

Thanks for addressing the review comments. Do you expect to submit a new patch soon? Thanks,
"
183561045,764,becketqin,2016-02-13T02:16:23Z,"Hi @junrao Thanks a lot for the review. I just submitted a new patch. Thanks!
"
185176868,764,ijuma,2016-02-17T12:24:32Z,"@becketqin, thanks for the PR and for the quick response in addressing review comments. I filed a PR with your branch as the target with some additional minor improvements:

https://github.com/becketqin/kafka/pull/1

Please integrate into your PR (merge, cherry-pick or any other way you prefer) if you agree with the proposed changes.

I expect to finish reviewing tomorrow. If Jun merges in the meantime, we can use follow-up PRs (if needed).
"
185363481,764,becketqin,2016-02-17T19:25:24Z,"@ijuma Thanks for the review. I just merged your PR. 
"
185510642,764,junrao,2016-02-18T02:16:08Z,"Thanks for the patch. Looks good overall. Just left a few minor comments. Also, in TopicCommand, when listing the available config options, could we add a description that messageFormat will be ignored if it's not consistent with the inter broker protocol setting?
"
185992097,764,becketqin,2016-02-19T00:27:41Z,"@junrao Thanks for the patient review. I think I have addressed previous comments. Could you take another look?
"
186009747,764,junrao,2016-02-19T01:54:55Z,"@becketqin : Thanks for the latest patch. It looks good to me. Once you address the last few minor comments, I can merge this in.
"
186274027,764,junrao,2016-02-19T15:53:53Z,"@becketqin : Thanks a lot for working on the patch! LGTM
"
186277555,764,ijuma,2016-02-19T16:04:01Z,"Nice work @becketqin. And the reviewers too. :)
"
186463160,764,becketqin,2016-02-20T00:15:15Z,"Thank @junrao and @ijuma so much for the great help on review!
"
189037289,764,guozhangwang,2016-02-25T23:44:22Z,"@junrao @becketqin Some of the streams tests were incorrect when adding the timestamp. For example in ProcessorStateManagerTest: 

`new ConsumerRecord<>(persistentStoreTopicName, 2, 0L, offset, TimestampType.CREATE_TIME, key, 0)` should be

`new ConsumerRecord<>(persistentStoreTopicName, 2, offset, 0L, TimestampType.CREATE_TIME, key, 0)`

Actually I'm thinking if it harms to keep the old constructor for ConsumerRecord and make default values of 0L and TimestampType.CREATE_TIME, and revert all the changes in stream tests? That way we can be free of incorporating the metadata timestamp until it is supported.
"
189040692,764,ijuma,2016-02-25T23:56:32Z,"@guozhangwang Well-spotted. I actually wanted to suggest moving `TimestampType` before the timestamp to make this kind of error harder, but I only noticed this potential problem late in the process and then wasn't sure if it was worth the effort. Having real bugs instead of theoretical ones adds motivation.

I would prefer if we don't add the old `ConsumerRecord` constructor personally as `ConsumerRecord` is used outside of streams too. Maybe we could add a utility method in streams in the meantime?
"
189047528,764,guozhangwang,2016-02-26T00:13:40Z,"@ijuma Makes sense. The only place streams use `ConsumerRecord` directly is in `TimestampExtractor`, what kind of utility method do you have in mind?
"
189048926,764,ijuma,2016-02-26T00:19:09Z,"@guozhangwang I just mean a method like `newConsumerRecord` that behaves exactly like the old constructor. Then you could revert the changes in the streams tests and then do a search and replace in the streams folder.
"
189049558,764,guozhangwang,2016-02-26T00:22:43Z,"@ijuma Sounds good.
"
1625488949,13870,dajac,2023-07-07T14:21:16Z,There are issues with the build as well. Could you look into this?
1625762579,13870,jeffkbkim,2023-07-07T17:57:38Z,"@dajac @CalvinConfluent 

thanks for the review, i have addressed your comments."
1631769383,13870,jeffkbkim,2023-07-12T02:46:42Z,@dajac thanks for the review. I have addressed your comments.
1632143234,13870,dajac,2023-07-12T09:13:11Z,"@jeffkbkim Now that https://github.com/apache/kafka/pull/13963 is merged, could you update your PR?"
1633127990,13870,jeffkbkim,2023-07-12T19:58:50Z,@dajac I have updated with latest and unified the MockCoordinatorTimer.
1641304041,13870,jeffkbkim,2023-07-19T02:47:59Z,the test failures are unrelated
767804738,9944,jolshan,2021-01-26T20:23:03Z,"Added fetch session components. Will add some versioning tests and final cleanups, then open for review
"
767987726,9944,jolshan,2021-01-27T03:16:34Z,I'm aware that the latest changes to ensure the correct fetch version is sent seem to be causing more test timeouts. Will need to investigate further and hopefully decrease the time needed for fetch requests. I may need to check some other flaky tests related to my changes.
776097476,9944,jolshan,2021-02-09T17:14:49Z,"> 
> Also, it seems that for more partitions, the performance with this PR is noticeably worse than trunk?
> 
> ```
> FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20           500  avgt   15    671785.374    2631.210  ns/op
> FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20          1000  avgt   15   1235326.349    6101.974  ns/op
> 
> trunk
> FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20           500  avgt   15   1407744.600   10321.934  ns/op
> FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20          1000  avgt   15   2901845.093   59557.128  ns/op
> ```

Just to clarify this, the top is the Fetch branch, so I think it is better than trunk
I do want to take another look at the FetcherThread and FetchSession benchmarks which are slightly worse."
790133100,9944,jolshan,2021-03-03T22:59:01Z,waiting on https://github.com/apache/kafka/pull/9758 before proceeding since this PR touches a lot of the same files.
794755556,9944,jolshan,2021-03-10T02:24:37Z,"Currently working on merge conflicts. Should have a first pass out in the next day or so. There are a few changes that don't work with the previous refactor. @chia7712, can you take a look when I have the commit ready?"
794897185,9944,chia7712,2021-03-10T05:17:42Z,"> can you take a look when I have the commit ready?

sure :)"
796317046,9944,jolshan,2021-03-11T00:23:38Z,"@chia7712 here's the commit. As mentioned in the commit message:
> Left a few todos. Still need to clean up, optimize code, and fix flaky and broken tests in FetchRequestBetweenDifferentIbpTest

I also acknowledge that this code creates a ton of data structures in FetchRequest and FetchResponse. I hope to clean those up soon."
796886007,9944,jolshan,2021-03-11T16:56:59Z,"@chia7712 There was a lot of back and forth about whether we should simply include the topic ID and the name in the protocol (to uniquely identify the topic--which is something we may want to do to make sure we are consuming the right topic) versus use the topic ID to replace the topic name. The main arguments were:
1. Topic IDs are in most cases shorter than topic names so we can shorten an already somewhat large protocol
2. We will eventually want to move over to using topic IDs for everything so we might as well make this change now. 

I do agree that the TopicPartitions used make this task much harder and I tried to find a clean way to accomplish this. The main idea is that we keep track of topics that did not have topic ids server-side (receiving-side) and send an error response back. The idea is that the sender will always have the topic ID -> name conversion for topic IDs it sends. "
796902838,9944,chia7712,2021-03-11T17:19:57Z,"> The idea is that the sender will always have the topic ID -> name conversion for topic IDs it sends.

Do you mean the client (consumer) must have a mapping (name -> ID) in local cache before fetching data?"
796906853,9944,jolshan,2021-03-11T17:26:03Z,"@chia7712 yes. The consumer already gets periodic metadata updates, so I use that to get the mapping."
798608479,9944,jolshan,2021-03-13T16:41:18Z,"> @jolshan Have you run both performance test and compatibility test?

I've run jmh benchmarks before but not since the merge. I plan to do so again once I figure out how to improve the efficiency regarding unconvertedFetchResponse and fix up some the one test that is failing. I also plan to rerun system tests to make sure those are passing as well."
804157913,9944,jolshan,2021-03-22T15:35:26Z,"New benchmark results: These are pretty similar to the previous results, but thought I should include to compare to updated code (both this branch and trunk). Note: I have modified `FetchResponseBenchmark.testConstructFetchResponse` on trunk to match the benchmark in this branch (so it doesn't count the time to build the map) and added `FetchResponseBenchmark.testPartitionMapFromData` to the trunk code to capture that benchmark.


```
Fetch
Benchmark                                                   (partitionCount)  (topicCount)  Mode  Cnt         Score        Error  Units
FetchRequestBenchmark.testFetchRequestForConsumer                          3            10  avgt   15      2513.943     415.731  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                          3           500  avgt   15    152716.705    1169.791  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                          3          1000  avgt   15    314334.583    1230.334  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         10            10  avgt   15      7483.647     109.257  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         10           500  avgt   15    539688.305    2178.270  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         10          1000  avgt   15   1106936.678    4969.437  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         20            10  avgt   15     15174.929      67.138  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         20           500  avgt   15   1061724.363    6329.465  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         20          1000  avgt   15   2190539.013    8329.123  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                           3            10  avgt   15      2191.833      16.787  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                           3           500  avgt   15    148223.696     575.298  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                           3          1000  avgt   15    317739.517    1580.710  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          10            10  avgt   15      7479.487      46.636  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          10           500  avgt   15    534501.931    5759.415  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          10          1000  avgt   15   1082065.823    5310.751  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          20            10  avgt   15     15014.392      66.179  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          20           500  avgt   15   1079151.542    6971.914  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          20          1000  avgt   15   2204921.180   18698.376  ns/op
FetchRequestBenchmark.testRequestToJson                                    3            10  avgt   15     15060.391      76.234  ns/op
FetchRequestBenchmark.testRequestToJson                                    3           500  avgt   15   1037511.828   10186.816  ns/op
FetchRequestBenchmark.testRequestToJson                                    3          1000  avgt   15   2428605.795   46757.277  ns/op
FetchRequestBenchmark.testRequestToJson                                   10            10  avgt   15     51935.569     520.989  ns/op
FetchRequestBenchmark.testRequestToJson                                   10           500  avgt   15   4133608.072   84115.319  ns/op
FetchRequestBenchmark.testRequestToJson                                   10          1000  avgt   15   8401540.617  171000.272  ns/op
FetchRequestBenchmark.testRequestToJson                                   20            10  avgt   15    110340.442    5405.039  ns/op
FetchRequestBenchmark.testRequestToJson                                   20           500  avgt   15   8488368.643   92951.458  ns/op
FetchRequestBenchmark.testRequestToJson                                   20          1000  avgt   15  17055209.055  332859.772  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                 3            10  avgt   15      1837.114     170.773  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                 3           500  avgt   15     89634.386     506.178  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                 3          1000  avgt   15    184862.727    1543.592  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                10            10  avgt   15      5115.297      70.363  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                10           500  avgt   15    298023.785    1871.931  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                10          1000  avgt   15    659030.320    3412.489  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                20            10  avgt   15      9698.646      46.020  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                20           500  avgt   15    657611.389    5235.813  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                20          1000  avgt   15   1339394.227    9291.111  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                  3            10  avgt   15      1640.041       6.537  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                  3           500  avgt   15     93345.364     343.904  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                  3          1000  avgt   15    183216.037     945.579  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 10            10  avgt   15      5066.202      47.965  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 10           500  avgt   15    303592.758    2061.786  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 10          1000  avgt   15    662593.894    2737.712  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20            10  avgt   15      9680.561      52.885  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20           500  avgt   15    656429.749    4207.257  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20          1000  avgt   15   1341316.420    8622.927  ns/op

Benchmark                                          (partitionCount)  (topicCount)  Mode  Cnt        Score       Error  Units
FetchResponseBenchmark.testConstructFetchResponse                 3            10  avgt   15      429.689     49.203  ns/op
FetchResponseBenchmark.testConstructFetchResponse                 3           500  avgt   15    27191.314    969.304  ns/op
FetchResponseBenchmark.testConstructFetchResponse                 3          1000  avgt   15    54609.939    169.692  ns/op
FetchResponseBenchmark.testConstructFetchResponse                10            10  avgt   15      996.322     14.198  ns/op
FetchResponseBenchmark.testConstructFetchResponse                10           500  avgt   15    57757.996    366.309  ns/op
FetchResponseBenchmark.testConstructFetchResponse                10          1000  avgt   15   130669.716   6234.372  ns/op
FetchResponseBenchmark.testConstructFetchResponse                20            10  avgt   15     2038.619     20.194  ns/op
FetchResponseBenchmark.testConstructFetchResponse                20           500  avgt   15   124396.366    805.140  ns/op
FetchResponseBenchmark.testConstructFetchResponse                20          1000  avgt   15   261012.164    733.139  ns/op
FetchResponseBenchmark.testPartitionMapFromData                   3            10  avgt   15      560.513      2.225  ns/op
FetchResponseBenchmark.testPartitionMapFromData                   3           500  avgt   15    52098.726    406.047  ns/op
FetchResponseBenchmark.testPartitionMapFromData                   3          1000  avgt   15   128846.871   4509.988  ns/op
FetchResponseBenchmark.testPartitionMapFromData                  10            10  avgt   15     2037.149      5.601  ns/op
FetchResponseBenchmark.testPartitionMapFromData                  10           500  avgt   15   201449.659   2690.096  ns/op
FetchResponseBenchmark.testPartitionMapFromData                  10          1000  avgt   15   427601.877   5691.952  ns/op
FetchResponseBenchmark.testPartitionMapFromData                  20            10  avgt   15     4030.068     12.218  ns/op
FetchResponseBenchmark.testPartitionMapFromData                  20           500  avgt   15   387226.408   2523.183  ns/op
FetchResponseBenchmark.testPartitionMapFromData                  20          1000  avgt   15   770982.841  11992.521  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                 3            10  avgt   15     2955.411     14.808  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                 3           500  avgt   15   143436.083   1294.461  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                 3          1000  avgt   15   293561.954   2578.587  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                10            10  avgt   15     8933.056     38.957  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                10           500  avgt   15   496541.930  34518.643  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                10          1000  avgt   15   957331.163   7047.265  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                20            10  avgt   15    17760.601    105.999  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                20           500  avgt   15   937904.439  14690.636  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                20          1000  avgt   15  2065319.843  46528.926  ns/op

Benchmark                                           (partitionCount)  (presize)  (updatedPercentage)  Mode  Cnt      Score      Error  Units
FetchSessionBenchmark.incrementalFetchSessionBuild                10      false                    0  avgt   10    576.115     2.859  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10      false                   10  avgt   10    582.959    10.097  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10      false                  100  avgt   10    606.183     2.631  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10       true                    0  avgt   10    600.190     8.237  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10       true                   10  avgt   10    594.572     4.345  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10       true                  100  avgt   10    584.634     4.611  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100      false                    0  avgt   10   4656.369    19.004  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100      false                   10  avgt   10   4568.124    54.588  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100      false                  100  avgt   10   4530.730    27.031  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100       true                    0  avgt   10   4589.372    22.713  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100       true                   10  avgt   10   4752.522    21.197  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100       true                  100  avgt   10   4586.602    26.438  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000      false                    0  avgt   10  44526.038   265.997  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000      false                   10  avgt   10  47580.321  1031.519  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000      false                  100  avgt   10  44789.123   285.570  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000       true                    0  avgt   10  40801.054   182.021  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000       true                   10  avgt   10  44409.136   307.825  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000       true                  100  avgt   10  40502.234   175.220  ns/op

Benchmark                                  (partitionCount)  Mode  Cnt       Score      Error  Units
ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    9377.012    68.996  ns/op
ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15   62766.217  3180.239  ns/op
ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15  128464.083  8549.638  ns/op
ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  787717.622  7260.589  ns/op
```

```
Trunk
Benchmark                                                   (partitionCount)  (topicCount)  Mode  Cnt         Score        Error  Units
FetchRequestBenchmark.testFetchRequestForConsumer                          3            10  avgt   15      1936.125      45.532  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                          3           500  avgt   15    127336.689    8173.394  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                          3          1000  avgt   15    270903.955   16727.003  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         10            10  avgt   15      7060.618      19.812  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         10           500  avgt   15    450851.068    2833.910  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         10          1000  avgt   15    910310.719    9051.000  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         20            10  avgt   15     13825.501      38.096  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         20           500  avgt   15    900659.720    3961.264  ns/op
FetchRequestBenchmark.testFetchRequestForConsumer                         20          1000  avgt   15   1862614.126   10699.114  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                           3            10  avgt   15      1839.562      17.133  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                           3           500  avgt   15    122223.311    1906.955  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                           3          1000  avgt   15    248782.848    1009.325  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          10            10  avgt   15      6787.174      29.854  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          10           500  avgt   15    466199.553   39672.780  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          10          1000  avgt   15    907680.623   10247.477  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          20            10  avgt   15     13562.511      50.098  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          20           500  avgt   15    898779.141    6018.894  ns/op
FetchRequestBenchmark.testFetchRequestForReplica                          20          1000  avgt   15   1867214.540   13506.135  ns/op
FetchRequestBenchmark.testRequestToJson                                    3            10  avgt   15     14783.888      91.244  ns/op
FetchRequestBenchmark.testRequestToJson                                    3           500  avgt   15    948882.985   23252.097  ns/op
FetchRequestBenchmark.testRequestToJson                                    3          1000  avgt   15   2117792.795   24244.964  ns/op
FetchRequestBenchmark.testRequestToJson                                   10            10  avgt   15     47024.577     189.143  ns/op
FetchRequestBenchmark.testRequestToJson                                   10           500  avgt   15   3713719.090  163579.679  ns/op
FetchRequestBenchmark.testRequestToJson                                   10          1000  avgt   15   8002133.426   83660.082  ns/op
FetchRequestBenchmark.testRequestToJson                                   20            10  avgt   15     94757.317     557.963  ns/op
FetchRequestBenchmark.testRequestToJson                                   20           500  avgt   15   7800827.831  150606.560  ns/op
FetchRequestBenchmark.testRequestToJson                                   20          1000  avgt   15  15321074.716  398504.775  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                 3            10  avgt   15      3285.808      11.272  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                 3           500  avgt   15    210356.502    4789.004  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                 3          1000  avgt   15    440500.520   10693.950  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                10            10  avgt   15     10876.320      51.664  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                10           500  avgt   15    676672.498    3663.045  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                10          1000  avgt   15   1421419.755    8848.171  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                20            10  avgt   15     19990.447      99.136  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                20           500  avgt   15   1342374.799    7937.563  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForConsumer                20          1000  avgt   15   2791607.418   16509.206  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                  3            10  avgt   15      3003.903      14.183  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                  3           500  avgt   15    210524.005    2208.864  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                  3          1000  avgt   15    442131.780    3866.508  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 10            10  avgt   15     11764.283      90.278  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 10           500  avgt   15    685599.971    2927.185  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 10          1000  avgt   15   1504088.557  131029.444  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20            10  avgt   15     20774.527      51.876  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20           500  avgt   15   1371203.572    4469.235  ns/op
FetchRequestBenchmark.testSerializeFetchRequestForReplica                 20          1000  avgt   15   2809024.408   35531.292  ns/op

Benchmark                                          (partitionCount)  (topicCount)  Mode  Cnt        Score       Error  Units
FetchResponseBenchmark.testConstructFetchResponse                 3            10  avgt   15     356.305    12.221  ns/op
FetchResponseBenchmark.testConstructFetchResponse                 3           500  avgt   15   24022.377   886.332  ns/op
FetchResponseBenchmark.testConstructFetchResponse                 3          1000  avgt   15   40245.758  2128.028  ns/op
FetchResponseBenchmark.testConstructFetchResponse                10            10  avgt   15     790.751     8.224  ns/op
FetchResponseBenchmark.testConstructFetchResponse                10           500  avgt   15   49867.734   148.832  ns/op
FetchResponseBenchmark.testConstructFetchResponse                10          1000  avgt   15   98405.835  1057.373  ns/op
FetchResponseBenchmark.testConstructFetchResponse                20            10  avgt   15    1948.644   161.175  ns/op
FetchResponseBenchmark.testConstructFetchResponse                20           500  avgt   15  113545.392   780.223  ns/op
FetchResponseBenchmark.testConstructFetchResponse                20          1000  avgt   15  232817.481  2769.065  ns/op
FetchResponseBenchmark.testPartitionMapFromData                 3            10  avgt   15     710.229     38.289  ns/op
FetchResponseBenchmark.testPartitionMapFromData                 3           500  avgt   15   49871.098   2736.391  ns/op
FetchResponseBenchmark.testPartitionMapFromData                 3          1000  avgt   15  122454.268   4258.654  ns/op
FetchResponseBenchmark.testPartitionMapFromData                10            10  avgt   15    2169.361     52.146  ns/op
FetchResponseBenchmark.testPartitionMapFromData                10           500  avgt   15  207794.383   8358.626  ns/op
FetchResponseBenchmark.testPartitionMapFromData                10          1000  avgt   15  428577.924  25503.552  ns/op
FetchResponseBenchmark.testPartitionMapFromData                20            10  avgt   15    4181.377     14.551  ns/op
FetchResponseBenchmark.testPartitionMapFromData                20           500  avgt   15  385121.266   3287.874  ns/op
FetchResponseBenchmark.testPartitionMapFromData                20          1000  avgt   15  736875.677  14129.803  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                 3            10  avgt   15     3496.454     43.246  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                 3           500  avgt   15   190777.328   2319.712  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                 3          1000  avgt   15   382125.462   2050.872  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                10            10  avgt   15     9455.503    125.468  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                10           500  avgt   15   500987.000   4585.356  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                10          1000  avgt   15  1031388.308  12785.241  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                20            10  avgt   15    18419.122    144.977  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                20           500  avgt   15   970914.093  12905.127  ns/op
FetchResponseBenchmark.testSerializeFetchResponse                20          1000  avgt   15  2190984.813  36793.127  ns/op

Benchmark                                           (partitionCount)  (presize)  (updatedPercentage)  Mode  Cnt      Score      Error  Units
FetchSessionBenchmark.incrementalFetchSessionBuild                10      false                    0  avgt   10    371.769     6.228  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10      false                   10  avgt   10    373.283     9.136  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10      false                  100  avgt   10    380.163     9.308  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10       true                    0  avgt   10    351.078    27.354  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10       true                   10  avgt   10    337.239     1.760  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild                10       true                  100  avgt   10    338.392     2.119  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100      false                    0  avgt   10   3361.587    12.594  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100      false                   10  avgt   10   3343.437    17.323  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100      false                  100  avgt   10   3356.499    19.348  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100       true                    0  avgt   10   3231.819    17.032  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100       true                   10  avgt   10   3204.732    45.047  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild               100       true                  100  avgt   10   3219.814    17.733  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000      false                    0  avgt   10  38165.578   747.354  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000      false                   10  avgt   10  41128.785   429.810  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000      false                  100  avgt   10  38743.515   789.308  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000       true                    0  avgt   10  35758.991   694.407  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000       true                   10  avgt   10  35220.420   836.619  ns/op
FetchSessionBenchmark.incrementalFetchSessionBuild              1000       true                  100  avgt   10  32722.983  1424.316  ns/op

Benchmark                                  (partitionCount)  Mode  Cnt       Score      Error  Units
ReplicaFetcherThreadBenchmark.testFetcher               100  avgt   15    7436.534    54.461  ns/op
ReplicaFetcherThreadBenchmark.testFetcher               500  avgt   15   51704.338   716.880  ns/op
ReplicaFetcherThreadBenchmark.testFetcher              1000  avgt   15  111854.545  1755.612  ns/op
ReplicaFetcherThreadBenchmark.testFetcher              5000  avgt   15  787540.899  9549.430  ns/op
```
"
814408153,9944,jolshan,2021-04-06T20:11:09Z,"~currently blocked on https://github.com/apache/kafka/pull/10492 
(Need to add topic IDs to the metadata topic for fetching)~

No longer blocked"
828663698,9944,jolshan,2021-04-28T18:02:11Z,"Thanks @junrao for taking another look  
> A lot of the complexity is the additional logic for propagating unresolved partitions from FetchRequest to FetchSession and the maintenance of unresolved partitions within FetchSession.

I agree. This has been in the back of my mind for a while, specifically whether all the changes in FetchSession are necessary for such cases. So thanks for bringing this up. I think the only thing I was really concerned about was during a roll to upgrade/the new topic case you mentioned. But even using the current approach, I wasn't sure if the fetch session stuff was really helping. I think my biggest confusion comes from when the client will refresh metadata. Will returning a top level error guarantee a refresh? (vs  given an unknown topic ID response?) I think that the top level approach will likely be better. "
828795518,9944,junrao,2021-04-28T21:35:32Z,"I think the upgrade case is similar---it's rare and transient. So, we could choose to have a simpler and less optimized way for handling it. I am not sure if we trigger metadata refresh for top level error right now. If not, we could probably just add the logic to refresh metadata for all topics on topicId error at the top level."
830445376,9944,jolshan,2021-04-30T23:00:36Z,"Ok, updated the code. One thing I assumed here is that we don't switch from not using topic IDs in the session (requests versions 12 or below) to using 13+. I ensure this in the Fetcher +  AbstractFetcherThread code, but maybe we can't assume this for all clients. 

If we can update from not using ids in the session to using them, I'll update the code."
1540289311,13443,dajac,2023-05-09T14:52:18Z,@rreddy-22 The build failed due to compilation errors.
484241405,6592,yeralin,2019-04-17T20:07:29Z,Usage: `new ListSerde<>(Serdes.String())` to create a serde for `List<String>` values
485435436,6592,yeralin,2019-04-22T14:36:24Z,@mjsax 
485460971,6592,mjsax,2019-04-22T16:07:06Z,"Thanks a lot for the PR @yeralin! I like the idea and think it's a good addition.

However, adding those classes is a public API change and not a `MINOR` PR. Thus, it should be backed by a JIRA ticket and it also requires a KIP: https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals

Let me know if you have any question about the KIP process (should actually be well document in the wiki), or if you need any other assistance.

Btw: You should also add unit tests :)"
489700008,6592,mjsax,2019-05-06T17:16:45Z,"Great to see some progress on this PR, but we still need a KIP... I would recommend to work on the KIP first (or in parallel), @yeralin "
489709013,6592,yeralin,2019-05-06T17:42:39Z,"@mjsax KIP, JIRA, and DISCUSS thread are started: https://cwiki.apache.org/confluence/display/KAFKA/KIP-466%3A+Add+support+for+List%3CT%3E+serialization+and+deserialization"
492021215,6592,yeralin,2019-05-13T23:20:57Z,"Hmmm, strange I added a test case, but it still says that test results were not found"
516673508,6592,yeralin,2019-07-31T02:57:09Z,Any updates on this?
516952229,6592,yeralin,2019-07-31T17:48:54Z,"@mjsax Sorry, I thought to finish all the discussions first, that's why I did not push.
New commit is in place!  

UPD: Merged with apache:trunk"
518247929,6592,yeralin,2019-08-05T14:04:32Z,"Hey, I'm sorry I thought you wanted me to merge with `trunk` that's why so many files popped up.

I think I know how to resolve this. I'll force push **before** the merge, and then keep pushing my commits only related to `introduce_list_serde` work. And at the end, once all reviews are completed, I'll do the final merge and we can close this PR.

UPD: Ok @mjsax it should be much cleaner now :) "
519362418,6592,mjsax,2019-08-08T04:51:25Z,"@yeralin Seems there is a merge conflict again.

And yes, I usually rebase to `trunk` (not merge `trunk`) and then push force to update a PR."
519700266,6592,yeralin,2019-08-08T21:49:15Z,"Hmmm, I think I did it! I haven't worked with Git on a project of this scale, so bear with me :)

I think I performed a rebase with `apache/kafka:trunk`, resolved couple conflicts along the way, and now everything should be fine."
520019651,6592,mjsax,2019-08-09T18:29:35Z,"Build failed with spotbug issue. Can you address this?

To test locally before pushing, I recommend to run `./gradlew streams:clean streams:test` -- this will run checkstyle and spotbug, too. "
520549718,6592,yeralin,2019-08-12T18:51:10Z,"I fixed all checkstyle warnings, except for:
```
Disallowed import - org.apache.kafka.clients.CommonClientConfigs.
```
under `ListSerializer.java` and `ListDeserializer.java`

The build is failing bc of that.

---

Also, lots of tests are failing because for some reason they are expecting
`DEFAULT_LIST_KEY/VALUE_SERDE_INNER_CLASS`, `DEFAULT_LIST_KEY/VALUE_SERDE_TYPE_CLASS` and other properties to have default values:
```
org.apache.kafka.common.config.ConfigException: Missing required configuration ""..."" which has no default value.
```

I understand that I can set those default values during `.define(...` call. I guess I should set them to `null`?

"
520550127,6592,yeralin,2019-08-12T18:52:20Z,"As per unchecked warnings, I suppressed them at the variable or method levels."
526295380,6592,yeralin,2019-08-29T17:58:05Z,@mjsax Would you have time to take a look? :) 
530406657,6592,yeralin,2019-09-11T14:28:55Z,Rebased the branch with `trunk`. Still seeing unrelated `spotbugs` errors in failed Jenkins builds.
531210633,6592,cadonna,2019-09-13T12:04:17Z,"Retest this, please"
531377737,6592,yeralin,2019-09-13T20:20:26Z,"> Retest this, please

Those were actually _related_ spotbug errors, my fault  "
531892218,6592,yeralin,2019-09-16T18:09:20Z,"I think it is ready for the next round of review :) 

Thank you in advance!"
535305283,6592,yeralin,2019-09-26T02:35:32Z,Bump :) 
537524269,6592,yeralin,2019-10-02T14:38:51Z,Ready for another round :) 
539604139,6592,yeralin,2019-10-08T16:50:58Z,Bump  
541655992,6592,yeralin,2019-10-14T12:44:40Z,"Bump  

Also updated KIP and JIRA ticket"
545630718,6592,yeralin,2019-10-23T20:51:46Z,Bump  
547412636,6592,yeralin,2019-10-29T13:13:42Z,@mjsax would you be so kind to take another look? :) 
548114057,6592,yeralin,2019-10-30T21:11:15Z,"Rrrrready for another round :)

Thank you  "
549672088,6592,vvcephei,2019-11-05T05:46:39Z,"Thanks for looking into that, @mjsax , I also tried it out just now, and I think you're right.

The only thing I think we could do to be able to spit out a ""fully typed"" result is to introduce a ""dummy interface"" like Jackson's `TypeReference`. This kind of interface can be used to capture the full generic argument list at run time and use it to construct the collection. Short of that, I believe we'd be doomed to be producing `rawtypes` warnings in user code. Which is probably not a good choice for the API. Note, this is all in retrospect. It's not obvious at all that this would crop up until you try it.

In light of that, I think we probably need to change up the interfaces a little. Something like:
`class ListDeserializer<Inner> extends Deserializer<List<Inner>>` with a constructor like `public <L extends List> ListDeserializer(Class<L> listClass, Deserializer<Inner> innerDeserializer)`. When you call `deserialize`, you'd get a `List<Inner>` (e.g., `List<Integer>`) out, but this is probably fine, since best practice is to declare the variable type as the interface, not the implementation anyway. And the advantage is that the calling code (aka ""user code"") wouldn't have any ""rawtypes"" or ""unsafe"" warnings at all.

Again, thanks a million for catching this!"
550966966,6592,mjsax,2019-11-07T07:59:18Z,"@vvcephei Thanks for your input! I agree that that it would be better to preserve the ""inner type"" instead of the ""list type"", ie, `deserialize()` should return `List<Integer>` instead of raw `ArrayList`.

@yeralin -- Do you agree this this assessment? If yes, please update the PR accordingly."
553067625,6592,yeralin,2019-11-12T19:08:58Z,@vvcephei Just pushed a commit with changes according to your description. Not entirely sure if I got everything right.
555213976,6592,yeralin,2019-11-18T21:19:36Z,Updated KIP and JIRA ticket as well
555233480,6592,mjsax,2019-11-18T22:13:00Z,Thanks @yeralin -- I am currently fully loaded with `2.4` release issues... Will try to cycle back to your PR but might take some time.
571205357,6592,yeralin,2020-01-06T16:24:39Z,"@mjsax Happy New Year! :) 

I rebased the branch according to `upstream/trunk`. I noticed that branches aren't getting built anymore.

Let me know if there is anything else I can do here! "
585361037,6592,mjsax,2020-02-12T18:52:35Z,@yeralin Any progress on this PR?
585364165,6592,yeralin,2020-02-12T18:59:49Z,"Sorry, have been busy at work and personal stuff.
Should start making progress soon!"
585969307,6592,yeralin,2020-02-13T21:00:38Z,"@mjsax Introduced 
```
org.apache.kafka.common.serialization.ListSerializerTest
org.apache.kafka.common.serialization.ListDeserializerTest
```
They test non-arg-constructors and use `configure` methods.

I had to also update `configure` logic in both classes.

Take a look :)

Thank you!"
587819699,6592,yeralin,2020-02-18T20:46:35Z,"Hmmm I was thinking about this magic flag, and there are some corner cases that I want to discuss.

My initial idea was to create an enum:
```
public enum SerializationStrategy {
    NULL_INDEX_LIST,
    NEGATIVE_SIZE
}
```
Then it becomes counterintuitive:

Case 1: encoding primitives `arr={1, 2, null, 3}`
User chooses `SerializationStrategy.NEGATIVE_SIZE`, I guess we'd have to throw an exception saying that this serialization strategy is forbidden?

Case 2: encoding non-primitives `arr={'a', 'b', null, 'c'}`
User chooses `SerializationStrategy.NULL_INDEX_LIST`, but then what do we store for size of `arr[2]`? Zero? Negative one? We are still wasting 4 bytes that way.

I think our best solution is to simply use ""null index list"" for all primitives, and `-1` strategy for all non-primitives avoiding any magical, serialization strategy flags.

What do you think? 
@mjsax @vvcephei "
606241093,6592,mjsax,2020-03-30T20:47:32Z,"Sorry for the late reply... Adding an enum sounds like a good idea.

> Case 1: encoding primitives with `NEGATIVE_SIZE`

I don't think we would need to throw, however, we would need to choose the non-optimized encoding and encode the length for each list item (even if it is the same length) over and over again.

> Case 2: encoding non-primitives with `NULL_INDEX_LIST`

We would store zero bytes, because we can skip the entire entry. During deserialization we can just do a `list.add(null)` when we hit the corresponding index as encoded in the null-index-list header, ie, during deserialization we maintain a counter for the index of the element we deserialize.

Does this make sense?

> I think our best solution is to simply use ""null index list"" for all primitives, and -1 strategy for all non-primitives avoiding any magical, serialization strategy flags.

That would also be possible as an initial non-configurable default implementation, but I would still encode a magic header byte in case we want to change it later (just to be future prove). My suggestion to only implement one strategy was aiming to limit the scope of the PR. But if you want to implement both strategies, I have no objection either. However, if we actually do implement both strategies, than we could make it configurable from the very beginning on?"
606252503,6592,yeralin,2020-03-30T21:12:22Z,"> However, if we actually do implement both strategies, then we could make it configurable from the very beginning on?

I will implement both strategies. My problem with making it configurable is

> ..., however, we would need to choose the non-optimized encoding and encode the length for each list item (even if it is the same length) over and over again.

User might choose inefficient strategy and shoot him/herself in the foot.

That's why I was thinking just
> use ""null index list"" for all primitives, and ""-1"" strategy for all non-primitives avoiding any magical, serialization strategy flags.

But I do agree on leaving future prove 1 byte magic flag in the header.

"
606277356,6592,mjsax,2020-03-30T22:08:55Z,"> User might choose inefficient strategy and shoot him/herself in the foot.

I guess it's called freedom of choice :) If we feel strong about it, we could of course disallow the ""negative size"" strategy for primitive types. However, it would have the disadvantage that we have a config that, depending on the data type you are using, would either be ignored or even throw an error if set incorrectly. From a usability point of view, this would be a disadvantage. It's always a mental burden to users if they have to think about if-then-else cases.

I guess, in the end it's subjective which approach is better. But we could discuss pros/cons on the mailing list. Feel free to propose whatever you prefer personally (and list all pros/cons for different approaches) and people can just chime in.

Personally, I have a slight preference to allow both strategies for all types as I think easy of use is more important, but I am also fine otherwise."
657756747,6592,yeralin,2020-07-13T19:49:04Z,"Aaaaaan I am back!   Sorry for such a long break, was dealing with some personal stuff and work.

Pushed a few commits with `null-index-list` and `negative-size` serialization strategy functionality.

Rrrready for another round!

The code syntactically is kinda raw, made it so to facilitate the reviewing process.

The following set of commits is recommended for reviewing:
https://github.com/apache/kafka/pull/6592/files/b9ab94486e8d58903e2271978521050c3e1ffab3..bc9cd4bc47f8524db255ff65480868315541ef84

@mjsax @vvcephei "
665078937,6592,yeralin,2020-07-28T14:37:11Z,Any updates?
665304810,6592,mjsax,2020-07-28T21:58:28Z,"@yeralin -- your PR is in my review queue. Not sure how quickly I will find time to have a look though atm -- maybe next week, but I can't promise."
698414958,6592,yeralin,2020-09-24T15:23:04Z,Any updates :)? 
701300767,6592,helpermethod,2020-09-30T10:19:00Z,"This would be an awesome addition, as I wouldn't have to maintain my own impl .
"
702774321,6592,yeralin,2020-10-02T14:42:16Z,@mjsax Let me know when I can jump back to it :)
730815895,6592,yeralin,2020-11-20T03:06:24Z,"Hi, any updates on this?"
731626798,6592,mjsax,2020-11-21T19:41:30Z,"@yeralin -- sorry for the delay. Happy to hear that you are still on top of your KIP!

I did not find time yet to review the PR after your latest updates. (Not sure if it makes you feel any better, I did also not review any other KIP related PR for a while, and yours is actually at the top of my list...) -- I hope to find time soon and hope we can get this merged before the end of the year... Cannot guarantee it though. At least, I would like to get it into 2.8 release."
763952940,6592,yeralin,2021-01-20T21:18:34Z,"@mjsax Hey, just rebased my branch with the trunk, updated my tests to use JUnit 5.
Let me know when you guys will have time to review it."
776422927,6592,yeralin,2021-02-10T04:01:44Z,Any updates on this? :)
776934773,6592,mjsax,2021-02-10T18:56:01Z,"@yeralin -- Sorry that I did not get to review your updates yet... I need to finish #9744 first that is very close to get merged. Your PR is next in the list... (Just trying to work through my KIP PR backlog one-by-one...)

Btw: seems there is a merge conflict. Could you rebase the PR in the mean time?"
778674834,6592,yeralin,2021-02-13T20:37:53Z,"Ok, resolved all conflicts. Thank you for dedicating your time into this :)"
808566124,6592,yeralin,2021-03-26T22:41:48Z,Bump
819183273,6592,ableegoldman,2021-04-14T02:49:38Z,"Hey @yeralin , are you still working on this/looking for reviews? I know everyone is often busy but if you can just respond to let us know you're still active, we can try to pitch in to get this across the finish line.

cc @mjsax @cadonna @lct45 @wcarlson5 for help reviewing"
819226355,6592,yeralin,2021-04-14T04:53:39Z,"I am! :) Routinely checking this PR.

I totally understand, we all get busy sometimes   "
821515617,6592,yeralin,2021-04-16T19:29:23Z,"Replied under each review comment, waiting for response before pushing the review changes."
823384191,6592,yeralin,2021-04-20T15:47:46Z,@ableegoldman Did you have time to look at it? Let me know if you need more context on my comments.
829779733,6592,ableegoldman,2021-04-30T03:26:34Z,"Hey @yeralin one other thing, can you give the KIP document a quick pass and make sure everything in there is up to date with what we've discussed and anything else that's evolved during the PR review? For example we might want to point out the `null` handling, though it's technically an implementation detail it would be good to clarify what is and isn't possible with this new Serde"
840015831,6592,yeralin,2021-05-12T18:46:11Z,"Ok, updated the KIP with serializing nulls for different strategies."
840856470,6592,yeralin,2021-05-13T21:52:33Z,@ableegoldman Seems like all checks passed :)
840878936,6592,ableegoldman,2021-05-13T22:50:10Z,"Build has only some unrelated test failures in known flaky `KTableKTableForeignKeyInnerJoinMultiIntegrationTest#shouldInnerJoinMultiPartitionQueryable` and 
`RaftClusterTest`"
840880983,6592,ableegoldman,2021-05-13T22:56:16Z,"Merged to trunk! Thanks @yeralin for all the work and patience it took to get this PR in, and to all the reviewers who helped get it here along the way.

@yeralin, can you update the KIP to note that it's completed, and then move this KIP to the ""Adopted"" section on both the KIP main page and the Streams KIPs subpage?  "
840925233,6592,mjsax,2021-05-14T01:04:32Z,Thanks @yeralin for pushing it through and thanks @ableegoldman for helping out reviewing! Really nice addition!
1603267651,6592,venkatesh010,2023-06-22T20:23:12Z,"Hey @yeralin @mjsax @ableegoldman  Getting SerializationException in this serde 
SerializationException: Invalid serialization strategy flag value

Flag value is derived from bytes size (which is coming >2) which is amount of enum variables hence its breaking
seems to be a bug

please check"
1603284146,6592,venkatesh010,2023-06-22T20:37:10Z,"This is while using List<E> where E is class which is Inner
Serde used for Inner is JsonSerde of Type E
"
1603491428,6592,yeralin,2023-06-23T00:59:45Z,@venkatesh010 could you please provide a code snippet?
416341868,5567,vvcephei,2018-08-27T19:34:03Z,"Note: both builds passed, but Jenkins got rate-limited trying to report it to Github.

And I'm just now wondering... if Jenkins failed to report the job status to Github... why do we see the status here on Github? O_o"
416457836,5567,guozhangwang,2018-08-28T05:40:43Z,@bbejeck could you take a look?
417199290,5567,guozhangwang,2018-08-30T05:58:54Z,"@vvcephei I have not reviewed the latest changes on this PR yet, but here are two meta-level thought I'd like to share:

1. Serdes: here's my reasoning on whether we need to enforce serdes. We have the following scenarios:

a) The KTable-to-be-suppressed (I will just call it KTable from now on) has user-specified serdes. In this case we do not need to require serdes again for suppression.

b) The KTable is materialized and users do not specify serdes during materialization. In this case we will try to use the default ones from config (or we can use the inherited ones in the future, but that is not guaranteed to be always correct anyways), so if the default serde to use is incorrect, we will get the exception even before suppression at all. So we do not need to require serdes either.

c) The KTable is NOT materialized and users do not specify serdes. Today this case is not possible but in the future it may be the case due to optimizations, e.g. `KTable#filter / mapValues` generated KTable. In this case if we do not require users to specify serdes and default ones are not correct, it will indeed have unexpected exceptions. But I think this case can still be walk-around by users to provide the `Materialized` object in those operators; plus in the future we can have further optimization to ""push the suppression ahead"" which I will talk about later in this comment.

So in sum, I think it is not necessary to always enforce users to provide serdes in the buffer config.

2. Changelogs:

About whether or not we should add new changelog topics for the suppression buffer itself, I think it depends on how we will implement the commit behavior. My suggestion is the following:

a) for size / interval based intermediate suppression, we will still honor the commit operation to always ""flush"" the suppression buffer, i.e. the intermediate suppression is still best-effort which yields to commits. In this case, we do not need to bookkeep ""what records have been buffered and not emitted"" in the changelog either but can simply assume none have been emitted until commit.

b) for final result suppression of window stores, we cannot yield to commit because that will violate the intended guarantees, BUT since we will not emit any records before the grace deadline anyways, the ""state"" of the buffer does not need to be book-kept anyways: if it is beyond the grace deadline, then every records should be flushed, otherwise, none should be flushed.

Note that the above approach works for both EOS and non-EOS: for non-EOS, if there is a crash in between commits, we may emit a single record multiple times but that is fine for non-EOS; for EOS, if there is a crash in between commits we need to restore the whole state from the beginning anyways as of now (until we have consistent checkpoints someday), so this is also fine.

One caveat though is that 2.b) above relies on the current stream time to determine upon re-start whether or not the window store data have been emitted to downstream; but stream time today is not deterministically defined even with KIP-353 merged in so if we re-process, it may generate different behavior. I think this is acceptable as of now and can be fixed in the future: for example, we can include the ""current stream time"" in the commit message when adding consistent checkpoints to make the stream time deterministic on those checkpoints.

So in sum, I think we can accept to not have changelogs for the buffer itself as long as we still respect commits except for final result suppression, which we can implement as a special case. In addition, the KTable-before-suppression's changelog can be suppressed along with the buffer as well: we can only write to the change logger when the buffer emits. More details in the next section.


3. Future roadmap:

We have discussed about how to re-design KIP-63 after this is done, and one thing is to consider how to maintain the suppression effect on the state store's changelog topics as well. Together it has some implications on our memory management story as well. Here's my thinking following the above proposal on changelogs:

a) Say if we remove the buffer on top of the state stores, the saved memory can be given to 1) the added buffer ""behind"" the state stores, and 2) to enlarge the state store's only write buffer (e.g. rocksDB). 

b) We can optimize the topology to ""implicitly"" add suppression when necessary in addition to user-requested suppressions to reduce the traffic to the original KTable's store changlog topics. More concretely, think about the following examples:

```
table2 = table1.filter();
```

* with logical materialization, only table1 will be materialized.
* with logical materialization plus implicit suppression, the above will become:

```
table 2 = table1.suppress().filter()
```

in which case table1's store changelog will be suppressed as well: we only write to the change logger when we emit.

Now if users explicitly calls suppression:

```
table2 = table1.filter().suppress();
```

It will also be re-written to

```
table 2 = table1.suppress().filter();
```

As well, in which case table1's changelog will be suppressed still based on the `suppress()` config, and then the suppressed changelog stream will be filtered to table2, which can be logically materialized still.

c) Finally about memory management: we can deprecate the current `max.cache.bytes` and replace with the `total.memory.bytes` which controls the total amount of memory that Streams will use, similarly to what we described in https://cwiki.apache.org/confluence/display/KAFKA/Discussion%3A+Memory+Management+in+Kafka+Streams. Note this total bytes will cover both user-requested and implicitly added suppression buffers. In other words, each buffer's own buffer config's semantics will be a soft-limit which is only best-effort full-filled, since it is yield to commit interval, AND the total bytes usable.

This is just a sketchy thought and may need more detailed implementation discussions. Let me know WDYT."
417352990,5567,vvcephei,2018-08-30T15:05:21Z,"Hi @guozhangwang ,

Thanks for the thoughtful feedback.

1. I think your argument about the serdes is sound.

* for any non-k/v-changing operation that produces a KTable, we will be able to forward serdes from upstream, through the operator, and to suppress
* the rest of the operators may change keys or values, but in all cases, it's possible to provide serdes at the key/value-changing operator, and then forward to suppress.

So in all cases, we don't need to ask for serdes in suppress, which I vastly prefer. Thanks!

2. for changelogs,

I think it would be much better if we offered tight semantics in all cases. Forcing people to reason about how the commit interval interplays with the suppression is needlessly complicated.

But I do think that we can still optimize it to avoid the extra changelog. The good news is that at this stage, realizing that we can get serdes without asking for them in the suppression config means that we don't have to worry about the changelog or commit behavior.

So this PR is not blocked on that conversation.

I'll take it as a design goal to avoid an extra changelog and spend some time to see what I can come up with. At the least, you've offered a way to do it by relaxing the suppression semantics.

3. Yes, I think that's a good long-term vision. And it would be all the more important to avoid an extra changelog if we wind up tacking a suppression on to every ktable.

3c. Thanks for that reference. It would be nice to have a simple control bounding the memory usage. However, I'm not sure I agree that that config should be allowed to alter the program we've been asked to execute. 

If we were to add a `streams.memory.bytes`, we will also have to consider what to do if it's overconstrained. Clearly, we cannot execute a Streams program within 7 bytes, so we would have some validation on startup that says ""hey, you asked for no more than 7 bytes, but we need at least 800MB for this program"". Rather than relaxing the suppression semantics, I'd advocate for explicit user-specified buffer sizes to be included in this arithmetic.

But that is again a problem for the future.

So in conclusion: I'll drop the serdes from the API, and forward from the source KTable instead. We should then be able to resume the review of this PR, right?"
417429229,5567,guozhangwang,2018-08-30T18:54:59Z,"> But that is again a problem for the future.
> 
> So in conclusion: I'll drop the serdes from the API, and forward from the source KTable instead. We should then be able to resume the review of this PR, right?

Yup, thanks!



> But I do think that we can still optimize it to avoid the extra changelog.

Yeah I'm not married to my approach, so I'm all ears if you do already have some concrete ideas :)"
417804756,5567,bbejeck,2018-08-31T22:26:44Z,"@guozhangwang @vvcephei 

I also agree with the approach for Serdes management approach."
418515105,5567,vvcephei,2018-09-04T20:56:44Z,"@guozhangwang @bbejeck ,

I've added the missing tests.

I think this is the remainder of the comments. I'll update the KIP and send out a notice now.

Thanks,
-John"
418892024,5567,vvcephei,2018-09-05T21:45:55Z,"@bbejeck I think I've addressed all your concerns.

About the system tests / performance concerns: yes, I plan to follow up with that once the features themselves are merged:

1. this PR (API only)
2. independent names for suppression nodes
3. in-memory buffering (not resilient to node shutdown/crash; spill-to-disk not implemented)
4. resilient buffering (wrap the buffer in a changelog)
5. spill-to-disk buffering
6. performance testing & optimization

This was the set of smallest diffs I could think of that made sense to review. Please let me know if some other subdivision makes sense.

/cc @guozhangwang "
419145301,5567,vvcephei,2018-09-06T15:49:38Z,Thanks for the review @bbejeck ! I know this one was a lot of work.
419982389,5567,guozhangwang,2018-09-10T16:50:25Z,I do not have further comments. LGTM.
422444318,5567,vvcephei,2018-09-18T15:39:50Z,"@guozhangwang @bbejeck @mjsax , 

I believe this PR is ready for a final pass. In response to your comments, I have revised the `Suppressed` interface:
* renaming the static/instance methods to follow our conventions
* flattened the IntermediateSuppression config into the top-level Suppressed interface
* renamed the Suppressed methods to be clearer (like `untilWindowCloses` instead of `finalResults`) and also match each other (`untilWindowCloses` and `untilTimeElapses`).
* added javadocs"
422910901,5567,vvcephei,2018-09-19T18:31:56Z,"@mjsax Thanks so much for the detailed review. I realized that your concern about ambiguity between the configured suppression time and the grace period was actually due to a regression I introduced earlier in refactoring. Apologies for not realizing this sooner.

I have made a couple of changes to resolve this:
* `Suppressed` now only declares static factory methods (as it did initially). In particular, this ensures that we can rely on the config for `untilWindowClose` to be just as we create it in that method. It's not possible to mutate it afterwards.
* I replaced the `isFinalResults` boolean in `SuppressedImpl` with a `FinalResultsSuppressionBuilder`. Previously, the code just assumed that some fields, like the suppress duration, would be null for final results `Suppressed` configs. Now, there is no ambiguity about that.

I'm going to make a final pass over this diff myself, but otherwise, I think it's ready for (another) final review.

Thanks!"
422992786,5567,vvcephei,2018-09-19T23:39:50Z,"Hey @mjsax , the tests have (finally) passed.

Did you have any other feedback, or are we good to go?"
423587312,5567,vvcephei,2018-09-21T16:05:44Z,"Ok @mjsax , I've addressed your concerns. I'll second the request for a final review from @guozhangwang  and @bbejeck ."
423589986,5567,guozhangwang,2018-09-21T16:13:35Z,"@vvcephei @bbejeck I've made a pass over the updated KIP wiki and the API lgtm. One clarification question:

What's the semantics of

```
static <K> Suppressed<K> untilTimeLimit(final Duration timeToWaitForMoreEvents);
```

compared with

```
static <K> Suppressed<K> untilTimeLimit(final Duration timeToWaitForMoreEvents, final BufferConfig bufferConfig);
```

The javadoc of these two functions are exactly the same?"
423594358,5567,vvcephei,2018-09-21T16:26:53Z,"@guozhangwang Thanks for that catch. I failed to add the extra parameter to the javadoc for the second method. The semantics are basically the same. The first one defaults to an unbounded buffer.

In retrospect, this seems like an unnecessary shortcut. I think I'll just ditch the first method and document the buffer config on the second."
423614660,5567,vvcephei,2018-09-21T17:32:42Z,"Hmm. That java 10 build is failing for something in core, but I can't figure out what (see also https://github.com/apache/kafka/pull/5672)"
423625474,5567,vvcephei,2018-09-21T18:10:15Z,Update: I've submitted https://github.com/apache/kafka/pull/5674 to fix trunk.
423994933,5567,vvcephei,2018-09-24T14:29:44Z,"This PR was incompatible with the rename of `InternalProcessorContext.initialized`, so I rebased and fixed it."
423995090,5567,vvcephei,2018-09-24T14:30:10Z,"@guozhangwang I think this is ready to merge now, once the tests pass."
424120339,5567,vvcephei,2018-09-24T20:50:58Z,"Thanks so much for all your time reviewing, @mjsax @bbejeck  and @guozhangwang "
932750960,11331,ijuma,2021-10-02T13:17:52Z,"Thanks for the PR. A high-level question, what are we trying to optimize for here?
1. Requests that don't include topic ids
2. Requests that include topic ids
3. Both
4. Some kind of balance of both where we compromise a bit to keep the code maintainable"
933821031,11331,jolshan,2021-10-04T20:15:01Z,"@ijuma 
> Thanks for the PR. A high-level question, what are we trying to optimize for here?
>Requests that don't include topic ids
Requests that include topic ids
Both
Some kind of balance of both where we compromise a bit to keep the code maintainable

The goal of this PR is to gracefully handle the new topic case. Currently in kafka, when we create a new topic, the leader and Isr request is sent first, then the update metadata request. This means that we will often encounter transient ""unknown_topic_id"" errors. In the new world of topic IDs, we will see this as ""unknown topic ID"" errors. The current logic returns a top level error and delays all partitions. This is a regression from previous behavior, and so this PR's goal is to return to the behavior where we store the unknown partition in the session until it can be resolved. See https://issues.apache.org/jira/browse/KAFKA-13111 for more information.

"
949274575,11331,jolshan,2021-10-22T04:20:15Z,"TODOs:
1. ~Change inconsistent topic ID so it is no longer a top level error~
2. ~Maybe refactor some of the receiving side code, we have a map with TopicIdPartition, PartitionData and both contain topic ID~ Decided to hold off on this as there are still usages for TopicIdPartition in the receiving side.
3. Maybe change FetchSession to update newly unresolved partitions to no longer include topic name."
961259072,11331,dajac,2021-11-04T17:24:58Z,"@jolshan It seems that there are a few compilation errors, at least for `JDK 8 and Scala 2.12`. Could you check?"
965553054,11331,jolshan,2021-11-10T17:07:04Z,"System test results: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2021-11-10--001.system-test-kafka-branch-builder--1636543025--jolshan--KAFKA-13111--165a106bf3/report.html

A previous run was all green, so will need to confirm the 3 failed tests are unrelated to this change."
965677394,11331,jolshan,2021-11-10T19:38:05Z,Looks like the topic id partition changes broke the build. I'll probably need to pull the latest version.
966339841,11331,dajac,2021-11-11T14:19:24Z,"> System test results: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2021-11-10--001.system-test-kafka-branch-builder--1636543025--jolshan--KAFKA-13111--165a106bf3/report.html
> 
> A previous run was all green, so will need to confirm the 3 failed tests are unrelated to this change.

@jolshan Have you been able to triage these failures?"
968676970,11331,dajac,2021-11-15T09:06:16Z,System test failures are not related. Merged to trunk and to 3.1.
375409892,4756,mjsax,2018-03-22T18:22:51Z,\cc @bbejeck @vvcephei 
375547735,4756,guozhangwang,2018-03-23T05:16:18Z,retest this please
375974891,4756,debasishg,2018-03-25T14:32:28Z,Added commit with changes for code review feedback. Waiting for more feedback and some of the still unresolved questions. Should I rebase now or wait till all questions resolved ?
376668126,4756,mjsax,2018-03-27T20:42:00Z,"Meta comment: For JavaDocs, so we need to set up some pipeline to get JavaDocs published? Or will this happen automatically?

Additionally, this PR should include updates to the web docs in `docs/streams/...` and in ""notable changes"" in `docs/upgrade.html` ?"
377176774,4756,debasishg,2018-03-29T09:23:03Z,"Meta Comment - Besides updating documentation and Javadoc, is there any outstanding item in this PR that needs to be addressed ?"
377706175,4756,seglo,2018-03-31T16:43:04Z,"For user documentation my plan is to update the following:

* [Streams homepage](https://kafka.apache.org/11/documentation/streams/) - Update the Scala example to use Scala DSL
* Developer Guide
  * [Writing a Streams Application](https://kafka.apache.org/11/documentation/streams/developer-guide/write-streams.html) - Add a reference to the streams-scala artifact and add Scala examples
  * [Configuring a Streams Application](https://kafka.apache.org/11/documentation/streams/developer-guide/config-streams.html) - Note that default key/value serdes not required for Scala DSL.
  * [Streams DSL](https://kafka.apache.org/11/documentation/streams/developer-guide/dsl-api.html) - Add a new section for the Scala DSL.  Packages, examples, implicit serdes and other differences from Java DSL.  The bulk of the content would go here.
  * [Data Types & Serialization](https://kafka.apache.org/11/documentation/streams/developer-guide/datatypes.html) - Reference implicit serdes from Streams DSL page which includes implicit serdes explanation and usage, or vice versa.

I could use some advice on how to actually go about updating the docs efficiently.  The [Contributing Website Documentation Changes](https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Website+Documentation+Changes) doesn't go into detail about a workflow I can use while developing docs (make a change, preview, etc).  I noticed the code samples for the streams docs are formatted as HTML, but I couldn't find the source for these examples in the project, are they generated somehow?

EDIT: I also added to the https://kafka.apache.org/documentation/#streamsapi section, to explain how to include the dependency in maven build file, e.g. to use the scala wrapper.

EDIT 2: I also updated the section 2.3 (streams api) to reference the new client library.

EDIT 3: I also updated the streams upgrade guide."
377755547,4756,mjsax,2018-04-01T05:29:57Z,"@seglo Sounds great.

You are right that it is HTML -- just edit it directly. I have no concrete workflow suggestion. What code examples do you mean? It should all be in the HTML files. There is nothing in streams docs thats generated."
377786142,4756,seglo,2018-04-01T13:16:09Z,"@mjsax I'm referring to the code snippets found in pages like the DSL API.  They look like they've been generated with syntax highlightling.

```
                        <div class=""highlight-java""><div class=""highlight""><pre><span></span><span class=""kn"">import</span> <span class=""nn"">org.apache.kafka.common.serialization.Serdes</span><span class=""o"">;</span>
<span class=""kn"">import</span> <span class=""nn"">org.apache.kafka.streams.StreamsBuilder</span><span class=""o"">;</span>
<span class=""kn"">import</span> <span class=""nn"">org.apache.kafka.streams.kstream.GlobalKTable</span><span class=""o"">;</span>

<span class=""n"">StreamsBuilder</span> <span class=""n"">builder</span> <span class=""o"">=</span> <span class=""k"">new</span> <span class=""n"">StreamsBuilder</span><span class=""o"">();</span>

<span class=""n"">GlobalKTable</span><span class=""o"">&lt;</span><span class=""n"">String</span><span class=""o"">,</span> <span class=""n"">Long</span><span class=""o"">&gt;</span> <span class=""n"">wordCounts</span> <span class=""o"">=</span> <span class=""n"">builder</span><span class=""o"">.</span><span class=""na"">globalTable</span><span class=""o"">(</span>
    <span class=""s"">&quot;word-counts-input-topic&quot;</span><span class=""o"">,</span>
    <span class=""n"">Materialized</span><span class=""o"">.&lt;</span><span class=""n"">String</span><span class=""o"">,</span> <span class=""n"">Long</span><span class=""o"">,</span> <span class=""n"">KeyValueStore</span><span class=""o"">&lt;</span><span class=""n"">Bytes</span><span class=""o"">,</span> <span class=""kt"">byte</span><span class=""o"">[]&gt;&gt;</span><span class=""n"">as</span><span class=""o"">(</span>
      <span class=""s"">&quot;word-counts-global-store&quot;</span> <span class=""cm"">/* table/store name */</span><span class=""o"">)</span>
      <span class=""o"">.</span><span class=""na"">withKeySerde</span><span class=""o"">(</span><span class=""n"">Serdes</span><span class=""o"">.</span><span class=""na"">String</span><span class=""o"">())</span> <span class=""cm"">/* key serde */</span>
      <span class=""o"">.</span><span class=""na"">withValueSerde</span><span class=""o"">(</span><span class=""n"">Serdes</span><span class=""o"">.</span><span class=""na"">Long</span><span class=""o"">())</span> <span class=""cm"">/* value serde */</span>
    <span class=""o"">);</span>
</pre></div>
                        </div>
```

https://github.com/apache/kafka/blob/trunk/docs/streams/developer-guide/dsl-api.html#L189

"
377829608,4756,guozhangwang,2018-04-02T00:41:39Z,"@seglo For the doc changes, as I mentioned before we should also add a new section in the `https://kafka.apache.org/documentation/#streamsapi` section, to explain how to include the dependency in maven build file, e.g. to use the scala wrapper."
377863998,4756,mjsax,2018-04-02T06:14:56Z,"@seglo I see -- I guess, somebody used some special HTML editor... \cc @joel-hamill might be able to shed some light... Most people just use plain text editors. You can just add plain HTML without any syntax highlighting. Cf https://github.com/apache/kafka/blob/trunk/docs/streams/developer-guide/testing.html#L76-L93

(I was just not sure what you mean by ""generated"" because the configs HTML is generated from the source code directly -- cf. https://kafka.apache.org/documentation/#configuration and https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/config/ConfigDef.java#L1095)"
378364212,4756,seglo,2018-04-03T19:14:01Z,"I've implemented the user docs.  However I don't know how to view the documentation in my browser.  None of the HTML files in `docs` subdir render when I open them locally with Chrome (even before I made any edits).  There are likely some formatting mistakes since they're difficult to spot in the markup alone.  Can someone familiar with editing these docs please describe a workflow I can use to view rendered documentation locally? 

In the meantime I encourage people to review the content.

/cc @mjsax @guozhangwang @joel-hamill "
378411984,4756,guozhangwang,2018-04-03T21:54:42Z,"@seglo you can read this wiki page for render the pages locally: https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Website+Documentation+Changes

To do that you'd need to first copy-paste the docs to `kafka-site` repo, and start the apache server locally in order to see the difference."
378682617,4756,seglo,2018-04-04T17:34:10Z,"@guozhangwang I was able to setup a local apache2 webserver (wow, I haven't done that in 10 years!)  I fixed formatting issues, typos, added the blurb to the streams upgrade guide, and other misc. feedback you provided."
378730925,4756,guozhangwang,2018-04-04T20:16:22Z,"> @guozhangwang I was able to setup a local apache2 webserver (wow, I haven't done that in 10 years!) I fixed formatting issues, typos, added the blurb to the streams upgrade guide, and other misc. feedback you provided.

Thanks!

Could you also rebase your PR against trunk to resolve the conflicts as well?"
378796513,4756,seglo,2018-04-05T01:44:33Z,@guozhangwang Done!
379035892,4756,guozhangwang,2018-04-05T18:37:56Z,retest this please
379273985,4756,debasishg,2018-04-06T14:39:48Z,"@dguy - One of the reasons we wrote the tests was to demonstrate the idiomatic usage of the Scala APIs. If you think we should write additional tests to verify if the correct topology is built, what kind of tests can do this verification ? Is there any example test in the Java APIs that does this verification of building the correct topology ?"
379284118,4756,dguy,2018-04-06T15:11:56Z,"@debasishg - good enough reason. For additional tests i can think of a few of ways of doing it.
1. use the `TopologyTestDriver` - so you can build the topology for say `mapValues` and then run that thought the `TopologyTestDriver` to verify the output.
2. call `describe` on the built `Topology` and verify it has the correct nodes (see `TopologyTest`)
3. mock/stub the the java interfaces and verify the correct interactions."
379284887,4756,debasishg,2018-04-06T15:14:25Z,@dguy - Thanks! We will take a look and have additional tests.
379378808,4756,guozhangwang,2018-04-06T21:06:03Z,"The jenkins failures seems relevant:

```
19:16:23 :rat
19:16:24 Unknown license: /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk7-scala2.11/streams/streams-scala/logs/kafka-streams-scala.log
19:16:24 Unknown license: /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk7-scala2.11/streams/streams-scala/logs/kafka-streams-scala.log.1
19:16:24 :rat FAILED
19:16:24 
19:16:24 FAILURE: Build failed with an exception.
19:16:24 
19:16:24 * Where:
19:16:24 Script '/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk7-scala2.11/gradle/rat.gradle' line: 63
```

We should exclude `logs/kafka-streams-scala.log` from rat check."
379477483,4756,debasishg,2018-04-07T15:27:33Z,retest this please
379628965,4756,debasishg,2018-04-09T04:38:58Z,"@dguy - The Kafka Streams Scala API is a DSL level API rather than a `Processor` level one.

So are u suggesting that for a specific Scala API (say `KStream#mapValues`) we write the equivalent `Processor` level code and verify if the results match with the Scala DSL API ?

Another option could be to use both the Scala and the Java API for `KStream#mapValues` and verify if we get the same results. 

Isn't the latter option better in the sense that we are comparing APIs at the same level ?"
379758448,4756,dguy,2018-04-09T13:49:01Z,@debasishg - yes comparing them would be fine. 
380139596,4756,debasishg,2018-04-10T15:18:56Z,@dguy - In the last commit I wrote the original test cases using the Java APIs in Scala. This verifies if the 2 APIs deliver the same result. Is this what u meant ?
380143884,4756,dguy,2018-04-10T15:31:01Z,"@debasishg - i was thinking more along the lines of using the scala api to
build simple streams topologies, i.e, `Topology topology =
streams.mapValues(..).build()` and then using the `TopologyTestDriver` to
pipe some input through the topology and verify the output is as expected.

On Tue, 10 Apr 2018 at 16:19 Debasish Ghosh <notifications@github.com>
wrote:

> @dguy <https://github.com/dguy> - In the last commit I wrote the original
> test cases using the Java APIs in Scala. This verifies if the 2 APIs
> deliver the same result. Is this what u meant ?
>
> 
> You are receiving this because you were mentioned.
>
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/kafka/pull/4756#issuecomment-380139596>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AADWLUP53hOOLSdIh2na8QsWv11ZUjXbks5tnM1-gaJpZM4S3B3->
> .
>
"
380151904,4756,debasishg,2018-04-10T15:52:56Z,"@dguy - May be I am missing something .. in your example `Topology topology =
streams.mapValues(..).build()`, what is `streams` ? `mapValues` is a method on `KStream` - right ? Is there any example in the code base that I can refer to for similar stuff ?"
380154612,4756,dguy,2018-04-10T16:00:32Z,"@debasishg - yes that is correct. Sorry, my bad. Yes you would do something
like:
```
val builder = new StreamBuilder(..)
builder.stream(..).mapValues(..)
val topology = builder.build()
val testDriver = new TopologyTestDriver(topology, properties,
initialWallClockTime)
...
```
You can have a look at
https://github.com/apache/kafka/blob/trunk/streams/test-utils/src/test/java/org/apache/kafka/streams/TopologyTestDriverTest.java
for how to pipe input and verify the output etc.

HTH


On Tue, 10 Apr 2018 at 16:53 Debasish Ghosh <notifications@github.com>
wrote:

> @dguy <https://github.com/dguy> - May be I am missing something .. in
> your example Topology topology = streams.mapValues(..).build(), what is
> streams ? mapValues is a method on KStream - right ? Is there any example
> in the code base that I can refer to for similar stuff ?
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/kafka/pull/4756#issuecomment-380151904>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AADWLeVuI9MlFKrhRzfbEp_xsBmMcAZuks5tnNWKgaJpZM4S3B3->
> .
>
"
380381008,4756,debasishg,2018-04-11T09:03:05Z,@dguy - I added a suite `TopologyTest` which verifies that the topology generated by the Java and Scala APIs are identical. Please let me know if this works. I did not use `TopologyTestDriver` as I did not want to execute any topology. The earlier tests verify that the results we get from the Java and the Scala API are identical. These tests verify that the topologies are identical as well.
380382707,4756,dguy,2018-04-11T09:08:53Z,Thanks @debasishg - that works
380383532,4756,dguy,2018-04-11T09:11:43Z,BTW - did you start a voting thread for the KIP yet?
380384424,4756,debasishg,2018-04-11T09:14:43Z,"Thanks @dguy .. 

Regarding voting thread, no we haven't done yet. Also we are not sure how to do it :-) .. How do we start a voting thread ?"
380392395,4756,dguy,2018-04-11T09:42:50Z,"On the dev@kafka.apache.org list you start a thread, similar to the
[DISCUSS] thread you already started, but use [VOTE] instead of [DISCUSS].
The vote needs to run for at least 72 hours and needs at least 3 binding
votes.

On Wed, 11 Apr 2018 at 10:15 Debasish Ghosh <notifications@github.com>
wrote:

> Thanks @dguy <https://github.com/dguy> ..
>
> Regarding voting thread, no we haven't done yet. Also we are not sure how
> to do it :-) .. How do we start a voting thread ?
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/kafka/pull/4756#issuecomment-380384424>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AADWLaYWH_zM7jx11Z79p_yiyythUUTlks5tncmkgaJpZM4S3B3->
> .
>
"
380400070,4756,debasishg,2018-04-11T10:09:36Z,done ..
380672649,4756,debasishg,2018-04-12T04:14:44Z,"@dguy, @guozhangwang, @mjsax  - sent out the [VOTE] email on `kafka-dev`  "
382386413,4756,ijuma,2018-04-18T13:29:10Z,"@debasishg, for some reason GitHub doesn't let me reply inline. Your ""more efficient version"" is still allocating a tuple, right? To avoid it, mapper would have to return a KeyValue."
382389088,4756,debasishg,2018-04-18T13:37:43Z,"@ijuma - The `map` function doesn't allocate the tuple. It takes a function of the form `(K, V) => (KR, VR)` and constructs the `KeyValue` from the tuple that `mapper` returns. Wanted to abstract the construction of `KeyValue` to avoid noise.
```
def map[KR, VR](mapper: (K, V) => (KR, VR)): KStream[KR, VR] = {
  val mapperJ: KeyValueMapper[K, V, KeyValue[KR, VR]] = ((k: K, v: V) => {
    val res = mapper(k, v)
    new KeyValue[KR, VR](res._1, res._2)
  }).asKeyValueMapper
  inner.map[KR, VR](mapperJ)
}
```"
382391983,4756,ijuma,2018-04-18T13:46:42Z,"My point is that there's still a throwaway tuple for each mapped item which is the issue @guozhangwang raised. The GC is generally good at dealing with such short lived objects, so not sure if it matters in this case, but it's clearly adding some overhead when compared to the Java implementation."
382401057,4756,debasishg,2018-04-18T14:13:16Z,"Yeah .. the point is this tuple is generated within the `map` function through the call of the `mapper`. To avoid this, the way out will be to change the `mapper` to `(K, V) => KeyValue[KR, VR]`. Which can be done .. but makes the API contract a bit noisy. OTOH we can rely on the JIT and the ability of GC to deal with such short lived objects. Suggestions welcome :-)"
382462315,4756,guozhangwang,2018-04-18T17:16:39Z,"@debasish Maybe I was a bit paranoid on the GC pressure before, just raised it as a concern. Since you mentioned `We did run some tests in bulk to check the diff in performance between the 2 versions. Couldn't find much of a difference though.` I'm fine with keeping the API as elegant as of now and assume JIT doing the right thing."
383103073,4756,debasishg,2018-04-20T13:53:05Z,retest this please
383113457,4756,ijuma,2018-04-20T14:26:31Z,"Out of curiosity, how long does it take to build (compile and run the tests) kafka-streams-scala."
383122538,4756,debasishg,2018-04-20T14:54:20Z,"@ijuma 

```
$ ./gradlew -Dtest.single=*Test* streams:streams-scala:test

> Configure project : 
Building project 'core' with Scala version 2.11.12
Building project 'streams-scala' with Scala version 2.11.12

> Task :streams:streams-scala:test 
org.apache.kafka.streams.scala.TopologyTest > shouldBuildIdenticalTopologyInJavaNScalaJoin PASSED

org.apache.kafka.streams.scala.TopologyTest > shouldBuildIdenticalTopologyInJavaNScalaSimple PASSED

org.apache.kafka.streams.scala.TopologyTest > shouldBuildIdenticalTopologyInJavaNScalaAggregate PASSED

org.apache.kafka.streams.scala.WordCountTest > testShouldCountWordsJava PASSED

org.apache.kafka.streams.scala.WordCountMergeTest > testShouldCountWords PASSED

org.apache.kafka.streams.scala.StreamToTableJoinScalaIntegrationTestImplicitSerdes > testShouldCountClicksPerRegionJava PASSED

org.apache.kafka.streams.scala.WordCountTest > testShouldCountWords PASSED

org.apache.kafka.streams.scala.StreamToTableJoinScalaIntegrationTestImplicitSerdes > testShouldCountClicksPerRegion PASSED

BUILD SUCCESSFUL in 37s
```"
385124698,4756,ijuma,2018-04-28T00:26:12Z,"I have been unable to reply to this PR since it has continuously failed to
load for me for 1 week. So I am resorting to ""reply by email"". :)

""@ijuma <https://github.com/ijuma> how are multiple versions of Kafka core
published at part of the release process? Is the build script called twice
with appropriate scalaVersion parameter?""

We currently invoke it twice with -PscalaVersion=2.12 for the second
invocation because we haven't enabled Scala 2.12 builds by default. The
reason is that Scala 2.12 requires Java 8. Once we switch our build to
require Java 8 (should happen soon), we will enable 2.12 by default and
then a single command with the *All suffix will do it for all Scala
versions supported. To give an example, `test` will run the tests with the
default Scala version while `testAll` will run the tests with all supported
Scala versions.

For what it's worth, I strongly agree that we should do the same we do for
core, i.e. publish for both Scala 2.11 and Scala 2.12 and include the Scala
version in the artifact id, as it's standard practice for Scala libraries.

Ismael

On Wed, Apr 25, 2018 at 1:21 PM, Sean Glover <notifications@github.com>
wrote:

> *@seglo* commented on this pull request.
> ------------------------------
>
> In docs/streams/index.html
> <https://github.com/apache/kafka/pull/4756#discussion_r184193680>:
>
> > +import org.apache.kafka.streams.kstream.Materialized
> +import org.apache.kafka.streams.scala.kstream._
> +import org.apache.kafka.streams.{KafkaStreams, StreamsConfig}
> +
> +object WordCountApplication extends App {
> +  import DefaultSerdes._
> +  import ImplicitConversions._
> +
> +  val config: Properties = {
> +    val p = new Properties()
> +    p.put(StreamsConfig.APPLICATION_ID_CONFIG, ""wordcount-application"")
> +    p.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, ""kafka-broker1:9092"")
> +    p
> +  }
> +
> +  val builder: StreamsBuilder = new StreamsBuilder()
>
> Yes. I'll test the snippets in the build PR.
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/kafka/pull/4756#discussion_r184193680>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AABgq2Xsaj25ViUqKl-yS6dxK1zDwkq2ks5tsNq4gaJpZM4S3B3->
> .
>
"
222823006,1446,aartigupta,2016-05-31T21:14:52Z,"@guozhangwang, what do you think? I was able to run the examples and see the metrics per node in a jmx console.
"
222835496,1446,guozhangwang,2016-05-31T22:05:17Z,"Thanks @aartigupta , @enothereska could you take a look first at this ticket? I have assigned you as the reviewer on the ticket, and please feel free to re-assign to me otherwise.
"
223304691,1446,enothereska,2016-06-02T14:14:46Z,"@aartigupta perhaps the PR name should be ""KAFKA-3715: add granular metrics per node""? The JIRA number is usually part of the PR name. Minor thing but just for consistency.
"
224061436,1446,enothereska,2016-06-06T19:25:36Z,"Thanks @aartigupta . Two higher level questions: does it make sense to add a unit test or two for the new metrics? And do we have any overhead measurements in the sense of how much to the new recordings add to the end to end latency?
"
225488213,1446,aartigupta,2016-06-13T04:43:05Z,"<img width=""1011"" alt=""simplebenchmarknostatestorenochanges"" src=""https://cloud.githubusercontent.com/assets/3031383/15997257/c1657140-30e6-11e6-8994-beba3b4470a2.png"">
<img width=""1279"" alt=""simplebenchmarkpernodemetrics"" src=""https://cloud.githubusercontent.com/assets/3031383/15997258/c165b9f2-30e6-11e6-9e52-f8e14c2eda3c.png"">

Ran org.apache.kafka.streams.perf.SimpleBenchmark with the following configuration (i.e. without state store backed streams and simple print statements indicating which part of the benchmark is being run)

```
   System.out.println(""producer"");
    benchmark.produce();
    System.out.println(""consumer "");
    benchmark.consume();
    System.out.println(""simple stream performance source->process"");
    // simple stream performance source->process
    benchmark.processStream();
    System.out.println(""simple stream performance source->sink"");
    // simple stream performance source->sink
    benchmark.processStreamWithSink();
    // simple stream performance source->store
```

//        benchmark.processStreamWithStateStore();

then attached a yourkit profiler and saw the following differences (see attached screenshots)

without any changes to the code and using CPU Sampling in yourkit saw 61% cpu contention
with the per node metrics and using CPU  sampling in yourkit, saw 70% cpu contention, 

without any changes

org.apache.kafka.streams.perf.SimpleBenchmark
producer
Producer Performance [MB/sec write]: 8.853212193170378
consumer 
[YourKit Java Profiler 2016.02-b38] Log file: /Users/aartikumargupta/.yjp/log/SimpleBenchmark-1754.log
Consumer Performance [MB/sec read]: 4.596191726854892
simple stream performance source->process
Streams Performance [MB/sec read]: 14.361679855964493
simple stream performance source->sink
Streams Performance [MB/sec read+write]: 4.535097423059803

with node metrics
Producer Performance [MB/sec write]: 5.035256582778346
consumer 
[YourKit Java Profiler 2016.02-b38] Log file: /Users/aartikumargupta/.yjp/log/SimpleBenchmark-1549.log
Consumer Performance [MB/sec read]: 2.751484036579496
simple stream performance source->process
Streams Performance [MB/sec read]: 8.014018691588785
simple stream performance source->sink
Streams Performance [MB/sec read+write]: 6.562667414985077

Ran this multiple times and the results varied between 63%(no changes) and 72%(with per node metrics) The difference seems to be around the point at which yourkit profiler is attached 

That said, not sure if this is a valid load simulating scenario
@guozhangwang mentions in https://github.com/apache/kafka/pull/1490 that 

> > if your traffic is very small and consumer is already at the log tail throughout your test, it will cause the polling / processing to be called with less batched data and hence further increased overhead.

@guozhangwang Is the simpleBenchmark a good scenario to be profiling ?
If not any suggestions on another scenario, maybe we can add (check in) such a scenario under examples, which can be used for all similar future profiling exercises

Still working on the unit tests for per node metrics.
"
227377187,1446,gfodor,2016-06-21T08:41:38Z,"hey @aartigupta it's kind of hard to tell based on your screenshots where the time is going since I don't see any drilldown into the call stacks of the StreamThread run loops. It's probably necessary for you to flip things on in the YourKit profiler so you can get the full call stacks and determine if `Sensor.record` is the source of most of the time.
"
227602467,1446,guozhangwang,2016-06-21T23:29:08Z,"Thanks @aartigupta , some general comments:
1. For naming consistency as with other metrics objects, for finer grained metrics we tend to name the sensors as ""level-name.level-id.metrics-name"", for example in `SenderMetrics` we used `topic.[topic-name].records-per-batch` etc for per topic-level metrics and in `SelectorMetrics` we used `node-[node-id].bytes-sent` etc for per node-level metrics, and in my latest PR #1530 I was doing similar naming. You may already notice that this is for creating different sensors as we synchronize at the per-sensor basis, and since in producer / consumer we always has single-thread, today we do not have any contentions for the lock yet, and in Streams we are trying to add per-thread metrics and consider adding global metrics only after the syncrhonization is removed in KAFKA-3155 since as we have discussed in other PRs with multiple threads contention overhead can be large.
2. Different metrics reporter has the freedom of constructing their reporting metrics name from the hierarchy of ""metrics-prefix, group-name, metrics-name, metrics-tags"" where metrics-prefix are ""kafka.producer"" / ""kafka.consumer"" / ""kafka.streams"" depending on which client library you are using. And in this case the sensor names are actually ignored as they are used internally of the metrics object for grouping different metrics only. For example in `JmxReporter` we create the mbeanName / attributeName as

```
mbean: ""metrics-prefix"": type=""group-name"", ""tag1key""=""tag1value"", ..., ""tagNkey""=""tagNvalue""
    attribute1: ""metrics-name1""
    attribute2: ""metrics-name2""
    ...
```

So we need to make sure that the hierarchy is sufficient for different reporters to differentiate these metrics in their own space.
"
227604083,1446,guozhangwang,2016-06-21T23:39:13Z,"Btw the `SimpleBenchmark` numbers are pretty low compared to my laptop (4GB memory, and low-end CPUs). What environment did you run the profiler?
"
228123517,1446,aartigupta,2016-06-23T17:33:13Z,"@guozhangwang     Mackbook 12 inch 2015 early edition, 1.3GHz dual-core Intel Core M processor (Turbo Boost up to 2.9GHz) with 4MB shared L3 cache.
8GB of 1600MHz LPDDR3 onboard memory
I think that it has to do with attaching yourkit profiler.
Without the profiler I get the following 

producer
Producer Performance [MB/sec write]: 22.247686586525987
consumer 
Consumer Performance [MB/sec read]: 56.39283169836138
simple stream performance source->process
Streams Performance [MB/sec read]: 40.33237957119899
simple stream performance source->sink
Streams Performance [MB/sec read+write]: 18.71113212350438

Process finished with exit code 0
"
229234020,1446,theduderog,2016-06-29T01:43:32Z,"Is there a way to register user-defined metrics?
"
263255635,1446,enothereska,2016-11-28T12:10:00Z,@aartigupta would you still have time for this PR or should I have a look? Thanks.
264912720,1446,enothereska,2016-12-05T17:06:54Z,"![picture1](https://cloud.githubusercontent.com/assets/14234336/20894377/10456c6a-bb0d-11e6-81f9-bbae1debae8e.png)

Added two levels of logging metrics, as described in KAFKA-3811. Showing performance of PR when logging all metrics, vs just some higher level metrics in streams (see last two bars of each benchmark, the rest is details). Average of 3 runs for each test shown.

All data at: [sensors-perf.xlsx](https://github.com/apache/kafka/files/698656/sensors-perf.xlsx)
"
264913402,1446,enothereska,2016-12-05T17:09:14Z,"@ijuma @guozhangwang As part of this PR we are introducing different levels of logging for sensors, as initially described in KAFKA-3811 https://issues.apache.org/jira/browse/KAFKA-3811?focusedCommentId=15324669. Could you see if you are satisfied with the way these levels are introduced? In particular, @ijuma I'd focus on everything outside of streams, while @guozhangwang can focus on the streams part too. Thanks. cc @ewencp @gwenshap too if they have time for looking at the sensor changes. Thanks."
264927773,1446,enothereska,2016-12-05T18:01:45Z,(PR failures due to known Reset tests)
264964781,1446,enothereska,2016-12-05T20:17:54Z,@theduderog registering user-defined metrics will be in another JIRA. Thanks.
265074848,1446,aartiguptaa,2016-12-06T06:50:30Z,"@enothereska Looks good! 
Thanks for cleaning this up and adding the missing metrics. When I took this on earlier,  Jay mentioned on a related jira (https://issues.apache.org/jira/browse/KAFKA-3811), that the usage of metrics in streams incurs an overhead because we walk through a list of sensors inside a synchronized method. When I ran the code in the PR and the simple benchmark tests, and looked at the hrof counter recordings using yourkit profiler, I observed a 5 percent CPU overhead with my fix as compared to without it. Wondering now if that was a measurement error/ test setup error?
Out of curiosity, can you describe your perf testing experiment.
Did you use simple benchmark for the perf counters? , can you share the example use case /automated test that you ran? How many nodes? How many tasks, topology , and which profiler tool, flight recorder/yourkit profiler?
"
265104382,1446,enothereska,2016-12-06T09:39:37Z,"Thanks @aartigupta 

I did use yourkit, but the final numbers I put on the PR are using SimpleBenchmark.java that is included with Streams. That runs locally on my MacBook Pro 16GB, SSD, dual core. So on one node we have Kafka and the SimpleBenchmark application. I don't think yours was a measurement error, but some metrics were missing back then.
"
265452922,1446,enothereska,2016-12-07T13:56:27Z,Test passed. Weird error on jenkins: `hudson.remoting.Channel@8690f26:ubuntu-4: java.io.IOException: Remote call on ubuntu-4 failed`
265559198,1446,enothereska,2016-12-07T20:09:22Z,@dguy @mjsax if you have time.
265830060,1446,guozhangwang,2016-12-08T19:23:26Z,"@aartigupta @enothereska About the synchronization overhead, I think Jay was referring to KAFKA-3769 and I have fixed in some time ago (you can see the graphs before and after the fix): https://github.com/apache/kafka/pull/1530"
265839772,1446,asfbot,2016-12-08T20:01:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/14/
Test FAILed (JDK 8 and Scala 2.12).
"
265841016,1446,asfbot,2016-12-08T20:06:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/13/
Test PASSed (JDK 7 and Scala 2.10).
"
265886855,1446,asfbot,2016-12-08T23:26:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/15/
Test FAILed (JDK 8 and Scala 2.11).
"
265981553,1446,asfbot,2016-12-09T10:25:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/29/
Test FAILed (JDK 8 and Scala 2.11).
"
265981865,1446,asfbot,2016-12-09T10:27:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/27/
Test FAILed (JDK 7 and Scala 2.10).
"
265988991,1446,asfbot,2016-12-09T11:05:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/28/
Test FAILed (JDK 8 and Scala 2.12).
"
266430444,1446,asfbot,2016-12-12T13:26:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/73/
Test FAILed (JDK 7 and Scala 2.10).
"
266431232,1446,asfbot,2016-12-12T13:30:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/74/
Test FAILed (JDK 8 and Scala 2.12).
"
266431326,1446,asfbot,2016-12-12T13:31:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/75/
Test FAILed (JDK 8 and Scala 2.11).
"
266478150,1446,asfbot,2016-12-12T16:31:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/78/
Test FAILed (JDK 8 and Scala 2.11).
"
266479987,1446,asfbot,2016-12-12T16:37:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/76/
Test FAILed (JDK 7 and Scala 2.10).
"
266492829,1446,asfbot,2016-12-12T17:21:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/77/
Test PASSed (JDK 8 and Scala 2.12).
"
266758773,1446,guozhangwang,2016-12-13T14:53:47Z,"@enothereska Thinking about it a bit more, I now understand your arguments before: tags were not used to distinguish sensors in the sensor registry (i.e. `ConcurrentMap<String, Sensor> sensors`), so without the processor-node as part of the sensor name they will collapse into the same sensor, that can be potentially accessed by multiple threads.

In that case, you are right that we do need to have both the processor node name as well as the task id string as part of the `NodeMetrics` sensors, and have the task id string as the `TaskMetrics` sensors. 

Your current naming as the format of

```
thread.streams-thread-XXX.task.1-1PROCESSOR-XXX-processor-throughput
```

where `thread.THREAD-ID` comes from the prefix in `StreamsMetricsImpl`. And there is no separators between `taskId` and `processor name`, and we use the `-` separator between `processor name` and the actually sensor name. I am wondering if we could remove this prefix and refactor the rest prefixes as

```
task.1-1.PROCESSOR-XXX.processor-throughput
```

The reason is that tasks can migrated between threads from time to time, and when that happens some of the existing sensor will not have any data any more while some more sensors need to be created if we keep the thread id as the prefix, and for using the `.` separator it is mainly for consistency with other sensors. 

Ditto for task metrics as well."
266827021,1446,enothereska,2016-12-13T18:52:35Z,"@guozhangwang about your latest comment, when a task migrates, when we close the topology, I remove all previous sensors. So when the task migrates it will only have new sensors. The old sensors will be gone. Let me know what you think. Thanks."
266848739,1446,asfbot,2016-12-13T20:14:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/111/
Test PASSed (JDK 8 and Scala 2.11).
"
266849073,1446,asfbot,2016-12-13T20:16:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/109/
Test FAILed (JDK 7 and Scala 2.10).
"
266860793,1446,asfbot,2016-12-13T21:01:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/110/
Test PASSed (JDK 8 and Scala 2.12).
"
266873164,1446,asfbot,2016-12-13T21:50:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/115/
Test PASSed (JDK 8 and Scala 2.11).
"
266873303,1446,asfbot,2016-12-13T21:51:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/114/
Test PASSed (JDK 8 and Scala 2.12).
"
266873779,1446,asfbot,2016-12-13T21:53:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/113/
Test PASSed (JDK 7 and Scala 2.10).
"
267017054,1446,asfbot,2016-12-14T12:01:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/135/
Test PASSed (JDK 8 and Scala 2.12).
"
267017196,1446,asfbot,2016-12-14T12:02:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/136/
Test PASSed (JDK 8 and Scala 2.11).
"
267045359,1446,asfbot,2016-12-14T14:21:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/134/
Test FAILed (JDK 7 and Scala 2.10).
"
267309387,1446,enothereska,2016-12-15T11:49:09Z,Will be adding unit tests shortly.
267433279,1446,asfbot,2016-12-15T20:20:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/171/
Test PASSed (JDK 8 and Scala 2.12).
"
267438706,1446,asfbot,2016-12-15T20:43:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/170/
Test FAILed (JDK 7 and Scala 2.10).
"
267451583,1446,asfbot,2016-12-15T21:37:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/172/
Test PASSed (JDK 8 and Scala 2.11).
"
267466910,1446,guozhangwang,2016-12-15T22:45:59Z,"@enothereska What I meant is mainly around user experience: by having task / processor-node level metrics to be distinguished by the thread name (e.g. define sensor name as  `thread.streams-thread-XXX.task.1-1.PROCESSOR-XXX-processor-throughput`), users then need to enumerate all possibly created sensors in their monitoring system and / or web UI, while only one of them will be valid / having non-blank graphs at a given point of time; on the other hand, since at any given time only one thread will be owning the task / processor node, it is safe to collapse them into a single sensor that could be accessed and updated by different threads at different times, and users then can simply polling / monitoring this single sensor (e.g. `task.1-1.PROCESSOR-XXX-processor-throughput`)."
267954921,1446,asfbot,2016-12-19T12:40:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/234/
Test FAILed (JDK 7 and Scala 2.10).
"
267954936,1446,asfbot,2016-12-19T12:40:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/235/
Test PASSed (JDK 8 and Scala 2.12).
"
267954980,1446,asfbot,2016-12-19T12:40:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/236/
Test PASSed (JDK 8 and Scala 2.11).
"
268056393,1446,enothereska,2016-12-19T19:32:58Z,@guozhangwang I agree with you on removing the thread prefix. Will do so with next commit. Thanks.
268058592,1446,asfbot,2016-12-19T19:42:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/242/
Test FAILed (JDK 7 and Scala 2.10).
"
268067715,1446,asfbot,2016-12-19T20:20:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/243/
Test PASSed (JDK 8 and Scala 2.12).
"
268067908,1446,asfbot,2016-12-19T20:21:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/244/
Test PASSed (JDK 8 and Scala 2.11).
"
268075855,1446,guozhangwang,2016-12-19T20:55:46Z,"@enothereska Is this relevant?

```

* Where:
Build file '/home/jenkins/jenkins-slave/workspace/kafka-pr-jdk7-scala2.10/build.gradle' line: 385

* What went wrong:
A problem occurred evaluating root project 'kafka-pr-jdk7-scala2.10'.
> Failed to apply plugin [id 'org.scoverage']
   > Could not create an instance of type org.scoverage.ScoverageExtension_Decorated.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output.

```"
268085496,1446,enothereska,2016-12-19T21:35:25Z,"@guozhangwang I don't think so, looks like an environment one."
268106260,1446,enothereska,2016-12-19T23:13:24Z,"![graph-19-dec-2016](https://cloud.githubusercontent.com/assets/14234336/21332338/948cf442-c63f-11e6-94e5-3df8eb2aa754.png)

Updated performance graph (avg of 3 runs). A couple of takeaways:

- Trunk and PR have comparable performance in the ""no metrics recorded"" case. Worse case difference is around 2% better for trunk in one case and 5% better for PR in another case.

- Both trunk and PR see a similar drop in performance when collecting some metrics (INFO)

- Notice how the PR's INFO metrics does better than Trunk's metrics. That is because in the PR the state store metrics are now DEBUG, so we're collecting fewer metrics by default. I'll get another graph where the PR temporarily assigns INFO to the store metrics, for a better apple-to-apple comparison.
- Worst case scenario drop in perf for DEBUG: 60%. Best case drop: 20%

- Worst case drop in perf for INFO: 11%. Best case drop: 3% (compare with current trunk below).

- Worst case drop in perf for trunk metrics: 17%. Best case drop: 9%.

- Not relevant for this PR, but looks like all numbers have moved up from the last graph. That's good.
"
268108598,1446,guozhangwang,2016-12-19T23:26:24Z,"Thanks for the newly updated stats @enothereska . A few quick question:

0. For the comparison you mentioned above ""worst case .. best case"", they are all compared with trunk with metrics turned on right?

1. Regarding `Worst case drop in perf for INFO: 11%. Best case drop: 3% (compare with current trunk below).` since you mention this PR's INFO metrics perf is better than trunk's metrics, I was assuming that the best case perf drop should be a negative ratio?

2. Regarding `Worst case drop in perf for trunk metrics: 17%. Best case drop: 9%.` What does this mean? I thought for `trunk metrics` it means INFO + state store metrics, but it seems you have not got the results for this yet.

"
268219056,1446,enothereska,2016-12-20T11:20:13Z,"@guozhangwang not quite:

0. No, they are not compared with trunk, they are compared with the PR with metrics turned off. 

1. Again, not compared with trunk, just with no metrics. Sure, if you compare with trunk you'd get the negative ratio.

2. This is trunk today, so yes, it is INFO + state store metrics. I'm just pointing out that their cost is between 9-17% today. Compared with no metrics. "
268227910,1446,enothereska,2016-12-20T12:09:56Z,"![graph-20-dec-2016](https://cloud.githubusercontent.com/assets/14234336/21350018/10d1d7b2-c6ad-11e6-836a-054646bdbe86.png)

Updated with an additional run where we collect both INFO + state store metrics (just like trunk does now). Overall, the main takeaway is that we're not introducing any particular overhead with the PR with an apples-to-apples comparison with Trunk. 

@guozhangwang at this point I'm personally happy with the numbers. Thanks."
268304113,1446,enothereska,2016-12-20T17:26:23Z,Thanks @ijuma I think I addressed your comments.
268304198,1446,asfbot,2016-12-20T17:26:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/273/
Test FAILed (JDK 8 and Scala 2.11).
"
268304208,1446,asfbot,2016-12-20T17:26:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/271/
Test FAILed (JDK 7 and Scala 2.10).
"
268304451,1446,asfbot,2016-12-20T17:27:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/272/
Test FAILed (JDK 8 and Scala 2.12).
"
268313617,1446,asfbot,2016-12-20T18:04:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/274/
Test FAILed (JDK 8 and Scala 2.11).
"
268314676,1446,asfbot,2016-12-20T18:09:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/273/
Test PASSed (JDK 8 and Scala 2.12).
"
268316614,1446,asfbot,2016-12-20T18:17:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/272/
Test PASSed (JDK 7 and Scala 2.10).
"
268346907,1446,asfbot,2016-12-20T20:16:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/280/
Test PASSed (JDK 8 and Scala 2.12).
"
268347530,1446,asfbot,2016-12-20T20:18:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/281/
Test PASSed (JDK 8 and Scala 2.11).
"
268348130,1446,asfbot,2016-12-20T20:21:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/279/
Test PASSed (JDK 7 and Scala 2.10).
"
270527621,1446,guozhangwang,2017-01-05T00:28:31Z,"@aartigupta @enothereska Some of the comments seem not addressed yet, and could you rebase as well?"
270619070,1446,asfbot,2017-01-05T11:02:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/518/
Test FAILed (JDK 8 and Scala 2.11).
"
270619470,1446,enothereska,2017-01-05T11:04:34Z,"@guozhangwang I removed the extra sensor registration APIs as you suggested, and I also agree with the rest of your comments and have addressed them. Thank you."
270620026,1446,enothereska,2017-01-05T11:07:40Z,"@guozhangwang I removed the extra sensor registration APIs as you suggested, and I also agree with the rest of your comments and have addressed them. Thank you."
270626625,1446,asfbot,2017-01-05T11:47:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/521/
Test PASSed (JDK 8 and Scala 2.11).
"
270633321,1446,asfbot,2017-01-05T12:28:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/524/
Test FAILed (JDK 8 and Scala 2.11).
"
270633385,1446,asfbot,2017-01-05T12:28:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/523/
Test FAILed (JDK 8 and Scala 2.12).
"
270642475,1446,asfbot,2017-01-05T13:21:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/529/
Test PASSed (JDK 8 and Scala 2.11).
"
270642500,1446,asfbot,2017-01-05T13:21:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/528/
Test PASSed (JDK 8 and Scala 2.12).
"
270645468,1446,asfbot,2017-01-05T13:36:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/522/
Test FAILed (JDK 7 and Scala 2.10).
"
270649120,1446,asfbot,2017-01-05T13:55:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/527/
Test PASSed (JDK 7 and Scala 2.10).
"
270651675,1446,asfbot,2017-01-05T14:08:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/520/
Test FAILed (JDK 8 and Scala 2.12).
"
271024649,1446,asfbot,2017-01-06T22:16:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/583/
Test PASSed (JDK 8 and Scala 2.12).
"
271025900,1446,asfbot,2017-01-06T22:18:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/584/
Test PASSed (JDK 8 and Scala 2.11).
"
271039039,1446,asfbot,2017-01-06T23:34:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/582/
Test PASSed (JDK 7 and Scala 2.10).
"
271050182,1446,asfbot,2017-01-07T01:07:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/592/
Test PASSed (JDK 8 and Scala 2.12).
"
271055440,1446,asfbot,2017-01-07T02:09:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/593/
Test FAILed (JDK 8 and Scala 2.11).
"
271055618,1446,asfbot,2017-01-07T02:12:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/591/
Test FAILed (JDK 7 and Scala 2.10).
"
271103036,1446,enothereska,2017-01-07T19:04:31Z,"Failures unrelated to PR, e.g., kafka.api.SslProducerSendTest.testSendCompressedMessageWithCreateTime"
271233493,1446,asfbot,2017-01-09T08:46:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/615/
Test PASSed (JDK 8 and Scala 2.11).
"
271241336,1446,asfbot,2017-01-09T09:32:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/613/
Test FAILed (JDK 7 and Scala 2.10).
"
271247856,1446,asfbot,2017-01-09T10:07:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/614/
Test PASSed (JDK 8 and Scala 2.12).
"
271259703,1446,asfbot,2017-01-09T11:11:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/616/
Test PASSed (JDK 8 and Scala 2.11).
"
271260949,1446,asfbot,2017-01-09T11:18:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/614/
Test PASSed (JDK 7 and Scala 2.10).
"
271268466,1446,asfbot,2017-01-09T12:02:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/615/
Test PASSed (JDK 8 and Scala 2.12).
"
271372167,1446,asfbot,2017-01-09T18:55:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/633/
Test PASSed (JDK 8 and Scala 2.11).
"
271372175,1446,asfbot,2017-01-09T18:55:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/632/
Test PASSed (JDK 8 and Scala 2.12).
"
271394648,1446,asfbot,2017-01-09T20:14:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/631/
Test PASSed (JDK 7 and Scala 2.10).
"
271408633,1446,asfbot,2017-01-09T21:10:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/644/
Test PASSed (JDK 8 and Scala 2.11).
"
271408765,1446,asfbot,2017-01-09T21:10:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/643/
Test PASSed (JDK 8 and Scala 2.12).
"
271410851,1446,asfbot,2017-01-09T21:19:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/642/
Test PASSed (JDK 7 and Scala 2.10).
"
271436295,1446,guozhangwang,2017-01-09T23:04:19Z,"@enothereska @aartigupta Made another pass over the latest patch. LGTM overall. Just one minor comment about testing coverage: would you consider adding some test cases for `StreamMetricsImpl` inside `StreamThreadTest` as well, for testing is naming conventions, etc?"
271549835,1446,asfbot,2017-01-10T11:15:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/662/
Test PASSed (JDK 8 and Scala 2.11).
"
271555646,1446,asfbot,2017-01-10T11:47:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/661/
Test PASSed (JDK 8 and Scala 2.12).
"
271555756,1446,asfbot,2017-01-10T11:47:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/660/
Test PASSed (JDK 7 and Scala 2.10).
"
271558246,1446,asfbot,2017-01-10T12:00:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/663/
Test FAILed (JDK 7 and Scala 2.10).
"
271580329,1446,asfbot,2017-01-10T13:52:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/666/
Test FAILed (JDK 8 and Scala 2.11).
"
271582616,1446,asfbot,2017-01-10T14:02:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/664/
Test PASSed (JDK 7 and Scala 2.10).
"
271583554,1446,asfbot,2017-01-10T14:06:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/665/
Test PASSed (JDK 8 and Scala 2.12).
"
271605126,1446,asfbot,2017-01-10T15:28:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/670/
Test PASSed (JDK 8 and Scala 2.12).
"
271618831,1446,asfbot,2017-01-10T16:13:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/669/
Test FAILed (JDK 7 and Scala 2.10).
"
271620265,1446,asfbot,2017-01-10T16:17:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/671/
Test PASSed (JDK 8 and Scala 2.11).
"
271652489,1446,asfbot,2017-01-10T18:12:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/675/
Test FAILed (JDK 7 and Scala 2.10).
"
271653716,1446,asfbot,2017-01-10T18:16:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/674/
Test PASSed (JDK 8 and Scala 2.12).
"
271654008,1446,asfbot,2017-01-10T18:18:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/675/
Test PASSed (JDK 8 and Scala 2.11).
"
271662250,1446,asfbot,2017-01-10T18:49:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/676/
Test PASSed (JDK 8 and Scala 2.12).
"
271662704,1446,asfbot,2017-01-10T18:51:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/677/
Test FAILed (JDK 8 and Scala 2.11).
"
271662859,1446,guozhangwang,2017-01-10T18:52:10Z,test this please
271671814,1446,enothereska,2017-01-10T19:24:11Z,@guozhangwang what do you mean by `test this please`? Which part? Thanks.
271674358,1446,asfbot,2017-01-10T19:33:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/680/
Test PASSed (JDK 8 and Scala 2.12).
"
271674969,1446,asfbot,2017-01-10T19:35:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/681/
Test FAILed (JDK 8 and Scala 2.11).
"
271675126,1446,asfbot,2017-01-10T19:36:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/673/
Test PASSed (JDK 7 and Scala 2.10).
"
271675184,1446,guozhangwang,2017-01-10T19:36:40Z,"@enothereska 

https://wiki.jenkins-ci.org/display/JENKINS/GitHub+pull+request+builder+plugin :)"
271679307,1446,enothereska,2017-01-10T19:51:48Z,wow!
271681499,1446,asfbot,2017-01-10T20:00:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/685/
Test FAILed (JDK 8 and Scala 2.12).
"
271681953,1446,enothereska,2017-01-10T20:02:04Z,Unrelated error: kafka.integration.UncleanLeaderElectionTest.testUncleanLeaderElectionDisabled
271682911,1446,asfbot,2017-01-10T20:05:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/687/
Test PASSed (JDK 8 and Scala 2.11).
"
271683074,1446,asfbot,2017-01-10T20:06:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/679/
Test PASSed (JDK 7 and Scala 2.10).
"
271684160,1446,asfbot,2017-01-10T20:10:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/685/
Test FAILed (JDK 7 and Scala 2.10).
"
271704650,1446,asfbot,2017-01-10T21:33:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/695/
Test PASSed (JDK 8 and Scala 2.12).
"
271704729,1446,asfbot,2017-01-10T21:33:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/697/
Test PASSed (JDK 8 and Scala 2.11).
"
271705130,1446,asfbot,2017-01-10T21:35:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/695/
Test FAILed (JDK 7 and Scala 2.10).
"
271824683,1446,asfbot,2017-01-11T09:50:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/721/
Test PASSed (JDK 7 and Scala 2.10).
"
271829127,1446,asfbot,2017-01-11T10:09:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/721/
Test FAILed (JDK 8 and Scala 2.12).
"
271829959,1446,asfbot,2017-01-11T10:13:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/723/
Test FAILed (JDK 8 and Scala 2.11).
"
271844800,1446,asfbot,2017-01-11T11:25:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/729/
Test FAILed (JDK 8 and Scala 2.11).
"
271844828,1446,asfbot,2017-01-11T11:25:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/727/
Test FAILed (JDK 8 and Scala 2.12).
"
271844878,1446,asfbot,2017-01-11T11:25:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/727/
Test FAILed (JDK 7 and Scala 2.10).
"
271852774,1446,asfbot,2017-01-11T12:08:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/730/
Test PASSed (JDK 8 and Scala 2.11).
"
271861919,1446,asfbot,2017-01-11T12:58:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/728/
Test PASSed (JDK 8 and Scala 2.12).
"
271867629,1446,asfbot,2017-01-11T13:26:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/728/
Test PASSed (JDK 7 and Scala 2.10).
"
271949565,1446,asfbot,2017-01-11T18:17:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/745/
Test PASSed (JDK 8 and Scala 2.11).
"
271950927,1446,asfbot,2017-01-11T18:21:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/743/
Test FAILed (JDK 7 and Scala 2.10).
"
271951164,1446,enothereska,2017-01-11T18:22:42Z,unrelated: kafka.api.SslProducerSendTest.testCloseWithZeroTimeoutFromSenderThread
271969606,1446,enothereska,2017-01-11T19:29:48Z,The old org.apache.kafka.streams.integration.ResetIntegrationTest failure is back but shouldn't be related to PR.
271977388,1446,guozhangwang,2017-01-11T19:59:40Z,Merged to trunk. Many thanks to @aartigupta and @enothereska !!
271986174,1446,asfbot,2017-01-11T20:35:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/743/
Test FAILed (JDK 8 and Scala 2.12).
"
395236826,5101,hzxa21,2018-06-06T22:42:10Z,"@lindong28 Dong, I have updated the PR to address the comments. Could you take a second look? Thanks!"
406153652,5101,lindong28,2018-07-19T04:48:45Z,"Hey @hzxa21, is this patch ready for review?"
406507085,5101,hzxa21,2018-07-20T06:57:20Z,@lindong28 Yes. Can you help take a look?
406659794,5101,lindong28,2018-07-20T16:50:12Z,@hzxa21 Sure. Can you rebase the patch to resolve the conflict?
406738020,5101,hzxa21,2018-07-20T22:01:50Z,@lindong28 Rebased onto trunk. Thanks Dong!
412941942,5101,hzxa21,2018-08-14T16:55:52Z,@lindong28 I have updated the PR to add more logging and address the naming issues. Could you take a look again? Thanks!
414773731,5101,hzxa21,2018-08-21T18:22:43Z,@lindong28 Thanks for your comments. I have rebased the patch. Can you take a look?
415120454,5101,hzxa21,2018-08-22T17:51:15Z,Thanks Dong for the review. I have addressed the comments and will add more tests for the controller behavior.
415132003,5101,hzxa21,2018-08-22T18:26:44Z,Rebased.
415160017,5101,lindong28,2018-08-22T19:59:14Z,"Hey @junrao, I have finished reviewing this patch except tests. I will wait for @hzxa21 to add tests. Would you like to review this patch as well?"
415241065,5101,junrao,2018-08-23T01:18:44Z,"@hzxa21 : Another thing is that I am wondering if you have done any perf testing. We probably don't expect any slowdown with the additional controller epoch check. However, it would be useful to verify that common operations such as controlled shutdown, broker startup, leader balancing are not slower after this patch."
415954168,5101,hzxa21,2018-08-25T08:48:10Z,@junrao @lindong28 Thanks for the comments. I have added some controller integration tests for the patch and address the comments. I will do some perf testing and post the results once I get the numbers.
415955473,5101,hzxa21,2018-08-25T09:12:44Z,"Also, we didn't have the protection against a stale controller deleting `\controller` znode when `ControllerMovedException` is thrown in `onControllerFailOver()` during controller initialization. This can cause controller switch storm. Consider the following events:
1. broker A creates /controller znode
2. broker A reads the controller epoch zkVersion
3. broker A lose its zk session -> /controller node gets deleted
4. broker B creates /controller znode
5. broker B reads the controller epoch zkVersion
6. broker B increments the controller epoch and bumps the zkVersion of /controller_epoch znode
7. broker A tries to update zookeeper (e.g. update `/admin/reassign_partitions`) but fails because /controller_epoch zkVersion mismatch. A ControllerMovedException is thrown.
8. broker A removes the /controller znode (currently owned by broker B) because we didn't explicitly catch `ControllerMovedException` in `elect()` and trigger a controller move in the following code block:
```
try {
  ...
  onControllerFailOver()
} catch {
  ...
  case e2: Throwable =>
        error(""Error while electing or becoming controller on broker %d"".format(config.brokerId), e2)
        triggerControllerMove()
}
```
9. broker C creates /controller znode
...

This loop may never end and the controller role will keep switching across brokers without making any progress. I have also updated the PR to include the fixes:
1. Catch `ControllerMovedException` in `elect()` without triggering the controller move. This makes sense because when the `ControllerMovedException` is thrown, the new controller has been elected and there is no need to trigger another round of controller election.
2. Guard against the `\controller` deletion by controller epoch zkVersion. This ensures stale controller cannot delete `\controller` znode."
416999642,5101,omkreddy,2018-08-29T15:39:20Z,"looks like tests are getting stuck while shutting down the controller. 
Observed this while running tests  using ```./gradlew cleanTest :core:test```. 
Not able to reproduce If i run individual tests . looks like some race condition.

```
""Test worker"" #11 prio=5 os_prio=31 tid=0x00007fb95b8e1800 nid=0x5603 in Object.wait() [0x00007000015d2000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:502)
        at scala.concurrent.forkjoin.ForkJoinTask.externalAwaitDone(ForkJoinTask.java:295)
        - locked <0x0000000795b0f330> (a scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask)
        at scala.concurrent.forkjoin.ForkJoinTask.doJoin(ForkJoinTask.java:341)
        at scala.concurrent.forkjoin.ForkJoinTask.join(ForkJoinTask.java:673)
        at scala.collection.parallel.ForkJoinTasks$WrappedTask$class.sync(Tasks.scala:378)
        at scala.collection.parallel.AdaptiveWorkStealingForkJoinTasks$WrappedTask.sync(Tasks.scala:443)
        at scala.collection.parallel.ForkJoinTasks$class.executeAndWaitResult(Tasks.scala:426)
        at scala.collection.parallel.ForkJoinTaskSupport.executeAndWaitResult(TaskSupport.scala:56)
        at scala.collection.parallel.ExecutionContextTasks$class.executeAndWaitResult(Tasks.scala:558)
        at scala.collection.parallel.ExecutionContextTaskSupport.executeAndWaitResult(TaskSupport.scala:80)
        at scala.collection.parallel.ParIterableLike$class.foreach(ParIterableLike.scala:463)
        at scala.collection.parallel.immutable.ParVector.foreach(ParVector.scala:38)
        at kafka.utils.TestUtils$.shutdownServers(TestUtils.scala:191)
        at kafka.admin.ReassignPartitionsClusterTest.tearDown(ReassignPartitionsClusterTest.scala:77)        
        
""ForkJoinPool-1-worker-7"" #229 daemon prio=5 os_prio=31 tid=0x00007fb961c0f800 nid=0xcc07 waiting on condition [0x00007000019e0000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007b236f8e0> (a java.util.concurrent.CountDownLatch$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:997)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.CountDownLatch.await(CountDownLatch.java:231)
        at kafka.utils.ShutdownableThread.awaitShutdown(ShutdownableThread.scala:57)
        at kafka.controller.ControllerEventManager.close(ControllerEventManager.scala:64)
        at kafka.controller.KafkaController.shutdown(KafkaController.scala:180)
        at kafka.server.KafkaServer$$anonfun$shutdown$14.apply$mcV$sp(KafkaServer.scala:601)
        at kafka.utils.CoreUtils$.swallow(CoreUtils.scala:86)
        at kafka.server.KafkaServer.shutdown(KafkaServer.scala:601)
        at kafka.utils.TestUtils$$anonfun$shutdownServers$1.apply(TestUtils.scala:192)
        at kafka.utils.TestUtils$$anonfun$shutdownServers$1.apply(TestUtils.scala:191)
        
 ""controller-event-thread"" #1914 prio=5 os_prio=31 tid=0x00007fb961d3f000 nid=0x1ba07 waiting on condition [0x000070000beca000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00000007b236f250> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2039)
        at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
        at kafka.controller.ControllerEventManager$ControllerEventThread.doWork(ControllerEventManager.scala:80)
        at kafka.utils.ShutdownableThread.run(ShutdownableThread.scala:82)
```"
417161498,5101,hzxa21,2018-08-30T01:40:34Z,"@omkreddy Thanks for the comments. There is indeed a race condition between clearing the queue when handling `ControllerMovedException` and closing ControllerEventManager. Because we use a special event to interrupt and shutdown the controller event thread, if  `ControllerMovedException` happens after we put the shutdown event into the queue and before event thread picks it up, the shutdown event get cleared when handling `ControllerMovedException`.

To resolve it, I added the `isShuttingDown` flag to ensure that the event queue will not be cleared if the shutdown event has been put into the queue."
417161614,5101,hzxa21,2018-08-30T01:41:27Z,@omkreddy @lindong28 @lindong28 Thanks a lot for the review and I really appreciated the comments. I have updated the PR to address the issues. Could you take a look again?
417195949,5101,lindong28,2018-08-30T05:38:21Z,"@hzxa21 It seems that if controller receives ControllerMovedException, the controller should not simply remove all events from the controller queue. Events such as `Reelect` and `ControlledShutdown` probably should stay in the queue as these events need to be processed even if the broker is not controller. If we do this, we can solve the race condition without having the additional `isShuttingDown`. And we can also make the change that Jun suggested previously without worrying about `Reelect` event being removed from the controller event queue. What do you think?"
417217971,5101,hzxa21,2018-08-30T07:27:55Z,"@lindong28 Thanks for the comment. You brought up a very good point. We need to differentiate between events that need to be processed by the active controller and events that need to be processed by every broker. Because `ControllerMovedException` only indicates active controller role switched, we shouldn't try to stop processing events in the latter category. 

From the implementation point of view, we can just mark the controller as inactive and resign without touching the event queue at all instead of doing a conditional clear operation on the queue. The reason why we want to clear the queue before is that if controller has moved, we don't want the event thread to waste cycles on unnecessary events and want it to get to `Reelect` as soon as possible by clearing the potential backlog. Since we already have logic like `if (!isActive) return` to guard against events related to active controller, we just need to preempt the ""mark inactive and resign"" operation when `ControllerMovedException` happens. This will ensure correctness as well as simplicity and resolve the event queue backlog with small overhead (most cases we just dequeue controller event and do a boolean check)"
417919248,5101,hzxa21,2018-09-02T10:17:22Z,@lindong28 Thanks for the review. I have updated the PR to address your comments. Appreciated if you can take a look again when available.
418243421,5101,hzxa21,2018-09-04T05:14:40Z,"Perf testing has finished. Overall, there is no significant overhead after fencing zookeeper updates for common controller events (ControllerFailOver, ControlledShutdown, BrokerStartUp, PreferredReplicaLeaderElection).

The environment:
- 5 node zookeeper and 5 broker kafka cluster with brokers on different racks
- 2,000 topics each with 50 partitions and RF = 1
- 10k single partition topics with RF=1 + 10k 3 partitions topics with RF=2

Here are the results:

**1. Controller fails over**
Trunk (`bf0675`) - run 1
```
[2018-09-01 00:47:20,564] INFO [Controller id=1495] 1495 successfully elected as the controller (kafka.controller.KafkaController)
[2018-09-01 00:47:27,938] INFO [Controller id=1495] Starting the controller scheduler (kafka.controller.KafkaController)
```
Trunk (`bf0675`) - run 2
```
[2018-09-01 00:54:44,615] INFO [Controller id=1496] 1496 successfully elected as the controller (kafka.controller.KafkaController)
[2018-09-01 00:54:59,529] INFO [Controller id=1496] Starting the controller scheduler (kafka.controller.KafkaController)
```

KAFKA-6082 - run 1
```
[2018-09-01 00:23:45,949] INFO [Controller id=1495] 1495 successfully elected as the controller. Epoch incremented to 9 and epoch zk version is now 9 (kafka.controller.KafkaController)
[2018-09-01 00:23:53,251] INFO [Controller id=1495] Starting the controller scheduler (kafka.controller.KafkaController)
```
KAFKA-6082 - run 2
```
[2018-09-01 00:29:08,524] INFO [Controller id=1494] 1494 successfully elected as the controller. Epoch incremented to 10 and epoch zk version is now 10 (kafka.controller.KafkaController)
[2018-09-01 00:29:19,121] INFO [Controller id=1494] Starting the controller scheduler (kafka.controller.KafkaController)
```
Trunk avg = ~11s
KAFKA-6082 avg = ~9s

**2. Preferred replica leader election**
Trunk (`bf0675`) - ~2.4k leadership movements
```
[2018-09-03 23:25:12,201] INFO [Controller id=1497] Starting preferred replica leader election for partitions
[2018-09-03 23:25:12,692] INFO [Controller id=1497] Partition 10-1149-0 completed preferred replica leader election. New leader is 1496 (kafka.controller.KafkaController)
```
KAFKA-6082 - ~2.4k leadership movements
```
[2018-09-04 04:35:25,592] INFO [Controller id=1497] Starting preferred replica leader election for partitions
...
[2018-09-04 04:35:26,136] INFO [Controller id=1497] Partition 7-1801-0 completed preferred replica leader election. New leader is 1495 (kafka.controller.KafkaController
```
Trunk = 491ms
KAFKA-6082 = 544ms

Trunk (`bf0675`) - ~4.8k leadership movements
```
[2018-09-03 23:25:09,378] INFO [Controller id=1497] Starting preferred replica leader election for partitions ...
...
[2018-09-03 23:25:10,482] INFO [Controller id=1497] Partition 8-1915-0 completed preferred replica leader election. New leader is 1497 (kafka.controller.KafkaController)
```
KAFKA-6082 - broker 1497 (~4.8k leader)
```
[2018-09-04 04:35:22,313] TRACE [Controller id=1497] Starting preferred replica leader election for partitions
...
[2018-09-04 04:35:23,397] INFO [Controller id=1497] Partition 8-1915-0 completed preferred replica leader election. New leader is 1497 (kafka.controller.KafkaController)
```
Trunk = 1.104s
KAFKA-6082 = 1.084s

Trunk (`bf0675`) - ~6k leadership movements
```
[2018-09-03 23:25:10,521] INFO [Controller id=1497] Starting preferred replica leader election for partitions ...
...
[2018-09-03 23:25:11,860] INFO [Controller id=1497] Partition 5-1613-2 completed preferred replica leader election. New leader is 1494 (kafka.controller.KafkaController)
```
KAFKA-6082 - broker 1494 (~6k leadership movements)
```
[2018-09-04 04:35:23,431] INFO [Controller id=1497] Starting preferred replica leader election for
...
[2018-09-04 04:35:24,583] INFO [Controller id=1497] Partition 5-1613-2 completed preferred replica leader election. New leader is 1494 (kafka.controller.KafkaController)
```
Trunk = 1.339s
KAFKA-6082 = 1.152s

**3. Controlled shutdown**
Trunk (`bf0675`) - run 1
```
[2018-09-04 00:05:28,098] INFO [Controller id=1497] Shutting down broker 1495 (kafka.controller.KafkaController)
[2018-09-04 00:05:31,773] TRACE [Controller id=1497] All leaders = ...
```
Trunk (`bf0675`) - run 2
```
[2018-09-04 04:15:05,235] INFO [Controller id=1494] Shutting down broker 1496 (kafka.controller.KafkaController)
[2018-09-04 04:15:08,823] TRACE [Controller id=1494] All leaders = ...
```

KAFKA-6082 - run 1
```
[2018-09-04 04:51:10,175] INFO [Controller id=1497] Shutting down broker 1495 (kafka.controller.KafkaController)
[2018-09-04 04:51:13,825] TRACE [Controller id=1497] All leaders ...
```
KAFKA-6082 - run 2
```
[2018-09-04 05:11:32,197] INFO [Controller id=1497] Shutting down broker 1496 (kafka.controller.KafkaController)
[2018-09-04 05:11:35,846] TRACE [Controller id=1497] All leaders ...
```
Trunk avg = 3.63s
KAFKA-6082 avg = 3.65s

**4. Broker start**
Trunk (`bf0675`) - run 1
```
[2018-09-04 04:19:20,353] INFO [Controller id=1494] Newly added brokers: 1496, deleted brokers: , all live brokers: 1493,1494,1495,1496,1497 (kafka.controller.KafkaController)
[2018-09-04 04:19:25,233] DEBUG [Controller id=1494] Register BrokerModifications handler for ArrayBuffer(1496) (kafka.controller.KafkaController)
```
Trunk (`bf0675`) - run 2
```
[2018-09-04 00:09:49,320] INFO [Controller id=1497] Newly added brokers: 1495, deleted brokers: , all live brokers: 1493,1494,1495,1496,1497 (kafka.controller.KafkaController)
[2018-09-04 00:09:53,772] DEBUG [Controller id=1497] Register BrokerModifications handler for ArrayBuffer(1495) (kafka.controller.KafkaController)
```

KAFKA-6082 - run 1
```
[2018-09-04 04:53:49,998] INFO [Controller id=1497] Newly added brokers: 1495, deleted brokers: , all live brokers: 1493,1494,1495,1496,1497 (kafka.controller.KafkaController)
[2018-09-04 04:53:52,314] DEBUG [Controller id=1497] Register BrokerModifications handler for ArrayBuffer(1495) (kafka.controller.KafkaController)
```
KAFKA-6082 - run 2
```
[2018-09-04 05:13:23,159] INFO [Controller id=1497] Newly added brokers: 1496, deleted brokers: , all live brokers: 1493,1494,1495,1496,1497 (kafka.controller.KafkaController)
[2018-09-04 05:13:25,254] DEBUG [Controller id=1497] Register BrokerModifications handler for ArrayBuffer(1496) (kafka.controller.KafkaController)
```

Trunk avg = 4.67s
KAFKA-6082 avg = 2.21s "
418570274,5101,junrao,2018-09-05T01:34:03Z,@hzxa21 : Thanks for the perf results. They look good.
419242869,5101,hzxa21,2018-09-06T21:12:07Z,@junrao Thanks so much for all of your comments and suggestions.
419568907,5101,lindong28,2018-09-07T21:20:50Z,Thanks much for the patch @hzxa21. LGTM. Merged to trunk.
419572127,5101,hzxa21,2018-09-07T21:35:37Z,@lindong28 Thanks a lot for the reviews.
419750329,5101,ijuma,2018-09-09T22:41:16Z,"Did we check the performance impact of this change? If so, it would be great to include the details in the PR description."
420325390,5101,junrao,2018-09-11T15:58:44Z,@ijuma : There were some perf results. See the above comment on Sep. 4.
420347877,5101,ijuma,2018-09-11T17:07:40Z,Thanks @junrao. That's great. I suggest adding a summary of the results to the PR description.
420371333,5101,hzxa21,2018-09-11T18:21:21Z,@ijuma Thanks for the suggestion. I have updated the description to include the perf test.
472781135,6295,enothereska,2019-03-14T09:57:32Z,"- Checked new topic creation in remote cluster
- Checked ACL syncing
The code looks good and ideally we get some tests for the above as well. As part of the above check I'm wondering if it makes sense to add an integration test plan to the KIP (apologies if I missed) since some of this functionality needs full e2e testing."
493280895,6295,ryannedolan,2019-05-17T00:58:35Z,"> The topic config sync might have to deal with some tricky situations.
> Like message.timestamp.type=LogAppendTime (this upon syncing downstream as is would overwrite message times at each replica cluster)
> min.insync.replicas>1 may cause warning/error if destination replication factor is 1 (though we are not allowing write on remote topics).

Thanks @arunmathew88 for taking a look. I could see min.insync.replicas causing problems if sync'd. I'll add min.insync.replicas to config.properties.blacklist by default to avoid that problem."
498269358,6295,dvirgiln,2019-06-03T14:01:23Z,"Any idea when this PR will be merged? We are thinking to use mirrormaker, but depending when this PR is merged, we could wait for it instead of using the current mirrormaker v1.0.

THanks"
498439837,6295,ryannedolan,2019-06-03T21:50:50Z,"> Any idea when this PR will be merged?

Thanks for your interest @dvirgiln. Current plan is with the 2.4 release. The best way to make sure this happens is to contribute reviews!"
500217111,6295,williamhammond,2019-06-09T14:40:34Z,"> > The topic config sync might have to deal with some tricky situations.
> > Like message.timestamp.type=LogAppendTime (this upon syncing downstream as is would overwrite message times at each replica cluster)
> > min.insync.replicas>1 may cause warning/error if destination replication factor is 1 (though we are not allowing write on remote topics).
> 
> Thanks @arunmathew88 for taking a look. I could see min.insync.replicas causing problems if sync'd. I'll add min.insync.replicas to config.properties.blacklist by default to avoid that problem.

Does it make sense to mention this in the KIP or document it elsewhere? This seems important enough that it ought to jump out at people the first time they deploy mm2"
503503275,6295,enothereska,2019-06-19T10:28:15Z,Did some more testing and happy to see the progress from the last comments. There are some usability issues still but we can probably address them in a separate round. At this point it would be good if committers had a look and reviewed. Thanks.
505123732,6295,jeremy-l-ford,2019-06-24T18:22:13Z,"I have been testing with this for a while and messages are being copied.  However, I noticed that record headers are not copying correctly.  It appears that the record header data received by the mirrored broker data is base64 encoded.  By default, connect uses SimpleHeaderConverter.  I noticed that the MirrorMakerConfig is not setting a header converter.  I think it should default the header converter to the byte array converter just like the key and value converters.

Clients: kafka 0.11 based clients.  
Brokers: 2.1
MM2.0 - latest branch

Manually defaulting the convert via configuration resolves the issue."
506736259,6295,vpernin,2019-06-28T13:35:28Z,"What would be the proper way to monitor each replication lag with this new architecture, the offsets of upstreams topics being stored in the Kafka backing store topic of Kafka connect ?"
506793879,6295,ryannedolan,2019-06-28T16:22:43Z,"> What would be the proper way to monitor each replication lag with this new architecture, the offsets of upstreams topics being stored in the Kafka backing store topic of Kafka connect ?

@vpernin This is an interesting question. The KIP does not emit offset lag metrics -- only ""replication latency"", which is a measure of time. Offset lag is measured from the perspective of a particular consumer, and it would be possible to supply an interceptor to MM2's consumers in order to get this measure, if you like. Looking at the connect offsets is an interesting idea as well.

That said, I think latency is a better metric to monitor wrt replication, as there is no good way to aggregate offset lag over a bunch of unrelated topic-partitions. Offset lag is more meaningful when looking at a particular consumer and a particular topic."
517308971,6295,realradical,2019-08-01T14:21:17Z,"Hey,

I use kubernettes to spin up 2 kafka clusters locally (3 brokers each). And then I run MM2 locally as well to sync topic messages. When I send a message to source topic, source.topic is created in the sink cluster, but the message is not delivered. An exception is thrown in the console (see below). When I restart MM2, the message arrives in the source.topic. Does anyone recognize this error? Moreover, when I move one of the kafka cluster to a different machine, everything works again. I tried to increase network/io threads in the local setup, it still doesn't solve the issue.


[2019-08-01 10:18:29,033] INFO WorkerSourceTask{id=MirrorSourceConnector-0} flushing 21 outstanding messages for offset commit (org.apache.kafka.connect.runtime.WorkerSourceTask:418)
[2019-08-01 10:18:29,072] INFO WorkerSourceTask{id=MirrorSourceConnector-0} Finished commitOffsets successfully in 39 ms (org.apache.kafka.connect.runtime.WorkerSourceTask:500)
[2019-08-01 10:18:29,072] ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.WorkerTask:179)
java.lang.NullPointerException
at org.apache.kafka.connect.mirror.MirrorSourceTask.poll(MirrorSourceTask.java:140)
at org.apache.kafka.connect.runtime.WorkerSourceTask.poll(WorkerSourceTask.java:245)
at org.apache.kafka.connect.runtime.WorkerSourceTask.execute(WorkerSourceTask.java:221)
at org.apache.kafka.connect.runtime.WorkerTask.doRun(WorkerTask.java:177)
at org.apache.kafka.connect.runtime.WorkerTask.run(WorkerTask.java:227)
at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)
at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
at java.base/java.lang.Thread.run(Thread.java:834)
[2019-08-01 10:18:29,073] ERROR WorkerSourceTask{id=MirrorSourceConnector-0} Task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.WorkerTask:180)
[2019-08-01 10:18:29,073] INFO [Producer clientId=connector-producer-MirrorSourceConnector-0] Closing the Kafka producer with timeoutMillis = 30000 ms. (org.apache.kafka.clients.producer.KafkaProducer:1153)
[2019-08-01 10:18:29,080] INFO [Producer clientId=producer-7] Closing the Kafka producer with timeoutMillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.KafkaProducer:1153)
[2019-08-01 10:18:44,602] INFO [Worker clientId=connect-2, groupId=sec-mm2] Attempt to heartbeat failed since coordinator localhost:31000 (id: 2147483647 rack: null) is either not started or not valid. (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:931)
[2019-08-01 10:18:44,602] INFO [Worker clientId=connect-2, groupId=sec-mm2] Group coordinator localhost:31000 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:780)
[2019-08-01 10:18:44,612] INFO [Worker clientId=connect-2, groupId=sec-mm2] Discovered group coordinator localhost:31002 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.AbstractCoordinator:728)"
519080763,6295,mimaison,2019-08-07T12:46:51Z,"Upon startup, I'm also getting this exception:
```
[2019-08-07 13:44:57,012] WARN [MirrorHeartbeatConnector|task-0] Error registering AppInfo mbean (org.apache.kafka.common.utils.AppInfoParser:68)
javax.management.InstanceAlreadyExistsException: kafka.producer:type=app-info,id=connector-producer-MirrorHeartbeatConnector-0
	at com.sun.jmx.mbeanserver.Repository.addMBean(Repository.java:437)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerWithRepository(DefaultMBeanServerInterceptor.java:1898)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:966)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:900)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:324)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:522)
	at org.apache.kafka.common.utils.AppInfoParser.registerAppInfo(AppInfoParser.java:64)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:427)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:270)
	at org.apache.kafka.connect.runtime.Worker.buildWorkerTask(Worker.java:514)
	at org.apache.kafka.connect.runtime.Worker.startTask(Worker.java:459)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.startTask(DistributedHerder.java:1036)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder.access$28(DistributedHerder.java:1034)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder$13.call(DistributedHerder.java:1051)
	at org.apache.kafka.connect.runtime.distributed.DistributedHerder$13.call(DistributedHerder.java:1)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:748)
```"
520463091,6295,mimaison,2019-08-12T15:03:37Z,"@ryannedolan I can replicate clusters when running as dedicated MirrorMaker cluster but I'm having issues running on top of Kafka Connect (no errors but topics are not replicated). Is that supposed to work at the moment?

IMHO Kafka Connect support is key. A REST API to manage connectors is much better than having to start processes on remote hosts"
520863798,6295,mimaison,2019-08-13T14:40:51Z,"There also seem to be an issue with metrics.
After starting MM2 in dedicated MirrorMaker cluster mode, I only have the following metrics registered:
<img width=""335"" alt=""image"" src=""https://user-images.githubusercontent.com/903615/62950541-25710a80-bde0-11e9-8bf4-624fba112582.png"">

According to [connect/mirror/README.md](https://github.com/apache/kafka/blob/7a89f3aa5e1003091d244cf88f4089d49391746b/connect/mirror/README.md), there should also be `MirrorCheckpointConnector`. 

Looking at different values, I can strange behaviours to like `record-count` going up and down even with no traffic!
"
522074048,6295,ryannedolan,2019-08-16T16:42:12Z,"@mimaison 
> IMHO Kafka Connect support is key

The connectors will work with Connect just fine, but the configuration properties differ slightly from the mm2.properties file. Try something like:

```
source.cluster.alias = primary
target.cluster.alias = backup
source.cluster.bootstrap.servers = ...
target.cluster.bootstrap.servers = ...
topics = ...
```

N.B. properties like ""primary->backup.topics"" are understood by the mm2 driver but not the underlying Connectors.

Here's a working configuration as returned from the Connect REST API during MirrorConnectorsIntegrationTest (new!):

```
{
  ""connector.class"": ""org.apache.kafka.connect.mirror.MirrorSourceConnector"",
  ""replication.factor"": ""1"",
  ""source.cluster.producer.bootstrap.servers"": ""localhost:63599"",
  ""sync.topic.acls.enabled"": ""false"",
  ""topics"": "".*test-topic-.*"",
  ""emit.checkpoints.interval.seconds"": ""1"",
  ""source.cluster.alias"": ""backup"",
  ""groups"": ""consumer-group-.*"",
  ""source.cluster.bootstrap.servers"": ""localhost:63599"",
  ""target.cluster.producer.bootstrap.servers"": ""localhost:63532"",
  ""enabled"": ""true"",
  ""target.cluster.admin.bootstrap.servers"": ""localhost:63532"",
  ""target.cluster.alias"": ""primary"",
  ""target.cluster.consumer.bootstrap.servers"": ""localhost:63532"",
  ""name"": ""MirrorSourceConnector"",
  ""target.cluster.bootstrap.servers"": ""localhost:63532"",
  ""emit.heartbeats.interval.seconds"": ""1"",
  ""source.cluster.admin.bootstrap.servers"": ""localhost:63599"",
  ""source.cluster.consumer.bootstrap.servers"": ""localhost:63599""
}
```

Most of those properties are redundant, but `MirrorMakerConfig.connectorBaseConfig()` fills them in anyway."
522078873,6295,ryannedolan,2019-08-16T16:57:37Z,"@mimaison 

> According to connect/mirror/README.md, there should also be MirrorCheckpointConnector.

MirrorCheckpointConnector won't emit any metrics unless it is actively producing checkpoints, which won't happen until there is a whitelisted consumer group subscribed to a whitelisted topic. By default kafka-console-consumer groups are blacklisted, so you need to set a specific --group-id to see this happen.

> Looking at different values, I can strange behaviours to like record-count going up and down even with no traffic!

Record-count is windowed, so it resets after a short while. Might make sense to change that. Also, even if there is no data to replicate, MirrorSourceConnector always replicates heartbeats, so there is always _some_ replication happening."
525802843,6295,mimaison,2019-08-28T15:41:44Z,"@ryannedolan With 2.4.0 approching, do you think the PR is ready to be considered for that release? Or do you still have work planned?
If it's ready, let's grab the attention of committers asap"
526404615,6295,ryannedolan,2019-08-30T00:05:26Z,"> do you think the PR is ready to be considered for that release

Yes, we should get some committers to take a look."
531890077,6295,harshach,2019-09-16T18:03:47Z,"Thanks, @ryannedolan for the contribution and patience in getting reviewed and thanks to everyone for reviewing the code. 
I went through the code a few times to get a better understanding, Overall LGTM. I don't have any further comments on the code itself.
I am running a few tests on our clusters and will report back my findings."
533214612,6295,harshach,2019-09-19T16:43:35Z,"Thanks for your patience and following upon the PR @ryannedolan 
@junrao do you want to take a look. We have 4 reviewers approved already. So let us know your thoughts otherwise I would like to merge this in."
533322760,6295,junrao,2019-09-19T21:51:55Z,@harshach @ryannedolan : I will make another pass of the PR by early next week.
533557824,6295,omkreddy,2019-09-20T13:39:31Z,"@ryannedolan Since this is a major feature, we need to add few system tests. It is good have system tests before code freeze. For tracking purpose, Can you please create JIRAs for system tests and docs? "
533628482,6295,ryannedolan,2019-09-20T16:45:12Z,"@omkreddy I've created KAFKA-8929 and KAFKA-8930 to track tests and docs. I will update the javadocs as part of this PR, but I'll create a separate PR for the system tests. Should be done next week."
535781371,6295,junrao,2019-09-27T04:33:06Z,"@ryannedolan : Also, the KIP includes both a  MirrorSourceConnector and a MirrorSinkConnector, but the PR only has the former?"
538051233,6295,ryannedolan,2019-10-03T17:44:33Z,"@omkreddy I've got ducktape tests implemented and will create a second PR for those, which we can merge before 2.4 code freeze. Please note PR 6295 (this one) includes MirrorConnectorsIntegrationTest, which uses Connect's integration test framework. In particular, these cover replication, topic detection, task rebalances, offset translation, and consumer migration. The ducktape tests cover basic replication and bouncing the cluster. Basically they are the same ducktape tests from legacy mirrormaker, just with mm2 dropped in.

@junrao MirrorSinkConnector (the missing fourth connector) will come in a subsequent PR. The sink connector is not necessary for the driver to function so I've deprioritized it for this PR. There will be very little new code for the sink connector, as almost everything will be extracted from MirrorSourceConnector. Also missing from the KIP is the ""legacy mode"" script, which will eventually replace the existing mirror-maker.sh. Both should land in 2.5, with legacy mm being deprecated as soon as 2.6.

Looks like some churn on master has caused a few failed builds, but once green this is ready to merge."
538107371,6295,harshach,2019-10-03T20:07:30Z,"@omkreddy @junrao looks like the deadline to make the 2.4 release is on Friday.
Can you please take a look at the PR and see if there is anything else missing in merging it in for 2.4.
cc @ryannedolan "
538144648,6295,junrao,2019-10-03T21:55:41Z,@harshach : There are still a couple of comments that haven't been addressed. The biggest one is on dealing with compacted topic for offset translation.
538159605,6295,ryannedolan,2019-10-03T22:53:17Z,"@junrao sorry, I didn't see a few comments that were folded/hidden for some reason. Hopefully I've addressed everything that would otherwise delay the merge."
538399248,6295,omkreddy,2019-10-04T13:35:09Z,retest this please
538609224,6295,omkreddy,2019-10-05T02:36:22Z,"@ryannedolan Any update on test failures? 

@harshach Please merge the PR once the Jun's comments are addressed and we have green builds."
538614075,6295,ryannedolan,2019-10-05T04:04:16Z,@omkreddy I traced the build failures to an NPE from KIP-507 committed yesterday. It is breaking MM2's and other Connect integration tests. I'll fix here I guess.
538614836,6295,ryannedolan,2019-10-05T04:18:13Z,I fixed the NPE from KIP-507 -- let's see if we can get a green build now.
538645845,6295,ryannedolan,2019-10-05T12:32:24Z,@harshach good to go!
538650932,6295,rhauch,2019-10-05T13:42:25Z,"@ryannedolan, @junrao, @harshach: As mentioned above in one of the comments on `WorkerSourceTask`, this PR also implements [KIP-416](https://cwiki.apache.org/confluence/display/KAFKA/KIP-416%3A+Notify+SourceTask+of+ACK%27d+offsets%2C+metadata). I've edited the description of the PR to mention this.

According to the [vote thread](http://mail-archives.apache.org/mod_mbox/kafka-dev/201910.mbox/%3cCAJQG2dCqTw-9focpyep9McChdPgy4yyeA2WSYyzn0_hS4LvaCQ@mail.gmail.com%3e), KIP-415 has not yet been approved, and per the [AK 2.4.0 Release Plan](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=125307901) the KIP deadline was on Sept 25. 

@junrao, what's required here?"
538669326,6295,junrao,2019-10-05T17:10:40Z,"@rhauch : We can fold the changes in WorkerSourceTask into this KIP. Ryan, could you update your kip wiki and send an email to the voting thread about the additional changes to make sure that no one objects to this? If kip-416 is completely subsumed by this, we can just cancel it. Otherwise, kip-416 can cover the remaining part."
538671064,6295,rhauch,2019-10-05T17:30:37Z,"Thanks, @junrao. KIP-416 is straightforward, limited to the one new method change we discussed (and agreed to), and required for KIP-382, so pulling those changes into KIP-382 makes sense to me."
538830899,6295,harshach,2019-10-07T03:58:35Z,"@junrao some of the comments mentioned here can be handled in a follow-up patch and can be part of minor release that will follow-up.
Given the no.of users that are interested and the number of reviews, we had it in the PR and also maintaining this big of patch as the trunk continues to evolve will be challenging.
If you don't have any major concerns lets merge this in for 2.4 release and address any new comments in follow-up patch. cc @omkreddy 
"
538890813,6295,omkreddy,2019-10-07T08:21:47Z,"@ryannedolan Thanks for the PR. Merging the PR. Lets address any issues in follow-up PRs. Pls raise JIRAs for any pending work (MirrorSinkConnector, legacy mode etc.) for next releases."
603995709,6295,bpux,2020-03-25T18:02:27Z,@ryannedolan try to find JIRA of MirrorSinkConnector but search return nothing. can you please point me where i can find status of this work? thanks!
603998987,6295,ewencp,2020-03-25T18:08:29Z,"@bpux https://issues.apache.org/jira/browse/KAFKA-7500, the primary JIRA for KIPs are linked from the KIP document itself: https://cwiki.apache.org/confluence/display/KAFKA/KIP-382%3A+MirrorMaker+2.0"
604003001,6295,bpux,2020-03-25T18:15:44Z,@ewencp  thanks! will comment in there
620318837,6295,michael-trelinski,2020-04-28T01:13:39Z,"<img width=""1288"" alt=""Screen Shot 2020-04-21 at 2 17 59 PM"" src=""https://user-images.githubusercontent.com/4055823/80436066-191bd780-88b3-11ea-8b3d-8e2c6b6c0b87.png"">
Hi @ryannedolan -

I just got done setting up Mirror Maker V2 where I work.  It was successful, but the documentation doesn't match the code in various places.

A couple of things that ""got me"" -

- The JMX Metrics in the documentation:

>  The mbean name for these metrics will be: kafka.mirror.connect:type=MirrorSourceConnect,target=([.\w]+),topic=([.\w]+),partition=([.\d]+) and kafka.mirror.connect:type=MirrorCheckpointConnector,target=([.\w]+),source=([.\w]+),group=([.\w]+)

It should actually be something like (pardon me, I'm using the JMX-prometheus exporter): 
`kafka.connect.mirror<type=MirrorSourceConnector, target=([\-\w]+), topic=([\-\w]+), partition=(\d+)>`. Note how it's actually **kafka.connect.mirror** not **kafka.mirror.connect**, also it's MirrorSourceConnector, not MirrorSourceConnect.

- In the documentation, there is a part about blacklisting groups: 

> groups.blacklist | empty string | groups to exclude from replication

In the code, it is `public static final String GROUPS_BLACKLIST_DEFAULT = ""console-consumer-.*, connect-.*, __.*"";`; here: https://github.com/apache/kafka/blob/trunk/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/DefaultGroupFilter.java#L36 - this got me because we're actually using some Kafka Connect stuff for other projects, but we're running Mirror Maker as it's own cluster.  Maybe you could blacklist only Mirror Maker's Kafka Connect connect- consumer group?

- This one is kind of a nit-pick, but in RemoteClusterUtils.translateOffsets, the timeout parameter isn't really a timeout in the conventional sense.  Stepping further into the call that's made using timeout,  `return client.remoteConsumerOffsets(consumerGroupId, remoteClusterAlias, timeout);` we can see that it's being used as a deadline, not a timeout.  In MirrorClient's remoteConsumerOffsets method, it's being used as a timeout, and as a deadline.  Setting this too low caused me to miss some offsets because I didn't understand how it was being used.  Setting this absurdly high got me the results that I wanted.

All in all a great product and vastly superior to Mirror Maker V1.  Thanks!"
1004211004,6295,Migueljfs,2022-01-03T16:33:16Z,"I can't seem to find the new metrics mentioned in the KIP.
Like @michael-trelinski said above, I've tried both kafka.connect.mirror / kafka.mirror.connect AND MirrorSourceConnector / MirrorSourceConnect and all combinations of them but still I don't see the mirror domain the respective mbeans.

I installed JMXTERM on my mirror-maker pod and this is what I got:

Domains:
```
$>domains
#following domains are available
JMImplementation
com.sun.management
java.lang
java.nio
java.util.logging
jdk.management.jfr
kafka.connect
kafka.consumer
kafka.producer
```

Beans for connect:
```
#domain = kafka.connect:
kafka.connect:client-id=connect-1,node-id=node--1,type=connect-node-metrics
kafka.connect:client-id=connect-1,node-id=node-7,type=connect-node-metrics
kafka.connect:client-id=connect-1,type=app-info
kafka.connect:client-id=connect-1,type=connect-coordinator-metrics
kafka.connect:client-id=connect-1,type=connect-metrics
kafka.connect:client-id=connect-1,type=kafka-metrics-count
kafka.connect:id=""west->central"",type=app-info
kafka.connect:id=connect-1,type=app-info
kafka.connect:type=app-info
kafka.connect:type=connect-worker-metrics
kafka.connect:type=connect-worker-rebalance-metrics
kafka.connect:type=kafka-metrics-count
#domain = kafka.consumer:
```

As you can see, no mentions of ""mirror"" of any kind anywhere.

Anyone able to help me understand what's going on?"
736700777,9485,ctan888,2020-12-01T17:25:49Z,"BUILD SUCCESSFUL in 18s
122 actionable tasks: 3 executed, 119 up-to-date
docker exec ducker01 bash -c ""cd /opt/kafka-dev && ducktape --cluster-file /opt/kafka-dev/tests/docker/build/cluster.json  ./tests/kafkatest/ --subset 0 --subsets 15""
Traceback (most recent call last):
  File ""/usr/local/bin/ducktape"", line 8, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python3.7/dist-packages/ducktape/command_line/main.py"", line 118, in main
    os.makedirs(ConsoleDefaults.METADATA_DIR)
  File ""/usr/lib/python3.7/os.py"", line 211, in makedirs
    makedirs(head, exist_ok=exist_ok)
  File ""/usr/lib/python3.7/os.py"", line 221, in makedirs
    mkdir(name, mode)
PermissionError: [Errno 13] Permission denied: '.ducktape'
ducker-ak test failed

Thanks for the review @rajinisivaram. I've addressed all the previous comments. Can you take another look at this PR? Also, any idea how to resolve this ducktape PermissionError?"
736701573,9485,ctan888,2020-12-01T17:27:09Z,"6206tests1failures61ignored6m45.08sduration | 6206tests | 1failures | 61ignored | 6m45.08sduration | 99%successful
-- | -- | -- | -- | -- | --
6206tests | 1failures | 61ignored | 6m45.08sduration

Failed tests
Ignored tests
Packages
Classes
KafkaProducerTest. testInitTransactionTimeout

which is flaky"
738942757,9485,ctan888,2020-12-04T18:24:17Z,"Make a new aggregated index called ResourceIndex, which is a combination of ACE, ResourceType, and PatternType. 

New benchmark looks much better: https://paste.ubuntu.com/p/FkyGhb9MrQ/

When `aclCount=100` and  `resourceCount=200000`, in the worst case where every ""allow resource"" of a given ACE has a dominant ""deny resource"" and the ""deny resource"" name is allow_string.substring(0, len-1), the time cost is 
 45.897          4.748   ms/op"
740036778,9485,ctan888,2020-12-07T16:43:09Z,"https://paste.ubuntu.com/p/zKYc6rk6WY/

Benchmark with AclAuthorizer::updateCache

With aclCount=50 and resourceCount=200000,

200000 call to updateCache took ~59 seconds

The cpu overhead is mainly on re-hashing (everytime when the resourceCache hits 75% of the capacity). But we can overlook it.

The memory overhead is M = (aclCount * 8) * (resourceCount * 8) + unique(resourceCount) * (average_resource_name_length) 
bytes, since strings are cached in the constant string pool. When aclCount=50 and resourceCount=200000, M < 20MB."
740056125,9485,ctan888,2020-12-07T17:14:45Z,"Hi @rajinisivaram. Thanks for the detailed review! Ive addressed all comments you left. Also, after some optimization on the `resourceCache`, the benchmark looks much better now. Please take another look and let me know if there's anything we need to change. Thanks."
740199487,9485,ctan888,2020-12-07T21:47:08Z,"Benchmark results run against trunk:

https://paste.ubuntu.com/p/zwKyQSrmDX/"
740501882,9485,ctan888,2020-12-08T09:32:43Z,"![image](https://user-images.githubusercontent.com/31675100/101454358-b62cfd00-38e5-11eb-9eee-200604895b16.png)
Chart description (from left to right)
1. The performance comparison btw the trunk's and my branch's of `aclAuthorizer#updateCache`.
2. The performance comparison btw the trunk's and my branch's of `aclAuthorizer#authorize`.
3. The performance comparison btw my branch's `aclAuthorizer#authorize` and `aclAuthorizer#authorizeByResourceType`

1 and 2 indicate that the newly added `resourceCache` doesn't add too much overhead to the time complexity. The time complexity increasing is constant (my branch doubled the hashmap add/remove operation).

In chart 3, the horizontal axis ""Percentage of dominant deny"" means the percentage of ""allow resource"" which has a ""deny resource"" added to the authorizer.

The benchmark used in chart 3 has the ""dominant deny"" distributed evenly.

Define the average of resource name length is L, the expected ""allow resource"" in the ""allow resource set"" to iterate is E_A, the time complexity of my implementation of `authorizeByResourceType` is L * E_A. How to compute E_A?

Define the ""percentage of dominant deny"" as P. Every resource in the ""allow resource"" set has the probability (100-P)/100 to be allowed by the authorizer. So 

expected_elements_iterated_in_the_allow_resource_set.

1. The time complexity of `AclAuthorizer#authorize` is irrelevant to the ""percentage of dominant deny""
2. If we treat L as a constant, the time complexity of `AclAuthorizer#authorizeByResourceType` is only relevant to E_A, where E_A = 1/[(100-P)/100] = 100/(100-p), which is a hyperbola. Chart 3 can confirm this.

![image](https://user-images.githubusercontent.com/31675100/101465299-9f41d700-38f4-11eb-9415-5eb9a6a0b5f3.png)

From the benchmark, when P = 99.99, if ""dominant deny"" distributed evenly, authorizeByResourceType only took 2.5ms. In the worst case, if ""dominant deny"" distributed unevenly or P = 100, the API will take ~40ms, but this is very unlikely."
743884225,9485,ctan888,2020-12-12T21:17:45Z,"@rajinisivaram 

Thanks for another round of detailed review. I've
1. Add the corner case checks in AclAuthorizer when there's no deny ACL binding in AclAuthorizer.
2. Re-write the AclWrapper logic to use the `BaseAuthorizer` to give the information about if we should ""allow everyone if no ACL found"" in the configure() method.
3. Fix the ""canDenyAll"" logic in AclWrapper.
4. Revert the unnecessary changes in the tests and benchmark.

There're a few outstanding comments that need further discussion as I replied, mostly the NIT changes. Please let me know what you think about those. Thanks."
744493191,9485,ctan888,2020-12-14T14:53:25Z,"shouldUpgradeFromEosAlphaToEosBeta is flaky. Other than this, all tests passed.
"
745712214,9485,ctan888,2020-12-16T01:55:24Z,"Thanks, @rajinisivaram for the review. According to the comments, I've
1. Support super user in Authorizer, AclAuthorizer, and AuthorizerWrapper. Add tests to the three corresponding testing classes.
2. Rename ResourceIndex to ResourceTypeKey, and make it an inner class of AclAuthorizer.
3. Cleaned the tear-off logic in the unit tests since ZK starts in setup() indeed.
4. Addressed other NIT suggestions.

Please let me know if anything else needs to be changed, especially if the super user part looks good to you. Thank you."
747116007,9485,ctan888,2020-12-17T00:10:03Z,Thanks @rajinisivaram for the NIT and test structure suggestions. I've adopted those and re-struct the test classes. Please let me know if we are good to merge now.
747881257,9485,ctan888,2020-12-18T05:43:47Z,"@rajinisivaram Thanks for going through the PR once again. I really appreciate your time spent on this work. 

I've addressed all the new comments. Please let me know if we're good to go after the PR built."
748190567,9485,rajinisivaram,2020-12-18T16:30:50Z,"@d8tltanc Thanks for your patience with this PR, builds look good, merging to trunk."
748242438,9485,ctan888,2020-12-18T18:19:53Z,Thank you @rajinisivaram 
756982595,9485,ijuma,2021-01-08T20:26:48Z,"I don't see any updates to the release notes. Unless I missed it, we should add one since Authorizers should implement the new method we introduced. "
756985555,9485,ctan888,2021-01-08T20:34:05Z,Sure. Could someone point me to the release note page so I can edit it?
756987814,9485,ijuma,2021-01-08T20:40:01Z,There you go: https://github.com/apache/kafka/blob/trunk/docs/upgrade.html
247191884,1776,junrao,2016-09-14T23:51:01Z,"I left the following comment on SimpleRate earlier.
Having two different rates is going to make it harder for developers to decide which one to use. If this is strictly better than Rate, perhaps we should just change Rate.windowSize(). If this is just for testing, perhaps we can create SimpleRate in test?
"
247520427,1776,junrao,2016-09-16T05:20:06Z,"Thanks for the patch. LGTM. We can address the remaining minor issues in a followup jira.
"
1789830091,14690,kirktrue,2023-11-01T23:29:51Z,"I am a bear of very little brain, so the chained `Future` mechanism is simultaneously interesting, hard to follow, but ultimately useful.

My one concern is that we don't allow those `Future`s to be `complete`d by the application thread, since we don't want it to run all the chained logic. That would be bad "
1793014796,14690,philipnee,2023-11-03T19:52:20Z,"Hi @lianetm and @kirktrue @dajac  - Thank you for putting this PR out, much appreciate for the effort.  I have concern about the extensive use of callback here because it is kind of against the original design goal, i.e. making background thread operate in a linear fashion.  With the main thread involved, I'm also seeing the risk of multithreading access to the background thread.  Here are my questions:

Do we need to use the completable future to facilitate the rebalance callback completion cycle?  My original thought was to use Queues to relay the callback invocation and completion, because I see that in some cases, it is unclear to me which thread is completing which callback.  To avoid the confusion all together, I think we should try to use the queue as much as possible.  Also, it make the program to operate more linearly.

If we don't use completable future for the callback, we just need to do two things
1. if the listen isPresent, we just need to enqueue an event to the backgroundEventQueue and wait for the consumer to poll.  Once the main thread completes the callback, it enqueues an ack event for background thread to consume, to transition to the next state.
2. if the listener is not presented.  Then we directly invoke the next state transition method.

Also, it seems like 

`reconciliationResult.whenComplete((result, error) -> {`

this is completed by the application thread no? I think we really want the background thread to perform in a single-threading fashion, I'm afraid using callback might result in a some sort of race condition."
1793496177,14690,AndrewJSchofield,2023-11-04T17:01:19Z,"Hi @philipnee, thanks for you comment. It does raise an interesting point.

My view is that CompletableFuture provides a really nice abstraction for asynchronous processing. Using it in this PR is entirely appropriate. However, I also take your overall point.

The threading for CompletableFuture depends upon the details of how it is used. If a future is completed on the ""wrong"" thread, then processing which is dependent upon completion of that future will also execute on the same ""wrong"" thread. If one of the more complex methods such as `supplyAsync` is used, it runs on a thread from a worker pool. I don't think we're using that pattern here.

If we actually need to achieve signalling back to a specific thread, we could use the CF to enqueue an event. That compromise seems pretty sensible to me, but I'm not sure whether we need this with the PR as written. I need to re-read the code before I'll be certain."
1793538245,14690,philipnee,2023-11-04T19:41:57Z,"Hi @AndrewJSchofield - Thank you for your inputs here, your point is absolutely valid in terms of the abstraction it provides.  My main concern is the rebalance callback invocation.  Maybe I misunderstood the PR, but I think the state transition in the whenComplete can be completed by the main thread if the user supplies a callback, unless the callback is not supplied.  I will double-check the syntax.

Could you be more specific about `using CF to enqueue an event`? Do you mean by enqueuing a callback completion signal to some background thread queue, to notify the thread to perform the subsequent action?"
1793877944,14690,AndrewJSchofield,2023-11-05T23:15:02Z,@philipnee I meant that the code which runs in the `CompletableFuture.whenComplete()` could enqueue an action for the background thread to ensure that the main logic all runs in the background thread.
1793892733,14690,philipnee,2023-11-06T00:08:09Z,"@AndrewJSchofield - Thanks, I think that's what I have in mind as well."
1794951522,14690,lianetm,2023-11-06T14:26:47Z,"Comment regarding the async reconciliation process and callback execution. I get your concerns @philipnee , but this PR does not include callback execution for now, the membership manager triggers the callbacks and needs to know when they complete. I expect that will follow in a separate PR based on events as @AndrewJSchofield mentioned.

Without having gotten into the implementation details yet, I expect that the membershipManager (background thread) will enqueue an app event to execute the callbacks in the app thread, and when that app thread completes the callbacks, it will enqueue a background event to notify the manager about the completion (btw, nice here again that according to the protocol there will be only one assignment being reconciled at a time @dajac).

The async nature of the membership manager in this PR it is not only due to the callbacks. The reconciliation process handles 3 main async operations: metadata, commit, callbacks (triggering and notification when completed, no execution). By basing the them on completable futures we ensure that the background thread continues it operations while there is a reconciliation in process. 

Note on the reconciliation not being time bounded on the client. The reasoning is that the time boundaries are set by the broker, that keeps a rebalance timer. If the reconciliation takes longer than allowed (ex. stuck in any of the 3 async operations), the expectation is that the broker will re-assign the partitions and kick the member out of the group. So from the client side we just care about triggering the async reconciliation, making sure we keep sending HB and processing responses (will let us know if kicked-out), and make sure that when reconciliation completes we check if it is still relevant."
1818883304,14690,dajac,2023-11-20T11:35:43Z,I just merged trunk to fix conflicts. We can merge it when the build completes.
1830640924,14690,lianetm,2023-11-28T20:07:52Z,Follow-up PR https://github.com/apache/kafka/pull/14857
276160087,2466,eliaslevy,2017-01-30T19:14:01Z, 
276164814,2466,asfbot,2017-01-30T19:30:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1337/
Test FAILed (JDK 8 and Scala 2.11).
"
276165119,2466,asfbot,2017-01-30T19:31:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1333/
Test FAILed (JDK 8 and Scala 2.12).
"
276173321,2466,asfbot,2017-01-30T20:00:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1333/
Test FAILed (JDK 7 and Scala 2.10).
"
276214966,2466,mjsax,2017-01-30T22:39:04Z,@jeyhunkarimov One more thing. Please add some tests!
276293619,2466,mjsax,2017-01-31T07:29:54Z,"I don't see a good reason why you ""wanted to get rid of"" this line... Just leave it, as I suggested. Furthermore, semantically the timestamp extractor does not belong to the context, and IMHO it would not be a good design/abstraction adding it their."
276335874,2466,jeyhunkarimov,2017-01-31T11:08:39Z,Sorry it is my bad. I realized this short after writing comment and removed comment immediately. 
276465102,2466,asfbot,2017-01-31T19:26:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1360/
Test PASSed (JDK 8 and Scala 2.11).
"
276465170,2466,asfbot,2017-01-31T19:26:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1356/
Test PASSed (JDK 8 and Scala 2.12).
"
276466419,2466,asfbot,2017-01-31T19:31:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1356/
Test PASSed (JDK 7 and Scala 2.10).
"
276482452,2466,asfbot,2017-01-31T20:29:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1369/
Test PASSed (JDK 8 and Scala 2.12).
"
276482513,2466,asfbot,2017-01-31T20:29:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1369/
Test PASSed (JDK 7 and Scala 2.10).
"
276491081,2466,asfbot,2017-01-31T21:02:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1373/
Test PASSed (JDK 8 and Scala 2.11).
"
276491890,2466,jeyhunkarimov,2017-01-31T21:05:49Z,@mjsax please review
277004397,2466,asfbot,2017-02-02T16:20:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1432/
Test PASSed (JDK 8 and Scala 2.11).
"
277004793,2466,asfbot,2017-02-02T16:21:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1429/
Test PASSed (JDK 7 and Scala 2.10).
"
277006630,2466,jeyhunkarimov,2017-02-02T16:28:00Z,"@dguy 
 - I added extra test for `KStreamBuilder.addSource` with `TimestampExtract` arguments. Not sure it is much different from `TopologyBuilder.addSource` test.
- In `SourceNode` class, not all variables can be final as they are initialized in `init()` method.
- In `StreamTask` 121, the `SourceNode` was getting `null` if the source is defined with `Pattern`.   I deleted ""Pattern [ ]"" string from `SourceNode`name  (it was initialized like ""Pattern [ "" + regex + ""]"" )  so that I can compare with `topic` names.

@mjsax 
I could not get the reason to insert new overloaded methods further below. The next method below is the most generic one (with most arguments) which is 
```
public <K, V> KStream<K, V> stream(final AutoOffsetReset offsetReset,
                                       final TimestampExtractor timestampExtractor,
                                       final Serde<K> keySerde,
                                       final Serde<V> valSerde,
                                       final Pattern topicPattern) 
```"
277013504,2466,asfbot,2017-02-02T16:50:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1429/
Test FAILed (JDK 8 and Scala 2.12).
"
277108644,2466,asfbot,2017-02-02T22:43:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1444/
Test PASSed (JDK 8 and Scala 2.11).
"
277115513,2466,asfbot,2017-02-02T23:16:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1441/
Test PASSed (JDK 8 and Scala 2.12).
"
277118223,2466,asfbot,2017-02-02T23:29:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1441/
Test FAILed (JDK 7 and Scala 2.10).
"
277353548,2466,jeyhunkarimov,2017-02-03T20:27:01Z,@mjsax @dguy would you mind to review again please?
277392859,2466,mjsax,2017-02-03T23:39:32Z,"Would you mind fixing this typo, too: https://github.com/jeyhunkarimov/kafka/blob/8acf04847fb729155ca72a48e9b29601b781d706/streams/src/main/java/org/apache/kafka/streams/kstream/KStreamBuilder.java#L829"
277394196,2466,asfbot,2017-02-03T23:47:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1481/
Test FAILed (JDK 8 and Scala 2.12).
"
277394353,2466,asfbot,2017-02-03T23:48:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1484/
Test FAILed (JDK 8 and Scala 2.11).
"
277394537,2466,asfbot,2017-02-03T23:50:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1481/
Test FAILed (JDK 7 and Scala 2.10).
"
277395085,2466,mjsax,2017-02-03T23:53:52Z,There is a build error. Please fix before we can do a review.
277395661,2466,jeyhunkarimov,2017-02-03T23:57:46Z,"Yes, some PR changed the (test) classes related with this PR so. I will fix them."
277395841,2466,mjsax,2017-02-03T23:58:54Z,"One more general comment. I would be better if you would not squash your commits on every update -- this can simplify reviewing the PR. And when the PR get's merged, all commits get squashed automatically anyway."
277403268,2466,asfbot,2017-02-04T00:56:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1489/
Test FAILed (JDK 8 and Scala 2.11).
"
277403348,2466,asfbot,2017-02-04T00:56:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1486/
Test FAILed (JDK 7 and Scala 2.10).
"
277403409,2466,asfbot,2017-02-04T00:57:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1486/
Test FAILed (JDK 8 and Scala 2.12).
"
277405180,2466,asfbot,2017-02-04T01:13:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1488/
Test FAILed (JDK 8 and Scala 2.12).
"
277408242,2466,asfbot,2017-02-04T01:44:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1493/
Test FAILed (JDK 8 and Scala 2.11).
"
277408285,2466,asfbot,2017-02-04T01:44:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1490/
Test FAILed (JDK 7 and Scala 2.10).
"
277408320,2466,asfbot,2017-02-04T01:45:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1490/
Test FAILed (JDK 8 and Scala 2.12).
"
277413314,2466,asfbot,2017-02-04T02:52:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1494/
Test FAILed (JDK 8 and Scala 2.11).
"
277413403,2466,asfbot,2017-02-04T02:54:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1491/
Test FAILed (JDK 8 and Scala 2.12).
"
277413508,2466,asfbot,2017-02-04T02:55:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1491/
Test FAILed (JDK 7 and Scala 2.10).
"
277442083,2466,asfbot,2017-02-04T12:24:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1500/
Test PASSed (JDK 8 and Scala 2.11).
"
277442515,2466,asfbot,2017-02-04T12:29:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1497/
Test PASSed (JDK 7 and Scala 2.10).
"
277446441,2466,asfbot,2017-02-04T13:38:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1497/
Test FAILed (JDK 8 and Scala 2.12).
"
277454386,2466,asfbot,2017-02-04T15:49:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1501/
Test PASSed (JDK 8 and Scala 2.11).
"
277457827,2466,asfbot,2017-02-04T16:37:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1498/
Test FAILed (JDK 8 and Scala 2.12).
"
277460507,2466,asfbot,2017-02-04T17:17:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1498/
Test PASSed (JDK 7 and Scala 2.10).
"
277471760,2466,jeyhunkarimov,2017-02-04T19:56:02Z,I fixed the errors
278659444,2466,asfbot,2017-02-09T14:37:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1594/
Test PASSed (JDK 8 and Scala 2.11).
"
278660843,2466,asfbot,2017-02-09T14:42:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1591/
Test PASSed (JDK 7 and Scala 2.10).
"
278669904,2466,asfbot,2017-02-09T15:10:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1591/
Test FAILed (JDK 8 and Scala 2.12).
"
278685807,2466,jeyhunkarimov,2017-02-09T16:02:07Z,I had to rebase because of conflicts in different commits
278694384,2466,asfbot,2017-02-09T16:29:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1605/
Test PASSed (JDK 8 and Scala 2.11).
"
278695163,2466,asfbot,2017-02-09T16:31:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1602/
Test PASSed (JDK 8 and Scala 2.12).
"
278696546,2466,asfbot,2017-02-09T16:36:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1602/
Test PASSed (JDK 7 and Scala 2.10).
"
278729643,2466,mjsax,2017-02-09T18:28:34Z,anyof @dguy @enothereska call for second review
278931202,2466,asfbot,2017-02-10T12:28:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1622/
Test FAILed (JDK 7 and Scala 2.10).
"
278938319,2466,asfbot,2017-02-10T13:09:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1625/
Test PASSed (JDK 8 and Scala 2.11).
"
278943998,2466,asfbot,2017-02-10T13:39:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1622/
Test PASSed (JDK 8 and Scala 2.12).
"
278970132,2466,jeyhunkarimov,2017-02-10T15:11:44Z,I did changes
279079856,2466,mjsax,2017-02-10T22:08:46Z,"I just realized, that we do have a KIP for this. I am very sorry that I missed to mention this from the beginning on.

@jeyhunkarimov Are you familiar with the KIP process? Would you like to do the KIP?

See https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals"
279081152,2466,jeyhunkarimov,2017-02-10T22:14:52Z,"@mjsax np. In general, I am familiar with KIP process
So is there some existing KIP related with this PR (if yes, can you provide the number) or should I create KIP for this PR and discuss in mailing list?"
279091209,2466,mjsax,2017-02-10T23:07:24Z,"We don't have a KIP. Please, use the template to create a new KIP page, add it to the table ""KIPs under discussion"" (and to Kafka Streams page in the Wiki) and start a DISCUSS thread on the dev mailing list. Thanks a lot!"
279102611,2466,jeyhunkarimov,2017-02-11T00:25:30Z,"I don't have `Create` button on my confluence page (https://cwiki.apache.org/confluence/display/~jeyhun.karimov)
Do I have to ask somebody to give me authorization to create KIP?
"
279119214,2466,mjsax,2017-02-11T04:00:55Z,@guozhangwang Can you give write access to @jeyhunkarimov to the Kafka wiki so he can prepare the KIP ?
279120938,2466,guozhangwang,2017-02-11T04:42:24Z,@jeyhunkarimov what is your apache id?
279188872,2466,mjsax,2017-02-12T01:19:44Z,"jeyhun.karimov
see link in Jeyhun's comment :)"
279241592,2466,guozhangwang,2017-02-12T19:25:56Z,Oh right. Done.
281547989,2466,asfbot,2017-02-22T02:21:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1791/
Test FAILed (JDK 7 and Scala 2.10).
"
281548260,2466,mjsax,2017-02-22T02:22:39Z,"Ignore my last comment (deleted it already)... I did update the JIRA, but it does not require a change of this PR.

@jeyhunkarimov You might want to sent another reminder to dev list about the KIP. Ask for further feedback and if nodoby response, you might want to start the VOTE thread."
281554085,2466,asfbot,2017-02-22T02:58:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1792/
Test FAILed (JDK 8 and Scala 2.12).
"
281554091,2466,asfbot,2017-02-22T02:58:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1794/
Test FAILed (JDK 8 and Scala 2.11).
"
281740050,2466,mjsax,2017-02-22T17:27:17Z,Retest this please.
281752894,2466,asfbot,2017-02-22T18:11:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1808/
Test FAILed (JDK 8 and Scala 2.12).
"
281753358,2466,asfbot,2017-02-22T18:12:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1810/
Test FAILed (JDK 8 and Scala 2.11).
"
281753995,2466,asfbot,2017-02-22T18:14:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1807/
Test FAILed (JDK 7 and Scala 2.10).
"
281839301,2466,mjsax,2017-02-22T23:27:26Z,The test errors are a little weird. The test that fails is not part of you current PR branch but was added to `trunk` later on. Can you please rebase to `trunk` -- I hope this fixed the issue. Not sure why it occurred in the first place.
282328998,2466,jeyhunkarimov,2017-02-24T16:03:52Z,"The tests will fail when there are no source nodes defined in the test topology. Also, if the defined source nodes are not related with partitions, the tests will also fail due to the NullPointerException in `StreamTask` 125: 
               
   ```
final SourceNode source = findSource(partition);   // <- here.  if partitions and source nodes don't match source will be null
final TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor() != null ? source.getTimestampExtractor() : defaultTimestampExtractor;
```


Maybe we should add extra check specifically for that issue, when validating the topology. "
282337368,2466,asfbot,2017-02-24T16:34:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1837/
Test FAILed (JDK 8 and Scala 2.11).
"
282339145,2466,asfbot,2017-02-24T16:40:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1835/
Test PASSed (JDK 8 and Scala 2.12).
"
282339228,2466,asfbot,2017-02-24T16:41:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1834/
Test FAILed (JDK 7 and Scala 2.10).
"
282346689,2466,mjsax,2017-02-24T17:09:16Z,"> The tests will fail when there are no source nodes defined in the test topology.

I think that's fine -- for tests it would be a testing error to not add a source. Not sure if we have a proper test for real topologies to guard against it (only required for PAPI) -- for KStreamBuilder it cannot happen. Maybe `TopologyBuilder` should contain a test that ensures that the DAG is connected and ""well-formed"" ? But I actually think, we would not even generate the `ProcessorTopology` in the first place -- we start building it from the sources, and if no source was added, the whole `ProcessorTopology` would be empty.

> Also, if the defined source nodes are not related with partitions, the tests will also fail due to the NullPointerException in StreamTask 125:

This would also be a testing error only. For real topologies, if there are not partitions, we would not create a task for the `SourceNode` in the first place and thus, this code should never be executed.

\cc @enothereska @dguy @guozhangwang (does this make sense?)"
282424079,2466,jeyhunkarimov,2017-02-24T22:33:57Z,"Sorry I had to rebase again, because of conflicts."
282426549,2466,mjsax,2017-02-24T22:46:47Z,"No worries @jeyhunkarimov For rebasing, you actually don't need to squash previous commit (but I know that rebasing is simple if you have a single commit). However, you could squash and rebase before you do the PR update and put a new commit with the change on top :)"
282431695,2466,asfbot,2017-02-24T23:16:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1846/
Test PASSed (JDK 8 and Scala 2.12).
"
282431752,2466,asfbot,2017-02-24T23:17:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1848/
Test PASSed (JDK 8 and Scala 2.11).
"
282436824,2466,asfbot,2017-02-24T23:51:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1845/
Test PASSed (JDK 7 and Scala 2.10).
"
287639801,2466,asfbot,2017-03-19T19:17:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2268/
Test PASSed (JDK 8 and Scala 2.11).
"
287639957,2466,asfbot,2017-03-19T19:19:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2265/
Test PASSed (JDK 7 and Scala 2.10).
"
287642221,2466,asfbot,2017-03-19T19:49:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2265/
Test PASSed (JDK 8 and Scala 2.12).
"
288574029,2466,asfbot,2017-03-22T23:45:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2346/
Test FAILed (JDK 8 and Scala 2.11).
"
288574130,2466,asfbot,2017-03-22T23:45:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2342/
Test FAILed (JDK 8 and Scala 2.12).
"
288574134,2466,asfbot,2017-03-22T23:45:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2342/
Test FAILed (JDK 7 and Scala 2.10).
"
288585051,2466,asfbot,2017-03-23T00:53:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2343/
Test FAILed (JDK 8 and Scala 2.12).
"
288585432,2466,asfbot,2017-03-23T00:56:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2347/
Test FAILed (JDK 8 and Scala 2.11).
"
288585759,2466,asfbot,2017-03-23T00:58:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2343/
Test FAILed (JDK 7 and Scala 2.10).
"
288672562,2466,asfbot,2017-03-23T10:08:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2345/
Test PASSed (JDK 7 and Scala 2.10).
"
288672629,2466,asfbot,2017-03-23T10:08:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2345/
Test PASSed (JDK 8 and Scala 2.12).
"
288674025,2466,jeyhunkarimov,2017-03-23T10:14:55Z,"Sorry, I had to rebase again as there were a lot of conflicts among my commits and among mine and other commits. I deprecated  
```
StreamConfig.TIMESTAMP_EXTRACTOR_CLASS_CONFIG
StreamConfig.TIMESTAMP_EXTRACTOR_CLASS_DOC
StreamConfig.KEY_SERDE_CLASS_CONFIG
StreamConfig.KEY_SERDE_CLASS_DOC
StreamConfig.VALUE_SERDE_CLASS_CONFIG
StreamConfig.VALUE_SERDE_CLASS_DOC
StreamConfig.keySerde()
StreamConfig.valueSerde()
```

For test classes, I replaced the deprecated variables with new ones. For non-test classes, I put an if condition to check both of the variables. "
288700938,2466,asfbot,2017-03-23T12:17:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2349/
Test FAILed (JDK 8 and Scala 2.11).
"
289076234,2466,asfbot,2017-03-24T16:42:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2365/
Test PASSed (JDK 7 and Scala 2.10).
"
289078717,2466,asfbot,2017-03-24T16:51:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2369/
Test PASSed (JDK 8 and Scala 2.11).
"
289082529,2466,asfbot,2017-03-24T17:04:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2365/
Test FAILed (JDK 8 and Scala 2.12).
"
289434341,2466,jeyhunkarimov,2017-03-27T12:08:54Z,@mjsax can you please check?
297852198,2466,asfbot,2017-04-27T22:05:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3230/
Test PASSed (JDK 8 and Scala 2.12).
"
297853361,2466,asfbot,2017-04-27T22:10:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3234/
Test PASSed (JDK 7 and Scala 2.10).
"
297855439,2466,asfbot,2017-04-27T22:22:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3240/
Test PASSed (JDK 8 and Scala 2.11).
"
297880899,2466,mjsax,2017-04-28T01:12:47Z,"@jeyhunkarimov Can you rebase once again -- we merged a few conflicting PR today. Sorry for that.

Call for review @enothereska @dguy "
297976181,2466,asfbot,2017-04-28T11:37:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3275/
Test PASSed (JDK 7 and Scala 2.10).
"
297977017,2466,asfbot,2017-04-28T11:42:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3280/
Test PASSed (JDK 8 and Scala 2.11).
"
297980007,2466,asfbot,2017-04-28T11:59:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3271/
Test PASSed (JDK 8 and Scala 2.12).
"
298576227,2466,enothereska,2017-05-02T09:30:30Z,@guozhangwang could we get this in please? Thanks.
300709543,2466,asfbot,2017-05-11T07:44:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3743/
Test FAILed (JDK 7 and Scala 2.10).
"
300730767,2466,asfbot,2017-05-11T09:10:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/3749/
Test PASSed (JDK 7 and Scala 2.11).
"
300732365,2466,asfbot,2017-05-11T09:16:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3739/
Test PASSed (JDK 8 and Scala 2.12).
"
301222605,2466,asfbot,2017-05-13T03:39:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/3859/
Test PASSed (JDK 7 and Scala 2.11).
"
301223588,2466,asfbot,2017-05-13T04:06:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3848/
Test PASSed (JDK 8 and Scala 2.12).
"
490292130,6694,ConcurrencyPractitioner,2019-05-07T23:36:19Z,ping @mjsax @guozhangwang @vvcephei for review
490298156,6694,ableegoldman,2019-05-08T00:07:02Z,Thanks for the PR! I agree this seems like a straightforward patch but I'm wondering if we shouldn't try and think through the eos case a bit more? Or is there really no way to safely cover it as well?
490311761,6694,ConcurrencyPractitioner,2019-05-08T01:25:09Z,"Hi @ableegoldman Thanks for reviewing! I was planning on attacking the eos case. But as you could guess from the code, trying to retrieve the metadata committed is not as simple as in the non eos case. I was hoping for some input on that. So some small amount of advice is greatly appreciated. :)"
490746569,6694,ConcurrencyPractitioner,2019-05-09T05:14:43Z,"Oh, just found out something. Regardless if it is eos case or not, calling ```KafkaConsumer#committed()``` should be sufficient. So there shouldn't be any big problems with this. I will try to add a test case. :)"
490750108,6694,ableegoldman,2019-05-09T05:34:30Z,"That sounds reasonable. Looks like the build failed on checkstyle, can you try running it?

+1 on adding a test case(s)"
491130846,6694,ConcurrencyPractitioner,2019-05-10T02:23:09Z,"Alright, done. @mjsax Added a test case as well. Would be good if you could take a look. :)"
493146367,6694,ConcurrencyPractitioner,2019-05-16T16:49:44Z,pinging @mjsax and @guozhangwang  for review
493695650,6694,ConcurrencyPractitioner,2019-05-18T18:01:32Z,"Oh sorry, my bad. Underestimated the scope of the PR. Sorry for pinging you guys. Will dig some more."
497908544,6694,ConcurrencyPractitioner,2019-06-01T03:31:40Z,cc @mjsax I have rebased the PR. Want to take a look?
498844777,6694,ConcurrencyPractitioner,2019-06-04T21:09:44Z,@guozhangwang You mind taking a look?
509822799,6694,ConcurrencyPractitioner,2019-07-09T21:49:15Z,"@mjsax Ok, so I added a test to try and see if we can handle the eos enabled scenario and it doesn't appear to be that straightforward (since the test fails). We either have to mess around with the ```application.id``` config or think of something else. 

You could see the test failure in Jenkins, and the NullPointerException indicates that we cannot retrieve the timestamp from consumer when eos is enabled (probably since we used producer's commit API instead of consumer's)."
509877987,6694,ConcurrencyPractitioner,2019-07-10T02:05:31Z,"@mjsax There is a question relating to TimestampExtractor and its usage that has been bugging me a little. I have added a ```partitionTime``` instance field to RecordQueue, and then we use that as the ```previousTimestamp``` input argument. Is that the right way to use it? After all, from what I could understand of TimestampExtractor, the previous timestamp should be the previous _record_'s timestamp rather than the max timestamp seen so far."
510540171,6694,bbejeck,2019-07-11T15:42:11Z,retest this please
510693180,6694,ConcurrencyPractitioner,2019-07-11T23:47:07Z,@mjsax You want to check this out?
511595872,6694,ConcurrencyPractitioner,2019-07-15T22:38:48Z,@bbejeck @mjsax might want some of your input.
512005573,6694,mjsax,2019-07-16T22:03:58Z,With regard to `previousTime` it's actually a miss leading name. There is already a PR to clean this up: https://github.com/apache/kafka/pull/7054 -- it should be called `partitionTime`.
512015515,6694,mjsax,2019-07-16T22:21:59Z,"The test result are not available any longer. However, looking into the test code, I assume that the issue is, that it is a unit test. For this case, the commit of a producer does not make it to the consumer (because there is no broker, and the consumer is mocked). Hence, we need to mock that the consumer is returning to correct metadata -- I guess, the test setup code will need to call `mockConsumer#commitSync(Map<TopicPartition, OffsetAndMetadata> offsets)` to do this.

However, we also need to have an integration test to verify that the producer committed offsets make it to the consumer, too. I think we need a completely new test (maybe called `PartitionTimeIntegrationTest`) that uses an `EmbeddedKafkaCluster`. Compare the existing integration tests for this. The integration test should run twice, once with EOS enabled and once with EOS disabled.

Does this make sense?"
512031374,6694,ConcurrencyPractitioner,2019-07-16T22:49:57Z,"Oh, so that was the reason why the eos case wasn't working. It had never occurred to me that there was no broker, causing the failure. Thanks for the heads up on that one. 

The integration test I will add. We will definitely need one.

"
512606577,6694,ConcurrencyPractitioner,2019-07-17T23:32:22Z,Retest this please.
512892534,6694,ConcurrencyPractitioner,2019-07-18T16:36:07Z,"Alright, so after some checks, it appears that if we move the logic for restoring the correct partitionTime to newTasks, it will cause several tests relating to resetting streams to break, particularly in ResetIntegrationTest. I've traced the root cause to be possibly both the committed() call / the setPartitionTime() call. The why of what happened is still unknown, but we need to dig deeper.
"
513067124,6694,ConcurrencyPractitioner,2019-07-19T02:36:29Z,"@mjsax What do you think about this? When moving this logic into newTasks(), several tests seem to break due to it. Your thoughts?"
513328157,6694,mjsax,2019-07-19T18:20:32Z,"Test results are not available any longer. Checked out the PR and ran it locally.

- `AssignedStreamsTasksTest` -> just needs some updates to the mocks

Bunch of integration test fail all with the same error message:
```
java.lang.NumberFormatException: For input string: """"
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
	at java.lang.Long.parseLong(Long.java:601)
	at java.lang.Long.parseLong(Long.java:631)
	at org.apache.kafka.streams.processor.internals.StreamTask.retrieveCommittedTimestamp(StreamTask.java:722)
	at org.apache.kafka.streams.processor.internals.StreamTask.setAssignmentToStoredTimestamps(StreamTask.java:734)
	at org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks(AssignedTasks.java:74)
	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:325)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:863)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:793)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:762)
```
or
```
java.lang.NullPointerException
	at org.apache.kafka.streams.processor.internals.PartitionGroup.setPartitionTimestamp(PartitionGroup.java:95)
	at org.apache.kafka.streams.processor.internals.StreamTask.retrieveCommittedTimestamp(StreamTask.java:723)
	at org.apache.kafka.streams.processor.internals.StreamTask.setAssignmentToStoredTimestamps(StreamTask.java:734)
	at org.apache.kafka.streams.processor.internals.AssignedTasks.initializeNewTasks(AssignedTasks.java:74)
	at org.apache.kafka.streams.processor.internals.TaskManager.updateNewAndRestoringTasks(TaskManager.java:325)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:863)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:793)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:762)
```

This is definitely your new code that does not yet work properly. You will need to dig into those issues and fix your code."
513356215,6694,tedyu,2019-07-19T19:51:23Z,retest this please
513479272,6694,ConcurrencyPractitioner,2019-07-20T16:03:46Z,"There is some System.out.println statements I added for debugging purposes. Later, I will remove them."
513562958,6694,ConcurrencyPractitioner,2019-07-21T15:15:57Z,Retest this please.
513981439,6694,ConcurrencyPractitioner,2019-07-22T22:38:42Z,"@mjsax Alright, added an integration test for both eos and non eos case. You think we need anything else?"
514459621,6694,ConcurrencyPractitioner,2019-07-24T02:58:26Z,Call for review from @ableegoldman @guozhangwang @vvcephei @mjsax.
516211153,6694,ConcurrencyPractitioner,2019-07-30T00:26:28Z,"@mjsax I guess we want to try to wrap this PR up?
"
517098808,6694,ConcurrencyPractitioner,2019-08-01T02:59:49Z,"Ok, test modifications are a little more complicated since when I converted the test I added to a more simple ```builder.stream().to()``` test, it failed. Still working to identify the cause. (Some debugging statements has been added, they will be removed shortly)."
517497668,6694,ConcurrencyPractitioner,2019-08-01T23:55:09Z,"Alright, I have proof that initializeTaskTime is called at the wrong location. It appears that when we switch to a simple builder.stream().to() program, it is _never called_. I've found that if you put it in front of the addRecords() method, it will be called regardless. So I thought it was a pretty safe bet."
517778823,6694,ConcurrencyPractitioner,2019-08-02T17:13:49Z,Okay @mjsax Addressed most of your comments.
517871590,6694,ConcurrencyPractitioner,2019-08-02T23:30:56Z,"@mjsax About why initializeTaskTime() and the reasons it was moved. In a comment above, I actually stated the reason:

""Alright, I have proof that initializeTaskTime is called at the wrong location. It appears that when we switch to a simple builder.stream().to() program, it is never called [where it is right now in initializeNewTasks()]. I've found that if you put it in front of the addRecords() method, it will be called regardless. So I thought it was a pretty safe bet.""

Strange, I know. Integration test however seems to indicate this should be the better approach.



"
517874623,6694,mjsax,2019-08-02T23:53:49Z,"Thinking about it again, maybe you added it to the wrong line in `initializeNewTasks()`
```
                if (!entry.getValue().initializeStateStores()) {
                    log.debug(""Transitioning {} {} to restoring"", taskTypeName, entry.getKey());
                    ((AssignedStreamsTasks) this).addToRestoring((StreamTask) entry.getValue());
                } else {
 // should be added here
                    transitionToRunning(entry.getValue());
                }
```

However, we call `transitionToRunning()` multiple times and need to call `initializeTaskTime()` each time before we. Hence, it would be simpler to call initializeTaskTime() within `transitionToRunning()`
"
517876334,6694,ConcurrencyPractitioner,2019-08-03T00:06:56Z,"Oh, nice catch! Tried it out and it worked. I didn't think that we would have to move it to the else branch of the method."
517888203,6694,ConcurrencyPractitioner,2019-08-03T02:45:34Z,"Oh @mjsax Looks like I can get rid of the sleep now. Weird, could no longer replicate the behavior any longer. Might have been a fluke."
518775221,6694,ConcurrencyPractitioner,2019-08-06T17:51:41Z,@mjsax @guozhangwang Do you think that this PR as it is could be merged? Feel like its pretty close to completion.
519186670,6694,ConcurrencyPractitioner,2019-08-07T17:08:13Z,Retest this please.
519339761,6694,ConcurrencyPractitioner,2019-08-08T02:37:49Z,"@mjsax Actually, bouncycastle dependency has been added. There shouldn't be any problems from here on out. So everything should be fine from here on out."
519598495,6694,ConcurrencyPractitioner,2019-08-08T16:46:28Z,"@mjsax all comments has been addressed. 

A thought did occur to me though while doing this PR. If a streams has been restarted by the user, shouldn't the user expect the timestamp of a StreamTask to be -1 before it starts processing (meaning that there is no retrieval of committed timestamps) ?  I don't really know if a streams restart is a good simulation of a crash because it might perform contrary to user expectations."
519618724,6694,mjsax,2019-08-08T17:42:22Z,"Not sure. Stopping and restarting an application is the same as fail-over. If there are committed offsets you want to continue where you left off, and the application should be in the exact some state as when you stopped it. Hence, why would you expect -1 as start timestamp?

Of course, if you reset the application and loose the offsets, you trigger auto-offset-reset and you would start with -1 (similar if you use the reset tool to seek to a specific start offset).

Hence, I don't think this would be a problem."
519659608,6694,ConcurrencyPractitioner,2019-08-08T19:40:20Z,@mjsax Think we need anything else?
519725293,6694,ConcurrencyPractitioner,2019-08-08T23:40:36Z,Retest this please.
519753100,6694,ConcurrencyPractitioner,2019-08-09T02:24:25Z,"@mjsax Ok, comments should be addressed. "
520000977,6694,mjsax,2019-08-09T17:30:01Z,"Java 8:
```
kafka.api.SaslGssapiSslEndToEndAuthorizationTest.testNoDescribeProduceOrConsumeWithoutTopicDescribeAcl
kafka.server.DescribeLogDirsRequestTest.testDescribeLogDirsRequest
```
Java 11 / 2.13:
```
org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector
kafka.api.ConsumerBounceTest.testClose
```
Java 11 / 2.12:
```
org.apache.kafka.connect.integration.RebalanceSourceConnectorsIntegrationTest.testReconfigConnector
kafka.api.AdminClientIntegrationTest.testIncrementalAlterConfigsForLog4jLogLevels
kafka.api.AdminClientIntegrationTest.testIncrementalAlterConfigsForLog4jLogLevelsCanResetLoggerToCurrentRoot
kafka.api.SaslSslAdminClientIntegrationTest.testIncrementalAlterConfigsForLog4jLogLevels
kafka.api.SaslSslAdminClientIntegrationTest.testIncrementalAlterConfigsForLog4jLogLevelsCanResetLoggerToCurrentRoot
```"
520058640,6694,ConcurrencyPractitioner,2019-08-09T20:48:03Z,"Looks like we still need to manually commit offsetAndMetadata. Mock producer doesn't do that for us, interestingly enough."
520096521,6694,mjsax,2019-08-09T23:42:17Z,"Well. I guess I missed to explain something:

The producer adds and commits the offsets as part of the transaction: https://github.com/apache/kafka/pull/6694/files#diff-f1f98a92fd97ac1290e04045f45168e4L468-R474

The `MockProducer` first adds the `OffsetAndMetadata` into its ""uncommitted cache"": https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java

On commit, it moves the offsets to ""committed offsets"": https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/MockProducer.java#L185

Thus, `task.initializeTaskTime();` will not find them because the mock consumer and mock producer are not connected to each other. Hence, your observation is correct.

However, committing the offsets using the mock consumer in the test code, dose not verify if the producer did commit the correct offsets and metadata. Therefore, instead of calling `task.initializeTaskTime()`, the test should check if the producer did the correct commit.

If we really want to keep `initializeTaskTime()`, the test code should extract the offsets from the producer and commit the on the consumer to ""connect"" both mocks. (this is still different to the current code, that provide the offset as part of the test what does not help to test what we want to verify). Does this make sense?

"
520113404,6694,ConcurrencyPractitioner,2019-08-10T03:17:01Z,"@mjsax Thanks for the tip!

Got everything done and updated the StreamTask test. We hopefully should be pretty close, so one last push. :)"
520463985,6694,ConcurrencyPractitioner,2019-08-12T15:05:47Z,@mjsax Any more comments?
520910285,6694,ConcurrencyPractitioner,2019-08-13T16:34:59Z,Retest this please.
521360908,6694,ConcurrencyPractitioner,2019-08-14T18:22:08Z,pinging @mjsax @ableegoldman  @abbccdda @guozhangwang for final review.
521716728,6694,ConcurrencyPractitioner,2019-08-15T17:00:32Z,"@cadonna Alright, done. "
522779481,6694,ConcurrencyPractitioner,2019-08-19T22:33:24Z,"@mjsax @abbccdda @guozhangwang @ableegoldman 
Another round of comments would be good or approval of PR if everything looks right. :)"
524449143,6694,marcospassos,2019-08-23T20:20:27Z,"This issue is the cause of critical bugs we recently faced up in our applications that rely on the `SessionStore` for processing retroactive events.

@mjsax do you think this fix can be included as part of 2.3.1?"
525998259,6694,ConcurrencyPractitioner,2019-08-29T02:45:41Z,@mjsax @cadonna Any last comments?
528057138,6694,ConcurrencyPractitioner,2019-09-04T19:48:23Z,@mjsax pinging.
528148998,6694,mjsax,2019-09-05T00:50:38Z,"@marcospassos I don't think that we will include it in 2.3.1 -- it's not really a bug fix but an improvement.


@ConcurrencyPractitioner I try to review again in the next days."
529068486,6694,ConcurrencyPractitioner,2019-09-07T03:39:25Z,Retest this please.
530728164,6694,cadonna,2019-09-12T08:43:57Z,@ConcurrencyPractitioner All builds reported SpotBug issues.
531017385,6694,ConcurrencyPractitioner,2019-09-12T21:40:26Z,"Yeah, got it fixed."
531208299,6694,cadonna,2019-09-13T11:55:33Z,"The following test failures seem related:
```
org.apache.kafka.streams.integration.ResetPartitionTimeIntegrationTest.shouldPreservePartitionTimeOnKafkaStreamRestart[0: eosEnabled=false]
org.apache.kafka.streams.integration.ResetPartitionTimeIntegrationTest.shouldPreservePartitionTimeOnKafkaStreamRestart[1: eosEnabled=true]
```"
531255894,6694,ConcurrencyPractitioner,2019-09-13T14:20:46Z,"@cadonna  Oh, just realized that @mjsax's comment caused a regression. If you would look earlier in the conversation, you would find a segment where a call to close() resets all partition times to negative one. Therefore, we need to store the partition times in a map _before_ they are reset, and then they are passed into the commit() method.

The extra parameter is needed after all due to the order of operations in close(). We will need to rollback some changes."
533262618,6694,mjsax,2019-09-19T18:53:29Z,Thanks for the hard work @ConcurrencyPractitioner!
385120644,4931,mageshn,2018-04-27T23:47:53Z,@randall thanks for the comments. I will certainly be adding more javadocs when ready for the final review. I still didn't do the Java ServiceProvider API changes because I just wanted to finalize the public changes here. 
390345195,4931,mageshn,2018-05-18T22:06:46Z,retest this please
390345486,4931,ewencp,2018-05-18T22:08:17Z,retest this please
390535156,4931,mageshn,2018-05-21T02:16:08Z,retest this please
391540475,4931,mageshn,2018-05-23T23:56:12Z,retest this please
391870227,4931,mageshn,2018-05-24T21:33:56Z,@rhauch any final thoughts here before I request a committer to take a look?
391898959,4931,rhauch,2018-05-24T23:44:49Z,@mageshn there are a couple of unaddressed comments.
392278971,4931,mageshn,2018-05-26T18:22:33Z,@rhauch I have addressed all the comments from your last pass.
392637083,4931,mageshn,2018-05-29T02:40:02Z,@ewencp @rhauch What are you thoughts about enforcing a version to be non null & nonempty? I think we can enforce this for rest extension only when one tries to use it.
392879147,4931,mageshn,2018-05-29T18:08:32Z,@ewencp @rhauch I have pushed a change where i check for the version when the plugin gets instantiated. This should be effective for all plugins that use the newPlugin() method in future.
393006763,4931,mageshn,2018-05-30T02:06:28Z,Thanks @rhauch . Lots of great detailed comments that helped this PR evolve.
393030354,4931,mageshn,2018-05-30T04:56:00Z,"Thanks @ewencp . Regarding the ConnectClusterState being mutable, I think that's the real use of it. However in the implementation the 2 methods that it exposes are currently not atomic. Also, the ConnectClusterState doesn't expose the connector configs at all, so I'm not how KIP-297 would affect this."
456933267,6177,abbccdda,2019-01-23T19:24:55Z,@guozhangwang @hachikuji Call for review folks!
458241606,6177,abbccdda,2019-01-28T18:15:08Z,Thanks Stanis for the great suggestions. Could I get some further review? @hachikuji @stanislavkozlovski @guozhangwang ?
459831480,6177,abbccdda,2019-02-01T19:02:24Z,Pinging for more reviews @hachikuji @stanislavkozlovski @guozhangwang thanks a lot!
460243061,6177,stanislavkozlovski,2019-02-04T13:05:05Z,"@abbccdda for future reference, could we not force-push changes after review comments? We will squash everything in the end anyway. It makes subsequent reviews much easier"
460350699,6177,abbccdda,2019-02-04T18:07:10Z,"@stanislavkozlovski Sounds good, I will try to see if I could work around that when I rebase to trunk."
466228114,6177,abbccdda,2019-02-22T00:46:06Z,Retest this please
468482972,6177,abbccdda,2019-02-28T23:35:59Z,Retest this please
468744310,6177,abbccdda,2019-03-01T17:28:12Z,Retest this please
468834722,6177,abbccdda,2019-03-01T22:33:31Z,@hachikuji Do you have time to take a closer look at the group coordinator logic?
470380031,6177,abbccdda,2019-03-07T04:26:42Z,"Retest this please
"
470385891,6177,abbccdda,2019-03-07T05:03:16Z,"@hachikuji I have addressed most comments except two things:
1. the assertion vs exception
2. use Option[String] instead of Option

I have written my thoughts on both questions. For #2, my question is primarily on whether we should still use option when upstream is confirming to pass in a valid string?
Let me hear your feedback, thank you!"
470752834,6177,hachikuji,2019-03-08T00:12:05Z,"@abbccdda Thanks for the updates. There is one additional point that I wanted to discuss. Let's use the following example. Suppose we have three consumers in the group with static instance ids: A, B, and C. Assume we have a stable group and the respective memberIds are 1, 2, and 3. So inside the group coordinator, we have the following state:

```
members: {A=1, B=2, C=3}
generation: 5
```

In fact, the consumer leader of the group is not aware of the instance ids of the members. So it sees the membership as:

```
members: {1, 2, 3}.
generation: 5
```

Now suppose that A does a rolling restart. After restarting, the coordinator will assign a new memberId to A and let it continue using the previous assignment. So we now have the following state:

```
members: {A=4, B=2, C=3}
generation: 5
```

The leader on the other hand still sees the members in the group as {1, 2, 3} because it does not know that member A restarted and was given a new memberId. Suppose that eventually something causes the group to rebalance (e.g. maybe a new topic was created).

When the leader attempts its assignment, it will see the members {2, 3, 4}. My basic question is how can it know that 4 is the same member as 1? I think the proposal essentially relies on some auxiliary information provided by streams in order to determine this, but I cannot think of a good reason why we should not just use the instance id itself. In other words, we can change the JoinGroup response to include both the instance id and the member id.

```
    { ""name"": ""Members"", ""type"": ""[]JoinGroupResponseMember"", ""versions"": ""0+"", ""fields"": [
      { ""name"": ""InstanceId"", ""type"": ""string"", ""versions"": ""0+"",
        ""about"": ""The group instance ID or null for dynamic members"" },
      { ""name"": ""MemberId"", ""type"": ""string"", ""versions"": ""0+"",
        ""about"": ""The group member ID."" },
      { ""name"": ""Metadata"", ""type"": ""bytes"", ""versions"": ""0+"",
        ""about"": ""The group member metadata."" }
    ]}
```

This approach provides some benefit even for the simple partition assignors. Consider, the default range assignor, for example. Basically it works by sorting the members in the group and then assigning partition ranges to achieve balance. Suppose we have a partition with 9 partitions. If the membership were {1, 2, 3}, then the assignment would be the following:

```
memberId: 1, assignment: {0, 1, 2}
memberId: 2, assignment: {3, 4, 5}
memberId: 3, assignment: {6, 7, 8}
```

Now when the membership changes to {2, 3, 4}, then all the assignments change as well:

```
memberId: 2, assignment: {0, 1, 2}
memberId: 3, assignment: {3, 4, 5}
memberId: 4, assignment: {6, 7, 8}
```

So basically all of the assignments change even though it's the same static members. However, if we could consider the instanceId as the first sort key, then we can compute the assignment consistently even across restarts:

```
instanceId: A, memberId: 1, assignment: {0, 1, 2}
instanceId: B, memberId: 2, assignment: {3, 4, 5}
instanceId: C, memberId: 3, assignment: {6, 7, 8}
```

And after the restart:

```
instanceId: A, memberId: 4, assignment: {0, 1, 2}
instanceId: B, memberId: 2, assignment: {3, 4, 5}
instanceId: C, memberId: 3, assignment: {6, 7, 8}
```

To summarize, I think basically what I'm saying is that the full benefit of static assignment can only be realized if the assignor knows the instance ids of the members in the group. It shouldn't be necessary to do anything fancy with additional metadata. 

If that makes sense, I'd suggest that we alter the JoinGroup response here since we are bumping the protocol anyway. We can leave any changes to the `PartitionAssignor` interface for another PR."
471014920,6177,guozhangwang,2019-03-08T17:45:30Z,"I like @hachikuji 's idea, it looks a good thing to have with very small cost on protocol changes."
471020583,6177,abbccdda,2019-03-08T18:03:41Z,"@hachikuji Thanks for the great proposal Jason! I think the benefit of this change is more valuable for range or round-robin assignors because they rely on member id to do the sharding. For sticky assignors, my understanding is that as long as the application is not rolling restart, we should preserve the same subscription info when rebalance happens. The leader should be able to see the current subscription info for all members IIUC?  I would definitely take some time to read through the code and make sure I understand the sticky assignment better."
472058830,6177,hachikuji,2019-03-12T15:51:31Z,"@abbccdda Yeah, my point is that it is generally useful for assignors to have access to the static ids of the current group members. The static id allows us to reliably detect the same instance across restarts which frees users from having to either embed the static id in the user data or implement some kind of heuristic to try and guess the same member. Anyway, I think it would be a missed opportunity to introduce this fancy new notion of static id and not allow the assignors to leverage it. Do you see any downsides?"
472079669,6177,abbccdda,2019-03-12T16:37:25Z,"@hachikuji Nope, after reading the code I think the idea is brilliant! Could you first take a look at
https://github.com/apache/kafka/pull/6419
since I need to automate join group protocol first, thank you!"
472462552,6177,stanislavkozlovski,2019-03-13T15:07:43Z,fwiw I also think that idea is great. Could we make sure to update the KIP if we'll be following this approach?
472539911,6177,abbccdda,2019-03-13T18:04:28Z,"@stanislavkozlovski yep, I will definitely do that!"
476050340,6177,abbccdda,2019-03-25T04:23:21Z,Retest this please
478372433,6177,guozhangwang,2019-03-31T19:25:53Z,"> If that makes sense, I'd suggest that we alter the JoinGroup response here since we are bumping the protocol anyway. We can leave any changes to the `PartitionAssignor` interface for another PR.

We should also update KIP-345 itself for both the protocol change, as well as the Susbscription class (as it's a public class) which is passed via `PartitionAssignor#assign`."
478382417,6177,abbccdda,2019-03-31T20:53:04Z,"@guozhangwang thanks Guozhang for the great summary! I will address the agreed changes and update the KIP for subscription class change.
"
480142674,6177,abbccdda,2019-04-05T04:15:04Z,"@guozhangwang ah, I checked the `Subscription` class and it doesn't contain any id field
```
class Subscription {
        private final List<String> topics;
        private final ByteBuffer userData;
```
What are we trying to add here?"
480145428,6177,guozhangwang,2019-04-05T04:36:14Z,"> What are we trying to add here?

As @hachikuji suggested, we want to encode the instance.id into the subscription so that assignor can behave on sticky assignment."
480400795,6177,hachikuji,2019-04-05T19:48:42Z,"@abbccdda @guozhangwang I wanted to suggest an alternative to the proposal above for handling the ""dueling consumers"" problem. I would say the underlying issue is that the coordinator cannot distinguish the two cases when a memberId has been replaced and when it has been removed. If it could, then we would be able to use MEMBER_ID_MISMATCH consistently and the consumer could treat this error as fatal. Here are two options:

1. Include InstanceId in the Heartbeat and OffsetCommit APIs. Then the coordinator can return the proper error code.
2. We can can use a convention to embed the instanceId into the generated memberId. At the moment, the current format is `{clientId}-{random uuid}`. For static members, I think instanceId is more useful than clientId and we could probably use timestamp as a more concise alternative to uuid. So we could have `{instanceId}-{timestamp}` as the memberId for static members. Then we would be able to extract this from any request and the coordinator could use the proper error code.

My preference is probably for option 1 because it is explicit, but I know Guozhang expressed some unwillingness to change additional APIs. A nice benefit of option 2 is that it will be easy to see the instanceId of existing members using tools like `kafka-consumer-groups.sh`. This will make debugging easier."
480439917,6177,guozhangwang,2019-04-05T22:19:20Z,"For option 1), today we call resetGeneration on five places upon `UNKNOWN_PRODUCER_ID`: leave, join, sync, commit, heartbeat. I think for leave-group we can probably not adding instance.id, but for other three excluding join, we will need to add instance.id to make it work. My personal feeling is it is a bit too intrusive to add on all these request protocols, but honestly I do not have a very objective rationale either.

Option 2) looks better to me, although it requires to make such string construction rules as part of the public APIs since other client implementations would need to rely on that as well."
480445141,6177,hachikuji,2019-04-05T22:46:33Z,"@guozhangwang I think we should probably use something like the member ID generation scheme proposed in 2) in any case, whether or not we rely on it internally. I think it will be useful for debugging to always be able to derive the instance id from the member id. 

I'm a bit torn on whether or not to include instance ID in APIs that you mentioned. It seems like a cleaner solution to me because it allows the coordinator to be unambiguous about the error. I think the approach which requires retrying the JoinGroup in order to disambiguate the error is a little hard to explain. I also feel a little annoyed about modifying more protocols, but I'm not really sure we should be trying to optimize for the number of protocol bumps.

By the way, if we cannot rely on the generated memberId, then it seems inevitable that we will also need to modify the DescribeGroup API so that it is easy to find the instance Ids associated with current members."
480647194,6177,abbccdda,2019-04-08T00:30:34Z,"Thanks for the discussion here! @guozhangwang @hachikuji In high level, I think both approaches make sense here. The issue with 1) is that we are altering many protocols all at once just for the purpose of solving a client mis-configured scenario, which seems like an over-kill to me. The attractive side of 2) is extra benefit in terms of debugging, however it still involves a lot of logic changes to double check static member's identity in all consumer protocols and client needs to handle `MEMBER_ID_MISMATCH` exception everywhere, not mentioning corresponding unit tests.

No matter which approach we eventually take, it seems better to have the change go in with another PR to reduce the review burden for the current one, which is already high. The severity of instance id conflict should be tolerable for now, and I think we still have enough time to get the fix diff ready before 2.3 ddl. So if you both agree, I could revert the generation reset change and leave the id conflict fix for next diff if possible. WDYT?"
480672408,6177,guozhangwang,2019-04-08T03:38:44Z,"> So if you both agree, I could revert the generation reset change and leave the id conflict fix for next diff if possible. WDYT?

Yeah I think we should keep this PR small as it already grows from 600 to about 1200 LOC anyways. I'm fine with having a follow-up PR with whatever approach we've agreed upon to handle the instance id conflict issue."
480693499,6177,abbccdda,2019-04-08T05:55:07Z,"@guozhangwang sounds good, will revert the current changes of `resetGeneration()`."
481373001,6177,hachikuji,2019-04-09T18:22:06Z,@abbccdda @guozhangwang Sounds good to me to resolve this problem separately. I'll do another pass on the PR today and hopefully we can merge this week.
481796564,6177,abbccdda,2019-04-10T17:56:37Z,"`Let me try one more time. How about FENCED_INSTANCE_ID?`
Lol, you got it. @hachikuji "
482182541,6177,abbccdda,2019-04-11T16:18:32Z,Retest this please
482215766,6177,abbccdda,2019-04-11T17:26:35Z,"@hachikuji Added most comments except ones that we haven't cleared, do you mind taking another look when possible? Thank you!"
482400070,6177,guozhangwang,2019-04-12T01:24:51Z,"I've made another pass on it. One meta comment is that we've not yet updated the web docs for this -- if you want to do it in another PR that's fine, just pointing it out so we do not forget about it.

Otherwise, I'm +1 modulo @hachikuji 's comments. "
482429127,6177,abbccdda,2019-04-12T04:02:49Z,@guozhangwang That sounds good Guozhang! Will do it in another PR.
483362126,6177,abbccdda,2019-04-15T18:17:43Z,@hachikuji Could you take another when you got time? Thank you!
484186080,6177,abbccdda,2019-04-17T17:27:51Z,Retest this please
484348012,6177,abbccdda,2019-04-18T04:04:25Z,"@hachikuji I refactored the usage of `group.instance.id` on client side to use `Option<String>` so that we could avoid string comparison. Let me know if this works, thank you! "
484684160,6177,hachikuji,2019-04-18T20:46:45Z,retest this please
484955018,6177,abbccdda,2019-04-19T16:52:21Z,Retest this please
484955097,6177,abbccdda,2019-04-19T16:52:41Z,@hachikuji @guozhangwang do you think we have luck to get this done today?
485038316,6177,guozhangwang,2019-04-19T23:37:08Z,retest this please
485040283,6177,abbccdda,2019-04-19T23:55:03Z,@guozhangwang I think the failure is due to a flakey test FYI
485060306,6177,guozhangwang,2019-04-20T05:32:15Z,Some of the consumer / upgrade tests have failed: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2019-04-20--001.1555729714--abbccdda--345_merge--469cdd8/report.html
485294349,6177,abbccdda,2019-04-22T00:39:09Z,"@guozhangwang @hachikuji FYI, in `VerifiableConsumer.java`, I realized that I couldn't do 
`consumerProps.put(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, null);`
because hashtable rejects null value. So I did a workaround to use empty string as default value for `group.instance.id` but still keeps all the Optional classes in the consumer/broker code paths.
"
485474245,6177,hachikuji,2019-04-22T16:52:54Z,"> @guozhangwang @hachikuji FYI, in VerifiableConsumer.java, I realized that I couldn't do
> consumerProps.put(ConsumerConfig.GROUP_INSTANCE_ID_CONFIG, null);
> because hashtable rejects null value. So I did a workaround to use empty string as default value for group.instance.id but still keeps all the Optional classes in the consumer/broker code paths.

Shouldn't really be necessary. For `Properties` objects, you can just leave out the key if the value is null."
485553178,6177,abbccdda,2019-04-22T21:07:49Z,"@hachikuji That's a bit tricky since non-existing key get will through exception:
```
protected Object get(String key) {
        if (!values.containsKey(key))
            throw new ConfigException(String.format(""Unknown configuration '%s'"", key));
        used.add(key);
        return values.get(key);
    }
```
Should we do a try-catch in `KafkaConsumer` for instance id retrieval then?"
485599691,6177,guozhangwang,2019-04-23T00:38:26Z,"> Should we do a try-catch in `KafkaConsumer` for instance id retrieval then?

Hmm that's weird, if we have a default value defined already, then upon the config object construction if there's no user-provided value the default value will be used to fill in the `values` map (note only the `originals` map contains user-overridden values)."
486749973,6177,abbccdda,2019-04-25T16:44:51Z,"Retest this please
"
486845915,6177,guozhangwang,2019-04-25T21:33:15Z,"I've taken another look at the added commits, and besides this minor https://github.com/apache/kafka/pull/6177/commits/edb7ba416726a1a1becc8e6b4c4bae8f6f03ced0#r278739994 it lgtm."
486848243,6177,abbccdda,2019-04-25T21:41:50Z,https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2540/ This shows that the system tests are fixed now.
486848349,6177,guozhangwang,2019-04-25T21:42:09Z,Triggered https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2540/parameters/ (`345_system_test` is on top of `345_merge`) and passed.
486867096,6177,abbccdda,2019-04-25T22:58:37Z,https://jenkins.confluent.io/job/system-test-kafka-branch-builder/2541/ triggered and passed exactly on top of current `345_merge` branch.
486885313,6177,abbccdda,2019-04-26T00:40:41Z,Retest this please
487161856,6177,guozhangwang,2019-04-26T18:45:23Z,"Thanks for the great patience and great work on the first step towards KIP-345, @abbccdda."
290519242,2772,asfbot,2017-03-30T19:28:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2546/
Test FAILed (JDK 8 and Scala 2.12).
"
290519257,2772,asfbot,2017-03-30T19:28:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2546/
Test FAILed (JDK 7 and Scala 2.10).
"
290519311,2772,asfbot,2017-03-30T19:29:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2550/
Test FAILed (JDK 8 and Scala 2.11).
"
290521965,2772,asfbot,2017-03-30T19:39:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2547/
Test FAILed (JDK 8 and Scala 2.12).
"
290521968,2772,asfbot,2017-03-30T19:39:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2547/
Test FAILed (JDK 7 and Scala 2.10).
"
290522013,2772,asfbot,2017-03-30T19:39:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2551/
Test FAILed (JDK 8 and Scala 2.11).
"
290523073,2772,asfbot,2017-03-30T19:43:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2548/
Test FAILed (JDK 7 and Scala 2.10).
"
290523123,2772,asfbot,2017-03-30T19:43:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2548/
Test FAILed (JDK 8 and Scala 2.12).
"
290523152,2772,asfbot,2017-03-30T19:44:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2552/
Test FAILed (JDK 8 and Scala 2.11).
"
290524952,2772,asfbot,2017-03-30T19:51:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2549/
Test FAILed (JDK 8 and Scala 2.12).
"
290525019,2772,asfbot,2017-03-30T19:51:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2553/
Test FAILed (JDK 8 and Scala 2.11).
"
290525170,2772,asfbot,2017-03-30T19:51:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2549/
Test FAILed (JDK 7 and Scala 2.10).
"
290528486,2772,asfbot,2017-03-30T20:03:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2551/
Test FAILed (JDK 8 and Scala 2.12).
"
290528647,2772,asfbot,2017-03-30T20:03:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2555/
Test FAILed (JDK 8 and Scala 2.11).
"
290528765,2772,asfbot,2017-03-30T20:04:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2551/
Test FAILed (JDK 7 and Scala 2.10).
"
290543017,2772,asfbot,2017-03-30T20:58:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2553/
Test PASSed (JDK 8 and Scala 2.12).
"
290543757,2772,asfbot,2017-03-30T21:01:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2553/
Test PASSed (JDK 7 and Scala 2.10).
"
290545494,2772,asfbot,2017-03-30T21:08:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2557/
Test PASSed (JDK 8 and Scala 2.11).
"
290642239,2772,asfbot,2017-03-31T07:50:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2567/
Test FAILed (JDK 8 and Scala 2.12).
"
290642248,2772,asfbot,2017-03-31T07:50:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2567/
Test FAILed (JDK 7 and Scala 2.10).
"
290642291,2772,asfbot,2017-03-31T07:50:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2571/
Test FAILed (JDK 8 and Scala 2.11).
"
290643473,2772,michaelandrepearce,2017-03-31T07:56:22Z,"need to rebase, dd71e4a8d830c9de40b5ec3f987f60a1d2f26b39 changed test class's breaking FetcherTest on CI build"
290663336,2772,asfbot,2017-03-31T09:21:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2569/
Test PASSed (JDK 7 and Scala 2.10).
"
290687921,2772,asfbot,2017-03-31T11:26:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2573/
Test FAILed (JDK 8 and Scala 2.11).
"
290687964,2772,asfbot,2017-03-31T11:27:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2569/
Test FAILed (JDK 8 and Scala 2.12).
"
290711477,2772,asfbot,2017-03-31T13:27:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2576/
Test PASSed (JDK 8 and Scala 2.12).
"
290712121,2772,asfbot,2017-03-31T13:29:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2580/
Test PASSed (JDK 8 and Scala 2.11).
"
290713404,2772,asfbot,2017-03-31T13:35:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2576/
Test PASSed (JDK 7 and Scala 2.10).
"
290943803,2772,asfbot,2017-04-01T20:03:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2610/
Test FAILed (JDK 8 and Scala 2.11).
"
290943839,2772,asfbot,2017-04-01T20:03:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2606/
Test FAILed (JDK 8 and Scala 2.12).
"
290943890,2772,asfbot,2017-04-01T20:04:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2606/
Test FAILed (JDK 7 and Scala 2.10).
"
290944301,2772,asfbot,2017-04-01T20:11:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2611/
Test FAILed (JDK 8 and Scala 2.11).
"
290944811,2772,michaelandrepearce,2017-04-01T20:21:41Z,"Can someone check trunk or the pr build plan, the build failure is due to:

pure virtual method called
terminate called without an active exception
:streams:unitTest FAILED

I note other preceding builds for other PR's also have this issues, e.g. 

https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2609/console
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2608/console

"
290946873,2772,asfbot,2017-04-01T20:59:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2607/
Test PASSed (JDK 8 and Scala 2.12).
"
290948772,2772,michaelandrepearce,2017-04-01T21:37:04Z,"@radai-rosenblatt thanks for the time and the review feedback :), I've committed some changes based on it, if you wish to re-review. 

p.s. it seems the pr builds in jenkins ci plans are not stable atm. I assume someone is looking into as its not just my PR affected."
290949266,2772,asfbot,2017-04-01T21:46:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2613/
Test FAILed (JDK 8 and Scala 2.11).
"
290950335,2772,asfbot,2017-04-01T22:08:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2609/
Test PASSed (JDK 7 and Scala 2.10).
"
290951405,2772,asfbot,2017-04-01T22:31:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2609/
Test PASSed (JDK 8 and Scala 2.12).
"
290953079,2772,asfbot,2017-04-01T23:06:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2607/
Test FAILed (JDK 7 and Scala 2.10).
"
290974240,2772,asfbot,2017-04-02T09:08:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2624/
Test FAILed (JDK 8 and Scala 2.11).
"
290975231,2772,asfbot,2017-04-02T09:30:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2620/
Test PASSed (JDK 7 and Scala 2.10).
"
290976082,2772,asfbot,2017-04-02T09:52:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2620/
Test PASSed (JDK 8 and Scala 2.12).
"
291396480,2772,michaelandrepearce,2017-04-04T05:23:26Z,retest this please
291397416,2772,asfbot,2017-04-04T05:30:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2675/
Test FAILed (JDK 8 and Scala 2.11).
"
291400304,2772,asfbot,2017-04-04T05:52:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2671/
Test PASSed (JDK 7 and Scala 2.10).
"
291402998,2772,asfbot,2017-04-04T06:10:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2676/
Test FAILed (JDK 8 and Scala 2.11).
"
291403021,2772,asfbot,2017-04-04T06:10:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2672/
Test FAILed (JDK 8 and Scala 2.12).
"
291403079,2772,asfbot,2017-04-04T06:10:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2672/
Test FAILed (JDK 7 and Scala 2.10).
"
291404463,2772,asfbot,2017-04-04T06:19:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2677/
Test FAILed (JDK 8 and Scala 2.11).
"
291408285,2772,asfbot,2017-04-04T06:42:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2673/
Test FAILed (JDK 7 and Scala 2.10).
"
291414830,2772,asfbot,2017-04-04T07:14:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2673/
Test PASSed (JDK 8 and Scala 2.12).
"
291421176,2772,ijuma,2017-04-04T07:46:10Z,"@becketqin, would you like to review this one?"
291509576,2772,asfbot,2017-04-04T14:02:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2689/
Test FAILed (JDK 8 and Scala 2.12).
"
291509664,2772,asfbot,2017-04-04T14:03:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2693/
Test FAILed (JDK 8 and Scala 2.11).
"
291509713,2772,asfbot,2017-04-04T14:03:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2689/
Test FAILed (JDK 7 and Scala 2.10).
"
291511107,2772,asfbot,2017-04-04T14:08:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2694/
Test FAILed (JDK 8 and Scala 2.11).
"
291511129,2772,asfbot,2017-04-04T14:08:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2690/
Test FAILed (JDK 8 and Scala 2.12).
"
291511471,2772,asfbot,2017-04-04T14:09:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2690/
Test FAILed (JDK 7 and Scala 2.10).
"
291531025,2772,asfbot,2017-04-04T15:10:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2692/
Test FAILed (JDK 8 and Scala 2.12).
"
291531063,2772,asfbot,2017-04-04T15:11:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2696/
Test FAILed (JDK 8 and Scala 2.11).
"
291531137,2772,asfbot,2017-04-04T15:11:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2692/
Test FAILed (JDK 7 and Scala 2.10).
"
291534785,2772,asfbot,2017-04-04T15:22:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2700/
Test FAILed (JDK 8 and Scala 2.11).
"
291542274,2772,asfbot,2017-04-04T15:46:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2696/
Test PASSed (JDK 8 and Scala 2.12).
"
291542765,2772,asfbot,2017-04-04T15:47:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2696/
Test PASSed (JDK 7 and Scala 2.10).
"
291900408,2772,asfbot,2017-04-05T15:34:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2749/
Test FAILed (JDK 7 and Scala 2.10).
"
291900415,2772,asfbot,2017-04-05T15:34:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2749/
Test FAILed (JDK 8 and Scala 2.12).
"
291900567,2772,asfbot,2017-04-05T15:34:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2753/
Test FAILed (JDK 8 and Scala 2.11).
"
291903342,2772,asfbot,2017-04-05T15:43:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2750/
Test FAILed (JDK 8 and Scala 2.12).
"
291903406,2772,asfbot,2017-04-05T15:43:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2750/
Test FAILed (JDK 7 and Scala 2.10).
"
291903482,2772,asfbot,2017-04-05T15:43:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2754/
Test FAILed (JDK 8 and Scala 2.11).
"
291933137,2772,asfbot,2017-04-05T17:20:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2758/
Test PASSed (JDK 8 and Scala 2.11).
"
291935707,2772,asfbot,2017-04-05T17:28:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2754/
Test PASSed (JDK 7 and Scala 2.10).
"
291943929,2772,asfbot,2017-04-05T17:56:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2754/
Test PASSed (JDK 8 and Scala 2.12).
"
291990286,2772,asfbot,2017-04-05T20:41:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2765/
Test PASSed (JDK 8 and Scala 2.11).
"
291990765,2772,becketqin,2017-04-05T20:43:16Z,@michaelandrepearce Thanks for the patch Michael. I'll take a look either today or tomorrow.
291991304,2772,asfbot,2017-04-05T20:45:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2761/
Test PASSed (JDK 7 and Scala 2.10).
"
292010061,2772,asfbot,2017-04-05T22:00:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2771/
Test PASSed (JDK 8 and Scala 2.11).
"
292011898,2772,asfbot,2017-04-05T22:09:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2767/
Test PASSed (JDK 7 and Scala 2.10).
"
292017361,2772,asfbot,2017-04-05T22:37:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2767/
Test PASSed (JDK 8 and Scala 2.12).
"
292019515,2772,asfbot,2017-04-05T22:49:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2761/
Test FAILed (JDK 8 and Scala 2.12).
"
292451094,2772,michaelandrepearce,2017-04-07T06:17:40Z,"@becketqin thanks for the review, I have left response on all, i hopefully have marked clearly the ones we don't need any discussion on and will just implement, will aim to do today.

If you could read and comment on those with responses."
292467731,2772,asfbot,2017-04-07T07:53:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2811/
Test FAILed (JDK 7 and Scala 2.10).
"
292468097,2772,asfbot,2017-04-07T07:54:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2815/
Test FAILed (JDK 8 and Scala 2.11).
"
292468165,2772,asfbot,2017-04-07T07:55:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2811/
Test FAILed (JDK 8 and Scala 2.12).
"
292468749,2772,asfbot,2017-04-07T07:58:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2812/
Test FAILed (JDK 7 and Scala 2.10).
"
292469131,2772,asfbot,2017-04-07T08:00:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2816/
Test FAILed (JDK 8 and Scala 2.11).
"
292469171,2772,asfbot,2017-04-07T08:00:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2812/
Test FAILed (JDK 8 and Scala 2.12).
"
292472108,2772,asfbot,2017-04-07T08:14:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2813/
Test FAILed (JDK 7 and Scala 2.10).
"
292482952,2772,asfbot,2017-04-07T09:05:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2817/
Test PASSed (JDK 8 and Scala 2.11).
"
292490925,2772,asfbot,2017-04-07T09:41:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2813/
Test PASSed (JDK 8 and Scala 2.12).
"
292577795,2772,asfbot,2017-04-07T16:03:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2822/
Test FAILed (JDK 8 and Scala 2.12).
"
292577832,2772,asfbot,2017-04-07T16:03:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2826/
Test FAILed (JDK 8 and Scala 2.11).
"
292579117,2772,asfbot,2017-04-07T16:08:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2823/
Test FAILed (JDK 8 and Scala 2.12).
"
292579127,2772,asfbot,2017-04-07T16:08:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2827/
Test FAILed (JDK 8 and Scala 2.11).
"
292593099,2772,asfbot,2017-04-07T17:01:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2825/
Test PASSed (JDK 7 and Scala 2.10).
"
292593213,2772,asfbot,2017-04-07T17:01:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2825/
Test PASSed (JDK 8 and Scala 2.12).
"
292593758,2772,asfbot,2017-04-07T17:03:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2829/
Test PASSed (JDK 8 and Scala 2.11).
"
296407266,2772,asfbot,2017-04-22T23:04:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3122/
Test FAILed (JDK 8 and Scala 2.11).
"
296411045,2772,michaelandrepearce,2017-04-23T00:37:43Z,"@hachikuji  thanks for the time and the review feedback :), I've committed some changes based on it. Also have added the additional integration tests.

plus yet another rebase"
296418096,2772,asfbot,2017-04-23T04:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3117/
Test FAILed (JDK 7 and Scala 2.10).
"
296418097,2772,asfbot,2017-04-23T04:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3117/
Test FAILed (JDK 8 and Scala 2.12).
"
296432456,2772,asfbot,2017-04-23T10:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3119/
Test FAILed (JDK 7 and Scala 2.10).
"
296432457,2772,asfbot,2017-04-23T10:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3120/
Test FAILed (JDK 7 and Scala 2.10).
"
296432459,2772,asfbot,2017-04-23T10:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3124/
Test FAILed (JDK 7 and Scala 2.10).
"
296432463,2772,asfbot,2017-04-23T10:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3128/
Test FAILed (JDK 8 and Scala 2.11).
"
296435284,2772,asfbot,2017-04-23T11:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3121/
Test FAILed (JDK 7 and Scala 2.10).
"
296435285,2772,asfbot,2017-04-23T11:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3120/
Test FAILed (JDK 8 and Scala 2.12).
"
296435286,2772,asfbot,2017-04-23T11:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3125/
Test FAILed (JDK 8 and Scala 2.12).
"
296435287,2772,asfbot,2017-04-23T11:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3125/
Test FAILed (JDK 8 and Scala 2.11).
"
296435288,2772,asfbot,2017-04-23T11:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3126/
Test FAILed (JDK 8 and Scala 2.11).
"
296435290,2772,asfbot,2017-04-23T11:01:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3126/
Test FAILed (JDK 8 and Scala 2.12).
"
296435292,2772,asfbot,2017-04-23T11:01:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3123/
Test FAILed (JDK 8 and Scala 2.12).
"
296441833,2772,asfbot,2017-04-23T13:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3126/
Test FAILed (JDK 7 and Scala 2.10).
"
296441835,2772,asfbot,2017-04-23T13:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3131/
Test FAILed (JDK 8 and Scala 2.11).
"
296441836,2772,asfbot,2017-04-23T13:01:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3130/
Test FAILed (JDK 8 and Scala 2.11).
"
296824410,2772,asfbot,2017-04-24T21:15:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3150/
Test PASSed (JDK 8 and Scala 2.11).
"
296824411,2772,asfbot,2017-04-24T21:15:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3144/
Test FAILed (JDK 7 and Scala 2.10).
"
296824412,2772,asfbot,2017-04-24T21:15:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3143/
Test PASSed (JDK 8 and Scala 2.12).
"
297570869,2772,becketqin,2017-04-26T23:37:40Z,"Thanks for the patch. LGTM. We need to update `upgrade.html` as well, but that can also be done in the follow up patch that removes the Serde hack."
297660629,2772,asfbot,2017-04-27T09:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3207/
Test PASSed (JDK 8 and Scala 2.11).
"
297660634,2772,asfbot,2017-04-27T09:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3203/
Test FAILed (JDK 8 and Scala 2.12).
"
297697231,2772,asfbot,2017-04-27T12:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3203/
Test PASSed (JDK 7 and Scala 2.10).
"
297862361,2772,asfbot,2017-04-27T23:01:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3246/
Test FAILed (JDK 8 and Scala 2.11).
"
297863022,2772,asfbot,2017-04-27T23:05:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3237/
Test FAILed (JDK 8 and Scala 2.12).
"
297863235,2772,asfbot,2017-04-27T23:06:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3241/
Test FAILed (JDK 7 and Scala 2.10).
"
297872799,2772,asfbot,2017-04-28T00:10:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3248/
Test PASSed (JDK 8 and Scala 2.11).
"
297874000,2772,asfbot,2017-04-28T00:19:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3243/
Test FAILed (JDK 7 and Scala 2.10).
"
297874309,2772,hachikuji,2017-04-28T00:22:05Z,Thanks for the patch! I'm planning to merge this tomorrow. It would probably be a good idea to send an e-mail to the dev list with the changes to the KIP to make sure there are no concerns. 
297875113,2772,asfbot,2017-04-28T00:28:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3239/
Test FAILed (JDK 8 and Scala 2.12).
"
297883375,2772,michaelandrepearce,2017-04-28T01:31:36Z,@ijuma battery on laptop just died at 2am here. Managed to quickly commit some javadoc changes. So I don't have to go through yet another annoying rebase tomorrow could we merge as is and I commit to raise a new pr with the extra tests you mentioned.
297883733,2772,asfbot,2017-04-28T01:34:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3251/
Test FAILed (JDK 8 and Scala 2.12).
"
297889302,2772,asfbot,2017-04-28T02:17:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3255/
Test PASSed (JDK 7 and Scala 2.10).
"
297889699,2772,asfbot,2017-04-28T02:21:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3260/
Test PASSed (JDK 8 and Scala 2.11).
"
297928916,2772,asfbot,2017-04-28T07:31:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3270/
Test FAILed (JDK 8 and Scala 2.11).
"
297928979,2772,asfbot,2017-04-28T07:31:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3261/
Test FAILed (JDK 8 and Scala 2.12).
"
297929071,2772,asfbot,2017-04-28T07:32:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3265/
Test FAILed (JDK 7 and Scala 2.10).
"
297929598,2772,asfbot,2017-04-28T07:35:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3271/
Test FAILed (JDK 8 and Scala 2.11).
"
297929624,2772,asfbot,2017-04-28T07:35:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3262/
Test FAILed (JDK 8 and Scala 2.12).
"
297929702,2772,asfbot,2017-04-28T07:35:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3266/
Test FAILed (JDK 7 and Scala 2.10).
"
297931479,2772,asfbot,2017-04-28T07:45:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3263/
Test FAILed (JDK 8 and Scala 2.12).
"
297931569,2772,asfbot,2017-04-28T07:46:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3267/
Test FAILed (JDK 7 and Scala 2.10).
"
297931588,2772,asfbot,2017-04-28T07:46:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3272/
Test FAILed (JDK 8 and Scala 2.11).
"
297933355,2772,michaelandrepearce,2017-04-28T07:55:25Z,"rebased .... again ....

https://s3.amazonaws.com/media-p.slid.es/uploads/403474/images/1927371/conflictRseolutionCartoon.jpg

"
297942969,2772,asfbot,2017-04-28T08:43:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3268/
Test PASSed (JDK 7 and Scala 2.10).
"
297943005,2772,asfbot,2017-04-28T08:43:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3264/
Test PASSed (JDK 8 and Scala 2.12).
"
297943759,2772,asfbot,2017-04-28T08:47:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3273/
Test PASSed (JDK 8 and Scala 2.11).
"
298133490,2772,hachikuji,2017-04-29T00:14:16Z,"Merge to trunk is imminent. The record header saga is finally drawing to a close. What battles await tomorrow?!

(In all seriousness, thanks for the patch and the persistence with this KIP)"
298144836,2772,simplesteph,2017-04-29T03:48:17Z,"Sorry I'm late to the party and I'm going to be a pain, but if the headers are making it to the ProducerRecord, they should also make it to the SourceRecord, so that the Connect Framework directly benefits from it. 
Also, IMO this would have been a perfection opportunity of adding such headers as a method or something... Right now, many constructors are missing the headers parameter, and the only way to get to add headers is the very dangerous https://github.com/apache/kafka/pull/2772/files#diff-6deeb698c2574836af58336908d76ae5R66 (because of the partition parameter). Just saw this got merged an hour ago...

CF https://cwiki.apache.org/confluence/display/KAFKA/KIP+141+-+ProducerRecord+Interface+Improvements 

cc @ijuma @mjsax "
298147071,2772,michaelandrepearce,2017-04-29T04:49:12Z,"Hi Steph,

You actually add a header in normal usage by calling record.headers.add(key,value), this constructor was added in the record really only aimed at mirror making solutions.

Also adding headers is mostly aimed to be added by interceptors (where record is given) thus mutable at that stage. This is why on produce once sent the headers are made read only.

Yes connect framework isn't done as wasn't covered in this KIP, once this KIP was done I was actially going to raise another KIP immediately to add them (a bit like timestamp was first added to core and then a second kip was raised to add them to connect), but first step it was important to get headers added at the core level. We actually have the code for that ready so hold on :) "
298147275,2772,simplesteph,2017-04-29T04:55:22Z,That helps thanks !
298152782,2772,michaelandrepearce,2017-04-29T07:23:05Z,"@simplesteph 
Was going to raise all this in a week or so time, but as we had the code ready and obviously you have a need thus raised the point re connect, i have raised it all now.

Here's KIP
https://cwiki.apache.org/confluence/display/KAFKA/KIP-145+-+Expose+Record+Headers+in+Kafka+Connect

Here's PR (which we had the code for)
https://github.com/apache/kafka/pull/2942

Obviously first needs KIP discussion and vote. So please feel free to get active in that space supporting that :)
"
298181606,2772,mjsax,2017-04-29T17:11:57Z,@simplesteph See https://cwiki.apache.org/confluence/display/KAFKA/KIP-82+-+Add+Record+Headers for details about this work. You can just update your own KIP accordingly.
271036840,2330,asfbot,2017-01-06T23:19:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/587/
Test PASSed (JDK 8 and Scala 2.12).
"
271041167,2330,asfbot,2017-01-06T23:49:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/586/
Test PASSed (JDK 7 and Scala 2.10).
"
271047332,2330,asfbot,2017-01-07T00:39:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/588/
Test FAILed (JDK 8 and Scala 2.11).
"
271833277,2330,ijuma,2017-01-11T10:27:51Z,"@rajinisivaram, you know the networking code pretty well, so maybe you could help with this review.

Also @junrao, but his time is very limited this week."
271888557,2330,mimaison,2017-01-11T14:53:42Z,"One concern that was raised also in KIP-81 (reusing the same memory pool logic on the consumer side) is whether the muting might impact group coordination. 

One suggestion is to only allocate in the memory pool requests that are larger than a small threshold (~256bytes for example) and let smaller requests be allocated on the heap as usual. This should not add too much load/memory pressure on the broker (since we also have the queued.max.requests setting) and would allow heartbeats and other group requests to be processed uninterrupted."
272593522,2330,asfbot,2017-01-14T02:08:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/877/
Test FAILed (JDK 7 and Scala 2.10).
"
272594006,2330,asfbot,2017-01-14T02:14:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/877/
Test PASSed (JDK 8 and Scala 2.12).
"
272601013,2330,asfbot,2017-01-14T04:27:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/879/
Test FAILed (JDK 8 and Scala 2.11).
"
273306152,2330,asfbot,2017-01-17T21:30:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/945/
Test FAILed (JDK 7 and Scala 2.10).
"
273309380,2330,radai-rosenblatt,2017-01-17T21:43:10Z,"@mimaison - if you want to bypass the memory pool for small allocations (which is perfectly valid) you could implement a CompositeMemoryPool that delegates requests under a certain size to the NONE pool and requests over that size to a ""real"" pool. that would give you the behaviour you want."
273312661,2330,asfbot,2017-01-17T21:56:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/945/
Test PASSed (JDK 8 and Scala 2.12).
"
273327786,2330,asfbot,2017-01-17T22:56:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/948/
Test FAILed (JDK 8 and Scala 2.12).
"
273328230,2330,asfbot,2017-01-17T22:58:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/950/
Test PASSed (JDK 8 and Scala 2.11).
"
273328775,2330,asfbot,2017-01-17T23:01:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/948/
Test PASSed (JDK 7 and Scala 2.10).
"
273665885,2330,asfbot,2017-01-19T02:36:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1009/
Test PASSed (JDK 8 and Scala 2.12).
"
273665887,2330,asfbot,2017-01-19T02:36:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1011/
Test PASSed (JDK 8 and Scala 2.11).
"
273675645,2330,asfbot,2017-01-19T03:54:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1009/
Test PASSed (JDK 7 and Scala 2.10).
"
273972279,2330,radai-rosenblatt,2017-01-20T04:02:37Z,@rajinisivaram - i've added a basic test of OOM functionality to SelectorTest. i didnt write an SSL equivalent yet
273981203,2330,asfbot,2017-01-20T05:29:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1061/
Test PASSed (JDK 8 and Scala 2.11).
"
273981635,2330,asfbot,2017-01-20T05:33:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1059/
Test PASSed (JDK 8 and Scala 2.12).
"
273981696,2330,asfbot,2017-01-20T05:33:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1059/
Test PASSed (JDK 7 and Scala 2.10).
"
274049159,2330,rajinisivaram,2017-01-20T11:18:33Z,"@radai-rosenblatt The implementation looks good to me. For testing SSL, it may be easier to add a memory pool to `NioEchoServer` and add a test to `SslTransportLayerTest`."
274122627,2330,radai-rosenblatt,2017-01-20T16:57:05Z,"@ijuma for some reason i cant respond directly to your comment above on NetworkReceive.java, but some of the ""no-pool"" CTR calls come from SASL, for example.
reasons for not using a pool would be:
1. cost-benefit - SASL for example probably doesnt allocate large buffers and allowing small allocations to bypass the central pool is probably a good idea
2. focus of this change set. my immediate concern is broker being overwhelmed by large produce requests, as that is a common (relatively) scenario for me. future work could broaden the scope to include:
   2.1 (de)compression - you'd want to use the pool for decompression target buffers. doing this efficiently might require a wire format change (would be nice to know decompressed size up front)
   2.2 maybe send buffers? without SSL they get zero-copied from disk out to socket, but under SSL theres a byte buffer allocated. havent fully investigated so dont know how much of a potential OOM risk that is.
   2.3 usage on the client-side - both producer and consumer, also interacts with (de)compression."
274196525,2330,radai-rosenblatt,2017-01-20T22:25:48Z,"@rajinisivaram - i've implemented the ssl version of testMuteOnOOM(). its not pretty, but it directly tests the core functionality. also, thank you very much for your time spent reviewing this."
274197034,2330,asfbot,2017-01-20T22:28:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1076/
Test FAILed (JDK 7 and Scala 2.10).
"
274204440,2330,asfbot,2017-01-20T23:10:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1078/
Test PASSed (JDK 8 and Scala 2.11).
"
274204889,2330,asfbot,2017-01-20T23:13:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1076/
Test PASSed (JDK 8 and Scala 2.12).
"
274616488,2330,asfbot,2017-01-23T21:05:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1116/
Test FAILed (JDK 7 and Scala 2.10).
"
274617983,2330,radai-rosenblatt,2017-01-23T21:10:51Z,"@rajinisivaram - i've changed the tests to use the non-strict mode, like actual code.

note that the scenario you've described of constantly muting and unmuting is also possible with a non strict pool:

1. server has N incoming connections
2. at every iteration pool only has enough memory to accommodate a single request
3. at every call to poll, N connections are unmuted, 1 is services, N-1 are muted again, progress indication is set to true
4. next iteration either makes no further progress (in which case the next after that will actually wait for a while), or again repeats this.

this isn't as tight a loop as with a strict pool (as the 2nd invocation will not have memory and will not make progress), but its still sub-optimal.

a more optimal solution would be to somehow remember the smallest unfulfilled request, have some pool.canAllocate(long bytes) - which will account for strict vs non-struct - and only unmute is the result is true. since there's one pool and multiple selectors this isn't guaranteed to not unmute for nothing.

to me this is a corner case though, as under these conditions currently the broker just explodes. i can implement the canAllocate(bytes) API if you think its required, but I think this is optimizing a corner case."
274628341,2330,asfbot,2017-01-23T21:49:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1118/
Test PASSed (JDK 8 and Scala 2.11).
"
274637003,2330,asfbot,2017-01-23T22:23:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1116/
Test PASSed (JDK 8 and Scala 2.12).
"
274669290,2330,asfbot,2017-01-24T01:03:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1128/
Test FAILed (JDK 7 and Scala 2.10).
"
274676581,2330,asfbot,2017-01-24T01:46:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1128/
Test PASSed (JDK 8 and Scala 2.12).
"
274682627,2330,asfbot,2017-01-24T02:24:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1130/
Test PASSed (JDK 8 and Scala 2.11).
"
275179023,2330,asfbot,2017-01-25T17:44:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1192/
Test FAILed (JDK 7 and Scala 2.10).
"
275189094,2330,asfbot,2017-01-25T18:20:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1194/
Test FAILed (JDK 8 and Scala 2.11).
"
275190913,2330,asfbot,2017-01-25T18:27:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1192/
Test PASSed (JDK 8 and Scala 2.12).
"
276516615,2330,asfbot,2017-01-31T22:39:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1377/
Test FAILed (JDK 8 and Scala 2.12).
"
276516646,2330,asfbot,2017-01-31T22:40:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1377/
Test FAILed (JDK 7 and Scala 2.10).
"
276516672,2330,asfbot,2017-01-31T22:40:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1381/
Test FAILed (JDK 8 and Scala 2.11).
"
276528755,2330,asfbot,2017-01-31T23:35:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1382/
Test PASSed (JDK 8 and Scala 2.11).
"
276528952,2330,asfbot,2017-01-31T23:37:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1379/
Test PASSed (JDK 8 and Scala 2.12).
"
276529319,2330,asfbot,2017-01-31T23:39:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1379/
Test PASSed (JDK 7 and Scala 2.10).
"
276545101,2330,asfbot,2017-02-01T01:12:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1385/
Test PASSed (JDK 8 and Scala 2.11).
"
276550331,2330,asfbot,2017-02-01T01:46:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1382/
Test PASSed (JDK 8 and Scala 2.12).
"
276556832,2330,asfbot,2017-02-01T02:31:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1392/
Test PASSed (JDK 8 and Scala 2.11).
"
276556848,2330,asfbot,2017-02-01T02:31:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1389/
Test PASSed (JDK 8 and Scala 2.12).
"
276557370,2330,asfbot,2017-02-01T02:35:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1382/
Test PASSed (JDK 7 and Scala 2.10).
"
276571334,2330,asfbot,2017-02-01T04:45:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1389/
Test FAILed (JDK 7 and Scala 2.10).
"
277827623,2330,asfbot,2017-02-06T21:59:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1527/
Test PASSed (JDK 8 and Scala 2.11).
"
277827637,2330,asfbot,2017-02-06T21:59:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1524/
Test PASSed (JDK 8 and Scala 2.12).
"
277828021,2330,asfbot,2017-02-06T22:00:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1524/
Test PASSed (JDK 7 and Scala 2.10).
"
282927726,2330,asfbot,2017-02-28T03:01:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1876/
Test PASSed (JDK 8 and Scala 2.11).
"
282928080,2330,radai-rosenblatt,2017-02-28T03:03:42Z,@ijuma @junrao - to the best of my understanding i've addressed/answered everything? anything still pending from you guys?
282928108,2330,asfbot,2017-02-28T03:03:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1874/
Test PASSed (JDK 8 and Scala 2.12).
"
282928263,2330,asfbot,2017-02-28T03:04:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1873/
Test PASSed (JDK 7 and Scala 2.10).
"
290570834,2330,asfbot,2017-03-30T23:11:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2563/
Test PASSed (JDK 8 and Scala 2.11).
"
290571351,2330,radai-rosenblatt,2017-03-30T23:14:20Z,"@ijuma @junrao - rebased again. slight modifications to comply with method complexity checks that were introduced to master since i last rebased. 

also kicked off an integration test, will report the results (everything passes locally)"
290572928,2330,asfbot,2017-03-30T23:23:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2559/
Test PASSed (JDK 7 and Scala 2.10).
"
290590098,2330,asfbot,2017-03-31T01:24:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2559/
Test FAILed (JDK 8 and Scala 2.12).
"
290735579,2330,radai-rosenblatt,2017-03-31T14:57:55Z,successful integration test run - https://jenkins.confluent.io/job/system-test-kafka-branch-builder/817/
299710401,2330,radai-rosenblatt,2017-05-07T14:37:00Z,"@junrao @ijuma just rebased this again on trunk. the following tests are being flaky (on trunk as well as on my other PR):
```
DescribeConsumerGroupTest. testDescribeGroupWithNewConsumerWithShortInitializationTimeout
RequestQuotaTest. testResponseThrottleTime
```
"
299716223,2330,asfbot,2017-05-07T16:09:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3605/
Test PASSed (JDK 8 and Scala 2.12).
"
299716359,2330,asfbot,2017-05-07T16:11:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3614/
Test FAILed (JDK 8 and Scala 2.11).
"
299716709,2330,asfbot,2017-05-07T16:17:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3608/
Test FAILed (JDK 7 and Scala 2.10).
"
301856243,2330,asfbot,2017-05-16T17:31:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/3997/
Test PASSed (JDK 7 and Scala 2.11).
"
301864490,2330,asfbot,2017-05-16T18:01:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3985/
Test PASSed (JDK 8 and Scala 2.12).
"
301866341,2330,asfbot,2017-05-16T18:06:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4001/
Test PASSed (JDK 7 and Scala 2.11).
"
301873030,2330,asfbot,2017-05-16T18:28:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3989/
Test PASSed (JDK 8 and Scala 2.12).
"
314579410,2330,asfgit,2017-07-11T21:36:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6034/
Test PASSed (JDK 7 and Scala 2.11).
"
314584547,2330,asfgit,2017-07-11T21:59:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6019/
Test PASSed (JDK 8 and Scala 2.12).
"
314771034,2330,mimaison,2017-07-12T13:36:28Z,"What's the status of this PR ?
I'd like to start working on KIP-81 but it has a dependency on this."
314807274,2330,junrao,2017-07-12T15:32:54Z,@mimaison : I dropped the ball of finishing the review. I will make another pass in the next few days.
314819925,2330,radai-rosenblatt,2017-07-12T16:15:25Z,I've rebased it just yesterday. passes tests both locally and on the build server.
317422141,2330,rajinisivaram,2017-07-24T13:28:04Z,"@junrao @radai-rosenblatt I had a look through the code and it is looking good. It is a bit harder to review because the commits were squashed. The last time I had reviewed this, `Selector#pollSelectionKeys()` was returning a boolean which indicated if progress was made. Some of those changes seem to have been reverted - perhaps those changes are still required to update `Selector#madeProgressLastPoll`?"
317587446,2330,radai-rosenblatt,2017-07-24T23:49:40Z,@junrao I've fixed all the issues youve pointed out above
317632136,2330,asfgit,2017-07-25T05:18:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6321/
Test FAILed (JDK 7 and Scala 2.11).
"
317640037,2330,asfgit,2017-07-25T06:14:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6308/
Test PASSed (JDK 8 and Scala 2.12).
"
317880700,2330,radai-rosenblatt,2017-07-25T21:39:39Z,@junrao - removed setting the progress flag where it wasnt strictly required.
317892081,2330,asfgit,2017-07-25T22:30:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6331/
Test FAILed (JDK 7 and Scala 2.11).
"
317898276,2330,asfgit,2017-07-25T23:03:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6316/
Test PASSed (JDK 8 and Scala 2.12).
"
317958785,2330,junrao,2017-07-26T06:16:49Z,@radai-rosenblatt : Thanks for the latest patch. LGTM.
317960830,2330,ijuma,2017-07-26T06:29:02Z,Thanks for sticking with us @radai-rosenblatt. And thanks @junrao for merging this during your holiday!
518840417,7170,mjsax,2019-08-06T20:55:25Z,@lkokhreidze Thanks for the PR. There are some checkstyle errors. Can you please fix them before we review your PR?
518955481,7170,lkokhreidze,2019-08-07T06:12:46Z,@mjsax done
519222258,7170,lkokhreidze,2019-08-07T18:45:30Z,"> Thanks for the PR.
> 
> Made an initial pass. I still need to wrap my head around the optimization layer and how we merge repartition nodes. We need to add more test to `RepartitiontTopicNameTest` and/or `StreamsGraphTest` IMHO, to verify that the new `repartition()` operator works as intended.
> 
> Also, it seems you forgot to update `groupBy()` and `groupByKey()`.
> 
> Finally, thinking about the KIP once more: as we extend `groupBy` to configure the internal repartition topic, I am wondering if we should extend the KIP and also allow to do this for `join()` that may also create repartition topics? \cc @guozhangwang @bbejeck @vvcephei @ableegoldman @cadonna @abbccdda

@mjsax this is the first PR (written in PR description) `KStream#groupBy` changes will be part of next PR if this one passes. I didn't want to create big PR without validating underline machinery and logic in the first place. I'm okay doing all the changes here, I was just thinking it will make review harder."
519225997,7170,lkokhreidze,2019-08-07T18:56:12Z,"> Made an initial pass. I still need to wrap my head around the optimization layer and how we merge repartition nodes. We need to add more test to `RepartitiontTopicNameTest` and/or `StreamsGraphTest` IMHO, to verify that the new `repartition()` operator works as intended.

Agreed, I'll do that. I wanted to tag @bbejeck as seems like he's the main author behind optimization logic. I'll add tests for optimization logic to make sure nothing breaks.
"
519303125,7170,mjsax,2019-08-07T23:19:50Z,"Ah. I guess, I skipped the PR description... Sorry for that.

I discussed the proposal with @vvcephei in person, and thinking about the semantics once more, I am actually wondering if it is wise to change `groupBy` at all (this thought also affects my previous comment to include `join()` -- I would like to retract this idea :) )

We have basically two dimensions which 2 cases each to consider for `groupBy` (and also `join()`:

1) repartition not required -- no `Repartitioned` object provided
2) repartition not required -- `Repartitioned` object is provided
3) repartition required -- no `Repartitioned` object provided
4) repartition required --`Repartitioned` object is provided

Case (1), (2), and (4) are straight forward. However, case (2) is somewhat awkward because we actually want to treat `Repartitioned` as a configuration object but specifying it in `groupBy` should not enforce a repartitioning (if one want to enforce a repartitioning, they should use the new `repartition()` operator). Hence, for case (2) the `Repartitioned` configuration would be ignored.

Therefore, only case (4) is left in which passing in `Repartitioned` would have an effect. For this case, it would be possible to change the number of partitions or to specify a different partitioner. (I ignore the serde and naming because this can be achieved via `Grouped` anyway). However, if one wants to change the number of partitions or wants to set a specific partitioner, it seems they want to to apply (ie enforce) this configuration independent of the upstream topic configuration; ie, it is actually a case for which the user wants to _enforce_ a repartitioning. Hence, it seems perfectly fine (and actually better, because it's semantically more explicit) that a user should call `repartition()` instead.

Therefore, I don't see a good use case for which it make sense to pass in `Repartitioned` into `groupBy`.

Would be great if you could share your thoughts about it?


A second point I discussed with @vvcephei is about the optimization. We both have the impression that `repartition()` should not be subject to the repartition topic optimization. Instead, an enforced repartitioning step, should be added to the topology in a hard coded way similar to a call to `through()`. Maybe we can be some advanced optimization rules later, but it seems difficult (and potentially unsafe/incorrect) to apply the repartition topic optimization for this case. Hence, we would suggest to skip this optimization in this PR."
519307224,7170,ableegoldman,2019-08-07T23:40:44Z,"> However, if one wants to change the number of partitions or wants to set a specific partitioner, it seems they want to to apply (ie enforce) this configuration independent of the upstream topic configuration; ie, it is actually a case for which the user wants to enforce a repartitioning.

Not sure I agree @mjsax  -- maybe you just want to control the parallelism *in case* a repartition is required? You could enforce users to step through their whole topology, figure out when/where repartitioning is needed, and use `repartition` to set the parallelism. Or, you could let Streams do this for you -- as it currently does, way more conveniently and probably less error-prone -- and supply a configuration to be used if Streams figures out you need to repartition."
519309839,7170,mjsax,2019-08-07T23:54:42Z,"> maybe you just want to control the parallelism in case a repartition is required

I don't see this as a use case in practice. Why would one want to change the parallelism? Because, the aggregation operation is over or under provisions and thus one wants to decrease or increase the parallelism. If I am ok with the ""default"" parallelism in case there is no repartitioning, why would I not be ok with it if data is repartitioned?

> You could enforce users to step through their whole topology, figure out when/where repartitioning is needed, and use repartition to set the parallelism. 

This is less an issue IMHO, because if I want to scale up for example, it's sufficient to insert `repartition()` once upstream and all downstream auto-created topics would inherit the parallelism implicitly (as they inherit it now from source topics). Hence, I don't need to insert `repartition` all over the place."
519315591,7170,ableegoldman,2019-08-08T00:24:40Z,"> If I am ok with the ""default"" parallelism in case there is no repartitioning, why would I not be ok with it if data is repartitioned?
Maybe you're processing data from a topic on your company's cluster, which has a huge number of partitions to begin with. Maybe your workload needs nowhere near this many partitions (you're filtering out most records, it's overpartitioned to begin, your just testing). You run your Streams app which creates some N topics all of which have this huge number of partitions. Your brokers struggle and your boss gets mad? 
Why does anyone want to change this ever? (ie with `repartition`)

> it's sufficient to insert repartition() once upstream
That's a good point though. So the `repartition` operator is really a ""auto-create topic"" + ""set parallelism"" operator -- should be sure to document this well. Now, what if someone wants to configure the parallelism of a certain repartition topic(s) but would like to continue using the source topic's parallelism as the default? 
Not saying we should necessarily support that, but we should at least make it very clear to users how using this will affect downstream topics."
519319181,7170,mjsax,2019-08-08T00:44:35Z,"> Why does anyone want to change this ever? (ie with repartition)

My question is, why do you need `groupByKey(Repartitioned.with())`? If you want to scale down, it seems better to explicitly call `repartition(Repartitioned.with()).groupByKey()` -- otherwise, you might not scale down if not auto-repartition topic is created and it seems rather error prone that we allow to specify the number of partitions and than ignore the config entirely.

Similar to your second comment, if you want to ""scale up"" again later, you call `repartition()` again. I agree that we need to document this explicitly if we follow this route. However, it's similar behavior as we have as-of now. If you insert a `through()` all downstream operators inherit the parallelism from it."
519328010,7170,ableegoldman,2019-08-08T01:34:05Z,"Ok, well I am fine with this framing it as a ""set parallelism"" operation...I don't want to stall this KIP/PR further, but what if this was split into a new set of `setParallelism` and `repartition` operators, where the `repartition` just auto-creates the topic while upstream `setParallelism` is responsible for setting the number of partitions? 
Just wondering if it's worth making this more explicit, since there's really two new functionalities being introduced here. Doesn't hurt to bundle them into one operator, as long as users know what two things it actually does..."
519385569,7170,lkokhreidze,2019-08-08T06:30:43Z,"Hello @mjsax @ableegoldman @vvcephei 
Thanks a lot for valuable insights and interesting discussion. 
@mjsax - your arguments make sense, but I'm leaning more towards @ableegoldman points. 
In addition, in my mind, one other important point that we need to take into account is not only parallelism but general configuration of repartition topics. In my mind, this KIP can be the foundation of actually giving users control over each individual repartition topic configuration. To be honest, I was tempted to propose deprecating `KStream#groupBy(Grouped)` operation altogether. Let me explain my reasoning a bit. After this KIP, I don't see any actual benefit nor need of actually using `KStream#groupBy(Grouped)`. With `KStream#groupBy(Repartitioned)` user can do exact same things, plus more. Right now, in KStream there're `Grouped` and `Joined` configuration classes (and maybe some others that I'm missing) that are used for specifying 
a) topology name (which translates to repartition topic naming)
b) producer serdes
c) partitioner
All those configurations can be encapsulated under `Repartitioned`, in addition with all other topic level configurations that user may want to pass to internal topic creation. Maybe this was discussed before, and there's a good reason why there're separate configuration classes for each operation (besides giving api the nice look, of course :) ).
One argument that comes to mind why we may actually want to have `Repartitioned` for `groupBy` is simple syntax sugar. For example, there isn't fundamental different between this two topologies:

1)

```
builder
    .stream()
    .repartition((key, value) -> value.newKey(), Repartitioned.with(""by-new-key"").withNumberOfPartitions(2))
    .groupByKey()
```

2)

```
builder
    .stream()
    .groupBy((key, value) -> value.newKey(), Repartitioned.with(""by-new-key"").withNumberOfPartitions(2))
```

While, for me, as a user, 2nd option looks much more appealing, similarly how key selector for `KStream#groupBy` merges together two operations (`selectKey().groupBy()`).

Again, your arguments are totally valid, and all can be achieved just by having `repartition(Repartitioned)` operation. But on the other hand, I don't see anything bad with adding `Repartitioned` option to groupBy. It won't break API semantics (at least I think it won't) and will give the user extra flexibility around controlling repartition topics."
519387441,7170,lkokhreidze,2019-08-08T06:37:26Z,"> A second point I discussed with @vvcephei is about the optimization. We both have the impression that `repartition()` should not be subject to the repartition topic optimization. Instead, an enforced repartitioning step, should be added to the topology in a hard coded way similar to a call to `through()`. Maybe we can be some advanced optimization rules later, but it seems difficult (and potentially unsafe/incorrect) to apply the repartition topic optimization for this case. Hence, we would suggest to skip this optimization in this PR.

It's harder to comment on this. First, I would like to see how optimization logic behaves with this changes. Based on the first glance, it should be fine, but would need to verify this by adding more tests. If necessary, optimization can be easily removed I guess. @bbejeck would love to hear your thoughts/suggestions as well on this."
520079726,7170,vvcephei,2019-08-09T22:08:46Z,"Hey, all,

Just to wade in (hopefully not stalling this PR too long):

We should exclude this change from optimizations. The optimization logic around repartitions is already quite complex. It pushes repartition nodes around the processor graph and merges them together when possible (and the merge results in picking some repartition topic names over others). Adding a new class of repartition nodes (manually specified ones via this new operator) would only complicate the algorithm further. It's always safe to skip an optimization, so we can just skip it for now and consider it in a separate scope of follow-on work.

Further arguments against optimization: It's not clear that, if I put a `repartition()` node at a specific point in my topology, I would _want_ Streams to move it somewhere else. Also, if I have two different `repartition()` calls with _different_ parallelisms, and the optimizer wants to merge them, what can it do? What about if it wants to merge a groupBy repartition (which implicitly has a parallelism already) and an explicit `repartition()` with a different parallelism?

We can reason through all these cases, but it will still:
* drag out the KIP discussion (since we have to revisit all these cases)
* increase code complexity in the optimizer
* increase system complexity for the people using Streams (who will have a hard time understanding the results of the optimization)

We can avoid all this by just excluding the manual repartitions from optimization for now.

Regarding the other question about overlap with Grouped (and Joined): we should keep all these config objects separate. We have had a lot of trouble in the past trying to use one operation's configuration objects on other operations. It may seem like a good match, but it inevitably puts us in an awkward bind later on. When we have two different operations, we should have two different config classes as well. `Grouped` is for configuring `groupBy`, `Joined` is for configuring `join`, and `Repartitioned` is for configuring `repartition`. This may result in code duplication, but it also results in an API that is very consistent, clear, easy to learn, and easy to use.

Likewise, nesting one operation's configuration inside another operation's configuration is again sacrificing a clean API in favor of code de-duplication. It's not a good tradeoff. Far better to configure `groupBy` with `Grouped` using setters for the direct properties we want to set, and likewise configure `repartition` with `Repartitioned` using setters for the direct properties we want to set. Then, we don't have to worry about the implications for `groupBy` every time we consider changes for `repartition`. As long as they are separate operations, they should have separate configuration classes.

If we want to give people control over the parallelism in `groupBy`, we should just add `Grouped#numberOfPartitions(final int numberOfPartitions)`.

Anyway, that's my 2 cents ;)"
520097324,7170,mjsax,2019-08-09T23:48:22Z,"> If we want to give people control over the parallelism in groupBy, we should just add Grouped#numberOfPartitions(final int numberOfPartitions).

This does not resolve my main concern, that one passed in `numberOfPartitions` and we just ignore the configuration..."
520181014,7170,vvcephei,2019-08-10T21:30:26Z,"Thanks, @mjsax ,

I'm not saying that we should (you have a good point). I was just saying that we can add/deprecate methods in different config objects independently, but if we switch to use the same config object in two operations, it ties our hands.

On the optimization front, what we could do is make a small change to the optimization algorithm that, when searching upstream to find out if a repartition is necessary, if it finds a repartition operation, it can decide that one is not necessary. Then, if you do `KTable...map(...).repartition(Repartitioned.numberOfPartitions(1234)).groupBy(...)`, we won't get a double-repartition, just the one we specifically requested to scale out parallelism.

Thanks,
-John"
520222537,7170,lkokhreidze,2019-08-11T12:00:59Z,"Thank you @mjsax @vvcephei for the thorough review, much appreciated!

I think I've addressed all of the comments mentioned in this PR. To summarize:
- Repartitioning is now always performed when calling `KStream#repartition` operations.
- I've added separate, `UnoptimizableRepartitionNode` which is **NOT** subject of optimization algorithm. `KStream#repartition` operations create `UnoptimizableRepartitionNode` when they're called. We can create followup ticket to investigate optimization possibilities for `KStream#repartition`.

@vvcephei thank you for the clarification. Motivation behind having separate configuration classes per operation does make a lot of sense after your explanation.

> On the optimization front, what we could do is make a small change to the optimization algorithm that, when searching upstream to find out if a repartition is necessary, if it finds a repartition operation, it can decide that one is not necessary. Then, if you do `KTable...map(...).repartition(Repartitioned.numberOfPartitions(1234)).groupBy(...)`, we won't get a double-repartition, just the one we specifically requested to scale out parallelism.

Actually, it won't do double re-partitioning even now (there's even test for this `KStreamRepartitionIntegrationTest#shouldCreateOnlyOneRepartitionTopicWhenRepartitionIsFollowedByGroupByKey`). Thing is, `KStream#repartition` creates `KStreamImpl` with `repartitionRequred` as `false`.

@mjsax @vvcephei 
Regarding changes for `groupBy` operations - those changes are not part of this PR either way, so I think we can leave that discussion for now. I propose to resurrect discussion in mailing list when this PR is merged and have followup discussion on that, wdyt? . 
Even if we go through with `groupBy` changes, after reading @vvcephei arguments on why it's not good idea to have same configuration class for multiple operations, it makes more sense to have `numberOfPartitions` in `Grouped` class for `groupBy` operations. 

Personally, I still feel that control over repartition topic configurations is necessary in the long run (sometimes I really want to configure retention per specific repartition topics), and if we gonna have configurations duplicated in each operation class, it may create a lot of pain as well. Anyway, since there isn't other use-case yet besides specifying number of partitions per repartition topic, we can cross that bridge when we get there."
520973806,7170,lkokhreidze,2019-08-13T19:26:35Z,@mjsax @vvcephei would appreciate second review.
522163447,7170,ableegoldman,2019-08-16T21:54:50Z,"@lkokhreidze Hey, not sure if you looked into this or not but there may be some changes needed in StreamsPartitionAssignor. It tries to validate that repartition topics have been created with the correct number of partitions (defined as the max number of partitions of any source topics)

This might not affect this PR so much as the `groupBy(Repartitioned)` one, but it might be good to verify with an integration test that goes through a rebalance"
522426073,7170,lkokhreidze,2019-08-19T06:05:09Z,"Hi @ableegoldman, thanks for the info. I've added integration test where rebalancing is triggered."
522645678,7170,vvcephei,2019-08-19T16:09:29Z,"Hey @lkokhreidze ,

Thanks for the update! I'll take a look as soon as I can.

-John"
523701860,7170,mjsax,2019-08-22T00:45:27Z,"> Regarding changes for groupBy operations - those changes are not part of this PR either way, so I think we can leave that discussion for now. I propose to resurrect discussion in mailing list when this PR is merged and have followup discussion on that, wdyt? .

SGTM.

> sometimes I really want to configure retention per specific repartition topics

Why would you need that? Repartition topics are configured with infinite retention anyway and Kafka Streams does explicit purge data calls after records are fully processed."
523808665,7170,lkokhreidze,2019-08-22T08:38:05Z,"> Why would you need that? Repartition topics are configured with infinite retention anyway and Kafka Streams does explicit purge data calls after records are fully processed.

@mjsax fair, I was thinking more about older version when retention wasn't set as -1. Missed the part that it's long-time fixed now :) Anyway, internal topic configuration options is subject for whole new discussion (if there will be ever need for that)."
524659120,7170,lkokhreidze,2019-08-25T19:59:07Z,"Hi @vvcephei, thank you for thorough review, much appreciated. I've addressed all of your comments, please have a look when you got time.

> If you haven't done it already, can you run your tests with coverage and verify that you're happy with the actual coverage in the main code you've changed? We've had a few embarrassing bugs in new features when we had a test coverage gap... It'd be nice to make sure this works for everyone the first time around!

I've run `./gradlew streams:reportCoverage` and can verify that main code that I've changed has over ~97% test coverage. I've also added more tests related to `Repartitioned#streamPartitioner` (both integration and unit tests)"
526316345,7170,lkokhreidze,2019-08-29T18:54:52Z,"Hello @vvcephei, @mjsax 

Can you please have a look at this PR one more time :) (pinging you just in case so that it won't get lost)

Regards,
Levani"
527985897,7170,vvcephei,2019-09-04T16:43:43Z,"Thanks, @lkokhreidze , FWIW, I've left a few replies on threads that are marked as ""outdated"" or ""resolved"". 

I think the only one that's really important is: https://github.com/apache/kafka/pull/7170/files/6187f50b332b68b134cc3d522cac12864fd57bcc#diff-1231aeaaf21a5bcd7506a158113fea49"
527991663,7170,lkokhreidze,2019-09-04T16:59:22Z,"Hi @vvcephei thanks for the review. I've left reply as well. btw, I tried my best to make tests pass on `JDK 11 and Scala 2.13` but they seem to fail, right now because of `kafka.api.PlaintextConsumerTest.testLowMaxFetchSizeForRequestAndPartition` is there anything I can do about it?"
528126203,7170,mjsax,2019-09-04T23:07:16Z,There is https://issues.apache.org/jira/browse/KAFKA-8264 -- feel free to comment on the ticket.
528230308,7170,lkokhreidze,2019-09-05T07:07:19Z,"Thanks @mjsax, left the comment.

@mjsax @vvcephei please let me know if you have any more concerns/comments about this PR. 

Thanks,
Levani"
529540016,7170,lkokhreidze,2019-09-09T15:43:05Z,"Hello @mjsax @vvcephei 

Just small update - there were conflicts with trunk around `InternalTopicConfig` class. I've synced the feature branch with trunk and resolved the conflicts.

Regards,
Levani"
534176196,7170,lkokhreidze,2019-09-23T16:25:49Z,"Hi @mjsax, bumping this thread so it won't get lost.
Any updates around this PR? would love to finalize this PR and move on to other KIPs :)

Regards,
Levani"
536229726,7170,lkokhreidze,2019-09-28T22:18:56Z,"Hello @mjsax,

Thanks for the review and thanks for pointing out challenges around `join`. I've added two integration test to address your concerns. Basically, in both cases `CopartitionedTopicsEnforcer` now chooses max partition number from the repartition topics and updates the number of partitions config accordingly. Initially, there was a bug in my implementation, basically co-partitioning wasn't working properly when using `repartition` operation. It's fixed by this commit: https://github.com/apache/kafka/pull/7170/commits/1af73c9678688c488946f7de53291160bb85b2de#diff-5142e1d4a6410459d6bf6df98828e5afR569

Problem was that in the `repartition` implementation, when creating new `KStreamImpl` I wasn't passing new set of source nodes, therefore `AbstractStream#ensureJoinableWith` was choosing wrong `sourceNodes` for ensuring co-partitioning. I guess this wasn't problem with other internal topics, since they were inheriting number of partitions from the source topic. But since `repartition` operation may change number of partition, it was necessary to set new `sourceNodes` when creating `KStreamImpl`.

New integration tests verify that max number of partitions is chosen when doing join operation. As you pointed out, it may be different what user specified, but not sure if it's a bad thing... in my mind it kinda makes sense if internal implementation of KStream chooses proper number of partitions based on the operations. Not sure what's the other way around it...

Also, tested it when topology optimization is specified and added integration test for it: `shouldChooseMaxPartitionNumberFromSourceTopicsForJoinOperationWhenTopologyOptimizationIsSpecified`. 
Seems like optimization algorithm is removing/updating graph nodes without modifying `InternalTopologyBuilder#copartitionSourceGroups`. This was breaking `CopartitionedTopicsEnforcer` because nodes that are added in `copartitionSourceGroups` maybe completely removed by the optimizer. Therefore, when calling `InternalTopologyBuilder#copartitionGroups`, it would try to get topic with old node name which results in null. I added `InternalTopologyBuilder#maybeUpdateCopartitionSourceGroups` and it's called during optimization when node is replaced.

This was quite fun debugging :) now I know much more how all co-partitioning works around repartition topics.

Regards,
Levani"
538763974,7170,lkokhreidze,2019-10-06T16:32:30Z,retest this please
538802836,7170,ableegoldman,2019-10-07T00:10:59Z,"Hey Levani, just a heads up in case you're struggling to get a 3/3 green build with all tests passing: given the number of flaky tests it can be difficult to get a clean pass, so it's useful to record which tests failed on a given run before retesting. That way we can tell if it's the same tests failing every time, or any potentially related streams test, vs flaky tests in connect or core (ideally also create a ticket for each flaky test or if a ticket already exists, comment with the failed link, to draw attention to the flakiest and hopefully get them addressed/fixed)"
538903669,7170,lkokhreidze,2019-10-07T08:55:58Z,"Hey @ableegoldman, thanks for the suggestion. I've added comments on relevant JIRA tickets."
539849065,7170,lkokhreidze,2019-10-09T06:06:11Z,retest this please
544478145,7170,lkokhreidze,2019-10-21T11:47:45Z,retest this please
554428862,7170,lkokhreidze,2019-11-15T16:24:42Z,"Latest commit has the code that verifies number of partitions when `repartiton()` operation is used next to `join()` notable changes are:

1) Introduced new `ImmutableRepartitionTopicConfig extends RepartitionTopicConfig` which has `setNumberOfPartitions` as NO-OP in order to make sure that number of partitions specified by the user won't be altered.
2) `CopartitionedTopicsEnforcer` has updated logic which throws exception whenever number of partitions do not pass the validation

All is covered with unit/integration tests."
559230758,7170,lkokhreidze,2019-11-27T19:55:10Z,retest this please
559492894,7170,lkokhreidze,2019-11-28T13:22:30Z,retest this please
571912141,7170,lkokhreidze,2020-01-08T06:38:14Z,retest this please
572542685,7170,lkokhreidze,2020-01-09T12:35:54Z,retest this please
573815440,7170,lkokhreidze,2020-01-13T18:55:03Z,"Hello @mjsax, 
Sorry for pinging, but would love to get some estimate when this PR will be reviewed. It's getting harder and harder to keep this branch in sync with the trunk. Would like to finalize this while I'm able to actively support this KIP.

Also, seems like branch builds aren't triggered, any ideas how to trigger the build?"
574055141,7170,cadonna,2020-01-14T08:11:05Z,"Retest this, please"
576140932,7170,lkokhreidze,2020-01-20T07:24:42Z,"> Sorry for the delay in review -- we are swamped with work and it's hard to get around... (it's also a quite big PR and thus even harder to find time to review...)

Totally understandable @mjsax and sorry for pushing, I'm just myself excited to move this KIP forward. Thanks for the review, I'll address your comments shortly.

"
581141756,7170,lkokhreidze,2020-02-02T14:39:02Z,"Ran `streams` test suite locally. The only test that failed was `KStreamImplTest#shouldSupportTriggerMaterializedWithKTableFromKStream`

```
[2020-02-02 16:37:45,891] INFO Opening store store in regular mode (org.apache.kafka.streams.state.internals.RocksDBTimestampedStore:100)
[2020-02-02 16:37:45,891] INFO Method #getTimestampedKeyValueStore() should be used to access a TimestampedKeyValueStore. (org.apache.kafka.streams.TopologyTestDriver:873)
[2020-02-02 16:37:45,911] WARN Closing 1 open iterators for store store (org.apache.kafka.streams.state.internals.RocksDBStore:465)
[2020-02-02 16:37:45,912] INFO stream-thread [main] Deleting state directory 0_0 for task 0_0 as user calling cleanup. (org.apache.kafka.streams.processor.internals.StateDirectory:295)

java.lang.ClassCastException: java.base/java.lang.Integer cannot be cast to java.base/java.lang.String

	at org.apache.kafka.streams.kstream.internals.KStreamImplTest.lambda$asMap$229(KStreamImplTest.java:2960)
	at java.base/java.util.Iterator.forEachRemaining(Iterator.java:133)
	at org.apache.kafka.streams.kstream.internals.KStreamImplTest.asMap(KStreamImplTest.java:2960)
	at org.apache.kafka.streams.kstream.internals.KStreamImplTest.shouldSupportTriggerMaterializedWithKTableFromKStream(KStreamImplTest.java:2944)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.base/java.lang.reflect.Method.invoke(Method.java:564)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:59)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:56)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:26)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.BlockJUnit4ClassRunner$1.evaluate(BlockJUnit4ClassRunner.java:100)
	at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:366)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:103)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:63)
	at org.junit.runners.ParentRunner$4.run(ParentRunner.java:331)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:79)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:329)
	at org.junit.runners.ParentRunner.access$100(ParentRunner.java:66)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:293)
	at org.junit.runners.ParentRunner$3.evaluate(ParentRunner.java:306)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:413)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:137)
	at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68)
	at com.intellij.rt.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:33)
	at com.intellij.rt.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:230)
	at com.intellij.rt.junit.JUnitStarter.main(JUnitStarter.java:58)
```

Seems like it's not connected to this PR. Haven't created JIRA ticket / PR to fix it because as per https://github.com/apache/kafka/pull/8027 seems like people are on it."
585833956,7170,lkokhreidze,2020-02-13T16:04:44Z,Ran `streams` module tests locally. All the tests passed. Can we somehow trigger the jenkins build? cc @mjsax 
593048414,7170,vvcephei,2020-03-01T03:31:45Z,test this please
596474986,7170,lkokhreidze,2020-03-09T11:35:12Z,"Hi @vvcephei @mjsax any chance we could push this changes by 2.6 release? It's been a while :) 

Regards,
Levani "
598798702,7170,lkokhreidze,2020-03-13T16:09:01Z,Ran `streams` module tests locally. All the tests passed.
599112529,7170,vvcephei,2020-03-14T18:07:44Z,"Hey @lkokhreidze , yes! I'm reviewing it _now_ :) 

Thanks so much for your patience. If it makes you feel better, I've been walking around with this sense of guilt over this PR hanging around."
599235445,7170,lkokhreidze,2020-03-15T16:48:04Z,Thanks @vvcephei for the update and no worries :) 
605279103,7170,vvcephei,2020-03-27T19:35:17Z,test this please
608775493,7170,lkokhreidze,2020-04-03T23:03:02Z,"Hi @mjsax, I've addressed your comments, would appreciate another review."
610434777,7170,lkokhreidze,2020-04-07T14:54:41Z,"Hi @mjsax @vvcephei 

Small update: https://github.com/apache/kafka/pull/7170/commits/f2bcdfe487ec13f41f4516d6bee6a1847f6d4ce2 In this commit I've added Topology optimization option as test parameter. This PR touches topology optimization (indirectly). In order to make sure that everything works as expected, I though it would beneficial in the integration tests verifying both, `topology.optimization: all` and `topology.optimization: none` configurations. Hope this makes sense.

Regards,
Levani"
610521827,7170,vvcephei,2020-04-07T17:33:24Z,"Wow, that's great. Thanks, @lkokhreidze !"
611807387,7170,mjsax,2020-04-09T23:53:25Z,Merged to `trunk`. Congrats @lkokhreidze! And thanks a lot for your hard work and patience!
612049574,7170,vvcephei,2020-04-10T14:20:21Z,"Yes, thank you @lkokhreidze for seeing this through!"
572334805,7884,ConcurrencyPractitioner,2020-01-09T01:14:53Z,@junrao This PR is ready for review. :)
572855268,7884,ConcurrencyPractitioner,2020-01-10T03:19:18Z,Retest this please.
574909313,7884,ConcurrencyPractitioner,2020-01-15T23:41:50Z,"Hi @junrao Thanks for the comments you left!

Overall, I managed to simplify the code somewhat and removed a couple of methods that was probably not necessary.  Notably, there is not as many calls involving ```batch``` as there was previously. Hope this was what you wanted. :)"
579021231,7884,ConcurrencyPractitioner,2020-01-28T00:24:28Z,"@junrao I've mostly resolved your comments. I'm working on how we could trigger a call for a clean when the latest delete horizon had been passed. Other than that, feel free to add anything else. :)"
581047464,7884,ConcurrencyPractitioner,2020-02-01T16:47:52Z,@junrao Do you want to take another look?
582220845,7884,ConcurrencyPractitioner,2020-02-05T03:11:56Z,@junrao pinging.
582709395,7884,junrao,2020-02-06T02:40:14Z,@ConcurrencyPractitioner : Thanks for the updated PR. Will take another look.
585015794,7884,ConcurrencyPractitioner,2020-02-12T03:57:23Z,"@junrao Alright, addressed most of the major comments. I thought that some of the comments might need some discussion before tackling. :)"
587968002,7884,ConcurrencyPractitioner,2020-02-19T00:09:56Z,"Alright, cool. @junrao All done. I decided to keep retrieveDeleteHorizon since the ```isBatchDiscardable``` flag is set in there, and afterwards, it is also used by ```checkBatchRetention``` as well. Since both of them use the same flag (which is located in LogCleaner), I think thats the best we can do."
588366375,7884,ConcurrencyPractitioner,2020-02-19T18:15:57Z,"@junrao Alright, I think this PR is pretty close. We might need to poll @hachikuji  for review if need be."
589856382,7884,ConcurrencyPractitioner,2020-02-21T22:04:03Z,"@junrao Cool, got these comments resolved."
591514108,7884,ConcurrencyPractitioner,2020-02-26T16:22:46Z,Alright @junrao Thanks for your patience! Got everything done.
592296237,7884,ConcurrencyPractitioner,2020-02-28T03:36:06Z,@junrao Thanks for the comprehensive review of my test code changes! It looks like I was a little to aggressive with my find / replace all usage. (that lead to a lot of Long.MaxValues being replaced sometimes unnecessarily by largeDeleteHorizon)
592297453,7884,ConcurrencyPractitioner,2020-02-28T03:40:43Z,Got these comments addressed.
592984991,7884,ConcurrencyPractitioner,2020-02-29T19:20:59Z,"@junrao Thanks for these comments!

About the comments that I left unaddressed. Turns out the tests failed if we only do two passes over the batch when we in fact need three.

I added some log statements, and this was the following behavior (this is for testAbortMarkerRemoval):

1. On this pass, containsTombstonesOrTxnMarker is false. i.e. we cannot remove the transaction marker yet (since onControlBatchRead() returned false).

2. On the second pass, containsTombstonesOrTxnMarker is now true. We can remove the transaction marker now. In this case, we have set the delete horizon, but have not _removed_ the delete horizon marker.

3. It is finally on the third pass that we can remove the transaction marker.

I found this was the resulting behavior (which lead to the need for three passes). I think the situation is similar for the other control batch tests as well.

"
596021569,7884,ConcurrencyPractitioner,2020-03-07T00:40:52Z,"@junrao Cool, we are good."
596026016,7884,junrao,2020-03-07T01:09:27Z,ok to test
596026481,7884,junrao,2020-03-07T01:12:35Z,"I also kick off system tests on this PR.

https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3823/"
596050250,7884,junrao,2020-03-07T05:44:45Z,retest this please
596050270,7884,junrao,2020-03-07T05:45:07Z,ok to test
596149722,7884,junrao,2020-03-07T23:55:49Z,add to whitelist
596151455,7884,ConcurrencyPractitioner,2020-03-08T00:18:54Z,test this please
596170283,7884,junrao,2020-03-08T05:59:17Z,retest this please
596226854,7884,ConcurrencyPractitioner,2020-03-08T16:59:34Z,"ok to test
"
596226994,7884,ConcurrencyPractitioner,2020-03-08T17:00:49Z,"@junrao Found the spotbugs violation. Turns out when I look through the console output, I can't find the error because its never logged. (Instead, its stored in some xml file which I cannot access)"
596231951,7884,junrao,2020-03-08T17:49:58Z,retest this please
596231999,7884,junrao,2020-03-08T17:50:26Z,ok to test
596667016,7884,junrao,2020-03-09T17:24:10Z,"Both tests seem to be failing at the following test.
```
11:34:01 org.apache.kafka.connect.util.KafkaBasedLogTest > testSendAndReadToEnd FAILED
11:34:01     java.lang.AssertionError: expected:<2> but was:<0>
11:34:01         at org.junit.Assert.fail(Assert.java:89)
11:34:01         at org.junit.Assert.failNotEquals(Assert.java:835)
11:34:01         at org.junit.Assert.assertEquals(Assert.java:647)
11:34:01         at org.junit.Assert.assertEquals(Assert.java:633)
11:34:01         at org.apache.kafka.connect.util.KafkaBasedLogTest.testSendAndReadToEnd(KafkaBasedLogTest.java:355)
```"
596825345,7884,ConcurrencyPractitioner,2020-03-09T23:21:53Z,@junrao Can you retrigger tests? I was not able to replicate the issue in local.
596841889,7884,junrao,2020-03-10T00:27:32Z,ok to test
596887928,7884,ConcurrencyPractitioner,2020-03-10T03:48:34Z,"retest this please
"
596896720,7884,junrao,2020-03-10T04:29:17Z,retest this please
596896786,7884,junrao,2020-03-10T04:29:38Z,ok to test
597320638,7884,ConcurrencyPractitioner,2020-03-10T21:13:25Z,"@junrao I did a little bit of research. It appears that my PR is _not_ responsible for this failing test. Perhaps, some prior PR broke it. See test result for this PR: https://github.com/apache/kafka/pull/8263."
597320967,7884,ConcurrencyPractitioner,2020-03-10T21:14:09Z,https://builds.apache.org/job/kafka-pr-jdk11-scala2.13/5097/testReport/
597348217,7884,junrao,2020-03-10T22:17:30Z,It seems that test failure was just fixed by KAFKA-9682. Triggering another test.
597348253,7884,junrao,2020-03-10T22:17:37Z,ok to test
597348410,7884,junrao,2020-03-10T22:18:06Z,retest this please
597348559,7884,junrao,2020-03-10T22:18:35Z,ok to test
597419512,7884,ConcurrencyPractitioner,2020-03-11T03:00:14Z,@junrao All tests are green.
599785168,7884,ConcurrencyPractitioner,2020-03-16T22:25:13Z,@hachikuji Any comments on LogCleaner?
600281660,7884,ConcurrencyPractitioner,2020-03-17T20:29:25Z,"@junrao I think I might know why the ""add to whitelist"" command doesn't work. Do you have to be a contributor in order to trigger a test? I checked the contributors list, but from what I could tell, my account handle isn't listed (which is kind of strange, since I've submitted a handful of PRs in the past).  "
600348752,7884,junrao,2020-03-17T23:28:52Z,@ConcurrencyPractitioner : Filed https://issues.apache.org/jira/browse/INFRA-19948 to figure out the whitelist issue.
600876622,7884,ConcurrencyPractitioner,2020-03-18T21:50:36Z,ok to test
601433830,7884,ConcurrencyPractitioner,2020-03-19T21:51:04Z,@hachikuji All comments addressed. See if there is anything else that we might need to account for.
605705828,7884,ConcurrencyPractitioner,2020-03-29T21:38:08Z,Pinging @hachikuji.
613061861,7884,ConcurrencyPractitioner,2020-04-13T19:42:34Z,@hachikuji Pinging for review
616751692,7884,junrao,2020-04-20T19:08:59Z,@ConcurrencyPractitioner : We now have https://github.com/apache/kafka/blob/trunk/.asf.yaml. You can add yourself to Jenkins's whitelist by following https://cwiki.apache.org/confluence/display/INFRA/.asf.yaml+features+for+git+repositories#id-.asf.yamlfeaturesforgitrepositories-JenkinsPRWhitelisting .
616773372,7884,ConcurrencyPractitioner,2020-04-20T19:52:32Z,"ok to test
"
616776257,7884,ConcurrencyPractitioner,2020-04-20T19:58:32Z,ok to test
616776614,7884,ConcurrencyPractitioner,2020-04-20T19:59:20Z,@junrao Cool. It's just that should I edit the ```.asf.yml``` as part of this PR? Or will I need to do it some other way?
616787194,7884,junrao,2020-04-20T20:21:52Z,@ConcurrencyPractitioner : You can just submit a separate PR to add yourself in .asf.yml.
616819297,7884,ConcurrencyPractitioner,2020-04-20T21:28:01Z,"@junrao Alright, got it done."
617537213,7884,ConcurrencyPractitioner,2020-04-22T04:09:02Z,"ok to test
"
617537261,7884,ConcurrencyPractitioner,2020-04-22T04:09:16Z,"test this please
"
618659683,7884,ConcurrencyPractitioner,2020-04-23T20:46:08Z,"@junrao I don't think the .asf.yaml worked. Tried to trigger a few test rounds, but Jenkins didn't respond."
618669412,7884,junrao,2020-04-23T21:03:35Z,"@ConcurrencyPractitioner Could you try ""retest this please""? If it still doesn't work, you can file an Apache infra jira for help."
618766295,7884,ConcurrencyPractitioner,2020-04-24T02:25:40Z,@junrao Did try on another PR. Looks like it didn't work. I will fire a JIRA.
618768078,7884,ConcurrencyPractitioner,2020-04-24T02:32:37Z,Reported in JIRA here: https://issues.apache.org/jira/browse/INFRA-20182
622490343,7884,ConcurrencyPractitioner,2020-05-01T17:48:29Z,@hachikuji Do you have time to review? Just give me a heads-up if there are some comments left unaddressed.
737674537,7884,wushujames,2020-12-03T05:26:37Z,Hi @ConcurrencyPractitioner . What is the status of this PR? We are also experiencing https://issues.apache.org/jira/browse/KAFKA-8522 . Thanks!
738119273,7884,junrao,2020-12-03T16:28:11Z,"@wushujames : This PR is mostly ready. It's just waiting for another committer more familiar with the transactional logic to take another look.

@ConcurrencyPractitioner : Would you be able to rebase this PR? Thanks."
760814414,7884,akamensky,2021-01-15T10:33:44Z,@ConcurrencyPractitioner @junrao this PR has been stale since April 2020. When would it be ready to merge? We are hitting this issue and it causes insanely long startup times in our applications as they need to read all the tombstones that are not being removed.
761223136,7884,ConcurrencyPractitioner,2021-01-15T22:06:10Z,@akamensky @wushujames @junrao Migrating to a new PR. You could find it here #9915.
289147056,2735,apurvam,2017-03-24T21:27:04Z,cc @junrao 
289160168,2735,asfbot,2017-03-24T22:41:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2371/
Test PASSed (JDK 7 and Scala 2.10).
"
289163885,2735,asfbot,2017-03-24T23:07:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2371/
Test FAILed (JDK 8 and Scala 2.12).
"
289164048,2735,asfbot,2017-03-24T23:08:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2375/
Test FAILed (JDK 8 and Scala 2.11).
"
289190917,2735,asfbot,2017-03-25T05:44:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2378/
Test FAILed (JDK 7 and Scala 2.10).
"
289191010,2735,asfbot,2017-03-25T05:46:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2382/
Test PASSed (JDK 8 and Scala 2.11).
"
289191988,2735,asfbot,2017-03-25T06:13:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2378/
Test FAILed (JDK 8 and Scala 2.12).
"
289222942,2735,apurvam,2017-03-25T16:34:23Z,"The newly introduced integration test seems to be flaky on Jenkins. There are produce exceptions when bouncing brokers. I didn't see this locally, and I have run it 100's of times. Will look into it."
289226093,2735,asfbot,2017-03-25T17:21:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2380/
Test FAILed (JDK 7 and Scala 2.10).
"
289226296,2735,asfbot,2017-03-25T17:24:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2384/
Test PASSed (JDK 8 and Scala 2.11).
"
289227879,2735,asfbot,2017-03-25T17:50:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2380/
Test FAILed (JDK 8 and Scala 2.12).
"
289621321,2735,asfbot,2017-03-27T23:59:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2431/
Test PASSed (JDK 7 and Scala 2.10).
"
289621810,2735,asfbot,2017-03-28T00:02:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2431/
Test PASSed (JDK 8 and Scala 2.12).
"
289626999,2735,apurvam,2017-03-28T00:35:32Z,"I found the reason why the new integration test was failing. With a recent refactor of `Sender.completeBatch`, we only requested a metadata update on non-retriable errors. This was a regression, and as a result, when the leader for a partition changed, the metadata would not get updated and all the retries would get exhausted.

I moved the metadata update code to the correct place, and the tests are passing now. I also addressed other PR comments. cc @junrao "
289627070,2735,apurvam,2017-03-28T00:36:08Z,"Somehow the new integration test only failed with JDK7 or scala 2.12, neither of which I run locally. So I never caught it until this PR."
289633931,2735,asfbot,2017-03-28T01:20:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2434/
Test PASSed (JDK 7 and Scala 2.10).
"
289633987,2735,asfbot,2017-03-28T01:20:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2438/
Test PASSed (JDK 8 and Scala 2.11).
"
289633997,2735,asfbot,2017-03-28T01:20:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2434/
Test PASSed (JDK 8 and Scala 2.12).
"
289641340,2735,asfbot,2017-03-28T02:11:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2435/
Test FAILed (JDK 8 and Scala 2.11).
"
289945166,2735,asfbot,2017-03-29T00:25:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2484/
Test PASSed (JDK 8 and Scala 2.11).
"
289950751,2735,asfbot,2017-03-29T01:03:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2480/
Test FAILed (JDK 8 and Scala 2.12).
"
289952136,2735,apurvam,2017-03-29T01:12:07Z,"Current TODOS from discussing with Jun offline:

1. Filed https://issues.apache.org/jira/browse/KAFKA-4970 as a future improvement. 
2. Need to remove the `Log.updateIdMap` which removes entries before the current dirty offset from the pid map. this is a bug which would result in real entries being lost. 
3. Need to name the snapshot files appropriately. Currently they are in the partition directory, but have the topic partition in their name, which is redundant. 
4. Need to ensure that `ProducerIdMapping.maybeTakeSnapshot` is called periodically. "
289965055,2735,asfbot,2017-03-29T02:41:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2480/
Test FAILed (JDK 7 and Scala 2.10).
"
289984726,2735,asfbot,2017-03-29T05:15:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2488/
Test PASSed (JDK 8 and Scala 2.11).
"
289984850,2735,asfbot,2017-03-29T05:16:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2484/
Test PASSed (JDK 7 and Scala 2.10).
"
290006973,2735,asfbot,2017-03-29T07:29:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2484/
Test FAILed (JDK 8 and Scala 2.12).
"
290159999,2735,asfbot,2017-03-29T17:19:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2501/
Test PASSed (JDK 7 and Scala 2.10).
"
290160270,2735,asfbot,2017-03-29T17:20:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2505/
Test PASSed (JDK 8 and Scala 2.11).
"
290171112,2735,asfbot,2017-03-29T17:57:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2501/
Test PASSed (JDK 8 and Scala 2.12).
"
290278536,2735,asfbot,2017-03-30T01:51:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2515/
Test PASSed (JDK 7 and Scala 2.10).
"
290283877,2735,asfbot,2017-03-30T02:28:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2519/
Test PASSed (JDK 8 and Scala 2.11).
"
290296670,2735,asfbot,2017-03-30T04:06:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2515/
Test FAILed (JDK 8 and Scala 2.12).
"
290304003,2735,asfbot,2017-03-30T05:09:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2517/
Test FAILed (JDK 8 and Scala 2.12).
"
290304038,2735,asfbot,2017-03-30T05:10:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2517/
Test FAILed (JDK 7 and Scala 2.10).
"
290304097,2735,asfbot,2017-03-30T05:10:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2521/
Test FAILed (JDK 8 and Scala 2.11).
"
290304811,2735,asfbot,2017-03-30T05:16:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2522/
Test FAILed (JDK 8 and Scala 2.11).
"
290304832,2735,asfbot,2017-03-30T05:16:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2518/
Test FAILed (JDK 7 and Scala 2.10).
"
290304835,2735,asfbot,2017-03-30T05:16:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2518/
Test FAILed (JDK 8 and Scala 2.12).
"
290309068,2735,apurvam,2017-03-30T05:50:04Z,"@junrao I addressed most of your comments, except two. Especially for `sendAndAwaitInitPidRequest` comment, I would like to sync face to face tomorrow. "
290311267,2735,asfbot,2017-03-30T06:05:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2523/
Test PASSed (JDK 8 and Scala 2.11).
"
290311356,2735,asfbot,2017-03-30T06:06:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2519/
Test PASSed (JDK 7 and Scala 2.10).
"
290317687,2735,asfbot,2017-03-30T06:45:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2519/
Test PASSed (JDK 8 and Scala 2.12).
"
290487975,2735,asfbot,2017-03-30T17:46:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2544/
Test FAILed (JDK 8 and Scala 2.11).
"
290488303,2735,asfbot,2017-03-30T17:47:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2540/
Test FAILed (JDK 8 and Scala 2.12).
"
290488441,2735,asfbot,2017-03-30T17:48:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2540/
Test FAILed (JDK 7 and Scala 2.10).
"
290511638,2735,asfbot,2017-03-30T19:00:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2542/
Test PASSed (JDK 7 and Scala 2.10).
"
290513197,2735,asfbot,2017-03-30T19:05:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2546/
Test PASSed (JDK 8 and Scala 2.11).
"
290522023,2735,asfbot,2017-03-30T19:39:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2542/
Test PASSed (JDK 8 and Scala 2.12).
"
290545801,2735,asfbot,2017-03-30T21:09:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2559/
Test PASSed (JDK 8 and Scala 2.11).
"
290545813,2735,asfbot,2017-03-30T21:10:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2555/
Test PASSed (JDK 7 and Scala 2.10).
"
290572861,2735,asfbot,2017-03-30T23:23:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2555/
Test FAILed (JDK 8 and Scala 2.12).
"
290622112,2735,apurvam,2017-03-31T05:47:44Z,"Latest run of the system tests on this branch succeeded: https://jenkins.confluent.io/job/idempotent-producer-system-tests/5/console

Kicked off a muckrake run against this branch too : http://jenkins.confluent.io/job/system-test-confluent-platform-branch-builder/203"
290625051,2735,apurvam,2017-03-31T06:04:45Z,"Thanks for the review @junrao, @hachikuji, and @ijuma .. I think I have addressed all the comments. The system tests are running as well (links above). "
290631609,2735,asfbot,2017-03-31T06:50:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2566/
Test PASSed (JDK 7 and Scala 2.10).
"
290638632,2735,asfbot,2017-03-31T07:31:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2566/
Test PASSed (JDK 8 and Scala 2.12).
"
290638643,2735,asfbot,2017-03-31T07:31:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2570/
Test PASSed (JDK 8 and Scala 2.11).
"
290825505,2735,asfbot,2017-03-31T20:46:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2585/
Test FAILed (JDK 7 and Scala 2.10).
"
290826739,2735,asfbot,2017-03-31T20:52:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2589/
Test FAILed (JDK 8 and Scala 2.11).
"
290858431,2735,asfbot,2017-03-31T23:15:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2596/
Test FAILed (JDK 8 and Scala 2.12).
"
290862653,2735,asfbot,2017-03-31T23:28:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2596/
Test PASSed (JDK 7 and Scala 2.10).
"
290865505,2735,asfbot,2017-03-31T23:37:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2585/
Test FAILed (JDK 8 and Scala 2.12).
"
290880753,2735,apurvam,2017-04-01T00:49:22Z,"The jenkins builds are puzzling. Mostly different streams tests fail in each run. There hasn't been a failure in core or clients for a while.

The perf numbers are being collected here: https://docs.google.com/spreadsheets/d/1dHY6M7qCiX-NFvsgvaE0YoVdNq26uA8608XIh_DUpI4/edit#gid=1276994626

So far, so good. Actually enabling the idempotent producer costs 20% throughput. But using the new code with idempotent producer turned off seems to have no effect. 

A muckrake test is finally running properly and seems to be passing: http://jenkins.confluent.io/job/system-test-confluent-platform-branch-builder/206/console

I also addressed most of the major comments from Ismael, ie. those which had to do with adding test cases or improving error messages or improving documentation. The remaining ones are more to with minor code style."
290881598,2735,asfbot,2017-04-01T00:59:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2597/
Test FAILed (JDK 7 and Scala 2.10).
"
290881624,2735,asfbot,2017-04-01T01:00:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2601/
Test FAILed (JDK 8 and Scala 2.11).
"
290881808,2735,asfbot,2017-04-01T01:02:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2597/
Test FAILed (JDK 8 and Scala 2.12).
"
290883092,2735,apurvam,2017-04-01T01:18:45Z,"retest this please
"
290883585,2735,asfbot,2017-04-01T01:25:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2602/
Test FAILed (JDK 8 and Scala 2.11).
"
290883589,2735,asfbot,2017-04-01T01:25:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2598/
Test FAILed (JDK 7 and Scala 2.10).
"
290883963,2735,asfbot,2017-04-01T01:30:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2598/
Test FAILed (JDK 8 and Scala 2.12).
"
290935845,2735,apurvam,2017-04-01T17:47:53Z,"Muckrake test passed: http://jenkins.confluent.io/job/system-test-confluent-platform-branch-builder/206/

System test against this branch also passed: https://jenkins.confluent.io/job/idempotent-producer-system-tests/7/

Investigating the jenkins branch builder failures."
290937183,2735,asfbot,2017-04-01T18:10:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2608/
Test FAILed (JDK 8 and Scala 2.11).
"
290937397,2735,apurvam,2017-04-01T18:13:52Z,"Latest failure (JDK8 and Scala 2.11): 

```
org.apache.kafka.streams.StreamsConfigTest > shouldSupportPrefixedRestoreConsumerConfigs PASSED
pure virtual method called
terminate called without an active exception
:streams:unitTest FAILED
```
Somehow my local runs are super stable. I don't know what would cause this kind of thing."
290938495,2735,asfbot,2017-04-01T18:31:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2604/
Test FAILed (JDK 7 and Scala 2.10).
"
290940229,2735,asfbot,2017-04-01T19:00:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2604/
Test PASSed (JDK 8 and Scala 2.12).
"
291037102,2735,junrao,2017-04-03T02:40:36Z,@apurvam : Thanks for the patch. LGTM
129675461,130,asfbot,2015-08-11T02:05:18Z,"[kafka-trunk-git-pr #122](https://builds.apache.org/job/kafka-trunk-git-pr/122/) SUCCESS
This pull request looks good
"
129892398,130,lazyval,2015-08-11T14:06:46Z,"It's quite awkward to see commits like this 

<img width=""407"" alt=""kafka 2015-08-11 17-06-37"" src=""https://cloud.githubusercontent.com/assets/235297/9199493/5497b816-404b-11e5-8521-df59a1e37918.png"">
"
129935567,130,guozhangwang,2015-08-11T15:42:11Z,"@lazyval Apologies for the commits history, I was fighting with git merge history back then from two branches and hence the commits was not well organized. I will create another PR with squashed commits after addressing the collected comments from this PR.
"
131564609,130,rhauch,2015-08-16T14:44:27Z,"This is an excellent proposal, and after a quick pass this PR looks good. More detailed comments and questions to follow.
"
132316453,130,rhauch,2015-08-18T18:50:23Z,"Is there a way to make the sources and processors created by the topology aware of or dependent upon the supplied configuration? For example, let's say my job's configuration has a parameter that, if set one way, will result in a slight variation of the topology (e.g., an extra filtering processor between the source and my primary processor). Is that possible?
"
132731196,130,guozhangwang,2015-08-19T18:20:11Z,"@rhauch @ijuma thanks for the comments. Just to be clear we are actively working on addressing them and will respond them individually once the next version of this patch is finished, with a squashed commit history.
"
133511793,130,asfbot,2015-08-21T17:53:02Z,"[kafka-trunk-git-pr #189](https://builds.apache.org/job/kafka-trunk-git-pr/189/) FAILURE
Looks like there's a problem with this pull request
"
133943310,130,rhauch,2015-08-23T21:54:11Z,"@guozhangwang, I'm willing to help resolve issues, add test cases, make suggestions via patches, and even add JavaDoc. But I suspect that'd be easier after after you squash commit history. Please let me know what you think.
"
135150897,130,asfbot,2015-08-26T19:47:09Z,"[kafka-trunk-git-pr #229](https://builds.apache.org/job/kafka-trunk-git-pr/229/) FAILURE
Looks like there's a problem with this pull request
"
135195397,130,asfbot,2015-08-26T22:25:16Z,"[kafka-trunk-git-pr #232](https://builds.apache.org/job/kafka-trunk-git-pr/232/) FAILURE
Looks like there's a problem with this pull request
"
135535170,130,asfbot,2015-08-27T19:52:32Z,"[kafka-trunk-git-pr #239](https://builds.apache.org/job/kafka-trunk-git-pr/239/) FAILURE
Looks like there's a problem with this pull request
"
135555195,130,asfbot,2015-08-27T21:07:24Z,"[kafka-trunk-git-pr #241](https://builds.apache.org/job/kafka-trunk-git-pr/241/) FAILURE
Looks like there's a problem with this pull request
"
135578985,130,asfbot,2015-08-27T23:05:34Z,"[kafka-trunk-git-pr #245](https://builds.apache.org/job/kafka-trunk-git-pr/245/) FAILURE
Looks like there's a problem with this pull request
"
135834017,130,asfbot,2015-08-28T17:07:02Z,"[kafka-trunk-git-pr #249](https://builds.apache.org/job/kafka-trunk-git-pr/249/) FAILURE
Looks like there's a problem with this pull request
"
135845024,130,asfbot,2015-08-28T17:52:34Z,"[kafka-trunk-git-pr #251](https://builds.apache.org/job/kafka-trunk-git-pr/251/) FAILURE
Looks like there's a problem with this pull request
"
135846035,130,asfbot,2015-08-28T17:56:02Z,"[kafka-trunk-git-pr #252](https://builds.apache.org/job/kafka-trunk-git-pr/252/) FAILURE
Looks like there's a problem with this pull request
"
135855418,130,asfbot,2015-08-28T18:33:40Z,"[kafka-trunk-git-pr #253](https://builds.apache.org/job/kafka-trunk-git-pr/253/) FAILURE
Looks like there's a problem with this pull request
"
135860057,130,asfbot,2015-08-28T18:55:49Z,"[kafka-trunk-git-pr #254](https://builds.apache.org/job/kafka-trunk-git-pr/254/) FAILURE
Looks like there's a problem with this pull request
"
135896629,130,asfbot,2015-08-28T21:52:08Z,"[kafka-trunk-git-pr #258](https://builds.apache.org/job/kafka-trunk-git-pr/258/) FAILURE
Looks like there's a problem with this pull request
"
135898637,130,asfbot,2015-08-28T22:04:34Z,"[kafka-trunk-git-pr #259](https://builds.apache.org/job/kafka-trunk-git-pr/259/) FAILURE
Looks like there's a problem with this pull request
"
135905217,130,asfbot,2015-08-28T22:46:56Z,"[kafka-trunk-git-pr #260](https://builds.apache.org/job/kafka-trunk-git-pr/260/) FAILURE
Looks like there's a problem with this pull request
"
135907602,130,guozhangwang,2015-08-28T23:05:51Z,"@rhauch For ""Is there a way to make the sources and processors created by the topology aware of or dependent upon the supplied configuration? For example, let's say my job's configuration has a parameter that, if set one way, will result in a slight variation of the topology (e.g., an extra filtering processor between the source and my primary processor). Is that possible?""

For this case, could it be written by adding a filter with a config that depending on its value, do no-op or the real filtering logic? Since all these operations will be in-memory, the only overhead will be checking the configured flag for each record, which should be negligible.
"
135908565,130,guozhangwang,2015-08-28T23:15:20Z,"@rhauch Thanks for being interested to contribute! We are currently shooting for a second patch next Monday / Tuesday, it changes the Processor APIs and threading models quite a bit from the original patch and hence I will also update the KIP wiki accordingly. It will be great if you can then take that patch and help adding more unit tests / java docs / reviews / anything.
"
136819392,130,asfbot,2015-09-01T18:23:13Z,"[kafka-trunk-git-pr #290](https://builds.apache.org/job/kafka-trunk-git-pr/290/) FAILURE
Looks like there's a problem with this pull request
"
136861020,130,asfbot,2015-09-01T21:04:13Z,"[kafka-trunk-git-pr #292](https://builds.apache.org/job/kafka-trunk-git-pr/292/) FAILURE
Looks like there's a problem with this pull request
"
136892486,130,asfbot,2015-09-01T23:38:33Z,"[kafka-trunk-git-pr #301](https://builds.apache.org/job/kafka-trunk-git-pr/301/) FAILURE
Looks like there's a problem with this pull request
"
137150826,130,asfbot,2015-09-02T16:18:14Z,"[kafka-trunk-git-pr #315](https://builds.apache.org/job/kafka-trunk-git-pr/315/) FAILURE
Looks like there's a problem with this pull request
"
137179775,130,asfbot,2015-09-02T17:28:25Z,"[kafka-trunk-git-pr #317](https://builds.apache.org/job/kafka-trunk-git-pr/317/) FAILURE
Looks like there's a problem with this pull request
"
137231357,130,asfbot,2015-09-02T20:18:36Z,"[kafka-trunk-git-pr #325](https://builds.apache.org/job/kafka-trunk-git-pr/325/) FAILURE
Looks like there's a problem with this pull request
"
137247874,130,asfbot,2015-09-02T21:20:27Z,"[kafka-trunk-git-pr #326](https://builds.apache.org/job/kafka-trunk-git-pr/326/) FAILURE
Looks like there's a problem with this pull request
"
137271667,130,asfbot,2015-09-02T23:19:49Z,"[kafka-trunk-git-pr #328](https://builds.apache.org/job/kafka-trunk-git-pr/328/) FAILURE
Looks like there's a problem with this pull request
"
137619337,130,asfbot,2015-09-04T01:35:25Z,"[kafka-trunk-git-pr #350](https://builds.apache.org/job/kafka-trunk-git-pr/350/) FAILURE
Looks like there's a problem with this pull request
"
137788596,130,asfbot,2015-09-04T16:48:42Z,"[kafka-trunk-git-pr #354](https://builds.apache.org/job/kafka-trunk-git-pr/354/) FAILURE
Looks like there's a problem with this pull request
"
137831817,130,asfbot,2015-09-04T19:24:24Z,"[kafka-trunk-git-pr #356](https://builds.apache.org/job/kafka-trunk-git-pr/356/) FAILURE
Looks like there's a problem with this pull request
"
138945194,130,rhauch,2015-09-09T15:22:45Z,"@guozhangwang, in case you missed it, here's a PR for the NPE in MeteredKeyValueStore mentioned [way above](https://github.com/apache/kafka/pull/130/files#r39055851): https://github.com/confluentinc/kafka/pull/38
"
139066958,130,asfbot,2015-09-09T22:56:16Z,"[kafka-trunk-git-pr #383](https://builds.apache.org/job/kafka-trunk-git-pr/383/) FAILURE
Looks like there's a problem with this pull request
"
139327040,130,asfbot,2015-09-10T17:57:07Z,"[kafka-trunk-git-pr #387](https://builds.apache.org/job/kafka-trunk-git-pr/387/) SUCCESS
This pull request looks good
"
139373847,130,asfbot,2015-09-10T20:44:53Z,"[kafka-trunk-git-pr #390](https://builds.apache.org/job/kafka-trunk-git-pr/390/) SUCCESS
This pull request looks good
"
139424021,130,asfbot,2015-09-11T01:19:45Z,"[kafka-trunk-git-pr #392](https://builds.apache.org/job/kafka-trunk-git-pr/392/) SUCCESS
This pull request looks good
"
139683605,130,asfbot,2015-09-11T23:07:03Z,"[kafka-trunk-git-pr #405](https://builds.apache.org/job/kafka-trunk-git-pr/405/) SUCCESS
This pull request looks good
"
139691461,130,asfbot,2015-09-12T00:19:12Z,"[kafka-trunk-git-pr #408](https://builds.apache.org/job/kafka-trunk-git-pr/408/) FAILURE
Looks like there's a problem with this pull request
"
140353211,130,asfbot,2015-09-15T10:57:05Z,"[kafka-trunk-git-pr #425](https://builds.apache.org/job/kafka-trunk-git-pr/425/) FAILURE
Looks like there's a problem with this pull request
"
140827956,130,asfbot,2015-09-16T18:15:27Z,"[kafka-trunk-git-pr #435](https://builds.apache.org/job/kafka-trunk-git-pr/435/) FAILURE
Looks like there's a problem with this pull request
"
141573139,130,asfbot,2015-09-18T21:26:31Z,"[kafka-trunk-git-pr #447](https://builds.apache.org/job/kafka-trunk-git-pr/447/) FAILURE
Looks like there's a problem with this pull request
"
142043832,130,asfbot,2015-09-21T17:03:52Z,"[kafka-trunk-git-pr #461](https://builds.apache.org/job/kafka-trunk-git-pr/461/) FAILURE
Looks like there's a problem with this pull request
"
142069100,130,asfbot,2015-09-21T18:31:33Z,"[kafka-trunk-git-pr #462](https://builds.apache.org/job/kafka-trunk-git-pr/462/) FAILURE
Looks like there's a problem with this pull request
"
142070680,130,asfbot,2015-09-21T18:37:38Z,"[kafka-trunk-git-pr #463](https://builds.apache.org/job/kafka-trunk-git-pr/463/) FAILURE
Looks like there's a problem with this pull request
"
142084786,130,asfbot,2015-09-21T19:25:32Z,"[kafka-trunk-git-pr #466](https://builds.apache.org/job/kafka-trunk-git-pr/466/) SUCCESS
This pull request looks good
"
142135424,130,asfbot,2015-09-21T23:17:35Z,"[kafka-trunk-git-pr #473](https://builds.apache.org/job/kafka-trunk-git-pr/473/) FAILURE
Looks like there's a problem with this pull request
"
142378899,130,asfbot,2015-09-22T18:39:12Z,"[kafka-trunk-git-pr #483](https://builds.apache.org/job/kafka-trunk-git-pr/483/) FAILURE
Looks like there's a problem with this pull request
"
142404004,130,asfbot,2015-09-22T20:07:22Z,"[kafka-trunk-git-pr #484](https://builds.apache.org/job/kafka-trunk-git-pr/484/) FAILURE
Looks like there's a problem with this pull request
"
142410903,130,asfbot,2015-09-22T20:30:35Z,"[kafka-trunk-git-pr #485](https://builds.apache.org/job/kafka-trunk-git-pr/485/) FAILURE
Looks like there's a problem with this pull request
"
142441147,130,asfbot,2015-09-22T22:37:47Z,"[kafka-trunk-git-pr #488](https://builds.apache.org/job/kafka-trunk-git-pr/488/) FAILURE
Looks like there's a problem with this pull request
"
142456715,130,asfbot,2015-09-22T23:59:31Z,"[kafka-trunk-git-pr #491](https://builds.apache.org/job/kafka-trunk-git-pr/491/) FAILURE
Looks like there's a problem with this pull request
"
142464075,130,asfbot,2015-09-23T00:50:51Z,"[kafka-trunk-git-pr #493](https://builds.apache.org/job/kafka-trunk-git-pr/493/) SUCCESS
This pull request looks good
"
142681892,130,rhauch,2015-09-23T18:03:50Z,"@guozhangwang, I'm not sure what happened, but `build.gradle` is still missing this code:

```
artifacts {
    archives testJar
}
```

and without it the build does not upload/publish the test JAR to Maven (at least locally). My original PR (#34) to add these and a few other lines was merged into this branch, but it looks like auto-merge in 74455f29f2 put them in the 'tools' area rather than 'streams'. Again, without it clients cannot reuse any of the test classes (like `KStreamTestDriver`) in their own tests.
"
142687808,130,guozhangwang,2015-09-23T18:23:47Z,"@rhauch you are right, the tools package was added at roughly the same time and hence probably messed up with the auto-merge. We can add that again.
"
142708558,130,asfbot,2015-09-23T19:46:35Z,"[kafka-trunk-git-pr #502](https://builds.apache.org/job/kafka-trunk-git-pr/502/) FAILURE
Looks like there's a problem with this pull request
"
142713668,130,asfbot,2015-09-23T20:04:44Z,"[kafka-trunk-git-pr #503](https://builds.apache.org/job/kafka-trunk-git-pr/503/) FAILURE
Looks like there's a problem with this pull request
"
142719926,130,rhauch,2015-09-23T20:31:04Z,"@guozhangwang, I also have a `KeyValueStore` implementation that maintains an in-memory, limited-size LRU cache (e.g., `InMemoryLRUCacheStore`) of recently used entries. Sometimes a processor wants to track up to _n_ recently-used items, and this is a convenient way to do this. I'm willing to contribute it via a new pull request if you are interested.

It does need a few enhancements to `MeteredKeyValueStore` to better support entries being ""automatically"" removed by the 'inner' store. 

But a larger problem is that `MeteredKeyValueStore` currently assumes it can get the key and value serializers and deserializers from the context. IIUC this is not always valid since those are merely the _default_ key and value serializers and deserializers. For example, I might have a topology that uses multiple processors with different key and value types, in which case the context's default (de)serializers will be valid for only one of the processors. Or, I might have a single processor that uses a key value store with different key and value types than the processor. So this seems like an invalid assumption that needs to be fixed.

If you agree that the latter is a problem, I'd be happy to create a PR to address it. If you're interested in the `InMemoryLRUCacheStore` class, that could be a separate PR.
"
142740387,130,asfbot,2015-09-23T21:49:48Z,"[kafka-trunk-git-pr #508](https://builds.apache.org/job/kafka-trunk-git-pr/508/) FAILURE
Looks like there's a problem with this pull request
"
142743162,130,asfbot,2015-09-23T21:57:21Z,"[kafka-trunk-git-pr #509](https://builds.apache.org/job/kafka-trunk-git-pr/509/) FAILURE
Looks like there's a problem with this pull request
"
142749083,130,guozhangwang,2015-09-23T22:31:58Z,"@rhauch I agree about ser-de, maybe we can allow users to pass in their (de)serializers upon construction of the key-value store.

About `InMemoryLRUCacheStore`, feel free to go ahead with another PR.
"
142762261,130,rhauch,2015-09-23T23:46:51Z,"@guozhangwang, see my [draft PR](https://github.com/confluentinc/kafka/pull/74) for the serdes changes; should be up-to-date with the `streaming` branch. Feedback appreciated.
"
143052732,130,rhauch,2015-09-24T21:14:15Z,"@guozhangwang, I created a PR for the [InMemoryLRUCacheStore](https://github.com/confluentinc/kafka/pull/75) and a mini test framework for key-value stores, with new unit tests for all current `KeyValueStore` implementations. It does depends on my proposed [serdes](https://github.com/confluentinc/kafka/pull/74) changes in key-value stores.
"
143324924,130,asfbot,2015-09-25T19:06:12Z,"[kafka-trunk-git-pr #555](https://builds.apache.org/job/kafka-trunk-git-pr/555/) FAILURE
Looks like there's a problem with this pull request
"
143379373,130,asfbot,2015-09-25T23:56:36Z,"[kafka-trunk-git-pr #562](https://builds.apache.org/job/kafka-trunk-git-pr/562/) SUCCESS
This pull request looks good
"
143380723,130,guozhangwang,2015-09-26T00:10:32Z,"Update the PR body before merging, and the original message is here:

---

Some open questions collected so far on the first patch. Thanks @gwenshap @jkreps @junrao .
- Topology API: requiring users to instantiate their own Topology class with the overridden build() function is a little awkward. Instead it would be great to let users explicitly build the topology in Main and pass it in as a class:

```
    Topology myTopology = new TopologyBuilder(defaultDeser)
                                                 .addProcessor(""my-processor"", MyProcessor.class, new Source(""my-source""))
                                                 .addProcessor(""my-other-processor"", MyOtherProcessor.class, ""my-processor"");
    KafkaStreaming streaming = new KafkaStreaming(config, myTopology);
   streaming.run();
```

So the implementation of KStream.filter look instead like this:

```
    public KStream<K, V> filter(Predicate<K, V> predicate) {
        KStreamFilter<K, V> filter = new KStreamFilter<>();
        topology.addProcessor(KStreamFilter.class, new Configs(""predicate"", predicate));
        return this;
    }
```

The advantage is that the user code can now get rid of the whole Topology class with the builder. I think the order of execution for that API is quite unintuitive.
- We can probably move the forward() function from Processor to ProcessorContext, and split ProcessorContext into two classes, one with all the function calls as commit / send / schedule / forward, and another with the metadata function calls as topic / partition / offset / timestamp.
- Can we hide the Chooser interface from users? In other words, if users can specify the ""time"" on each fetched messages from Kafka, would a hard-coded MinTimestampMessageChooser be sufficient so that we can move TimestampTracker / RecordQueue / Chooser / RecordCollector / etc all to the internal folders?
- Shall we split the o.a.k.clients into two folders, with o.a.k.clients.processor in stream? Or should we just remove o.a.k.clients.processor and make everything under o.a.k.stream? In addition, currently there is a cyclic dependency between that two, would better to break it in the end state.
- Consider moving the external dependencies such as RocksDB into a separate jar? For example we can just include a kafka-stream-rocksdb.jar which includes the RocksDBKeyValueStore only, and later on when we deprecate / remove such implementations we can simply remove the jar itself.
- The way the KeyValueStore is created seems a bit weird. Since this is part of the internal state managed by KafkaProcessorContext, it seems there should be an api to create the KeyValueStore from KafkaProcessorContext, instead of passing context to the constructor of KeyValueStore.
- Merge ProcessorConfigs with ProcessorProperties.
- We can potentially remove the processor argument in ProcessorContext.schedule().
"
320159365,3621,asfgit,2017-08-04T05:24:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6538/
Test FAILed (JDK 7 and Scala 2.11).
"
320159782,3621,asfgit,2017-08-04T05:27:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6539/
Test FAILed (JDK 7 and Scala 2.11).
"
320160629,3621,asfgit,2017-08-04T05:34:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6540/
Test FAILed (JDK 7 and Scala 2.11).
"
320162288,3621,lindong28,2017-08-04T05:48:45Z,"@becketqin @junrao @ijuma Can you help review this patch if you have time? Thank you!

This patch has implemented all features in KIP-113 except the ability to move replica between log directories on the same broker. The implementation of that feature is more complicated because it requires the creation of a temporary replica on the broker. Thus I choose to separate the implementation of KIP-113 into two patches.

Note that this patch allows user to specify the destination log directory of a replica if the replica has not been created on the broker yet. More specifically, user can use AdminClient.changeReplicaDir() before creating the reassignment znode so that the replica will be created in the destination log directory when broker receives LeaderAndIsrRequest later. This allows user to balance load across log directories using the `kafka-reassign-partitions.sh` as long as the replica is always moved to a given log directory on another broker. The caveat is that the replica may be created in the wrong log directory if the broker restarts after it received ChangeReplicaDirRequest but before it receives LeaderAndIsrRequest.




"
320171412,3621,asfgit,2017-08-04T06:49:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6541/
Test PASSed (JDK 7 and Scala 2.11).
"
320175820,3621,asfgit,2017-08-04T07:14:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6527/
Test PASSed (JDK 8 and Scala 2.12).
"
320349917,3621,asfgit,2017-08-04T20:49:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6586/
Test PASSed (JDK 7 and Scala 2.11).
"
320351504,3621,asfgit,2017-08-04T20:57:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6571/
Test PASSed (JDK 8 and Scala 2.12).
"
320467363,3621,asfgit,2017-08-05T20:04:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6576/
Test FAILed (JDK 8 and Scala 2.12).
"
320467378,3621,asfgit,2017-08-05T20:04:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6591/
Test FAILed (JDK 7 and Scala 2.11).
"
320470564,3621,asfgit,2017-08-05T21:09:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6592/
Test PASSed (JDK 7 and Scala 2.11).
"
320471511,3621,asfgit,2017-08-05T21:30:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6577/
Test PASSed (JDK 8 and Scala 2.12).
"
320573706,3621,asfgit,2017-08-07T05:43:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6599/
Test PASSed (JDK 7 and Scala 2.11).
"
320575781,3621,asfgit,2017-08-07T05:59:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6584/
Test FAILed (JDK 8 and Scala 2.12).
"
320807465,3621,asfgit,2017-08-07T23:25:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6627/
Test PASSed (JDK 7 and Scala 2.11).
"
320811339,3621,asfgit,2017-08-07T23:52:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6612/
Test PASSed (JDK 8 and Scala 2.12).
"
320899573,3621,asfgit,2017-08-08T09:16:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6623/
Test FAILed (JDK 8 and Scala 2.12).
"
320914017,3621,asfgit,2017-08-08T10:15:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6638/
Test PASSed (JDK 7 and Scala 2.11).
"
321052266,3621,asfgit,2017-08-08T19:08:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6628/
Test FAILed (JDK 8 and Scala 2.12).
"
321054272,3621,lindong28,2017-08-08T19:15:43Z,@becketqin Tests have been added to cover every new request/response/API and I have reviewed the patch end-to-end. It is fully ready for review now. Thanks!
321066245,3621,asfgit,2017-08-08T20:05:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6643/
Test PASSed (JDK 7 and Scala 2.11).
"
321724065,3621,asfgit,2017-08-11T03:16:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6695/
Test PASSed (JDK 7 and Scala 2.11).
"
321726508,3621,asfgit,2017-08-11T03:44:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6680/
Test PASSed (JDK 8 and Scala 2.12).
"
322026140,3621,asfgit,2017-08-13T07:10:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6713/
Test FAILed (JDK 8 and Scala 2.12).
"
322026164,3621,asfgit,2017-08-13T07:10:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6728/
Test FAILed (JDK 7 and Scala 2.11).
"
322028384,3621,asfgit,2017-08-13T08:11:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6729/
Test PASSed (JDK 7 and Scala 2.11).
"
322029332,3621,asfgit,2017-08-13T08:35:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6714/
Test PASSed (JDK 8 and Scala 2.12).
"
322340730,3621,asfgit,2017-08-14T23:52:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6745/
Test FAILed (JDK 8 and Scala 2.12).
"
322340768,3621,asfgit,2017-08-14T23:52:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6760/
Test FAILed (JDK 7 and Scala 2.11).
"
322348813,3621,asfgit,2017-08-15T00:54:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6761/
Test PASSed (JDK 7 and Scala 2.11).
"
322349033,3621,lindong28,2017-08-15T00:56:46Z,@becketqin Thanks much for your review. I have addressed all comments and reviewed the patch end-to-end. Can you take another look? Thanks!
322351703,3621,asfgit,2017-08-15T01:18:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6746/
Test PASSed (JDK 8 and Scala 2.12).
"
322372267,3621,asfgit,2017-08-15T04:04:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6764/
Test PASSed (JDK 7 and Scala 2.11).
"
322375260,3621,asfgit,2017-08-15T04:34:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6749/
Test PASSed (JDK 8 and Scala 2.12).
"
323557883,3621,lindong28,2017-08-20T01:44:34Z,"@becketqin Thanks much for taking time to review the patch! I have addressed most of the comments and rebased the patch onto trunk head. The only one that I am not sure is whether we should rename `AlterReplicaDirRequest` to `AlterReplicaLogDirRequest` (and similarly for other related classes). After this one is addressed, I will review the patch myself end-to-end and let you know."
323606600,3621,lindong28,2017-08-20T19:33:42Z,"@junrao Could you please provide some comment as to whether we should rename `AlterReplicaDirRequest` to `AlterReplicaLogDirRequest`?

@becketqin prefer `AlterReplicaLogDirRequest` so that it is more explicit that we are referring to log directory instead of arbitrary directory.

On the other hand, I prefer `AlterReplicaDirRequest` because I think `AlterReplicaLogDirRequest` is a bit verbose. I also think it should be clear enough to user/developer that the dir means log dir as long as the field name in `AlterReplicaLogDirRequest` is `log_dir`."
323897171,3621,junrao,2017-08-22T01:52:31Z,I can take a look at the patch in the next day or two.
324254692,3621,lindong28,2017-08-23T08:09:46Z,@junrao Thanks much for taking time to review the patch! I have addressed all the comment. Can you take another look when you get time?
324383296,3621,cmccabe,2017-08-23T16:04:46Z,"Thanks for the patch.  It looks good overall.

I think we should split the changes to the Options class, to create a base class, into a separate JIRA.  It's not related to KIP-113.  I'm also not completely convinced we should do it, but perhaps we can discuss that in another JIRA or on the mailing list."
324399622,3621,lindong28,2017-08-23T17:04:07Z,"@cmccabe Thanks much for the comment! I personally don't mind whether we refactor the Options class in this patch or in a separate patch. I just talked to @becketqin and he prefers to do it in this patch if there is no concern. The argument is that this patch adds three more Option classes to Apache Kafka and it is reasonable to try to reduce the extra code in this patch.

Can you explain a bit more what is your concern with the change to the Option class? We can move it to another JIRA if there is anything worth further discussion about this change."
325093325,3621,lindong28,2017-08-26T06:28:00Z,"@junrao Thanks much for your review! I have addressed all comments except for the last one which we are discussing. And I have gone over the patch end-to-end, removed unused imports I can find and improved comments. The patch has been rebased onto trunk HEAD."
326144106,3621,lindong28,2017-08-30T23:09:09Z,"@junrao Thanks for taking time to review the patch!
@becketqin I have rebased patch onto trunk head and addressed all comments from Jun. Do you have time to take a look and commit the patch? Thanks!"
326774010,3621,lindong28,2017-09-02T23:05:01Z,"@becketqin Thanks much for taking time to review the patch again in detail! I have updated the patch to add comments and throw exception if an expected error is found.

Regarding the error code in DescribeLogDirResponse, it seems that there is currently no realistic issue with assuming CLUSTER_AUTHORIZATION_FAILED when the response is empty -- it doesn't affect correctness or performance. Then the reason for adding the response level error is purely ideological, which applies to other requests as well (e.g. DESCRIBE_GROUPS_RESPONSE or DESCRIBE_CONFIGS_RESPONSE). We have discussed this offline previously. It probably makes sense to have a response level error for every response instead of only DescribeLogDirResponse. Can I open a separate ticket and make a separate pull request for this?

I would like to avoid making non-trivial change to this patch at this moment so that it can be finished soon. I feel that the use of this extra field in DescribeLogDirResponse alone is kind of unnecessary and less useful. It probably makes more sense to add the response level error in `AbstractResponse` to solve a bigger problem. Or we can add this response level error to `DescribeLogDirResponse` in the future when it is needed. Doe this make sense?




"
326786682,3621,becketqin,2017-09-03T06:14:19Z,"@lindong28 Yeah, it is probably worth adding a response level error for all the requests. We can do it in separate KIP."
326787087,3621,becketqin,2017-09-03T06:26:51Z,"@lindong28 Thanks for the patch. Merged to trunk. 
Thanks a lot for the review, @junrao and @cmccabe "
341874911,3621,ijuma,2017-11-04T06:04:11Z,"I think this PR broke binary compatibility for the AdminClient, code compiled against 0.11.0 fails like:

```text
org.apache.kafka.clients.admin.DescribeClusterOptions.timeoutMs(Ljava/lang/Integer;)Lorg/apache/kafka/clients/admin/DescribeClusterOptions;
```"
341942416,3621,lindong28,2017-11-05T01:30:42Z,"@ijuma This patch added new methods in the interface AdminClient. According to https://wiki.eclipse.org/Evolving_Java-based_APIs_2, this should not cause binary incompatibility. Not sure why you see that error.

I also tried the following steps to hopefully reproduce the error:

- git clone -b upgrade-to-11.0.0 https://github.com/lindong28/kafka-monitor.git
- compile with ./gradlew jar
- Replace ./build/dependant-libs/kafka-clients-0.11.0.0.jar with kafka-clients-1.0.0-SNAPSHOT.jar, where kafka-clients-1.0.0-SNAPSHOT.jar is generated by Apache Kafka adefc8ea076354e.
- Run ./bin/kafka-monitor-start.sh config/kafka-monitor.properties

It appears that the project can still be compiled and run after I replaced the jar.

Can you tell me how I can reproduce this error? Also, do you know which change in this patch can break binary incompatibility, e.g. addition of new methods?"
341957639,3621,ijuma,2017-11-05T09:43:27Z,"@lindong28 Sorry, I thought it was obvious from the error message, but I noticed now that the person that ran into it only pasted the method affected and not the actual error.

The binary compatibility issue is due to the removal of the `timeoutMs` method from various classes. The method now exists in the abstract class, so code that is recompiled works (source compatible), but code that is not recompiled breaks with a `NoSuchMethodError`. We should reintroduce the removed methods in 1.0.1 and trunk to fix the issue. Would you mind filing a JIRA, please?"
342006765,3621,lindong28,2017-11-05T21:22:01Z,@ijuma Sure. I created https://issues.apache.org/jira/browse/KAFKA-6174. We can continue discussion there.
571192193,7898,dongjinleekr,2020-01-06T15:51:36Z,"Note:

- [How to retrieve all existing loggers in log4j2](https://stackoverflow.com/a/17848045)
- [How do I set a loggers level programmatically in log4j2](https://logging.apache.org/log4j/2.x/faq.html#reconfig_level_from_code)"
571371789,7898,ijuma,2020-01-07T00:11:21Z,This requires a KIP since the log4j2 config is not compatible with log4j.
571505061,7898,dongjinleekr,2020-01-07T09:23:37Z,@ijuma No problem. Thank you for your guidance. :smile: Stay tuned!
574137873,7898,rgoers,2020-01-14T11:49:39Z,Log4J 2.13.0 contains experimental support for some Log4J 1 Configuration flies. See http://logging.apache.org/log4j/2.x/manual/compatibility.html.
574187380,7898,ijuma,2020-01-14T14:00:43Z,"That's awesome, thanks for sharing."
589484883,7898,akamensky,2020-02-21T03:47:49Z,"I just had to deal with configuring filtered logs in Kafka and was shocked to find it uses log4j 1.2.17.

log4j v1 has been dead since 2015. 5 years ago it was known that it should not be used for any new designs and applications should migrate to v2, yet still Apache Kafka stuck with that?

Wanted to raise a ticket, but there is this PR, which does not seem to be getting merged anywhere... dear lord..."
589655512,7898,dongjinleekr,2020-02-21T13:36:01Z,"@akamensky I am sorry to hear that. I am almost done the KIP and will start the discussion next week. If you are interested in this issue, please join in. Have a nice weekend."
603000463,7898,OneCricketeer,2020-03-24T03:59:39Z,"Man, I really wish more Apache projects started this ""upgrade"""
603269642,7898,dongjinleekr,2020-03-24T14:25:34Z,"@cricket007 Sorry for the delay.

While working on this issue, I found that this upgrade is much more complicated than I first expected; It is related to lots of module dependencies, API changes, test code modification, and providing backward-compatibility for the logging configuration. Anyway, it is almost done. I successfully upgraded the whole project and now working with some race conditions on test suites.

I hope I can complete it in a couple of days.  @akamensky"
603402312,7898,OneCricketeer,2020-03-24T17:45:47Z,"No worries. I didn't realize the backwards compatible issues either. I was under the impression that slf4j bridges handled that. Personally, I've been using logback successfully for years "
604911816,7898,OneCricketeer,2020-03-27T09:53:05Z,"So, I'm actually working on a project that I just started... 

Can verify (part of) this log4j-slf4j + slf4j-log4j12 bridging definitely works without issue

https://github.com/cricket007/kafka-streams-jib-example/blob/feature/connect-distributed/pom.xml#L67-L91

Notice too

```xml
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <exclusions>
                <exclusion>
                    <groupId>log4j</groupId>
                    <artifactId>log4j</artifactId>
                </exclusion>
            </exclusions>
        </dependency>
```"
605995589,7898,dongjinleekr,2020-03-30T13:22:25Z,"Here is the WIP update. I almost completed the migration into log4j2 `2.13.1` but here are some issues:

1. I migrated all test methods into logj42 API, using the way Log4j2 itself does; However, I failed to migrate `StateManagerUtilTest`. It seems like this class is tightly coupled with Kafka Streams' slf4j logging mechanism, but I don't understand it yet.
2. The updated test suites working correctly when running individually, but if run at once (e.g., `./gradlew :streams:test`), some test suites go so flaky.

I am also tracking down the reason. If you can give me some advice, it will be a great help! :smiley:"
607126286,7898,tombentley,2020-04-01T08:59:42Z,"@dongjinleekr I hadn't realised you were working on this via KAFKA-9366. I was looking at it via KAFKA-1368. You've made more progress than me, so happy for you to take it forward. But there are a couple of issues I noticed in the course of my effort:

1. The `Log4jControllerMBean.getLoggers` returns a scala wrapper implementation of `java.util.List`, which means that JMX tools (e.g. `jvisualvm`) can't deserialize the list unless they have scala library on their classpath. That's easily fixed by returning a `java.util.ArrayList` copy of the list. 

2. I suspect you already know this, but with log4j there were loggers for things like `kafka.controller` because they appeared in the config file, even though a logger with that name was never created in the code. Because log4j2 separates loggers and logger configurations the call `logContext.getLoggers` only returns the loggers created in code. So AFAICS you won't be able to change the log level for `kafka.controller`, and would have to change the level of all descendant loggers individually. I guess this is a regression, since `Log4jController` is used for `AlterConfigs` RPC."
607328633,7898,dongjinleekr,2020-04-01T15:45:53Z,"Here is the update, with rebasing onto the latest trunk. Now all tests run properly. However, it still has a problem:

When I ran the test suites individually (e.g., `./gradlew :streams:test --tests org.apache.kafka.streams.KafkaStreamsTest`) all tests passes clearly. However, if I tried to run them in package-wide (e.g., `./gradlew :streams:test --tests org.apache.kafka.streams.internal.*`) or all-in-once (e.g., `./gradlew :streams:test`) the tests go so flaky and some tests fail, with one of the following error messages:

```
// Type 1: Can't find LIST appender.
java.lang.AssertionError: No ListAppender named LIST found.
  at org.apache.logging.log4j.junit.LoggerContextRule.getListAppender(LoggerContextRule.java:210)
  at org.apache.kafka.streams.state.internals.AbstractKeyValueStoreTest.before(AbstractKeyValueStoreTest.java:71)
  at sun.reflect.GeneratedMethodAccessor48.invoke(Unknown Source)
  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
  at java.lang.reflect.Method.invoke(Method.java:498)
```

```
// Type 2: the log message did not forwarded to the List appender.
java.lang.AssertionError: 
Expected: a collection containing ""INFO Opening store db-name in upgrade mode ""
     but: mismatches were: [was ""...""]
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
  at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:6)
  at org.apache.kafka.streams.state.internals.RocksDBTimestampedStoreTest.shouldMigrateDataFromDefaultToTimestampColumnFamily(RocksDBTimestampedStoreTest.java:139)
  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
```

To understand these errors, here is a background: To validate the logging messages, Kafka has been used a test Appender (i.e., `ListAppender` here) attached to the root logger. Although this PR updates log4j 1.x to 2.x, the overall approach has not changed.

This approach works well when running individually, but when running in batch, the test suite randomly fails to initialize the `ListAppender` (Type 1 error) or forward the log message to `ListAppender`(Type 2 error); I ran the tests more than a hundred of times but What I found was it is totally random and be affected by gradle's `maxParallelForks` parameter - it seems like there is a problem deep inside of log4j here.

I have been working on this issue for the last week could not find any perfect solution. If any of you have some ideas on this, it would be a great help. Thanks in advance. :smiley:"
607331030,7898,dongjinleekr,2020-04-01T15:49:50Z,@tombentley Thanks for your valuable comments - absoultely it will be a great help! I am now applying the comments from @cricket007 so as soon as it is completed I will review your comments again and leave a feedback. :smile: 
607512708,7898,OneCricketeer,2020-04-01T22:04:13Z,"I've written test appenders in other projects just to verify contents of log messages, but not really sure the implications of race conditions on that. "
612980927,7898,dongjinleekr,2020-04-13T16:42:38Z,"@cricket007 It seems like we need to consult to log4j mailing list. Okay, I will have a try."
613082686,7898,OneCricketeer,2020-04-13T20:30:09Z,e.g. https://www.dontpanicblog.co.uk/2018/01/15/test-log4j-with-junit/
628732206,7898,jeffhuang26,2020-05-14T16:04:06Z,What is timeline for merging this PR? 
636188471,7898,ijuma,2020-05-29T20:56:07Z,"@dongjinleekr Can please submit a KIP for this? We should have a better good idea of the compatibility implications by now, right?"
636191680,7898,ijuma,2020-05-29T21:04:48Z,"Oh, I had missed the comment about the errors we are seeing when running the tests. It may be worth upgrading to the latest release in case it has been fixed."
637684403,7898,dongjinleekr,2020-06-02T17:04:48Z,"All // Sorry for being late, I just got out from my last project; I will have a look at this PR this weekend."
648795743,7898,dongjinleekr,2020-06-24T12:41:51Z,"Here is the fix. I completed to implement all the features, migrating tests to follow log4j2 API, and rebasing onto the latest trunk, but there is a problem in logging message validation.

![gradle-fail](https://user-images.githubusercontent.com/2375128/85557994-4abae000-b663-11ea-8986-7b5dbbf8f80e.png)

When I run `:stream:test` task in my dev environment, the following 10 tests fail:

- `StreamsConfigTest`: `shouldLogWarningWhenPartitionGrouperIsUsed`
- `KStreamKTableJoinTest`: `shouldLogAndMeterWhenSkippingNullLeft[Key,Value]WithBuiltInMetricsVersion[Latest,0100To24]`
- `InMemorySessionStoreTest`: `shouldLogAndMeasureExpiredRecordsWithBuiltInMetricsVersion[Latest,0100To24]`, `shouldNotThrowInvalidRangeExceptionWithNegativeFromKey`
- `RocksDBTimestampedStoreTest`: `shouldMigrateDataFromDefaultToTimestampColumnFamily`, `shouldOpenNewStoreInRegularMode`

However, If I run the test suites `StreamsConfigTest`, `KStreamKTableJoinTest`, and `InMemorySessionStoreTest` individually, they work fine.

And if I run `RocksDBTimestampedStoreTest` test suite, it fails; the expected log message does not forwarded to the appender.

![2](https://user-images.githubusercontent.com/2375128/85558065-5c03ec80-b663-11ea-8411-9aadb9c6d29a.png)

In contrast, if I run the test methods individually, they also work fine:

![1](https://user-images.githubusercontent.com/2375128/85558106-64f4be00-b663-11ea-9a86-89e67a980757.png)

It seems like there is a problem with log4j in forwarding the log message to the appender. (Or is the appender closed before the log message arrives?) But I can't certain; I followed the way log4j2 test suites do, but could not find similar cases in their codebase.

I tried to fix this problem for several days but not succeeded. If you have some spare time, could you check out this PR and run the tests on your machine? I am working with Ubuntu 20.04 + OpenJDK 8. I am curious the same tests also fail in the other environments.

cc/ @ijuma @OneCricketeer @jeffhuang26"
649282794,7898,dongjinleekr,2020-06-25T07:02:08Z,Retest this please.
668034540,7898,dongjinleekr,2020-08-03T13:51:17Z,"Finally, it is finished! :congratulations: All features and tests are now successfully migrated into log4j2 API and passes clearly!

I changed the PR title and preparing KIP. Stay tuned! :smiley:

cc/ @ijuma @OneCricketeer @jeffhuang26 @tombentley @jpechane "
669229785,7898,dongjinleekr,2020-08-05T14:34:36Z,"Here is the KIP - [KIP-653: Upgrade log4j to log4j2](https://cwiki.apache.org/confluence/display/KAFKA/KIP-653%3A+Upgrade+log4j+to+log4j2)

@omkreddy Could you have a look? I have thought you must be the perfect reviewer [for this feature](https://github.com/omkreddy/log4j2-kafka-appender). :smile:"
673145549,7898,svudutala,2020-08-12T22:41:54Z,@dongjinleekr What is timeline for merging this PR and making this  upgrade available?
673305906,7898,dongjinleekr,2020-08-13T07:14:37Z,"@svudutala

> What is timeline for merging this PR and making this upgrade available?

It is what exactly I hope to ask the PMC members and committers. It seems like they are too busy right now - let us wait for a while.

+1. Rebased onto the lastest trunk."
691668375,7898,dongjinleekr,2020-09-13T12:56:45Z,"Rebased onto the lastest trunk, with including Sliding window support.

@ijuma Could you have a look?"
693414587,7898,dongjinleekr,2020-09-16T13:43:50Z,@hachikuji @chia7712 @abbccdda @omkreddy @tombentley Could you have a look? :pray:
693453914,7898,ijuma,2020-09-16T14:44:37Z,@dongjinleekr Has the KIP been approved?
693552944,7898,dongjinleekr,2020-09-16T17:32:51Z,@ijuma Of course not. But I hoped to reboot the discussion. (Thanks for the kind reponse! :+1:)
701008405,7898,dongjinleekr,2020-09-29T21:45:36Z,"Hi All,

Here is the update. All compatibility breaks caused by the root logger name change between log4j and log4j2 (`""root""`  `""""`) is now resolved. Plus, I also migrated raft module into log4j2.

cc/ @ijuma @tombentley"
733591265,7898,dongjinleekr,2020-11-25T09:41:01Z,"Here is the update. I rebased the PR onto the latest trunk, with:

1. Upgrade log4j to log4j2: This is the main commit.
2. Trivial improvements (typos, etc.)
3. WIP: Enable ignored tests in `PlaintextAdminIntegrationTest` ([KAFKA-8779](https://issues.apache.org/jira/browse/KAFKA-8779), cc/ @stanislavkozlovski)

It now includes @tombentley's [KIP-676](https://cwiki.apache.org/confluence/display/KAFKA/KIP-676%3A+Respect+logging+hierarchy).

cc/ @ijuma"
754531924,7898,GandophSmida,2021-01-05T09:55:08Z,The newest kafka version is 3.7.0.Could I know the reseason that change the log4j 1.x to log4j 2.x isn't merged.
776772272,7898,dongjinleekr,2021-02-10T15:08:37Z,"Rebased onto the latest trunk, with including [KIP-676](https://cwiki.apache.org/confluence/display/KAFKA/KIP-676%3A+Respect+logging+hierarchy) and [KAFKA-8779](https://issues.apache.org/jira/browse/KAFKA-8779) by @tombentley 

@ijuma @chia7712 @abbccdda @omkreddy Could you have a look? :pray: As you already know, [this KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-653%3A+Upgrade+log4j+to+log4j2) is approved.

@GandophSmida Sorry for being late. I am planning to provide a customized preview version for this feature."
777453160,7898,ch4rl353y,2021-02-11T13:22:57Z,"Regarding the CVE-2019-17571 from https://issues.apache.org/jira/browse/KAFKA-9366: is there another way to mitigite the risk?

we're looking for a temporary solution until this PR finally gets approved, but are not sure if and how the vulnerability could even get exploited. Any thoughts?"
777454845,7898,dongjinleekr,2021-02-11T13:25:18Z,"@ch4rl353y which version are you using? I am now working for a customized patch for 2.6.0 and 2.7.0, with docker image."
777461164,7898,ch4rl353y,2021-02-11T13:34:46Z,"@dongjinleekr we're using strimzi/kafka / 0.21.0-kafka-2.7.0

Our SCA scanning Tool (JFrog XRay) found this CVE among many others (speaking of third party lib CVEs only).

We're just wondering if there's a way (e.g. via message sanitizing or logging config adjustments, etc.) to be sure the mentioned CVE cannot be exploited."
777469267,7898,dongjinleekr,2021-02-11T13:47:08Z,"@ch4rl353y Great. :+1: As soon as I complete the custom release, I will have a look at strimzi docker image. I guess it will take at least one week."
777482442,7898,ch4rl353y,2021-02-11T14:05:37Z,alright @dongjinleekr thx for your fast response! we'll keep an eye on this PR :)
783322141,7898,priyavj08,2021-02-22T11:55:54Z,"> @dongjinleekr we're using strimzi/kafka / 0.21.0-kafka-2.7.0
> 
> Our SCA scanning Tool (JFrog XRay) found this CVE among many others (speaking of third party lib CVEs only).
> 
> We're just wondering if there's a way (e.g. via message sanitizing or logging config adjustments, etc.) to be sure the mentioned CVE cannot be exploited.

I have similar question, can this security vulnerability CVE-2019-17571 get exploited. I use Kafka operator from Banzaicloud  0.12.3/ kafka:2.13-2.6.0

when will the custom release be available?

thanks"
784202224,7898,dongjinleekr,2021-02-23T13:27:28Z,"Sorry for being late. Finally, here it is! I succeeded to backport this feature to the latest 2.7.0 release. (commit 466b798412)

- working branch: [kafka-2.7+log4j2](https://github.com/dongjinleekr/kafka/tree/kafka-2.7%2Blog4j2)
- custom build distribution: [kafka_2.13-2.7.0+log4j2-0.tgz](https://drive.google.com/file/d/1rdFfQsT9qIkpsaq3sQ3Jokrkef37jLtA/view?usp=sharing)
- 2.7.0 patch: [kafka-2.7.0+log4j2-0.patch](https://drive.google.com/file/d/1ar5-vfuip5fSYkh4fN6Pjo2dkzKtafm1/view?usp=sharing)

Now I will consult to strimzi, Banzaicloud team for a custom release. I am also preparing a GraalVM based docker image distribuion [here](https://hub.docker.com/r/dongjinleekr/kafka). Stay tuned! :smile:

cc/ @priyavj08 @ch4rl353y"
784901850,7898,dongjinleekr,2021-02-24T08:29:43Z,"Rebased onto the latest trunk, with minor log4j2 configuration corrections. If you already downloaded the patch & tarball above, please re-download it.

+1. I built a [docker image (dongjinleekr/kafka:2.13-2.7.0-log4j2-0)](https://hub.docker.com/r/dongjinleekr/kafka) and validated it in my minikube cluster. As you can see here, it now works like a charm with `log4j-slf4j-impl-2.14.0`! :smile: 
![20210224-155608](https://user-images.githubusercontent.com/2375128/108969410-0d609380-76c5-11eb-8791-a6fe4f365a36.png)"
786531132,7898,priyavj08,2021-02-26T09:40:20Z,thanks @dongjinleekr have you consulted Banzaicloud about this patch?
786645278,7898,dongjinleekr,2021-02-26T13:23:19Z,"@priyavj08 Sure. here is a guide from the Banzaicloud team. (see: [banzaicloud/kafka-operator#565](https://github.com/banzaicloud/kafka-operator/issues/565))

By replacing the docker image to [dongjinleekr/kafka:2.13-2.7.0-log4j2-0](https://hub.docker.com/r/dongjinleekr/kafka), you can make use of this feature. (Don't forget to specify `KAFKA_LOG4J_OPTS` to `""-Dlog4j.configurationFile=file:{kafka.home}/bin/../config/log4j2.properties""`)

```yaml
spec:
  headlessServiceEnabled: true
  zkAddresses:
    - ""zookeeper-client.zookeeper:2181""
  propagateLabels: false
  oneBrokerPerNode: false
  clusterImage: ""dongjinleekr/kafka:2.13-2.7.0-log4j2-0""
```"
788957476,7898,fouadsemaan,2021-03-02T14:42:25Z,"> > @dongjinleekr we're using strimzi/kafka / 0.21.0-kafka-2.7.0
> > Our SCA scanning Tool (JFrog XRay) found this CVE among many others (speaking of third party lib CVEs only).
> > We're just wondering if there's a way (e.g. via message sanitizing or logging config adjustments, etc.) to be sure the mentioned CVE cannot be exploited.
> 
> I have similar question, can this security vulnerability [CVE-2019-17571](https://github.com/advisories/GHSA-2qrg-x229-3v8q) get exploited. I use Kafka operator from Banzaicloud 0.12.3/ kafka:2.13-2.6.0
> 
> when will the custom release be available?
> 
> thanks

To  @priyavj08's question, is the vulnerability invoked by Kafka or does it lie dormant?"
789662902,7898,priyavj08,2021-03-03T11:58:11Z,"@dongjinleekr really appreciate your guidance here.  thanks for the patch. 

If I chose to not to move to this patch right away,  can you please confirm that this vulnerability in log4j (CVE-2019-17571) doesn't affect Kafka? 

thanks "
789735966,7898,dongjinleekr,2021-03-03T14:03:00Z,"@priyavj08 I'm sorry that I can't be certain. But as far as I know, any project with log4j 1.2.7 is not safe. (It is why I have been working on this issue.)

+1. I also released 2.6.1 backport.

- working branch: [kafka-2.6+log4j2](https://github.com/dongjinleekr/kafka/tree/kafka-2.6%2Blog4j2)
- custom build distribution: [kafka_2.13-2.6.1+log4j2-0.tgz](https://drive.google.com/file/d/1PfFaUj0UAN9CpfBN52BCBF-W0UNB3EWT/view?usp=sharing)
- 2.6.1 patch: [kafka-2.6.1+log4j2-0.patch](https://drive.google.com/file/d/1KQgRqYtLaL65lrHz4SiuSiZ7FOAQ4Xxv/view?usp=sharing)"
790427075,7898,priyavj08,2021-03-04T08:30:32Z,"@dongjinleekr when will this fix make it in to one of Kafka upstream release?

thanks"
790429327,7898,sofarsoghood,2021-03-04T08:34:15Z,"> @dongjinleekr really appreciate your guidance here. thanks for the patch.
> 
> If I chose to not to move to this patch right away, can you please confirm that this vulnerability in log4j ([CVE-2019-17571](https://github.com/advisories/GHSA-2qrg-x229-3v8q)) doesn't affect Kafka?
> 
> thanks

@priyavj08 we now checked Kafka's source code for any appearances of the SocketServer class or corresponding config files but were not able to find any. Furthermore we took a closer look at the listening ports inside the running containers. 

Conclusion: it looks like the affected SocketServer class is not used by Kafka."
790473929,7898,dongjinleekr,2021-03-04T09:35:20Z,"@priyavj08 Since this KIP is already passed, it will be included in 2.8.0 release."
800184688,7898,dongjinleekr,2021-03-16T11:37:16Z,"Rebased onto the latest trunk, with migrating the tests module into log4j2 and windows support."
829970019,7898,dongjinleekr,2021-04-30T09:32:55Z,Rebased onto the latest trunk.
868564730,7898,svudutala,2021-06-25T15:04:54Z,@dongjinleekr Thanks for all the effort on this feature. Do you have a target ETA on this to be merged?
869010612,7898,dongjinleekr,2021-06-26T14:23:00Z,"@svudutala Thanks for the interest in this feature. This feature will be included in the 3.0.0 Release. If you need this feature urgently, I am now backporting it to 2.7.1 and 2.8.0. Please refer [here](http://home.apache.org/~dongjin/post/apache-kafka-log4j2-support/)."
869010720,7898,dongjinleekr,2021-06-26T14:23:43Z,"@svudutala If you need the log4j2 appender, see [here](https://github.com/apache/kafka/pull/10244)."
876149123,7898,kkonstantine,2021-07-08T05:52:24Z,It's been a week since feature freeze for AK 3.0 and this PR is open (with several conflicts). Please let me know asap if this PR is not ready and KIP-653 needs to be removed from the 3.0 plan
876265899,7898,dongjinleekr,2021-07-08T09:02:58Z,Rebased onto the latest trunk. Could anyone review this PR? :bow: cc/ @kkonstantine
876773034,7898,emveee,2021-07-08T21:59:30Z,"> @priyavj08 I'm sorry that I can't be certain. But as far as I know, any project with log4j 1.2.7 is not safe. (It is why I have been working on this issue.)
> 
> +1. I also released 2.6.1 backport.
> 
> * working branch: [kafka-2.6+log4j2](https://github.com/dongjinleekr/kafka/tree/kafka-2.6%2Blog4j2)
> * custom build distribution: [kafka_2.13-2.6.1+log4j2-0.tgz](https://drive.google.com/file/d/1PfFaUj0UAN9CpfBN52BCBF-W0UNB3EWT/view?usp=sharing)
> * 2.6.1 patch: [kafka-2.6.1+log4j2-0.patch](https://drive.google.com/file/d/1KQgRqYtLaL65lrHz4SiuSiZ7FOAQ4Xxv/view?usp=sharing)

@dongjinleekr Do you have a patch that will work with 2.8.0?"
879046873,7898,dongjinleekr,2021-07-13T12:31:46Z,@emveee Thank you so much for being so interested. 2.8.0 backport preview will be released this week.
911500667,7898,ashishpatil09,2021-09-02T10:18:36Z,"Hi Guys
Is there any plan to release this fix soon?
Thanks
Ashish"
915363359,7898,dongjinleekr,2021-09-08T15:56:50Z,"@ashishpatil09 Many thanks for your interest in this feature. I think it will be released with AK 3.1, but I can't be certain yet. Please refer [here](http://home.apache.org/~dongjin/post/apache-kafka-log4j2-support/) if you need a preview or a custom patch for [2.6.1, 2.8.0].

cc/ @emveee @svudutala @priyavj08"
950328049,7898,dongjinleekr,2021-10-24T13:44:53Z,Rebased onto the latest trunk. @dajac Could you have a look when you are free?
974801081,7898,naanagon,2021-11-21T11:44:52Z,"@dongjinleekr  @dajac 

The reason for migration isn't CVE-2019-17571. This doesn't impact us (kafka don't use SocketServer).

Upgrading from log4j 1.2.17 to log4j 2 can be done because of log4j 2 features but not for this(CVE-2019-17571)

Can you clarify this..?"
976480514,7898,dongjinleekr,2021-11-23T12:47:02Z,"Hi @naanagon,

> Upgrading from log4j 1.2.17 to log4j 2 can be done because of log4j 2 features but not for this([CVE-2019-17571](https://github.com/advisories/GHSA-2qrg-x229-3v8q))

Agree. After reconsidering the issue, I concluded that [CVE-2019-17571](https://github.com/advisories/GHSA-2qrg-x229-3v8q) is rather a minor issue; It is only problematic only when the user tries to use the `SocketServer` appender.

I think the primary reason for migration is log4j2 features like syntax. (For example, using log4j 1.x syntax is confusing since it is already obsolete several years ago.)"
991150391,7898,soumiksamanta,2021-12-10T17:17:15Z,Will this PR solve [CVE-2021-44228](https://github.com/advisories/GHSA-jfh8-c2jp-5v3q)?
991209328,7898,svudutala-vmware,2021-12-10T18:44:16Z,"> Will this PR solve [CVE-2021-44228](https://github.com/advisories/GHSA-jfh8-c2jp-5v3q)?

@soumiksamanta https://github.com/apache/kafka/blob/bd3038383265f7bb850c09fe0a74a48c5c2e6f99/gradle/dependencies.gradle#L78 should be upgraded to 2.15.0.  log4j <= 2.14.0 all have this issue. 

Initially I thought log4j 1.x is not impacted but as per https://github.com/apache/logging-log4j2/pull/608#issuecomment-990494126 it is. "
991705948,7898,unverified-user,2021-12-11T16:40:16Z,"> > Will this PR solve [CVE-2021-44228](https://github.com/advisories/GHSA-jfh8-c2jp-5v3q)?
> 
> @soumiksamanta
> 
> https://github.com/apache/kafka/blob/bd3038383265f7bb850c09fe0a74a48c5c2e6f99/gradle/dependencies.gradle#L78
> 
> should be upgraded to 2.15.0. log4j <= 2.14.0 all have this issue.
> Initially I thought log4j 1.x is not impacted but as per [apache/logging-log4j2#608 (comment)](https://github.com/apache/logging-log4j2/pull/608#issuecomment-990494126) it is.

Thank you for sharing the comment. Isn't that comment for log4j v1 in general. kafka by default does not use JMS appender. Do you think it is impacted under the default configuration.

Also refer to this post: https://lists.apache.org/thread/lgbtvvmy68p0059yoyn9qxzosdmx4jdv"
991894590,7898,vinayakshukre,2021-12-12T13:02:16Z,"@dongjinleekr  you must be already aware about the new log4j zero day vulnerability with log4j 2.14.1 versions and below that. Hope when you will be done with this merge, you will take it to 2.15.0 and then make it available in the final release. Thanks for your efforts. "
992556542,7898,svudutala-vmware,2021-12-13T14:50:50Z,"> > > Will this PR solve [CVE-2021-44228](https://github.com/advisories/GHSA-jfh8-c2jp-5v3q)?
> > 
> > 
> > @soumiksamanta
> > https://github.com/apache/kafka/blob/bd3038383265f7bb850c09fe0a74a48c5c2e6f99/gradle/dependencies.gradle#L78
> > 
> > should be upgraded to 2.15.0. log4j <= 2.14.0 all have this issue.
> > Initially I thought log4j 1.x is not impacted but as per [apache/logging-log4j2#608 (comment)](https://github.com/apache/logging-log4j2/pull/608#issuecomment-990494126) it is.
> 
> Thank you for sharing the comment. Isn't that comment for log4j v1 in general. kafka by default does not use JMS appender. Do you think it is impacted under the default configuration.
> 
> Also refer to this post: https://lists.apache.org/thread/lgbtvvmy68p0059yoyn9qxzosdmx4jdv

Yeah @unverified-user . My understanding is same too. This should not impact unless there is use of JMS."
994740102,7898,dongjinleekr,2021-12-15T12:21:55Z,"Hi All,

Sorry for being late. Here is the update! This PR is rebased onto the latest trunk and now uses log4j2 2.16.0 to address [CVE-2021-44228](https://nvd.nist.gov/vuln/detail/CVE-2021-44228) and [CVE-2021-45046](https://nvd.nist.gov/vuln/detail/CVE-2021-45046).

According to [this analysis](https://github.com/apache/logging-log4j2/pull/608#issuecomment-990494126), log4j 1.x is [affected by this vulnerability](https://nvd.nist.gov/vuln/detail/CVE-2021-4104) but, only when it uses JMS appender. So, unless you are using the JMS appender, you are safe from this vulnerability."
995250657,7898,dhruvp-8,2021-12-15T22:06:09Z,"> Hi All,
> 
> Sorry for being late. Here is the update! This PR is rebased onto the latest trunk and now uses log4j2 2.16.0 to address [CVE-2021-44228](https://nvd.nist.gov/vuln/detail/CVE-2021-44228) and [CVE-2021-45046](https://nvd.nist.gov/vuln/detail/CVE-2021-45046).
> 
> According to [this analysis](https://github.com/apache/logging-log4j2/pull/608#issuecomment-990494126), log4j 1.x is [affected by this vulnerability](https://nvd.nist.gov/vuln/detail/CVE-2021-4104) but, only when it uses JMS appender. So, unless you are using the JMS appender, you are safe from this vulnerability.

@dongjinleekr It would be great if you can provide this change as part of your preview releases for 2.7.1 and 2.8.0"
997166170,7898,dongjinleekr,2021-12-18T08:01:14Z,"All // You can find a preview of Apache Kafka 3.0.0 w/ log4j2 2.16.0 [here](http://home.apache.org/~dongjin/post/apache-kafka-log4j2-support/).

@dhruvp-8 I have no plan for 2.7.0 or 2.8.0 but, currently working on 2.8.1 now."
1005938463,7898,gabrieljones,2022-01-05T17:43:33Z,How hard is it to backport this all the way back to Kafka v2.3.x?
1006282789,7898,dongjinleekr,2022-01-06T04:50:11Z,"@gabrieljones

> How hard is it to backport this all the way back to Kafka v2.3.x?

Almost impossible. There are too many conflicts."
1006913721,7898,vbsiegem,2022-01-06T20:30:16Z,"Are there any firm plans at this time for when log4j v2 support in Kafka will be released, and in what release(s)?"
1006920498,7898,diegoazevedo,2022-01-06T20:42:01Z,"Just for information, the use of an obsolete version of log4j is now seen as a critical vulnerability for some security scanners. We received this ""Critical Warning"" from Tenable Nessus on our Kafka Cluster:

```
A logging library running on the remote host is no longer supported.

According to its self-reported version number, the installation of Apache Log4j on the remote host is no longer supported. Log4j reached its end of life prior to 2016.

Lack of support implies that no new security patches for the product will be released by the vendor. As a result, it is likely to contain security vulnerabilities.
```"
1007242660,7898,dongjinleekr,2022-01-07T09:00:52Z,"@vbsiegem @diegoazevedo

> Are there any firm plans at this time for when log4j v2 support in Kafka will be released, and in what release(s)?

I hope AK 3.2.0, but can't certain yet."
1007353055,7898,vbsiegem,2022-01-07T12:00:37Z,"> @vbsiegem @diegoazevedo
> 
> > Are there any firm plans at this time for when log4j v2 support in Kafka will be released, and in what release(s)?
> 
> I hope AK 3.2.0, but can't certain yet.

@dongjinleekr When is AK 3.2.0 targeted to be released?
As @diegoazevedo indicated, lack of support for log4j2 is increasingly seen as a critical vulnerability, and some corporations using AK are urgently looking for a resolution."
1007366645,7898,dongjinleekr,2022-01-07T12:23:29Z,"@vbsiegem I can't certain since I am not a committer. But If someone urgently needs a log4j2 based version, there is [a preview build based on AK 3.0](http://home.apache.org/~dongjin/post/apache-kafka-log4j2-support/). A 3.1 based one will be also released as soon as the official AK 3.1 is released."
1009197651,7898,vlsi,2022-01-10T18:16:17Z,"I might be slightly late for the party, however, have you explored the possibility of resurrecting and releasing 1.x?

I do think it is worth resurrecting 1.x and fix the critical issues.
Here are the relevant threads:
* dev@logging (my initial proposal): https://lists.apache.org/thread/tc6twt82f51c093km8r3qxthgj9lls1r
* general@incubator (after Logging PMC suggested re-incubating 1.x): https://lists.apache.org/thread/mx667xg7rps5b7rhgr7pojmj26o9qzq7 (<-- there are people who expressed interest in resurrecting 1.x)

If you are interested in log4j 1.x releases (e.g. review, test changes), it would be great if you could comment on general@incubator thread."
1009624788,7898,dongjinleekr,2022-01-11T06:01:14Z,"@vlsi

> Have you explored the possibility of resurrecting and releasing 1.x?

Not yet. KIP-653 has been started long before the log4j crisis of 2020."
1009629904,7898,vlsi,2022-01-11T06:11:59Z,"If the only motivation for moving 1.x -> 2.x is security, then it might be better for both Kafka and Kafka users to stick with 1.x and ask/wait/help with releasing a hardened 1.x.
Then the users won't need to learn new configuration/debugging/maintenance approaches of 2.x."
1010167705,7898,diegoazevedo,2022-01-11T16:58:42Z,"@vlsi the problem is that version 1.* is deprecated since 2015. 
Details: https://blogs.apache.org/foundation/entry/apache_logging_services_project_announces

The vulnerability reported by security scanners (like Tenable Nessus) is exactly that: the use of an ""unsupported Version"". So I think the movement for 2.* is correct."
1010208088,7898,vlsi,2022-01-11T17:44:20Z,"> problem is that version 1.* is deprecated since 2015

@diegoazevedo , let me explain: if there are people willing to support log4j 1.x, then it could be supported and maintained just fine.

https://reload4j.qos.ch/ is a fork by @ceki (the one who created log4j in the first place!)

As I highlighted above, there are individuals (including ASF committers like myself or even ASF members) who are willing to volunteer on supporting log4j 1.x. Currently, the question has not yet been decided by the ASF, however, I do not see how they can ""forbid"" maintaining 1.x provided the version is wildly used in the industry, there's a high demand on the fixes, and there are individuals to work on that.

Of course, if the ASF allows the volunteers to maintain 1.x, then reload4j will not be required. However, if the ASF blocks 1.x (for any reason), then reload4j might be way better for the consumers than migrating to 2.x or something else."
1011241406,7898,gabrieljones,2022-01-12T16:42:13Z,"confluent also has a fork of log4j 1
https://mvnrepository.com/artifact/io.confluent/confluent-log4j/1.2.17-cp6
The github repo seems to have disappeared though.
"
1014401948,7898,viktorsomogyi,2022-01-17T11:10:12Z,"@vlsi you can use other log libraries today as well. The other day I tried out logback runtime (on the broker side) and it works well. All you need is to include logback (or I guess reload4j for that matter) jars runtime and configure it up. I think what we should concentrate on is to be independent from logging frameworks. Kafka has two problems: KafkaLog4JAppender and Log4jController/LoggingResource. The other stuff is mostly testing related to which I say it doesn't really matter, those aren't exposed to production environment anyway. The appender can be replaced by a similar implementation in log4j2 and the controller could be transformed to a pluggable implementation so users won't lose functionality (right know if you switch to logback or something else you won't be able to manipulate log levels dynamically).

As the strongly dependent functionality I think isn't critical and users are mostly free to choose implementation today, I also think it's a good idea to upgrade to log4j2 as a new default. However I think this discussion should be had on the KIP discussion thread, please let us concentrate here on the code review itself."
1021440975,7898,mimaison,2022-01-25T17:36:14Z,"@dongjinleekr Thanks for your work.
I hope to start reviewing the log4j2 PRs later this week. Do you recommend starting with this one or with https://github.com/apache/kafka/pull/10244?"
1023168840,7898,dongjinleekr,2022-01-27T12:44:29Z,"@mimaison

> Do you recommend starting with this one or with #10244?

Many thanks for your effort. Sure, here it is; I rebased it against the latest trunk, also updating `LogCaptureContext` with @viktorsomogyi's comments. #10244 is also ready.

Resolving the conflicts, I also found a little inconsistency problem with `Admin` API (see `PlaintextAdminIntegrationTest#testIncrementalAlterConfigsForLog4jLogLevelsDoesNotWorkWithInvalidConfigs`) I will file a minor KIP addressing it."
1026731878,7898,Indupa,2022-02-01T11:14:20Z,"Hi @dongjinleekr ,

Can you please help me to build this latest patch you have released on top of Kafka_2.8,1. I tried using this following command to compile kafka source code always, but iam facing an issue to build this latest source code of  log4j2 patch .

The command i used is : gradle -PscalaVersion=2.13 releaseTarGz -x signArchives

1 . The Error iam getting is :   

                Build file 'C:\kafka-2.8.1-log4j2\build.gradle' line: 1339

* What went wrong:
A problem occurred evaluating root project 'kafka-2.8.1-log4j2(patch by github)'.
> Cannot convert a null value to an object of type Dependency.
  The following types/formats are supported:
    - Instances of Dependency.
    - String or CharSequence values, for example 'org.gradle:gradle-core:1.0'.
    - Maps, for example [group: 'org.gradle', name: 'gradle-core', version: '1.0'].
    - FileCollections, for example files('some.jar', 'someOther.jar').
    - Projects, for example project(':some:project:path').
    - ClassPathNotation, for example gradleApi().

  Comprehensive documentation on dependency notations is available in DSL reference for DependencyHandler type.

* Try:
Run with --stacktrace option to get the stack trace. Run with --info or --debug option to get more log output. Run with --scan to get full insights.

* Get more help at https://help.gradle.org

Deprecated Gradle features were used in this build, making it incompatible with Gradle 7.0.
Use '--warning-mode all' to show the individual deprecation warnings.
See https://docs.gradle.org/6.8.1/userguide/command_line_interface.html#sec:command_line_warnings


2 . When i tried commenting a line ""testCompile libs.mockitoJunitJupiter // supports MockitoExtension "" at line 1339 , Iam getting following Error.

C:\checkouts\kafka_2.8.1>gradle jar

> Configure project :
Building project 'core' with Scala version 2.13.5

FAILURE: Build failed with an exception.

* Where:
Build file 'C:\checkouts\kafka_2.8.1\build.gradle' line: 1362

* What went wrong:
A problem occurred evaluating root project 'kafka_2.8.1'.
> Project with path ':trogdor' could not be found in root project 'kafka_2.8.1'.
"
1027702595,7898,dongjinleekr,2022-02-02T08:40:33Z,@Indupa I'm sorry. There was a mistake rebasing onto 2.8.1. You can see the updated patch with built tarball [here](https://github.com/dongjinleekr/kafka/releases/tag/2.8.1%2Blog4j2).
1027741356,7898,Indupa,2022-02-02T09:28:16Z,"Thank you so much @dongjinleekr . Let me try to apply a patch and build.Will update you
"
1028142767,7898,Indupa,2022-02-02T16:52:41Z,"Hi @dongjinleekr ,I could able to build latest patch and also need one input from you.

Is All dependencies of log4j 1.x is completely Removed in this Patch............?, I could see,still dependency on log4j_1.2.17 in build.gradle and dependency.gradle.Also there are dependency on log4j.properties and tools-log4j.properties instead of log4j2.properties and tools-log4j2.properties in some of the files.Is it still require or we can remove those dependencies as well.............?.

The things I tried from my end is as follows,

1. I tried updating build.gradle and dependency.gradle by removing the dependency of log4j.
2. Also,i tried updating some of the files,where you have added echo statement to update log4j.properties into log4j2.properties in those places where u have mentioned in that patch file by removing log4j.properties and connect-log4j.properties and tools-log4j.properties file.
3. After that,i compiled the code and extracted  folder under ""C:\kafka_2.8.1\core\build\distributions\kafka_2.13-2.8.1\kafka_2.13-2.8.1"" and named it as kafka.zip file and using in our component by installing and run it as kafka Service.
4.But when i tried running kafka,iam getting following exception.

 2022-02-02 05:57:17.158  [INF] [Kafka] Connecting to localhost:2181
2022-02-02 05:57:27.571  [INF] [Kafka] WATCHER::
2022-02-02 05:57:27.571  [INF] [Kafka] WatchedEvent state:SyncConnected type:None path:null
2022-02-02 05:57:27.574  [INF] [Kafka] []
2022-02-02 05:58:17.227  [ERR] [Kafka] ERROR StatusLogger Reconfiguration failed: No configuration found for '764c12b6' at 'null' in 'null'
2022-02-02 05:58:17.684  [INF] [Kafka] DEPRECATED: using log4j 1.x configuration. To use log4j 2.x configuration, run with: 'set KAFKA_LOG4J_OPTS=-Dlog4j.configurationFile=file:C:\kafka/config/tools-log4j2.properties'

To brief about my requirement is , Currently the kafka package we using,contains some of the patches which we have added on top of kafka_2.8.1 source code.In which one the custom change we have made is,we are using apache-log4j-extras 1.2.17 with timebased triggering policy for rolling log files as it is not  available in log4j.1.2.17. Since this version has vulnerability ,we wanted to use that log4j2 api for this rolling policy logic which is working in your patch.

 Can you please help me on this...............?"
1028778134,7898,dongjinleekr,2022-02-03T09:26:52Z,"Hi @Indupa,

1. Sure, log4j 1.x is removed entirely. It is still defined in `build.gradle` for `log4j-appender`, deprecated with KIP-719, but not included in the classpath. (see below)
![20220203-173421](https://user-images.githubusercontent.com/2375128/152315500-8ce8706c-56f4-4e08-a372-5249dd69c773.png)

2. As you can see here, this patch includes both of log4j 1.x and 2.x properties files. But it runs with 1.x properties file by default for backward compatibility. To use log4j2 properties, set `KAFKA_LOG4J_OPTS` like the logging message:

```
To use log4j 2.x configuration, run with: 'set KAFKA_LOG4J_OPTS=-Dlog4j.configurationFile=file:C:\kafka/config/tools-log4j2.properties'
```

You can use included `config/log4j2.properties` for `TimeBasedTriggeringPolicy`. Please update it following your use case."
1028865528,7898,Indupa,2022-02-03T10:58:23Z,"Hi @dongjinleekr , yeah I already have made changes to  use log4j2 properties, set KAFKA_LOG4J_OPTS. But after that as well, as I sent in my previous comment, iam getting this following exception.

2022-02-02 05:57:17.158 [INF] [Kafka] Connecting to localhost:2181
2022-02-02 05:57:27.571 [INF] [Kafka] WATCHER::
2022-02-02 05:57:27.571 [INF] [Kafka] WatchedEvent state:SyncConnected type:None path:null
2022-02-02 05:57:27.574 [INF] [Kafka] []
2022-02-02 05:58:17.227 [ERR] [Kafka] ERROR StatusLogger Reconfiguration failed: No configuration found for '764c12b6' at 'null' in 'null'.

But still iam getting this exception,Kafka is not running.Can you help me with What is the cause of this exception......................?
"
1028870380,7898,dongjinleekr,2022-02-03T11:04:26Z,"Hi @Indupa,

Please refer [here](https://stackoverflow.com/questions/66545546/error-statuslogger-reconfiguration-failed-no-configuration-found-for-73d16e93)."
1028876857,7898,Indupa,2022-02-03T11:12:32Z,"Hi @dongjinleekr , Yeah I refered this article earlier,but didn't get to know the soluton what they are suggesting like Dlog4j.configuration in VM Arguments.I have not seen where we are using this VM arguments.

 How i can overcome from this issue................?"
1028885839,7898,dongjinleekr,2022-02-03T11:23:50Z,@Indupa Which VM paramater are you using? -Dlog4j.configuration or -Dlog4j.configuration**File**?
1028965316,7898,dongjinleekr,2022-02-03T12:59:05Z,@Indupa Please refer [the documentation](https://cwiki.apache.org/confluence/display/KAFKA/KIP-653%3A+Upgrade+log4j+to+log4j2) and the log messages.
1028979162,7898,Indupa,2022-02-03T13:14:15Z,"Sure @dongjinleekr .Thank you so much for sharing documentation ,Sorry I was not aware that -Dlog4j.configuration is VM parameter its pointing to in this  ,so was confused with that Comment. I got it now.let me through the changes what i have made to use log4j2 properties by setting KAFKA_LOG4J_OPTS in detail,whether i have missed to update it any of the file and Will update you.Thank you"
1047472293,7898,Indupa,2022-02-22T06:38:46Z,"Hi @dongjinleekr , Can you please provide me Some information on the Vulenerabiities regarding log4j features , Is this following vulnerable features of log4j are using in Kafka 2.8.1 for any of the kafka related things...................?

CVE-2019-17571 is a high severity issue targeting the
SocketServer. Log4j includes a SocketServer that accepts serialized
log events and deserializes them without verifying whether the
objects are allowed or not. This can provide an attack vector that
can be expoited.
=> **is log4j's socketServer used in kafka.......?**

CVE-2020-9488 is a moderate severity issue with the
SMTPAppender. Improper validation of certificate with host mismatch
in Apache Log4j SMTP appender. This could allow an SMTPS connection
to be intercepted by a man-in-the-middle attack which could leak any
log messages sent through that appender.
**=> is log4j's SMTPAppender is used in kafka..........?**

CVE-2022-23302 is a high severity deserialization vulnerability in
JMSSink. JMSSink uses JNDI in an unprotected manner allowing any
application using the JMSSink to be vulnerable if it is configured
to reference an untrusted site or if the site referenced can be
accesseed by the attacker. For example, the attacker can cause
remote code execution by manipulating the data in the LDAP store.
**=> Is is log4j's JMSSink is used in kafka..............?**


CVE-2022-23305 is a high serverity SQL injection flaw in
JDBCAppender that allows the data being logged to modify the
behavior of the component. By design, the JDBCAppender in Log4j
1.2.x accepts an SQL statement as a configuration parameter where
the values to be inserted are converters from PatternLayout. The
message converter, %m, is likely to always be included. This allows
attackers to manipulate the SQL by entering crafted strings into
input fields or headers of an application that are logged allowing
unintended SQL queries to be executed.
**=> Is is log4j's JDBCAppender is used in kafka.....................?**

Can you please help me and provide me the info on this....................?
"
1047473190,7898,dongjinleekr,2022-02-22T06:40:51Z,"@Indupa As long as the user explicitly configure the loggers to use them, they are not used yet."
1047496080,7898,Indupa,2022-02-22T07:22:42Z,"Ohhh OKk, In that case none of the 4 vulnerable things ,used in kafka as of now right.....................?"
1047501444,7898,Indupa,2022-02-22T07:30:40Z,"Currently in kafka we have not used Socketserver,SMTPAppender,JDBCAppender and JMSSink features from log4j,unless user explicitly use it in their Custom changes.................? 

Can you please confirm on this...................?"
1047522217,7898,rgoers,2022-02-22T08:03:15Z,"@Indupa Please note that the Apache Logging Services project continues to receive security vulnerability reports against Log4j 1.x. It is not typical to file CVE's against an EOL'd project. We recently did, however as we were made aware that a fork of Log4j 1 claimed to have fixed all the security issues. We may file more but Log4j 1 is not a high priority so I cannot say when more might be forthcoming. In addition to the security issues there are several serious bugs that will never be fixed.
"
1047536617,7898,ceki,2022-02-22T08:22:42Z,"@rgoers  If you receive a CVE against log4j 1.x, how difficult is it to forward to the reload4j project? More specifically, are you aware of a single CVE against log4j 1.x that was not fixed in reload4j?  As for the ""other serious bugs that will never be fixed"" that is a bold claim which will not stand the test of time.

It is unbecoming for an OSS project leader to engage in this sort of FUD. Frankly, it is embarrassing to watch. For the sake of your own credibility, please stop."
1062401482,7898,infa-rbliznet,2022-03-08T23:47:42Z,"Is there any progress on it? According to this: https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.2.0 , KIP should be completed by March 2 and Feature freeze is on March 16. "
1072452078,7898,dongjinleekr,2022-03-18T14:14:10Z,"@showuon Here is the update, rebasing onto the latest trunk. :bow: "
1073801293,7898,edoardocomar,2022-03-21T11:51:09Z,"Hi @dongjinleekr would you consider this patch to fix the compilation error 

```
diff --git streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
index 2bfbe4e36b..3cbb5c6369 100644
--- streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
+++ streams/src/test/java/org/apache/kafka/streams/StreamsConfigTest.java
@@ -1039,7 +1039,9 @@ public class StreamsConfigTest {
     @Test
     public void shouldSpecifyRocksdbWhenNotExplicitlyAddedToConfigs() {
         final String expectedDefaultStoreType = StreamsConfig.ROCKS_DB;
-        final String actualDefaultStoreType = streamsConfig.getString(DEFAULT_DSL_STORE_CONFIG);
+        props.put(DEFAULT_DSL_STORE_CONFIG, expectedDefaultStoreType);
+        final StreamsConfig config = new StreamsConfig(props);
+        final String actualDefaultStoreType = config.getString(DEFAULT_DSL_STORE_CONFIG);
         assertEquals(""default.dsl.store should be \""rocksDB\"""", expectedDefaultStoreType, actualDefaultStoreType);
     }

```"
1076403226,7898,dongjinleekr,2022-03-23T13:53:53Z,Rebased onto the latest trunk. cc/ @edoardocomar
1105464108,7898,dhruvp-8,2022-04-21T16:50:44Z,@dongjinleekr Thanks for working on this PR. Is there a timeline on when will this be merged? As per this doc https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+3.2.0 is it pushed to Kafka 3.3?
1113974839,7898,dongjinleekr,2022-04-30T11:50:22Z,"Rebased onto the latest trunk.

@dhruvp-8 Sorry for being late. For compatibility reasons, the adoption of this PR is postponed to 4.0. in 3.x, the reloadlog4j will be used instead. You can find out the custom build, patch with log4j2 [here](https://home.apache.org/~dongjin/post/apache-kafka-log4j2-support/) - I released the AK 3.1.0 based one this week and working on [3.2.0 based one](https://github.com/dongjinleekr/kafka/tree/preview/3.2.0+log4j2) now."
1321549707,7898,Indupa,2022-11-21T07:07:08Z,"Hi @dongjinleekr , 

we ran into the issue to run zookeepr and kafka , when we tried to use reload4j instead of log4j in kafka-2.8.1 Package. please find below for more details and could you please help us on how to resolve this issue............?

Issue : Currently we are using Kafka-2.8.1 in which it has log4j vulnerabilities reported .
Fix : So we tried to use reload4j-1.2.22 in kafka-2.8.1 to overcome all the vulnerabilities reported by log4j-1.2.1 and made the changes to point to reload4j instead of log4j as its done kafka 3.2.1 latest version
Below are the 2 files in kafka which we made changes to replace log4j with reload4j.
1. Build.gradle 
2. dependencies.gradle

After pointing to reload4j, its failing to run kafka and zookeeper with below errors.

**zookeeper.log** 
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:630)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.<clinit>(QuorumPeerMain.java:68)
log4j:ERROR Could not instantiate appender named ""kafkaAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.<clinit>(QuorumPeerMain.java:68)
log4j:ERROR Could not instantiate appender named ""requestAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.<clinit>(QuorumPeerMain.java:68)
log4j:ERROR Could not instantiate appender named ""authorizerAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.<clinit>(QuorumPeerMain.java:68)
log4j:ERROR Could not instantiate appender named ""controllerAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.<clinit>(QuorumPeerMain.java:68)
log4j:ERROR Could not instantiate appender named ""requestAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.<clinit>(QuorumPeerMain.java:68)
log4j:ERROR Could not instantiate appender named ""cleanerAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
        at org.apache.zookeeper.server.quorum.QuorumPeerMain.<clinit>(QuorumPeerMain.java:68)
log4j:ERROR Could not instantiate appender named ""stateChangeAppender"".

**kafka.log**

log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:630)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:516)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)
        at kafka.utils.Logging.$init$(Logging.scala:47)
        at kafka.Kafka$.<init>(Kafka.scala:30)
        at kafka.Kafka$.<clinit>(Kafka.scala)
        at kafka.Kafka.main(Kafka.scala)
log4j:ERROR Could not instantiate appender named ""kafkaAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)
        at kafka.utils.Logging.$init$(Logging.scala:47)
        at kafka.Kafka$.<init>(Kafka.scala:30)
        at kafka.Kafka$.<clinit>(Kafka.scala)
        at kafka.Kafka.main(Kafka.scala)
log4j:ERROR Could not instantiate appender named ""requestAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)
        at kafka.utils.Logging.$init$(Logging.scala:47)
        at kafka.Kafka$.<init>(Kafka.scala:30)
        at kafka.Kafka$.<clinit>(Kafka.scala)
        at kafka.Kafka.main(Kafka.scala)
log4j:ERROR Could not instantiate appender named ""authorizerAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)
        at kafka.utils.Logging.$init$(Logging.scala:47)
        at kafka.Kafka$.<init>(Kafka.scala:30)
        at kafka.Kafka$.<clinit>(Kafka.scala)
        at kafka.Kafka.main(Kafka.scala)
log4j:ERROR Could not instantiate appender named ""controllerAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)
        at kafka.utils.Logging.$init$(Logging.scala:47)
        at kafka.Kafka$.<init>(Kafka.scala:30)
        at kafka.Kafka$.<clinit>(Kafka.scala)
        at kafka.Kafka.main(Kafka.scala)
log4j:ERROR Could not instantiate appender named ""requestAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)
        at kafka.utils.Logging.$init$(Logging.scala:47)
        at kafka.Kafka$.<init>(Kafka.scala:30)
        at kafka.Kafka$.<clinit>(Kafka.scala)
        at kafka.Kafka.main(Kafka.scala)
log4j:ERROR Could not instantiate appender named ""cleanerAppender"".
log4j:ERROR Could not instantiate class [org.apache.log4j.rolling.RollingFileAppender].
java.lang.ClassNotFoundException: org.apache.log4j.rolling.RollingFileAppender
        at java.net.URLClassLoader.findClass(URLClassLoader.java:381)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:264)
        at org.apache.log4j.helpers.Loader.loadClass(Loader.java:190)
        at org.apache.log4j.helpers.OptionConverter.instantiateByClassName(OptionConverter.java:304)
        at org.apache.log4j.helpers.OptionConverter.instantiateByKey(OptionConverter.java:123)
        at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:755)
        at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:738)
        at org.apache.log4j.PropertyConfigurator.parseCatsAndRenderers(PropertyConfigurator.java:652)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:518)
        at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:577)
        at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:504)
        at org.apache.log4j.LogManager.<clinit>(LogManager.java:119)
        at org.slf4j.impl.Reload4jLoggerFactory.<init>(Reload4jLoggerFactory.java:67)
        at org.slf4j.impl.StaticLoggerBinder.<init>(StaticLoggerBinder.java:72)
        at org.slf4j.impl.StaticLoggerBinder.<clinit>(StaticLoggerBinder.java:45)
        at org.slf4j.LoggerFactory.bind(LoggerFactory.java:150)
        at org.slf4j.LoggerFactory.performInitialization(LoggerFactory.java:124)
        at org.slf4j.LoggerFactory.getILoggerFactory(LoggerFactory.java:417)
        at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:362)
        at com.typesafe.scalalogging.Logger$.apply(Logger.scala:48)
        at kafka.utils.Log4jControllerRegistration$.<init>(Logging.scala:25)
        at kafka.utils.Log4jControllerRegistration$.<clinit>(Logging.scala)
        at kafka.utils.Logging.$init$(Logging.scala:47)
        at kafka.Kafka$.<init>(Kafka.scala:30)
        at kafka.Kafka$.<clinit>(Kafka.scala)
        at kafka.Kafka.main(Kafka.scala)
log4j:ERROR Could not instantiate appender named ""stateChangeAppender"".

could you please help us if we made the changes to those 2 gradle files , is it enought to make kafka-2.8.1 to work using reload4j............?




"
1321685023,7898,ceki,2022-11-21T08:29:11Z,@Indupa  The correct fully qualified class name for `RollingFileAppender` is `org.apache.log4j.RollingFileAppender` and not `org.apache.log4j.rolling.RollingFileAppender`.
2386057956,7898,mimaison,2024-10-01T14:00:49Z,"@dongjinleekr We're now accepting changes for 4.0 in trunk. Do you think you can rebase this PR? If not, let us know so someone can complete this work. Thanks."
2441591375,7898,ppkarwasz,2024-10-28T13:27:46Z,"@mimaison,

What is the ETA for 4.0? I might be able to look at it mid-November."
2442079763,7898,mimaison,2024-10-28T16:33:49Z,We're actively working on it in https://github.com/apache/kafka/pull/17373. Hopefully that will be complete for 4.0.0.
2480964449,7898,dongjinleekr,2024-11-17T06:51:55Z,"@mimaison @ppkarwasz @ceki

Sorry for being late. I have been very busy nowadays and just got some free time by this end of the year.

@frankvicky

Thanks for taking the issue. I will join the PR and follow up for the review & improvements. Let's close this PR and continue on #17373! :smiley:"
668325203,9039,mjsax,2020-08-04T01:32:00Z,Retest this please.
679394512,9039,ableegoldman,2020-08-24T22:17:06Z,test this please
683916587,9039,ableegoldman,2020-08-31T17:22:20Z,test this please
683917229,9039,ableegoldman,2020-08-31T17:23:31Z,Retest this please
684074223,9039,vvcephei,2020-08-31T22:24:03Z,"Since Jenkins PR builds are still not functioning, I've merged in trunk and verified this pull request locally before merging it."
2262311490,16456,chirag-wadhwa5,2024-08-01T08:05:38Z,"Hi @junrao . Thanks a lot for the review. I have made the required changes in the last commit. I have also left some replies to your comments, it will be really helpful if you could leave your suggestions there."
2265855133,16456,chirag-wadhwa5,2024-08-02T17:38:18Z,"Hi @junrao, thanks a lot for the review. I have made the changes suggested by you. Also, I have replied to some of your comments and have left those conversations unresolved above. Could you pls take a look at those and let me know if anything else is required. Thanks a lot !

P.S. - I am looking into the test failure"
2269577431,16456,chirag-wadhwa5,2024-08-05T17:37:58Z,"Hi @junrao , looks like the sharePartitionManager wasn't being closed on the broker shutdown. I have pushed in a new commit with the changes, it should work now. Is there any other remaining gap in the PR ? If not can I get an approval on it ? Thanks a lot for all the suggestions !"
2270479029,16456,chirag-wadhwa5,2024-08-06T06:25:17Z,"@junrao : Looks like the tests are passing now, a couple of them fail probably because they are flaky"
1726880993,14364,philipnee,2023-09-20T03:50:17Z,"@dajac @lianetm - Made some changes based on the comments, but obviously broke some existing tests."
1728333915,14364,lianetm,2023-09-20T19:48:17Z,"Hey @philipnee, thanks for the changes! I completed another pass to all the non-test files. Agree with the move of the error handling to the HB manager but left a few other comments. 

I will wait for the fixes to the failing tests and then I will go over the test files. Thanks!"
1728878443,14364,philipnee,2023-09-21T05:29:39Z,"@lianetm @kirktrue @dajac - Much thanks for spending time reviewing the PR, I tried to address most of the comments in the PR. I'll check back with the unit test results - as I've only run them locally.

Let me know if you have more comments to follow up."
1731476330,14364,lianetm,2023-09-22T14:01:17Z,"Hey @philipnee , I see several updates here, thanks! let me know when you want to me give it another pass (there are still some test failures)"
1734529330,14364,philipnee,2023-09-25T22:05:51Z,Hey @lianetm - I made some updates based on your last comments. Let me know your thoughts!
1735694640,14364,lianetm,2023-09-26T14:46:05Z,"Thanks for the changes @philipnee, left a few other minor comments and questions but LGTM. 

As I see it, the main areas requiring follow-up in other PRs would be:

- fully integrate with the state defined in the membershipManager (getting rid of all the parallel `groupState` defined here)
- integrate with the assignment processing component, driving the logic to delegate callback execution and send HB on completion as required.
- extend HB manager test to cover successful path and timeout scenarios.

@dajac it would be helpful if you can take another look at it now, as it has evolved quite a bit. Thanks!"
1743351780,14364,philipnee,2023-10-02T16:38:04Z,Hello @dajac - Thanks for the review. I hope I've addressed most of your concerns in the recent reviews. Thanks!
1745567545,14364,philipnee,2023-10-03T19:11:46Z,"@dajac @lianetm Thanks for the feedback. I addressed more of your comments. I wanted to point out that I filed 3 follow-up tickets to close some of the gaps. These are:
- Propagate time during failure to avoid time.milliseconds(): https://issues.apache.org/jira/browse/KAFKA-15534
- Ensure some of the fields are only sent once: https://issues.apache.org/jira/browse/KAFKA-15533
- Ensure coordinator node is removed on disconnection: https://issues.apache.org/jira/browse/KAFKA-15531

I believe all of the open comments are addressed/replied, so let me know if there's anything else."
1746844528,14364,dajac,2023-10-04T13:07:47Z,"> @dajac @lianetm Thanks for the feedback. I addressed more of your comments. I wanted to point out that I filed 3 follow-up tickets to close some of the gaps. These are:
> 
> * Propagate time during failure to avoid time.milliseconds(): https://issues.apache.org/jira/browse/KAFKA-15534
> * Ensure some of the fields are only sent once: https://issues.apache.org/jira/browse/KAFKA-15533
> * Ensure coordinator node is removed on disconnection: https://issues.apache.org/jira/browse/KAFKA-15531
> 
> I believe all of the open comments are addressed/replied, so let me know if there's anything else.

Thanks. We also need to handle the consumer close case and send the final heartbeat."
1747674288,14364,philipnee,2023-10-04T21:34:07Z,"Thanks @dajac - I refactored some tests based on your comments. Thanks a lot for putting time into it.  Here I've got a list of Jira for the follow-ups:
- Propagate time during failure to avoid time.milliseconds(): https://issues.apache.org/jira/browse/KAFKA-15534
- Ensure some of the fields are only sent once: https://issues.apache.org/jira/browse/KAFKA-15533
- Ensure coordinator node is removed on disconnection: https://issues.apache.org/jira/browse/KAFKA-15531
- Send Heartbeat on closing the consumer as part of handling close(): https://issues.apache.org/jira/browse/KAFKA-15548
"
1751532067,14364,philipnee,2023-10-07T00:44:01Z,"JDK11 build failed with `Command ""git reset --hard"" returned status code 128:` , for the rest here is the list of failing tests:
```
Build / JDK 21 and Scala 2.13 / testMultiWorkerRestartOnlyConnector  org.apache.kafka.connect.integration.ConnectorRestartApiIntegrationTest
2m 26s
Build / JDK 21 and Scala 2.13 / testAuthentications(String).quorum=zk  kafka.api.SaslScramSslEndToEndAuthorizationTest
6s
Build / JDK 21 and Scala 2.13 / testDynamicIpConnectionRateQuota()  kafka.network.DynamicConnectionQuotaTest
45s
Build / JDK 21 and Scala 2.13 / testDescribeClusterRequestIncludingClusterAuthorizedOperations(String).quorum=kraft  kafka.server.DescribeClusterRequestTest
4s
Build / JDK 21 and Scala 2.13 / testTimeouts()  org.apache.kafka.controller.QuorumControllerTest
<1s
Build / JDK 21 and Scala 2.13 / shouldHaveSamePositionBoundActiveAndStandBy  org.apache.kafka.streams.integration.ConsistencyVectorIntegrationTest
21s
Build / JDK 17 and Scala 2.13 / shouldAddAndRemoveNamedTopologiesBeforeStartingAndRouteQueriesToCorrectTopology()  org.apache.kafka.streams.integration.NamedTopologyIntegrationTest
1m 10s
```

"
1751592366,14364,philipnee,2023-10-07T04:09:25Z,There's some issue with the jdk11 build - retriggering the tests don't seem to work. So I opened a [DRAFT PR](https://github.com/apache/kafka/pull/14509) to run the test.  It seems like that's the only way to pass the build. 
1752310508,14364,philipnee,2023-10-09T03:51:32Z,"@dajac - Not entirely sure what is the best way to fix the jdk11 build.  The rest of the builds seem to be fine with the following failures:

```
Build / JDK 21 and Scala 2.13 / randomClusterPerturbationsShouldConverge[enableRackAwareTaskAssignor=true]  org.apache.kafka.streams.processor.internals.assignment.TaskAssignorConvergenceTest
5s
Build / JDK 21 and Scala 2.13 / testTaskRequestWithOldStartMsGetsUpdated()  org.apache.kafka.trogdor.coordinator.CoordinatorTest
2m 0s
Build / JDK 17 and Scala 2.13 / shouldHaveSamePositionBoundActiveAndStandBy  org.apache.kafka.streams.integration.ConsistencyVectorIntegrationTest
12s
Build / JDK 17 and Scala 2.13 / shouldHaveSamePositionBoundActiveAndStandBy  org.apache.kafka.streams.integration.ConsistencyVectorIntegrationTest
8s
Build / JDK 17 and Scala 2.13 / shouldHonorEOSWhenUsingCachingAndStandbyReplicas  org.apache.kafka.streams.integration.StandbyTaskEOSMultiRebalanceIntegrationTest
2m 7s
Build / JDK 8 and Scala 2.12 / testMultiWorkerRestartOnlyConnector  org.apache.kafka.connect.integration.ConnectorRestartApiIntegrationTest
2m 24s
Build / JDK 8 and Scala 2.12 / testAbortTransactionTimeout(String).quorum=kraft  org.apache.kafka.tiered.storage.integration.TransactionsWithTieredStoreTest
```

However, I did open a draft PR from this branch and jdk11 was able to complete. "
1753238153,14364,dajac,2023-10-09T15:35:04Z,"If we combine the last two builds, I am confident that the changes are good so I will merge it to trunk."
2394094049,17373,frankvicky,2024-10-04T16:43:26Z,This is the initial version. I'd like to run it on CI first.
2395277491,17373,frankvicky,2024-10-06T03:32:10Z,"Hello @mumrah 
Thanks for your feedback. Unfortunately, I barely missed the KIP for some reason, but I'll take a look and adjust the PR accordingly.  "
2430811748,17373,showuon,2024-10-23T03:53:53Z,"> About zk stuff, I'm think about removing the zk configurations might be better handled as a follow-up PR. WDYT?

Sounds good to me. But if we decided to remove them later, please open a JIRA ticket for them. Thanks."
2430832465,17373,frankvicky,2024-10-23T04:02:51Z,"> > About zk stuff, I'm think about removing the zk configurations might be better handled as a follow-up PR. WDYT?
> 
> Sounds good to me. But if we decided to remove them later, please open a JIRA ticket for them. Thanks.

https://issues.apache.org/jira/browse/KAFKA-17858

I have filed a jira for it.  
"
2434890030,17373,chia7712,2024-10-24T10:24:44Z,"@frankvicky please fix the conflicts, thanks!"
2453429889,17373,frankvicky,2024-11-03T13:33:14Z,"There are lots of tests fail come out after merging `trunk`.  
I will take a look......"
2454941038,17373,frankvicky,2024-11-04T14:59:20Z,"Hello everyone,

I am having some trouble debugging the new failures.   
The root cause seems to be that these failing tests are unable to capture logs correctly (resulting in empty content), which leads to assertion failures. 
It appears that these issues are caused by #17615, although these test cases worked fine when using log4j1.

Any feedback or information would be greatly appreciated. 
Thank you!
"
2462639102,17373,frankvicky,2024-11-07T16:12:20Z,"Update: It seems that the root cause its because the log event could not be captured correctly. 
![Screenshot from 2024-11-07 23-18-43](https://github.com/user-attachments/assets/73ef0fe7-55e9-48ee-a0ae-ac7cfdae4bc7)
![Screenshot from 2024-11-07 23-40-15](https://github.com/user-attachments/assets/33b7efb1-9fb6-4b16-8843-046176d3c14c)
"
2472887781,17373,frankvicky,2024-11-13T08:57:45Z,"Hello @mimaison,@showuon,

Since this PR modifies a large number of files, particularly `build.gradle`, its highly susceptible to conflicts with other PRs, making it rather exhausting to resolve these conflicts frequently. 
It would be helpful if we could merge this PR into trunk sooner, as there arent any outstanding issues or points of contention with it. This would also allow us to begin addressing any follow-up issues.

Many thanks."
2478290150,17373,showuon,2024-11-15T09:06:17Z,"@mimaison @mumrah @ppkarwasz , do you have any other comments? I'll merge it tomorrow if no other comments. Thanks."
2480966246,17373,dongjinleekr,2024-11-17T06:55:17Z,"@mimaison @frankvicky 

FYI: I just moved from #7898 and Reviewing the differences between our 3.6.X in-house fork. It was already applied to our implementation and is actively running now :) Let me have a look!"
2485719308,17373,dongjinleekr,2024-11-19T13:29:51Z,"ALL //

To clearly state the reasoning for root logger's name, I just updated the [KIP document](https://cwiki.apache.org/confluence/display/KAFKA/KIP-653%3A+Upgrade+log4j+to+log4j2). Please have a look on ['Compatibility, Deprecation, and Migration Plan' section](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158870552#KIP653:Upgradelog4jtolog4j2-Compatibility,Deprecation,andMigrationPlan).

@frankvicky FEI CHANG GAN XIE :pray: (""Extremely thankful for your great help."") To keep a record of your effort taking over this huge issue, I added a new section mentioning [it is your implementation](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=158870552#KIP653:Upgradelog4jtolog4j2-Co-worknote).

@mimaison @showuon @chia7712 When you merge this PR, please don't omit mentioning my credit. :bowing_man:"
2485801878,17373,chia7712,2024-11-19T14:00:39Z,"> When you merge this PR, please don't omit mentioning my credit. 

will roger that :)"
2486208765,17373,mimaison,2024-11-19T16:36:59Z,"> When you merge this PR, please don't omit mentioning my credit. 

Yes you totally deserve being marked as a co-author. Thanks!"
2488600707,17373,mimaison,2024-11-20T13:32:52Z,"I'm having issues with the latest code (f3a68e1b9b).
```
$ bin/kafka-server-start.sh config/kraft/reconfig-server.properties
[0.002s][error][logging] Error opening log file '/Users/mickael/github/kafka/core/build/distributions/kafka_2.13-4.0.0-SNAPSHOT/bin/../logs/kafkaServer-gc.log': No such file or directory
[0.002s][error][logging] Initialization of output 'file=/Users/mickael/github/kafka/core/build/distributions/kafka_2.13-4.0.0-SNAPSHOT/bin/../logs/kafkaServer-gc.log' using options 'filecount=10,filesize=100M' failed.
Invalid -Xlog option '-Xlog:gc*:file=/Users/mickael/github/kafka/core/build/distributions/kafka_2.13-4.0.0-SNAPSHOT/bin/../logs/kafkaServer-gc.log:time,tags:filecount=10,filesize=100M', see error log for details.
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.
```

Running `trunk` the `logs` folder is automatically created. If I create the folder manually, then it seems to log at WARN level by default and I only get a single log line printed while my broker is running:
```
$ bin/kafka-server-start.sh config/kraft/reconfig-server.properties
[2024-11-20 14:33:39,425] WARN [QuorumController id=1] Performing controller activation. The metadata log appears to be empty. Appending 2 bootstrap record(s) in metadata transaction at metadata.version 4.0-IV0 from bootstrap source 'the binary bootstrap metadata file: /tmp/kraft-combined-logs/bootstrap.checkpoint'. Setting the ZK migration state to NONE since this is a de-novo KRaft cluster. (org.apache.kafka.controller.QuorumController)
```"
2503451783,17373,mimaison,2024-11-27T10:05:11Z,@frankvicky Can you rebase to fix the conflicts? Thanks
2504196849,17373,frankvicky,2024-11-27T15:42:21Z,"Hi @mimaison,

I have filed a JIRA for the issue with the Connect API:
https://issues.apache.org/jira/browse/KAFKA-18104

BTW, Im aware that this PR is expected to break some e2e tests, but so far, Ive only confirmed that `connect_test.py` is affected. 
Should we fix it in this PR, or should we file a separate JIRA to track it?
"
2504201143,17373,mimaison,2024-11-27T15:44:01Z,The Connect issue should be fixed in this PR. Adding new tests to verify the Connect REST API /admin/loggers endpoint can be done in another PR but not the bug fix.
2504207669,17373,mimaison,2024-11-27T15:46:45Z,"Apart if the changes to fix the system tests are very large, I'd rather do them in this PR as well. We typically don't merge PRs that we know will break tests."
2504369138,17373,mimaison,2024-11-27T16:59:32Z,"I just tested the Connect REST API with https://github.com/apache/kafka/pull/17373/commits/6d41bcd92758c7bc02c173b98dc4c53f88633ba0 and it worked fine.

You mentioned issues with the system tests. Have you run the full suite? Are there only issues with `connect_test.py`?"
2505169162,17373,frankvicky,2024-11-28T02:56:23Z,"Hi @mimaison,  
Unfortunately, I don't have the resources to perform the full test suite as it would take a very long time  
Instead, I selected a test related to this PR. The first one I picked is `connect_test.py`, but it failed.  

Here is the assertion failure for `test_file_source_and_sink.converter`:  
```
Expected [""foo"", ""bar"", ""baz"", ""razz"", ""ma"", ""tazz""]
Actual [""foo"", ""bar"", ""baz"", ""razz"", ""ma"", ""tazz"", ""01:39:47.507 [main] ERROR org.apache.kafka.tools.consumer.ConsoleConsumer - Error processing message, terminating consumer process:"", ""org.apache.kafka.common.errors.TimeoutException: null""]
```

As we can see, there are 2 exception logs at the end of the array that breaks the test. I am investigating but currently stuck.  
The rest of the test failures of `connect_test.py` also seem to be related to this `TimeoutException`."
2506901731,17373,showuon,2024-11-29T00:58:36Z,"@frankvicky , we ran system tests on connect related tests (tests/kafkatest/tests/connect) based on this PR, it failed quite a lot. [Here](https://gist.github.com/showuon/03b4586624ca6c7be95b8d4cc803d3af) is the result. Compared with the latest trunk, it only has 1 failure. So please help fix them. Thanks.
"
2507753807,17373,frankvicky,2024-11-29T12:48:03Z,"Since this issue is quite tricky for me, I went through a process of elimination to locate the bug.  
I found this line might be the root cause:  
https://github.com/apache/kafka/blob/064fe52b9ea66522b45d4f4af447ae4de43d71b2/tests/kafkatest/services/console_consumer.py#L164  

If we use `-Dlog4j2.configurationFile=file:`, it will lead to a `TimeoutException` in the console consumer. However, if we use `-Dlog4j.configuration=file:`, all tests in `connect_test.py` will pass.  

I'm still trying to figure it out."
2507855608,17373,mimaison,2024-11-29T13:47:16Z,"Thanks @frankvicky for investigating. I ran the system tests in our CI and confirm most the Connect tests are currently failing with your branch (I've not tried running other tests yet). I'll try to debug next week, if you can't find the issue. I'll also be able to run the tests in our environment once we have a fix."
2508123859,17373,chia7712,2024-11-29T16:34:13Z,"@frankvicky In the end-to-end tests, you must use the correct file extension when passing the log4j2 YAML configuration. If you use an incorrect extension - for example - the output of `LOG.error` called by the console consumer gets redirected to `console_consumer.stdout`. This is the root cause of the connection error as it see output which should not be existent.

As a straightforward solution, we can render the configuration file based on the specific node version.

```
        args['log4j_param'] = get_log4j_config_param(node)
        args['log4j_config'] = get_log4j_config_for_tools(node)

...

        # Create and upload log properties
        log_config = self.render(get_log4j_config_for_tools(node), log_file=ConsoleConsumer.LOG_FILE)
        node.account.create_file(get_log4j_config_for_tools(node), log_config)
```

noted that `LOG4J_CONFIG` can be removed as we don't use it anymore.

Additionally, you can apply this approach to all py files - such as `verifiable_producer.py`, `kafka.py`, `transactional_message_copier.py`, `consumer_performance.py`, `end_to_end_latency.py`, and `producer_performance.py`."
2508957854,17373,frankvicky,2024-11-30T13:10:23Z,"[Develocity](https://ge.apache.org/scans/tests?search.rootProjectNames=kafka&search.timeZoneId=Asia%2FTaipei&tests.container=org.apache.kafka.message.checker.MetadataSchemaCheckerToolTest&tests.test=testVerifyEvolutionGit()) 
`MetadataSchemaCheckerToolTest#testVerifyEvolutionGit` keep failing in CI, I think it's not related to this PR."
2508981008,17373,chia7712,2024-11-30T14:34:23Z,"> MetadataSchemaCheckerToolTest#testVerifyEvolutionGit keep failing in CI, I think it's not related to this PR.

the root cause is the test assumes the repo has ref ""refs/heads/trunk"" but it is not existent in PR, since our PR does not fetch that ref.

I have filed https://issues.apache.org/jira/browse/KAFKA-18128 to fix it
"
2510443802,17373,frankvicky,2024-12-02T02:38:30Z,Failed CI is handled by #17996 
2510857877,17373,chia7712,2024-12-02T08:19:41Z,@frankvicky could you please rebase code?
2511535886,17373,chia7712,2024-12-02T13:23:33Z,"connect e2e has another issue, and I have filed https://issues.apache.org/jira/browse/KAFKA-18132 to fix it"
2515161122,17373,chia7712,2024-12-03T17:23:48Z,this is another issue (https://issues.apache.org/jira/browse/KAFKA-18145) used to fix connect e2e
2518956077,17373,chia7712,2024-12-05T02:32:04Z,The last (maybe) Connect end-to-end issue is [KAFKA-18160](https://issues.apache.org/jira/browse/KAFKA-18160). We can revisit this PR after the existing Connect end-to-end issues are resolved.
2540936116,17373,mimaison,2024-12-13T09:07:33Z,@chia7712 Should we consider merging this and disabling the broken system test for now? WDYT?
2541679409,17373,chia7712,2024-12-13T15:17:20Z,"> Should we consider merging this and disabling the broken system test for now? WDYT?

Let me perform a final review later, and then I will merge it!"
2541895357,17373,chia7712,2024-12-13T17:15:18Z,@dongjinleekr I add you to the co-author. see https://github.com/apache/kafka/commit/b37b89c6686f5270b37d39cf3144bccb4f5bceb2
2542607479,17373,frankvicky,2024-12-14T01:07:41Z,"Thank you all for your patience in reviewing.
I truly appreciate all your help.  "
2543435727,17373,chia7712,2024-12-15T03:52:04Z,"I am currently testing this patch, and all related issues are tracked under [KAFKA-18161](https://issues.apache.org/jira/browse/KAFKA-18161). Please feel free to report any issues if I overlook any broken changes. :(

The known bugs caused by this PR are listed below.

1. `Logger` does not handle the compatibility correctly (see https://issues.apache.org/jira/browse/KAFKA-18243)
2. incorrect log4j2 config in e2e (see https://issues.apache.org/jira/browse/KAFKA-18247)"
2556096026,17373,jolshan,2024-12-20T01:44:08Z,"Hey -- not sure if there is something up with my setup, but after this PR, I see many tests failing due to kafka server not starting up (in the logs I see ` Port already in use: 9192; nested exception is: java.net.BindException: Address already in use`or because there are no process IDs (See https://github.com/apache/kafka/blob/e099fce567094b1143ded8516ffd748b5425667f/tests/kafkatest/services/kafka/kafka.py#L920) 

In both cases, it seems like something changed in startup -- perhaps some change in kafka.py here triggered the issue.

Did we run system tests after this change? I see some tests mentioned in KAFKA-18161, but not all. I'm seeing about 500 failures on trunk now (compared to 50 or so last week). https://confluent-open-source-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/trunk/2024-12-18--001.c0c25cd6-1ee3-4350-ae6d-e56e31779d50--1734569380--apache--trunk--0055ef0a49/report.html"
2556229381,17373,chia7712,2024-12-20T03:47:02Z,"@jolshan thanks for your report. the known issue caused by this PR is the ""logger"" of connect. And I run the one of test (`transactions_upgrade_test.py`) on my local:
```
[INFO:2024-12-19 19:38:00,219]: RunnerClient: Loading test {'directory': '/opt/kafka-dev/tests/kafkatest/tests/core', 'file_name': 'transactions_upgrade_test.py', 'cls_name': 'TransactionsUpgradeTest', 'method_name': 'test_transactions_upgrade', 'injected_args': {'from_kafka_version': '3.9.0', 'metadata_quorum': 'ISOLATED_KRAFT', 'use_new_coordinator': False, 'group_protocol': None}}
[INFO:2024-12-19 19:38:00,221]: RunnerClient: kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.9.0.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None: on run 1/1
[INFO:2024-12-19 19:38:00,221]: RunnerClient: kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.9.0.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None: Setting up...
[INFO:2024-12-19 19:38:00,222]: RunnerClient: kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.9.0.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None: Running...
[INFO:2024-12-19 19:39:50,549]: RunnerClient: kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.9.0.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None: Tearing down...
[INFO:2024-12-19 19:42:49,277]: RunnerClient: kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.9.0.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None: PASS
[INFO:2024-12-19 19:42:49,278]: RunnerClient: kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.9.0.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None: Data: None
================================================================================
SESSION REPORT (ALL TESTS)
ducktape version: 0.12.0
session_id:       2024-12-19--002
run time:         4 minutes 49.198 seconds
tests run:        1
passed:           1
flaky:            0
failed:           0
ignored:          0
================================================================================
test_id:    kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.9.0.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None
status:     PASS
run time:   4 minutes 49.056 seconds
--------------------------------------------------------------------------------
``` 

There is no `BindException`, and I will check other tests later."
2557032193,17373,ijuma,2024-12-20T13:35:45Z,I don't see any upgrade notes - did I miss something or did we forget to add that?
2557302557,17373,frankvicky,2024-12-20T16:16:09Z,I will open a PR adding upgrade notes.
2557311234,17373,chia7712,2024-12-20T16:21:41Z,"@frankvicky Could you please open minor to address following items?

1. move the deprecation warnings to `kafka-run-class` https://github.com/apache/kafka/pull/17373#discussion_r1893936536

2. remove unnecessary reference of `jacksonDatabindYaml` https://github.com/apache/kafka/pull/17373#discussion_r1894124425

3. add upgrade notes https://github.com/apache/kafka/pull/17373#issuecomment-2557032193"
2557355489,17373,jolshan,2024-12-20T16:37:36Z,"> @jolshan thanks for your report. the known issue caused by this PR is the ""logger"" of connect. And I run the one of test (transactions_upgrade_test.py) on my local:

Interesting. This one fails consistently on our infra (and succeeds without this change). I wonder if there is some different test configuration that causes the issue."
2557479743,17373,chia7712,2024-12-20T18:03:46Z,"There are many tests which are failed quickly. 
```
[DEBUG - 2024-12-18 22:36:58,718 - remoteaccount - _log - lineno:180]: ubuntu@worker47: Running ssh command: tail -c +1 /mnt/kafka/server-start-stdout-stderr.log | grep 'Kafka\s*Server.*started'
[DEBUG - 2024-12-18 22:36:58,974 - jmx - start_jmx_tool - lineno:56]: ubuntu@worker47: Not starting jmx tool because no jmx objects are defined
[DEBUG - 2024-12-18 22:36:58,974 - remoteaccount - _log - lineno:180]: ubuntu@worker47: Running ssh command: ps ax | grep -i kafka.Kafka | grep -v grep | awk '{print $1}'
[INFO  - 2024-12-18 22:36:58,983 - runner_client - log - lineno:459]: RunnerClient: kafkatest.tests.core.transactions_upgrade_test.TransactionsUpgradeTest.test_transactions_upgrade.from_kafka_version=3.1.2.metadata_quorum=ISOLATED_KRAFT.use_new_coordinator=False.group_protocol=None: Tearing down...
```
It can pass the check of `grep 'Kafka\s*Server.*started'` (kraft controller) but the check of process id fails. I open a MINOR to add more log (https://github.com/apache/kafka/pull/18286) @jolshan Could you please run the patch on your infra?"
2557576698,17373,jolshan,2024-12-20T19:10:54Z,"@chia7712 sure. I will run it and post the results
"
2557623349,17373,jolshan,2024-12-20T19:44:36Z,"Here's the result. https://confluent-open-source-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/trunk/2024-12-20--001.85d9ae90-e619-4689-a395-6d4075645b53--1734723292--chia7712--add_log_pid--acd9853691/report.html
"
2577813317,17373,trnguyencflt,2025-01-08T14:30:16Z,"This PR breaks downstream project that depends on kafka_2.13 and reload4j because there is clashing in class LoggingEvent, which exists in `reload4j` and `log4j-1.2-api` jars. The application will crash with this exception
```
Encountered fatal fault: caught exception
java.lang.IncompatibleClassChangeError: class org.apache.log4j.bridge.LogEventAdapter overrides final method org.apache.log4j.spi.LoggingEvent.getProperty(Ljava/lang/String;)Ljava/lang/String;
	at java.base/java.lang.ClassLoader.defineClass1(Native Method)
	at java.base/java.lang.ClassLoader.defineClass(ClassLoader.java:1027)
	at java.base/java.security.SecureClassLoader.defineClass(SecureClassLoader.java:150)
	at java.base/jdk.internal.loader.BuiltinClassLoader.defineClass(BuiltinClassLoader.java:862)
	at java.base/jdk.internal.loader.BuiltinClassLoader.findClassOnClassPathOrNull(BuiltinClassLoader.java:760)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClassOrNull(BuiltinClassLoader.java:681)
	at java.base/jdk.internal.loader.BuiltinClassLoader.loadClass(BuiltinClassLoader.java:639)
	at java.base/jdk.internal.loader.ClassLoaders$AppClassLoader.loadClass(ClassLoaders.java:188)
	at java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:526)
	at org.apache.log4j.builders.BuilderManager.<clinit>(BuilderManager.java:59)
	at org.apache.log4j.config.Log4j1Configuration.<init>(Log4j1Configuration.java:46)
	at org.apache.log4j.config.PropertiesConfiguration.<init>(PropertiesConfiguration.java:89)
	at org.apache.log4j.config.PropertiesConfigurationFactory.getConfiguration(PropertiesConfigurationFactory.java:58)
	at org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:499)
	at org.apache.logging.log4j.core.config.ConfigurationFactory$Factory.getConfiguration(ConfigurationFactory.java:404)
	at org.apache.logging.log4j.core.config.ConfigurationFactory.getConfiguration(ConfigurationFactory.java:318)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:690)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:711)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:253)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:137)
	at org.apache.commons.logging.impl.Log4jApiLogFactory$LogAdapter.getContext(Log4jApiLogFactory.java:161)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:47)
	at org.apache.commons.logging.impl.Log4jApiLogFactory.getInstance(Log4jApiLogFactory.java:210)
	at org.apache.commons.logging.impl.Log4jApiLogFactory.getInstance(Log4jApiLogFactory.java:205)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:987)
```"
2577875259,17373,ppkarwasz,2025-01-08T14:56:16Z,"> This PR breaks downstream project that depends on kafka_2.13 and reload4j because there is clashing in class LoggingEvent, which exists in `reload4j` and `log4j-1.2-api` jars.

You can not have both `reload4j` and `log4j-1.2-api` on your classpath, since they are both **replacements** of `log4j:log4j`."
2577897980,17373,ijuma,2025-01-08T15:05:14Z,"The point is that kafka was including `reload4j` before and hence many other projects aligned with that. It's one thing to cause logging to change, it's another to cause projects not to start anymore with a `IncompatibleClassChangeError` - that's a much bigger deal."
2578032374,17373,chia7712,2025-01-08T15:58:11Z,"> This PR breaks downstream project that depends on kafka_2.13 and reload4j

I assume the scenario you describe is to add ""kafka_2.13"" as dependency for the downstream project. The log4j2 are declared as ""implementation"" in core module, so downstream project should not ""include"" log4j2 in the dependencies automatically. not sure why `log4j-1.2-api` is included in the downstream project. @trnguyencflt Do you add `log4j-1.2-api` to your project manually? "
2578035232,17373,chia7712,2025-01-08T15:59:22Z,"@trnguyencflt thanks for your report. I'd like to reduce the gap of upgrading to kafka 4.0 as much as possible, so please share the details to me"
2580893706,17373,trnguyencflt,2025-01-09T17:35:30Z,"@chia7712 we include kafka_2.13 as dependency via maven https://github.com/confluentinc/kafka-rest/blob/d6405f714e0e453d6eb5b9a496faac4202c89cab/kafka-rest/pom.xml#L42, and it brings log4j-1.2-api as a runtime dependency (below is from mvn dependency:tree)

```
[INFO] |  +- org.apache.logging.log4j:log4j-1.2-api:jar:2.24.1:runtime
```"
2581051461,17373,chia7712,2025-01-09T19:03:24Z,"@trnguyencflt thanks for your response. `log4j-1.2-api` is used to convert log4j.properties at runtime, and hence maybe we can remove it from gradle runtime scope and then add it into distribution directly. With that change, `log4j-1.2-api` gets removed from published pom file but it remains in the distribution.

this is the pom file with above approach and there is not `log4j-1.2-api` 
```
chia7712@chia7712-ubuntu:~/.m2/repository/org/apache/kafka/kafka_2.13/4.1.0-SNAPSHOT$ cat kafka_2.13-4.1.0-SNAPSHOT.pom | grep log4j
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-slf4j-impl</artifactId>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-api</artifactId>
      <groupId>org.apache.logging.log4j</groupId>
      <artifactId>log4j-core</artifactId>
```

By contrast,  `log4j-1.2-api` is still included by distribution.
```
chia7712@chia7712-ubuntu:~/project/kafka/core/build/distributions/kafka_2.13-4.1.0-SNAPSHOT$ ls libs | grep log4j
log4j-1.2-api-2.24.1.jar
log4j-api-2.24.1.jar
log4j-core-2.24.1.jar
log4j-slf4j-impl-2.24.1.jar

```"
2581610985,17373,frankvicky,2025-01-10T02:21:07Z,"Hi @trnguyencflt 
Thanks for report.
I have filed KAFKA-18466 to track this issue.
I will filed a PR for this issue soon."
2586347959,17373,chia7712,2025-01-13T07:07:40Z,"@trnguyencflt we have resolved the issue in #18472. If you have some free time, could you please test it on your project?
Please don't hesitate to provide any feedback if you think something has been overlooked. Thank you!"
1508030768,13561,showuon,2023-04-14T07:07:32Z,"`KAFKA-14888: RemoteLogManager - deleting expired/size breached log segments to remote storage implementation` was created for this task. I've updated the PR title, FYI"
1555964010,13561,satishd,2023-05-20T18:08:18Z,The proposed approach with the changes introduced in this PR avoids inconsistency issues as mentioned [here](https://github.com/apache/kafka/pull/13561#discussion_r1181526976) and also avoids remote segment leaks in unclean leader election scenarios.
1571615955,13561,kamalcph,2023-06-01T08:44:40Z,"@satishd 

Will you open a separate PR to delete the active segment once it breaches the retention time? Or, will handle it in this patch."
1571973707,13561,satishd,2023-06-01T12:36:11Z,"> @satishd
> 
> Will you open a separate PR to delete the active segment once it breaches the retention time? Or, will handle it in this patch.

Planned to have it in a followup PR, filed https://issues.apache.org/jira/browse/KAFKA-15047 

Just to clarify , we need to roll the active segment incase remote storage is enabled and eligible for retention cleanup so that this segment can be copied by the remote storage subsystem and eventually picked up for retention cleanup. "
1573234538,13561,satishd,2023-06-02T06:43:46Z,Thanks @junrao for your review comments. Addressed them inline and/or with the latest commits. 
1573968832,13561,divijvaidya,2023-06-02T16:02:20Z,"Hey @satishd 
Are you planning to address the open comments such as https://github.com/apache/kafka/pull/13561/files#r1166767890 before I do another pass of code review?"
1575514622,13561,satishd,2023-06-04T10:44:20Z,@divijvaidya I will let you know once I address the remaining few comments in the next couple of days.
1576564925,13561,satishd,2023-06-05T10:51:48Z,"Pulled the latest trunk, resolved the conflicts, and pushed the changes."
1596907836,13561,showuon,2023-06-19T10:11:58Z,"@satishd , any update for this PR? If you don't have time on it, just let me know. :)"
1597034800,13561,satishd,2023-06-19T11:43:17Z,"> @satishd , any update for this PR? If you don't have time on it, just let me know. :)

@showuon addressing the review comments in progress, needs minor refactoring which is going on. Will have those changes pushed in the next couple of days. "
1597103576,13561,satishd,2023-06-19T12:30:50Z,"Rebased with the trunk as this PR had conflicts because of other introduced changes in the trunk.

"
1659937748,13561,showuon,2023-08-01T09:32:37Z,"@satishd , is this PR ready for another round of review?"
1665486770,13561,satishd,2023-08-04T11:52:22Z,Thanks @showuon for the review. Addressed them with the latest commits.
1665489584,13561,satishd,2023-08-04T11:53:48Z,@junrao Thanks for the review. Addressed your comments inline or with the latest commits.
1670963444,13561,satishd,2023-08-09T09:14:37Z,Thanks @divijvaidya for the review. Addressed your comments inline or with the latest commits. 
1672759994,13561,satishd,2023-08-10T08:08:10Z,Thanks @junrao for the review. Addressed the review comments inline and/or with the latest commits.
1680370332,13561,satishd,2023-08-16T10:41:13Z,"Thanks @junrao for the review. Addressed them with inline comments and/or with the latest commits. 

>Also, do we have any test covering the newly introduced local retention logic?

Will add more UTs. We will have integration tests in a followup PR once https://github.com/apache/kafka/pull/14116 is merged."
1682651711,13561,satishd,2023-08-17T17:01:16Z,">Do you know why there are 100+ test failures?

@junrao Those failures(except one) are not related to this PR. Updated with a few minor changes and tests. The latest run had a few failures which seem to be unrelated to this change. "
1685049750,13561,junrao,2023-08-19T16:59:54Z,@satishd : The latest build still has 120 test failures. 
1685264422,13561,satishd,2023-08-20T11:52:57Z,">The latest build still has 120 test failures.

@junrao Those tests are not related to the changes in the PR. The next [run](https://ci-builds.apache.org/blue/organizations/jenkins/Kafka%2Fkafka-pr/detail/PR-13561/51/tests/) had 
 one related test failure(PlaintextAdminIntegrationTest.testOffsetsForTimesAfterDeleteRecords) which is fixed with the latest commit."
1692562057,13561,satishd,2023-08-24T23:56:20Z,"There are a few failures unrelated to this PR, merging it to trunk."
1692563258,13561,satishd,2023-08-24T23:58:13Z,Merging it to 3.6 branch
396640388,5201,bbejeck,2018-06-12T15:52:00Z,"@guozhangwang @mjsax @vvcephei 

Apologies for the massive PR but it seems unavoidable at this point.  More updates to come to this PR soon, but it's worth reviewing the graph object types and the overall physical plan generation"
396673545,5201,bbejeck,2018-06-12T17:38:59Z,"failure unrelated.

retest this please"
397827935,5201,bbejeck,2018-06-16T17:35:22Z,updated this
397829288,5201,bbejeck,2018-06-16T18:01:37Z,rebased from trunk
398142054,5201,guozhangwang,2018-06-18T17:59:18Z,@bbejeck Could you rebase this PR?
399158304,5201,bbejeck,2018-06-21T16:09:02Z,retest this please
399225979,5201,bbejeck,2018-06-21T20:05:24Z,"failure unrelated 

retest this please"
399778525,5201,bbejeck,2018-06-24T18:49:39Z,"Fixed issue with optimizing joins, updated integration test to include comparing results between optimized and non-optimized results and included a join in the test case."
401196303,5201,bbejeck,2018-06-28T22:50:13Z,">I cannot tell how this includes the source topic reuse logic, could you explain a bit?

In this PR we're still relying on the fix put into 2.0, in 4th PR we'll revert that change and include it along with the repartition optimization."
401196999,5201,bbejeck,2018-06-28T22:54:00Z,">+1, if we are not going to write / serialize the logical plan anywhere we do not need to do that.

required to keep find-bugs happy"
401198070,5201,bbejeck,2018-06-28T23:00:17Z,updated this
401200256,5201,guozhangwang,2018-06-28T23:12:35Z,"@bbejeck I made another pass over the updated PR, and left two follow-up comments as in https://github.com/apache/kafka/pull/5201#discussion_r199012496 and https://github.com/apache/kafka/pull/5201/files#r199014145."
401467483,5201,bbejeck,2018-06-29T20:37:13Z,retest this please
401469361,5201,bbejeck,2018-06-29T20:45:16Z,retest this please
401493756,5201,guozhangwang,2018-06-29T22:57:21Z,"Regarding the meta comment about `AbstractStream#addGraphNode`, now I understand the duplicate logic is because `KStreamAggregateProcessorBuilder` is not `AbstractStream` and hence cannot use this function.

My personal preference would then be, moving the `addGraphNode` from `AbstractStream` to `InternalStreamsBuilder`, with a small change on API to require passing in both the child and the parent node reference. In this case 1) beyond `AbstractStream` we can still call it, and 2) moving forward if we need to have more then one StreamGraphNode within an AbstractStream it would be more natural to do so."
405652620,5201,guozhangwang,2018-07-17T16:55:49Z,@bbejeck please let us when this PR is ready for review again.
408222437,5201,bbejeck,2018-07-26T20:18:53Z,"updated this.  Moved `addGraphNode` from `AbstractStream` to `InternalStreamsBuilder`.  Also removed the restriction of only building topology only once and added a unit test for it.

Also rebased with trunk.

Ready for reviews again"
408230707,5201,bbejeck,2018-07-26T20:49:59Z,"just remembered, need to update `toString` for individual graph nodes"
408895387,5201,bbejeck,2018-07-30T15:03:38Z,"updated this 

address last comments, rebased from trunk"
408977910,5201,bbejeck,2018-07-30T19:14:15Z,retest this please
408991717,5201,bbejeck,2018-07-30T20:03:05Z,updated to include clean-up from @vvcephei on MINOR cleanup PRs
409269000,5201,bbejeck,2018-07-31T15:46:08Z,"Kicked off system tests https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1879/

Also kicked off streams smoke tests https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1880/"
409429334,5201,bbejeck,2018-08-01T02:32:20Z,">I thought it was a little weird previously that StatefulProcessorNode extends StatelessProcessorNode, since subclassing is generally an ""is a"" relationship. So it was previously saying ""StatefuProcessorNode is a StatelessProcessorNode"", which is silly of course.
>Maybe we can call it ProcessorGraphNode to avoid a collision?

I like `ProcessorGraphNode` @guozhangwang @mjsax WDYT?"
409433491,5201,bbejeck,2018-08-01T02:58:42Z,@guozhangwang @vvcephei updated this and rebased from trunk
409443610,5201,guozhangwang,2018-08-01T04:14:04Z,"> I like ProcessorGraphNode @guozhangwang @mjsax WDYT?

Sounds good to me."
409580226,5201,bbejeck,2018-08-01T13:46:48Z,"failure unrelated.

retest this please"
409619948,5201,bbejeck,2018-08-01T15:40:41Z,updated to rename `StatelessProcessorNode` to `ProcessorGraphNode` from comments.
409639309,5201,guozhangwang,2018-08-01T16:38:57Z,"One more meta comment: we had a bunch of warnings due to lack of types for ProcessorParameters. This can be resolved by adding the following, e.g. in `KStreamImpl#filter()`:

```
final ProcessorParameters<? super K, ? super V> processorParameters = new ProcessorParameters<>(new KStreamFilter<>(predicate, true), name);


final ProcessorGraphNode<? super K, ? super V> filterNotProcessorNode = new ProcessorGraphNode<>(name,
                                                                                         processorParameters,
                                                                                         repartitionRequired);
```

I.e. we can still maintain the typing information here when constructing the logical node."
409676911,5201,bbejeck,2018-08-01T18:32:42Z,updated this 
409739132,5201,guozhangwang,2018-08-01T22:01:33Z,LGTM. Merging to trunk.
1557159854,13639,dajac,2023-05-22T12:47:55Z,@jeffkbkim @jolshan Thanks for your reviews. I think that I have addressed all your comments.
1562650336,13639,dajac,2023-05-25T10:16:58Z,"@jolshan @jeffkbkim Thanks for your comments. I have addressed them, I think."
378068897,4812,vvcephei,2018-04-02T22:39:17Z,Just starting on this; still need to add the missing measurements and actually run the tests.
378777667,4812,guozhangwang,2018-04-04T23:35:56Z,Jenkins failures are relevant to `Unused import - ..`.
378780973,4812,vvcephei,2018-04-04T23:56:20Z,"Ah, thanks, by the time I went to look at the last failure, the logs were already gone.
"
378791813,4812,vvcephei,2018-04-05T01:10:36Z,"I took the liberty of resolving all my ide warnings for the test files. Let me know if I went too far, and I can revert them."
378792028,4812,vvcephei,2018-04-05T01:12:00Z,"Checkstyle complained because I explicitly imported log4j, which is appropriate in this case. I isolated the usage to a ""testutils"" package, so I could allow the usage without allowing it for all of ""stream.processor.internals"".
"
379272773,4812,vvcephei,2018-04-06T14:35:55Z,"@guozhangwang I think you're right. For Streams, the thread level is the most global scope we have atm. I think what you're pointing out is that I've conflated the global scope with the thread scope. Ideally, these would be two separate scopes.

Let me refactor a bit more, and see what you think."
379318433,4812,guozhangwang,2018-04-06T17:16:04Z,"> I think what you're pointing out is that I've conflated the global scope with the thread scope. Ideally, these would be two separate scopes.

Actually I'm arguing that `StreamsMetrics` should only be some sort of a util class, that 1) wraps the actual `Metrics` object as the metrics registry, 2) provides util functions like `addXXSensor`, `addSensor` etc. And then `ThreadMetrics`, `TaskMetrics` etc exposes the API of their defined sensors, and in their corresponding impl class they keep a reference of the `StreamsMetrics` to register new metrics. We still need to note that, since we allow users to register their custom metrics via the `StreamsMetrics`, we need to expose `StreamsMetrics` in user-facing APIs, while for other built-in `XXMetrics` they are only used internally, while we can still expose their `xxSensor` functions for other internal classes to use."
379376338,4812,vvcephei,2018-04-06T20:58:28Z,"@guozhangwang and @bbejeck about that experimental commit, I've decided to ditch it and implement the KIP with minimal changes to the structure of the metrics.

I think I'd like to submit a KIP to alter the metric registration strategy we're employing later on, but I don't want to pollute KIP-274."
379401092,4812,vvcephei,2018-04-06T22:27:00Z,"I don't understand why these tests are failing. The message says:
```
00:03:46.733 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.12@2/streams/src/test/java/org/apache/kafka/streams/processor/internals/StreamTaskTest.java:804: error: incompatible types: boolean cannot be converted to StreamsConfig
00:03:46.733         task = createStatelessTask(true);
```

but line 804 in StreamTaskTest is:
```
task = createStatelessTask(createConfig());
```

Retest this, please."
379406708,4812,mjsax,2018-04-06T23:04:36Z,"I see
```
task = createStatelessTask(true);
```

(line 804) -- this makes sense, it should be `task = createStatelessTask(createConfig(true));`"
379412401,4812,vvcephei,2018-04-06T23:43:20Z,"Ah, it was because Jenkins (surprisingly) merges with trunk before testing.

Also, there was an undetected merge conflict, resulting in the broken code.

I've rebased and corrected it. Once the tests pass for me, I'll push again."
379905976,4812,vvcephei,2018-04-09T21:54:21Z,"@guozhangwang I'm currently working on adding metrics there. I'm also adding warning logs, as it's totally silent right now."
379907479,4812,vvcephei,2018-04-09T22:00:57Z,"@guozhangwang and regarding this:

> Thanks for the PR. Overall it looks reasonable to me. I have only minor detailed comments, plus the meta comment about follow-up refactoring and the docs changes: 1) we should update upgrade-guide to warn users their monitoring needs to be updated, 2) in monitoring we'd better update the skipped records to list possible reasons it will be recorded.

Ack. I might do that in a follow-up PR (under the same Jira/KIP) to keep the LOC in this PR lower."
379914936,4812,vvcephei,2018-04-09T22:35:54Z,"@mjsax @bbejeck @guozhangwang 

I've rebased and pushed the latest changes. I still need to add tests for the processors' metrics, but this change is otherwise pretty much where I want this PR to be.

Note that I rebased and put the change to `Sensor` at the beginning so that I can send that one change as a separate PR, if needed, for review by the client library people. This change is necessary, since the existing `maybeAddMetric` implementation in Streams is unsafe. To be done properly, it has to be synchronized, which my change does. My change also allows us to differentiate between registering a metric twice for a particular sensor (ok) and registering the same metric name on two different sensors (not ok).

Also, reminder that we should agree on a log style. I left `[]` in the PR as a placeholder. The discussion for this is above."
380241162,4812,vvcephei,2018-04-10T20:46:46Z,"Ok, @bbejeck @mjsax @guozhangwang ,

This PR is ready for another pass.

I have completed all code and tests. Docs will follow in another PR.

Please comment on:
* my strategy for sharing the skipped metric around the code base
* my changes to Sensor (and whether I need to send a separate PR for that change)
* the aforementioned log enclosing delimiter discussion

Thanks, all."
380251006,4812,guozhangwang,2018-04-10T21:22:51Z,retest this please
380859168,4812,vvcephei,2018-04-12T16:07:55Z,"Huh, that's a new one. It looks like (aside from the KafkaAdminTest continuing to flake out), the tests failed because the Jenkins worker ran out of disk! I'll wait until the last job completes before starting them again.

I've rebased this PR on trunk now that #4853 is merged. I still have a few nits to clean up. I'll notify again when I'm ready for final reviews."
380971785,4812,mjsax,2018-04-12T23:11:48Z,Meta comment: please update the PR title with the JIRA number
380976453,4812,vvcephei,2018-04-12T23:40:43Z,Sorry about that @mjsax 
381155079,4812,vvcephei,2018-04-13T14:34:50Z,"The tests passed. The failure was a rate-limit exception publishing coverage to github:
```
01:57:52.862 ERROR: Step ?Publish coverage to GitHub? aborted due to exception: 
01:57:52.862 java.io.IOException: Exceeded rate limit for repository
01:57:52.864 	at com.github.terma.jenkins.githubprcoveragestatus.GitHubPullRequestRepository.getGitHubRepository(GitHubPullRequestRepository.java:46)
01:57:52.864 Caused: java.io.IOException: Error while accessing rate limit API
01:57:52.864 	at com.github.terma.jenkins.githubprcoveragestatus.GitHubPullRequestRepository.getGitHubRepository(GitHubPullRequestRepository.java:51)
01:57:52.864 	at com.github.terma.jenkins.githubprcoveragestatus.CompareCoverageAction.perform(CompareCoverageAction.java:98)
01:57:52.864 	at hudson.tasks.BuildStepCompatibilityLayer.perform(BuildStepCompatibilityLayer.java:81)
01:57:52.864 	at hudson.tasks.BuildStepMonitor$1.perform(BuildStepMonitor.java:20)
01:57:52.864 	at hudson.model.AbstractBuild$AbstractBuildExecution.perform(AbstractBuild.java:744)
01:57:52.864 	at hudson.model.AbstractBuild$AbstractBuildExecution.performAllBuildSteps(AbstractBuild.java:690)
01:57:52.864 	at hudson.model.Build$BuildExecution.post2(Build.java:186)
01:57:52.864 	at hudson.model.AbstractBuild$AbstractBuildExecution.post(AbstractBuild.java:635)
01:57:52.864 	at hudson.model.Run.execute(Run.java:1749)
01:57:52.864 	at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:43)
01:57:52.864 	at hudson.model.ResourceController.execute(ResourceController.java:97)
01:57:52.864 	at hudson.model.Executor.run(Executor.java:429)
01:57:52.931 Setting GRADLE_4_4_HOME=/home/jenkins/tools/gradle/4.4
01:57:54.209 Setting GRADLE_4_4_HOME=/home/jenkins/tools/gradle/4.4
01:57:54.210 Adding one-line test results to commit status...
01:57:54.212 Setting GRADLE_4_4_HOME=/home/jenkins/tools/gradle/4.4
01:57:54.213 Setting GRADLE_4_4_HOME=/home/jenkins/tools/gradle/4.4
01:57:54.214 Setting status of c55945d88f6683ee8500253dfd0db150beb45fde to FAILURE with url https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/12555/ and message: 'FAILURE
01:57:54.214  8808 tests run, 7 skipped, 0 failed.
```"
381174729,4812,vvcephei,2018-04-13T15:36:51Z,"Hey @guozhangwang , I hear you on the pollution of this PR, and the unsatisfactory state of metrics refactoring here.

Maybe I'll take a couple of hours and extract all the code cleanup (making variables final, etc.) into a separate PR and then rebase this one on that.

Then, this PR will be smaller, and I'll feel more comfortable proceeding with some more refactoring of the metrics code."
381276277,4812,vvcephei,2018-04-13T22:33:15Z,"I have pulled out the trivial changes into PR #4872 and rebased this PR on that one.
Please focus reviews on #4872 until it is merged."
382128932,4812,vvcephei,2018-04-17T20:20:42Z,rebased on trunk now that dependee PR is merged.
382191401,4812,guozhangwang,2018-04-17T23:34:47Z,"Jenkins failure:

```
20:41:17 /home/jenkins/jenkins-slave/workspace/kafka-pr-jdk10-scala2.12/streams/test-utils/src/main/java/org/apache/kafka/streams/processor/MockProcessorContext.java:197: error: constructor StreamsMetricsImpl in class StreamsMetricsImpl cannot be applied to given types;
20:41:17         this.metrics = new StreamsMetricsImpl(new Metrics(), ""mock-processor-context"", Collections.<String, String>emptyMap());
20:41:17                        ^
```"
382848909,4812,vvcephei,2018-04-19T19:11:18Z,"ok, @guozhangwang @mjsax @bbejeck ,

I believe this is ready for final review. I've made a pass over it to make sure the diff is clean and to comment on the rationale of some of the choices.

The diff is still quite long, but it's mostly because of all the processors that now record skipped metrics and the corresponding tests."
383137615,4812,vvcephei,2018-04-20T15:43:47Z,"@guozhangwang Thanks for the review!

I added https://github.com/apache/kafka/pull/4812/commits/18358f30445cd27c5155be7d73b8d5a98410886a and  https://github.com/apache/kafka/pull/4812/commits/36c065fa93b922668af87c8c7cc00460a7282ced in response to your comments. I also have a next PR queued up for after this one is merged (in response to our concerns about NamedCacheMetrics).

Please let me know what you think!
-John"
383219404,4812,bbejeck,2018-04-20T20:53:40Z,retest this please
383609305,4812,vvcephei,2018-04-23T15:07:23Z,Addressed Bill's comments and rebased.
534847170,7378,jukkakarvanen,2019-09-25T04:34:27Z,There is still a lot of old TopologyTestDriver test using deprecated methods needed to migrate to use new ones. So let me know if someone have possibility to help with those.
535173884,7378,jukkakarvanen,2019-09-25T19:24:32Z,Example class and develper guide updated.
536172291,7378,jukkakarvanen,2019-09-28T10:05:38Z,"Deprecated method migrated in streams and streams-scala packages
"
537780874,7378,mjsax,2019-10-03T04:29:19Z,"@jukkakarvanen Seems there are some conflicts -- can you rebase this PR? The feature freeze deadline was pushed to Friday, so we still have 2 days to get this into 2.4 :)"
538072445,7378,vvcephei,2019-10-03T18:38:22Z,"Hey @jukkakarvanen , I just had another thought while reading over @bbejeck's comments... For each of the ""deprecated"" javadoc lines, can you also mention the version it was deprecated in? Like, `@deprecated Since 2.4. Please use xxxxx instead...`

It just helps everyone understand when exactly the method should be removed."
538112784,7378,jukkakarvanen,2019-10-03T20:23:00Z,"@mjsax, Conflicts resolved
Changes made based on @bbejeck review
Also added deprecated Since 2.4 based on @vvcephei  suggestion"
538365070,7378,bbejeck,2019-10-04T11:50:12Z,"Java 11/2.12 failed with
```
org.apache.kafka.connect.integration.ConnectWorkerIntegrationTest.testAddAndRemoveWorker 
org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector
```
Java 8 timed out

retest this please

"
538501838,7378,jukkakarvanen,2019-10-04T18:01:52Z,@mjsax  I tried to cover all your comments
538528904,7378,mjsax,2019-10-04T19:22:09Z,LGTM.
538614484,7378,jukkakarvanen,2019-10-05T04:11:03Z,"Unrelated integration test failures.

retest this please"
538702218,7378,bbejeck,2019-10-06T01:02:43Z,"Java 11/2.12 failed with
```
org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector
org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSinkConnector
```

Java 11/2.13 failed with 
```
kafka.admin.ReassignPartitionsClusterTest.shouldPerformMultipleReassignmentOperationsOverVariousTopics
```
Java 8 failed with 
```
org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector
org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSinkConnector
org.apache.kafka.streams.integration.KTableKTableForeignKeyJoinIntegrationTest.doLeftJoinFromRightThenDeleteRightEntity
```

retest this please
"
538832801,7378,mjsax,2019-10-07T04:12:38Z,"JDK 11 / 2.12
```
org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector
```

JDK 11 / 2.13
```
kafka.api.AdminClientIntegrationTest.testElectUncleanLeadersForAllPartitions
```

Java8 no test failures, but timed out.

Retest this please."
538883648,7378,mjsax,2019-10-07T08:00:54Z,`org.apache.kafka.connect.integration.ExampleConnectIntegrationTest.testSourceConnector` failed. Unrelated.
425418058,5709,bbejeck,2018-09-28T12:20:56Z,"failure unrelated

retest this please"
425575819,5709,bbejeck,2018-09-28T21:49:47Z,@vvcephei  updated per comments
425780847,5709,bbejeck,2018-10-01T03:32:20Z,">Cannot follow here... Do you aim for existing topologies with generated names, and user update code to ""pin"" names? For this case, user would pass it name, without -repartition suffix? User, would also need to drop <application.id> prefix in the name she passed to Grouped.

>I cannot follow here too.. the createRepartitionedSource should always be called before the optimization kicks in, so the passed in name should always be the raw names right?

@mjsax @guozhangwang  

As we are re-using the repartition topic name when optimizing checking for the `-repartition` suffix was done during those cases when re-using the topic name which is already formatted with `appId-baseName-repartition` I have since cleaned up the code and re-use the topic name in the `InternalStreamsBuilder` when performing the optimization."
425788398,5709,guozhangwang,2018-10-01T04:47:37Z,"Hmm. I still cannot fully understand it since from the source code, the existing `repartitionForJoin` function seems not reused elsewhere in this PR."
425791386,5709,bbejeck,2018-10-01T05:13:33Z,updated this
425924162,5709,bbejeck,2018-10-01T14:16:25Z,"failures unrelated

retest this please"
426006255,5709,bbejeck,2018-10-01T18:07:30Z,retest this please
426015601,5709,bbejeck,2018-10-01T18:35:52Z,"Call for final review @mjsax @guozhangwang  - I believe I've addressed all of your comments, and this is ready for merging"
426085849,5709,bbejeck,2018-10-01T22:30:35Z,">Execution failed for task ':core:integrationTest'.

restest this please"
426094341,5709,lindong28,2018-10-01T23:12:56Z,@guozhangwang @mjsax This PR seems very close to be merged. Do you think we can merge it in a day or two for 2.1.0 release?
426096811,5709,mjsax,2018-10-01T23:26:41Z,@lindong28 The goal is to merge it today.
426135610,5709,bbejeck,2018-10-02T03:19:56Z,rebased this
426159558,5709,mjsax,2018-10-02T06:08:01Z,Merging this. Please address comments if follow up PR.
200954587,812,rajinisivaram,2016-03-24T18:12:33Z,"@ijuma @harshach @junrao Do you by any chance have time to review this PR? Thank you...
"
205629220,812,junrao,2016-04-05T03:47:46Z,"Could you also run the system tests on this patch?
"
205806557,812,ijuma,2016-04-05T13:32:21Z,"Quick note to mention that the PR does not merge cleanly to trunk at the moment.
"
206484021,812,rajinisivaram,2016-04-06T17:43:45Z,"@ijuma @junrao Thank you for the review. I have done the rebase and started a system test run.
"
206877257,812,rajinisivaram,2016-04-07T12:50:49Z,"System test results are here: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/406/. There was one failure (OffsetValidationTest) which is an existing transient failure already being addressed by KAFKA-3513.
"
209425539,812,rajinisivaram,2016-04-13T13:03:32Z,"The latest commit replaces the request-response to configure SASL mechanisms with aSaslHandshakeRequest/SaslHandshakeResponse pair that conforms to the standard Kafka protocol, as discussed in the KIP-35/KIP-43 threads in the mailing list. The overall code structure for authentication/handshake has not been changed.
"
212379418,812,rajinisivaram,2016-04-20T10:55:43Z,"@junrao Will you be able to review the latest changes so that this can be committed for 0.10.0? I have rebased and started another system test run. Thank you.
"
212644270,812,rajinisivaram,2016-04-20T22:56:23Z,"@junrao Thank you for the review. I have made most of the updates and left a couple of responses for you to review. Many thanks.
"
213326258,812,rajinisivaram,2016-04-22T08:37:37Z,"The latest system test results from this branch are here: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/413/
"
213821646,812,ijuma,2016-04-23T20:07:14Z,"@rajinisivaram Thanks for running the system tests. Are you going to update some of the system tests to test the mechanism functionality? Also, weren't there some unit (as opposed to integration) tests for SASL in a previous iteration of the PR?
"
214000262,812,junrao,2016-04-24T16:51:46Z,"@rajinisivaram : Thanks for the patch. Looks good to me. Just had a minor comment.

Also, once the vote for KIP-35 passes, would you be up for adding the support of ApiVersionRequest in the Sasl authenticator layer since you are more familiar with the logic there?
"
214094390,812,ijuma,2016-04-25T02:15:19Z,"@rajinisivaram Thanks for the PR. I have completed my review now and left some comments.
"
214276979,812,rajinisivaram,2016-04-25T11:39:15Z,"@ijuma @junrao Thank you for the reviews. I have made most of the updates suggested and left a few comments. I had moved the SASL unit tests out of this PR to make it easier to keep it updated (since the tests have quite a bit of refactoring that makes it hard to rebase). Once this is committed, I will rebase and submit the additional tests in another JIRA. I will also submit the PR for system tests of the changes - I think it should be sufficient to run only a few tests with PLAIN, PLAIN+GSSAPI.

@junrao I will be happy to submit the PR for handling ApiVersionRequests during SASL handshake as well, once the new request is added for KIP-35.
"
214356654,812,ijuma,2016-04-25T14:20:32Z,"@rajinisivaram, thanks for updating the PR. I reviewed your updates and left some very minor comments. The main item left on this PR in my opinion is how we name the inter-broker sasl mechanism property. I think it would be better if the config name is consistent with the property for inter-broker security protocol (ie there is an `inter.broker` in the name). Let's see what @junrao thinks.

Before 0.10.0.0, we need to:
1. Add unit, negative and system tests for this PR
2. Add handling of ApiVersionRequests during SASL handshake

Would you mind please filing JIRAs with 0.10.0.0 as the ""Fix version""?
"
214464394,812,rajinisivaram,2016-04-25T18:03:19Z,"@ijuma Thank you for reviewing the updates. Have filed new JIRAs for the remaining work:
- KAFKA-3617 : Add unit tests for SASL authenticator
- KAFKA-2693 : Ducktape tests
- KAFKA-3618: Handle ApiVersionRequest before SASL handshake
"
214563135,812,ijuma,2016-04-25T23:35:03Z,"Thanks @rajinisivaram, LGTM.
"
214868708,812,ijuma,2016-04-26T20:01:20Z,"@rajinisivaram, I ran the unit/integration tests with this branch twice and both times I got some failures in SASL tests. Do the tests pass reliably for you? Is it possible that a recent change is causing issues?
"
214900437,812,rajinisivaram,2016-04-26T21:57:36Z,"@ijuma The tests have been passing consistently for me. I reran them again a few times after rebasing on the latest level and they still pass. If you let me know which tests have been failing for you, I can rerun those in a loop to see if I can recreate the failure. Will also see how the automated PR build does after the rebase. Thanks.
"
214906615,812,ijuma,2016-04-26T22:27:01Z,"@rajinisivaram, thanks. I am running the tests again in case it was a mistake on my end. If they pass in Jenkins and locally, I'll merge the PR (I checked with Jun and he's happy with it). If not, I'll provide information about the failing tests.
"
214924091,812,ijuma,2016-04-26T23:57:31Z,"The failing tests were related to the flaky wireless network in the Kafka Summit, I got the same failure with SSL tests (where SASL wasn't involved). After I stabilised the network, the tests passed twice and they passed in Jenkins too.

Merged to trunk.
"
134471308,165,asfbot,2015-08-25T04:37:28Z,"[kafka-trunk-git-pr #209](https://builds.apache.org/job/kafka-trunk-git-pr/209/) FAILURE
Looks like there's a problem with this pull request
"
136892640,165,asfbot,2015-09-01T23:39:53Z,"[kafka-trunk-git-pr #302](https://builds.apache.org/job/kafka-trunk-git-pr/302/) FAILURE
Looks like there's a problem with this pull request
"
136898523,165,asfbot,2015-09-02T00:15:27Z,"[kafka-trunk-git-pr #303](https://builds.apache.org/job/kafka-trunk-git-pr/303/) FAILURE
Looks like there's a problem with this pull request
"
137300283,165,ewencp,2015-09-03T01:54:36Z,"I left a few comments, but mostly they were things that could potentially be clarified. Only one question about correctness. After those issues are followed up on, this looks good to me.
"
146709577,165,hachikuji,2015-10-08T22:47:58Z,"@ewencp @guozhangwang @onurkaraman I've updated this patch based on the latest proposal (https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Client-side+Assignment+Proposal), which uses a single client to do the group assignment. I'm still doing a bit of polishing and adding tests, but since most existing tests are passing, it seems ready to start reviewing. There is one failing kstreams test that I have yet to investigate.

A few general notes on the patch:
- One significant difference from the proposal is that I had to push the assignment strategies out of the consumer's embedded protocol and into the JoinGroup request as a ""subProtocols"" field. This allows the coordinator to do assignment strategy selection similar to the initial patch. It also lets the coordinator reject incompatible members without forcing a group rebalance. Otherwise, we can only detect inconsistencies after the group has rebalanced and the leader sees the metadata from all members. The problem then becomes how the group should handle the inconsistency? We could let the leader kick certain members out of the group, but that complicates the protocol. We could also just fail the entire group, but that seemed extreme. In the end, it seemed simplest to give the coordinator a mechanism to validate compatibility of group members. 
- This brings up the issue of whether each ""subProtocol"" (or assignment strategy) needs to have its own metadata and embedded as in the initial patch. Without it, any assignment-strategy-specific information must itself be embedded in the member metadata included in the JoinGroup request. This is easy if the consumer is using a metadata format like JSON, but trickier with packed arrays. The advantage of keeping everything in the same metadata, however, is that it simplifies the protocol and avoids redundancies such as including subscriptions multiple times.
- If the leader has an unexpected error during synchronization, the patch does not provide a way to propagate that error to the rest of the group as the proposal called for. Instead, the leader will propagate the exception to the user and the leader's session timeout will expire on the coordinator which will allow for a new leader to be elected. The problem I found when trying to implement this feature is that it requires a new state on the coordinator which is basically like Stable except contains an error to be propagated when the rest of the group syncs, which adds considerable complexity. It also creates a race condition where some members might see the error and immediately rejoin which forces the group out of that state. This means that some members wouldn't generally see the error. In the end, it felt like the cost of propagating the error through the coordinator was too high to justify capability it was exposing. Since unexpected leader failures should be rare, the simpler approach is just to let it timeout.
- The versioning in the consumer's embedded protocol (as contained in ConsumerProtocol.java) is clearly inadequate and will require further consideration. I'm inclined to push that to a separate JIRA so that we can consider it without the rest of the noise around this patch.
- The protocol implemented here provides no special support for regex handling. As before, each member watches metadata and adds a normal topic subscription when it finds a new matching topic. If we want to have special handling, that can also be pushed to another JIRA.
"
147134061,165,ewencp,2015-10-10T23:19:31Z,"@hachikuji Not critical for this patch, but do we want to reconsider the packages for some of the classes? For example, `GroupCoordinator` is currently in `org.apache.kafka.clients.consumer.internals`. Keeping it in _some_ `internals` package makes sense at the moment to indicate it shouldn't be relied on publicly, but since it's generalized functionality, housing it under `consumer` might not be ideal. Not sure if we want a new package (e.g. `org.apache.kafka.clients.group`), move it under `common`, or just leave it where it is and preserve some indication of its heritage.
"
147185767,165,ijuma,2015-10-11T11:47:38Z,"Just a heads-up: this branch doesn't merge cleanly against trunk anymore.
"
147262916,165,hachikuji,2015-10-12T00:16:20Z,"@ewencp Yeah, I think it makes sense to move it to a new package. To make that possible, I think we might have to merge ConsumerNetworkClient into NetworkClient, but that's the only complication I can think of.
"
147550028,165,guozhangwang,2015-10-12T23:40:58Z,"A general comment:

I feel adding this to the join-group to allow coordinator-side pre-checking would be OK, since although it adds an extra field in the join group request, it is not very hard to understand. But now the protocol field seems less useful to me given the group-id and sub-protocols, since we only use it to check all members use the same group-protocol string. But in practice how possible that members joining the same group name happen to be different types (e.g. one normal consumer and one copycat connector)? Having this field will not be so useful in excluding human-mistakes but even likely increase false-positives that two members that do belong to the same group got kicked out because they happen to specify different protocol names by mistake. 

So I would suggest we remove the protocol string and rename sub-protocols to protocols, doing this we may end up having group ids like ""kafka-consumer-xxx"", ""kafka-stream-yyy"" and ""kafka-copycat-zzz"", which is already sufficient to differentiate different types of members.
"
147768297,165,hachikuji,2015-10-13T16:27:51Z,"@guozhangwang That suggestion sounds fine to me, especially if it makes the protocol easier to understand. It leads to the question of how we are going to implement protocol-specific metadata. There's basically two options:

**Option 1:** Each protocol has its own metadata: this is probably the simplest to implement for client extensions. This would change the JoinGroup request to look something like this:

```
JoinGroupRequest => GroupId SessionTimeout MemberId Protocols
  GroupId => String
  SessionTimeout => int32
  MemberId => String
  Protocols => [Protocol MemberMetadata]
    Protocol => String
    MemberMetadata => Bytes
```

The downside is that this leads to redundant data in the request. For example, to support both the round-robin and range assignors in the consumer (at the same time), you'd have to pass the subscription list twice. Perhaps this is a rare enough case that we aren't too worried about the overhead? It also leads to somewhat more complex parsing, though I don't think this is a major concern.

**Option 2:** The second option, which is currently implemented, is that we have only one metadata field and we assume that any protocol-specific attributes are embedded in it. It would look like this:

```
JoinGroupRequest => GroupId SessionTimeout MemberId Protocols MemberMetadata
  GroupId => String
  SessionTimeout => int32
  MemberId => String
  Protocols => [String]
  MemberMetadata => Bytes
```

This is generally more difficult for clients to implement, but that depends largely on the format of the metadata. For example, if JSON is chosen, then protocol-specific attributes come naturally. However, it's a little tough to see how this would work for Kafka's packed structures, which have no field identifiers. As an advantage, since it allows protocols to share metadata, it avoids the duplication in the first approach. 

The GroupProtocol field in the current patch was basically intended to declare compatibility of the metadata as used in Option 2. If we get rid of it, then I would tend to favor the first option. What do you think?
"
147772825,165,ewencp,2015-10-13T16:44:34Z,"It might be worth making multiple assignors work to figure out how best to handle this. Right now the code only handles one assignor and I think that makes option 2 look like it can work, but the code is probably going to get messy or confusing if you try to actually implement it. Since you only have one metadata field, you'll need to somehow combine all the metadata required by different assignors. This means you'll have to assume some format like JSON and merge them.

I think the duplication in Option 1 isn't a big deal. It should definitely be rare. It also keeps things simpler -- the JoinGroup request is a bit more complex, but the whole implementation is easier and we don't end up with completely different protocols having to coordinate in order to make sure their metadata is compatible with each other.
"
147814103,165,guozhangwang,2015-10-13T18:58:33Z,"My understanding is that multiple protocols will only be used for upgrades, and most of time the protocols size should be 1. If that is the case:
1. 98% we will have 1 protocol.
2. 1.9999% we will have 2 protocols.
3. 0.0001% we will have 3 protocols.

So I think option 1) would be find, but @ewencp could clarify if there are no scenarios that multiple protocols are needed life-time.
"
147815469,165,ewencp,2015-10-13T19:04:04Z,"I think 3 protocols should never happen aside from misconfiguration. The only use case for 2 is to upgrade/switch assignors.
"
147816246,165,guozhangwang,2015-10-13T19:07:26Z,"Yeah, for upgrade / switch 2 protocols are temporary since we will usually do two rolling bounce to remove the old one in the second round.

Anyways, I think option 1) should be fine.
"
148478770,165,hachikuji,2015-10-15T18:19:44Z,"@ewencp @guozhangwang I've reworked the protocol as discussed to allow protocol-specific metadata with coordinator negotiation similar to the initial patch. Basically this resulted in pushing serialization concerns into AbstractCoordinator extensions. Also have a look at the new PartitionAssignor interface. It should be much clearer how assignor implementations can leverage custom metadata.
"
148802867,165,hachikuji,2015-10-16T18:43:34Z,"@ewencp @guozhangwang Made the changes discussed. I think this does simplify the assignor internals quite a bit.
"
148823145,165,guozhangwang,2015-10-16T20:17:10Z,"@hachikuji The client-side protocol and implementations LGTM overall. Thanks!

Do you want to refactor the server-side FSM a bit now? Also some response error codes like SyncGroupResponse need to be updated as well.
"
149285168,165,hachikuji,2015-10-19T17:14:01Z,"@guozhangwang Anything else to clear up before this can be merged?
"
149287269,165,guozhangwang,2015-10-19T17:21:07Z,"I'm still reviewing the changes made on the server-side coordinator, I thought you did the FSM refactoring right?
"
149288054,165,hachikuji,2015-10-19T17:23:52Z,"@guozhangwang  I updated the FSM documentation that was in GroupMetadata.scala. Was there anything else you wanted me to fix? By the way, I'm rebasing off of Flavio's commit and should be able to update shortly.
"
149288862,165,guozhangwang,2015-10-19T17:27:18Z,"The problem is that I cannot get the full diff now as it become too big. Right now I can only review it per-commits which is not optimal..
"
149308251,165,guozhangwang,2015-10-19T18:39:44Z,"This is what I think about the state machine (if we agree we can also update the diagram in the wiki), with the assumption that for all request handling we first check coordinator availability and group ownership is correct. All the events except consumer failure is triggered by requests.

State Down: group is either not created or has no members.

-> onJoin: create group and add member if necessary (start HB in purgatory), transit to PrepareRebalance.
-> onSync: return REBALANCE_IN_PROGRESS (could be from AS -> PR -> Down).
-> onHeartBeat: return ILLEGAL_GENERATION.
-> onOffsetCommit: blindly accepts and return OK.
-> onOffsetFetch: blindly accepts and return OK.
-> onConsumerFailure: SHOULD NOT HAPPEN, throw exception.

State PrepareRebalance: waiting for all members to Join

-> onJoin: check if all members joined, if yes transit to AwaitingSync
-> onSync: return REBALANCE_IN_PROGRESS (could be from AS -> PR).
-> onHeartBeat: return REBALANCE_IN_PROGRESS.
-> onOffsetCommit: return REBALANCE_IN_PROGRESS.
-> onOffsetFetch: return REBALANCE_IN_PROGRESS.
-> onConsumerFailure: remove consumer (stop HB in purgatory), check if all rest members joined, if yes transit to AwaitingSync.

State AwaitingSync: waiting for leader to assign

-> onJoin: update / create consumer if necessary, transit to PrepareRebalance
-> onSync: if from follower, treat it as HB and park it (unmute further HBs); otherwise treat it as HB for the leader and transit to Stable.
-> onHeartBeat: return REBALANCE_IN_PROGRESS.
-> onOffsetCommit: return REBALANCE_IN_PROGRESS.
-> onOffsetFetch: return REBALANCE_IN_PROGRESS.
-> onConsumerFailure: remove consumer (stop HB in purgatory), if that is the leader transit to PrepareRebalance

State Stable: group formed with resource assigned and remembered by Coordinator

-> onJoin: update / create consumer if necessary, transit to PrepareRebalance
-> onSync: handle normally with remembered assignment
-> onHeartBeat: handle normally.
-> onOffsetCommit: check on member-id / generation-id and return OK or corresponding errors.
-> onOffsetFetch: check on member-id / generation-id and return OK or corresponding errors.
-> onConsumerFailure: remove consumer (stop HB in purgatory), transit to PrepareRebalance.

---

Transit action: from Down to PrepareRebalance:
        1) start delayed rebalance in purgatory if there are still members in the group
        2) mute delayed HB for all members in the group

Transit action: from PrepareRebalance to AwaitingSync:
        1) clear dalyed rebalance in purgatory, send back Join response to followers and leader
        2) unmute delayed HB for all members in the group 

Transit action: from AwaitingSync to PrepareRebalance:
        1) send back Sync response to all parked requests with REBALANCE_IN_PROGRESS
        2) start delayed rebalance in purgatory if there are still members in the group
        3) mute delayed HB for all members in the group

Transit action: from Stable to PrepareRebalance:
        1) start delayed rebalance in purgatory if there are still members in the group
        2) mute delayed HB for all members in the group

---

I think we are already doing this in this patch, but just want to make the comments in GroupMetadata to be more clear.

And not need to include in this JIRA, but moving forward we will possibly add a background scheduler that periodically remove groups with zero members or not belonged to coordinator any more and transit them to Dead.
"
149316362,165,hachikuji,2015-10-19T19:12:04Z,"@guozhangwang Thanks for writing that up! I think there's a couple slight differences in the current patch from what you wrote:

State Down: group is either not created or has no members.
-> onHeartBeat: return UNKNOWN_MEMBER_ID
-> onOffsetCommit: return ILLEGAL_GENERATION if generation is positive, otherwise accept commit
-> onOffsetFetch: return UNKNOWN_MEMBER_ID

State PrepareRebalance: waiting for all members to Join
-> onOffsetCommit: accept commits from previous generation, otherwise ILLEGAL_GENERATION
-> onOffsetFetch: return offsets blindly

State AwaitingSync: waiting for leader to assign
-> onOffsetCommit: allow commit from the joined generation
-> onOffsetFetch: return offsets blindly

For the AwaitingSync state, I think your suggestion to return REBALANCE_IN_PROGRESS for offset commits is better than the current behavior since there should be no need to commit offsets before receiving the assignment and the member still has the PreparingRebalance state to commit offsets before rebalances.

For fetching committed offsets, I actually wonder if we should skip all group checks and return the offsets blindly. Do we really need to impose the restriction that committed offsets can only be queried by members of the group?
"
149355804,165,guozhangwang,2015-10-19T21:42:12Z,"@hachikuji Could we make partitioner also Configurable as we discussed offline? Besides the above comments LGTM.
"
149362802,165,hachikuji,2015-10-19T22:13:13Z,"@guozhangwang Have a look at the updated state transitions. I wonder if we should have a `Ready` state for the initial state when no group members have joined. Currently the group starts up in `Stable` even though it has no members. 

@onurkaraman What do you think?
"
149372398,165,guozhangwang,2015-10-19T23:07:40Z,"I agree to the `Ready` state, since if we are going to clean up groups moving forward we will no longer need the `Dead` state any more.
"
149996276,165,guozhangwang,2015-10-21T19:07:29Z,"LGTM, the Jenkins failures seem irrelevant and transient. Also made a passing Ducktape system test build on this branch. @hachikuji Please remember to address @becketqin 's comments in the later follow-up path. Merging this large patch for now as it is getting very large.
"
2427455305,17539,adixitconfluent,2024-10-21T18:41:04Z,"Hi @junrao @apoorvmittal10 , please review my PR when you get a chance. Thanks!"
2433136716,17539,adixitconfluent,2024-10-23T18:35:35Z,"Hi @junrao @apoorvmittal10 , thanks for being kind enough to reviewing this PR. I have addressed both your comments. Please take a look when you can!"
2450647031,17539,adixitconfluent,2024-10-31T19:18:26Z,"> @adixitconfluent : Thanks for the updated PR. A few more comments. Also, it seems the previous comments on SharePartition haven't been addressed?

Hi @junrao, I am still in the middle of the refactor. I will raise a re-review once I am done. I was just resolving the comments for which I pushed the fixes in my recent commits, probably that gave a false idea that I was done addressing your changes, sorry about that."
2452271960,17539,adixitconfluent,2024-11-01T17:21:19Z,"Hi @junrao , I have addressed all your comments. Please re-review my PR when you get a chance. Thanks!"
2455446039,17539,adixitconfluent,2024-11-04T18:42:13Z,"Hi @junrao , I have addressed your comments. Please re-review my PR when you get a chance. Thanks!"
2461751459,17539,adixitconfluent,2024-11-07T09:38:03Z,"Hi @junrao , I've addressed the comments from the latest review. Please re-review my PR when you get a chance. Thanks!"
293814064,2849,dguy,2017-04-13T07:27:46Z,"@ijuma @junrao @guozhangwang @apurvam @mjsax for reviews please.
Note: this is not the complete TransactionCoordinator, i.e., transaction expiration, TransactionalId -> PID mapping expiration, and recovery in handling initPidRequest all haven't been done yet"
293819311,2849,asfbot,2017-04-13T07:54:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2928/
Test FAILed (JDK 8 and Scala 2.11).
"
293819380,2849,asfbot,2017-04-13T07:54:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2923/
Test FAILed (JDK 7 and Scala 2.10).
"
293819597,2849,asfbot,2017-04-13T07:56:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2924/
Test FAILed (JDK 8 and Scala 2.12).
"
293834203,2849,asfbot,2017-04-13T09:04:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2924/
Test PASSed (JDK 7 and Scala 2.10).
"
293842132,2849,asfbot,2017-04-13T09:40:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2925/
Test PASSed (JDK 8 and Scala 2.12).
"
293842426,2849,asfbot,2017-04-13T09:41:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2929/
Test PASSed (JDK 8 and Scala 2.11).
"
293864098,2849,asfbot,2017-04-13T11:30:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2932/
Test FAILed (JDK 8 and Scala 2.11).
"
293864152,2849,asfbot,2017-04-13T11:30:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2927/
Test FAILed (JDK 7 and Scala 2.10).
"
293864348,2849,asfbot,2017-04-13T11:31:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2928/
Test FAILed (JDK 8 and Scala 2.12).
"
293889964,2849,asfbot,2017-04-13T13:06:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2928/
Test PASSed (JDK 7 and Scala 2.10).
"
293891344,2849,asfbot,2017-04-13T13:12:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2933/
Test PASSed (JDK 8 and Scala 2.11).
"
293893069,2849,asfbot,2017-04-13T13:19:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2929/
Test PASSed (JDK 8 and Scala 2.12).
"
294775812,2849,asfbot,2017-04-18T10:41:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2984/
Test FAILed (JDK 7 and Scala 2.10).
"
294779846,2849,asfbot,2017-04-18T10:50:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2989/
Test FAILed (JDK 8 and Scala 2.11).
"
294785121,2849,asfbot,2017-04-18T11:03:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2985/
Test FAILed (JDK 8 and Scala 2.12).
"
294822791,2849,asfbot,2017-04-18T12:36:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2988/
Test PASSed (JDK 8 and Scala 2.12).
"
294823956,2849,asfbot,2017-04-18T12:39:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2992/
Test PASSed (JDK 8 and Scala 2.11).
"
294825097,2849,asfbot,2017-04-18T12:42:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2987/
Test FAILed (JDK 7 and Scala 2.10).
"
294856558,2849,asfbot,2017-04-18T14:09:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2989/
Test FAILed (JDK 7 and Scala 2.10).
"
294862073,2849,asfbot,2017-04-18T14:27:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2990/
Test FAILed (JDK 8 and Scala 2.12).
"
294873548,2849,asfbot,2017-04-18T15:03:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2994/
Test PASSed (JDK 8 and Scala 2.11).
"
294908535,2849,asfbot,2017-04-18T16:52:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2996/
Test PASSed (JDK 8 and Scala 2.11).
"
294909962,2849,asfbot,2017-04-18T16:57:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2991/
Test PASSed (JDK 7 and Scala 2.10).
"
294911650,2849,asfbot,2017-04-18T17:03:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2992/
Test PASSed (JDK 8 and Scala 2.12).
"
295174280,2849,asfbot,2017-04-19T09:00:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3015/
Test FAILed (JDK 7 and Scala 2.10).
"
295187088,2849,asfbot,2017-04-19T09:32:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3016/
Test PASSed (JDK 8 and Scala 2.12).
"
295199558,2849,asfbot,2017-04-19T10:04:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3020/
Test PASSed (JDK 8 and Scala 2.11).
"
295651149,2849,asfbot,2017-04-20T09:32:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3050/
Test FAILed (JDK 8 and Scala 2.11).
"
295677180,2849,asfbot,2017-04-20T10:42:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3048/
Test PASSed (JDK 7 and Scala 2.10).
"
295678357,2849,asfbot,2017-04-20T10:45:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3049/
Test PASSed (JDK 8 and Scala 2.12).
"
295682536,2849,asfbot,2017-04-20T10:57:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3051/
Test FAILed (JDK 8 and Scala 2.12).
"
295713087,2849,asfbot,2017-04-20T12:16:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3052/
Test PASSed (JDK 7 and Scala 2.10).
"
295715996,2849,asfbot,2017-04-20T12:23:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3053/
Test FAILed (JDK 8 and Scala 2.12).
"
295727022,2849,asfbot,2017-04-20T12:54:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3057/
Test PASSed (JDK 8 and Scala 2.11).
"
295774184,2849,asfbot,2017-04-20T15:15:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3056/
Test PASSed (JDK 7 and Scala 2.10).
"
295774584,2849,asfbot,2017-04-20T15:16:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3057/
Test PASSed (JDK 8 and Scala 2.12).
"
295777467,2849,asfbot,2017-04-20T15:24:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3061/
Test PASSed (JDK 8 and Scala 2.11).
"
295803177,2849,asfbot,2017-04-20T16:31:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3064/
Test FAILed (JDK 8 and Scala 2.11).
"
295805754,2849,asfbot,2017-04-20T16:38:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3059/
Test PASSed (JDK 7 and Scala 2.10).
"
295805815,2849,asfbot,2017-04-20T16:38:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3060/
Test PASSed (JDK 8 and Scala 2.12).
"
296382954,2849,asfbot,2017-04-22T16:01:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3118/
Test PASSed (JDK 8 and Scala 2.11).
"
296382955,2849,asfbot,2017-04-22T16:01:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3114/
Test PASSed (JDK 8 and Scala 2.12).
"
296382959,2849,asfbot,2017-04-22T16:01:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3113/
Test PASSed (JDK 7 and Scala 2.10).
"
296924449,2849,asfbot,2017-04-25T06:15:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3157/
Test PASSed (JDK 7 and Scala 2.10).
"
296940620,2849,dguy,2017-04-25T07:27:43Z,"@junrao thanks for taking the time to review again.
 Regarding:
>Also, it seems that we don't have the code to (1) abort a long transaction; (2) expire a transactional id not being actively used some time?

Correct they have not been done yet."
296955224,2849,asfbot,2017-04-25T08:15:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3163/
Test PASSed (JDK 8 and Scala 2.11).
"
296969712,2849,asfbot,2017-04-25T09:15:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3157/
Test PASSed (JDK 8 and Scala 2.12).
"
297542444,2849,guozhangwang,2017-04-26T21:11:18Z,Collapsed all commits and merged to trunk.
297552932,2849,ijuma,2017-04-26T21:55:47Z,"For large PRs like this, we should run the system tests before we merge. Can we post a link to a successful run for future record (assuming we've done that)?"
297553624,2849,hachikuji,2017-04-26T21:58:55Z,@ijuma I've kicked off a build here: https://jenkins.confluent.io/view/All/job/system-test-kafka-branch-builder-2/275/. Let's cross our fingers since it's already merged!
297584656,2849,asfbot,2017-04-27T01:19:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3190/
Test PASSed (JDK 7 and Scala 2.10).
"
297651197,2849,ijuma,2017-04-27T08:40:52Z,"Thanks @hachikuji, build passed. :)"
297710649,2849,asfbot,2017-04-27T13:19:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3189/
Test PASSed (JDK 8 and Scala 2.12).
"
298811252,2964,guozhangwang,2017-05-03T02:38:23Z,"@dguy @junrao 

I am still working to fixing all the unit tests but the non-testing code is ready for reviews."
298811373,2964,asfbot,2017-05-03T02:40:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3407/
Test FAILed (JDK 8 and Scala 2.11).
"
298811411,2964,asfbot,2017-05-03T02:40:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3398/
Test FAILed (JDK 8 and Scala 2.12).
"
298811741,2964,asfbot,2017-05-03T02:43:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3401/
Test FAILed (JDK 7 and Scala 2.10).
"
298820761,2964,apurvam,2017-05-03T04:26:29Z,Looks like the build fails with compilation errors..
298828273,2964,guozhangwang,2017-05-03T05:51:20Z,@apurvam those are unit test compilations. I'm still working on those but non-testing code does build.
298833334,2964,apurvam,2017-05-03T06:37:02Z,ah.. ok.. thanks! 
299080651,2964,asfbot,2017-05-04T01:49:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3462/
Test FAILed (JDK 8 and Scala 2.11).
"
299080688,2964,asfbot,2017-05-04T01:50:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3453/
Test FAILed (JDK 8 and Scala 2.12).
"
299080709,2964,asfbot,2017-05-04T01:50:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3456/
Test FAILed (JDK 7 and Scala 2.10).
"
299391786,2964,asfbot,2017-05-05T06:22:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3533/
Test FAILed (JDK 8 and Scala 2.11).
"
299391810,2964,asfbot,2017-05-05T06:22:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3527/
Test FAILed (JDK 7 and Scala 2.10).
"
299392006,2964,asfbot,2017-05-05T06:24:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3524/
Test FAILed (JDK 8 and Scala 2.12).
"
300638549,2964,guozhangwang,2017-05-10T23:14:48Z,"@hachikuji @dguy @junrao I have addressed your comments. Two major modifications:

1. `TransactionMarkerRequestCompletionHandler`: exhaust all possible error codes.
2. Merge `TransactionMarkerChannel` into `TransactionMarkerChannelManager`."
300989007,2964,asfbot,2017-05-12T05:43:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3782/
Test FAILed (JDK 8 and Scala 2.12).
"
300989188,2964,guozhangwang,2017-05-12T05:45:28Z,"@hachikuji addressed your comments, and fixed unit tests.

Also another observation is that we are unnecessarily sending multiple txn marker requests to a single broker for txnIds from different txn topic partitions, I have grouped them into a single request as well in this PR. cc @dguy "
300989959,2964,asfbot,2017-05-12T05:51:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/3794/
Test FAILed (JDK 7 and Scala 2.11).
"
301001608,2964,asfbot,2017-05-12T07:10:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/3795/
Test PASSed (JDK 7 and Scala 2.11).
"
301007400,2964,asfbot,2017-05-12T07:43:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3783/
Test PASSed (JDK 8 and Scala 2.12).
"
301194236,2964,asfbot,2017-05-12T21:51:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/3839/
Test PASSed (JDK 7 and Scala 2.11).
"
301195878,2964,guozhangwang,2017-05-12T22:01:31Z,Merged to trunk.
301196624,2964,asfbot,2017-05-12T22:06:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3827/
Test PASSed (JDK 8 and Scala 2.12).
"
301222344,2964,guozhangwang,2017-05-13T03:31:54Z,"@junrao Thanks for the comments, I will try to incorporate them in a follow-up PR."
248246823,1884,dguy,2016-09-20T09:10:47Z,"One general comment. `InternalTopicManager` isn't really doing anything anymore. I left this comment on the previous PR:

> I'm wondering if a lot of this code should be in StreamsKafkaClient or perhaps remove StreamsKafkaClient completely and construct the underlying NetworkClient in InternalTopicManager. If the functionality is something we can/will use elsewhere then I'd put it in StreamsKafkaClient

It seems to me that they should be merged into a single class? @guozhangwang, thoughts?
"
248379483,1884,hjafarpour,2016-09-20T17:53:45Z,"@dguy Made changes according to your feedback.
"
248967534,1884,hjafarpour,2016-09-22T17:14:23Z,"@dguy made changes according to your feedback and pushed the changes.
"
249255156,1884,hjafarpour,2016-09-23T17:36:00Z,"@dguy Applied the feedback and pushed the changes.
"
249611052,1884,dguy,2016-09-26T15:50:50Z,"oic - the problem with making it public on KafkaStreams is that it would
then be part of the public API. We probably should move the field from
KafkaStreams to an internal class and have KafkaStreams and this class
reference that. Problem is i'm not entirely sure where to put it right now.
@guozhangwang https://github.com/guozhangwang might have an idea.
Otherwise, just leave it here and we can fix it later

On Mon, 26 Sep 2016 at 16:42 hjafarpour notifications@github.com wrote:

> ## _@hjafarpour_ commented on this pull request.
> 
> In
> streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsKafkaClient.java
> https://github.com/apache/kafka/pull/1884:
> 
> > -        final Time time = new SystemTime();
> >   +
> > -        final Map<String, String> metricTags = new LinkedHashMap<>();
> > -        metricTags.put(""client-id"", StreamsConfig.CLIENT_ID_CONFIG);
> >   +
> > -        final Metadata metadata = new Metadata(streamsConfig.getLong(StreamsConfig.RETRY_BACKOFF_MS_CONFIG), streamsConfig.getLong(StreamsConfig.METADATA_MAX_AGE_CONFIG));
> > -        final List<InetSocketAddress> addresses = ClientUtils.parseAndValidateAddresses(streamsConfig.getList(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG));
> > -        metadata.update(Cluster.bootstrap(addresses), time.milliseconds());
> >   +
> > -        final MetricConfig metricConfig = new MetricConfig().samples(streamsConfig.getInt(CommonClientConfigs.METRICS_NUM_SAMPLES_CONFIG))
> > -                .timeWindow(streamsConfig.getLong(CommonClientConfigs.METRICS_SAMPLE_WINDOW_MS_CONFIG), TimeUnit.MILLISECONDS)
> > -                .tags(metricTags);
> > -        final List<MetricsReporter> reporters = streamsConfig.getConfiguredInstances(ProducerConfig.METRIC_REPORTER_CLASSES_CONFIG,
> > -                MetricsReporter.class);
> > -        // TODO: This should come from the KafkaStream
> > -        reporters.add(new JmxReporter(""kafka.streams""));
> 
> @dguy https://github.com/dguy in KafkaStreams.java JMX_PREFIX is
> defined private:
> private static final String JMX_PREFIX = ""kafka.streams"";
> 
> I could either make it public and use it here or define a new field here.
> Which one would you suggest?
> 
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> https://github.com/apache/kafka/pull/1884, or mute the thread
> https://github.com/notifications/unsubscribe-auth/AADWLZWjZDJwgxkR127lsm1YE6ligzLRks5qt-fkgaJpZM4KBGjN
> .
"
249612400,1884,hjafarpour,2016-09-26T15:55:27Z,"@dguy Alright, leaving it as it until we decide where we can move the field.
"
250317873,1884,hjafarpour,2016-09-28T22:16:18Z,"@guozhangwang Pushed new changes.
"
250342391,1884,guozhangwang,2016-09-29T00:49:50Z,"@hjafarpour I ran the unit test locally and the Jenkins failures seems to be pretty consistent, were your local unit test run passed?

```
KafkaStreamsTest. testStartAndClose
KStreamAggregationDedupIntegrationTest. shouldReduce
KStreamAggregationDedupIntegrationTest. shouldReduceWindowed
KStreamAggregationIntegrationTest. shouldAggregateWindowed[0]
KStreamAggregationIntegrationTest. shouldAggregateWindowed[1]
KStreamAggregationIntegrationTest. shouldAggregate[0]
KStreamAggregationIntegrationTest. shouldAggregate[1]
KStreamAggregationIntegrationTest. shouldCount[0]
KStreamAggregationIntegrationTest. shouldCount[1]
KStreamAggregationIntegrationTest. shouldReduceWindowed[0]
KStreamAggregationIntegrationTest. shouldReduceWindowed[1]
KStreamAggregationIntegrationTest. shouldReduce[0]
KStreamAggregationIntegrationTest. shouldReduce[1]
QueryableStateIntegrationTest. queryOnRebalance[0]
QueryableStateIntegrationTest. queryOnRebalance[1]
ResetIntegrationTest. testReprocessingFromScratchAfterReset
StreamPartitionAssignorTest. testAssignWithStates
```
"
250561766,1884,nitantbhartia,2016-09-29T19:07:45Z,"@enothereska @hjafarpour  Do you have a timeline for this fix? We need this fix as part of one of our client apps that we are building that uses spring cloud stream. Anything my team or I can do to help?
"
255222098,1884,guozhangwang,2016-10-20T20:41:37Z,"@nitantbhartia Sorry for the late reply, we are currently merging this PR as a post-0.10.1.0 PR, which means that it will likely to be included in the next minor release or in a bug-fix release (0.10.1.1).
"
256424024,1884,guozhangwang,2016-10-26T17:43:38Z,"@hjafarpour  Could you file a KIP for this API change as well?
"
270547965,1884,asfbot,2017-01-05T02:30:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/503/
Test FAILed (JDK 7 and Scala 2.10).
"
270547971,1884,asfbot,2017-01-05T02:30:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/505/
Test FAILed (JDK 8 and Scala 2.11).
"
270548013,1884,asfbot,2017-01-05T02:30:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/504/
Test FAILed (JDK 8 and Scala 2.12).
"
270991824,1884,guozhangwang,2017-01-06T20:00:09Z,test this please
271033715,1884,asfbot,2017-01-06T23:00:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/581/
Test FAILed (JDK 8 and Scala 2.11).
"
271033808,1884,asfbot,2017-01-06T23:01:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/579/
Test FAILed (JDK 7 and Scala 2.10).
"
271033942,1884,asfbot,2017-01-06T23:02:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/580/
Test FAILed (JDK 8 and Scala 2.12).
"
271056785,1884,asfbot,2017-01-07T02:32:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/594/
Test FAILed (JDK 8 and Scala 2.12).
"
271060463,1884,asfbot,2017-01-07T03:48:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/595/
Test FAILed (JDK 8 and Scala 2.11).
"
271060467,1884,asfbot,2017-01-07T03:48:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/593/
Test FAILed (JDK 7 and Scala 2.10).
"
271060729,1884,guozhangwang,2017-01-07T03:55:46Z,test this please
271067590,1884,asfbot,2017-01-07T06:56:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/601/
Test FAILed (JDK 8 and Scala 2.11).
"
271067599,1884,asfbot,2017-01-07T06:56:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/600/
Test FAILed (JDK 8 and Scala 2.12).
"
271067602,1884,asfbot,2017-01-07T06:56:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/599/
Test FAILed (JDK 7 and Scala 2.10).
"
271096444,1884,guozhangwang,2017-01-07T17:16:05Z,"@hjafarpour Jenkins failure seems related:

```
org.apache.kafka.streams.processor.internals.StreamPartitionAssignorTest > shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks FAILED
    org.apache.kafka.streams.errors.StreamsException: Could not create internal topics.
        at org.apache.kafka.streams.processor.internals.InternalTopicManager.makeReady(InternalTopicManager.java:73)
        at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.prepareTopic(StreamPartitionAssignor.java:611)
        at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.assign(StreamPartitionAssignor.java:370)
        at org.apache.kafka.streams.processor.internals.StreamPartitionAssignorTest.shouldNotLoopInfinitelyOnMissingMetadataAndShouldNotCreateRelatedTasks(StreamPartitionAssignorTest.java:892)

```

```
org.apache.kafka.streams.processor.internals.StreamPartitionAssignorTest > testAssignWithStates STARTED
Build timed out (after 180 minutes). Marking the build as aborted.
```"
271452298,1884,hjafarpour,2017-01-10T00:35:42Z,test this please
271458108,1884,asfbot,2017-01-10T01:12:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/652/
Test FAILed (JDK 8 and Scala 2.12).
"
271478083,1884,asfbot,2017-01-10T03:36:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/653/
Test FAILed (JDK 8 and Scala 2.11).
"
271478085,1884,asfbot,2017-01-10T03:36:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/651/
Test FAILed (JDK 7 and Scala 2.10).
"
271714172,1884,hjafarpour,2017-01-10T22:10:31Z,test this please
271724116,1884,asfbot,2017-01-10T22:53:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/700/
Test FAILed (JDK 8 and Scala 2.12).
"
271724713,1884,asfbot,2017-01-10T22:56:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/702/
Test PASSed (JDK 8 and Scala 2.11).
"
271738892,1884,guozhangwang,2017-01-11T00:11:56Z,retest this please
271745857,1884,asfbot,2017-01-11T00:54:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/710/
Test PASSed (JDK 8 and Scala 2.12).
"
271746128,1884,asfbot,2017-01-11T00:55:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/710/
Test PASSed (JDK 7 and Scala 2.10).
"
271746134,1884,asfbot,2017-01-11T00:56:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/712/
Test PASSed (JDK 8 and Scala 2.11).
"
271748780,1884,asfbot,2017-01-11T01:11:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/700/
Test FAILed (JDK 7 and Scala 2.10).
"
271930762,1884,guozhangwang,2017-01-11T17:16:25Z,"LGTM. Merged to trunk, thanks @hjafarpour !"
406449812,5379,stanislavkozlovski,2018-07-20T00:05:09Z,@rondagostino @rajinisivaram could you give this another look?
406665448,5379,stanislavkozlovski,2018-07-20T17:10:08Z,@rondagostino I think the final commit addresses all of the comments. Can you confirm?
406723141,5379,stanislavkozlovski,2018-07-20T20:50:22Z,@rondagostino made `SaslExtensionsCallback` return `SaslExtensions`. Please take a look
406733607,5379,stanislavkozlovski,2018-07-20T21:38:54Z,Seems like some related tests are failing. Currently investigating them but it's not obvious what caused them to fail at all
406750227,5379,stanislavkozlovski,2018-07-20T23:25:15Z,"@rondagostino I had some issues with failing tests due to inappropriate handling of `ScramExtensionsCallback`. I've now separated it from `SaslExtensionsCallback` entirely - it doesn't return `SaslExtensions` but a `Map<String, String`. Hopefully this shouldn't matter too much as we deprecated it.
Should be all clear now. Can you review?"
407474576,5379,stanislavkozlovski,2018-07-24T16:47:41Z,"This should address all comments @rondagostino, @rajinisivaram. Thanks for being patient with me"
407884676,5379,rondagostino,2018-07-25T20:28:34Z,LGTM
408931339,5379,stanislavkozlovski,2018-07-30T16:45:47Z,Retest this please
410683245,5379,stanislavkozlovski,2018-08-06T11:54:37Z,Retest this please
410763090,5379,rajinisivaram,2018-08-06T16:13:35Z,"Merging to trunk. 
@rondagostino Thanks for the reviews."
137326320,191,harshach,2015-09-03T04:15:32Z,"Anyone interested running this on vagrant , build kafka with this patch in and checkout https://github.com/harshach/kafka-vagrant . Drop the .tgz under kafka-vagrant. Vagrant up.
"
137339191,191,asfbot,2015-09-03T05:36:31Z,"[kafka-trunk-git-pr #340](https://builds.apache.org/job/kafka-trunk-git-pr/340/) FAILURE
Looks like there's a problem with this pull request
"
137453791,191,asfbot,2015-09-03T13:50:40Z,"[kafka-trunk-git-pr #344](https://builds.apache.org/job/kafka-trunk-git-pr/344/) FAILURE
Looks like there's a problem with this pull request
"
137460080,191,asfbot,2015-09-03T14:03:43Z,"[kafka-trunk-git-pr #345](https://builds.apache.org/job/kafka-trunk-git-pr/345/) FAILURE
Looks like there's a problem with this pull request
"
137876636,191,ijuma,2015-09-04T23:16:27Z,"Thanks Harsha, good to see this. In order to review this, it would be good to have a summary of the status of the PR, what areas (if any) you are still working on?
"
137877918,191,harshach,2015-09-04T23:28:55Z,"@ijuma cleaning up the code and adding sasl configs part, more unit tests.
"
137878143,191,ijuma,2015-09-04T23:31:49Z,"Thanks.
"
146770873,191,harshach,2015-10-09T06:44:43Z,"@junrao @ijuma  Please review the updated patch. Thanks.
"
146864969,191,ijuma,2015-10-09T13:03:05Z,"Thanks @harshach. I submitted a PR with your branch as the target: https://github.com/harshach/kafka/pull/1

It merges trunks (and resolves the conflicts) and it includes some fixes and improvements. Please review and integrate if you agree with the changes.

I am still going through the code and I will submit another PR later that will include a SaslConsumerTest.
"
146999246,191,harshach,2015-10-09T22:22:28Z,"@ijuma @junrao updated the pr with consumer test . I'll be updating with docs and other fixes mentioned in the above comments tonight.
"
147425105,191,ijuma,2015-10-12T15:00:44Z,"Here's another PR https://github.com/harshach/kafka/pull/2, which:
- Merges and resolves conflicts from trunk
- Address some of Jun's and Parth's comments
- Make fields final
- Reduce scope of variables where possible
- Remove unused fields and methods
- Fix javadoc
- Fix formatting and naming issues
- Return non-anonymous `KafkaPrincipal` in `SaslClientAuthenticator.principal`

Please review and integrate as appropriate @harshach.

There is still feedback to be addressed as well as more tests (particularly for inter-broker communication and SASL_SSL). I will continue working on these.
"
147777026,191,ijuma,2015-10-13T16:57:10Z,"Thanks for regularly merging my PRs Harsha. Here's another one: https://github.com/harshach/kafka/pull/3
"
147903851,191,junrao,2015-10-14T02:06:33Z,"Also, a general question, if the TGT ticket can't be renewed, do we get exceptions when reading/writing through the SASL port?
"
148224075,191,harshach,2015-10-14T22:41:49Z,"@junrao Yes. It will throw a KafkaException 
"
148396199,191,ijuma,2015-10-15T14:04:28Z,"PR 4 is ready: https://github.com/harshach/kafka/pull/4, it:
- Fixes issues with the LoginManager singleton (we need two instances, one for Mode.SERVER and one for Mode.CLIENT and closing it is not as simple as how it's done in the existing code)
- A number of logging clean-ups (avoid string concat so that we don't pay the cost if that particular level is disabled)
- Remove setConfiguration call as suggested by @rajinisivaram 

Next on my list:
- [x] removeInterestOps issues on server and client authenticator mentioned by Jun
- [x] Refactor tests to reduce duplication, test SASL_SSL, test SASL for inter-broker communication and - fix test failure when all the tests are run via Gradle (I know what the issue is)
- [x] Replace KerberosName system property with config
- [x] Investigate if there's a way to avoid using proprietary classes in `JaasUtils.defaultRealm`

I think we're getting closer.
"
148564929,191,harshach,2015-10-16T01:08:09Z,"@ijuma I see you changed LoginManager to do client and server. But in SASL case we use KafkaServer section for both KafkaServer and any inter broker calls. The reason for this is we want keep KafkaClient section for only the clients and use the keytab in KafkaServer section. Hence the reason we want to initialize the LoginManager eitehr with server or client but not with both. So in broker side we do LoginManager(server) and inside the controller or replica we use inter.broker.security.protocol and use client as mode initiate salsclientauthenticator with LoginManager.subject.
"
148567215,191,ijuma,2015-10-16T01:18:18Z,"@harshach Regarding `LoginManager`, thanks for the explanation, good to know that we want to use `KafkaServer` section for the broker, whether it's a client or a server.

However, if `LoginManager` doesn't support client and server simultaneously, how will it work in situations where you have a client and a server in the same JVM (like in tests, for example)? I think `LoginManager` should support both modes, but we should change how we call it from `SaslChannelBuilder` perhaps. What do you think?
"
148568777,191,harshach,2015-10-16T01:21:49Z,"@ijuma yes makes sense.
"
148571060,191,junrao,2015-10-16T01:34:25Z,"@harshach Is that right? In ControllerChannelManager, ReplicaFetcherThread, and KafkaServer (for controlled shutdown), we create the channel in client mode (instead of server). This seems correct since they initiate client connections. The SocketServer is always created in server mode. So, it seems that the broker will always need to support two modes for LoginManager.
"
148572207,191,ijuma,2015-10-16T01:45:02Z,"@junrao I think the confusion is due to the fact that `Mode` is being misused in `LoginManager`. It actually means `LoginContextType` in that context and it is only used to derive the loginContextName. I intend to change it along these lines so that it's clearer. @harshach, please correct me if I misunderstood.
"
148574665,191,junrao,2015-10-16T01:57:09Z,"Hmm, so all channels in ControllerChannelManager, ReplicaFetcherThread, and KafkaServer should be created in server mode to pick up the KafkaServer section?
"
148575986,191,ijuma,2015-10-16T02:06:16Z,"@junrao Not exactly, I think it's something like this:

https://github.com/ijuma/kafka/commit/c837d5f2b05dab21b3edde3de248f269c9423459

(I just did it quickly to show it in code, still need to double-check it)
"
148593273,191,harshach,2015-10-16T03:49:14Z,"@ijuma your code looks good to me. If you open a PR I can run some tests. Thanks.
"
148683145,191,rajinisivaram,2015-10-16T10:58:18Z,"@harshach I am not sure _""serviceName has always been used in jaas config""_. I can run Zookeeper 3.4.6 with SASL either with the default name `zookeeper` or by specifying the System property `zookeeper.sasl.client.username` to override the name. 

At the moment, using `serviceName` in jaas.conf as the only way to configure the name prevents SASL from being used with IBM JDK. At the very least, we need to set a default. The exceptions with IBM JDK when `serviceName` is set as well as the exception when `serviceName` is not set are below.

If `serviceName` is specified, the exception is:

```
javax.security.auth.login.LoginException: Bad JAAS configuration: unrecognized option: serviceName
    at com.ibm.security.jgss.i18n.I18NException.throwLoginException(I18NException.java:4)
    at com.ibm.security.auth.module.Krb5LoginModule.d(Krb5LoginModule.java:704)
    at com.ibm.security.auth.module.Krb5LoginModule.a(Krb5LoginModule.java:198)
    at com.ibm.security.auth.module.Krb5LoginModule.login(Krb5LoginModule.java:113)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:95)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:56)
    at java.lang.reflect.Method.invoke(Method.java:620)
    at javax.security.auth.login.LoginContext.invoke(LoginContext.java:781)
    at javax.security.auth.login.LoginContext.access$000(LoginContext.java:215)
    at javax.security.auth.login.LoginContext$4.run(LoginContext.java:706)
    at javax.security.auth.login.LoginContext$4.run(LoginContext.java:704)
    at java.security.AccessController.doPrivileged(AccessController.java:452)
    at javax.security.auth.login.LoginContext.invokePriv(LoginContext.java:703)
    at javax.security.auth.login.LoginContext.login(LoginContext.java:609)
    at org.apache.kafka.common.security.kerberos.Login.login(Login.java:298)
    at org.apache.kafka.common.security.kerberos.Login.<init>(Login.java:107)
    at org.apache.kafka.common.security.kerberos.LoginManager.<init>(LoginManager.java:42)
    at org.apache.kafka.common.security.kerberos.LoginManager.getLoginManager(LoginManager.java:65)
    at org.apache.kafka.common.network.SaslChannelBuilder.configure(SaslChannelBuilder.java:49)
```

If `serviceName` is not specified, the exception is:

```
com.ibm.security.krb5.KrbException, status code: 7
    message: Server not found in Kerberos database:null/localhost@EXAMPLE.COM
    at com.ibm.security.krb5.KrbTgsRep.<init>(KrbTgsRep.java:3)
    at com.ibm.security.krb5.KrbTgsReq.getReply(KrbTgsReq.java:155)
    at com.ibm.security.krb5.KrbTgsReq.sendAndGetCreds(KrbTgsReq.java:122)
    at com.ibm.security.krb5.internal.r.e(r.java:92)
    at com.ibm.security.krb5.internal.r.d(r.java:7)
    at com.ibm.security.krb5.Credentials.acquireSvcCreds(Credentials.java:102)
    at com.ibm.security.jgss.mech.krb5.n.a(n.java:195)
    at com.ibm.security.jgss.mech.krb5.n.initSecContext(n.java:375)
    at com.ibm.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:171)
    at com.ibm.security.jgss.GSSContextImpl.initSecContext(GSSContextImpl.java:310)
    at com.ibm.security.sasl.gsskerb.GssKrb5Client.evaluateChallenge(GssKrb5Client.java:174)
    at org.apache.kafka.common.security.authenticator.SaslClientAuthenticator$2.run(SaslClientAuthenticator.java:184)
```
"
148689700,191,ijuma,2015-10-16T11:27:12Z,"I agree that we should do something about this. My preference is to move it to the Kafka config.
"
148702719,191,ijuma,2015-10-16T12:22:14Z,"PR 5: https://github.com/harshach/kafka/pull/5:
- Introduces `LoginType` and simplifies caching code in `LoginManager`
- Make `auth_to_local` configurable via KafkaConfig instead of a system property. I went with the same name as the one Hadoop uses, but it's not the most intuitive so we may want to revise that (it should be a simple change though).
"
148716439,191,ijuma,2015-10-16T13:28:46Z,"I started on the inter-broker tests and things are looking good. SASL_PLAINTEXT and SASL_SSL work fine (the latter required a small fix).
"
148738345,191,harshach,2015-10-16T14:53:37Z,"@rajinisivaram @ijuma ok for serviceName I want to pick default via jaas config and if its not set we will look for it in the sasl configs. Will that work for you?
"
148739210,191,rajinisivaram,2015-10-16T14:57:13Z,"@harshach @ijuma Yes, thank you. That will work for us,
"
148740906,191,ijuma,2015-10-16T15:04:07Z,"I pushed the inter-broker tests for SASL_PLAINTEXT and SASL_SSL to https://github.com/harshach/kafka/pull/5

Working on SASL_SSL tests for producer and consumer now.
"
149203336,191,ijuma,2015-10-19T12:45:55Z,"@harshach, PR 6 is ready https://github.com/harshach/kafka/pull/6:
- Merged trunk and fixed conflicts
- Refactored producer and consumer tests to reduce duplication and to also test SASL_SSL (fixed an important, but small bug with SASL_SSL in the process)
- Fixed issue where interestOps was not being turned off when it should be
- Document `authenticate` in `SaslClientAuthenticator` and `SaslServerAuthenticator`
- Make it possible to configure serviceName via KafkaConfg

`./gradlew test` passed on my laptop.

We need to move fast in order to get this into 0.9.0.0, so I'd appreciate it if you could merge this into your branch so that Jun can review it.
"
149315813,191,junrao,2015-10-19T19:09:49Z,"@harshach : PR #6 from @ijuma looks good to me. Do you want to merge that into your branch? Once you do that, I think we can probably just commit the patch as it is and address other issues, if any, in followup jiras.
"
149746290,191,harshach,2015-10-21T00:52:02Z,"@ijuma can you open a PR against this branch so that I can merge it in
"
149750466,191,ijuma,2015-10-21T01:12:57Z,"Thanks for merging the PR to your branch Harsha. Since Jun merged the SASL PR via #334 (which is exactly the same as this one now), would you mind closing this one please?

Thanks for your work on this. We'll probably need a follow-up to tweak some things and some more tests (particularly ducktape ones) but we're most of the way there.
"
781745956,10070,cmccabe,2021-02-19T01:16:41Z,"> I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2.

Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred?"
782521288,10070,cmccabe,2021-02-20T02:01:21Z,"Thanks, @junrao ! :)"
326210780,3765,onurkaraman,2017-08-31T07:13:02Z,Note that this PR is WIP and only ReplicaStateMachineV2 currently attempts to do retries and error handling. Retries and error handling will be added to the rest of the controller in later rounds of review.
326461993,3765,onurkaraman,2017-09-01T01:20:46Z,@junrao @ijuma can you take a look?
332270742,3765,onurkaraman,2017-09-26T17:17:49Z,"I rebased, resolved merge conflicts, and force pushed the change."
332579208,3765,onurkaraman,2017-09-27T16:29:55Z,retest this please
335763940,3765,onurkaraman,2017-10-11T10:15:50Z,"I just ran an experiment that measures controller failover time before and after this PR.

The environment:
* 5 broker kafka cluster with brokers on different racks
* 5 node zookeeper ensemble with nodes on different racks
* 100,000 single-partition single-replica topics

Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585):
```
[2017-10-11 08:40:08,647] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[2017-10-11 08:41:06,774] INFO [Controller id=2] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 08:43:01,959] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[2017-10-11 08:44:01,901] INFO [Controller id=3] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 08:45:07,792] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[2017-10-11 08:46:08,908] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 08:47:21,205] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[2017-10-11 08:48:22,032] INFO [Controller id=3] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 08:49:51,158] INFO Creating /controller (is it secure? false) (kafka.utils.ZKCheckedEphemeral)
[2017-10-11 08:50:50,342] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
```

KAFKA-5642:
```
[2017-10-11 09:53:00,902] INFO Creating /controller (is it secure? false) (kafka.controller.KafkaControllerZkUtils)
[2017-10-11 09:53:24,904] INFO [Controller id=3] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 09:55:26,649] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-11 09:55:47,725] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 09:56:33,782] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-11 09:56:57,247] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 09:57:34,100] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-11 09:57:59,001] INFO [Controller id=4] Starting the controller scheduler (kafka.controller.KafkaController)

[2017-10-11 09:58:34,398] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-11 09:59:00,559] INFO [Controller id=2] Starting the controller scheduler (kafka.controller.KafkaController)
```

Trunk average controller failover time: ~60 seconds
KAFKA-5642 average controller failover time: ~24 seconds"
336032158,3765,onurkaraman,2017-10-12T06:20:36Z,"I compared the breakdowns of where time was getting spent in both the trunk and KAFKA-5642 runs of the experiment. I don't see any performance regressions in the parts that don't interact with zookeeper.

So the zookeeper parts got better with KAFKA-5642 and the rest stayed the same, which is what we expect."
336194567,3765,onurkaraman,2017-10-12T16:40:18Z,"I just realized that controlled shutdowns will not improve at all with the current PR because controlled shutdown still calls the state machines one partition at a time, so there won't be any zk pipelining.

We can just do a bunch of filters upfront in ControlledShutdown.doControlledShutdown that categorize partitions into the actions to take and then take the actions on the collections we've gathered.

I'll try to update the PR within the next few hours."
336318966,3765,onurkaraman,2017-10-13T00:41:12Z,"I just ran an experiment measuring controlled shutdown time before and after this PR.

The environment:
* 5 broker kafka cluster with brokers on different racks
* 5 node zookeeper ensemble with nodes on different racks
* 25,000 single-partition topics with RF = 2

The replicas were made with the following python script:
```
import itertools
for i, replicas in enumerate(itertools.cycle(itertools.permutations([0, 1, 2, 3, 4], 2))):
  if i >= 25000:
    break
  print """"""create /brokers/topics/t%s {""version"":1,""partitions"":{""0"":[%d,%d]}}"""""" % (i, list(replicas)[0], list(replicas)[1])
```

Special Configs:
```
controlled.shutdown.max.retries=2147483640
controller.socket.timeout.ms=600000
request.timeout.ms=600000
```

Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1:
```
^C[2017-10-12 22:51:50,440] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-12 22:51:50,445] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2017-10-12 22:51:50,447] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-12 22:58:25,850] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-12 22:58:29,755] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
```

Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2:
```
^C[2017-10-13 00:01:24,212] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-13 00:01:24,221] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2017-10-13 00:01:24,223] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-13 00:07:53,837] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-13 00:07:58,630] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
```

KAFKA-5642 run 1:
```
^C[2017-10-13 00:16:55,448] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-13 00:16:55,452] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2017-10-13 00:16:55,453] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-13 00:16:58,260] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-13 00:17:03,016] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
```

KAFKA-5642 run 2:
```
^C[2017-10-13 00:27:34,571] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-13 00:27:34,575] INFO [KafkaServer id=0] shutting down (kafka.server.KafkaServer)
[2017-10-13 00:27:34,576] INFO [KafkaServer id=0] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-13 00:27:37,238] INFO [KafkaServer id=0] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-13 00:27:40,810] INFO [KafkaServer id=0] shut down completed (kafka.server.KafkaServer)
```

Trunk average controlled shutdown time: ~6.5 minutes
KAFKA-5642 average controlled shutdown time: ~3 seconds"
336486274,3765,ijuma,2017-10-13T15:29:55Z,Can we update the PR description to include a summary of the changes done in this PR?
336520938,3765,onurkaraman,2017-10-13T17:43:53Z,"Today, ZookeeperClient and ZkClient couples the data/child change handlers with the actual watcher registration, which means handler registrations actually send out a request to the zookeeper ensemble to do the actual watcher registration.

In `KafkaController.onControllerFailover`, we register partition modification handlers (thereby registering watchers) and additionally lookup the partition assignments for every topic in the cluster. In an offline discussion, @junrao and I realized that we can shave a bit of time off failover if we merge these two operations. There are two ways we can do this:
1. Change ZookeeperClient to register data change watchers with GetDataRequests instead of ExistRequests. This means the handler and watcher registration will also return the znode data.
2. Decouple handler registration from watcher registration in ZookeeperClient. Handler registrations and unregistrations would be a purely local operation. Watcher registrations would be delegated to the user in a following lookup.

Option 1 made us realize a bug in the existing PR: we don't check the return codes of watcher registrations and attempt retries on connectionloss. Option 2 would just reuse the retry logic already in KafkaControllerZkUtils.

I prefer option 2, especially since it makes the ZookeeperClient registration/unregistration apis more consistent in that they would both now be purely in-memory operations."
336531088,3765,junrao,2017-10-13T18:23:42Z,"@onurkaraman : Yes, I agree that option 2 would be better. It would be useful to add a comment to describe the semantic of ZookeeperClient.registerZNodeChangeHandler(). If we do this, we will want to do this consistently with registerZNodeChangeHandler(). 

Also, could you remove WIP from the title of the PR?"
336737463,3765,onurkaraman,2017-10-15T20:03:19Z,"Something that was not factored into the above two experiments was logging inefficiencies.

One major issue with kafka today is its use of log4j synchronous logging instead of log4j2's asynchronous logging. This heavily distorts the experiment results.

If we want to more purely measure the performance change from this patch, I think it'll make sense to rerun the experiments with controller logs set to something like ERROR and above or just disable it entirely."
336750646,3765,onurkaraman,2017-10-15T23:41:42Z,"In an offline discussion, @ijuma suggested I look into determining whether the long controlled shutdown time was actually from the logger writing to disk or simply from creating the log message itself, in which case async logging wouldn't have helped.

To test this, I ran the same controlled shutdown experiment as above for trunk but with:
`log4j.appender.stateChangeAppender=org.apache.log4j.varia.NullAppender`
`log4j.appender.controllerAppender=org.apache.log4j.varia.NullAppender`

NullAppender simply doesn't output the message to a device. Controlled shutdown still took 6+ minutes.

I later reverted the two appenders back to `org.apache.log4j.DailyRollingFileAppender` but this time set the controller and state change log levels to FATAL so that they effectively never log. Controlled shutdown finished in 11 seconds.

This proves that the vast majority of time was getting spent in log message creation."
336792142,3765,onurkaraman,2017-10-16T06:28:22Z,"Alright I reran the controller failover and controlled shutdown experiments but with some logging tweaks to more purely compare trunk's controller using ZkClient with KAFKA-5642's controller using ZookeeperClient. The logging tweaks are the following:
- the state machine loggers set to ERROR
- the state change logger set to DEBUG
- the rest of the controller logger set to INFO

The full log4j.properties is here:
```
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Unspecified loggers and loggers with additivity=true output to server.log and stdout
# Note that INFO only applies to unspecified loggers, the log level of the child logger is used otherwise
log4j.rootLogger=INFO, stdout, kafkaAppender

log4j.appender.stdout=org.apache.log4j.ConsoleAppender
log4j.appender.stdout.layout=org.apache.log4j.PatternLayout
log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n

log4j.appender.kafkaAppender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.kafkaAppender.DatePattern='.'yyyy-MM-dd-HH
log4j.appender.kafkaAppender.File=${kafka.logs.dir}/server.log
log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

log4j.appender.stateChangeAppender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.stateChangeAppender.DatePattern='.'yyyy-MM-dd-HH
log4j.appender.stateChangeAppender.File=${kafka.logs.dir}/state-change.log
log4j.appender.stateChangeAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.stateChangeAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

log4j.appender.requestAppender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.requestAppender.DatePattern='.'yyyy-MM-dd-HH
log4j.appender.requestAppender.File=${kafka.logs.dir}/kafka-request.log
log4j.appender.requestAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.requestAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

log4j.appender.cleanerAppender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.cleanerAppender.DatePattern='.'yyyy-MM-dd-HH
log4j.appender.cleanerAppender.File=${kafka.logs.dir}/log-cleaner.log
log4j.appender.cleanerAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.cleanerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

log4j.appender.controllerAppender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.controllerAppender.DatePattern='.'yyyy-MM-dd-HH
log4j.appender.controllerAppender.File=${kafka.logs.dir}/controller.log
log4j.appender.controllerAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.controllerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

log4j.appender.authorizerAppender=org.apache.log4j.DailyRollingFileAppender
log4j.appender.authorizerAppender.DatePattern='.'yyyy-MM-dd-HH
log4j.appender.authorizerAppender.File=${kafka.logs.dir}/kafka-authorizer.log
log4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout
log4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n

# Change the two lines below to adjust ZK client logging
log4j.logger.org.I0Itec.zkclient.ZkClient=INFO
log4j.logger.org.apache.zookeeper=INFO

# Change the two lines below to adjust the general broker logging level (output to server.log and stdout)
log4j.logger.kafka=INFO
log4j.logger.org.apache.kafka=INFO

# Change to DEBUG or TRACE to enable request logging
log4j.logger.kafka.request.logger=WARN, requestAppender
log4j.additivity.kafka.request.logger=false

# Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE for additional output
# related to the handling of requests
#log4j.logger.kafka.network.Processor=TRACE, requestAppender
#log4j.logger.kafka.server.KafkaApis=TRACE, requestAppender
#log4j.additivity.kafka.server.KafkaApis=false
log4j.logger.kafka.network.RequestChannel$=WARN, requestAppender
log4j.additivity.kafka.network.RequestChannel$=false

log4j.logger.kafka.controller=INFO, controllerAppender
log4j.additivity.kafka.controller=false

log4j.logger.kafka.log.LogCleaner=INFO, cleanerAppender
log4j.additivity.kafka.log.LogCleaner=false

log4j.logger.state.change.logger=DEBUG, stateChangeAppender
log4j.additivity.state.change.logger=false

# Access denials are logged at INFO level, change to DEBUG to also log allowed accesses
log4j.logger.kafka.authorizer.logger=INFO, authorizerAppender
log4j.additivity.kafka.authorizer.logger=false
log4j.logger.kafka.controller.PartitionStateMachine=ERROR, controllerAppender
log4j.additivity.kafka.controller.PartitionStateMachine=false
log4j.logger.kafka.controller.ReplicaStateMachine=ERROR, controllerAppender
log4j.additivity.kafka.controller.ReplicaStateMachine=false
```
**Controller Failover**
The environment:
- 5 broker kafka cluster with brokers on different racks
- 5 node zookeeper ensemble with nodes on different racks
- 100,000 single-partition single-replica topics

Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1:
```
[2017-10-16 03:15:57,212] INFO [Controller id=2] Reading controller epoch from zookeeper (kafka.controller.KafkaController)
[2017-10-16 03:17:02,119] INFO [Controller id=2] Starting the controller scheduler (kafka.controller.KafkaController)
```
Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2:
```
[2017-10-16 04:29:55,205] INFO [Controller id=3] Reading controller epoch from zookeeper (kafka.controller.KafkaController)
[2017-10-16 04:30:57,703] INFO [Controller id=3] Starting the controller scheduler (kafka.controller.KafkaController)
```
KAFKA-5642 run 1:
```
[2017-10-16 05:41:07,374] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-16 05:41:24,507] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
```
KAFKA-5642 run 2:
```
[2017-10-16 05:42:37,788] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-16 05:42:53,911] INFO [Controller id=3] Starting the controller scheduler (kafka.controller.KafkaController)
```
Trunk average controller failover time: ~63 seconds
KAFKA-5642 average controller failover time: ~17 seconds

**Controlled Shutdown**
The environment:
- 5 broker kafka cluster with brokers on different racks
- 5 node zookeeper ensemble with nodes on different racks
- 25,000 single-partition topics with RF = 2

The replicas were made with the following python script:
```
import itertools
for i, replicas in enumerate(itertools.cycle(itertools.permutations([0, 1, 2, 3, 4], 2))):
  if i >= 25000:
    break
  print """"""create /brokers/topics/t%s {""version"":1,""partitions"":{""0"":[%d,%d]}}"""""" % (i, list(replicas)[0], list(replicas)[1])
```
Special Configs:
```
controlled.shutdown.max.retries=2147483640
controller.socket.timeout.ms=600000
request.timeout.ms=600000
```

Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1:
```
^C[2017-10-16 05:10:18,244] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-16 05:10:18,249] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
[2017-10-16 05:10:18,250] INFO [KafkaServer id=1] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-16 05:10:50,253] INFO [KafkaServer id=1] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-16 05:10:55,542] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)
```
Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2:
```
^C[2017-10-16 05:19:27,620] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-16 05:19:27,635] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
[2017-10-16 05:19:27,636] INFO [KafkaServer id=1] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-16 05:19:39,766] INFO [KafkaServer id=1] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-16 05:19:45,111] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)
```
KAFKA-5642 run 1:
```
^C[2017-10-16 05:56:31,517] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-16 05:56:31,522] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
[2017-10-16 05:56:31,523] INFO [KafkaServer id=1] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-16 05:56:34,092] INFO [KafkaServer id=1] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-16 05:56:38,144] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)
```
KAFKA-5642 run 2:
```
^C[2017-10-16 06:02:21,831] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-16 06:02:21,837] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
[2017-10-16 06:02:21,838] INFO [KafkaServer id=1] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-16 06:02:24,457] INFO [KafkaServer id=1] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-16 06:02:28,815] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)
```
Trunk average controlled shutdown time: ~22 seconds
KAFKA-5642 average controlled shutdown time: ~3 seconds"
336801412,3765,onurkaraman,2017-10-16T07:22:17Z,"I went back and looked into why logging resulted in distorted results in the controlled shutdown experiment for trunk: 6.5 minutes with logging vs 22 seconds without logging.

It's due to this line in PartitionStateMachine.electLeaderForPartition:
```
debug(s""After leader election, leader cache is updated to ${controllerContext.partitionLeadershipInfo}"")
```

I reran the experiment with the original log levels (controller and state change loggers at TRACE, no special state machine loggers) but removed the above debug log statement. This brings the controlled shutdown times back down to below 20 seconds:
```
^C[2017-10-16 07:05:36,688] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-16 07:05:36,691] INFO [KafkaServer id=2] shutting down (kafka.server.KafkaServer)
[2017-10-16 07:05:36,693] INFO [KafkaServer id=2] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-16 07:05:52,145] INFO [KafkaServer id=2] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-16 07:05:57,195] INFO [KafkaServer id=2] shut down completed (kafka.server.KafkaServer)
```

The reason why this one log line is so problematic is because for each partition undergoing election, it logs every partition state in the cluster. So you end up getting O(N^2) log behavior where N is the total number of partitions in the cluster.

Rather than log every partition's state per partition undergoing election, it would be sufficient to just log that single partition's state:
```
debug(s""After leader election, leader cache for $topicAndPartition is updated to ${controllerContext.partitionLeadershipInfo(topicAndPartition)}"")
```"
337302275,3765,onurkaraman,2017-10-17T17:15:15Z,"By the way, I reran the controlled shutdown test with the latest changes that adds state change logs for successful state machine transitions and still got around 3 seconds for controlled shutdown:
```
^C[2017-10-17 06:30:01,541] INFO Terminating process due to signal SIGINT (kafka.Kafka$)
[2017-10-17 06:30:01,561] INFO [KafkaServer id=1] shutting down (kafka.server.KafkaServer)
[2017-10-17 06:30:01,563] INFO [KafkaServer id=1] Starting controlled shutdown (kafka.server.KafkaServer)
[2017-10-17 06:30:04,886] INFO [KafkaServer id=1] Controlled shutdown succeeded (kafka.server.KafkaServer)
[2017-10-17 06:30:09,702] INFO [KafkaServer id=1] shut down completed (kafka.server.KafkaServer)
```"
337373454,3765,junrao,2017-10-17T21:13:18Z,@onurkaraman : Thanks for the latest patch. LGTM. Running all system tests now.
337373595,3765,onurkaraman,2017-10-17T21:13:41Z,Rebased against trunk and squashed all commits.
337513329,3765,onurkaraman,2017-10-18T09:00:25Z,"Looks like the system tests passed:
http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2017-10-17--001.1508316204--onurkaraman--KAFKA-5642--7057697/report.html

I also went ahead and ran a modified version of the controller failover test with the same overall partition counts, but with more partitions-per-topic and fewer topics.

The environment:
- 5 broker kafka cluster with brokers on different racks
- 5 node zookeeper ensemble with nodes on different racks
- 2,000 topics each with 50 partitions and RF = 1

Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1:
```
[2017-10-18 08:12:53,593] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)
[2017-10-18 08:13:22,822] INFO [Controller id=3] Starting the controller scheduler (kafka.controller.KafkaController)
```
Trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2:
```
[2017-10-18 08:14:18,344] INFO Result of znode creation is: OK (kafka.utils.ZKCheckedEphemeral)
[2017-10-18 08:14:46,074] INFO [Controller id=2] Starting the controller scheduler (kafka.controller.KafkaController)
```
KAFKA-5642 run 1:
```
[2017-10-18 08:31:08,318] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-18 08:31:22,736] INFO [Controller id=1] Starting the controller scheduler (kafka.controller.KafkaController)
```
KAFKA-5642 run 2:
```
[2017-10-18 08:33:56,894] INFO Result of znode creation at /controller is: OK (kafka.controller.KafkaControllerZkUtils)
[2017-10-18 08:34:10,928] INFO [Controller id=0] Starting the controller scheduler (kafka.controller.KafkaController)
```
Trunk average controller failover time: ~28 seconds
KAFKA-5642 average controller failover time: ~14 seconds"
337644430,3765,junrao,2017-10-18T16:13:01Z,@onurkaraman : Thanks a lot for working on this patiently! LGTM
341092491,3765,astubbs,2017-11-01T12:26:22Z,"Hi @onurkaraman, great work! Any chance you could give us some insight into the machines you used for the tests? How were they spec'd and the order of magnitude network latency?"
1428304503,13240,Hangleton,2023-02-13T17:01:57Z,"Hello David (@dajac), still working on this but opening a draft if you wish to start reviewing at your convenience."
1428396257,13240,dajac,2023-02-13T17:50:49Z,@Hangleton Thanks. I will take a look later this week.
1429942230,13240,Hangleton,2023-02-14T15:34:01Z,"Thanks for the review, David. I am working on adding unit tests for `OffsetCommitResponse` and the server-side handling of the request/response, and fix the bugs you have identified."
1437297790,13240,Hangleton,2023-02-20T16:42:13Z,"Hello David, I updated the PR to take into account your comments and have been adding tests."
1442380719,13240,Hangleton,2023-02-23T20:21:29Z,"Many thanks David for the review. I updated the PR to cover all the points you raised. I still have more unit tests to add for the broker-side code paths, need to check if any further change/test is required on the admin client side. Will update the PR with those. Thanks!"
1443590758,13240,Hangleton,2023-02-24T12:03:59Z,"- Added a test for `INVALID_REQUEST` in case of definition of topic id and name for a topic in the `OffsetCommit` request.
- Added a test exercising a valid `OffsetCommit` request across all schema version. We can see that the topic id and name is propagated to the group coordinator in all cases."
1446539881,13240,dajac,2023-02-27T15:28:10Z,"@Hangleton Thanks for the update. I will take a look shortly. In the meantime, could you already add a few integration test for the OffsetCommitRequest like we have for the OffsetFetchRequest in kafka.server.OffsetFetchRequestTest? I believe that topic ids are actually lost when they are passed to the group coordinator. Hence, I request with topic ids will very likely return a response with topic names instead of topic ids. The issue is that we don't catch those kind of issues with the existing tests because the logic in the consumer supports both ways."
1447864999,13240,Hangleton,2023-02-28T09:41:19Z,Thanks for the review David. Updating the PR to enforce the exclusive use of topic IDs from version 9 and adding the integration test you mentioned. Thanks for the guidance!
1449920535,13240,Hangleton,2023-03-01T11:16:38Z,"Hello David, thanks for the fast review. Apologies for being slow, I hadn't finished the previous revision. Will include your comments. Working on it right now. Thanks!"
1449935049,13240,dajac,2023-03-01T11:23:38Z,"> Hello David, thanks for the fast review. Apologies for being slow, I hadn't finished the previous revision. Will include your comments. Working on it right now. Thanks!

No worries. You are not slow. I noticed a few new commits so I had a quick look at them."
1449936916,13240,dajac,2023-03-01T11:24:33Z,"Sorry, used the wrong button..."
1452495333,13240,Hangleton,2023-03-02T20:21:17Z,"Thanks David for the review, have a few more tests to add but this should be eligible to another pass."
1453452130,13240,Hangleton,2023-03-03T12:20:21Z,The integration test `MirrorConnectorsIntegrationBaseTest` is failing due to unknown topic id detected when altering committed offsets with the admin client. This is because the `OffsetCommitRequest` generated by the admin client is using version 9 but should be 8 as discussed. This should have been captured by the unit tests on the admin client. This exhibits a gap in the test coverage of this PR.
1453619279,13240,Hangleton,2023-03-03T14:32:23Z,"Used v8 for AlterConsumerGroupOffsets in the admin client and added corresponding unit and integration tests. `MirrorConnectorsIntegrationBaseTest` is now successful.

Also just realized I need to update the `AuthorizerIntegrationTest`. The reason is the tests are committing offsets from a consumer without having the topic subscribed to, and without the topic id in the client metadata cache."
1454676488,13240,Hangleton,2023-03-04T09:27:52Z,"Hello David, found a case where the use of OffsetCommit requests version 9 from the consumer resulted in an error in the ACL authorization tests. The reason is that when the commit offsets are performed by the tests to exercise ACLs, the topic IDs are not yet present in the consumer metadata cache, in this case because the consumer hasn't subscribed to the topics or be assigned any of their partitions. As a result, the OffsetCommitRequest version 9 was sent with zero topic ids.

In order to avoid this, we could:

1. Enforce a metadata update for any topic in the offsets to commit, using the transient topics list exposed by the `ConsumerMetadata` and used for instance by the `OffsetFetcher` to fetch offsets for out-of-band topic-partitions which aren't part of any subscription. This approach however adds complexity to the offset commit implementation in the consumer and only address this specific use case. There could be other cases where metadata hasn't converged yet and the topic id would not be available to the consumer.

2. Adds a condition on the support of topic ids when constructing the `OffsetCommitRequest`. This is the approach used when constructing the `Fetch` request. The advantage of the approach is that it provides the invariant that any OffsetCommitRequest version >= 9 have valid (non-zero) topic ids in it. One downside is that if a bug in the consumer makes a topic id unavailable, the consumer will keep using version <= 8 permanently and silently, while we would want to know about it as implementors to address any potential gap.

The second approach is currently implemented in the PR. Happy to discuss more about it."
1454803355,13240,dajac,2023-03-04T16:57:01Z,I think that we have to do 2) anyway because topic ids may not be enabled on the server side.
1455667030,13240,Hangleton,2023-03-06T08:09:12Z,Took a look at the authorizer tests (`AuthorizerIntegrationTests`). It seems that authorization with topic and group `READ` permissions and unknown topic name is not currently being tested. We could add this use case and extend it for topic ids in a separate PR? (ref: [KAFKA-14779](https://issues.apache.org/jira/browse/KAFKA-14779))
1459774076,13240,Hangleton,2023-03-08T08:53:47Z,"Many thanks, David, for the review. Working on fixing the PR now. I will send the corrective commits by EOD. Thanks."
1460209520,13240,Hangleton,2023-03-08T14:07:47Z,"Hi David, thanks for the review. I addressed all your comments and updated the PR. ~I am just adding a test to add coverage on addition of topic ids in the response generated by the coordinator.~

_Edit: added said test._"
1469825143,13240,dajac,2023-03-15T11:21:23Z,@Hangleton I just merge https://github.com/apache/kafka/pull/13378. We can update this PR now.
1469956438,13240,Hangleton,2023-03-15T12:54:44Z,"Many thanks David. I will try to get to this in the next couple of days. Apologies for the delay, I wish i could get to this sooner."
1485037330,13240,Hangleton,2023-03-27T12:12:56Z,"Hello David (@dajac), this PR has been updated and is ready for review. Thanks!"
1505120343,13240,dajac,2023-04-12T11:38:40Z,@Hangleton There are a few conflicts. Could you please rebase the PR? I plan to make another pass on it afterwards.
1505154651,13240,Hangleton,2023-04-12T12:06:35Z,"> @Hangleton There are a few conflicts. Could you please rebase the PR? I plan to make another pass on it afterwards.

Sure, done. Thanks!"
1511110416,13240,Hangleton,2023-04-17T10:44:05Z,"> Hey @Hangleton. I just got back to this PR. I made a pass over the files in `core` and I left some comments. As a general ask, it would be great if we could keep avoid large refactoring in tests in this PR as they are very distracting. I am not against refactoring but I would do them in separate PRs.

Hi David, thanks for the review. Understand about refactoring, I will try to see if I can revert some of them if possible."
1553270438,13240,clolov,2023-05-18T15:57:56Z,"Heya @dajac! I hope I have addressed your comments on all files except `ConsumerCoordinatorTest` and the `OffsetCommitRequestTest`, could you review everything except those two and confirm whether this is the case? As far as I understand your general concerns with `ConsumerCoordinatorTest` and `OffsetCommitRequestTest` is that those tests are either not parameterised in a simple way or they are not parameterised at all - am I correct? For the ones which are not parameterised simply I cannot think of an easier approach - the setup is just long-winded, but the main idea behind the arguments is https://github.com/apache/kafka/pull/13240/files#diff-964e16515361b7d22acbad4795f13abe6af513be9588952bc6427aa7ce00938dR2875-R2880. I guess we can split them into separate tests were we vary just one of the arguments rather than all of them if that's what you mean? The ones which are not parameterised at all I believe are not parameterised because the same functions are used for tests which only test behaviour in versions >= 9. I am happy to implement any suggestions you might have to improve on what's already there."
1553290938,13240,Hangleton,2023-05-18T16:13:11Z,"Thanks Christo (@clolov) for your help on the PR, I will take a look at the changes tomorrow. Thanks!"
1562458362,13240,Hangleton,2023-05-25T08:00:45Z,"Hi David (@dajac), thanks for the review and apologies for the delayed reply. Thanks to Christo's help, I believe most of your comments have been addressed. I have one question regarding the behaviour of the offset commit consumer API that you identified [here](https://github.com/apache/kafka/pull/13240#discussion_r1168735466). Thanks!"
1571989946,13240,Hangleton,2023-06-01T12:47:18Z,"Hello David (@dajac), I was discussing this with Christo today as part of his work on the OffsetFetch API. Would you like this PR on OffsetCommit to be split to make the review easier and reduce risks?"
1741511578,13240,dajac,2023-09-29T21:32:07Z,Closing this PR for now as the topic id work will be done later. We can re-open it when we resume the work.
260852086,2140,hachikuji,2016-11-16T04:36:09Z,"ping @junrao @guozhangwang @ijuma @apurvam 

This patch modifies the server implementation to use the client-side `Record` objects for all internal processing. As you can see, this was a hefty bit of work, but fortunately most of the transformations are straightforward. The main thing to focus on is the implementation of `LogValidator`, which contains the offset assignment and record validation logic that was previously contained in `ByteBufferMessageSet`. I've been pretty careful to preserve the optimizations that were present previously (e.g. in-place assignment where possible), but don't take my word for it.

One quick note on naming. I've renamed the `Records` object and subclasses to `LogBuffer`. So `MemoryRecords` is now `MemoryLogBuffer`. The reason for this change was that it felt unintuitive for an instance of `Records` to be an `Iterable<LogEntry>`, with the `LogEntry` instances being the actual container for the records. A `LogBuffer` instead represents a range of the log and provides access to the log entries contained in it. That seemed more intuitive to me, but let me know if you agree or if you have other suggestions.
"
260854848,2140,onurkaraman,2016-11-16T05:02:37Z,"Slightly related, slightly tangential: is there a specific reason why we put the new broker-specific java classes under clients/ ?

I'm talking about stuff like:
FileRecords
LeaderAndIsrRequest / LeaderAndIsrResponse
StopReplicaRequest / StopReplicaResponse
UpdateMetadataRequest / UpdateMetadataResponse
"
260856355,2140,hachikuji,2016-11-16T05:15:56Z,"@onurkaraman Yeah, I've wondered a bit about that also. I'd be OK moving `FileRecords` to the server if people prefer that. I was thinking one potential benefit is that it opens the door to adding persistence to the client, which some users have requested (we have an ancient JIRA for this, but the use case might not be too compelling). In the end, I decided it wasn't that much code, so having it in clients didn't hurt too much and it kept all record-related stuff close together, which may make it easier to share common pieces.
"
260979285,2140,ijuma,2016-11-16T15:43:02Z,"@hachikuji, thanks for tackling this.

About the naming question, I also found it a bit confusing how we sometimes have an offset and sometimes don't when talking about records. That is, `MemoryRecords` includes the offset (and record size in the underlying buffer) for each record while `Record` does not. It all becomes clearer when one realises that `MemoryRecords` (renamed to `MemoryLogBuffer` in the PR) actually contains `LogEntry` instances, each being a pair of `offset` and `Record`.

One thing to think about is whether this fits with the other `Record` classes we have and whether that matters (maybe it doesn't). For example, `ConsumerRecord` contains the `offset` while `ProducerRecord` does not. Also, it would have been a bit easier to review if the rename had been done in a separate PR, but probably too late for that. :)

About having the classes in `clients`, I think that's OK as they are in an internal `common` package.
"
261073297,2140,hachikuji,2016-11-16T21:11:59Z,"@becketqin Would be nice to get your feedback also. Put on the coffee and lose yourself in code review!
"
261693933,2140,becketqin,2016-11-19T05:01:19Z,"Wow, a 5000 line change... I'll take a look this weekend...
"
261860811,2140,becketqin,2016-11-21T07:13:06Z,"@hachikuji I ran into some other stuff today and didn't finish reading the entire patch. Just some thoughts when I was reading the code. I think ""Message"" (and ""Record"" in the new clients) are a well established concept for Kafka. It is indeed a little weird that `Records` is an `Iterable<LogEntry>`, but I felt changing all the `Reocrds` to `LogBuffer` seems introducing a new concept (BTW `FileLogBuffer` sounds a little weird given it actually does not have a buffer). I would like to see if we can avoid solving the confusion by adding a new concept.

Not sure if there was any thinking on changing `LogEntry` to something like `LogRecord` which indicate it's something resides in the log? Then `Records` would contain a few `LogRecord` which contains (`Offset` + `Record`). We also have a pretty symmetric naming for `ProducerRecord`, `ConsumerRecord` and `LogRecord` clearly indicating where they are used. We can also consider renaming `Records` to `LogRecords` to make it clear. I am not sure if `Records` counts as a public interface or not, though. I know we have a page stating which packages are public and which are not, but I doubt if people follows that given we have an explicit `internals` package...

I'll save my other comments until I go through a full pass of the code in case some of them are not valid at all (I already found some...)

"
261887880,2140,ijuma,2016-11-21T09:32:06Z,"@becketqin, that's a fair point about the rename and introducing a new concept. I have similar concerns and was wondering how we could make things clearer without that. Your suggestion looks promising."
262029927,2140,onurkaraman,2016-11-21T18:48:03Z,Does it make sense to separate the renaming from the actual task of this patch?
262033620,2140,hachikuji,2016-11-21T19:01:08Z,"@becketqin Thanks for taking a look. I'm not sure I follow why you consider the renaming a conceptual change. The object works the same as before, but I felt the name fit closer to what the object actually represents, which is a range of bytes from the log. The name `Records` to me just suggests a container for `Record` objects.

The suggestion about `LogRecord` makes sense to me. I have actually done something similar in work building off of this patch. At the same time, I would like to preserve a concept of `LogEntry` as a container for records which sits between `LogBuffer` (or `Records`) and `LogRecord` (or `Record`). The basic idea is to treat the shallow messages as log entries, and the deep messages as log records (an uncompressed message is treated as a log entry containing just a single log record). 

To give a bit more background, we're trying to generalize the concept of a message set so that 1) it uses a separate schema from individual messages, and 2) it's extended to uncompressed data. This allows us to amortize the cost of additional metadata which is invariant for the messages contained in a message set. I'm happy to provide some additional detail if you're interested (there will be a KIP on the way some time in the next few weeks).

@onurkaraman Yeah, we can do that if it makes this patch easier to get in. Let's see what others think. Sigh, any suggestion to reduce lines of code is likely to be popular to all except me.
"
262306541,2140,hachikuji,2016-11-22T17:25:40Z,"I ran system tests on the latest patch and everything looks good: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2016-11-22--001.1479824556--hachikuji--KAFKA4390--24dc7ed/report.html. I will probably continue to add some additional test cases, but I'll leave the rest as is pending further review comments."
262421264,2140,junrao,2016-11-23T02:21:40Z,@hachikuji : Will also take a look at the patch. Just a quick note. Could you do some performance test to make sure there is no regression?
262422183,2140,hachikuji,2016-11-23T02:30:55Z,@junrao Thanks for taking a look. Performance testing is next on my plate after I fill in some of the gaps in test coverage.
263670626,2140,hachikuji,2016-11-29T19:22:00Z,Update: I've begun performance testing. I'm seeing a substantial performance degradation on the consumer side. I'll update this PR when I know more.
263687962,2140,hachikuji,2016-11-29T20:24:27Z,"I found the cause of the performance regression. When handling a fetch, we must read through the log to find the starting position of a given offset (starting from the position given by the index). To do so, we only need to read the offset and size, but one of my recent commits accidentally changed this behavior to unnecessarily read the full record. I've fixed this in the last commit and now it looks like performance is within 5% of trunk for the producer and consumer. Perhaps still on the slower side though, so I'll continue investigating."
264081910,2140,hachikuji,2016-12-01T05:24:09Z,"@junrao I really appreciate the thorough review. I've addressed the easier items and left a few replies. I'll get to the rest tomorrow.

By the way, in the near future, I'd like to squash commits to make rebasing a bit easier. It hasn't been too much of a problem yet, but it will get harder with more iterations."
264770009,2140,guozhangwang,2016-12-05T05:17:02Z,"About the naming of `Records` to `LogBuffer`, I share the same concern with @becketqin and @ijuma . My proposal would be to rename `LogEntry` to `RecordEntry` or simply `RecordAndOffset` (seems more Scala-ish), and to me it is OK to have `Records.iterator()` return `Iterator<RecordEntry>`. As for `ConsumerRecord` and `ProducerRecord`, it would be best if them both contain a `Record` field, and then `ConsumerRecord` to contain a separate `offset` field, but since it is public APIs we have to leave it as is, which is not too bad to me.

Also, I'd like to suggest we separate the renaming out of this PR for the ease of reviewing, if it is still possible to revert it back."
264923518,2140,hachikuji,2016-12-05T17:46:28Z,"@guozhangwang `RecordEntry` would work for me. Keep in mind that if KIP-98 is approved, it will do more than just track the offset, so I'd rather not use something specific like `RecordAndOffset`. Since my `LogBuffer` suggestion is not too popular, I'll go ahead and revert that change. Then the hierarchy will be `Records` -> `RecordEntry` -> `Record`. Does that sound reasonable?"
264983825,2140,hachikuji,2016-12-05T21:32:29Z,I've gone ahead and squashed commits. You can still find the old commit history here: https://github.com/hachikuji/kafka/tree/KAFKA-4390-UNSQUASHED.
265818436,2140,hachikuji,2016-12-08T18:38:55Z,FYI: here's a system test run from last night: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2016-12-08--001.1481199431--hachikuji--KAFKA4390--788f1bd/report.txt. There was one failure. I'll investigate and report back what I find.
265829314,2140,asfbot,2016-12-08T19:20:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/12/
Test PASSed (JDK 8 and Scala 2.11).
"
265830888,2140,asfbot,2016-12-08T19:26:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/10/
Test PASSed (JDK 7 and Scala 2.10).
"
265877527,2140,asfbot,2016-12-08T22:40:38Z,"Build finished. 3459 tests run, 0 skipped, 0 failed.
Test FAILed (JDK 8 and Scala 2.12).
"
265877530,2140,asfbot,2016-12-08T22:40:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/11/
Test FAILed (JDK 8 and Scala 2.12).
"
266518210,2140,asfbot,2016-12-12T18:55:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/81/
Test PASSed (JDK 8 and Scala 2.11).
"
266538764,2140,hachikuji,2016-12-12T20:10:32Z,"@junrao Latest round of comments addressed. Please take a look.

@ijuma On the question of `Iterator` vs `Iterable`, I'm pretty open. I used the former for consistency with the old code, but I agree it would be nice to be able to use the ""foreach"" syntax."
266547545,2140,asfbot,2016-12-12T20:45:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/86/
Test PASSed (JDK 8 and Scala 2.12).
"
266565058,2140,asfbot,2016-12-12T21:54:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/85/
Test FAILed (JDK 7 and Scala 2.10).
"
266582740,2140,asfbot,2016-12-12T23:07:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/87/
Test FAILed (JDK 8 and Scala 2.11).
"
266607505,2140,asfbot,2016-12-13T01:24:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/92/
Test PASSed (JDK 8 and Scala 2.11).
"
266607592,2140,asfbot,2016-12-13T01:25:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/91/
Test PASSed (JDK 8 and Scala 2.12).
"
266608699,2140,asfbot,2016-12-13T01:32:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/90/
Test PASSed (JDK 7 and Scala 2.10).
"
266684166,2140,hachikuji,2016-12-13T09:06:53Z,"@junrao Thanks for the reviews. I did some testing this evening. I thought I was seeing some performance difference initially in the producer, but it seems within the variance of the test runs. If I were guessing from the results, I'd say the non-compressed path is a tad slower while the compressed path might be a tad faster, but don't put much weight behind either conclusion. In any case, the results seem close enough that I'd recommend merging now. Note that I did add one commit to address a couple minor cleanups and tighten up the iteration code a little."
266703020,2140,asfbot,2016-12-13T10:30:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/96/
Test FAILed (JDK 7 and Scala 2.10).
"
266720210,2140,asfbot,2016-12-13T11:56:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/97/
Test FAILed (JDK 8 and Scala 2.12).
"
266720269,2140,asfbot,2016-12-13T11:56:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/98/
Test FAILed (JDK 8 and Scala 2.11).
"
266823888,2140,asfbot,2016-12-13T18:41:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/106/
Test PASSed (JDK 8 and Scala 2.11).
"
266824121,2140,asfbot,2016-12-13T18:41:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/104/
Test PASSed (JDK 7 and Scala 2.10).
"
266836868,2140,asfbot,2016-12-13T19:28:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/105/
Test PASSed (JDK 8 and Scala 2.12).
"
384162821,4830,lindong28,2018-04-25T05:13:22Z,@becketqin @junrao the patch LGTM. Would you have time to review this patch? Thanks much!
384163802,4830,jonlee2,2018-04-25T05:20:30Z,"Thank you very much for the detailed review, @lindong28. I talked to both @becketqin and @junrao and they are not available for review at the moment. I've asked @rajinisivaram if she can review. "
386722748,4830,jonlee2,2018-05-04T20:22:38Z,"The lastest commit includes the following fixes:
1. Bump up all request and response protocol versions
2. Add a method called shouldClientThrottling() to AbstractResponse to tell the client whether or not it should throttle based on the received response version.
3. Fix NetworkClient to do throttling based on the response version using the new method.
4. Add the THROTTLE_TIME_MS field to the following cluster actions that may be throttled: LeaderAndIsrResponse, UpdateMetadataResponse, SaslAuthenticateResponse, SaslHandshakeResponse
5. Update KafkaApis to pass throttleTimeMs to above responses

@lindong28, @rajinisivaram Can you take a look? Thank you. "
386752308,4830,rajinisivaram,2018-05-04T22:37:05Z,"Hi Jon,

Thanks for the updates. I will take a look at the PR early next week. We
were deliberately avoiding throttling LeaderAndIsrResponse,
UpdateMetadataResponse, SaslAuthenticateResponse and SaslHandshakeResponse.
And I don't think we discussed throttling these as part of the KIP. Can you
explain why the change was required? If we are changing request format, it
ought to be added to the DISCUSS thread and the KIP.


On Fri, May 4, 2018 at 9:23 PM, Jon Lee <notifications@github.com> wrote:

> The lastest commit includes the following fixes:
>
>    1. Bump up all request and response protocol versions
>    2. Add a method called shouldClientThrottling() to AbstractResponse to
>    tell the client whether or not it should throttle based on the received
>    response version.
>    3. Fix NetworkClient to do throttling based on the response version
>    using the new method.
>    4. Add the THROTTLE_TIME_MS field to the following cluster actions
>    that may be throttled: LeaderAndIsrResponse, UpdateMetadataResponse,
>    SaslAuthenticateResponse, SaslHandshakeResponse
>    5. Update KafkaApis to pass throttleTimeMs to above responses
>
> @lindong28 <https://github.com/lindong28>, @rajinisivaram
> <https://github.com/rajinisivaram> Can you take a look? Thank you.
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/kafka/pull/4830#issuecomment-386722748>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMjeKnyQmFIhloAq4xvYEf_FA3s81LRmks5tvLikgaJpZM4TJS90>
> .
>
"
386759605,4830,jonlee2,2018-05-04T23:29:10Z,"Hi Rajini,


Thanks for the comment.


The reason why I added THROTTLE_TIME_MS to LeaderAndIsrResponse, UpdateMetadataResponse, SaslAuthenticateResponse and SaslHandshakeResponse was that these responses are actually throttled on errors (for request quotas). And when they are throttled, I want throttle time to be passed back so that client can refrain from sending more requests to the broker until the throttle time is over.


For non-error cases, they won't be throttled, so there are no behavior changes there (except that the response will have zero throttle time).


I'll mention this change to the discussion thread. Meanwhile, please go ahead and review the changes. The response format changes are well contained and shouldn't block you from reviewing other changes. If we decide to take it back, it shouldn't be much work.


Thanks,

Jon




________________________________
From: Rajini Sivaram <notifications@github.com>
Sent: Friday, May 4, 2018 3:37:25 PM
To: apache/kafka
Cc: Jon Lee; Mention
Subject: Re: [apache/kafka] KAFKA-6028: Improve the quota throttle communication (KIP-219) (#4830)

Hi Jon,

Thanks for the updates. I will take a look at the PR early next week. We
were deliberately avoiding throttling LeaderAndIsrResponse,
UpdateMetadataResponse, SaslAuthenticateResponse and SaslHandshakeResponse.
And I don't think we discussed throttling these as part of the KIP. Can you
explain why the change was required? If we are changing request format, it
ought to be added to the DISCUSS thread and the KIP.


On Fri, May 4, 2018 at 9:23 PM, Jon Lee <notifications@github.com> wrote:

> The lastest commit includes the following fixes:
>
> 1. Bump up all request and response protocol versions
> 2. Add a method called shouldClientThrottling() to AbstractResponse to
> tell the client whether or not it should throttle based on the received
> response version.
> 3. Fix NetworkClient to do throttling based on the response version
> using the new method.
> 4. Add the THROTTLE_TIME_MS field to the following cluster actions
> that may be throttled: LeaderAndIsrResponse, UpdateMetadataResponse,
> SaslAuthenticateResponse, SaslHandshakeResponse
> 5. Update KafkaApis to pass throttleTimeMs to above responses
>
> @lindong28 <https://github.com/lindong28>, @rajinisivaram
> <https://github.com/rajinisivaram> Can you take a look? Thank you.
>
> 
> You are receiving this because you were mentioned.
> Reply to this email directly, view it on GitHub
> <https://github.com/apache/kafka/pull/4830#issuecomment-386722748>, or mute
> the thread
> <https://github.com/notifications/unsubscribe-auth/AMjeKnyQmFIhloAq4xvYEf_FA3s81LRmks5tvLikgaJpZM4TJS90>
> .
>


You are receiving this because you were mentioned.
Reply to this email directly, view it on GitHub<https://github.com/apache/kafka/pull/4830#issuecomment-386752308>, or mute the thread<https://github.com/notifications/unsubscribe-auth/Ah7_JvYA2GGJrCTSoKE4C9Dz8_sYv955ks5tvNglgaJpZM4TJS90>.
"
389022296,4830,jonlee2,2018-05-15T02:24:44Z,"Thanks for the review, @rajinisivaram. I added some questions to your first comment. Please let me know what you think."
389401521,4830,jonlee2,2018-05-16T05:45:31Z,"@rajinisivaram @lindong28 BTW, let me know if you want me to squash multiple commits into one. I didn't do that to keep the entire history, but I think I'll need to do that anyway at some point before submission."
389444225,4830,rajinisivaram,2018-05-16T08:48:44Z,"@jonlee2 Thanks for the updates. It is looking good. You don't need to squash the commits, they get squashed when the PR is merged. So the PR can keep the commits to maintain history.

Since it is a large PR, it will be good to do one full review before committing it. @lindong28 I was expecting that you would do the final review and commit this PR. Let me know if that is not the case. Thanks."
389447800,4830,lindong28,2018-05-16T09:00:48Z,"Thanks for the review @rajinisivaram! Yes, I will do one full review before committing this."
390454844,4830,lindong28,2018-05-20T03:25:06Z,@rajinisivaram Is there a public Jenkins service that we can use to run system tests for this patch?
390694484,4830,jonlee2,2018-05-21T15:46:23Z,"@rajinisivaram 

Thanks for running the tests. For some reason, however, I cannot access the link. I got an 404 error, saying ""Problem accessing /job/system-test-kafka-branch-builder/1754. Reason: Not Found"". Do I need a permission or anything else to access it?

Also, what are these system tests and where can I find them? I may need to change these tests as well since the throttling behavior has now changed."
390722724,4830,lindong28,2018-05-21T17:22:54Z,"@jonlee2 That is not a public URL. Here are the failed tests:

```
06:02:14 test_id:    kafkatest.benchmarks.streams.streams_simple_benchmark_test.StreamsSimpleBenchmarkTest.test_simple_benchmark.test=streams-join.scale=1
06:02:14 status:     FAIL
06:02:14 run time:   38 minutes 32.121 seconds
06:02:14 
06:02:14 
06:02:14     Streams Test process on ubuntu@worker6 took too long to exit
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/benchmarks/streams/streams_simple_benchmark_test.py"", line 119, in test_simple_benchmark
06:02:14     self.execute(single_test, scale)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/benchmarks/streams/streams_simple_benchmark_test.py"", line 146, in execute
06:02:14     self.driver[num].wait()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/services/streams.py"", line 226, in wait
06:02:14     self.wait_node(node, timeout_sec)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/services/streams.py"", line 230, in wait_node
06:02:14     wait_until(lambda: not node.account.alive(pid), timeout_sec=timeout_sec, err_msg=""Streams Test process on "" + str(node.account) + "" took too long to exit"")
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/utils/util.py"", line 36, in wait_until
06:02:14     raise TimeoutError(err_msg)
06:02:14 TimeoutError: Streams Test process on ubuntu@worker6 took too long to exit
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.streams.streams_broker_down_resilience_test.StreamsBrokerDownResilience.test_streams_runs_with_broker_down_initially
06:02:14 status:     FAIL
06:02:14 run time:   1 minute 37.529 seconds
06:02:14 
06:02:14 
06:02:14     Did expect to read 'Broker may not be available' from ubuntu@worker3
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/streams/streams_broker_down_resilience_test.py"", line 94, in test_streams_runs_with_broker_down_initially
06:02:14     self.wait_for_verification(processor, broker_unavailable_message, processor.LOG_FILE, 100)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/streams/base_streams_test.py"", line 94, in wait_for_verification
06:02:14     err_msg=""Did expect to read '%s' from %s"" % (message, processor.node.account))
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/utils/util.py"", line 36, in wait_until
06:02:14     raise TimeoutError(err_msg)
06:02:14 TimeoutError: Did expect to read 'Broker may not be available' from ubuntu@worker3
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.consumer_num=2.quota_type=client-id
06:02:14 status:     FAIL
06:02:14 run time:   3 minutes 5.118 seconds
06:02:14 
06:02:14 
06:02:14     number of produced messages 50000 doesn't equal number of consumed messages 45020
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 154, in test_quota
06:02:14     assert success, msg
06:02:14 AssertionError: number of produced messages 50000 doesn't equal number of consumed messages 45020
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=.user.client-id.override_quota=False
06:02:14 status:     FAIL
06:02:14 run time:   3 minutes 40.232 seconds
06:02:14 
06:02:14 
06:02:14     number of produced messages 50000 doesn't equal number of consumed messages 43370
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 154, in test_quota
06:02:14     assert success, msg
06:02:14 AssertionError: number of produced messages 50000 doesn't equal number of consumed messages 43370
06:02:14 
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=.user.client-id.override_quota=True
06:02:14 status:     FAIL
06:02:14 run time:   3 minutes 5.967 seconds
06:02:14 
06:02:14 
06:02:14     number of produced messages 50000 doesn't equal number of consumed messages 43200
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 154, in test_quota
06:02:14     assert success, msg
06:02:14 AssertionError: number of produced messages 50000 doesn't equal number of consumed messages 43200
06:02:14 
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=client-id.override_quota=False
06:02:14 status:     FAIL
06:02:14 run time:   3 minutes 51.163 seconds
06:02:14 
06:02:14 
06:02:14     number of produced messages 50000 doesn't equal number of consumed messages 45390
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 154, in test_quota
06:02:14     assert success, msg
06:02:14 AssertionError: number of produced messages 50000 doesn't equal number of consumed messages 45390
06:02:14 
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=client-id.override_quota=True
06:02:14 status:     FAIL
06:02:14 run time:   3 minutes 4.952 seconds
06:02:14 
06:02:14 
06:02:14     number of produced messages 50000 doesn't equal number of consumed messages 45020
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 154, in test_quota
06:02:14     assert success, msg
06:02:14 AssertionError: number of produced messages 50000 doesn't equal number of consumed messages 45020
06:02:14 
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=user.override_quota=False
06:02:14 status:     FAIL
06:02:14 run time:   3 minutes 39.709 seconds
06:02:14 
06:02:14 
06:02:14     number of produced messages 50000 doesn't equal number of consumed messages 43190
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 154, in test_quota
06:02:14     assert success, msg
06:02:14 AssertionError: number of produced messages 50000 doesn't equal number of consumed messages 43190
06:02:14 
06:02:14 --------------------------------------------------------------------------------
06:02:14 test_id:    kafkatest.tests.client.quota_test.QuotaTest.test_quota.quota_type=user.override_quota=True
06:02:14 status:     FAIL
06:02:14 run time:   3 minutes 6.331 seconds
06:02:14 
06:02:14 
06:02:14     number of produced messages 50000 doesn't equal number of consumed messages 45350
06:02:14 Traceback (most recent call last):
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 132, in run
06:02:14     data = self.run_test()
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/tests/runner_client.py"", line 185, in run_test
06:02:14     return self.test_context.function(self.test)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/venv/local/lib/python2.7/site-packages/ducktape-0.7.1-py2.7.egg/ducktape/mark/_mark.py"", line 324, in wrapper
06:02:14     return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
06:02:14   File ""/home/jenkins/workspace/system-test-kafka-branch-builder/kafka/tests/kafkatest/tests/client/quota_test.py"", line 154, in test_quota
06:02:14     assert success, msg
06:02:14 AssertionError: number of produced messages 50000 doesn't equal number of consumed messages 45350
```"
390754366,4830,lindong28,2018-05-21T19:17:22Z,@rajinisivaram Thanks much for helping running the test!
390902167,4830,jonlee2,2018-05-22T08:12:39Z,"I found a bug in Selector.java, which caused the test failures. For SocketServer, Selector.unmute()  unmutes the channel only if the mute state is MUTED. However, it removes the channel from explicitlyMutedChannels regardless of the outcome of unmute. This allows a new client request to be received by SocketServer when throttling for the previous request is still in progress and thus breaks the state machine. I ran the system tests locally after fixing this and all previously failed quota tests succeeded.

This was not really caught by the existing quota integration tests because they produce/consume till throttled and stop there.

I also noticed that passing INVALID_SESSION_ID in an empty fetch response (when throttled) will close the incremental fetch session. I added a special session id to keep the session open.

@rajinisivaram @lindong28 Please review and also start another round of system tests. Thank you. "
393244326,4830,lindong28,2018-05-30T17:17:03Z,This PR has been merged in the trunk with https://github.com/apache/kafka/pull/5064.
670693346,9103,cmccabe,2020-08-07T20:10:07Z,"I think it would be good to split this PR up a little bit.  It seems like we could have a split like this:

PR 1.  `Add flag to the RequestContext` and `Add initial principal name`

PR 2. Authorization changes for AlterConfigs / IncrementalAlterConfigs, forwarding if required, IBP check, bump RPC versions of AlterConfigs / IncrementalAlterConfigs

PR 3. AdminClient changes in behavior based on versions of AlterConfigs / IncrementalAlterConfigs, AlterConfigsUtil, etc."
693757638,9103,abbccdda,2020-09-17T01:45:08Z,retest this please
696225206,9103,cmccabe,2020-09-21T16:25:39Z,"Hi @abbccdda ,

Thanks for the PR!  It looks good.  I like the idea behind `ForwardRequestHandler`.

Since this class doesn't have any internal state, I wonder if it would be more Scala-like to just have a function which just takes some callbacks as arguments.

Can we get rid of the need for the `customizedAuthorizationError` callback by having `resourceSplitByAuthorization` return a `Map[RK, ApiError]` instead of `Map[RK, RV]`?  When would RV be needed for keys where authorization failed?"
696281629,9103,abbccdda,2020-09-21T18:11:18Z,@cmccabe Sounds good to me to remove the customized error helper.
713007022,9103,abbccdda,2020-10-20T17:04:29Z,retest this please
713244123,9103,abbccdda,2020-10-21T02:03:46Z,test this please
718051602,9103,abbccdda,2020-10-28T16:27:34Z,retest this please
719681493,9103,hachikuji,2020-10-30T17:12:47Z,"One more thing to verify. When the metadata quorum feature flag is not defined, I think we should not expose the envelope api in the `ApiVersion` request."
719959211,9103,abbccdda,2020-10-31T16:51:50Z,retest this please
722005317,9103,abbccdda,2020-11-04T22:17:54Z,"Mvn failure is not related, merging"
2614325456,9103,ray841,2025-01-26T11:00:15Z,"> This PR adds support for redirections of the following RPCs:
> 
> 1. Alter Configs
> 2. Incremental Alter Configs
> 3. Alter Client Quotas
> 4. Create Topics
> 
> The specific changes include:
> 
> 1. Use the flag `Request Context. from Privileged Listener` to indicate whether a request is **possibly** coming from the inter-broker communication, details in this [PR](https://github.com/apache/kafka/pull/9144). When a request is from a privileged listener, we will do a separate round of `CLUSTER_ACTION` authorization for a forwarding request.
> 2. Add Envelope RPC for the request forwarding, and corresponding handling logic in Kafka 
> 3. Add forwarding support in the Broker To Controller Channel Manager
> 4. Add a separate authorization of a forward request in the Authorizable Context with a forwarding principal, and audit logging changes.
> 5. Checks for the mentioned RPCs to do the redirection. Will do the forwarding when the request is not forwarded and the current broker is not the controller.
> 6. Add broker IBP to guard against redirection. If IBP is low, any broker would still try to mutate ZK data with the admin manager.
> 7. Add support for principal serialization as an extendable interface called `Kafka Principal Serde
> 8. Built a template called `Forward Request Handler` to formulate the workflow of forward request handling
> 9. Add SSL trust store/keystore path augment/trim logic to trigger file reload upon ZK notification
> 10. Add IBP constraint tests for redirected RPCs since version 2.8, and KIP-500 flag to disable redirection.
> 
> Co-authored-by: Jason Gustafson [jason@confluent.io](mailto:jason@confluent.io)
> 
> ### Committer Checklist (excluded from commit message)
> * [x]  Verify design and implementation
> * [x]  Verify test coverage and CI build status
> * [x]  Verify documentation (including upgrade notes)

"
130005348,132,asfbot,2015-08-11T18:30:57Z,"[kafka-trunk-git-pr #127](https://builds.apache.org/job/kafka-trunk-git-pr/127/) SUCCESS
This pull request looks good
"
131264143,132,asfbot,2015-08-14T23:00:16Z,"[kafka-trunk-git-pr #145](https://builds.apache.org/job/kafka-trunk-git-pr/145/) SUCCESS
This pull request looks good
"
133253732,132,asfbot,2015-08-21T02:38:32Z,"[kafka-trunk-git-pr #187](https://builds.apache.org/job/kafka-trunk-git-pr/187/) FAILURE
Looks like there's a problem with this pull request
"
133280679,132,allenxwang,2015-08-21T04:31:38Z,"The failed test in the last PR build passed on my laptop locally. It seems to be flaky as it also failed on an earlier build without my changes.
"
133577314,132,asfbot,2015-08-21T22:10:09Z,"[kafka-trunk-git-pr #193](https://builds.apache.org/job/kafka-trunk-git-pr/193/) SUCCESS
This pull request looks good
"
170421487,132,joestein,2016-01-11T03:00:15Z,"one overall general comment on the implementation is that the brokers properties themselves could cary this information making it so the topic creator doesn't have to know this. the fact is that still a lot of humans run the topic command but in many cases it is some software system operationally doing it. in either case if the broker had a property rack=0 or whatever it could then just be the way you have the topic distribute that information it should already be able to gather. Granted, this implementation saves a lot of having to store it in zookeeper so rationally speaking this is better than putting any code in to that. 

Sorry if I missed the entire discussion thread on this just seeing it for first time. I like it, would love to see this get into trunk and start to be used and also in the next release. 

Nice work so far!!!
"
170429303,132,allenxwang,2016-01-11T04:38:16Z,"Please see KIP-36 for the latest proposal. (https://cwiki.apache.org/confluence/display/KAFKA/KIP-36+Rack+aware+replica+assignment)

The biggest difference is that the rack information is added as a broker meta data in ZooKeeper. Consequently the inter-broker (UpdateMetadataRequest) and client to broker meta data query protocol (TopicMetadataResponse) will be changed to have rack information. 

Once the KIP is accepted, I will update this PR to incorporate these new ideas.
"
173002403,132,sslavic,2016-01-19T22:02:42Z,"Wouldn't it be nice to have metric per topic partition, in how many different racks do ISRs live?
"
190237447,132,ijuma,2016-02-29T14:42:34Z,"@allenxwang, can you please fix the merge conflicts?
"
190387743,132,allenxwang,2016-02-29T21:00:01Z,"@ijuma Done with resolving conflicts. Would appreciate if this can be reviewed and merged with a higher priority as it touches multiple areas and easy to get conflicts if sitting there for a bit longer.
"
190404703,132,hachikuji,2016-02-29T21:28:46Z,"@allenxwang I was looking for the changes to TopicMetadataRequest, but didn't see them. Were you planning to do them here?
"
190409503,132,granthenke,2016-02-29T21:45:16Z,"@hachikuji @allenxwang I need to make changes to TopicMetadata response too for KIP-4. We also need to migrate to the new TopicMetadata Request/Responses on the server side [KAFKA-2073](https://issues.apache.org/jira/browse/KAFKA-2073). We should probably migrate before making changes. Otherwise we need to make the changes in both the Java and Scala classes.
"
190410867,132,hachikuji,2016-02-29T21:47:43Z,"@granthenke Makes sense to me. I can pick up 2073 if you haven't already started.
"
190413309,132,granthenke,2016-02-29T21:52:23Z,"@hachikuji Feel free to pick it up. I have some intermediate changes I can share with you later tonight. The hardest part about migrating was MetadataCache. You more or less need to re-implement it because its tied very heavily to the old requests/responses. After that I had some test failures that I haven't had a chance to dig into. I was thinking adding some additional tests may help flush out the issues more clearly too.
"
190415650,132,allenxwang,2016-02-29T21:57:09Z,"@hachikuji @granthenke I am not going to include changes to TopicMetadataRequest/Response in this PR which was indicated in the KIP discussion thread. You are free to do it in later on after this is merged.
"
190418313,132,hachikuji,2016-02-29T22:06:49Z,"@allenxwang Have you opened another JIRA for the topic metadata changes? We should probably make sure we don't lose track of them.
"
190422237,132,granthenke,2016-02-29T22:13:44Z,"@hachikuji @allenxwang I can include them in my changes to the response for KIP-4. I will open a jira to track that.  It would be nice to make all the changes at once anyway. I will ask for your feedback @allenxwang when posting to proposed changes. 
"
190431869,132,granthenke,2016-02-29T22:39:59Z,"@hachikuji I have created [KAFKA-3306](https://issues.apache.org/jira/browse/KAFKA-3306) to track updating the metadata response. I should have a preliminary patch this week.
"
190547078,132,allenxwang,2016-03-01T05:00:06Z,"Flaky test? Passed on my laptop and on Jenkins before the last commit.

Test Result (2 failures / +1)
kafka.api.SslConsumerTest.testAutoCommitOnRebalance
kafka.network.SocketServerTest.testSocketsCloseOnShutdown
"
190795970,132,granthenke,2016-03-01T16:28:19Z,"@allenxwang Thanks for the work on this. I did a quick review. Will have to come back for a more in depth look.
"
191917048,132,junrao,2016-03-03T19:05:10Z,"Thanks for the patch. Now that we are changing the broker registration in ZK, could we update the upgrade doc to warn users that they need to upgrade from 0.9.0.1 (instead of 0.9.0.0) to 0.10.0? Otherwise, the old scala consumer in 0.9.0.0 will break once the broker is upgraded to 0.10.0.
"
192326728,132,ijuma,2016-03-04T15:39:33Z,"Thanks for the PR @allenxwang. I did an initial pass and left a few comments/questions and I submitted a PR with some fixes and improvements:

https://github.com/allenxwang/kafka/pull/1

Please review and integrate in any way you please (merge, cherry-pick, manually, etc.).

I will take a second and final pass by the end of Monday.
"
193290563,132,ijuma,2016-03-07T15:10:20Z,"@allenxwang, thanks for addressing the review comments and merging my PR. As promised, I completed my review today and I left one comment and created another PR with more code style improvements:

https://github.com/allenxwang/kafka/pull/2

I think we're pretty close (but we'll be sure once Jun takes another look). As far as I can see, outside of the PR I filed, the following is potentially outstanding:
- [ ] A couple of deprecated constructors in `UpdateMetadataRequest.java` are only used in tests. I'd like to @junrao to confirm that we actually need them.
- [x] I think we need to tweak the AdminUtils.assignReplicasToBrokers comment a little to integrate it better with what we had there previously.
- [ ] Jun asked if some tests are actually needed or if they can be simplified. @allenxwang explained why they are needed, so if Jun is happy with the explanation, then this is resolved.
- [x] Finally, the only non-minor change (although mechanical) is whether we should use a `BrokerMetadata` class instead of broker ids and a separate `rackInfo` map (suggested by Jun). Allen wasn't sure if this was better so it's still being discussed.
"
193465023,132,ijuma,2016-03-07T21:36:34Z,"A couple more things that we need:
- [x] Update `upgrade.html` to mention that old clients need to upgrade to 0.9.0.1 before the brokers are upgraded to 0.10.0.x.
- [x] System tests must pass.
"
193921244,132,allenxwang,2016-03-08T19:10:52Z,"@ijuma Can you please help look into test failures? It seems that the failures occur after merging your pull request #2. 
"
193945557,132,ijuma,2016-03-08T20:04:33Z,"@allenxwang Investigating now.
"
193978242,132,ijuma,2016-03-08T21:34:52Z,"@allenxwang Sorry about that, the following should fix it: https://github.com/allenxwang/kafka/pull/3 . Looks like I managed to push code to my PR that was slightly different to what I had tested.
"
194106914,132,junrao,2016-03-09T04:14:34Z,"@allenxwang : Could you also run the system tests with your patch? If needed, @granders can help you run your branch in Confluent's jenkins.
"
194129518,132,ijuma,2016-03-09T06:11:49Z,"@junrao @allenxwang I actually started a system tests build yesterday:

https://jenkins.confluent.io/job/kafka_systest_branch_builder2/40/console

There are 7 failures, but some are unrelated. I am going to analyse it in more detail today.
"
194365328,132,ijuma,2016-03-09T16:05:52Z,"With regards to system failures, they are:

```
test_id:    2016-03-08--001.kafkatest.sanity_checks.test_verifiable_producer.TestVerifiableProducer.test_simple_run.producer_version=trunk
test_id:    2016-03-08--001.kafkatest.tests.streams_smoke_test.StreamsSmokeTest.test_streams
test_id:    2016-03-08--001.kafkatest.tests.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_PLAINTEXT.client_protocol=SSL
test_id:    2016-03-08--001.kafkatest.tests.compatibility_test.ClientCompatibilityTest.test_producer_back_compatibility
test_id:    2016-03-08--001.kafkatest.tests.upgrade_test.TestUpgrade.test_upgrade
test_id:    2016-03-08--001.kafkatest.tests.streams_bounce_test.StreamsBounceTest.test_bounce
test_id:    2016-03-08--001.kafkatest.tests.consumer_test.OffsetValidationTest.test_consumer_bounce.clean_shutdown=True.bounce_mode=all
```

I think these were all (transiently) failing in trunk based on the last time trunk was merged to this branch. @allenxwang, can you please merge from trunk again? The streams tests have since been disabled and `test_upgrade` has been fixed. It's particularly important that we know that it passes even after the changes in this branch. I will re-run the system tests once you do this.
"
194432464,132,allenxwang,2016-03-09T18:14:50Z,"Merged with trunk and run systemTest task on my laptop. No test failures.
"
194576557,132,ijuma,2016-03-10T00:02:29Z,"Thanks @allenxwang, I started another system tests run:

https://jenkins.confluent.io/job/kafka_system_tests_branch_builder/389/

Are you sure you ran the system tests? Setting them up is a bit involved:

https://cwiki.apache.org/confluence/display/KAFKA/tutorial+-+set+up+and+run+Kafka+system+tests+with+ducktape
"
194595633,132,allenxwang,2016-03-10T00:55:30Z,"@ijuma I got it wrong. All I did is `gradlew systemTest`. 
"
194641381,132,ijuma,2016-03-10T03:17:42Z,"To settle the `BrokerMetadata` versus `brokerList` and `rackInfo` question, I made the change in the following commit so people can check how it could look:

https://github.com/ijuma/kafka/commit/7cb35cb4dec7478d42605e9b79d4cac7c1550044

@junrao @allenxwang let me know what you think. Is it better, similar or worse?
"
194766656,132,ijuma,2016-03-10T10:02:42Z,"There were 4 failures in the last system tests run, none of them seem to be specific to this branch:

```
test_id:    2016-03-09--001.kafkatest.tests.compatibility_test.ClientCompatibilityTest.test_producer_back_compatibility
This is fall-out from KIP-31/32, ticket is KAFKA-3371

test_id:    2016-03-09--001.kafkatest.tests.consumer_group_command_test.ConsumerGroupCommandTest.test_describe_consumer_group.security_protocol=SSL
test_id:    2016-03-09--001.kafkatest.tests.consumer_group_command_test.ConsumerGroupCommandTest.test_describe_consumer_group.security_protocol=PLAINTEXT
There's a PR to fix both of these (the tests are a bit brittle): https://github.com/apache/kafka/pull/1039

test_id:    2016-03-09--001.kafkatest.tests.security_rolling_upgrade_test.TestSecurityRollingUpgrade.test_rolling_upgrade_phase_two.broker_protocol=SASL_PLAINTEXT.client_protocol=SASL_SSL
This is failing transiently and the ticket for it is KAFKA-3374
```
"
194975802,132,allenxwang,2016-03-10T17:52:49Z,"@ijuma Your approach looks good to me. It has the advantage that the data structure is clear and allows future addition if more metadata is required to do replica assignment. 

A further variant would be making `RackAwareMode` an argument to `AdminUtils.assignReplicaToBrokers`. But I like the way it is now because it keeps   `AdminUtils.assignReplicaToBrokers` simple and free of user input and context.
"
195082363,132,ijuma,2016-03-10T22:38:02Z,"@allenxwang OK, great. I cleaned up the commit a little and created a PR:

https://github.com/allenxwang/kafka/pull/4

I will let you decide whether you want to merge it or if you would prefer to wait for @junrao's opinion.
"
195110017,132,allenxwang,2016-03-11T00:10:32Z,"@ijuma I will wait for @junrao's comments on this.
"
195245185,132,ijuma,2016-03-11T08:05:08Z,"I ran the system tests on a branch with https://github.com/allenxwang/kafka/pull/4 and master merged into it and there are no new failures when compared to trunk: https://jenkins.confluent.io/job/kafka_systest_branch_builder2/45/console
"
195570860,132,junrao,2016-03-11T22:03:13Z,"@allenxwang : Yes, I agree that using BrokerMetadata in @ijuma's patch is better.
"
195573879,132,ijuma,2016-03-11T22:15:45Z,"Also, some conflicts need to be fixed with trunk. @allenxwang, let me know if you'd like me to merge master into https://github.com/allenxwang/kafka/pull/4 in order to fix the conflicts.
"
195574433,132,allenxwang,2016-03-11T22:17:59Z,"@ijuma Yes, please merge with the trunk in your pull request. Thanks.
"
195596529,132,ijuma,2016-03-11T23:06:07Z,"@allenxwang Merged master into https://github.com/allenxwang/kafka/pull/4, fixed conflicts and verified that tests pass.
"
195605604,132,ijuma,2016-03-11T23:48:05Z,"@junrao Allen merged my PR, so this is ready for review.
"
195644348,132,junrao,2016-03-12T02:37:48Z,"@granthenke : For your previous question on the json version for ZK registration, my preference is still to do the version change now. This way, our hands are not tied for potential future changes and it's also easier to document this. As for compatibility, most people will probably be on 0.9.0.1 before they upgrade to 0.10.0. So the impact should be limited.
"
196364260,132,granthenke,2016-03-14T15:18:07Z,"@junrao Thanks for the confirmation. I understand json version will change.
"
196844027,132,junrao,2016-03-15T14:24:37Z,"Thanks for the patch. LGTM. Could you rebase?
"
196845140,132,ijuma,2016-03-15T14:28:14Z,"@allenxwang, the following merges master into your branch: https://github.com/allenxwang/kafka/pull/5
"
424496003,5693,vvcephei,2018-09-25T20:47:48Z,"@guozhangwang @bbejeck When you get a chance, please review this code.

I have done my best locally to produce a nice, clean implementation, but now that the diff is published, I'll make another pass over it looking for sharp edges."
424781723,5693,vvcephei,2018-09-26T16:26:11Z,"I've partially addressed the comments so far. Notably, I've dropped the punctuator and now handle both time and size constraints during `process`."
424843948,5693,vvcephei,2018-09-26T19:40:20Z,"Hi @guozhangwang , @mjsax , and @bbejeck ,

I believe I've addressed all the comments thus far, with the exception of whether to store the values serialized.

I did notice a low-effort optimization in the current implementation to skip serializing if the buffer isn't size-constrained, which would carry over even when we add the changelog, if the buffer delays serializing until flush, so for high-turnover buffers, many records may never be serialized at all.

But that's all a little beside the point... I'm still happy to change the whole implementation to store serialized data if that's the reviewers' preference."
425098149,5693,bbejeck,2018-09-27T13:44:53Z,"test failure unrelated

retest this please"
425156616,5693,vvcephei,2018-09-27T16:25:32Z,"Hey all, FYI, I've just updated this PR to store serialized data instead.

It was a bit more work than I anticipated because I ran into some snags related to preserving the window end information for TimeWindows (and also discovered I was using the wrong serde for session windows). These issues are both fixed now."
425168281,5693,vvcephei,2018-09-27T17:02:30Z,"Ok, @bbejeck ,

I have added some unit tests that verify that the processor throws the exception, and also some integration tests that verifies that Streams shuts down for the same conditions.

Thanks for that catch."
425171800,5693,vvcephei,2018-09-27T17:13:51Z,"Also, I just had a better idea for maintaining the minTimestamp value that cleans up the implementation quite a bit."
425724425,5693,vvcephei,2018-09-30T14:25:23Z,"Just to document the current state...

I think the latest set of commits should resolve all the open comments, so this PR should be ready to merge, pending a rebase once #5521 is merged (or we could merge this first, if #5521 needs more reviews).

"
426046990,5693,vvcephei,2018-10-01T20:14:55Z,"Java 8 failure unrelated: 
```
org.apache.kafka.common.network.SslSelectorTest > testCloseConnectionInClosingState FAILED
    java.lang.AssertionError: Channel not expired expected null, but was:<org.apache.kafka.common.network.KafkaChannel@4f>
...
FAILURE: Build failed with an exception.

* What went wrong:
Execution failed for task ':clients:unitTest'.
> There were failing tests. See the report at: file:///home/jenkins/jenkins-slave/workspace/kafka-pr-jdk8-scala2.11/clients/build/reports/tests/unitTest/index.html
```"
426059684,5693,vvcephei,2018-10-01T20:55:22Z,"@mjsax / @guozhangwang FYI, the tests have passed."
426086754,5693,bbejeck,2018-10-01T22:34:41Z,">  Execution failed for task ':clients:unitTest'.

retest this please"
426103858,5693,vvcephei,2018-10-02T00:08:51Z,I had an unused import. The tests pass for me locally now.
435276167,5821,hzxa21,2018-11-02T05:34:56Z,@junrao @ijuma @lindong28 Can you help to review this PR when you have time? Thanks!
438036817,5821,lindong28,2018-11-12T21:36:29Z,"@hzxa21 LGTM. Thanks for the update. Could you rebase the patch?

Hey @junrao @ijuma, would you like to review the patch or it is OK for me to merge it?"
438039462,5821,junrao,2018-11-12T21:46:12Z,"@hzxa21 , @lindong28 : Yes, I will make another pass in the next couple of days."
442712153,5821,hzxa21,2018-11-29T05:33:35Z,@lindong28 @junrao I have updated the PR to address all the comments. Could you take a second look?
443155235,5821,hzxa21,2018-11-30T10:08:28Z,@junrao Thanks for the prompt reply. I have updated the PR to address your comments and rebase.
443357329,5821,hzxa21,2018-11-30T22:21:20Z,@junrao Thanks for the comments. I have addressed your comments in the updated commit.
443358641,5821,junrao,2018-11-30T22:27:28Z,@lindong28 : Do you want to take another look of this PR before merging?
443404840,5821,lindong36,2018-12-01T06:48:08Z,LGTM. Thanks for the in-depth review @junrao! I am currently not able to access my original github account due to loss of my phone number and can not merge this PR myself.
443437923,5821,junrao,2018-12-01T16:23:00Z,@hzxa21 : Could you address the last comment on line 415 in KafkaZkClient.scala?
443440384,5821,hzxa21,2018-12-01T16:58:56Z,@junrao My bad. Updated the PR to fix that.
443601653,5821,hzxa21,2018-12-03T06:24:02Z,"@junrao Thanks a lot for the review. There are several follow-ups (KAFKA-6029 & KAFKA-7283) we can do after getting the broker epoch. I will address them accordingly and submit PRs. If you are aware of anything that can also benefit from broker epoch, do let me know and I am open to more discussion. Thanks again!"
408690457,5428,guozhangwang,2018-07-29T16:53:16Z,retest this please
408691202,5428,guozhangwang,2018-07-29T17:04:22Z,retest this please
410306248,5428,guozhangwang,2018-08-03T16:27:37Z,"@vvcephei @mjsax @bbejeck for reviews.

Note the logic of processing enforcement is open for discussion, and currently I'm thinking to change it to time based than iteration based. I will include a KIP for this change for open discussions."
411805845,5428,guozhangwang,2018-08-09T15:51:11Z,"System test passed at https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1898/parameters/

Need to wait for KIP-345 to be discussed and voted."
412220468,5428,guozhangwang,2018-08-10T22:14:47Z,retest this please
412221182,5428,guozhangwang,2018-08-10T22:18:53Z,"I've done some simple benchmark comparing

* trunk

http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2018-08-07--001.1533607755--apache--trunk--cf98144/report.html

* this branch (default max.idle = 0ms)

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-10--001.1533927971--guozhangwang--K3514-main-thread-iteration--7cc556f/report.html

* this branch (default max.idle = 500ms)

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-10--001.1533927768--guozhangwang--K3514-main-thread-iteration-500ms--8740e88/report.html

* this branch (default max.idle = 5000ms)

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-10--001.1533928261--guozhangwang--K3514-main-thread-iteration-5000ms--fb51d7a/report.html

My observations:

1. branch-0ms and branch-500ms have similar perf compared to trunk with one single input partition, with two partitions (i.e. joins) branch-0ms has degraded 25% compared to trunk, while branch-500ms outperforms slightly over trunk. Still need to investigate why branch-0ms is worse with trunk.

2. branch-5000ms performs worse on single partitions with branch-0ms and branch-500ms, while its performance is comparable on two partitions (joins)."
412682055,5428,guozhangwang,2018-08-13T22:09:41Z,retest this please
413359579,5428,guozhangwang,2018-08-15T22:31:16Z,"I've found out the actual perf difference is that for joins, we are using leftJoin actually. And we are populating the data before starting the run the tests, hence it is actually a ""catching up"" mode where the first fetch request may just return one partition of the data. With no synchronization (ms = 0), it means that there are less records being joined and output to sink topics.

I've changing leftJoin to join and the results are following:

trunk:

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-15--001.1534370019--guozhangwang--trunk-join--46ac266/report.html

max.idle = 0ms:

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-15--001.1534370062--guozhangwang--K3514-main-thread-iteration-join--551f2ae/report.html

max.idle = 500ms:

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-15--001.1534371559--guozhangwang--K3514-main-thread-iteration-500ms--f6860ce/report.html

max.idle = 5000ms:

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-15--001.1534369924--guozhangwang--K3514-main-thread-iteration-5000ms--fb51d7a/report.html 

Some observations:

1. From the stream-stream join results we can see that the old manner (max.waits = 5 iterations) is actually very close to max.idle = 0ms (I guess it was maybe a few millis idle time).

2. From the table-table join results, the new code actually processes more data than the old code."
413940230,5428,mjsax,2018-08-17T17:47:21Z,@guozhangwang Can you rebase?
414789168,5428,guozhangwang,2018-08-21T19:12:11Z,"The benchmark results looks good to me:

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-21--001.1534818582--guozhangwang--K3514-main-thread-iteration--e454e81/report.html

http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2018-08-21--001.1534820334--apache--trunk--504824b/report.html"
415487809,5428,guozhangwang,2018-08-23T16:48:48Z,"> But with regard to my comments, I am wondering if we should add more tests to check the behavior we want in a tighter fashion?

The tests I added in unit suite checks that 1) `idle time is respected, and timer is reset correctly once enforce has happened`, 2) optimization on skipping commit indeed happens.

I think I can add more on 3) punctuate / commit happens as expected, 4) break loop and calls poll as expected. Anything else comes to your mind? @mjsax "
415540813,5428,guozhangwang,2018-08-23T19:24:58Z,"I'm now working on add more tests (@mjsax let me know if the above plan is sufficient).


I tried about separating commit for active and standby tasks but after some investigation I realized it is related to https://issues.apache.org/jira/browse/KAFKA-6108 and hence I'd leave it to this JIRA / PR (also left a comment in the ticket for some clarification questions)."
415562779,5428,guozhangwang,2018-08-23T20:36:57Z,retest this please
415849669,5428,guozhangwang,2018-08-24T18:50:35Z,"The latest perf numbers can be found here:

http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2018-08-24--001.1535136054--guozhangwang--K3514-main-thread-iteration--d28ee44/report.html"
416423200,5428,guozhangwang,2018-08-28T01:43:45Z,"@vvcephei @mjsax comments replied / addressed, please take another look."
417518708,5428,guozhangwang,2018-08-31T01:19:34Z,"Just to clarify, the reason I did this change is that I think recording the sensor every time we enforce processing actually makes more sense than only recording it the first time we start enforcing, since each enforce-processing period may differ and hence the number of records it processed may differ, recording only at the first time may not give us right amount of ""risk assessment"" when it happens."
417686888,5428,vvcephei,2018-08-31T14:46:22Z,"LGTM! Thanks, @guozhangwang "
417743966,5428,bbejeck,2018-08-31T17:58:18Z,"failures unrelated

retest this please"
417778438,5428,guozhangwang,2018-08-31T20:18:50Z,"@bbejeck That's a good question.

Personally I think we should make strict guarantees that if `ProcessorContext.commit()` is called, we make sure the state is committed before the next record for this task is going to be processed, because some application logic may rely on that (think: I did some external changes, and hence I do want to make sure the state is committed right aware before processing the next record). And hence the implementation of `maybeCommitPerUserRequested` in stream thread actually always check for `commitRequest` and `commitNeeded` while ignoring the commit interval.

Could you point out which part of the doc indicates that `ProcessorContext.commit()` is more of a suggestion? Maybe we do need to update it."
419097804,5428,bbejeck,2018-09-06T13:42:20Z,">Could you point out which part of the doc indicates that ProcessorContext.commit() is more of a suggestion? Maybe we do need to update it.

@guozhangwang I can't seem to find it, I must be mistaken so ignore my previous comment."
419097914,5428,bbejeck,2018-09-06T13:42:39Z,retest this please
419674466,5428,mjsax,2018-09-08T21:36:45Z,"Test failures seem to be related.
```
java.lang.AssertionError: 
Expected: <2>
     but: was <1>
	at org.hamcrest.MatcherAssert.assertThat(MatcherAssert.java:20)
	at org.junit.Assert.assertThat(Assert.java:956)
	at org.junit.Assert.assertThat(Assert.java:923)
	at org.apache.kafka.streams.processor.internals.StreamThreadTest.shouldRespectNumIterationsInMainLoop(StreamThreadTest.java:360)
```"
420112814,5428,guozhangwang,2018-09-11T01:15:42Z,@mjsax could you give it another look?
420150651,5428,mjsax,2018-09-11T05:23:21Z,Retest this please
420474848,5428,mjsax,2018-09-12T00:54:03Z,Nice! Super happy we got this finally in! Great work @guozhangwang!
289507362,2743,asfbot,2017-03-27T16:27:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2416/
Test FAILed (JDK 8 and Scala 2.12).
"
289507730,2743,asfbot,2017-03-27T16:29:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2416/
Test FAILed (JDK 7 and Scala 2.10).
"
289508178,2743,asfbot,2017-03-27T16:30:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2420/
Test FAILed (JDK 8 and Scala 2.11).
"
289541446,2743,asfbot,2017-03-27T18:25:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2418/
Test FAILed (JDK 7 and Scala 2.10).
"
289541718,2743,asfbot,2017-03-27T18:26:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2422/
Test FAILed (JDK 8 and Scala 2.11).
"
289541722,2743,asfbot,2017-03-27T18:26:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2418/
Test FAILed (JDK 8 and Scala 2.12).
"
289595963,2743,asfbot,2017-03-27T21:46:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2426/
Test FAILed (JDK 8 and Scala 2.12).
"
289596108,2743,asfbot,2017-03-27T21:46:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2426/
Test FAILed (JDK 7 and Scala 2.10).
"
289596828,2743,asfbot,2017-03-27T21:49:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2430/
Test FAILed (JDK 8 and Scala 2.11).
"
289702373,2743,asfbot,2017-03-28T08:39:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2448/
Test FAILed (JDK 8 and Scala 2.11).
"
289702456,2743,asfbot,2017-03-28T08:39:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2444/
Test FAILed (JDK 8 and Scala 2.12).
"
289702494,2743,asfbot,2017-03-28T08:40:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2444/
Test FAILed (JDK 7 and Scala 2.10).
"
289703842,2743,asfbot,2017-03-28T08:45:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2450/
Test FAILed (JDK 8 and Scala 2.11).
"
289703938,2743,asfbot,2017-03-28T08:46:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2446/
Test FAILed (JDK 8 and Scala 2.12).
"
289704345,2743,asfbot,2017-03-28T08:47:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2446/
Test FAILed (JDK 7 and Scala 2.10).
"
289730122,2743,ijuma,2017-03-28T10:35:38Z,"Once the Jenkins build is passing, it would be great to run the system tests via the branch builder."
289753653,2743,asfbot,2017-03-28T12:27:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2458/
Test FAILed (JDK 8 and Scala 2.11).
"
289755819,2743,asfbot,2017-03-28T12:37:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2454/
Test FAILed (JDK 7 and Scala 2.10).
"
289811741,2743,asfbot,2017-03-28T15:40:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2459/
Test FAILed (JDK 7 and Scala 2.10).
"
289812166,2743,asfbot,2017-03-28T15:42:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2463/
Test FAILed (JDK 8 and Scala 2.11).
"
289812260,2743,asfbot,2017-03-28T15:42:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2455/
Test FAILed (JDK 8 and Scala 2.12).
"
289812339,2743,asfbot,2017-03-28T15:42:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2455/
Test FAILed (JDK 7 and Scala 2.10).
"
289822127,2743,asfbot,2017-03-28T16:13:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2459/
Test FAILed (JDK 8 and Scala 2.12).
"
289955048,2743,ijuma,2017-03-29T01:31:52Z,"@benstopford, I merged a PR that disables the test_zk_security_upgrade test until this PR lands. When you rebase, can you please remove the ignore annotation from that test? Thanks."
290071422,2743,asfbot,2017-03-29T12:15:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2491/
Test FAILed (JDK 7 and Scala 2.10).
"
290080826,2743,asfbot,2017-03-29T12:55:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2491/
Test FAILed (JDK 8 and Scala 2.12).
"
290100433,2743,asfbot,2017-03-29T14:05:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2499/
Test FAILed (JDK 8 and Scala 2.11).
"
290103297,2743,asfbot,2017-03-29T14:14:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2495/
Test FAILed (JDK 8 and Scala 2.12).
"
290108980,2743,asfbot,2017-03-29T14:33:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2495/
Test FAILed (JDK 7 and Scala 2.10).
"
290145364,2743,asfbot,2017-03-29T16:29:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2499/
Test FAILed (JDK 8 and Scala 2.12).
"
290146637,2743,asfbot,2017-03-29T16:33:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2499/
Test FAILed (JDK 7 and Scala 2.10).
"
290146935,2743,asfbot,2017-03-29T16:34:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2503/
Test FAILed (JDK 8 and Scala 2.11).
"
290330213,2743,asfbot,2017-03-30T07:48:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2520/
Test FAILed (JDK 7 and Scala 2.10).
"
290330464,2743,asfbot,2017-03-30T07:49:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2524/
Test FAILed (JDK 8 and Scala 2.11).
"
290343889,2743,asfbot,2017-03-30T08:45:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2525/
Test FAILed (JDK 8 and Scala 2.11).
"
290344202,2743,asfbot,2017-03-30T08:47:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2521/
Test FAILed (JDK 7 and Scala 2.10).
"
290351650,2743,asfbot,2017-03-30T09:17:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2521/
Test FAILed (JDK 8 and Scala 2.12).
"
290388349,2743,asfbot,2017-03-30T11:53:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2530/
Test FAILed (JDK 8 and Scala 2.12).
"
290389193,2743,asfbot,2017-03-30T11:57:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2530/
Test FAILed (JDK 7 and Scala 2.10).
"
290390333,2743,asfbot,2017-03-30T12:03:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2534/
Test FAILed (JDK 8 and Scala 2.11).
"
290489990,2743,benstopford,2017-03-30T17:53:14Z,System tests running here: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/815/
290503239,2743,asfbot,2017-03-30T18:31:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2541/
Test FAILed (JDK 8 and Scala 2.12).
"
290503276,2743,asfbot,2017-03-30T18:31:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2541/
Test PASSed (JDK 7 and Scala 2.10).
"
290503893,2743,asfbot,2017-03-30T18:34:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2545/
Test PASSed (JDK 8 and Scala 2.11).
"
291005088,2743,asfbot,2017-04-02T18:31:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2622/
Test FAILed (JDK 8 and Scala 2.12).
"
291005092,2743,asfbot,2017-04-02T18:31:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2626/
Test FAILed (JDK 8 and Scala 2.11).
"
291005119,2743,asfbot,2017-04-02T18:31:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2622/
Test FAILed (JDK 7 and Scala 2.10).
"
291012533,2743,asfbot,2017-04-02T20:27:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2627/
Test FAILed (JDK 8 and Scala 2.11).
"
291013072,2743,benstopford,2017-04-02T20:35:25Z,"@junrao I've addressed your second round of comments. 
I still need to add some code to protect earlier versions of the message format. I'll dig into that tomorrow. "
291014113,2743,asfbot,2017-04-02T20:50:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2623/
Test PASSed (JDK 7 and Scala 2.10).
"
291015777,2743,asfbot,2017-04-02T21:17:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2623/
Test PASSed (JDK 8 and Scala 2.12).
"
291101379,2743,asfbot,2017-04-03T10:08:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2630/
Test FAILed (JDK 8 and Scala 2.11).
"
291107328,2743,asfbot,2017-04-03T10:37:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2626/
Test PASSed (JDK 7 and Scala 2.10).
"
291131165,2743,asfbot,2017-04-03T12:41:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2633/
Test PASSed (JDK 8 and Scala 2.11).
"
291135978,2743,asfbot,2017-04-03T13:02:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2626/
Test FAILed (JDK 8 and Scala 2.12).
"
291136035,2743,asfbot,2017-04-03T13:02:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2629/
Test PASSed (JDK 8 and Scala 2.12).
"
291173465,2743,asfbot,2017-04-03T15:12:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2629/
Test FAILed (JDK 7 and Scala 2.10).
"
291186771,2743,asfbot,2017-04-03T15:55:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2635/
Test FAILed (JDK 8 and Scala 2.12).
"
291187248,2743,asfbot,2017-04-03T15:57:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2635/
Test PASSed (JDK 7 and Scala 2.10).
"
291188110,2743,asfbot,2017-04-03T16:00:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2639/
Test FAILed (JDK 8 and Scala 2.11).
"
291472829,2743,asfbot,2017-04-04T11:32:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2683/
Test FAILed (JDK 8 and Scala 2.12).
"
291473712,2743,asfbot,2017-04-04T11:36:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2687/
Test FAILed (JDK 8 and Scala 2.11).
"
291478140,2743,asfbot,2017-04-04T11:58:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2683/
Test FAILed (JDK 7 and Scala 2.10).
"
291484786,2743,asfbot,2017-04-04T12:30:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2684/
Test PASSed (JDK 8 and Scala 2.12).
"
291485778,2743,asfbot,2017-04-04T12:34:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2688/
Test FAILed (JDK 8 and Scala 2.11).
"
291490678,2743,asfbot,2017-04-04T12:55:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2684/
Test FAILed (JDK 7 and Scala 2.10).
"
291499576,2743,asfbot,2017-04-04T13:29:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2691/
Test FAILed (JDK 8 and Scala 2.11).
"
291499960,2743,asfbot,2017-04-04T13:30:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2687/
Test PASSed (JDK 7 and Scala 2.10).
"
291501505,2743,asfbot,2017-04-04T13:35:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2687/
Test PASSed (JDK 8 and Scala 2.12).
"
291670976,2743,benstopford,2017-04-04T23:32:48Z,This PR was re-raised from the confluent repo: https://github.com/apache/kafka/pull/2808
2565744411,18240,ahuang98,2024-12-30T17:29:24Z,"> It makes sense to me that after the resign state the replica should always increase its epoch. The replica resigned from leadership at epoch X so eventually the epoch will be at least X + 1. Did you consider transitioning to candidate and relaxing the transition functions to allow both resigned and prospective to transition to candidate?

yes, I decided not to list that as an option because I felt it was equal to if not worse than the option of having resigned transition to prospective in epoch X + 1. personally I felt it was nicer to have less edge cases to the invariant that only prospective should transition to candidate "
266410201,2244,dguy,2016-12-12T11:47:49Z,"@guozhangwang @mjsax @enothereska 
I've not added any metrics to this at the moment as i would rather wait until the metrics PRs from @enothereska make it in (so i can avoid more merge conflicts)"
266418955,2244,asfbot,2016-12-12T12:24:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/75/
Test PASSed (JDK 8 and Scala 2.12).
"
266419082,2244,asfbot,2016-12-12T12:25:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/74/
Test FAILed (JDK 7 and Scala 2.10).
"
266419110,2244,asfbot,2016-12-12T12:25:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/76/
Test PASSed (JDK 8 and Scala 2.11).
"
267309754,2244,asfbot,2016-12-15T11:51:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/158/
Test PASSed (JDK 7 and Scala 2.10).
"
267345180,2244,asfbot,2016-12-15T14:47:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/162/
Test PASSed (JDK 8 and Scala 2.12).
"
267351820,2244,asfbot,2016-12-15T15:13:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/163/
Test FAILed (JDK 8 and Scala 2.12).
"
267376007,2244,asfbot,2016-12-15T16:40:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/162/
Test PASSed (JDK 7 and Scala 2.10).
"
267381237,2244,asfbot,2016-12-15T16:59:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/164/
Test PASSed (JDK 8 and Scala 2.11).
"
267565714,2244,asfbot,2016-12-16T10:37:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/188/
Test PASSed (JDK 8 and Scala 2.12).
"
267565801,2244,asfbot,2016-12-16T10:37:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/189/
Test PASSed (JDK 8 and Scala 2.11).
"
267588693,2244,asfbot,2016-12-16T12:56:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/187/
Test FAILed (JDK 7 and Scala 2.10).
"
267960956,2244,asfbot,2016-12-19T13:13:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/236/
Test FAILed (JDK 7 and Scala 2.10).
"
267962730,2244,asfbot,2016-12-19T13:22:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/238/
Test FAILed (JDK 8 and Scala 2.11).
"
267963586,2244,asfbot,2016-12-19T13:27:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/237/
Test PASSed (JDK 8 and Scala 2.12).
"
270099517,2244,asfbot,2017-01-03T11:56:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/435/
Test PASSed (JDK 8 and Scala 2.11).
"
270100687,2244,asfbot,2017-01-03T12:03:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/433/
Test PASSed (JDK 7 and Scala 2.10).
"
270110170,2244,asfbot,2017-01-03T13:05:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/436/
Test FAILed (JDK 8 and Scala 2.11).
"
270110201,2244,asfbot,2017-01-03T13:05:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/435/
Test FAILed (JDK 8 and Scala 2.12).
"
270111040,2244,asfbot,2017-01-03T13:11:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/434/
Test FAILed (JDK 7 and Scala 2.10).
"
270612637,2244,asfbot,2017-01-05T10:28:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/515/
Test PASSed (JDK 7 and Scala 2.10).
"
270621596,2244,asfbot,2017-01-05T11:16:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/517/
Test FAILed (JDK 8 and Scala 2.11).
"
270622767,2244,asfbot,2017-01-05T11:23:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/516/
Test FAILed (JDK 8 and Scala 2.12).
"
270882471,2244,enothereska,2017-01-06T11:06:28Z,So far reviewed only top level APIs and LGTM. Need to review rest.
270884592,2244,asfbot,2017-01-06T11:18:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/571/
Test PASSed (JDK 8 and Scala 2.12).
"
270884626,2244,asfbot,2017-01-06T11:18:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/572/
Test PASSed (JDK 8 and Scala 2.11).
"
270886316,2244,asfbot,2017-01-06T11:29:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/570/
Test FAILed (JDK 7 and Scala 2.10).
"
270926726,2244,asfbot,2017-01-06T15:25:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/572/
Test PASSed (JDK 7 and Scala 2.10).
"
270939817,2244,asfbot,2017-01-06T16:19:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/573/
Test FAILed (JDK 8 and Scala 2.12).
"
270943903,2244,asfbot,2017-01-06T16:36:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/574/
Test FAILed (JDK 8 and Scala 2.11).
"
270945528,2244,enothereska,2017-01-06T16:43:36Z,I've reviewed next level down up to tests. Overall LGTM. There are opportunities for doing a code refactor later on (post-PR) because the separate thread and state manager are similar to the existing code. 
271262349,2244,asfbot,2017-01-09T11:26:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/617/
Test FAILed (JDK 8 and Scala 2.12).
"
271262503,2244,asfbot,2017-01-09T11:27:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/616/
Test PASSed (JDK 7 and Scala 2.10).
"
271264571,2244,enothereska,2017-01-09T11:39:43Z,"Had a look at tests, they LGTM. Overall I personally feel this optimisation adds a lot of complexity but I suppose that ship has sailed."
271276988,2244,asfbot,2017-01-09T12:51:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/618/
Test PASSed (JDK 8 and Scala 2.11).
"
271393703,2244,guozhangwang,2017-01-09T20:10:03Z,"@dguy @mjsax About the `old value computation` issue, I cannot think of a quick solution on top of my head and you may not like my ""long"" solution, but here it goes:

1. Materialize the KTable-KTable Join results as well as the joining KTales. As we discussed in another thread of KTable API confusions this is the last place where we may not materialize a KTable.

2. After all KTables are materialized, we then do not need to pass the ""<old value, new value>"" pair before the join operations at all, but only before the aggregation operations, since we can always read the old joined value from the materialized store.

3. For Global KTables, if we add its self joins with global KTables as well in the future, it will no be `virtual` either but also materialized.

4. One potential issue is, say if we have a multi-way table joins, then it may not be really worthwhile to materialize the intermediate tables as well. This can possibly be resolved when we extend the DSL parsing to global context to optimize some of the topologies."
271529418,2244,dguy,2017-01-10T09:39:26Z,"@guozhangwang - your points above - that is the same solution i had in mind. I didn't really want to go down that path due to the overhead of introducing another store, changelog etc, but maybe that is the only way. 
There is also an issue i discovered on KTable/KTable join which means we might need to do something like this anyway - https://issues.apache.org/jira/browse/KAFKA-4609"
271534897,2244,asfbot,2017-01-10T10:03:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/658/
Test PASSed (JDK 8 and Scala 2.11).
"
271535371,2244,asfbot,2017-01-10T10:06:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/656/
Test PASSed (JDK 7 and Scala 2.10).
"
271550625,2244,asfbot,2017-01-10T11:19:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/657/
Test FAILed (JDK 8 and Scala 2.12).
"
271551637,2244,asfbot,2017-01-10T11:24:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/663/
Test PASSed (JDK 8 and Scala 2.11).
"
271551902,2244,asfbot,2017-01-10T11:26:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/661/
Test FAILed (JDK 7 and Scala 2.10).
"
271560509,2244,dguy,2017-01-10T12:13:50Z,"@mjsax, @guozhangwang - i've fixed the issue by materializing the join as G mentioned above. This does mean i'll need to update the KIP to add 2 overloaded methods as we need the Serdes to write to the store.
At the moment i think we either go this way or we remove the KTable/GlobalKTable joins for now. My preference is probably to go with it"
271560885,2244,asfbot,2017-01-10T12:15:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/662/
Test PASSed (JDK 8 and Scala 2.12).
"
271567308,2244,asfbot,2017-01-10T12:50:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/666/
Test PASSed (JDK 8 and Scala 2.12).
"
271567414,2244,asfbot,2017-01-10T12:50:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/667/
Test PASSed (JDK 8 and Scala 2.11).
"
271600216,2244,asfbot,2017-01-10T15:11:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/665/
Test FAILed (JDK 7 and Scala 2.10).
"
271671444,2244,asfbot,2017-01-10T19:22:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/680/
Test PASSed (JDK 8 and Scala 2.11).
"
271693026,2244,asfbot,2017-01-10T20:46:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/679/
Test FAILed (JDK 8 and Scala 2.12).
"
271693217,2244,asfbot,2017-01-10T20:47:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/678/
Test PASSed (JDK 7 and Scala 2.10).
"
271705967,2244,asfbot,2017-01-10T21:38:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/699/
Test PASSed (JDK 8 and Scala 2.11).
"
271706099,2244,asfbot,2017-01-10T21:39:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/697/
Test PASSed (JDK 8 and Scala 2.12).
"
271714351,2244,asfbot,2017-01-10T22:11:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/697/
Test PASSed (JDK 7 and Scala 2.10).
"
271955933,2244,asfbot,2017-01-11T18:39:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/748/
Test FAILed (JDK 8 and Scala 2.11).
"
271956809,2244,asfbot,2017-01-11T18:43:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/746/
Test FAILed (JDK 8 and Scala 2.12).
"
271956950,2244,asfbot,2017-01-11T18:43:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/746/
Test PASSed (JDK 7 and Scala 2.10).
"
272077407,2244,guozhangwang,2017-01-12T05:08:25Z,LGTM. Could you rebase?
272127373,2244,dguy,2017-01-12T10:24:03Z,@guozhangwang - done.
272155580,2244,asfbot,2017-01-12T12:51:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/781/
Test FAILed (JDK 8 and Scala 2.12).
"
272155615,2244,asfbot,2017-01-12T12:52:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/783/
Test FAILed (JDK 8 and Scala 2.11).
"
272155620,2244,asfbot,2017-01-12T12:52:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/781/
Test FAILed (JDK 7 and Scala 2.10).
"
272216338,2244,dguy,2017-01-12T16:49:11Z,test this please
272228680,2244,asfbot,2017-01-12T17:34:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/800/
Test PASSed (JDK 8 and Scala 2.11).
"
272229062,2244,asfbot,2017-01-12T17:35:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/798/
Test PASSed (JDK 8 and Scala 2.12).
"
272230059,2244,asfbot,2017-01-12T17:39:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/798/
Test PASSed (JDK 7 and Scala 2.10).
"
272262967,2244,guozhangwang,2017-01-12T19:46:40Z,"Merged to trunk. Thanks @dguy !!

Please feel free to close the corresponding JIRA when ASF is back online."
257023225,2074,vahidhashemian,2016-10-28T20:42:10Z,"@hachikuji This PR also fixes another issue that was introduced by the fix for KAFKA-3144. I point to it inline. Let me know if you think that issue should be fixed and merged (more quickly) in a separate PR. Thanks.
"
266516580,2074,asfbot,2016-12-12T18:50:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/83/
Test FAILed (JDK 8 and Scala 2.11).
"
266562616,2074,asfbot,2016-12-12T21:45:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/81/
Test FAILed (JDK 7 and Scala 2.10).
"
266562653,2074,asfbot,2016-12-12T21:45:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/82/
Test FAILed (JDK 8 and Scala 2.12).
"
266854543,2074,asfbot,2016-12-13T20:37:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/112/
Test PASSed (JDK 8 and Scala 2.11).
"
266854550,2074,asfbot,2016-12-13T20:37:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/111/
Test PASSed (JDK 8 and Scala 2.12).
"
266855052,2074,asfbot,2016-12-13T20:39:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/110/
Test FAILed (JDK 7 and Scala 2.10).
"
266855542,2074,vahidhashemian,2016-12-13T20:41:13Z,@hachikuji I've tried to address both KIP-88 and KAFKA-3853 in the same PR here. I'd appreciate your feedback when you get a chance to review this. Thanks.
270800460,2074,asfbot,2017-01-06T00:43:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/551/
Test PASSed (JDK 8 and Scala 2.12).
"
270803737,2074,asfbot,2017-01-06T01:04:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/552/
Test FAILed (JDK 8 and Scala 2.11).
"
270805819,2074,asfbot,2017-01-06T01:18:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/550/
Test PASSed (JDK 7 and Scala 2.10).
"
270983985,2074,asfbot,2017-01-06T19:26:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/580/
Test PASSed (JDK 8 and Scala 2.11).
"
270984004,2074,asfbot,2017-01-06T19:26:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/579/
Test FAILed (JDK 8 and Scala 2.12).
"
271001497,2074,asfbot,2017-01-06T20:45:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/578/
Test PASSed (JDK 7 and Scala 2.10).
"
271678517,2074,vahidhashemian,2017-01-10T19:48:44Z,"@ewencp Thanks for your thorough review of the PR and your feedback. I updated the PR based on your comments (except for the part that's related to KIP-97, as discussed)."
271678585,2074,asfbot,2017-01-10T19:48:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/693/
Test FAILed (JDK 8 and Scala 2.11).
"
271705182,2074,asfbot,2017-01-10T21:35:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/691/
Test FAILed (JDK 7 and Scala 2.10).
"
271705665,2074,asfbot,2017-01-10T21:37:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/691/
Test FAILed (JDK 8 and Scala 2.12).
"
271732620,2074,asfbot,2017-01-10T23:35:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/703/
Test FAILed (JDK 7 and Scala 2.10).
"
271739093,2074,asfbot,2017-01-11T00:13:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/703/
Test FAILed (JDK 8 and Scala 2.12).
"
271742743,2074,hachikuji,2017-01-11T00:35:18Z,@vahidhashemian @ewencp Took a very quick look at this and it seems like it's not too far. I'm thinking we might want to let the client compatibility KIP go in first since it is already quite large and it will be easier to make the changes here to continue supporting the old version of the offset fetch request. Does that seem reasonable?
271743697,2074,asfbot,2017-01-11T00:41:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/705/
Test FAILed (JDK 8 and Scala 2.11).
"
271751793,2074,vahidhashemian,2017-01-11T01:28:59Z,"@hachikuji Yes, that sounds good to me. Thanks for reviewing the patch. I'll work on the rest of your review items in the meantime."
271970642,2074,hachikuji,2017-01-11T19:33:55Z,@vahidhashemian Looks like KIP-97 (client compatibility) was merged.
272019686,2074,vahidhashemian,2017-01-11T22:44:25Z,@ewencp @hachikuji Thanks again for your feedback. I've tried to address them in the latest update. Feel free to take a look while I work on rebasing the PR to cover client compatibility (KIP-97).
272028519,2074,asfbot,2017-01-11T23:26:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/762/
Test PASSed (JDK 8 and Scala 2.12).
"
272029184,2074,asfbot,2017-01-11T23:29:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/764/
Test FAILed (JDK 8 and Scala 2.11).
"
272051929,2074,asfbot,2017-01-12T01:43:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/762/
Test FAILed (JDK 7 and Scala 2.10).
"
272351325,2074,asfbot,2017-01-13T03:22:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/825/
Test FAILed (JDK 8 and Scala 2.12).
"
272355611,2074,asfbot,2017-01-13T04:01:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/827/
Test PASSed (JDK 8 and Scala 2.11).
"
272356386,2074,asfbot,2017-01-13T04:09:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/825/
Test PASSed (JDK 7 and Scala 2.10).
"
272361345,2074,hachikuji,2017-01-13T04:57:23Z,"@vahidhashemian Saw your question about why we are only checking 3 of the 5 error codes. Haha, I noticed the same thing when reviewing and opened KAFKA-4622."
272362673,2074,vahidhashemian,2017-01-13T05:10:48Z,Thanks for clarifying. Then I can leave that to be fixed as part of that JIRA.
272507729,2074,hachikuji,2017-01-13T18:15:21Z,"@vahidhashemian I merged KIP-103, which looks like it causes some conflicts in `ApiVersion`. Hopefully shouldn't be too much trouble to resolve."
272572167,2074,asfbot,2017-01-13T23:05:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/866/
Test PASSed (JDK 8 and Scala 2.11).
"
272589147,2074,asfbot,2017-01-14T01:15:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/875/
Test PASSed (JDK 8 and Scala 2.11).
"
272589246,2074,asfbot,2017-01-14T01:17:04Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/864/
Test FAILed (JDK 7 and Scala 2.10).
"
272589379,2074,asfbot,2017-01-14T01:18:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/864/
Test FAILed (JDK 8 and Scala 2.12).
"
272598360,2074,asfbot,2017-01-14T03:28:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/873/
Test FAILed (JDK 7 and Scala 2.10).
"
272598361,2074,asfbot,2017-01-14T03:28:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/873/
Test FAILed (JDK 8 and Scala 2.12).
"
329325194,3849,apurvam,2017-09-13T23:27:51Z,"Thanks for the PR @sutambe . Looking over the changes, it seems that there are two cases from the KIP which don't seem to be covered: 

1. Setting the `pollTimeout` to be the expiryTime of the oldest batch being sent in the produce request. I think we need this to make sure that we expire batches in a timely manner.
2. Related to the previous point, the current patch doesn't seem to expire inflight requests, which is another feature that the KIP seems to promise.

Have I missed something? Or are you planning on adding the functionality above?"
329635100,3849,apurvam,2017-09-14T23:22:39Z,"Heads up @sutambe , the following PR just got merged, and may have some conflicts with the current patch : https://github.com/apache/kafka/pull/3743

There shouldn't be any impact on the logic, however."
330281613,3849,ijuma,2017-09-18T16:41:36Z,Friendly reminder that the feature freeze is this Wednesday.
330318612,3849,becketqin,2017-09-18T18:43:40Z,"@ijuma Just want to check. Do you think this feature is a ""minor"" feature?"
330426320,3849,ijuma,2017-09-19T04:31:01Z,"@becketqin, it is possible to classify this as a minor feature, but the fact that it affects a core part of the Producer puts it in a bit of a grey area. If the PR is almost ready and we miss the feature freeze, my take is that it would be OK to merge it by the end of this week. Later than that and it seems a bit risky.

It's a bit worrying that the merge conflicts haven't been resolved since last week."
330584441,3849,sutambe,2017-09-19T15:52:00Z,@ijuma  @becketqin I've an new PR but after a rebase I've to fix one more test. Working on that now.
330586776,3849,ijuma,2017-09-19T15:58:56Z,Thanks @sutambe!
330600198,3849,sutambe,2017-09-19T16:44:02Z,@apurvam It's not clear to me why `testExpiryOfFirstBatchShouldCauseResetIfFutureBatchesFail` and `testExpiryOfFirstBatchShouldNotCauseUnresolvedSequencesIfFutureBatchesSucceed` are failing. It looks like a batch that should be in IncompleteBatches isn't there. Any thoughts?
330674030,3849,tedyu,2017-09-19T21:08:10Z,"I added the following to testExpiryOfFirstBatchShouldCauseResetIfFutureBatchesFail before the first sender.run() call
```
        Sender sender = new Sender(logContext, client, metadata, this.accumulator, true, MAX_REQUEST_SIZE, ACKS_ALL, 10,
            new Metrics(), new SenderMetricsRegistry(), time, REQUEST_TIMEOUT, DELIVERY_TIMEOUT, 50, transactionManager, apiVersions);
```
The test still fails."
330699245,3849,apurvam,2017-09-19T23:10:16Z,@sutambe where are those tests failing? The latest PR builder suggests that the clients and core tests all passed.
330700192,3849,sutambe,2017-09-19T23:16:08Z,@apurvam @ijuma @becketqin The Sender and RecordAccumulator are passing now. The failing tests are connect tests that are irrelevant. 
330701042,3849,apurvam,2017-09-19T23:21:58Z,"@sutambe I don't think the test failures are irrelevant since the same 3 tests failed in all the runs. Further, the cause of the failure is: 

```
java.lang.AssertionError: 
  Unexpected method call Listener.onFailure(job-0, org.apache.kafka.common.KafkaException: Failed to construct kafka producer):
```

I think their mocks may need to be updated to take account of the new configs and attendant `ConfigExceptions`"
331024421,3849,apurvam,2017-09-21T01:24:08Z,"@sutambe I had a look at the failing Sender expiry tests. What is happening is that the tests are not modified to account for the fact that the inflight batches can be expired. In the tests, we used to expire a batch sitting in the accumulator but _not_ the inflight batch. When the inflight batch returned, it would be re queued. 

But now, the test sends the response for the inflight batch, but when it goes to requeue, it discovers that there shouldn't be an inflight request an raises an exception.

The tests should be updated to account for the new behavior and make sure that the inflight batch is _not_ expired."
331024720,3849,apurvam,2017-09-21T01:26:25Z,"Actually, the test reveals a bug in the current patch: the response for the inflight batch which expired is not being handled correctly. We should not be trying to requeue it to start with.

So we need two tests: one where the inflight batch is not expired, and the current case. The reenqueue logic in the sender needs to be updated to not reenqueue the expired batches."
331324500,3849,sutambe,2017-09-22T01:22:37Z,"@becketqin  Added a couple of tests. 
Test a batch is correctly inserted into the in.flight.batches if needed. And not inserted if not needed.
`RecordAccumulatorTest.testSoontoExpireBatchesArePickedUp`
Test the callback of an expired batch is fired in time when it is in-flight/not in-flight
`SenderTest.testInflightBatchesExpireOnDeliveryTimeout`"
331497207,3849,hachikuji,2017-09-22T16:36:14Z,@sutambe I don't see any changes since my comments. Are you sure you have pushed them?
331581751,3849,sutambe,2017-09-22T23:21:09Z,"@becketqin  Added more tests
Test when expire an in-flight batch, we still wait for the request to finish before sending the next batch = `SenderTest.testWhenFirstBatchExpireNoSendSecondBatchIfGuranteeOrder`
Test we are not going to retry an already expired batch = `SenderTest.testExpiredBatchDoesNotRetry`

Regarding the last test ""Test when batch is expired prematurely, the buffer pool is only deallocated after the response is returned. (because we are still holding the batch until the response is returned)"". I'm not sure if it's really needed because `RecordAccumulator.abortBatches` deallocates all incomplete batches (whether they have been sent or not). 
"
331584625,3849,becketqin,2017-09-22T23:49:16Z,"@sutambe I am looking at the update patch.

> Regarding the last test ""Test when batch is expired prematurely, the buffer pool is only deallocated after the response is returned. (because we are still holding the batch until the response is returned)"". I'm not sure if it's really needed because RecordAccumulator.abortBatches deallocates all incomplete batches (whether they have been sent or not).

`RecordAccumulator.abortBatches()` is only used when the producer is force closed. We do not need to worry about the memory consumption at that point anymore because the producer has stopped accepting new messages."
331585074,3849,becketqin,2017-09-22T23:53:45Z,@sutambe It looks there are 3 test failures from connect. Can you check?
331613171,3849,becketqin,2017-09-23T05:59:15Z,"@sutambe BTW, I did not see the tests to ensure we are not double deallocating memory. Will you add those tests?"
331983684,3849,apurvam,2017-09-25T19:15:38Z,"FYI, the connect tests are still failing with:

```
Caused by: org.apache.kafka.common.config.ConfigException: Must set delivery.timeout.ms higher than linger.ms + request.timeout.ms
	at org.apache.kafka.clients.producer.KafkaProducer.configureDeliveryTimeout(KafkaProducer.java:452)
	at org.apache.kafka.clients.producer.KafkaProducer.<init>(KafkaProducer.java:380)
	... 35 more
```

It would be good to get some green builds before this can be considered safe for merge."
331987993,3849,sutambe,2017-09-25T19:33:20Z,@apurvam @becketqin Fixed Connect tests.
332034321,3849,apurvam,2017-09-25T22:40:11Z,"I have a high level comment about the inflight batch handling. I was wondering what the advantages/disadvantages of the current approach (where batches are grouped by creation time) would be against adding a `Map<TopicPartition, PriorityQueue<ProducerBatch>>` to the RecordAccumulator.  This would complement the existing `Map<TopicPartition, Deque<ProducerBatch>>`. 

The priority queue would be ordered by the batch creation time (similar to the current `NavigableMap`).

We would add to the queue on drain. And we would remove from the queue in `Sender.failBatch` and `Sender.completeBatch`, just like we do for the `TransactionManager` (see: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/producer/internals/Sender.java#L589). So we don't have any special logic for 

The `RecordAccumulator.expiredBatches` would now iterate over both maps (`TopicPartition -> Deque` and `TopicPartition -> PriorityQueue`), with the priority queue being iterated on first. This doesn't change the time complexity of the algorithm, since we are iterating over all partitions in `expiredBatches` once anyway.

The advantage of this approach is that managing inflight batches is much simpler (add to the queue on drain, remove on completion). It doesn't complicate the expiry in any way.

The main disadvantage that I see is in computing the `earliestDeliveryTimeout`. For this you would need to peek at the head of the queue for each partition which has an inflight batch. But this should be fairly cheap, since we would be iterating over 1000's of objects even in the largest installations.

What do you think?
"
332035491,3849,apurvam,2017-09-25T22:46:19Z,retest this please
332039241,3849,apurvam,2017-09-25T23:08:58Z,"I made this comment in a thread, but that may be buried because github collapses comments. There is a bug in the current implementation which means that we will retain references to `ProducerBatches` in the producer long after they have been successfully completed. The following sequence of assertions in any `SenderTest` shows the presence of this bug: 

```java
    assertFalse(client.hasInFlightRequests());
    // the following should be true, since there are no in flight requests.
    assertFalse(accumulator.soonToExpireInFlightBatches().isEmpty());
    time.sleep(130000);
    // we now 'expire' the completed inflight batches
    sender.run(time.milliseconds());
    // the following shows that we hang on to references for 120 seconds by default.
    assertTrue(accumulator.soonToExpireInFlightBatches().isEmpty());
```"
332043954,3849,becketqin,2017-09-25T23:38:36Z,"@apurvam Initially I was under the same impression. One concern of the priority queue is that whenever we need to remove things out of the queue, we will have to iterate over it. Since a producer can poll thousands of times per seconds, and maybe remove tens of thousands of batches. That said, in most cases, the queue would be empty because most of the batches are sent when remaining delivery.timeout.ms is greater than request.timeout.ms. However, it may still slowdown things when retries occur. My take on this is that if the implementation complexity does not increase much, we may want to follow the robust way instead.

Regarding the memory issue. We could not release the memory of an expired batch right away when the batch is still in-flight. This is because the in-flight batch is still referred by the RequestCompletionHandler of the request. We could remove it from the RequestCompletionHandler, but this is a little tricky."
332044968,3849,apurvam,2017-09-25T23:45:40Z,"@becketqin I am not sure I follow this: 

> Regarding the memory issue. We could not release the memory of an expired batch right away when the batch is still in-flight. This is because the in-flight batch is still referred by the RequestCompletionHandler of the request. We could remove it from the RequestCompletionHandler, but this is a little tricky.

Is this a response to my comment above where I shared a snippet from `SenderTest` to show that we are retaining batches after they are completed? If so, I didn't mean that we need to release an inflight batch right away. What I was pointing out is that the _only_ way the current patch is deleting batches is by expiring them. This may not be a big deal, since the deallocation is done when the batch is completed, but it still seems like an unnecessary anomaly."
332045109,3849,apurvam,2017-09-25T23:46:29Z,"Also, it looks like recent commits are causing threads to leak, @sutambe ."
332046231,3849,apurvam,2017-09-25T23:54:07Z,"> One concern of the priority queue is that whenever we need to remove things out of the queue, we will have to iterate over it. Since a producer can poll thousands of times per seconds, and maybe remove tens of thousands of batches. That said, in most cases, the queue would be empty because most of the batches are sent when remaining delivery.timeout.ms is greater than request.timeout.ms. However, it may still slowdown things when retries occur. My take on this is that if the implementation complexity does not increase much, we may want to follow the robust way instead.

Not sure I follow. It would be no worse that what we do today with the `Deque` except it would be limited only to partitions which had in flight batches. So it is not making things much worse."
332058975,3849,becketqin,2017-09-26T01:26:11Z,"@apurvam 
> Not sure I follow. It would be no worse that what we do today with the Deque except it would be limited only to partitions which had in flight batches. So it is not making things much worse.

I misunderstood your proposal. I though you were suggesting to use a priority queue to store all the in-flight batches. A `Map<TopicPartition, PriorityQueue<ProducerBatch>>` sounds reasonable."
332080828,3849,apurvam,2017-09-26T04:06:38Z,"@becketqin 

> I misunderstood your proposal. I though you were suggesting to use a priority queue to store all the in-flight batches. A Map<TopicPartition, PriorityQueue<ProducerBatch>> sounds reasonable.

Yes, I meant a separate `Map<TopicPartition, PriorityQueue<ProducerBatch>>`, to replace the current `NavigableMap`. "
332662977,3849,hachikuji,2017-09-27T21:36:49Z,@sutambe Any updates on this? Do you need help addressing the test failures?
332973054,3849,sutambe,2017-09-28T21:46:31Z,@hachikuji  Sorry for disappearing on you guys. I got sucked into an oncall issue. So I had to context switch. I plan to resume work on this PR next week. I've not had a chance to respond to the comments so far. Thank for you the review so far. It's been very useful.
343013804,3849,asfgit,2017-11-09T01:04:29Z,"FAILURE
 7505 tests run, 2 skipped, 168 failed.
--none--"
343017903,3849,asfgit,2017-11-09T01:28:16Z,"FAILURE
 7910 tests run, 5 skipped, 48 failed.
--none--"
343023020,3849,asfgit,2017-11-09T01:58:03Z,"FAILURE
 7916 tests run, 5 skipped, 46 failed.
--none--"
343593982,3849,asfgit,2017-11-10T21:37:27Z,"FAILURE
 7280 tests run, 1 skipped, 0 failed.
--none--"
343606533,3849,asfgit,2017-11-10T22:42:11Z,"FAILURE
 7964 tests run, 5 skipped, 1 failed.
--none--"
343608954,3849,asfgit,2017-11-10T22:57:34Z,"FAILURE
 7964 tests run, 5 skipped, 1 failed.
--none--"
344707856,3849,sutambe,2017-11-15T19:49:41Z,@becketqin @apurvam I've updated the PR. Please take a look
344730375,3849,asfgit,2017-11-15T21:14:47Z,"FAILURE
 7964 tests run, 5 skipped, 17 failed.
--none--"
344731637,3849,asfgit,2017-11-15T21:19:39Z,"FAILURE
 7964 tests run, 5 skipped, 16 failed.
--none--"
344734652,3849,apurvam,2017-11-15T21:30:07Z,"Please fix the failing tests, there is a config exception due to the new delivery timeout config."
344755906,3849,asfgit,2017-11-15T22:50:29Z,"FAILURE
 7280 tests run, 1 skipped, 0 failed.
--none--"
344772831,3849,asfgit,2017-11-16T00:15:33Z,"FAILURE
 4026 tests run, 1 skipped, 1 failed.
--none--"
344773127,3849,asfgit,2017-11-16T00:17:19Z,"FAILURE
 4026 tests run, 1 skipped, 1 failed.
--none--"
344773309,3849,asfgit,2017-11-16T00:18:23Z,"FAILURE
 4026 tests run, 1 skipped, 1 failed.
--none--"
345016755,3849,asfgit,2017-11-16T18:36:57Z,"SUCCESS 
 8077 tests run, 5 skipped, 0 failed.
--none--"
345027740,3849,asfgit,2017-11-16T18:59:12Z,"SUCCESS 
 8077 tests run, 5 skipped, 0 failed.
--none--"
345028513,3849,asfgit,2017-11-16T19:01:45Z,"SUCCESS 
 8077 tests run, 5 skipped, 0 failed.
--none--"
345031109,3849,sutambe,2017-11-16T19:11:24Z,@becketqin @apurvam All checks passed
353216067,3849,sutambe,2017-12-20T23:47:57Z,"@apurvam @becketqin I updated the implementation to use `ConcurrentMap<TopicPartition, Deque<ProducerBatch>>`. Please take a look. I don't see the above test failures on my machine."
358116907,3849,apurvam,2018-01-16T21:42:58Z,retest this please
358416813,3849,apurvam,2018-01-17T19:32:17Z,retest this please
358480286,3849,apurvam,2018-01-17T23:08:17Z,"So the `org.apache.kafka.clients.producer.internals.SenderTest.testMetadataTopicExpiry` test has failed twice in a row with: 

```
java.lang.ArrayIndexOutOfBoundsException
	at java.base/java.util.zip.CRC32C.update(CRC32C.java:151)
	at org.apache.kafka.common.utils.Checksums.update(Checksums.java:42)
	at org.apache.kafka.common.utils.Crc32C.compute(Crc32C.java:72)
	at org.apache.kafka.common.record.DefaultRecordBatch.writeHeader(DefaultRecordBatch.java:468)
	at org.apache.kafka.common.record.MemoryRecordsBuilder.writeDefaultBatchHeader(MemoryRecordsBuilder.java:357)
	at org.apache.kafka.common.record.MemoryRecordsBuilder.close(MemoryRecordsBuilder.java:311)
	at org.apache.kafka.clients.producer.internals.ProducerBatch.close(ProducerBatch.java:427)
	at org.apache.kafka.clients.producer.internals.RecordAccumulator.drain(RecordAccumulator.java:614)
	at org.apache.kafka.clients.producer.internals.Sender.sendProducerData(Sender.java:270)
	at org.apache.kafka.clients.producer.internals.Sender.run(Sender.java:238)
	at org.apache.kafka.clients.producer.internals.SenderTest.testMetadataTopicExpiry(SenderTest.java:473)
```

Given that these changes are on the same code, and given the consistent failure of this test, it is probably a regression. @sutambe can you reproduce the failure locally?"
358480445,3849,apurvam,2018-01-17T23:08:58Z,"Just looking at the stack trace and the test, it may be that an expired batch is being closed twice in some cases."
368266171,3849,hachikuji,2018-02-24T22:43:34Z,@sutambe @becketqin It would be nice to unblock this. Can someone else pick up the work?
370507706,3849,becketqin,2018-03-05T18:02:09Z,"@hachikuji Yeah, this has been pending for too long. I have spoken to @sutambe and he said he still wants to finish the patch. He will figure out the ETA and see if that works."
385707559,3849,bbejeck,2018-05-01T15:56:47Z,@sutambe @becketqin is there any update on the status of this PR?  It would be great if we could get this in the next release.  
387590454,3849,Ishiihara,2018-05-09T01:18:58Z,@guozhangwang @junrao @bbejeck @becketqin We also hit this issue when running Kafka Streams library with some high volume output topics. It would be nice to get this moving and push it to the next release.
387592858,3849,radai-rosenblatt,2018-05-09T01:35:41Z,"becket cant load this page for some reason (some weird issue with his github profile?).
we are ok with you taking over this patch."
387609246,3849,sutambe,2018-05-09T03:28:23Z,"@bbejeck @apurvam @becketqin @hachikuji I don't have any update since Dec last year. Sorry, the work has stalled and it has been very hard to find cycles for this effort. I don't mind if Confluent wants to take this effort forward. Better later than never."
387636464,3849,Ishiihara,2018-05-09T06:41:26Z,cc @abbccdda @yuyang08
393617255,3849,yuyang08,2018-05-31T17:49:33Z,"@sutambe   i made some change based on your pull request to fix style check and test failure. do yo mind I amend the change to this pull request?  cc @becketqin  @apurvam @hachikuji  
  https://github.com/yuyang08/kafka/commit/69fc79a91d0556408c8037649f1e03aa56206ef2"
393700104,3849,guozhangwang,2018-05-31T22:22:49Z,@yuyang08 I'd suggest you create your own PR against apache kafka trunk and let other reviewers to continue reviewing that one.
393760744,3849,yuyang08,2018-06-01T05:18:09Z,@guozhangwang  sure.  will create a separate pull request 
399271744,3849,yuyang08,2018-06-21T23:12:09Z,@guozhangwang  @apurvam @becketqin  created new pr https://github.com/apache/kafka/pull/5270 for KAFKA-5886
464845906,3849,ijuma,2019-02-18T19:07:09Z,"This has been merged via a different PR, closing."
217925465,1336,vahidhashemian,2016-05-09T17:05:54Z,"@junrao Would appreciate your input on this pull request.
"
226875059,1336,vahidhashemian,2016-06-17T20:35:45Z,"@junrao Thank you for your feedback. The PR was updated to address your comments.
"
227269465,1336,hachikuji,2016-06-20T21:06:06Z,"@vahidhashemian I'm wondering about approaches for testing this patch. One possibility is to parse the output in the test case and build assertions off from that. Another option might be to decouple the command logic from the print logic so that you can build test cases using a convenient object representation. What do you think?
"
227306696,1336,vahidhashemian,2016-06-21T00:12:42Z,"@junrao @hachikuji Thank you very much for your careful review and feedback. I made some changes in the new patch based on the feedback received.

@junrao The only part that is outstanding in addressing your feedback is [your comment about how to find members with no topic partition](https://github.com/apache/kafka/pull/1336#discussion_r67726141). I responded to your comment and will wait for your clarification as I may be missing something.

@hachikuji Both approaches you suggested for designing unit tests for this patch should work. Parsing the output and writing assertions based on that would be faster but probably not the cleaner option, in my opinion. Separating the logic from reporting seems more reasonable to me but it would require more time and thinking. I can take either direction once I hear your (and others') feedback about it.
"
227503027,1336,hachikuji,2016-06-21T16:56:47Z,"@vahidhashemian Yeah, I agree that separating the logic would make testing easier. It's more painful now, but making the code more testable would probably pay off in the long run. I wonder if we could introduce a simple trait to encapsulate the output writing. For example:

``` scala
trait OutputWriter {
  def printError(msg)
  def printAssignmentHeader()
  def printAssignment(topic, partition, memberId,.... etc)
}
```

Then we could test the command using a mock implementation. Anyway, if the level of effort to get some basic coverage using this or another pattern is not too high, I'd suggest we do it here. Otherwise, we could create a followup JIRA.
"
227780324,1336,vahidhashemian,2016-06-22T15:26:15Z,"@hachikuji Thanks for your suggestion on how to go about separating the logic. I'll give it a try and let you know how it goes.
"
228908052,1336,vahidhashemian,2016-06-27T23:39:00Z,"@hachikuji I've spent some time on this and it seems to involve restructuring of the code to some extent (assuming I haven't over-complicated things). So I'm wondering whether we should keep the changes simpler and do this work and unit tests in separate work items.
"
228910259,1336,hachikuji,2016-06-27T23:52:18Z,"@vahidhashemian Either way works for me. I don't think there's any particular hurry to get this patch in since it will probably go in 0.10.1, so if restructuring the code seems like a good idea, I'm probably slightly in favor of doing it here so we don't forget about it later. That said, I know it can be painful to fill in some of these testing gaps (they're gaps because testing wasn't easy!), so if @junrao is OK with it, then doing it separately seems fine to me.
"
229165709,1336,vahidhashemian,2016-06-28T20:03:32Z,"@hachikuji Sounds good. I'm okay both ways and will wait for @junrao's feedback.

Regarding the unit tests, I'm trying to wrap my head around how to write them with proper mocking. You can see [here](https://github.com/vahidhashemian/kafka/compare/KAFKA-3144...vahidhashemian:KAFKA-3144-WITH-UNIT-TESTS?expand=1) how I've made changes to the last patch so the `OutputWriter` trait that you suggested is used. The issue now is I don't see clearly how to write the unit tests with mocking (for example, for `--describe` the main functionality is inside the protected `describeGroup()` method). This tells me perhaps further restructuring is required to make the command properly mockable, but I wanted to ask your opinion before heading down that road. Thanks.
"
232168291,1336,vahidhashemian,2016-07-12T20:22:52Z,"@hachikuji I restructured `ConsumerGroupCommand.scala` a little bit for testing purposes according to your suggestion and added a number of unit tests. I would appreciate your feedback.
"
233806677,1336,vahidhashemian,2016-07-20T00:32:44Z,"@hachikuji This in another pending PR that conflicts with changes required for [KAFKA-3853](https://issues.apache.org/jira/browse/KAFKA-3853), and would be great if we can finalize it soon. Thanks.
"
234130456,1336,vahidhashemian,2016-07-21T01:16:08Z,"@hachikuji  Thanks for the thorough feedback. I hope I addressed all of them in the recent update.
"
235795010,1336,vahidhashemian,2016-07-28T04:01:06Z,"@hachikuji Thanks for another round of reviews. The PR is updated to address your comments.
"
236029039,1336,vahidhashemian,2016-07-28T21:21:22Z,"Thanks @hachikuji. Made those changes.
"
245448879,1336,vahidhashemian,2016-09-07T23:19:52Z,"@junrao The patch is updated to include coordinator info when `--new-consumer` is used. Since the group column and the new coordinator column report redundant information I modified the output of `--new-consumer` option to

```
Note: This will only show information about consumers that use the new consumer API (non-ZooKeeper-based consumers).

Consumer group: cgroup1
Consumer group coordinator broker id: 0

TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        MEMBER-ID                                          MEMBER-HOST                    CLIENT-ID
bar                            0          234             234             0          consumer-1-a0d7940a-c90a-40df-b995-64d2e104bbb6    /127.0.0.1                     consumer-1
```

The ZooKeeper based output will not print the coordinator line above.

Your feedback is appreciated.
"
254966900,1336,vahidhashemian,2016-10-19T23:09:19Z,"@hachikuji Thanks for the thorough feedback. I hope I was able to address your comments in the recent patch.
"
255177309,1336,vahidhashemian,2016-10-20T17:46:36Z,"@hachikuji I submitted another update taking into account your comments. Thanks again for the thorough reviews and constructive feedback.
"
255270819,1336,vahidhashemian,2016-10-21T01:25:25Z,"@hachikuji I had one more crack at this. If you spot any more potential improvements please let me know. Thanks.

BTW, the junkins error seems to be unrelated to this.
"
255469685,1336,vahidhashemian,2016-10-21T21:28:39Z,"Just did some cleanup of variable/method/field names based on the recent `member id` to `consumer id` renaming.
"
255487773,1336,hachikuji,2016-10-21T23:22:08Z,"LGTM. Really appreciate the effort on this PR. It's a huge contribution when you can not only add functionality, but improve the testability of the code. 
"
255492874,1336,vahidhashemian,2016-10-22T00:08:19Z,"Thanks @hachikuji for your feedback along the way. Very much appreciated.
"
289544312,2744,asfbot,2017-03-27T18:35:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2417/
Test FAILed (JDK 7 and Scala 2.10).
"
289545357,2744,asfbot,2017-03-27T18:39:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2421/
Test FAILed (JDK 8 and Scala 2.11).
"
289545389,2744,asfbot,2017-03-27T18:39:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2417/
Test FAILed (JDK 8 and Scala 2.12).
"
289600365,2744,asfbot,2017-03-27T22:05:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2424/
Test FAILed (JDK 7 and Scala 2.10).
"
289602082,2744,asfbot,2017-03-27T22:13:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2424/
Test FAILed (JDK 8 and Scala 2.12).
"
289603467,2744,asfbot,2017-03-27T22:20:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2428/
Test FAILed (JDK 8 and Scala 2.11).
"
289729990,2744,asfbot,2017-03-28T10:34:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2449/
Test PASSed (JDK 8 and Scala 2.12).
"
289730038,2744,asfbot,2017-03-28T10:35:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2449/
Test PASSed (JDK 7 and Scala 2.10).
"
289758759,2744,asfbot,2017-03-28T12:48:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2453/
Test FAILed (JDK 8 and Scala 2.11).
"
289794025,2744,asfbot,2017-03-28T14:47:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2457/
Test FAILed (JDK 8 and Scala 2.12).
"
289797575,2744,asfbot,2017-03-28T14:57:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2457/
Test FAILed (JDK 7 and Scala 2.10).
"
289806303,2744,asfbot,2017-03-28T15:24:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2461/
Test FAILed (JDK 8 and Scala 2.11).
"
289918595,2744,asfbot,2017-03-28T22:01:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2473/
Test FAILed (JDK 7 and Scala 2.10).
"
289918826,2744,asfbot,2017-03-28T22:02:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2473/
Test FAILed (JDK 8 and Scala 2.12).
"
289923862,2744,asfbot,2017-03-28T22:26:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2477/
Test FAILed (JDK 8 and Scala 2.11).
"
290057333,2744,asfbot,2017-03-29T11:06:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2492/
Test PASSed (JDK 8 and Scala 2.11).
"
290058472,2744,asfbot,2017-03-29T11:11:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2488/
Test FAILed (JDK 8 and Scala 2.12).
"
290087131,2744,asfbot,2017-03-29T13:19:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2488/
Test FAILed (JDK 7 and Scala 2.10).
"
290099754,2744,asfbot,2017-03-29T14:03:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2494/
Test FAILed (JDK 8 and Scala 2.12).
"
290103007,2744,asfbot,2017-03-29T14:13:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2498/
Test FAILed (JDK 8 and Scala 2.11).
"
290103100,2744,asfbot,2017-03-29T14:14:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2494/
Test FAILed (JDK 7 and Scala 2.10).
"
290130197,2744,asfbot,2017-03-29T15:38:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2500/
Test FAILed (JDK 8 and Scala 2.11).
"
290130247,2744,asfbot,2017-03-29T15:38:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2496/
Test FAILed (JDK 7 and Scala 2.10).
"
290140656,2744,asfbot,2017-03-29T16:12:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2496/
Test PASSed (JDK 8 and Scala 2.12).
"
290147412,2744,asfbot,2017-03-29T16:35:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2500/
Test PASSed (JDK 7 and Scala 2.10).
"
290148928,2744,asfbot,2017-03-29T16:41:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2500/
Test FAILed (JDK 8 and Scala 2.12).
"
290160071,2744,asfbot,2017-03-29T17:19:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2504/
Test PASSed (JDK 8 and Scala 2.11).
"
290404058,2744,rajinisivaram,2017-03-30T13:03:31Z,"@junrao Thank you for the review, I have addressed the comments and left one question.

System tests passed here: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/814/
"
290404481,2744,asfbot,2017-03-30T13:05:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2531/
Test FAILed (JDK 8 and Scala 2.12).
"
290404552,2744,asfbot,2017-03-30T13:05:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2535/
Test FAILed (JDK 8 and Scala 2.11).
"
290420593,2744,asfbot,2017-03-30T14:04:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2531/
Test PASSed (JDK 7 and Scala 2.10).
"
290422163,2744,asfbot,2017-03-30T14:09:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2538/
Test PASSed (JDK 8 and Scala 2.11).
"
290424250,2744,asfbot,2017-03-30T14:16:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2534/
Test PASSed (JDK 8 and Scala 2.12).
"
290434378,2744,asfbot,2017-03-30T14:47:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2534/
Test FAILed (JDK 7 and Scala 2.10).
"
290704773,2744,asfbot,2017-03-31T12:57:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2575/
Test PASSed (JDK 7 and Scala 2.10).
"
290705369,2744,asfbot,2017-03-31T13:00:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2575/
Test PASSed (JDK 8 and Scala 2.12).
"
290713268,2744,asfbot,2017-03-31T13:34:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2579/
Test PASSed (JDK 8 and Scala 2.11).
"
290761695,2744,asfbot,2017-03-31T16:29:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2577/
Test PASSed (JDK 8 and Scala 2.12).
"
290762873,2744,asfbot,2017-03-31T16:34:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2581/
Test FAILed (JDK 8 and Scala 2.11).
"
290765462,2744,asfbot,2017-03-31T16:44:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2577/
Test PASSed (JDK 7 and Scala 2.10).
"
290820520,2744,asfbot,2017-03-31T20:24:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2584/
Test FAILed (JDK 8 and Scala 2.12).
"
290820529,2744,asfbot,2017-03-31T20:24:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2584/
Test FAILed (JDK 7 and Scala 2.10).
"
290823748,2744,asfbot,2017-03-31T20:38:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2588/
Test PASSed (JDK 8 and Scala 2.11).
"
292580163,2744,rajinisivaram,2017-04-07T16:12:14Z,"@ijuma Thank you for the review. I haven't moved throttle-time to the response header as explained in the previous comment. One alternative to making the code in `KafkaApis` neater would be to make `throttleTimeMs` a non-final field. That way the response could be created as it is now, and throttle time can just be updated in the response. I didn't want to do that initially because it makes the code inconsistent, but perhaps that is ok for this case?"
292585909,2744,asfbot,2017-04-07T16:34:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2825/
Test PASSed (JDK 8 and Scala 2.11).
"
292586337,2744,asfbot,2017-04-07T16:35:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2821/
Test PASSed (JDK 8 and Scala 2.12).
"
292588406,2744,asfbot,2017-04-07T16:43:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2821/
Test PASSed (JDK 7 and Scala 2.10).
"
292655619,2744,asfbot,2017-04-07T21:24:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2834/
Test PASSed (JDK 8 and Scala 2.11).
"
292658390,2744,asfbot,2017-04-07T21:38:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2830/
Test PASSed (JDK 8 and Scala 2.12).
"
292659119,2744,asfbot,2017-04-07T21:42:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2830/
Test PASSed (JDK 7 and Scala 2.10).
"
296140304,2744,ijuma,2017-04-21T09:22:51Z,"When it comes to performance tests, it would also be good to run them on AWS as VMs like Xen often have an impact of time-related methods. For example: https://blog.packagecloud.io/eng/2017/03/08/system-calls-are-much-slower-on-ec2/"
296242715,2744,rajinisivaram,2017-04-21T16:44:49Z,"@junrao @ijuma Thank you for the reviews. I have rebased.

I have performance tests on my laptop with 8 threads and see no noticeable difference.
- Producer (100 byte messages): 161.92 MB/s (before) and 164.4 MB/s(after)
- Consumer (100 byte messages) : 168.27 MB/se (before) and 173.5 MB/s (after)

I don't have an account to run the tests on AWS, but I will start a system test run to compare."
296253492,2744,asfbot,2017-04-21T17:28:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3100/
Test PASSed (JDK 8 and Scala 2.11).
"
296254025,2744,asfbot,2017-04-21T17:30:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3095/
Test PASSed (JDK 7 and Scala 2.10).
"
296255430,2744,asfbot,2017-04-21T17:36:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3096/
Test PASSed (JDK 8 and Scala 2.12).
"
296455215,2744,ijuma,2017-04-23T16:32:17Z,"By the way, since the additional `System.nanoTime` call per selection key only happens if the new config is enabled, I think it's OK to merge this and do additional performance testing later. I think this is one of the cases where issues only show themselves when the number of threads and CPUs is larger than one can test on a laptop."
296608816,2744,rajinisivaram,2017-04-24T10:24:47Z,"@ijuma Yes, agree that it is difficult to assess the impact of the change on a laptop.
@junrao I have addressed most of the issues. Left comments for the others.

Many thanks for the reviews."
296814266,2744,rajinisivaram,2017-04-24T20:35:12Z,"@junrao Thank you for the review.
I have run the performance tests on my laptop with high request quota configured. Again, there was no noticeable difference.
Producer (100 bytes): trunk: 151.33 MB/s   requestQuota: 155.78 MB/s
Consumer (100 bytes): trunk: 332.32 MB/s   requestQuota: 339.64 MB/s

Also started a system test run with the latest level of code: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/855/"
296837249,2744,asfbot,2017-04-24T22:15:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3144/
Test FAILed (JDK 8 and Scala 2.12).
"
296847675,2744,asfbot,2017-04-24T23:15:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3148/
Test FAILed (JDK 8 and Scala 2.11).
"
296847676,2744,asfbot,2017-04-24T23:15:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3145/
Test PASSed (JDK 7 and Scala 2.10).
"
296856040,2744,asfbot,2017-04-25T00:15:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3148/
Test PASSed (JDK 8 and Scala 2.12).
"
296911327,2744,asfbot,2017-04-25T05:15:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3154/
Test PASSed (JDK 8 and Scala 2.11).
"
296955219,2744,asfbot,2017-04-25T08:15:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3156/
Test FAILed (JDK 8 and Scala 2.12).
"
296969710,2744,asfbot,2017-04-25T09:15:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3161/
Test FAILed (JDK 8 and Scala 2.11).
"
297013271,2744,rajinisivaram,2017-04-25T12:24:17Z,System tests have passed: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/855/
297026135,2744,asfbot,2017-04-25T13:15:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3158/
Test PASSed (JDK 7 and Scala 2.10).
"
298118490,2744,rajinisivaram,2017-04-28T22:11:50Z,@junrao Thank you for the reviews. Have rebased and addressed the comments.
298143929,2744,asfbot,2017-04-29T03:24:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3289/
Test FAILed (JDK 7 and Scala 2.10).
"
298143990,2744,asfbot,2017-04-29T03:26:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3291/
Test PASSed (JDK 8 and Scala 2.11).
"
298144177,2744,asfbot,2017-04-29T03:31:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3285/
Test PASSed (JDK 8 and Scala 2.12).
"
298363968,2744,junrao,2017-05-01T16:11:47Z,@rajinisivaram : Thanks for the latest patch. LGTM.
729396557,9487,ableegoldman,2020-11-18T04:23:42Z,System test run (still running but so far it's all PASS) -- https://jenkins.confluent.io/job/system-test-kafka-branch-builder/4292/
729474257,9487,ableegoldman,2020-11-18T06:53:32Z,"System tests passed, all three Java builds passed. Merging now"
729475252,9487,ableegoldman,2020-11-18T06:55:22Z,Merged to trunk  
764961250,9487,mjsax,2021-01-21T21:45:25Z,@wcarlson5 Seems we forgot the update the docs for this KIP. Can you do a follow up PR? This follow up PR should also cover the changes of https://issues.apache.org/jira/browse/KAFKA-10810 that is part if the same KIP.
857564039,10851,lkokhreidze,2021-06-09T10:04:17Z,Call for review @cadonna @vvcephei @ableegoldman
862166415,10851,lkokhreidze,2021-06-16T08:33:17Z,gentle nudge @cadonna @vvcephei @ableegoldman
879032262,10851,cadonna,2021-07-13T12:08:48Z,"@lkokhreidze Sorry for the long silence! Some other tasks got in my way. I plan to review this PR tomorrow.
"
880127165,10851,lkokhreidze,2021-07-14T18:46:39Z,"Thanks @cadonna for the feedback.
I've replied/addressed all of your comments."
882283484,10851,lkokhreidze,2021-07-19T06:41:50Z,"Thanks @cadonna for the valuable feedback  
I've replied/addressed your comments."
885645624,10851,cadonna,2021-07-23T13:38:45Z,@lkokhreidze FYI: I will be offline the next two weeks. I am sorry that I haven't had time to review this PR this week.
899245208,10851,lkokhreidze,2021-08-16T06:01:13Z,"Hi @cadonna !
Thanks for the feedback. I will address your comments this week."
902900779,10851,lkokhreidze,2021-08-20T19:11:41Z,"Hi @cadonna 
Thank you for valuable feedback. 
I've addressed your comments. Please have a look once you got time.
"
905490438,10851,lkokhreidze,2021-08-25T13:13:26Z,"Thanks for the feedback @cadonna , I've pushed the new changes."
905514160,10851,lkokhreidze,2021-08-25T13:44:11Z,"The general thought on the implementation. 
As of now, we choose concrete `StandbyTaskAssignor` implementation based on passed `AssignmentConfigs` value.
Instead, an alternative approach would be to have a chained standby task assignment based on multiple implementations. For instance, when required client tag configuration is present, we can try to assign the standby tasks based on `List.of(new ClientTagAwareStandbyTaskAssignor(), new LeastLoadedClientStandbyTaskAssignor())`. This way, `ClientTagAwareStandbyTaskAssignor` can try to distribute the standby tasks based on tags dimensions. If there are no more client tag dimensions available, and we still have `any(tasksToRemainingStandbys) > 0`, the next implementation in the chain (in this case `LeastLoadedClientStandbyTaskAssignor`) will be called. With this, `LeastLoadedClientStandbyTaskAssignor` can default to assigning the remaining standbys to the least loaded clients. Right now, `ClientTagAwareStandbyTaskAssignor` does both assignments based on available dimensions and fallback to the least loaded if there are no enough tag dimensions. Current implementation leads to more complex code. While with the approach above, we can clearly separate the implementations without duplication (there's no code duplication, rather ""logic"" duplication).
For now, I've chosen to go with the simplest approach - having two independent implementations and selecting an appropriate one based on passed `AssignmentConfigs.` Still, I wanted to share this idea here, just in case. "
911752624,10851,lkokhreidze,2021-09-02T14:33:30Z,"Hi @cadonna,
I've addressed/replied to your comments. Thanks for the feedback.
FYI - I'll be offline from next week for 2 weeks."
925659295,10851,lkokhreidze,2021-09-23T09:45:41Z,Hi @cadonna is it possible to continue pushing this PR forward? I'm back from my holidays.
927659618,10851,cadonna,2021-09-27T08:47:34Z,Welcome back @lkokhreidze! Sorry for the silence around this PR. I got side-tracked by other tasks. I will try to review this PR this week.
934149747,10851,lkokhreidze,2021-10-05T07:40:28Z,"Thanks for the feedback @cadonna 
I will review and address your comments this week."
944314838,10851,lkokhreidze,2021-10-15T13:45:39Z,"Hi @cadonna,
Small update on my side - wasn't able to find time to work on this PR this week.
Will try to prioritise this for the next week."
959050366,10851,lkokhreidze,2021-11-03T13:16:55Z,"Hi @cadonna 
Very sorry for disappearing, didn't find time to deal with this PR.
I've addressed your comments. Please have another look when you got time."
983959899,10851,lkokhreidze,2021-12-01T18:55:18Z,"Hi @cadonna ,
Will you have time to look at this PR again?"
984453564,10851,cadonna,2021-12-02T09:36:10Z,@lkokhreidze I am really sorry that I haven't found the time to look at your updated PR. I will put it on my list for next week. 
984477740,10851,lkokhreidze,2021-12-02T10:06:16Z,@cadonna no worries and thank you.
993399413,10851,lkokhreidze,2021-12-14T10:30:39Z,"Hi @cadonna thanks for the feedback.
I'll address your comments shortly."
1017438641,10851,lkokhreidze,2022-01-20T12:17:45Z,"Hi @cadonna small update from my side.
I came back from holidays a week ago so will continue working on this PR this week.
Sorry for the delay."
1017815908,10851,lkokhreidze,2022-01-20T18:51:38Z,"Hi @cadonna,
I've address your comments with the latest commit.
Please have a look when you got time.
Thanks. "
1018535614,10851,lkokhreidze,2022-01-21T14:08:34Z,"The latest commit (be3dcff4dc463fd8d23998537e36f852b99ec083) has a few changes.

1. There's explicit fallback to fallback to `DefaultStandbyTaskAssignor` logic if rack aware standby task assignment fails to assign standby tasks.
2. Rack aware standby task assignment logic takes into account client capacity when assigning the standby task. If client has reached the capacity, algorithm won't assign standby task to it. When that happens, we will fallback to the least loaded clients. I believe this is better approach, as we will avoid overloading the clients. There will be a warning log, to inform the user.

**EDIT:** Instead of just checking if capacity is reached on the client, we also now check if load can be balanced [here](https://github.com/apache/kafka/pull/10851/commits/a010d9093889f8e985022e2f727254d087f54891#diff-9988f8ab5112be9826a66ecc4b2df28eea61e8ceb15ed19a1b94b747a6fb48c8R213).

Looking forward hearing your thoughts, @cadonna "
1025503003,10851,lkokhreidze,2022-01-31T08:48:28Z,"Hi @cadonna ,
Sorry for the ping. Any chance we could review PR this week?
Thanks"
1030773435,10851,showuon,2022-02-06T08:06:28Z,"@lkokhreidze , thanks for the PR. I'll take a look next week. Thanks."
1032412988,10851,cadonna,2022-02-08T09:48:50Z,@lkokhreidze I will try to review this Friday. Sorry for the delay but I was sick for the last two weeks.
1041380775,10851,lkokhreidze,2022-02-16T11:16:09Z,"Hi @showuon, @cadonna 

Thanks for the valuable feedback. I've addressed your comments and pushed the changes. I also resolved conversations feel free to unresolve them if you think I haven't addressed your comments.
Looking forward hearing your thoughts.
"
1044574529,10851,lkokhreidze,2022-02-18T14:02:00Z,"Hi @showuon Thanks for the feedback.
I've addressed your comments."
1046758399,10851,lkokhreidze,2022-02-21T11:08:33Z,"Thanks @showuon for the valuable feedback.
I've addressed your comments, please have a look when you got time."
1048612254,10851,lkokhreidze,2022-02-23T09:56:15Z,"Hi @cadonna @showuon 
I've addressed your comments. Please have a look.
Thank you!"
1050043629,10851,lkokhreidze,2022-02-24T16:38:30Z,"Hi @cadonna 
Mind having another look? Hoping to finalise this PR as soon as possible :)
Thanks"
1055442027,10851,cadonna,2022-03-01T13:26:38Z,"@lkokhreidze I will try to have a look this week. I would also like to get this PR merged as soon as possible. Since @showuon  has already approved it. If I will not manage to have a look, @showuon can merge it. "
1055489018,10851,lkokhreidze,2022-03-01T14:14:22Z,"Thanks @cadonna, appreciate it.
@showuon please also note that next PR, the protocol change, is also ready to be reviewed. https://github.com/apache/kafka/pull/10802

Whenever it's possible, please have a look at that too.
"
1056919176,10851,cadonna,2022-03-02T13:15:29Z,"@lkokhreidze Let me know if you want to address my minor comments in this PR. After that @showuon or I can merge this PR.

Nice work!"
1056940382,10851,lkokhreidze,2022-03-02T13:37:19Z,"Hi @cadonna , thanks for the review.
I agree with what you said and made a note to myself to address your comments in the follow-up PRs. So if it's okay, I think we can merge this one.

Thank you!"
1057630852,10851,showuon,2022-03-03T03:30:46Z,"Failed tests are unrelated and also failed in `trunk` build.
```
Build / ARM / org.apache.kafka.streams.integration.NamedTopologyIntegrationTest.shouldRemoveOneNamedTopologyWhileAnotherContinuesProcessing
Build / JDK 11 and Scala 2.13 / kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize()
Build / JDK 11 and Scala 2.13 / kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize()
Build / JDK 11 and Scala 2.13 / org.apache.kafka.streams.integration.NamedTopologyIntegrationTest.shouldRemoveOneNamedTopologyWhileAnotherContinuesProcessing
Build / JDK 17 and Scala 2.13 / kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize()
Build / JDK 17 and Scala 2.13 / kafka.server.DynamicBrokerReconfigurationTest.testThreadPoolResize()
Build / JDK 17 and Scala 2.13 / org.apache.kafka.streams.integration.NamedTopologyIntegrationTest.shouldRemoveOneNamedTopologyWhileAnotherContinuesProcessing
Build / JDK 8 and Scala 2.12 / org.apache.kafka.connect.integration.RebalanceSourceConnectorsIntegrationTest.testDeleteConnector
Build / JDK 8 and Scala 2.12 / org.apache.kafka.connect.integration.RebalanceSourceConnectorsIntegrationTest.testDeleteConnector
Build / JDK 8 and Scala 2.12 / org.apache.kafka.streams.integration.NamedTopologyIntegrationTest.shouldRemoveOneNamedTopologyWhileAnotherContinuesProcessing
```"
1057900673,10851,cadonna,2022-03-03T10:28:49Z, 
137871412,195,asfbot,2015-09-04T22:30:23Z,"[kafka-trunk-git-pr #359](https://builds.apache.org/job/kafka-trunk-git-pr/359/) FAILURE
Looks like there's a problem with this pull request
"
137872274,195,Parth-Brahmbhatt,2015-09-04T22:37:57Z,"The only test failure reported by the bot was 
kafka.producer.ProducerTest > testSendWithDeadBroker FAILED
    java.lang.AssertionError: Message set should have 1 message
        at org.junit.Assert.fail(Assert.java:88)
        at org.junit.Assert.assertTrue(Assert.java:41)
        at kafka.producer.ProducerTest.testSendWithDeadBroker(ProducerTest.scala:260)

Which passes locally and seems very unrelated to the changes i made. I will try and run the test in a loop to see if I can reproduce.
"
137878169,195,asfbot,2015-09-04T23:32:08Z,"[kafka-trunk-git-pr #363](https://builds.apache.org/job/kafka-trunk-git-pr/363/) SUCCESS
This pull request looks good
"
139683279,195,Parth-Brahmbhatt,2015-09-11T23:04:29Z,"@junrao Added a generic ZkNodeChangeListener and removed the scheduler thread. Couldn't think of a better name for the class so if you have a better name let me know. Addressed all other comments in this PR.
"
139694395,195,asfbot,2015-09-12T01:04:57Z,"[kafka-trunk-git-pr #410](https://builds.apache.org/job/kafka-trunk-git-pr/410/) FAILURE
Looks like there's a problem with this pull request
"
139699066,195,asfbot,2015-09-12T01:31:33Z,"[kafka-trunk-git-pr #411](https://builds.apache.org/job/kafka-trunk-git-pr/411/) FAILURE
Looks like there's a problem with this pull request
"
140487639,195,asfbot,2015-09-15T18:14:00Z,"[kafka-trunk-git-pr #430](https://builds.apache.org/job/kafka-trunk-git-pr/430/) SUCCESS
This pull request looks good
"
140901623,195,asfbot,2015-09-16T21:36:56Z,"[kafka-trunk-git-pr #437](https://builds.apache.org/job/kafka-trunk-git-pr/437/) FAILURE
Looks like there's a problem with this pull request
"
141147224,195,asfbot,2015-09-17T16:56:15Z,"[kafka-trunk-git-pr #439](https://builds.apache.org/job/kafka-trunk-git-pr/439/) FAILURE
Looks like there's a problem with this pull request
"
141181822,195,asfbot,2015-09-17T18:37:52Z,"[kafka-trunk-git-pr #440](https://builds.apache.org/job/kafka-trunk-git-pr/440/) FAILURE
Looks like there's a problem with this pull request
"
141352763,195,Parth-Brahmbhatt,2015-09-18T05:53:26Z,"@junrao  @ijuma all comments addressed. There is no return statement in the code now.
"
141354083,195,asfbot,2015-09-18T06:05:15Z,"[kafka-trunk-git-pr #442](https://builds.apache.org/job/kafka-trunk-git-pr/442/) SUCCESS
This pull request looks good
"
141530173,195,asfbot,2015-09-18T18:35:27Z,"[kafka-trunk-git-pr #444](https://builds.apache.org/job/kafka-trunk-git-pr/444/) FAILURE
Looks like there's a problem with this pull request
"
141551857,195,asfbot,2015-09-18T19:58:47Z,"[kafka-trunk-git-pr #445](https://builds.apache.org/job/kafka-trunk-git-pr/445/) FAILURE
Looks like there's a problem with this pull request
"
142060970,195,ijuma,2015-09-21T17:56:29Z,"@Parth-Brahmbhatt I did a PR to your branch (https://github.com/Parth-Brahmbhatt/kafka/pull/1) with some suggested code readability improvements. If you agree you can merge it to your branch.
"
142071503,195,Parth-Brahmbhatt,2015-09-21T18:41:03Z,"@ijuma changes to ZkNodeChangeNotificationListener can not be accepted as @junrao wants the import to be local.

Other than that  have incorporated your proposed changes.
"
142071956,195,ijuma,2015-09-21T18:43:00Z,"@Parth-Brahmbhatt, Thanks for incorporating the changes. My understanding is that @junrao wanted the import to be local if `JavaConversions` was used. With `JavaConverters`, this is not needed because you have the `asScala` and `asJava` to show you that a conversion is happening (unlike the `JavaConversions` case where everything happens silently).
"
142073897,195,asfbot,2015-09-21T18:51:00Z,"[kafka-trunk-git-pr #464](https://builds.apache.org/job/kafka-trunk-git-pr/464/) FAILURE
Looks like there's a problem with this pull request
"
142079878,195,asfbot,2015-09-21T19:06:50Z,"[kafka-trunk-git-pr #465](https://builds.apache.org/job/kafka-trunk-git-pr/465/) FAILURE
Looks like there's a problem with this pull request
"
142098566,195,junrao,2015-09-21T20:24:01Z,"Also, for JavaConverters, it's ok to import it globally since we are using asScala explicitly.
"
142102372,195,Parth-Brahmbhatt,2015-09-21T20:40:46Z,"All comments addressed.
"
142126737,195,asfbot,2015-09-21T22:24:30Z,"[kafka-trunk-git-pr #469](https://builds.apache.org/job/kafka-trunk-git-pr/469/) FAILURE
Looks like there's a problem with this pull request
"
142141700,195,asfbot,2015-09-21T23:56:37Z,"[kafka-trunk-git-pr #475](https://builds.apache.org/job/kafka-trunk-git-pr/475/) SUCCESS
This pull request looks good
"
142148112,195,junrao,2015-09-22T00:45:15Z,"Thanks a lot for the patch. LGTM.
"
142148924,195,ijuma,2015-09-22T00:54:08Z,"Thanks Parth!
"
297192374,2910,ijuma,2017-04-25T23:16:30Z,retest this please
297201690,2910,hachikuji,2017-04-26T00:17:09Z,"I realized that the relationship between the LSO and the high watermark is trickier than I originally thought. I had assumed that it was sufficient to take the minimum of the two as the maximum offset that a client could fetch in READ_COMMITTED mode. The issue is when the COMMIT or ABORT marker itself is larger than the high watermark. We need to ensure that none of the records from the associated transaction becomes readable until the marker itself is replicated (even if the records have offsets lower than the high watermark). This requires a little more bookkeeping as we need to keep track of which transactions have been completed, but are not replicated. I'll update the patch as soon as I have a solution ready."
297266526,2910,ijuma,2017-04-26T07:29:22Z,retest this please
297508059,2910,hachikuji,2017-04-26T18:56:31Z,"note to self: once the transactional client patch lands, we should update `Fetcher` to use the control sequence number when finding the commit/abort markers."
297584653,2910,asfbot,2017-04-27T01:19:07Z,"Build finished. 2798 tests run, 0 skipped, 3 failed.
Test FAILed (JDK 8 and Scala 2.12).
"
297584654,2910,asfbot,2017-04-27T01:19:07Z,"Build finished. 2140 tests run, 0 skipped, 0 failed.
Test FAILed (JDK 7 and Scala 2.10).
"
297600345,2910,asfbot,2017-04-27T03:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3182/
Test FAILed (JDK 8 and Scala 2.12).
"
297607513,2910,asfbot,2017-04-27T04:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3198/
Test FAILed (JDK 8 and Scala 2.11).
"
297622327,2910,asfbot,2017-04-27T06:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3196/
Test FAILed (JDK 7 and Scala 2.10).
"
297646233,2910,asfbot,2017-04-27T08:19:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3203/
Test FAILed (JDK 8 and Scala 2.11).
"
297660630,2910,asfbot,2017-04-27T09:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3195/
Test PASSed (JDK 8 and Scala 2.12).
"
297674183,2910,asfbot,2017-04-27T10:19:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3201/
Test FAILed (JDK 7 and Scala 2.10).
"
297727649,2910,asfbot,2017-04-27T14:20:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3208/
Test FAILed (JDK 8 and Scala 2.12).
"
297746937,2910,asfbot,2017-04-27T15:20:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3214/
Test FAILed (JDK 8 and Scala 2.11).
"
297750879,2910,asfbot,2017-04-27T15:34:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3224/
Test FAILed (JDK 7 and Scala 2.10).
"
297751381,2910,asfbot,2017-04-27T15:35:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3208/
Test FAILed (JDK 7 and Scala 2.10).
"
297754428,2910,asfbot,2017-04-27T15:46:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3216/
Test FAILed (JDK 8 and Scala 2.12).
"
297783722,2910,asfbot,2017-04-27T17:28:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3230/
Test PASSed (JDK 8 and Scala 2.11).
"
297831007,2910,asfbot,2017-04-27T20:32:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3234/
Test FAILed (JDK 8 and Scala 2.11).
"
297837720,2910,asfbot,2017-04-27T20:59:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3226/
Test FAILed (JDK 8 and Scala 2.12).
"
297844061,2910,asfbot,2017-04-27T21:26:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3231/
Test FAILed (JDK 7 and Scala 2.10).
"
297866165,2910,asfbot,2017-04-27T23:25:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3234/
Test PASSed (JDK 8 and Scala 2.12).
"
297867657,2910,asfbot,2017-04-27T23:34:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3237/
Test PASSed (JDK 7 and Scala 2.10).
"
297867836,2910,asfbot,2017-04-27T23:36:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3251/
Test FAILed (JDK 8 and Scala 2.11).
"
297867953,2910,asfbot,2017-04-27T23:37:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3242/
Test FAILed (JDK 8 and Scala 2.12).
"
297868220,2910,asfbot,2017-04-27T23:38:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3247/
Test FAILed (JDK 7 and Scala 2.10).
"
297869782,2910,asfbot,2017-04-27T23:49:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3244/
Test FAILed (JDK 8 and Scala 2.11).
"
298104360,2910,asfbot,2017-04-28T20:55:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3285/
Test FAILed (JDK 7 and Scala 2.10).
"
298104686,2910,asfbot,2017-04-28T20:56:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3290/
Test PASSed (JDK 8 and Scala 2.11).
"
298111383,2910,asfbot,2017-04-28T21:30:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3281/
Test PASSed (JDK 8 and Scala 2.12).
"
298430411,2910,asfbot,2017-05-01T20:56:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3339/
Test FAILed (JDK 8 and Scala 2.11).
"
298436227,2910,asfbot,2017-05-01T21:21:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3334/
Test FAILed (JDK 7 and Scala 2.10).
"
298438385,2910,asfbot,2017-05-01T21:32:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3330/
Test FAILed (JDK 8 and Scala 2.12).
"
298485145,2910,asfbot,2017-05-02T02:52:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3347/
Test FAILed (JDK 8 and Scala 2.11).
"
298485670,2910,asfbot,2017-05-02T02:57:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3342/
Test PASSed (JDK 7 and Scala 2.10).
"
298489437,2910,asfbot,2017-05-02T03:34:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3338/
Test PASSed (JDK 8 and Scala 2.12).
"
298735836,2910,asfbot,2017-05-02T19:26:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3378/
Test FAILed (JDK 7 and Scala 2.10).
"
298737058,2910,asfbot,2017-05-02T19:31:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3375/
Test FAILed (JDK 8 and Scala 2.12).
"
298737106,2910,asfbot,2017-05-02T19:31:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3384/
Test FAILed (JDK 8 and Scala 2.11).
"
298774220,2910,asfbot,2017-05-02T22:00:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3393/
Test FAILed (JDK 8 and Scala 2.11).
"
298774299,2910,asfbot,2017-05-02T22:01:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3384/
Test FAILed (JDK 8 and Scala 2.12).
"
298777089,2910,asfbot,2017-05-02T22:15:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3387/
Test FAILed (JDK 7 and Scala 2.10).
"
298787333,2910,asfbot,2017-05-02T23:15:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3396/
Test FAILed (JDK 8 and Scala 2.11).
"
298789682,2910,asfbot,2017-05-02T23:31:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3387/
Test PASSed (JDK 8 and Scala 2.12).
"
298791380,2910,asfbot,2017-05-02T23:41:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3390/
Test FAILed (JDK 7 and Scala 2.10).
"
298828817,2910,hachikuji,2017-05-03T05:56:40Z,"@junrao I've added a commit to do transaction index recovery. The approach I've taken is the following:

1. I've removed the periodic PID snapshot.
2. Instead, when the log rolls, I take a new snapshot using the offset of the new segment.
3. When a segment has been flushed to disk, I remove the corresponding snapshot. The snapshot corresponding the start of the active segment is never removed.
4. When recovering a segment, I have to read the log from the starting segment in the worst case in order to rebuild the transaction index. Once finished rebuilding an individual segment, I take another snapshot so that we don't have to repeat the same work if we have to recover another segment. To make this efficient, I ensure that logs are loaded/recovered in ascending order.
5. When the log is cleanly closed, I take another snapshot. I was a bit unsure of the benefit of this. If we can always or usually expect to truncate upon startup, then perhaps the benefit is low and we need to revisit periodic PID expiration to reduce the chance that we have to scan the last segment upon initialization."
298839924,2910,asfbot,2017-05-03T07:24:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3403/
Test PASSed (JDK 7 and Scala 2.10).
"
298841835,2910,asfbot,2017-05-03T07:36:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3400/
Test FAILed (JDK 8 and Scala 2.12).
"
298843411,2910,asfbot,2017-05-03T07:45:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3409/
Test PASSed (JDK 8 and Scala 2.11).
"
298974390,2910,asfbot,2017-05-03T17:08:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3430/
Test FAILed (JDK 8 and Scala 2.11).
"
298974579,2910,asfbot,2017-05-03T17:09:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3424/
Test FAILed (JDK 7 and Scala 2.10).
"
298976294,2910,asfbot,2017-05-03T17:16:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3421/
Test FAILed (JDK 8 and Scala 2.12).
"
299007011,2910,asfbot,2017-05-03T19:11:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3435/
Test PASSed (JDK 8 and Scala 2.11).
"
299008010,2910,asfbot,2017-05-03T19:15:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3426/
Test PASSed (JDK 8 and Scala 2.12).
"
299008253,2910,asfbot,2017-05-03T19:16:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3429/
Test FAILed (JDK 7 and Scala 2.10).
"
299056819,2910,asfbot,2017-05-03T22:47:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3451/
Test FAILed (JDK 8 and Scala 2.11).
"
299059212,2910,asfbot,2017-05-03T23:00:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3442/
Test FAILed (JDK 8 and Scala 2.12).
"
299072768,2910,asfbot,2017-05-04T00:35:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3445/
Test FAILed (JDK 7 and Scala 2.10).
"
299079303,2910,asfbot,2017-05-04T01:35:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3458/
Test PASSed (JDK 8 and Scala 2.11).
"
299081022,2910,asfbot,2017-05-04T01:53:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3452/
Test FAILed (JDK 7 and Scala 2.10).
"
299082733,2910,asfbot,2017-05-04T02:12:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3449/
Test FAILed (JDK 8 and Scala 2.12).
"
299088415,2910,asfbot,2017-05-04T03:15:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3455/
Test FAILed (JDK 8 and Scala 2.12).
"
299088615,2910,asfbot,2017-05-04T03:18:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3464/
Test FAILed (JDK 8 and Scala 2.11).
"
299090180,2910,asfbot,2017-05-04T03:36:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3458/
Test FAILed (JDK 7 and Scala 2.10).
"
299104955,2910,asfbot,2017-05-04T06:22:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3462/
Test PASSed (JDK 7 and Scala 2.10).
"
299107267,2910,asfbot,2017-05-04T06:37:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3468/
Test FAILed (JDK 8 and Scala 2.11).
"
299110383,2910,asfbot,2017-05-04T06:58:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3459/
Test FAILed (JDK 8 and Scala 2.12).
"
299138194,2910,asfbot,2017-05-04T09:33:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3479/
Test FAILed (JDK 8 and Scala 2.11).
"
299144089,2910,asfbot,2017-05-04T10:03:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3473/
Test FAILed (JDK 7 and Scala 2.10).
"
299144563,2910,asfbot,2017-05-04T10:05:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3470/
Test FAILed (JDK 8 and Scala 2.12).
"
299306796,2910,asfbot,2017-05-04T20:56:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3501/
Test FAILed (JDK 7 and Scala 2.10).
"
299307800,2910,asfbot,2017-05-04T21:00:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3498/
Test PASSed (JDK 8 and Scala 2.12).
"
299313942,2910,asfbot,2017-05-04T21:26:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3507/
Test PASSed (JDK 8 and Scala 2.11).
"
299342928,2910,asfbot,2017-05-05T00:12:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3515/
Test FAILed (JDK 8 and Scala 2.11).
"
299343027,2910,asfbot,2017-05-05T00:13:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3506/
Test FAILed (JDK 8 and Scala 2.12).
"
299343514,2910,asfbot,2017-05-05T00:17:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3509/
Test FAILed (JDK 7 and Scala 2.10).
"
299346976,2910,asfbot,2017-05-05T00:50:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3512/
Test FAILed (JDK 8 and Scala 2.12).
"
299346990,2910,asfbot,2017-05-05T00:50:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3521/
Test FAILed (JDK 8 and Scala 2.11).
"
299347145,2910,asfbot,2017-05-05T00:52:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3515/
Test FAILed (JDK 7 and Scala 2.10).
"
299354739,2910,asfbot,2017-05-05T02:07:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3527/
Test FAILed (JDK 8 and Scala 2.11).
"
299354844,2910,asfbot,2017-05-05T02:08:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3521/
Test FAILed (JDK 7 and Scala 2.10).
"
299354907,2910,asfbot,2017-05-05T02:09:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3518/
Test FAILed (JDK 8 and Scala 2.12).
"
299359548,2910,asfbot,2017-05-05T03:02:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3529/
Test FAILed (JDK 8 and Scala 2.11).
"
299359930,2910,asfbot,2017-05-05T03:05:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3520/
Test FAILed (JDK 8 and Scala 2.12).
"
299360666,2910,asfbot,2017-05-05T03:10:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3523/
Test FAILed (JDK 7 and Scala 2.10).
"
299530318,2910,asfbot,2017-05-05T17:46:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3560/
Test FAILed (JDK 8 and Scala 2.11).
"
299530541,2910,asfbot,2017-05-05T17:47:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3554/
Test FAILed (JDK 7 and Scala 2.10).
"
299533174,2910,asfbot,2017-05-05T17:57:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3551/
Test FAILed (JDK 8 and Scala 2.12).
"
299537395,2910,asfbot,2017-05-05T18:14:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3553/
Test FAILed (JDK 8 and Scala 2.12).
"
299552663,2910,asfbot,2017-05-05T19:17:49Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3558/
Test PASSed (JDK 7 and Scala 2.10).
"
299552918,2910,asfbot,2017-05-05T19:19:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3557/
Test PASSed (JDK 7 and Scala 2.10).
"
299561259,2910,asfbot,2017-05-05T19:58:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3564/
Test FAILed (JDK 8 and Scala 2.11).
"
299562514,2910,asfbot,2017-05-05T20:03:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3554/
Test FAILed (JDK 8 and Scala 2.12).
"
299584453,2910,asfbot,2017-05-05T21:52:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3568/
Test FAILed (JDK 8 and Scala 2.12).
"
299584764,2910,asfbot,2017-05-05T21:54:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3578/
Test FAILed (JDK 8 and Scala 2.11).
"
299585072,2910,asfbot,2017-05-05T21:56:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3572/
Test FAILed (JDK 7 and Scala 2.10).
"
299586221,2910,asfbot,2017-05-05T22:02:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3568/
Test PASSed (JDK 7 and Scala 2.10).
"
299589138,2910,asfbot,2017-05-05T22:21:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3564/
Test FAILed (JDK 8 and Scala 2.12).
"
299589619,2910,asfbot,2017-05-05T22:25:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3574/
Test PASSed (JDK 8 and Scala 2.11).
"
299591718,2910,junrao,2017-05-05T22:40:11Z,@hachikuji : Thanks for the patch. LGTM.
299591871,2910,junrao,2017-05-05T22:41:13Z,I will let you merge this after running the system tests.
299598781,2910,asfbot,2017-05-05T23:38:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3581/
Test FAILed (JDK 8 and Scala 2.11).
"
299600407,2910,asfbot,2017-05-05T23:55:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3575/
Test FAILed (JDK 7 and Scala 2.10).
"
299600588,2910,asfbot,2017-05-05T23:57:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3571/
Test FAILed (JDK 8 and Scala 2.12).
"
299648323,2910,hachikuji,2017-05-06T15:49:45Z,@junrao Thanks for the review cycles. System tests are looking good: http://confluent-kafka-branch-builder-system-test-results.s3-us-west-2.amazonaws.com/2017-05-05--001.1494059152--hachikuji--eos-txn-index--64671d8/report.html. I am going to look at a couple of the build failures above before merging to see if they could be related.
299648334,2910,hachikuji,2017-05-06T15:49:55Z,retest this please
299648509,2910,asfbot,2017-05-06T15:52:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3581/
Test FAILed (JDK 8 and Scala 2.12).
"
299652635,2910,asfbot,2017-05-06T16:57:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3591/
Test FAILed (JDK 8 and Scala 2.11).
"
299652903,2910,asfbot,2017-05-06T17:02:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3585/
Test PASSed (JDK 7 and Scala 2.10).
"
299653246,2910,hachikuji,2017-05-06T17:08:06Z,retest this please
299653658,2910,hachikuji,2017-05-06T17:14:56Z,"It looks like the recent transient failures are known issues: https://issues.apache.org/jira/browse/KAFKA-5183, https://issues.apache.org/jira/browse/KAFKA-5182, https://issues.apache.org/jira/browse/KAFKA-5175. In one of the previous builds, I saw a failure in kafka.server.LogRecoveryTest.testHWCheckpointWithFailuresSingleLogSegment. This could be related to this patch and there is no active JIRA, so I'm trying to reproduce it locally."
299657791,2910,asfbot,2017-05-06T18:29:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3594/
Test PASSed (JDK 8 and Scala 2.11).
"
299657862,2910,asfbot,2017-05-06T18:30:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3588/
Test PASSed (JDK 7 and Scala 2.10).
"
299658671,2910,asfbot,2017-05-06T18:45:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3584/
Test PASSed (JDK 8 and Scala 2.12).
"
299658852,2910,hachikuji,2017-05-06T18:48:02Z,I wasn't able to reproduce the failure in `LogRecoveryTest` locally and it didn't occur in the last few builds. I went ahead and increased the timeout that failed just in case. I'm going to go ahead and merge this to trunk.
843289802,10579,satishd,2021-05-18T15:54:18Z,@junrao @kowshik This PR is ready for review. Pl let me know your comments. 
851023730,10579,satishd,2021-05-30T16:12:48Z,Thanks @junrao for your comment. Addressed most of them with the latest commit.
859400725,10579,satishd,2021-06-11T08:53:38Z,Thanks @junrao @kowshik for the review comments. Addressed them with the latest commits. 
868083433,10579,satishd,2021-06-25T00:01:17Z,Thanks @ccding for the review. Addressed them with the latest commit https://github.com/apache/kafka/commit/25c4ccb7c5ad44963a9708f0df67b982cf91ac15
874000725,10579,satishd,2021-07-05T10:27:20Z,Thanks @junrao for your comments. Addressed them with the latest commit. 
875750234,10579,satishd,2021-07-07T16:30:24Z,Thanks @junrao for the comments. Pl take a look at my inline replies and the latest commit. 
881556895,10579,satishd,2021-07-16T16:05:44Z,"Thanks @junrao for your latest comments, replied to them inline."
881805037,10579,satishd,2021-07-17T02:51:25Z,Thanks @junrao for the comment. I addressed it with the latest commit. 
308242425,3325,bbejeck,2017-06-13T20:42:43Z,"ping @mjsax @guozhangwang @enothereska @dguy  for initial review

This still need more test coverage, but I wanted to submit the PR to move the discussion forward.

EDIT: The batch restoration benchmark will undergo some refactoring as well. 

EDIT round 2: Batch restoration benchmark will be in a follow in PR"
308246006,3325,asfbot,2017-06-13T20:56:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5236/
Test FAILed (JDK 8 and Scala 2.12).
"
308246447,3325,bbejeck,2017-06-13T20:57:40Z,Fixing checkstyle errors
308257513,3325,asfbot,2017-06-13T21:43:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5254/
Test FAILed (JDK 7 and Scala 2.11).
"
308257949,3325,asfbot,2017-06-13T21:45:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5239/
Test FAILed (JDK 8 and Scala 2.12).
"
308258160,3325,bbejeck,2017-06-13T21:46:00Z,addressing errors
308272164,3325,asfbot,2017-06-13T22:57:39Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5258/
Test PASSed (JDK 7 and Scala 2.11).
"
308273772,3325,asfbot,2017-06-13T23:07:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5242/
Test PASSed (JDK 8 and Scala 2.12).
"
308442647,3325,asfbot,2017-06-14T14:08:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5298/
Test FAILed (JDK 7 and Scala 2.11).
"
308445355,3325,bbejeck,2017-06-14T14:17:19Z,Error seems unrelated to this PR `Execution failed for task ':core:compileScala'.` Can't reproduce locally either.
308463117,3325,asfbot,2017-06-14T15:13:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5282/
Test PASSed (JDK 8 and Scala 2.12).
"
308464132,3325,bbejeck,2017-06-14T15:16:21Z,retest this please.
308476305,3325,enothereska,2017-06-14T15:55:50Z,"I think the restore callbacks etc look good. My main question will be around the bulk loading part, i.e., if we go with batch loading, will we still be able to look into KAFKA-4868 or are they mutually exclusive? Thanks @bbejeck ."
308480397,3325,asfbot,2017-06-14T16:09:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5285/
Test FAILed (JDK 8 and Scala 2.12).
"
308483354,3325,bbejeck,2017-06-14T16:20:25Z,@enothereska  current plan is to have KAFKA-4868 be part of the bulk/batch loading in this PR 
308490937,3325,asfbot,2017-06-14T16:47:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5301/
Test FAILed (JDK 7 and Scala 2.11).
"
308511600,3325,bbejeck,2017-06-14T18:02:37Z,retest this please.
308532303,3325,asfbot,2017-06-14T19:22:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5317/
Test PASSed (JDK 7 and Scala 2.11).
"
308532320,3325,asfbot,2017-06-14T19:22:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5301/
Test FAILed (JDK 8 and Scala 2.12).
"
308873259,3325,asfgit,2017-06-15T21:39:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5369/
Test PASSed (JDK 7 and Scala 2.11).
"
308880317,3325,asfgit,2017-06-15T22:13:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5354/
Test PASSed (JDK 8 and Scala 2.12).
"
309618705,3325,bbejeck,2017-06-20T01:31:15Z,"@enothereska - updated code to handle KAFKA-4868

EDIT: Still owe tests on this PR"
309627297,3325,asfgit,2017-06-20T02:32:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5476/
Test PASSed (JDK 8 and Scala 2.12).
"
309627306,3325,asfgit,2017-06-20T02:32:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5491/
Test PASSed (JDK 7 and Scala 2.11).
"
312100069,3325,asfgit,2017-06-29T20:41:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/5811/
Test PASSed (JDK 7 and Scala 2.11).
"
312102705,3325,asfgit,2017-06-29T20:47:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/5797/
Test PASSed (JDK 8 and Scala 2.12).
"
312245974,3325,bbejeck,2017-06-30T11:41:16Z,Still owe tests
316110967,3325,bbejeck,2017-07-18T15:58:34Z,"@dguy updated per comments - added javadoc and tests.

cc\ @mjsax @enothereska @guozhangwang "
316126028,3325,asfgit,2017-07-18T16:49:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6149/
Test PASSed (JDK 7 and Scala 2.11).
"
316132350,3325,asfgit,2017-07-18T17:12:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6133/
Test PASSed (JDK 8 and Scala 2.12).
"
316818632,3325,bbejeck,2017-07-20T20:20:50Z,"@dguy @guozhangwang  updated per comments

\cc @mjsax  @enothereska "
316833131,3325,asfgit,2017-07-20T21:18:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6209/
Test PASSed (JDK 8 and Scala 2.12).
"
316833622,3325,asfgit,2017-07-20T21:20:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6225/
Test PASSed (JDK 7 and Scala 2.11).
"
317077171,3325,guozhangwang,2017-07-21T18:26:16Z,"> If we put it in config then they lose the ability to use capture any objects/state etc in their application.

Good point. Let's add the check that users can only call this before calling start() then (and hence calling it whatever times are fine then). Similarly for `KafkaStreams#setStateListener` and `#setUncaughtExceptionHandler` let's do the same enforcement."
317484949,3325,bbejeck,2017-07-24T16:54:05Z,"> Similarly for KafkaStreams#setStateListener and #setUncaughtExceptionHandler let's do the same enforcement.

Agreed, but as this PR is large enough I'll make those changes in a follow-up PR."
317509633,3325,bbejeck,2017-07-24T18:17:09Z,"updates per comments

@dguy @guozhangwang  @mjsax - I believe I've addressed all issues."
317530363,3325,guozhangwang,2017-07-24T19:33:12Z,"> Agreed, but as this PR is large enough I'll make those changes in a follow-up PR.

That is what I meant :) Not needed necessarily for this PR, just to make sure we do not forget."
317546040,3325,asfgit,2017-07-24T20:34:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6317/
Test PASSed (JDK 7 and Scala 2.11).
"
317563231,3325,asfgit,2017-07-24T21:40:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6301/
Test PASSed (JDK 8 and Scala 2.12).
"
318182744,3325,bbejeck,2017-07-26T21:06:23Z,rebasing now
318187014,3325,bbejeck,2017-07-26T21:24:04Z,"@guozhangwang @mjsax 
rebased and comments addressed"
318197137,3325,asfgit,2017-07-26T22:06:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6369/
Test PASSed (JDK 7 and Scala 2.11).
"
318200523,3325,asfgit,2017-07-26T22:23:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6372/
Test PASSed (JDK 7 and Scala 2.11).
"
318201717,3325,asfgit,2017-07-26T22:29:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6354/
Test PASSed (JDK 8 and Scala 2.12).
"
318204244,3325,asfgit,2017-07-26T22:43:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6357/
Test PASSed (JDK 8 and Scala 2.12).
"
318470546,3325,bbejeck,2017-07-27T20:02:09Z,ping @guozhangwang for final review and merge
318514748,3325,bbejeck,2017-07-27T23:29:43Z,@guozhangwang @mjsax updates per comments
318522994,3325,asfgit,2017-07-28T00:27:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/6402/
Test PASSed (JDK 7 and Scala 2.11).
"
318526739,3325,asfgit,2017-07-28T00:55:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/6387/
Test PASSed (JDK 8 and Scala 2.12).
"
318729754,3325,guozhangwang,2017-07-28T18:31:26Z,Merged to trunk. THanks @bbejeck !
797234253,10218,satishd,2021-03-12T04:56:35Z,"thanks @junrao @kowshik for the review comments, addressed with the latest commits/comments.
"
806359943,10218,satishd,2021-03-25T04:45:09Z,"@kowshik I discussed the proposed changes in the call on 23rd,  and I mentioned that PR is not updated with those changes. I will let you know once those changes are pushed into this PR.  "
812572844,10218,satishd,2021-04-02T15:14:03Z,"@junrao @kowshik thanks for your review and comments. In the latest commit, addressed the review comments. I have also refactored the code with better abstractions. "
813897020,10218,satishd,2021-04-06T07:32:23Z,Thanks @junrao for the review.  Addressed them with the latest [commit](https://github.com/apache/kafka/pull/10218/commits/83050f3e4c03c5ac58c0ba2a9d8fcbf49661a1a9). 
814275837,10218,satishd,2021-04-06T16:54:41Z,@junrao I renamed `remote-storage` module to `storage` module as you suggested in the commit [009d7fa](https://github.com/apache/kafka/pull/10218/commits/009d7fa6c861c671f3b2aa85c05fdd5ff467e195).
815062947,10218,satishd,2021-04-07T16:44:27Z,"@junrao This PR is on top of https://github.com/apache/kafka/pull/10489 . So, https://github.com/apache/kafka/pull/10489 be merged before merging this PR. "
815513359,10218,satishd,2021-04-08T07:14:14Z,@junrao Thanks for your comments. Replied on comments and addressed them with the [b5c0081](https://github.com/apache/kafka/pull/10218/commits/b5c0081b6f2740606e8458a3e784994ac5a5ee85) commit. 
816736436,10218,satishd,2021-04-09T14:47:16Z,Thanks @junrao @kowshik for the comments. Addressed them with the [d12c77a](https://github.com/apache/kafka/pull/10218/commits/d12c77a8f91d15cc9dd8b4c3f664f38c51c033b7).
817018916,10218,satishd,2021-04-09T23:09:05Z,"Thanks @junrao for the comment, addressed with the commit [317be7a](https://github.com/apache/kafka/pull/10218/commits/317be7abe50264b6ed09bb85f36eacdc6b09599f). "
817452410,10218,satishd,2021-04-12T03:24:39Z,@junrao @kowshik It looks like failures are not related to this PR.
1732507180,14432,vamossagar12,2023-09-24T07:14:12Z,"@dajac , do you think it is ok to have this small PR focussed only on static member departure? I plan to create such small PRs for other cases when a static member joins etc. Plz let me know if you prefer this approach or a single PR with all changes. Thanks!"
1732788357,14432,vamossagar12,2023-09-25T01:58:46Z,Test failures seem unrelated.
1736982966,14432,vamossagar12,2023-09-27T08:52:46Z,"Thanks for the review David! Responses inline

> I don't mind doing a few small PRs at all.

Ack. Thanks for the confirmation.

> The title of the PR is a bit confusing because the member does not send a LeaveGroup request.

Sorry about the confusion. I have corrected it.

> I think that we need to keep the -2 epoch. I think that it could be useful in the future if we want to extend the Consumer to have the ability to chose whether it wants to leave forever or temporarily.

Got it. I have reverted to using member epoch -2 for static member leave request. 

> Ignoring the ""leave request"" is not enough. If you look at the other side, we need to know whether the member has released the instance id or not. We could perhaps capture this by updating the member epoch to -2 or something like that.

I have updated the member epoch to -2 if the departing worker is a static member. I am not sure if that will be enough though because I am not returning any corresponding results to be written to the topic. All that's going to happen is that we will complete this current request with a result having member epoch id equal to -2. Also, the meaning of `member releasing the instance id` is not clear to me. 
"
1737732772,14432,vamossagar12,2023-09-27T16:35:14Z,Thanks for the review @kirktrue ! I have addressed the comments.
1739714081,14432,vamossagar12,2023-09-28T17:09:25Z,"I realised that a logic similar to https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/generic/GenericGroup.java#L774-L780 is missing in this PR. I think that's important. To get this working though, I would also need to add the logic to add a different map for staticMembers and also support adding a static member to the group. In that case, I might need to enhance this PR to support both Leave request and join request. "
1747026910,14432,vamossagar12,2023-10-04T14:45:31Z,"> I realised that a logic similar to https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/generic/GenericGroup.java#L774-L780 is missing in this PR. I think that's important. To get this working though, I would also need to add the logic to add a different map for staticMembers and also support adding a static member to the group. In that case, I might need to enhance this PR to support both Leave request and join request.

I updated this PR to handle this case and to also handle (re)-join and leaving the group using ConsumerGroupHeartbeat api.
"
1747117455,14432,dajac,2023-10-04T15:23:34Z,Thanks @vamossagar12! I will get to this PR soon.
1747157933,14432,vamossagar12,2023-10-04T15:39:37Z,"> Thanks @vamossagar12! I will get to this PR soon.

Thank you. I am still coming to terms with the new group coordinator so there might be rough edges in this PR (upfront apologies for that )"
1757221757,14432,vamossagar12,2023-10-11T09:11:35Z,"Thanks @dajac , I have addressed the comments. "
1767770461,14432,vamossagar12,2023-10-18T06:40:07Z,"Thanks for the review @dajac , Most of my changes are on my local, but before I push those, wanted your thoughts on [this](https://github.com/apache/kafka/pull/14432/files#r1363301832) and [this](https://github.com/apache/kafka/pull/14432/files#r1363322519) comment. I can proceed with the rest of the changes post that."
1767897337,14432,dajac,2023-10-18T07:59:11Z,@vamossagar12 Thanks. I just replied to your questions.
1777654947,14432,vamossagar12,2023-10-24T16:59:21Z,"Thanks @dajac , I have addressed all the comments. Have added a couple of more questions. Thanks for all the pointers, the changes look cleaner than the previous iteration (to me atleast)."
1795371595,14432,vamossagar12,2023-11-06T16:32:52Z,"Thanks @dajac , I have addressed the review comments. Please let me know how the changes are looking now."
1824047001,14432,vamossagar12,2023-11-23T09:19:45Z,"Thanks @dajac . I have addressed all the comments. The second part of [this](https://github.com/apache/kafka/pull/14432/files#r1397382111) comment, I am not sure which new data structures being referred here. The test `testStaticMemberGetsBackAssignmentUponRejoin` asserts all the records returned after replay. Is that what you are referring to?"
1827266684,14432,vamossagar12,2023-11-27T07:29:18Z,"@dajac , thank you for another round of review. I have handled all review comments. "
692859976,9100,mumrah,2020-09-15T17:25:32Z,"Recent changes include:

* Controller no longer sends LeaderAndIsr, only UpdateMetadata (more more leader epoch bump)
* Update Partition's ISR from AlterIsr response
* Periodic scheduled thread for calling ""propagateIsrChanges"" in AlterIsrManager (we could miss updates otherwise)
* Top-level error handling in AlterIsrManager
* Additional checks in AlterIsrManager to prevent multiple in-flight requests and also prevent multiple ISR changes for a 
 given partition
* Changes to the callback logic to ensure we don't leave any partitions stuck

"
697953307,9100,mumrah,2020-09-23T20:25:08Z,"@hachikuji yea, good catch. This works today using a ZK watch on the partition ""/state"" znode which is still getting triggered with this PR. We can modify the new ISR update path to explicitly call `onPartitionReassignment` after writing out the ISR. How about we save this as a follow-on?"
698458937,9100,hachikuji,2020-09-24T16:42:08Z,"Yeah, I'm ok leaving that for https://issues.apache.org/jira/browse/KAFKA-10521."
698637507,9100,hachikuji,2020-09-24T23:24:24Z,I think the failing tests are known flakes and should be fixed by https://issues.apache.org/jira/browse/KAFKA-10514.
699438281,9100,jacky1193610322,2020-09-26T06:27:27Z,"I got a thought, there is a scenario that A leader can see its followers, but cannot see Zookeeper, and then the leader will be fenced when it attempts to shink isr or expand isr because it holds the leaderIsrUpdateLock, but now the leader can't be fenced because it just sends the message and will process message normally. when the ack=1, it will continue processing the msg, and will be lost. should we reject process the msg when the alterisr find the broker is not in liveBrokerIdAndEpochs? [click me](https://jack-vanlightly.com/blog/2018/9/2/rabbitmq-vs-kafka-part-6-fault-tolerance-and-high-availability-with-kafka)"
729316632,9100,hachikuji,2020-11-18T01:27:41Z,"@jacky1193610322 I missed this comment before. It's a good question. In general, the leader will continue in its current state as long as possible. As you say, as soon as it needs to shrink/expand the ISR, it grabs the leaderAndIsr update and attempts to synchronously update the state. If Zookeeper can't be reached, then the thread gets stuck. Eventually this causes the broker to effectively deadlock, which has the side effect of preventing any Produce requests (and any other requests) from getting through.

I think it's a fair point that this affords some protection for acks=1 requests, but I think we tend to view the side effect of deadlocking the broker as worse than any benefit. In KIP-500, we have an alternative approach for self-fencing. The analogous case is when the leader cannot reach the controller. We use a heartbeating mechanism to maintain liveness in the cluster. Unlike with Zookeeper, we do not rely on the session expiration event in order to tell that a broker has been declared dead. Instead if we do not get a heartbeat response from the controller before some timeout, then we will stop accepting Produce requests. 

I have been thinking a little bit about your suggestion to self-fence after getting an invalid version error from AlterIsr. It might help in the interim before KIP-500 is complete. I think our expectation here was that if we get an invalid version error, then the LeaderAndIsr with the updated state should soon be on the way. I suppose we could come up with reasons why that assumption might fail, so it might make sense to be a little more defensive. I will file a jira about this and we can see what others think. Thanks for the suggestion!"
729349575,9100,jacky1193610322,2020-11-18T03:15:10Z,"Thanks for your reply, I also have read KIP-500 and other KIP-631, It's good about the fence. but it will be released in a few months, before that, I think we also need to try the best to fence the broker when the controller already think the broker has died. In other words, we should fence 2-way. 
`
self-fence after getting an invalid version error from AlterIsr
`
yes, I think we need self-fence when the session is lost, we can't rely on receiving the other machines response because we can't receive the response when the Broker2ControllerChannel is broken.
please let me know if you create a jira. "
276459630,2472,asfbot,2017-01-31T19:07:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1362/
Test FAILed (JDK 8 and Scala 2.11).
"
276467900,2472,asfbot,2017-01-31T19:36:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1358/
Test PASSed (JDK 8 and Scala 2.12).
"
276474948,2472,asfbot,2017-01-31T20:00:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1358/
Test FAILed (JDK 7 and Scala 2.10).
"
276547350,2472,asfbot,2017-02-01T01:26:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1389/
Test FAILed (JDK 8 and Scala 2.11).
"
276547511,2472,asfbot,2017-02-01T01:27:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1386/
Test FAILed (JDK 8 and Scala 2.12).
"
276547756,2472,asfbot,2017-02-01T01:29:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1386/
Test FAILed (JDK 7 and Scala 2.10).
"
276844638,2472,twbecker,2017-02-02T02:00:34Z,Good to see some progress on KIP-4! One question - does this allow new topics to be created with the cluster defaults for number of partitions and replication factor? Having to have a priori knowledge of those configs on the client is a big pain point for us using AdminUtils and I thought the elimination of that requirement was a goal.
277339758,2472,cmccabe,2017-02-03T19:30:02Z,"Hi @twbecker, thanks for taking a look!  Be sure to also check out the discussion thread here: https://www.mail-archive.com/dev@kafka.apache.org/msg65697.html

Unfortunately, as far as I know, there is no way to allow the server to use a default replication factor when making a CreateTopicsRequest.  The RPC must have either a manual partitioning assignment, or a number of replicas and number of partitions specified.

I think supporting server-side defaults would be a nice improvement, but it would require a separate KIP to implement."
277356611,2472,asfbot,2017-02-03T20:40:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1471/
Test FAILed (JDK 8 and Scala 2.12).
"
277362598,2472,asfbot,2017-02-03T21:07:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1474/
Test PASSed (JDK 8 and Scala 2.11).
"
277378281,2472,asfbot,2017-02-03T22:20:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1471/
Test FAILed (JDK 7 and Scala 2.10).
"
277831745,2472,asfbot,2017-02-06T22:15:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/1525/
Test PASSed (JDK 8 and Scala 2.12).
"
277832071,2472,asfbot,2017-02-06T22:16:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/1528/
Test PASSed (JDK 8 and Scala 2.11).
"
277832251,2472,asfbot,2017-02-06T22:17:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/1525/
Test PASSed (JDK 7 and Scala 2.10).
"
293026463,2472,asfbot,2017-04-10T17:49:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2850/
Test FAILed (JDK 7 and Scala 2.10).
"
293026882,2472,asfbot,2017-04-10T17:51:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2855/
Test FAILed (JDK 8 and Scala 2.11).
"
293027947,2472,asfbot,2017-04-10T17:55:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2851/
Test FAILed (JDK 8 and Scala 2.12).
"
293034817,2472,asfbot,2017-04-10T18:19:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2851/
Test FAILed (JDK 7 and Scala 2.10).
"
293053593,2472,asfbot,2017-04-10T19:28:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2856/
Test PASSed (JDK 8 and Scala 2.11).
"
293061399,2472,asfbot,2017-04-10T19:58:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2852/
Test PASSed (JDK 8 and Scala 2.12).
"
293073677,2472,asfbot,2017-04-10T20:46:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2862/
Test FAILed (JDK 7 and Scala 2.10).
"
293089074,2472,asfbot,2017-04-10T21:48:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2863/
Test PASSed (JDK 8 and Scala 2.12).
"
293093904,2472,asfbot,2017-04-10T22:11:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2867/
Test PASSed (JDK 8 and Scala 2.11).
"
294614528,2472,asfbot,2017-04-17T22:31:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2976/
Test FAILed (JDK 7 and Scala 2.10).
"
294615133,2472,asfbot,2017-04-17T22:33:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2977/
Test FAILed (JDK 8 and Scala 2.12).
"
294626011,2472,asfbot,2017-04-17T23:31:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2982/
Test PASSed (JDK 8 and Scala 2.11).
"
294626635,2472,asfbot,2017-04-17T23:34:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2977/
Test PASSed (JDK 7 and Scala 2.10).
"
294630985,2472,asfbot,2017-04-18T00:03:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2978/
Test PASSed (JDK 8 and Scala 2.12).
"
294986514,2472,asfbot,2017-04-18T21:22:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2998/
Test PASSed (JDK 7 and Scala 2.10).
"
294987065,2472,asfbot,2017-04-18T21:24:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3003/
Test PASSed (JDK 8 and Scala 2.11).
"
294988173,2472,asfbot,2017-04-18T21:29:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2999/
Test PASSed (JDK 8 and Scala 2.12).
"
295503994,2472,asfbot,2017-04-20T00:09:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3041/
Test FAILed (JDK 8 and Scala 2.11).
"
295504003,2472,asfbot,2017-04-20T00:09:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3037/
Test FAILed (JDK 8 and Scala 2.12).
"
295504998,2472,asfbot,2017-04-20T00:12:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3036/
Test FAILed (JDK 7 and Scala 2.10).
"
295848818,2472,asfbot,2017-04-20T18:33:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3067/
Test FAILed (JDK 8 and Scala 2.11).
"
295848819,2472,asfbot,2017-04-20T18:33:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3062/
Test FAILed (JDK 7 and Scala 2.10).
"
295848874,2472,asfbot,2017-04-20T18:33:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3063/
Test FAILed (JDK 8 and Scala 2.12).
"
295907265,2472,asfbot,2017-04-20T20:51:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3071/
Test FAILed (JDK 8 and Scala 2.11).
"
295908551,2472,asfbot,2017-04-20T20:54:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3066/
Test FAILed (JDK 7 and Scala 2.10).
"
295911579,2472,asfbot,2017-04-20T21:02:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3067/
Test FAILed (JDK 8 and Scala 2.12).
"
296267131,2472,asfbot,2017-04-21T18:25:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3098/
Test PASSed (JDK 7 and Scala 2.10).
"
296267792,2472,asfbot,2017-04-21T18:28:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3103/
Test PASSed (JDK 8 and Scala 2.11).
"
296277576,2472,asfbot,2017-04-21T18:58:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3099/
Test FAILed (JDK 8 and Scala 2.12).
"
298144085,2472,asfbot,2017-04-29T03:29:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3292/
Test FAILed (JDK 8 and Scala 2.11).
"
298144094,2472,asfbot,2017-04-29T03:29:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3286/
Test FAILed (JDK 8 and Scala 2.12).
"
298144269,2472,asfbot,2017-04-29T03:34:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3290/
Test FAILed (JDK 7 and Scala 2.10).
"
298144387,2472,ijuma,2017-04-29T03:37:04Z,"3 test failures:

```text
kafka.api.SaslSslAdminClientIntegrationTest.testGetAllBrokerVersions
kafka.api.SaslSslAdminClientIntegrationTest.testListNodes
kafka.api.SaslSslAdminClientIntegrationTest.testCreateDeleteTopics
```"
298358441,2472,ijuma,2017-05-01T15:46:14Z,"I fixed the test failure and some trivial clean-ups:

https://github.com/cmccabe/kafka/pull/4

System tests passed:

https://jenkins.confluent.io/job/system-test-kafka-branch-builder/866/

I think we can merge this after the PR above is merged. I will file a JIRA with additional clean-ups that I noticed, but those can be done later."
298366874,2472,cmccabe,2017-05-01T16:25:50Z,"Thanks, @ijuma.  The cleanups look good to me."
298367921,2472,asfbot,2017-05-01T16:30:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3330/
Test FAILed (JDK 8 and Scala 2.11).
"
298367978,2472,asfbot,2017-05-01T16:30:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3321/
Test FAILed (JDK 8 and Scala 2.12).
"
298368037,2472,asfbot,2017-05-01T16:31:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3325/
Test FAILed (JDK 7 and Scala 2.10).
"
298382012,2472,asfbot,2017-05-01T17:33:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3332/
Test FAILed (JDK 8 and Scala 2.11).
"
298397168,2472,asfbot,2017-05-01T18:36:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3333/
Test PASSed (JDK 8 and Scala 2.11).
"
298397991,2472,asfbot,2017-05-01T18:39:11Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3328/
Test PASSed (JDK 7 and Scala 2.10).
"
298400512,2472,asfbot,2017-05-01T18:49:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3324/
Test FAILed (JDK 8 and Scala 2.12).
"
298403436,2472,cmccabe,2017-05-01T19:01:05Z,retest this please
298415755,2472,asfbot,2017-05-01T19:54:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3337/
Test FAILed (JDK 8 and Scala 2.11).
"
298420894,2472,asfbot,2017-05-01T20:15:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3332/
Test FAILed (JDK 7 and Scala 2.10).
"
298426086,2472,asfbot,2017-05-01T20:38:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3328/
Test PASSed (JDK 8 and Scala 2.12).
"
298443327,2472,cmccabe,2017-05-01T21:55:23Z,retest this please
298454041,2472,asfbot,2017-05-01T22:53:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3341/
Test PASSed (JDK 8 and Scala 2.11).
"
298456399,2472,asfbot,2017-05-01T23:08:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3336/
Test PASSed (JDK 7 and Scala 2.10).
"
298460097,2472,asfbot,2017-05-01T23:33:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3332/
Test FAILed (JDK 8 and Scala 2.12).
"
594313472,8218,abbccdda,2020-03-04T03:58:30Z,Could we have a summary for this PR?
594314018,8218,mjsax,2020-03-04T04:00:38Z,"The summary is not longer than the PR title. We make `TaskManager` the class that is responsible for committing (to allow to commit all tasks at once, instead of each task individually)"
594706982,8218,abbccdda,2020-03-04T17:56:26Z,"The title sounds pretty vague to me. The description could at least include what the committing behavior look like under `TaskManager`, what's the motivation, etc as we are already overloading JIRA 9441. In general we could try to be more specific about the changes in the PR description as I could see you are also adding upgrade flags inside this PR. Major side effects should be better to document at first IMHO."
598504502,8218,guozhangwang,2020-03-13T01:28:09Z,Thanks for the rebasing @mjsax ! will take another look soon.
599368368,8218,mjsax,2020-03-16T06:36:14Z,"Rebased to resolve merge conflicts. Addressed more review comments and added more tests.

Also added a fix (from https://github.com/apache/kafka/pull/8301) as `trunk` is broken atm, to make Jenkins pass.

Triggered systems tests: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3833/"
599779177,8218,ableegoldman,2020-03-16T22:06:19Z,"I linked the system test failures I saw on the meeting notes for this week, but just to clarify:
1) Two failing consistently: `StreamsEOSTest.test_failure_and_recovery{_complex}` 
2) One flaky: `StreamsStandbyTask.test_standby_tasks_rebalance`
Anything outside of that is a new failure"
599794804,8218,mjsax,2020-03-16T22:57:28Z,"Thanks for pointing out. In the run I triggered, the following failed:
 - `StreamsCooperativeRebalanceUpgradeTest.test_upgrade_to_cooperative_rebalance`
 - `StreamsUpgradeTest.test_metadata_upgrade`
 - `StreamsEosTest.test_failure_and_recovery`
 - `StreamsEosTest.test_failure_and_recovery_complex`
 - `StreamsEosTest.test_rebalance_complex`
 - `StreamsEosTest.test_rebalance_simple`

Not sure if this is any helpful to have a safety net for this PR..."
599816765,8218,mjsax,2020-03-17T00:23:00Z,Address review comments and rebased to pick up fixed from `trunk`.
600345056,8218,mjsax,2020-03-17T23:14:32Z,"Addressed review comments and fix bug exposed by system tests. Also added more tests.

New system test run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3834/"
600402163,8218,guozhangwang,2020-03-18T03:10:24Z,LGTM. Please feel free to merge after green builds.
600417507,8218,abbccdda,2020-03-18T04:22:27Z,"The failed test seems to be just flaky
org.apache.kafka.streams.processor.internals.StateDirectoryTest.shouldReturnEmptyArrayIfListFilesReturnsNull"
600422344,8218,mjsax,2020-03-18T04:45:14Z,"Actually the same test failed in both previous runs -- but it passes locally for me. As this PR has nothing to do with the failing test, I might just want go ahead and merge. The last PR that changed the failing test was merged recently: https://github.com/apache/kafka/commit/605d55dc173156471fb05c40d715ab318ce74952

There is one concerning comment on that  PR: `Merged to trunk and 2.5 after tests green locally.` ?

Seems the nightly Jenkins job also fails on this test: - 
 - `trunk` -> https://builds.apache.org/blue/organizations/jenkins/kafka-trunk-jdk11/detail/kafka-trunk-jdk11/1252/tests
 - `2.5` -> https://builds.apache.org/blue/organizations/jenkins/kafka-2.5-jdk8/detail/kafka-2.5-jdk8/65/tests

Did this PR break the Jenkins job? \cc @guozhangwang "
600424167,8218,abbccdda,2020-03-18T04:53:45Z,"@mjsax My understanding is that this PR will fix the problem https://github.com/apache/kafka/pull/8310/

You could choose to merge it first if looks good, and rebase the current one
"
600432582,8218,guozhangwang,2020-03-18T05:30:17Z,@mjsax I just double checked and the failure in trunk is fixed as https://github.com/apache/kafka/pull/8310 while the failure (a different one) in 2.5 is also fixed by @abbccdda 's PR. We should be fine now.
600432616,8218,guozhangwang,2020-03-18T05:30:25Z,test this please
600799765,8218,mjsax,2020-03-18T18:45:38Z,"System test failure `StreamsEosTest` (4 different test as mentioned by Sophie above).

Given that some more fixed got pushed to `trunk` I rebased. This should give us a green Jenkins and we should be able to merge."
600815165,8218,ableegoldman,2020-03-18T19:21:58Z,@mjsax only two `StreamsEOSTest` system tests were failing when I ran them. `test_rebalance_complex` and `test_rebalance_simple` are either newly failing or due to this PR
600815624,8218,ableegoldman,2020-03-18T19:23:03Z,"Also, the two already-failing tests (`test_failure_and_recovery`) failed with a different error than they were trunk:

```
java.lang.IllegalStateException: Unknown TaskId: 0_0
	at org.apache.kafka.streams.processor.internals.ActiveTaskCreator.streamsProducerForTask(ActiveTaskCreator.java:118)
	at org.apache.kafka.streams.processor.internals.TaskManager.commitOffsetsOrTransaction(TaskManager.java:749)
	at org.apache.kafka.streams.processor.internals.TaskManager.commitAll(TaskManager.java:706)
	at org.apache.kafka.streams.processor.internals.StreamThread.maybeCommit(StreamThread.java:772)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:645)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:501)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:475)
```"
601059160,8218,mjsax,2020-03-19T08:49:47Z,"@ableegoldman Thanks for pointing it out! Pushed a fix and updated the unit test accordingly. \cc @guozhangwang 

New system test run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/3835/"
601318837,8218,mjsax,2020-03-19T17:38:40Z,"`StreamsEosTest.test_failure_and_recovery` and `StreamsEosTest.test_failure_and_recovery_complex` failed. I only see:

```
[2020-03-19 12:12:47,231] ERROR stream-thread [EosTest-e1bb7363-be65-43b7-a35b-326c73533bdf-StreamThread-1] Encountered the following exception during processing and the thread is going to shut down:  (org.apache.kafka.streams.processor.internals.StreamThread)
org.apache.kafka.streams.errors.ProcessorStateException: Error opening store KSTREAM-AGGREGATE-STATE-STORE-0000000003 at location /mnt/streams/EosTest/0_0/rocksdb/KSTREAM-AGGREGATE-STATE-STORE-0000000003
	at org.apache.kafka.streams.state.internals.RocksDBTimestampedStore.openRocksDB(RocksDBTimestampedStore.java:87)
	at org.apache.kafka.streams.state.internals.RocksDBStore.openDB(RocksDBStore.java:195)
	at org.apache.kafka.streams.state.internals.RocksDBStore.init(RocksDBStore.java:231)
	at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)
	at org.apache.kafka.streams.state.internals.ChangeLoggingKeyValueBytesStore.init(ChangeLoggingKeyValueBytesStore.java:44)
	at org.apache.kafka.streams.state.internals.WrappedStateStore.init(WrappedStateStore.java:48)
	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.lambda$init$0(MeteredKeyValueStore.java:101)
	at org.apache.kafka.streams.processor.internals.metrics.StreamsMetricsImpl.maybeMeasureLatency(StreamsMetricsImpl.java:806)
	at org.apache.kafka.streams.state.internals.MeteredKeyValueStore.init(MeteredKeyValueStore.java:101)
	at org.apache.kafka.streams.processor.internals.StateManagerUtil.registerStateStores(StateManagerUtil.java:81)
	at org.apache.kafka.streams.processor.internals.StandbyTask.initializeIfNeeded(StandbyTask.java:87)
	at org.apache.kafka.streams.processor.internals.TaskManager.tryToCompleteRestoration(TaskManager.java:331)
	at org.apache.kafka.streams.processor.internals.StreamThread.runOnce(StreamThread.java:586)
	at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:501)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:475)
Caused by: org.rocksdb.RocksDBException: lock : /mnt/streams/EosTest/0_0/rocksdb/KSTREAM-AGGREGATE-STATE-STORE-0000000003/LOCK: No locks available
	at org.rocksdb.RocksDB.open(Native Method)
	at org.rocksdb.RocksDB.open(RocksDB.java:286)
	at org.apache.kafka.streams.state.internals.RocksDBTimestampedStore.openRocksDB(RocksDBTimestampedStore.java:75)
	... 14 more
```
followed by
```
java.lang.NullPointerException
	at org.apache.kafka.streams.processor.internals.StoreChangelogReader.remove(StoreChangelogReader.java:801)
	at org.apache.kafka.streams.processor.internals.TaskManager.cleanupTask(TaskManager.java:549)
	at org.apache.kafka.streams.processor.internals.TaskManager.shutdown(TaskManager.java:567)
	at org.apache.kafka.streams.processor.internals.StreamThread.completeShutdown(StreamThread.java:839)
	at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:484)
```

Seems to be unrelated to this PR. Are we good to merge?"
601320091,8218,abbccdda,2020-03-19T17:41:17Z,@mjsax We should be good. I have another PR to fix those https://github.com/apache/kafka/pull/8307
198455269,1095,granthenke,2016-03-18T17:03:11Z,"@junrao @gwenshap @ijuma 
It would be nice to get this into 0.10. It should be the only required changes for existing protocol messages for KIP-4, which would simplify a backport into 0.10.1 when the rest is agreed upon and ready. 

It also adds rack data to the metadata response which goes along with KIP-36 that was added in this release. 
"
198461416,1095,ijuma,2016-03-18T17:23:11Z,"@granthenke, I agree that it would be great to get this in if there is agreement on the protocol changes. I haven't reviewed it in detail yet, but 2 things:
- The PR needs to be rebased
- I think it would be great to make it possible to ask for an update with `0` topics in cases where we just want to get the updated cluster data. People have run into performance issues when clients have 0 topics, but they ask for _all_ topics because the protocol only supports that.
"
198463871,1095,granthenke,2016-03-18T17:32:10Z,"@ijuma I will rebase and look at supporting the idea of `null` (versus empty list) indicating that you don't want to request topic metadata at all. Its a bit nuanced, but its the most compatible and has fewer edge cases. Worst case scenario if someone passes an empty list is performance. This would help solve [KAFKA-3358](https://issues.apache.org/jira/browse/KAFKA-3358).
"
198519711,1095,granthenke,2016-03-18T20:03:16Z,"@ijuma I rebased and added the ""no topics"" functionality. 
"
198557490,1095,gwenshap,2016-03-18T21:49:35Z,"Sorry, noticed a bit late that this PR includes protocol changes that didn't go through a community discussion. Lets make sure there are 3 committers who are ok with the protocol modification?
"
208938334,1095,granthenke,2016-04-12T14:38:04Z,"@junrao @gwenshap @guozhangwang @ewencp @ijuma 
This patch represents the current vote thread for the KIP-4 Metadata changes. Please feel free to review at your convenience.
"
212985925,1095,granthenke,2016-04-21T15:57:33Z,"@junrao @gwenshap @guozhangwang @ewencp @ijuma 
Pinging for review. The KIP-4 Metadata changes vote passed and I rebased on trunk. 
"
213592590,1095,ijuma,2016-04-22T21:21:30Z,"@granthenke thanks for the PR. I left a few minor comments, looks good otherwise. I think one thing worth paying special attention to is the `errorUnavailableEndpoints` change, but it seems like Gwen has already done that.
"
214567207,1095,granthenke,2016-04-25T23:59:32Z,"@gwenshap  @ijuma @SinghAsDev @hachikuji 
I updated the patch based on the reviews. I added more tests, and some static factory methods. 

Most notably I changed from using and allTopics boolean to using null and a static method. It required a little bit of re-work on internal apis but looks reasonable to me. If you think the allTopics boolean is better don't hesitate to weigh in. The specific commit with that change is here: https://github.com/apache/kafka/pull/1095/commits/52aa74d479e39479a9c79d09b0e0fe77391fa219
"
214889420,1095,ijuma,2016-04-26T21:16:56Z,"Thanks @granthenke. Aside from a few minor comments, LGTM. I started a system tests build here:

https://jenkins.confluent.io/job/system-test-kafka-branch-builder/415/console
"
214915320,1095,granthenke,2016-04-26T23:11:13Z,"@ijuma I updated the patch based on your feedback
"
214920024,1095,ijuma,2016-04-26T23:36:47Z,"Thanks @granthenke, LGTM. @gwenshap, do you want to take a look as well?
"
214924898,1095,gwenshap,2016-04-27T00:02:35Z,"Looks perfect. I was waiting for you to finish nitpicking tests ;)
"
548594725,7629,bbejeck,2019-10-31T22:30:27Z,ping @mjsax @vvcephei and @ableegoldman for review
548594856,7629,bbejeck,2019-10-31T22:31:01Z,"I still need to test this out with a local deployment of the docs, but I wanted to get feedback on the content sooner."
548596297,7629,bbejeck,2019-10-31T22:36:54Z,\cc @JimGalasyn and @londoncalling 
548596456,7629,bbejeck,2019-10-31T22:37:35Z,I'll also do a follow-up PR to `kafka-site` for the `2.4` folder if necessary.
549929236,7629,bbejeck,2019-11-05T17:34:21Z,@mjsax @ableegoldman rebased and updated per comments
552029806,7629,bbejeck,2019-11-08T23:24:27Z,@ableegoldman updated per comments
553465499,7629,bbejeck,2019-11-13T15:55:13Z,Merged #7629 into trunk.
622095405,8589,mjsax,2020-04-30T20:34:34Z,"Retest this please.

Call for review @abbccdda "
622372958,8589,feyman2016,2020-05-01T12:43:42Z,"oh, sorry I didn't run the Checkstyle and spotbugs quality checks locally, I will update shortly with these fixed"
622417550,8589,feyman2016,2020-05-01T14:47:11Z,Retest this please.
622614438,8589,mjsax,2020-05-01T23:52:46Z,"@feyman2016 Only committers can trigger Jenkins retesting...

Retest this please."
622630109,8589,mjsax,2020-05-02T00:25:33Z,Retest this please
622661261,8589,feyman2016,2020-05-02T03:20:44Z,"@mjsax I see, thanks!
And call for review :)"
624403133,8589,feyman2016,2020-05-06T01:55:23Z,"@abbccdda Thanks a lot for the review, will update soon."
625008508,8589,feyman2016,2020-05-07T03:29:40Z,"@abbccdda Hey, updated based on comments, and also left some comments there, thanks. "
626141315,8589,feyman2016,2020-05-09T10:03:20Z,"Call for retest and review, thanks!"
627772018,8589,abbccdda,2020-05-13T06:21:20Z,@feyman2016 Sure thing!
629055884,8589,feyman2016,2020-05-15T06:32:14Z,@abbccdda Sorry for causing too much trouble about style....will make sure style check correctly executed before requesting for review next time.
629591570,8589,abbccdda,2020-05-16T05:30:05Z,@feyman2016 no worry! It's just for first time :)
630714942,8589,feyman2016,2020-05-19T09:51:54Z,"@abbccdda I'm fixing the style error, and I found that it showed success if I run the below cmd locally :
`
./gradlew checkstyleMain checkstyleTest spotbugsMain spotbugsTest spotbugsScoverage compileTestJava
`
I'm didn't find the reason after some investigating, but the style check cmdline can capture style error if I cherry-pick the first commit of this PR to another branch, guessing that it might be the second commit: `merge trunk` somehow made the style check doesn't work...
So would it break our convention if I revert to the first commit:`Add option to force delete active members in StreamsResetter` and fix style error from there ? This may change the commit history of this PR.
I'm asking because I noticed :""Please address feedback via additional commits instead of amending existing commits."" on https://cwiki.apache.org/confluence/display/KAFKA/Contributing+Code+Changes#ContributingCodeChanges-PullRequest
Thanks!"
630918452,8589,abbccdda,2020-05-19T16:03:34Z,"@feyman2016 Thanks for the context. I don't worry too much for comment vanish if you change the commit history, as they would not be gone but just show as `outdated` on github. Just do whatever you feel makes sense.

Also just a reminder that we are one week away from the feature freeze, so let's try to ramp up and get this into 2.6."
631299671,8589,feyman2016,2020-05-20T07:44:46Z,"@abbccdda Thanks, will try my best to get this into 2.6"
632027791,8589,feyman2016,2020-05-21T11:12:04Z,"@abbccdda Updated, call for review, thanks!"
632246270,8589,feyman2016,2020-05-21T17:41:00Z,"@abbccdda Thanks so much for the timely and detailed comments, I will update soon."
632722266,8589,feyman2016,2020-05-22T14:30:00Z,"@abbccdda Updated and req for review agian.. really appreciated your help to pick out so much style violations, also wondering if we can use some format tool like `scalafmt` to automatic format~"
632974316,8589,feyman2016,2020-05-23T02:48:20Z,"@abbccdda Updated, thanks!"
632987873,8589,feyman2016,2020-05-23T05:21:22Z,"@abbccdda ahh, sorry, checking, will fix soon"
632991430,8589,feyman2016,2020-05-23T06:00:50Z,"@abbccdda Just fixed, thanks!"
633084014,8589,feyman2016,2020-05-23T16:07:34Z,"@abbccdda Thanks very much for the review and comments! Wonder if it is still possible to get this into 2.6~
@mjsax  Also call for retest, thanks!"
633667548,8589,guozhangwang,2020-05-25T17:51:38Z,test this
633733992,8589,abbccdda,2020-05-25T22:39:22Z,"Only known flaky EOS tests are failing:
```
org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta[false]
org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta[true]
```"
634846209,8589,mjsax,2020-05-27T18:09:55Z,Retest this please.
634846336,8589,mjsax,2020-05-27T18:10:10Z,Retest this please.
634855359,8589,feyman2016,2020-05-27T18:26:17Z,@mjsax Thanks a lot for the review and tests triggering!
634941215,8589,mjsax,2020-05-27T21:09:07Z,"Java 8:
```
org.apache.kafka.streams.integration.RegexSourceIntegrationTest.testRegexRecordsAreProcessedAfterReassignment
org.apache.kafka.streams.integration.RegexSourceIntegrationTest.testRegexMatchesTopicsAWhenCreated
```
Java 11:
```
org.apache.kafka.connect.integration.InternalTopicsIntegrationTest.testCreateInternalTopicsWithDefaultSettings
org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta[true]
```
Java 14:
```
org.apache.kafka.streams.integration.EosBetaUpgradeIntegrationTest.shouldUpgradeFromEosAlphaToEosBeta[true]
```"
634941290,8589,mjsax,2020-05-27T21:09:15Z,Retest this please.
634941557,8589,mjsax,2020-05-27T21:09:41Z,Retest this please.
634943565,8589,mjsax,2020-05-27T21:13:31Z,Retest this please.
634944109,8589,mjsax,2020-05-27T21:14:33Z,Retest this please.
634944577,8589,mjsax,2020-05-27T21:15:23Z,Jenkins does not cooperate... will try again later
634954721,8589,mjsax,2020-05-27T21:35:42Z,Retest this please.
635025385,8589,feyman2016,2020-05-28T00:53:44Z,"Finally, jenkins cooperated :)"
635041171,8589,mjsax,2020-05-28T01:41:15Z,Thanks for the KIP and PR @feyman2016!
635041609,8589,abbccdda,2020-05-28T01:42:36Z,"Great work, @feyman2016 @mjsax !"
635293293,8589,feyman2016,2020-05-28T11:55:17Z,"@abbccdda @mjsax Thanks a lot for your kindly review and help! Are you guys using some formatting tool, I asked this because this PR had too many formatting issues, would be good if next time I could effectively avoid them, thanks!"
635335273,8589,feyman2016,2020-05-28T13:01:08Z,"FYI, since we took a slightly different implementation(leveraging the empty members rather than introducing a new field to imply the `removeAll` scenario), I updated the KIP-571 accordingly to keep them consistent."
635616335,8589,mjsax,2020-05-28T21:24:49Z,"> Are you guys using some formatting tool

I use IntelliJ that does have some auto-formatting. Use it basically with default settings. In doubt, disable auto-formatting to avoid unnecessary reformatting. -- Some things are also a little bit of ""personal taste""...

> I updated the KIP-571 accordingly to keep them consistent.

Thanks! Can you send a follow up email to the voting thread explaining the change? To make sure nobody has concerns about it."
636274254,8589,feyman2016,2020-05-30T04:30:43Z,"@mjsax Sure, updated in the voting thread of KIP-571"
402306389,5322,guozhangwang,2018-07-03T22:08:58Z,"Another class that have some part of the logic is `DefaultPartitionGrouper`, and in that impl we should not expect any

```
if (partitions.isEmpty()) {

                log.warn(""Skipping creating tasks for the topic group {} since topic {}'s metadata is not available yet;""
                         + "" no tasks for this topic group will be assigned to any client.\n""
                         + "" Make sure all supplied topics in the topology are created before starting""
                         + "" as this could lead to unexpected results"", topics, topic);
                return StreamsPartitionAssignor.NOT_AVAILABLE;
            } 
```

Any more.

Sorry the whole task assignment logic was scattered across multiple classes / functions, we just need to make sure we would clean up all the corner cases and do not introduce any regressions. For that we'd better add some integration test for this case as well."
402316622,5322,tedyu,2018-07-03T23:06:59Z,"Thanks for the comments.
I am trying to accommodate meta comment first.
Please take a look at current form to see if I removed code which wouldn't be executed with the early check in place.

"
402335251,5322,tedyu,2018-07-04T01:20:23Z,"Currently I am looking at the following code (discovered thru shouldUpdateClusterMetadataAndHostInfoOnAssignment) in onAssignment():
```
        if (receivedAssignmentMetadataVersion > usedSubscriptionMetadataVersion) {
            throw new IllegalStateException(""Sent a version "" + usedSubscriptionMetadataVersion
                + "" subscription but got an assignment with higher version "" + receivedAssignmentMetadataVersion + ""."");
        }
```
To make the above check pass, receivedAssignmentMetadataVersion is downgraded to 3 if receivedAssignmentMetadataVersion is 4 and there is no error in the AssignmentInfo.

"
402379446,5322,tedyu,2018-07-04T06:42:21Z,"DefaultPartitionGrouperTest#shouldNotCreateAnyTasksBecauseOneTopicHasUnknownPartitions still fails.
It tests against PartitionGrouper where there is no relevant logic after the PR."
403171070,5322,guozhangwang,2018-07-06T23:40:48Z,"> To make the above check pass, receivedAssignmentMetadataVersion is downgraded to 3 if receivedAssignmentMetadataVersion is 4 and there is no error in the AssignmentInfo.

About the version control semantics, please read KIP-268 (https://cwiki.apache.org/confluence/display/KAFKA/KIP-268%3A+Simplify+Kafka+Streams+Rebalance+Metadata+Upgrade) for the detailed explanation. As for this scenario, if `I` am using the latest Kafka version (2.1.0-SNAPSHOT) then `usedSubscriptionMetadataVersion = SubscriptionInfo.LATEST_SUPPORTED_VERSION` should be 4.

In your PR you've only bumped up the LATEST_SUPPORTED_VERSION for AssignmentInfo to 4, but not `SubscriptionInfo.LATEST_SUPPORTED_VERSION`, which is why it's failing."
403172175,5322,tedyu,2018-07-06T23:51:04Z,"When calling completeShutdown in StreamsThread, what should be the value for cleanRun ?
```
            completeShutdown(cleanRun);
```
I assume it should be false."
403173358,5322,guozhangwang,2018-07-07T00:01:55Z,"> When calling completeShutdown in StreamsThread, what should be the value for cleanRun ?

We can just call `shutdown()` which will simply change the state to `PENDING_SHUTDOWN`,  then in the StreamThread's main loop `while (isRunning()) {` will exit and continue to `completeShutdown`."
403259061,5322,tedyu,2018-07-08T03:22:38Z,retest this please
403279407,5322,tedyu,2018-07-08T10:55:51Z,"When I used the following command line:
```
./gradlew -Dtest.single=WordCountTest streams:test
```
I was told:
```
> Could not find matching test for pattern: WordCountTest
```
Also tried the following command:
```
./gradlew -Dtest.single=org.apache.kafka.streams.scala.StreamToTableJoinScalaIntegrationTestImplicitSerdes streams:test
```
```
Execution failed for task ':streams:test'.
> Could not find matching test for pattern: org.apache.kafka.streams.scala.StreamToTableJoinScalaIntegrationTestImplicitSerdes
```"
403366866,5322,tedyu,2018-07-09T05:49:22Z,"In the test output, e.g. https://builds.apache.org/job/kafka-pr-jdk10-scala2.12/2562/testReport/junit/org.apache.kafka.streams.scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes/testShouldCountClicksPerRegionJava/ , I don't see detailed log.

How can I enable -i for the integration tests ?"
403634460,5322,guozhangwang,2018-07-09T22:01:28Z,"> DefaultPartitionGrouperTest#shouldNotCreateAnyTasksBecauseOneTopicHasUnknownPartitions still fails.
> It tests against PartitionGrouper where there is no relevant logic after the PR.

I think this test should be modified as we should not expect this case any more."
403634927,5322,guozhangwang,2018-07-09T22:03:27Z,"try replace `streams:test` with `streams:streams-scala:test`, note that the package hierarchy is defined as sub-scope in `build.gradle`."
403635159,5322,guozhangwang,2018-07-09T22:04:23Z,"> In the test output, e.g. https://builds.apache.org/job/kafka-pr-jdk10-scala2.12/2562/testReport/junit/org.apache.kafka.streams.scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes/testShouldCountClicksPerRegionJava/ , I don't see detailed log.
> 
> How can I enable -i for the integration tests ?

You need to run your branch with log4j.properties updated, but I would not recommend that. I think running locally on a single test case that was failed is more effective."
403742335,5322,tedyu,2018-07-10T08:27:14Z,"I observed the following pattern for the timed out test testShouldCountClicksPerRegion :
```
    unknown numPartitions
    stream-table-join-scala-integration-test-KSTREAM-REDUCE-STATE-STORE-0000000006-repartition found
    sourceTopicName user-clicks numPartitionsCandidate 1 numPartitions -1
    sourceTopicName user-regions numPartitionsCandidate 1 numPartitions 1
    unknown numPartitions
    stream-table-join-scala-integration-test-KSTREAM-REDUCE-STATE-STORE-0000000006-repartition found
    sourceTopicName user-clicks numPartitionsCandidate 1 numPartitions -1
    sourceTopicName user-regions numPartitionsCandidate 1 numPartitions 1
```
The above repeats till the timeout. Here is how the additional log is added around line 495:
```
                    if (numPartitions == UNKNOWN) {
                        System.out.println(""unknown numPartitions for "" + topicName);
                        for (final InternalTopologyBuilder.TopicsInfo otherTopicsInfo : topicGroups.values()) {
...
                                    System.out.println(""sourceTopicName "" + sourceTopicName + "" numPartitionsCandidate ""
                                        + numPartitionsCandidate + "" numPartitions "" + numPartitions);
                                    if (numPartitionsCandidate > numPartitions) {
                                        numPartitions = numPartitionsCandidate;
                                    }
```"
403753271,5322,tedyu,2018-07-10T09:04:28Z,"The timeout was caused by rebalancing (streams/streams-scala/logs/kafka-streams-scala.log):
{code}
59150 [stream-table-join-scala-integration-test-0544d201-4cfa-41d9-846c-bd8efb7e94d6-StreamThread-1] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - stream-  thread [stream-table-join-scala-integration-test-0544d201-4cfa-41d9-846c-bd8efb7e94d6-StreamThread-1] Version probing detected. Triggering new rebalance.
{code}
which is not observed in the output of successful run."
403757169,5322,tedyu,2018-07-10T09:18:07Z,"From StreamThread:
```
                if (versionProbingFlag.get()) {
                    log.info(""Version probing detected. Triggering new rebalance."");
```
I added the following in StreamsPartitionAssignor :
```
            log.info(""receivedAssignmentMetadataVersion "" + receivedAssignmentMetadataVersion +
                "" usedSubscriptionMetadataVersion "" + usedSubscriptionMetadataVersion + "" receivedAssignmentMetadataVersion ""
                + receivedAssignmentMetadataVersion);
            versionProbingFlag.set(true);
```
In streams/streams-scala/logs/kafka-streams-scala.log , receivedAssignmentMetadataVersion was 3 while usedSubscriptionMetadataVersion was 4."
403768210,5322,tedyu,2018-07-10T09:57:09Z,"I looked at related code in StreamsPartitionAssignor and AssignmentInfo - it seems  receivedAssignmentMetadataVersion was 3 because 
```
            final SubscriptionInfo info = SubscriptionInfo.decode(subscription.userData());
            final int usedVersion = info.version();
```
usedVersion was 3, decoded from SubscriptionInfo"
403807709,5322,tedyu,2018-07-10T12:41:00Z,"ReassignPartitionsClusterTest.shouldExecuteThrottledReassignment was not related to the PR.

I ran it locally with my changes and it passed."
405027255,5322,tedyu,2018-07-14T14:32:02Z,"If I make the following change in StreamToTableJoinScalaIntegrationTestImplicitSerdes.testShouldCountClicksPerRegion :
```
    val userRegionsTable: KTable[String, String] = builder.table(userRegionsTopic+""1"")
```
In kafka-streams-scala.log , I observe:
```
45623 [stream-table-join-scala-integration-test-cd7d63e4-d0f8-45ae-940d-bd64170b9727-StreamThread-2] INFO  org.apache.kafka.streams.processor.internals.StreamThread  - stream-  thread [stream-table-join-scala-integration-test-cd7d63e4-d0f8-45ae-940d-bd64170b9727-StreamThread-2] Received error code 1 - shutdown
```
The test fails with:
```
org.apache.kafka.streams.scala.StreamToTableJoinScalaIntegrationTestImplicitSerdes > testShouldCountClicksPerRegion FAILED
    java.lang.AssertionError: Condition not met within timeout 30000. Did not receive all 3 records from topic output-topic
        at org.apache.kafka.test.TestUtils.waitForCondition(TestUtils.java:276)
        at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:295)
        at org.apache.kafka.streams.integration.utils.IntegrationTestUtils.waitUntilMinKeyValueRecordsReceived(IntegrationTestUtils.java:266)
        at org.apache.kafka.streams.scala.StreamToTableJoinScalaIntegrationTestImplicitSerdes.produceNConsume(StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala:243)
        at org.apache.kafka.streams.scala.StreamToTableJoinScalaIntegrationTestImplicitSerdes.testShouldCountClicksPerRegion(StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala:103)
```"
405285036,5322,tedyu,2018-07-16T15:24:26Z,"By registering KafkaStreams.StateListener, I was able to see the following transitions:
```
    old state CREATED -> new state RUNNING
    old state RUNNING -> new state REBALANCING
```"
405289444,5322,tedyu,2018-07-16T15:37:27Z,"By registering StreamThread.StateListener, I was able to see the following transitions:
```
    old state CREATED -> new state RUNNING
    old state RUNNING -> new state PARTITIONS_REVOKED
    old state PARTITIONS_REVOKED -> new state PENDING_SHUTDOWN
```"
405498057,5322,tedyu,2018-07-17T08:12:08Z,retest this please
405577056,5322,tedyu,2018-07-17T13:13:50Z,I ran the two failed tests locally with my PR - they passed.
405640617,5322,guozhangwang,2018-07-17T16:16:59Z,Re-triggered system test https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1855/ for the newest changes.
405725260,5322,guozhangwang,2018-07-17T20:57:05Z,"System test seems relevant, you can go to for more detailed logs:

https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1855/console

http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2018-07-17--001.1531851513--tedyu--trunk--f2a80c0/report.html

The failed one is this:

```
Module: kafkatest.tests.streams.streams_upgrade_test
Class:  StreamsUpgradeTest
Method: test_version_probing_upgrade
```"
405734184,5322,tedyu,2018-07-17T21:29:06Z,"https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1855/console gave me 404.

There is no clickable link on report.html related to the failed test.

The test failure probably is related to the new way of passing version probing parameter.
Should I restore the existing approach ?"
405737220,5322,guozhangwang,2018-07-17T21:40:35Z,"If you click on `Detail` link (the last column) on the `StreamsUpgradeTest` row (it is the only row in orange, meaning failed case), you'll be able to download the logs"
405740804,5322,tedyu,2018-07-17T21:54:16Z,"The following timed out:
```
                    leader_monitor.wait_until(""Received a future (version probing) subscription (version: 4). Sending empty assignment back (with supported version 3)."",
                                              timeout_sec=60,
                                              err_msg=""Could not detect 'version probing' attempt at leader "" + str(self.leader.node.account))
```
Trying to see why."
405743077,5322,tedyu,2018-07-17T22:02:43Z,"This log, from output of StreamsUpgradeTestJobRunnerService-0-140702392667856/worker4, is probably the reason:
```
[2018-07-17 16:59:10,489] INFO Unable to decode subscription data: used version: 5; latest supported version: 4 (org.apache.kafka.streams.processor.internals.assignment.        SubscriptionInfo)
[2018-07-17 16:59:10,489] INFO stream-thread [StreamsUpgradeTest-18562d20-8691-495b-b439-971fae7b8744-StreamThread-1-consumer] Received a future (version probing) subscription  (version: 5). Sending empty assignment back (with supported version 4). (org.apache.kafka.streams.processor.internals.StreamsPartitionAssignor)
```
Meaning the actual version numbers are bumped from existing values."
405743406,5322,tedyu,2018-07-17T22:04:09Z,"I can bump the hardcoded version numbers in streams_upgrade_test.py

Is there better way of modifying streams_upgrade_test.py ?"
405749066,5322,guozhangwang,2018-07-17T22:29:37Z,"> I can bump the hardcoded version numbers in streams_upgrade_test.py
> 
> Is there better way of modifying streams_upgrade_test.py ?

@mjsax could you share some thoughts here? My understanding is that `test_version_probing_upgrade` is testing a ""future"" version which is relying on `StreamsUpgradeTest.java` code, but am not 100% percent sure why bumping up the version to 4 now would break this test."
405750218,5322,tedyu,2018-07-17T22:35:01Z,"There're many StreamsUpgradeTest.java in the codebase
e.g.
```
./streams/upgrade-system-tests-0100/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java
./streams/upgrade-system-tests-0101/src/test/java/org/apache/kafka/streams/tests/StreamsUpgradeTest.java
```
From http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2018-07-17--001.1531851513--tedyu--trunk--f2a80c0/report.html , It was not immediately obvious which class led to the failure."
405752440,5322,tedyu,2018-07-17T22:46:22Z,"From 46/StreamsUpgradeTestJobRunnerService-2-140702392665808/worker6 
```
[2018-07-17 16:59:10,502] INFO stream-thread [StreamsUpgradeTest-5abf4265-b364-4ecd-9cee-afcd2aeecce8-StreamThread-1-consumer] Sent a version 5 subscription and got version 4   assignment back (successful version probing). Downgrading subscription metadata to received version and trigger new rebalance. (org.apache.kafka.streams.tests.                  StreamsUpgradeTest$FutureStreamsPartitionAssignor)
```"
405753197,5322,tedyu,2018-07-17T22:50:04Z,"@guozhangwang 
Mind triggering another system test run ?

Thanks"
405758276,5322,mjsax,2018-07-17T23:18:29Z,"Please make also sure, that we extend all existing test with regard to version probing, and supported assignment, subscription numbers (ie, `AssignmentInfoTest`, `SubscriptionInfoTest`, `StreamsPartitionAssignorTest`)"
405759633,5322,tedyu,2018-07-17T23:26:11Z,I have gone over the listed test classes above and seen new tests added.
405794452,5322,tedyu,2018-07-18T03:06:33Z,"From Guozhang 11 days ago:

We can just call shutdown() which will simply change the state to PENDING_SHUTDOWN, then in the StreamThread's main loop ```while (isRunning()) {``` will exit and continue to completeShutdown."
405970419,5322,tedyu,2018-07-18T15:22:55Z,retest this please
405999388,5322,tedyu,2018-07-18T16:48:20Z,"@guozhangwang 
Can you trigger another system test run ?

Thanks"
406054511,5322,guozhangwang,2018-07-18T19:56:23Z,Re-triggered https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1856/
406090972,5322,guozhangwang,2018-07-18T22:13:34Z,"@tedyu @mjsax it still fails: http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2018-07-18--001.1531951124--tedyu--trunk--48bdc94/report.html

logs to download: http://confluent-kafka-system-test-results.s3-us-west-2.amazonaws.com/2018-07-18--001.1531951124--tedyu--trunk--48bdc94/StreamsUpgradeTest/test_version_probing_upgrade/46.tgz"
406095151,5322,tedyu,2018-07-18T22:34:06Z,"From 46/StreamsUpgradeTestJobRunnerService-0-139790769699792/worker4 
```
[2018-07-18 20:39:09,375] INFO stream-thread [StreamsUpgradeTest-e16ee71e-f2f0-4f70-be71-9bb331ef9fd3-StreamThread-1-consumer] Sent a version 5 subscription and got version 4 assignment back (successful version probing). Setting subscription metadata to leaders supported version 5 and trigger new rebalance. (org.apache.kafka.streams.tests.StreamsUpgradeTest$FutureStreamsPartitionAssignor)
```
I will update streams_upgrade_test.py covering missed version numbers."
406096393,5322,tedyu,2018-07-18T22:40:33Z,"@guozhangwang 
Please trigger one more system test run.

Thanks"
406098237,5322,guozhangwang,2018-07-18T22:50:14Z,@tedyu done: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1861
406123254,5322,guozhangwang,2018-07-19T01:15:55Z,https://jenkins.confluent.io/job/system-test-kafka-branch-builder/1861/ has passed.
406155515,5322,tedyu,2018-07-19T05:01:34Z,"From the Scala 2.12 test run console log:
```
02:54:57     java.io.IOException: No space left on device
```"
406229613,5322,tedyu,2018-07-19T10:22:56Z,retest this please
406403161,5322,tedyu,2018-07-19T20:25:19Z,"w.r.t. integration test, is it Okay to leave that to a follow-on PR (the functionality has been verified in the current integration test) ?

I will add hashCode() and equals() in the next update."
406433536,5322,guozhangwang,2018-07-19T22:27:52Z,"Merged to trunk.

While working on fixing forward the test migration, I realized I would need more time, and during which new commits may got it to veto this one, So I've decided to work on it in a separate PR."
406448483,5322,mjsax,2018-07-19T23:56:23Z,Thanks for the PR @tedyu!
406449043,5322,tedyu,2018-07-20T00:00:17Z,"@mjsax 
Thanks for your detailed review."
1474405568,13391,jolshan,2023-03-17T21:09:38Z,"TODO:
~* config for the feature~
* request handling optimizations + bookeeping
~* confirm compatibility --> we may need Ibp~"
1504291314,13391,jolshan,2023-04-11T23:58:12Z,fyi I found this: https://issues.apache.org/jira/browse/KAFKA-14896 
1504315306,13391,jolshan,2023-04-12T00:16:02Z,^ Seems like this still may be initially caused by my change so I'm investigating.
1505528988,13391,jolshan,2023-04-12T15:56:31Z,"I realized I didn't push sorry :(
Just pushed now. I will also look at that failure some more today."
1505555154,13391,jolshan,2023-04-12T16:12:13Z,"I found the issue. I will fix.
```
java.lang.NullPointerException: Cannot invoke ""org.apache.kafka.common.requests.AddPartitionsToTxnResponse.data()"" because the return value of ""org.apache.kafka.clients.ClientResponse.responseBody()"" is null
	at kafka.server.AddPartitionsToTxnManager$AddPartitionsToTxnHandler.onComplete(AddPartitionsToTxnManager.scala:95)
```"
1505622263,13391,jolshan,2023-04-12T17:04:03Z,Found the issue -- we didn't correctly handle disconnects which would cause NPEs and force close the producer. Pushed the code to handle disconnects -- locally I did not see the issue after running 40 times (typically would see it in the first two runs before the fix)
1506123475,13391,jolshan,2023-04-13T00:07:39Z,"Here are errors on the latest build on trunk I could find:
https://ci-builds.apache.org/job/Kafka/job/kafka/job/trunk/1754/#showFailuresLink

Seems to roughly correlate with the failures I see. 

I think the only suspicious one is [Build / JDK 8 and Scala 2.12 / kafka.api.TransactionsBounceTest.testWithGroupId()](https://ci-builds.apache.org/job/Kafka/job/kafka-pr/job/PR-13391/26/testReport/junit/kafka.api/TransactionsBounceTest/Build___JDK_8_and_Scala_2_12___testWithGroupId__/) which was failing before without my fix. This error is slightly different and related to endTxn. I did see this flake one time when I was repeatedly testing on my branch. I can look on trunk as well. 
"
213117738,1251,SinghAsDev,2016-04-21T21:17:19Z,"@hachikuji @gwenshap @edenhill mind reviewing this?
"
215311447,1251,SinghAsDev,2016-04-28T05:09:48Z,"Rebased on trunk.
"
215454261,1251,granthenke,2016-04-28T15:00:37Z,"@SinghAsDev I think we should have some tests for this functionality.
"
215487593,1251,gwenshap,2016-04-28T16:34:56Z,"@SinghAsDev: Mind explaining why you chose to plug in the validation where you did? 
Both why do you think a new state is needed and why do you think handleConnection() (which until now just did some metrics) is the correct place for the validation? 
"
215489877,1251,gwenshap,2016-04-28T16:43:01Z,"@SinghAsDev: Another design level question:
It looks like you are validation every response. But - brokers can't respond in anything other than the request version, which the client provides.

I thought we discussed only validating the broker APIs when connecting. What made you change your mind?
"
215490039,1251,gwenshap,2016-04-28T16:43:32Z,"I'm pausing the line-level review until I have more clarity on the design decisions / scope.
"
220498557,1251,SinghAsDev,2016-05-20T01:55:18Z,"@gwenshap thanks for the review.

Below are the goals.
- Allow clients to not utilize KIP-35.
- Allow clients interested in KIP-35 to be able to check if all api versions it needs is supported by a broker.
- Api version check has to be done for each new connection, even for re-connects.

Below is the design overview.
- For each new connection established, if client, i.e., `KafkaProducer`/ `KafkaConsumer`, has specified api versions it will be using, send `ApiVersions` request.
- When `ApiVersions` response is received, client validates that client's requested api versions are supported by broker. If not, throw `KafkaException`.
- Whether a connection is ready to send requests is determined as below.
  - If client does not want to check api versions, i.e, client has not specified required api versions, the connection must be in `CONNECTED` state.
  - If client wants to check api versions, i.e., client has specified required api versions, the connection must be in `READY` state.

Valid connection states transitions.
- Client does not want to perform api versions check. Note that this is what it is right now.
  `DISCONNECTED` ->  `CONNECTING` -> `CONNECTED`
- Client wants to perform api versions check. Added with this patch.
  `DISCONNECTED` ->  `CONNECTING` -> `CONNECTED` -> `CHECKING_API_VERSIONS` -> `READY`

> Mind explaining why you chose to plug in the validation where you did? 
> Both why do you think a new state is needed and why do you think handleConnection() (which until now just did some metrics) is the correct place for the validation?

A new state is needed as a connection can be ready to send requests on either in `CONNECTED` or in `READY` state, based on client's need to check api versions.

As the api versions check is done for each connection, it is best to do it when we have a connection established. If check passes transition it to `READY` state that indicates successful api versions check. If the check fails, the connection is closed and an `KafkaException` is thrown.

> It looks like you are validation every response. But - brokers can't respond in anything other than the request version, which the client provides.
> I thought we discussed only validating the broker APIs when connecting. What made you change your mind?

I don't think every response is getting validated, only `ApiVersion` response will be validated. `ApiVersion` request is only sent while connecting.
"
225725972,1251,SinghAsDev,2016-06-13T22:19:15Z,"@gwenshap does the above explanation answer your question?
"
227901779,1251,SinghAsDev,2016-06-22T22:54:43Z,"@ijuma would you mind reviewing this?
"
227978150,1251,ijuma,2016-06-23T08:02:55Z,"@SinghAsDev, I left some initial comments, I didn't do a detailed review.

Personally, I think it's really important to have system tests for this feature to make sure it works as expected. Some of the system tests can be written now (for brokers 0.8.2.x and 0.9.0.x), but we probably can only write the ones for 0.10.0.0 once trunk has bumped the version of request types used by the producer and consumer. Testing at the `NetworkClient` level is not really enough as we want to make sure the errors are propagated all the way to the user.
"
228110393,1251,SinghAsDev,2016-06-23T16:45:11Z,"@ijuma I am planning on adding system tests for it. However, I want to get some feedback on overall approach. Though I had outlined the same approach as a response to Jay's question on KIP-35 discuss list, it wasn't discussed in great details. Gwen too had some questions on overall approach, to which I have posted answers earlier. I have discussed the approach with @hachikuji though, but review from a committer who can eventually help to commit this will help. Meanwhile, I can work on system tests.
"
228185438,1251,ijuma,2016-06-23T21:12:40Z,"@SinghAsDev Yes, it makes sense to get agreement on the overall approach before spending more time on this. I am a bit undecided on this one. It seems a bit wasteful to make an additional request to check the versions to simply fail if they don't match. The broker should really be returning an error when an unsupported request arrives.

What would be really cool is making the Java client support multiple broker versions. But that would require a discussion in the mailing list and it would be a bigger change.
"
228204101,1251,SinghAsDev,2016-06-23T22:25:03Z,"> It seems a bit wasteful to make an additional request to check the versions to simply fail if they don't match. The broker should really be returning an error when an unsupported request arrives.

This was discussed and agreed upon as part of KIP-35. Below is an excerpt.

> Java clients will use this API to assert that current API versions are all supported by the brokers it is talking to, and raise an exception if they are not. Java clients check will test the new API.

I think it has value to know that clients are not compatible with brokers it is trying to talk to, rather than getting connection dropped or similar not so sure response.

> What would be really cool is making the Java client support multiple broker versions. But that would require a discussion in the mailing list and it would be a bigger change.

This was proposed in KIP-35 and was shot down. We should definitely give it a try again, but as you said this will be a larger and lengthier discussion. Getting everyone on same page when it comes to compatibility definition, has not been so easy :).
"
228292359,1251,ijuma,2016-06-24T08:51:47Z,"Yes, I am aware of KIP-35 discussions and outcome. All those discussions happened with 0.10.0 in mind, which has now been released and this part of the KIP didn't make it into that release. Part of the motivation for this part was to ensure that ApiVersions request worked properly. That's a weaker argument now since we have already released it and we relied on other clients to test it for us.

I agree that there is value in knowing that clients are not compatible with brokers (which was the main motivation). However, we are only doing it this way because we don't have a way to return a generic error on any request, which is a bit of a shame.

On the topic of supporting multiple broker versions, yes, KIP-35 wasn't the right place for that. However, now that KIP-35 is in, it could be considered again. It's weird that the Java clients don't support this when third-party clients do. If we did this, there would be less of a need to backport client fixes to older versions (which people often can't upgrade because of the brokers). Anyway, it would be interesting to think if the approach we are doing now could be extended in that direction.

It just occurred to me that we are not handling the SASL case correctly. For that case, we need to send the `ApiVersionsRequest` before the `SaslHandshakeRequest` (see `SaslClientAuthenticator`). Right?

Finally, I don't mean to block this from going in, I was just sharing my thoughts on the subject. Since there is value and it was approved as part of KIP-35, as long as we can ensure that we don't break existing behaviour and that the new functionality works correctly, it seems like it can go in.
"
228835703,1251,SinghAsDev,2016-06-27T18:38:15Z,"@ijuma I agree with all you said. However, one of the issues I keep running into is that the longer a KIP or a PR remains open the more times its intent changes. Could we agree on adhering to what we decided on KIP-35 scope and let this go in, sure I will work on fixing any issues with it. This will make sure that we will have at least this basic check in Java clients in the next release. I have plans on using KIP-35's support to make rolling upgrade easier and along that we can initiate thoughts around adding support for multiple brokers. I am sure that it is going to take some time, however we definitely will have more insight as third party clients would have some working examples for us to point at. If you agree, then I can start looking into SASL issues you pointed out and adding tests. Will wait for your response.
"
230643285,1251,ijuma,2016-07-06T00:43:50Z,"@SinghAsDev, I checked with Jun and he's happy for this to go in as long as it's not too complicated. Perhaps the best thing is to first look at the SASL case to see how we handle that before spending effort on anything else?
"
232095144,1251,SinghAsDev,2016-07-12T16:03:12Z,"Sounds good @ijuma. I looked into SASL scenario and it looks like SASL is a bit off than other scenarios where we handle `ApiVersionRequest` in `SaslServerAuthenticator` itself, which means if a client wants to get api versions before SASL authentication, it is allowed. I think in KIP-43 we wanted to add this support to let clients know supported versions of `SaslHandshakeRequest`. Do we currently have a reason to do so? Maybe @rajinisivaram will have some thought here. The current changes will work even for SASL scenario, however if we really want to handle multiple versions of `SaslHandshakeRequest`, we need to get api versions before SASL authentication, and we have following options for that.
1. Move down whole api version checking from `NetworkClient` to `KafkaChannel`. Have `KafkaChannel` maintain supported api versions. Clients will access api versions info from the channel.
2. As Sasl authentication has a special handling for `ApiVersionRequest`, only have api versions fetched for the Sasl scenario in `authenticator` and put a check in `NetworkClient` to go to `READY` state from `CONNECTED`, skipping `CHECKING_API_VERSIONS`.

Thoughts?
"
232279950,1251,rajinisivaram,2016-07-13T07:40:48Z,"@SinghAsDev At the moment, there is only one version of `SaslHandshakeRequest`, but it will still be good to consider how this can be validated as well while you are doing the others. Perhaps a separate request from `SaslClientAuthenticator`to fetch only the supported versions of `SaslHandshakeRequest` would be neater? That way, you don't need to propagate the list of used APIs to the authenticator or the versions back to the NetworkClient. The rest of the code can stay as is. It would mean an additional `ApiVersionsRequest` for Sasl, but perhaps that is ok. If that is the agreed approach, you could add a comment in `SaslClientAuthenticator` and perhaps defer implementation until a new version of `SaslHandshakeRequest` is required. What do you think?
"
232622378,1251,ijuma,2016-07-14T10:04:26Z,"@rajinisivaram, your suggestion makes sense to me. Since the current SaslHandshakeRequest will continue to be supported, it is fair to only add the check once the client needs to use a hypothetical new version of SaslHandshakeRequest (hopefully never).
"
232727718,1251,SinghAsDev,2016-07-14T17:03:52Z,"@rajinisivaram @ijuma sounds like we do not have a strong reason to add special handling for SASL case as of now, and as such the current design for API version checking does not have to change.

@ijuma does this take care of your concern for SASL scenario, if it does I will start looking at other review comments.
"
232730951,1251,ijuma,2016-07-14T17:15:37Z,"@SinghAsDev Yes, worth adding a comment as Rajini suggested.
"
234559193,1251,SinghAsDev,2016-07-22T14:29:33Z,"@ijuma this should be good for you to take a look. The two test failures are not reproducible, tried locally multiple times with java 7.
"
236014652,1251,SinghAsDev,2016-07-28T20:26:15Z,"@ijuma one more ping :). Let me know if there is still something blocking this from getting merged.
"
266899476,1251,SinghAsDev,2016-12-13T23:51:19Z,"@cmccabe @hachikuji @ijuma  thanks for the review, updated PR."
266906090,1251,asfbot,2016-12-14T00:30:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/116/
Test PASSed (JDK 8 and Scala 2.12).
"
266906405,1251,asfbot,2016-12-14T00:32:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/117/
Test PASSed (JDK 8 and Scala 2.11).
"
266912696,1251,SinghAsDev,2016-12-14T01:10:38Z,"@ijuma so I added `LATEST_0_10` to `ApiVersionsCheckTest` and apparently due to KAFKA-4093, trunk's client won't be able to talk to `0.10.1.0`. Not sure if this is known. If not, let me know and I can file a JIRA."
266917584,1251,ijuma,2016-12-14T01:40:33Z,@SinghAsDev Not sure I understand. KAFKA-4093 was included in 0.10.1.0. Why would that prevent trunk clients from talking to 0.10.1.0 brokers?
266918243,1251,asfbot,2016-12-14T01:44:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/115/
Test PASSed (JDK 7 and Scala 2.10).
"
266919305,1251,SinghAsDev,2016-12-14T01:50:53Z,"@ijuma I meant `0.10.0.1`, that is what `LATEST_0_10_0` is pointing to."
266921743,1251,ijuma,2016-12-14T02:06:31Z,"It's true that trunk can't talk to `LATEST_0_10_0`, supposedly we detect this via `ApiVersionsResponse`? And for the `LATEST_0_10_1` case, it should work atm."
266926910,1251,SinghAsDev,2016-12-14T02:42:20Z,"Well, as this was supposed to be a step before client compatibility, in current PR API version checks are based on if there is any version overlap between client and broker for an API. However, as the client always sends the latest API version, to be able to catch that I will have to set expected minimum API version to latest version of the API. With that the version mismatch will be caught. However, soon client compatibility work will have to revert it back to what it is right now. I am changing the min expected version to be the latest API version and adding a comment indicating that it needs to be changed to min version when backwards client compatibility is added. Makes sense?"
266927464,1251,ijuma,2016-12-14T02:46:14Z,"@SinghAsDev if it's a simple change, that sounds good to me. If not, then we can leave as is."
266933894,1251,asfbot,2016-12-14T03:36:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/128/
Test PASSed (JDK 8 and Scala 2.11).
"
266934013,1251,asfbot,2016-12-14T03:37:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/126/
Test FAILed (JDK 7 and Scala 2.10).
"
266942422,1251,asfbot,2016-12-14T04:53:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/127/
Test FAILed (JDK 8 and Scala 2.12).
"
266943521,1251,asfbot,2016-12-14T05:02:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/131/
Test PASSed (JDK 8 and Scala 2.11).
"
266943547,1251,asfbot,2016-12-14T05:02:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/130/
Test PASSed (JDK 8 and Scala 2.12).
"
266952123,1251,asfbot,2016-12-14T06:14:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/129/
Test FAILed (JDK 7 and Scala 2.10).
"
267177446,1251,asfbot,2016-12-14T22:30:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/152/
Test PASSed (JDK 8 and Scala 2.11).
"
267177501,1251,asfbot,2016-12-14T22:30:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/151/
Test PASSed (JDK 8 and Scala 2.12).
"
267188425,1251,hachikuji,2016-12-14T23:22:29Z,"@SinghAsDev Thanks. Left one nitpick, but the latest changes are looking good. "
267196058,1251,asfbot,2016-12-15T00:04:35Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/151/
Test PASSed (JDK 7 and Scala 2.10).
"
267207997,1251,asfbot,2016-12-15T01:15:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/153/
Test FAILed (JDK 8 and Scala 2.11).
"
267208187,1251,asfbot,2016-12-15T01:16:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/152/
Test PASSed (JDK 8 and Scala 2.12).
"
267339807,1251,ijuma,2016-12-15T14:26:35Z,Started system tests build here: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/626/
267714157,1251,ijuma,2016-12-16T22:43:59Z,"@SinghAsDev, I created a PR to your branch that addresses the feedback that I think we need to address before this is merged:

https://github.com/SinghAsDev/kafka/pull/2

The rest can be done as part of the client compatibility KIP. If you are happy with the changes, can you please merge them to your branch and then merge trunk to your branch?"
267714603,1251,SinghAsDev,2016-12-16T22:46:57Z,"@ijuma thanks for the changes, I was working on addressing them myself. But, glad you commented here, merging your changes."
267714875,1251,ijuma,2016-12-16T22:48:36Z,"@SinghAsDev, sorry, I was afraid you may have been busy and wanted to take advantage of the momentum to get this merged. :)"
267715133,1251,asfbot,2016-12-16T22:49:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/201/
Test FAILed (JDK 8 and Scala 2.11).
"
267715144,1251,asfbot,2016-12-16T22:50:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/200/
Test FAILed (JDK 8 and Scala 2.12).
"
267715211,1251,asfbot,2016-12-16T22:50:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/199/
Test FAILed (JDK 7 and Scala 2.10).
"
267715212,1251,SinghAsDev,2016-12-16T22:50:22Z,"@ijuma no worries, rather thanks!"
267716319,1251,ijuma,2016-12-16T22:57:12Z,"@SinghAsDev looks like I didn't include a local change in my PR to get one of the tests to compile. It should be trivial to fix. Also, can you please address the overflow issue? I wasn't sure what the intent was on that one."
267725252,1251,asfbot,2016-12-17T00:01:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/203/
Test PASSed (JDK 7 and Scala 2.10).
"
267725348,1251,ijuma,2016-12-17T00:02:13Z,System tests run: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/630/console
267730195,1251,asfbot,2016-12-17T00:47:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/210/
Test PASSed (JDK 8 and Scala 2.11).
"
267730766,1251,asfbot,2016-12-17T00:53:33Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/208/
Test PASSed (JDK 7 and Scala 2.10).
"
267731876,1251,asfbot,2016-12-17T01:06:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/204/
Test PASSed (JDK 8 and Scala 2.12).
"
267731978,1251,asfbot,2016-12-17T01:07:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/205/
Test PASSed (JDK 8 and Scala 2.11).
"
267733559,1251,asfbot,2016-12-17T01:25:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/209/
Test FAILed (JDK 8 and Scala 2.12).
"
267759785,1251,ijuma,2016-12-17T12:19:04Z,"The system tests failures are also happening in trunk with the exception of the new system test that seems to be failing. Because the newly introduced system test will change as part of the client compatibility work, I will remove it and merge this PR."
267760134,1251,ijuma,2016-12-17T12:27:35Z,"LGTM, merged to trunk (without the system test as explained in the previous comment)."
132635211,151,ijuma,2015-08-19T15:16:22Z,"Tests passed locally.
"
132720656,151,asfbot,2015-08-19T17:54:56Z,"[kafka-trunk-git-pr #174](https://builds.apache.org/job/kafka-trunk-git-pr/174/) SUCCESS
This pull request looks good
"
133041845,151,ijuma,2015-08-20T15:01:22Z,"@gwenshap, I believe I have addressed everything that was discussed. I also merged trunk into this branch to fix a minor import conflict in a test. Tests pass locally. Please have a look and if you're happy, this may be good to go.
"
133049683,151,asfbot,2015-08-20T15:26:57Z,"[kafka-trunk-git-pr #177](https://builds.apache.org/job/kafka-trunk-git-pr/177/) FAILURE
Looks like there's a problem with this pull request
"
133056252,151,ijuma,2015-08-20T15:44:27Z,"Closed/reopened to trigger another CI build.
"
133057646,151,asfbot,2015-08-20T15:49:43Z,"[kafka-trunk-git-pr #178](https://builds.apache.org/job/kafka-trunk-git-pr/178/) FAILURE
Looks like there's a problem with this pull request
"
133058687,151,ijuma,2015-08-20T15:54:02Z,"The output doesn't actually say what (if anything) failed in the last CI build.
"
133070221,151,ijuma,2015-08-20T16:37:59Z,"Ran the tests locally two more times and they passed both times.
"
133213237,151,gwenshap,2015-08-20T23:48:49Z,"I think this needs a rebase, but otherwise LGTM. @junrao, do you want to take a look too?
"
133217955,151,gwenshap,2015-08-20T23:58:37Z,"actually, not good - maybe rebase related - but cloning the PR doesn't pass unit tests on my machine:
uncaught exception during compilation: java.lang.StackOverflowError

(Scala 2.10.5, Java 8)
"
133224996,151,gwenshap,2015-08-21T00:18:07Z,"ok, the stack size issue is intermittent and seem unrelated to the patch (happened to me once or twice on trunk too). My LGTM is back. @junrao wanted to take a look, so I'll hold off committing to give him a chance to comment.
"
133225449,151,asfbot,2015-08-21T00:20:09Z,"[kafka-trunk-git-pr #182](https://builds.apache.org/job/kafka-trunk-git-pr/182/) FAILURE
Looks like there's a problem with this pull request
"
133226071,151,ijuma,2015-08-21T00:23:27Z,"I fixed the conflict (just an import)
"
133226348,151,gwenshap,2015-08-21T00:26:29Z,"Cool. The test failure seems like an unrelated flaky. Opened KAFKA-2455 for it.
"
133227314,151,ijuma,2015-08-21T00:34:13Z,"Thanks!
"
133377428,151,ijuma,2015-08-21T11:27:10Z,"@junrao, thanks for the review. Learned a couple of new things about the `Selector` and metrics. :) I pushed two commits to address two of your comments. The other two I was not sure about so I asked some questions. 
"
133491198,151,ijuma,2015-08-21T16:49:03Z,"Closing this for now.
"
135916524,151,ijuma,2015-08-28T23:52:09Z,"Tests passed locally.
"
135916843,151,asfbot,2015-08-28T23:55:32Z,"[kafka-trunk-git-pr #261](https://builds.apache.org/job/kafka-trunk-git-pr/261/) SUCCESS
This pull request looks good
"
135918768,151,asfbot,2015-08-29T00:06:18Z,"[kafka-trunk-git-pr #262](https://builds.apache.org/job/kafka-trunk-git-pr/262/) FAILURE
Looks like there's a problem with this pull request
"
136154088,151,ijuma,2015-08-30T15:31:26Z,"Tests passed locally and in Jenkins once. The second time failed due to an unrelated failure that has been seen in other PRs (testMetricsLeak).
"
136378962,151,ijuma,2015-08-31T13:58:50Z,"Thanks for the review @junrao. I have addressed a number of the issues raised and left comments/questions for the rest.
"
136383173,151,asfbot,2015-08-31T14:15:26Z,"[kafka-trunk-git-pr #269](https://builds.apache.org/job/kafka-trunk-git-pr/269/) FAILURE
Looks like there's a problem with this pull request
"
136400954,151,ijuma,2015-08-31T15:09:02Z,"`./gradlew test` passed locally (I ran it 3 times). However, there is one system test failure:

test_id:    2015-08-31--001.kafkatest.tests.replication_test.ReplicationTest.test_hard_bounce
status:     FAIL
run time:   7 minutes 32.756 seconds

http://jenkins.confluent.io/job/kafka_system_tests_branch_builder/60/console

I will investigate this tomorrow (pointers are welcome though).
"
136737500,151,ijuma,2015-09-01T14:20:58Z,"@junrao, pushed two additional commits to address the two outstanding issues. I implemented things in a slightly different way than what was discussed. I added some comments to the PR explaining. Let me know what you think.

`gradlew test` passed locally and system tests passed too:

http://jenkins.confluent.io/job/kafka_system_tests_branch_builder/63/console 
"
136738664,151,asfbot,2015-09-01T14:24:15Z,"[kafka-trunk-git-pr #286](https://builds.apache.org/job/kafka-trunk-git-pr/286/) FAILURE
Looks like there's a problem with this pull request
"
136881820,151,ijuma,2015-09-01T22:30:26Z,"Pushed a couple of commits addressing review comments.
"
136882875,151,asfbot,2015-09-01T22:37:04Z,"[kafka-trunk-git-pr #298](https://builds.apache.org/job/kafka-trunk-git-pr/298/) FAILURE
Looks like there's a problem with this pull request
"
137035090,151,asfbot,2015-09-02T11:16:05Z,"[kafka-trunk-git-pr #305](https://builds.apache.org/job/kafka-trunk-git-pr/305/) FAILURE
Looks like there's a problem with this pull request
"
137036180,151,asfbot,2015-09-02T11:21:21Z,"[kafka-trunk-git-pr #306](https://builds.apache.org/job/kafka-trunk-git-pr/306/) FAILURE
Looks like there's a problem with this pull request
"
137114214,151,ijuma,2015-09-02T14:58:55Z,"@junrao, merged trunk and added missed scaladoc/javadoc.

`gradlew test` passed locally (as expected since there hasn't been any change that would affect behaviour)
"
137174188,151,ijuma,2015-09-02T17:09:52Z,"@junrao started a Kafka 0.8.2.1 cluster with 3 brokers manually and did a rolling upgrade (with kill -9) to to this branch. Used console producer and console consumer as well as describe-topic and things seemed to work as expected.
"
137178231,151,junrao,2015-09-02T17:23:22Z,"Thanks for the latest patch. Just a minor comment on the names of alive_brokers. Otherwise LGTM.
"
137185985,151,ijuma,2015-09-02T17:47:34Z,"@junrao Renamed to live_brokers as agreed. Tests passed locally.
"
137190266,151,asfbot,2015-09-02T17:55:49Z,"[kafka-trunk-git-pr #318](https://builds.apache.org/job/kafka-trunk-git-pr/318/) SUCCESS
This pull request looks good
"
299034856,2967,asfbot,2017-05-03T21:03:52Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3448/
Test FAILed (JDK 8 and Scala 2.11).
"
299035130,2967,asfbot,2017-05-03T21:04:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3439/
Test FAILed (JDK 8 and Scala 2.12).
"
299036119,2967,asfbot,2017-05-03T21:08:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3442/
Test FAILed (JDK 7 and Scala 2.10).
"
299043716,2967,xvrl,2017-05-03T21:40:22Z,"@ijuma sure, I can add the benchmarks, wasn't aware of the module"
299056708,2967,asfbot,2017-05-03T22:46:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3452/
Test PASSed (JDK 8 and Scala 2.11).
"
299058552,2967,asfbot,2017-05-03T22:56:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3446/
Test FAILed (JDK 7 and Scala 2.10).
"
299060242,2967,asfbot,2017-05-03T23:06:58Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3454/
Test FAILed (JDK 8 and Scala 2.11).
"
299060299,2967,asfbot,2017-05-03T23:07:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3445/
Test FAILed (JDK 8 and Scala 2.12).
"
299060394,2967,asfbot,2017-05-03T23:07:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3448/
Test FAILed (JDK 7 and Scala 2.10).
"
299062186,2967,asfbot,2017-05-03T23:19:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3449/
Test FAILed (JDK 7 and Scala 2.10).
"
299062194,2967,asfbot,2017-05-03T23:19:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3455/
Test FAILed (JDK 8 and Scala 2.11).
"
299062200,2967,asfbot,2017-05-03T23:19:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3446/
Test FAILed (JDK 8 and Scala 2.12).
"
299073205,2967,asfbot,2017-05-04T00:39:23Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3443/
Test FAILed (JDK 8 and Scala 2.12).
"
299105144,2967,asfbot,2017-05-04T06:24:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3461/
Test FAILed (JDK 8 and Scala 2.12).
"
299105363,2967,asfbot,2017-05-04T06:25:59Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3470/
Test FAILed (JDK 8 and Scala 2.11).
"
299107361,2967,asfbot,2017-05-04T06:37:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3471/
Test FAILed (JDK 8 and Scala 2.11).
"
299107625,2967,asfbot,2017-05-04T06:38:50Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3462/
Test FAILed (JDK 8 and Scala 2.12).
"
299108914,2967,asfbot,2017-05-04T06:47:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3465/
Test FAILed (JDK 7 and Scala 2.10).
"
299129309,2967,asfbot,2017-05-04T08:50:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3474/
Test PASSed (JDK 8 and Scala 2.11).
"
299134334,2967,asfbot,2017-05-04T09:15:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3468/
Test FAILed (JDK 7 and Scala 2.10).
"
299137997,2967,asfbot,2017-05-04T09:32:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3465/
Test FAILed (JDK 8 and Scala 2.12).
"
299254452,2967,asfbot,2017-05-04T17:30:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3496/
Test FAILed (JDK 8 and Scala 2.11).
"
299254733,2967,asfbot,2017-05-04T17:31:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3490/
Test FAILed (JDK 7 and Scala 2.10).
"
299254832,2967,asfbot,2017-05-04T17:31:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3487/
Test FAILed (JDK 8 and Scala 2.12).
"
299267431,2967,asfbot,2017-05-04T18:18:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3494/
Test FAILed (JDK 7 and Scala 2.10).
"
299279463,2967,asfbot,2017-05-04T19:04:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3500/
Test PASSed (JDK 8 and Scala 2.11).
"
299281125,2967,asfbot,2017-05-04T19:11:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3491/
Test FAILed (JDK 8 and Scala 2.12).
"
299305404,2967,asfbot,2017-05-04T20:51:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3506/
Test PASSed (JDK 8 and Scala 2.11).
"
299308889,2967,asfbot,2017-05-04T21:05:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3500/
Test PASSed (JDK 7 and Scala 2.10).
"
299311577,2967,asfbot,2017-05-04T21:16:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3497/
Test FAILed (JDK 8 and Scala 2.12).
"
299318451,2967,xvrl,2017-05-04T21:46:45Z,retest this please
299335008,2967,asfbot,2017-05-04T23:17:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3499/
Test FAILed (JDK 8 and Scala 2.12).
"
299335964,2967,xvrl,2017-05-04T23:24:28Z,retest this please
299336266,2967,asfbot,2017-05-04T23:26:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3502/
Test FAILed (JDK 7 and Scala 2.10).
"
299337375,2967,asfbot,2017-05-04T23:34:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3508/
Test FAILed (JDK 8 and Scala 2.11).
"
299347286,2967,asfbot,2017-05-05T00:53:27Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3503/
Test FAILed (JDK 8 and Scala 2.12).
"
299347347,2967,asfbot,2017-05-05T00:54:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3506/
Test FAILed (JDK 7 and Scala 2.10).
"
299348319,2967,xvrl,2017-05-05T01:03:11Z,Retest this please
299349040,2967,asfbot,2017-05-05T01:09:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3512/
Test FAILed (JDK 8 and Scala 2.11).
"
299355405,2967,asfbot,2017-05-05T02:14:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3516/
Test PASSed (JDK 7 and Scala 2.10).
"
299355632,2967,asfbot,2017-05-05T02:17:01Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3522/
Test PASSed (JDK 8 and Scala 2.11).
"
299357091,2967,asfbot,2017-05-05T02:32:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3513/
Test PASSed (JDK 8 and Scala 2.12).
"
299523823,2967,asfbot,2017-05-05T17:18:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3547/
Test FAILed (JDK 8 and Scala 2.12).
"
299524120,2967,asfbot,2017-05-05T17:20:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3556/
Test FAILed (JDK 8 and Scala 2.11).
"
299524274,2967,asfbot,2017-05-05T17:20:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3550/
Test FAILed (JDK 7 and Scala 2.10).
"
299556099,2967,asfbot,2017-05-05T19:33:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3562/
Test FAILed (JDK 7 and Scala 2.10).
"
299556106,2967,asfbot,2017-05-05T19:33:47Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3568/
Test FAILed (JDK 8 and Scala 2.11).
"
299556223,2967,asfbot,2017-05-05T19:34:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3558/
Test FAILed (JDK 8 and Scala 2.12).
"
299557186,2967,asfbot,2017-05-05T19:39:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3559/
Test FAILed (JDK 8 and Scala 2.12).
"
299569847,2967,asfbot,2017-05-05T20:38:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3552/
Test FAILed (JDK 8 and Scala 2.12).
"
299569850,2967,asfbot,2017-05-05T20:38:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3561/
Test FAILed (JDK 8 and Scala 2.11).
"
299569870,2967,asfbot,2017-05-05T20:38:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3555/
Test FAILed (JDK 7 and Scala 2.10).
"
299570948,2967,asfbot,2017-05-05T20:43:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3569/
Test FAILed (JDK 8 and Scala 2.11).
"
299575215,2967,asfbot,2017-05-05T21:03:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3563/
Test FAILed (JDK 7 and Scala 2.10).
"
299580471,2967,xvrl,2017-05-05T21:29:51Z,retest this please
299590163,2967,asfbot,2017-05-05T22:28:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3567/
Test FAILed (JDK 8 and Scala 2.12).
"
299590587,2967,asfbot,2017-05-05T22:31:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3571/
Test PASSed (JDK 7 and Scala 2.10).
"
299591587,2967,asfbot,2017-05-05T22:39:14Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3577/
Test FAILed (JDK 8 and Scala 2.11).
"
299594861,2967,xvrl,2017-05-05T23:03:27Z,retest this please
299595224,2967,xvrl,2017-05-05T23:06:28Z,"@ijuma adressed all the comments I believe, now just waiting on getting the build to pass.
@hachikuji you might be interested in reviewing this part https://github.com/apache/kafka/pull/2967/commits/97e3ce81cea2736232a9527dc6a8cbd29e366ccd"
299602227,2967,asfbot,2017-05-06T00:16:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3582/
Test PASSed (JDK 8 and Scala 2.11).
"
299603231,2967,asfbot,2017-05-06T00:29:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3576/
Test FAILed (JDK 7 and Scala 2.10).
"
299603670,2967,asfbot,2017-05-06T00:35:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3572/
Test FAILed (JDK 8 and Scala 2.12).
"
299604947,2967,xvrl,2017-05-06T00:53:23Z,retest this please
299609344,2967,asfbot,2017-05-06T02:11:08Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3574/
Test FAILed (JDK 8 and Scala 2.12).
"
299609666,2967,asfbot,2017-05-06T02:17:53Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3578/
Test FAILed (JDK 7 and Scala 2.10).
"
299610585,2967,asfbot,2017-05-06T02:39:15Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3584/
Test FAILed (JDK 8 and Scala 2.11).
"
299610626,2967,xvrl,2017-05-06T02:40:38Z,Retest this please
299613369,2967,asfbot,2017-05-06T03:49:48Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3585/
Test PASSed (JDK 8 and Scala 2.11).
"
299613416,2967,asfbot,2017-05-06T03:51:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3579/
Test PASSed (JDK 7 and Scala 2.10).
"
299614425,2967,asfbot,2017-05-06T04:15:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3575/
Test FAILed (JDK 8 and Scala 2.12).
"
299616481,2967,xvrl,2017-05-06T05:13:35Z,retest this please
299619391,2967,asfbot,2017-05-06T06:26:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3587/
Test PASSed (JDK 8 and Scala 2.11).
"
299620745,2967,asfbot,2017-05-06T07:01:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3577/
Test FAILed (JDK 8 and Scala 2.12).
"
299623951,2967,asfbot,2017-05-06T08:14:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3581/
Test FAILed (JDK 7 and Scala 2.10).
"
299643706,2967,xvrl,2017-05-06T14:33:35Z,retest this please
299647176,2967,asfbot,2017-05-06T15:30:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3589/
Test FAILed (JDK 8 and Scala 2.11).
"
299648009,2967,asfbot,2017-05-06T15:44:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3583/
Test PASSed (JDK 7 and Scala 2.10).
"
299650027,2967,asfbot,2017-05-06T16:16:16Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3579/
Test FAILed (JDK 8 and Scala 2.12).
"
299651792,2967,xvrl,2017-05-06T16:44:11Z,Retest this please
299654668,2967,hachikuji,2017-05-06T17:33:03Z,"@xvrl Probably not necessary to get a full green build if the failures are known issues. On the other hand, if you're just trying to shake out some more transient failures so that we can add JIRAs, have at it!"
299655833,2967,asfbot,2017-05-06T17:52:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/3586/
Test PASSed (JDK 7 and Scala 2.10).
"
299656114,2967,asfbot,2017-05-06T17:57:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/3592/
Test PASSed (JDK 8 and Scala 2.11).
"
299658015,2967,asfbot,2017-05-06T18:33:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/3582/
Test FAILed (JDK 8 and Scala 2.12).
"
299658313,2967,xvrl,2017-05-06T18:39:08Z,@hachikuji I give up   I think I've have each Java/Scala combination succeed at least  once. Also haven't seen any new failures in the last few runs.
302262542,2967,xvrl,2017-05-17T23:52:56Z,retest this please
302270173,2967,asfbot,2017-05-18T00:49:12Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4108/
Test PASSed (JDK 7 and Scala 2.11).
"
302274236,2967,asfbot,2017-05-18T01:20:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4094/
Test PASSed (JDK 8 and Scala 2.12).
"
302277607,2967,asfbot,2017-05-18T01:45:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4099/
Test PASSed (JDK 8 and Scala 2.12).
"
302491147,2967,asfbot,2017-05-18T17:58:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4141/
Test PASSed (JDK 7 and Scala 2.11).
"
302499095,2967,asfbot,2017-05-18T18:20:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4128/
Test PASSed (JDK 8 and Scala 2.12).
"
302507386,2967,xvrl,2017-05-18T18:47:27Z,@ijuma I have addressed all your comments
304417982,2967,asfbot,2017-05-27T01:35:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4466/
Test FAILed (JDK 7 and Scala 2.11).
"
304418147,2967,xvrl,2017-05-27T01:38:49Z,Retest this please
304419224,2967,asfbot,2017-05-27T01:57:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4452/
Test FAILed (JDK 8 and Scala 2.12).
"
304419373,2967,xvrl,2017-05-27T02:00:23Z,Retest this please
304423036,2967,asfbot,2017-05-27T03:08:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4473/
Test PASSed (JDK 7 and Scala 2.11).
"
304424563,2967,asfbot,2017-05-27T03:39:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4459/
Test FAILed (JDK 8 and Scala 2.12).
"
305020724,2967,asfbot,2017-05-30T21:58:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4576/
Test FAILed (JDK 7 and Scala 2.11).
"
305029768,2967,asfbot,2017-05-30T22:43:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4560/
Test FAILed (JDK 8 and Scala 2.12).
"
305030913,2967,xvrl,2017-05-30T22:50:10Z,retest this please
305035542,2967,ijuma,2017-05-30T23:15:39Z,Client system tests build will appear in the following link when it starts: https://jenkins.confluent.io/job/system-test-kafka-branch-builder/903/
305045147,2967,asfbot,2017-05-31T00:16:44Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.11/4587/
Test PASSed (JDK 7 and Scala 2.11).
"
305049908,2967,asfbot,2017-05-31T00:49:25Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/4572/
Test PASSed (JDK 8 and Scala 2.12).
"
305054123,2967,ijuma,2017-05-31T01:16:43Z,Clients system tests passed: https://jenkins.confluent.io/job/system-test-kafka-branch-builder-2/304/
305054234,2967,ijuma,2017-05-31T01:17:29Z,"LGTM, merging to trunk and 0.11.0."
288180972,2719,enothereska,2017-03-21T18:48:55Z,"Already getting an error from this basic test, that needs to be fixed before this PR goes in.
org.apache.kafka.streams.errors.StreamsException: Could not create internal topics.
	at org.apache.kafka.streams.processor.internals.InternalTopicManager.makeReady(InternalTopicManager.java:69). Looks like this might not be a real error though, since the replication factor of the internal topics is not maintained. Adding another broker and re-testing."
288194647,2719,asfbot,2017-03-21T19:38:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2303/
Test PASSed (JDK 7 and Scala 2.10).
"
288200402,2719,asfbot,2017-03-21T20:00:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2307/
Test PASSed (JDK 8 and Scala 2.11).
"
288231399,2719,asfbot,2017-03-21T21:54:29Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2303/
Test FAILed (JDK 8 and Scala 2.12).
"
288361587,2719,asfbot,2017-03-22T10:46:43Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2320/
Test PASSed (JDK 8 and Scala 2.12).
"
288362971,2719,asfbot,2017-03-22T10:52:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2320/
Test PASSed (JDK 7 and Scala 2.10).
"
288368865,2719,asfbot,2017-03-22T11:18:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2324/
Test PASSed (JDK 8 and Scala 2.11).
"
288384076,2719,enothereska,2017-03-22T12:30:36Z,@dguy want to have a look? Thanks.
288394312,2719,asfbot,2017-03-22T13:15:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2321/
Test PASSed (JDK 8 and Scala 2.12).
"
288394750,2719,asfbot,2017-03-22T13:17:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2325/
Test PASSed (JDK 8 and Scala 2.11).
"
288436867,2719,asfbot,2017-03-22T15:31:32Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2321/
Test FAILed (JDK 7 and Scala 2.10).
"
288461109,2719,enothereska,2017-03-22T16:41:54Z,"@dguy Making as WiP again since 2 tests are failing and I need to investigate.

[2017-03-22 16:25:05,575] ERROR stream-thread [SmokeTest-b25363c8-5bce-4d72-8c51-8fe0b49715f6-StreamThread-1] Streams application error during processing:  (org.apache.kafka.streams.processor.internals.StreamThread)
java.lang.IllegalStateException: Attempt to send a request to node 1 which is not ready.
        at org.apache.kafka.clients.NetworkClient.doSend(NetworkClient.java:284)
        at org.apache.kafka.clients.NetworkClient.send(NetworkClient.java:265)
        at org.apache.kafka.streams.processor.internals.StreamsKafkaClient.sendRequest(StreamsKafkaClient.java:238)
        at org.apache.kafka.streams.processor.internals.StreamsKafkaClient.createTopics(StreamsKafkaClient.java:170)
        at org.apache.kafka.streams.processor.internals.InternalTopicManager.makeReady(InternalTopicManager.java:63)
        at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.prepareTopic(StreamPartitionAssignor.java:615)
        at org.apache.kafka.streams.processor.internals.StreamPartitionAssignor.assign(StreamPartitionAssignor.java:445)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.performAssignment(ConsumerCoordinator.java:347)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.onJoinLeader(AbstractCoordinator.java:505)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.access$1100(AbstractCoordinator.java:93)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:455)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$JoinGroupResponseHandler.handle(AbstractCoordinator.java:437)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:788)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator$CoordinatorResponseHandler.onSuccess(AbstractCoordinator.java:769)
        at org.apache.kafka.clients.consumer.internals.RequestFuture$1.onSuccess(RequestFuture.java:190)
        at org.apache.kafka.clients.consumer.internals.RequestFuture.fireSuccess(RequestFuture.java:153)
        at org.apache.kafka.clients.consumer.internals.RequestFuture.complete(RequestFuture.java:120)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient$RequestFutureCompletionHandler.fireCompletion(ConsumerNetworkClient.java:498)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.firePendingCompletedRequests(ConsumerNetworkClient.java:342)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:251)
        at org.apache.kafka.clients.consumer.internals.ConsumerNetworkClient.poll(ConsumerNetworkClient.java:167)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.joinGroupIfNeeded(AbstractCoordinator.java:351)
        at org.apache.kafka.clients.consumer.internals.AbstractCoordinator.ensureActiveGroup(AbstractCoordinator.java:307)
        at org.apache.kafka.clients.consumer.internals.ConsumerCoordinator.poll(ConsumerCoordinator.java:294)
        at org.apache.kafka.clients.consumer.KafkaConsumer.pollOnce(KafkaConsumer.java:1033)
        at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:999)
        at org.apache.kafka.streams.processor.internals.StreamThread.runLoop(StreamThread.java:542)
        at org.apache.kafka.streams.processor.internals.StreamThread.run(StreamThread.java:326)"
288476090,2719,asfbot,2017-03-22T17:27:45Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2328/
Test PASSed (JDK 8 and Scala 2.12).
"
288476203,2719,asfbot,2017-03-22T17:28:03Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2328/
Test PASSed (JDK 7 and Scala 2.10).
"
288502187,2719,asfbot,2017-03-22T18:51:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2332/
Test FAILed (JDK 8 and Scala 2.11).
"
288688926,2719,asfbot,2017-03-23T11:21:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2346/
Test PASSed (JDK 8 and Scala 2.12).
"
288689964,2719,asfbot,2017-03-23T11:26:06Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2346/
Test PASSed (JDK 7 and Scala 2.10).
"
288694368,2719,asfbot,2017-03-23T11:48:22Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2350/
Test PASSed (JDK 8 and Scala 2.11).
"
289304695,2719,asfbot,2017-03-26T18:33:24Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2397/
Test FAILed (JDK 8 and Scala 2.11).
"
289304708,2719,asfbot,2017-03-26T18:33:38Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2393/
Test FAILed (JDK 8 and Scala 2.12).
"
289304733,2719,asfbot,2017-03-26T18:33:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2393/
Test FAILed (JDK 7 and Scala 2.10).
"
289372151,2719,asfbot,2017-03-27T07:14:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2405/
Test FAILed (JDK 8 and Scala 2.11).
"
289372211,2719,asfbot,2017-03-27T07:14:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2401/
Test FAILed (JDK 8 and Scala 2.12).
"
289372273,2719,asfbot,2017-03-27T07:14:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2401/
Test FAILed (JDK 7 and Scala 2.10).
"
289386933,2719,asfbot,2017-03-27T08:28:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2404/
Test FAILed (JDK 8 and Scala 2.12).
"
289386970,2719,asfbot,2017-03-27T08:28:19Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2408/
Test FAILed (JDK 8 and Scala 2.11).
"
289387113,2719,asfbot,2017-03-27T08:28:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2404/
Test FAILed (JDK 7 and Scala 2.10).
"
289389207,2719,asfbot,2017-03-27T08:37:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2405/
Test FAILed (JDK 8 and Scala 2.12).
"
289389277,2719,asfbot,2017-03-27T08:38:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2409/
Test FAILed (JDK 8 and Scala 2.11).
"
289389441,2719,asfbot,2017-03-27T08:38:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2405/
Test FAILed (JDK 7 and Scala 2.10).
"
289405099,2719,asfbot,2017-03-27T09:44:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2410/
Test PASSed (JDK 8 and Scala 2.11).
"
289405370,2719,asfbot,2017-03-27T09:46:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2406/
Test PASSed (JDK 8 and Scala 2.12).
"
289405893,2719,asfbot,2017-03-27T09:48:21Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2406/
Test PASSed (JDK 7 and Scala 2.10).
"
289466183,2719,asfbot,2017-03-27T14:13:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2409/
Test PASSed (JDK 7 and Scala 2.10).
"
289468696,2719,asfbot,2017-03-27T14:22:20Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2409/
Test FAILed (JDK 8 and Scala 2.12).
"
289475597,2719,asfbot,2017-03-27T14:44:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2413/
Test PASSed (JDK 8 and Scala 2.11).
"
289475648,2719,enothereska,2017-03-27T14:44:56Z,@dguy @mjsax @guozhangwang your review is appreciated. This should eventually got to 0.10.2 bug release as well I believe.
289489414,2719,asfbot,2017-03-27T15:28:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2418/
Test FAILed (JDK 8 and Scala 2.11).
"
289490594,2719,asfbot,2017-03-27T15:32:13Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2414/
Test PASSed (JDK 7 and Scala 2.10).
"
289490674,2719,asfbot,2017-03-27T15:32:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2414/
Test PASSed (JDK 8 and Scala 2.12).
"
289514114,2719,enothereska,2017-03-27T16:51:53Z,System tests pass on jenkins too: https://jenkins.confluent.io/job/system-test-kafka-branch-builder-2/220/console
289601989,2719,enothereska,2017-03-27T22:13:06Z,@guozhangwang we wrap exceptions as streams exceptions since the code that calls these functions and retries only understands streams exceptions. I'd argue that's a good thing since that code cannot possibly know all other exceptions in the lower layers.
289602348,2719,enothereska,2017-03-27T22:14:48Z,"@guozhangwang about  `I still think it's better to fail fast and educate users retry creating their apps after the broker is fully up than trying to wait for, say 5 seconds and hopefully it will succeed.` Remember that the broker can fail anytime and the user cannot have any guarantee that once a broker is up it won't fail again. In a subsequent JIRA we can consider unifying the backoff times to match other standard backoff times that clients use."
289720444,2719,enothereska,2017-03-28T09:52:58Z,"Note: I'll split this PR into multiple PRs, one that requires a KIP and another that doesn't and is needed for bug fix. cc @guozhangwang "
289745426,2719,asfbot,2017-03-28T11:50:46Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2453/
Test PASSed (JDK 7 and Scala 2.10).
"
289746837,2719,asfbot,2017-03-28T11:57:42Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2453/
Test PASSed (JDK 8 and Scala 2.12).
"
289747458,2719,asfbot,2017-03-28T12:00:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2457/
Test PASSed (JDK 8 and Scala 2.11).
"
289919770,2719,enothereska,2017-03-28T22:06:54Z,@mjsax @guozhangwang @dguy I've adjusted this PR so it doesn't require a KIP. Needs re-reviewing though. Thanks.
289930996,2719,asfbot,2017-03-28T23:00:07Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2481/
Test FAILed (JDK 8 and Scala 2.11).
"
289931415,2719,asfbot,2017-03-28T23:02:41Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2477/
Test FAILed (JDK 7 and Scala 2.10).
"
289936470,2719,asfbot,2017-03-28T23:31:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2477/
Test FAILed (JDK 8 and Scala 2.12).
"
290004512,2719,asfbot,2017-03-29T07:17:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2486/
Test PASSed (JDK 8 and Scala 2.12).
"
290014961,2719,asfbot,2017-03-29T08:05:55Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2490/
Test FAILed (JDK 8 and Scala 2.11).
"
290035472,2719,asfbot,2017-03-29T09:30:30Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2486/
Test FAILed (JDK 7 and Scala 2.10).
"
290206501,2719,asfbot,2017-03-29T19:55:56Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2507/
Test PASSed (JDK 7 and Scala 2.10).
"
290217528,2719,asfbot,2017-03-29T20:35:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2511/
Test PASSed (JDK 8 and Scala 2.11).
"
290217702,2719,asfbot,2017-03-29T20:36:05Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2507/
Test PASSed (JDK 8 and Scala 2.12).
"
290361924,2719,asfbot,2017-03-30T09:57:57Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2523/
Test PASSed (JDK 7 and Scala 2.10).
"
290362800,2719,asfbot,2017-03-30T10:01:28Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2523/
Test PASSed (JDK 8 and Scala 2.12).
"
290362875,2719,enothereska,2017-03-30T10:01:45Z,@mjsax @dguy one last look please? Thanks.
290370708,2719,asfbot,2017-03-30T10:31:34Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2527/
Test PASSed (JDK 8 and Scala 2.11).
"
290527834,2719,asfbot,2017-03-30T20:01:02Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2545/
Test PASSed (JDK 7 and Scala 2.10).
"
290527976,2719,asfbot,2017-03-30T20:01:31Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2545/
Test PASSed (JDK 8 and Scala 2.12).
"
290537299,2719,asfbot,2017-03-30T20:37:09Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2549/
Test PASSed (JDK 8 and Scala 2.11).
"
290670319,2719,asfbot,2017-03-31T09:53:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2572/
Test PASSed (JDK 8 and Scala 2.12).
"
290670393,2719,asfbot,2017-03-31T09:53:40Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2572/
Test PASSed (JDK 7 and Scala 2.10).
"
290677095,2719,asfbot,2017-03-31T10:26:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2576/
Test PASSed (JDK 8 and Scala 2.11).
"
290928787,2719,enothereska,2017-04-01T15:55:28Z,https://jenkins.confluent.io/job/system-test-kafka-branch-builder/820/ is green
291110511,2719,enothereska,2017-04-03T10:54:47Z,@mjsax @dguy can I get a final approve so that @ijuma or others can check in? Also just opened 0.10.2 cherry picked version of this PR. Thanks.
291134009,2719,asfbot,2017-04-03T12:54:18Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2634/
Test FAILed (JDK 8 and Scala 2.11).
"
291134152,2719,asfbot,2017-04-03T12:54:54Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2630/
Test FAILed (JDK 8 and Scala 2.12).
"
291134330,2719,asfbot,2017-04-03T12:55:36Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2630/
Test FAILed (JDK 7 and Scala 2.10).
"
291137107,2719,enothereska,2017-04-03T13:06:56Z,retest this please
291144856,2719,asfbot,2017-04-03T13:37:00Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2631/
Test PASSed (JDK 8 and Scala 2.12).
"
291144969,2719,asfbot,2017-04-03T13:37:26Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2631/
Test PASSed (JDK 7 and Scala 2.10).
"
291148346,2719,asfbot,2017-04-03T13:49:37Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2635/
Test FAILed (JDK 8 and Scala 2.11).
"
291204138,2719,enothereska,2017-04-03T16:55:06Z,"Env failure, unrelated to PR"
291277682,2719,enothereska,2017-04-03T21:21:08Z,"@gwenshap @ewencp @ijuma this can go in, thanks. Also the 0.10.2 version of this: https://github.com/apache/kafka/pull/2793. Thanks."
291639853,2719,asfbot,2017-04-04T21:39:10Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.12/2717/
Test PASSed (JDK 8 and Scala 2.12).
"
291640483,2719,asfbot,2017-04-04T21:41:51Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/2721/
Test PASSed (JDK 8 and Scala 2.11).
"
291647345,2719,asfbot,2017-04-04T22:10:17Z,"
Refer to this link for build results (access rights to CI server needed): 
https://builds.apache.org/job/kafka-pr-jdk7-scala2.10/2717/
Test FAILed (JDK 7 and Scala 2.10).
"
857875970,10822,kpatelatwork,2021-06-09T17:07:38Z,"Below are some logs when both connector and tasks are restarted.

> [2021-06-09 12:02:26,045] DEBUG Writing RestartRequest{connectorName='simple-source', onlyFailed=false, includeTasks=true} to Kafka (org.apache.kafka.connect.storage.KafkaConfigBackingStore:487)
[2021-06-09 12:02:26,049] INFO [Worker clientId=connect-1, groupId=connect-integration-test-connect-cluster] Received RestartRequest{connectorName='simple-source', onlyFailed=false, includeTasks=true} (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1709)
[2021-06-09 12:02:26,051] INFO [Worker clientId=connect-1, groupId=connect-integration-test-connect-cluster] Executing plan to restart connector and 4 of 4 tasks for RestartRequest{connectorName='simple-source', onlyFailed=false, includeTasks=true} (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1137)
[2021-06-09 12:02:26,063] DEBUG [Worker clientId=connect-1, groupId=connect-integration-test-connect-cluster] Restarting 4 of 4 tasks for RestartRequest{connectorName='simple-source', onlyFailed=false, includeTasks=true} (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1169)
[2021-06-09 12:02:26,114] DEBUG [Worker clientId=connect-1, groupId=connect-integration-test-connect-cluster] Restarted 4 of 4 tasks for RestartRequest{connectorName='simple-source', onlyFailed=false, includeTasks=true} as requested (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1171)
[2021-06-09 12:02:26,114] INFO [Worker clientId=connect-1, groupId=connect-integration-test-connect-cluster] Completed plan to restart connector and 4 of 4 tasks for RestartRequest{connectorName='simple-source', onlyFailed=false, includeTasks=true} (org.apache.kafka.connect.runtime.distributed.DistributedHerder:1173)"
859064545,10822,kpatelatwork,2021-06-10T21:02:17Z,"Passing system tests results
![image](https://user-images.githubusercontent.com/29556518/121596558-32270f80-ca05-11eb-843d-9a23824b98fd.png)


"
863639444,10822,kpatelatwork,2021-06-18T00:12:45Z,@rhauch I have taken care of most of the review comments and resolved them. I left review comments with follow up questions as unresolved. If you get some time could you please review and guide me? 
864493282,10822,kpatelatwork,2021-06-20T03:15:09Z,"Fired up local kafka server and kafka connect and was able to curl the API locally.

>   kafka git:(KAFKA-4793)  curl http://192.168.1.220:8083/connectors/local-file-source/status        
{""name"":""local-file-source"",""connector"":{""state"":""RUNNING"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""RUNNING"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%                              kafka git:(KAFKA-4793)  curl -XPOST http://192.168.1.220:8083/connectors/local-file-source/restart\?includeTasks\=true\&onlyFailed\=false
{""name"":""local-file-source"",""connector"":{""state"":""RESTARTING"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""RESTARTING"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%                        kafka git:(KAFKA-4793)  curl -XPOST http://192.168.1.220:8083/connectors/local-file-source/restart\?includeTasks\=false\&onlyFailed\=false
  kafka git:(KAFKA-4793)  curl -XPOST http://192.168.1.220:8083/connectors/local-file-source/restart\?includeTasks\=false\&onlyFailed\=true 
{""name"":""local-file-source"",""connector"":{""state"":""RUNNING"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""RUNNING"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%                              kafka git:(KAFKA-4793)  curl -XPOST http://192.168.1.220:8083/connectors/local-file-source/restart\?includeTasks\=true\&onlyFailed\=true
{""name"":""local-file-source"",""connector"":{""state"":""RUNNING"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""RUNNING"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%                              kafka git:(KAFKA-4793)  curl -XPOST http://192.168.1.220:8083/connectors/local-file-source/restart\?includeTasks\=true                  
{""name"":""local-file-source"",""connector"":{""state"":""RESTARTING"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""RESTARTING"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%  "
870820269,10822,kpatelatwork,2021-06-29T18:29:01Z,"@kkonstantine Thanks a lot for taking the time and posting the detailed review comments. I was able to resolve all of them except for a few where I left some additional comments.   

Please let me know how you want to proceed with them."
870979782,10822,kkonstantine,2021-06-29T23:16:56Z,"fyi, a test failure seems relevant: 
`testCORSEnabled  org.apache.kafka.connect.runtime.rest.RestServerTest`
I don't remember this test being flaky and it failed in both builders. "
871880686,10822,kpatelatwork,2021-07-01T03:14:34Z,"> fyi, a test failure seems relevant:
> `testCORSEnabled  org.apache.kafka.connect.runtime.rest.RestServerTest`
> I don't remember this test being flaky and it failed in both builders.

After rebase I do not see the failures"
2050761892,15640,kirktrue,2024-04-12T00:29:10Z,@cadonna @lianetm @lucasbru @philipneeplease review this PR if you have some spare time. Thanks!
2077870915,15640,kirktrue,2024-04-25T18:09:51Z,"@cadonnathanks for your review. I have made the requested changes, so please take another pass. Thanks!"
2083370524,15640,kirktrue,2024-04-29T18:19:47Z,"> Here I have a comment, I could not put at the right location in the code:
> 
> On line 1362, in `commitSync()` the consumer waits on the `commitFuture` with a timer. I think, it should not wait on a timer there since we already wait on a timer in the background thread.

I agree. What about the timed wait in `awaitPendingAsyncCommitsAndExecuteCommitCallbacks()`?"
2083394774,15640,kirktrue,2024-04-29T18:32:54Z,"@cadonnaIn most places I removed use of a `Timer` to calculate the deadline. Event classes no longer require a `Timer`, it is the caller who must call `CompletableEvent.calculateDeadlineMs()` when creating the event."
2084906500,15640,cadonna,2024-04-30T10:11:21Z,"> > Here I have a comment, I could not put at the right location in the code:
> > On line 1362, in `commitSync()` the consumer waits on the `commitFuture` with a timer. I think, it should not wait on a timer there since we already wait on a timer in the background thread.
> 
> I agree. What about the timed wait in `awaitPendingAsyncCommitsAndExecuteCommitCallbacks()`?

Do you have something in mind? Like using the `commitFuture` as a timeout for the awaiting of the execution of the callback?  "
2087355371,15640,lianetm,2024-04-30T21:17:39Z,"> > Here I have a comment, I could not put at the right location in the code:
> > 
> > On line 1362, in commitSync() the consumer waits on the commitFuture with a timer. I think, it should not wait on a timer there since we already wait on a timer in the background thread.
> 
> I agree. What about the timed wait in awaitPendingAsyncCommitsAndExecuteCommitCallbacks()?

Agree we should not wait on the `commitFuture` with a timer because the deadline is contained in the event we submitted, and already enforced by the reaper, and not clear about what the proposed relationship with `awaitPendingAsyncCommitsAndExecuteCommitCallbacks` is??

I would expect we only need to call `ConsumerUtils.getResult(commitFuture);`, and that is consistent with how we get results for all other completable events now:
- we create an event with a deadline
- we call `applicationEventHandler.addAndGet(event)`   

For the commit case that flow has a different shape just because we use `applicationEventHandler.add(event)` [here](https://github.com/apache/kafka/blob/097522abd6b51bca2407ea0de7009ed6a2d970b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L775), to cater for commit sync and async, but we should still apply the same approach and just call get without any time boundary I would say. "
2087359722,15640,lianetm,2024-04-30T21:19:03Z,"Hey @kirktrue , thanks a lot for the PR, this is a big piece! I completed a pass of all the non-test files, left some comments. "
2089079055,15640,kirktrue,2024-05-01T20:26:23Z,"> > > Here I have a comment, I could not put at the right location in the code:
> > > On line 1362, in commitSync() the consumer waits on the commitFuture with a timer. I think, it should not wait on a timer there since we already wait on a timer in the background thread.
> > 
> > 
> > I agree. What about the timed wait in awaitPendingAsyncCommitsAndExecuteCommitCallbacks()?
> 
> Agree we should not wait on the `commitFuture` with a timer because the deadline is contained in the event we submitted, and already enforced by the reaper, and not clear about what the proposed relationship with `awaitPendingAsyncCommitsAndExecuteCommitCallbacks` is??
> 
> I would expect we only need to call `ConsumerUtils.getResult(commitFuture);`, and that is consistent with how we get results for all other completable events now:
> 
> * we create an event with a deadline
> * we call `applicationEventHandler.addAndGet(event)`
> 
> For the commit case that flow has a different shape just because we use `applicationEventHandler.add(event)` [here](https://github.com/apache/kafka/blob/097522abd6b51bca2407ea0de7009ed6a2d970b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L775), to cater for commit sync and async, but we should still apply the same approach and just call get without any time boundary I would say.

Here's my reasoning on the need for a `Timer`-based `get()` in `awaitPendingAsyncCommitsAndExecuteCommitCallbacks()`... 

The `Future` that's referenced in `lastPendingAsyncCommit` comes from an `AsyncCommitEvent` and has a hard-coded deadline of `Long.MAX_VALUE`. As such, the `CompetableEventReaper` in the network thread will never prune that event. Without a timeout when calling `get()` on the `lastPendingAsyncCommit`, the caller could hang for up to `request.timeout.ms` while we wait for the network I/O request to complete (or timeout).

@cadonna @lianetm @lucasbrudoes that make sense? CMIIW, please "
2090517636,15640,cadonna,2024-05-02T13:34:30Z,"> > > > Here I have a comment, I could not put at the right location in the code:
> > > > On line 1362, in commitSync() the consumer waits on the commitFuture with a timer. I think, it should not wait on a timer there since we already wait on a timer in the background thread.
> > > 
> > > 
> > > I agree. What about the timed wait in awaitPendingAsyncCommitsAndExecuteCommitCallbacks()?
> > 
> > 
> > Agree we should not wait on the `commitFuture` with a timer because the deadline is contained in the event we submitted, and already enforced by the reaper, and not clear about what the proposed relationship with `awaitPendingAsyncCommitsAndExecuteCommitCallbacks` is??
> > I would expect we only need to call `ConsumerUtils.getResult(commitFuture);`, and that is consistent with how we get results for all other completable events now:
> > 
> > * we create an event with a deadline
> > * we call `applicationEventHandler.addAndGet(event)`
> > 
> > For the commit case that flow has a different shape just because we use `applicationEventHandler.add(event)` [here](https://github.com/apache/kafka/blob/097522abd6b51bca2407ea0de7009ed6a2d970b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L775), to cater for commit sync and async, but we should still apply the same approach and just call get without any time boundary I would say.
> 
> Here's my reasoning on the need for a `Timer`-based `get()` in `awaitPendingAsyncCommitsAndExecuteCommitCallbacks()`...
> 
> The `Future` that's referenced in `lastPendingAsyncCommit` comes from an `AsyncCommitEvent` and has a hard-coded deadline of `Long.MAX_VALUE`. As such, the `CompetableEventReaper` in the network thread will never prune that event. Without a timeout when calling `get()` on the `lastPendingAsyncCommit`, the caller could hang for up to `request.timeout.ms` while we wait for the network I/O request to complete (or timeout).
> 
> @cadonna @lianetm @lucasbrudoes that make sense? CMIIW, please 

I think, we are talking about two separate things here:

1. change `ConsumerUtils.getResult(commitFuture, requestTimer);` to `ConsumerUtils.getResult(commitFuture);`
2. Do we need a timer for `awaitPendingAsyncCommitsAndExecuteCommitCallbacks(requestTimer, true);`

As far as I understand, we agree on 1 but do not know if there is a better solution for 2. Is this correct?

Currently, I do not see a better way than using a timer on awaiting the execution of the async commit callback. As @kirktrue pointed out, since the async commit does basically not have a timeout, we cannot wait on the deadline of the `AsyncCommitEvent`. We can also not wait for the deadline of the `SyncCommitEvent` since if that event completes before the timeout, we would not wait enough for the completion of the async commit callback.   "
2090837554,15640,cadonna,2024-05-02T15:40:18Z,"> > > > > Here I have a comment, I could not put at the right location in the code:
> > > > > On line 1362, in commitSync() the consumer waits on the commitFuture with a timer. I think, it should not wait on a timer there since we already wait on a timer in the background thread.
> > > > 
> > > > 
> > > > I agree. What about the timed wait in awaitPendingAsyncCommitsAndExecuteCommitCallbacks()?
> > > 
> > > 
> > > Agree we should not wait on the `commitFuture` with a timer because the deadline is contained in the event we submitted, and already enforced by the reaper, and not clear about what the proposed relationship with `awaitPendingAsyncCommitsAndExecuteCommitCallbacks` is??
> > > I would expect we only need to call `ConsumerUtils.getResult(commitFuture);`, and that is consistent with how we get results for all other completable events now:
> > > 
> > > * we create an event with a deadline
> > > * we call `applicationEventHandler.addAndGet(event)`
> > > 
> > > For the commit case that flow has a different shape just because we use `applicationEventHandler.add(event)` [here](https://github.com/apache/kafka/blob/097522abd6b51bca2407ea0de7009ed6a2d970b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L775), to cater for commit sync and async, but we should still apply the same approach and just call get without any time boundary I would say.
> > 
> > 
> > Here's my reasoning on the need for a `Timer`-based `get()` in `awaitPendingAsyncCommitsAndExecuteCommitCallbacks()`...
> > The `Future` that's referenced in `lastPendingAsyncCommit` comes from an `AsyncCommitEvent` and has a hard-coded deadline of `Long.MAX_VALUE`. As such, the `CompetableEventReaper` in the network thread will never prune that event. Without a timeout when calling `get()` on the `lastPendingAsyncCommit`, the caller could hang for up to `request.timeout.ms` while we wait for the network I/O request to complete (or timeout).
> > @cadonna @lianetm @lucasbrudoes that make sense? CMIIW, please 
> 
> I think, we are talking about two separate things here:
> 
>     1. change `ConsumerUtils.getResult(commitFuture, requestTimer);` to `ConsumerUtils.getResult(commitFuture);`
> 
>     2. Do we need a timer for `awaitPendingAsyncCommitsAndExecuteCommitCallbacks(requestTimer, true);`
> 
> 
> As far as I understand, we agree on 1 but do not know if there is a better solution for 2. Is this correct?
> 
> Currently, I do not see a better way than using a timer on awaiting the execution of the async commit callback. As @kirktrue pointed out, since the async commit does basically not have a timeout, we cannot wait on the deadline of the `AsyncCommitEvent`. We can also not wait for the deadline of the `SyncCommitEvent` since if that event completes before the timeout, we would not wait enough for the completion of the async commit callback.

Thinking about it, I guess having a thread-safe timer and using it in the background thread and the application thread would be the cleanest solution. However, I do not think it is worth blocking this PR on that. "
2092616162,15640,lucasbru,2024-05-03T09:13:49Z,Is there any problem if we leave `awaitPendingAsyncCommitsAndExecuteCommitCallbacks` as is? It clearly needs the timer
2092808466,15640,cadonna,2024-05-03T11:22:05Z,"> Is there any problem if we leave `awaitPendingAsyncCommitsAndExecuteCommitCallbacks` as is? It clearly needs the timer

Yeah, I think that we should leave this as it is for now."
2115938603,15640,lianetm,2024-05-16T18:31:18Z,"High level comment, just to clarify and make sure it's something we are considering and will cover with the follow-up PRs for timeouts. Here we're introducing a component to ensures that app events are expired only after having one chance, but that's only at the app thread level, and not for all events, but only for unsubscribe, and poll. Thing is that events can also be expired indirectly when a request is expired (so playing against this changes). So even if the `processBackgroundEvents` introduced here gives an expired event a change to run one, that may actually not happen (because the underlying request expires). I expect that side needed to make this whole intention work in practice will be a follow-up PR, am I right?

Also note, almost none of our integration test cover the poll(ZERO) case (helper funcs [here](https://github.com/apache/kafka/blob/056d232f4e28bf8e67e00f8ed2c103fdb0f3b78e/core/src/test/scala/unit/kafka/utils/TestUtils.scala#L892-L910), used by most of the test, poll with timeout > 0). That's probably why we did not find the expiration issues we have with poll(0) before. I guess that after addressing the timeout/expiration (this PR and follow-up), we should be able to add some. "
2117012544,15640,cadonna,2024-05-17T08:23:24Z,"> High level comment, just to clarify and make sure it's something we are considering and will cover with the follow-up PRs for timeouts. Here we're introducing a component to ensures that app events are expired only after having one chance, but that's only at the app thread level, and not for all events, but only for unsubscribe, and poll. Thing is that events can also be expired indirectly when a request is expired (so playing against this changes). So even if the `processBackgroundEvents` introduced here gives an expired event a change to run one, that may actually not happen (because the underlying request expires). I expect that side needed to make this whole intention work in practice will be a follow-up PR, am I right?
> 
> Also note, almost none of our integration test cover the poll(ZERO) case (helper funcs [here](https://github.com/apache/kafka/blob/056d232f4e28bf8e67e00f8ed2c103fdb0f3b78e/core/src/test/scala/unit/kafka/utils/TestUtils.scala#L892-L910), used by most of the test, poll with timeout > 0). That's probably why we did not find the expiration issues we have with poll(0) before. I guess that after addressing the timeout/expiration (this PR and follow-up), we should be able to add some.

@lianetm Could you elaborate on this a bit. I cannot completely follow. I followed the `SyncEvent`. The event is processed in the `ApplicationEventHandler` and a request is added to the commit request manager. Then the commit request manager is polled, the requests are added to the network client and the the network client is polled. As far as I can see the `SyncEvent` is given a chance to complete.
I did also not understand the connection to `processBackgroundEvents`. That is for events that originate from the background thread. How does that influence if an application event was given a chance to complete?
What do you mean with the ""underlying requests expires""? Do you mean it exceeds the request timeout? Does the legacy consumer ensure that a request completed when timeout is set to `0`?
Sorry for all these questions, but I would like to understand.     "
2117763853,15640,lianetm,2024-05-17T14:43:23Z,"Hey @cadonna, the tricky bit is that, for some events, the request managers do expire requests too, so in this flow you described:

> The event is processed in the ApplicationEventHandler and a request is added to the commit request manager. **Then the commit request manager is polled**, the requests are added to the network client and the the network client is polled

When the manager is polled, if the event had timeout 0, it will be expired/cancelled before making it to the network thread. Currently we have 2 managers that do this (that I can remember): [TopicMetadataManager](https://github.com/apache/kafka/blob/f9db4fa19cce975a6bbaeb09fbe9c91b81846b5a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/TopicMetadataRequestManager.java#L86) and [CommitRequestManager](https://github.com/apache/kafka/blob/f9db4fa19cce975a6bbaeb09fbe9c91b81846b5a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java#L1168). So for those events, even with this PR, if they have timeout 0, they won't have a chance to complete.

My point is not to bring more changes into this PR, only to have the whole situation in mind so we can address it properly (with multiple PRs). This other [PR](https://github.com/apache/kafka/pull/15844) attempts to address this situation I described, but only in the `CommitRequestManager` for instance. We still have to align on the approach there, and also handle it in the `TopicMetadataManager` I would say. I would expect that a combination of this PR and those others would allow us to get to a better point (now, even with this PR, we cannot make basic progress with a consumer being continuously polled with timeout 0 because `FetchCommittedOffsets` is always expired by the manager, for instance). I can easily repro it with the following integration test + poll(ZERO) (that I was surprised we have not covered, because TestUtils always polls with a non-zero timeout)


```
  // Ensure TestUtils polls with ZERO. This fails for the new consumer only.
  @ParameterizedTest(name = TestInfoUtils.TestWithParameterizedQuorumAndGroupProtocolNames)
  @MethodSource(Array(""getTestQuorumAndGroupProtocolParametersAll""))
  def testPollEventuallyReturnsRecordsWithZeroTimeout(quorum: String, groupProtocol: String): Unit = {
    val numMessages = 100
    val producer = createProducer()
    sendRecords(producer, numMessages, tp)

    val consumer = createConsumer()
    consumer.subscribe(Set(topic).asJava)
    val records = awaitNonEmptyRecords(consumer, tp)
    assertEquals(numMessages, records.count())
  }
```


Makes sense?
"
2118578191,15640,kirktrue,2024-05-18T01:56:19Z,"> Hey @cadonna, the tricky bit is that, for some events, the request managers do expire requests too, so in this flow you described:
> 
> > The event is processed in the ApplicationEventHandler and a request is added to the commit request manager. **Then the commit request manager is polled**, the requests are added to the network client and the the network client is polled
> 
> When the manager is polled, if the event had timeout 0, it will be expired/cancelled before making it to the network thread. Currently we have 2 managers that do this (that I can remember): [TopicMetadataManager](https://github.com/apache/kafka/blob/f9db4fa19cce975a6bbaeb09fbe9c91b81846b5a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/TopicMetadataRequestManager.java#L86) and [CommitRequestManager](https://github.com/apache/kafka/blob/f9db4fa19cce975a6bbaeb09fbe9c91b81846b5a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java#L1168). So for those events, even with this PR, if they have timeout 0, they won't have a chance to complete.
> 
> My point is not to bring more changes into this PR, only to have the whole situation in mind so we can address it properly (with multiple PRs). This other [PR](https://github.com/apache/kafka/pull/15844) attempts to address this situation I described, but only in the `CommitRequestManager` for instance. We still have to align on the approach there, and also handle it in the `TopicMetadataManager` I would say. I would expect that a combination of this PR and those others would allow us to get to a better point (now, even with this PR, we cannot make basic progress with a consumer being continuously polled with timeout 0 because `FetchCommittedOffsets` is always expired by the manager, for instance). I can easily repro it with the following integration test + poll(ZERO) (that I was surprised we have not covered, because TestUtils always polls with a non-zero timeout)
> 
> ```
>   // Ensure TestUtils polls with ZERO. This fails for the new consumer only.
>   @ParameterizedTest(name = TestInfoUtils.TestWithParameterizedQuorumAndGroupProtocolNames)
>   @MethodSource(Array(""getTestQuorumAndGroupProtocolParametersAll""))
>   def testPollEventuallyReturnsRecordsWithZeroTimeout(quorum: String, groupProtocol: String): Unit = {
>     val numMessages = 100
>     val producer = createProducer()
>     sendRecords(producer, numMessages, tp)
> 
>     val consumer = createConsumer()
>     consumer.subscribe(Set(topic).asJava)
>     val records = awaitNonEmptyRecords(consumer, tp)
>     assertEquals(numMessages, records.count())
>   }
> ```
> 
> Makes sense?

Yes, the network layer changes are captured in KAFKA-16200 and build on top of this PR."
2118579935,15640,kirktrue,2024-05-18T01:57:33Z,@lianetm @cadonnaI believe I have addressed all the actionable feedback. Are there additional concerns about this PR that prevent it from being merged? Thanks.
2122575967,15640,cadonna,2024-05-21T12:56:15Z,"> Hey @cadonna, the tricky bit is that, for some events, the request managers do expire requests too, so in this flow you described:
> 
> > The event is processed in the ApplicationEventHandler and a request is added to the commit request manager. **Then the commit request manager is polled**, the requests are added to the network client and the the network client is polled
> 
> When the manager is polled, if the event had timeout 0, it will be expired/cancelled before making it to the network thread. Currently we have 2 managers that do this (that I can remember): [TopicMetadataManager](https://github.com/apache/kafka/blob/f9db4fa19cce975a6bbaeb09fbe9c91b81846b5a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/TopicMetadataRequestManager.java#L86) and [CommitRequestManager](https://github.com/apache/kafka/blob/f9db4fa19cce975a6bbaeb09fbe9c91b81846b5a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/CommitRequestManager.java#L1168). So for those events, even with this PR, if they have timeout 0, they won't have a chance to complete.
> 
> My point is not to bring more changes into this PR, only to have the whole situation in mind so we can address it properly (with multiple PRs). This other [PR](https://github.com/apache/kafka/pull/15844) attempts to address this situation I described, but only in the `CommitRequestManager` for instance. We still have to align on the approach there, and also handle it in the `TopicMetadataManager` I would say. I would expect that a combination of this PR and those others would allow us to get to a better point (now, even with this PR, we cannot make basic progress with a consumer being continuously polled with timeout 0 because `FetchCommittedOffsets` is always expired by the manager, for instance). I can easily repro it with the following integration test + poll(ZERO) (that I was surprised we have not covered, because TestUtils always polls with a non-zero timeout)
> 
> ```
>   // Ensure TestUtils polls with ZERO. This fails for the new consumer only.
>   @ParameterizedTest(name = TestInfoUtils.TestWithParameterizedQuorumAndGroupProtocolNames)
>   @MethodSource(Array(""getTestQuorumAndGroupProtocolParametersAll""))
>   def testPollEventuallyReturnsRecordsWithZeroTimeout(quorum: String, groupProtocol: String): Unit = {
>     val numMessages = 100
>     val producer = createProducer()
>     sendRecords(producer, numMessages, tp)
> 
>     val consumer = createConsumer()
>     consumer.subscribe(Set(topic).asJava)
>     val records = awaitNonEmptyRecords(consumer, tp)
>     assertEquals(numMessages, records.count())
>   }
> ```
> 
> Makes sense?

@lianetm Thanks for the explanation!"
2123163240,15640,kirktrue,2024-05-21T18:04:40Z,@lianetm @cadonnaThe latest batch of feedback has been addressed. Thanks!
2125302873,15640,kirktrue,2024-05-22T16:52:58Z,"I added KAFKA-16818 to cover the cases to refactor/migrate/remove tests.

Thanks @cadonna & @lianetm for your reviews!"
