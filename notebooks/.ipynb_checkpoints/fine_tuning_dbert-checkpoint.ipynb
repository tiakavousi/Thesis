{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-08 12:16:48.183425: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import torch\n",
    "from transformers import DistilBertForSequenceClassification, DistilBertTokenizerFast\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Add the project root to path to enable imports\n",
    "if not '..' in sys.path:\n",
    "    sys.path.append('..')  # Adjust if needed to point to your project root\n",
    "\n",
    "# Import your modules\n",
    "from src.config import CONFIG\n",
    "from src.models.distilbert.distilbert_finetuner import DistilBERTFineTuner\n",
    "from src.data_pipeline.data_loader import DataModule\n",
    "from src.training.model_trainer import ModelTrainer\n",
    "from src.training.model_evaluator import ModelEvaluator\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current DistilBERT Configuration:\n",
      "  batch_size: 16\n",
      "  max_length: 128\n",
      "  learning_rate: 2e-05\n",
      "  epochs: 3\n",
      "  early_stopping_patience: 2\n",
      "  device: cpu\n",
      "  pretrained_model_name: distilbert-base-uncased\n",
      "  num_labels: 2\n"
     ]
    }
   ],
   "source": [
    "# Access the DistilBERT configuration\n",
    "distilbert_config = CONFIG[\"models\"][\"distilbert\"]\n",
    "\n",
    "# Merge with common training parameters\n",
    "distilbert_config = {**CONFIG[\"training\"], **CONFIG[\"models\"][\"distilbert\"]}\n",
    "\n",
    "print(\"Current DistilBERT Configuration:\")\n",
    "for key, value in distilbert_config.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'dataset_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load the dataset and explore its structure\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m distilbert_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_path\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(dataset_path)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'dataset_path'"
     ]
    }
   ],
   "source": [
    "# Load the dataset and explore its structure\n",
    "dataset_path = CONFIG[\"dataset\"][\"balanced_dataset_path\"]\n",
    "df = pd.read_csv(dataset_path)\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(\"\\nColumns:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  {col}\")\n",
    "\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_counts = df[\"is_toxic\"].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    print(f\"  Label {label}: {count} samples ({count/len(df)*100:.2f}%)\")\n",
    "\n",
    "# Display a few examples\n",
    "print(\"\\nSample entries:\")\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the fine-tuner\n",
    "fine_tuner = DistilBERTFineTuner(distilbert_config)\n",
    "\n",
    "# Run the fine-tuning process\n",
    "fine_tuner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load the saved model for testing specific examples\n",
    "model_path = distilbert_config[\"model_save_path\"]\n",
    "model = DistilBertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "# Function to predict sentiment of individual text\n",
    "def predict_sentiment(text, model=model, tokenizer=tokenizer):\n",
    "    device = distilbert_config[\"device\"]\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        predictions = torch.softmax(outputs.logits, dim=1)\n",
    "        \n",
    "    toxic_prob = predictions[0][1].item()\n",
    "    label = \"Toxic\" if toxic_prob > 0.5 else \"Non-toxic\"\n",
    "    \n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"label\": label,\n",
    "        \"toxic_probability\": toxic_prob,\n",
    "        \"non_toxic_probability\": predictions[0][0].item()\n",
    "    }\n",
    "\n",
    "# Test with some examples\n",
    "test_texts = [\n",
    "    \"This code looks good, well structured!\",\n",
    "    \"This is the worst implementation I've ever seen. Are you stupid?\",\n",
    "    \"I think we should refactor this part for better performance\",\n",
    "    \"Why would you even try to submit this garbage code?\"\n",
    "]\n",
    "\n",
    "for text in test_texts:\n",
    "    result = predict_sentiment(text)\n",
    "    print(f\"Text: {result['text']}\")\n",
    "    print(f\"Prediction: {result['label']} (Toxic prob: {result['toxic_probability']:.4f})\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Load evaluation results (assuming they were saved to a file)\n",
    "# Alternative: you can extract these metrics from the fine_tuner.evaluate_model() directly\n",
    "\n",
    "# Visualize metrics\n",
    "test_metrics = {\n",
    "    \"accuracy\": 0.92,  # Replace with actual metrics\n",
    "    \"precision\": 0.89,\n",
    "    \"recall\": 0.91,\n",
    "    \"f1\": 0.90\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(test_metrics.keys(), test_metrics.values(), color='royalblue')\n",
    "plt.ylim(0, 1.0)\n",
    "plt.xlabel('Metrics')\n",
    "plt.ylabel('Score')\n",
    "plt.title('DistilBERT Model Performance')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Once you've trained all three models, you can compare their results\n",
    "models_metrics = {\n",
    "    'DistilBERT': {'accuracy': 0.92, 'precision': 0.89, 'recall': 0.91, 'f1': 0.90},\n",
    "    'CodeBERT': {'accuracy': 0.94, 'precision': 0.92, 'recall': 0.93, 'f1': 0.92},\n",
    "    'DeBERTa': {'accuracy': 0.95, 'precision': 0.94, 'recall': 0.93, 'f1': 0.93}\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(models_metrics).T\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
