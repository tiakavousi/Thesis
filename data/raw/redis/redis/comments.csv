id,pr_number,user,created_at,body
1374636485,11695,sjpotter,2023-01-07T22:30:53Z,"> getRandomKey - now needs to not only select a random key, from the random bucket, but also needs to select a random dictionary. In my implementation this uses shuffling algorithm that randomly selects any non-empty owned dict with the same probability and then applies existing logic for random key selection there.

Note, this isn't making keys evenly distributed, as you note, different buckets can have different amount of keys.  This might not be a big deal, but should be noted."
1377703989,11695,vitarb,2023-01-10T18:51:51Z,"@zuiderkwast @sjpotter thanks for looking at this and providing feedback. As it stands couple areas for improvement are:
* Making random more fair.
* Caching/pushing through slot id so we don't need to calculate hashes again.

Another small debt on my side might be writing some tests for SCAN in cluster mode, to ensure that combined cursor+slot logic works properly (I've manually tested it so far and was relying on few tests that already exist, I haven't checked if this case is covered by existing tests).

I'll try to address these issues, meanwhile let's continue discussion in the comments and see if any other area needs to be improved as well."
1378046083,11695,vitarb,2023-01-10T23:49:35Z,"@zuiderkwast I've tested your approach with caching hashes for the last key, and at first couldn't believe the numbers, read performance doubled, write performance increased 20-30%! But then I've realized that it's unfortunately broken, the problem seems to be that addresses are getting reused and we can't be sure that pointers being the same mean actually keys being the same.

![Screenshot from 2023-01-10 15-47-34](https://user-images.githubusercontent.com/1311694/211685978-64b4aac3-3b10-4e81-af0e-6b5476239128.png)

On this screenshot you can see lastkey is pointing to the same place as the key, but hash is actually different from the one calculated before. (I've added extra call to _keyHashSlot just to catch this issue more easily)

In fact jemalloc seem to be reusing same address over and over again for different keys.
So we'll probably need to use a different approach for this.
"
1379322469,11695,vitarb,2023-01-11T18:37:16Z,"@molian6 
> +1 that 48 bits should be good enough, the public doc suggested max number of keys is 2^32 (though I didn't find this hard limit > in the code, I guess it is the limit in a 32 bit system)
> I'm curious where the other 2 bits goes. We should have 64 bits in the unsigned long long, if cursor takes 48 bits and slot takes > 14 bits, that adds up to 62 bits.

If we do what @zuiderkwast suggested and move slot id to LSB then cursor can occupy remaining 50 bits. My reasoning for using 48 was to leave sign bit alone to make sure that numbers look the same on the client, even if signed long is used there, also left 1 extra bit for parity and potential future improvements."
1379410975,11695,molian6,2023-01-11T19:59:01Z,"> @molian6
> 
> > +1 that 48 bits should be good enough, the public doc suggested max number of keys is 2^32 (though I didn't find this hard limit > in the code, I guess it is the limit in a 32 bit system)
> > I'm curious where the other 2 bits goes. We should have 64 bits in the unsigned long long, if cursor takes 48 bits and slot takes > 14 bits, that adds up to 62 bits.
> 
> If we do what @zuiderkwast suggested and move slot id to LSB then cursor can occupy remaining 50 bits. My reasoning for using 48 was to leave sign bit alone to make sure that numbers look the same on the client, even if signed long is used there, also left 1 extra bit for parity and potential future improvements.

@vitarb  Got you. I do like the idea of LSB because in reality cursor will not take 48 or 50 bits, we would have more MSB left for other potential uses in future. It gave use more flexibilities."
1379665652,11695,madolson,2023-01-12T00:46:05Z,"> Yes I think we need to do this, at least for checking if it affects performance. A simple way could be to cache the last computed hash inside hashKeySlot itself:

We had some conversation in another thread about a key handle that would cache the crc and the hash value after the initial lookup. Re-using this handle would remove the extra computation. I think this is still the most promising way forward."
1381397812,11695,vitarb,2023-01-13T07:04:13Z,"**Updated PR with following changes:**
* Now using LSB for slot in the SCAN cursor.
* Caching current dictionary in the redis DB to avoid CRC hash calculation.
* Rebased on top of `unstable` and resolved some conflicts.

**Benchmarks**
I ran some benchmarks on gets and sets the result is that this implementation and `unstable` branch look almost identical (sets are on the left, gets are on the right, branch is mentioned in the terminal prompt):
![Screenshot from 2023-01-12 22-53-03](https://user-images.githubusercontent.com/1311694/212257332-c443d151-840a-4ca1-8ecc-15ca4dfb81ae.png)

**Commands that were used:**
Sets: `src/redis-benchmark -r 1000000000 -t set -n 100000000 -P 10`
Gets: `src/redis-benchmark -r 1000000000 -t get -n 10000000 -P 10`

Next I'm going to run more benchmarks, for other potentially affected commands as well, and will make some flamegraphs, but this looks promising."
1406127392,11695,vitarb,2023-01-27T07:38:08Z,"Folks, I think I've addressed all feedback, the last bit remaining is performance. I'm going to run a set of benchmarks and try to optimize any rough edges. Please take another look and let me know if there are any more suggestions or concerns."
1413240475,11695,vitarb,2023-02-02T06:58:32Z,"As promised before, I've done some performance analysis.

**Methodology**
I've tested GETs/SETs with cluster mode yes/no, using request pipelining to make any dictionary related issues more distinctive, 50M keys and 100 bytes of data per entry using `redis-benchmark`.  (see exact commands on screenshots below). Tests were ran on `c5.9xlarge x86_64 Linux` cloud machine.


**Here is a summary of results.**

**Cluster mode enabled**
* SETs are **~7-8% faster** on this branch.
* GETs are **~1% faster** on this branch.

Benchmark comparison in cluster mode:
![image](https://user-images.githubusercontent.com/1311694/216244782-afa2f087-af47-4a1a-b545-d6e62215d704.png)

Here are flamegraphs for SETs in cluster mode:
this branch
![dss set cluster yes](https://user-images.githubusercontent.com/1311694/216245637-6abf89b1-f851-4178-ac6d-c0769931d0c7.svg)
unstable
![unstable set cluster yes](https://user-images.githubusercontent.com/1311694/216245639-287dd64f-f39d-44d6-8131-b1ac08a9ba9e.svg)

Looking at the flamegraph, we can see that primary reason for SETs being faster is because `slotToKeyAddEntry` has been eliminated  which was eating ~5% of performance.

GETs:
this branch
![dss get cluster yes](https://user-images.githubusercontent.com/1311694/216245840-8dcc7bf6-6937-4742-8ae7-9277959b5297.svg)
unstable
![unstable get cluster yes](https://user-images.githubusercontent.com/1311694/216245842-12722963-7a5e-4a2b-a0b4-0dea6b291792.svg)

One curious thing to note here is that `dictSdsKeyCompare` reduced by almost 8%, but other code paths, including `dictFind` gained slightly. Need to look a bit more into this, maybe there is some potential to improve it even further.

**In cluster mode disabled**
* SETs have about same performance.
* GETs are **~1-2% slower** on this branch.

![Screenshot from 2023-02-01 22-44-06](https://user-images.githubusercontent.com/1311694/216251525-d54b24a2-7462-4d68-b0b7-837b4c914e5f.png)

Looking at the flamegraphs I couldn't pinpoint the reason for slowness on GET.
this branch
![dss get cluster no](https://user-images.githubusercontent.com/1311694/216251990-d414661c-52d6-4a3c-9bba-c9597add27f9.svg)
unstable
![unstable get cluster no](https://user-images.githubusercontent.com/1311694/216251997-68767dfb-b10f-4eef-bef9-fe143ee07d62.svg)

I'm going to investigate this added slowness on GET when cluster is mode disabled, meanwhile let me know if you have any thoughts or recommendations on testing process or methodology.
"
1413456481,11695,zuiderkwast,2023-02-02T09:58:58Z,@filipecosta90 might have an idea about performance. Do any of the benchmarks on grafana run in cluster mode?
1415549997,11695,oranagra,2023-02-03T10:00:00Z,"Hi, i quickly skimmed though the discussion (not the code, or code-review comments).
i'm quite concerned about this change, here's a random set of concerns:
1. as you said, we have an impact on dbsize, which used to be very fast and maybe callers abuse it. let's see a benchmark and consider optimizing it.
2. the effects on incremental rehashing, random and probably other things we didn't realize yet need some thinking and may take time till we can fully trust this.
3. i would like to learn more about the possible negative impacts of this on non-clustered deployments. the first thing that comes to mind is memory usage, specifically on small databases.
4. i didn't yet look at the code and the attempt to avoid re-hashing or re-finding the slot (if we have a cache that cleared after each command), i worry that maybe you made some optimizations to reduce the negative impacts of this PR on some cases, but can't use these tricks for other cases and they're left unoptimized and the PR has a negative impact. e.g. multi-slot scripts, MGET, SUNIONSTORE, etc.

i'd note that in both 6.2 and 7.2 we added some severe performance regression for certain (even popular) workloads without realizing it, and it took some time until the community reported these regressions, so this drastic change worries me and i would want to seek more confidence."
1415770526,11695,zuiderkwast,2023-02-03T11:59:47Z,"Some thoughts on points 3-4:

3: I don't think there's any memory impact in standalone mode. There's only one dict, not 16K as in cluster mode.

4: MGET and SUNIONSTORE raise -CROSSSLOT if keys are of different slots, so that shoudn't be an issue. Multi-slot scripts might be a problem though, if they're supported in cluster mode. I'm not sure what we allow in this area."
1415780546,11695,oranagra,2023-02-03T12:04:51Z,"Ohh. I didn't know that in non cluster mode there's one dictionary (didn't look at the code yet).
So what impact does it have on non cluster mode, or any other negative impact on cluster mode?"
1415790426,11695,zuiderkwast,2023-02-03T12:12:47Z,"For performance impact in cluster and non-cluster mode, see vitarb's comment above, the one with the flame graphs. (Theoretically, there shouldn't be any impact on non-cluster. I don't know where it comes from.)

For cluster mode, there is a large fixed memory impact (16K dicts), but a 16B/key save and improved latency."
1416678563,11695,vitarb,2023-02-04T06:39:18Z,"@oranagra, thanks for your feedback. Let me try to address some of your concerns.

>    as you said, we have an impact on dbsize, which used to be very fast and maybe callers abuse it. let's see a benchmark and consider optimizing it.

We should be able to get back to O(1) by simply adding a key counter at the redisDb level.

>    the effects on incremental rehashing, random and probably other things we didn't realize yet need some thinking and may take time till we can fully trust this.

Incremental rehashing should not be any less efficient than it was before.

>    i would like to learn more about the possible negative impacts of this on non-clustered deployments. the first thing that comes to mind is memory usage, specifically on small databases.

We are not adding any additional memory usage per entry, as well as we won't be allocating extra dictionaries if cluster mode is disabled. Can you please elaborate what exactly concerns you?

>    i didn't yet look at the code and the attempt to avoid re-hashing or re-finding the slot (if we have a cache that cleared after each command), i worry that maybe you made some optimizations to reduce the negative impacts of this PR on some cases, but can't use these tricks for other cases and they're left unoptimized and the PR has a negative impact. e.g. multi-slot scripts, MGET, SUNIONSTORE, etc.

It is true that slot caching optimization works only for single slot access (e.g. GET/SET/etc), multislot scripts would take a small hit requiring key hash calculation on DB access. I will measure the impact to make it more clear if it is concerning or not, but I don't expect significant performance degradation there."
1423659044,11695,vitarb,2023-02-09T05:34:30Z,"Updated PR with following changes:
* Slot ID is now used from the current client instead of a new field in `redisDb`
* DBSIZE operation is O(1), instead of O(slot count), thanks to a new `key_count` field in the `redisDb`
* Used 64 bit random instead of 32 bit version to cleaner support key spaces larger than 2^32.

Also I've done another round of performance testing, and I don't see any regressions in cluster mode disabled anymore."
1430070543,11695,madolson,2023-02-14T16:56:23Z,"We reviewed the high level of the PR today with the core team. Consensus was that it's a complex change, but we are still inclined to push forward for 7.2. It needs to be in RC1 since it will require the most time to bake before GA."
1449688772,11695,zuiderkwast,2023-03-01T09:31:22Z,@vitarb Reviewers usually don't like force-pushes. It makes it harder to see what's new since last review. It's better to use merge commits. When the PR is ready it will be squash-merged anyways.
1459641699,11695,oranagra,2023-03-08T07:08:09Z,"@vitarb please avoid force-pushes, it makes it harder to keep track of what was reviewed and what wasn't.
instead, since this is gonna be squash-merged, if you wanna rebase from unstable, just use a plain merge."
1461102753,11695,vitarb,2023-03-09T00:59:12Z,"Just ran a quick test, filled 1GB database with keys using benchmark (standard small values), was able to fit 22% more keys on this branch (12998272 keys) comparing to the unstable (10644416 keys). Entry size (entry+key+value) went down from 88 to 72 bytes (in line with expected 16 bytes reduction per entry). Substantial improvement for small key/value pairs."
1467404704,11695,vitarb,2023-03-14T05:47:24Z,"Here is some data that visualizes LRU eviction on this branch for random keys:
![Screenshot from 2023-03-13 22-43-53](https://user-images.githubusercontent.com/1311694/224907356-c7280adb-bfa0-49f9-b037-301906308860.png)

Versus unstable:
![Screenshot from 2023-03-13 22-44-04](https://user-images.githubusercontent.com/1311694/224907372-3aff860d-987c-401c-8e14-1900a9063f43.png)

I don't see any quality degradation based on the above and other tests that I ran."
1476349204,11695,madolson,2023-03-20T14:35:59Z,@vitarb Where your workloads perfectly uniform? 
1476411517,11695,vitarb,2023-03-20T15:10:51Z,"> @vitarb Where your workloads perfectly uniform?

Random keys, so not perfectly uniform, but not artificially skewed either."
1478718305,11695,vitarb,2023-03-21T23:19:38Z,"As per recent feedback, I re-ran benchmarks, and perf results are looking good (even better than during my original run).
Cluster mode disabled:
![Screenshot from 2023-03-21 15-00-12](https://user-images.githubusercontent.com/1311694/226761618-8c7b2fa1-e2c0-4f4a-85e4-6c1d1583f1f7.png)
![Screenshot from 2023-03-21 15-00-04](https://user-images.githubusercontent.com/1311694/226761623-8ad83d85-36e2-4a26-82bb-f8ea0fd87b16.png)
Cluster mode enabled:
![Screenshot from 2023-03-21 14-52-22](https://user-images.githubusercontent.com/1311694/226761626-af76e3f0-f3ca-4182-9bac-63a0c26b8913.png)
![Screenshot from 2023-03-21 14-59-50](https://user-images.githubusercontent.com/1311694/226761624-e33c95a0-05ec-4d03-93f0-0f084a5d82b6.png)

Also I've engineered an absolute worst case scenario for the allkeys LRU, when entire DB is filled with keys that are in the single slot, and then more keys are added into random slots. As expected, LRU has poor performance in this case, evicting most keys from the recent batch, because random slot selection is not weighted by the slot size and is as likely to return a slot with 1 key as it is to return a slot with 1k keys.

![Screenshot from 2023-03-21 16-04-59](https://user-images.githubusercontent.com/1311694/226762413-ad6c93b5-5680-48e1-87be-714915b930cd.png)

I see a couple of ideas for how to improve fairness for large slots, let me explore them and I'll post an update."
1480454348,11695,vitarb,2023-03-23T01:24:38Z,"Good news, folks, I came up with an implementation that provides fair random and does it fast enough.
Basic idea is quite simple, we randomly select a target element from 1 to DBSIZE and find a slot that contains it (in a list of linearly ordered slots), main caveat is that unlike my first naive implementation that used to iterate through all slots in the dictionary (which was extremely slow), it uses specialized data structure called Binary Index Tree (also known as Fenwick tree, best explanation of this data structure can be found [here](https://www.topcoder.com/thrive/articles/Binary%20Indexed%20Trees)), that gives us a way to query cumulative number of keys in a range of slots (e.g. from 0 to N) in O(log(N)) time (in our case O(log(CLUSTER_SLOTS)).
With this capability, we can do a simple binary search and find which slot contains target-th element. Total complexity of this algorithm is O(log^2(CLUSTER_SLOTS)). I ran a brief performance benchmark and numbers are looking good.
This fixes LRU fairness problem that we had before when slot sizes were uneven, here is how eviction looks now for the same scenario that degraded LRU performance of the previous implementation (packing one slot to `maxmemory` and then writing into random slots with `allkeys-lru` policy).
![Screenshot from 2023-03-22 18-26-19](https://user-images.githubusercontent.com/1311694/227075319-15c07354-10a8-4613-be18-2209fdff9cc0.png)

In addition to fast and fair LRU, this binary indexed tree should also allow us to remove `non_empty_dicts` intset field from the `redisDb` and implement quick iteration that would allow us to skip empty dictionaries (to find next slot, all you need is to find total number of keys up until current slot, and then find the slot where next key is, which can be done with exact same binary search as `getFairRandomDict` uses in log time).

"
1487683828,11695,vitarb,2023-03-28T22:27:53Z,"@oranagra @madolson with random fairness issue out of the way, I think this change is ready to go, is there anything else you want me to address before we get final approvals and merge it in?"
1495089198,11695,madolson,2023-04-03T22:55:29Z,@yossigo Pinging you to still take a look if you can.
1498548988,11695,vitarb,2023-04-06T06:16:37Z,@madolson @oranagra all your previous comments should be addressed at this point.
1501063389,11695,yossigo,2023-04-09T07:36:52Z,"@madolson @vitarb
LGTM, this is a significant improvement overall. I still think this change should be parked in unstable for some time before being released.

One minor comment about SCAN API: so far, the cursor was considered opaque (with an exception of zero value), and I think we should officially keep it so."
1503565310,11695,madolson,2023-04-11T15:07:53Z,"Decision was to move this to merge into Redis 8, so we will plan to merge this in right after we have stabilized the Redis 7.2 release."
1550187420,11695,madolson,2023-05-16T18:47:36Z,"In the previous core group we also decided that we wanted to see if we could make the implementation cleaner, and I spent some time reviewing it again and didn't come up with much. @yossigo Pinging you again to see if there is anything specifically you wish was better integrated."
1643311555,11695,vitarb,2023-07-20T06:16:15Z,Merged latest changes from unstable.
1708675329,11695,hpatro,2023-09-06T16:05:46Z,"> thanks. please go over the comments that you resolved and mark them as such, so it'll be easy to keep track of what's left.

I thought the reviewer resolves it if they find the changes fine. Anyway it's not allowing me to mark the comments as resolved any longer. I've marked with a thumbs up for the one's I've addressed. "
1709544673,11695,oranagra,2023-09-07T06:23:44Z,"i don't think it matters who resolves them as long as they're resolved when each one is addressed or the discussion ended (no further arguments).
i have a limited attention span, so since you're going over them one by one, i'd rather you keep them organized, but anyway if you can't then i just went over them and resolved the ones you marked."
1741712813,11695,hpatro,2023-09-30T08:20:27Z,This took longer than expected. @madolson / @oranagra If you could take a look again that would be great. Thanks 🙏 
1741713587,11695,hpatro,2023-09-30T08:24:57Z,"@filipecosta90 Would we be able to run the suite of benchmark on this change. That would be really helpful. 

@roshkhatri/I will also be posting some of the benchmark results we've ran early next week. "
1745496137,11695,madolson,2023-10-03T18:21:57Z,"> there are still some that aren't, and it's hard to go over everything each time.

I tried to find all of the major pending items and put them in a section at the top. (Or at least those without a very recent comment)"
1745603500,11695,hpatro,2023-10-03T19:37:21Z,"I'm unable to modify the top comment as I'm neither the original author nor maintainer. 

Thanks @madolson for summarizing it above. 

* Renamed the `dictSlots`/`dbSlots` to `dictBuckets`/`dbBuckets`.
* Introduced defragCtx to pass on db/slot to defragKey and avoid recompute of slot info from key.
* Remove unnecessary bitwise AND operation while retrieving cursor.
* Updated test for data verification after cross slot write operation from lua scripts/functions.

[Madelyn's list of pendings items copied from the top level comment]

- [x] : For scripts and module commands, we need to be invalidating the client sends multiple sub-commands across multiple slots that are local to the node. The original script code was just designed for tracking invalid uses, and is not always set correctly. Modules has no such tracking. Both modules and scripts can access cross slots within their execution. (Ref: https://github.com/redis/redis/pull/11695/files#r1294236533) We are missing tests here too. 
es#r1226137086 Seems to have been forgotten.
- [x] : Defrag code generally seems unoptimized for slot tracking. (Ref https://github.com/redis/redis/pull/11695#discussion_r1320167467) Is this something we think we need to optimize. (Assuming yes) - @hpatro addressed this particular feedback however the time spent is still higher compared to standalone setup.
- [x] : Defrag later step could avoid slot recomputation if done serially. (Ref https://github.com/redis/redis/pull/11695#discussion_r1101221987)
- [x] : Fix the naming of dictSlots to be dictBuckets (Ref https://github.com/redis/redis/pull/11695#discussion_r1338191872). Either should be a separate commit or a followup PR.
- [x] : Cross slot operations from scripts (ref https://github.com/redis/redis/pull/11695#discussion_r1294236533): Updated some test for validating data integrity with cross slot operations via scripts/functions in `tests/unit/cluster/scripting.tcl`."
1745872485,11695,hpatro,2023-10-03T23:16:55Z,">  : Performance optimization for the index tree to find fair item in O(Log(N)) instead of O(Log^2(N)) (Followup seems okay here?)

@madolson I didn't follow this. Is it a possibility we've discussed somewhere or in general we want to improve the perf ?"
1746137883,11695,madolson,2023-10-04T04:58:52Z,"> I'm unable to modify the top comment as I'm neither the original author nor maintainer.

Right, I forgot, let me move the checklist down to your comment."
1746152545,11695,madolson,2023-10-04T05:17:20Z,"> @madolson I didn't follow this. Is it a possibility we've discussed somewhere or in general we want to improve the perf ?

It's somewhere in this thread. I think the idea was that instead of repeatedly calling `cumulativeKeyCountRead` to do a binary search, we can instead do an efficient traversal by starting at the root and doing the following:
1. Scan the children from largest to smallest until we've found a child node that is smaller than the target.
2. Subtract the value of that child node from the target, and return to 1) but operating on that found child node.

The result should give you the same target slot as the current implementation and in Log(N) time. "
1749300162,11695,roshkhatri,2023-10-05T16:48:30Z,"I have done some redis benchmark tests and performance analysis. Here are the results:

**Methodology**
Methodology is similar to what it was done before [here](https://github.com/redis/redis/pull/11695#issuecomment-1413240475): GETs/SETs with cluster mode yes/no, additionally also for SETs under the load of evictions , 50M keys and 100 bytes of data per entry using redis-benchmark, exact commands on screenshots below. Tests were ran on `c5.12xlarge` x86_64 Linux cloud machine. For testing under the load of evictions the `maxmemory` was set to `100mb`.

CME - cluster mode enabled 
CMD - cluster mode disabled (standalone)

**Results.**

**Cluster mode enabled**:
- SETs are ~5% faster on this branch.
- GETs are ~4-5% faster on this branch.

![CME-comparision](https://github.com/redis/redis/assets/117414976/44a6aeb5-aa8b-4b77-b7d9-ae347ead9f50)

Flamegraphs for SETs in cluster mode:
this branch:
![set-graph](https://github.com/redis/redis/assets/117414976/e90331f4-e9d6-42ee-8b3c-bfd28586a176)

unstable:
![set-graph](https://github.com/redis/redis/assets/117414976/609d9bf2-1c29-4449-8cf4-990ccec1ff72)

Flamegraphs for GETs in cluster mode:
this branch:
![get-graph](https://github.com/redis/redis/assets/117414976/369eb226-6672-494e-9ac1-841370543235)

unstable:
![get-graph](https://github.com/redis/redis/assets/117414976/7288b3a4-6ebb-40a5-94e8-0706a937b916)

**Cluster mode enabled - with eviction**:
- SETs are ~2-3% faster on this branch.

![CME-with-evictions](https://github.com/redis/redis/assets/117414976/34b19aaa-7302-4ff0-9f5e-868553deb779)

Flamegraphs for SETs in cluster mode - with eviction:
this branch:
![cme-pr-set-w-evictions-graph](https://github.com/redis/redis/assets/117414976/98bb49d1-a093-4eb4-a1e7-31ddfcc160a6)

unstable:
![cme-unstable-set-w-evictions-graph](https://github.com/redis/redis/assets/117414976/82ba9a25-3571-459b-a477-66e988422158)

**Cluster mode disabled**:
- SETs are ~1-2% faster on this branch.
- GETs have a very similar performance ~0.5% faster on this branch.

![CMD-comparision](https://github.com/redis/redis/assets/117414976/eca4dd0f-c346-4eab-8b6b-c5421538102c)

Flamegraphs for SETs in cluster disabled:
this branch:
![set-graph](https://github.com/redis/redis/assets/117414976/361902d3-5633-4155-a4bd-c8f27a0642d3)

unstable:
![set-graph](https://github.com/redis/redis/assets/117414976/7d7889df-40d7-4e12-b4ec-0d9c47d26117)

Flamegraphs for GETs in cluster disabled:
this branch:
![get-graph](https://github.com/redis/redis/assets/117414976/7dbd77e0-e0a1-4d6a-bab2-601a6c8b923c)

unstable:
![get-graph](https://github.com/redis/redis/assets/117414976/979f4d28-7bd0-40c7-80fa-3a407509fc40)

**Cluster mode disabled - with eviction**:
- SETs are similar on this branch.

![addnl-eviction-cmd-runs](https://github.com/redis/redis/assets/117414976/7aa66a7b-0ffe-4507-9db4-c9a70c0c1710)

Flamegraphs for SETs in cluster disabled - with eviction:
this branch:
![cmd-pr-set-w-evictions-graph](https://github.com/redis/redis/assets/117414976/a9544fc6-d696-43d2-941d-7e2da1e91974)

unstable:
![cmd-unstable-set-w-evictions-graph](https://github.com/redis/redis/assets/117414976/b7e27926-ade2-4908-a3d3-be128f66df8d)

Let me know if you have any questions or any suggestions/recommendation over the method of carrying out performance benchmark."
1749313581,11695,hpatro,2023-10-05T16:58:24Z,@roshkhatri Did we ran the test multiple times? There could be also some deviation across runs. Ideally standalone (cluster-enabled false) shouldn't have any performance variance from unstable.
1749316823,11695,roshkhatri,2023-10-05T17:00:47Z,"> @roshkhatri Did we ran the test multiple times? There could be also some deviation across runs. Ideally standalone (cluster-enabled false) shouldn't have any performance variance from unstable.

Yes, but I can run it few more times to confirm.

Edited the above comment with the results
"
1749319023,11695,hpatro,2023-10-05T17:02:21Z,@madolson @oranagra There are possibly two minor optimizations (see https://github.com/redis/redis/pull/11695#issuecomment-1745603500) left out which I'm not sure if there is a lot of major gain. Could we iron out if there is anything else left to be addressed or if it's in a good state to be merged ?
1749828644,11695,madolson,2023-10-06T00:26:44Z,"> SETs are ~1-2% faster on this branch.
GETs have a very similar performance ~0.5% faster on this branch.

I don't get this. We expect no performance change. I suppose ultimately it's okay though, since it's just net faster.

> Could we iron out if there is anything else left to be addressed or if it's in a good state to be merged ?

I'm okay creating follow up for those two things. They don't seem essential to the implementation and we are still seeing a new improvement without them. "
1750075805,11695,hpatro,2023-10-06T06:54:03Z,"> > SETs are ~1-2% faster on this branch.
> > GETs have a very similar performance ~0.5% faster on this branch.
> 
> I don't get this. We expect no performance change. I suppose ultimately it's okay though, since it's just net faster.

Those are minor deviation (+/-) seen across runs. I wouldn't read much into it. Overall, it's similar in performance to unstable with 16 bytes of memory saving per key.
"
1756822198,11695,madolson,2023-10-11T05:13:59Z,"> https://github.com/redis/redis/pull/11695#discussion_r1292811014 - about benchmarking MSET

I believe this got translated into the pending item about evaluating `multi performance`. I left a comment that we can benchmark and evaluate it independently. I think it would reduce readability too much and it's not clear it would have a major benefit, but it would be easier to benchmark it independently. 

```
: Multi performance (Ref https://github.com/redis/redis/pull/11695). Is this something we believe needs to be done now? [Madelyn votes no]
```
"
1756834647,11695,roshkhatri,2023-10-11T05:30:45Z,"I ran some tests for benchmarking the performance of MSET in cluster enabled.
Here I executed 1M MSET commands to fill data across different slot on each MSET command, 10 keys at a time.
The script that I ran:
```
while {$idx < 1000000} {
            set hashtag [expr { int(10000000000 * rand()) }]
            r mset ""a:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""a"" ""b:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""b"" ""c:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""c"" ""d:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""d"" ""e:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""e"" ""f:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""f"" ""g:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""g"" ""h:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""h"" ""i:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""i"" ""j:{${hashtag}}:[expr { int(1000000 * rand()) }]"" ""j"" 
            set idx [expr {$idx + 1}]
}
  ```
For Unstable I got these results:
`cmdstat_mset:calls=1000000,usec=13701672,usec_per_call=13.70,rejected_calls=0,failed_calls=0`
`cmdstat_mset:calls=1000000,usec=13555544,usec_per_call=13.56,rejected_calls=0,failed_calls=0`

For this PR:
`cmdstat_mset:calls=1000000,usec=10463197,usec_per_call=10.46,rejected_calls=0,failed_calls=0`
`cmdstat_mset:calls=1000000,usec=10466570,usec_per_call=10.47,rejected_calls=0,failed_calls=0`

Its ~20% faster performance of MSET for the PR as compared to current unstable."
1756838681,11695,madolson,2023-10-11T05:36:05Z,"> Its ~20% faster performance of MSET for the PR as compared to current unstable.

This seems compelling to me such that at least right now I don't think we need to pursue more optimizations."
1757559694,11695,oranagra,2023-10-11T12:14:08Z,"> > https://github.com/redis/redis/pull/11695#discussion_r1292811014 - about benchmarking MSET
> 
> I believe this got translated into the pending item about evaluating `multi performance`. I left a comment that we can benchmark and evaluate it independently. I think it would reduce readability too much and it's not clear it would have a major benefit, but it would be easier to benchmark it independently.
> 
> ```
> : Multi performance (Ref https://github.com/redis/redis/pull/11695). Is this something we believe needs to be done now? [Madelyn votes no]
> ```

ok, so i'd ask to add MSET and fix the link: :smile: "
1757563906,11695,oranagra,2023-10-11T12:16:57Z,"> Its ~20% faster performance of MSET for the PR as compared to current unstable.

i think my request was to check how much more such an optimization can help (update the BIT only once), not to fix any regression this PR has. 

> This seems compelling to me such that at least right now I don't think we need to pursue more optimizations.

i agree it's not a blocker, and should be left for later."
1758051123,11695,hpatro,2023-10-11T16:18:29Z,"@oranagra / @madolson I've updated the tracking comment to further look into MSET performance improvement independently.
https://github.com/redis/redis/pull/11695#issuecomment-1745603500"
1758566426,11695,madolson,2023-10-11T21:24:58Z,"@oranagra From my perspective the only thing that seems pending is your thread https://github.com/redis/redis/pull/11695#discussion_r1349679408. I'm not familiar enough with the code, so assuming we can merge it once we have a decision there."
1758567295,11695,madolson,2023-10-11T21:25:39Z,"We reviewed this in the core team, and no-one else wanted to take a look. So once all outstanding comments have been addressed this should be good to merge."
1759098441,11695,oranagra,2023-10-12T07:41:39Z,"the defrag discussion is resolved, from my perspective we can proceed.
please try to do a quick skim through the code, and make sure the top comment (and quash-merge commit comment) has all the details it should have.
i.e. the justification for the change, what was changed, any specific areas that should be described in more detail, and any other side effects.
thanks."
1763206897,11695,madolson,2023-10-14T23:23:48Z,Full test run https://github.com/redis/redis/actions/runs/6520239250/job/17707613508.
1763253811,11695,madolson,2023-10-15T03:21:17Z,New test run: https://github.com/redis/redis/actions/runs/6521695483
1763298243,11695,madolson,2023-10-15T06:59:38Z,"@hpatro @roshkhatri The new memory efficiency test does seem a bit flakey (See https://github.com/redis/redis/actions/runs/6521695483/job/17710527501#step:6:4840) valgrind makes it worse. Didn't seem worth continuing to iterate here, but let's try to make it more stable."
1763480051,11695,zuiderkwast,2023-10-15T19:17:57Z,"It's merged! Thank you all for helping push this through! I think it's a very good improvement to Redis Cluster.

Regarding the follow up item mentioned in the top comment...

> Currently we don't do anything to reject old cursors, since the cursor will just look like a valid cursor for slot 0. I think we don't care, but just pinning to make sure folks agree.

Didn't we end up storing the slot in the 14 LSB of the cursor, meaning an old cursor can map to any slot, not just slot 0?
"
1765298314,11695,hpatro,2023-10-16T21:22:09Z,"> It's merged! Thank you all for helping push this through! I think it's a very good improvement to Redis Cluster.
> 
> Regarding the follow up item mentioned in the top comment...
> 
> > Currently we don't do anything to reject old cursors, since the cursor will just look like a valid cursor for slot 0. I think we don't care, but just pinning to make sure folks agree.
> 
> Didn't we end up storing the slot in the 14 LSB of the cursor, meaning an old cursor can map to any slot, not just slot 0?

Yes, I think the `Important changes` section has the correct statement. We could remove this. "
1765980175,11695,oranagra,2023-10-17T08:59:28Z,"i see that in addition to the defrag test, the `expire scan should skip dictionaries with lot's of empty buckets` test is also unstable.
@hpatro can you look into these and make a PR to fix them?"
970329168,9788,eduardobr,2021-11-16T14:29:47Z,"@chenyang8094 Just out of curiosity, does this create a foundation to extend https://github.com/redis/redis/pull/8015 to AOF-only and not only RDB-only setups? Thank you"
971143029,9788,chenyang8094,2021-11-17T03:31:56Z,"> @chenyang8094 Just out of curiosity, does this create a foundation to extend #8015 to AOF-only and not only RDB-only setups? Thank you

Sure, you can see some followup PR plans mentioned by oranagra [herer](https://github.com/redis/redis/pull/9539#issuecomment-966244053).

"
971228464,9788,oranagra,2021-11-17T06:23:11Z,"I've just created a few new issues to track these ideas (don't like them sitting around in a comment in a closed PR)
I moved one note of that post to a todo bullet in the top comment of this PR.
The first one of the other issues should be done for 7.0 IMHO:
* https://github.com/redis/redis/issues/9794

The other ones might not make it for this release, but we can at least start discussion and mark them for next release:
* https://github.com/redis/redis/issues/9795
* https://github.com/redis/redis/issues/9796"
971323580,9788,chenyang8094,2021-11-17T07:53:48Z,"@oranagra  About upgrades from old redis versions, I think we can't directly rename the old AOF to the new name, because we still can't solve the atomic problem of rename and update manifest.

We can write the old AOF name directly into the manifest, so that we can start loading normally. Even if the process crashes after writing the manifest, that is no problem, because we can find the AOF from the manifest. Once AOFRW is executed once, all files will have new file names. （current code is implemented according to this idea）"
971328700,9788,oranagra,2021-11-17T08:02:30Z,"ok, sounds good to me. so the AOF file (possibly with preamble content) will have it's old name, and will be listed in the manifest.
once a rewrite happens it'll be tagged as history and later deleted.
let's be sure to add a test for it (storing an old AOF preamble file in the assets folder)"
979149935,9788,oranagra,2021-11-25T12:07:13Z,"@chenyang8094 please avoid force-push (`git commit --amend` and `git rebase`), it make it harder for me to track the changes.
we're gonna squash merge the PR eventually, so it doesn't matter if it has ton of incremental changes."
979171438,9788,chenyang8094,2021-11-25T12:28:16Z,"> @chenyang8094 please avoid force-push (`git commit --amend` and `git rebase`), it make it harder for me to track the changes. we're gonna squash merge the PR eventually, so it doesn't matter if it has ton of incremental changes.

Sorry for that...   Today I encountered a merge conflict, so I rebase directly..."
981725242,9788,oranagra,2021-11-29T15:10:15Z,"Regarding Yossi's proposal for using a folder with the name specified in `appendfilename`, i support that.
I think it'll make things easier, specifically maybe to detect if we're upgrading from an earlier deployment.
It does cause some inconvenience on startup, since we'll need to [create a folder, create manifest, move a file], and maybe handle power failures during this sequence, but i think it worth it.

i wanna mention a few different cases for upgrade:
1. a deployment which has a modified redis.conf and an aof file on the disk (e.g. named ""mydb.aof"")
2. a deployment that uses a default config that's taken from the distro, and an aof file on the disk named (e.g. named ""appendonly.aof"")

In that light, **i think my previous advise to change the default value for that config, was probably wrong**. i.e. in case 2, we may be looking for the wrong file, since the config file has changed, but the existing file on the disk was not.

One more note however, i think we may still want to apply some user configured prefix on the files themselves, not just rely on the folder name, so that if a user decides to copy them somewhere without the folder he'll still know who they belong to.
i.e. imagine the folders are named `redis-1`, `redis-2`, etc. it might be nice that the files themselves are also prefixed with that.
but on the other hand, if by default we keep the name `appendonly.aof`, then it'll look odd that the files have that prefix."
982233425,9788,chenyang8094,2021-11-30T02:52:11Z,"> weird

@oranagra @yossigo 

Regarding `appendfilename` as a directory, it is indeed convenient for users to copy and backup AOF files, which I also agree with. But I have a problem:

If the current user has a deployment and uses the default configuration (`appendfilename` is `appendonly.aof`), then there will be a file `appendonly.aof` in the dir directory. If the user starts with the binary `redis-server` of the Multi Part AOF version, what do we need to do? First create a directory named `appendonly.aof`? This obviously doesn't work, because it conflicts with the `appendonly.aof` file. Create the directory under another name or rename `appendonly.aof` to another temporary name first? This does not work either, we may lose this file forever.
```
if existsDir(server.appendfilename) {
    load in multi part mode according manifest file
} else if existsFile(server.appendfilename) {
    load server.appendfilename
    rename server.appendfilename to tempxxxxxx ?
    mkdir server.appendfilename
    mv tempxxxxxx to server.appendfilename
    add tempxxxxxx info to  manifest file and persist
} else {
    NOT EXISTS
}
```

So, do you have any more detailed suggestions?

In addition, I have another idea. Regarding backup and copying, manifest is the best reference. Can we provide a tool binary `redis-package-aof` (similar to `redis-check-aof` and `redis-check-rdb`), which receives the manifest file name and target directory name, and according to the instructions of the manifest, all AOF files (including manifest Itself) are packaged in a target directory.

In addition, regarding the issue that the files we generate may conflict with the user's scripts or tool files, I don't mind that we can add timestamp information to our file names. I think this is the limit we can do.
```
appendonly.aof_1638243191225.rdb
appendonly.aof_1638243224831.aof
appendonly.aof_1638243224947.aof
```
And we have realized that we cannot change the default configuration of `appendfilename` (it will always be `appendonly.aof`) . Do we need to reconsider the following naming method, intuitively I think it looks better.
```
appendonly.aof_1638243191225.base
appendonly.aof_1638243224831.incr
appendonly.aof_1638243224947.incr
```
 Or even a more simplified version:
```
appendonly.aof_1638243191225
appendonly.aof_1638243224831
appendonly.aof_1638243224947
```"
982743389,9788,oranagra,2021-11-30T15:26:54Z,"I have a feeling that we may be able to make it work by something like this:
1. create a temp dir named ""<appendfilename>.tmp""
2. create a manifest in that dir (points to a yet non-existing file)
3. move the original AOF file into that dir
4. rename the dir.

the only problematic point of failure is if there was a crash between 3 and 4, in which case when we recover from that crash we can look for ""<appendfilename>.tmp"" and resume that operation.

another idea is to decide from the get go that the dir is always named ""<appendfilename>.files"" or alike.
i.e. everything we create is based on that base file name, but always with some suffix.
and then the upgrade procedure only has one rename in it and not two.

regarding the `weird` thing of having rdb files with an intermediate `aof` suffix (i.e. `appendonly.aof.1.rdb`), i don't like it, but i'm wiling to accept it.
i don't like the timestamps in the file name though.

regarding tooling, that's a nice idea to add tools, but i feel they should not be binary tools, but rather scripts.
the advantage is that they're maybe easier for users to adjust or mimic to create something similar of their own.
i.e. as long as we don't need to parse binary files (e.g. rdb.c) or match things against a complicated format and maybe the redis command table (aof.c), maybe we better not write it in C?"
983142615,9788,chenyang8094,2021-12-01T00:09:49Z,"> I have a feeling that we may be able to make it work by something like this:
> 
> 1. create a temp dir named "".tmp""
> 2. create a manifest in that dir (points to a yet non-existing file)
> 3. move the original AOF file into that dir
> 4. rename the dir.
> 
> the only problematic point of failure is if there was a crash between 3 and 4, in which case when we recover from that crash we can look for "".tmp"" and resume that operation.
> 
> another idea is to decide from the get go that the dir is always named "".files"" or alike. i.e. everything we create is based on that base file name, but always with some suffix. and then the upgrade procedure only has one rename in it and not two.
> 
> regarding the `weird` thing of having rdb files with an intermediate `aof` suffix (i.e. `appendonly.aof.1.rdb`), i don't like it, but i'm wiling to accept it. i don't like the timestamps in the file name though.
> 
> regarding tooling, that's a nice idea to add tools, but i feel they should not be binary tools, but rather scripts. the advantage is that they're maybe easier for users to adjust or mimic to create something similar of their own. i.e. as long as we don't need to parse binary files (e.g. rdb.c) or match things against a complicated format and maybe the redis command table (aof.c), maybe we better not write it in C?

@oranagra The method you mentioned seems to work, but there are many problems. Remember the redo solution we discussed earlier? The idea you mentioned involves `createDir`, `moveFile`, and `renameDir`. If any one of them fails, we may be in an intermediate state. If there is no redo, redis will start complex and ugly if-else judgments, and this is not completely To solve the problem, we have been demonstrating this for a long time before. The lesson we learned is not to modify the original AOF file (even move or rename) before ensuring that the user’s data has been loaded correctly.

Let’s review the current implementation:
```
int loadAppendOnlyFiles() {
    if aof_manifest not exists {
        if server.appendfilename exists {
            Construct a base type aofInfo with server.appendfilename
            persist aof_manifest
        } else {
            return NOT EXISTS
        }
    } 

    for aofinfo in aof_manifest {
        load aofinfo->filename
    }
}
```
As you can see, there is no modification to the user's original file, and the user does not even feel the change (the difference is that there are a few more files, that is, the manifest file and an INCR AOF file), Even if he wants to use the old version of redis now(rollback), it’s okay, because the original AOF is still there. The base file will also use our new naming rule after AOFRW.  So I think the current upgrade method is fine.

Secondly, why do we want to do this, are we just to put all AOF files into a dir for easy copying and backup? What I want to say is that in 99% of the cases, we will only have three AOF-related files: `appendonly.aof.manifest`, `appendonly.aof_1.rdb`, `appendonly.aof_1.aof` (It's just two more files than only `appendonly.aof` before), and they all have a common prefix, I think Anyone knows how to quickly copy these files: `cp appendonly.aof* targetDir`.

In addition, for the remaining 1% case, frequent failures of AOFRW resulted in a lot of INCR AOFs (Fortunately we have AOFRW limit, so it won't be too many). Like the above, they still have a common prefix, and regular copy can still work.

Even, we can provide an AOF tool (indeed, scripting is enough). We can provide the following convenient functions:
```
1.list: List the current AOF list in order
2.gc: delete all AOF files in the history state (the redis process remains due to the crash)
3.package: package all AOFs (including manifest files) into a specified directory
4. Or other functions
```

Regarding the issue of file name conflicts, I think this probability exists but is not critical. Now `bgaofrewrite` will generate a file name similar to `temp-rewriteaof-bg-pid.aof`. This file name changes every time. Who can guarantee that it will not conflict with other file names in the dir directory (yes, from the probability it exists). But I think that an online business that really uses redis will not name their tools or scripts that way. And under normal circumstances, users do not put their scripts in the working directory of redis (Remember that `dir` is redis's own working directory), they usually put them in their working directory. Therefore, this should not be the reason why we forcibly put AOFs into a directory. @oranagra @yossigo WDYT？looking forward to your feedback.
"
983373880,9788,oranagra,2021-12-01T07:44:45Z,"> If there is no redo, redis will start complex and ugly if-else judgments

I don't think it'll be that bad. looking at my 1..4, i think in all cases except 3, we can just restart the process from scratch, and the only one that needs special handling is a crash between 3. and 4.

> As you can see, there is no modification to the user's original file, and the user does not even feel the change (the difference is that there are a few more files, that is, the manifest file and an INCR AOF file), Even if he wants to use the old version of redis now(rollback), it’s okay, because the original AOF is still there

yes, but:
1. that's exactly what Yossi was concerned about... that an old backup script will keep working since the file is still there, and it'll take time till the user realizes his script is broken.
2. after startup we'll start writing new commands to that old AOF file, these commands could be ones that didn't exist in the previous versions, so the user is not allowed to downgrade anyway. (p.s. even if we start a new part at startup, that's even worse, since a downgrade will only look at ""part 1""). so anyway, downgrade is out of the scope, it's not an argument for this discussion.

> Secondly, why do we want to do this, are we just to put all AOF files into a dir for easy copying and backup? What I want to say is that in 99% of the cases, we will only have three AOF-related files: appendonly.aof.manifest, appendonly.aof_1.rdb, appendonly.aof_1.aof (It's just two more files than only appendonly.aof before), and they all have a common prefix, I think Anyone knows how to quickly copy these files: `cp appendonly.aof* targetDir`.

> In addition, for the remaining 1% case, frequent failures of AOFRW resulted in a lot of INCR AOFs (Fortunately we have AOFRW limit, so it won't be too many). Like the above, they still have a common prefix, and regular copy can still work.

we don't need to design things according to 99% of the cases, that's exactly what could lead users to write a script that backs up only 3 files, and be broken on the 1% (without user ever realizing it before it's too late).

I agree that if we don't change or mess with the default of `appendfilename` in any way, then `cp appendonly.aof* targetDir` is sufficient. but then i breaks Yossi's concern about backup script after upgrade failing silently.
so we're just trying to find a something better.
these two concerns are actually concerns are actually contradicting each other (one require no rename, and the other requires not to use the old name), so we must decide which one is more important."
983403537,9788,chenyang8094,2021-12-01T08:26:46Z,"> we can just restart the process from scratch,

@oranagra 
Yes, I very much agree that we should work hard for that 1%, which is why we have been discussing here, otherwise things will be very simple.
Regarding the problem you mentioned (the backup script found that they were backing up a wrong AOF for a long time), I think this is easy to solve. Once we find that we are starting from an old redis when we start, We can do AOFRW immediately after loading this AOF (this is easy to do: `server.aof_rewrite_scheduled = 1`), so that the old AOF will become our new naming rule in a short time."
983410653,9788,oranagra,2021-12-01T08:36:59Z,"> We can do AOFRW immediately after loading this AOF

This will come at a cost... (time, CPU, latency, COW).
Yossi suggested to create use hard links, which i didn't like since i'm afraid of unexpected platform specific issues, and for users to accidentally copy both files.

But maybe it's not that bad to only temporarily use them for a short period?
i.e. what you're suggesting about an AOFRW at upgrade / startup, is in some way similar to making a temporary hard link, and then delete the original one (i.e. you create another copy of the data, so you can later delete the old one)."
983495609,9788,chenyang8094,2021-12-01T10:21:23Z,"> But maybe it's not that bad to only temporarily use them for a short period? i.e. what you're suggesting about an AOFRW at upgrade / startup, is in some way similar to making a temporary hard link, and then delete the original one (i.e. you create another copy of the data, so you can later delete the old one).

We all know that upgrading redis online does not directly restart the master, but first upgrade the slave and do HA again, and then upgrade the previous master. Therefore, I think that the AOFRW after loading will not affect the real online requests. At the same time, what we are doing now (Multi Part Aof) is to reduce the overhead caused by AOFRW. Isn't it ?

If the user directly stops the service and restarts the master, then the overhead caused by AOFRW does not need to be concerned.

Moreover, this upgrade is a one-time thing, I don't think we need to introduce such a complicated logic for this one-time thing (compared to the current implementation).

Below is a piece of pseudo-code I tried to understand and write, we can compare it（If redis crashes after the 3 step, the user will find that his original AOF file is gone (actually it was moved to .tmp)）:
```
int loadAppendOnlyFiles() {
    if existsDir("".temp"") {
        // Found a interrupted upgrade
        if existsFile(server.appendfilename) {
            // may last crashed between step 1 - 3
            assert(!existsFile("".tmp/server.appendfilename""))
            
            deleteDir("".temp"");

            goto upgrade
        } else {
            // last crashed after step 3
            assert(existsFile("".tmp/server.appendfilename""))

            // redo step 4
            // rename the dir
            rename("".tmp"", server.appendfilename)
        }
    }

upgrade:
    if !existsDir(server.appendfilename) {
        // 'server.appendfilename' dir not exists, we may upgrade from old redis
        if existsFile(server.appendfilename) {
            // 1. create a temp dir named "".tmp""
            createDir("".tmp"")

            // 2. create a manifest in that dir (points to a yet non-existing file)
            // the content: ""file appendonly.aof seq 1 type b""
            createAndWriteFile("".tmp/aof_manifest"")

            // 3. move the original AOF file into that dir
            rename(server.appendfilename, "".tmp/server.appendfilename"")

            // 4. rename the dir
            rename("".tmp"", server.appendfilename)        
        } else {
            return NOT EXISTS
        }
    } 

    for aofinfo in aof_manifest {
        load aofinfo->filename
    }
}
```
Below is the current implementation:
```
int loadAppendOnlyFiles() {
    if !existsFile(aof_manifest) {
        if existsFile(server.appendfilename) {
            // the content: ""file appendonly.aof seq 1 type b""
            createAndWriteFile(aof_manifest)
        } else {
            return NOT EXISTS
        }
    } 

    for aofinfo in aof_manifest {
        load aofinfo->filename
    }
}
```

Finally, I want to declare that I don't care about writing a few more lines of code, but once this code is added, it will always remain in the redis code (because the version upgrade is a long-term thing). But the upgrade operation is an instant and one-time thing (we do not support rollback), at least for now, I think the problems you mentioned can be solved at present, unless there is something that cannot be solved now. I like this discussion very much. It makes our views on things clearer. I look forward to your feedback, thank you @oranagra 
"
983622957,9788,oranagra,2021-12-01T13:07:04Z,"i think your pseudo code can be simplified, we don't need explicit code to re-do steps 2,3,4.
just delete the tmp dir and let the process start again normally (i.e. fall back to the code below).
the only part that needs special handling is in case the temp dir exists, and the aof file is already in it.

i think that upgrading a standalone server (no replication) is a common thing.
introducing an additional AOFRW will also add some complexity (and a special state, and testing), and considering the performance overhead, i personally rather have a few lines of code and comments with fast execution than a long rewrite process.
maybe the hard link solution is a valid one too (will make the recovery simpler, but it's harder for me to see where it could backfire).

anyway, i think this is a viable solution to upgrade if we choose using a folder, but IIRC there are other concerns about this approach (like having a folder named `appendonly.aof` or maybe `redis-13.aof`), and possibly the files in it will also have the prefix (e.g. `redis-13.aof.incr.1234.aof`).
or other concerns i already forgot?"
983643111,9788,chenyang8094,2021-12-01T13:29:51Z,"As I said earlier, to upgrade an standaone server, the user can only restart the `redis-server` in place. Does he still care about the cost of AOFRW? No request to come in at this time.

AOFRW is only triggered when upgrading from the old version redis, not after every loading, it will not bring any complexity (including test), you can see my latest commit.

I really haven't found any benefits (Just pack the AOF file in advance) that using a directory can bring us, and what are the problems with the current solution.

Regarding the issue of backup, I suddenly remembered that we all made a mistake in order. Normally, users will not upgrade the redis-server first without thinking of upgrading its backup script in advance, which is difficult to understand(Instead of waiting here for the backup script to report an error to know that we have upgraded the redis-server). I believe that redis 7.0 will definitely introduce some other major features, which must be prominently reminded in the release notes and let users know. We introduce the Multi Part Aof mechanism, and the surrounding operation and maintenance tools will definitely need to be changed and adapted. This is unavoidable.

In addition, if we use `appendonly.aof` as the directory name, we don't need to use it as the prefix of the AOF file, otherwise it will be a very redundant design(Let us free ourselves from this annoying naming convention). Ithink It should look like this:
```
── appendonly.aof
   ├── manifest
   ├── 1.base.rdb
   ├── 1.incr.aof
   └── 2.incr.aof
```
The directory name will be used to distinguish different AOF sets. As for how to distinguish AOF files when backing up, I think this is a matter of the backup script, not a matter that redis should care about. The backup script should directly type the entire directory into a zip or other format compressed package, so that it will truly become a file, and subsequent backups and downloads will use this file, so there will be no question of who belongs to whom.

@oranagra @yossigo @soloestoy I think we have been discussing this issue for a long time, but I feel that we are now thinking about the design of the solution for the 1% possibility (the backup script is not adapted in advance). I think my point of view has been expressed very clearly, at least so far I still think that there is no need to introduce so much code in order to upgrade from the old version redis, which just one-time thing.
But if we have been holding our own opinions, this PR will be blocked for a long time (I think this is the last question of this PR, right? exclude tests). So, I think this decision is left to your core team. If you all still insist on doing this, then I will start to change the code.
"
987724649,9788,oranagra,2021-12-07T09:19:01Z,"We discussed this in a core-team meeting.
We decided to proceed with keeping the old config and it's default, and creating a folder with that name.
Some of the reasons:
* we want to implicitly handle upgrades properly for the wide range of ways redis is used (without failing on startup, or doing something that will result in data loss)
* we want it to be very easy for users to realize which files needs to be copied or managed
* we want old scripts that look for the old file to either keep working, or immediately fail
* we're ok to have a few extra lines of code to handle complicated upgrade, or even fail and require manual intervention in the very rare case of a crash during the upgrade process."
988472067,9788,soloestoy,2021-12-08T03:46:28Z,"@oranagra I don't think we reached an agreement as you described above.

Using the old config `appendfilename` to create a folder is ambiguous and hard to understand, and maybe we have to spend more time and words in release note to explain the odd design to most users.

You give me an example that maybe some users manage some redis instances in same `dir` but using different `appendfilename` to distinguish their data. And then after upgrade to redis 7, it may lead to clash because of the new version have same default manifest and other file name.

But I want to say these users are so smart they give redis instances different `appendfilename`, I believe they could be smart again to give redis 7 different `dir`.

And upgrading needs restart redis, it's easy to modify config file, I don't think it's a big work. And actually our control plane system is doing that everyday, upgrade old version to new version (we already implement the multi part aof in the manifest way).

BTW, some other popular OSS project like rocksdb manages its files like that, I think it's easy to understand and maintain."
988510692,9788,chenyang8094,2021-12-08T05:25:05Z,"@oranagra @yossigo @soloestoy Well, it seems we still haven't reached an agreement.

Things have progressed here, I think this is beyond the ability of this PR. I am curious, when we are designing, we should give priority to satisfying most common scenarios, or should satisfy some very marginalized scenarios. I believe most people will choose the former, because there is no one thing in the world that can be good at everything, and `redis` is the same.

For example, `redis` is an in-memory database, but many users use it as a persistent database. We cannot change `redis` to synchronous replication and synchronous disk writing (or add LSM tree like rocksdb) just because of these users. I think this is the design principle of `redis`.

@oranagra You have been opposed to my current implementation (I think it is simple and effective), because many users will deploy many `redis` on one machine, or they all use the same `dir`. First of all, I think this usage is surprising, I don't know if you have seen such users in reality. Assuming they do exist, it must be the edge scene I mentioned above. Think about it, all `redis` running on one host (and possibly using the same `dir`) itself is a very big security risk. Do they still care about the overhead caused by AOFRW when `redis` is upgraded? I don't think it will.

There is also the problem of the backup script you mentioned. This is even more puzzling. Don't users know that they want to upgrade `redis 7.0`, and upgrade or stop its backup script in advance?

If we always use the 1% edge scene to guide our design, then we will lose 99% of the user experience.

In private, I have implemented the code that uses `appendfilename` as a directory, but I think it is ugly for the following reasons:
1. `Appendfilename` configuration is literally a file name, but in fact it is indeed a directory name, which is very confusing
2. Compared with the flat directory structure with only `dir`, the two-layer directory structure will cause many path joins in the `redis` code and tests.
3. A directory named `appendonly.aof` looks very strange


Current implementation (please allow me to list it again for comparison):
1. `Appendfilename`  as base name,  name rules: `appendonly.aof_manifest`、`appendonly.aof_1.base`、`appendonly.aof_1.incr`, I think it's simple and beautiful
2. Can smoothly support users to upgrade from the old version `redis`, User files will not be tampered with during the upgrade process, any crashes can be recovered again
3. The code is clearer and more concise, as you can see


Regarding what you mentioned:
>
>You give me an example that maybe some users manage some redis instances in same dir but using different appendfilename to distinguish their data. And then after upgrade to redis 7, it may lead to clash because of the new version >have same default manifest and other file name.

This completely misunderstood my implementation, even if all files are in the same `dir` directory, they will be distinguished by `appendfilename` prefix，declare the naming rules again: `appendfilename_seq.type`

For unreasonable `redis` usage, we should try our best to guide them to the correct method, rather than cater to them. At the same time, in terms of functions, we still support this unreasonable usage, But these users should stop complaining about why the CPU of the machine is so high, the reason is because he put all the `redis` on the same machine (When `redis` is doing AOFRW at the same time, which doesn't just happen when upgrade) "
988594773,9788,oranagra,2021-12-08T08:20:11Z,"> Using the old config `appendfilename` to create a folder is ambiguous and hard to understand, and maybe we have to spend more time and words in release note to explain the odd design to most users.

I think the same can be said about using this as a prefix (it's hard to understand and we'll have to explain it, i.e. the prefix part).
That whole change about multiple files and a manifest is hard to explain. maybe using a folder will make it easier to explain, i.e. we don't need to give detail about the individual files suffixes.
it's right that the config name is odd since it has the word `file` in it. had it been just `appendname` or `aof_name` it might look a bit cleaner.

> But I want to say these users are so smart they give redis instances different appendfilename, I believe they could be smart again to give redis 7 different dir.

My thinking that users might have been smart or not when the initially created some configuration that worked for them, and since then they moved away and forgot what they did.. now they might just be updating their binary to get fixes and improvements, without re-thinking their configuration, and i don't want it to break.

> And upgrading needs restart redis, it's easy to modify config file, I don't think it's a big work. And actually our control plane system is doing that everyday, upgrade old version to new version (we already implement the multi part aof in the manifest way).

Some users have fancy control plain and modify it when integrating new version (have an R&D department),
Others may have something simpler, but still test it in a staging environment before upgrading their production environment.
Some may just place the new binary and restart their production environment, or type `apt-get upgrade`.
Some may not even be aware they have redis, they use some other tool that relies on redis internally.

My point is, i imagine redis is used in a very wide array of configurations, as we've seen by the popular abuse of `protected-mode`, initially Salvatore didn't want to create that feature, [saying](http://antirez.com/news/96) that uses should know that they're doing, later it became a burden telling people they're doing the wrong thing, and he added this feature, but today we see that even that wasn't enough... some people just disable it without reading, and then open an [issue](https://github.com/redis/redis/issues/9060) here to ask why they lost their data (without realizing they were also hacked).

> Think about it, all redis running on one host (and possibly using the same dir) itself is a very big security risk. Do they still care about the overhead caused by AOFRW when redis is upgraded?

I don't see the security risk, and yes, i do think the excessive AOFRW can cause an overhead, and even OOM kills due to CoW.

> There is also the problem of the backup script you mentioned. This is even more puzzling. Don't users know that they want to upgrade redis 7.0, and upgrade or stop its backup script in advance?

Yes, i think many who upgrade don't even realize they're doing an upgrade, and wont read the release notes. they have something they've set up in the past, it works for them, and they just replace the binary or do `apt-get upgrade` without reading the release notes.

> If we always use the 1% edge scene to guide our design, then we will lose 99% of the user experience.

we should aim for the 99%, but even for the 1% we don't want to break them too hard, or inflict data loss or security issues on people who are careless or have odd configurations.

> In private, I have implemented the code that uses appendfilename as a directory, but I think it is ugly for the following reasons...

I can understand why you think flat is elegant, and folder name i'm proposing is odd (due to the name of the config and the name of the folder, and due to the complications with rename on upgrades).
But i think you also have to admit that having all files related to a certain mechanism sit in a folder is also elegant and less confusing.
i.e. Imaging we didn't have any backwards compatibility concerns. if we would be creating redis 1.0 today. wouldn't it be cleaner to put all these AOF related files in a folder? isn't it easier for the user to realize that's where all the content for the AOF mechanism sits, instead of having to pick files scattered in another folder and mixed with other files?

@soloestoy mentioned RocksDB and other systems, RocksDB indeed creates many files of various formats and puts them in that folder, but IIRC it isn't mixing other files in there (like config?), it's just data.
And you have to admit that it is better to put files serving a certain purpose (when you have more than one) in a folder.
For example a navigation app, will store the maps in one folder and the voice recordings in another. same as we store the tests separated from the code and the docs.
I think storing the AOF related files in a folder will means our documentation can be simpler! (we don't have to explain the individual file names / purposes and structure), and considering users are likely to not read the docs anyway, and just maybe write a backup script backing up what they empirically see, they might write a script that copies the 3 files they normally see, and completely miss the rare cases where we create a 4th file.
putting it in a folder makes it easier to understand without reading anything.

> > You give me an example that maybe some users manage some redis instances in same dir but using different appendfilename to distinguish their data. And then after upgrade to redis 7, it may lead to clash because of the new version have same default manifest and other file name.

> This completely misunderstood my implementation, even if all files are in the same dir directory, they will be distinguished by appendfilename prefix，declare the naming rules again: appendfilename_seq.type

This example was about a discussion on a scenario in which we **do** create a folder, but use a new config to control its name (and deprecate the old config), in which case, users who just upgrade the binary of redis without changing their config files, will use the default name of the folder, and the files will clash.
Another proposal that was made in discussion was that if we find that the user didn't update his config file, we'll fail to start, demanding that the user reads the docs and updates the config properly. i don't like that approach at all."
988599827,9788,soloestoy,2021-12-08T08:27:49Z,">My thinking that users might have been smart or not when the initially created some configuration that worked for them, and since then they moved away and forgot what they did.. now they might just be updating their binary to get fixes and improvements, without re-thinking their configuration, and i don't want it to break.

I have no words about this scenario, Schrodinger's user?"
992736972,9788,yossigo,2021-12-13T18:09:12Z,"One use case to consider, that was not brought up, is Redis that is deployed and managed by an upstream Linux distro (as a deb/rpm).

Typically, these package systems will handle replacement of binaries and restarting of services easily, and may even replace a config file if it was not modified. However, if a config file is modified then it may or may not be replaced - depending on user choice or system settings.

@soloestoy @chenyang8094 I realize you have a very firm opinion about the design details, but are we in agreement on the goals we try to achieve? To me, the *must-have* goals are:

* Upgrade from previous versions should be as simple as replacing the binary and restarting, no additional / manual steps involved
* Redis 7 should behave in a reasonable way when loaded with a config file of a previous version
* We should minimize the cost of backwards compatibility (i.e. avoid redundant code, duplicate mechanism, ugly config in the future due to the past)
* We must minimize the chances for data loss due to users not being aware that AOF data is no longer stored in just `appendfilename`

@oranagra @madolson @itamarhaber Before diving into the design details, do you have any thoughts about these goals?"
992801279,9788,oranagra,2021-12-13T19:31:16Z,"I agree with these goals, the first two you mentioned are also ""mast-haves"" in my eyes too.
the other two are prefixed by ""minimize"" so we can always argue about how badly these goals are compromised and their priority, so maybe they're not mast-haves.

in that case, here are other (non-must-have) goals:
* we wish the new behavior (file names) to make sense (i.e. we don't want new deployments to look awkward).
* we wish to make it easy for users to conclude where their data is located and easy to copy it (probably falls into your last bullet, but i think you meant existing deployments in yours)."
993021347,9788,chenyang8094,2021-12-14T00:02:15Z,"I also agree with this goal. I also know that putting AOF files in a separate folder is easier to manage and copy. My only entanglement is to use `appendfilename` as a directory name (Frankly speaking, I don’t recommend adding a new configuration such as `appenddirname`. Maybe this will increase the burden of configuration compatibility when upgrading). So, I want to summarize the things we need to agree on now:
1. Whether to put AOFs and manifest in a separate directory?
2. Whether to use `appendfilename` as the directory name?
3. Do we need to use `appendfilename` as the prefix of AOF and manifest file names? (such as: `appendonly.aof_manifest、appendonly.aof_1.base、appendonly.aof_1.incr`)
4. Do we need to use the encoding format (AOF or RDB) as the suffix of the file name? (such as: `appendonly.aof_manifest、appendonly.aof_1.base.rdb、appendonly.aof_1.incr.aof`)

@oranagra @yossigo @soloestoy Or do you have any other specific things to add, I hope we can reach an agreement on the above issues ASAP.

"
993155026,9788,madolson,2021-12-14T04:47:29Z,"Trying to catch up on this thread, sorry if I say something addressed earlier, but there is a lot to read.

@yossigo I also agree with those goals.

> Whether to put AOFs and manifest in a separate directory?

As a new user to Redis, I would probably prefer to store all my data in a directory. Motivation being it's easier for my to tar and upload it, which is what I do with Redis AOF. The esoteric case of what if someone is running multiple Redis instances seems slightly better for directories as well, as there is more structure. (Although that does seem like a major anti-pattern)

I don't have a great grasp on the added complexity of moving everything in to directories the first time. I was also going to suggest hard links (seems like Yossi mentioned that previously) for handling atomically moving the files around. That is a common pattern in AWS deployments. The user might accidentally upload both versions while we're moving the data, but that should only be for a short time. 

> Whether to use appendfilename as the directory name?

People seem to have really strong opinions one way or the other here, but I don't think either approach would be unexpected. Given my previous preference to store all the data in a single directory, I suppose I would be okay using it as the directory name. We could also introduce an alias so that either `appendfilename` or `appenddirname` both work.

> Do we need to use appendfilename as the prefix of AOF and manifest file names? (such as: appendonly.aof_manifest、appendonly.aof_1.base、appendonly.aof_1.incr)

If they are in a folder, I don't see that as a requirement

> Do we need to use the encoding format (AOF or RDB) as the suffix of the file name? (such as: appendonly.aof_manifest、appendonly.aof_1.base.rdb、appendonly.aof_1.incr.aof)

I must have missed the conversation about this, why would we omit the suffix? It's useful for identifying the file type.

I'll also give the disclaimer that I probably know the least about AOF here. I do use AOF for a pet project, and checked to see what happened if I were to upgrade, and found that my automation just recursively uploads the entire dir folder to S3, I might have been paranoid though."
993340859,9788,oranagra,2021-12-14T09:25:09Z,"We discussed this again in a core-team meeting.
The concrete decisions are:
* we wanna put all the AOF files in a folder
* we wanna move the existing AOF file into the folder on upgrades at startup.

The moving of the file on startup might be done in a few different ways:
1. it doesn't have to be something that needs to be done right away (in can be done in the background, but not by using an AOFRW, or waiting for the next one), i.e. it may be ok to do that in cron / thread, but i don't see the advantage in that.
2. It may be ok to do that using hard links (although there is a concern some on some deployments that's an issue)
3. it may be ok to resort to manual intervention in case there's a failure in between two renames (extremely unlikely scenario).

Regarding the configuration we didn't reach a decision.
The options are:
1. use the original appendfilename config
2. use an alias (same default)
3. introduce a new config, and keep the old one just in order to find the old file on upgrades.

options 1 and 2 may be confusing for users, and option 3 may look clean, but there might be some deployments in which it'll not behave correctly.
The only one i can think of at the moment, is multiple instances of redis writing to the same `dir`, and using a different `appendfilename`, in which case on upgrade without modifying the config, they'll all write to the same folder.
I think we can explore the possibility of using a new config directive for for the folder, if we can handle that case in some way that will not lead to a catastrophic disaster (like data loss), even if it'll require manual intervention (assuming this case is the only problematic one).
So one solution could be to use the `appendfilename` also as a prefix for the files we create in the folder.
Another solution is to somehow detect that problematic configuration and fail to start redis with some error message (if that's just this one odd case, then maybe that's a valid outcome).

in any case, i think this means we can now proceed with the implementation (use folders, move the file, and use a new config), while we discuss the remaining details (won't be too hard to change these details later)."
994407872,9788,chenyang8094,2021-12-15T07:14:28Z,"> The options are:

Okay, it seems that we have reached an agreement on the use a directory. I will implement it and make a commit ASAP, thanks."
998365902,9788,chenyang8094,2021-12-21T00:19:24Z,"> options 1 and 2 may be confusing for users, and option 3 may look clean, but there might be some deployments in which it'll not behave correctly.
> The only one i can think of at the moment, is multiple instances of redis writing to the same `dir`, and using a different `appendfilename`, in which case on upgrade without modifying the config, they'll all write to the same folder.
> I think we can explore the possibility of using a new config directive for for the folder, if we can handle that case in some way that will not lead to a catastrophic disaster (like data loss), even if it'll require manual intervention (assuming this case is the only problematic one).

Add a new configuration, we will encounter the following problems (for the scenario you mentioned, that is, multiple redis share the same dir): 
Suppose there are three `redis` processes, They use different AOF file names: `redis1.aof, redis2.aof, redis3.aof`, and they share the same dir.
```
dir:
      redis1.aof  redis2.aof  redis3.aof
```

If we add a new config named `appendonlydir`, which use `appendonlydir` as defualt value.

When they upgrade, suppose `redis1` completes the upgrade first, then we will have the following dir structure:
```
dir:
     appendonlydir  redis2.aof  redis3.aof
```

When `redis2` or `redis3` starts to upgrade, because they both use the same `appendonlydir` default configuration, our upgrade process will not work at this time.





"
998683032,9788,oranagra,2021-12-21T11:00:58Z,"since our upgrade isn't atomic anyway (we have a recovery procedure), what if we slightly change our upgrade procedure to be like so:
1. create the new dir if missing.
2. create a manifest in the new dir pointing to a missing file
3. move the file into the dir.

in that case:
- we don't have a temp dir (named with a unique suffix)
- if we upgrade multiple servers at the same time, one will create the dir, and others should detect the creation fails and use the existing dir
- no other problem of any conflicts between servers during upgrade.

if the upgrade of any server fails half way, we can recover like this:
1. failure before 1, nothing special, we just retry the whole thing.
2. failure between steps 1 and 2 above, is no concern (the dir exists and that's fine)
3. failure between 2 and 3 above, we can detect that the manifest exists but it refers to a missing file, and at the same time detect that the original AOF is still in the root dir, and do the move again.

* the upside is that users have a cleaner config (no folder that's named after a config that refers to a file).
* normally it all looks very clean, and the only complication is during upgrades, and we can handle it properly (i think).
* since all the files in the folder are still using the old config name, there's no clashing in case multiple servers are using the same dir.
* the old config is still being used and documented, not only for backwards compatibility concerns.
* all scenarios in which user has an existing config file which he may or may not have modified are handled.

maybe the only down side is that if a user has a script that used to copy the old aof file, he'll need to update that script to use the dir (different config), but it's likely that his script didn't use `-r` so it would have failed anyway."
999321239,9788,chenyang8094,2021-12-22T06:29:38Z,"@oranagra  Yes, in fact, I think the upgrade process will be simpler in this way. I have modified the code according to this idea and ensured that there is a corresponding tcl to test it. PLZ review this again, thanks."
1001177341,9788,oranagra,2021-12-26T13:03:22Z,"Full CI: https://github.com/redis/redis/runs/4635323320?check_suite_focus=true

valgrind complains on some `FATAL AOF MANIFEST FILE ERROR` when it looks for leaks and finds an unrecognized error).

freebsd complain:
```
util.c:853:18: warning: implicit declaration of function 'open' is invalid in C99 [-Wimplicit-function-declaration]
        int fd = open(full_path, O_RDONLY|O_NONBLOCK);
                 ^
util.c:853:34: error: use of undeclared identifier 'O_RDONLY'
        int fd = open(full_path, O_RDONLY|O_NONBLOCK);
                                 ^
util.c:853:43: error: use of undeclared identifier 'O_NONBLOCK'
        int fd = open(full_path, O_RDONLY|O_NONBLOCK);
                                          ^
```"
1001178617,9788,oranagra,2021-12-26T13:13:41Z,"@redis/core-team please read the top comment and approve.
in my eyes, the only part that's still debatable and i think i'd like to change is the deletion of the `aof_rewrite_buffer_length` INFO field, see [discussion](https://github.com/redis/redis/pull/9788#discussion_r775208776)"
1001183295,9788,chenyang8094,2021-12-26T13:39:10Z,Add `#include <fcntl.h>`
1002230058,9788,oranagra,2021-12-28T18:26:34Z,@chenyang8094 There are some valgrind warnings please look into them
1002355800,9788,chenyang8094,2021-12-29T01:43:59Z,"> @chenyang8094 There are some valgrind warnings please look into them

Yes，i find it. just a variable （`line` in  aofLoadManifestFromDisk） is not initialized. Fixed.

and i tested (runtest with valgrind) in my linux machine, it work."
1004037676,9788,yossigo,2022-01-03T11:44:55Z,"@chenyang8094 I've added another commit with a few updates to comments/redis.conf text and removing trailing whitespace. Please ack, thanks!"
1004070163,9788,chenyang8094,2022-01-03T12:51:21Z,"LGTM, thanks."
1004114535,9788,yossigo,2022-01-03T14:05:38Z,"Almost forgot, one more comment - the manifest file format will break if `appendfilename` includes spaces or newlines, which are otherwise valid today (just an sds). We need to address that either by rejecting such filenames (slightly not backwards compatible) or escaping such chars in the manifest file."
1004118393,9788,chenyang8094,2022-01-03T14:11:28Z,"> Almost forgot, one more comment - the manifest file format will break if `appendfilename` includes spaces or newlines, which are otherwise valid today (just an sds). We need to address that either by rejecting such filenames (slightly not backwards compatible) or escaping such chars in the manifest file.

I tend to ban it directly. After all, I believe that no one configures it like this in reality."
1004133640,9788,yoav-steinberg,2022-01-03T14:33:07Z,"Hi, I think we need to consider updating _redis-check-aof_ to be able to parse the manifest file and figure out on its own which AOF needs to be checked. Perhaps even pass the `appendonlydir` to it as an argument.
@chenyang8094 Any chance you can look into this and create a separate PR once this is merged?"
1004145545,9788,chenyang8094,2022-01-03T14:53:10Z,"> Hi, I think we need to consider updating _redis-check-aof_ to be able to parse the manifest file and figure out on its own which AOF needs to be checked. Perhaps even pass the `appendonlydir` to it as an argument.
> @chenyang8094 Any chance you can look into this and create a separate PR once this is merged?

Okay, thank you for your reminder."
1004158655,9788,oranagra,2022-01-03T15:12:46Z,"i'd like to merge this big-boy and postpone these improvements to a followup PRs.
i suppose they won't induce any massive change in the existing code.
@chenyang8094 is that ok?"
1004240664,9788,oranagra,2022-01-03T17:24:15Z,":tada: more than 400 comments 2500 LOC, and some 3 months (counting form #9539)
@chenyang8094 thank you for all the time you put in."
1004431811,9788,chenyang8094,2022-01-03T23:46:04Z,"> 🎉 more than 400 comments 2500 LOC, and some 3 months (counting form #9539) @chenyang8094 thank you for all the time you put in.

Thank you, now I am going to deal with the next PR.
"
1019428620,9788,oranagra,2022-01-23T07:08:32Z,"@chenyang8094 can you please look into this failure:
https://github.com/redis/redis/runs/4890129519?check_suite_focus=true
```
*** [err]: AOF will trigger limit when AOFRW fails many times in tests/integration/aof-multi-part.tcl
aof rewrite did trigger limit
```

another one:
https://github.com/redis/redis/runs/4875669104?check_suite_focus=true"
1019443025,9788,enjoy-binbin,2022-01-23T09:08:20Z,"i also saw it in my daily. `DB 9: 13640 keys `
i guess `r config set auto-aof-rewrite-min-size 1mb`, in FreeBSD, sometimes it was so slow, and there are not enought key to tigger the rewrite.

so maybe change it to 1KB, or do a for loop and insert some keys (maybe at least 20000 keys)
"
1019444550,9788,oranagra,2022-01-23T09:17:56Z,"maybe.. 
maybe we just need a longer timeout for this check.

maybe instead of using `start_write_load` we rather just add just enough data manually, and only then start the wait_for_condition.
i suppose someone needs to add more prints and reproduce this so that whatever fix we add we can have some certainty it's gonna solve the problem."
1019480825,9788,enjoy-binbin,2022-01-23T13:02:27Z,"it is easy to reproduce if we delete all the `start_write_load` (in case they generate enough keys), and replace it with some `for loop set k v`, but not let it reach 20000 keys.

like in this commit (https://github.com/redis/redis/commit/7d4f50f21ef23215b100c56fd6c1d2b81360e068), if we change  the 20000 to be smaller, it will fail because not able to touch 1MB. (i can trigger a CI in FreeBSD and see the result, but i think this is the right fix, the `start_write_load` seems a bit fragile). it also speed up the test (total: 32s -> 25s, in my machine. Centos7. 

but the time it should be similar in FreeBSD, because we need to generate that many keys all the time (successful use case too))

ci: https://github.com/enjoy-binbin/redis/runs/4912825635?check_suite_focus=true#step:4:3028

> maybe we just need a longer timeout for this check.

btw, i think it is not enought, the `start_write_load` will last 10s, in this case, i think in thoes case of failures none of them produce enough keys."
1019883342,9788,chenyang8094,2022-01-24T09:20:14Z,I think we can change `1MB` to `1KB` and Then manually construct a 1KB more (which is easy) data to get a definite test status. @oranagra @enjoy-binbin WDYT？
1188920772,11012,ranshid,2022-07-19T11:10:05Z,"@oranagra/@madolson this is following the issue https://github.com/redis/redis/issues/7551 
lots of small changes in this PR, but I think it brings some order onto things.
@oranagra in case the change of bpop --> bstate is problematic I can remake it, but it killed me to look at it :) "
1188926970,11012,ranshid,2022-07-19T11:17:02Z,"Just to report that a week ago I run this change on the daily CI:  https://github.com/ranshid/redis/actions/runs/2656180654
some errors but I am still not sure it is related to this change
"
1188987447,11012,oranagra,2022-07-19T12:24:32Z,"@ranshid thanks (it'll take some time till i'll get to review this).
for the record, i do support renaming `bpop`, i think it's about time.."
1265331774,11012,ranshid,2022-10-03T11:55:17Z,"@oranagra , @madolson , @enjoy-binbin I forced push in order to rebase my version on top of the updated repo (merge would have made some salad of commits) I was unable to accommodate all comments, but most of them have been fixed. 
"
1265571008,11012,oranagra,2022-10-03T14:46:04Z,"@ranshid i don't understand why a merge commit would be a problem. i'd prefer you avoid that next time.
anyway, water under the bridge now, so please tell me which commits are new?
or is the above force-push only pushing an rebase, and no actual new content?"
1265645545,11012,ranshid,2022-10-03T15:30:30Z,"> @ranshid i don't understand why a merge commit would be a problem. i'd prefer you avoid that next time. anyway, water under the bridge now, so please tell me which commits are new? or is the above force-push only pushing an rebase, and no actual new content?

[402e4e0](https://github.com/redis/redis/pull/11012/commits/402e4e0d8de3e7eb2e68172b3be2b421bef3bf6a) 
and 
[c116025](https://github.com/redis/redis/pull/11012/commits/c1160250872e23941217235c8a49e3705a41e9cd)

are the new fixes.
"
1268142945,11012,guybe7,2022-10-05T08:51:52Z,"@ranshid i've reviewed some of this PR but in order to continue please undo some code moving (db.c, blocked.c, remove blocked.h) so the diff won't be as big"
1270456026,11012,ranshid,2022-10-06T17:39:12Z,"I have read the new comments and will address all of them. 
However due to some work related obligations and the holidays I will only be able to work on it next week.
I am sorry for the delay."
1276340048,11012,ranshid,2022-10-12T15:10:09Z,"> @ranshid i've reviewed some of this PR but in order to continue please undo some code moving (db.c, blocked.c, remove blocked.h) so the diff won't be as big

@guybe7 I made some effort to make the diff cleaner. tell me what you think"
1282799135,11012,oranagra,2022-10-18T18:01:29Z,"@ranshid the other PR was merged, please merge unstable into this one,
p.s. discussed it and conceptually approved it in a core-team meeting."
1290461004,11012,ranshid,2022-10-25T12:24:33Z,"> @ranshid the other PR was merged, please merge unstable into this one, p.s. discussed it and conceptually approved it in a core-team meeting.

@oranagra @guybe7 I have committed the merge and some followup commit to fix a small issue from https://github.com/redis/redis/pull/11310

Please review and tell me what you think."
1296935784,11012,guybe7,2022-10-31T11:12:38Z,"@ranshid not sure if it was already discussed, but do we plan to let modules enjoy the new logic? i.e. instead of a module having to provide a `reply_callback`, just reprocess the module command"
1296966277,11012,ranshid,2022-10-31T11:41:20Z,"> @ranshid not sure if it was already discussed, but do we plan to let modules enjoy the new logic? i.e. instead of a module having to provide a `reply_callback`, just reprocess the module command

@guybe7 indeed I discussed it internally with @madolson before I issued this PR (seems like ages ago) and we thought at first stage we will exclude the modules refactor in order to reduce the blast radius. 
I plan to invest more in refactoring but it is getting hard to hold back this current phase :) "
1321089119,11012,oranagra,2022-11-20T10:15:04Z,@ranshid what's the status here? i'm afraid that it'll go stale and be hard to pick back up.
1321090865,11012,ranshid,2022-11-20T10:23:57Z,"> @ranshid what's the status here? i'm afraid that it'll go stale and be hard to pick back up.

@oranagra I do not think there is a new status to report on my side. 
@guybe7 suggested to consider aligning the modules blocking infra to this change, but I do not think we should do that at this point given the magnitude of the change as it is.

aside for that I will go over the open comments, but I do not think there are any open issues to fix."
1321659856,11012,ranshid,2022-11-21T08:17:07Z,@oranagra I merged unstable and checked that local tests are passing. Can we progress with this PR or do you identify something still missing?
1321927344,11012,oranagra,2022-11-21T11:41:39Z,"triggered a full CI
https://github.com/redis/redis/actions/runs/3513572497/jobs/5886551987
some failures seem unrelated."
1322005194,11012,ranshid,2022-11-21T12:38:57Z,@oranagra took your small fixes (aside from one comment I did not understand) as well as place some more points to the top comment. From my pov this is ready to be merged.
1322086194,11012,oranagra,2022-11-21T13:43:30Z,"@redis/core-team please take a look and approve (read the top comment).
specifically the first 3 sections (include interface and behavior changes)

other than the major cleanup, note that it (""blindly"") deletes some big chunks of code that used to process the commands in the unblocked flow. i hope there are no critical differences between these flows and the main ones use when the command didn't had to block.
"
1322230354,11012,ranshid,2022-11-21T15:24:51Z,@oranagra - please note the last commit to handle the race condition in `Blocking XREADGROUP for stream key that has clients blocked on list`
1324702689,11012,soloestoy,2022-11-23T08:30:41Z,"reprocess command by re-call `processCommand`, so the new call may meet errors like `OOM`, acl rule changed, replication stop, disk errors, we should flag it as breaking change I think."
1324824587,11012,oranagra,2022-11-23T10:15:12Z,"> reprocess command by re-call `processCommand`, so the new call may meet errors like `OOM`, acl rule changed, replication stop, disk errors, we should flag it as breaking change I think.

good point, @ranshid please mention it in the top comment as well.
however, i don't think it's really a breaking change, since the client can't really distinguish between a race that results in the command getting blocked and then getting into an error state, or a command that got processed after the error state started.
so in any case the client should expect that error and handle it."
1367100614,11012,ranshid,2022-12-29T06:32:08Z,"@oranagra I updated the top comment.
I think we are ready to go (unless you find some unclosed issue I missed)"
1367232353,11012,oranagra,2022-12-29T10:49:25Z,triggered a full CI: https://github.com/redis/redis/actions/runs/3800019098
1367459136,11012,ranshid,2022-12-29T16:49:51Z,@oranagra I made some small fix to prevent the race condition in the valgrind tests.
1367565363,11012,oranagra,2022-12-29T20:14:56Z,new valgrind CI: https://github.com/redis/redis/actions/runs/3802880131
1368463023,11012,ranshid,2023-01-01T15:02:18Z,"> can you please add some trivial monitor + blocked client test in unit/instospection.tcl? [edit] and make sure the top comment is updated

@oranagra. done"
1368541932,11012,oranagra,2023-01-01T21:36:15Z,Merged! thank you for your patients.
1396442627,11012,KarthikSubbarao,2023-01-19T05:01:11Z,"One small difference I noticed from BEFORE this commit and AFTER this commit.

Before this commit, when a client is killed in the middle of a blocking command (example: BRPOP), we were still tracking the command in INFO command stats as attempted.
```
# Commandstats
cmdstat_brpop:calls=1,usec=15,usec_per_call=15.00,rejected_calls=0,failed_calls=0
```

After this commit, when a client is killed in the middle of a blocking command, we are not tracking it in the command stats.

This difference in behavior may be fine, but it is probably worth documenting."
1396566618,11012,oranagra,2023-01-19T07:52:30Z,"we're aware of that (command stats are updated when the command actually runs / executes it's logic).
the notes at the top include
> One of the changes in this PR is that no command stats are being updated once the command is blocked (all stats will be updated once the client is unblocked). This implies that when we have many clients blocked, users will no longer be able to get that information from the command stats. However the information can still be gathered from the client list.

Added:
> This also means that clients that get killed or timeout while blocked, will not increment the command stats. 

@ranshid let me know if i'm missing something, and feel free to re-edit."
1441482323,11012,sjpotter,2023-02-23T10:01:50Z,"found a possible bug in this when blocking on keys within a module

we always set the flag

`c->flags |= CLIENT_PENDING_COMMAND;`
 
even when blocked on module, which causes redis to reissue the command even after the module has returned data to the user and has unblocked it.

per conversation with @oranagra this seems like a bug, and that line maybe should be protected with a 

`if btype != BLOCKED_MODULE {}`"
1441493241,11012,ranshid,2023-02-23T10:09:54Z,"> CLIENT_PENDING_COMMAND

@sjpotter can you please describe the bug in some more details? 
This flag is cleared one we call moduleUnblockClientOnKey. do you mean that it is not cleared in some other route?"
1441500960,11012,sjpotter,2023-02-23T10:14:11Z,"perhaps, need more investigation, just saw it not being reissued when I commented it out (which isn't the right general fix, but pointed @oranagra in that direction).  will investigate more and get back
"
1441519295,11012,sjpotter,2023-02-23T10:27:40Z,"@ranshid this is what it does is moduleUnblockClientOnKey

```
    if (moduleTryServeClientBlockedOnKey(c, key)) {
        updateStatsOnUnblock(c, 0, elapsedUs(replyTimer), server.stat_total_error_replies != prev_error_replies);
        if (c->flags & CLIENT_PENDING_COMMAND)
            c->flags &= ~CLIENT_PENDING_COMMAND;
        moduleUnblockClient(c);
    }
```

this assumes that that the module replied in the reply_callback.  I'm not, I'm using a thread safe  context to reply outside the reply_callback (my reply_callback always returns ERR, and I handle unblocking external to it).  That's probably why."
1441521355,11012,sjpotter,2023-02-23T10:29:12Z,"so the followup Q is, for modules should this ever be set in the first place?  My guess is that @oranagra instict was correct.  if its auto unset when replied from reply_callback, there's no point to set it in the first place as just breaks people like me who reply/unblock manually elsewhere"
1441540374,11012,ranshid,2023-02-23T10:44:06Z,"@sjpotter, @oranagra . Interesting. So for the simple fix would just be cleaning this flag before/after we issue moduleTryServeClientBlockedOnKey any case (I do not see any problem with clearing this flag as it does not serve modules anyway)

A larger refactor (IMO) would be to make sure this flag is set on ly in the cases the command itself was blocked on keys and require reprocessing. 
Im am currently writing an issue on how we should better improve the blocking infra - so will make sure this idea will get there.
"
1441547150,11012,ranshid,2023-02-23T10:49:33Z,"BTW - I am just wondering regarding your implementation: So when is the module becomes unblocked? 
Or it simply doesn't? as you do not need extra processing?
Sounds like a fragile implementation IMO"
1441574493,11012,sjpotter,2023-02-23T11:11:27Z,"think thread safe contexts.  i.e. when processing outside of main thread.

Currently, just working on a POC and wanted a module ""signal"" when specific keys became ready, so leveraged BlockClietOnKeys, in reality, I'd really want a regular callback that will be called when a specific key is signaled as ready, as BlockClientOnKeys needs a timeout_callback and the current timeout callback callers do not play nice for out of main thread usage, as can be called while processing and unlike reply_callback, it doesn't respect an RM_ERR response (it ignores all return values).  Doesn't significantly harm my POC, just want a more flexible API"
1441602619,11012,ranshid,2023-02-23T11:31:41Z,"Thank you. That is exactly one of the scenarios my upcoming issue is meant to deal with.
@oranagra I can push a PR for that, but testing it is something else... probably blockedclient shuld have some way of failing the reply_callback "
1441608650,11012,oranagra,2023-02-23T11:35:28Z,"you can push a PR to start a discussion, even if it's far from being ready.
but if there are bugs in the current code (for certain module use cases) that are easy to fix, let's do that before 7.2 goes out."
1441662232,11012,sjpotter,2023-02-23T12:16:51Z,"another thing to note in the modules RM_BlockCientOnKeys is that if you Unblock the client manually, like I am doing, it forces timeout_callback to be called (and actually error out if not provided)

```
int RM_UnblockClient(RedisModuleBlockedClient *bc, void *privdata) {
    if (bc->blocked_on_keys) {
        /* In theory the user should always pass the timeout handler as an
         * argument, but better to be safe than sorry. */
        if (bc->timeout_callback == NULL) return REDISMODULE_ERR;
        if (bc->unblocked) return REDISMODULE_OK;
        if (bc->client) moduleBlockedClientTimedOut(bc->client);
    }
    moduleUnblockClientByHandle(bc,privdata);
    return REDISMODULE_OK;
}
```

I'd note that the comment for AbortBlock isn't quite accurate in this case, as callbacks are called

```
/* Abort a blocked client blocking operation: the client will be unblocked
 * without firing any callback. */
int RM_AbortBlock(RedisModuleBlockedClient *bc) {
    bc->reply_callback = NULL;
    bc->disconnect_callback = NULL;
    return RM_UnblockClient(bc,NULL);
}
```"
1441885275,11012,ranshid,2023-02-23T14:24:25Z,"> another thing to note in the modules RM_BlockCientOnKeys is that if you Unblock the client manually, like I am doing, it forces timeout_callback to be called (and actually error out if not provided)
> 
> ```
> int RM_UnblockClient(RedisModuleBlockedClient *bc, void *privdata) {
>     if (bc->blocked_on_keys) {
>         /* In theory the user should always pass the timeout handler as an
>          * argument, but better to be safe than sorry. */
>         if (bc->timeout_callback == NULL) return REDISMODULE_ERR;
>         if (bc->unblocked) return REDISMODULE_OK;
>         if (bc->client) moduleBlockedClientTimedOut(bc->client);
>     }
>     moduleUnblockClientByHandle(bc,privdata);
>     return REDISMODULE_OK;
> }
> ```
> 
> I'd note that the comment for AbortBlock isn't quite accurate in this case, as callbacks are called
> 
> ```
> /* Abort a blocked client blocking operation: the client will be unblocked
>  * without firing any callback. */
> int RM_AbortBlock(RedisModuleBlockedClient *bc) {
>     bc->reply_callback = NULL;
>     bc->disconnect_callback = NULL;
>     return RM_UnblockClient(bc,NULL);
> }
> ```

I do not think this is something that was introduced by this PR. this was the way it was handled before.
@oranagra I can provide a fix , but that usecase is basically wrong IMO.
returning REDISMODULE_ERR from the reply_callback is basically saying you wish to continue blocking."
1442297521,11012,ranshid,2023-02-23T19:11:18Z,"@oranagra @sjpotter  - please take a look at: https://github.com/redis/redis/pull/11832
"
1446178570,11012,guybe7,2023-02-27T11:39:22Z,"@sjpotter i think the way you use `RM_BlockClientOnKeys` - can you please explain why don't you unblock the client from the reply_callback? IIUC you use this API not to block a client, but rather to know when a certain key is ready... which is not the right way to use it. why don't you use the ""new"" keyspace notification?

@ranshid does the fix in #11832 only handles the case where one unblocks a blocked-on-key by using RM_UnblcokClient? or does it handle more common use-case like CLIENT UNBLOCK?"
1446215902,11012,ranshid,2023-02-27T12:09:02Z,"
> @ranshid does the fix in #11832 only handles the case where one unblocks a blocked-on-key by using RM_UnblcokClient? or does it handle more common use-case like CLIENT UNBLOCK?

AFAIK this only applies to client unblocking  by module when blocked on keys.
"
1461742686,11012,sjpotter,2023-03-09T10:22:43Z,"found another bug with blocking commands in a module, when reprocessing them. https://github.com/redis/redis/issues/11894"
1914002903,11012,enjoy-binbin,2024-01-29T05:46:14Z,"It looks like we have this problem #12998

when reprocessing the XREADGROUP BLOCK, we will reset the timeout in blockForKeys (since we will get a new timeout when we unblock a key):
```
void blockForKeys(client *c, int btype, robj **keys, int numkeys, mstime_t timeout, int unblock_on_nokey) {
    dictEntry *db_blocked_entry, *db_blocked_existing_entry, *client_blocked_entry;
    list *l;
    int j;

    c->bstate.timeout = timeout;
```

In this case, should we first determine whether c->bstate.timeout has a value?
```
    if (!c->bstate.timeout)
        c->bstate.timeout = timeout;
```"
1914058975,11012,oranagra,2024-01-29T06:43:43Z,"if we do that, we'll need to make sure to reset `bstate.timeout` in `resetClient()` and `createClient()`.
but maybe it's a better idea to add a client flag and explicitly let the command know that it is being re-processed?"
1914080236,11012,enjoy-binbin,2024-01-29T07:03:23Z,"> but maybe it's a better idea to add a client flag and explicitly let the command know that it is being re-processed?

That's a good idea, I've thought about it before. But I haven't figured out where to check this flag, something like this?
```
        c->flags |= CLIENT_RE_PROCESSING_COMMAND;
        processCommandAndResetClient(c);
        c->flags &= ~CLIENT_RE_PROCESSING_COMMAND;

    if (!(c->flags & CLIENT_RE_PROCESSING_COMMAND))
        c->bstate.timeout = timeout;
```"
1914169133,11012,oranagra,2024-01-29T08:11:40Z,There's a reprocessing flag in `call()`. Set and clear the client flag there
896805203,9357,perryitay,2021-08-11T12:59:14Z,"in this pr I refactored the internal implementation of Redis list in order to add elements larger then 4gb. curretnly redis list is built on quicklist data structure, every node in the quicklist is a ziplist, after my refactor if the the entry is bigger then 4gb it will be added to the quicklist node as is. "
896837837,9357,sundb,2021-08-11T13:41:51Z,"@perryitay I took a cursory look at the code.
1) I'm not sure if we really need to support 4g nodes, such large data should be more suitable for files.
2) In order to support 4g, I think it would be more appropriate to add a new container type for quicklsit, rather than just using `QUICKLIST_NODE_CONTAINER_NONE`, the way you implemented it makes the code confusing."
896866734,9357,oranagra,2021-08-11T14:18:00Z,"@sundb you mean add `QUICKLIST_NODE_CONTAINER_SDS` rather than use `QUICKLIST_NODE_CONTAINER_NONE`?
so what does NONE stands for?
they way i looked at it, sds is not a container of multiple records (unlike ziplist / listpack)

i think we do want to support large nodes in a list, same as we support large ones in hash."
896899896,9357,sundb,2021-08-11T14:58:05Z,"@oranagra Yes, I think QUICKLIST_NODE_CONTAINER_SDS would be more appropriate.
QUICKLIST_NODE_CONTAINER_NONE should only be for future (for now) multi-container support (as you said to me once, and I removed it), just like having quicklist support both ziplist and listpack.
We can dynamically convert its container type in a node, converting the container to SDS when it is larger than a certain threshold."
896928039,9357,oranagra,2021-08-11T15:32:33Z,"@sundb i don't follow you.
IIRC you wanted to completely drop the container field (i.e. `unsigned int container : 2;  /* NONE==1 or ZIPLIST==2 */`), and i said that we better keep is so we can represent a case where one quicklist node is `zliplist` and another one is `listpack`, in that sense `sds` is the 3rd. and note that either one of these can be RAW, or LZF (the `int encoding` field).
But i don't see how this discussion applies on whatever SDS is considered an `SDS` ""container"" or a `NONE` ""container"".
anyway, it's just an enum.. we can change it at any point. (unless i'm completely missing your point)"
897286952,9357,sundb,2021-08-12T01:57:49Z,"@oranagra Yes, unrelated to naming, I left out my ultimate intention.
Because if we modify the code directly with `QUICKLIST_NODE_CONTAINER_NONE` (or `QUICKLIST_NODE_CONTAINER_SDS`), we might end up with a lot of ifs in the quicklist, which would make the quicklist unmaintainable.
I would like to add a layer of adapters, such as `QuicklistContainerSds` or `QuciklistContainerZiplist`, to reduce the number of ifs."
897375047,9357,oranagra,2021-08-12T06:10:03Z,"@sundb i'm not sure i follow you again. are you referring to a common interface (struct with pointers) to reduce the `if`s?
maybe you wanna draft a quick pseudo code example?

I don't think that's really that important, there aren't that many `if`s here.
i think the main complexity of quicklist is about splitting, joining and iterating, inserting and deleting elements inside its containers. but in this case the node is not a container, ti's a plain one element in one node, so the modifications and ""early exit"" `if`s are not reaching the complex parts."
897382485,9357,sundb,2021-08-12T06:26:42Z,"> @sundb i'm not sure i follow you again. are you referring to a common interface (struct with pointers) to reduce the `if`s?
> maybe you wanna draft a quick pseudo code example?

This is what I mean. I tried it during the qucilist ziplist->listpack migration, maybe we can wait for the finished version of @perryitay.

@perryitay, This is a version of my previous implementation.
https://github.com/sundb/redis/blob/listpack-migration-quicklist/src/quicklist.h
https://github.com/sundb/redis/blob/listpack-migration-quicklist/src/quicklist.c
"
897386954,9357,oranagra,2021-08-12T06:37:27Z,"yeah, that's what i imagined (`quicklistContainerType`), i think that case is very different because the work with the container is a lot more complicated (insertBefore, insertAfter, merge, etc).

ironically, this also gives us a good hint that `QUICKLIST_NODE_CONTAINER_NONE` is not a bad name after all..."
950288034,9357,oranagra,2021-10-24T09:04:51Z,@redis/core-team please approve (see the top comment for details)
950290368,9357,oranagra,2021-10-24T09:20:57Z,triggered full daily CI: https://github.com/redis/redis/actions/runs/1377489604
950338877,9357,oranagra,2021-10-24T14:54:00Z,"@perryitay there are a few leaks, please have a look at https://github.com/redis/redis/actions/runs/1377489604 and fix."
951519847,9357,sundb,2021-10-26T03:14:35Z,"@perryitay Missing copy `quicklistNode->container` in `quicklistDup`.
Maybe we should make up for this test.
https://github.com/redis/redis/blob/fddd9a47dc35993627903a80bcb2d3d9826ce017/src/quicklist.c#L1394"
951592259,9357,sundb,2021-10-26T06:08:14Z,"@perryitay I ran a daily CI and found some valgrind issues, and they were reproduced locally, but I haven't found out what's wrong yet.
https://github.com/sundb/redis/runs/4004514189?check_suite_focus=true

The following command can be used to reproduce:
```sh
make valgrind REDIS_CFLAGS='-DREDIS_TEST'
valgrind --track-origins=yes --suppressions=./src/valgrind.sup --show-reachable=no --show-possibly-lost=no --leak-check=full ./src/redis-server test quicklist
```"
951599897,9357,perryitay,2021-10-26T06:22:05Z,@sundb I will take a look on the valgrind issues. thanks !
951631153,9357,sundb,2021-10-26T07:16:25Z,"@perryitay Not sure if you saw my comment (it was hidden).
The problem is caused by the truncation of `size*` to `unsigned int*` in
`ziplistGet(p, &tmp, (unsigned int*) &sz, &longval);`."
958921341,9357,oranagra,2021-11-03T11:01:16Z,@sundb @perryitay what's the status now? i see all comments are addressed.. let me know when this is ready to be merged.
958923658,9357,oranagra,2021-11-03T11:02:22Z,Full CI: https://github.com/redis/redis/actions/runs/1416389471
958944711,9357,sundb,2021-11-03T11:31:13Z,@oranagra LGTM. I have no more comments.
959367519,9357,oranagra,2021-11-03T15:05:04Z,@perryitay there are some walgrind [warnings](https://github.com/redis/redis/runs/4091532336?check_suite_focus=true) for the unit tests. please fix them and we can finally merge this
1036817946,10285,chenyang8094,2022-02-12T00:09:46Z,"I think Module developers will all love this feature, thanks."
1036968056,10285,madolson,2022-02-12T03:04:49Z,"@redis/core-team Please take a look at the top comment and provide feedback about the implementation. There are still a couple of checks failing in the CI, so once those are cleaned up and we have consensus on the finalized implementation I'll ping the group again."
1039955402,10285,madolson,2022-02-15T07:48:07Z,"Core group meeting decisions:
1. We should add a new API to register an apply callback that is executed after all of values have been set during `config set`. This allows simple modules to define just set callbacks, but more advanced used cases should be able to register this apply config. The apply callback should provide at least a list of config names.
2. The current Apply API is likely okay, it makes it straight forward to reason about the values.
3. We didn't fully align on default values. The major points are:
* We would prefer defining default values once, perhaps during registration. This value is what is compared against when doing rewrites.
* We aren't sure if configs should be able to force being included or excluded from a rewrite. "
1039998689,10285,oranagra,2022-02-15T08:34:40Z,"few notes:

i'm not certain the new apply callback must get names of all configs that were changed (it's a complexity i think we can leave out, and let modules that care handle on their own. 
i see two options here: one is to have just one global apply callback per module, and the other is to have an optional apply callback per config (in which case redis can detect that several modified configs had the same apply callback, and call it only once, like we do for our internal configs).

regarding defaults, the important requirements in my eyes are: first that we avoid making something explicit in rewrite so that we can later change default and still distinguish between explicit changes and implicit ones.
and secondly that if we do let the module provide a default value, we must avoid a case where the real default (for startup) could be different than the one used for rewrite, so i think it must be at registration time, and then the module will always get a callback at startup, and can't distinguish between an implicit callback, and a case where the user actually provided a config (i think this is acceptable)

just to mention that we agree that enums need better handling, not just an array of valid strings."
1042680283,10285,sjpotter,2022-02-17T08:14:36Z,"@madolson @oranagra one thing that has come up with our work on RedisRaft is that we want to base some RedisRaft configuration based on redis configuration (tls configuration in this case). It could be nice if one could get the callback as well on general redis ""config set"" commands.

thoughts?"
1042738668,10285,oranagra,2022-02-17T09:25:13Z,"@sjpotter i don't think this should be part of the configuration infrastructure. i think that maybe similarly to `RM_GetServerInfo` and `RM_GetCommandKeysWithFlags`, we should add some API to access configuration (i.e. as an alternative to getting it with `RM_Call(""config"")`.

And we could add add an event / hook like `RedisModuleEvent_ReplicationRoleChanged`, notifying modules about configuration changes.
i.e. not an event per config, but rather a generic `RedisModuleEvent_ConfigChanged` that carries the config name."
1045063189,10285,nichchun,2022-02-18T19:29:29Z,"Few updates here. ARGS is no longer required as an argument to loadex. There's now also an optional apply callback, and a default value that can be specified during registration. Enums now work based off ints (I went with the implicit index implementation.) Renamed RM_ApplyConfigs to RM_LoadConfigs to avoid confusion with new apply callbacks. Also a few documentation updates, and removed '.' restriction in config names. module configs always have a '.', but standardConfigs in the future can as well."
1045254608,10285,nichchun,2022-02-18T22:07:19Z,Note the forcepush was to clean up a bunch of small commits that made up the new features
1047528709,10285,madolson,2022-02-22T08:12:07Z,"Notes from core meeting I took (@oranagra keep me honest):
* We want to try to merge in the common code from module.c into config.c, specifically all the code that does parsing and decoding. The ideal state is that the only difference between module configs and regular configs is the bit of code that ""fetches"" the value out of a config.  This will likely require some refactoring of config.c, since the abstraction is too opaque. Perhaps some of this refactoring can be done independently to speed up the PR. The goal is to minimize the differences between module and redis configs in the long run.
* Basically allow all the config options to be shared between module and redis except for DEBUG and MULTI_ARG
* We ideally want to move the config to a dict for the PR. I think we can do that as a separate PR since it should be small. 
* We should officially support binary strings. We should theoretically support them since everything is being exchanged in a RedisString, but we should codify that and make sure it works with rewrite and configs.
* Enums should be able to support explicit values? Since we want to share infrastructure, we decided to also pass in a list of enum values.
* LoadConfigs probably doesn't need to do the apply, instead applying can be called manually.
* RedisModuleConfigApplyFunc should also take in a RedisModuleCtx and private data. There isn't a clear consensus if we should add guardrails around the context right now. For now assume we don't need them and it can be documented. 

"
1065818550,10285,nichchun,2022-03-12T05:27:38Z,"Did an absolute ton of refactoring here, there's a leak in register enum that I'm still trying to track down, but wanted to get this out."
1066276586,10285,madolson,2022-03-14T02:31:26Z,"@oranagra I think most of the high level comments have been addressed at this point, feel free to take another look if you have time (although there are still some outstanding comments, so you can wait for those to be addressed as well)."
1079852597,10285,oranagra,2022-03-27T06:45:33Z,"triggered daily CI for this branch: https://github.com/redis/redis/actions/runs/2046968711
passed (errors are unrelated).
started one for valgrind with modules: https://github.com/redis/redis/actions/runs/2047368995
passed!"
1082295785,10285,oranagra,2022-03-29T19:33:03Z,"@redis/core-team please approve the API. 
Note the unresolved comment above (currently using char* as error). 

@nichchun please update the top comment about recent changes. "
1082647548,10285,oranagra,2022-03-30T05:49:07Z,"I also prefer `char*`, using a heap allocated string is more burden for the common use case (static errors).
if we stick with `char*` only the ones that need to format a string have more burden.
@yossigo please look at the last commit and state if you agree to revert it.
(if not, we should at least unify some common code)"
1082808322,10285,yossigo,2022-03-30T08:56:44Z,"@oranagra As discussed, I don't feel strongly about it but do think the `RedisModuleString` option is uglier/more boiler plate but also more correct from an API perspective. Pushed a commit to reduce duplication and a small test fix."
1083097315,10285,oranagra,2022-03-30T12:47:36Z,"merged :tada: 
Thank you @nichchun and everyone involved."
1083994080,10285,sundb,2022-03-31T02:16:03Z,"CI failed on alpine. 
https://github.com/redis/redis/runs/5763628464?check_suite_focus=true
https://github.com/redis/redis/runs/5763628410?check_suite_focus=true"
1084074781,10285,madolson,2022-03-31T04:56:17Z,@nichchun ^ 
1576523944,12109,naglera,2023-06-05T10:18:35Z,"Replication buffer memory during bgsave with rdb-channel on vs rdb-channel off. The upper graph shows primary's (in blue) and replica's replication buffer size during bgsave, with `repl-rdb-channel` off.  

![Figure_1](https://github.com/redis/redis/assets/58042354/a17da221-6efd-473f-929a-541d5118d0e1)
## Explanation
- The rdb channel (on the lower graph) allows us to transfer most of the incremental data memory load to the replica.
- After about 15sec, the primary replication buffer start growing, this is the point where the replica start synchronies loading the snapshot into db.
## Test details
- Initialized the primary database with 3GB.
- I used redis-benchmark to continuously set a single key with a random value of 8 bytes.
- The primary's buffer size is measured by mem_total_replication_buffers, and the replica' buffer size by replicas_replication_buffers."
1609319456,12109,soloestoy,2023-06-27T11:32:11Z,"Have you tried using multiplexing to achieve it? The current method of using two channels is a bit too complicated, I think multiplexing would be much simpler and can also solve the problem with PING."
1616490663,12109,naglera,2023-07-02T09:01:42Z,"HI @soloestoy, I have considered multiplexing. Although multiplexing also allows the replication data and rdb to be sent simultaneously, the key point is that using another channel, the child process can write directly to the replica. That completely eliminates the need for a pipeline to redis main process. In addition to removing a lot of pipeline complex code, this will increase the responsiveness of the main process during synchronization.
The design is better explained in https://github.com/redis/redis/issues/11678"
1873314337,12109,naglera,2024-01-01T12:48:04Z,"Hi @oranagra, @madolson, sorry for the delay. I'm still working on previous comments. At the meantime I have some exciting results I want to share. I worked on reviving connset structure and handlers, in order to directly stream online changes from the child process to the replica, without pipeline to main process. Here are some of the results.
## Data
<img src=""https://github.com/redis/redis/assets/58042354/ee08e9b5-5912-40d1-a71f-0a5612ee2219"" data-canonical-src=""https://github.com/redis/redis/assets/58042354/ee08e9b5-5912-40d1-a71f-0a5612ee2219"" width=""600"" height=""400"" />
<img src=""https://github.com/redis/redis/assets/58042354/9935fe5c-fc17-40d9-961b-62ffd2145b2b"" data-canonical-src=""https://github.com/redis/redis/assets/58042354/9935fe5c-fc17-40d9-961b-62ffd2145b2b"" width=""400"" height=""266"" />
<img src=""https://github.com/redis/redis/assets/58042354/c18c1810-1af8-48e1-aefa-4faa0dbbed2c"" data-canonical-src=""https://github.com/redis/redis/assets/58042354/c18c1810-1af8-48e1-aefa-4faa0dbbed2c"" width=""400"" height=""266 "" />

## Explanation 
These graphs demonstrate performance improvements during full sync sessions using rdb-channel + streaming rdb directly from the background process to the replica.

First graph- with at most 50 clients and light weight commands, we saw 5%-7.5% improvement in write latency during sync session.
Two graphs below-  full sync was tested during heavy read commands from the primary (such as sdiff, sunion on large sets). In that case, the child process writes to the replica without sharing CPU with the loaded main process. As a result, this not only improves client response time, but may also shorten sync time by about 50%. The shorter sync time results in less memory being used to store replication diffs (>60% in some of the tested cases).

## Test setup 
Both primary and replica in the performance tests ran on the same machine. RDB size in all tests is 3.7gb. I generated write load using redis-benchmark ` ./redis-benchmark -r 100000 -n 6000000 lpush my_list __rand_int__`.

----

I will soon create a second PR for this change (on top of this PR), to avoid making this PR more complex then it already is. "
1873376624,12109,oranagra,2024-01-01T15:45:15Z,"nice results.
i imagine that we can improve the latency spikes (p99) with the old approach too (split some memory copying to smaller bulks).
and the huge improvement in time is probably due to having redis fully utilizing it's core, and another core is completely free (which may not always be the case, either because redis isn't busy, or because all other core are).
it'll still be an improvement even if these conditions were not true, but probably not that noticeable.

anyway, regardless of this being a 100% improvement or an 10% improvement, it is a good one, and now that it's possible to do (due to the separate channel which i mainly wanted to move the memory to the other end), i'd like to proceed.

but what are the benefits of a second PR?
before we merge this one, the diff will show both of them.
i think we can incrementally review and merge both of them in this one."
1873386977,12109,naglera,2024-01-01T16:17:15Z,I wanted second PR just because this change can live without pipeline removal. Beside making this PR shorter there is no benefit. Lets continue on this PR then.
1918660775,12109,naglera,2024-01-31T08:56:33Z,"Main changes:
- Background process will directly stream RDB into the replicas sockets.
- Revived connection set mechanism.
- Fixed most of the comments.

Still working on:
- Peer replicas connections at the master end.
- Remove feature flag and simplify replica's state machine.
- Test coverage for sync failure scenarios. 
- Refactor replication.c.

TBD:
- Avoid copping master query buffer. 
      In my opinion, this will improve performance, but we can push it into a later patch.
- Send estimated rdb size to replica, and use it to determine buffer limits. 
       Since the estimate is not very accurate, we agreed that this feature is at most nice to have. 

"
2016980828,12109,CLAassistant,2024-03-24T23:08:22Z,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/redis/redis?pullRequest=12109) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you all sign our [Contributor License Agreement](https://cla-assistant.io/redis/redis?pullRequest=12109) before we can accept your contribution.<br/>**0** out of **2** committers have signed the CLA.<br/><br/>:x: amitnagl<br/>:x: naglera<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/redis/redis?pullRequest=12109) it.</sub>"
893596364,9323,eduardobr,2021-08-05T16:29:08Z,"Relates to https://github.com/redis/redis/issues/4624, resolving it in some setups"
915928959,9323,oranagra,2021-09-09T09:41:30Z,"@eduardobr are you gonna finish this? any major issues? IIRC it's only renaming / doc and some minor changes. 
Let me know if you want me to pick it up instead. "
915946186,9323,eduardobr,2021-09-09T10:04:57Z,"> @eduardobr are you gonna finish this? any major issues? IIRC it's only renaming / doc and some minor changes.
> Let me know if you want me to pick it up instead.

@oranagra I can move on, was just waiting for some answer on the questions you raised, so I can finish in one seating.
I’ll apply the suggestions soon.
Thanks"
917631348,9323,eduardobr,2021-09-12T12:57:01Z,"@oranagra Things got quite complicated due to many conflicting changes
Especially related to https://github.com/redis/redis/pull/9356 and https://github.com/redis/redis/pull/9398
So I decided to squash everything then rebase to reduce my cognitive load. The rebase was commited in 2 steps due to the huge amount of changes (5676f0c and ca922aa).

In relation to  ""Delay to discard cached master when full synchronization #9398"", at replication.c `readSyncBulkPayload()`, I repositioned this new block and run it in a specific situation for SWAPDB mode, and another specific situation otherwise (please review). It could be wrapped in a method for reuse, but how to call a routine like this?:
```
       /* Replica starts to apply data from new master, we must discard the cached
        * master structure. */
        serverAssert(server.master == NULL);
        replicationDiscardCachedMaster();

        /* We want our slaves to resync with us as well, if we have any sub-slaves.
        * The master already transferred us an entirely different data set and we
        * have no way to incrementally feed our slaves after that. */
        disconnectSlaves(); /* Force our slaves to resync with us as well. */
        freeReplicationBacklog(); /* Don't allow our chained slaves to PSYNC. */
```

In relation to  ""Slot-to-keys using dict entry metadata #9356"", it hit this PR in some points and even required moving this to server.h:
```
/* State for the Slot to Key API, for a single slot. The keys in the same slot
 * are linked together using dictEntry metadata. See also ""Slot to Key API"" in
 * cluster.c. */
struct clusterSlotToKeys {
    uint64_t count;             /* Number of keys in the slot. */
    dictEntry *head;            /* The first key-value entry in the slot. */
};
typedef struct clusterSlotToKeys clusterSlotsToKeysData[CLUSTER_SLOTS];
```

Additionally, raised question about a missing bit:
Naming for Pre and Post versions of some module events"
917784414,9323,ShooterIT,2021-09-13T02:21:48Z,"For #9356, if you have uncertain things, we can ask @zuiderkwast for suggestions, he is a friendly man! "
917957805,9323,zuiderkwast,2021-09-13T08:24:01Z,"> In relation to ""Slot-to-keys using dict entry metadata #9356"", it hit this PR in some points and even required moving this to server.h
> `struct clusterSlotToKeys [...]`

I'm here to help. :smile: I tried to move everything cluster-related to `cluster.{c,h}`.

Isn't it enough to add `#include ""cluster.h""` in some files? I can't see exactly why you had to move them. Maybe create some new `.h` file (replication.h) which includes cluster.h and defines some new structs..?"
918484591,9323,eduardobr,2021-09-13T18:57:54Z,"> > In relation to ""Slot-to-keys using dict entry metadata #9356"", it hit this PR in some points and even required moving this to server.h
> > `struct clusterSlotToKeys [...]`
> 
> I'm here to help. smile I tried to move everything cluster-related to `cluster.{c,h}`.
> 
> Isn't it enough to add `#include ""cluster.h""` in some files? I can't see exactly why you had to move them. Maybe create some new `.h` file (replication.h) which includes cluster.h and defines some new structs..?

@zuiderkwast Thank you ;)
The reason it was moved is that tempDb struct needs clusterSlotsToKeysData (which is not a cluster specific concept)
tempDb is a broad concept and is used by
- db.c (depends on cluster)
- replication.c (depends on cluster)

If it lived in one of these 2, one of them wouldn't be able to access it.
So keeping in server.h was a better choice.

I'm afraid I'm still not familiar with the code base as a whole and C best practices to know if that was a decent solution though. I believe that can be also a separated commit after the storm of changes and rebases done here if necessary, what do you think?

But most importantly, I believe it would be great to have your review on the correctness of this change after I integrated your PR, for example:
I noticed your change includes this cleanup in db.c `restoreDbBackup`:

```
    /* Restore slots to keys map backup if enable cluster. */
    if (server.cluster_enabled) {
        serverAssert(server.cluster->slots_to_keys->numele == 0);
        raxFree(server.cluster->slots_to_keys);
        server.cluster->slots_to_keys = backup->slots_to_keys;
        memcpy(server.cluster->slots_keys_count, backup->slots_keys_count,
                sizeof(server.cluster->slots_keys_count));
    }
```
into
```
    /* Restore slots to keys map backup if enable cluster. */
    if (server.cluster_enabled) slotToKeyRestoreBackup(&backup->slots_to_keys);
```

Which inlining, is:
`memcpy(server.cluster->slots_to_keys, backup, sizeof(server.cluster->slots_to_keys));`

And that's what I follow here after swapping tempDb with main db:
https://github.com/redis/redis/pull/9323/commits/ca922aab007168f3f5a9d8c6509c9fb2a9cd7f7a

Does it make sense, is the part removed now unnecessary?

"
945356226,9323,oranagra,2021-10-18T04:31:44Z,@redis/core-team please approve or comment. see top comment for details
950334226,9323,oranagra,2021-10-24T14:24:32Z,"@yossigo good catch.
in `emptyDb` we call `signalFlushedDb` which does both `touchAllWatchedKeysInDb` and `trackingInvalidateKeysOnFlush`.

but in `swapMainDbWithTempDb` we call `touchAllWatchedKeysInDb` directly (since we need to pass another argument, and we need to call `trackingInvalidateKeysOnFlush` the same way as `dbSwapDatabases` does."
950371730,9323,eduardobr,2021-10-24T18:27:19Z,"> since we need to pass another argument, and we need to call `trackingInvalidateKeysOnFlush` the same way as `dbSwapDatabases` does.

Oh, actually `dbSwapDatabases` also doesn't have a call to invalidate (but I see in documentation that it's on purpose db numbers are ignored in server-assisted client side caching, so no need for that call there).
Did you mean we need a single call to `trackingInvalidateKeysOnFlush(1)` at the end of `swapMainDbWithTempDb`?"
950376407,9323,oranagra,2021-10-24T18:59:58Z,"i'm sorry.. i must be either blind or drunk.
let me try again 8-)

signalFlushedDb which we skip in this mode calls touchAllWatchedKeysInDb and trackingInvalidateKeysOnFlush,
we can't call signalFlushedDb because we need to pass different arguments to touchAllWatchedKeysInDb, but we should mimic the other things it does, and maybe even add a cross reference note, so that if someone ever updates it, they won't forget to update swapMainDbWithTempDb too.

another thing i see that emptyDb does is flushSlaveKeysWithExpireList.
i suppose we need to do that as well. (and drop a similar comment in emptyDb).

sounds right?"
951897982,9323,eduardobr,2021-10-26T12:39:19Z,"@oranagra I noticed that cluster tests cascade server config from one test file to another, and this change affects the test ""Replica in loading state is hidden"" in the sense that it will run in swap mode (even though 22-replica-in-sync.tcl doesn't specify that).
So, is the configuration leaking though tests on purpose and the fix is to explicitly change ""Replica in loading state is hidden"" to disable swapdb, or the configuration leaking is a bug in the test suite?"
951935933,9323,oranagra,2021-10-26T13:24:26Z,"@eduardobr i'm not sure if that's on purpose, but it is indeed a fact.
the cluster test infra spins up a bunch of serves and runs all test on them sequentially one by one.
you can add a line to the `Cluster nodes hard reset` ""test"" in `tests/cluster/tests/includes/init-tests.tcl` and that will clear that config to the default before each unit.
or alternatively you can make the new `tests/cluster/tests/17-diskless-load-swapdb.tcl` unit clean up after it (maybe this is slightly better since it's not a common config we expect other tests to mess with)"
954642214,9323,eduardobr,2021-10-29T10:44:39Z,"@oranagra I would like to change the description in redis.conf to something like this. Please let me know if it's ok:
```
#      ""swapdb"" - Keep current db contents in RAM while parsing the data directly
#                 from the socket. Note that this requires sufficient memory,
#                 if you don't have it, you risk an OOM kill.
#                 From Redis 7.0, replicas in this mode can keep serving current
#                 data set while replication is in progress, except for cases where
#                 the master can't recognize replica as having a data set from same
#                 replication history.
```"
954687590,9323,oranagra,2021-10-29T12:04:13Z,"The description for redis.conf seems good, I'd suggest two changes:
1. Since this is redis.conf that ships with Redis 7, i don't think we need to mention what this meant before. For example for new configs we add in which version they where added. 
2. It seems odd to me that the text implies that the master is the one that makes the decision if the replica serves reads during loading. "
954723391,9323,eduardobr,2021-10-29T12:59:02Z,"Thanks, then maybe:
```
# ""swapdb""      - Keep current db contents in RAM while parsing the data directly
#                 from the socket. Replicas in this mode can keep serving current
#                 data set while replication is in progress, except for cases where
#                 they can't recognize master as having a data set from same
#                 replication history.
#                 Note that this requires sufficient memory, if you don't have it,
#                 you risk an OOM kill.
```"
954805959,9323,oranagra,2021-10-29T14:49:41Z,"LGTM, please add a commit. "
957257038,9323,oranagra,2021-11-02T09:24:14Z,"triggered full CI on the new tests:
https://github.com/redis/redis/actions/runs/1411604214
https://github.com/redis/redis/actions/runs/1411604447"
957314621,9323,oranagra,2021-11-02T10:33:34Z,"looks like we have a few possible issues.
1. `Diskless load swapdb (different replid): new database is exposed after swapping` fails to terminate one of the servers, and this ends up reporting an [empty valgrind](https://github.com/redis/redis/runs/4078027384?check_suite_focus=true) report after doing `Forcing process 7292 to exit`
2. some [sporadic failures](https://github.com/redis/redis/runs/4078027974?check_suite_focus=true) in the module test (possibly timing issues) see below:

```
*** [err]: Diskless load swapdb RedisModuleEvent_ReplAsyncLoad handling: during loading, can keep module variable same as before in tests/unit/moduleapi/testrdb.tcl
Expected '510' to be equal to '2000' (context: type eval line 9 cmd {assert_equal [$replica dbsize] 2000} proc ::test)
*** [err]: Diskless load swapdb RedisModuleEvent_ReplAsyncLoad handling: when loading aborted, can keep module variable same as before in tests/unit/moduleapi/testrdb.tcl
Expected '510' to be equal to '2000' (context: type eval line 9 cmd {assert_equal [$replica dbsize] 2000} proc ::test)
```"
958104160,9323,eduardobr,2021-11-02T19:32:07Z,"What I could think of, delaying server termination is the async flushing of temp db after the test on successful replication. I reduced the replica amount of keys here to accelerate that.
Do you think we could have any issues terminating servers, or the valgrind test is just very slow?
Btw, got this on last run, seems unrelated to the PR:
```
*** [err]: Tracking invalidation message of eviction keys should be before response in tests/unit/tracking.tcl
Expected '0' to be equal to 'invalidate volatile-key' (context: type eval line 25 cmd {assert_equal $res {invalidate volatile-key}} proc ::test)
```
https://github.com/redis/redis/runs/4084589167?check_suite_focus=true"
958108818,9323,oranagra,2021-11-02T19:38:47Z,"that failing tracking test is unrelated (unstable is broken, still trying to figure it out).
regarding the other failures i reported, maybe you can try to reproduce them first and then try to figure out what's wrong (i haven't looked myself yet)"
958112700,9323,oranagra,2021-11-02T19:44:34Z,"i triggered a re-run of the tests (accessible in the same links i posted above)
p.s. valgrind can be very slow, up to 100x slower. but the server doesn't wait for the lazy free queue when terminating."
958157596,9323,oranagra,2021-11-02T20:52:44Z,second run came back clean (the valgrind errors it has are ones already solved in unstable)
958162337,9323,eduardobr,2021-11-02T21:00:15Z,"> second run came back clean (the valgrind errors it has are ones already solved in unstable)

Though I could reproduce in my machine that the actual time it's taking to terminate the server is the RDB being flushed to disk. Changing keys to 200 ""fixed"" because it's faster, but I'll making a commit with $replica config set save """" for no delay."
960559883,9323,oranagra,2021-11-04T08:47:24Z,"@eduardobr merged :tada:
Thank you for your patience."
960560741,9323,eduardobr,2021-11-04T08:48:52Z,"> @eduardobr merged 🎉
> Thank you for your patience.

Thank YOU for the patience 😁"
962561824,9323,oranagra,2021-11-07T07:01:38Z,"@eduardobr i noticed some sporadic timeout errors in the valgrind runs
https://github.com/redis/redis/runs/4123007165?check_suite_focus=true
```
Diskless load swapdb (different replid): replica enter loading
```
and
```
Diskless load swapdb (async_loading): old database is exposed while async replication is in progress
```
they did not reproduce they day before / after so it might be nothing.
maybe you wanna look into that, or we can wait to gather more evidence."
964834630,9323,oranagra,2021-11-10T06:50:22Z,"another timeout in the test under valgrind:
https://github.com/redis/redis/runs/4159338778?check_suite_focus=true
```
[TIMEOUT]: clients state report follows.
sock55c4ba1cc3c0 => (OK) Diskless load swapdb (async_loading): old database is exposed while async replication is in progress
```"
965508778,9323,oranagra,2021-11-10T16:23:13Z,"i think the timeout is not a fault of the tests, hope it'll get solved by https://github.com/redis/redis/pull/9767"
874587207,9166,oranagra,2021-07-06T08:58:57Z,"@ShooterIT thank you for this PR.
I've briefly reviewed the code, trying to sum up what i understand:
1. the server struct has a mechanism similar to the reply list that exists in every client, this one has a refcount in each node indicating how many replicas are sharing that node.
2. a replica client can also have a normal (private) reply list, which is normally used on successful psync.
3. each replica client tracks how many bytes in the shared reply list are meaningful for it (in addition to tracking it's own reply buffers size).
4. when considering the obuf limits disconnection, we look at both the private and shared memory (similar total as before this PR).
5. when considering the total memory used for replication in the server (e.g. for eviction and info), we count the shared memory just once (counting actual memory usage).

My thoughts:
We have a long term plan to (almost) completely eliminate the slave buffers, see: https://github.com/redis/redis/pull/8440#issuecomment-771623319
Had this PR provide a significant improvement with a small price of complexity, i would have merged it right away..
but considering the added complexity, and considering the long term roadmap, i think we may wanna avoid it.

Note that the above mentioned roadmap plan doesn't indeed solve a case where a sudden traffic spike on the master accumulate a large replica output buffer on multiple repliacs (your PR will mitigate that), but i'm not sure that justifies the complexity price.

Note that other cases (slow replicas that can't catch up with the master traffic) can just be considered bad configuration.

@redis/core-team @yoav-steinberg feel free to argue or share additional thoughts."
874641846,9166,ShooterIT,2021-07-06T10:21:27Z,"Thanks for your review @oranagra i want to argue for my PR😜
For your plan, i ever tried to design, there are some problems i encountered, please correct me if i am wrong.
- Replica stores output buffers during the RDB transfer, we need to reserve this big size memory for developing every instance, because the buffer still exist when finishing loading entire RDB, and master may change into replica, replica also may change into master. So i think we can use this reserved memory only on master.

- On full synchronization with multiple replicas, every replica uses one output buffer, for total memory used in one shard, this method still cost much memory.

- You already said, your plan still has this bad case, actually, we often encounter it, because some replicas may be thousands of kilometers away from replicas, network sometime is not unstable. 
  > Note that the above mentioned roadmap plan doesn't indeed solve a case where a sudden traffic spike on the master accumulate a large replica output buffer on multiple replicas.

- Send replication stream during transferring RDB, for diskless replication, there may be more COW memory use because child process exists more time.

- Multiplexing replication packet is much complicated, replicas need to distinguish different packet types and install different read/write handlers. especially for diskless load solution, i think it is more complicated. Moreover, we need to concern compatibility for syncing with old version instance.

- It will be not easy to many replicas share one `RDB` on disk-based replication, because we send  replication output buffer ASAP for every replication, new full sync replicas can't copy old replicas' output buffer.

For this PR, i have another idea if we allow, we can discard replication backlog memory and use global shared replication buffer to implement replication backlog mechanism, replication backlog just is a consumer of replication buffer, this also may save some memory, and saving memory copy because we just increase refcount of some node when one replica hits replication backlog content. Moreover, replication backlog size may be the biggest size of replication buffer that is kept by slow replicas."
874830933,9166,oranagra,2021-07-06T14:52:55Z,"I don't have answers to everything, i'll try to respond to the parts i can..

* regarding reservation of memory on the replica machine, i think this is better than having to reserve that memory on the master machine. one case that's easy to reason with is a case where the machine hosting redis (master) is running low of memory (because the master grew), and now you wanna migrate that master to another machine, so we set up a bigger machine, but we can't get the data out of the old machine, since there's not enough memory on it to host the replica buffers and CoW. if we move the replica buffers to be a problem of the replica machine, the admin can allocate enough memory and save the day.
* for multiple replicas, they're usually on multiple machines, and each should have enough spare, it's right that if we had hosted the replica buffers on the master and share them we can save the total memory, but when the replica is promoted it'll have to have that spare too, and considering a replica per machine, that's the same total.
* i think the admin should make sure there's enough memory on the machine to support these spikes, or we can throttle them. i agree it's better to share memory if we can, it's just a matter of complexity vs gain.
* yes indeed it can slow down replication and cause more COW.. but on the bright side it save the replica buffers, not sure if that's not overall beneficial.
* first, we don't really need to support old replicas, we don't do that in the rdb format either.. a replica must never be of an older version than the master. however, we may still want to use some `replconf capa` mechanism to switch this off for compatibility with other tools that pose as a redis replica.
* yes, disk-based replicas will not be able to join an ongoing fork in that mode.. same as we have with diskless.

i also thought about that idea of using the shared output buffers for replication backlog.. basically saying that refcount of 0 is still not freed in some conditions (up to a certain size).

overall, this PR is certainly useful... my concern is about the extra complexity it adds and whether of not it is needed if we had the other solution in place.. i.e. imagine a case where the other solution is already implemented, then the problem this PR comes to solve is not really that painful.

i'll try to give it another look, maybe we can simplify it, or maybe it isn't as complex as i think it is.."
891135723,9166,ShooterIT,2021-08-02T15:52:26Z,"Yes, actually, this mechanism is not complex, i also lightly refactor `writeToClient` which is a bit long. If no tests and refactor, there are less changes.

I must acknowledge that reservation of memory on the replica machine is better than on the master because master data safety is much more important, especially, CoW expense also is on master. In fact, i think my this PR doesn't conflict with your solution(sending RDB and replication buffer by multiplexing), your solution may mitigate the risk of OOM on full synchronization. My PR aims to reduce total memory for all replicas' output buffer, when there are many replicas and output buffer is accumulated since of slow network or waiting RDB finished.

We always hope the consuming memory of redis is predictable and controllable after setting maxmemory. We general set memory quota of redis as 1.5~2 multiple of maxmemory (CoW, Replication backlog, replicas output buffer) in our deployment, but redis may eat too much memory when more replicas and worse network. Reserving more memory is costly, but not adding memory is risky in this case. In some other disk storage services, such as MySQL, there is only one binlog for replication whatever how many replicas, actually, copying writing commands to every replica output buffer also is writing amplification.

In future, for replication backlog, i want we could regard the shared replication buffer as replication backlog if replication backlog is less than shared replication buffer because copying replication buffer is very light, i.e. if replication backlog is 100MB but replicas output buffer limit is 1GB, due to that one replica is slow and keep the replication buffer to 1GB, so another  replica could start partially synchronization when reconnecting even its offset gap is more than 100MB with master, of course, offset gap should be not more than 1GB."
900175350,9166,oranagra,2021-08-17T10:21:47Z,"@ShooterIT we discussed this PR in a core-team meeting and decided we wanna proceed.
I didn't review it in detail, and i'm not certain what's the current state (i did take a brief look to realize the concepts).
I suppose we'd want to also proceed with using this for the replication backlog too (unless you think it's a bad idea to add it now, and prefer to add it in a followup PR).
please let me know what you think the next step is... if you want to do some refresh and add the replication backlog into the mix, or you think it's ready for a detailed review.
thanks."
901225681,9166,ShooterIT,2021-08-18T15:48:07Z,"thanks @oranagra Wow! This branch has some conflicts with unstable branch, i need to resolve them, I think current code already implemented the function that all replicas use one global shared replication buffer.
 I want to apply this mechanism to the replication backlog too, but initially, to make it easy to review, i prefer to make another PR to do that, in fact, i didn't implement it yet. if you want me implement them in this one PR, i also feel fine."
901272123,9166,oranagra,2021-08-18T16:53:14Z,"my hunch is that this is better done together, it'll probably impose a few changes on the current mechanisms.
but if you feel that it'll be too complicated to review and reason with in one go, i'm ok to do it later. "
915168325,9166,ShooterIT,2021-09-08T11:54:00Z,Generally it is ready to review and update top comment @redis/core-team do you have any thought?
926420330,9166,ShooterIT,2021-09-24T07:48:33Z,"There are more 150 conversation, it is not easy to review, so i tried to resolved conversation ASAP, maybe i want to do it later. if i forget and it is important, please let me know."
926459415,9166,oranagra,2021-09-24T08:50:58Z,"think it's ok to mark comments as resolved as soon as you address them on your local working copy (or even put a TODO in the code). specifically if the comment is trivial suggestion.
i think this is important for you to be able to keep track of what you handled and what's still pending.

i think it would also be a good idea to comment or add an emoji reaction indicating that you decided to take it or skip it.
since emoji don't trigger notification, that's a good idea to use them when the suggestion is trivial or non-critical.
but if you decide to reject a more complicated one, it's best to comment back as to why, so we can have a discussion and make sure there's no misunderstanding..

thanks for the great work on this PR.
i wanna hammer it and merge it quickly before it gets outdated or we all lose focus and go elsewhere.
it was pending for quite a while, but now that we picked it up, let's keep pushing to be merged soon."
943085727,9166,oranagra,2021-10-14T07:23:53Z,triggered full CI to see that there are no leaks and flaky tests: https://github.com/redis/redis/actions/runs/1340763843
943197172,9166,oranagra,2021-10-14T09:50:34Z,"there are many failures in the triggered CI that are unrelated.
but this one (happens in valgrind) is a timing issue in the new tests:
```
*** [err]: Replication backlog memory will become smaller if replicas disconnect in tests/integration/replication-buffer.tcl
Expected [s repl_backlog_histlen] > [expr 2*10000*10000] (context: type eval line 2 cmd {assert {[s repl_backlog_histlen] > [expr 2*10000*10000]}} proc ::test)
```"
943211073,9166,ShooterIT,2021-10-14T10:08:10Z,"yes, i found that, and i noticed `replication-buffer` costed 188s, so i suspected `replica2` finished  full synchronization.
`assert_equal [$master debug digest] [$replica1 debug digest]` is slow.
let me increase `rdb-key-save-delay` to make dumping rdb longer.
i will do that ASAP.
"
945438580,9166,oranagra,2021-10-18T07:22:26Z,"@redis/core-team this one is finally ready for merge, please approve.
see full details about the design, and implications in the top comment."
950570406,9166,oranagra,2021-10-25T06:25:33Z,"@ShooterIT finally merged.. thank you for proposing, coding, and following my comments."
953506603,9166,oranagra,2021-10-28T05:08:30Z,"@ShooterIT can you please have a look at this test failure (valgrind / timing):
https://github.com/redis/redis/runs/4028904569?check_suite_focus=true
```
*** [err]: Replication backlog memory will become smaller if disconnecting with replica in tests/integration/replication-buffer.tcl
Expected [s repl_backlog_histlen] > [expr 2*10000*10000] (context: type eval line 2 cmd {assert {[s repl_backlog_histlen] > [expr 2*10000*10000]}} proc ::test)
```"
1088062118,10517,madolson,2022-04-04T22:10:38Z,"Right now slot migration is entirely administrator driven, where they execute a lot of commands and handles reconciliation of failures. This PR definitely does a lot to make this more ""managed"", however I still believe the fundamental gap is that slot migration should just be exposed as an atomic operation to admins. 

Replicating ""setslot"" also introduces the awkward situation where there are now two competing sources of truth for slot ownership, the first from the clusterbus and the second from the replication stream. It might make some sense to have a shard own its slot, and then passively observe the state changes from other shards."
1088216640,10517,PingXie,2022-04-05T02:56:58Z,"> Right now slot migration is entirely administrator driven, where they execute a lot of commands and handles reconciliation of failures. This PR definitely does a lot to make this more ""managed"", however I still believe the fundamental gap is that slot migration should just be exposed as an atomic operation to admins.

The ""atomicity"" (for lack of a better term) is a nice thing to have but I am not sure if it is truly the core problem or it could be dealt with separately. In other words, I wonder if there is a way to solve the two key issues in slot migration (no replication of migration states and the fragility in the slot finalization logic) without changing the current slot migration protocol.

> Replicating ""setslot"" also introduces the awkward situation where there are now two competing sources of truth for slot ownership, the first from the clusterbus and the second from the replication stream. It might make some sense to have a shard own its slow, and then passively observe the state changes from other shards.

I am guessing ""setslot node"" is the one causing some uneasiness and I agree ""setslot node"" is not strictly required. ""setslot migrating"" and ""setslot importing"" on the other hand should be fine.

I added you to the main thread for #6339. I will also think some more about your proposal on #2807.  
"
1093605831,10517,PingXie,2022-04-09T02:36:04Z,"@zuiderkwast @madolson I updated the PR to enforce the ordering of `SETSLOT NODE` as follows

1. Client `C` issues `SETSLOT n NODE B` against node `B`
2. Node `B` replicates `SETSLOT n NODE B` to all of its replicas, such as `B'`, `B''`, etc
3. On replication completion, node `B` executes `SETSLOT n NODE B` and returns control back to client `C`
4. The following steps can happen in parallel
- Client `C` issues `SETSLOT n NODE B` against node `A`
- node `B` gossips its new slot ownership to the cluster including `A`, `A'`, etc

Where `A` is the source primary and `B` is the destination primary.

This PR is now complete and it handles the two core issues as we discussed earlier (no replication of migration states and the fragility in the slot finalization logic). This PR should be compatible with the existing slot migration protocol. Thoughts?"
1097762893,10517,zuiderkwast,2022-04-13T09:10:52Z,"I like this change and I think it can be merged regardless of the other plans for CLUSTER MIGRATE, because it improves the current migration process, which we can't deprecate any time soon anyway. The change is backwards compatible with old client applications and old Redis nodes. Old replicas would simply error (ignore) the replicated SETSLOT.

What it doesn't solve is a migration interrupted by a failover. The migration doesn't continue automatically. It can be handled by `redis-cli --cluster fix` which can continue the migration (which it can do even without this PR with some guesswork and using CLUSTER COUNTKEYSINSLOT, but now with less guesswork when there is one importing and one migrating node).

The problem is that there's still manual administration required in this case. If we can do anything to automate this scenario? For example, can we make `redis-cli --cluster rebalance` detect a failover during migration and automatically continue the migration after a failover?"
1098289333,10517,PingXie,2022-04-13T17:09:13Z,"
> What it doesn't solve is a migration interrupted by a failover. The migration doesn't continue automatically. It can be handled by `redis-cli --cluster fix` which can continue the migration (which it can do even without this PR with some guesswork and using CLUSTER COUNTKEYSINSLOT, but now with less guesswork when there is one importing and one migrating node).
> 
> The problem is that there's still manual administration required in this case. If we can do anything to automate this scenario? For example, can we make `redis-cli --cluster rebalance` detect a failover during migration and automatically continue the migration after a failover?

I think both `cluster fix` and `cluster rebalance` are good callouts. I can certainly help improve redis-cli after this PR is merged."
1111435763,10517,zuiderkwast,2022-04-27T20:11:51Z,"Just noticed: When `CLUSTER SETSLOT ... MIGRATING|IMPORTING` is replicated, until now this was backward compatible, because replicase used to ignore failing commands, but with #10504 included in Redis 7, replicas can (optionally, configurable) panic on replication errors. This means that this feature is not compatible with a replica running Redis 7.0 if it's configured to panic on replication errors."
1111760933,10517,PingXie,2022-04-28T05:27:01Z,"> Just noticed: When `CLUSTER SETSLOT ... MIGRATING|IMPORTING` is replicated, until now this was backward compatible, because replicase used to ignore failing commands, but with #10504 included in Redis 7, replicas can (optionally, configurable) panic on replication errors. This means that this feature is not compatible with a replica running Redis 7.0 if it's configured to panic on replication errors.

@zuiderkwast it looks to me that replicas would panic on WRITE commands only (if there was a disk error). I turned on `MAY_REPLICATE` for `SETSLOT` but left `WRITE` out so I think the change should still be compatible?

PS, now that 7.0 has GA'd, I will get my shard ID change in shape next. There are a few places in this PR where I need to detect the same-shard relationship and I think they could benefit from the shard ID change."
1114315327,10517,PingXie,2022-05-01T19:21:40Z,FYI. The latest push includes shard id changes from #10536. I will rebase once #10536 is merged.
1114550639,10517,PingXie,2022-05-02T06:53:51Z,FYI @yossigo @oranagra - with this change both 20\* and 21\* cluster migration tests pass consistently without further modifications. Also I believe there is a way to extend this solution to achieve a forkless solution for #2807 as well though I haven't started yet. Curious to hear your thoughts on this approach. Let me know if there is anything I can help to make some progress on this PR.
1239175688,10517,zuiderkwast,2022-09-07T09:58:47Z,"I'd like to see these incremental improvements to slot migration in 7.2, since I don't expect atomic slot migration to be ready soon enough. @redis/core-team WDYT?"
1241111383,10517,madolson,2022-09-08T19:02:00Z,"I agree about getting these improvements in 7.2, we don't really have much of a concrete plan for when 7.2 is going to launch."
1242413656,10517,PingXie,2022-09-09T20:09:14Z,"Please hold off on the review of this PR. #10536 is the prerequisite to this PR and it has gone through some churns recently. As soon as #10536 gets merged, I will rebase this PR and ping the thread for a review.  Stay tuned..."
1328357194,10517,PingXie,2022-11-27T22:37:36Z,PR is ready for review
1422297639,10517,zuiderkwast,2023-02-08T09:32:27Z,"Ping @redis/core-team to make sure this isn't forgotten.

During slot migration, a master is a SPOF, so a master crash during slot migration is quite bad. This PR solves that in a fairly simple way. Two test suites that were previously disabled and wrapped in `if (false) {...}` are now stabilized and can be enabled again."
1423080666,10517,madolson,2023-02-08T18:45:02Z,"I pinged @PingXie offline to see if he was around to address feedback now. Otherwise I was focusing on other PRs. I've lost context, so would rather get this done and merged quickly. "
1423100417,10517,PingXie,2023-02-08T19:03:11Z,Hey @madolson sorry I missed your offline message. I am around to discuss this PR. I will also need to page in the context as well :-)
1424985397,10517,PingXie,2023-02-09T23:45:37Z,"Thanks for the review, @oranagra @madolson. Let me resolve the merge conflicts and revert all white space changes first and then start addressing your comments. I think I also need to fix my editor settings too to not automatically stripping the trailing white spaces."
1449132068,10517,madolson,2023-03-01T00:30:16Z,"So, we discussed in the core meeting, and we are all generally not super happy with the fake-synchronous replication. So we would prefer one of the two following alternatives:
1. Allow set slot to support a mode where it is not applied locally but only replicated. The redis-cli can then send this command during finalization. It can then wait after it. Once the command the wait is returned, the normal set-slot can be sent. 
2. Implement the logic I suggested, where if the node is a primary with an importing slot, and the source that it was supposed to be importing from is no longer importing the data, finalize the slot and take ownership of it. This may have transient periods where the slot isn't being served if the source was unavailable, but as soon as the source shard is up again the slot will finalize itself."
1455297802,10517,PingXie,2023-03-06T01:35:37Z,"Between the two, my preference is option 1 for its (more) determinism. I need to think a bit on how to achieve it but at a high level, I am imagining that I would introduce a new command to replicate the state to the replicas and then a second command to finalize the slot ownership on the primary. The second command would be the command that we are using today, for the backcompat reason. If the caller misses the first command, they are still susceptible to the SPOF that brought us here in the first place but functionality wise it should still work. Does this sound good to everyone?

> 1. Allow set slot to support a mode where it is not applied locally but only replicated. The redis-cli can then send this command during finalization. It can then wait after it. Once the command the wait is returned, the normal set-slot can be sent.
> 2. Implement the logic I suggested, where if the node is a primary with an importing slot, and the source that it was supposed to be importing from is no longer importing the data, finalize the slot and take ownership of it. This may have transient periods where the slot isn't being served if the source was unavailable, but as soon as the source shard is up again the slot will finalize itself.

"
1456844572,10517,zuiderkwast,2023-03-06T19:39:27Z,"Sounds good to me.

What would this command be? CLUSTER SETSLOT REPLICAS? You call this on the dst node, then WAIT, then CLUSTER SETSLOT NODE?

First, I was going to write that I like option 2 because it requires no changes to the admin procedure (not everyone is using redis-cli), but it not perfect that it can leave the slot without owner for a while. Btw, are 1 and 2 mutually exclusive? If we can have both, even better. With 2, a cluster can self-heal if it gets in this state (for example due to admin not using the new command)."
1456853190,10517,PingXie,2023-03-06T19:44:52Z,"
> What would this command be? CLUSTER SETSLOT REPLICAS? You call this on the dst node, then WAIT, then CLUSTER SETSLOT NODE?

Yep - something along that line. I don't have a name yet. I will certainly consider this name but also open to other suggestions.

> If we can have both, even better. With 2, a cluster can self-heal if it gets in this state (for example due to admin not using the new command).

That is a really great point, @zuiderkwast! Will definitely give a try next.
"
1506098802,10517,PingXie,2023-04-12T23:36:19Z,"@madolson @zuiderkwast @oranagra 

I have updated the PR to include both recommendations from the core team

1. Introduced a new variant of `CLUSTER SETSLOT` , i.e., `CLUSTER SETSLOT <n> NODE <node-id> REPLICAONLY` that finalizes the slot ownership on the replicas only and returns the number of replicas on which the finalization succeeds. 
2. Added logic to automatically take over a slot that is in the importing state but is no longer owned by the original primary. Also made sure that the logic can handle a case where the slot is explicitly assigned to another shard. 


Let me know what you think. I'd like to close on the high level idea and implementation first before addressing the remaining comments in the test code (i.e., rewriting them using the new test framework) and patching up redis-cli.
"
1513623267,10517,PingXie,2023-04-18T18:30:49Z,"> doesn't the new approach require some changes in redis-cli?

I am hoping to get a quick review on the engine changes first and make sure we are aligned directionally. I will make the changes in redis-cli next."
1517326665,10517,PingXie,2023-04-21T06:23:10Z,@oranagra @madolson @zuiderkwast I have patched up redis-cli. I would like to request a review from you guys on the production code. I will be refactoring the test code in parallel and my plan is to move 29-*.tcl over to the new cluster test framework under tests/unit/cluster. Please let know if this is aligned with your thoughts as well. Thanks!
1521173689,10517,PingXie,2023-04-25T05:29:45Z,"FYI. I have removed 29-*.tcl and rewritten the tests in `tests/unit/cluster/slot-migration.tcl`. All open comments should've been addressed by now. 

BTW, CI/build-debian-old has been failing outside this PR as well. Is this a known issue?

```
Err:9 http://deb.debian.org/debian oldoldstable/main amd64 Packages
  404  Not Found
```
"
1521251173,10517,oranagra,2023-04-25T07:00:08Z,"We are aware of the debian CI issue (unrelated to this PR).
Madelyn is currently off the grid, i suppose she'll review it when she's back."
1559665412,10517,oranagra,2023-05-23T15:26:22Z,"we discussed this in a core-team meeting, we're afraid to merge this now to 7.2 for fear of causing stability issues.
it should get merged right after 7.2 is decoupled from the unstable branch."
1561766885,10517,madolson,2023-05-24T18:49:04Z,"> we discussed this in a core-team meeting, we're afraid to merge this now to 7.2 for fear of causing stability issues.
it should get merged right after 7.2 is decoupled from the unstable branch.

I think we have more time than we originally thought to merge this, so I think we should at least try. 8.0 might be quite a bit aways."
1562272371,10517,oranagra,2023-05-25T05:03:00Z,"@madolson if you feel comfortable, go ahead..."
1562765856,10517,zuiderkwast,2023-05-25T11:48:42Z,"This isn't really a new feature. It solves some stability and consistency issues in the slot migration which can cause data loss, so IMO it is better to merge it than to wait."
1582421264,10517,zuiderkwast,2023-06-08T11:35:24Z,"> I think we have more time than we originally thought to merge this, so I think we should at least try.

@madolson Will you try? :)"
1606595817,10517,PingXie,2023-06-26T04:48:51Z,"> Big open question for me is still the thinking around replicating open slots as a burst of commands after the full sync.

@madolson and I sync'ed up offline. I will look into replicating the open slots in the RDB itself next."
1608924434,10517,PingXie,2023-06-27T07:11:07Z,"> > Big open question for me is still the thinking around replicating open slots as a burst of commands after the full sync.
> 
> @madolson and I sync'ed up offline. I will look into replicating the open slots in the RDB itself next.

We had one more discussion and agreed that it is a correct solution to replicate the open slot states via bursting of commands. Replicating the states via the RDB would be a bigger change with higher risks. 

I have incorporated all other feedback. It is a go from my end. @madolson FYI"
1609787384,10517,madolson,2023-06-27T15:47:04Z,Conceptually approved during the core meeting if we can get it merged this week. @soloestoy will also take a look.
1610649738,10517,soloestoy,2023-06-28T03:54:13Z,"didn't look deep into the codes, but to be honest, I don't like using the replication stream to propagate MIGRATING/IMPORTING status, this makes the replication stream muddier.

IIRC, we said that we need to eliminate unnecessary replication and separate non-data propagation from the data replication stream, so that the replication stream only contains data, or in other words, we can distinguish which commands are data and which are non-data via multiplexing. Therefore, before that, we should avoid adding more MAY_REPLICATE type commands.

And, I don't see a strong reason that we have to use the replication stream, I think the cluster bus can achieve it well, we can send `CLUSTERMSG_TYPE_PING`(by adding a new extended `clusterMsgData`) to replicas to notify them of the change in MIGRATING/IMPORTING status, and replicas can reply with `CLUSTERMSG_TYPE_PONG` to ack. "
1610921787,10517,PingXie,2023-06-28T07:38:57Z,"> didn't look deep into the codes, but to be honest, I don't like using the replication stream to propagate MIGRATING/IMPORTING status, this makes the replication stream muddier.
> 
> IIRC, we said that we need to eliminate unnecessary replication and separate non-data propagation from the data replication stream, so that the replication stream only contains data, or in other words, we can distinguish which commands are data and which are non-data via multiplexing. Therefore, before that, we should avoid adding more MAY_REPLICATE type commands.

@soloestoy can you please provide some more context on the ""multiplexing"" part? I would like to better understand your concern on mixing data with non-data in replication. Is it more about the amount of non-data replication or the mere existence of non-data replication? if it is the former, it's worth noting that this replication is only necessary when a slot migration is initiated. 

> 
> And, I don't see a strong reason that we have to use the replication stream, I think the cluster bus can achieve it well, we can send `CLUSTERMSG_TYPE_PING`(by adding a new extended `clusterMsgData`) to replicas to notify them of the change in MIGRATING/IMPORTING status, and replicas can reply with `CLUSTERMSG_TYPE_PONG` to ack.

There are a few reasons why it is not a good idea to route this to the cluster bus:

First, the cluster message size will increase significantly since we need to include the to/from node-ids. Additionally, imagine that there are tens or hundreds of slots being moved at any given time on different shards. We are also unnecessarily asking nodes that don't need the information to receive/process it, only to later drop it.

While an ACK mechanism helps reduce overhead by allowing the node to stop broadcasting when it receives confirmation, it introduces more HA policy questions related. For example, how many replicas should a primary require ACKs from? The trickier part is that there is likely no one-size-fits-all answer. This is why I prefer the current solution, as it essentially converges on the WAIT semantics. It is not very intuitive to couple HA with the cluster.

Another complexity arise when a new replica joins the shard. We need to ramp it up about the slot migration happening in its primary. This means that we can't stop broadcasting just because existing replicas have acknowledged the states. Instead, we should continue broadcasting the slot migration states until the migration is complete. This prolonged broadcasting would significantly burden the cluster.

The fact that the slot migration states only concern the primary and replicas in question is a good indicator that it is a replication problem in my opinion. "
1610936078,10517,oranagra,2023-06-28T07:50:03Z,"it is a little odd that we replicate that open slots command to all replicas, every time one becomes online.
when we'll have the multiplexing feature we'll be able to send it to just the desired replica, and since it's not gonna affect the repl offset, maybe we can also send it earlier (rather than at the tail of the pending command stream).

I think the questions we wanna focus on now are:
1. is merging this as is, something that we're gonna regret and can cause complications in the future?
2. do we have another simple solution that can be implemented right away instead?
3. is it important to fix this ASAP or we can leave it unsolved for another lengthy period of time?

i suppose the answers are:
1. doesn't look like something that will cause complications, and we can easily drop it later (or better yet, move it to another virtual command stream when muxing is introduced)
2. looking at the last response, i  understand that doing it via the cluster bus isn't easy.
3. although it is already broken for many years (indicating that it's not urgent), it is quite annoying, and can cause quite some damage, right?

if the above is correct, i'm in favor of merging it for now."
1610967445,10517,soloestoy,2023-06-28T08:13:15Z,">First, the cluster message size will increase significantly since we need to include the to/from node-ids. Additionally, imagine that there are tens or hundreds of slots being moved at any given time on different shards. We are also unnecessarily asking nodes that don't need the information to receive/process it, only to later drop it.

We don't need to broadcast, we just need to send messages(without gossip part) to the replicas of the current master node, just like the SPUBLISH command. And the CLUSTER SETSLOT command only modifies one slot at a time, we only need to indicate the change in state for this one slot in the message, so the message size shouldn't be too large.

>While an ACK mechanism helps reduce overhead by allowing the node to stop broadcasting when it receives confirmation, it introduces more HA policy questions related. For example, how many replicas should a primary require ACKs from? The trickier part is that there is likely no one-size-fits-all answer. This is why I prefer the current solution, as it essentially converges on the WAIT semantics. It is not very intuitive to couple HA with the cluster.

As I mentioned earlier, we don't need to broadcast. We can just send these messages within a single shard. And about the number of ACKs, we can keep it the same as the number of replicas specified in the WAIT command.

>Another complexity arise when a new replica joins the shard. We need to ramp it up about the slot migration happening in its primary. This means that we can't stop broadcasting just because existing replicas have acknowledged the states. Instead, we should continue broadcasting the slot migration states until the migration is complete. This prolonged broadcasting would significantly burden the cluster.

For newly added replicas, we can transmit the state of all slots when they perform the handshake with the master. Besides, currently, adding CLUSTER SETSLOT command to the replication stream won't enable new replicas to perceive MIGRATING/IMPORTING status since this information is not included in the RDB during full synchronization."
1610990980,10517,soloestoy,2023-06-28T08:29:25Z,"""it is already broken for many years (indicating that it's not urgent)"" and I believe we will drop it later (IMHO, it's very strange to replicate an ADMIN command). So, I suggest we consider using the cluster bus to implement it, if we don't have enough time, we can put it into 8.0."
1611676840,10517,zuiderkwast,2023-06-28T15:45:15Z,"We suffer from these problems when scaling, when many slots are moved. We have seen slots that get two or zero owners. And there is potential data loss.

The long term solution would be either Atomic Slot Migration or the Cluster V2 ""flotilla"". (Is there any decision about that?) In the mean time, I though this PR was supposed to be a quick backwards compatible fix, yet it has been reviewed since before 7.0 was released.

> although it is already broken for many years (indicating that it's not urgent)

How can reliability issues like this be not urgent?

AWS has their own slot migration implementation. Why? Can it be that open source slot migration has never really been reliable..."
1612042388,10517,madolson,2023-06-28T20:15:20Z,"> How can reliability issues like this be not urgent?

The cynic in me is that most people are using managed providers that either do cluster different or wrote around it (like AWS as you mentioned). You're right though, this should be a higher priority than it is. 

> didn't look deep into the codes, but to be honest, I don't like using the replication stream to propagate MIGRATING/IMPORTING status, this makes the replication stream muddier.

I think there is a good debate here, and I have mixed feelings as well. I think it could be reduced into two different ways of thinking:
1. Slot ownership is data. Whether or not a slot is currently owned and can be served by a node can and should come from the replication stream. A replica will receive the slot state during load and will receive the state changes in the stream. This prevents subtle divergences which can happen, such as the replica thinking it can serve data that was deleted.
2. Slot ownership is independent of data. This is more of the current scheme, which is to say the replica will eventually learn what is going on from the primary based off the cluster state.

I think ultimately one is the more correct way to think about the data transfer, which may be divergent from the way flotilla is thinking about it and how we do it today. It's also divergent from zhao's ""So, I suggest we consider using the cluster bus to implement it"". I don't agree with that.

> For newly added replicas, we can transmit the state of all slots when they perform the handshake with the master. Besides, currently, adding CLUSTER SETSLOT command to the replication stream won't enable new replicas to perceive MIGRATING/IMPORTING status since this information is not included in the RDB during full synchronization.

We discussed this and I agreed with the approach you are suggesting originally, but Ping made the case that it was a lot more complexity. 

> In the mean time, I though this PR was supposed to be a quick backwards compatible fix

I just want to say I don't think this is all that fair of an assessment. I still believe there are structural issues with the way slot migration is broken today, and we need more effort to fix it. There are other issues as well such as the epoch after bump issue."
1612486886,10517,soloestoy,2023-06-29T06:17:09Z,"Of course, I support fixing this problem. However, but the key point is not whether it is urgent, but whether the method of fixing it is correct. As far as I am concerned, using the replication stream method is wrong.

@PingXie Have you tried the method that I suggested in https://github.com/redis/redis/pull/10517#issuecomment-1610967445? IIUC, the discussion before was mainly about using the cluster bus to broadcast. My suggestion is to spread it only to the affected replica, just like how the replication affects the nodes."
1612624269,10517,PingXie,2023-06-29T08:27:57Z,"> Of course, I support fixing this problem. However, but the key point is not whether it is urgent, but whether the method of fixing it is correct. As far as I am concerned, using the replication stream method is wrong.

I'm not convinced about replication being the wrong approach, and I'd like to understand your concerns regarding the challenges with ""multiplexing"" that you mentioned earlier. It would be really helpful if you can expand on it some more or maybe share some pointers.

FWIW, we currently have a few commands marked for replication. While they may not be categorized as ADMIN commands, the argument of data Vs non-data can be quite blurry (for example, `evalsha`). On the other hand, one could argue that these are exceptional cases, which would be acceptable if we can clearly articulate the differences.

```
eval.json:13:            ""MAY_REPLICATE"",
evalsha.json:13:            ""MAY_REPLICATE"",
fcall.json:13:            ""MAY_REPLICATE"",
pfcount.json:11:            ""MAY_REPLICATE""
publish.json:14:            ""MAY_REPLICATE"",
spublish.json:14:            ""MAY_REPLICATE""
```

I think it's worth diving deeper into this discussion of replication vs broadcasting, data vs non-data, etc here on GitHub, even if it's more philosophical in nature. The knowledge sharing would be extremely valuable for the community as well as for me personally.

> @PingXie Have you tried the method that I suggested in [#10517 (comment)](https://github.com/redis/redis/pull/10517#issuecomment-1610967445)? IIUC, the discussion before was mainly about using the cluster bus to broadcast. My suggestion is to spread it only to the affected replica, just like how the replication affects the nodes.

I think the idea of sharded broadcast is quite interesting (thanks for the suggestion!). I'm definitely open to giving it a shot and seeing how it plays out. However, I don't think it is a guaranteed solution. Until I spend some quality time exploring the idea, I can't be sure what other issues might arise. Note that the current PR has already gone through multiple peer reviews and lots of testing, including our internal testing. If we consider getting a good grasp of this idea as a prerequisite for making a decision on whether to fix the reliability issue in 7.2, then the answer seems pretty clear to me: we won't be fixing it in 7.2.

So, here's an alternative approach I'd like to suggest. How about we focus on addressing the remaining feedback on the existing implementation and merge it into 7.2? At the same time, we can start evaluating the sharded broadcast idea for the future versions. I do have one concern, though. The current design, which relies on replication, gives the caller (redis-cli in this case) explicit control over the number of replicas that need to acknowledge the receipt of slot migration states using the `WAIT` command. This is an important aspect of the user/admin experience that ideally should be maintained. If we believe `WAIT` is the right approach (orthogonal to the replication vs broadcasting discussion) or we see no concern of dropping it, then I don't foresee any issues with replacing the internal implementation in the future."
1613772962,10517,PingXie,2023-06-29T20:31:01Z,">I just want to say I don't think this is all that fair of an assessment. I still believe there are structural issues with the way slot migration is broken today, and we need more effort to fix it. There are other issues as well such as the epoch after bump issue.

Indeed, the root cause of all these problems can be traced back to the absence of a consensus-driven epoch bump. Until this fundamental aspect is addressed, I think we are just playing a never-ending game of ""whack-a-mole."""
1615932552,10517,madolson,2023-07-01T14:04:29Z,"> I'm not convinced about replication being the wrong approach, and I'd like to understand your concerns regarding the challenges with ""multiplexing"" that you mentioned earlier. 

I don't think multiplexing is right here. The point of multiplexing is to support out of band messages, in which case we would  have used the cluster-bus as was mentioned. We still can run into issues when a replica fails before receiving a message that it's primary has started importing or migrating a slot. The current implementation relies on the cli to send a wait, but we could have also changed the cli to wait for the replica to acknowledge the new state. I still have a preference for the current replication based implementation."
1617601220,10517,zuiderkwast,2023-07-03T08:17:41Z,"One more point for having SETSLOT in the replication stream is consistency for clients that are reading from replicas.

When a key is migrated, it is deleted from replicas. If the replica knows the slot is being migrated, it can return an ASK-redirect even if DEL comes immediately after SETSLOT MIGRATING. (#11312 is not yet supported but it's easy to implement after this.)

The cluster bus is not synchronized with the replication stream. If we use the cluster bus for SETSLOT, the admin would need to wait for all replicas to ack the new state before starting to move keys, if we want replica read consistency.
"
1619584910,10517,soloestoy,2023-07-04T06:32:43Z,"I still believe cluster bus is the right way.

But, maybe we can change our perspective. Why do we have to rely on replication or cluster bus to propagate the metadata? Isn't it simpler and more explicit to execute ""cluster setslot with master node id"" on the replica nodes directly? After all, ""REPLICATE"" also needs to be explicitly specified by the administrator. For the old version of the cluster manager, this is a breaking change. It's better to let the administrator be responsible for making sure that the replica nodes are aware of the migration status."
1620292208,10517,madolson,2023-07-04T13:52:54Z,"> Why do we have to rely on replication or cluster bus to propagate the metadata? Isn't it simpler and more explicit to execute ""cluster setslot with master node id"" on the replica nodes directly? After all, ""REPLICATE"" also needs to be explicitly specified by the administrator.

Although your proposal is simpler, it is adding failure points into the system and also requires all existing systems to adopt the new API calls. I don't think we should be pushing too much onto administrators, especially with flotilla wanting to be more of a stateless controller.

I think the main decision point is this though, how do we want the migrating and importing state to be recognized on the replica. Although we haven't implemented it yet, I also want to discuss how it may relate to Cluster V2 (flotilla) as well as an atomic slot migration.

## Option 1: Have the slot state be transferred through the replication state. Ideal state is a RDB full sync includes slot information + replicating set slot commands. 
- Will make sure the replica is serving replication data with snapshot consistency. 
- Clients can use the wait command to make sure that data is replicated. Clients are required to make a change to get the benefit of data being replicated though.
- Atomic slot migration will need a way to indicate to clients that they have the full sync of a slot, to indicate they can successfully serve the data. 
- Flotilla will set importing and migrating state and rely on the server to coordinate transferring the data.

## Option 2: Have the slot state be transferred through the cluster state
- Replica may serve a slot is empty or has stale data for since the data has already been fully migrated to another slot. The most serious places I've seen this is right have a sync, the replica might believe it ""owns"" a slot, but have partial data for it. 
- Clients would need to implement logic to check to see that all replicas have acknowledged the new state.
- Atomic slot migrations would need to wait for replicas to acknowledge the importing/migrating state through cluster messages. 
- Flotilla would replace the importing/migrating state messages sent by the client. 

"
1621124228,10517,PingXie,2023-07-05T06:42:51Z,"> I still believe cluster bus is the right way.

I haven't had a chance to evaluate the cluster bus solution in depth, but one thing I'm pretty certain about is that we'll need to establish a new mechanism parallel to 'WAIT'. This will be essential to attain parity with the replication solution, which enables the caller to explicitly synchronize with replicas regarding the slot migration states. That's why I still think that this issue fundamentally revolves around replication.

> It's better to let the administrator be responsible for making sure that the replica nodes are aware of the migration status.

Philosophically speaking, I beg to differ with this statement. If we were to adopt the approach of ""admin handling everything,"" I believe there would be minimal need to address any reliability concerns since, ultimately, the admin would be responsible. IMO, the main objective here is to reduce human intervention, or the involvement of the control plane to a greater extent, allowing the Redis service (managed or not) to have an opportunity to achieve 3/4/5 9s of SLOs by itself."
1621254208,10517,soloestoy,2023-07-05T08:09:32Z,">Philosophically speaking, I beg to differ with this statement. If we were to adopt the approach of ""admin handling everything,"" I believe there would be minimal need to address any reliability concerns since, ultimately, the admin would be responsible. IMO, the main objective here is to reduce human intervention, or the involvement of the control plane to a greater extent, allowing the Redis service (managed or not) to have an opportunity to achieve 3/4/5 9s of SLOs by itself.

Seems you misunderstood me, my point is that the administrator is responsible for all metadata, such as passwords, common configuration items such as ""appendonly"", and cluster configuration items such as ""cluster-allow-reads-when-down"". It is the responsibility of the administrator to ensure that these are set correctly and consistently on both the master and replicas. In my opinion, slot information is also metadata or a configuration item, and it is the right choice for the administrator to ensure consistency between the master and replicas. Even when using the ""WAIT"" command, it is not transparent to the administrator, and the administrator is still responsible for making the final consistent judgment."
1621399866,10517,zuiderkwast,2023-07-05T09:38:57Z,"Requiring manual intervention is quite bad IMO. It's too easy to mess up.

A cluster does guarantee some things by itself, like making sure there is exactly one master per shard. Guaranteeing slot ownership consistency is one such thing it should do IMO.

If some configuration (or ACL, etc.) can be inconsistent between different nodes, that's a design flaw IMO and we better fix that; propagate that config to make sure all nodes have consistent configuration. That's a different discussion though."
1621467683,10517,ushachar,2023-07-05T10:20:15Z,"Flotilla orchestrates all slot changes through the Topology Director (the admin never needs to communicate directly with the data nodes), and guarantees that a newly elected primary node is at least as up to date on slot ownership than any of its replicas.
(Since a replica won't be promoted before it acks the latest topology known by the Failover Coordinator).

There's a potential for stale reads when read-from-replica is enabled -- but that's inherent in any read from replica usecase...."
1621517422,10517,soloestoy,2023-07-05T10:54:43Z,">Flotilla orchestrates all slot changes through the Topology Director (the admin never needs to communicate directly with the data nodes)

I strongly agree with this approach, and I also want to point out that in this case, the Topology Director is the real administrator, as it is doing the job of an administrator."
1621524755,10517,soloestoy,2023-07-05T10:59:59Z,">There's a potential for stale reads when read-from-replica is enabled -- but that's inherent in any read from replica usecase....

As far as I know, in areas outside of Redis, there are many systems that can achieve consistent reads on replica databases, such as Polardb. However, this is another interesting topic."
1621580719,10517,zuiderkwast,2023-07-05T11:36:55Z,"> the Topology Director is the real administrator, as it is doing the job of an administrator.

That's very bad analogy IMO.

The Topology Director is a raft cluster, which achieves consensus among shards. The consistency between master and replicas is handled by Failover Coordinator, which is another raft cluster per shard. These are part of the cluster, not admins.

An administrator is a user, typically a human, orchestrating the whole thing. An admin should not be able to induce inconsistent slot ownerships by sending SETSLOT differently to different nodes."
1621612581,10517,soloestoy,2023-07-05T11:57:14Z,"I don't think so, an administrator is not a specific person, it is a role, as you said, responsible for resource orchestration, metadata management, and other tasks."
1621677689,10517,ushachar,2023-07-05T12:40:04Z,"@soloestoy Sure -- I meant in the Redis replica sense.... Not as a global statement for all dist systems :)
FWIW I'd argue that a Paxos learner (from what I see PolarDB uses that for read-only members) isn't really a 'replica'...."
1827160862,10517,PingXie,2023-11-27T05:47:02Z,"Hi @soloestoy and team, revisiting our conversation on this PR, I've been considering the cluster bus method. My preference for the replication approach primarily stems from how the `WAIT` command for replica acknowledgment naturally aligns with it. The proven functionality of this approach and the extensive time invested in its development are also key factors. Moving to the cluster bus method would mean developing a new mechanism for acknowledgment. While there are often multiple viable solutions to a problem, I think it is crucial to understand the concrete advantages of each. @soloestoy, your insights on its benefits would be invaluable.

@oranagra @zuiderkwast @madolson "
1830395001,10517,zuiderkwast,2023-11-28T17:57:54Z,"I'm still waiting for a merge. :) It's a good improvement.

A bonus point for making replicas aware of migrations: It opens for replicas to return -ASK redirects. Currently, they don't know about ongoing migration so they just return null for already migrated keys."
1849513293,10517,PingXie,2023-12-11T08:05:46Z,"Hey team, can we bring this PR to the table at your next core team meeting? It feels like the ideal time to reach a decision and keep things moving. I'm ready to walk you through my thought process and am keen to hear your  perspectives. "
1974667161,10517,PingXie,2024-03-02T07:40:10Z,all tests have passed. This PR is ready for merge. 
2010963060,13157,duncan-bayne,2024-03-21T00:27:20Z,"![dark-side](https://github.com/redis/redis/assets/65949/c6fa4055-3678-42d9-b2e2-df41391250ce)
"
2011033559,13157,CharlesChen888,2024-03-21T01:44:37Z,"![image](https://github.com/redis/redis/assets/112807411/4c8073b1-a53a-4651-8983-94e6626699ab)
"
2011297105,13157,JesFEREM,2024-03-21T06:19:01Z,booooo
2011311146,13157,adulau,2024-03-21T06:32:50Z,It would be nice to list the new forks in this pull-request along with the ones without CLA and shared copyrights with Developer Certificate of Origin. This would avoid this exact situation.
2011733104,13157,zuiderkwast,2024-03-21T09:27:38Z,My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.
2011836479,13157,jirutka,2024-03-21T10:16:11Z,"Are you aware that you have just removed Redis from the vast majority of Linux distributions? 😒

This is a very bad decision that will not end well for you in the end. Look at ElasticSearch vs OpenSearch, MySQL vs MariaDB, Oracle JDK vs OpenJDK, OpenOffice vs LibreOffice etc.

There are some precedents with Terraform, Elasticsearch, Red Hat, and a few other big players now dealing with a lot of their target users and potential customers depending on open source forks. As a business strategy alienating future users like that seems misguided. ([source](https://news.ycombinator.com/item?id=39775800))

If you want an example of how to do it better, check out TimescaleDB."
2011837681,13157,VictorWesterlund,2024-03-21T10:16:47Z,Time for a Libredis fork!
2012079375,13157,fisx,2024-03-21T11:57:32Z,"> Time for a Libredis fork!

that'd be a way out, yes, but it would be so much less wasteful if we could convince you to take this change back.

sure, making money producing open source software is tricky, but given the fact that this is carried out with such obvious mistakes as the unlawful removal of BSD headers, are you sure you have done your research and established this won't backfire?  i am working for a commercial reddis user, and i am certainly going to look out for forks now.

btw good reputation comes from reconsidering and fixing mistakes, not doubling down on them!  :)"
2012153631,13157,mochaaP,2024-03-21T12:27:25Z,"> Time for a Libredis fork!

Edit: please use sircwncmp's redict: <https://codeberg.org/redict/redict>

~~Really hope someone could take care of either <https://github.com/librekvdb/librekv> or <https://github.com/Snapchat/KeyDB>.~~

~~However, since a new Redis fork isn't backed by any experienced players in the database field (at least for now), I'm not sure will it last long.~~

~~That being said, if anyone is interested in taking over LibreKV please contact me :)~~"
2012360667,13157,tebowy,2024-03-21T13:51:10Z,"> My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.

@zuiderkwast 
As a copyright holder you might consider pursuing a DMCA process towards the files with mangled license headers."
2012764160,13157,joshmanders,2024-03-21T15:57:24Z,"> [Redis will remain BSD licensed](http://antirez.com/news/120) (2,038 days ago)

5.5 years it took for that to become untrue."
2012958877,13157,jwbowen,2024-03-21T16:44:33Z,Fork by Drew DeVault: https://codeberg.org/redict/redict
2012998974,13157,XtremeOwnageDotCom,2024-03-21T16:54:39Z,"freedis, here we come."
2013219637,13157,impredicative,2024-03-21T18:16:53Z,"This change is probably uncalled for since it has been claimed that AWS has funded one or more Redis developers for years.

The relevant forks of Redis I see are:

* https://codeberg.org/redict/redict
* https://github.com/placeholderkv/placeholderkv
* https://github.com/mindcrime-forks/redis (perhaps just for record-keeping)

Alternatively, the legacy of Redis may be the RESP protocol rather than Redis itself. Here is a list of projects that claim to support the Redis protocol:

* https://github.com/microsoft/garnet
* https://github.com/apache/kvrocks
* https://github.com/dragonflydb/dragonfly"
2013282230,13157,Cyberes,2024-03-21T18:42:34Z,wow this is dumb
2013692730,13157,alexwennerberg,2024-03-21T20:42:10Z,no
2013741227,13157,natoscott,2024-03-21T21:09:59Z,"> My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.

My commit 11cd983d5819 to allow Redis modules to function with modern compilers was also made under the BSD license and permission is **not** granted for it to be used under any other license.  Copyright for changes in this commit is held by my employer, Red Hat."
2014071938,13157,JosTheDude,2024-03-22T00:02:52Z,no.
2014100831,13157,dieperdev,2024-03-22T00:27:22Z,👎
2014186987,13157,baronwangr,2024-03-22T02:12:36Z,"It is NOT ALLOWED to use the contributions of cloud vendors to run against them by simply changing a famous ""anti-cloud"" license on it.
There's no CLA signed. You should have better explanations and we'd better be impressed."
2014336681,13157,GamerGirlandCo,2024-03-22T04:31:21Z,obligatory go fuck yourselves @redis :3 
2014340803,13157,owenrummage,2024-03-22T04:37:35Z,"Obligatory fuck redis. 

I cant wait for you guys to get sued into the ground by everyone who has ever committed that disagrees with this change."
2014355620,13157,madolson,2024-03-22T04:58:13Z,">This change is probably uncalled for since it has been claimed that AWS has funded one or more Redis developers for years.

I appreciate that people claim I exist. I'm moving to development here, https://github.com/madolson/placeholderkv. Not a great name, but trying to get a lot of the old contributors here to help resume developing where we left off. Trying to keep as much the same as possible for now, but I'm sure we'll want to change."
2014361960,13157,nulldg,2024-03-22T05:07:04Z,"going forward, best option for users downstream is to pin version or switch upstream to a fork depending on use-case (preferably the latter). contributors especially under contract have had their terms breached and this relicense is unwelcome especially after the assurance BSD would remain. fuck off redis, you backstabbers."
2014389264,13157,russeg,2024-03-22T05:34:46Z,Just go closed-source ffs.
2014462566,13157,riking,2024-03-22T06:49:44Z,"wording to consider in your copyright violation demand letters:

> To remedy this violation, one option is to reset your repository to a state before the change that introduced this problem:
> `git push --force origin e64d91c37105bc2e23816b6f81b9ffc5e5d99801:patch-1`

where patch-1 is replaced with the problematic branch."
2014465395,13157,LiiNen,2024-03-22T06:52:33Z,"understand that there is no significant change in terms of individual usability.
However, there are some concerns in terms of the opensource ecosystem, so that it would rather hinder, not improve.
Others have already pointed out problems with BSD deletion.

I would like to ask maintainers if there is any possibility of this decision being rollback.
Thanks a lot.

(impressed by what **fisx** said above, so quoting it.)
> btw good reputation comes from reconsidering and fixing mistakes, not doubling down on them! :)"
2014600274,13157,that-guy-iain,2024-03-22T08:27:13Z,"A lot of people in here acting like suddenly they're being asked to pay for Redis or they're being told they can't do something. Considering this new licenses are to restrict competitors, seems silly to be mad that your competitor stopped you taking their work.

And @natoscott I believe that commit is not significant enough to warrant copyright. "
2014627803,13157,FeepingCreature,2024-03-22T08:46:52Z,"Open source licenses are supposed to enable and foster competitors, not restrict them.

If this seems a problem to you, you don't understand open source."
2014678928,13157,Doctor-Scott,2024-03-22T09:21:52Z,"Here comes AWS ElastiCache for ""red'ish"""
2014708862,13157,nulldg,2024-03-22T09:38:42Z,"the community is right to be irritated.

non-free licenses are the enemy of FOSS no matter how you shake it. OSS is fundamentally altruistic, so none of us care how many competitors redis has. copying is not theft but relicensing open source contributions without permission is. doing so stands in grave contrast to the philosophy of sharing under which these third-party developers contributed in the first place.

""don't steal from me what i have stolen"" just isn't a very convincing defense."
2014709005,13157,ThaFireDragonOfDeath,2024-03-22T09:38:47Z,"> A lot of people in here acting like suddenly they're being asked to pay for Redis or they're being told they can't do something. Considering this new licenses are to restrict competitors, seems silly to be mad that your competitor stopped you taking their work.

You forgot that those competitors like AWS contributed to Redis development (see [Pull requests of @madolson who works on AWS](https://github.com/redis/redis/pulls?q=is%3Apr+author%3Amadolson)).
So in my eyes it looks like Redis is now backstabbing after it profited from the community which includes cloud service providers."
2014716686,13157,filipesmedeiros,2024-03-22T09:43:39Z,"Also I'd be curious to know if, legally, this clause of the BSD licence:

> Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.

Aren't new versions included in ""Redistributions""? Because, in a sense, you are modifying and distributing a new source code. If that is the case, not even Redis Labs (or whoever) is allowed to change the licence of the code. :)"
2014720805,13157,filipesmedeiros,2024-03-22T09:46:12Z,"https://redis.com/blog/the-future-of-redis/

> Making Redis the Go-To for Generative AI

Oh, that's explained then"
2014737480,13157,soloestoy,2024-03-22T09:56:41Z,"The contributions from Alibaba Cloud:

https://github.com/redis/redis/pulls?q=is%3Apr+author%3Asoloestoy
https://github.com/redis/redis/pulls?q=is%3Apr+author%3Achenyang8094
https://github.com/redis/redis/pulls?q=is%3Apr+author%3Ayangbodong22011
https://github.com/redis/redis/pulls?q=is%3Apr+author%3ACharlesChen888
https://github.com/redis/redis/pulls?q=is%3Apr+author%3ADarrenJiang13
https://github.com/redis/redis/pulls?q=is%3Apr+author%3Alyq2333"
2014742606,13157,tygoegmond,2024-03-22T09:59:56Z,You can't just hijack work from 700+ contributors. 
2014831737,13157,filipesmedeiros,2024-03-22T10:52:47Z,"Also funny how no dev wanted to do this(?), so it had to be a project manager ahah (not a direct attack at the specific person who did this, just at the corporate structure itself)"
2014840953,13157,Rithari,2024-03-22T10:58:48Z,Congratulations on destroying your reputation
2014873851,13157,dlq84,2024-03-22T11:20:20Z,"How to kill your product in 1 easy step.... What do people recommend as alternatives? Fill the thread with that. Also, contributors should consider a class-action, this is theft of intellectual property."
2014943582,13157,AlyoshaVasilieva,2024-03-22T12:08:23Z,"https://docs.freebsd.org/en/articles/bsdl-gpl/#current-bsdl

>Any BSD code can be sold or included in proprietary products without any restrictions on the availability of your code or your future behavior.

Being able to do things like what Redis is doing is *the point* of BSD licenses. Don't contribute to BSD-licensed projects if you don't want a corporation to grab your code and start selling it."
2015084965,13157,rilysh,2024-03-22T13:18:36Z,"> https://docs.freebsd.org/en/articles/bsdl-gpl/#current-bsdl
> 
> > Any BSD code can be sold or included in proprietary products without any restrictions on the availability of your code or your future behavior.
> 
> Being able to do things like what Redis is doing is _the point_ of BSD licenses. Don't contribute to BSD-licensed projects if you don't want a corporation to grab your code and start selling it.

That's not the actual point here, but it's that Redis project itself, completely moved to a proprietary license model which restricts users freedom. Every free software/open source license gives freedom to sell the copies of the software, with a clause that you'd also distribute the source under a free license. BSD* licenses aren't something new in this.

I think many GNU/Linux distributions will remove Redis from the package database, unless it's critical enough to support a few more years before they purge it off. It would be great if few active contributors can fork Redis (the previous version before it got relicensed) and relicensed (keeping the previous BSD license) to a copyleft license, preferably friends of GPL or itself."
2015098673,13157,AlyoshaVasilieva,2024-03-22T13:27:00Z,">Every free software/open source license gives freedom to sell the copies of the software, **with a clause that you'd also distribute the source under a free license**. BSD* licenses aren't something new in this.

No, you're thinking of *copyleft* licenses, which the BSD licenses aren't.

The BSD license text is still in the repo here: https://github.com/redis/redis/blob/0b34396924eca4edc524469886dc5be6c77ec4ed/REDISCONTRIBUTIONS.txt

>Redistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:

>1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.

The notice is still in the repo - they're compliant.

>2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.

I can't imagine they're going to suddenly stop doing something so simple, so they're going to comply with this.

>3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.

They weren't doing this before, they aren't doing this now - they're compliant.

That is *all the BSD license requires*. There are no other requirements, no ""you must put your code under an open license"" or even ""you must release code"".

>I think many GNU/Linux distributions will remove Redis from the package database

Yes, certainly, the new licenses aren't free. But what they're doing is legal, even if it's annoying/not FOSS/will make them unpopular/etc. They have not violated the BSD license in any way."
2015110649,13157,rilysh,2024-03-22T13:34:10Z,"> No, you're thinking of copyleft licenses, which the BSD licenses aren't.

You didn't interpret it correctly. Although I didn't explicitly said there but I'm well aware of this. What I'm saying is ""Every free software/open source license gives freedom to sell the copies of the software"" and so BSD licenses aren't new in this.

BSD licenses are source of confusion when you'll use them in larger projects. There are a lot of variants exists of this license (originated from Berkley).

> Yes, certainly, the new licenses aren't free. But what they're doing is legal, even if it's annoying/not FOSS/will make them unpopular/etc. They have not violated the BSD license in any way.

Indeed. Permissive licenses allow software to go under proprietary control, it just that Redis took the advantage of OSS and now they go into deep silence.

Edit: Forgot to mention but I think some distributions may continue distributing Redis. For example, Ubuntu maintains snap and they allow proprietary product to be install on the system as snap. Also, distributions such as RHEL... may also keep Redis. Fedora (very likely) has a non-free repository, so they can keep it there as well, if community wishes to do so."
2015179373,13157,kingthrillgore,2024-03-22T14:04:28Z,Blink twice if the private equity folks have you in an awkward position.
2015217903,13157,filipesmedeiros,2024-03-22T14:23:48Z,"@rilysh apparently that's already being done :) 

> This change is probably uncalled for since it has been claimed that AWS has funded one or more Redis developers for years.
>
> The relevant forks of Redis I see are:
> 
> https://codeberg.org/redict/redict
> https://github.com/mindcrime-forks/redis (perhaps just for record-keeping)
> Alternatively, the legacy of Redis may be the RESP protocol rather than Redis itself. Here is a list of projects that claim to support the Redis protocol:
> 
> https://github.com/microsoft/garnet
> https://github.com/apache/kvrocks

By: @impredicative above

I like Redict especially :)"
2015244132,13157,afonsofrancof,2024-03-22T14:38:02Z,Very good move if you want to kill your project. Congrats
2015266561,13157,Asutherland8219,2024-03-22T14:48:50Z,"New redis logo 

![image](https://github.com/redis/redis/assets/60415557/ec520219-3689-432c-bf1d-4c3599c19476)
"
2015278240,13157,rilysh,2024-03-22T14:54:37Z,"> @rilysh apparently that's already being done :)
> 
> > This change is probably uncalled for since it has been claimed that AWS has funded one or more Redis developers for years.
> > The relevant forks of Redis I see are:
> > https://codeberg.org/redict/redict
> > https://github.com/mindcrime-forks/redis (perhaps just for record-keeping)
> > Alternatively, the legacy of Redis may be the RESP protocol rather than Redis itself. Here is a list of projects that claim to support the Redis protocol:
> > https://github.com/microsoft/garnet
> > https://github.com/apache/kvrocks
> 
> By: @impredicative above
> 
> I like Redict especially :)

Nice :) Thanks for sorting it out."
2015476486,13157,gingebeard36,2024-03-22T16:37:53Z,"Hey all. Quick disclaimer - I am a Redis employee, not a dev though. We worked with various open source experts on this. The LICENSE.txt file is the first-party notice for REDIS 7.4 and subsequent versions. [REDISCONTRIBUTIONS ](https://github.com/redis/redis/blob/unstable/REDISCONTRIBUTIONS.txt)is the file that contains the 3BSD license (a third-party notice for the applicable portions of the project). We would never violate the licenses and do not seek to alter the licenses of any existing (or in-flight) contributions. Any contributions that are still in process will fall under 3BSD. "
2015547447,13157,XtremeOwnageDotCom,2024-03-22T17:16:57Z,"> We would never violate the licenses and do not seek to alter the licenses of any existing (or in-flight) contributions.

But- you did. You removed the header, from all of the code contributed under the BSD license. None of the previous contributions, under BSD, are labeled as BSD anymore. Instead, EVERYTHING is labeled with the new license."
2015645044,13157,gingebeard36,2024-03-22T18:07:20Z,"> > We would never violate the licenses and do not seek to alter the licenses of any existing (or in-flight) contributions.
> 
> But- you did. You removed the header, from all of the code contributed under the BSD license. None of the previous contributions, under BSD, are labeled as BSD anymore. Instead, EVERYTHING is labeled with the new license.

Am I missing something? If the 3BSD license was on a file, we left it in place. Example: https://github.com/redis/redis/blob/unstable/src/ae_evport.c"
2015676832,13157,XtremeOwnageDotCom,2024-03-22T18:27:47Z,"> Am I missing something? If the 3BSD license was on a file, we left it in place. Example: https://github.com/redis/redis/blob/unstable/src/ae_evport.c

Ok, let me pick a random file.

https://github.com/redis/redis/blob/unstable/src/cluster.c

This file, has commits from [MANY users](https://github.com/redis/redis/commits/unstable/src/cluster.c).

MANY of those users, contributed code under BSD terms. 

Did, you ask ALL of those users if they wish to change the license?

Is the BSD license mentioned ANYWHERE in that file?

How about this [Commit](https://github.com/redis/redis/commit/eacca729a55501508c434bab30c2432e58728aee) from [ranshid](https://github.com/redis/redis/commits?author=ranshid), originally committed under BSD terms. 

Did- you ask him if he agreed to remove the BSD license from his commits?

No. You internally agreed to remove the license. You did not consult the community. You did not ask previous contributors if they agreed to this change.

As a result, as we speak, redis is being actively pinned to older versions,  or just outright removed from repositories, linux distributions, etc, because you INTERNALLY decided you no longer want to do open source.

Open source doesn't mean you get to own the code, and just take everyone's contributions for free. No. Open source means, OPEN SOURCE.

We can argue about this all day, and in the end, that won't change a single thing. However, I would urge your leadership to go evaluate the ramifications which are currently occurring. 

* https://github.com/Homebrew/homebrew-core/pull/166769
* https://github.com/alpinelinux/aports/commit/57959e206db177fb4c2c641dac24eb0bf1d528ae
* https://github.com/searxng/searxng/issues/3348

Know what is going to happen when a CVE gets released for version 7.2? The remaining user base you have, is going to look elsewhere, as you cannot back-port the fix, due to your proprietary license. 

I'd urge you to go review the history of a few other projects, with similar fates.

* Owncloud / Nextcloud.
* PFSense / Opnsense.
* MySQL / MariaDB
* OpenOffice / LibreOffice

Although, I suppose in the end, it doesn't matter. You have already lost the trust of your user base.

[Don't worry, redis will remain BSD.](http://antirez.com/news/120)"
2015712860,13157,rpeden,2024-03-22T18:49:29Z,"@XtremeOwnageDotCom is there anything in the 3-clause BSD license that says it needs to be included in every file? It doesn't look like it, but maybe I'm misreading it. The parts where it reads ""Redistributions of source code must retain the above copyright notice"" wording sure seems like keeping a copy of it in the repository (which they've done) meets the requirement. 

They mentioned that the change only applies to new code, not existing code, so it seems like this change, although perhaps distasteful meets the requirements of the BSD license. 

I'm not saying this to be a jerk. I think it's reasonable to be upset about this kind of unexpected change. But I also don't believe that trying to make issues out of non-issues is helpful."
2015727491,13157,XtremeOwnageDotCom,2024-03-22T18:58:14Z,"> is there anything in the 3-clause BSD license that says it needs to be included in every file?

Likely not. But, it's not going to stop me from getting on my soapbox, and fussing about a decision to commercialize the many contributions of others, committed under BSD terms. 

I am sure they have lawyers who went over the fine print, to double-check everything. 

But, in the end, a few people will give a thumbs up to my rant, and likely nothing will change at all.... and 6 months from now, everyone will have swapped over to redict, or whatever popular fork emerges from this... and redis can join the dozens of services which took similar steps like this in the past. 

I should also add- MySQL / MariaDB to my list."
2015802209,13157,ThaFireDragonOfDeath,2024-03-22T19:50:18Z,"> They screwed up, and they know it. They failed to have a CDL in the first place, and now they're trying magic tricks with ""only new code"": IMNAL but I'd be curious to see how the judges would view this: If one changes (delete old, add new) code under BSD this code can not be magically become code under another license...

IANAL but I think that relicensing the BSD-3 source code under the terms of a different license is possible even without the explicit approval of the contributors as long the terms of the BSD license are still fulfilled under the new license (see topic license compatibility).
Otherwise the redict fork would also be illegal since Drew DeVault relicensed the BSD-3 code under the terms of LGPLv3.
But relicensing a community involved project under a proprietary or source available license is still a massive d***move."
2015965802,13157,AlyoshaVasilieva,2024-03-22T21:37:50Z,">This is illegal. You just blatantly stripped out people's copyright.

The BSD license text is [still in the repo](https://github.com/redis/redis/blob/0b34396924eca4edc524469886dc5be6c77ec4ed/REDISCONTRIBUTIONS.txt) and they're still complying with its requirements. It does not require license headers in files."
2015968626,13157,Zipdox2,2024-03-22T21:40:22Z,Oh yeah I completely overlooked that it was licensed under a [cuck license](https://lukesmith.xyz/articles/why-i-use-the-gpl-and-not-cuck-licenses/).
2015993737,13157,mrboen94,2024-03-22T21:57:39Z,"Well, this makes it harder to suggest redis to clients in the future, at least with a good conscience. Best of luck, thanks for all the fish."
2016249492,13157,jhaisley,2024-03-23T01:04:19Z,"> The BSD license text is still in the repo and they're still complying with its requirements. It does not require license headers in files.

No it doesn't, and had they stopped at removing the license header they would probably be fine, but they added a new license header, and that likely crosses a line."
2016435846,13157,imfunniee,2024-03-23T10:09:09Z,"### ""hope our contributions don't end up funding someone's yacht""
always think of this before pouring hours into open source projects, imo github (cc: @ashtom, @nat) should enforce or have a rule of some sort where you can't change licenses until all contributors signs it off. not just licenses but major changes in general.

"
2016446328,13157,50-Course,2024-03-23T10:45:45Z,"> Are you aware that you have just removed Redis from the vast majority of Linux distributions? 😒
> 
> This is a very bad decision that will not end well for you in the end. Look at ElasticSearch vs OpenSearch, MySQL vs MariaDB, Oracle JDK vs OpenJDK, OpenOffice vs LibreOffice etc.
> 
> There are some precedents with Terraform, Elasticsearch, Red Hat, and a few other big players now dealing with a lot of their target users and potential customers depending on open source forks. As a business strategy alienating future users like that seems misguided. ([source](https://news.ycombinator.com/item?id=39775800))
> 
> If you want an example of how to do it better, check out TimescaleDB.

industry keeps evolving and shape-shifting in weird ways, and in this case, beats me _to thinking if Olympus that bad that it went south enough_ to kill a fantastic software and community?"
2016475934,13157,nemobis,2024-03-23T12:17:10Z,"> You removed the header, from all of the code contributed under the BSD license.

To clarify, they removed the text of the license from _most_ headers, although it remains in about ~26 files under `src/` (mostly files originally contributed elsewhere).

As for the attribution statements in the headers, this patch almost exclusively removes those referring the original author Salvatore Sanfilippo, while leaving other names alone (for example in [src/crc16.c](https://github.com/redis/redis/commit/0b34396924eca4edc524469886dc5be6c77ec4ed#diff-d05ea72b3ff1f93f16cad7b441d0adbf02bd2ceefd94104acfeff65275b080d1)).

Some of the names remaining in headers, whose contributions are seemingly still distributed under BSD3, are:
* Joyent
* Harish Mallipeddi
* Georges Menie
* Matt Stancliff
* Amazon Web Services / ""Amazon.com, Inc. or its affiliates""
* yinqiwen
* Pieter Noordhuis
* Marc Alexander Lehmann
* Makoto Matsumoto and Takuji Nishimura
* The Regents of the University of California

Since so many files are distributed under a license different from the project's default, it would be useful to provide granular machine-readable statements as recommended e.g. in https://reuse.software."
2016508771,13157,longregen,2024-03-23T14:23:12Z,"Clear as water: the name Redis is being used to endorse products derived from this software without specific prior written permission.

![image](https://github.com/redis/redis/assets/114724657/121fa489-e78e-42cd-b0a4-ff0d8d7e247d)
"
2016523357,13157,hopeseekr,2024-03-23T15:21:05Z,"I have done a deep analysis of the commits to this project. See https://gist.github.com/hopeseekr/e840bd2346b7eaab62f77f0c798e5668 for the details.

Here's a list of all the contributors who have more than 1,000 active lines of code (not counting whitespace changes). If any or all of them haven't approved / given over their copyright, they have good rights to fight this:

```
+----------------------------------+---------+---------+-------+--------------------+
| name                             | loc     | commits | files | distribution (%)   |
+----------------------------------+---------+---------+-------+--------------------+
| antirez                          | 109,355 | 6,024   | 505   | 26.3 / 63.3 / 32.2 |
| oranagra                         | 68,017  | 23      | 846   | 16.4 /  0.2 / 54.0 |
| yoav@monfort.co.il               | 60,166  | 1       | 278   | 14.5 /  0.0 / 17.8 |
| guybe7                           | 29,010  | 93      | 512   |  7.0 /  1.0 / 32.7 |
| Yossi Gottlieb                   | 11,911  | 218     | 237   |  2.9 /  2.3 / 15.1 |
| michael-grunder                  | 8,857   | 16      | 43    |  2.1 /  0.2 /  2.7 |
| Pieter Noordhuis                 | 8,819   | 510     | 82    |  2.1 /  5.4 /  5.2 |
| 彬彬同学丶                            | 8,761   | 1       | 319   |  2.1 /  0.0 / 20.4 |
| Jason Elbaum                     | 8,353   | 3       | 17    |  2.0 /  0.0 /  1.1 |
| Viktor Söderqvist                | 7,484   | 63      | 111   |  1.8 /  0.7 /  7.1 |
| Madelyn Olson                    | 6,004   | 128     | 177   |  1.4 /  1.3 / 11.3 |
| artix                            | 5,951   | 122     | 11    |  1.4 /  1.3 /  0.7 |
| meir                             | 5,789   | 4       | 81    |  1.4 /  0.0 /  5.2 |
| sundb                            | 5,088   | 81      | 127   |  1.2 /  0.9 /  8.1 |
| Matt Stancliff                   | 4,908   | 151     | 79    |  1.2 /  1.6 /  5.0 |
| filipecosta90                    | 4,885   | 3       | 66    |  1.2 /  0.0 /  4.2 |
| chenyangyang                     | 4,545   | 3       | 46    |  1.1 /  0.0 /  2.9 |
| meir@redislabs.com               | 4,437   | 12      | 54    |  1.1 /  0.1 /  3.4 |
| hwware                           | 2,881   | 23      | 120   |  0.7 /  0.2 /  7.7 |
| Josh Hershberg                   | 2,128   | 15      | 11    |  0.5 /  0.2 /  0.7 |
| wangyuan21                       | 2,014   | 1       | 45    |  0.5 /  0.0 /  2.9 |
| zhaozhao.zz                      | 1,784   | 207     | 85    |  0.4 /  2.2 /  5.4 |
| Harkrishn Patro                  | 1,723   | 29      | 64    |  0.4 /  0.3 /  4.1 |
| 杨博东                              | 1,674   | 6       | 39    |  0.4 /  0.1 /  2.5 |
| Itamar Haber                     | 1,647   | 20      | 469   |  0.4 /  0.2 / 29.9 |
| huangzhw                         | 1,627   | 5       | 51    |  0.4 /  0.1 /  3.3 |
| Ozan Tezcan                      | 1,373   | 30      | 60    |  0.3 /  0.3 /  3.8 |
| Nick Chun                        | 1,369   | 1       | 11    |  0.3 /  0.0 /  0.7 |
| itamar                           | 1,359   | 2       | 42    |  0.3 /  0.0 /  2.7 |
| Pieter Cailliau                  | 1,332   | 1       | 165   |  0.3 /  0.0 / 10.5 |
| Chen Tianjie                     | 1,259   | 22      | 43    |  0.3 /  0.2 /  2.7 |
| ranshid                          | 1,186   | 13      | 48    |  0.3 /  0.1 /  3.1 |
| YaacovHazan                      | 1,180   | 25      | 58    |  0.3 /  0.3 /  3.7 |
| perryitay                        | 1,146   | 6       | 55    |  0.3 /  0.1 /  3.5 |
| zhenwei pi                       | 1,129   | 22      | 24    |  0.3 /  0.2 /  1.5 |
| KarthikSubbarao                  | 1,019   | 1       | 11    |  0.2 /  0.0 /  0.7 |
```"
2016524624,13157,hopeseekr,2024-03-23T15:26:26Z,"@zuiderkwast 

> My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.

```
	 Viktor Söderqvist <viktor@zuiderkwast.se>:
	  insertions:    334	(0%)
	  deletions:     94	(0%)
	  files:         8	(0%)
	  commits:       3	(0%)
	  lines changed: 428	(0%)
	  first commit:  Tue Apr 13 23:58:05 2021 +0200
	  last commit:   Tue Dec 6 10:25:51 2022 +0100

	 Viktor Söderqvist <viktor.soderqvist@est.tech>:
	  insertions:    9121	(1%)
	  deletions:     3352	(1%)
	  files:         273	(1%)
	  commits:       60	(1%)
	  lines changed: 12473	(1%)
	  first commit:  Wed Jan 13 15:14:51 2021 +0100
	  last commit:   Wed Mar 13 16:02:00 2024 +0100
```

Active lines of code:

```
+----------------------------------+---------+---------+-------+--------------------+
| name                             | loc     | commits | files | distribution (%)   |
+----------------------------------+---------+---------+-------+--------------------+
| Viktor Söderqvist                | 7,484   | 63      | 111   |  1.8 /  0.7 /  7.1 |
```

1.8% of the active code base. You, sir, have good standing in a court of law."
2016529095,13157,hopeseekr,2024-03-23T15:44:43Z,"@madolson  wrote:

> > This change is probably uncalled for since it has been claimed that AWS has funded one or more Redis developers for years.
> 
> I appreciate that people claim I exist. I'm moving to development here, https://github.com/madolson/placeholderkv. Not a great name, but trying to get a lot of the old contributors here to help resume developing where we left off. Trying to keep as much the same as possible for now, but I'm sure we'll want to change.

madolson's contributions:

```
	 Madelyn Olson <34459052+madolson@users.noreply.github.com>:
	  insertions:    6307	(1%)
	  deletions:     2673	(1%)
	  files:         389	(1%)
	  commits:       86	(1%)
	  lines changed: 8980	(1%)
	  first commit:  Tue Jul 21 17:00:13 2020 -0700
	  last commit:   Sun Mar 17 00:06:51 2024 -0700

+----------------------------------+---------+---------+-------+--------------------+
| name                             | loc     | commits | files | distribution (%)   |
+----------------------------------+---------+---------+-------+--------------------+
| Madelyn Olson                    | 6,004   | 128     | 177   |  1.4 /  1.3 / 11.3 
```

With 1.4% of the active source code, you and Viktor have contributed 4.2% of the active source code, which has been relicensed without your consent. You have good standing in court."
2016533469,13157,hopeseekr,2024-03-23T16:02:34Z,"> @XtremeOwnageDotCom is there anything in the 3-clause BSD license that says it needs to be included in every file? It doesn't look like it, but maybe I'm misreading it. The parts where it reads ""Redistributions of source code must retain the above copyright notice"" wording sure seems like keeping a copy of it in the repository (which they've done) meets the requirement.

This is covered by contract law as ajudicated in both the United States, UK, and several countries of the EU (notably France, Germany, and Switzerland).

The courts have universally decreed that copyright and license clauses on top of files of a source code project act as additional explicit conveyances of rights and restrictions per the authors of those files.

Especially in the case of closed-source software, such clauses act as safeguards against unsupervised leaks or stealing of the code, especially when used in other projects or when inappropriately relicensed without 100% of contributors' with active source code consent... This particular contract law has been affirmatively adjudicated in the UK, US and Germany in separate cases.

Every developer *should* add copyright clauses to the top of their files, such as this one from one of my projects:

```
/*
 * This file is part of Uplifted Animals, a Better Rimworlds Project.
 *
 * Copyright © 2024 Theodore R. Smith
 * Author: Theodore R. Smith <hopeseekr@gmail.com>
 *   GPG Fingerprint: D8EA 6E4D 5952 159D 7759  2BB4 EEB6 CE72 F441 EC41
 *   https://github.com/BetterRimworlds/UpliftedAnimals
 *
 * This file is licensed under the Creative Commons No-Derivations v4.0 License.
 * Most rights are reserved.
 */
```

Altering these clauses without 100% written consent or copyright assignment contracts from all contributors to that file (with active lines of code) is tanamount to breach of contract, and all rights to publishing / making a product are liable for damages from the rights holders. It technically no lnoger becomes ""legal"" software, as it is a breach of contract.

Because GitHub is a US-based company, and this project is hosted there, the most direct line of legal action is for the contributors to submit DMCA takedown requests for individual files they have active lines of code on. I have [compiled the list of authors here](https://gist.github.com/hopeseekr/e840bd2346b7eaab62f77f0c798e5668#file-redis-active-lines-log-L11), if anyone reaches out, I can run reports that show the exact lines they have rights to.

https://docs.github.com/en/site-policy/content-removal-policies/guide-to-submitting-a-dmca-takedown-notice

Since Redis, Ltd., is a UK company, you would need to file claims against the Copyright, Designs, and Patents Act 1988 (CDPA). "
2016537682,13157,GreyXor,2024-03-23T16:19:01Z,"https://redict.io/posts/2024-03-22-redict-is-an-independent-fork/
So long Redis. Hello [Redict](https://codeberg.org/redict/redict)"
2016570120,13157,ListsOfArrays,2024-03-23T18:32:31Z,"![ship it revert!](https://github.com/redis/redis/assets/10949346/b16435e4-052b-466e-bc45-a989a1981307)
"
2016594843,13157,Samk13,2024-03-23T20:27:18Z,"Are there other links or discussions for legit communities on Github that are willing to fork and maintain Redis from its previous license version and create a solution like OpenSearch following Elasticsearch's licensing changes?
please share links here so we get notified!"
2016620957,13157,tcarrio,2024-03-23T22:37:52Z,"@Samk13 and others looking for that kind of info: LOTS of links have been shared to forks like **Redict.** Read through the feed instead of commenting, it's just burying them further."
2016620997,13157,Exagone313,2024-03-23T22:38:08Z,"IANAL

I think there are two points that do not seem to be explained here:
* If I get a shallow copy of latest Redis source code, I am not able to determine which parts of the code are covered by the original license terms.
* It is not clear if you can add additional terms to the parts of the code covered by the original license terms. If you sign a contract with someone, obviously you can't add new terms to the signed contract. It could be different from a country to another, though."
2016624891,13157,Samk13,2024-03-23T22:59:59Z,">  LOTS of links have been shared to forks like **Redict.** 

I'm a bit skeptical since it's not hosted on GitHub, so I'm wondering why this fork wasn't hosted here instead of some other site I'm not familiar with. 
It makes me question the legitimacy of it. "
2016658885,13157,NicholasFlamy,2024-03-24T01:41:58Z,"> > LOTS of links have been shared to forks like **Redict.**
> 
> Thanks for the emphatic comment @tcarrio as you describe your self :) it seems you misinterpreted my comment. I appreciate you mentioning that Redict link, but I'm a bit skeptical since it's not hosted on GitHub. as is the standard for open-source projects, so I'm wondering why this fork wasn't hosted here instead of some other site I'm not familiar with. It makes me question the legitimacy of it.

Codeberg is a more open alternative to GitHub, kinda like GitLab."
2016668327,13157,impredicative,2024-03-24T02:24:57Z,"In the public interest, I will note that GitLab and others are infrastructurally more fly-by-night, subject to its MBA parasite induced revenue constraints that put their long-term operations at risk. GitLab already diminished their free offerings not too long ago, with a very small notice period. In contrast, Microsoft is a behemoth whose existence is at this point tied to the national security of the United States.

The point is that despite any free code by Codeberg/Gitea and GitLab, allowing for potential self-hosting, as much as I applaud their efforts, **too many people will not take any project seriously if it's not hosted on GitHub**. If GitHub is good enough for Apache projects, and it increasingly is, then it's good enough for most projects. Certainly any forks of Redis do not merit any special infrastructural exemption like some ""base layer"" or ""small hobby"" projects might."
2016698144,13157,Tertle950,2024-03-24T05:22:13Z,"> Codeberg/Gitea

_It's pedantism time~!_
Because of concerns with Gitea Ltd, Codeberg currently uses their own fork of Gitea that they've called Forgejo - and has done so for a while now.
It started as a soft fork, but [it became a hard fork somewhat recently.](https://forgejo.org/2024-02-forking-forward/)"
2016776663,13157,Samk13,2024-03-24T11:22:46Z,"> Codeberg is a more open alternative to GitHub, kinda like GitLab.

While Codeberg could be a good open-source alternative, GitHub has become the de facto standard platform for many developers and open-source projects. We all ""live"" here. Migrating to a new platform would create friction and barriers for existing contributors and It will split the community and scatter the efforts. 
If there isn't a strong reason for the move, it raises a significant question mark for me.  

The decision to move away from GitHub may not be worth the effort and potential loss of contributors.  the new initiative should prioritize the long-term sustainability and growth of the project and its community.
I would fully support the [`Redict`](https://codeberg.org/redict/redict) like [other initiatives](https://github.com/redis/redis/pull/13157#issuecomment-2013219637) if it remains entirely on GitHub. 🙌
"
2016820000,13157,yannick-was-taken,2024-03-24T14:03:15Z,"I see people calling this ""misguided"". I think the word you're all looking for is ""incredibly fucking stupid""."
2016870631,13157,filipesmedeiros,2024-03-24T17:00:07Z,"It's funny that in three days we went from ""Redis is changing licences"" to ""where is the best place to host the new Redis fork"""
2016939572,13157,dani0854,2024-03-24T20:52:36Z,"It's also would be nice to have a non copyleft fork of redis. Preferably with the original BSD-3 license, since what's a point of changing the license at all."
2016942127,13157,PenguRinTwo,2024-03-24T21:02:42Z,"> It's also would be nice to have a non copyleft fork of redis. Preferably with the original BSD-3 license, since what's a point of changing the license at all.

From what I've read [PlaceHolderKV](https://github.com/placeholderkv/placeholderkv) by @madolson will most likely be sticking with the BSD-3 license."
2016958057,13157,OutOfThisPlanet,2024-03-24T21:56:28Z,"> Read more about the license change here: [Redis Adopts Dual Source-Available Licensing](https://redis.com/blog/redis-adopts-dual-source-available-licensing/) Live long and prosper 🖖

What a strange way to kill a product.
Not bad for a first pull request! ;)
I need to point out that you're a bit early for April Fools though.

You should probably change your profile to be more accurate.
""Director of Product Mismanagement""

Has a nice ring to it, doesn't it? ;)

Also, are you licensed to use that phrase from Star Trek?"
2016966845,13157,dani0854,2024-03-24T22:22:38Z,"Also, please correct me if understand it wrong.

According to their FAQ in question 21. If you create a product using Redis, which start to earn money. Redis can just look at it, see it commercially viable and then create a similar product. And you will be forced to fix Redis version in order not to become a competitor and break their license.

That's like the worst way to make Redis appealing to anyone."
2017023944,13157,jameshilliard,2024-03-25T00:43:36Z,"Note that as Redis Labs executives have made explicit statements such as [""First, let me assure you that Redis remains and always will remain, open source, BSD license.""](https://news.ycombinator.com/item?id=17819392) one may presumably continue to use future Redis releases under the original BSD license terms in addition to any other listed licenses as a reasonable person would likely interpret this statement to be a perpetual irrevocable license grant to use any version of Redis as distributed by Redis Labs under the original BSD license terms(IANAL but I would presume written statements made by company executives regarding licensing to be legally enforceable). Note that this likely would not apply to versions of Redis distributed by entities other than Redis Labs as this implied perpetual irrevocable license grant was only explicitly made by Redis Labs."
2017082556,13157,MisterrrX,2024-03-25T02:06:05Z,That's one way to kill a project
2017088097,13157,madolson,2024-03-25T02:13:00Z,"> From what I've read [placeholderkv](https://github.com/placeholderkv/placeholderkv) by @madolson will most likely be sticking with the BSD-3 license.

Yeah, our plan is to keep it the same. I'm trying my best to follow the will of the community there, and the general preference seems to be to keep the license the same. "
2017185715,13157,PatrickJS,2024-03-25T04:24:56Z,"can everyone review my PR to fix this
https://github.com/redis/redis/pull/13169

update: 
looks like they closed it 😢 long live the community fork https://github.com/placeholderkv/placeholderkv"
2017548630,13157,Jimmy-ahlberg,2024-03-25T09:22:43Z,"> > @XtremeOwnageDotCom is there anything in the 3-clause BSD license that says it needs to be included in every file? It doesn't look like it, but maybe I'm misreading it. The parts where it reads ""Redistributions of source code must retain the above copyright notice"" wording sure seems like keeping a copy of it in the repository (which they've done) meets the requirement.
> 
> This is covered by contract law as ajudicated in both the United States, UK, and several countries of the EU (notably France, Germany, and Switzerland).
> 
> The courts have universally decreed that copyright and license clauses on top of files of a source code project act as additional explicit conveyances of rights and restrictions per the authors of those files.
> 
> Especially in the case of closed-source software, such clauses act as safeguards against unsupervised leaks or stealing of the code, especially when used in other projects or when inappropriately relicensed without 100% of contributors' with active source code consent... This particular contract law has been affirmatively adjudicated in the UK, US and Germany in separate cases.
> 
> Every developer _should_ add copyright clauses to the top of their files, such as this one from one of my projects:
> 
> ```
> /*
>  * This file is part of Uplifted Animals, a Better Rimworlds Project.
>  *
>  * Copyright © 2024 Theodore R. Smith
>  * Author: Theodore R. Smith <hopeseekr@gmail.com>
>  *   GPG Fingerprint: D8EA 6E4D 5952 159D 7759  2BB4 EEB6 CE72 F441 EC41
>  *   https://github.com/BetterRimworlds/UpliftedAnimals
>  *
>  * This file is licensed under the Creative Commons No-Derivations v4.0 License.
>  * Most rights are reserved.
>  */
> ```
> 
> Altering these clauses without 100% written consent or copyright assignment contracts from all contributors to that file (with active lines of code) is tanamount to breach of contract, and all rights to publishing / making a product are liable for damages from the rights holders. It technically no lnoger becomes ""legal"" software, as it is a breach of contract.
> 
> Because GitHub is a US-based company, and this project is hosted there, the most direct line of legal action is for the contributors to submit DMCA takedown requests for individual files they have active lines of code on. I have [compiled the list of authors here](https://gist.github.com/hopeseekr/e840bd2346b7eaab62f77f0c798e5668#file-redis-active-lines-log-L11), if anyone reaches out, I can run reports that show the exact lines they have rights to.
> 
> https://docs.github.com/en/site-policy/content-removal-policies/guide-to-submitting-a-dmca-takedown-notice
> 
> Since Redis, Ltd., is a UK company, you would need to file claims against the Copyright, Designs, and Patents Act 1988 (CDPA).

Sound reasoning. Being more familiar with Swedish law myself I'm not confident that my analysis under that legal system would be the same. Could you provide the references to the case law mentioned, it would be interesting to read up on at least a few of them :) "
2017702376,13157,OutOfThisPlanet,2024-03-25T10:41:37Z,"In case this is in any way useful:

> If you believe that content on GitHub infringes a valid copyright you own, please see our [Copyright - DMCA Takedown Policy](https://docs.github.com/github/site-policy/dmca-takedown-policy) and our [Guide to Submitting a DMCA Takedown Notice](https://docs.github.com/github/site-policy/guide-to-submitting-a-dmca-takedown-notice)."
2019063472,13157,TwiN,2024-03-25T22:58:37Z,"Legalities aside, I can't help but feel saddened by the gradual rise in instances of widely used previously open-source software being re-licensed to licenses that deviate from the core essence of open source.

I also can't help but fear what the future of open source will look like if the norm becomes such that the moment a project becomes popular enough, it gets taken away from the community that helped build said project in the name of monetary gains.

While I don't agree with the approach taken, I'm also not oblivious to the fact that it is true that large companies (e.g. cloud providers) stand to benefit a lot from leveraging existing open source solutions and selling it as their own without necessarily giving an equivalent amount of ""love"" back to the maintainers of the open source software they profit from, I wish we could all sit at a round table and think about the greater good of open source instead of each company focusing on their own profit margins."
2019537782,13157,dani0854,2024-03-26T07:10:40Z,"> Live long and prosper 🖖

Now each time I remember this phrase, it associates with this PR, and with the problem of re-licensing to source available licenses. It was such a good phrase, why did it have to be used in such a controversial context. I am in the middle of watching Voyager, and it's just stuck with me."
2019715484,13157,rdesgroppes,2024-03-26T07:58:29Z,"This unilateral merge is a betrayal of how many, about 700 (BSD) contributors?

:vomiting_face:"
2019797807,13157,50-Course,2024-03-26T08:22:44Z,@TwiN just articulated my thoughts clearly in few paragraphs. 
2019827864,13157,romange,2024-03-26T08:39:30Z,"> Legalities aside, I can't help but feel saddened by the gradual rise in instances of widely used previously open-source software being re-licensed to licenses that deviate from the core essence of open source.
> 
> I also can't help but fear what the future of open source will look like if the norm becomes such that the moment a project becomes popular enough, it gets taken away from the community that helped build said project in the name of monetary gains.
> 
> While I don't agree with the approach taken, I'm also not oblivious to the fact that it is true that large companies (e.g. cloud providers) stand to benefit a lot from leveraging existing open source solutions and selling it as their own without necessarily giving an equivalent amount of ""love"" back to the maintainers of the open source software they profit from, I wish we could all sit at a round table and think about the greater good of open source instead of each company focusing on their own profit margins.

I do not think that this path is gonna repeat itself for the next wave of companies, in fact it does not even for companies that started a few years ago. All the next-gen startups operating in infra space chose a license that protects them right from the start from such situations. I am talking about ScyllaDB, Redpanda, cockroachdb, DragonflyDB. I can personally say, we chose to be upfront with Dragonfly community, knowing the price we gonna pay for having fewer distribution options for Dragonfly. 

Redis is a bit different and unique - it started early, when OSS movement was still naive, and it was founded by a single developer. 
"
2019996298,13157,dani0854,2024-03-26T09:59:42Z,"> All the next-gen startups operating in infra space chose a license that protects them right from the start

Definitely not all chose to go with ""source available"" licenses. And if we count ""source available"" vs open source, I have a feeling that open source will have more (though I didn't check). For example, we can look at new projects in Linux foundation, CNCF and Apache foundation.

Although I don't have much experience in OSS. To me, it looks like this path will lead back to where it started, with two opposite sides, proprietary and copyleft software. This doesn't look that optimistic."
2020032794,13157,kevin-schmid,2024-03-26T10:15:56Z,So let me get this straight. A company that hosted redis [was and is unable to operate profitably since it was founded.](https://techcrunch.com/2019/02/19/redis-labs-raises-a-60m-series-e-round/) A company which [needed $347 million in funding](https://www.zdnet.com/article/redis-labs-surpasses-2b-valuation-with-latest-funding-round/). A company who acquired the name Redis in **2021**. This very company which only exists because redis was FOSS has the audacity to change the license.
2020673140,13157,hopeseekr,2024-03-26T15:01:08Z,"After analyzing this new redict fork, it's all bs... 

**The Redict maintainer has illegally relicensed all of redis code to a much more restrictive license: The LGPL.**

This is *blatant* breach of ethics, even worse than what Redis, Ltd., has done (since they arguably have permission to relicense at least 50% of the code)."
2020715437,13157,hopeseekr,2024-03-26T15:16:53Z,"@Jimmy-ahlberg :
> Could you provide the references to the case law mentioned, it would be interesting to read up on at least a few of them :)

The most defining case was decided by the United States Supreme Court, decided on 5 April 2021: Google, LLC, v. Oracle America, Inc. (185-96).

https://copyrightalliance.org/copyright-cases/oracle-america-v-google/

The Supreme Court decided that 

1. Copyright clauses on top of files do, in fact, dictate the license terms of the individual file, as an additional safeguard against improper obtaining (e.g., via accidental plublishing, espionage, and/or theft).
2. They delineated 4 fair-use cases for using the class and function signatures (e.g., ""source code API"") and even ""binary API"".
3. They ruled in favor of Google in all four cases. 
4. They upheld that the license headers of Oracle remained in the Google Android source code, unmodified, and thus Google did not violate that pivotal part of now-firm contract law.

What I got out of it, in revisiting it for this Redis relicensing debacle is that Redis, Ltd's, lawyers who OK'd this license change need to immediately file a claim with their Accidental Damages, Errors and Omissions insurance, because they gave very, very bad legal advice.

If Google hadn't left the Oracle copyright + licensing clauses in the Java SDK code they used in Android OS (while they did add a couple lines of their own, such as dictating their own copyright claims), then they would have been in breach of the read-only Oracle license and would have been infringing on both the license and Oracle's copyrights.

Put in another way, that's exactly what Redis has done on about 90% of its code base. They are deifnitely in violation. And that Redict fork is even more egregious, because that guy has no claims whatsoever. If I were him, I'd both immediately delete this repo and hire an attorney.

https://www.lewisrice.com/publications/supreme-court-rules-that-googles-use-of-java-source-code-for-android-phone-is-permissible-fair-use/ "
2020724192,13157,rpeden,2024-03-26T15:19:30Z,"> After analyzing this new redict fork, it's all bs...
> 
> **The Redict maintainer has illegally relicensed all of redis code to a much more restrictive license: The LGPL.**
> 
> This is _blatant_ breach of ethics, even worse than what Redis, Ltd., has done

How so? This kind of sublicense is permitted by the BSD license, and Redict isn't removing the BSD license from existing code, as is clear if you look at the additions to the source files.

It's described in more detail here: https://redict.io/posts/2024-03-22-redict-is-an-independent-fork/

There seem to be a lot of folks confused about what's allowed and what's not.

Note that I'm not saying whether this is a good idea is not - that's a separate discussion."
2020739350,13157,hopeseekr,2024-03-26T15:25:27Z,"> How so? This kind of sublicense is permitted by the BSD license, and Redict isn't removing the BSD license from existing code, as is clear if you look at the additions to the source files.

You can't relicense code that you don't own rights to.

That's the end of it.

He has absolutely no right to relicense any line of code he doesn't own rights to. 

In this case, it's a form of outright copyright infringement, since he's making the license more restrictive, and thus infringing on their rights:

> Choosing a more restrictive license for the forked project's new contributions can introduce legal complexities, especially around the compatibility of licenses. If the new license imposes restrictions that contradict the freedoms granted by the MIT License, it could potentially lead to legal conflicts or limit the usability and community acceptance of the forked project.

We ran into this problem a LOT in the mid-to-late 2000s with all the inappropriately GPL and LGPL code out there. The industry standardized on using MIT / BSD Licenses for almost everything. Now, 20 years later, we're swinging back into proprietary source land.

P.S. For the flipside of this, look at all the controversies in the 2010s of people dedicating their entire lives to building amazing Wordpress and Drupal plugins and themes, only to be FORCED and COMPELLED to live by the GPL of Wordpress, even tho they didn't use any of its code.

Inconsiderate leakers would continuously publish (completely legally) GPL forks of the project, for free, and the modders had no ability to sell it or fight back or anything. Because unlike the BSDL and MIT License, with GPL or LGPL or AGPL, you as the author are not able to license new code in a more restrictive way to preserve your rights to sell and fight hostile forks.

This si why I had to quit the project I dedicated my life to from 2002-2007, hostile forkers of a GPL project.

There is a middle ground and that seems to be the [Creative Commons Attribution License](https://wellcome.org/grant-funding/guidance/open-access-guidance/creative-commons-attribution-licence-cc) (very permissive), coupled with the [OSSAL Anti-GPL license](https://lists.opensource.org/pipermail/license-discuss_lists.opensource.org/2003-September/007132.html) (that prohibits the project from being used by projects licensed under the GPL and AGPL)."
2020948450,13157,ddevault,2024-03-26T16:40:30Z,"Hello, I'm the one who started the Redict project.

A lot of people are struggling to understand how this sublicensing works, so I understand the confusion here. However, Redict is perfectly in compliance with the Redis OSS license. Permissive licenses like Redis OSS's 3-Clause BSD license are, almost to a generalization, *defined* by the fact that they allow you to do this, in contrast to copyleft licenses. Note that Redis Ltd *also* does not hold the copyright over the Redis code (they never obliged contributors to agree to a copyright assignment or CLA), and the sub-licensing that Redis Ltd has done with the SSPL is dependent on the exact same legal framework that allows for Redict to do so.

Redict has not unilaterally changed the license, rather we have applied the LGPL to any changes we have made to the project *on top* of the original BSD licensed codebase, and the resulting combined work uses the LGPL. The original 3-Clause BSD license is still there and all of its terms (namely attribution and the reproduction of the copyright notice and license terms) are all present and in order. The BSD license can be found under `LICENSES/` and in every source file's copyright notice, and the repository as a whole has been brought into compliance with the [REUSE](https://reuse.software) standard to make the overall licensing situation clear.

The [license FAQ](https://redict.io/docs/license/) goes into more detail, and I would be happy to answer specific questions here.

---

Anyway: I came here to mention that Redict 7.3.0-rc1 is available and ready for general testing:

https://redict.io/posts/2024-03-26-redict-release-candidate-7.3.0-rc1/"
2021615724,13157,Samk13,2024-03-26T23:07:04Z,">  I would be happy to answer specific questions here.

@ddevault Could you provide more details on the decision to move away from GitHub and its impact on the community?
"
2021876788,13157,madolson,2024-03-27T03:55:19Z,"Guys, we're almost at 1000 down votes. Keep going!
![image](https://github.com/redis/redis/assets/34459052/386cff10-9209-4727-8fad-dd70a20efc89)
"
2022090118,13157,50-Course,2024-03-27T07:04:42Z,"> > I would be happy to answer specific questions here.
> 
> @ddevault Could you provide more details on the decision to move away from GitHub and its impact on the community?

actually, i'd like to understand too 👍🏼 "
2022163197,13157,bu6n,2024-03-27T08:04:24Z,"> > > I would be happy to answer specific questions here.
> > 
> > 
> > @ddevault Could you provide more details on the decision to move away from GitHub and its impact on the community?
> 
> actually, i'd like to understand too 👍🏼

[In the blog post](https://redict.io/posts/2024-03-22-redict-is-an-independent-fork/):

> This opportunity is being used to establish a community independent of proprietary infrastructure, such as GitHub and Slack. The source code is hosted on [Codeberg](https://codeberg.org/redict/redict), a Forgejo instance operated by a German non-profit, which should provide a comfortable and familiar user experience for anyone comfortable with the GitHub-based community of Redis® OSS. Additionally, we have established an IRC channel, [#redict on libera.chat](https://web.libera.chat/?channel=#redict), where the burgeoning community is being organized.

Not sure this needs more explanation?"
2022190405,13157,duncan-bayne,2024-03-27T08:23:46Z,"Yup, it's covered on the blog post:

> This opportunity is being used to establish a community independent of proprietary infrastructure, such as GitHub and Slack. 

IMO that's great news ❤️. I wish more FLOSS projects would take this direction."
2022380438,13157,ddevault,2024-03-27T10:09:52Z,"> > I would be happy to answer specific questions here.
> 
> @ddevault Could you provide more details on the decision to move away from GitHub and its impact on the community?

Certainly. Forgive me for answering at length.

For context and full disclosure, I have a vested interest in projects making use of platforms other than GitHub, since I am the founder of one, namely SourceHut. However, I do *not* have a vested interest in Codeberg beyond the fact that it serves all of our interests for the GitHub monoculture to be weakened. I selected Codeberg to balance the following constraints:

1. Using free software infrastructure to host free software projects is important.
2. Codeberg is more familiar to the existing community from GitHub than SourceHut.
3. To reduce the appearance that I am serving my own commercial interests by hosting Redict on SourceHut.

To elaborate on the first point:

This is a traumatic moment for the Redis community, but also for the free software community. This has happened too many times: MongoDB, ElasticSearch, now Redis, and many other projects, hell, even Solaris, and many projects with commercial stewards are transparently keeping the same strategy in their back pocket with CLAs and copyright assignments from contributors. We could just repeat history, gather the diaspora, and try to rally behind a hastily made fork, throw it up on GitHub, BSD license it, get the commercial interests on board, and press on like everything is normal. But, I think that this is an important moment to question our values, and how we ended up here, and what kind of changes we need to make to curtail this *trend*.

It's obvious that we are upset by this change, as succinctly demonstrated by the 1000+ :-1:'s on this thread. Why? Is it because we are annoyed by the nuisance of having to set up a fork and change all of our software over, or else pony up for Redis SAL? Or simply because it's a transparent and poorly justifiable cash grab and that offends our sensibilities no matter how it presents? These are valid, but shallow reasons to be upset. I want to look deeper.

Redis Ltd né Redis Labs has done something which, in my view, is morally wrong, and violates the spirit and social contract of free software. That is the root of the problem. Redis Ltd did not make Redis, they were major contributors but it was built with the hard work of many individuals independently of them, at first antirez, but also individuals representing AWS, GCP, and other commercial interests, as well as, most importantly, a community of independent contributors. All of these people hold the copyright to Redis in aggregate, and Redis Ltd's changes are only possible because of the permissive BSD license, but not because of any [moral rights](https://en.wikipedia.org/wiki/Moral_rights) to the software. That's not to mention the community of users and downstream maintainers who helped popularize it on the back of the social contract of free software, incorporating it into Linux distributions, building more free software that made use of it, and so on, none of which is likely to have happened at the same scale if not for this social contract.

So, when we are confronted with this crisis, I want to know: what are our values, and how can we better assert our values to prevent these catastrophes from occurring? We believe in the value of free software, so we should walk the walk. This is a moment for solidarity. We can use our strength to re-enforce free software communities like Codeberg, rather than lending our legitimacy to non-free communities like GitHub and Slack. It's true that GitHub has popularity going for it, but how can we address that as a community? By putting *more* projects on it? The discomfort we'll face in bringing people to a new platform is a rounding error compared to the discomfort caused by this event as a whole, and there's no better time to reconsider these choices than now.

This is a political event, and this is a political solution. This is what has guided my thoughts on the formation of this fork, including the platform it's hosted on, and the license, and so on. This is also why I am calling on you to support a grassroots approach over depending on commercial interests once again.

I hope that helps."
2022626960,13157,Samk13,2024-03-27T12:16:39Z,"> I hope that helps.

I fully support your stance @ddevault and share the sense of betrayal over Redi's actions and the broader trend of companies monetizing and controlling open-source projects. 
corporate greed will not stop on this project only.  so deeper actions are needed to protect the spirit of free software.

Merely migrating to alternative platforms may not fully solve the problem Look at OpenAI, even non-profit entities can be co-opted by big money interests and have their open-source ethos compromised. 
Platforms surviving on donations could face similar risks of being bought out or influenced when they gain enough traction.
In my opinion and I am no one except another open source maintainer and an immigrant myself, rather than just migrating to different platforms:

- Broader political actions at the governmental level are required, like within the EU or any entity capable. Enacting regulations restrictions and funding to help safeguard major open-source projects from appropriation by commercial interests. 
- Oversight and regulation central platforms like GitHub are also well needed to ensure alignment with free software principles.

This is a call for the open-source community to reassert its values through a multi-pronged effort!  
grassroots initiatives coupled with concrete regulatory measures are needed for sure. 

Keep running away will not solve the root cause issue!"
2022635958,13157,ddevault,2024-03-27T12:22:10Z,"I agree with you, this is but one of many lines of activism and organizing that I am pursing. I set up a [website](https://writefreesoftware.org) to popularize and document the philosophy and practices of free software late last year, and a few weeks ago I set up [a discussion forum](https://discourse.writefreesoftware.org) as a part of that to build a stronger community with a focus on free software and political action with respect to free software. Please join us and help expand the movement :)"
2022972435,13157,PenguRinTwo,2024-03-27T14:51:50Z,"For these that want to stay on GitHub with the original BSD-3 license, [this](https://github.com/placeholderkv/placeholderkv) this is what the main community is gonna be working on."
2022994196,13157,q0rban,2024-03-27T14:59:07Z,"> please use sircwncmp's redict: https://codeberg.org/redict/redict

> For these that want to stay on GitHub with the original BSD-3 license, [this](https://github.com/placeholderkv/placeholderkv) this is what the main community is gonna be working on.

I know these things can be challenging to navigate, given differing values between core contributors, but I think it would benefit the entire community if the forking (ahem) contributors could collaborate on a single fork, working through whatever compromises that need to take place. When MySQL did this, we ended up with Percona and MariaDB, which created confusion for both contributors and end-users alike."
2023005224,13157,ddevault,2024-03-27T15:04:12Z,">I know these things can be challenging to navigate, given differing values between core contributors, but I think it would benefit the entire community if the forking (ahem) contributors could collaborate on a single fork, working through whatever compromises that need to take place. When MySQL did this, we ended up with Percona and MariaDB, which created confusion for both contributors and end-users alike.

The Redict camp is committed to our approach and in particular the copyleft license for the reasons I stated above, but we'd be happy to work with anyone that can get on board with that plan. We approached placeholderkv but they aren't particularly interested. The discussion mostly took place here:

https://github.com/placeholderkv/placeholderkv/issues/18"
2023012653,13157,madolson,2024-03-27T15:07:20Z,"> The Redict camp is committed to our approach and in particular the copyleft license for the reasons I stated above, but we'd be happy to work with anyone that can get on board with that plan. We approached placeholderkv but they aren't particularly interested. The discussion mostly took place here:

> I know these things can be challenging to navigate, given differing values between core contributors, but I think it would benefit the entire community if the forking (ahem) contributors could collaborate on a single fork, working through whatever compromises that need to take place. When MySQL did this, we ended up with Percona and MariaDB, which created confusion for both contributors and end-users alike.

It is a fundamentally different set of opinions. We (placeholderkv/new BSD-3 fork) want to keep Redis more or less as it was but just keep developing it in open-source, and the Redict folks don't. I didn't see a way to reconcile them. Drew listed out the detailed discussion. "
2023062657,13157,ddevault,2024-03-27T15:29:39Z,"> It is a fundamentally different set of opinions. We (placeholderkv/new BSD-3 fork) want to keep Redis more or less as it was but just keep developing it in open-source, and the Redict folks don't. I didn't see a way to reconcile them. Drew listed out the detailed discussion.

To phrase the same differences in my own words, I would say that placeholderkv represents a small quorum among commercial stakeholders in Redis (e.g. AWS, GCP, etc) who are interested in resuming business as usual as soon as possible, without being willing to have a conversation about the points I raised in this thread, and the fact that business as usual is what led us to this situation to begin with. placeholderkv is about minimizing the financial impact of the Redis re-licensing as it pertains to AWS et al, and ensuring that control over Redis is maintained in the hands of its commercial interests rather than a broader community.

I am disappointed, but not surprised, by this outcome. The only hard requirement we posed to placeholderkv for merging our forks and keeping the community united was the use of a copyleft license, and we proposed one that would meet the needs of all of the commercial stakeholders without imposing onerous compliance regimes (like the AGPL, for instance, would have). Nevertheless, this was rejected by the commercial stakeholders, and so we are moving forward with an independent, grassroots approach which places a greater emphasis on the needs of the free software community as a whole than on the wishes of a bloc of commercial interests.

It is unfortunate that our forks must compete, but I hope for the future of free software that Redict wins out."
2023132694,13157,madolson,2024-03-27T15:58:51Z,I'm disappointed you view it in such a black and white way.
2023152770,13157,q0rban,2024-03-27T16:06:22Z,"It sounds like all parties _want_ to come to an accord, but it's been challenging to come to agreed upon compromises. Would it be possible for the interested parties to hop on a call to discuss this? Sometimes nuance can get lost when communication is held solely via text on a screen. If it would be helpful, I'd be happy to set that up, as a mostly unbiased third party."
2023219421,13157,dani0854,2024-03-27T16:30:22Z,"All this pull towards copyleft will just polarize the community into 2 sides, of proprietary software and copyleft software, which is bad outcome (in my opinion). Although it is a natural reaction after companies like Redis switch to source available licenses, but it's not a reason to destabilize things further playing tug of war with big companies, since it is a lost battle from the beginning (in my opinion).

To me true freedom is the permissive licenses, since it the best way businesses and community can work together where community doesn't restrict businesses freedom and vice versa. And if businesses start to pull their way, the best thing is to continue advocating for the freedom permissive licenses give, rather than jumping ship towards copyleft further polarizing community.

P.S. After all this ""copyleft activism"", licenses like [OSSAL Anti-GPL license](https://lists.opensource.org/pipermail/license-discuss_lists.opensource.org/2003-September/007132.html) start to look more appealing, but it also restricts freedom in a way."
2023253445,13157,ddevault,2024-03-27T16:40:36Z,"I don't see any reason why copyleft should polarize anyone. By all means provide an argument against the use of LGPL if you wish; so far I have heard none. I have spoken at length about why it is important.

If I understand correctly, what you are arguing for is a passive freedom, which is the absence of obligations; I am arguing for a positive freedom, which is the guarantee of rights."
2023297306,13157,madolson,2024-03-27T16:51:47Z,"The license is not what is polarizing me. It's the unilateral decision to change it without gathering input. I want to build a coalition of individual and corporate contributors to help maintain a new version of Redis. If some people find the license polarizing, I would rather include them in the discussion and decide later. That is what it means to me to build a community around a project. That is what https://github.com/placeholderkv/placeholderkv is right now. There have been successful examples of these types of groups, PostgreSQL is one that comes to mind that has been quite successful. 

The other comment, which you didn't mention here, was about committing to the name redict, which (Again, I'm not a lawyer) sounds way too much like redis to me. I neither want them coming after us with a cease and desist or to cause confusion about what we are."
2023335256,13157,ddevault,2024-03-27T17:09:13Z,"I hate to raise our dirty laundry here, but alas, this is politics so eventually we must. You have never participated in a discussion on these questions in a manner which is collaborative and open to external input. The consensus you seem to value is transparently the consensus of the four representatives of commercial interests that you have gathered into an insular group in which the ultimate decision making authority is vested. The input of the community, as it were, seems to be relegated to that of an advisory group. The consensus making process which you cite as necessary to change the license is utterly opaque to us.

Redict has strongly encouraged collaboration in all matters related to decision making and governance within the constraints that the raison d'etre of the Redict fork is to create a community which is protected from the kind of abuse of commercial power that led to us being here today, constraints we distilled into the simple choice of a copyleft license for the future of the project. We have invited you to participate at every opportunity, to adopt positions of authority within the project with deference to your experience and resources, and to help establish processes and in every other respect have a foundational role in establishing what Redict shall be. As a matter of fact, @madolson, you are currently an admin on the Redict organization Codeberg, and have since the first time we spoke on the matter a few days ago. We were never extended anything close to this level of respect and willingness to collaborate by the people working on placeholderkv.

>The other comment, which you didn't mention here, was about committing to the name redict, which (Again, I'm not a lawyer) sounds way too much like redis to me. I neither want them coming after us with a cease and desist or to cause confusion about what we are.

I don't think that Redict has any problems with the Redis trademark. If we were adopting a more collaborative approach, perhaps something the commercial stakeholders could offer is the services of your legal team(s) to issue an opinion on the matter."
2023386885,13157,dani0854,2024-03-27T17:34:42Z,"> I don't see any reason why copyleft should polarize anyone. By all means provide an argument against the use of LGPL if you wish; so far I have heard none. I have spoken at length about why it is important.

A quick google search will yield a lot for and against points for copyleft licenses. I am not an expert in LGPL, and don't have a lot of experience, but there are a few people above who showed examples where copyleft is problematic.

Also, I am not really anti GPL, I believe that in certain areas it is a fine license, like in Linux kernel for example.

> If I understand correctly, what you are arguing for is a passive freedom, which is the absence of obligations; I am arguing for a positive freedom, which is the guarantee of rights.

The whole questions, is about whose freedom you are fighting for, since any restriction to maintain a certain group freedom, restrict other groups freedoms. 

In this specific example you are advocating for positive freedom of the free software community, but what about businesses, what about people who want to make a derivative work and monetize it in the way they see fit. I am not saying I agree with them, but why should we take their freedom away just because we disagree on something. The whole point of freedom, is that everyone has it, not just a specific group. 

That's why I like BSD-0 clause license, it's simple, and have pretty much no restrictions at all. Since any restriction, restricts someone's freedom by the definition of the word.

> If some people find the license polarizing, I would rather include them in the discussion and decide later.

I was never involved with Redis (other than as a user), and is not wright for me to participate in the specific decision makings of the fork, and it's community. 

The only reason I posted my opinions above, is that this trend as a whole worries me. Where a permissive license project goes to source available, and then a fork goes completely other way to copyleft. And a broader discussion of this trend was already started above by some people."
2023403080,13157,ddevault,2024-03-27T17:41:51Z,">A quick google search will yield a lot for and against points for copyleft licenses. I am not an expert in LGPL, and don't have a lot of experience, but there are a few people above who showed examples where copyleft is problematic.

I looked through this thread again and I couldn't find any. Can you not make any arguments for yourself?

>The whole questions, is about whose freedom you are fighting for, since any restriction to maintain a certain group freedom, restrict other groups freedoms.

This is a very basic understanding of how freedom works. Your logic taken to its conclusion is just an argument for anarchy. Workers rights are guaranteed by restricting the behavior of businesses. Freedom of the press is guaranteed by restricting the government's ability to censor media.

>In this specific example you are advocating for positive freedom of the free software community, but what about businesses, what about people who want to make a derivative work and monetize it in the way they see fit. I am not saying I agree with them, but why should we take their freedom away just because we disagree on something. The whole point of freedom, is that everyone has it, not just a specific group.

The LGPL was chosen with this case in mind. The most common approach to commercializing Redis, including the approach favored by the four commercial stakeholders working on placeholderkv, is provided for by the LGPL license without imposing any further obligations on these stakeholders, which is in part why it was selected over more aggressive copyleft licenses like AGPL or EUPL.

By the way, I represent a commercial interest and I make my living by selling software that uses a copyleft license. Copyleft does not and has never prevented the commercialization of software."
2023468183,13157,PenguRinTwo,2024-03-27T18:10:44Z,"@ddevault I'm still not convinced you're for the community. 
You came up with the name on the spot without consulting the actual community, unlike PlaceHolderKV, which is still taking a lot of input to decide. 
You decided to use the LGPL without gathering more feedback either and lastly you want to move everyone, devs and users alike to an entire different platform when GitHub itself was never the issue and you are still trying to redirect people there. Noone besides the few involved in Redict were in favor of that either. All these points represent your personal interest rather than the of the actual community that is here on GitHub."
2023476291,13157,dani0854,2024-03-27T18:12:51Z,"> Can you not make any arguments for yourself?

I can, but I won't. Since I believe this is not the best place for such a debate. And I am not an anti GPL activist (yet), so I don't have required knowledge and time to keep up with this debate. I am just a regular person with some opinions (which I have rights to have)

> Your logic taken to its conclusion is just an argument for anarchy.

In a **relatively** simple system anarchy can be good, specifically when it allows people with different opinions to work together. And I am **not** talking about much more complex systems such as humanity and governments. 

> By the way, I represent a commercial interest and I make my living by selling software that uses a copyleft license. Copyleft does not and has never prevented the commercialization of software.

And that's a great thing. But my argument was about people who wish to monetize derivative work the way they see fit (that includes ways LGPL forbids/or makes it hard). And personally I don't agree with their methods of monetization, but that doesn't mean they should not have rights to do that.

Also, it's not like copyleft and GPL specifically have not been abused in the past, red hat drama is great example of that. So to me, it much more important to advocate with projects success rather than play with licenses and laws. People who want to abuse a license will find a way to do that. And the longer and more complicated the license is, the more potential holes it contains. In my opinion it's better to advocate for world where open source is considered the norm by users and big companies, rather than creating complicated licenses to hide behind laws."
2023478409,13157,ddevault,2024-03-27T18:13:17Z,"> @ddevault I'm still not convinced you're for the community. You came up with the name on the spot without consulting the actual community, unlike PlaceHolderKV, which is still taking a lot of input to decide. You decided to use the LGPL without gathering more feedback either and lastly you want to move everyone, devs and users alike to an entire different platform when GitHub itself was never the issue and are still trying to redirect people there. Noone besides the few involved in Redict were in favor of that either. So they all represent your personal interest rather than the of the actual community that is here on GitHub.

For the record, Redict has the same number of contributors as placeholderkv at the time of writing, which is 8, all of whom are quite pleased to be using Codeberg and agree with the choice of license."
2023482059,13157,ddevault,2024-03-27T18:14:05Z,"Ah, correction, Redict has more contributors than placeholderkv as of about 30 minutes ago; now 9."
2023563875,13157,rakoo,2024-03-27T18:26:24Z,"> This is a very basic understanding of how freedom works. Your logic taken to its conclusion is just an argument for anarchy. Workers rights are guaranteed by restricting the behavior of businesses. Freedom of the press is guaranteed by restricting the government's ability to censor media.

Just to be precise here, anarchy as in the political governance system is exactly what you are trying to achieve with redict: a community-based system where everyone has equal rights, putting collective weight behind decisions to enforce positive rights and emancipation for the community. It's the thinking that there is no real freedom without a society working all together towards it, not only for the collective, but also for the individuals.

What you are talking about, allowing as much individual freedom regardless of others, is libertarianism."
2023576683,13157,ddevault,2024-03-27T18:27:53Z,"Yeah, I'm aware of the nuance there, I just thought that calling out @dani0854's arguments as libertarian nonsense directly would be unfair."
2023703633,13157,thedaviddelta,2024-03-27T18:46:28Z,"It's pretty surreal to me that most of the reactions to Redis' announcement along the first week have totally rejected the idea of making it unilaterally non-free, even to the point of thinking the BSD-3 license protected us from that move (that sadly it doesn't) or saying GitHub as a platform should prohibit that move, and now when someone brings to the table the concept of a *copyleft* license, whose mere reason to exist (or one of them) is to protect that freedom, suddenly people dislike the idea and start calling it *polarizing*. If we fork and keep the license, it's a matter of time another bad actor uses all the contributor's work for their benefit and privatizes it again. This should be a call to action not just for this project but for every similar FOSS project."
2023960784,13157,duncan-bayne,2024-03-27T20:50:08Z,"> So they all represent your personal interest rather than the of the actual community that is here on GitHub.

Well, yes. As far as I'm  aware though @ddevault's interests are that (a) the new Redis fork remains free software in perpetuity, and (b) relies on as little proprietary software as is feasible.

As someone who grew up with the proprietary stranglehold over tech in the 90s, I'm astounded to see contributors to free software themselves continue to argue *for* permissive licenses and proprietary technology,  even in a thread discussing the manifest harms those have brought to the *very software they contributed to*.

> For these that want to stay on GitHub with the original BSD-3 license

@PenguRin surely repeating the same steps as last time will lead to the same result again?"
2024011357,13157,duncan-bayne,2024-03-27T21:21:09Z,"> I'm disappointed you view it in such a black and white way.

@madolson are you referring to the choice of license? If you'd advocate a compromise - what would it be?

As far as I can tell there *is* no compromise position possible between *GPL and BSD licensing. One is permissive (and preferred by Redis' commercial users), the other is not (and preferred by the community - just, as measured by contributors so far).

Have you a suggestion for what compromise would look like in this case? Because I genuinely don't. 

I don't see how you can water down either a BSD or LGPL license, and I  don't see how to reconcile the interests of both groups (Redis' old commercial users, and contributors wishing it to remain free in perpetuity)."
2024061735,13157,XtremeOwnageDotCom,2024-03-27T21:58:18Z,"@ddevault @dani0854 @madolson 

Just FYI... both projects can co-exist, and share contributions.... you don't HAVE to agree on a license.

That being said, this is likely not the time, nor place to discuss. Both viewpoints do have valid points, and ideas.

Granted, agreeing, and making a singular project, would be more beneficial in the long run. 

But, then, would come the above discussions between Codeburg, and Github, also- both sides have valid reasons."
2024126032,13157,ddevault,2024-03-27T23:04:04Z,"> Just FYI... both projects can co-exist, and share contributions.... you don't HAVE to agree on a license.

To clarify, Redict can take code from placeholderkv (and we will, if it makes sense to), but not the other way around, because of the difference in license between the two projects."
2024131870,13157,joshmanders,2024-03-27T23:10:43Z,"> > For these that want to stay on GitHub with the original BSD-3 license
> 
> @PenguRin surely repeating the same steps as last time will lead to the same result again?

The problem wasn't the license, the problem was that a single company had complete control over the project including the ability to change said license.

What @madolson is trying to do will solve that issue."
2024135476,13157,ddevault,2024-03-27T23:14:54Z,">The problem wasn't the license, the problem was that a single company had complete control over the project including the ability to change said license.
>
>What @madolson is trying to do will solve that issue.

This is not true. Redis Ltd did not hold any kind of privileged position with respect to the copyright of Redis and control over its licensing. The same is true of placeholderkv: it offers exactly the same protections over the code as Redis did, which is to say none. In fact, Redis Ltd can pull improvements from placeholderkv to incorporate into their proprietary software even now."
2024139694,13157,joshmanders,2024-03-27T23:18:08Z,"> > The problem wasn't the license, the problem was that a single company had complete control over the project including the ability to change said license.
> > What @madolson is trying to do will solve that issue.
> 
> This is not true. Redis Ltd did not hold any kind of privileged position with respect to the copyright of Redis and control over its licensing. The same is true of placeholderkv: it offers exactly the same protections over the code as Redis did, which is to say none. In fact, Redis Ltd can pull improvements from placeholderkv to incorporate into their proprietary software even now.

They owned the trademark to the name of the project and all the material around it. They essentially controlled the codebase regardless of what the license was.

From what I see, what madolson is trying to do is create a governance so that way the community/contributors control it, not a corporation who can, will, and has already decided to take their toys out of the sandbox and not share with anyone else."
2024142094,13157,duncan-bayne,2024-03-27T23:19:44Z,"@joshmanders I think the issue that @ddevault has with @madolson's proposal, though, is that the aforementioned corporation can just copy any improvements the community creates into their own closed codebase.  The purpose of the LGPL license, vs. the BSD license, is to prevent that."
2024146613,13157,joshmanders,2024-03-27T23:25:06Z,"@duncan-bayne I don't think the idea of this fork is really to say nobody could do this, but to say ""You don't get free direct work from us, if we cannot compete with you"".

Sure they don't get to say they offer hosted redis anymore, but they can say they offer hosted PlaceholderKV which is a fork and drop in replacement for redis, maintained by the same people who maintained redis.

The benefit here is that since Redis themselves took their toys out of the sandbox, the people doing this fork are the people who built the toys, and work for the companies who can push the new project into the standard, thus making people go ""why would I use Redis over PlaceholderKV which is available at all major infrastructure providers, and Redis isn't?"""
2024162314,13157,duncan-bayne,2024-03-27T23:45:04Z,">  ... but to say ""You don't get free direct work from us, if we cannot compete with you"".

But if any Redis fork continues to use the BSD license, the Redis folks can simply take the community's free contributions, and roll them into their own proprietary version, in perpetuity.

If you want to prevent your free contributions from being used in this way, one of the GPL-style licenses is the only way to go."
2024178706,13157,joshmanders,2024-03-28T00:07:22Z,"> > ... but to say ""You don't get free direct work from us, if we cannot compete with you"".
> 
> But if any Redis fork continues to use the BSD license, the Redis folks can simply take the community's free contributions, and roll them into their own proprietary version, in perpetuity.
> 
> If you want to prevent your free contributions from being used in this way, one of the GPL-style licenses is the only way to go.

Yes, that's the point. Unless the new license specifically prohibits Redis, Inc from doing it, then it's going to prevent everyone from doing it, including Amazon, Alibaba, and Red Hat. Or more importantly known as significant contributors to Redis."
2024230864,13157,mskiptr,2024-03-28T01:17:02Z,"> Unless the new license specifically prohibits Redis, Inc from doing it, then it's going to prevent everyone from doing it, including Amazon, Alibaba, and Red Hat. Or more importantly known as significant contributors to Redis.

Well, no. As far as I can tell, all those companies are in the business of just _hosting_ Redis. And besides, they still could very well be taking LGPL or GPL-licensed forks, modifying them internally and offering them as SaaS without the obligation of releasing sources. Only licenses like AGPL or EUPL try to prevent this.

Redis Inc. on the other hand is in the business of hosting Redis, preventing others from doing so and maybe also licensing it to others. This model is still able to take advantage of any permissively-licensed forks, but cannot pull in any changes from copyleft ones."
2024296252,13157,gitservers,2024-03-28T02:39:11Z,it's time for a solid fork with custom restrictions banning redislab from ever using new code from new forks. let's starve them
2024305857,13157,XtremeOwnageDotCom,2024-03-28T02:53:00Z,"I honestly don't care if redis would use code from a new fork. 

Its open source for a reason.

We can't become the thing we are bashing. And- blocking redis from pulling commits, really defeats the purpose of the original license we are all upset over them removing."
2024333835,13157,duncan-bayne,2024-03-28T03:31:22Z,"> Its open source for a reason.

That _particular_ reason, is the reason we are here having this conversation: Redis has taken all the hard work from its volunteer community, and re-licensed it under a proprietary license.

Perhaps this is the core difference between those proposing the use of the LGPL, and those proposing the use of the BSD license: whether or not you see that behaviour as a bug, or a feature."
2024393466,13157,reidja,2024-03-28T04:56:54Z,"Fundamentally this ultimately only results in a name change for what we used to call redis (at least for users). The community might fracture for a bit between the LGPL or BSD licensed forks but ultimately one will likely win. This means nothing, and will change nothing, I will literally just change the `redis` label in my container to something else. I will never use defacto redis again, nor will I have to change anything about how I used to use Redis.

"
2024677658,13157,dani0854,2024-03-28T08:40:53Z,"> But if any Redis fork continues to use the BSD license, the Redis folks can simply take the community's free contributions, and roll them into their own proprietary version, in perpetuity.

> This model is still able to take advantage of any permissively-licensed forks, but cannot pull in any changes from copyleft ones.

But that's the whole point of open source (at least to me). We have one group who says AWS uses their software to earn money and doesn't share profit, and another group who says Redis uses their software and adds changes, and doesn't share source code.

> when someone brings to the table the concept of a copyleft license, whose mere reason to exist (or one of them) is to protect that freedom

> That particular reason, is the reason we are here having this conversation

Is copyleft a solution to the problem we are seeing here, sure, I agree with that. But the different question, is it the best solution? Every solution to some problem, creates other problems. And in my view copyleft creates more problems than it solves. Another way to solve this problem would be not to publish any code at all, and that's also a solution, yet a terrible one.

Permissive licenses have a solution to that problem, called creating a fork. And if a new permissive fork goes proprietary, another fork is created and so on. Sure you can argue that each time it happens community shatters, and user base too, but in reality it rarely happens more than once within one project. And if it's a particular problem, there are big non-profit foundations, which have some goals and regulation to keep projects in open source in perpetuity (though I might need to do more reading on that one).

> The community might fracture for a bit between the LGPL or BSD licensed forks but ultimately one will likely win.

Please note again, that I am not a part of redis developer community, I only wished to participate in the broader discussion about how open source projects moving to proprietary licenses causes a shift towards copyleft. And if it wasn't clear, I would still prefer copyleft to proprietary, but not to permissive.

So the most important thing here is what the community chooses. And to me judging by discussion above @madolson is just doing that, trying to listen to the community. While @ddevault playing politics and advocating people for his views, and it is seem to be working quite well. But that's not surprising given the place and the situation here. Still I think it is more important for the community to decide on their own, without politics and other.

Since I have also been advocating for my views, and it was never my intention to influence this community into making a specific decision, this will be my last message in this PR regarding permissive/copyleft debate."
2024866146,13157,dkarlovi,2024-03-28T10:28:43Z,"@mskiptr 

> As far as I can tell, all those companies are in the business of just _hosting_ Redis.

That doesn't seem correct AFAIK, at least Amazon was mentioned in this thread as providing substantial contributions via an AWS developer working on Redis on the clock? I'm not sure if that's true or not, I just saw it mentioned here and elsewhere."
2025023377,13157,zuiderkwast,2024-03-28T12:05:04Z,"A majority of contributions come from companies hosting databases as as service. They all benefit from upstreaming their improvements, so that their internal forks don't diverge too much from the main project. They all benefit from that. That's why they hire people to do that.

Talking about ""community"" as if those companies weren't part of it. They are a very important part of the community."
2025586419,13157,mskiptr,2024-03-28T16:08:29Z,"@dkarlovi
> > As far as I can tell, all those companies are in the business of just _hosting_ Redis.
> 
> That doesn't seem correct AFAIK, at least Amazon was mentioned in this thread as providing substantial contributions via an AWS developer working on Redis on the clock?

That's true. I didn't mention it in my comment because it wasn't really relevant and also because that upstreaming work isn't making them money directly.

They can continue to do that just the same regardless of the fork being BSD or LGPL licensed. They can still choose to upstream their changes, but if they have any local modifications they _do not want to upstream_ they can keep them private just as well."
2026418088,13157,PenguRinTwo,2024-03-29T01:04:15Z,"The name of PlaceHolderKV was set to [Valkey](https://github.com/valkey-io/valkey) and it seems to be the primary fork of the community with good support and endorsement by the Linux Foundation as you can read in the blog at the bottom. 

""[Valkey](https://github.com/valkey-io/valkey) will continue development on Redis 7.2.4 and will keep the project available for use and distribution under the open source Berkeley Software Distribution (BSD) 3-clause license.""
""Industry participants, including Amazon Web Services (AWS), Google Cloud, Oracle, Ericsson, and Snap Inc. are supporting [Valkey](https://github.com/valkey-io/valkey). They are focused on making contributions that support the long-term health and viability of the project so that everyone can benefit from it.""

https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community"
2026850356,13157,mcepl,2024-03-29T08:14:46Z,"> In this specific example you are advocating for positive freedom of the free software community, but what about businesses, what about people who want to make a derivative work and monetize it in the way they see fit.

This sentence just makes me confused: have you ever read LGPL? This is the specific use case it was made for … so that business can use the library in any type of product they wish with any licence they wish ON THEIR PRODUCT while maintaining openness of the library they use. 

> I am not saying I agree with them, but why should we take their freedom away just because we disagree on something. The whole point of freedom, is that everyone has it, not just a specific group.

Have you ever read LGPL?"
2026851902,13157,mcepl,2024-03-29T08:16:22Z,"> Yeah, I'm aware of the nuance there, I just thought that calling out @dani0854's arguments as libertarian nonsense directly would be unfair.

You don’t have to add any ideological adjectives on it … just a noun is sufficient."
2026951363,13157,stalkerg,2024-03-29T09:37:58Z,"@dani0854 base on votes under your post, you can be wrong about copyleft and community. ;) "
2027040271,13157,dani0854,2024-03-29T10:28:14Z,"> @dani0854 base on votes under your post, you can be wrong about copyleft and community. ;)

Votes in this PR are quite subjective, especially in this discussion since copyleft is a natural pushback. Also, this PR has a lot of copyleft and FSF activists.

That not to say that I am wright, I could be wrong as any of us here. And popularity of opinions doesn't always correlate with what's wright or wrong. Furthermore, I find it incorrect to call someone opinion completely wright or wrong, since it all depends on context, and even within a context it is not black and white.

According to @ddevault own blog, he also used to hold similar opinions in the past, which changed over time. And that's fine, I might change my views on this matter over time too, or I might not. I actually used to like GPL license at some point before. But today those are the opinions I hold on permissive/copyleft debate, even if they are unpopular in this discussion."
2027145348,13157,filipesmedeiros,2024-03-29T11:50:38Z,"Well, in the end, it seems that the majority will get behind Valkey. But if Redict exists and is maintained, I think that is very positive. Having both a copyleft and a more permissive(?) versions is never a good outcome :)

Hopefully both of them are completely legal ahah

Also, it seems to me that the Linux Foundation blog post officially marks the end of Redis Labs. That's harsh. Signs of the times, maybe?"
2027153571,13157,Lucienest,2024-03-29T11:57:56Z,"The ultimate betrayal of the decade, hope they get sued for breach of contract"
2027155346,13157,theodesp,2024-03-29T11:59:46Z,But why?
2027269660,13157,vednig,2024-03-29T13:45:51Z,"Why not restrict the use/license of name Redis™️ rather than licensing the whole project? like [this](https://www.setindiabiz.com/learning/licensing-of-trademark#:~:text=Trademark%20licensing%20is%20a%20process,a%20mutually%20agreed%20licensing%20agreement.)"
2028083057,13157,milahu,2024-03-30T14:13:17Z,"go woke, go broke -- bye redis, hello [valkey](https://github.com/valkey-io/valkey)"
2028606629,13157,Feridinha,2024-03-31T08:54:58Z,[valkey](https://github.com/valkey-io/valkey)
2029044182,13157,stalkerg,2024-04-01T02:20:10Z,@Feridinha Redict - https://codeberg.org/redict/redict
2029065766,13157,XtremeOwnageDotCom,2024-04-01T02:46:20Z,"@Feridinha @stalkerg 

[Valkey](https://github.com/valkey-io/valkey) if you prefer the original BSD license.

[Redict](https://codeberg.org/redict/redict) if you prefer a stronger copyleft license.

EITHER option is an improvement over what redis has done here. 

Lets not nick-pick the individual options, and instead, work on moving away from redis.

Also- the linux foundation is backing valkey. I am going to guess, it will win the redis forking wars.
"
2029206579,13157,stalkerg,2024-04-01T05:51:09Z,"> Also- the linux foundation is backing valkey. I am going to guess, it will win the redis forking wars.

not sure, I am very skeptical for a such organization, also Valkey it's mostly big companies fork, and Redict it mostly community and small contributes fork. As small user of Redis I prefer Redict, but I understand why big companies try to push Valkey."
2029613429,13157,andersonpem,2024-04-01T11:25:42Z,"> > My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.
> 
> My commit [11cd983](https://github.com/redis/redis/commit/11cd983d58199b6ac7fa54049734457bd767a0b5) to allow Redis modules to function with modern compilers was also made under the BSD license and permission is **not** granted for it to be used under any other license. Copyright for changes in this commit is held by my employer, Red Hat.

Nothing justifies this.
Very high profile companies invest resources and effort to make this software better.

Well, KeyDB is better anyways. Goodbye Redis."
2034665144,13157,nyabinary,2024-04-03T13:47:18Z,"> go woke, go broke -- bye redis, hello [valkey](https://github.com/valkey-io/valkey)

How does this have anything to do with “wokeness” LMAO. It is corporate greed not wokeness you dork."
2034711971,13157,XtremeOwnageDotCom,2024-04-03T14:04:00Z,"> > Also- the linux foundation is backing valkey. I am going to guess, it will win the redis forking wars.
> 
> not sure, I am very skeptical for a such organization, also Valkey it's mostly big companies fork, and Redict it mostly community and small contributes fork. As small user of Redis I prefer Redict, but I understand why big companies try to push Valkey.

One thing for you to consider- The developers such as myself, who work for big companies... We get paid to produce quality code. Given, there is generally a large financial impact to said companies, based on the performance and reliability of said open source projects- I can assure you my development team will push a lot more high quality code, as opposed to random joe chilling at his house.

That being said, I'd elect to stick with the fork, that has quality developers... the ones who understands unit tests, etc."
2034731398,13157,ddevault,2024-04-03T14:10:53Z,"Redict 7.3.0 is now available:

https://redict.io/posts/2024-04-03-redict-7.3.0-released/

---

>One thing for you to consider- The developers such as myself, who work for big companies... We get paid to produce quality code. Given, there is generally a large financial impact to said companies, based on the performance and reliability of said open source projects- I can assure you my development team will push a lot more high quality code, as opposed to random joe chilling at his house.

As for who writes better code, grassroots communities or commercial developers... I'll encourage anyone with doubts to browse our code, or our documentation in particular, and let you be the judge.

Aside: here's the diffstat comparing Valkey (top) and Redict (bottom) in terms of divergence from Redis:

![](https://github.com/redis/redis/assets/1310872/585e78a7-53c6-4f49-b993-529b76770357)

That's just the core codebase -- Redict also has comprehensive documentation and official containers ready to use. If you want to focus on where the development action is, Valkey has a long ways to go.

Don't forget where Redis started, either."
2034760872,13157,XtremeOwnageDotCom,2024-04-03T14:20:11Z,"@ddevault Very fair. 

That being said though, instead of comparing the two- we should just celebrate that we are away from redis's proprietary license."
2034766922,13157,ddevault,2024-04-03T14:21:33Z,">That being said though, instead of comparing the two- we should just celebrate that we are away from redis's proprietary license.

Fair enough. Hooray for forks! :tada: "
2035593159,13157,PenguRinTwo,2024-04-03T21:08:09Z,"@ddevault 
Showing changed files is not a good comparison as Valkey was initially focused on discussions about how to move forward and it also depends how significant the changes were. Most of the changes that happened with your fork were mostly refactoring rather than features coming from 3 devs by the looks of it, so the comparison is rather misleading. The main support will be on Valkey and it will show with time, this is indisputable.

Even the first line from the release notes made me stop reading any further.

> ""Why Redict? [#](https://redict.io/posts/2024-04-03-redict-7.3.0-released/#why-redict)
> 
> You may be wondering why Redict would be of interest to you, particularly when compared with [Valkey](https:// www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community), another Redis® fork that was announced on Thursday.
> 
> In technical terms, we are focusing on stability and long-term maintenance, and on achieving excellence within our current scope.""

You make it sound like as if that wasn't the goal of Valkey, which is again misleading. I'm sure there is more if I continued reading but it is getting rather weird how you're trying people to focus on your fork and the words you are using."
2036606287,13157,ddevault,2024-04-04T09:05:46Z,">>In technical terms, we are focusing on stability and long-term maintenance, and on achieving excellence within our current scope.
>
>You make it sound like as if that wasn't the goal of Valkey, which is again misleading. I'm sure there is more if I continued reading but it is getting rather weird how you're trying people to focus on your fork and the words you are using.

This is not a slight against Valkey, it's a factual difference between our projects. Everyone wants to converge towards stability but it is a matter of fact, not opinion, that adding a bunch of new features and undertaking a lot of major refactoring creates churn and reduces stability, or at least makes it more difficult to achieve. This is a trade-off we make when fostering innovation -- and trade-offs are factual but value neutral."
2038418066,13157,jvinkovic,2024-04-04T23:08:07Z,NO
2038921980,13157,rudyorre,2024-04-05T04:49:02Z,"RIP redis, is Valkey the future?"
2042485893,13157,victor95pc,2024-04-08T11:18:00Z,"> RIP redis, is Valkey the future?

Sure its, lock Redis on the last BSD version in your package manager and wait for a stable Valkey release"
2042804777,13157,NikOverflow,2024-04-08T13:46:04Z,Redis made the worst decision. There's not much more to say except that https://github.com/valkey-io/valkey is the future.
2042817147,13157,choutianxius,2024-04-08T13:51:27Z,"> Reddit made the worst decision. There's not much more to say except that https://github.com/valkey-io/valkey is the future.

Reddit seems innocent😂"
2042839194,13157,rdesgroppes,2024-04-08T14:01:03Z,"> Reddit made the worst decision. There's not much more to say except that https://github.com/valkey-io/valkey is the future.

Better _read it_ twice..."
2043367792,13157,nyabinary,2024-04-08T18:05:22Z,"> > Reddit made the worst decision. There's not much more to say except that https://github.com/valkey-io/valkey is the future.
> 
> Reddit seems innocent😂

I meannnnn
https://www.reddit.com/r/changelog/comments/6xfyfg/an_update_on_the_state_of_the_redditreddit_and/"
2044807080,13157,acharalampous,2024-04-09T11:23:00Z,I was not Redis for this.
2045327681,13157,NikOverflow,2024-04-09T14:31:00Z,"> > Reddit made the worst decision. There's not much more to say except that https://github.com/valkey-io/valkey is the future.
> 
> Reddit seems innocent😂

yeah i did a misstake i obviously mean Redis."
2055941051,13157,richardARPANET,2024-04-15T07:44:52Z,lol
2065470446,13157,pete,2024-04-18T23:16:11Z,"> I'll encourage anyone with doubts to browse our code, or our documentation in particular, and let you be the judge.
> Aside: here's the diffstat comparing Valkey (top) and Redict (bottom) in terms of divergence from Redis:

This bothered me.  Churn is a bad statistic for gauging productivity, and since I am certain @ddevault knows that, I thought I'd have a look at what was actually different.

If one actually *reads* the diff between HEAD and e64d91c37105bc2e23816b6f81b9ffc5e5d99801, the overwhelming majority of changes are @ddevault adding `SPDX-FileCopyrightText:` comment blocks to the top of every file (to the tune of six lines added per file), juggling metadata files (e.g., removing everything under `.github/`, `.codespell`, etc.), and doing `s/redis/redict/g;s/Redis/Redict/g;s/REDIS/REDICT/g`.  That is, the churn is almost entirely fluff.

Since the changes were made by @ddevault, meaning that he knows that the amount of churn is mostly fluff, and since he knows that churn is a bad metric to begin with, I can't come up with a characterization of this other than to say it is intentionally misleading and relies on a lack of scrutiny (which, as a hacker, almost feels like a personal insult to me).

That made his claim about the number of contributors suspect, so I thought I'd have a look.  85.4% of the commits in Redict are from @ddevault, who has made 88 of the 103 commits as of d2e5e9655, while in Valkey, the curve is much flatter, with the top contributor (@0del) producing 16.2% of the commits as of cc94c98a9.  (Number of commits is about as misleading as churn; the numbers are cited only to demonstrate that the claim of open-source contributors stampeding towards Redict is also false.  You can check this with `git log e64d91c37105bc2e23816b6f81b9ffc5e5d99801..HEAD | awk '$1==""Author:""{a[$0]++;t++}END{for(i in a)printf ""%6.02f%% (%d) %s\n"",(100*a[i])/t, a[i], i; print t, ""total""}'` in either repository.)  The majority of committers to both projects have a single commit, but only @ddevault has more than a few commits in Redict.

To head off any suggestion of a hidden agenda, I don't have a dog in this fight:  I'm not pushing either fork, just waiting for the dust to settle, and I have no corporate overlords.  I just do not like FUD and misinformation, and I think the mudslinging in this thread is shameful.  (I do think, long-term, a project probably has a better shot promising community governance rather than being run by someone that launches ad hominems against the other project, to say nothing of the misleading statistics, but looking at the commits, I see a trustworthy friend of mine involved in Redict.)"
2066114551,13157,ddevault,2024-04-19T08:44:45Z,"I don't think that changed lines is the same thing is churn, and I also don't think changed lines is a measure of quality. What I'm measuring here has more to do with the tangible work that both Redict and Valkey have had to do in order to create a fork, which is the substantial effort of renaming everything and which necessitates a lot of lines changed. Valkey will *have* to complete a similar level of work in order to complete the forking process, and Redict is far ahead of them in this respect. That's all I'm pointing out with this comparison.

>If one actually reads the diff between HEAD and e64d91c, the overwhelming majority of changes are @ddevault adding SPDX-FileCopyrightText: comment blocks to the top of every file (to the tune of six lines added per file), juggling metadata files (e.g., removing everything under .github/, .codespell, etc.), and doing s/redis/redict/g;s/Redis/Redict/g;s/REDIS/REDICT/g. That is, the churn is almost entirely fluff.

This ""s/Redis/Redict/g"" work is the bulk of the ""churn"", and it is not actually as simple as running ""sed"" over the codebase. It is a largely manual process which requires attention given to each case; in some respects it can be automated but in order to be done properly it requires a lot of manual attention and labor.

>That made his claim about the number of contributors suspect, so I thought I'd have a look. 85.4% of the commits in Redict are from @ddevault, who has made 88 of the 103 commits as of d2e5e9655, while in Valkey, the curve is much flatter, with the top contributor (@0del) producing 16.2% of the commits as of cc94c98a9.

It is true that I have written most of the commits per-se as well as most of the lines changed, but as you said these are not a good measure of much. But, importantly, I have done most of the work for *redict*, i.e. the C language implementation of the Redict server and client, while other contributors have done most of the work in other respects; equally valuable and necessary work which, as it happens, has also mostly been completed on Redict whereas Valkey hasn't really started. This includes forking and cleaning up hiredict, which was led by Anna, most of the work on the Redict containers, which was done by Hugo and Micke, as well as writing and rewriting Redict's comprehensive documentation, which includes both content derived from the CC-BY-SA portions of the Redis documentation as well as original content written for Redict, most of which was led by Lucas.

>To head off any suggestion of a hidden agenda, I don't have a dog in this fight: I'm not pushing either fork, just waiting for the dust to settle, and I have no corporate overlords. I just do not like FUD and misinformation, and I think the mudslinging in this thread is shameful. (I do think, long-term, a project probably has a better shot promising community governance rather than being run by someone that launches ad hominems against the other project, to say nothing of the misleading statistics, but looking at the commits, I see a trustworthy friend of mine involved in Redict.)

Redict does have an agenda, though it's not hidden, which is to create a grassroots, community-governed fork of Redis with a copyleft license to protect our community from future exploitation. Valkey is only ""community"" led insofar as the main commercial interests in Redis constitute a community, as they are the sole decision makers and leaders of the Valkey community. This is not some kind of slander or ad-hominem, but a value-neutral statement of fact; in fact I acknowledge that some users of Redis would prefer the project to be led by commercial interests."
2066211637,13157,zuiderkwast,2024-04-19T09:42:19Z,"@ddevault https://www.gnu.org/philosophy/open-source-misses-the-point.html

Valkey :heart: Redict
"
2066394491,13157,pete,2024-04-19T11:40:08Z,"Quoting @ddevault:

> Valkey will _have_ to complete a similar level of work in order to complete the forking process

As Valkey is not changing licenses (yet) and not moving away from Github (yet?), no, none of those things need to be done.  Licensing headers don't need to be added to example scripts at all, and are not a hard requirement to have a viable fork.  You know this.

> it is not actually as simple as running ""sed"" over the codebase

I'm aware.  It is still not a meaningful change and is not an indicator that Valkey is somehow ""behind"", and it doesn't do anyone any good to suggest that.  MariaDB kept the exported symbols and executable/library names for the sake of compatibility; I imagine Redis Labs is far less litigious than Oracle.  There's plenty of value in doing things that way:  merges from upstream are easier, people have less to rewrite, and eventually merging forks is easier.

> Redict does have an agenda, though it's not hidden

What I meant to head off was the repeated insinuations Redict's self-appointed BD has been making about Valkey; they wouldn't apply in my case.

> Valkey is only ""community"" led insofar as the main commercial interests in Redis constitute a community

This is disingenuous.  They have professed an aim to produce a community-led effort; that's as much as you have said.  There's not been time for a community to coalesce.  Digging trenches and encouraging a rush to judgment is a very bad move, the opposite of trying to get a community to coalesce.  The sensible thing to do, if a project actually were community-led, would be to keep alterations minimal until there's some kind of consensus, and there is no consensus until there are enough people for a consensus to be meaningful.  It seems like even if they moved to the LGPL, you'd not be interested in merging the forks:  is this accurate?  (I would like for it to not be accurate, but I suspect that if it were not true, then you'd have said as much already.)

> This is not some kind of slander or ad-hominem, but a value-neutral statement of fact

It is completely impossible to construe this as fact with a straight face.  Making various and sundry claims about their motivations is FUD, and claiming that this is an objective, verifiable fact is impossible for me to perceive as good faith.  It is a fact that some companies that do not like Redis's new license have started backing Valkey:  that is a fact, that's verifiable.  You could say it's probable that they prefer a license that lets them continue business as usual, or that they are making a conservative move to the fork that has not changed licenses yet.  If you said that, I'd agree that it was probable, though not a fact.  Several steps past this, declaring that it's because Valkey (who seem to have expressed some openness about changing licenses to something more free in this thread) wants all of us crushed under the corporate boot is neither a fact nor probable, but FUD.

Speculating about their motivations is absolutely an ad hominem, and antagonizing them instead of asking is indicative of of nothing good.  I could drop the qualifiers on this kind of speculation and start insisting things I say about your motivations are value-neutral statements of fact, but it seems extremely unhelpful to do so.  It seems like a better idea to figure out common ground:  there might not need to be so many forks.  That'd be a much better outcome for everyone, but it's impossible to get to that outcome while you're antagonizing them and producing these ""facts"".  (If I'm composing a wish list, I'd prefer whichever fork drops the line-editor and builds cleanly on Plan 9, but I'm fairly certain I'm in a small enough minority that I'd have to do the latter myself.  On the other hand, I suspect that more people would agree that one fork would be more manageable than N forks that all hate each other--some of which have introduced intentional breaks in compatibility--and probably that LGPL would be a preferable license.)"
2067103668,13157,Lucienest,2024-04-19T18:40:07Z,"> Quoting @ddevault:
> 
> > Valkey will _have_ to complete a similar level of work in order to complete the forking process
> 
> As Valkey is not changing licenses (yet) and not moving away from Github (yet?), no, none of those things need to be done. Licensing headers don't need to be added to example scripts at all, and are not a hard requirement to have a viable fork. You know this.
> 
> > it is not actually as simple as running ""sed"" over the codebase
> 
> I'm aware. It is still not a meaningful change and is not an indicator that Valkey is somehow ""behind"", and it doesn't do anyone any good to suggest that. MariaDB kept the exported symbols and executable/library names for the sake of compatibility; I imagine Redis Labs is far less litigious than Oracle. There's plenty of value in doing things that way: merges from upstream are easier, people have less to rewrite, and eventually merging forks is easier.
> 
> > Redict does have an agenda, though it's not hidden
> 
> What I meant to head off was the repeated insinuations Redict's self-appointed BD has been making about Valkey; they wouldn't apply in my case.
> 
> > Valkey is only ""community"" led insofar as the main commercial interests in Redis constitute a community
> 
> This is disingenuous. They have professed an aim to produce a community-led effort; that's as much as you have said. There's not been time for a community to coalesce. Digging trenches and encouraging a rush to judgment is a very bad move, the opposite of trying to get a community to coalesce. The sensible thing to do, if a project actually were community-led, would be to keep alterations minimal until there's some kind of consensus, and there is no consensus until there are enough people for a consensus to be meaningful. It seems like even if they moved to the LGPL, you'd not be interested in merging the forks: is this accurate? (I would like for it to not be accurate, but I suspect that if it were not true, then you'd have said as much already.)
> 
> > This is not some kind of slander or ad-hominem, but a value-neutral statement of fact
> 
> It is completely impossible to construe this as fact with a straight face. Making various and sundry claims about their motivations is FUD, and claiming that this is an objective, verifiable fact is impossible for me to perceive as good faith. It is a fact that some companies that do not like Redis's new license have started backing Valkey: that is a fact, that's verifiable. You could say it's probable that they prefer a license that lets them continue business as usual, or that they are making a conservative move to the fork that has not changed licenses yet. If you said that, I'd agree that it was probable, though not a fact. Several steps past this, declaring that it's because Valkey (who seem to have expressed some openness about changing licenses to something more free in this thread) wants all of us crushed under the corporate boot is neither a fact nor probable, but FUD.
> 
> Speculating about their motivations is absolutely an ad hominem, and antagonizing them instead of asking is indicative of of nothing good. I could drop the qualifiers on this kind of speculation and start insisting things I say about your motivations are value-neutral statements of fact, but it seems extremely unhelpful to do so. It seems like a better idea to figure out common ground: there might not need to be so many forks. That'd be a much better outcome for everyone, but it's impossible to get to that outcome while you're antagonizing them and producing these ""facts"". (If I'm composing a wish list, I'd prefer whichever fork drops the line-editor and builds cleanly on Plan 9, but I'm fairly certain I'm in a small enough minority that I'd have to do the latter myself. On the other hand, I suspect that more people would agree that one fork would be more manageable than N forks that all hate each other--some of which have introduced intentional breaks in compatibility--and probably that LGPL would be a preferable license.)

I could summarize this in a single word `sabotage`"
2314787666,13157,jarvis8x7b,2024-08-28T09:19:18Z,rip
2328537955,13157,Samk13,2024-09-04T10:42:11Z,"Oh, so Elastic admits it was a mistake after all... who would've guessed? 😏
https://www.elastic.co/blog/elasticsearch-is-open-source-again"
2328551640,13157,andersonpem,2024-09-04T10:47:13Z,"Damage is done. If there's one thing that is hard to regain is trust. I have already moved on to 
https://github.com/valkey-io/valkey"
2328693902,13157,xeraa,2024-09-04T11:31:59Z,"> Oh, so Elastic admits it was a mistake after all... who would've guessed? 😏

That is neither what we said nor as simple as something being right or wrong — it's much more actions and reactions. https://www.infoworld.com/article/3499400/elastics-return-to-open-source.html has a more nuanced take from someone who was at AWS when the OpenSearch fork happened — if you're actually interested. It also touches on Redis and Valkey.

PS: Having been involved in one relicense and following others closely, it's interesting how the context is still so different in each."
2328821297,13157,Samk13,2024-09-04T12:16:26Z,"> That is neither what we said nor as simple as something being right or wrong — it's much more actions and reactions. 
> https://www.infoworld.com/article/3499400/elastics-return-to-open-source.html 

Thanks for the link. Like with Redis, the trust issue goes beyond resolving trademark disputes with AWS. 
Moving 'kind of' away from open source initially made it seem like control was prioritized over collaboration. 
Returning now is a step, but with OpenSearch and Valkey growing, it’ll take more than a license change to fully rebuild trust in the community. 
I hope we can all focus on open collaboration moving forward, rather than fracturing the community again."
2332814366,13157,xeraa,2024-09-05T22:52:25Z,"I think that comes back to the different context to some degree: For Elastic >95% of the code came from Elastic employees and while open for collaboration, it was never an open governance (and that's also a separate topic than open source licensing).
Anyway, I don't want to derail the discussion around Redis. I'll just clarify when I see us mentioned."
2352742702,13157,prochac,2024-09-16T12:11:39Z,"> My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.

I would be really pissed off. Crossed fingers, you will solve this and get your rights. And I wish Redis happy rebasing."
2352984443,13157,mskiptr,2024-09-16T13:50:21Z,">> My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.
> 
> I would be really pissed off. Crossed fingers, you will solve this and get your rights. And I wish Redis happy rebasing.

The BSD license _is_ what lets them do this. Since anyone can use such code in projects licensed differently, they can use these contributions in a proprietary library. Yes, they still have to obey the license terms (i.e. keep the original license text around[0]) and everyone can still use those files using the old license[2]. But the new changes copyrighted by Redis Ltd. are not BSD-licensed.

If you don't like it, you should look into share-alike (aka copyleft) licenses like the GPL or the MPL.


0: They do. See the file `REDISCONTRIBUTIONS.txt`. As for retaining ""above copyright notice"", the only ones removed[1] were either from Salvatore or from Redis Ltd. employees and they presumably got their permission to do so.
1: As far as I can see, all other copyright notices are left intact. To be extra safe they even left the whole license text in the files that contain those.
2: The easiest and legally safest way to do this is to only look at the commits prior to 0b343969."
2353415020,13157,prochac,2024-09-16T16:42:50Z,"> >> My contributions are made open source under the BSD license. You are not allowed to redistribute nor use them in source or binary forms without the original copyright notice and the BSD license. You must remove my contributions.
> > 
> > I would be really pissed off. Crossed fingers, you will solve this and get your rights. And I wish Redis happy rebasing.
> 
> The BSD license _is_ what lets them do this. Since anyone can use such code in projects licensed differently, they can use these contributions in a proprietary library. Yes, they still have to obey the license terms (i.e. keep the original license text around[0]) and everyone can still use those files using the old license[2]. But the new changes copyrighted by Redis Ltd. are not BSD-licensed.
> 
> If you don't like it, you should look into share-alike (aka copyleft) licenses like the GPL or the MPL.
> 
> 
> 0: They do. See the file `REDISCONTRIBUTIONS.txt`. As for retaining ""above copyright notice"", the only ones removed[1] were either from Salvatore or from Redis Ltd. employees and they presumably got their permission to do so.
> 1: As far as I can see, all other copyright notices are left intact. To be extra safe they even left the whole license text in the files that contain those.
> 2: The easiest and legally safest way to do this is to only look at the commits prior to 0b343969.

Yup, but those BSD-3 parts of code should be marked as BSD-3. Only the new code is under the new license. Except for the code where the authors *explicitly* allowed relicensing (not sure about this part).
The fact that it's quite inconvenient is at the side of the one who did the rug pull.

725 contributors, I bet they didn't ask a single one about the relicensing. And just deleted their licence under which they published their work."
2354132975,13157,mskiptr,2024-09-16T22:22:52Z,"> Yup, but those BSD-3 parts of code should be marked as BSD-3.

Do they? I don't see any such clauses in the license. It does mandate that both the license and any copyright notices have to be retained when distributed in source form, so I guess one could argue that it has to be within the same file. But that's not really stated so we can only know if a court rules one way or the other.

But even if they had to keep it in all those files, they could simply prepend something like this to each one
```
/* Copyright (c) 2006-Present, Redis Ltd.
 * All rights reserved.
 * 
 * Parts of this file are subject to:
```
and call it a day. They definitely do not have to keep their new proprietary code in separate files. That's what the Mozilla Public License had to be created for.

> Only the new code is under the new license. Except for the code where the authors _explicitly_ allowed relicensing (not sure about this part).

They are sub-licensing that code. It's a general consensus that you can sub-license permissively-licensed code under any license you want."
894797972,9320,yossigo,2021-08-08T13:28:38Z,"Hello @pizhenwei, this looks like a big chunk of work - thank you! I only quickly looked at the code and plan to do so more in-depth in a few days, but I wanted to ping @redis/core-team about this.

I think we need to agree on the supported use cases and scope here, and the kind of problems that can and cannot be solved by pluggable connection modules. There are many constraints that will make it difficult to support any arbitrary configuration:

* Configuration assumes a single TCP, TLS, Unix socket listener
* Cluster bus protocol assumes a single TCP and TLS listener
* Integration with different channels (Clients, Replication, Cluster Bus, Sentinel)

Based on that, use cases that could reasonably be supported are:

* Single (optional) instance of a TLS module, for all channels (this PR)
* Arbitrary (optional) instances for user-defined connection types, only used to accept client connections. I assume the future RDMA transport could fall here, as well as others.

I think we need to agree that supporting the ""all channels"" for arbitrary connection types is out of scope for now.

Another issue I noticed with this PR is that it implements a dedicated mechanism for loading the module. I lean towards using the existing modules mechanism, even if this kind of third party modules have a surface area that's much bigger than the Redis Module API. Would be happy to hear other thoughts."
894805688,9320,oranagra,2021-08-08T14:25:13Z,"@yossigo just to be clear, you mean that for client connections we can support multiple plugins, even working side by side, but these won't (at least in this stage) integrate with cluster bus and replication channels.
And on the other hand, the only plugin that integrate with all channels, is specifically a TLS plugin, and also we only allow one at the time, so someone can replace the OpenSSL one with an s2n one, but can't have them both loaded, and can't use that interface (all channels) to implement any other extension (other than for TLS).

Regarding the module loading interface, i actually believe this mechanism should be completely separate from the current Redis Modules one, i don't wanna see module capabilities or limitations get mixed with the capabilities and limitations of this mechanism."
894916621,9320,pizhenwei,2021-08-09T02:40:11Z,"> Hello @pizhenwei, this looks like a big chunk of work - thank you! I only quickly looked at the code and plan to do so more in-depth in a few days, but I wanted to ping @redis/core-team about this.
> 
Sure, I'm looking forward to seeing it."
908588894,9320,oranagra,2021-08-30T18:37:47Z,"@yossigo it makes sense to split it into several stages, and review each separately, but then i may be worried that we're not seeing the end goal properly when working on stage one, so i'm also willing to consider splitting to several commits, each with a clear separate purpose. not sure which approach is better in this case, i leave it to you to decide."
912462600,9320,pizhenwei,2021-09-03T11:18:38Z,"Hi, @oranagra @yossigo 
Before the next stage, I guess reviewing the abstract part seems also necessary, it changes a lot.
How about reviewing this feature patch by patch in this PR, and merge it until the full job done?"
919091375,9320,pizhenwei,2021-09-14T12:12:57Z,"> @pizhenwei I've started reviewing the code and made a few comments. My main input is we need to break down this PR into several phases to make it easier to handle.
> 
> I believe the first phase should be about connection API change/cleanup only - fully encapsulate `ConnectionType`, only use generic `connCreate`, `connAccept`, etc. Leave everything in `connection.c` to avoid big diffs due to code reorganization at this point.
> 
> At this point I'd also want to be sure we don't introduce any unexpected performance regressions.
> 
> The next stages would be --
> 
> * Add `tls-module-path` and the ability to _optionally_ build TLS support separately (i.e. `BUILD_TLS=yes|no|ext`). Maybe at this point already consider a more generic `load-extension` mechanism (the term ""module"" is used for Redis Modules, we don't want to confuse the two).
> * Consider if/how additional arbitrary connection types can be supported.
> * Finalize code reorganization.
> 
> @oranagra Do you have any other/additional thoughts?

Hi, @yossigo , I have fixed several problems as your comments.
`fully encapsulate ConnectionType` & `tls-load-extension` are ready to review, and the performance test seems OK.

I'm a little confused about `Consider if/how additional arbitrary connection types can be supported`:
Currently before using a connection type, we must declare the type(CONN_TYPE_SOCKET and CONN_TYPE_TLS), so an arbitrary connection type can't be loaded without the type define. But it's can be implemented by this mechanism:
* remove type define from connection layer
* introduce type string ""socket"" & ""tls"" in each connection type
* connection layer keeps a linked list to store all the types

Should I take the next step to separate connection.c into connection.c(abstract layer) and socket.c?
"
944306255,9320,pizhenwei,2021-10-15T13:33:40Z,"@yossigo @oranagra 
The main part of this job is almost complete.
And base these patches, the RDMA feature([PR](https://github.com/pizhenwei/redis/tree/feature-rdma-v2)) also supports built-in/extension mode, the features of RDMA have been implemented:
     - rdma server built-in & extension mode work fine
     - replication works fine
     - cluster also works fine

Could you take a look at this PR?
"
1137927591,9320,pizhenwei,2022-05-25T22:59:49Z,"Hi, @yossigo @oranagra @yoav-steinberg 

I reworked this series, and separated a large patch into 9 small changes:
Implement a complete connection framework, Redis could hide all the connection related functions, uplayer accesses the connection via framework only.
Currently, for both socket and TLS, all the methods of connection type are declared as static functions.

I would appreciate it if you could review this series!"
1141600005,9320,pizhenwei,2022-05-31T02:18:31Z,"> Hi, @yossigo @oranagra @yoav-steinberg
> 
> I reworked this series, and separated a large patch into 9 small changes: Implement a complete connection framework, Redis could hide all the connection related functions, uplayer accesses the connection via framework only. Currently, for both socket and TLS, all the methods of connection type are declared as static functions.
> 
> I would appreciate it if you could review this series!

PING @yossigo @oranagra @yoav-steinberg "
1142512213,9320,oranagra,2022-05-31T18:32:00Z,"@pizhenwei i'm really sorry for not responding sooner (wanted to wait till i have a chance to look at the code)
we're still in the vortex of redis 7.0 (trying to quickly fix issues in 7.0.0), i hope one of us will be able to dive into it in the coming weeks and give provide the necessary feedback to push this forward.
we haven't forgot, just still too busy."
1142779360,9320,pizhenwei,2022-05-31T23:54:09Z,"> @pizhenwei i'm really sorry for not responding sooner (wanted to wait till i have a chance to look at the code) we're still in the vortex of redis 7.0 (trying to quickly fix issues in 7.0.0), i hope one of us will be able to dive into it in the coming weeks and give provide the necessary feedback to push this forward. we haven't forgot, just still too busy.

Some guys contacted me to know more about the test result and the plan of Redis over RDMA, this makes me anxious to do this work. Sorry to let you feel pushy, that was not my intention.

Thanks a lot!"
1155133437,9320,pizhenwei,2022-06-14T12:41:21Z,"@oranagra according to your suggestion, I did the changes except:
- instead of adding a forward declaration of TLS functions, move 'ConnectionType CT_TLS' to the end of source code to avoid chunk of changes.

Other changes:
- Rebase code, based on commit ffa007704122a3e4947252f3dbbef3e2be4b6033.
- Define CONN_ADDR_STR_LEN as 128, remove 'TODO' in code.
- Remove 'FD_TO_PEER_NAME' & 'FD_TO_SOCK_NAME', use 'remote' instead.
- Add comments for new functions: connTypeInitialize, connTypeRegister, connTypeConfigure, connTypeCleanup, connTypeCleanupAll.
- Add connTypeCleanupAll() to cleanup all connection types for https://github.com/redis/redis/pull/8589.
- Move 'connTypeInitialize' after moduleInitModulesSystem().
- Use 'reconfigure' instead of 'force' in 'tlsConfigure()'.
- Remove extra space in 'tlsConfigure'."
1155146114,9320,oranagra,2022-06-14T12:52:50Z,"@pizhenwei thank you for the adjustments and the detailed list of changes.
in the future, if you have to rebase, please force-push it in a separate push, so that we can click on github's `force` link and see the actual changes you did, ignoring the rebase ones.
in this one your list of changes was sufficient, so no harm done 8-)

please also mark the resolved comments as resolved, and let's figure out what we wanna do next."
1155886386,9320,pizhenwei,2022-06-15T01:51:14Z,"> @pizhenwei thank you for the adjustments and the detailed list of changes. in the future, if you have to rebase, please force-push it in a separate push, so that we can click on github's `force` link and see the actual changes you did, ignoring the rebase ones. in this one your list of changes was sufficient, so no harm done 8-)
> 
> please also mark the resolved comments as resolved, and let's figure out what we wanna do next.

Hi, @oranagra 
I'm not sure if I hit a bug of github ...
When I write comment after this
```
don't we need to also abstract all the listen and accept code?
i.e. this way we can replace the OpenSSL based TLS dynamically with another implementation, but what happens when we add another medium?
i guess to judge this change we need to experiment with a branch that adds RDMA on top of this, and see if indeed the only change needed in redis is initializing another entry in the connection type array.
did you try this? (the commit comments suggest that maybe you did)
p.s. maybe to be really flexible we'll also have to break apart ae.c
```

Github always shows I started a review on myself ...
So I have to resolve this, and write comment here:
```
There are already 9 commits in this PR, also abstracting all the listen and accept code in this PR or not, either is OK to me, I'll follow your decision.
```"
1156075739,9320,oranagra,2022-06-15T07:12:47Z,"@pizhenwei not sure about that GH issue, but let's put it aside and try to move on.
i wrote that comment before i saw your other branch and wrote the review summary [comment](https://github.com/redis/redis/pull/9320#pullrequestreview-1003594233), so it was somewhat outdated.

reading that again and re-thinking, i think i would like to add more content into this PR.
* move the listen and accept logic into the extensions
* have an array of extensions were we loop on them to listen instead of hard coded calls to each.
* don't have a hard coded indexes to the extensions, instead we can have a global variable holding the index (e.g. for the TLS extension) that's set at init time.
* allow TLS to be compiled as dynamic lib and loaded at runtime.

what i would not add at this time is:
* any RDMA specific code.
* a way for an extension to add configs (meanwhile, the TLS configs will remain hard coded)
* any change that would break the existing TLS configs.

i hope i'm not rushing too much content into this PR, would love to hear @yossigo feedback.
i feel that as long as we just refactor things to be more generic without adding or changing anything user facing, it can fit here.
we should still attempt to avoid unnecessary code re-location, or if we do that, keep that to a separate commit that only moves code without modifying it, so that reviewing this one commit at the time is easy.

the only exception about the above (not changing anything user facing), is the runtime loadable extensions (which does add a config and a makefile option), but i think we can let it slide so that we can better judge this PR, but we can also leave it out for the next one.

please let me know what you think."
1156123194,9320,pizhenwei,2022-06-15T07:55:54Z,"> @pizhenwei not sure about that GH issue, but let's put it aside and try to move on. i wrote that comment before i saw your other branch and wrote the review summary [comment](https://github.com/redis/redis/pull/9320#pullrequestreview-1003594233), so it was somewhat outdated.
> 
> reading that again and re-thinking, i think i would like to add more content into this PR.
> 
> * move the listen and accept logic into the extensions
> * have an array of extensions were we loop on them to listen instead of hard coded calls to each.
> * don't have a hard coded indexes to the extensions, instead we can have a global variable holding the index (e.g. for the TLS extension) that's set at init time.
> * allow TLS to be compiled as dynamic lib and loaded at runtime.
> 
> what i would not add at this time is:
> 
> * any RDMA specific code.
> * a way for an extension to add configs (meanwhile, the TLS configs will remain hard coded)
> * any change that would break the existing TLS configs.
> 
> i hope i'm not rushing too much content into this PR, would love to hear @yossigo feedback. i feel that as long as we just refactor things to be more generic without adding or changing anything user facing, it can fit here. we should still attempt to avoid unnecessary code re-location, or if we do that, keep that to a separate commit that only moves code without modifying it, so that reviewing this one commit at the time is easy.
> 
> the only exception about the above (not changing anything user facing), is the runtime loadable extensions (which does add a config and a makefile option), but i think we can let it slide so that we can better judge this PR, but we can also leave it out for the next one.
> 
> please let me know what you think.

Hi, Oran
This comment is quite clear, I know the goal of this PR, thanks!
The only question is that:  I have a plan to drop the index of connection types, use a list to store all the types, and search the connection type by string(Ex, ""socket"", ""tls""),  what do you think about this?"
1156135630,9320,oranagra,2022-06-15T08:07:18Z,"i think that considering that we don't expect to have more than 4 extensions, maybe we can start with an array rather than a list (faster index), and that in the few places that we need to search for a specific item in that list, i'd rather avoid the string matching, and instead find the right index (using string matching) at init time, and then save a global with that index.
but i guess it'll be just the same if we save a pointer directly to that connection class instead of save an index, so the fast indexing doesn't matter.

so to reply to your question, do what feels natural, but avoid iterating on the list at runtime doing string matching, and instead cache the element or it's index in a global for use in the few places that specifically require a certain class (we shouldn't have many of them)."
1163008128,9320,pizhenwei,2022-06-22T11:57:58Z,"Hi, @oranagra
I pushed another 7 commits to achieve the goal as you mentioned. The detailed changes is described in the commit message.
I also take a further step to abstract the Unix socket connection type, and it's possible to build it as a shared library too. If this is not reasonable to you, I'd remove this change.

Every commit in this PR is tested by:
```
make distclean; make BUILD_TLS=no
./runtest-cluster && ./runtest-moduleapi && ./runtest-sentinel && ./runtest

make distclean; make BUILD_TLS=yes
./runtest-cluster && ./runtest-moduleapi && ./runtest-sentinel && ./runtest
./runtest-cluster --tls && ./runtest-moduleapi --tls && ./runtest-sentinel --tls && ./runtest --tls
```

And since connection extension get supported:
```
make distclean; make BUILD_TLS=yes
./runtest-cluster --tls --tls-ext && ./runtest-moduleapi --tls --tls-ext && ./runtest-sentinel --tls --tls-ext && ./runtest --tls --tls-ext
```"
1165290265,9320,pizhenwei,2022-06-24T07:32:28Z,"Hi, @oranagra 
There is a conflict in src/cluster.c. If it's an obstacle to review, please let me know, I'll fix this and force-push a new version."
1169598282,9320,pizhenwei,2022-06-29T06:47:51Z,"Hi, @oranagra 

I fixed several problems as your suggestions, and force pushed. There is still a conflict in src/cluster.c. I did not rebase code to avoid unnecessary changes(I did tests on my local server)."
1170817510,9320,pizhenwei,2022-06-30T06:26:13Z,"Hi, @oranagra 
I force pushed twice. The [first version](https://github.com/redis/redis/compare/d292639a140a5ce7c2d65a69b3bb8b03ffdf9e22..5fe445242fa52abb861fd70f653a2fbaec6e6de5) fixes the problems as you suggested. The latest version re-bases the code from the latest commit from unstable branch to fix the code conflict.
Thanks for your review suggestions!"
1173626347,9320,pizhenwei,2022-07-04T10:09:37Z,"Hi, @oranagra 
I force pushed a new version as you suggested, and leave TLS part to do in the next version(from your and Yossi's feedback)."
1174671595,9320,pizhenwei,2022-07-05T06:33:07Z,"Hi, @oranagra @yossigo 
I append this [commit](https://github.com/redis/redis/pull/9320/commits/fc8fb47ca04e72931c55e65200304a2f32754ac7) to hide TLS specified methods, use connControl to call connection type methods. Could you please take a look at this idea?"
1174731038,9320,pizhenwei,2022-07-05T07:48:09Z,"Hi, @oranagra 
During developing this feature, I have the same feeling with you. Even to the Unix socket, it also has a `bind` address, ignore the `port` ...
Base on the latest code, RDMA extension works fine: it take the argument like this `./src/redis-server --connection-extension src/redis-rdma.so bind=xx.xx.xx.xx port=6379`(this is almost ready to push, if you are interested in this, please let me know, I'll push it to another branch). Of cause, RDMA may need more arguments in the future to optimize performance."
1174742765,9320,oranagra,2022-07-05T08:01:17Z,"yeah, i realize you used the startup argument for now.
i think it'll be nice to expose better integration with configs so they can be changed / queried at runtime, and i assume some extensions will need additional configurations (like TLS has).
but i'm now under the understanding that the only reason we need the CTRL_TLS_SET_CONFIG interface is because the TLS configs are built into redis, and we extract the extension code.
so this means other extensions won't need a similar interface."
1174806924,9320,pizhenwei,2022-07-05T09:01:46Z,"> yeah, i realize you used the startup argument for now. i think it'll be nice to expose better integration with configs so they can be changed / queried at runtime, and i assume some extensions will need additional configurations (like TLS has). but i'm now under the understanding that the only reason we need the CTRL_TLS_SET_CONFIG interface is because the TLS configs are built into redis, and we extract the extension code. so this means other extensions won't need a similar interface.

`CTRL_TLS_SET_CONFIG` can be done in the callback function during TLS config change. But `CTRL_TLS_GET_CTX, CTRL_TLS_GET_CLIENT_CTX & CTRL_TLS_GET_PEER_CERT` do not come from the config, so I guess we still need an interface to get these.

Frankly, I don't know other extensions need the similar interface or not. From my understanding, if the user context of a connection type is simple, it may not need this(to store the context in the `connection` is enough); if the user context is complex, it probably need this."
1175676468,9320,pizhenwei,2022-07-06T01:30:45Z,"Hi, @oranagra 
Could you please give me any hint about the next step? should I revert the last [commit](https://github.com/redis/redis/pull/9320/commits/70f5eb24a92704cb6abbcd1d2905e9fc9f1380b9)?"
1181539080,9320,pizhenwei,2022-07-12T09:32:07Z,"Hi, @oranagra 
Sorry for not replying sooner. In fact, I Implemented a version in another [branch](https://github.com/pizhenwei/redis/commits/connection-module-with-rdma) , it supports command like `config set rdma-port 6380` to rebind on a new port. it may be a choice.
showing connection types is reasonable to me too. I will support it later(I'm on vacation this week, I will support this feature ASAP in next week)."
1181616832,9320,oranagra,2022-07-12T10:57:33Z,"thanks take your time....
p.s. i took at look at the branch you linked, what you did will only allow interacting with the CONFIG command, not with the config file (and i'm not sure what'll happen after rewrite).
i.e. when loaded from an extension, the configs are registered after the config file was already parsed.
considering that we already have the mechanism we added for modules, i feel we should use it."
1181645923,9320,pizhenwei,2022-07-12T11:30:59Z,"> thanks take your time.... p.s. i took at look at the branch you linked, what you did will only allow interacting with the CONFIG command, not with the config file (and i'm not sure what'll happen after rewrite). i.e. when loaded from an extension, the configs are registered after the config file was already parsed. considering that we already have the mechanism we added for modules, i feel we should use it.

OK, I will try/test as soon as this weekend. (I left my pc home, currently reading code by a mobile phone is quite hard...)"
1189718650,9320,pizhenwei,2022-07-20T01:56:52Z,"Hi, @oranagra 
I force-pushed a new [version](https://github.com/redis/redis/pull/9320/commits/316a6a82165484ae51836ef5760288cb4c61fb7a) as you suggested, `INFO SERVER` shows like
```
listener0:name=tcp,bind=127.0.0.1,port=6380
listener1:name=unix,bind=/run/redis.sock
listener2:name=tls,bind=127.0.0.1,port=6379
```

I'll fix the conflict when you make the decision about connection extension."
1197536392,9320,pizhenwei,2022-07-28T01:07:20Z,"Hi, @oranagra 
I force pushed 16 commits:
- fully abstract connection except dynamically loadable TLS library.
- fix conflict against the latest unstable branch with only one change: connPeerToString -> connSocketAddr. see [commit](https://github.com/redis/redis/commit/d00b8af89265c501fb4a0cdd546702b90432a896). @uvletter I would be appreciated it if you could review this change.

And during I try to build the TLS as a module, I hit several problems:
- the connection type libraries MUST be loaded before the listeners setup, it means that we need to call moduleLoadFromQueue() early than initServer(), and monotonicInit() is needed by module system.
- the TLS connection type is expected to build into 2 types(ignore BUILD_TLS=no): `BUILD_TLS=yes, BUILD_TLS=mod`. if we choose `BUILD_TLS=yes`, tls.c needs `#include ""server.h"" #include ""connhelpers.h"" #include ""adlist.h""`; if we choose `BUILD_TLS=mod`, tls.c needs `#include ""redismodule.h""` only.

Something conflict from the two case:
```
server.h

#define REDISMODULE_CORE 1
#include ""redismodule.h""    /* Redis modules API defines. */
```

This means that we MUST remove server.h from tls.c to build it into a redis module. To reproduce this problem, please apply the following diff code on this branch:
```
diff --git a/src/Makefile b/src/Makefile
index 814f3c0aa..eb3ed1802 100644
--- a/src/Makefile
+++ b/src/Makefile
@@ -267,24 +267,40 @@ ifeq ($(MALLOC),jemalloc)
 	FINAL_LIBS := ../deps/jemalloc/lib/libjemalloc.a $(FINAL_LIBS)
 endif
 
-ifeq ($(BUILD_TLS),yes)
-	FINAL_CFLAGS+=-DUSE_OPENSSL $(OPENSSL_CFLAGS)
-	FINAL_LDFLAGS+=$(OPENSSL_LDFLAGS)
-	LIBSSL_PKGCONFIG := $(shell $(PKG_CONFIG) --exists libssl && echo $$?)
+# LIBSSL & LIBCRYPTO
+LIBSSL_LIBS=
+LIBSSL_PKGCONFIG := $(shell $(PKG_CONFIG) --exists libssl && echo $$?)
 ifeq ($(LIBSSL_PKGCONFIG),0)
 	LIBSSL_LIBS=$(shell $(PKG_CONFIG) --libs libssl)
 else
 	LIBSSL_LIBS=-lssl
 endif
-	LIBCRYPTO_PKGCONFIG := $(shell $(PKG_CONFIG) --exists libcrypto && echo $$?)
+LIBCRYPTO_LIBS=
+LIBCRYPTO_PKGCONFIG := $(shell $(PKG_CONFIG) --exists libcrypto && echo $$?)
 ifeq ($(LIBCRYPTO_PKGCONFIG),0)
 	LIBCRYPTO_LIBS=$(shell $(PKG_CONFIG) --libs libcrypto)
 else
 	LIBCRYPTO_LIBS=-lcrypto
 endif
+
+BUILD_YES:=1
+BUILD_MOD:=2
+ifeq ($(BUILD_TLS),yes)
+	FINAL_CFLAGS+=-DUSE_OPENSSL=$(BUILD_YES) $(OPENSSL_CFLAGS)
+	FINAL_LDFLAGS+=$(OPENSSL_LDFLAGS)
 	FINAL_LIBS += ../deps/hiredis/libhiredis_ssl.a $(LIBSSL_LIBS) $(LIBCRYPTO_LIBS)
 endif
 
+REDIS_TLS_MOD=
+REDIS_TLS_MOD_NAME:=redis-tls$(PROG_SUFFIX).so
+REDIS_TLS_MOD_CFLAGS:=$(FINAL_CFLAGS)
+ifeq ($(BUILD_TLS),mod)
+	FINAL_CFLAGS+=-DUSE_OPENSSL=$(BUILD_MOD) $(OPENSSL_CFLAGS)
+	FINAL_LIBS += ../deps/hiredis/libhiredis_ssl.a $(LIBSSL_LIBS) $(LIBCRYPTO_LIBS)
+	REDIS_TLS_MOD=$(REDIS_TLS_MOD_NAME)
+	REDIS_TLS_MOD_CFLAGS+=-DUSE_OPENSSL=$(BUILD_YES) $(OPENSSL_CFLAGS) -DBUILD_TLS_MOD
+endif
+
 ifndef V
     define MAKE_INSTALL
         @printf '    %b %b\n' $(LINKCOLOR)INSTALL$(ENDCOLOR) $(BINCOLOR)$(1)$(ENDCOLOR) 1>&2
@@ -325,7 +341,7 @@ REDIS_CHECK_RDB_NAME=redis-check-rdb$(PROG_SUFFIX)
 REDIS_CHECK_AOF_NAME=redis-check-aof$(PROG_SUFFIX)
 ALL_SOURCES=$(sort $(patsubst %.o,%.c,$(REDIS_SERVER_OBJ) $(REDIS_CLI_OBJ) $(REDIS_BENCHMARK_OBJ)))
 
-all: $(REDIS_SERVER_NAME) $(REDIS_SENTINEL_NAME) $(REDIS_CLI_NAME) $(REDIS_BENCHMARK_NAME) $(REDIS_CHECK_RDB_NAME) $(REDIS_CHECK_AOF_NAME)
+all: $(REDIS_SERVER_NAME) $(REDIS_SENTINEL_NAME) $(REDIS_CLI_NAME) $(REDIS_BENCHMARK_NAME) $(REDIS_CHECK_RDB_NAME) $(REDIS_CHECK_AOF_NAME) $(REDIS_TLS_MOD)
 	@echo """"
 	@echo ""Hint: It's a good idea to run 'make test' ;)""
 	@echo """"
@@ -385,6 +401,10 @@ $(REDIS_CHECK_RDB_NAME): $(REDIS_SERVER_NAME)
 $(REDIS_CHECK_AOF_NAME): $(REDIS_SERVER_NAME)
 	$(REDIS_INSTALL) $(REDIS_SERVER_NAME) $(REDIS_CHECK_AOF_NAME)
 
+# redis-tls.so
+$(REDIS_TLS_MOD_NAME): $(REDIS_SERVER_NAME)
+	$(QUIET_CC)$(CC) -o $@ tls.c -shared -fPIC $(REDIS_TLS_MOD_CFLAGS)
+
 # redis-cli
 $(REDIS_CLI_NAME): $(REDIS_CLI_OBJ)
 	$(REDIS_LD) -o $@ $^ ../deps/hiredis/libhiredis.a ../deps/linenoise/linenoise.o $(FINAL_LIBS)
@@ -410,7 +430,7 @@ commands.c: commands/*.json ../utils/generate-command-code.py
 endif
 
 clean:
-	rm -rf $(REDIS_SERVER_NAME) $(REDIS_SENTINEL_NAME) $(REDIS_CLI_NAME) $(REDIS_BENCHMARK_NAME) $(REDIS_CHECK_RDB_NAME) $(REDIS_CHECK_AOF_NAME) *.o *.gcda *.gcno *.gcov redis.info lcov-html Makefile.dep
+	rm -rf $(REDIS_SERVER_NAME) $(REDIS_SENTINEL_NAME) $(REDIS_CLI_NAME) $(REDIS_BENCHMARK_NAME) $(REDIS_CHECK_RDB_NAME) $(REDIS_CHECK_AOF_NAME) *.o *.gcda *.gcno *.gcov redis.info lcov-html Makefile.dep *.so
 	rm -f $(DEP)
 
 .PHONY: clean
diff --git a/src/server.c b/src/server.c
index 9abfca0d8..7971e2591 100644
--- a/src/server.c
+++ b/src/server.c
@@ -2454,8 +2454,6 @@ void initServer(void) {
 
     createSharedObjects();
     adjustOpenFilesLimit();
-    const char *clk_msg = monotonicInit();
-    serverLog(LL_NOTICE, ""monotonic clock: %s"", clk_msg);
     server.el = aeCreateEventLoop(server.maxclients+CONFIG_FDSET_INCR);
     if (server.el == NULL) {
         serverLog(LL_WARNING,
@@ -6842,7 +6840,6 @@ int main(int argc, char **argv) {
     ACLInit(); /* The ACL subsystem must be initialized ASAP because the
                   basic networking code and client creation depends on it. */
     moduleInitModulesSystem();
-    connTypeInitialize();
 
     /* Store the executable path and arguments in a safe place in order
      * to be able to restart the server later. */
@@ -6962,6 +6959,11 @@ int main(int argc, char **argv) {
         if (server.sentinel_mode) loadSentinelConfigFromQueue();
         sdsfree(options);
     }
+    const char *clk_msg = monotonicInit();
+    serverLog(LL_NOTICE, ""monotonic clock: %s"", clk_msg);
+    connTypeInitialize();
+    moduleInitModulesSystemLast();
+    moduleLoadFromQueue();
     if (server.sentinel_mode) sentinelCheckConfigFile();
     server.supervised = redisIsSupervised(server.supervised_mode);
     int background = server.daemonize && !server.supervised;
@@ -7015,8 +7017,6 @@ int main(int argc, char **argv) {
         }
     #endif /* __arm64__ */
     #endif /* __linux__ */
-        moduleInitModulesSystemLast();
-        moduleLoadFromQueue();
         ACLLoadUsersAtStartup();
         InitServerLast();
         aofLoadManifestFromDisk();
diff --git a/src/tls.c b/src/tls.c
index 6be55946b..1742e6086 100644
--- a/src/tls.c
+++ b/src/tls.c
@@ -32,7 +32,7 @@
 #include ""connhelpers.h""
 #include ""adlist.h""
 
-#ifdef USE_OPENSSL
+#if USE_OPENSSL == 1 /* BUILD_YES */
 
 #include <openssl/conf.h>
 #include <openssl/ssl.h>
@@ -1164,7 +1164,25 @@ int RedisRegisterConnectionTypeTLS()
 
 int RedisRegisterConnectionTypeTLS()
 {
+    serverLog(LL_VERBOSE, ""Connection type %s not builtin"", CONN_TYPE_TLS);
     return C_ERR;
 }
 
 #endif
+
+#ifdef BUILD_TLS_MOD
+
+int RedisModule_OnLoad(void *ctx, RedisModuleString **argv, int argc)
+{
+    UNUSED(argv);
+    UNUSED(argc);
+
+    if (RedisModule_Init(ctx,""tls"",1,REDISMODULE_APIVER_1) == REDISMODULE_ERR)
+        return REDISMODULE_ERR;
+
+    if(connTypeRegister(&CT_TLS) != C_OK)
+        return REDISMODULE_ERR;
+
+    return REDISMODULE_OK;
+}
+#endif
```

Could you please give me any hint about this?"
1198585779,9320,oranagra,2022-07-28T20:11:24Z,"@pizhenwei thank you.
I've sorted this mess out (actually it's even messier, but works), and pushed a commit with your code and my adjustments.
I see we already have one minor conflict, but it's easy to solve, i'll leave it for later.

please have a look at my commit and commit comment for a summary of what i did.
note that the changes you made in server.c were problematic.
first you moved many things that used to be after daemonization to be before it, these things need to be done with care.
secondly, we can't afford to load the modules before initServer (many things will crash), so instead i just extracted the listener creation to a separate function to be done after the modules are loaded.

i've added some POC code to tls.c to test that the various module APIs are working and we don't get funny compilation or link errors due to the tricks i did in redismodule.h, but these needs to be either reverted or put to a good use.

besides that, i think this commit needs to be amended with some of the other loadable extension things you reverted (like readme file and probably others)"
1199088330,9320,pizhenwei,2022-07-29T09:53:00Z,"Hi, @oranagra 
I force pushed a new version to fix conflict. And a new [commit](https://github.com/redis/redis/pull/9320/commits/d0de112f5f9615639e841a69b395add98d7c292a) to introduce ""bootup-only"" & ""deny-unload"" for redis module option. Until this commit, all the auto-test works fine for both TLS mode and regular mode.

Then I modified the latest code you pushed, changes in this [commit](https://github.com/redis/redis/pull/9320/commits/9121331c24eac73c729d88da3132639518a7a7bf):
* Config TLS after initialization of listeners.
* Init cluster after initialization of listeners.
* Set TLS module options as ""bootup-only|deny-unload""
* Add information in TLS.md and auto-test code.

An issue can be reproduced by `./runtest-sentinel --tls --tls-mod` because sentinel timer works before initialization of listeners. During I developed this part,  I realized that moving code in server.c may probably be problematic by myself. So could you please continue this part or give me any hint?"
1203085606,9320,yossigo,2022-08-02T18:42:29Z,@oranagra I think it makes more sense to support loadable TLS in Sentinel.
1203128947,9320,oranagra,2022-08-02T19:31:25Z,"ok.. it means sentinel supports modules.. including registering commands, etc."
1203678442,9320,pizhenwei,2022-08-03T09:00:32Z,"Hi, @oranagra 

The [first version](https://github.com/redis/redis/compare/9121331c24eac73c729d88da3132639518a7a7bf..5e7ffcb9ff82f65b49daa1f28fdaef46a62af082) fixes the previous problem you mentioned.

The [second version](https://github.com/redis/redis/compare/5e7ffcb9ff82f65b49daa1f28fdaef46a62af082..3b358d7ac67775e901078d840e403c1d70ea74b3) fixes the compiling error for hiredis.

There are something remained:
`./runtest-cluster --tls --tls-mod` fails because of `""Skipping diskless-load because there are modules that are not aware of async replication.""`, I'm not familiar with this background, so I did not add code blendly like `RedisModule_SetModuleOptions(ctx, REDISMODULE_OPTIONS_HANDLE_REPL_ASYNC_LOAD);`. (And yes, adding this will lead auto-test passed).
CI build on centos7 fails, `redismodule.h:759:30: note: previous declaration of 'RedisModuleIO' was here` seems to be related to macro definition."
1203738481,9320,oranagra,2022-08-03T09:58:05Z,"i'm looking into that compilation issue on CentOS, i'll let you know when i realize how to resolve it.

regarding REDISMODULE_OPTIONS_HANDLE_REPL_ASYNC_LOAD.
maybe the code in moduleAllModulesHandleReplAsyncLoad should be modified to only check modules with registered data types? or with registered commands?
@eduardobr WDTY?
i.e. a module that doesn't care about data should not fail async diskless loading.. shall we still require it to declare that flag?"
1204099432,9320,oranagra,2022-08-03T15:20:50Z,I force pushed an update to the last commit to resolve the problems with gcc 4.8.
1204963788,9320,pizhenwei,2022-08-04T08:54:35Z,"Hi, @oranagra 
It seems that we have only one thing(about REDISMODULE_OPTIONS_HANDLE_REPL_ASYNC_LOAD) remained, could you please give any hint about the next step?"
1204991051,9320,oranagra,2022-08-04T09:19:39Z,"@pizhenwei let's mark it with that flag. we may conclude that the current logic in moduleAllModulesHandleReplAsyncLoad is wrong and change it as part of another PR.
it'll just mean that this flag this PR adds will be excessive, and not harmful...

so with that final change this PR is ready for merge?
if so, could you please skim though the changes and make sure everything is specified in the top comment or commit comments.
when i'll ask the team to approve it, they're likely to skip reading the code, but they should be aware of the implications of what we changed, any unrelated or dangerous changes, and all interface changes.
in that respect, it may be enough if the top comment just describes the interface changes, and directs people to read the commit comments for the technical ones. "
1205059500,9320,pizhenwei,2022-08-04T10:22:06Z,"> @pizhenwei let's mark it with that flag. we may conclude that the current logic in moduleAllModulesHandleReplAsyncLoad is wrong and change it as part of another PR. it'll just mean that this flag this PR adds will be excessive, and not harmful...
> 
> so with that final change this PR is ready for merge? if so, could you please skim though the changes and make sure everything is specified in the top comment or commit comments. when i'll ask the team to approve it, they're likely to skip reading the code, but they should be aware of the implications of what we changed, any unrelated or dangerous changes, and all interface changes. in that respect, it may be enough if the top comment just describes the interface changes, and directs people to read the commit comments for the technical ones.

Hi, I force pushed the latest version, and modify the main changes in the top comment."
1205143557,9320,oranagra,2022-08-04T11:43:30Z,"@pizhenwei thank you. i edited the top comment and added a few more details, PTAL.
besides, i now remember that we wanted to make some effort to prevent people from loading the TLS module into a different redis version / build.
i think about including release.h and version.h and matching them against the values we'll get from the functions in release.c.
can you look into it?"
1205144104,9320,oranagra,2022-08-04T11:44:09Z,"@redis/core-team this PR is ready for approval and merged, please comment."
1205747796,9320,eduardobr,2022-08-04T20:43:35Z,"> i'm looking into that compilation issue on CentOS, i'll let you know when i realize how to resolve it.
> 
> regarding REDISMODULE_OPTIONS_HANDLE_REPL_ASYNC_LOAD. maybe the code in moduleAllModulesHandleReplAsyncLoad should be modified to only check modules with registered data types? or with registered commands? @eduardobr WDTY? i.e. a module that doesn't care about data should not fail async diskless loading.. shall we still require it to declare that flag?

@oranagra It seems to make sense. If not urgent, I can make a PR next week to loosen the restriction. Right now it should fallback to disk replication."
1205976199,9320,pizhenwei,2022-08-05T02:35:17Z,"> module

Hi, @oranagra 

Since connection type gets driven by Redis module subsystem, I think we have several ways to implement this:
* `RedisModule_Init(ctx,""tls"",REDIS_VERSION_NUM,REDISMODULE_APIVER_1);` tells the module version to Redis server, then Redis server can test this, and allow/refuse to load it. This would be a common method, and other modules can also use this. If so, I prefer a followup PR.
* Compare `REDIS_VERSION_NUM` with `RedisModule_GetServerVersion()` in TLS module. If so, I prefer a followup commit.

Which one do you prefer?"
1206150331,9320,oranagra,2022-08-05T07:46:39Z,"@eduardobr, i had a discussion with @MeirShpilraien and we concluded we rather keep things as they are.
the example case we had in mind is a module that doesn't register any data types, but does have a command that accesses basic type keys, and keeps some cache of it's results in a global. such a module (although being an abuse), will need to properly handle the new hooks we created for the async loading mechanism (despite not registering data types).

@pizhenwei i don't think we need to modify the module API for this. this ""capability"" is something only needed for modules that include server.h and are built as part of redis, so i'd rather that extra check be done directly against it's internals.
I force pushed an update to the last commit to perform that check."
1209290180,9320,pizhenwei,2022-08-09T12:02:08Z,"Hi, @oranagra 
I pushed a [RDMA module](https://github.com/pizhenwei/redis/commit/068a312543627e142db58cbe33ca0a53ad04cadb#diff-e5e60264e3d88cd54b75efbf64b4f0d59f6a585d001cd3c537cc41df82a242b3) which is based on the [version](https://github.com/redis/redis/commit/aeb6380adcadf0c28402e77fe677b8077c77f75c#diff-bb132d62ad3f3a5eac6006e2a383ad909e30072360265a49a962ac765962be34) you pushed, it works fine!(redis-benchmark works fine, and CONFIG GET/SET rdma.port&rdma.bind works fine)

I think it's a good hint, module based connection types seems good!"
1210183655,9320,oranagra,2022-08-10T05:36:46Z,"@pizhenwei thank you.
I think we're nearly ready to merge this.
let me know if you think there's anything still missing.

maybe the last thing that's missing is to add it to the CI.
I made some changes which i'm testing in my repo, i'll squash them into the last commit if they work.
https://github.com/oranagra/redis/commit/656b92c98721c1a94cf42356e2bdc5866671031d
CI runs: [per-push](https://github.com/oranagra/redis/actions/runs/2830281585), [daily](https://github.com/oranagra/redis/actions/runs/2830285334)"
1210219943,9320,pizhenwei,2022-08-10T06:29:42Z,"> @pizhenwei thank you. I think we're nearly ready to merge this. let me know if you think there's anything still missing.
> 
> maybe the last thing that's missing is to add it to the CI. I made some changes which i'm testing in my repo, i'll squash them into the last commit if they work. [oranagra@656b92c](https://github.com/oranagra/redis/commit/656b92c98721c1a94cf42356e2bdc5866671031d) CI runs: [per-push](https://github.com/oranagra/redis/actions/runs/2830281585), [daily](https://github.com/oranagra/redis/actions/runs/2830285334)

Hi @oranagra 
I have nothing missing in this PR. Thanks!
By the way, do you have plan to support TLS module only(remove BUILD_TLS=yes in the future)?"
1210231537,9320,oranagra,2022-08-10T06:43:06Z,"@pizhenwei i don't think we'll want to remove `BUILD_TLS=yes`, since we have it working it seems it would be an unnecessary breakage for people who already use it in this way.
is there any reason why we would want to do that?

p.s. i see quite a lot of failures in the links i provided above.
some are really unclear, trying to figure them out.
but there are also some issues reported by valgrind, maybe you can handle them?"
1210256285,9320,pizhenwei,2022-08-10T07:11:23Z,"> @pizhenwei i don't think we'll want to remove `BUILD_TLS=yes`, since we have it working it seems it would be an unnecessary breakage for people who already use it in this way. is there any reason why we would want to do that?
> 

RDMA connection type supports module only, the changes in Makefile is small and easy. Originally, I think removing `BUILD_TLS=yes` makes tls.c&Makefile clear, keeping the two style is also fine to me.

> p.s. i see quite a lot of failures in the links i provided above. some are really unclear, trying to figure them out. but there are also some issues reported by valgrind, maybe you can handle them?

I'm using debian, I need to setup the same environment to reproduce issues, so it may takes a long time. I'll try my best to fix these."
1210287208,9320,oranagra,2022-08-10T07:42:50Z,"should be as simple as
```
make BUILD_TLS=mod valgrind
./runtest --single unit/latency-monitor --valgrind --tls --tls-mod
```
(i'm not sure if all / other the reports are due to the same issue, or additional ones)"
1210812686,9320,oranagra,2022-08-10T15:09:50Z,"added a few more fixes to the last commit:
```
* Add TLS module to CI, and fix a test suite race conditions
  Now that the listeners are initialized later, it's not sufficient to
  wait for the PID message in the log, we need to wait for the ""Server
  Initialized"" message.
* Fix issues with moduleconfigs test as a result from start_server
  waiting for ""Server Initialized""
* Fix issues with modules/infra test as a result of an additional module
  present
```"
1211638942,9320,pizhenwei,2022-08-11T07:26:09Z,"Hi @madolson 
Thanks for your suggestions! I force pushed a new [version](https://github.com/redis/redis/compare/bc4f43ea6f563979afaadbf924ec28f6a42184b9..7ec3c2e593ca6ef7f3302a104962122bd22d4686), fixed the coding style."
1217887558,9320,pizhenwei,2022-08-17T11:30:55Z,"Hi @yossigo 

I pushed a single [commit](https://github.com/redis/redis/pull/9320/commits/28c1523d4a2070e01f8d9cb01f090f9dd0514e9b) as your suggestion. If squashing this into the previous commit looks better, please let me know."
1222031240,9320,pizhenwei,2022-08-22T08:34:32Z,"Hi @oranagra @yossigo 

The first [version](https://github.com/redis/redis/compare/28c1523d4a2070e01f8d9cb01f090f9dd0514e9b..f71656f048012ac2ff9e3e8133f8ff98b5ab169d) fixes:
* revert `.control` method
* run TCP sentinel when make BUILD_TLS=module
* test sentinel flag in TLS module

The second [version](https://github.com/redis/redis/compare/f71656f048012ac2ff9e3e8133f8ff98b5ab169d..a8be91cc0d2ffaca15b16ebf183383a844194f2e) changes a little, just fixes the conflict in server.c."
1222422966,9320,oranagra,2022-08-22T14:14:43Z,FULL CI: https://github.com/oranagra/redis/actions/runs/2904590091
1222847131,9320,oranagra,2022-08-22T19:44:32Z,"summary of test failures (as far as i can tell):
1. `lmap` isn't supported on tcl 8.5 (fails on macos and centos)
2. some connection issue in `tests/integration/aof.tcl` which fails in all libc malloc based tests (valgrind and sanitizer included) "
1223492757,9320,pizhenwei,2022-08-23T03:38:48Z,"> summary of test failures (as far as i can tell):
> 
> 1. `lmap` isn't supported on tcl 8.5 (fails on macos and centos)
> 2. some connection issue in `tests/integration/aof.tcl` which fails in all libc malloc based tests (valgrind and sanitizer included)

Hi,

I force-pushed a new [version](https://github.com/redis/redis/compare/ffe1fe1a15ef2395339c52019e634f085995f3ad..3c8c3de830a510340c66e41720f80e1f28411249) to fix crash when `make BUILD_TLS=module && ./runtest-sentinel --tls-module`.

About the CI fail issue, I noticed `./runtest --valgrind --no-latency --verbose --clients 1 --timeout 2400 --dump-logs --single integration/aof` fails because:
* the VM is a little slower than a bare metal server. (this test usually succeed on a bare metal server)
* running in valgrind uses more time. (this test usually succeed without valgrind)
* during testing aof, `server_is_up` is ignored. because running valgrind in a VM, client has a chance to try to connect an not-ready port. The following change makes the test always succeed.(I'm not familiar with TCL, this is absolutely wrong, just the easiest way to test...)
```
tests/support/server.tcl

-        if {$code ne ""undefined""} {
-            set serverisup [server_is_up $::host $port $retrynum]
-        } else {
-            set serverisup 1
-        }
+        set serverisup [server_is_up $::host $port $retrynum]
```"
1223832608,9320,oranagra,2022-08-23T09:50:43Z,"@pizhenwei please have a look at my last batch of [changes](https://github.com/redis/redis/compare/3c8c3de830a510340c66e41720f80e1f28411249..4faddf18ca8ca3adb93cf1e4e620be9eaf0f6bf4)

other than fixing the test failures with old TCL and the test framework race, i also made sure that when redis is built with tls as a module, it will not link against the tls libraries (only redis-cli, redis-benchmark, and the redis-tls.so will, but not redis-server and thereby redis-sentinel, which is the same binary).

new CI run: https://github.com/oranagra/redis/actions/runs/2904590091

[edit] some test fail on `MASTER and SLAVE consistency with expire`, that's a known issue in `unstable`

[edit] triggered another CI batch for the ones that failed: https://github.com/oranagra/redis/actions/runs/2911398026"
1223862800,9320,pizhenwei,2022-08-23T10:17:34Z,"> * share

This looks good to me! Thanks!"
1225220438,9320,oranagra,2022-08-24T05:38:06Z,"WOW... just slightly over a year after the PR was opened, and nearly 1.5 years after the original RDMA PR was submitted, this journey is finally complete (or maybe it's just it's first step?).
@pizhenwei thank you for pushing this through!"
1225248746,9320,pizhenwei,2022-08-24T06:17:51Z,"> 

Hi @oranagra @yossigo @madolson 

Thanks for your suggestions and patience in this journey!"
991876178,9934,panjf2000,2021-12-12T10:58:01Z,"This code change passed all tests on my macOS but now fail on linux:

![image](https://user-images.githubusercontent.com/7496278/145709580-577683f9-d2fd-4983-a9e7-d9a7c1ad710f.png)

I fail to see the connection between this code change and `tests/unit/scripting.tcl`, any clue for this?
"
991886804,9934,oranagra,2021-12-12T12:11:34Z,"@panjf2000 thank you.
Note that this isn't only saving system calls, there's also a good chance it saves TCP packets, specifically when the reply list is composed of many small objects, which is the case of many calls for addReplyDeferredLen, see #7123

regarding the tests, the failures seem very consistent, i suppose it's some side effect of your change."
992398718,9934,panjf2000,2021-12-13T11:54:34Z,"Hi @oranagra , could you share some details about these two failed tests with me? I have no idea what might cause these failures and how to fix it, thanks!"
992550252,9934,sundb,2021-12-13T14:44:26Z,"@panjf2000 When use `loglevel verbose`config to start server and run` eval ""local a = {}; local b = {a}; a[1] = b; return a” 0`, you can see `Error writing to client: Invalid argument` log.
`connWritev` always returns -1 in `Script return recursive object` test."
992576707,9934,sundb,2021-12-13T15:11:52Z,"@oranagra I have a question, whether using `writev` may cause blocking of other connections.
For example, if connection A has 100M replied data (10 clientReplyBlocks), in the old code, it would be issued in 10 event cycles, but now it is issued in 10 clientReplyBlocks in one cycle."
992615186,9934,panjf2000,2021-12-13T15:52:37Z,"> @oranagra I have a question, whether using `writev` may cause blocking of other connections. For example, if connection A has 100M replied data (10 clientReplyBlocks), in the old code, it would be issued in 10 event cycles, but now it is issued in 10 clientReplyBlocks in one cycle.

I don't think that would be a problem cuz it will stop sending data when it reaches the limitation of NET_MAX_WRITES_PER_EVENT (1024*64).

Furthermore, the `writev()` call ought to return immediately since the socket is non-blocking, therefore, calling `writev()` will not result in blocking even if there is a large number of bytes in the reply list."
993122959,9934,sundb,2021-12-14T03:26:35Z,"> > @oranagra I have a question, whether using `writev` may cause blocking of other connections. For example, if connection A has 100M replied data (10 clientReplyBlocks), in the old code, it would be issued in 10 event cycles, but now it is issued in 10 clientReplyBlocks in one cycle.
> 
> I don't think that would be a problem cuz it will stop sending data when it reaches the limitation of NET_MAX_WRITES_PER_EVENT (1024*64).

    The replBufBlock size is 16k, which means we can easily break the NET_MAX_WRITES_PER_EVENT 16k limit.
> 
> Furthermore, the `writev()` call ought to return immediately since the socket is non-blocking, therefore, calling `writev()` will not result in blocking even if there is a large number of bytes in the reply list.
    
    Yes, but writev needs to copy memory from user state to kernel anyway,
    which is probably the reason for the NET_MAX_WRITES_PER_EVENT limit.


"
993128358,9934,panjf2000,2021-12-14T03:42:45Z,"> > > @oranagra I have a question, whether using `writev` may cause blocking of other connections. For example, if connection A has 100M replied data (10 clientReplyBlocks), in the old code, it would be issued in 10 event cycles, but now it is issued in 10 clientReplyBlocks in one cycle.
> > 
> > 
> > I don't think that would be a problem cuz it will stop sending data when it reaches the limitation of NET_MAX_WRITES_PER_EVENT (1024*64).
> 
> ```
> The replBufBlock size is 16k, which means we can easily break the NET_MAX_WRITES_PER_EVENT 16k limit.
> ```
> 
> > Furthermore, the `writev()` call ought to return immediately since the socket is non-blocking, therefore, calling `writev()` will not result in blocking even if there is a large number of bytes in the reply list.
> 
> ```
> Yes, but writev needs to copy memory from user state to kernel anyway,
> which is probably the reason for the NET_MAX_WRITES_PER_EVENT limit.
> ```

Good point, maybe we should put `NET_MAX_WRITES_PER_EVENT` into _writeToClient() for `writev()`."
993212994,9934,panjf2000,2021-12-14T06:51:50Z,"@oranagra @sundb @moticless 
PTAL"
993320589,9934,panjf2000,2021-12-14T09:01:40Z,"> Yes, but writev needs to copy memory from user state to kernel anyway,
which is probably the reason for the NET_MAX_WRITES_PER_EVENT limit.

Actually, there's no explicit copying of data within pointers of iov into the kernel's memory. see https://elixir.bootlin.com/linux/v5.0/source/net/ipv4/tcp.c#L1174 and https://lwn.net/Articles/604287/, therefore, even if we don't limit the number of bytes to NET_MAX_WRITES_PER_EVENT, kernel will write bytes up to its maximum of socket send buffer (defined by /proc/sys/net/ipv4/tcp_wmem) instead of all bytes from user space(like 100MB)
@sundb "
993321265,9934,oranagra,2021-12-14T09:02:27Z,"I'm sorry guys, i'm really busy elsewhere, and didn't review the code or correspondence here.
keep up the good work, i'll get to it some day."
993651193,9934,moticless,2021-12-14T15:20:58Z,"> whether using `writev` may cause blocking of other connections. For example, if connection A has 100M replied data (10 clientReplyBlocks), in the old cod

Another aspect around this matter - we might deteriorate the performance in case of big replies, especially with TLS.
I think we better always verify that the total aggregated bytes is not more than NET_MAX_WRITES_PER_EVENT, as like before by the caller. This includes refining the first entry condition `(listLength(c->reply) > 1)` to a condition that verifies the sum of first two replies is no more than NET_MAX_WRITES_PER_EVENT. We also let the higher logic at function writeToClient() to check its own conditions of whether to continue loop, or break."
993756335,9934,panjf2000,2021-12-14T16:48:41Z,"> > whether using `writev` may cause blocking of other connections. For example, if connection A has 100M replied data (10 clientReplyBlocks), in the old cod
> 
> Another aspect around this matter - we might deteriorate the performance in case of big replies, especially with TLS. I think we better always verify that the total aggregated bytes is not more than NET_MAX_WRITES_PER_EVENT, as like before by the caller. This includes refining the first entry condition `(listLength(c->reply) > 1)` to a condition that verifies the sum of first two replies is no more than NET_MAX_WRITES_PER_EVENT. We also let the higher logic at function writeToClient() to check its own conditions of whether to continue loop, or break.

I think `MAX_IOV_SIZE_PER_EVENT` (latest commit in this PR) can achieve the same goal as using `NET_MAX_WRITES_PER_EVENT`, or you have some other comments on the latest commit?
@moticless "
993865373,9934,moticless,2021-12-14T18:33:03Z,"> > > whether using `writev` may cause blocking of other connections. For example, if connection A has 100M replied data (10 clientReplyBlocks), in the old cod
> > 
> > 
> > Another aspect around this matter - we might deteriorate the performance in case of big replies, especially with TLS. I think we better always verify that the total aggregated bytes is not more than NET_MAX_WRITES_PER_EVENT, as like before by the caller. This includes refining the first entry condition `(listLength(c->reply) > 1)` to a condition that verifies the sum of first two replies is no more than NET_MAX_WRITES_PER_EVENT. We also let the higher logic at function writeToClient() to check its own conditions of whether to continue loop, or break.
> 
> I think `MAX_IOV_SIZE_PER_EVENT` (latest commit in this PR) can achieve the same goal as using `NET_MAX_WRITES_PER_EVENT`, or you have some other comments on the latest commit? @moticless

`MAX_IOV_SIZE_PER_EVENT` limits the number of buffers to write. Whereas `NET_MAX_WRITES_PER_EVENT` limits the total number of bytes to write. 

Current implementation regulates the number of transmitted bytes, at some level, by returning from `_writeToClient()` after each single write reply and testing the conditions to continue writing or breaking - such as checking `totwritten > NET_MAX_WRITES_PER_EVENT`. On new implementation, we are writing multiple replies, up-to `MAX_IOV_SIZE_PER_EVENT` replies, of any size, for a single call to `_writeToClient()`. 

IMO  we need to be more careful as long as we don't test it thoroughly. In addition, note that the big impact will be with small replies that can be aggregated easily. For big ones, say of size 1MB, we might find worse performance, especially around SSL, which in the new implementation forces us to make  another memcopy of all the replies into a new multi-buffer (note that your writev ssl implementation might allocates huge buffer on stack). 

In other words, restricting writev() to multiple replies up-to size of `MAX_IOV_SIZE_PER_EVENT` , might be a safe bet, that won't change behavior, effective for small packets, and won't worsen performance in case of huge replies.
"
996655479,9934,sundb,2021-12-17T11:36:54Z,"@panjf2000 I read the implementation of `writev` in `glibc`, which memcpy unconnected memory to a new buffer, and its implementation looks similar to your implementation in `tls`.

https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/posix/writev.c;h=bb19ad3128b7e81675d1c8c2eb7367aed9df4e21;hb=HEAD"
996932271,9934,panjf2000,2021-12-17T18:30:16Z,"> @panjf2000 I read the implementation of `writev` in `glibc`, which memcpy unconnected memory to a new buffer, and its implementation looks similar to your implementation in `tls`.
> 
> https://sourceware.org/git/?p=glibc.git;a=blob;f=sysdeps/posix/writev.c;h=bb19ad3128b7e81675d1c8c2eb7367aed9df4e21;hb=HEAD

This is just a generic implementation of glibc, whereas different devices have their own implementations.

Actually, Linux has carried out a standardized standardization of the device model. For example, devices are divided into character devices, block devices, network devices, etc. For developers, to implement a driver for a device, it must be implemented in accordance with the specifications provided by Linux. 

For the interaction with the user layer, the kernel requires developers to implement a structure called `file_operations`. This structure defines callback pointers for a series of operations, such as `read`, `write`, and other operations. When the user calls methods such as `read()` and `write()`, the kernel will call back to the `file_operations.read` and `file_operations.write` methods of this device essentially, and these rules also apply to `writev`.

The path of `writev` is `writev -> vfs_writev -> ... -> write_iter`, and the `write_iter` implemented by socket is `sock_write_iter`, eventually the kernel will call `tcp_sendmsg_locked` to send out data, just like I mentioned before:

> > Yes, but writev needs to copy memory from user state to kernel anyway,
> > which is probably the reason for the NET_MAX_WRITES_PER_EVENT limit.
> 
> Actually, there's no explicit copying of data within pointers of iov into the kernel's memory. see https://elixir.bootlin.com/linux/v5.0/source/net/ipv4/tcp.c#L1174 and https://lwn.net/Articles/604287/, therefore, even if we don't limit the number of bytes to NET_MAX_WRITES_PER_EVENT, kernel will write bytes up to its maximum of socket send buffer (defined by /proc/sys/net/ipv4/tcp_wmem) instead of all bytes from user space(like 100MB) @sundb

The kernel only copies the iovec array consisting of data addresses without copying any actual data, in `tcp_sendmsg_locked()`, the data in user space goes directly into the socket send buffer in the kernel without any intermediary buffer like it has in the generic `writev()` of glibc. Think it this way: what's the point if `writev()` just simply copies all scattered buffers into one big successive buffer and then calls `write()`?
@sundb "
998461974,9934,panjf2000,2021-12-21T04:26:16Z,"Hi @oranagra , got some time to take a look at this now? We've had a lot of discussions last week and I think we need more opinions and advice here."
998520957,9934,oranagra,2021-12-21T06:55:59Z,"I'm sorry. i'm extremely busy preparing for 7.0 RC1 (need to get all the API and major changes in time).
i'll have to look at this and merge it after RC1 (since it's not an interface / breaking change, or major roadmap feature, we can merge it in any version we like).
If you need further input in order to proceed, maybe try to write a short summary of your conclusions and questions (something i can read, make my opinion, and answer in some 10 minutes), please post and i'll try, if it's more complicated than that and require a deep look at the code, i'll just get to it after RC1 (few more weeks).
sorry."
999272557,9934,panjf2000,2021-12-22T04:14:27Z,"> I'm sorry. i'm extremely busy preparing for 7.0 RC1 (need to get all the API and major changes in time). i'll have to look at this and merge it after RC1 (since it's not an interface / breaking change, or major roadmap feature, we can merge it in any version we like). If you need further input in order to proceed, maybe try to write a short summary of your conclusions and questions (something i can read, make my opinion, and answer in some 10 minutes), please post and i'll try, if it's more complicated than that and require a deep look at the code, i'll just get to it after RC1 (few more weeks). sorry.

OK, we'll wait until then, when is 7.0 RC1 planned to release？ @oranagra "
999328647,9934,oranagra,2021-12-22T06:49:06Z,"few more weeks, unless there are delays."
1006566353,9934,filipecosta90,2022-01-06T12:54:54Z,@panjf2000 @redis/core-team if you agree I would like to extend the https://github.com/redis/redis-benchmarks-specification and use it to verify this PR performance impact across the commands. 
1006575763,9934,oranagra,2022-01-06T13:06:09Z,@filipecosta90 i obviously won't object.. what is there to extend?
1006580388,9934,filipecosta90,2022-01-06T13:11:50Z,"> @filipecosta90 i obviously won't object.. what is there to extend?

I believe extending it to include:

> that the big impact will be with small replies that can be aggregated easily. For big ones, say of size 1MB, we might find worse performance, especially around SSL, which in the new implementation forces us to make another memcopy of all the replies into a new multi-buffer (note that your writev ssl implementation might allocates huge buffer on stack).
> In other words, restricting writev() to multiple replies up-to size of MAX_IOV_SIZE_PER_EVENT , might be a safe bet, that won't change behavior, effective for small packets, and won't worsen performance in case of huge replies.

might be good. "
1011907115,9934,panjf2000,2022-01-13T08:28:04Z,Everything goes well with 7.0 RC1? @oranagra 
1012012352,9934,oranagra,2022-01-13T10:42:37Z,sorry.. still busy. RC1 delayed to end of January.
1026423298,9934,panjf2000,2022-02-01T02:37:56Z,"> sorry.. still busy. RC1 delayed to end of January.

Just to confirm, I guess 7.0-RC is delayed again？"
1026509627,9934,oranagra,2022-02-01T06:11:00Z,"no.. it was just released...
and you're even listed in the release notes: 8-)
https://raw.githubusercontent.com/redis/redis/7.0/00-RELEASENOTES"
1026512014,9934,panjf2000,2022-02-01T06:16:06Z,"> no.. it was just released... and you're even listed in the release notes: 8-) https://raw.githubusercontent.com/redis/redis/7.0/00-RELEASENOTES

Oh sorry, 7.0-rc didn't show up in https://github.com/redis/redis and now I know it's because it's a pre-release."
1028631548,9934,panjf2000,2022-02-03T06:04:37Z,"> sorry it took me so much time
> 
> the conversation was too long for me to read, and the top comment is well.... missing!. so i went in to read the code hiding / ignoring the current comments (made the code hard to read).
> 
> aside from responding to my comments, please also update the top comment, mark any resolved threads as resolved, and please `@` mention me again in the pending threads, trying to sum up the pending decision in case the thread is really long.
> 
> thank you.

Not so sure what it is when you said ""top comment"", did you mean the comments in this issue thread or comments of code review? I think those comments are still there?
@oranagra "
1028672798,9934,oranagra,2022-02-03T07:14:47Z,"by top comment i meant the description of the PR, the one currently says ""As title.""
i'd like it to contain an up to date description of what it does, why, and if there are any other special considerations or unrelated changes it contains."
1029020994,9934,filipecosta90,2022-02-03T14:02:16Z,"> the code LGTM, we need to conclude the open discussion and re-run some benchmarks.

Since the last month I've added more benchmarks to the OSS automation @oranagra . I'm just missing the TLS performance check. Until sunday should a table with an extensive comparison on this branch vs unstable. "
1029025455,9934,panjf2000,2022-02-03T14:07:22Z,"> > the code LGTM, we need to conclude the open discussion and re-run some benchmarks.
> 
> Since the last month I've added more benchmarks to the OSS automation @oranagra . I'm just missing the TLS performance check. Until sunday should a table with an extensive comparison on this branch vs unstable.

Please make sure that the benchmark is run on the latest commit of this branch, thanks!"
1032266676,9934,panjf2000,2022-02-08T06:47:46Z,"> > the code LGTM, we need to conclude the open discussion and re-run some benchmarks.
> 
> Since the last month I've added more benchmarks to the OSS automation @oranagra . I'm just missing the TLS performance check. Until sunday should a table with an extensive comparison on this branch vs unstable.

any update?
@filipecosta90 "
1032459063,9934,filipecosta90,2022-02-08T10:36:12Z,"> > > the code LGTM, we need to conclude the open discussion and re-run some benchmarks.
> > 
> > 
> > Since the last month I've added more benchmarks to the OSS automation @oranagra . I'm just missing the TLS performance check. Until sunday should a table with an extensive comparison on this branch vs unstable.
> 
> any update? @filipecosta90

Need some more days. I'm taking this chance to extend our standard benchmarks (As seen in https://github.com/redis/redis-benchmarks-specification/releases/tag/v0.1.19 ) so I just need one extra change we will have a deeper comparison. "
1032497946,9934,panjf2000,2022-02-08T11:19:40Z,"> > > > the code LGTM, we need to conclude the open discussion and re-run some benchmarks.
> > > 
> > > 
> > > Since the last month I've added more benchmarks to the OSS automation @oranagra . I'm just missing the TLS performance check. Until sunday should a table with an extensive comparison on this branch vs unstable.
> > 
> > 
> > any update? @filipecosta90
> 
> Need some more days. I'm taking this chance to extend our standard benchmarks (As seen in https://github.com/redis/redis-benchmarks-specification/releases/tag/v0.1.19 ) so I just need one extra change we will have a deeper comparison.

Got it, thanks for your efforts."
1033661623,9934,filipecosta90,2022-02-09T11:30:23Z,"@panjf2000 @oranagra and @yossigo a bit of detail on the test:
-  using 13 different tests ( each test is specified in https://github.com/redis/redis-benchmarks-specification/tree/main/redis_benchmarks_specification/test-suites ), that cover pipelined/single command, HASHES, LISTS, and STRINGS.
- Each test has the plaintext/tls variant.
- The tests were run with m6i.8xlarge Vms ( one for client, one for DB ).both on same placement group to allow for the less/most stable network overhead. 
- CPU: Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz, MEM: 128GB. Notice that this is the latest gen intel on AWS (code-named Ice Lake).
- OS was ubuntu 18.04 ( kernel version 5.3.0 ).
- Compiler was gcc-10.3.0
- Used commit hashes:
    - unstable : 34c288fe11dd690686a294f55f0be60e9c5b629d
    - panj200:use-writev: be5e56cd115ecda2509b0c9a373df3099f3b3ec3 

**Based on the above we've observed**:
- PLAIN TEXT:
  - It seems that all benchmarks that are ""heavy"" readers/ ""heavy"" on the reply ( vs for example HSET in which we do the opposite ) are affected negatively by this change, up to -13.8%. 

- TLS 
  - Across all variations, only 1 produced a slight improvement, the remaining 12 test-suites had a negative impact up to -12% achievable ops/sec.
  - The most affected benchmarks were the ones using small values ( 10Bytes - memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Totals ). The exact same benchmark with 100B values had a negative impact of -6.5%.


It seems like we need to further investigate why this change is not having the desired effect. The compiler/os/kernel are rather recent and common, and the benchmarks seem simpler to me.
If you agree, I suggest:
- we profile this further to understand exactly if the CPU cycles of write are indeed being reduced/not reduced and what's taking that time.

Please share your thoughts. 
The detailed results can be checked on the tables below:

-----------------------
Detail:

### Overall achievable ops/sec per testcase/command on a standalone deployment (plain text)
|                                   Test-case                                    |unstable |panjf2000/use-writev|%% diff|
|--------------------------------------------------------------------------------|--------:|-------------------:|-------|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10:Hsets |325447.83|           317522.67|-2.4%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10:Totals|325447.83|           317522.67|-2.4%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values:Hsets             |148311.58|           152999.86|3.2%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values:Totals            |148311.58|           152999.86|3.2%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10:Hsets  |374918.59|           374745.06|-0.0%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10:Totals |374918.59|           374745.06|-0.0%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values:Hsets              |179309.14|           195132.74|8.8%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values:Totals             |179309.14|           195132.74|8.8%   |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Dels                              | 73620.71|            65928.45|-10.4% |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Gets                              | 73620.78|            65928.54|-10.4% |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Setexs                            | 73620.98|            65928.70|-10.4% |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Sets                              | 73620.91|            65928.64|-10.4% |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Totals                            |294483.38|           263714.34|-10.4% |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Dels                               | 75220.92|            65712.19|-12.6% |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Gets                               | 75220.98|            65712.26|-12.6% |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Setexs                             | 75221.20|            65712.41|-12.6% |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Sets                               | 75221.10|            65712.35|-12.6% |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Totals                             |300884.20|           262849.21|-12.6% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Dels                              | 75318.32|            67571.10|-10.3% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Gets                              | 75318.39|            67571.19|-10.3% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Setexs                            | 75318.56|            67571.36|-10.3% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Sets                              | 75318.47|            67571.29|-10.3% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Totals                            |301273.75|           270284.94|-10.3% |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Dels                              | 74074.63|            64732.67|-12.6% |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Gets                              | 74074.73|            64732.74|-12.6% |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Setexs                            | 74074.88|            64732.90|-12.6% |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Sets                              | 74074.78|            64732.84|-12.6% |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Totals                            |296299.02|           258931.14|-12.6% |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hgetalls| 79863.67|            70134.25|-12.2% |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hgets   | 79862.84|            70133.44|-12.2% |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hkeyss  | 79863.39|            70134.01|-12.2% |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hvalss  | 79863.13|            70133.72|-12.2% |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Totals  |319453.03|           280535.42|-12.2% |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Lpops                  |159140.28|           140805.72|-11.5% |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Rpops                  |159139.73|           140805.18|-11.5% |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Totals                 |318280.01|           281610.90|-11.5% |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10:Hsets |148671.87|           128134.92|-13.8% |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10:Totals|148671.87|           128134.92|-13.8% |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values:Hsets             | 94748.89|            91954.06|-2.9%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values:Totals            | 94748.89|            91954.06|-2.9%  |
|memtier_benchmark-1Mkeys-load-list-with-100B-values:Lpushs                      |223176.90|           206434.21|-7.5%  |
|memtier_benchmark-1Mkeys-load-list-with-100B-values:Totals                      |223176.90|           206434.21|-7.5%  |


### Overall achievable ops/sec per testcase/command on a standalone deployment (TLS enabled)
|                                     Test-case                                      |unstable |panjf2000/use-writev|%% diff|
|------------------------------------------------------------------------------------|--------:|-------------------:|-------|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10-tls:Hsets |117441.71|           123563.30|5.2%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10-tls:Totals|117441.71|           123563.30|5.2%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-tls:Hsets             |120857.40|           115852.08|-4.1%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-tls:Totals            |120857.40|           115852.08|-4.1%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10-tls:Hsets  |291553.99|           283106.18|-2.9%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10-tls:Totals |291553.99|           283106.18|-2.9%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-tls:Hsets              |133132.45|           127968.33|-3.9%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-tls:Totals             |133132.45|           127968.33|-3.9%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Dels                              | 48705.14|            45558.85|-6.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Gets                              | 48705.20|            45558.91|-6.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Setexs                            | 48705.37|            45559.10|-6.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Sets                              | 48705.30|            45558.99|-6.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Totals                            |194821.02|           182235.85|-6.5%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Dels                               | 48926.57|            42953.74|-12.2% |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Gets                               | 48926.63|            42953.82|-12.2% |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Setexs                             | 48926.81|            42954.01|-12.2% |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Sets                               | 48926.74|            42953.91|-12.2% |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Totals                             |195706.76|           171815.48|-12.2% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Dels                              | 49220.23|            44032.11|-10.5% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Gets                              | 49220.35|            44032.19|-10.5% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Setexs                            | 49220.50|            44032.35|-10.5% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Sets                              | 49220.42|            44032.26|-10.5% |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Totals                            |196881.49|           176128.92|-10.5% |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Dels                              | 48864.90|            47031.52|-3.8%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Gets                              | 48864.97|            47031.57|-3.8%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Setexs                            | 48865.13|            47031.76|-3.8%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Sets                              | 48865.05|            47031.70|-3.8%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Totals                            |195460.04|           188126.55|-3.8%  |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hgetalls| 48640.77|            45790.97|-5.9%  |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hgets   | 48639.91|            45790.08|-5.9%  |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hkeyss  | 48640.49|            45790.68|-5.9%  |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hvalss  | 48640.19|            45790.37|-5.9%  |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Totals  |194561.36|           183162.10|-5.9%  |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values-tls:Lpops                  | 98082.82|            93128.32|-5.1%  |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values-tls:Rpops                  | 98082.33|            93127.78|-5.1%  |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values-tls:Totals                 |196165.15|           186256.10|-5.1%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10-tls:Hsets | 56816.08|            56344.07|-0.8%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10-tls:Totals| 56816.08|            56344.07|-0.8%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-tls:Hsets             | 52479.04|            49139.47|-6.4%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-tls:Totals            | 52479.04|            49139.47|-6.4%  |
|memtier_benchmark-1Mkeys-load-list-with-100B-values-tls:Lpushs                      |149302.79|           147217.18|-1.4%  |
|memtier_benchmark-1Mkeys-load-list-with-100B-values-tls:Totals                      |149302.79|           147217.18|-1.4%  |
"
1033761181,9934,oranagra,2022-02-09T13:27:49Z,"@filipecosta90 the branch in this PR is nearly 200 commits behind unstable, so it could be that this big impact is due to something else.
we can match them by either testing the merge-base (b93ccee45136992fe08398cc9058f9546708062b) instead of unstable, or by merging unstable into this PR.
I wouldn't want to re-do the entire benchmark and result review, since it might be a dead end, but let's look into it by running one benchmark from the above mentioned commit and check if it changes the picture dramatically."
1033872788,9934,filipecosta90,2022-02-09T15:18:51Z,"> @filipecosta90 the branch in this PR is nearly 200 commits behind unstable, so it could be that this big impact is due to something else. we can match them by either testing the merge-base ([b93ccee](https://github.com/redis/redis/commit/b93ccee45136992fe08398cc9058f9546708062b)) instead of unstable, or by merging unstable into this PR. I wouldn't want to re-do the entire benchmark and result review, since it might be a dead end, but let's look into it by running one benchmark from the above mentioned commit and check if it changes the picture dramatically.

It's relatively easy to trigger a new run for all variations for b93ccee45136992fe08398cc9058f9546708062b. In a couple of hours we should have more clarity :)"
1034044932,9934,filipecosta90,2022-02-09T18:02:32Z,"@oranagra @panjf2000 got new numbers using the merge-base ([b93ccee](https://github.com/redis/redis/commit/b93ccee45136992fe08398cc9058f9546708062b)) instead of unstable.

- PLAINTEXT
   - We still see negative impact on multiple results. Our test with the largest value size (4KiB -- memtier_benchmark-1Mkeys-4KiB-expire-use-case ) seems to have been the most affected. I suggest we test larger value sizes and more reply variations.

- TLS
  -    We see a very interesting pattern here. The only regression on this test is for smaller values (10B). Same like the above I suggest we extend further the variations with more tests with 10B values to ensure this is an expected pattern. 

IMO, we need at least more 5 tests:
TODO:
  - add memtier_benchmark-1Mkeys-8KiB-expire-use-case
  - add memtier_benchmark-1Mkeys-16KiB-expire-use-case
  - add memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-1KiB-values
  - add memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-4KiB-values
  - add memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-10B-values

agree @oranagra? 
@yossigo WRT to your TLS concerns, the bellow numbers seem to ease them correct?


### Overall achievable ops/sec per testcase/command on a standalone deployment (plain text)
|                                   Test-case                                    |b93ccee |panjf2000/use-writev|%% diff|
|--------------------------------------------------------------------------------|--------:|-------------------:|-------|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10:Hsets |323704.54|           317522.67|-1.9%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10:Totals|323704.54|           317522.67|-1.9%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values:Hsets             |158263.45|           152999.86|-3.3%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values:Totals            |158263.45|           152999.86|-3.3%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10:Hsets  |372609.69|           374745.06|0.6%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10:Totals |372609.69|           374745.06|0.6%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values:Hsets              |187243.13|           195132.74|4.2%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values:Totals             |187243.13|           195132.74|4.2%   |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Dels                              | 69777.39|            65928.45|-5.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Gets                              | 69777.47|            65928.54|-5.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Setexs                            | 69777.66|            65928.70|-5.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Sets                              | 69777.55|            65928.64|-5.5%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Totals                            |279110.06|           263714.34|-5.5%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Dels                               | 66342.19|            65712.19|-0.9%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Gets                               | 66342.29|            65712.26|-0.9%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Setexs                             | 66342.46|            65712.41|-0.9%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Sets                               | 66342.38|            65712.35|-0.9%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Totals                             |265369.32|           262849.21|-0.9%  |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Dels                              | 65474.00|            67571.10|3.2%   |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Gets                              | 65474.11|            67571.19|3.2%   |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Setexs                            | 65474.25|            67571.36|3.2%   |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Sets                              | 65474.17|            67571.29|3.2%   |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Totals                            |261896.52|           270284.94|3.2%   |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Dels                              | 69943.17|            64732.67|-7.4%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Gets                              | 69943.26|            64732.74|-7.4%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Setexs                            | 69943.43|            64732.90|-7.4%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Sets                              | 69943.36|            64732.84|-7.4%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Totals                            |279773.23|           258931.14|-7.4%  |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hgetalls| 68454.63|            70134.25|2.5%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hgets   | 68453.78|            70133.44|2.5%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hkeyss  | 68454.38|            70134.01|2.5%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Hvalss  | 68454.08|            70133.72|2.5%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Totals  |273816.88|           280535.42|2.5%   |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Lpops                  |138687.46|           140805.72|1.5%   |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Rpops                  |138686.86|           140805.18|1.5%   |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Totals                 |277374.32|           281610.90|1.5%   |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10:Hsets |135387.58|           128134.92|-5.4%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10:Totals|135387.58|           128134.92|-5.4%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values:Hsets             | 88446.85|            91954.06|4.0%   |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values:Totals            | 88446.85|            91954.06|4.0%   |
|memtier_benchmark-1Mkeys-load-list-with-100B-values:Lpushs                      |199970.61|           206434.21|3.2%   |
|memtier_benchmark-1Mkeys-load-list-with-100B-values:Totals                      |199970.61|           206434.21|3.2%   |


### Overall achievable ops/sec per testcase/command on a standalone deployment (TLS enabled)
|                                     Test-case                                      |unstable |panjf2000/use-writev|%% diff|
|------------------------------------------------------------------------------------|--------:|-------------------:|-------|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10-tls:Hsets |115093.05|           123563.30|7.4%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10-tls:Totals|115093.05|           123563.30|7.4%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-tls:Hsets             |114486.81|           115852.08|1.2%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-tls:Totals            |114486.81|           115852.08|1.2%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10-tls:Hsets  |280377.81|           283106.18|1.0%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10-tls:Totals |280377.81|           283106.18|1.0%   |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-tls:Hsets              |133307.04|           127968.33|-4.0%  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-tls:Totals             |133307.04|           127968.33|-4.0%  |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Dels                              | 43154.22|            45558.85|5.6%   |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Gets                              | 43154.29|            45558.91|5.6%   |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Setexs                            | 43154.44|            45559.10|5.6%   |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Sets                              | 43154.38|            45558.99|5.6%   |
|memtier_benchmark-1Mkeys-100B-expire-use-case-tls:Totals                            |172617.33|           182235.85|5.6%   |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Dels                               | 46602.27|            42953.74|-7.8%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Gets                               | 46602.38|            42953.82|-7.8%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Setexs                             | 46602.54|            42954.01|-7.8%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Sets                               | 46602.46|            42953.91|-7.8%  |
|memtier_benchmark-1Mkeys-10B-expire-use-case-tls:Totals                             |186409.65|           171815.48|-7.8%  |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Dels                              | 44389.86|            44032.11|-0.8%  |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Gets                              | 44389.94|            44032.19|-0.8%  |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Setexs                            | 44390.09|            44032.35|-0.8%  |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Sets                              | 44390.02|            44032.26|-0.8%  |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case-tls:Totals                            |177559.91|           176128.92|-0.8%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Dels                              | 47189.52|            47031.52|-0.3%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Gets                              | 47189.58|            47031.57|-0.3%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Setexs                            | 47189.77|            47031.76|-0.3%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Sets                              | 47189.66|            47031.70|-0.3%  |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case-tls:Totals                            |188758.54|           188126.55|-0.3%  |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hgetalls| 44342.74|            45790.97|3.3%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hgets   | 44341.92|            45790.08|3.3%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hkeyss  | 44342.46|            45790.68|3.3%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Hvalss  | 44342.16|            45790.37|3.3%   |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values-tls:Totals  |177369.27|           183162.10|3.3%   |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values-tls:Lpops                  | 89297.76|            93128.32|4.3%   |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values-tls:Rpops                  | 89297.20|            93127.78|4.3%   |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values-tls:Totals                 |178594.96|           186256.10|4.3%   |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10-tls:Hsets | 56357.19|            56344.07|-0.0%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10-tls:Totals| 56357.19|            56344.07|-0.0%  |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-tls:Hsets             | 48555.22|            49139.47|1.2%   |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-tls:Totals            | 48555.22|            49139.47|1.2%   |
|memtier_benchmark-1Mkeys-load-list-with-100B-values-tls:Lpushs                      |143990.62|           147217.18|2.2%   |
|memtier_benchmark-1Mkeys-load-list-with-100B-values-tls:Totals                      |143990.62|           147217.18|2.2%   |"
1034129564,9934,oranagra,2022-02-09T19:44:29Z,"from these benchmarks it looks like this PR doesn't do any good.
so just to put things in proportions:

This PR:
```
src/redis-benchmark -n 10000 command
  throughput summary: 913.99 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       19.076     2.552    17.999    31.199    46.527    74.623
```

merge-base (b93ccee):
```
src/redis-benchmark -n 10000 command
  throughput summary: 330.73 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
      109.745     5.248   114.431   147.071   152.319   184.447
```

This is because back then, COMMAND command was a heavy user of addReplyDeferredLen, which adds ton of nodes to the reply buffer.

If we want to find something equivalent in normal data commands, we can use a pipeline of ZRANGE, or ZINTER.
or maybe just something with many big objects like HGETALL on hash with several values of 50kb each.

but putting that aside, it should be easy to find the places where this PR gives great value, even if not common in redis, modules can have more of these, what's bothering me is why we appear to have a regression on common cases.

@panjf2000 maybe you can reproduce one of these, profile and and realize where the regression comes from?"
1034429060,9934,panjf2000,2022-02-10T02:31:19Z,"> from these benchmarks it looks like this PR doesn't do any good. so just to put things in proportions:
> 
> This PR:
> 
> ```
> src/redis-benchmark -n 10000 command
>   throughput summary: 913.99 requests per second
>   latency summary (msec):
>           avg       min       p50       p95       p99       max
>        19.076     2.552    17.999    31.199    46.527    74.623
> ```
> 
> merge-base ([b93ccee](https://github.com/redis/redis/commit/b93ccee45136992fe08398cc9058f9546708062b)):
> 
> ```
> src/redis-benchmark -n 10000 command
>   throughput summary: 330.73 requests per second
>   latency summary (msec):
>           avg       min       p50       p95       p99       max
>       109.745     5.248   114.431   147.071   152.319   184.447
> ```
> 
> This is because back then, COMMAND command was a heavy user of addReplyDeferredLen, which adds ton of nodes to the reply buffer.
> 
> If we want to find something equivalent in normal data commands, we can use a pipeline of ZRANGE, or ZINTER. or maybe just something with many big objects like HGETALL on hash with several values of 50kb each.
> 
> but putting that aside, it should be easy to find the places where this PR gives great value, even if not common in redis, modules can have more of these, what's bothering me is why we appear to have a regression on common cases.
> 
> @panjf2000 maybe you can reproduce one of these, profile and and realize where the regression comes from?

OK, I will take time to reproduce it and try to find out the root cause."
1039912051,9934,oranagra,2022-02-15T06:40:37Z,"@panjf2000 any news or plans when you can look into it?
if we're gonna merge this, i'd rather it'll be part of 7.0 RC2 (possibly around a week from now). don't wanna introduce that late before the 7.0.0 release."
1039914493,9934,panjf2000,2022-02-15T06:45:14Z,"Sorry, I've been a little busy lately and am about to diagnose this issue today, get some profiling results.
@oranagra "
1040001259,9934,panjf2000,2022-02-15T08:37:47Z,"A funny fact is that I ran the benchmark on a [comman case which has the biggest regression in the above table](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-4KiB-expire-use-case.yml) manually and got the results which indicate that there was no performance difference between `unstable` and `use-writev`, they had almost the same performance.

```
8  AMD EPYC 7K62 48-Core Processor
16GB mem
```

unstable: 
```
memtier_benchmark ""--data-size"" ""4000"" --command ""SETEX __key__ 10 __value__"" --command-key-pattern=""R"" --command ""SET __key__ __value__"" --command-key-pattern=""R"" --command ""GET __key__"" --command-key-pattern=""R"" --command ""DEL __key__"" --command-key-pattern=""R""  -c 50 -t 2 --hide-histogram --test-time 60
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  60 secs]  0 threads:    13290271 ops,  210625 (avg:  221464) ops/sec, 9.98MB/sec (avg: 10.49MB/sec),  0.47 (avg:  0.45) msec latency

2         Threads
50        Connections per thread
60        Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
--------------------------------------------------------------------------------------------------
Setexs      55367.12         0.45193         0.41500         0.75100         2.89500      3508.40
Sets        55366.65         0.45135         0.41500         0.75100         2.91100      2967.77
Gets        55366.12         0.45108         0.40700         0.75100         3.00700      2165.52
Dels        55365.84         0.45094         0.40700         0.75100         2.86300      2102.65
Totals     221465.72         0.45133         0.41500         0.75100         2.91100     10744.34
```

use-writev:

```
memtier_benchmark ""--data-size"" ""4000"" --command ""SETEX __key__ 10 __value__"" --command-key-pattern=""R"" --command ""SET __key__ __value__"" --command-key-pattern=""R"" --command ""GET __key__"" --command-key-pattern=""R"" --command ""DEL __key__"" --command-key-pattern=""R""  -c 50 -t 2 --hide-histogram --test-time 60
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  60 secs]  0 threads:    13289816 ops,  227680 (avg:  221460) ops/sec, 10.79MB/sec (avg: 10.49MB/sec),  0.44 (avg:  0.45) msec latency

2         Threads
50        Connections per thread
60        Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
--------------------------------------------------------------------------------------------------
Setexs      55366.13         0.45179         0.41500         0.75100         2.87900      3508.34
Sets        55365.66         0.45143         0.41500         0.75100         3.03900      2967.72
Gets        55365.26         0.45101         0.41500         0.75100         3.07100      2165.49
Dels        55364.86         0.45114         0.41500         0.75100         3.05500      2102.61
Totals     221461.92         0.45134         0.41500         0.75100         2.99100     10744.16
```

Besides, they basically share the same hierarchical structure of flame graph:
unstable:

![redis_unstable](https://user-images.githubusercontent.com/7496278/154022530-352c2fe5-e495-40b7-9789-414c1aeb3792.svg)

user-writev:
![redis_writev](https://user-images.githubusercontent.com/7496278/154022554-7d6de217-1a58-49bb-abed-fa915e74a174.svg)

Am I doing anything wrong about the benchmarks?
@oranagra @filipecosta90 "
1040007112,9934,panjf2000,2022-02-15T08:44:42Z,"It seems that there were not many calls to `writev()` in this common case, which should also be as expected."
1040007713,9934,oranagra,2022-02-15T08:45:26Z,"@panjf2000 did you test unstable? or the merge-base (b93ccee45136992fe08398cc9058f9546708062b)?
since unstable contains other changes, let's be sure to test the merge base to eliminate them, so if you did test unstable, and you repeat that test?"
1040008835,9934,panjf2000,2022-02-15T08:46:47Z,"> @panjf2000 did you test unstable? or the merge-base ([b93ccee](https://github.com/redis/redis/commit/b93ccee45136992fe08398cc9058f9546708062b))? since unstable contains other changes, let's be sure to test the merge base to eliminate them, so if you did test unstable, and you repeat that test?

Yes, I created a new branch from commit [b93ccee](https://github.com/redis/redis/commit/b93ccee45136992fe08398cc9058f9546708062b) and I've repeated the benchmark multiple times, it gets the same results."
1040013976,9934,oranagra,2022-02-15T08:52:33Z,"@panjf2000 while we're waiting for Filipe to explain this, can you maybe do some simple benchmark comparing the COMMAND command with TLS?
i.e. similar to what i did here: https://github.com/redis/redis/pull/9934#issuecomment-1034129564
so we can decide if we wanna keep the current code we have for TLS, or take Yossi's advise and revert that in some way if we see a negative or neutral impact."
1040029452,9934,panjf2000,2022-02-15T09:09:43Z,"> @panjf2000 while we're waiting for Filipe to explain this, can you maybe do some simple benchmark comparing the COMMAND command with TLS? i.e. similar to what i did here: [#9934 (comment)](https://github.com/redis/redis/pull/9934#issuecomment-1034129564) so we can decide if we wanna keep the current code we have for TLS, or take Yossi's advise and revert that in some way if we see a negative or neutral impact.

I assume that I should use `memtier_benchmark` instead of `redis-benchmark`? cuz the former support TLS and the latter doesn't? but I still don't know how to send `COMMAND` command via `memtier_benchmark`, I only ran the general test and the result shows below:

unstable:

```
memtier_benchmark --hide-histogram --tls --cert=./tests/tls/redis.crt --key=./tests/tls/redis.key --cacert=./tests/tls/ca.crt -n 10000
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  25 secs]  0 threads:     2000000 ops,   76915 (avg:   79674) ops/sec, 3.19MB/sec (avg: 3.32MB/sec),  2.60 (avg:  2.51) msec latency

4         Threads
50        Connections per thread
10000     Requests per client


ALL STATS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets         7255.52          ---          ---         3.88510         3.85500         6.17500        19.07100       558.80
Gets        72475.46       542.17     71933.29         2.37170         2.36700         3.93500        11.19900      2841.23
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals      79730.98       542.17     71933.29         2.50942         2.39900         4.63900        11.83900      3400.04
```

use-writev:

```
memtier_benchmark --hide-histogram --tls --cert=./tests/tls/redis.crt --key=./tests/tls/redis.key --cacert=./tests/tls/ca.crt -n 10000
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  24 secs]  0 threads:     2000000 ops,   75905 (avg:   82053) ops/sec, 3.15MB/sec (avg: 3.42MB/sec),  2.63 (avg:  2.44) msec latency

4         Threads
50        Connections per thread
10000     Requests per client


ALL STATS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
----------------------------------------------------------------------------------------------------------------------------
Sets         7484.16          ---          ---         3.77369         3.56700         6.11100        20.09500       576.41
Gets        74759.36       559.26     74200.11         2.30274         2.28700         3.90300        11.07100      2930.77
Waits           0.00          ---          ---             ---             ---             ---             ---          ---
Totals      82243.52       559.26     74200.11         2.43660         2.31900         4.57500        12.15900      3507.18
```
@oranagra "
1040038251,9934,panjf2000,2022-02-15T09:18:47Z,"Sorry, I think I've already known how to do that, working on it."
1040038781,9934,YaacovHazan,2022-02-15T09:19:23Z,"Hi @panjf2000 in memtier-benchmark you have the option to send an arbitrary command. for that please use the `--command` option.
For ex': memtier-benchmark --command=""COMMAND""."
1040043226,9934,panjf2000,2022-02-15T09:24:11Z,"unstable:

```
memtier_benchmark --hide-histogram --tls --cert=./tests/tls/redis.crt --key=./tests/tls/redis.key --cacert=./tests/tls/ca.crt --command=""COMMAND"" --test-time=30
[RUN #1] Launching threads now... -n 1000
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 101%,  30 secs]  0 threads:       14881 ops,     533 (avg:     493) ops/sec, 39.26MB/sec (avg: 36.27MB/sec), 383.51 (avg: 405.45) msec latency

4         Threads
50        Connections per thread
30        Seconds


ALL STATS
===================================================================================================
Type          Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
---------------------------------------------------------------------------------------------------
Commands       477.46       405.45653       403.45500       481.27900       483.32700     35956.27
Totals         477.46       405.45653       403.45500       481.27900       483.32700     35956.27
```

use-writev:

```
memtier_benchmark --hide-histogram --tls --cert=./tests/tls/redis.crt --key=./tests/tls/redis.key --cacert=./tests/tls/ca.crt --command=""COMMAND"" --test-time=30
[RUN #1] Launching threads now... -n 1000
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,  30 secs]  0 threads:       16052 ops,     551 (avg:     532) ops/sec, 40.59MB/sec (avg: 39.16MB/sec), 364.16 (avg: 375.35) msec latency

4         Threads
50        Connections per thread
30        Seconds


ALL STATS
===================================================================================================
Type          Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
---------------------------------------------------------------------------------------------------
Commands       532.42       375.34021       374.78300       428.03100       436.22300     40095.31
Totals         532.42       375.34021       374.78300       428.03100       436.22300     40095.31
```
@oranagra "
1040464720,9934,panjf2000,2022-02-15T16:08:09Z,"Since I've merged the latest commits of `unstable` into this PR, we might want to trigger a new benchmark test for all variations to get the latest result, would you please run the new benchmark based on the latest `unstable` and this PR? thanks~
@filipecosta90 "
1040504480,9934,filipecosta90,2022-02-15T16:44:20Z,"> Since I've merged the latest commits of `unstable` into this PR, we might want to trigger a new benchmark test for all variations to get the latest result, would you please run the new benchmark based on the latest `unstable` and this PR? thanks~ @filipecosta90

will do @panjf2000 . 
In the meantime, I've run manually the 4KiB expire use case ( notice that you missed the prepopulation ) on 2x AWS m6i.8xlarge VMs for:
- panjf2000/use-writev: 7b59cffe075e4adda8832b84f8f087b6bc637a5c .  265K ops/sec. p50=0.383
- unstable: 3881f7850f9f81720315bd4f33f2f9dedcc242bb  .  276K ops/sec. p50=0.367
```
# redis-server command
taskset -c 0 ./src/redis-server --protected-mode no --requirepass perf --port 16379 --save """" --daemonize yes

# prepopulation
memtier_benchmark ""--data-size"" ""4000"" ""--command"" ""SET __key__ __value__"" ""--command-key-pattern"" ""P"" ""-c"" ""50"" ""-t"" ""2"" ""--hide-histogram"" -s 172.31.54.45 -a perf -p 16379 

# benchmark
memtier_benchmark ""--data-size"" ""4000"" --command ""SETEX __key__ 10 __value__"" --command-key-pattern=""R"" --command ""SET __key__ __value__"" --command-key-pattern=""R"" --command ""GET __key__"" --command-key-pattern=""R"" --command ""DEL __key__"" --command-key-pattern=""R""  -c 50 -t 2 --hide-histogram --test-time 300 -s 172.31.54.45 -a perf -p 16379
```

and noticed the exact same pattern of regression

### unstable 3881f7850f9f81720315bd4f33f2f9dedcc242bb
```
memtier_benchmark ""--data-size"" ""4000"" --command ""SETEX __key__ 10 __value__"" --command-key-pattern=""R"" --command ""SET __key__ __value__"" --command-key-pattern=""R"" --command ""GET __key__"" --command-key-pattern=""R"" --command ""DEL __key__"" --command-key-pattern=""R""  -c 50 -t 2 --hide-histogram --test-time 300 -s 172.31.54.45 -a perf -p 16379
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%, 300 secs]  0 threads:    82805244 ops,  273053 (avg:  276016) ops/sec, 13.00MB/sec (avg: 13.14MB/sec),  0.37 (avg:  0.36) msec latency

2         Threads
50        Connections per thread
300       Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Setexs      69004.12         0.36276         0.36700         0.53500         0.68700      4372.56 
Sets        69004.04         0.36264         0.36700         0.52700         0.68700      3698.80 
Gets        69003.95         0.36219         0.36700         0.52700         0.68700      2761.87 
Dels        69003.88         0.36230         0.36700         0.53500         0.67900      2620.63 
Totals     276015.99         0.36247         0.36700         0.53500         0.68700     13453.85 
```

### panjf2000/use-writev: 7b59cffe075e4adda8832b84f8f087b6bc637a5c

```
memtier_benchmark ""--data-size"" ""4000"" --command ""SETEX __key__ 10 __value__"" --command-key-pattern=""R"" --command ""SET __key__ __value__"" --command-key-pattern=""R"" --command ""GET __key__"" --command-key-pattern=""R"" --command ""DEL __key__"" --command-key-pattern=""R""  -c 50 -t 2 --hide-histogram --test-time 300 -s 172.31.54.45 -a perf -p 16379
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%, 300 secs]  0 threads:    79465025 ops,  264809 (avg:  264881) ops/sec, 12.61MB/sec (avg: 12.61MB/sec),  0.38 (avg:  0.38) msec latency

2         Threads
50        Connections per thread
300       Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Setexs      66220.63         0.37800         0.38300         0.53500         0.71100      4196.16 
Sets        66220.55         0.37783         0.38300         0.53500         0.71100      3549.60 
Gets        66220.47         0.37740         0.38300         0.53500         0.71100      2650.11 
Dels        66220.40         0.37753         0.38300         0.53500         0.71100      2514.91 
Totals     264882.05         0.37769         0.38300         0.53500         0.71100     12910.78 
```

It seems we're detecting some slight regression. I'm fixing some automation issues that will allow us to re-run all tests with no manual work. Give me some hours. "
1040519945,9934,oranagra,2022-02-15T16:52:55Z,"@panjf2000 please try again to reproduce it (with pre-population maybe), so that you can try to profile it and realize where it comes from, or we need to keep trying to figure out what's different between these two benchmarks."
1040971890,9934,panjf2000,2022-02-16T01:06:31Z,"> @panjf2000 please try again to reproduce it (with pre-population maybe), so that you can try to profile it and realize where it comes from, or we need to keep trying to figure out what's different between these two benchmarks.

I will try it today and see if that makes a difference."
1041136298,9934,panjf2000,2022-02-16T05:55:47Z,"I've run the benchmark based on the latest commit of both `unstable` and `use-writev`, with pre-population, and this is the result:

unstable:
```
memtier_benchmark --hide-histogram --data-size ""4000"" --command ""SET __key__ __value__"" --command-key-pattern ""P"" -c 50 -t 2
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,   4 secs]  0 threads:     1000000 ops,  221704 (avg:  204181) ops/sec, 11.60MB/sec (avg: 10.69MB/sec),  0.45 (avg:  0.49) msec latency

2         Threads
50        Connections per thread
10000     Requests per client


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
--------------------------------------------------------------------------------------------------
Sets       204361.65         0.48970         0.43900         0.82300         2.57500     10952.28
Totals     204361.65         0.48970         0.43900         0.82300         2.57500     10952.28


memtier_benchmark --data-size ""4000"" --command ""SETEX __key__ 10 __value__"" --command-key-pattern=""R"" --command ""SET __key__ __value__"" --command-key-pattern=""R"" --command ""GET __key__"" --command-key-pattern=""R"" --command ""DEL __key__"" --command-key-pattern=""R""  -c 50 -t 2 --hide-histogram --test-time 300
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%, 300 secs]  0 threads:    64353309 ops,  213534 (avg:  214503) ops/sec, 10.31MB/sec (avg: 10.36MB/sec),  0.47 (avg:  0.47) msec latency

2         Threads
50        Connections per thread
300       Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
--------------------------------------------------------------------------------------------------
Setexs      53625.94         0.46644         0.41500         0.79900         3.85500      3398.09
Sets        53625.83         0.46607         0.41500         0.79900         3.96700      2874.47
Gets        53625.75         0.46566         0.41500         0.79900         3.98300      2296.61
Dels        53625.68         0.46576         0.41500         0.79900         3.93500      2036.59
Totals     214503.19         0.46598         0.41500         0.79900         3.93500     10605.77
```

use-writev:

```
memtier_benchmark --hide-histogram --data-size ""4000"" --command ""SET __key__ __value__"" --command-key-pattern ""P"" -c 50 -t 2
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%,   4 secs]  0 threads:     1000000 ops,  206796 (avg:  204968) ops/sec, 10.82MB/sec (avg: 10.73MB/sec),  0.48 (avg:  0.49) msec latency

2         Threads
50        Connections per thread
10000     Requests per client


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
--------------------------------------------------------------------------------------------------
Sets       205408.23         0.48781         0.43900         0.82300         3.71100     11008.37
Totals     205408.23         0.48781         0.43900         0.82300         3.71100     11008.37

memtier_benchmark --data-size ""4000"" --command ""SETEX __key__ 10 __value__"" --command-key-pattern=""R"" --command ""SET __key__ __value__"" --command-key-pattern=""R"" --command ""GET __key__"" --command-key-pattern=""R"" --command ""DEL __key__"" --command-key-pattern=""R""  -c 50 -t 2 --hide-histogram --test-time 300
Writing results to stdout
[RUN #1] Preparing benchmark client...
[RUN #1] Launching threads now...
[RUN #1 100%, 300 secs]  0 threads:    64578994 ops,  210243 (avg:  215255) ops/sec, 10.17MB/sec (avg: 10.39MB/sec),  0.47 (avg:  0.46) msec latency

2         Threads
50        Connections per thread
300       Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency    p100 Latency       KB/sec
--------------------------------------------------------------------------------------------------
Setexs      53814.03         0.46494         0.41500         0.79900         3.88700      3410.01
Sets        53813.94         0.46437         0.41500         0.79900         3.88700      2884.56
Gets        53813.86         0.46407         0.41500         0.79900         3.88700      2304.63
Dels        53813.77         0.46399         0.41500         0.79900         3.85500      2043.74
Totals     215255.61         0.46434         0.41500         0.79900         3.87100     10642.94
```"
1044682329,9934,panjf2000,2022-02-18T15:14:05Z,"I still failed to reproduce the regression results on my own machine, it seemed that the performance data of `unstable` and `use-writev` is almost the same across generic scenarios, and the profiling results of two branches are basically the same, besides, `writev()` system call didn't even show up in the flame graph of `use-writev`, which could mean that most of the time it used `write()` like `unstable` did, so I think there shouldn't be a performance gap between these two branches in theory.

## unstable
<img width=""1234"" alt=""image"" src=""https://user-images.githubusercontent.com/7496278/154706426-126eba74-82d1-4c87-9895-6a900eb612c6.png"">

## use-writev
<img width=""1200"" alt=""image"" src=""https://user-images.githubusercontent.com/7496278/154706283-437fee06-118f-46ca-bc73-2fb9b00b79c6.png"">

Is there any chance I can make a program profiling on your AWS VMs? since the regression results seem to be reproduced on those VMs easily. @filipecosta90 

@oranagra 

"
1045006210,9934,oranagra,2022-02-18T18:51:15Z,"Maybe the difference in the networking of TCP/IP stack.
I.e. the difference is not in CPU time so will not be visible in flame graph, but rather some difference or side effect of using writev on TCP / network behavior.
I'm not sure how Filipe's benchmarks are run, but i assume Andy's are local.
It could also be some issue with AWS hypervisor, but maybe the first step is to try comparing them on a real network (commonly the actual use case for redis) ."
1046117934,9934,filipecosta90,2022-02-19T22:43:52Z,"@oranagra / @panjf2000, WRT to:
>  I'm not sure how Filipe's benchmarks are run, but i assume Andy's are local.

The benchmarks are using 2 VMs ( 1DB and 1 CLIENT Virtual machines (KVM) on AWS ).


WRT to benchmark automation, I noticed some variance on multiple runs of the same benchmark for unstable and the comparison branch. 
Apart from 4 unstable benchmarks ( let's discard them if you agree there is no change that can impact it ), we have 9 benchmarks with no change. 
```
redisbench-admin compare --triggering_env ci   --baseline-branch unstable --comparison-branch panjf2000:use-writev  --github_repo redis --github_org redis --use_metric_context_path true --metric_name Ops/sec --testname_regex .+Totals
Effective log level set to INFO
2022-02-19 21:52:52,199 INFO Using: redisbench-admin 0.6.23
2022-02-19 21:52:52,793 INFO Using deployment_type=oss-standalone and deployment_name=oss-standalone for the analysis
2022-02-19 21:52:52,793 INFO Using a time-delta from 7 days ago to now
2022-02-19 21:52:52,984 WARNING Based on test-cases set (key=ci.benchmarks.redislabs/ci/redis/redis:testcases_AND_metric_context_path) we have 14 comparison points. 
2022-02-19 21:53:02,096 INFO Printing differential analysis between branches
# Comparison between unstable and panjf2000:use-writev for metric: Ops/sec. Time Period from 7 days ago to now
|                                   Test Case                                    |   Baseline (median obs. +- std.dev)    |  Comparison (median obs. +- std.dev)   |% change (higher-better)|            Note             |
|--------------------------------------------------------------------------------|----------------------------------------|----------------------------------------|------------------------|-----------------------------|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10:Totals| 312220 +- 20.6% UNSTABLE (5 datapoints)| 315862 +- 1.6%  (2 datapoints)         |1.2%                    |UNSTABLE (very high variance)|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values:Totals            | 134161 +- 0.5%  (5 datapoints)         | 117642 +- 18.6% UNSTABLE (2 datapoints)|-12.3%                  |UNSTABLE (very high variance)|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10:Totals | 364426 +- 4.0%  (5 datapoints)         | 371266 +- 0.2%  (2 datapoints)         |1.9%                    |-- no change --              |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values:Totals             | 143372 +- 4.3%  (5 datapoints)         | 141731 +- 2.3%  (2 datapoints)         |-1.1%                   |-- no change --              |
|memtier_benchmark-1Mkeys-100B-expire-use-case:Totals                            | 219014 +- 2.3%  (5 datapoints)         | 219491 +- 0.3%  (2 datapoints)         |0.2%                    |-- no change --              |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Totals                             | 217413 +- 0.4%  (5 datapoints)         | 219192 +- 0.1%  (2 datapoints)         |0.8%                    |-- no change --              |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Totals                            | 217806 +- 0.4%  (5 datapoints)         | 219764 +- 0.8%  (2 datapoints)         |0.9%                    |-- no change --              |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Totals                            | 217860 +- 2.0%  (5 datapoints)         | 218195 +- 0.7%  (2 datapoints)         |0.2%                    |-- no change --              |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Totals  | 169042 +- 1.9%  (5 datapoints)         | 167368 +- 1.3%  (2 datapoints)         |-1.0%                   |-- no change --              |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Totals                 | 165841 +- 4.1%  (5 datapoints)         | 183398 +- 10.4% UNSTABLE (2 datapoints)|10.6%                   |UNSTABLE (very high variance)|
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10:Totals| 140570 +- 0.3%  (5 datapoints)         | 139839 +- 0.7%  (2 datapoints)         |-0.5%                   |-- no change --              |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values:Totals            | 99571 +- 14.4% UNSTABLE (5 datapoints) | 100428 +- 0.9%  (2 datapoints)         |0.9%                    |UNSTABLE (very high variance)|
|memtier_benchmark-1Mkeys-load-list-with-100B-values:Totals                      | 150928 +- 1.8%  (5 datapoints)         | 152043 +- 0.4%  (2 datapoints)         |0.7%                    |-- no change --              |
2022-02-19 21:53:02,162 INFO Detected a total of 9 stable tests between versions.
2022-02-19 21:53:02,162 WARNING Detected a total of 4 highly unstable benchmarks.
```

@oranagra and @panjf2000 still about the impact of commands with deferred len I've tested a RedisTimeSeries module use-case, that uses deferred replies (cc @gkorland @OfirMos @dann). I was expecting higher impact on the numbers. Even thought we've got a slight improvement over unstable, the change is not as meaningful on the `COMMAND` numbers shared (that I can indeed reproduce ). TLDR, the deferred positive impact is only ""measurable/meaningfull"" on a really large number of deferred writes per reply (otherwise the impact will be faded away like the one bellow ). 

unstable
```
memtier_benchmark --command=""TS.RANGE ts - + COUNT 1000""  --hide-histogram --test-time 600 -s 10.3.0.175 -p 6380 
(...)
ALL STATS
====================================================================================================
Type           Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------
Ts.ranges      5218.29        38.29828        38.39900        42.23900        43.77500    139793.09 
Totals         5218.29        38.29828        38.39900        42.23900        43.77500    139793.09 
```

panjf2000/redis
```
ALL STATS
====================================================================================================
Type           Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------
Ts.ranges      5315.26        37.60366        37.63100        43.00700        45.82300    142390.83 
Totals         5315.26        37.60366        37.63100        43.00700        45.82300    142390.83 
```

### To conclude
TLDR I was expecting a larger impact, but on the ""common case"" this is not happening. Nonetheless, there are indeed some ""not-so-usuall"" use-cases that are improved. I see reason to merge it :) "
1046173772,9934,oranagra,2022-02-20T06:36:05Z,"> I've tested a RedisTimeSeries module use-case, that uses deferred replies. I was expecting higher impact on the numbers

@filipecosta90 i'm not sure how RedisRimeSeries uses deferred reply, if it is just one per command, or inside a loop (like COMMAND command and CLUSTER SLOTS used to do, see #10056, #7123).
if it's just one per command, same as COMMAND command still does, it should not have a high impact.

> the change is not as meaningful on the COMMAND numbers shared (that I can indeed reproduce )

did you reproduce this on the latest? or the old copy of this branch and it's merge-base?

> Apart from 4 unstable benchmarks ( let's discard them if you agree there is no change that can impact it ), we have 9 benchmarks with no change.

so we now conclude that this PR doesn't do any damage in the common use cases? (even over real network).

just to point out again, this PR does get some 300% performance improvement for the old code of COMMAND command, see:
https://github.com/redis/redis/pull/9934#issuecomment-1034129564
and about 10% performance improvement with TLS, see:
https://github.com/redis/redis/pull/9934#issuecomment-1040043226

@yossigo please ack.

"
1047593997,9934,panjf2000,2022-02-22T09:29:11Z,Any updates on this?
1047600393,9934,oranagra,2022-02-22T09:36:18Z,"we discussed and approved this today in a core-team meeting.
maybe the the last possible pending concern is to realize why it didn't have the expected impact on #10310.
but maybe none of that is gonna affect this PR.

So anyone has any other concerns or something i forgot before i merge it?"
1047606503,9934,panjf2000,2022-02-22T09:43:17Z,"I just took a quick look at #10310, and I wonder if the root cause for the regression of v6.2.6 compared to v5.0.2 has been found?
@oranagra "
1047611962,9934,oranagra,2022-02-22T09:49:30Z,"the root cause is that in 6.2.x we started using deferred replies https://github.com/redis/redis/pull/7844.
Filipe is suppose to try to ""revert"" this on unstable and see if this explains the whole regression, or just part of it (which would then explain why writev didn't completely undo the regression)."
1047619015,9934,panjf2000,2022-02-22T09:57:09Z,"Is there any chance that those deferred replies are way larger than IOV_MAX=1024, which might explain why `writev()` only alleviates rather than completely cures the regression?"
1047643692,9934,oranagra,2022-02-22T10:22:59Z,"Filipe reproduced two cases (i assume the sizes of the elements in the zset are small):
[One](https://github.com/redis/redis/issues/10310#issuecomment-1046698483) without pipeline, in which case writev didn't help. not sure if this regression is from the fact there are two writes instead of 1, or maybe another change.
[Second](https://github.com/redis/redis/issues/10310#issuecomment-1046768789) with pipeline of 16, so there should be 32 nodes in the reply list (i assume they're all small). and writev improved the case, but not nearly to what it was in the past.

I suppose we can merge this PR, and keep discussing this in the other issue.
either we'll realize there's an additional regression, or we'll figure out why writev can't solve it entirely.
either way, i don't see how the conclusion could affect this PR."
1047705557,9934,oranagra,2022-02-22T11:35:08Z,"@panjf2000 i edited the top comment (to be used for the squash-merge commit comment). let me know, or just fix if you see any issues or missing info."
1047711460,9934,panjf2000,2022-02-22T11:42:49Z,"> @panjf2000 i edited the top comment (to be used for the squash-merge commit comment). let me know, or just fix if you see any issues or missing info.

It looks good to me.
@oranagra "
874740404,9202,MeirShpilraien,2021-07-06T13:03:17Z,"Thanks @zuiderkwast 
> Documentation in Redis is in the .c files, not in the .h files.

Sure, will change that

> Hiredis already contains a reply parser. It's a simple protocol, so maybe there is not problem to have one more parser, but it can be considered if we want to improve/refactor/export the one in hiredis instead.

The hiredis parser checks for protocol errors, here the parser assumes there are no protocol errors (because the reply was generated by Redis itself) and it can be faster. and in general, this PR is not adding one more parser, its actually removes one extra parser :) "
875635240,9202,oranagra,2021-07-07T14:07:21Z,"@MeirShpilraien please improve the top comment to include the intensive for doing this PR (share code between Lua, Modules, and the future Functions).
and if you can, elaborate a bit more on what you did in this PR, i.e. the exact roles of the new units, and any other change you did, so that it'll be easier to review.

p.s. you say ""call_reply wraps the RedisModuleCallReply object"" but isn't it the other way around? or actually that it replaces (renames) it?"
878807802,9202,MeirShpilraien,2021-07-13T06:11:20Z,"@oranagra I realize we have another issue with `RM_ReplyWithCallReply`. After this PR call reply might contain resp3 formats and if you use it with `RM_ReplyWithCallReply` we might send resp3 formats to resp2 clients. I suggest taking the same approach as suggested here https://github.com/redis/redis/pull/8521#discussion_r667915746 and return error if the reply contains resp3 format and try to reply it to resp 2 client, WDYT?"
878936618,9202,oranagra,2021-07-13T09:38:11Z,"@MeirShpilraien we can do that (error), but i don't think that's sufficient.
maybe we also need to automatically fallback to resp2 in some way (i.e. ignore the `3` char).
this is really a not good idea since in some cases the module intends to parse the response, and in some it intends to forward it to the user.
so maybe we need two modes (two chars), and soft `3` and a bold `3`.
or have `3` explicitly refer to RESP3, and another char (e.g. `0`) refer to an automatic mode that uses the real client's preference."
878945275,9202,MeirShpilraien,2021-07-13T09:51:08Z,"@oranagra but then if you use `0` you do not know what to expect.

Why not just give information on the current protocol used by the client (using context flags maybe)? this way the module can choose what to do and he has all the information he needs. If he wants to forward the reply he can check the protocol used by the client and use the same on `RM_Call`. And regardless I think we also need the error in case of a mistake.
 
 WDYT?"
878952417,9202,oranagra,2021-07-13T10:01:33Z,"i agree the error is needed anyway, and i agree that we need to expose a context flag.
but i think that in cases where all the module wishes to do is forward the request, forcing it to write code to check the context flags and decide whether or not it should add `3`, is awkward, so i thought of suggesting some auto mode."
878964359,9202,MeirShpilraien,2021-07-13T10:17:41Z,"OK, I can add the auto mode, should I add the new context flag on this PR?"
878998541,9202,MeirShpilraien,2021-07-13T11:13:05Z,"> btw, did you add some loop in the test that runs the resp3 module tests in resp2 mode?

I commented here: https://github.com/redis/redis/pull/9202#discussion_r668199551
Let me know what you think"
879155274,9202,oranagra,2021-07-13T14:50:16Z,"@redis/core-team please approve this PR.
it started as a refactoring preparation for Functions, but it ended up also improving a few things along the way.
top comment has all the details."
885090670,9202,oranagra,2021-07-22T17:35:39Z,"@redis/core-team please approve the new API (till now we were focused on the refactoring and doc/comments).
Please see the top comment for the full list.
it's basically 3 things:
1. new module APIs for parsing RESP3 responses that come back from RM_Calll
2. new arguments for RM_Call: `3` for RESP3, and `0` for auto
3. new flag for RM_GetContextFlags for detecting the protocol of the current client
4. new lua capabilities for parsing RESP3 features unused till now (bignum, verbatim, attributes)

some of these will be needed if RM_Call and Lua will call module functions that use RESP3 features that redis doesn't yet yet (attributes, bignum), i.e. #8521"
998869058,9974,oranagra,2021-12-21T15:25:31Z,"> LPUSH returns metadata when it returns the length of the list.
SCARD processes the data to return the cardinality.

I'm a bit uncomfortable with calling list length ""metadata"" in LPUSH, and the cardinality in SCARD ""data""
in that sense is LLEN data or metadata?

I think we should clearly state where INCR, and SETNX falls into, and maybe even a hypothetical case where INCR would have returned +OK (i.e the new data is a derivative of the existing one).
in that case (ignoring the return value), if we mark INCR as R+W, would that mean that APPEND is R+W too?

> RENAME: currently marks both keys as write, now the first key will also be a read.

i think that in the past, the `read` / `write` command flags where about getting data out of redis (i.e. that's why RENAME is `read`), but now for ACL and keyspace restrictions, we must consider the source key as a read one.
so the key-spec flags can say the first key is has `read` flag, even if the command flag doesn't have the `read` flag.

> LPOP: Currently marks the key as only write. (This is also inconsistent with LMOVE, which marks the popped key as read and write)

I think this is just a bug (missing read flag).
P.s. in the distinct the `r` meant ""will never modify the key space"" (so SELECT had it too), and in v3.2 i pushed to clean this and change the meaning of the flag.
But it still refers to keyspace, i.e. dbsize is read.

> The set/get problem

That's an interesting downside (maybe not too late to change).
another bad option is to add a ""write-only"" variant, like we have for BITFIELD_RO.
I'm not sure what do do. i'm uncomfortable with changing it now, but also think it's a serious limitation.
maybe the way out is to perform some ACL permission checks inside the command proc itself?
i.e. it'll not get blocked by processCommand, and instead fail internally?

> Modules and key based permissions

Regarding ACLCheckKey, let's make that change ASAP (API not released yet)"
1000512833,9974,oranagra,2021-12-23T20:33:50Z,"I'm sorry. I was so happy that I finally finished reviewing the code, that I posted my comments without a summary. 
all in all it looks great. 
I think most of the comments are minor (or a few major ones that are easy to resolve). 
I have a feeling that the tests should be enhanced, maybe test some odd commands and corner cases. As well as ACL SAVE and LOAD"
1000514768,9974,madolson,2021-12-23T20:39:51Z,"@oranagra Thanks for looking through, it's not as polished as I normally like to put up PRs but knew I was going to be busy this week, so probably worth getting feedback. Tests was one thing I mentioned needs more work, so I'll definitely revamp them."
1003144015,9974,itamarhaber,2021-12-30T18:49:01Z,"As a chore/aside to this, would it make/be against common sense to add a capability to introspect the effective ACL permissions for a given user?

Ideally, this would be supported by an interface that, for a given user, would:

1. Report the verbatim allowed commands (and subcommand)
2. Report the allowed key patterns
3. Optionally, allow a ""dry run"" to test the possible ability to execute a specific command in terms the context's permissions alone

Mock API:

```
redis> HELLO 3 AUTH foo bar
OK
redis> COMMAND ALLOWED
1# ""ping"" => null
2# ""echo"" => null
3# ""client|myid"" => null
4# ""get"" =>
   1) ""foo:*""
redis> COMMAND ALLOWED default
(error) NOPERM ...
redis> AUTH default """"
OK
redis> COMMAND ALLOWED foo
....

# Optional part
redis> AUTH foo bar
OK
redis> COMMAND DRYRUN get bar:123
(error) NOPERM ...
redis> COMMAND DRYRUN get foo:123
OK
```"
1003668517,9974,madolson,2022-01-02T05:56:26Z,"@itamarhaber I have a dry-run command actually implemented locally for testing, I've been flip flopping if it's worth committing. My variant has the form:
```
ACL DRYRUN USER <command> <args>
```

Will add it as a follow-up. Not sure about the super detailed listing of commands though."
1011844584,9974,madolson,2022-01-13T06:48:16Z,"NOTE: I didn't fix all the db.c stuff, I still need to spend some more time to fully understand the right way to fetch the keyspecs. However, I don't think the prevents reviewing the rest of the decisions."
1012148711,9974,oranagra,2022-01-13T13:43:25Z,"@madolson i'd rather avoid rebase and force-push (please just merge from unstable), it makes it hard to review changes.
if we have to rebase (like in the original functions PR that was merged without squash), we must use a separate force-push for the rebase from any other commit pushes.

anyway, for now, i'll assume the fist commit of this PR is identical to what i already reviewed. and i should review the two after it. right?"
1012487709,9974,madolson,2022-01-13T20:23:28Z,"@oranagra sorry about the rebase, I wanted to pull in the changes from the pubsub v2 PR which included shard channels. There was quite a bit of changes all over the place, so I just decided to rebase. I normally agree, and wait until the end to rebase."
1013843571,9974,oranagra,2022-01-16T09:50:36Z,"something i'm missing or forgot.
the spec in https://github.com/redis/redis/issues/7291#issuecomment-966684673 mentioned `%(R|W|D)~<pattern>`
and the top comment here says `read + write + delete`, but later only specifies `%(R|W)~<pattern>`
i do remember that we decided to treat delete as write, but i can't find any written reference for that (and also the top comment may be outdated)"
1014080990,9974,madolson,2022-01-17T02:25:26Z,"@oranagra In one of the meetings we discussed that anything more fine grain than read and modify was likely too hard to conceptualize. So I dropped it from the implementation here. I can post that comment back on the main CR.

> sorry.. i didn't understand your response.
> 1. isn't %R~* and read permissions on ""*"" the same?
> 2. can people use the %R~* notation on the root selector?
> 3. are you saying that in the root segment, you rather fold %RW~ into just ~ (for backwards compatibility), and the non-root selectors you rather be explicit (i.e. user who provided ~ will get back %RW~)?

I was trying to convey that if the root segment had two key permissions, `%R~*` (Read permission on all keys) and `~%R~*` (All permissions on `%R~*`) they would show up as the same string, namely ""%R~*"".  So I was trying to differentiate the two by only showing the full key permissions in the selector field."
1014263219,9974,oranagra,2022-01-17T08:34:52Z,"@madolson when you say
>  `~%R~*` (All permissions on `%R~*`)

do you mean `%R~*` is key name pattern (keys starting with ""%R~"" in their name)?
do you mean they'll show the same because of the de-duplication? i.e. one of them is a subset of the other? (i don't think that's the case).

or they'll show the same because all permission would show as `~<pattern>` rather than `%RW~<pattern>`?
in which case i don't think that will happen since one will be shown as `%R~*`  (read on all keys), and the other as either `~%R~*` (simple / old notation) or `%RW~%R~*` (move verbose notation, but still not the one the `verbose` arg in your code will use)"
1015014056,9974,madolson,2022-01-18T02:22:47Z,"@oranagra I didn't follow your question, so I'll try to take a step back and explain again. The 6.2 implementation of `ACL GETUSER` returns just the string value of the pattern. If the ACL user looked like, `user foo ~foo*`, the keys field of the getuser response would just have ""foo"".

Now that we have key based permissions, we want to make the distinction between a keypattern that is read-write and a keypattern that is read only or write only.  We could prepend the permission information onto the getuser response, in our example transforming it from `foo` to either `~foo` or `%RW~foo`. This seemed like a backwards breaking change to me, so I wanted to avoid it.

We could also prepend the permission information on just the key patterns that aren't RW, so our example would stay as ""foo"", and if we added a key pattern using the new permissions, say `%R~bar`, getuser would return a list of two values: [`%R~bar`, `foo`]. The problem I was trying to describe was the case of if instead of ""foo"" the original key pattern was the literal string ""%R~bar"". Then the getuser response would have returned [`%R~bar`, `%R~bar`], which is describing two separate permissions.

My proposal, which is still implemented, was to show the literal string patterns in the top level of get the GETKEYS response. In the individual selectors, since this is net new, it returns the string patterns prepended with their permissions. If we don't think it's a backwards breaking change to prepend the permission information, that is going to be a simpler change.  "
1015318619,9974,oranagra,2022-01-18T11:24:15Z,"ohh, now i understand what i was missing. for some reason i thought that `ACL GETUSER` returns a DSL like `ACL SETUSER`, but in fact it returns a RESP response with an array of literal key name patterns the user can access (without the `~` DSL prefix).
so anything we do there will be either a breaking change (adding `~` or `%` prefix), or be disambiguate since both `~` and `%` could be part of a key name / pattern.

I think we can't afford the disambiguity, and unless i'm still missing something, it exists with your non-verbose.

I think we should do one of the following:
1. remove this `keys` field from the map, and introduce another one like `key_rules` which is always verbose (always emits the `~` prefix and possibly the `RW` flags (can be omitted if the key has full access)
2.  keep the `keys` map field, but let it include only keys that are fully accessible, and in addition to it add a `keys_r` and `keys_w` map fields with list of keys that are either read only or write only.
3. keep the `keys` map field and let it include any key that is in some way accessible (ignoring the R and W flags). and add 3 new map fields: `keys_r`, `keys_w` and `keys_rw`

i.e. the reason i prefer 2 or 3 is because the `keys` field, and this entire ACL GETUSER response is designed to be human friendly (not using the ACL DSL).
the difference between 2 and 3 is that 3 is in some way a bit more backwards compatible (clients will still get a list of all the keys that could be accessible).
but on the other hand, if the client isn't aware of the new `selectors` map field, it'll miss some granted accesses anyway, right? so in that sense, maybe there's no need to keep the old `keys` field at all (unless maybe a case where both the new selectors feature and the new RW key access features are in use)"
1015660610,9974,madolson,2022-01-18T17:46:07Z,"@oranagra I agree with most of what you said about the GETUSER response, except the GETUSER response already includes `ACL DSL` for the commands response. 

```
> acl getuser default
 1) ""flags""
 2) 1) ""on""
    2) ""nopass""
 3) ""passwords""
 4) (empty array)
 5) ""commands""
 6) ""+@all"" <----- DSL
 7) ""keys""
 8) 1) ""*""
 9) ""channels""
10) 1) ""*""
```

Which is why I thought moving DSL into the getuser response was reasonable. If the concern is about human readability, I'd like to propose the following:
1. The Key field becomes all accessible keys, the same as your option 3.
2. We add a new key-permissions field, which is a map of the different permissions supported by the user along with the list of keys that have that permission. E.G.

```
 7) ""keys""
 8) 1) ""foo""
    2) ""bar""
 9) ""key-permissions""
 10) 1) ""read+write""
     2) 1) ""foo""
     3) ""read""
     4) 1) ""bar""
```"
1015672723,9974,oranagra,2022-01-18T18:01:03Z,"your last suggestion is very similar to my 3rd one, considering the situation i think it's a good compromise.
in any case, i think the purpose of the`keys` map field was to be human readable, so i feel that adding DSL flags into is is wrong. "
1016069265,9974,madolson,2022-01-19T04:25:07Z,"I ran the test verifying the keys are the same between the previous implementation and the new one, see https://github.com/madolson/redis/commit/6d59ca3c1a36800b57c742c2383bc40f09493928. I found a couple of bugs which have been fixed, including one in your update which I submitted here: https://github.com/redis/redis/pull/10137, so I feel good it will have caught most things.

I think all of my planned items are done now, PTAL and let's see if we can close this before the end of the week. :D"
1017096736,9974,madolson,2022-01-20T03:58:54Z,"@redis/core-team I ended up with the following type of output for ACLs:
```
127.0.0.1:6379> acl getuser foo
 1) ""flags""
 2) 1) ""off""
 3) ""passwords""
 4) (empty array)
 5) ""commands""
 6) ""-@all +get""
 7) ""keys""
 8) ""%R~foo1 %W~bar2 ~whatever""
 9) ""channels""
10) ""&channel1""
11) ""selectors""
12) 1) 1) ""commands""
       2) ""-@all +get""
       3) ""keys""
       4) ""%R~selector1""
       5) ""channels""
       6) ""&*""
    2) 1) ""commands""
       2) ""-@all""
       3) ""keys""
       4) """"
       5) ""channels""
       6) ""&*""
```
The breaking change is that channels and keys are now shown in DSL instead of just as their patterns as flat string instead of as an array. I think it's useful to also update the channels so that it's all consistent."
1017561277,9974,oranagra,2022-01-20T14:25:46Z,"@madolson i've reviewed the whole thing again top to bottom, found a few bugs and fixed them, please CR the last commit.

i've also re-run the tests with the code that asserts both key extraction functions give the same results (put the assert in processCommand, since putting it in `call` will miss many of the test suite calls)

I have a feeling that the `Test various odd commands for key permissions` could be extended for more edge cases (we don't have anything that verifies the keyspecs and flags), but i don't wanna dive into it now.

I think this is ready for merge."
1017754798,9974,madolson,2022-01-20T17:38:50Z,"@oranagra Just wanted to clarify the shard-channel keyspec stuff. In an earlier revision there was code to support both channels and sharded-channels in the keyspecs, but there wasn't clarity of it we wanted to do that, so I removed support for both. Right now modules have to do their own manual ACL checks for channels and there is an item in the follow up list to think this behavior through more.

The issues that I saw with modules support:
1. Should modules be able to also exempt themselves from ACL checks, like UNSUBSCRIBE and SUNSUBSCRIBE does.
2. Should we be able to differentiate channels and shard channels in key specs so modules can define either. Shard channels route clients while regular channels can be sent to any node (for stuff like keyspec notifications)."
1017769385,9974,oranagra,2022-01-20T17:56:23Z,"Let's discuss modules later. 
What I didn't like (or actually didn't understand) in your previous attempt, was that you added key-spec flags for the old pubsub, but these commands don't declare key-specs (and I think they shouldn't, since it'll confuse cluster clients). 
So it's only ACL that has that requirement, and I think ACL should be the one to handle that (not key-specs) 
I don't like to see key-spec flags to suppose non-key-spec usages. "
1107745718,10273,oranagra,2022-04-24T06:22:46Z,"@redis/core-team please have a look and comment.
The design is now complete, and the ""coding"" for now covers only part of the command set (the ones that are important in order to judge this feature and run into all the complicated cases).
i'd like to get a conceptual approval for that, and then merge it into unstable after 7.0 is stable, and before starting to merge any command 7.2 related changes into unstable."
1186572454,10273,valkum,2022-07-17T17:00:19Z,"Just my 2 cents from a first glance. Overall I think this will help to generate type safe interfaces for client libraries.
The only things I would nag about are:

 #### Variants
> It is ok that some commands' reply structure depends on the arguments and it's the caller's responsibility to know which is the relevant one. this comes after looking at other request-reply systems like OpenAPI, where the reply schema can also be oneOf and the caller is responsible to know which schema is the relevant one.

To get a clean high-level API I would generate one method for each command (maybe this would be considered a medium-level API)) and if a subset of arguments causes a specific reply schema to be used, I would create a method for that too (this would be the high-level API).
If I am only interested in one kind of reply schema variant I most certainly will always pass Option::None or null to the arguments that are not needed for this reply schema variant. So for a very ergonomic high level interface a single method for each of those argument => response variant combinations would be desired.
The questions is how one could resemble these dependencies between arguments and reply schema variants.

Getting to my favorite example: CLIENT KILL
Here we have a clear mapping of old-format vs new-format (now a top-level oneof argument type) to one of the two reply schema variants.
Basically we have two options: annotate the arguments or annotate the reply schema top level oneOf variants.

While annotating the argument for the CLIENT KILL command seems to be the easier solution as you can simply say e.g. ""yields: variant a of response_schema top level oneOf"", this could get messy if the dependency is not as clear as with CLIENT KILL. E.g. there is no top level oneof argument type.

Thus, I guess augmenting the reply schema would be the better (more general) solution as you can directly annotate which arguments in which combination yield the specific variant. 
There are a couple of open questions though. How would you address the specific arguments (they currently do not have unique identifier). What is the most powerful argument dependency for a response variant (something that is not a simple ""if argument x is passed, this is the result""). 

I mean in the end we (as in client library authors) can aggregate these information ourselves and feed that into the code generator but I think having this upstream would be a benefit to all client lib generators.
Maybe, to not block this PR, a new issue to discuss different solutions for this problem is desired?

 #### Unique ids
   
Another thing I would like to see would be `$id`s for the different responses. This could help with associating generated types to resources in the reply schema (Putting that in the docs of that type and maybe use this to derive a name for the data type)"
1189896926,10273,oranagra,2022-07-20T06:51:39Z,"I agree we better solve this mapping of arguments to reply types.
i think it could be nice to give a unique name (ID) to both args and reply types, but i fear that, specifically for the args it could be a lot of work, so maybe it should be optional and we'l add it only to the ones needed.
The IDs for the reply type could just be indexes (like we have for key-specs), since they're not nested.
What worries me is that it's hard to add a reference on each and every arg or combination as to what reply type is generates. maybe we should have some way to define a default and a list of exceptions?"
1346014720,10273,guybe7,2022-12-12T07:31:58Z,"@valkum @leibale in order to solve the problem of strongly-typed languages and anyOf/oneOf schema I propose adding a new `condition` field to any sub-schema inside oneOf/anyOf.
this new field will use C-style syntax with regard to the existence of certain arguments that cause this specific reply

example for `SET`:
```
""reply_schema"": {
      ""oneOf"":[
          {
              ""description"": ""Operation was aborted (conflict with one of the `XX`/`NX` options) or `GET` was given and the key didn't exist"",
              ""condition"": ""arguments.get||arguments.condition"",
              ""type"": ""null""
          },
          {
              ""description"": ""`GET` not given: The key was set."",
              ""condition"": ""!arguments.get"",
              ""const"": ""OK""
          },
          {
              ""description"": ""`GET` given: The previous value of the key"",
              ""condition"": ""arguments.get"",
              ""type"": ""string""
          }
      ]
  }
  ```
  
  WDYT?"
1370973068,10273,guybe7,2023-01-04T14:08:05Z,@valkum @leibale ping^
1370993256,10273,valkum,2023-01-04T14:23:46Z,"Hey, thanks for the reminder and your work on this matter. 
Nothing immediate comes to mind problem-wise regarding this solution. I have some remarks, though.

I can see that the first oneOf variant is the error case. It might be hard to generate good types for languages which commonly use error monads.
I would appreciate it, if we could tag the error variant in some way. This should also reduce the needed amount of conditions for the error variant in some cases.
On the other side, it could be hard if there are multiple error cases with different reply schemas, but they would have problems with your solution right now as well. I am uncertain if there are currently such cases in redis."
1371144589,10273,leibale,2023-01-04T16:25:44Z,"Parsing `condition` will make generating a client a lot harder, what do you think about something like:

```json
{
  ""reply_schema"": {
    ""oneOf"": [{
      ""description"": ""Operation was aborted (conflict with one of the `XX`/`NX` options) or `GET` was given and the key didn't exist"",
      ""if"": {
        ""oneOf"": [{
          ""properties"": {
            ""get"": {
              ""required"": true
            }
          }
        }, {
          ""properties"": {
            ""condition"": {
              ""required"": true
            }
          }
        }]
      },
      ""then"": {
        ""type"": ""null""
      }
    }, {
      ""if"": {
        ""properties"": {
          ""get"": {
            ""required"": true
          }
        }
      },
      ""then"": {
        ""const"": ""OK""
      },
      ""else"": {
        ""type"": ""string""
      }
    }]
  }
}
```"
1371211255,10273,valkum,2023-01-04T17:24:33Z,"> Parsing `condition` will make generating a client a lot harder, what do you think about something like:
> 


I feel this is a bit too verbose and has problems with the descriptions. If we want to remove the need to parse the condition from a different format I would change your example to something like this:

```
{
  ""reply_schema"": {
    ""oneOf"":[
        {
            ""description"": ""Operation was aborted (conflict with one of the `XX`/`NX` options) or `GET` was given and the key didn't exist"",
            ""condition"": {""or"": [{""arguments"": {""get"": {""required"": true}}},{""arguments"": {""condition"": {""required"": true}}}]},
            ""type"": ""null""
        },
        {
            ""description"": ""`GET` not given: The key was set."",
            ""condition"": {""arguments"": {""get"": {""required"": false}}},
            ""const"": ""OK""
        },
        {
            ""description"": ""`GET` given: The previous value of the key"",
            ""condition"": {""arguments"": {""get"": {""required"": true}}},
            ""type"": ""string""
        }
    ]
  }
}
```
 with condition roughly in the following format:
 ```
{
    ""$schema"": ""https://json-schema.org/draft/2020-12/schema"",
    ""$ref"": ""#/definitions/Condition"",    
    ""definitions"": {
        ""Condition"": {
            ""type"": ""object"",
            ""additionalProperties"": false,
            ""properties"": {
                ""argument"": {
                    ""$ref"": ""#/definitions/Argument""
                },
                ""or"": {
                    ""$ref"": ""#/definitions/Or""
                },
                ""and"": {
                    ""$ref"": ""#/definitions/And""
                }
            }
        },
        ""Or"": {
            ""type"": ""array"",
            ""items"": {""$ref"": ""#/definitions/Condition""},
            ""title"": ""Or""
        },
        ""And"": {
            ""type"": ""array"",
            ""items"": {""$ref"": ""#/definitions/Condition""},
            ""title"": ""Or""
        },
        ""Field"": {""type"": ""object"",
              ""properties"": {
                ""required"": {
                  ""type"": ""boolean""
                }
            }
        },
        ""Argument"": {
            ""type"": ""object"",
            ""additionalProperties"" : {
                ""$ref"": ""#/definitions/Get""
            },
            ""title"": ""Argument""
        }
    }
}
```"
1371249460,10273,leibale,2023-01-04T18:01:55Z,@valkum I tried to mimic the original [JSON schema if-then-else](https://json-schema.org/understanding-json-schema/reference/conditionals.html).. I think we should try to stay as close as possible to the spec.
1430192931,10273,oranagra,2023-02-14T18:31:02Z,"Full CI
https://github.com/redis/redis/actions/runs/4176923677"
1455713531,10273,oranagra,2023-03-06T08:43:12Z,"@redis/core-team FYI, as discussed, we're going to merge this to unstable soon in order to proceed perfecting the schemas gradually in unstable (could take a long time, require collaboration, which will be hard to do in a side branch).
for now this should have no visible interfaces, and it doesn't add any refactory (just some new code that we can delete if we someday regret it)."
1460695875,10273,guybe7,2023-03-08T18:51:24Z,daily: https://github.com/guybe7/redis/actions/runs/4367366638/jobs/7638585360
1460782643,10273,oranagra,2023-03-08T20:02:52Z,"i see we have an issue with 32bit build and one of the new GEO tests.
the reclaim and freebsd failures are probably unrelated.
waiting for valgrind but i don't expect anything to fail there."
1462003838,10273,guybe7,2023-03-09T12:48:42Z,with fix to GEO on 32bit: https://github.com/guybe7/redis/actions/runs/4374580524
1462219668,10273,oranagra,2023-03-09T15:09:08Z,"@redis/core-team FYI, i would like to merge this one tomorrow.. let me know if you have any concerns."
1462868398,10273,madolson,2023-03-09T21:45:35Z,"I have no concerns, the actual change to the core seems small."
1463381272,10273,soloestoy,2023-03-10T07:22:29Z,"didn't CR, only a question about the test part: does it validate the key_specs and other sections?"
1463585389,10273,guybe7,2023-03-10T10:19:24Z,"@soloestoy no, it only validates the reply schema
validating the reply schemas is kinda easy, because the SOT is the existing Redis replies, so we just have to adjust the schemas according to them (the replies themselves don't rely on the reply-schema)
validating the key specs is a bit harder because we don't have an independent SOT (COMMAND GETKEYS relies on the key-specs)"
1463729776,10273,oranagra,2023-03-10T12:22:28Z,We can log key lookups of each command and then try to match these. But it's a topic for a different campaign..
1907552539,10273,enjoy-binbin,2024-01-24T07:31:23Z,do we need to add reply_schema for command.json? (the same reply as COMMAND INFO)
1907588314,10273,oranagra,2024-01-24T07:58:54Z,"i guess that technically we do. but:
1. it's a huge pile of lines to clone
2. i hate that COMMAND command (the fact it has sub-commands but also works as a plain command), and wish we could someday deprecate it.
3. maybe the fact that we don't need it (yet, for the tests), we can hold this back for now?

@guybe7 maybe you remember some other reason why we didn't do that?"
903669447,9406,huangzhw,2021-08-23T11:09:59Z,"I think about three ways:
* We add `dictFind` that's only used if there's some module subscribed to that event
* `unlink2` is used for module types. Whether we can use it for other native types?
* Add `notify_callback2` take addition value for module."
903723138,9406,oranagra,2021-08-23T12:33:56Z,"the last two suggestions are only valid if we're only talking about module values (specifically the module that created that key).
i think the request was about keyspace notification since people may want to use it on other types of keys."
904230785,9406,huangzhw,2021-08-24T00:40:35Z,"When this notification triggered by lpop hdel, etc, the object is zero length. I suspect whether this is useful.
If notification callback can access the object to be deleted, it can delete it or do something else. I think it's dangerous.
If do it, I think we can add two function
```
dictEntry *dictFindWithPlink(dict *d, const void *key, dictEntry ***plink)
// We can't modify db between this
void dictUnlinkWithPlink(dict *d,  dictEntry *entry, dictEntry **plink)
```"
904337520,9406,oranagra,2021-08-24T05:38:29Z,"interesting idea. so it's like when we broke dictDelete into two: dictUnlink and dictFreeUnlinkedEntry.
but this time we break it in a different place (before the unlinking).

you're right, that this API is becoming more dangerous the more we think of it:
1. as you said, the key the module may access may be in an odd state (like empty list)
2. it can even be in a state that can lead to a crash, maybe `o->ptr` was already released, but not set to NULL.
3. obviously the module can't modify the key
4. the module can't even do any other database modifications since that can get `plink` out of sync.
5. even if we only expect the module to do read-only database lookup, we have to disable the incremental dict rehashing in that sensitive time (otherwise dictFind can mess things up).

* 1, 3 and 4 don't worry me too much, we'll document these and if the module violates that, that's they're mess to handle.
* 5 is messy but actually easily solvable.
* currently 2 worries me the most.

@guybe7 @MeirShpilraien any advise?"
904390849,9406,guybe7,2021-08-24T07:23:40Z,@oranagra can you give an example of (2)?
904403860,9406,oranagra,2021-08-24T07:43:52Z,"no, i can't find any.. maybe i'm wrong and it doesn't exist..
it'll eventually do `decrRefcount`, which attempts to release `o->ptr` again, so for sure what i said about freeing it and not setting it to NULL doesn't exist, and i can't currently find anyone that frees and sets it to NULL either.

btw, another interesting case is the RENAME command. it does call dbDelete, but it adds the same object (and increments the refcount) before the dbDelete call.

i suppose that in this case there's a logical deletion and addition, but we're not gonna send the removed notification in that case, and we assume the module will explicitly handle that. "
904474408,9406,guybe7,2021-08-24T09:20:50Z,"@oranagra why aren't we gonna send the remove notification in that case? even though we don't actually release the memory, `moduleNotifyKeyUnlink` is still called (both for the src and the dst, if exists)
or maybe I'm missing something?"
904483037,9406,oranagra,2021-08-24T09:33:34Z,@guybe7 i don't see any call to moduleNotifyKeyUnlink in RENAME
904486653,9406,guybe7,2021-08-24T09:39:00Z,@oranagra it's inside `dbDeleteSync` called by `dbDelete` (which is called up to two times in renameCommand)
904496903,9406,oranagra,2021-08-24T09:54:16Z,"@guybe7 the first call is the overwrite, so that's a plain deletion and not part of this discussion.
the second deletion will indeed send that notification (don't recall what was going though my head earlier).

so anyway, i guess we can consider that second notification a problem, and may wanna hide it.
on one hand, logically we're deleting one key and adding another, so the removed notification is due.
but on the other hand, that same object is already referenced in the dictionary elsewhere.

so either we make sure to disable the notification in that case and let the module handle RENAME notification differently,
or maybe we wanna incrRefcount of the robj, then do the deletion, and only last add it to the new key.
whatever we do, we may need to handle MOVE too."
907626587,9406,huangzhw,2021-08-28T13:29:27Z,"The function name maybe should change.
I still wonder whether this is OK. It is a little ugly and potential dangerous."
957827295,9406,oranagra,2021-11-02T15:29:43Z,@huangzhw @daniel-house can we proceed with this? what's the status here?
958474760,9406,huangzhw,2021-11-03T00:04:37Z,I'm done with it.
1217768384,9406,oranagra,2022-08-17T09:35:36Z,"@huangzhw it didn't make the cut for 7.0, but i'm hoping to put it in 7.2.
if you can, please refresh the PR and i'll try to promote it."
1217836614,9406,huangzhw,2022-08-17T10:35:51Z,@oranagra Let me take some time to recall it.
1221684054,9406,huangzhw,2022-08-22T01:27:48Z,"@oranagra I found a problem.
In module if we call `RM_StringDMA`,  if the key is a string and not raw, `dbUnshareStringValue` will be called. The `val` will be freed. But we still access the `val` in `moduleNotifyKeyUnlink`.

Same problem happens in `SET` command.
`setCommand -> setGenericCommand -> notifyKeyspaceEvent -> moduleNotifyKeyspaceEvent -> RM_StringDMA`.
The value will be freed, but we still use it in `rewriteClientCommandVector`.
"
1225293625,9406,guybe7,2022-08-24T07:12:31Z,"I'd like to chime in: I'm against adding a special notification for modules. I think ""loaded"" was a mistake and we should treat these things as module events."
1225302767,9406,oranagra,2022-08-24T07:22:25Z,"@guybe7 i think you're right about this one (being an event or some other hook, and not a key-space notification).
i'm not sure it applies for `loaded` as well, but that's water under the bridge."
1225303224,9406,oranagra,2022-08-24T07:22:54Z,"@huangzhw sorry for not responding to your comment, i need to dive it (on my todo list)"
1229409811,9406,oranagra,2022-08-28T08:51:36Z,"@huangzhw sorry for the delay, and thank you for exposing this issue.
i'm guessing that it we should be rejecting RM_StringDMA from within keyspace notifications, or maybe specifically if the string is now RAW.
the main reason to use DMA if for efficiency for huge strings, so other than the inconvenience for the module to work around it (resort to other APIs to read from the string without a DMA), there's no real performance penalty.

p.s. regardless, i've also recently discussed with @MeirShpilraien about discouraging or even disallowing writes from within keyspace notifications, so maybe we need some global flag to know that we're inside a KSN?

@MeirShpilraien please see https://github.com/redis/redis/pull/9406#issuecomment-1221684054 and let us know if you can think of another solution besides rejecting the RM_StringDMA call."
1229415152,9406,MeirShpilraien,2022-08-28T09:22:36Z,Maybe we should just document that `RM_StringDMA` should not be used inside key space notification (just like we plane to do with write commands) because it has a potential of changing the data?
1229423036,9406,oranagra,2022-08-28T10:00:48Z,"in some cases (reading huge strings), there's no reason not to use it.
i'd rather just document (and enforce) that it can fail and the module should handle it."
1233905416,9406,guybe7,2022-09-01T08:07:28Z,@oranagra are we going with the event/hook approach?
1233931564,9406,oranagra,2022-09-01T08:28:41Z,"yes, let's do that.

@huangzhw please look into defining an interface for that that's separate from the KSN hooks.

also, please solve the problem with StringDMA, by returning an error in that case, and fixing the test module to handle it."
1234137836,9406,huangzhw,2022-09-01T11:19:22Z,"> yes, let's do that.
> 
> @huangzhw please look into defining an interface for that that's separate from the KSN hooks.
> 
> also, please solve the problem with StringDMA, by returning an error in that case, and fixing the test module to handle it.

@oranagra 

You mean use `moduleFireServerEvent` to support this notification?

Fixing StringDMA is also break change.

"
1234178680,9406,oranagra,2022-09-01T11:58:14Z,"well, i was thinking of an new completely separate callback, but come to think of it the event and sub-event mechanism can maybe fit here nicely.
i.e. we add REDISMODULE_EVENT_KEY and a REDISMODULE_SUBEVENT_KEY_DELETED (and maybe EXPIRED, and EVICTED too).
in the future we can add other key-related events...
@guybe7 @MeirShpilraien WDYT?

regarding StringDMA want to fix that only for the cases where it's broken (corrupting memory), so it'll be a fix (for something that was completely broken before), not a breaking change.
e.g. in some way reject RM_StringDMA when used inside KSN or event hooks, on a string that's not raw encoded."
1247598935,9406,MeirShpilraien,2022-09-15T05:26:09Z,"@oranagra I like the idea of using the events mechanism, I do not see a need to add a new mechanism."
1247768075,9406,oranagra,2022-09-15T08:32:58Z,@huangzhw i think you can proceed with the above plan
1247780987,9406,huangzhw,2022-09-15T08:43:14Z,OK
1249985922,9406,huangzhw,2022-09-17T03:06:05Z,"We simply pass a key to the events or pass the value too? without value, it is more simple."
1250242584,9406,MeirShpilraien,2022-09-18T10:49:18Z,"> Same problem happens in SET command.
setCommand -> setGenericCommand -> notifyKeyspaceEvent -> moduleNotifyKeyspaceEvent -> RM_StringDMA.
The value will be freed, but we still use it in rewriteClientCommandVector.

@huangzhw I tried to reproduce this scenario you describe but I could not, I could not see where we use this freed value on `replaceClientCommandVector`

Modified:
Ok found it, its when you use it with expiration option :+1: "
1250246284,9406,MeirShpilraien,2022-09-18T11:09:05Z,"So this is not really an issue because the value is not really freed, it is refcounted and client arguments array is keeping the refcount on it. I verified with valgrind and there is no memory violation."
1251063817,9406,MeirShpilraien,2022-09-19T14:02:23Z,@huangzhw after consulting with @oranagra we believe that we can solve this StringDMA issue in your case by increasing the `val` refcount so even if StringDMA is used the value will not be freed and will still be usable. WDYT?
1251692000,9406,huangzhw,2022-09-20T00:03:26Z,"> @huangzhw after consulting with @oranagra we believe that we can solve this StringDMA issue in your case by increasing the `val` refcount so even if StringDMA is used the value will not be freed and will still be usable. WDYT?

we need `incr` before `RM_StringDMA` and `decr` after `RM_StringDMA` by ourself?"
1254068440,9406,MeirShpilraien,2022-09-21T18:21:11Z,"> we need incr before RM_StringDMA and decr after RM_StringDMA by ourself?

No, I actually meant that you will do it on the value that you might want to use after the unlink event finishes."
1257118781,9406,huangzhw,2022-09-25T04:39:37Z,"Simple change. We need to discuss that what `RedisModuleKeyInfo` should be.

```
typedef struct RedisModuleKeyInfo {
    uint64_t version;       /* Not used since this structure is never passed
                               from the module to the core right now. Here
                               for future compatibility. */
    const char *key;        /* key name */
} RedisModuleKeyInfoV1;
```"
1264586742,9406,huangzhw,2022-10-02T08:45:39Z,@oranagra I'm not done. Still with this problem https://github.com/huangzhw/redis/actions/runs/3162914989/jobs/5149971464
1264588918,9406,oranagra,2022-10-02T08:58:07Z,"that's odd, in many other places (e.g. `RM_RetainString`) we simply pass them around as equivalent, without even a casting attempt.
maybe you can try to remove the struct initializer `{}` and just set the fields one by one to resolve it."
1265035001,9406,huangzhw,2022-10-03T07:19:27Z,"> > we need incr before RM_StringDMA and decr after RM_StringDMA by ourself?
> 
> No, I actually meant that you will do it on the value that you might want to use after the unlink event finishes.

@MeirShpilraien Do you mean we `incr` `val` before `moduleNotifyKeyUnlink` in `dbGenericDelete`?"
1265330269,9406,oranagra,2022-10-03T11:53:41Z,"@huangzhw yes, i think that that's what he meant.
it'll cause `dbUnshareStringValue` to create a new one for the module, but will not destroy the original which `dbGenericDelete` holds"
1265423687,9406,huangzhw,2022-10-03T13:15:35Z,"> that's odd, in many other places (e.g. `RM_RetainString`) we simply pass them around as equivalent, without even a casting attempt. maybe you can try to remove the struct initializer `{}` and just set the fields one by one to resolve it.

I still can't resolve this problem. Because in core `RedisModuleString` is `robj`, but we can't use `robj` in `RedisModuleKeyInfoV1`, we need to define `RedisModuleKeyInfoV1` in both core and module."
1266272471,9406,huangzhw,2022-10-04T01:04:38Z,"Another question. Why do we put this event to `moduleNotifyKeyUnlink`. If  we need expire evict, we'd better put this in front of `dbGenericDelete` and `dbGenericDelete`. Then we don't need `dictTwoPhaseUnlinkFind`, so we need not to handler `StringDMA`."
1266566445,9406,oranagra,2022-10-04T08:13:39Z,"@huangzhw i'm not sure i understand your last question.
we wanted to let the module get notified on any key that's removed from the database, before it is removed, but only if the key exists. AFAIK for that we either need another dictFind to make sure the key exists, or a two-phase removal."
1266582418,9406,huangzhw,2022-10-04T08:28:13Z,"> @huangzhw i'm not sure i understand your last question. we wanted to let the module get notified on any key that's removed from the database, before it is removed, but only if the key exists. AFAIK for that we either need another dictFind to make sure the key exists, or a two-phase removal.

Sorry, I made a mistake. In `dbGenericDelete` we not always know we will remove an entry.

If we want to add expire and evcit, will they be mutual exclusion with `removed`?
Two choices:
* A key expired, we fire two event, `removed` and `expired`;
* One event, `expired`?"
1266913284,9406,oranagra,2022-10-04T12:27:01Z,"I think we can add another flags / hint argument to dbGenericDelete and use it in evict.c and expire.c
then we pass that hint to moduleNotifyKeyUnlink and use it as the sub-event."
1279921623,9406,oranagra,2022-10-16T08:34:28Z,"@huangzhw what's the status here?
we're almost done, right?
any reason not to mark it as ready and ask for core-team approval?

anything missing or needs a decision?
other than minor things, the only major one i see is the one about EVICTED and EXPIRED indication."
1279958257,9406,huangzhw,2022-10-16T12:17:49Z,"> @huangzhw what's the status here? we're almost done, right? any reason not to mark it as ready and ask for core-team approval?
> 
> anything missing or needs a decision? other than minor things, the only major one i see is the one about EVICTED and EXPIRED indication.

We are almost done. Only one left is EVICTED and EXPIRED, i will commit it these days.

One thing is should we support `OVERWRITE`? And something about overwrite is wrong, should we fix here or another pr?"
1279971681,9406,oranagra,2022-10-16T13:36:12Z,"What do you mean about overwrite? What's wrong? 
I'm not sure the module needs a different indication between delete and overwrite. Anyone thinks otherwise? "
1279975293,9406,huangzhw,2022-10-16T13:57:17Z,"> yes, as i commented here [#9406 (comment)](https://github.com/redis/redis/pull/9406#discussion_r979359382) i think we should keep the same approach we had when this was a KSN, and call it from moduleNotifyKeyUnlink. the only difference is that we took the hook out of KSN, but it's still something that needs to be done every time a key is removed from the database. the only thing extra i think it would be nice to add a distinction between DELETE, OVERWRITE, EXPIRED, and EVICTED.

You metion it or we don't need it.


`dbOverwrite` in spopWithCountCommand is wrong."
1280005030,9406,oranagra,2022-10-16T16:37:30Z,"Ohh, I already forgot. 
Well, thinking of it again, I think that semantically, unlike evict and expire, in which the deletion is not a direct result of a user action, for overwrite there's no real difference from delete + write. 
In fact, IIRC some places in the code use dbOverwrite, and others use dbDelete and dbAdd. 
I'm not in front of the code right now, maybe we already sorted these out to be consistent. 
on the other hand, I guess that if we can easily provide that indication, some modules will find is useful, and others can just handle it together with normal deletion, so let's look into that. 

@guybe7 @MeirShpilraien any thoughts about that?"
1280455895,9406,oranagra,2022-10-17T08:09:23Z,"I think maybe the diff will look better if we don't add an argument to dbDelete, and have an implicit DELETED, then in the few places that need EXPIRED and EVICTED, use dbDeleteGeneric. 

Regarding OVERWRITE, I looked into it a bit and it seems complicated.
For instance spopWithCount uses dbOverwrite in one of its cases, although logically it's not an overwrite. "
1280466999,9406,oranagra,2022-10-17T08:18:22Z,Please add documentation for the new sub-events in module.c
1281423824,9406,oranagra,2022-10-17T20:07:46Z,"Looking into the overwrite topic deeper this time, I realize is was wrong to look at `dbOverwrite`, it's actually `setKey` and in fact our projects isn't actually complete without hooking into it. 
Without that, when SUNIONSTORE completely replaces (deletes) the target key (including the TTL), the module would miss it. 

In the past there was a mix, some places used `setKey` and others `dbDelete` and `dbAdd`, but I think I sorted this out long ago, and with the exception of RESTORE and alike, all of the overwrite commands now use `setKey`. 

Bottom line, I think we should add the OVERWRITE indication, and use it in `setKey`"
1281672891,9406,huangzhw,2022-10-18T00:50:15Z,"> Looking into the overwrite topic deeper this time, I realize is was wrong to look at `dbOverwrite`, it's actually `setKey` and in fact our projects isn't actually complete without hooking into it. Without that, when SUNIONSTORE completely replaces (deletes) the target key (including the TTL), the module would miss it.
> 
> In the past there was a mix, some places used `setKey` and others `dbDelete` and `dbAdd`, but I think I sorted this out long ago, and with the exception of RESTORE and alike, all of the overwrite commands now use `setKey`.
> 
> Bottom line, I think we should add the OVERWRITE indication, and use it in `setKey`

Why module would miss SUNIONSTORE event? It calls `dbOverwrite`."
1281883415,9406,oranagra,2022-10-18T06:39:10Z,"i'm sorry, i think i missed something again, but then again, i still see some issues (pre-existing ones) that need to be sorted out.
reviewing all the calls to `setKey`, it seems that these are all a case of OVERWRITE (and we can add a new sub-event type for it),

but some (actually **all**) of the calls to `dbOverwrite` in which we already call `moduleNotifyKeyUnlink` seem to be wrong.
e.g. `spopWithCountCommand`, `incrDecrCommand`, `incrbyfloatCommand`, `dbUnshareStringValue` (e.g. on behalf of `setrangeCommand`, `lookupStringForBitCommand`, etc)
i.e. in all these cases, we don't really want to signal a DELETED event.

what i think we should do:
1. move out the call to `moduleNotifyKeyUnlink` and also the call to `signalKeyAsReady` from `dbOverwrite` to `setKey`
2. i think in that case we want to issue an OVERWRITE sub-event.
3. let's rename `dbOverwrite` to `dbReplaceValue` or alike, to indicate it works on the value, not the key (unlike `setKey` it doesn't mess with the TTL, and doesn't logically replace the key, just modifies the assigned value)

@guybe7 @MeirShpilraien @soloestoy i'd like a reassurance."
1282490418,9406,soloestoy,2022-10-18T14:29:51Z,need a deep review
1282736376,9406,oranagra,2022-10-18T17:06:54Z,conceptually approved in a core-team meeting.
1285219991,9406,huangzhw,2022-10-20T09:27:30Z,"> conceptually approved in a core-team meeting.

Will we do it in this pr?"
1285757077,9406,oranagra,2022-10-20T15:34:32Z,"@huangzhw not sure what you're asking.
i just noted that the core team discussed the general concept of the PR and okay-ed it.
there's still a bit of work to do before merging it, which is to implement the [changes](https://github.com/redis/redis/pull/9406#issuecomment-1281883415) i asked regarding overwrite (please go ahead and add them to this PR), and get at least one more set of eyes on both the API, and the overwrite related concerns."
1293419632,9406,huangzhw,2022-10-27T11:59:06Z,@oranagra  I just finished it and fixed some bugs.
1296118406,9406,oranagra,2022-10-30T05:33:58Z,full CI: https://github.com/redis/redis/actions/runs/3354247979
1296261072,9406,oranagra,2022-10-30T13:37:12Z,@yossigo @guybe7 @MeirShpilraien @soloestoy i'd like another pair of eyes on this one before it's merged.
1331902280,9406,oranagra,2022-11-30T09:58:37Z,"@huangzhw merged (more than a year after the initial version).
thank you for the patients and for following all the requests."
913901295,9462,madolson,2021-09-06T23:51:05Z,"@filipecosta90 For the sake of completeness, can you run your perf test again with an r values greater than 100k. I'd like to make sure the benchmark holds up when basically everything isn't in some level of the CPU cache you probably tested on.

I think this is really exciting!"
938234485,9462,YacineTALEB,2021-10-07T23:55:17Z,"The size of `hdr_histogram` struct from https://github.com/redis/redis/blob/d96f47cf06b1cc24b82109e0e87ac5428517525a/deps/hdr_histogram/hdr_histogram.h#L17-L35  is `104 bytes` itself. The `hdr_histogram` count array is stored in `int64_t* counts;` and is of size `counts_len` when initialized. 

Initializing a histogram with the values from https://github.com/redis/redis/pull/9462/commits/aaac1653ada3e7418e9e1a585573ca37fa562ef5, i.e., 
```
LATENCY_HISTOGRAM_MIN_VALUE 1L 
LATENCY_HISTOGRAM_MAX_VALUE 1000000L 
LATENCY_HISTOGRAM_PRECISION 3
``` 
Would lead to the creation of a histogram with a `counts_len` of `11264`. Overall the overhead size of a single histogram, according to https://github.com/redis/redis/blob/d96f47cf06b1cc24b82109e0e87ac5428517525a/deps/hdr_histogram/hdr_histogram.c#L319-L358 would be `(11264 * 8) + 104 = 90216 bytes`"
957879661,9462,oranagra,2021-11-02T16:01:22Z,@filipecosta90 do you wanna pick this up?
984537461,9462,filipecosta90,2021-12-02T11:23:10Z,"> @filipecosta90 do you wanna pick this up?

@oranagra this weekend will focus on it. :+1: "
999089804,9462,filipecosta90,2021-12-21T20:58:43Z,"@oranagra and @madolson I've picked this one again, and now all tests pass, and added the ability to enable/disable this at runtime, and to configure the exported percentiles. 
IMO missing extra testing, and extra benchmarks, that I will address in the following days. "
1001717449,9462,filipecosta90,2021-12-27T19:46:54Z,"> @filipecosta90 For the sake of completeness, can you run your perf test again with an r values greater than 100k. I'd like to make sure the benchmark holds up when basically everything isn't in some level of the CPU cache you probably tested on.

@madolson I've updated the benchmark results with the latest unstable vs this feature branch ( with the feature enabled disabled ) and a larger keyspace len as requested


### overhead test using redis-benchmark on unstable vs commands.latency.histogram

I've tested using ping and set commands ( fast ones ) to assess the largest possible overhead of histogram latency tracking. 
As seen below there is no measurable overhead on the achievable ops/sec or full latency spectrum on the client. This matches the expected behaviour, given on previous microbenchmarks of the hdr_record_value (the function used) [>>see reference<<](https://github.com/HdrHistogram/HdrHistogram_c/pull/76) took on average 5-6 ns/op.
 
#### results on unstable ( commit hash = af0b50f83a997ca3eb2e21fd9ee823ef15e12183 )
Used redis-server command
```
taskset -c 0 ./src/redis-server --save """" --daemonize yes
```

```
taskset -c 1,2 redis-benchmark -c 50 --threads 2 -d 1024 -r 1000000 -n 10000000 --csv -t ping,set,get
""test"",""rps"",""avg_latency_ms"",""min_latency_ms"",""p50_latency_ms"",""p95_latency_ms"",""p99_latency_ms"",""max_latency_ms""
""PING_INLINE"",""212512.75"",""0.220"",""0.040"",""0.215"",""0.279"",""0.295"",""24.383""
""PING_MBULK"",""220716.45"",""0.213"",""0.040"",""0.215"",""0.263"",""0.287"",""28.127""
""SET"",""175198.86"",""0.269"",""0.040"",""0.263"",""0.359"",""0.391"",""20.383""
""GET"",""179121.59"",""0.263"",""0.040"",""0.263"",""0.351"",""0.367"",""32.127""
```

#### results on commands.latency.histogram branch with latency track disabled
Used redis-server command
```
taskset -c 0 ./src/redis-server --save """" --latency-track no --daemonize yes
```
```
taskset -c 1,2 redis-benchmark -c 50 --threads 2 -d 1024 -r 1000000 -n 10000000 --csv -t ping,set,get
""test"",""rps"",""avg_latency_ms"",""min_latency_ms"",""p50_latency_ms"",""p95_latency_ms"",""p99_latency_ms"",""max_latency_ms""
""PING_INLINE"",""212408.94"",""0.221"",""0.040"",""0.223"",""0.279"",""0.295"",""24.383""
""PING_MBULK"",""219375.22"",""0.214"",""0.032"",""0.215"",""0.263"",""0.287"",""28.367""
""SET"",""173644.27"",""0.270"",""0.040"",""0.263"",""0.367"",""0.391"",""28.207""
""GET"",""177619.89"",""0.265"",""0.040"",""0.263"",""0.351"",""0.375"",""28.127""
```

#### results on commands.latency.histogram branch with latency track enabled
Used redis-server command
```
taskset -c 0 ./src/redis-server --save """" --latency-track yes --daemonize yes
```
```
taskset -c 1,2 redis-benchmark -c 50 --threads 2 -d 1024 -r 1000000 -n 10000000 --csv -t ping,set,get
""test"",""rps"",""avg_latency_ms"",""min_latency_ms"",""p50_latency_ms"",""p95_latency_ms"",""p99_latency_ms"",""max_latency_ms""
""PING_INLINE"",""211528.28"",""0.223"",""0.040"",""0.223"",""0.279"",""0.295"",""24.191""
""PING_MBULK"",""219664.36"",""0.214"",""0.040"",""0.215"",""0.263"",""0.287"",""24.127""
""SET"",""175204.98"",""0.271"",""0.040"",""0.263"",""0.359"",""0.383"",""24.399""
""GET"",""179172.94"",""0.263"",""0.040"",""0.263"",""0.351"",""0.375"",""24.239""
```

"
1001748424,9462,filipecosta90,2021-12-27T20:54:41Z,@madolson and @oranagra I believe all requested changes have been addressed. When you have the time can you check it?
1003905690,9462,oranagra,2022-01-03T07:12:00Z,@redis/core-team please approve (see top comment)
1004624983,9462,yoav-steinberg,2022-01-04T08:50:35Z,@filipecosta90 does `CONFIG RESETSTAT` zero the latency stats?
1004641564,9462,filipecosta90,2022-01-04T09:16:04Z,"> @filipecosta90 does `CONFIG RESETSTAT` zero the latency stats?

yes. On https://github.com/redis/redis/pull/9462/files#diff-1abc5651133d108c0c420d9411925373c711133e7748d9e4f4c97d5fb543fdd9R2633"
1004728979,9462,oranagra,2022-01-04T11:25:54Z,"@filipecosta90, a concern raised by @soloestoy is that hdrhistogram us using malloc directly, and will not be accounted for in used_memory.
we would like to change that. can you look into it?"
1004733408,9462,filipecosta90,2022-01-04T11:33:15Z,"> @filipecosta90, a concern raised by @soloestoy is that hdrhistogram us using malloc directly, and will not be accounted for in used_memory. we would like to change that. can you look into it?

Yes, will raise it in https://github.com/HdrHistogram/HdrHistogram_c and address the change with a PR to the project while also including the minimum changes to our deps folder :+1: "
1005213570,9462,filipecosta90,2022-01-04T22:20:09Z,"> > @filipecosta90, a concern raised by @soloestoy is that hdrhistogram us using malloc directly, and will not be accounted for in used_memory. we would like to change that. can you look into it?
> 
> Yes, will raise it in https://github.com/HdrHistogram/HdrHistogram_c and address the change with a PR to the project while also including the minimum changes to our deps folder +1

@oranagra I've added the runtime allocator change to the hdrhistogram on the deps folder: https://github.com/redis/redis/pull/9462/commits/2b66f5249f4ad3623bde77def23de6b9f79706ed

notice that as discussed we need the additional `zcalloc_num` given the zmalloc's zcalloc is not with the right signature as calloc: https://github.com/redis/redis/pull/9462/commits/2b66f5249f4ad3623bde77def23de6b9f79706ed#diff-b601b5ff4c0d077e82c78d4059ce3d10de7a54a8965ec9e7b6515b799f930ac8R186"
1005622797,9462,oranagra,2022-01-05T11:56:37Z,"For the record, the core-team approved this PR on a voice meeting."
1005625801,9462,oranagra,2022-01-05T12:01:41Z,"@filipecosta90 thank you.
can you please make a redis-doc PR for LATENCY HISTOGRAM and INFO"
1005635935,9462,filipecosta90,2022-01-05T12:16:11Z,"> @filipecosta90 thank you. can you please make a redis-doc PR for LATENCY HISTOGRAM and INFO

thank you all the review cycles! 

> can you please make a redis-doc PR for LATENCY HISTOGRAM and INFO

will do so"
1005719877,9462,filipecosta90,2022-01-05T14:13:30Z,@oranagra @yoav-steinberg @itamarhaber redis-doc PR: https://github.com/redis/redis-doc/pull/1733
919892541,9504,guybe7,2021-09-15T10:20:28Z,modules: maybe we want something like RM_CreateContainerCommand + RM_CreateSubcommand ?
919895275,9504,guybe7,2021-09-15T10:24:16Z,"@oranagra @madolson please go over all subcommands' flags: I've put some thought in some (CONFIG's subcommand) but in others, I just copied the flags from the parent command (CLUSTER)"
920655900,9504,soloestoy,2021-09-16T07:29:33Z,"I didn't go deep into this PR, the subcommand idea is good, but a problem that came to my mind is about module, how to let module to register subcommands?"
920661463,9504,guybe7,2021-09-16T07:33:48Z,"@soloestoy I guess we should introduce a new API `RM_CreateSubcommand` which is identical to RM_CreateCommand except that it gets `const char* container_name` (and maybe also drop the old [first,last,step] scheme, not sure)
"
921001125,9504,guybe7,2021-09-16T15:24:09Z,"TODO:
1. module API
2. `COMMANDS LIST [FILTERBY (MODULE <module-name>|ACLCAT <cat>|GROUP <group>|PATTERN <pattern>)]`
3. different ACL command-id per subcommand. pay special attention to backward-compatibility.

@oranagra please ack"
943590840,9504,oranagra,2021-10-14T17:59:13Z,@redis/core-team please approve. Have a look at the the top comment for a comprehensive list of the changes and their implications. 
833697827,8887,oranagra,2021-05-06T17:14:21Z,"One more thing, we need an option to convert the encoding at rdb loading time.
either when doing full sanitization, since in that case we already do O(N) operation anyway, but also maybe some people will want to do that conversion at upgrade time (slave will take longer to go online), maybe we'll even make it the default one day.

i think it would also be a good idea to benchmark such a conversion, and get a feeling of how much longer it takes to load an rdb file if we do a conversion during loading vs one that just loads the ziplists as they are. can you try doing such a benchmark?
"
834000583,8887,sundb,2021-05-07T01:54:28Z,"> 1. i don't like the term ""list container"". in ""ziplist"" and ""listpack"", we mention it is zipped, or packed, i.e. encoded. this container can be more easily mistaken to be something related to list data type rather than an encoding type.
>    maybe a term like ""encoded list"" or ""packed list"" (i.e. something that is generic, and both ziplist and listpack comply to, i.e. both ziplist and listpack are encoded and packed).

    I also do not like ""list container"", it is too difficult to come up with a good name.

> 2. currently we only create new hashes with listpack, and when modifying an existing hashes we'll always keep them as ziplists.
>    we need to see if we can come up with a plan to gradually convert them.
>    i.e. i don't want an O(N) operation at load time or any other time, but eventually i want them all gone.

    Maybe we can convert ziplist to listpack when call hashTypeInitIterator.

> 3. we have a problem with testing. the majority of the tests work by creating a new db from scratch, so the ziplist code is now mostly unreachable.
>    maybe we need some DEBUG sub-command that will tell redis to default to creating ziplists, and then run the entire testsuite in that mode (in the daily CI)
>    i'm specifically worried about the corrupt-dump-fuzzer test. we either need to save an old rdb file in the assets folder, or let it run twice with that DEBUG tweak i suggested.

    I have write corrupt-dump-fuzzer test in #8761, I will complete these tests.
"
835439285,8887,sundb,2021-05-08T17:31:27Z,"@oranagra I tested the speed of rdb loading, before(keep ziplsit) and after(convert ziplist) using the same dump.rdb, creating 100000 keys per test and filling value with random strings.
It looks like the speed becomes 3 times of the original.
| entries num of one ziplist | max value size| rdb loading time without convert | rdb loading time with convert|
 | ------ | ------ | ------ | ------ | 
| 256 |  64bytes | 6.888s | 18.078s |
| 256 |  32bytes | 6.849s | 17.869s |
| 256 |  16bytes | 6.991s | 18.0842s |
 | 128|  64bytes | 3.553s | 8.983s |
| 128|  32bytes | 3.521s | 9.0222s |
 | 128|  16bytes | 3.558s | 8.982s |
"
835609233,8887,huangzhw,2021-05-09T01:11:09Z,"When convert ziplist to listpack, `lpPushTail` is used. `malloc` will be called everytime. Maybe prealloc make sense. But I'm not sure whether it can make the speed up."
835637925,8887,sundb,2021-05-09T02:31:17Z,"@huangzhw Good idea, I try again."
838405443,8887,sundb,2021-05-11T12:41:13Z,"@oranagra Performance test comparison of ```listpack``` and ```ziplist```.
Data of ```find```,``` index``` and ```validateIntegrity``` is 100000 entry.
The main reason for the difference in ```find``` speed between ```ziplist``` and ```listpack``` is that the traversal speed of ```ziplist``` is 2/3 of  ```listpack```.
|  | ziplist | listpack|
 | ------ | ------ | ------ | 
| pushTail 100000 entry |  85ms | 84.7ms | 
| find 2000 times from head |  1.157s | 1.851s |
| index 2000 times |  1.412s| 1.458s| 
 | validateIntegrity 2000 times|  959ms| 1.739s| 

"
838418022,8887,sundb,2021-05-11T12:50:52Z,"Thanks @huangzhw 
Using pre-allocated memory for listpacks results in a huge speedup in loading.
| entries num of one ziplist | max value size| rdb loading time without convert | rdb loading time with convert|
 | ------ | ------ | ------ | ------ | 
| 256 |  64bytes | 3.875s| 5.675s |
| 256 |  32bytes | 4.178s | 5.724s |
| 256 |  16bytes | 4.087s | 5.684s |
 | 128|  64bytes | 1.803s | 3.237s |
| 128|  32bytes | 1.747s | 2.725s |
 | 128|  16bytes | 1.771s | 2.695s |"
839862802,8887,oranagra,2021-05-12T15:24:01Z,"@sundb the conversion [benchmark](https://github.com/redis/redis/pull/8887#issuecomment-838418022) you showed during rdb loading looks very promising.
maybe with that (rather low impact), we can afford to just always do that conversion unconditionally.
if we do that, we can drop all the runtime code that allows t_hash.c to optionally work with ziplists, and the configuration that let's us run the test suite with ziplists rather than listpacks.
and if we don't do that, we still need to come up with a strategy of how we gradually do that at runtime without impacting latency. 

@redis/core-team WDYT?"
839875558,8887,sundb,2021-05-12T15:40:24Z,@oranagra I'll run test again tomorrow on a very large amount of data (millions and more).
840437330,8887,sundb,2021-05-13T09:26:54Z,"I ran another rdb load time test on a larger amount of data.
Due to memory limitation, only 500,000 keys are filled when entry num is 256.
The data of this test, and the above test results are almost proportional, almost 50% increase in time consumption.

|num of keys| entries num of one ziplist | max value size| rdb loading time without convert | rdb loading time with convert|
| ------  | ------ | ------ | ------ | ------ | 
| 500,000 | 256 | 64bytes | 16.575s | 25.860s |
| 500,000 | 256 | 32bytes | 16.710s | 26.662s |
| 500,000 | 256 | 16bytes | 16.635s | 25.608s |
| 1,000,000 | 128 | 64bytes | 19.541s | 28.423s |
| 1,000,000 | 128 | 32bytes | 19.427s | 31.656s |
| 1,000,000 | 128 | 16bytes | 17.649s | 27.367s |"
841205469,8887,sundb,2021-05-14T12:13:29Z,"@oranagra I made two changes to listpack's lpFind.
1) Use lpNext instead of lpSkip.
2) Modify entry length calculation.
I conducted a test, 500 entries, lpFind 2000000 times.
Performance Comparison:

|  | use lpNext | use lpSkip | use lpSkip and change entry cal |  ziplist |
| ------  | ------ | ------ | ------ |-----|
| find string    | 6.915s | 5.074s | 3.695s| 2.912s|
| find number | 6.264s | 5.148s | 3.085s | 3.383s|

"
841213594,8887,sundb,2021-05-14T12:30:47Z,"hset benchmark test.
config: ```hash-max-ziplist-entries 9999999```
command: ```redis-benchmark -r 1000000 -n 50000 -t hset```

listpack(packed)
```
  throughput summary: 3090.81 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       16.088     0.104    10.151    39.519    41.695    49.375
```

ziplist(packed)
```
  throughput summary: 3479.71 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       14.284     0.104     8.759    35.647    38.111    51.071
```

ziplsit(unstable)
```
  throughput summary: 3424.19 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       14.511     0.112     8.751    36.191    38.399    44.799
```"
865357318,8887,madolson,2021-06-21T21:28:02Z,"@sundb So using listpack seems to be slower across a lot of dimensions? Since for the most part listpacks aren't really an optimization for the small hashes, do we really think this is that much better?"
865469028,8887,sundb,2021-06-22T02:17:31Z,"@madolson Yes, since ```listpack``` uses many ``` if``` judgments to calculate the length of the current element, this is the main reason why it is slow.
Recently I've been wondering if it's possible to loop from backward to forward, maybe it can be improved to the same speed as ziplist."
866010767,8887,sundb,2021-06-22T14:01:29Z,"@oranagra We should consider the issue raised by @madolson, since listpack cannot get the size of the elements directly, this will result in listpack traversal speed being 85% of ziplist, and sometimes even less.
I tried to improve listpack traversal speed in various ways, but in the end they all failed."
866025110,8887,oranagra,2021-06-22T14:18:53Z,"@sundb @madolson Please keep in mind that the reason to replace ziplists with listpacks is not in order to get better memory or speed optimization, it's actually in order to get rid of the unreadable bug-prone code of ziplist.

Also note that when an object (e.g. hash) is ziplist encoded, it is usually done in order to trade performance in favor of memory, i.e. we switch from an O(1) dict to an O(n) ziplist, so i don't think it really matters if listpack is 85% of the speed of ziplist.

On the other hand, i don't wanna slow redis, and i'm not ready to give up (yet)!
I don't (yet) agree that this performance regression is inevitable, and i think we should try to solve it (in some way or another).

For instance, when i recently added the sanitization feature, i did add a performance regression, which i didn't want to live with. I couldn't find a way to avoid the regression, so instead i profiled the code and added an optimization (in a different place) to counteract the regression i added (had i added that optimization in a separate / earlier PR, my sanitization PR would have still added a regression).

Another thing to consider is that maybe for some reason the benchmark you're doing is unrealistic, maybe we can realize that a different, more realistic benchmark will be more forgiving to our cause."
866030487,8887,sundb,2021-06-22T14:25:20Z,"Tahnks @oranagra, I'll do more testing and experimentation."
866679365,8887,sundb,2021-06-23T09:25:04Z,"@oranagra @madolson In my last commit, I made a big performance improvement to listpack, and I had been ignoring the function call overhead, which is the main reason why listpack is slower than ziplist.

hset benchmark test.
config: ```hash-max-ziplist-entries 9999999```

1) command: ```redis-benchmark -r 1000000 -n 50000 -t hset```
listpack(packed)
```
  throughput summary: 3428.41 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       14.471     0.112     8.895    35.679    40.031    48.831
```

ziplist(packed)
```
  throughput summary: 3169.77 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       15.664     0.136     9.823    38.111    42.591    56.415
```

2) command: ```redis-benchmark -r 1000000 -n 50000 -t -x hset myhash __rand_int__ 1```
listpack(packed)
```
  throughput summary: 3988.83 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       12.426     0.120     7.911    30.943    34.943    40.639
```

ziplsit(unstable)
```
  throughput summary: 3157.56 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
       15.723     0.112    10.327    38.111    42.815    52.991
```"
867612312,8887,oranagra,2021-06-24T12:53:15Z,"@sundb great!.. so now listpack is faster than ziplist? (at least in ""find"", on a non-realistically long list).
do you know if that was mainly the interlining or skipping the call to lpStringToInt64?

anyway, as i said, in normal conditions, we won't have such long lists, and on short ones the network / command processing overhead is likely to hide any differences. if we'll ever want, i'm sure we can optimize it further. but i don't think we do at this point.

so the next thing to re-consider is if we can afford to force an ""upgrade"" to listpack at rdb loading, or do we need to keep the complication of supporting both formats side by side at runtime, right?
do we have all the details to summon a decision about that already? or is something missing?"
868120359,8887,sundb,2021-06-25T01:27:14Z,"> @sundb great!.. so now listpack is faster than ziplist? (at least in ""find"", on a non-realistically long list).
> do you know if that was mainly the interlining or skipping the call to lpStringToInt64?

    Mainly due to interlining.

> 
> anyway, as i said, in normal conditions, we won't have such long lists, and on short ones the network / command processing overhead is likely to hide any differences. if we'll ever want, i'm sure we can optimize it further. but i don't think we do at this point.

    All methods of listpack will bring improvements, I will test all commands.
> 
> so the next thing to re-consider is if we can afford to force an ""upgrade"" to listpack at rdb loading, or do we need to keep the complication of supporting both formats side by side at runtime, right?
> do we have all the details to summon a decision about that already? or is something missing?

    Yes, this test is still in progress. In yesterday’s test,
    the speed of convert listpack to ziplist has also been improved.

"
869539715,8887,sundb,2021-06-28T09:45:20Z,"@oranagra @madolson  The consumption of listpack to ziplist is mainly lpPushTail, and it is not affected by the size of ziplsit item, the conversion time depends on the total number of all ziplsit.
For example, if there are 1 million keys, each key has 128 entries, then the conversion time is proportional to the number of entries (1 million * 128).
On my pc, 128 million entries are converted in about 7s.

|num of keys| entries num of one ziplist | entry size | loading time without convert | rdb loading time with convert|
| ------  | ------ | ------ | ------ | ------ | 
| 1 million | 128 | 8 bytes | 5.893s | 12.747s |
| 1 million | 128 | 16 bytes | 7.263s | 14.727s |
| 1 million | 128 | 24 bytes | 9.794s | 16.058s |
| 1 million | 128 | 32 bytes | 11.636s | 18.537s |
| 2 million | 64 | 8 bytes | 7.234s | 14.366s |
| 2 million | 64 | 16 bytes | 8.759s | 16.478s |
| 2 million | 64 | 24 bytes | 11.095s | 18.372s |
| 2 million | 64 | 32 bytes | 14.413s | 20.607s |
| 4 million | 32 | 8 bytes | 9.452s | 16.886s |
| 4 million | 32 | 16 bytes | 10.685s | 18.975s |
| 4 million | 32 | 24 bytes | 13.474s | 20.824s |
| 4 million | 32 | 32 bytes | 16.041s | 22.712s |

Btw. After testing other hash-related commands, the tested hash's size is 512, listpack and ziplist have similar performance."
870370971,8887,oranagra,2021-06-29T08:01:14Z,"@redis/core-team 
we need to take a decision about conversion of ziplists to listpacks.
our raw options can be:
1. don't convert old ones, just create new ones as listpacks (we can never get rid of ziplist from our codebase this way)
2. convert each key when modified for the first time (we need to keep maintaining runtime support for ziplist encoded objects, and also have the test suite able to reach that code somehow).
3. convert all ziplists to listpack on rdb loading, so that there are never any ziplists at runtime (this is an O(N) operation on upgrades)

please have a look at the above benchmark and share your thoughts."
874187999,8887,oranagra,2021-07-05T15:17:52Z,"We discussed that topic in the core-team meeting.
we would like to proceed with option 3 mentioned above.
i.e. to convert all ziplists to listpacks on load time.
@sundb i suppose it means the code (and tests) can be simplified a lot now."
874393180,8887,sundb,2021-07-06T01:15:23Z,"@oranagra Great, I continue to make changes."
874684916,8887,sundb,2021-07-06T11:35:29Z,"@oranagra I'm not sure if we need to make t_hash compatible with ziplist, if not, ```packedClass``` is not necessary."
874732876,8887,oranagra,2021-07-06T12:53:07Z,yeah.. i'm sorry for both of us investing so much time on that.. we can drop it.
876986860,8887,sundb,2021-07-09T07:45:27Z,"@oranagra I add a new corrupt-dump test for ```t_stream```.
It doesn't error in the old code, because in the old lpSkip method, ```lpCurrentEncodedSizeUnsafe``` returns 0 when `p` is corrupted and ```lpEncodeBacklen``` will return 1, which then triggers the out-of-bounds assertion."
877909422,8887,sundb,2021-07-12T01:43:22Z,"> regarding the new corrupt payload test you added, is it related to the new assertions in lpEntrySizeUnsafe? did the fuzzer found it?

    Yes, it was found thanks to fuzzer, and the assertion was added to lpEntrySizeUnsafe.
"
880454690,8887,oranagra,2021-07-15T07:11:15Z,"@sundb i've reviewed the recent changes. LGTM.
are there any other topics left to code or decide before i do the final top to bottom review and merge this?"
880691922,8887,sundb,2021-07-15T13:24:12Z,"@oranagra There are still some leftover method names and comments that need to be modified, I can't be in front of the computer for a few days, I'll come back and make changes."
882382397,8887,sundb,2021-07-19T09:08:25Z,@oranagra It's ready for review.
893782051,8887,oranagra,2021-08-05T20:42:48Z,"@sundb thank you.. i added two minor comments about naming an doc comments.
next thing is to merge recent unstable into it, you'll get some conflicts since some code from this PR found its way to unstable already, and then we can merge it."
1131624604,8887,dev-lemontree,2022-05-19T12:29:02Z,"@sundb @oranagra I have a question. It can still support old RDB Version?
If we try to migrate Redis 6 to Redis  7 with replication, Maybe Redis 6 creates RDB with old version(using ziplist), Does Redis 7 load it(Redis 6 RDB)?
It is very common pattern to upgrade redis server.
"
1131626477,8887,sundb,2022-05-19T12:30:58Z,"@dev-lemontree Indeed, It is backward compatible."
1296250741,11303,oranagra,2022-10-30T12:49:02Z,"@sundb please avoid force-pushes, it's harder for me to keep track of what changed since my last review.
also, if you can go over the comments and mark the resolved ones as resolved, so it'll be easier to focus on what's left."
1305777263,11303,oranagra,2022-11-07T15:26:05Z,"@redis/core-team please approve, although basically there's no real interface change here other than a new output for OBJECT ENCODING.
it doesn't add any new config or change in the RDB format, can be thought as just an optimization for a single-node quicklist to avoid the quicklist header overhead."
1308341260,11303,oranagra,2022-11-09T07:44:07Z,"PR was conceptually approved in a core-team meeting.
@sundb anything left before merging it?

can you please add an example case of the memory impact in the top comment.
e.g. the effect on a list of 5 items of 10 chars each."
1308341980,11303,sundb,2022-11-09T07:44:58Z,"@oranagra OK, working on it."
1309185060,11303,oranagra,2022-11-09T18:22:32Z,"@sundb i added an example memory saving at the top (obtained with MEMORY USAGE).
the 40 bytes savings you estimated was just the quicklist struct, without the quicklist node, the saving is actually about 80 bytes."
1309190694,11303,zuiderkwast,2022-11-09T18:27:34Z,"Regarding the benchmark using `redis-benchmark`, I recommend using at least 2 threads. Otherwise, `redis-benchmark` tends to be the bottleneck rather than redis itself. In another benchmark, I noted using `top` while the benchmark was running, redis-benchmark being on 100% CPU and redis-server on ~95%."
1309196589,11303,oranagra,2022-11-09T18:33:19Z,i now notice an `!!Waiting for completion` section in the top comment. so i'll wait to hear if we want to do that or skip it.
1311647716,11303,sundb,2022-11-11T12:35:19Z,"@oranagra Please see the top comment again, I think the benchmarks are as expected except for the lrange.
I tried testing it on another branch, and it saved 1-2% performance, the downside is that the readability got worse, not sure if I want to implement it.
https://github.com/sundb/redis/commit/998444380db4e3a4670bf47f321524461764b169"
1311679135,11303,oranagra,2022-11-11T13:09:34Z,"@sundb i'm ok with applying the ""optimization"" (moving the `if` outside the loop, and repeating it).
let's just add a comment that ""we're not using listTypeIterator in order to optimize the loop"".
please apply and update the top comment."
1311720142,11303,zuiderkwast,2022-11-11T13:53:43Z,"I see you added `--threads 1` to the benchmark command line. Doesn't this too mean it's single threaded?

Anyway, I tested `./src/redis-benchmark -n 1000000 --threads 1 -r 10000000 eval 'for i=1,50,1 do redis.call(""rpush"", ""lst:""..ARGV[1], ""hellohello"") end' 0 __rand_int_` and I see that redis-benchmark is on 30% CPU while redis-server is on 100% CPU so that concern is not valid in this case. :-)

A few more benchmarking ideas. In order to maximize the work done on the data structures rather than networking, you could use pipelining too (e.g. `-P 10`) and I'm not sure how much CPU is used by Lua. Without Lua we can probably see even better results. I don't think it's necessary to redo the benchmark. Just good to know about."
1311730812,11303,sundb,2022-11-11T14:03:52Z,"@zuiderkwast Grateful indeed for your suggestions, I'll try again.
The reason for using 1 is that redis-benchmark is almost 100% utilized and I am not sure if the VM performance is too bad."
1312719876,11303,sundb,2022-11-13T12:31:37Z,"@oranagra @zuiderkwast 
I tried the benchmark with a new HW (Intel Xeon Cooper Lake, 8cores, 16G mem).
It looks like https://github.com/sundb/redis/commit/998444380db4e3a4670bf47f321524461764b169 doesn't offer a benefit, it just reduces the performance in the case of lack of performance will be a little faster, but still 20% slower than the old code of `addListRangeReply()`.
Please see the new benchmark results in the top comment, `LRANGE` performance will drop a little."
1312743748,11303,sundb,2022-11-13T14:26:32Z,"Here is what is seen via perf.
There is basically no difference between 1 and 2, and when I remove the `else if (o->encoding == OBJ_ENCODING_LISTPACK)` from 2, 2 and 3 are basically the same, so the performance drop is unavoidable.

1) This PR
![before3](https://user-images.githubusercontent.com/965798/201526739-405e2e5c-b9e1-4e35-a41b-4d1f5616e824.png)

2) https://github.com/sundb/redis/commit/998444380db4e3a4670bf47f321524461764b169
![after2](https://user-images.githubusercontent.com/965798/201526743-f91263bb-d694-4e25-b898-96469f8d105b.png)

3) Unstable
![unstable3](https://user-images.githubusercontent.com/965798/201526751-b3f001d7-a4a4-49ee-98c5-8c78ab3005c9.png)
"
1313551751,11303,sundb,2022-11-14T11:43:09Z,"@oranagra I re-benchmarked using memtier_benchmark with pipeline, but note that I still use `--threads=1`, otherwise redis-server would reach 100% cpu usage.
The results are close to redis-benchmark, with `LRANGE` reaching a performance degradation of <=0.5.
BTW,  I didn't test `LPOP `, `RPOP` and `LSET` with memtier_benchmark, because their OPS results have problems (probably a bug in memtier_benchmark)."
1313616165,11303,zuiderkwast,2022-11-14T12:27:04Z,"redis-server 100% CPU usage is what we want, isn't it?"
1313675650,11303,oranagra,2022-11-14T13:12:07Z,"> redis-server 100% CPU usage is what we want, isn't it?

yes, we do want the server (CPU) to be the bottle neck, otherwise, it means something else is the bottleneck and we're not measuring the throughput of redis."
1313685137,11303,sundb,2022-11-14T13:18:38Z,"@zuiderkwast @oranagra Yes, that's what I did yesterday (100% cpu usage), normally the new code would be slower than the old code, but when I didn't use `-threads=1` yesterday, the test results confused me, the new code was sometimes as fast as the old code, making me wonder if the 100% usage was causing the uncertainty.
Anyway, let me do it again."
1315208649,11303,sundb,2022-11-15T11:58:57Z,"prepare dataset : `redis-benchmark -q -n 1000000 RPUSH mylist hello`
benchmark: `memtier_benchmark --hide-histogram -n 50000 --pipeline=10 --command=""LRANGE mylist 0 300""`

Unstable
```
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lranges     67365.18        30.13873        31.87100        49.66300        60.15900    221107.78 
Totals      67365.18        30.13873        31.87100        49.66300        60.15900    221107.78 
```

This PR with commit
```
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lranges     66585.07        29.82411        31.23100        52.99100        61.43900    218547.27 
Totals      66585.07        29.82411        31.23100        52.99100        61.43900    218547.27
```

This PR without commit 
```
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lranges     61896.84        32.42271        34.55900        55.80700        63.99900    203159.46 
Totals      61896.84        32.42271        34.55900        55.80700        63.99900    203159.46
```

The performance degradation is mainly caused by the inline of `addListRangeReply()`.
The code in the loop in `addListRangeReply()` will be expanded in the old code, but not in the new one, resulting in a lot of function call overhead.
In the last commit, I split the processing of quicklist and listpack into smaller methods, making it easier to expand the iterators in each of them, which works as evidenced by the test results and perf.
But I'm not sure if it works under a different compiler or platform.
@oranagra @zuiderkwast WDYT?

![image](https://user-images.githubusercontent.com/965798/201912799-c646fe14-b01d-418f-8da2-3f8d7472405b.png)

![image](https://user-images.githubusercontent.com/965798/201912867-94ff5962-5b85-4b7f-b976-c3d41fcf6cdb.png)
"
1315244650,11303,oranagra,2022-11-15T12:29:40Z,"@sundb so the difference between your last commit and https://github.com/sundb/redis/commit/998444380db4e3a4670bf47f321524461764b169 is that you've put each of these in a separate function instead of an if-else in the outer function, and that allowed for a better inlining?

i don't think we wanna go deeper in that direction (being one step away from manual inlining of code, or conversion to assembly).
moving the `if` outside the loop should have been enough.
if it isn't then we can mark these functions as `inline` or convert them to macros, but in this case it's not applicable (unless we want to make two variants of each of these, one as macro and the other without).

bottom line, if what you did was enough to convince the compiler to inline, i'd be ok with that (just ask to write a comment in the code specifying that).
i'd like to check that it works on both gcc and clang (some recent version), but i don't think it'll change anything if we find out it doesn't."
1316843960,11303,sundb,2022-11-16T11:27:20Z,"1. (unstable + clang)(1) is slower than (Unstable + gcc)(3)
    Because gcc will inline the code in the loop, but clang will not.
    So 4 will also be faster than 2

2. Commits [`a542fc0` (#11303)](https://github.com/redis/redis/pull/11303/commits/a542fc01b1c20fa2ed288e9e094a75b872ec4873) is always valid, whether it's clang or gcc, it's a little faster than old code

benchmark: `memtier_benchmark --hide-histogram -n 50000 --pipeline=10 --command=""LRANGE mylist 0 300""`

### clang 14.0.0

1) Unstable
```
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lranges     65372.17        31.03445        33.02300        53.24700        60.92700    214566.27 
Totals      65372.17        31.03445        33.02300        53.24700        60.92700    214566.27
```

2) This PR with [`a542fc0` (#11303)](https://github.com/redis/redis/pull/11303/commits/a542fc01b1c20fa2ed288e9e094a75b872ec4873)
```
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lranges     65429.01        30.64463        32.12700        52.73500        61.18300    214752.85 
Totals      65429.01        30.64463        32.12700        52.73500        61.18300    214752.85 
```

### gcc 11.3
3) Unstable
```
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lranges     66881.59        30.50183        32.12700        52.73500        61.95100    219520.54 
Totals      66881.59        30.50183        32.12700        52.73500        61.95100    219520.54
```

4) This PR with  [`a542fc0` (#11303)](https://github.com/redis/redis/pull/11303/commits/a542fc01b1c20fa2ed288e9e094a75b872ec4873)
```
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lranges     67100.45        30.10728        31.23100        53.50300        63.48700    220238.87 
Totals      67100.45        30.10728        31.23100        53.50300        63.48700    220238.87
```
"
1316868526,11303,oranagra,2022-11-16T11:44:30Z,"so clang didn't used to do any inlining in that code, and thus unaffected by the change in this PR anyway.
and for GCC, which did do inlining, your recent change convinced it to keep doing it, and thus no regression either.
did i get it right? in which case i think we're good to go (i rather not dive into macros, and just let the compiler do what it can)."
1316873035,11303,sundb,2022-11-16T11:47:50Z,"> so clang didn't used to do any inlining in that code, and thus unaffected by the change in this PR anyway. and for GCC, which did do inlining, your recent change convinced it to keep doing it, and thus no regression either. did i get it right? in which case i think we're good to go (i rather not dive into macros, and just let the compiler do what it can).

Yes."
1316966194,11303,oranagra,2022-11-16T12:45:41Z,@sundb please update the top comment and let me know when ready to be merged.
1317079082,11303,sundb,2022-11-16T14:07:22Z,"@oranagra Updated, mainly the LRANGE part, which seems to bring a huge performance boost when list is listpack encoding, and I verified this boost on my local pc."
1321075039,11303,oranagra,2022-11-20T09:04:11Z,@sundb wanted to take a moment thank you again for the time you invested in this project and others!
882898143,8621,madolson,2021-07-19T22:17:24Z,"@redis/core-team Hey, wanted to get your guy's opinions about a couple of open questions. Skip to the third section if you are aware of the context. @zuiderkwast I know you're away, but take a look when you're back as well.

## High level changes

-   Three new commands will be introduced: publishlocal, subscribelocal, unsubsribelocal that add support for ""local channels"". (Feel free to suggest alternative naming now)
-   Subscribing and publishing to a local channel works just like a regular channel, except it exists in a separate namespace and is bound to a slot. Example:
	-   Client A does a Localsubscribe to local channel foo
	-   Client B does a normal subscribe to channel foo
	-   Client C does a local publish to channel foo. Client A receives the message.
-   Local channels will be assigned to slots based on the CRC16 mod 2^14, like they are for regular keys.
-   Sending a local message to a server not serving the slot will be redirected to the primary that owns the slot.
-   Subscribing to a local channel on a server not serving the slot will be redirect to the primary that owns the slot.
-   Messages are broadcasted between nodes over the clusterbus and not via replication. This reduces write amplification from the number of nodes in the cluster to the number of nodes in the shard.
-   local channels are supported in cluster mode disabled, but don't provide any specific value.
-   Patterns are not in the initial scope.

## Assumptions that are staying the same for local channels

1.  Local messages only provide at most once delivery guarantees
2.  There is no ordering guarantees between servers, if you are subscribing on a replica you might get a different order subscribing on the primary.
3.  There is also no delivery guarantees for local messages originating from the same client. If you are client that sends 3 messages, the middle one can be dropped.(In this case the clusterbus can drop the message) Messages will not be re-ordered though.

## Open questions

Here are the open questions that have come up while reviewing that the requester wanted to close on before continuing. They're summarized as multiple options, I've included my recommendation.  

-   Should publishlocal be restricted just to primaries?
	-   Today only primaries own slots, replicas have a lagged view and might disagree. Therefor in a cluster down scenario a replica could accept a publish it doesn't know it should redirect (This might change with some of the Cluster V2). Forcing publishing to happen on primaries will give us a stronger consistency model, but it's pubsub, so it's not that important. <- My preference
	-   The reason against publishing just to primaries is that today cluster pubsub allows you to send to wherever you feel like. Since it's not generating any write traffic into the keyspace, publishing on replicas will better diffuse the load across the shard.

-   Should the channel be considered a ""key"" and be included in the key positions of ""command"". Also, internal implementation detail, should it be considered a ""key"" to simplify command routing?
	-   A channel is not a key, and should not be considered one. The `Command` command will list it as having no keys. Internally we will add code to getNodeFromQuery() for routing. <- My preference
	-   It's ""basically a key"", so might as well re-use the existing key specification (or used/extend the keyspec that was proposed). We will need to explicitly handle the cases where getKeys() is called like client side tracking.

-   What is the behavior of pubsublocal commands during slot migration? (Including replica migration) Only one option listed here, but maybe there are others?
	-   During migration, it is assumed that ALL channels are ""immediately migrated"" to the target. As soon as a slot is moved into the migrating state, all clients connected are disconnected.
	-   The migration process needs to start with the target being put into importing before the source is put into migrating. I believe that is the required path, but worth validating.

-   What is the ""no longer serving slot behavior"". I.E. when a replica migrates away or when a slot migration happen, what happens to clients subscribing to the local channel.
	-   Disconnect the client after all the pending messages have been written out. Clients should know how to handle this case, and seems consistent with other mechanisms we have today like ACL changes. <- My preferred option
	-   We can send a message indicating that the server is no longer serving a given slot. This requires clients to understand the message and take action. Seems more complex.

-   Can a single client subscribe to multiple different slots?
	-   Be consistent with cluster, and only allow consuming from a single slot at a given time. You can mix the global channels + local channels. <- My preferred option
	-   Allow consuming from multiple slots. This doesn't really break any consistency guarantees and allows for fewer connections. However it seems semantically wrong, since regular cluster forces you not to mix slots.

-   Should subscribing on a replica require the `READONLY` command to be executed:
	-   Yes, since replicas world view can be stale, it seems like we should maintain the assumption. ←My preference
	-   The counter argument is that pubsub has poor guarantees today, so it would unnecessary to require the READONLY command."
883639994,8621,oranagra,2021-07-20T19:29:15Z,"not sure how i agree about these limitations:
> * local channels are not supported in cluster mode disabled.
> * Patterns are not in the initial scope.

regarding open questions:
1. i also feel that we want to only accept local publish in masters, seems that following the existing practice we won't run into new surprises.
2. i agree these are not keys, but we'll need COMMAND command to give some hints to clients one way or another (unless we expect them to handle this command explicitly). maybe with the changes we plan for this mechanism in 7.0 we can show them as keys, but also add some flag indicating that they're not. (some way of still supporting old clients with a good default).
3. i agree that the easy thing is to drop the client when the slot is migrated, it's also easier for the client maybe. but maybe we can add a feature in which the client declares ahead of time if it can handle a slot invalidation message, in which case we send one rather than drop it (more efficient in case multiple channels are used by one client), we can add such a feature in the future (not a must for the first version)
4. i think a single client should be able to subscribe to multiple slots. i also said at the top that i think this feature should be supported in non-cluster mode, i think these two go together.

i feel i'm not knowledgeable enough in these topics, so take my opinion with a grain of salt."
883721797,8621,madolson,2021-07-20T21:31:00Z,"@oranagra 
* I updated to cluster mode being in scope, I suppose the initial thought was that there was no value, but it's also trivial to support, so there is no harm. The name doesn't make as much sense though?
* Nothing explicitly requires patterns, if we feel strongly about that we can do it.
* The cross slot subscribe is the position I hold the least strongly. One thought I've had in the past is that you should be able to send a command like CLIENT ALLOW-MULTI-SLOT, which is just a signal that you are making an optimization to fetch data from multiple slots. In this case, if you want to subscribe to multiple local channels, you just send that command first (similar to READONLY)"
922069361,8621,hpatro,2021-09-17T20:41:27Z,"Few of the considerations made.

* Local channels are allocated to the same node as keys (same hashing logic).
* Client(s) will be redirected to the master node of the slot if they try to connect to a node not serving the slot.
* Client(s) can connect to replica node(s) serving the slot and perform operations. 
* Global channels and local channels share the ACL rule.
* Client(s) will be disconnected on slot migration, need to reconnect again. 
* Client(s) can connect to channels across multi slot.
* Client(s) can connect to both local and global channels together.
* Client(s) would enter `pubsub` mode either on local/global channels subscription.
* During the `pubsub` mode, only certain commands are allowed i.e. `pingCommand`, `subscribeCommand`, `subscribeLocalCommand`, `unsubscribeCommand`, `unsubscribeLocalCommand`, `psubscribeCommand`, `punsubscribeCommand` and `resetCommand`.
* READ_ONLY flag doesn't impact pubsublocal feature. As pubsub operations are neither read/write.
*  No additional operation to be done during slot migration.  Local channels are migrated as part of `CLUSTER SETSLOT <slot> NODE <node>`

@madolson @oranagra Please review the PR. "
963677293,8621,hpatro,2021-11-08T23:44:07Z,"@madolson Thanks for your feedback. I've addressed them, please have a look."
980835330,8621,madolson,2021-11-28T03:53:41Z,@hpatro Can you rebase with the latest changes? I made some minor updates as well.
983808321,8621,hpatro,2021-12-01T16:23:46Z,Will update it tomorrow. 👍🏻
985968499,8621,hpatro,2021-12-04T04:59:44Z,@madolson Updated.
993156637,8621,hpatro,2021-12-14T04:51:55Z,"> LGTM (didn't review the code, just skimmed though some files).
> 
> I suppose subscribing to patterns is useful, specifically if the pattern includes a tag. i.e. `SUBSCRIBELOCAL {tag}-something*` I wouldn't say that it's a must-have (don't know enough of the use cases), but if we're gonna add it, i think we should add it now rather than later.

@oranagra Thanks for the feedback. I feel this could be easily added as a follow up post 7.0. Why do you think we should add it right away? Would love to know your thoughts."
993414462,8621,oranagra,2021-12-14T10:48:09Z,"> @oranagra Thanks for the feedback. I feel this could be easily added as a follow up post 7.0. Why do you think we should add it right away? Would love to know your thoughts.

by ""now"" i meant Redis 7.0 (so we don't end up with backwards compatibility concerns, and troll client libraries with separate changes).

anyway, i don't have a concrete opinion about it, was just responding to @madolson [ask](https://github.com/redis/redis/pull/8621#pullrequestreview-826000518) to state my thoughts."
1003660244,8621,madolson,2022-01-02T03:59:50Z,"The reason this wasn't merged was I found a bug while running cluster tests before merging, the keyspec was off. I patched it and am running tests again. https://github.com/redis/redis/actions/runs/1645404997"
1003810916,8621,madolson,2022-01-03T00:52:37Z,"Tests are all good now, merging: https://github.com/redis/redis/actions/runs/1646797738"
1008263338,8621,oranagra,2022-01-09T09:39:09Z,"can someone look into this CI failure:
https://github.com/redis/redis/runs/4745050797?check_suite_focus=true
```
00:50:22> Disconnect link when send buffer limit reached: FAILED: Expected [get_info_field [::redis::redisHandle1666 cluster info] total_cluster_links_buffer_limit_exceeded] eq 1 (context: type eval line 36 cmd {assert {[get_info_field [$primary1 cluster info] total_cluster_links_buffer_limit_exceeded] eq 1}} proc ::test)
```"
1008463803,8621,madolson,2022-01-10T00:54:44Z,"@oranagra Seems like that isn't related to this issue, seems related to the https://github.com/redis/redis/pull/9774#issuecomment-1008262857"
1013823779,8621,oranagra,2022-01-16T07:10:46Z,"looks like the test hung here (saw it only once, so may be a false report)
https://github.com/redis/redis/runs/4829401183?check_suite_focus=true#step:9:632"
1014072824,8621,madolson,2022-01-17T02:08:42Z,"@hpatro FYI. I'm not aware of any place it's likely to get stuck, let's keep a look out if it happens again."
1014404934,8621,tezc,2022-01-17T11:13:26Z,"Test hung on my fork : https://github.com/tezc/redis/runs/4834585930?check_suite_focus=true#step:9:630
I had some changes on this branch but don't think it effects this test. "
1014421959,8621,hpatro,2022-01-17T11:32:17Z,Taking a look.
1014497331,8621,tezc,2022-01-17T12:56:25Z,"```
00:46:57> Disconnect link when send buffer limit reached: FAILED: Expected [get_info_field [::redis::redisHandle1876 cluster info] total_cluster_links_buffer_limit_exceeded] eq 1 (context: type eval line 36 cmd {assert {[get_info_field [$primary1 cluster info] total_cluster_links_buffer_limit_exceeded] eq 1}} proc ::test)
(Jumping to next unit after error)
```

@oranagra @hpatro Just realized in both failures, previous test case failed. Maybe, this is not related to this PR at all. Maybe it's about https://github.com/redis/redis/pull/9774/ , something is leaking to pubsub test after the failure. `node-timeout` is set to 1 hour in that test, maybe that's why pubsub test hangs? Just fyi."
1021633856,8621,elena-kolevska,2022-01-25T21:39:10Z,"Thanks for the work on this @hpatro! Sorry for commenting on an already closed PR but I just want to point out a small inconsistency between the behaviour of redis-cli for `SUBSCRIBE` and `SSUBSCRIBE`. Namely, when we run `SSUBSCRIBE`  we don't enter [pub sub mode](https://redis.io/topics/rediscli#pubsub-mode) as we do with `SUBSCRIBE`.
Let me know if you'd prefer this in an issue."
1021972998,8621,hpatro,2022-01-26T08:29:40Z,"@elena-kolevska I get the below view after `SSUBSCRIBE`, based on this change https://github.com/redis/redis/blob/unstable/src/redis-cli.c#L1418. Is there anything else expected ?

```
harkrisp@147dda4813f2 redis % src/redis-cli
127.0.0.1:6379> ssubscribe hp
Reading messages... (press Ctrl-C to quit)
1) ""ssubscribe""
2) ""hp""
3) (integer) 1
```"
1022067420,8621,elena-kolevska,2022-01-26T10:29:47Z,"@hpatro I'm sorry, this was a silly (and embarrassing) mistake, I had ran redis-cli from my path by mistake, not from unstable. Works as expected from unstable."
1022172694,8621,hpatro,2022-01-26T12:57:14Z,"@elena-kolevska Ah, makes sense. Np."
953141117,9656,guybe7,2021-10-27T17:17:52Z,"comments from Oran:
1. remove both help.h and commands.c from makefile. document somewhere how to re-gen commands.c
2. in the command json files, i suggest to put the arity, acl, key_specs and all other short metadata before the arguments (put the short ones first and the long ones last) (@itamarhaber FYI - it's better than alphabetical ordering - basically have ""arguments"" as the last field, and sort alphabetically the N-1 first fields)
3. i'd rather use ""resp3"" over just ""3"" in the json files.
4. re-arrange return_types: it should be a list oj objects. each object will have ""summary"", ""type"" (which can be either a string or an object of RESP2 and RESP3) and ""contant_value"" (optional, only for simple string)
5. delete return_summary from jsons
6. split the doc in commands.c - the first half should be next to struct redisCommand.
7. try to avoid using the preprocessor in commands.c
8. turn all non-freetext fields into enum (command.group, arg.type and return_type.type) we will no longer need redisCommandValueArgType (see https://github.com/redis/redis/pull/9656#discussion_r737618303) "
954929504,9656,guybe7,2021-10-29T17:43:32Z,"(note to self: pushed fixes to all above, except (7))"
971725858,9656,yossigo,2021-11-17T16:06:33Z,"@guybe7 I'm addressing the open issues at the top comment:

> another issue is whether we want to provide a full-blown description of the return-type or just the immediate one? e.g. ZPOP returns an array of (member, score) tuples in RESP3 and a flat-map array for RESP2. in the current design we will just state that it returns an ""array"" for both, is that enough?

Because the differences between RESP2 and RESP3 are only semantic, I tend to say it probably makes sense to have a single `returns` element (and description) that refers to both versions, and branch only the `type` part as you listed in the example. If this proves to be not good enough, I'd just consider the entire `returns` element version specific and include an optional `versions` field that lists applicable protocol versions (with default being `versions: [2,3]`). I think we need to choose one of these two approaches but not both.

> we have a few open questions about pure tokens:
should they have a ""type""? usually, type is used to validate some free-test user input
is the token itself be under ""token"" or under ""value""? if it's under ""value"" we can declare it as a mandatory attribute

I think there can be a `type: flag` that indicates there are no user supplied values, and the specified value indicates that flag was set."
974777459,9656,jhelbaum,2021-11-21T08:48:52Z,"@guybe7 

I've started to work on integrating these changes into `redis-cli.c`. So far I see at least two issues:
1. `static char *commandGroups[]` is missing - this is used for outputting command help.
2. `struct redisCommand` is defined in `server.h`. This requires `redis-cli.c` to include `server.h` for the struct definition. Aside from the massive header coupling this introduces (`server.h` desperately needs to be broken up regardless), it's incompatible with `redis-cli.c`, which uses the `hiredis` implementation of `sds.h` instead of the one included from `server.h`. I'm not familiar with the differences between these implementations so I don't know how important this is. But it seems to me a poor design choice to require a client library to include `server.h`. The command-related definitions should be broken out into a separate header file."
981069745,9656,guybe7,2021-11-28T11:36:50Z,"@jhelbaum redis-cli should not depend on any structs/definitions from redis itself.
it should execute COMMAND on startup and build its own structures based in what COMMAND returned"
981070055,9656,jhelbaum,2021-11-28T11:39:40Z,"> redis-cli should not depend on any structs/definitions from redis itself.
> it should execute COMMAND on startup and build its own structures based in what COMMAND returned

Got it - that makes sense. I'll give it another shot.
"
983976104,9656,oranagra,2021-12-01T19:16:23Z,"@redis/core-team please approve. have a look at the top comment for details, as well as the pending redis-doc PR if you want."
993163998,9656,madolson,2021-12-14T05:10:52Z,"@guybe7 👏👏👏 

I don't have any major concerns. The only thing I'm not certain about is the module API. I would almost prefer we pull that out so we can get the rest of this merged and start guiding new PRs to create the new JSON files. 

> Argument's ""value"" and ""name""

Chosen option sounds fine.

> Usage of versions in module API
When a module sets ""since"", should it be the debut Redis version or the module version?

I don't see any other option but to show the module version. 

> Optional args interchangeability

Definitely the f$*-it option."
993349251,9656,oranagra,2021-12-14T09:34:51Z,"We discussed this PR in a core-team meeting.
The **only** thing that's possibly an issue is the module API.
We want to try and invest more time on searching for nicer alternatives (which will be easier for the module authors to use).
One example is maybe expose APIs that take compound structs like the ones in commands.c.
We think the quick way forward is to trim the module API (and tests) from this PR, merge it, and open another one to introduce them.
@guybe7 WDYT?"
994936562,9656,oranagra,2021-12-15T16:08:25Z,Full CI: https://github.com/redis/redis/actions/runs/1583574163
1421157765,11659,madolson,2023-02-07T17:26:57Z,"@oranagra @yossigo We are stuck between discussing two options for the module authentication and how it works with blocking and would like your input. Both options assume that we are creating a new AuthCallback function that is being registered, and it will be executed in either `AUTH` or `HELLO`. 

1. Have the blocking behave like module blocking, where a custom callback is executed when the client is unblocked. E.g.

```
int unblockCallback(RedisModuleCtx *ctx, RedisModuleString *username, RedisModuleString *password, const char **err) {
    user *u = RedisModule_GetBlockedClientPrivateData(ctx);
    RedisModule_AuthenticateClientWithACLUser(user);
    return REDISMODULE_AUTH_HANDLED;
}

int authenticationCallback(RedisModuleCtx *ctx, RedisModuleString *username, RedisModuleString *password, const char **err) {
    /* Block the client from the Module, assuming some background job does the work. */
    RedisModuleBlockedClient *bc = RedisModule_BlockClientOnAuth(ctx, unblockCallback, NULL);
    return REDISMODULE_AUTH_BLOCKED;
}
```

This option is a little bit odd since we are configuring module blocking for a normal command, but is more consistent with out module blocking.

2. Have the blocking behave like the new key based blocking introduced (https://github.com/redis/redis/pull/11012), where the command is reprocessed when the command is unblocked. E.g.

```
int authenticationCallback(RedisModuleCtx *ctx, RedisModuleString *username, RedisModuleString *password, const char **err) {
    user *u = RedisModule_GetBlockedClientPrivateData(ctx);
    if (!u) {
        /* This is the first time the function is called, so block. Note there is no callback declared, since this function is called again. */
        RedisModuleBlockedClient *bc = RedisModule_BlockClientOnAuth(ctx, NULL);
        return REDISMODULE_AUTH_BLOCKED;
     }

    /* We have the user from the blocked client, let's do something about it. */
    RedisModule_AuthenticateClientWithACLUser(user);
    return REDISMODULE_AUTH_HANDLED;
}
```

I think this type of blocking should be included as a new way for modules to block, so we can setup some of the primitives here. Overall I think this is a simpler API for modules to implement against. We just need to maintain a little bit of state for whether or not we have process a given authentication handler. "
1422157192,11659,oranagra,2023-02-08T07:34:41Z,"i haven't thought about this too deeply, and i must say it scares me a bit that we have to block the client and do asynchronous authentication.

looking at your two proposed alternatives, i think i prefer the first one, letting the module control it more closely and explicitly.
unlike key locking, i think this is a lower level API that's ok to be more complicated."
1423107819,11659,madolson,2023-02-08T19:09:17Z,"The question is not about control. The module still has the ability to trigger a given callback or modulate the behavior of the same function. Looking back, I think the way I framed it wasn't actually helpful. The current implementation is basically adding a new type of blocking, which is specific to just authentication, which I would rather not have. The question is more about do we want this to conform more to module blocking, which has two different flows for the initial call and the unblock flow vs the new blocking code, which has the same flow for both blocking and unblocking flows. 

Edit: we discussed more internally and came to a conclusion."
1423112042,11659,madolson,2023-02-08T19:12:56Z,"> i haven't thought about this too deeply, and i must say it scares me a bit that we have to block the client and do asynchronous authentication.

Yes, we need to make sure we do it right."
1444542535,11659,madolson,2023-02-24T21:34:49Z,"@redis/core-team Hey, I think this is in a good enough state to pull folks in for approval. Please see the top level comment for the HLD. There are still some pending comments, but most of them are waiting for general consensus about the design."
1469320307,11659,madolson,2023-03-15T04:42:45Z,Core group conceptually approved. What is pending is to update the naming and implement the new API to make it simple to log the events.
1469466348,11659,oranagra,2023-03-15T07:14:08Z,"Please make sure to update the top comment (to be used as squash-merge commit comment)

p.s. triggered a daily CI for the module tests: https://github.com/redis/redis/actions/runs/4423722686"
1470356676,11659,KarthikSubbarao,2023-03-15T16:28:16Z,"Yes, thanks! I updated the top comment"
945072492,9572,oranagra,2021-10-17T08:26:54Z,"@zuiderkwast i think you should avoid doing `git rebase` and `push -f`, just stick to merge and incremental commits.
it's hard to keep track of what's new and what was already reviewed, and since we're gonna squash-merge this anyway, there's no real need for modifying existing commits."
951762777,9572,oranagra,2021-10-26T09:39:01Z,"@redis/core-team not sure if we need a major decision for this, but since it's a delicate subject, i'd love for you to review."
968296729,9572,oranagra,2021-11-14T14:02:37Z,"@soloestoy please re-review and post if you have an example of a case you think this PR breaks.
considering cases where writable replica only write to temporary keys, i think this PR only fixes things.
Also, FYI: https://github.com/redis/redis/discussions/9760"
968619479,9572,soloestoy,2021-11-15T07:50:35Z,"@oranagra @zuiderkwast  first of all, I agree we should cleanup the functions `lookupKey*()`, but except the change about writable-replica:

```c
    /* Deleting expired keys on a replica makes the replica inconsistent with
     * the master. The reason it's allowed for write commands is to make
     * writable replicas behave consistently. It shall not be used in readonly
     * commands. Modules are accepted so that we don't break old modules. */
    if (expire_on_replica) {
        client *c = server.in_eval ? server.lua_client : server.current_client;
        serverAssert(!c || !c->cmd || (c->cmd->flags & (CMD_WRITE|CMD_MODULE)));
    }
```

IMHO, writable-replica is a bad design, I'm strongly against it(I even wanna deprecate it in 7.0), maybe it's useful in some scenarios if people use redis as cache, but I believe more and more people use redis as database (IIRC redislabs defines redis as a primary database), and data inconsistency is a very serious problem in database.

Back to writable-replica change in this PR, it seems try to fix a ""bug"" about expire, it also confuses me how to define a bug?

1. If we wanna allow write commands delete expired key, then how to deal with read commands?

    Maybe you would say because users can send write commands to writable-replica, and just insert new keys and set expire time on the new keys, but we don't have any way to restrict people to use it in the ""correct way"" we expect.

    OK, if people really use it in right way (only use ""slave keys""), then the read commands also need delete the expired keys.
2. This change let the lazy expire work in write command on all keys, but the active expire only work on the ""slave keys"".

    I don't think it's a good idea that we use different approaches on expire mechanism, the active expire can only delete ""slave expired keys"" but the lazy can delete all (especially ""master"") expired keys.

And I can't understand why we need ""fix"" it, is it an actual bug(I don't see any public issues, in my opinion it's an abuse case)? And I(we) need a uniform standards, to decide what is bug and if it is necessary to fix, or this change cannot convince me.

Bottom in line, my opinion is we should stop any development on writable-replica, just keep it as what it is currently. If we think it's a useful feature we should improve and fix lots of places about it, not only expire (and I think the ""bugfix"" in this PR is not right)."
968768300,9572,oranagra,2021-11-15T10:52:19Z,"@soloestoy generally, i could argue that i don't mind not fixing bugs in writable replicas, but i do wanna clean the code and make it consistent, so that when people read it or add new commands, they know what's the right thing to do (this lookupKey thing was a problem in 6.2).
if while doing that cleanup i'm affecting writable replicas, i don't mind that much since it's a bad feature.
however, since people do use it, i rather not introduce new problems to it or cause it to stop serving their needs, but if such a cleanup is fixing things issues them rather than break things, i'm certainly ok with it.

more to the point, if we consider for a moment that writable replicas are for now valid to be used to create temporary keys on replicas, and that the keys they write to are keys that don't exist on the master (in that case there's really no data inconsistency between the master and replica).
so in that case:
1. same as ZUNION won't let you read from an already expired key, ZUNIONSTORE+ZRANGE shouldn't let you read from it either, so lookupKeyRead should be used for source keys, and it should filter out expired keys, even if the command doesn't arrive from the master. **This is a bug this PR fixes, and IIUC that's not the one you strongly object to, right?**
2. if you consider the destination key a write command e.g. let's say it's a ZUNIONSTORE followed by ZPOP, then the keys we write to should in theory do an expiration test. **This is the change / fix you're objecting to, right? let's discuss it below.**

So for the above case [2], you are correct that we're actually it's missing the point since it doesn't check for the expiration in the right dict. I don't mind improving / fixing it (makes the internal consistency of our code better), but also since this bug was already always there, i don't mind leaving it as is either.
As i said before, this is the part i really hate about writable replicas (the `slaveKeysWithExpire`), and i'm willing to break it, delete it, or leave it broken. i don't want to invest much effort in fixing it.

Please let me know if i'm missing anything, i.e. my technical analysis of the problem is wrong, or are we just holding different opinions on what's the right thing to do."
968880012,9572,soloestoy,2021-11-15T12:51:48Z,">if while doing that cleanup i'm affecting writable replicas, i don't mind that much since it's a bad feature.

I think it's unnecessary to affect writable-replica in this PR.

>So for the above case [2], you are correct that we're actually it's missing the point since it doesn't check for the expiration in the right dict. I don't mind improving / fixing it (makes the internal consistency of our code better), but also since this bug was already always there, i don't mind leaving it as is either.

That's the point I don't understand and I want to argue, this bug was already always there and if you also agree we don't need to fix it, this PR should not affect writable-replica. But why we fix it incompletely in this PR? I think we should revert the change about writable-replica in this PR, or we should check expire not only in write command but also in read command, and the key we check is in the `slaveKeysWithExpire` dict.

>As i said before, this is the part i really hate about writable replicas (the slaveKeysWithExpire), and i'm willing to break it, delete it, or leave it broken. i don't want to invest much effort in fixing it.

I also agree this action.

>Please let me know if i'm missing anything, i.e. my technical analysis of the problem is wrong, or are we just holding different opinions on what's the right thing to do.

My opinion is we should fix bug completely or we don't fix it, I don't know are we holding different opinions？ : )"
968930747,9572,zuiderkwast,2021-11-15T13:52:17Z,"@soloestoy Regarding your suggestion

> we should check expire not only in write command but also in read command, and the key we check is in the `slaveKeysWithExpire` dict

To be clear, in this PR we ""check expire"" in both read and write, but we only delete the key in the write case.

Are you suggesting that we delete the expired key in both read and write cases? Then we have the inconsistency described in this comment: https://github.com/redis/redis/issues/6842#issuecomment-929887180. The example doesn't include slaveKeysWithExpire.

Are you suggesting that we delete the expired key only if it's in the slaveKeysWithExpire dict? If yes, then it means we still return null in lookupKey for expired keys for read and write, even if we don't delete it? Maybe this works... but a lookup for write is always followed by some write operation, so it will behave the same way as if it would be deleted already in lookup. (Correct me if I'm wrong.)"
968953109,9572,soloestoy,2021-11-15T14:16:05Z,"@zuiderkwast sorry I didn't explain it clear, I mean we should delete expired keys only in `slaveKeysWithExpire` dict in both write and read command on writable-replica.

But at the same time I don't like the fix, if writable-replica is a bad design, then the expire management on writable-replica is a horrible design. I prefer much more removing the `slaveKeysWithExpire` code and don't do any expire delete on writable-replica."
969001441,9572,oranagra,2021-11-15T15:06:15Z,"@soloestoy please make a distinction between the following use cases:
1. a writable replica that only writes to temporary keys, and does not use EXPIRE command.
2. a writable replica that writes to keys that also exist on the master.
3. a writable replica that uses EXPIRE on the replica.

The bug fix in lookupKeyRead, which refers to case [1] in this list, and also [1] in my previous post should be considered separately from the change in lookupKeyWrite, and separately from anything involving `slaveKeysWithExpire` IMHO.

I think i wanna insist that the code we have now in lookupKeyRead is ok, and we can argue about what lookupKeyWrite does.

Regarding lookupKeyWrite, i do think it should behave the same as a write command on the master, i.e. if the key already expired, delete it.
if we wanna go the extra mile and check the `slaveKeysWithExpire` dict, we can do that too, but i'm also willing to overlook it."
969032145,9572,oranagra,2021-11-15T15:36:30Z,"for the record, the reason i make this distinction is because:
1. case 1 won't cause any commands that are propagated form the master to fail. i don't feel comfortable to deprecate or break that yet, and in the meanwhile i rather fix it since it improves the code consistency and reasoning.
2. case 2, will cause commands propagated from the master to fail. so i consider it a serious abuse.
3. case 3, may in theory be ok, but it is already very buggy and the code is inconsistent and hard to reason with, so i don't care to fix it."
969079943,9572,zuiderkwast,2021-11-15T16:24:16Z,"I tried changing expireIfNeeded so that it doesn't delete expired keys for write commands. First, an assert in dbAdd fails, but after fixing that, the following tests fail:

```
[err]: SWAPDB awakes blocked client, but the key already expired in tests/unit/type/list.tcl
Expected 'id=70 addr=127.0.0.1:44053 laddr=127.0.0.1:25611 fd=74 name= age=0 idle=0 flags=N db=9 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=20474 argv-mem=0 multi-mem=0 obl=0 oll=0 omem=0 tot-mem=40960 events=r cmd=brpop user=default redir=-1 resp=2
' to match '*flags=b*' (context: type eval line 19 cmd {assert_match ""*flags=b*"" [r client list id $id]} proc ::test)
[err]: Writable replica doesn't return expired keys in tests/integration/replication-3.tcl
Expected '-1' to be equal to '-2' (context: type eval line 16 cmd {assert_equal -1 [r -1 ttl key1] } proc ::test)
[err]: PFCOUNT updates cache on readonly replica in tests/integration/replication-3.tcl
Expected [r dbsize] == 0 (context: type eval line 3 cmd {assert {[r dbsize] == 0}} proc ::test)
[err]: PFCOUNT doesn't use expired key on readonly replica in tests/integration/replication-3.tcl
Expected [r dbsize] == 0 (context: type eval line 3 cmd {assert {[r dbsize] == 0}} proc ::test)
```

Note that these tests are all added in this PR.

It may be possible to work around the fact that the dict entry can still exists even after lookupKeyWrite returns NULL, but it's **much more custom code for writable replicas** in various commands, instead of only one special case for writable replicas in expireIfNeeded."
969086579,9572,zuiderkwast,2021-11-15T16:31:06Z,"I suggest we deprecate writable replicas in 7.0 and remove writable replicas in 8.0. We can't do it faster than this I think. For now, let's merge this PR so we can focus on more important things...?"
969228355,9572,oranagra,2021-11-15T19:09:06Z,"> It may be possible to work around the fact that the dict entry can still exists even after lookupKeyWrite returns NULL, but it's much more custom code for writable replicas in various commands, instead of only one special case for writable replicas in expireIfNeeded.

I really rather not go there, if we do that then the flow on a writable replica will be different than the one on the master, and who knows what that'll lead to (other than leaks), I.e. Considering this part will not be heavily tested. 

We need lookupKeyWrite to delete the key when it returns NULL. I think the current core is ok, and the only thing to improve is the check for expiration in the other dict. 

Personally, I don't feel confident enough to depreciate that feature just yet. "
971392705,9572,soloestoy,2021-11-17T09:25:33Z,">1. a writable replica that only writes to temporary keys, and does not use EXPIRE command.
>2. a writable replica that writes to keys that also exist on the master.
>3. a writable replica that uses EXPIRE on the replica.

To be clear, I'm not talking about the cleanup on `lookupKey*()` function I totally agree with the cleanup, what I'm talking about and against is just only the expire mechanism change on writable-replica, only the case [3].

But this PR not only cleanup the `lookupKey*()` function, but also change the expire mechanism on writable-replica, before this PR:
1. ***read*** expired keys in writable-replica ***doesn't*** delete the key, just return null.
2. ***write*** expired keys in writable-replica ***doesn't*** delete the key, but return the key (which is expired from replica's POV).
3. `databasesCron` only delete ***slave*** expired keys.

OK, case [2] is buggy, this PR try to fix it, so after this PR:
1. ***read*** expired keys in writable-replica ***doesn't*** delete the key, just return null.
2. ***write*** expired keys in writable-replica ***would*** delete the key, and return null.
3. `databasesCron` only delete ***slave*** expired keys.

This PR seems fix the ""bug"", but I think case [1] and case [2] should be consistent with case [3], expire check and only check `slaveKeysWithExpire` dict both in read and write command.

But, I don't like any ""fix"" above, I prefer keeping the status quo, I hate waste time on improving a bad design, even if it just change one line code."
971457099,9572,zuiderkwast,2021-11-17T10:48:46Z,"What's the point of keeping a status quo? The risk that fixing one scenario breaks another scenario? That's a valid point, but keeping a buggy feature without any plan to fix it or to remove it is the worst option.

If we can't provide consistency, we can't support that feature if we want to look like a serious database. I must agree with @soloestoy that ""data inconsistency is a very serious problem in database"".

So, if I revert the ""fix"" and keep only the refactoring, what do we reply in #6842? I think we can solve it by documentation. Or do you prefer to keep status quo also for the documentation? ;-)

I think we need to define exactly which usage of writable replicas we can support and then update documentation to explain that any other usage is very risky and can cause inconsistency. I think it can be very simple: ""Writable replicas should only write to keys that are never used on the master."" (Even the scenario where the replica does expire on a key replicated from master is going to cause inconsistency.)

@oranagra do you have a different documentation idea to support the changes in this PR?

Currently, the [documentation for writable replicas](https://redis.io/topics/replication#read-only-replica) has no mention of inconsistency for writable replicas. It only mentions a problem that was solved in version 4.0:

> So in general mixing writable replicas (previous version 4.0) and keys with TTL is going to create issues.
>
> Redis 4.0 RC3 and greater versions totally solve this problem and now writable replicas are able to evict keys with TTL as masters do, with the exceptions of keys written in DB numbers greater than 63 (but by default Redis instances only have 16 databases)."
971483449,9572,oranagra,2021-11-17T11:23:53Z,"> OK, case [2] is buggy, this PR try to fix it, so after this PR:

I think this bug fix is important since it fixes a bug in the only case in writable-replica that i consider valid (i.e. the one that uses temp keys and won't cause any command from the master to fail).

> This PR seems fix the ""bug"", but I think case [1] and case [2] should be consistent with case [3], expire check and only check slaveKeysWithExpire dict both in read and write command.

we can't do that, this will damage read-only replicas too.
what we can do to consolidate that, is fix the lazy expire to look at slaveKeysWithExpire too.

But as you said, we rather not fix bugs around slaveKeysWithExpire at the moment, so if we didn't introduce new ones (i.e. lazy expire never checked this data dict), then i'm ok leaving it as broken as it was."
971485294,9572,soloestoy,2021-11-17T11:26:25Z,">we can't do that, this will damage read-only replicas too.

No, I mean if this is a writable-replica, then do lazy expire check in `slaveKeysWithExpire`."
971487060,9572,oranagra,2021-11-17T11:28:57Z,"> If we can't provide consistency, we can't support that feature if we want to look like a serious database. I must agree with @soloestoy that ""data inconsistency is a very serious problem in database"".

I think people who use writable replicas don't use redis as a database, and don't expect data consistency. but still we need to remember that redis has many use cases, and caching is one of them, for now not yet comfortable hurting this use case, and i'm willing to introduce minimal changes to make parts of it less broken.

> Currently, the documentation for writable replicas has no mention of inconsistency for writable replicas. It only mentions a problem that was solved in version 4.0:

We certainly need to update the docs, i think they're misleading."
971488521,9572,oranagra,2021-11-17T11:31:06Z,"> > we can't do that, this will damage read-only replicas too.

> No, I mean if this is a writable-replica, then do lazy expire check in slaveKeysWithExpire.

you mean only in slaveKeysWithExpire or in both?
i'm ok to check both (fixing an additional bug), i was under the impression you're advocating to avoid any bug fix in that area.
but if you're ok with fixing the two bugs (better internal consistency in reids), i'm fine with that (since the change will only add some 2 lines to the code, no real complications)."
971490079,9572,soloestoy,2021-11-17T11:33:21Z," >I think people who use writable replicas don't use redis as a database, and don't expect data consistency. but still we need to remember that redis has many use cases, and caching is one of them, for now not yet comfortable hurting this use case, and i'm willing to introduce minimal changes to make parts of it less broken.

I have different opinion, I think redis is a database at first, and then because redis is ultra-fast so it can be used as cache, cache or database just depends on users, mysql also can be used as cache, but it's too slow so nobody use it as cache ; )"
971492177,9572,soloestoy,2021-11-17T11:36:30Z,">i'm ok to check both (fixing an additional bug), i was under the impression you're advocating to avoid any bug fix in that area.
but if you're ok with fixing the two bugs (better internal consistency in reids), i'm fine with that (since the change will only add some 2 lines to the code, no real complications).

seems you misunderstand me 😅 , I always believe we need fix bug completely or do nothing, and I prefer do nothing in the writable-replica case, but I'm ok with fix it completely.

And, I still believe we should remove the writable-replica feature finally, all the use cases are abuse from my POV, I think writable-replica is a bug, at the beginning of redis born, redis doesn't have any protection on slave, I don't know why salvatore didn't fix it, but instead he develop it as a feature."
971510591,9572,oranagra,2021-11-17T11:57:16Z,"Ok, we'll keep considering to remove it in the future, but for now, let's fix lazy-expire to check slaveKeysWithExpire too.
@zuiderkwast please go head.

but let's keep this cleanup effort is limited to lazy expire and lookupkey, i do not think we wanna handle other problems with slaveKeysWithExpire at the moment (like what happens on replica promotion), where the solution is not straight forward (both clear what should be done, and very easy to implement)."
971563284,9572,zuiderkwast,2021-11-17T13:08:51Z,"> for now, let's fix lazy-expire to check slaveKeysWithExpire too. @zuiderkwast please go head.

To confirm what to do: In lookup for write, we delete the expired key only if it's in slaveKeysWithExpire. Otherwise we return the expired key?

That means that we don't fix the example in #6842. The test cases in this PR will have to be changed so they instead test the slaveKeysWithExpire feature instead of the #6842 issue. Is this what you want?"
971594299,9572,oranagra,2021-11-17T13:41:56Z,"@zuiderkwast i meant to check both the `expires` and `slaveKeysWithExpire`
if the key is expired in any of them, act the same.
i.e. obviously on a writable replica if the key you write to is logically expired in `slaveKeysWithExpire`, it should be lazy deleted (p.s. this is another bugfix to mention in the top comment).
but also if the key is logically expired in the `expires` dict, in lookupKeyWrite on a writable replica (if the command doesn't arrive from our master), we wanna delete it (like we do in lookupKeyWrite on the master itself)"
971690624,9572,zuiderkwast,2021-11-17T15:28:39Z,"> check both the expires and slaveKeysWithExpire
> if the key is expired in any of them, act the same.

@oranagra This doesn't make sense. `slaveKeysWithExpire` doesn't contain any timestamp.

```
 * The dictionary has an SDS string representing the key as the hash table
 * key, while the value is a 64 bit unsigned integer with the bits corresponding
 * to the DB where the keys may exist set to 1.
```"
971707248,9572,oranagra,2021-11-17T15:46:18Z,"ohh, i'm sorry.. (responding from memory without checking the details).
so EXPIRE on a writable replica also sets the expiration in the main expires dict.
There is no problem then with doing lazy expire like we already do in this PR.

The reason the `slaveKeysWithExpire` exists is so that the active expire that runs on the replica, will **not** look at keys that where **not** explicitly set EXPIRE on the writable replica.
But the lazy expire on lookupKeyWrite is still ok to delete both types (the ones that are in slaveKeysWithExpire and the ones that aren't).

maybe we can modify lookupKeyRead to also do that (do lazy expiry), but only if the key is in slaveKeysWithExpire.
i.e. expireIfNeeded will do the deletion in either of these cases:
1. we're on the master
2. we're writing to a key on a replica
3. we're reading form a key on a replica and the key is in slaveKeysWithExpire"
971751829,9572,zuiderkwast,2021-11-17T16:35:02Z,"@oranagra OK, this makes sense, thx. @soloestoy are you OK with this suggestion?

Btw, I'd like that we define what exactly is supposed to work. The concept of ""more consistent"" or ""less consistent"" is very vague. Maybe we can do it (and update docs) based off this PR when it's accepted."
972461880,9572,soloestoy,2021-11-18T02:35:51Z,">Btw, I'd like that we define what exactly is supposed to work. The concept of ""more consistent"" or ""less consistent"" is very vague. Maybe we can do it (and update docs) based off this PR when it's accepted.

I think it's better to open a new PR to discuss the expire mechanism on writable-replica (but need revert the expire change in this PR first)."
972620616,9572,oranagra,2021-11-18T07:55:44Z,@soloestoy what's wrong with the current [plan](https://github.com/redis/redis/pull/9572#issuecomment-971707248) ?
973843091,9572,soloestoy,2021-11-19T07:59:38Z,">@soloestoy what's wrong with the current plan ?

nothing, I just worry this PR may become more and more huge due to the writable-replica (another question is if we need remove the key from `slaveKeysWithExpire` when delete it in `expireIfNeeded`), if you think it's not a problem, just move on."
974138588,9572,oranagra,2021-11-19T14:52:55Z,"I rather move on and do it all in this PR, it's hard for me to think off changing parts of it (mess up the lookupKey without affecting writable replica at all), rather than think of where we wanna get at the end, and look at the code to see if it does it. 
Also, I won't think this PR is gonna be huge. 

And yes, I think we should delete the key from slaveKeysWithExpire when it gets deleted for whatever reason. (didn't check if that currently happens, and I'm not sure i care much, since that feature is already quite broken). 

@zuiderkwast please go ahead and implement what was discussed. "
974157635,9572,zuiderkwast,2021-11-19T15:15:29Z,"> And yes, I think we should delete the key from slaveKeysWithExpire when it gets deleted for whatever reason. (didn't check if that currently happens, and I'm not sure i care much, since that feature is already quite broken).

Keys in slaveKeysWithExpire are only deleted in expireSlaveKeys() which is part of active expire. They're not deleted by e.g. the DEL command. To be able to delete them in e.g. lookupKey() or expireIfNeeded(), I would need to refactor the expireSlaveKeys() function a bit to break that part out."
974173196,9572,oranagra,2021-11-19T15:33:31Z,"Seems very much broken to me, or maybe I don't understand the design of that feature (considering I do much of the correspondence on this subject from Android, that may very well be). 
I feel we better not touch it then (don't remove keys from that dict) "
974768368,9572,oranagra,2021-11-21T07:25:49Z,"i finally bothered to allocate some time and take a proper look at the code.
since each entry in `slaveKeysWithExpire` may represent multiple databases, we can't really afford to delete keys from there in lazy expire (easily). to do so, we'll need to update the database id bit mask each time, until we see this was the last bit that's set.
Instead, the design is that active expire (`expireSlaveKeys`) will do all that cleanup and it's not necessary for lazy expire or DEL command to do that, and there's no harm (other than maybe performance) in skipping it when the key is deleted.
Of course we can improve, but i don't think we should bother.

Also, since the volatile keys in slaveKeysWithExpire are actually also set the TTL in the normal `db.expires` dict, lazy expire is (and was always) actually working on these fine (i.e. as far as the indication to the caller goes), the only other reason to have lazy expire look at slaveKeysWithExpire is in order to delete the key (not the entry in slaveKeysWithExpire dict), which i think we should do at least in lookupKeyWrite, so that the command will be processed the same way it is processed in the master, but for that concern we should delete the key even if it's not in slaveKeysWithExpire.

i.e. for lookupKeyRead, it is normal to return an indication that they key is missing even if it wasn't delete, but for LookupKeyWrite, returning an indication that the key is missing without actually deleting it, could cause the command to later fail (when it calls setKey or dbAdd), and that's true regardless of whether or not the key was in slaveKeysWithExpire.

so bottom line, looking at the current code of this PR, we already do all of that, and i don't think anything needs to be changed."
975182961,9572,soloestoy,2021-11-22T06:56:07Z,"one scenario is:

1. insert a temp key with ttl in a writable replica, like `SETEX FOO 10 BAR`
2. call del command to delete it, `DEL FOO`, but the key `foo` is still in `slaveKeysWithExpire`
3. then insert a same name key with ttl in master, and propagate to the writable replica, like `SETEX FOO 11 BAR2`
4. call `PERSIST FOO` in master before the key foo become expired
5. before the writable replica receive the `PERSIST FOO` command, it check `slaveKeysWithExpire` and find key `foo` is expired (replication may delay) and delete it

I'm also tired of discussing about writable-replica feature(it's bug, not feature IMHO), fix a bug completely or do nothing is my working style, if you think it's not a problem or it's a problem but just fix part of it is enough, just go ahead, I can remove my `request changes`, but I feel shame we keep ugly design in redis."
975223060,9572,oranagra,2021-11-22T08:03:05Z,"The scenario you described is a (known) limitation of this `slaveKeysWithExpire` mechanism, and it was like that before and we don't change it..
The limitation is that the temporary keys that the writable replica works with, should never be used in the master (these are key names that should be reserved only as temporary keys in the writable replica). 
This PR doesn't change it, and i agree we shouldn't fix it or worry about it (bad feature and a pile of bugs).

I agree with you that this writable replica feature is an issue (causing inconsistencies), and especially the `slaveKeysWithExpire` mechanism which is very buggy and incomplete).
But i don't feel we can trim this feature yet, and we both agree that we shouldn't attempt to fix bugs in it.

The changes in this PR don't aim to fix bugs in this mechanism (`slaveKeysWithExpire`), just clean up the code and make it consistent and easy to work with.
Along they way they do affect this feature slightly (the effect is a small improvement, not a degradation), so i don't see why reject them.

If we look at the only arguably valid use case for writable replicas (ones that only write to temporary keys, and never use EXPIRE), this PR fixes some bugs, which are:
1. avoid exposing data that is already logically expired.
2. if lookup returns NULL, delete the key, so the command will be executed similarly as it would on the master, and not crash on some error."
975251154,9572,soloestoy,2021-11-22T08:25:59Z,">Along they way they do affect this feature slightly (the effect is a small improvement, not a degradation), so i don't see why reject them.

the reason I believe is, improve the writable-replica can give the users who are abusing it more excuses to continue abuse it.

And to make it clear, I mean (in my opinion) all use cases with writable-replica are abuse, there is no reasonable use case on writable-replica. It's ok we have different opinions, so just go head."
978897154,9572,oranagra,2021-11-25T07:13:27Z,"@zuiderkwast it occurred to me that maybe we wanna add a test for a case a replica attempts to write to a key that's already logically expired. 
maybe both INCR (uses dbAdd), and SUNIONSTORE (uses setKey). just to be sure there are no crashes.

do you mind adding that?"
979033018,9572,zuiderkwast,2021-11-25T09:40:23Z,"> @zuiderkwast it occurred to me that maybe we wanna add a test for a case a replica attempts to write to a key that's already logically expired. maybe both INCR (uses dbAdd), and SUNIONSTORE (uses setKey). just to be sure there are no crashes.
> 
> do you mind adding that?

We already have [Writable replica doesn't return expired keys](https://github.com/redis/redis/pull/9572/files#diff-a3f90baade34055af62824e804cd6b2ed6fc2119d98e68fd1f8636cca2dfb8b8R68) which does this for INCR.

setKey is using dbAdd or dbOverwrite and uses lookupKeyWrite to find out if it exists or not, just like most commands do, so I see no reason to add tests specifically for this unless we add this kind of test to all commands. (Obviously setKey might crash when you pass the wrong flag `SETKEY_ALREADY_EXIST` or `SETKEY_DOESNT_EXIST` mismatching the actual existence, but it might crash regardless of this PR.)"
979036562,9572,zuiderkwast,2021-11-25T09:43:27Z,"There are some unstable test cases here though, because of race conditions with expire I think. This build failed yesterday: https://github.com/zuiderkwast/redis/runs/4314056519?check_suite_focus=true

On a master, there is no way to check that a key is expired without deleting it. Should I add a few milliseconds extra sleep to extra sure the key is expired?"
979057289,9572,oranagra,2021-11-25T10:10:38Z,"there is a way to check if the key exists without deleting it (DEBUG OBJECT).
but i don't see why there's a race...
the Tcl code has `after 100` so we know at least 100ms passed since we got the reply from PEXPIRE.
this means that when we execute SWAPDB, the check in it that tests the expiration time will surely find that it already expired.
maybe there's an off by one issue, i.e. `> 100` vs `>= 100`, so when the test is super fast is fails.
if that's the case, we can sleep for 101, but i don't see any other explanation.
please look into it and then we'll merge."
2587471562,13740,CLAassistant,2025-01-13T15:42:43Z,[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/redis/redis?pullRequest=13740) <br/>All committers have signed the CLA.
2615150182,13740,MeirShpilraien,2025-01-27T08:46:57Z,"> after ACL, i think we would better not leave the plaintext password on redis side, of course, masterauth must be plaintext, but this has also been complained about by users, so is it possible that we don't persist internal secret which has super permission?

We can decide not to persist, persisting it just help us to converge quicker. @oranagra @yossigo WDYT?"
2615288618,13740,oranagra,2025-01-27T09:51:24Z,"i'm not sure i know enough about cluster to decide, i suppose it's safer not to persist it, and also maybe it helps with downgrades?

i suppose that if we avoid persisting it, we do that so that if someone gains access to the disk, or a copy of the config, he's getting ""root"" permissions, but on the other hand, if they get permissions to connect to the cluster bus, they can get it from there too.
still, i suppose it's safer, and still acceptable behavior after restarts."
2615291754,13740,oranagra,2025-01-27T09:52:45Z,"btw, it's a shame we're mixing these topics, we should move this discussion to #13763"
2615306566,13740,MeirShpilraien,2025-01-27T09:59:16Z,"> btw, it's a shame we're mixing these topics, we should move this discussion to https://github.com/redis/redis/pull/13763

Yes I agree gone copy the comment and continue the discussion there."
2616875588,13740,raz-mon,2025-01-27T20:56:44Z,"As per a discussion with @oranagra and @MeirShpilraien, we decided to remove the internal commands from the monitor created by a non-internal connection, while for internal connections we will display the internal commands.
This was added to the PR.

Note that we do not display the internal commands in all `COMMAND <subcommand>` commands for non-internal connections as well."
2618042137,13740,oranagra,2025-01-28T06:33:40Z,"@raz-mon please revert the changes of #13763 from this PR, so it's easier to review and focus on the internal commands mechanism."
2618053047,13740,raz-mon,2025-01-28T06:42:56Z,"@oranagra My changes rely on these changes, I can use a static password but then I will need to make changes after the review which is not ideal.
The changes of https://github.com/redis/redis/pull/13763 are quite contained in the cluster files, if it possible to ignore them until https://github.com/redis/redis/pull/13763 is merged that would be best. If you think it will take a long time please let me know and I'll revert the changes for this review, but anyways this PR can be merged only after https://github.com/redis/redis/pull/13763 is merged and used here."
2618376805,13740,oranagra,2025-01-28T09:11:19Z,"personally, i think a temporary hard coded password is better for review.
and then the other PR is merged first, and this one is updated."
2618635859,13740,raz-mon,2025-01-28T10:46:02Z,"OK @oranagra, will update the PR soon."
2624094336,13740,oranagra,2025-01-30T10:27:26Z,@YaacovHazan please ACK about the interface changes listed in the top comment.
2624107498,13740,raz-mon,2025-01-30T10:33:03Z,">can you get someone else to review the tests?

@ShooterIT @MeirShpilraien?"
989741920,9872,oranagra,2021-12-09T10:57:18Z,"@zuiderkwast it occurred to me that there's probably an opportunity for some semi-related cleanup.
when we get SIGTERM during loading, we immediately `exit`.
IIRC this code was written before prepareForShutdown had the NOSAVE option (which was added for the SHUTDOWN command).
Now that it has it, i think it's better to go though prepareForShutdown, and pass the appropriate flags (NOSAVE / FORCE).
There are no should be no fork children at that time, but it's still a good idea to go though other cleanup steps (e.g. modules).

while we are no the subject, i did notice that we're terminating the AOF child only if AOF is enabled, but currently it is also possible to do BGREWRITEAOF when it's disabled (see #9794), so i think the child should be stopped.
So, if you're already working in that area, we can make additional cleanups."
994405196,9872,oranagra,2021-12-15T07:10:03Z,Full daily CI: https://github.com/redis/redis/actions/runs/1581569033
996156270,9872,oranagra,2021-12-16T20:05:16Z,"@zuiderkwast unstable now contains shutdown.json, which this PR needs to modify.
the two things to modify are to provide details on the new arguments (FORCE, and NOW).
and also describe the complexity which was missing till now, but i think now is the chance to describe it.
i.e. if we end up doing foreground save, then it's the same complexity of the SAVE command."
998529519,9872,madolson,2021-12-21T07:13:56Z,"Approval is for the high level design, I didn't look at the details."
1003040369,9872,zuiderkwast,2021-12-30T13:58:10Z,"@redis/core-team May I have your attention again? Two more things added in this PR:

* Client pause per purpose (failover, shutdown, client pause command)
* SHUTDOWN ABORT"
1003677040,9872,oranagra,2022-01-02T07:51:39Z,"@zuiderkwast thank you.
I suppose there are some docs that will benefit from an update about this change.
obviously the SHUTDOWN command (which must refer to the new config, and new args), but maybe some other places too?"
1003772766,9872,zuiderkwast,2022-01-02T20:46:52Z,"@oranagra Thank YOU! Yes, I have updated the page about signal handling. It's merged already in redis/redis-doc#1711."
1089528659,10536,madolson,2022-04-05T23:39:42Z,"@PingXie Given that we are after the code cut-off for Redis 7, we probably won't include this, will probably merge this after 7 goes GA."
1089539657,10536,PingXie,2022-04-05T23:52:18Z,"@madolson I see strong value in getting this change rolled out together with `CLUSTER SHARDS` as it helps complete the shard scenario end 2 end. Additionally, it has externally visible impact in areas like `CLUSTER NODES`, `CLUSTER SHARDS`, and nodes.conf, which are already updated in 7.0 so it will be great if we can piggy back on the same release (as opposed to churning these areas release after release). I will close the remaining two issues today (the missing tests and a way to return own shard_id). I am happy to prioritize this fix for 7.0 GA on my end.

@oranagra FYI"
1089784398,10536,madolson,2022-04-06T04:08:17Z,"@PingXie I see a lot of your points, and I agree with most of them. I have two concerns. Let's focus on the implementation, if it's ready we can of course make the decision to merge it earlier.
1. The replica should follow the shard ID of the primary, and reconcile itself automatically.
2. You should be able to enforce a shard id, since I don't think shard ids should rotate if all nodes in a shard die."
1089822179,10536,PingXie,2022-04-06T05:00:43Z,"> @PingXie I see a lot of your points, and I agree with most of them. I have two concerns. Let's focus on the implementation, if it's ready we can of course make the decision to merge it earlier.

Sounds good @madolson.

> 1. The replica should follow the shard ID of the primary, and reconcile itself automatically.

Yes, replicas will pick up their primary's shard id upon joining the shard via `CLUSTER REPLICATE`

> 2. You should be able to enforce a shard id, since I don't think shard ids should rotate if all nodes in a shard die.

Shard ids are persisted in nodes.conf. Nodes should be able to retrieve their previous shard ids on restart, assuming their nodes.conf are not corrupted. In the case where replicas lost their nodes.conf, they can still recover their shard ids via the gossip message from their primary, if the primary's nodes.conf is still good. If, for whatever reason, all nodes.conf files in the same shard are lost, we will rotate to a new shard id.

I added a new command `CLUSTER MYSHARDID` to expose a node's shard id, similar to `CLUSTER MYID`. The reason for a new command is to reduce backcompat risk but let me know if you think otherwise. 

I can update the documentation after we close this PR.

"
1098286982,10536,PingXie,2022-04-13T17:06:35Z,"@madolson I pushed another commit to make `CLUSTER SHARDS` work better with failed primaries and this should make the shard id support complete. 

Below are the externally visible changes (hence my preference to include them along with `CLUSTER SHARDS` in a major release).

* Nodes.conf - added a new field in the end point column *before* hostname
```
208c9c887b8d3c148ca3b19f6585748816be54cc 127.0.0.1:30002@40002,5ec50a13c34fc1d4db518770c34c04d364b19593,hostname-1 master - 0 1649868449278 2 connected 5461-10922
```
1. This is not the most logical place but it seems to me the only extensible column with minimum backcompat risk
2. With this change, since `shard-id` is always on, I moved it ahead of `hostname`. I am assuming this incompatible change is OK as we are still in the RC phase but let me know if you have concerns

* `CLUSTER SHARDS` has a new `shard-id` row
```
1) ""shard-id""
   2) ""5ec50a13c34fc1d4db518770c34c04d364b19593""
```

* `CLUSTER SHARDS`  now groups failed primaries with their old replicas/new primary in the same shard; previously they are separated after the failover
```
2) 1) ""shard-id""
   2) ""9eac95730a6d80fdaac65bfbea09b74647677171""
   3) ""slots""
   4) 1) ""0""
      2) ""5460""
   5) ""nodes""
   6) 1)  1) ""id""
          2) ""f8e798e4ea98bb2339c1b07237aab93121b25664""
          3) ""port""
          4) (integer) 30001
          5) ""ip""
          6) ""127.0.0.1""
          7) ""endpoint""
          8) ""127.0.0.1""
          9) ""hostname""
         10) """"
         11) ""role""
         12) ""master""
         13) ""replication-offset""
         14) (integer) 602
         15) ""health""
         16) ""fail""
      2)  1) ""id""
          2) ""d0595e25f43a6242482f7380b8039af5a88f4adb""
          3) ""port""
          4) (integer) 30005
          5) ""ip""
          6) ""127.0.0.1""
          7) ""endpoint""
          8) ""127.0.0.1""
          9) ""hostname""
         10) """"
         11) ""role""
         12) ""master""
         13) ""replication-offset""
         14) (integer) 602
         15) ""health""
         16) ""online""
```

@zuiderkwast in case you are interested - this is to improve the native shard support started with `CLUSTER SHARDS`"
1099445527,10536,PingXie,2022-04-14T17:28:05Z,"> Regarding the form `ip:port@cport,shard,hostname` (which is already a pretty strange format without shard), I have two incompatible suggestions:
> 
> 1. Since every part so far uses a different separator (`:@,`), we could pick yet another separator for shard-id, for example `$` (looks like S for shard). Then, we could be able to parse these strings from 7.0 release candidates too (with hostname, without shard), so the format would be `ip:port@cport[$shard][,hostname]`.
> 2. Now might also be a good time to make this format future-proof in some way, so that more fields can be added without breaking old nodes and clients. For example, add an URI query string in the end `?shard=s98s98dus98du&something=foo&bla=bla` where unknown future keys can be safely ignored.
> 
> WDYT?

Great point on future-proofing the format now, especially your 2nd point. It is going to be super hard to introduce new fields in the future if we don't come up with a systematic solution in 7.0, even with just the hostname field. Mulling over your suggestion now ..."
1099765509,10536,PingXie,2022-04-15T01:12:41Z,"@zuiderkwast I think JSON would be the ideal long term solution. A potential migration/upgrade strategy would be to attempting to load `nodes.json` first and on `not found` fall back to loading `nodes.conf` and immediately writing out `nodes.json`.  This is IMO better than hacking `nodes.conf` further. So one option could be to stick with `ip:port@cport,shard[,hostname]` in 7.0 and then make the JSON switch in 7.2. Thoughts?

Here is an example of nodes.json for illustration:

```json
{
    ""nodes"": {
        ""node"": {
            ""id"": ""fbc2329fc9e9f2c9f9ed9e6cdcee656df65d191c"",
            ""ip"": ""127.0.0.1"",
            ""port"": ""30001"",
            ""cluster-port"": ""40001"",
            ""hostname"": ""hostname-1"",
            ""role"": ""master"",
            ""shard-id"": ""11829057fd6af31adeb6db3ce43464e51defe0fd"",
            ""slots"": [{
                    ""start"": 0,
                    ""end"": 10
                },
                {
                    ""start"": 10,
                    ""end"": 20
                },
                {
                    ""start"": 30,
                    ""end"": 40
                }
            ],
            ""importing"": [{
                    ""slot"": 9,
                    ""from"": ""e6977cc6c45110a05f3e862717b1120b30589498""
                },
                {
                    ""slot"": 15,
                    ""from"": ""45110a05f3e862717b1120b30589498e6977cc6c""
                }
            ],
            ""migrating"": [{
                    ""slot"": 12,
                    ""to"": ""e6977cc6c45110a05f3e862717b1120b30589498""
                },
                {
                    ""slot"": 34,
                    ""to"": ""45110a05f3e862717b1120b30589498e6977cc6c""
                }
            ]
        }
    }
}
```

Update - I think an in-place update scheme that reuses the same file name `nodes.conf` for the JSON content would work fine too. "
1111778369,10536,PingXie,2022-04-28T05:58:06Z,"@madolson this change is ready. I moved shard-id behind hostname so it should be compatible with 7.0 GA. Can you please take a look?

Pre 7.0:  `ip:port[@cport]` 
7.0:  `ip:port[@cport]`  or  `ip:port[@cport],hostname` 
7.0 + shard-id:  `ip:port[@cport],,shard_id`  or  `ip:port[@cport],hostname,shard_id`
"
1114552279,10536,PingXie,2022-05-02T06:57:32Z,FYI @yossigo @oranagra - in case @madolson's bandwidth is limited recently. This PR brings native shard-id to 7.0.0 in a fully compatible way. It is ready for code review. Please feel free to let me know if there is anything that I can help to make some progress on this PR.
1129601213,10536,PingXie,2022-05-18T05:59:49Z,@madolson can you please review this change when you get a chance? Thanks!
1146478507,10536,madolson,2022-06-04T00:35:36Z,"@PingXie Ok!

I don't know about json, we don't have a JSON parser today in Redis and I don't think we really want to take a dependency on one. I think there are easier and more extensible ways to improve node.conf without moving to JSON though. In another PR I proposed having key/value fields both the slots data."
1150684893,10536,PingXie,2022-06-09T05:26:22Z,"> I don't know about json, we don't have a JSON parser today in Redis and I don't think we really want to take a dependency on one. I think there are easier and more extensible ways to improve node.conf without moving to JSON though. In another PR I proposed having key/value fields both the slots data.

My primary point is to show that there is a way forward. I don't have a strong opinion that we should use JSON. Can you point me to your other PR where key/value fields were proposed? The idea sounds reasonable to me."
1160928838,10536,madolson,2022-06-20T22:59:57Z,"> My primary point is to show that there is a way forward. I don't have a strong opinion that we should use JSON. Can you point me to your other PR where key/value fields were proposed? The idea sounds reasonable to me.

It was discussed here: https://github.com/redis/redis/pull/9564. It still uses the older convention though, one of the two PRs can implement it and we can cross port it.

"
1161235780,10536,madolson,2022-06-21T04:05:03Z,"@ushachar Will you take a look at this since we presumably want the same ""shard ID"" to be the same between cluster v2 and cluster v1."
1164977371,10536,ushachar,2022-06-23T22:32:52Z,"> @ushachar Will you take a look at this since we presumably want the same ""shard ID"" to be the same between cluster v2 and cluster v1.

IDs in flotilla are monotonically increasing numbers (padded in order to preserve compatibility with Cluster v1), but the general concept of this PR does seem to align well with our plans there."
1168182709,10536,madolson,2022-06-28T03:40:51Z,"@ushachar One of the assumptions being discussed here is that ShardIDs may not necessarily be monotonically increasing, and externally imposed."
1173167515,10536,PingXie,2022-07-03T20:36:22Z,"> @ushachar One of the assumptions being discussed here is that ShardIDs may not necessarily be monotonically increasing, and externally imposed.

I commented on the ""monotonically increasing"" property on @ushachar's cluster V2 spec."
1173854154,10536,ushachar,2022-07-04T13:58:24Z,"@madolson / @PingXie  - Externally imposed is fine - the IDs allocated by Flotilla will still be monotonically increasing (internally, we maintain a counter and verify that the value is indeed unused before returning it)"
1223503850,10536,madolson,2022-08-23T03:58:54Z,"On the other thread we decided to go with the aux format:
```
823ca5eb86404530a2cd1f6beee7ed9c00e786fb 127.0.0.1:30001@40001,host-name master aux1:val1 aux2:val2 aux3:val3 ... - 0 1656441238093 1 connected 0-5460
```"
1223504687,10536,madolson,2022-08-23T04:00:32Z,"The only other change is unit tests can now be written for cluster features so they run in the ci, in tests/unit/cluster/*, so all of your changes need to get moves to those files now. Other than that I think we should be able to wrap this up quickly and merge."
1237608277,10536,PingXie,2022-09-06T03:06:23Z,"> The only other change is unit tests can now be written for cluster features so they run in the ci, in tests/unit/cluster/*, so all of your changes need to get moves to those files now. Other than that I think we should be able to wrap this up quickly and merge.

Sounds good. Will move the unit tests to tests/unit/cluster/* next."
1237611817,10536,PingXie,2022-09-06T03:13:38Z,@hwware @zuiderkwast the shard_id change is ready for your review. This PR includes the auxiliary field support as we agreed upon in PR https://github.com/redis/redis/pull/9564 (see https://github.com/redis/redis/pull/9564#discussion_r956177431). I also refactored the logic that builds various PING extensions including both HOSTNAME and FORGOTTEN_NODE.  Please ignore PR #11239 (same payload but wrong branch)
1241535699,10536,PingXie,2022-09-09T06:05:24Z,"> > The only other change is unit tests can now be written for cluster features so they run in the ci, in tests/unit/cluster/*, so all of your changes need to get moves to those files now. Other than that I think we should be able to wrap this up quickly and merge.
> 
> Sounds good. Will move the unit tests to tests/unit/cluster/* next.

This test conversion is harder than I thought. @madolson I don't want to block your code review on the test code refactoring. I can take your feedback on other parts of the changes while working through the test issues."
1242828284,10536,madolson,2022-09-11T00:48:42Z,"> This test conversion is harder than I thought. @madolson I don't want to block your code review on the test code refactoring. I can take your feedback on other parts of the changes while working through the test issues.

Sorry :). Once we get all the tests converted life will be easier, but for now it's a bit of a pain since the two frameworks don't have all the same functions."
1246107780,10536,PingXie,2022-09-14T01:08:09Z,">making sure replicas follow primaries correctly during slot migrations. We'll also be able to support the ""empty"" primary with replicas use case.

Thanks @madolson! I have follow-up PRs to address both callouts (replicas following primaries and empty primaries) in the pipeline. 

PS, the feature gap between the old and new test infra is indeed the blocker. Still trying to find a way to create a new Redis node and control how it is introduced to the cluster. I will continue the refactoring work but I am also open to moving the test refactoring work to another PR and unblocking other PRs that depend on the aux field change.
"
1246131755,10536,madolson,2022-09-14T01:54:55Z,"@PingXie I'm fine punting it out of this PR to unblock as you mentioned. let me know if i can help, since I am the major one pushing for the refractor. "
1255199670,10536,madolson,2022-09-22T15:33:14Z,"Core group approved, but waiting on @yossigo to take a look before merging."
1256800735,10536,PingXie,2022-09-23T23:45:11Z,"> I think CLUSTER SHARDS is a client-oriented command (as opposed to an administrative command), and I don't see clients' value in getting a shard-id. It may not do much harm, but I think it's an unnecessary long-term commitment to an implementation detail.

I am certainly biased here but here are a few thoughts of mine:

1. I think `CLUSTER SHARDS` is a very useful admin command as well and more so than `CLUSTER NODES` thanks to its native RESP format;
2. Identity is an integral part of any ""objects"" so having a `CLUSTER SHARDS` command that outputs a list of shards without their identities leaves a cognitive hole IMO;
3. Just like how we spec'ed out the shard id format and its properties in cluster V2, the V1 shard id along with its format choice (40 bytes) is not an implementation detail IMHO;
4. In the future, we could introduce commands that take shard ids as parameter, such as [`CLUSTER MIGRATE SLOT <n> SHARD <shard-id>`](https://github.com/redis/redis/issues/2807#issuecomment-1161374995). So being able to retrieve the shard-id easily using a RESP parser is valuable;
5. This is just a minor point, we currently print out the node id in `CLUSTER SHARDS` too.

@zuiderkwast curious to hear your thoughts on whether to include shard-id in the `CLUSTER SHARDS` output.
"
1257057383,10536,zuiderkwast,2022-09-24T20:13:07Z,I agree with Ping here. Good points.
1257192190,10536,madolson,2022-09-25T13:15:28Z,"@PingXie Some counter points:
1. I actually think `CLUSTER SHARDS` is a *bad* Admin command. Why? It doesn't expose all of the internal state of cluster nodes. We ideally should also include additional metadata such ongoing data migrations. The intention, or at least the one I had, for `CLUSTER SHARDS` was that it would be a better replacement for `CLUTSER SLOTS`. While talking with Yossi, we thought that maybe we should introduce a new admin focused command to replace `CLUSTER NODES`. I will advocate for that here. 
2. Kind of, but going back to the idea that this is primarily for clients. What will ""clients"" do with this mapping? Nothing, they don't actually care. 
3. Sure? I'm fine stamping it as part of the spec here anyways since I'm not arguing against exposing it in `CLUSTER MYSHARDID`.
4. See recommendation on 1.
5. Clients should use node ID to understand continuity. When they see a node_id in a new shard, they should understand that it was moved to that shard and tear down all relevant connections to the old node_id.  

Most of this presumes that we should build a new command `CLUSTER ADMININFO` or something, that is an administration command that shows administration info. EDIT: I also want to provide a pretty crisp boundary between the ""admin"" command and the ""client"" commands, since it's usually two different groups accessing these commands."
1259147189,10536,PingXie,2022-09-27T08:13:03Z,"I am not sure I understand the long term commitment part when shard-id is exposed in `CLUSTER NODES` (and `MYSHARDID`) and, more importantly, when we have gone a long way to establish shards as the first class citizen in Redis and given them names. Also, there is benefit to the clients in the future if we introduce a slot migration command that takes shard-id like `CLUSTER MIGRATE slot <N> shard <shard-id>` such that the user doesn't need to figure out the primary node on the target shard. 

""client"" vs ""admin"". Yes, `CLUSTER SHARDS` is incomplete compared to `CLUSTER NODES`, for now, but it doesn't mean an admin can't/won't use it. There is no enforcement on what admins can or can't call today. So my preference has been to ensure the information returned by any command is complete, conceptually. Philosophically speaking, not having the identity included in a command whose purpose is display detailed information about a first-class entity in the system is confusing IMO, especially when the entity has a name. Imagine that `ifconfig` returns everything about the network interfaces but leaves out the interface names ...

"
1260532894,10536,madolson,2022-09-28T08:01:24Z,">  Also, there is benefit to the clients in the future if we introduce a slot migration command that takes shard-id like CLUSTER MIGRATE slot <N> shard <shard-id> such that the user doesn't need to figure out the primary node on the target shard.

Maybe you misunderstand what I mean by admin clients. There are two distinct sets of APIs for cluster, one that application clients execute for topology discovery and routing and the commands that administration runs for managing cluster state (adding nodes, resharding, etc etc). I think shard-id is purely an administration concept. `Cluster shards` was intended to be an application client API, not an admin API. 

>  Yes, CLUSTER SHARDS is incomplete compared to CLUSTER NODES, for now, but it doesn't mean an admin can't/won't use it.

I think we want to have two separate commands, one for administration and one for application clients. They can even have the same format if we really want, just expose different sets of information. We aren't launching 7.2 tomorrow, and we can block the release of 7.2 until such a command is ready. I don't want us to make an optimization for ""now"" when the right change isn't a significant change from what we have."
1260555423,10536,zuiderkwast,2022-09-28T08:20:59Z,"A cluster client lib that utilizes replicas for reads can benefit from having the shard id available. It could use the master's id as the shard id in the internal slot mapping structure, but then when there is a failover, the identity of the shard would change to the new master that the client would need to reorganize its mappings. A failover doesn't affect the ability to read from replicas within in the shard.

Regarding other potential admin-only information, perhaps it's not useful for clients, but how do we know that? Theoretically, why can't a sophisticated client make use of information about ongoing slot migrations?"
1260791570,10536,madolson,2022-09-28T11:52:17Z,"> A cluster client lib that utilizes replicas for reads can benefit from having the shard id available. It could use the master's id as the shard id in the internal slot mapping structure, but then when there is a failover, the identity of the shard would change to the new master that the client would need to reorganize its mappings. A failover doesn't affect the ability to read from replicas within in the shard.

Not to introduce an AWSism, but this is a oneway door. Once clients become aware of the concept, they might use it for reasons we don't want them to. Clients don't really care about a failover today, since they will just rediscover the topology and should have logic for refreshing the entire state. Specifically handling failovers is just extra work I'm not sure clients could do. I'll be the clients slack channel to see if I'm missing something though.

> Regarding other potential admin-only information, perhaps it's not useful for clients, but how do we know that? Theoretically, why can't a sophisticated client make use of information about ongoing slot migrations?

I don't want clients to know about ongoing slot migrations :). It should be magic."
1263606156,10536,madolson,2022-09-30T13:53:27Z,@yossigo Please review the comments as well to make sure you agree.
1264128852,10536,PingXie,2022-09-30T23:33:11Z,">Once clients become aware of the concept, they might use it for reasons we don't want them to.

What prevents an ""abusive"" client from using `CLUSTER NODES` or `CLUSTER MYSHARDID`? I think what is missing here is a comprehensive framework that formally establishes the concept of ""client"" and ""admin"" commands. The proposal of removing ""shard-id"" from `CLUSTER SHARDS` feels ad hoc to me and I am more concerned about the non-intuitive behavior introduced. 

> I don't want clients to know about ongoing slot migrations :). It should be magic.

That is a separate topic and it goes beyond the identity. It would require the atomic migration feature you proposed in the other thread. Otherwise, clients will notice the effect on the data path, irrespective of shard-id being returned via `CLUSTER SHARDS` or not.
"
1267750768,10536,madolson,2022-10-05T00:09:43Z,"> What prevents an ""abusive"" client from using CLUSTER NODES or CLUSTER MYSHARDID? I think what is missing here is a comprehensive framework that formally establishes the concept of ""client"" and ""admin"" commands.

Nothing prevents clients from implementing something we don't want them to do. I understand your meta-point that there isn't a good ""Client spec"" that exists that explains how clients should implement cluster mode. That is a gap we should address.

> The proposal of removing ""shard-id"" from CLUSTER SHARDS feels ad hoc to me and I am more concerned about the non-intuitive behavior introduced.

We aren't proposing to remove ""shard-id"" from CLUSTER SHARDS, we are proposing not adding it at all. We can always consider adding it later. The main point you identified for adding it is to help with stabilizing the existing slot migration in addition to providing better administration controls."
1267800525,10536,PingXie,2022-10-05T01:30:33Z,"Sync'd up with @madolson offline. In the interest of unblocking other PRs, I filed #11354 to continue the discussion on whether we should include shard_id in `CLUSTER SHARDS` or define a new command. It is a trivial change that we can add any time, if needed. There is also no appcompat risk given that the `CLUSTER SHARDS` output is of the proper RESP format. 

I will remove the shard_id field and rebase my changes sometime this week. "
1281917318,10536,PingXie,2022-10-18T07:14:56Z,@madolson @yossigo sorry for the delay. I have removed the shard-id field per offline discussion.
1303750112,10536,madolson,2022-11-04T15:19:59Z,"@yossigo This is still waiting on you, do you have any other concerns with the API besides the point we talked about?"
1306079175,10536,hwware,2022-11-07T19:21:16Z,"@PingXie Could you please resolve the code conflict and then Yossi maybe do a final code review, Thanks"
1306468091,10536,madolson,2022-11-08T01:14:55Z,"I will follow up with Yossi tomorrow about it. The merge is trivial, I can do it if Ping is busy."
1308154017,10536,madolson,2022-11-09T03:22:18Z,"@PingXie Thanks for your patience, I think we're all good to merge based on the design. Only thing left is to create a documentation PR and do the rebase. If you're busy, I'm happy to clean it up and get it over the line. We can start working on the next PR."
1308154073,10536,madolson,2022-11-09T03:22:24Z,"@PingXie Thanks for your patience, I think we're all good to merge based on the design. Only thing left is to create a documentation PR and do the rebase. If you're busy, I'm happy to clean it up and get it over the line. We can start working on the next PR."
1308157007,10536,PingXie,2022-11-09T03:27:51Z,Thanks for the offer @madolson. I will rebase this PR tonight and get on the documentation PR sometime this week. 
1616417805,10536,oranagra,2023-07-02T07:17:26Z,"@madolson please help me figure out something.
i see this PR is marked for release-notes, but i don't see it mentioned in the 7.2 release notes.
maybe you know / remember why?
it bothers me since i need to mention breaking it in 7.2-RC3 (#12166)"
1620489933,10536,madolson,2023-07-04T16:03:28Z,"> i see this PR is marked for release-notes, but i don't see it mentioned in the 7.2 release notes.

There is still the `myshardid` command, which reveals the shardID used by all the nodes in the shard."
1621095167,10536,oranagra,2023-07-05T06:16:04Z,"so how come it wasn't mentioned in RC1 release notes?
could it be that i just missed it somehow? or did we discuss it and decide to skip it?

in any case, we need to decide what to do in RC3 release notes.
i can retroactively edit the RC1 release notes and then mention it was dropped in RC3 (breaking change), or just add a new note just about MYSHARDID in RC3.

btw, i don't remember being aware of that new command, and i was under the impression that when we removed the metadata in #12166 we reverted all the interface changes of this one. either i missed something or have a memory corruption."
1621098767,10536,oranagra,2023-07-05T06:19:36Z,"i guess editing the RC1 release notes and mentioning the breaking change is the better alternative.

@madolson either way, can you please update the top comment in this PR (mention the new command) and mention the other interface changes were reverted and when..."
1622032072,10536,madolson,2023-07-05T15:46:14Z,"> so how come it wasn't mentioned in RC1 release notes?

I think I missed adding it, the bigger decision at the time was showing it in `CLUSTER NODES`, so it probably got overshadowed.

> i can retroactively edit the RC1 release notes and then mention it was dropped in RC3 (breaking change), or just add a new note just about MYSHARDID in RC3.

I think adding that it was dropped in RC3 is the right documentation (we did break it, but in a way that makes it more compatible). This also means that for GA it's more compatible. I think documenting the command in RC3 is fine."
871363204,9127,itamarhaber,2021-06-30T12:33:29Z,"@oranagra thank you for the CR. I addressed most of the findings, with the exception of the ongoing question about getting the first non-tombstone entry from the stream and the associated work/nesting. We can consider a) uniting tip and edge getters b) keeping as is or c) just maintain 'first_id'. The latter avoids seeking/iterating but will make XTRIM/XDEL/XADD work a little harder.

As for legacy persistence, ""it should work"" and there's a test that uses a v5 stream DUMP payload to check that migration is successful (ref: https://github.com/redis/redis/pull/9127/files#diff-4c5f2c034539b46fa4bebee128f4409782f376d5f419ce96a6728fdc10fbd900R703-R726)."
927350702,9127,itamarhaber,2021-09-26T18:38:04Z,@oranagra I believe we're ready for a CR on this.
981599725,9127,itamarhaber,2021-11-29T12:43:24Z,@redis/core-team please review with the intent to approve - the top comment is up to date.
1047553448,9127,oranagra,2022-02-22T08:43:46Z,"Approved by the core-team in a meeting..
@itamarhaber please mark the resolved comments as resolved.
make sure the top comment is up to date
and let me know when ready for merge."
1870330385,9127,enjoy-binbin,2023-12-27T13:57:45Z,"Do we need to propagate group->entries_read in streamPropagateXCLAIM? 
The following sequence will cause the entries-read of replica to always be nul

master:
```
xadd mystream0 * a b c d e f
xgroup create  mystream0 group1 $
xreadgroup group group1 ryan count 1 streams mystream0 >
xadd mystream0 * a1 b1 a1 b2
xadd mystream0 * name v1 name v1
xreadgroup group group1 ryan count 1 streams mystream0 >
xreadgroup group group1 ryan count 1 streams mystream0 >

127.0.0.1:6379> xinfo groups mystream0
1)  1) ""name""
    2) ""group1""
    3) ""consumers""
    4) (integer) 1
    5) ""pending""
    6) (integer) 2
    7) ""last-delivered-id""
    8) ""1703685291682-0""
    9) ""entries-read""
   10) (integer) 3
   11) ""lag""
   12) (integer) 0

```

replica:
```
127.0.0.1:7000> xinfo groups mystream0
1)  1) ""name""
    2) ""group1""
    3) ""consumers""
    4) (integer) 1
    5) ""pending""
    6) (integer) 2
    7) ""last-delivered-id""
    8) ""1703685291682-0""
    9) ""entries-read""
   10) (nil)
   11) ""lag""
   12) (integer) 0
```"
1870368217,9127,oranagra,2023-12-27T14:41:56Z,"i don't recall if we discussed and dismissed it, or simply overlooked it.
considering that XREADGROUP is a write command and propagates commands to the replica anyway, i suppose it should handle this as well.

@guybe7 @itamarhaber please ack."
1870418723,9127,enjoy-binbin,2023-12-27T15:42:00Z,"ohh, i forget to mention that we will propagate it entries_read streamPropagateGroupID, so when we use NOACK, the entries-read is fine:
```
xadd mystream0 * a b c d e f
xgroup create  mystream0 group1 $
xreadgroup group group1 ryan count 1 streams mystream0 >
xadd mystream0 * a1 b1 a1 b2
xadd mystream0 * name v1 name v1
xreadgroup group group1 ryan count 1 NOACK streams mystream0 >      # NOACK
xreadgroup group group1 ryan count 1 NOACK streams mystream0 >      # NOACK

127.0.0.1:6379> xinfo groups mystream0
1)  1) ""name""
    2) ""group1""
    3) ""consumers""
    4) (integer) 1
    5) ""pending""
    6) (integer) 2
    7) ""last-delivered-id""
    8) ""1703685291682-0""
    9) ""entries-read""
   10) (integer) 3
   11) ""lag""
   12) (integer) 0
```

replica:
```
127.0.0.1:7000> xinfo groups mystream0
1)  1) ""name""
    2) ""group1""
    3) ""consumers""
    4) (integer) 0
    5) ""pending""
    6) (integer) 0
    7) ""last-delivered-id""
    8) ""1703691495978-0""
    9) ""entries-read""
   10) (integer) 3
   11) ""lag""
   12) (integer) 0
```

so i think it is an overlook. But fixing it means we have to add a new option to XCLAIM, and the old server will not recognize it."
1870454962,9127,oranagra,2023-12-27T16:28:23Z,"i don't like to have XCLAIM fail on the replica (and have it's other responsibilities skipped too).
the two options i see are:
1. send an additional command just for that purpose, and let it fail,
2. start negotiating version / capabilities between the master and replica.

i don't like either of them, but i suppose option 1 is the better one of the two.
i see that when this PR was merged it was easy because XSETID used to ignore excessive arguments (and it no longer does).
any other options you see?"
1870765197,9127,enjoy-binbin,2023-12-28T02:28:16Z,"I don't like these options either. I didn't think of any other good options. Depending on your ideas and how you need to proceed, I can do the coding.
"
1870929191,9127,guybe7,2023-12-28T08:18:29Z,"@oranagra @enjoy-binbin yes, it feels like a bug

I suggest calling `streamPropagateGroupID` unconditionally, not only if `NOACK` was provided.
That will normalize entries_read on the replica"
1870937154,9127,enjoy-binbin,2023-12-28T08:30:13Z,"> I suggest calling streamPropagateGroupID unconditionally, not only if NOACK was provided.
That will normalize entries_read on the replica

thanks, i tried it and it is OK now. Is there any other impact (according to the comments)? want me to submit the PR?
```diff
             /* Group last ID should be propagated only if NOACK was
              * specified, otherwise the last id will be included
              * in the propagation of XCLAIM itself. */
-            if (noack) propagate_last_id = 1;
+            propagate_last_id = 1;

```

PR: #12898"
1870944665,9127,guybe7,2023-12-28T08:40:50Z,"@enjoy-binbin yes, thanks

just don't forget to update the comment there
and add tests"
1028755770,10108,oranagra,2022-02-03T08:59:55Z,@redis/core-team please approve the new module API
1028872496,10108,yossigo,2022-02-03T11:07:01Z,"@zuiderkwast @oranagra I've pushed a private commit with an alternative here:
https://github.com/yossigo/redis/commit/13219d3bcf20101786a144bd16ec7b55bbb8a856

This seems a bit more straight forward to me. Callers need to pass an extra `REDISMODULE_COMMANDINFO_VERSION` argument to `RM_SetCommandInfo()`, but that is pretty much aligned with how other APIs work. The upside is we can avoid the inline function and name mangling."
1028898274,10108,oranagra,2022-02-03T11:30:43Z,"it is slightly more verbose to the user to use, but i guess that's a drop in the ocean compared to the declarative part of the command metadata, and indeed more inline with other APIs (in which we ask the user to explicitly set a version field in a struct), and of course avoids the static function and special naming.
so anyway, i'm in favor of what Yossi proposed."
1028920528,10108,zuiderkwast,2022-02-03T12:01:49Z,"> @zuiderkwast @oranagra I've pushed a private commit with an alternative here: [yossigo@13219d3](https://github.com/yossigo/redis/commit/13219d3bcf20101786a144bd16ec7b55bbb8a856)
> 
> This seems a bit more straight forward to me. Callers need to pass an extra `REDISMODULE_COMMANDINFO_VERSION` argument to `RM_SetCommandInfo()`, but that is pretty much aligned with how other APIs work. The upside is we can avoid the inline function and name mangling.

@yossigo OK. I agree it's more strait forward. In that case, isn't it better to put the version field back inside the info struct, like in the other APIs, and make it a struct pointer? The version macro can expand to a pointer to a static constant struct defined in redismodule.h. WDYT?

One more reason to get rid of the `RM_SetCommandInfo_` trick is that this trick doesn't work:

```C
if (RedisModule_SetCommandInfo != NULL) {
    RedisModule_SetCommandInfo(cmd, info);
}"
1028922616,10108,oranagra,2022-02-03T12:04:28Z,putting this as a first first member of the info struct means we can't extend it with more `size` fields.
1028922914,10108,zuiderkwast,2022-02-03T12:04:54Z,"> putting this as a first first member of the info struct means we can't extend it with more `size` fields.

We can if it's a *pointer*  to a version struct."
1028924542,10108,yossigo,2022-02-03T12:07:01Z,"@zuiderkwast  I thought about that, but wasn't very happy with defining a static variable in `redismodule.h`. We'd need to silence errors about unused definition, and we'd rely on the toolchain to make sure it doesn't *really* end up in every object file. This version makes it completely ephemeral and the API is relatively clean (pass a `VERSION` macro like everywhere else)."
1028929050,10108,oranagra,2022-02-03T12:13:02Z,"we actually already have these.
see `RedisModuleEvent_ReplicationRoleChanged` and friends."
1028931263,10108,zuiderkwast,2022-02-03T12:16:02Z,"Exactly, the events are all static const. They end up in every object file, in modules which don't use events. Also, all the API functions are global pointers. Most of them are not used by most of the modules."
1028940850,10108,yossigo,2022-02-03T12:28:22Z,"@zuiderkwast @oranagra Good points, so the pointer approach makes sense."
1334259234,11568,oranagra,2022-12-01T19:30:47Z,"i didn't look at the code yet, but from the description, the one thing i don't like is the API.
the fact we pass the callback in the varargs means we have no type checking, and i'm also not happy with the NULL return and the dependence of errno.

regarding the NO_SCRIPT flag, i don't think that it should be a problem removing the flag from the blocking commands, existing scripts couldn't use these commands anyway, so they won't be affected, but we have to take into consideration few other external aspects:
1. existing modules that exposed blocking commands to redis and may have also set the NO_SCRIPT flag.
2. external clients introspecting commands with COMMAND INFO and how it affects them.

one interesting thing is that the BLOCKING flag (which modules can also use when they register commands) is completely new, and likely not in use yet, that probably means we can't use it.
however, the NO_MULTI flag isn't exposed to modules to this day, so i guess modules couldn't have relied on setting NO_SCRIPT, and had to react to CLIENT_DENY_BLOCKING anyway."
1334302657,11568,MeirShpilraien,2022-12-01T19:47:04Z,"> i didn't look at the code yet, but from the description, the one thing i don't like is the API.
the fact we pass the callback in the varargs means we have no type checking, and i'm also not happy with the NULL return and the dependence of errno.

Sure, this is just a suggestion but totally debatable. We can have a new API, `RM_AsyncCall` that gets the callback as one on the arguments, this will solve the type checking. The new function can also have an out param indicating whether it was blocked or not. For the rest, this new function can behave the same as RM_Call. Let me know how it sounds and I also like to hear @yossigo @itamarhaber @guybe7 and others about this.

> existing modules that exposed blocking commands to redis and may have also set the NO_SCRIPT flag

The `NO_SCRIPT` flag remains the same, the command will not be able to be call inside a script or in RM_Call script mode. The change is on this list of those specific commands where we remove this flag.

> external clients introspecting commands with COMMAND INFO and how it affects them.

Well, before, when call those list of commands from script, it would have count as an error. Now it will not be counted as error. But as you said, there was not point of calling those commands from script so not sure it is a breaking change.

> one interesting thing is that the BLOCKING flag (which modules can also use when they register commands) is completely new, and likely not in use yet, that probably means we can't use it.
however, the NO_MULTI flag isn't exposed to modules to this day, so i guess modules couldn't have relied on setting NO_SCRIPT, and had to react to CLIENT_DENY_BLOCKING anyway.

Right, currently modules have to respect the `CLIENT_DENY_BLOCKING` flag."
1436004306,11568,oranagra,2023-02-19T14:30:08Z,triggered valgrind https://github.com/redis/redis/actions/runs/4216703412
1436612140,11568,MeirShpilraien,2023-02-20T09:22:27Z,@oranagra fixed the comments except for https://github.com/redis/redis/pull/11568#discussion_r1111219557 which I am waiting for input.
1436848227,11568,MeirShpilraien,2023-02-20T11:51:32Z,"> @MeirShpilraien what will happen in case of pipelined module command which will attempt to trigger RM_Call with blocking command?
basically we could have more commands pending in the input buffer but I am not sure we have a way to turn back and process the input buffer (did I miss something?)

@ranshid I am not sure I am follow, each blocking RM_Call is invoke on a separate fake client. Maybe I do not understand the problem you refer to?"
1436905245,11568,ranshid,2023-02-20T12:16:49Z,"> > @MeirShpilraien what will happen in case of pipelined module command which will attempt to trigger RM_Call with blocking command?
> > basically we could have more commands pending in the input buffer but I am not sure we have a way to turn back and process the input buffer (did I miss something?)
> 
> @ranshid I am not sure I am follow, each blocking RM_Call is invoke on a separate fake client. Maybe I do not understand the problem you refer to?

well correct me if I am wrong: for regular client blocking via modules, the executing client is blocked and then later when it is unblocked by the module is queued on 2 lists:
 - the moduleUnblockedClients - which is processed in the before sleep right before processing unblocked clients
 - the unblockedClients list which is processed after the moduleHandleBlockedClients() and is also processing what is left to be processed in the inputBuffer.
 
 In your case if the module command is currently executed and is spinning a fake client which is then unblocked. the fake client is reprocessed and execute the module continuation callback. but what will cause the processing of what is left in the executing client inputBuffer? (again I might be missing something, but wanted to raise that anyway)"
1437028354,11568,MeirShpilraien,2023-02-20T13:29:01Z,"@ranshid is this covers what you refer to?
https://github.com/redis/redis/pull/11568/commits/4c3c5517ec2c39604520a866b323f632ca87fb02#diff-052dc1917e6974ad638ff8a9d0b531f421bff61da15c378a0b253748a5b704fbR143"
1438064567,11568,ranshid,2023-02-21T08:44:53Z,"> > > @MeirShpilraien what will happen in case of pipelined module command which will attempt to trigger RM_Call with blocking command?
> > > basically we could have more commands pending in the input buffer but I am not sure we have a way to turn back and process the input buffer (did I miss something?)
> > 
> > 
> > @ranshid I am not sure I am follow, each blocking RM_Call is invoke on a separate fake client. Maybe I do not understand the problem you refer to?
> 
> well correct me if I am wrong: for regular client blocking via modules, the executing client is blocked and then later when it is unblocked by the module is queued on 2 lists:
> 
> * the moduleUnblockedClients - which is processed in the before sleep right before processing unblocked clients
> * the unblockedClients list which is processed after the moduleHandleBlockedClients() and is also processing what is left to be processed in the inputBuffer.
> 
> In your case if the module command is currently executed and is spinning a fake client which is then unblocked. the fake client is reprocessed and execute the module continuation callback. but what will cause the processing of what is left in the executing client inputBuffer? (again I might be missing something, but wanted to raise that anyway)

@MeirShpilraien. O.K now I understand how this is not a problem in the blockedclient implementation. since the test module is also making sure to create a RedisModuleBlockedClient on do_rm_call_async and is unblocking the original client on the on_unblock callback (rm_call_async_on_unblocked) so the original client will be reprocessed as a regular blocked client. 
We better make sure to document that using promise on calling blocked commands require blocking the originating client as well (Or we change the implementation to handle that)"
1440050572,11568,MeirShpilraien,2023-02-22T13:51:16Z,"> @MeirShpilraien. O.K now I understand how this is not a problem in the blockedclient implementation. since the test module is also making sure to create a RedisModuleBlockedClient on do_rm_call_async and is unblocking the original client on the on_unblock callback (rm_call_async_on_unblocked) so the original client will be reprocessed as a regular blocked client.
We better make sure to document that using promise on calling blocked commands require blocking the originating client as well (Or we change the implementation to handle that)

@ranshid the async rm_call is not responsible to release the original client that started it all, there might not even be an original client. It just give you a way to call blocking commands and continue your logic when unblocked."
1440335894,11568,ranshid,2023-02-22T16:15:40Z,"> > @MeirShpilraien. O.K now I understand how this is not a problem in the blockedclient implementation. since the test module is also making sure to create a RedisModuleBlockedClient on do_rm_call_async and is unblocking the original client on the on_unblock callback (rm_call_async_on_unblocked) so the original client will be reprocessed as a regular blocked client.
> > We better make sure to document that using promise on calling blocked commands require blocking the originating client as well (Or we change the implementation to handle that)
> 
> @ranshid the async rm_call is not responsible to release the original client that started it all, there might not even be an original client. It just give you a way to call blocking commands and continue your logic when unblocked.


I understand. you are right that the context from which the blocking RM_call might not be a real client context. maybe we should just make sure to document this as I suspect it is not trivial (at least it wasn't for me :) )"
1445319025,11568,oranagra,2023-02-26T10:20:11Z,"@redis/core-team please approve the new module API, and slight behavior change of blocking commands in scripts. see the top comment for details."
1448368027,11568,sjpotter,2023-02-28T15:18:07Z,"a few thoughts (haven't gone through all comments yet, so perhaps some of its discussed below initial comment

1) re ""The module should not free the promise call reply.""

I'm not sure I personally like that, the general behavior is to free the CallReply objects returned from RM_Call.  This changes those semantics.  Might it be better for there to be a counter, so that when one registers the handler, it increases it, and a Free will behave as one expects?

This is especially true in the context of ""WATCH"", where in reality there is no promise/callback for future, its just persistent state.

2) any promise/asynch type functionality, I think should have the ability to abort it as well.  i.e. the ""promise"" should be abortable, which would remove the pending client.  

the idea here would be a timeout could be implemented external to the RM_Call.  I'd argue that basically blocking commands should always in this model be implemented with an infinite timeout, and if the module wants to timeout the call, they'll simply set up an RM_Timer callback with the abort functionality built in."
1448794619,11568,sjpotter,2023-02-28T19:59:37Z,"another thing re command name i.e. AsyncCall. Why not really think about just creating a new RM_Call with a better API, not just take the old API and change it slightly.  its had so many things retrofitted onto it over the years.

in general, redis avoids adding new APIs, but this is now a good reason to create a new one and deprecate RM_Call."
1449426112,11568,oranagra,2023-03-01T06:28:25Z,"@sjpotter we considered adding another API (which would be confusing since people will need to decide which one to use), and eventually concluded that there's no need for an additional API, and all the new (and future capabilities we're considering) can be handled by the current one (either by passing flags, changing context state before calling, or manipulating the reply object).
So despite the current API being awkward, we rather not have two.

Calling WATCH (when we'll some day support virtual clients being attached to the context before calling RM_Call), would not block and return a promise, it'll return OK.

I agree we need a way for the module to abort the call if possible before being executed, not sure about the freeing issue, i'll let Meir reply."
1449512146,11568,sjpotter,2023-03-01T07:58:50Z,"@oranagra re ""which would be confusing since people will need to decide which one to us"", I'd imagine it as not being confusing, the old one would be depcrecated, and the new one would be the ones to be used for new code, it support all use cases of the old one, but be smarter.

Ex: today we conflate ""flag strings"" and what we can call arg format strings.  I'd argue that, 

a) they should be separated and b) I'm unconvinced that we even need the format strings.  RM_Call itself having to do arg handling like this (with all the memory allocations (ala moduleCreateArgvFromUserFormat) slows it down for use cases where that's not neccessary.  If Redis is supposed to be fast, we shouldn't require extra work to be done in cases where its not neccessary.

i.e. I'd argue something along the lines of

`RM_Call2(RedisModuleContext ctx, uint64_t flags, callback, uint num_args, RedisModuleString **args)`

if callback is passed, we return everything to it (blocking or not), its essentially the same RM_Call as today (minus formatting of args).  One can pass an `RM_CALL_BLOCK` flag, which would be the `K` in the args here.

For users today we sho want to be creative with their args there will be an

`RedisModuleString **RM_ParseArgs(uint *count, char *format, ...)`

the big advantage is that it removes significant overhead (and in my opinion) is a cleaner API.  

just my 2c."
1449707732,11568,oranagra,2023-03-01T09:40:19Z,"@sjpotter we considered that option.
we concluded that either way, we won't be adding more and more arguments to the API (and create RM_Call3 and RM_CallUser, and RM_CallBlocking, and all other variants).
And then we concluded that since we go with setting various options to the context, there's no need to change the API.
since there are masses of code already using the existing one, deprecating it won't do much, and we'll remain with 2 variants and a lot of confusion.

we didn't discuss the overheads at this point, if we wanna resolve just that, we can create an additional interface with a different way to pass the argv, but keep the same approach with all the rest, so both can support the same feature set, and it's just a matter of how the argv is passed."
1457601844,11568,MeirShpilraien,2023-03-07T06:11:28Z,"> I'm not sure I personally like that, the general behavior is to free the CallReply objects returned from RM_Call. This changes those semantics. Might it be better for there to be a counter, so that when one registers the handler, it increases it, and a Free will behave as one expects?

Today, if the module will try to free the promise call reply he will get an assert. We can change it to just ignore. @oranagra  WDYT?

> I agree we need a way for the module to abort the call if possible before being executed, not sure about the freeing issue, i'll let Meir reply.

When we discussed we said that abort will be part of virtual client API that will follow. If we believe this should be part of this API then we can allow calling abort on the promise object (if it was not yet resolved). Let me know what you think."
1457641865,11568,oranagra,2023-03-07T06:58:05Z,"> Today, if the module will try to free the promise call reply he will get an assert. We can change it to just ignore. @oranagra WDYT?

if the module doesn't need to release it elsewhere, and ignoring it doesn't add any risks, i think it may be ok (then we should document that it's unnecessary to release it as it is ignored).
i'm also ok with the assertion, i'm not certain which one of these options is worse.

> When we discussed we said that abort will be part of virtual client API that will follow. If we believe this should be part of this API then we can allow calling abort on the promise object (if it was not yet resolved). Let me know what you think

it seems logical to me to handle it now, it makes the API more complete and doesn't stray away from the purpose of this PR."
1468065559,11568,MeirShpilraien,2023-03-14T13:02:29Z,"For reviewers who already reviewed the code, here are the summery of the recent changes:

### Allow Aborting the async RM_Call
Introducing the ability to abort the async rm_call in case it was not yet invoke. To achieve this, a new API was introduce, `RM_CallReplyPromiseAbort`, that gets the promise CallReply and abort it. This new API also enforce a changes of the ownership definition of the promise CallReply. Before it was only borrowed by the module, the module got the promise CallReply just so he can set the unblock handler and never touch it again. Now the module should be able to keep it for abort purposes. To support that we introduce a new ref counted object (shared ownership) that keep the required information for the unblocking operation (the unblock callback, private data, ...). This object is owned both by the promise CallReply and the fake client that was used to invoke the command. In addition it is now also the module responsibility to free the promise CallReply, and he must do it when the GIL is acquired.

### Better atomicity garentee on pure Redis command
Though the documentation says nothing about atomicty guarantees other then the fact that the unblock handler run atomically as an execution unit. We did want the implementation to give some better guarantees. We have the flexibility to decide in the future if we want to strengthen those guarantees or to drop them. The better guarantees are, on pure Redis blocking commands, the unblock handler will run atomically with the command that got unblocked. To achieve that, we call the unblock handler right after `processCommandAndResetClient` which is called from `unblockClientOnKey` (if the client got unblock), and in addition we make sure all this code is running as a single execution unit."
1468129859,11568,oranagra,2023-03-14T13:43:47Z,"Anyone who's following this PR, please also see the new `Additional RedisModule API and changes` section in the top comment."
1468410368,11568,oranagra,2023-03-14T16:18:07Z,This PR was discussed (again) and conceptually approved in a core-team meeting.
1470640820,11568,oranagra,2023-03-15T19:11:08Z,Full CI: https://github.com/redis/redis/actions/runs/4430049150
1484595612,11963,CharlesChen888,2023-03-27T06:51:05Z,"> i see few things i listed in the issue are missing, let's add them in the PR later on, after we're happy with the ones you already added.
> 
> > * eventloop_duration_cron_sum - total time spent in serverCron, and beforeSleep (excluding writes and aof)
> > * instantaneous_eventloop_cycles - cycles per second in the last 1.6 seconds
> > * instantaneous_eventloop_duration_avg - average duration in the last 1.6 seconds
> > * instantaneous_eventloop_duration_max - max duration in the last 1.6 seconds

@oranagra 

Is read (in beforeSleep) considered a part of cron?

And about the 3 instantaneous stats, are they updated every 1.6 seconds, or updated every loop? I think we can leave these to external monitoring tools, so the server can just provide basic stats."
1484668938,11963,oranagra,2023-03-27T07:52:29Z,"@CharlesChen888 my idea for a ""cron"" metric was to measure the sum of active-expire, active-defrag and all other tasks done by cron and beforeSleep that are not counted by the other set of metrics (so we should exclude read, write and AOF from these).

regardign the ""instantaneous"" metrics, i agree they're unnecessary for people who have a proper monitoring software connected to redis. they're there for easy consumption from command prompt by using redis-cli.
i meant to add them to the STATS_METRIC_COUNT mechanism."
1487863781,11963,soloestoy,2023-03-29T02:35:26Z,">* eventloop_duration_cron_sum - total time spent in serverCron, and beforeSleep (excluding writes and aof)

agree, cron sum is useful, especially about the expire cycle and defrag cycle in `serverCron`.

moreover, we also have two other time events, `moduleTimerHandler` and `evictionTimeProc`, do you think we should record these two? @oranagra 

>* instantaneous_eventloop_cycles - cycles per second in the last 1.6 seconds
>* instantaneous_eventloop_duration_avg - average duration in the last 1.6 seconds
>* instantaneous_eventloop_duration_max - max duration in the last 1.6 seconds

I'm not sure if they're useful, and `instantaneous`...`max` these two words are a bit strange together.

Maybe you wanna record how many eventloop cycles in last xxx seconds, and average/max duration in the last xxx seconds. But I think it should take longer like 1 min, and we should introduce another way to record them instead of using the current instantaneous metric.

"
1488063869,11963,oranagra,2023-03-29T07:14:21Z,"i don't think we wanna break down cron to each and every task it contains.
we already have latency monitor metrics for active expiry and defrag, so for the purpose of event loop monitoring i think it can be inclusive without a breakdown (other than IO which we exclude from it).
then if someone realizes the event loop takes long because of cron, they can dig into it with other ways.

regarding the instantaneous metrics, IIRC last time we argued about these metrics (in #10062), i argued that we don't need them and you argued that we do...
i do think we need to keep them to a minimum, maybe we should only include only `instantaneous_eventloop_cycles`, or none of them.
having just `duration_avg` without `duration_max` feels a bit odd to me, and having just cycles per second without any duration metric also feels a bit odd. if the problem is the contradiction of ""instantaneous"" and ""max"" terms, maybe we can add ""recent"".

regarding the duration being 1 second or 1 minute, i don't wanna introduce another ""infrastructure"" for this, so i'd rather keep the 1.6 seconds thing we have now.
under constant load, it would be sufficient to have them represent 1 second (will show same values as 1 minute), so let's keep that. in that case maybe having an average duration, and no max is actually ok.

bottom line, if you are ok with it, let's drop the max and add just two."
1488251253,11963,CharlesChen888,2023-03-29T09:24:47Z,"So do we have a final design for this? INFO or histogram? And what metrics to expose?

If we just show the metrics in info, we may have a new section `INFO EVENTLOOP`, and it contains the list below (may be drop the instantaneous_eventloop_duration_max).
```
eventloop_cycles
eventloop_duration_sum
eventloop_duration_max
eventloop_duration_cmd_sum
eventloop_duration_io_read_sum
eventloop_duration_io_write_sum
eventloop_duration_aof_sum
eventloop_duration_cron_sum

instantaneous_eventloop_cycles
instantaneous_eventloop_duration_avg 
instantaneous_eventloop_duration_max
```
If we want a histogram, we may have a new command (or perhaps a new subcommand under `LATENCY`?), and it returns the information below.
```
1) ""eventloop""
2) 1) ""count""
   2) (integer) 24
   3) ""histogram_usec""
   4) 1) (integer) 132
      2) (integer) 18
      3) (integer) 264
      4) (integer) 24
3) ""eventloop|commands""
4) 1) ""count""
   2) (integer) 1
   3) ""histogram_usec""
   4) 1) (integer) 4227
      2) (integer) 1
5) ""eventloop|ioread""
6) 1) ""count""
   2) (integer) 1
   3) ""histogram_usec""
   4) 1) (integer) 4227
      2) (integer) 1
......
```
Or we show sum in info, and details in histogram?
@oranagra @madolson @soloestoy"
1488285062,11963,oranagra,2023-03-29T09:49:36Z,"i'm leaning towards keeping the simple metrics in INFO STATS (with the instantaneous but without the `max`), and exposing the histogram in LATENCY EVENTLOOP.

the histogram data itself in the event loop should probably be plain (similar to the LATENCY HISTOGRAM output), but maybe put under a ""histogram"" field in a map, so we can extend that map in the future."
1488317820,11963,soloestoy,2023-03-29T10:11:40Z,">regarding the instantaneous metrics, IIRC last time we argued about these metrics (in https://github.com/redis/redis/pull/10062), i argued that we don't need them and you argued that we do...

I need clear it, I mean instantaneous max metric is strange. IMHO instantaneous is always used on xxx per second metric, like qps for query per second and bandwidth for net bytes per second, I don't know how to understand ""duration time per second""."
1488382440,11963,oranagra,2023-03-29T10:59:18Z,"i view the ""instantaneous"" as a metric representing the very recent time (last 1.6 seconds), it doesn't have to be a per-second metric.

`instantaneous_ops_per_sec` is a per-sec, but it doesn't mean there couldn't be an `instantaneous_cpu_usage`.
i.e. unlike `used_memory_rss` which represents the current state, CPU usage, and ops/sec represent delta of some metric over time, and we choose to show the delta over the last 1.6 seconds, i don't see a problem showing `instantaneous_eventloop_duration_avg`"
1488412307,11963,soloestoy,2023-03-29T11:20:08Z,">i view the ""instantaneous"" as a metric representing the very recent time (last 1.6 seconds), it doesn't have to be a per-second metric.

seems you misunderstand the current instantaneous mechanism or I missed something? it is the average of the past 16 per-second metric samples, in another word it is indeed the per-second metric that just calculated as an average of 16 samples, nothing todo with 1.6 seconds.

```c
/* Add a sample to the operations per second array of samples. */
void trackInstantaneousMetric(int metric, long long current_reading) {
    long long now = mstime();
    long long t = now - server.inst_metric[metric].last_sample_time;
    long long ops = current_reading -
                    server.inst_metric[metric].last_sample_count;
    long long ops_sec;

    ops_sec = t > 0 ? (ops*1000/t) : 0;

    server.inst_metric[metric].samples[server.inst_metric[metric].idx] =
        ops_sec;
    server.inst_metric[metric].idx++;
    server.inst_metric[metric].idx %= STATS_METRIC_SAMPLES;
    server.inst_metric[metric].last_sample_time = now;
    server.inst_metric[metric].last_sample_count = current_reading;
}

/* Return the mean of all the samples. */
long long getInstantaneousMetric(int metric) {
    int j;
    long long sum = 0;

    for (j = 0; j < STATS_METRIC_SAMPLES; j++)
        sum += server.inst_metric[metric].samples[j];
    return sum / STATS_METRIC_SAMPLES;
}
```"
1488506328,11963,oranagra,2023-03-29T12:24:38Z,"i think the confusion is that you're referring to the way they're implemented and i'm referring to the way they're presented to the user.
if we go my way, we can rename `trackInstantaneousMetric` to `trackInstantaneousRateMetric`, and add a similar `trackInstantaneousAvgMetric`.

i.e. the current mechanism takes a rate sample (ops / sec) once in 100ms, and then computes an average of these over 1.6 seconds.
but we can add another similar function that takes a sum and count samples every 100ms to compute an average (average duration in this case), and then compute an average for these over 1.6 seconds.

i'm saying 1.6 seconds because there are 16 samples and we take one every 100ms."
1496036321,11963,oranagra,2023-04-04T14:05:32Z,"since this was stale for a while, i'll try to promote a minimal version, so the status from my perspective (if we put aside the [histogram](https://github.com/redis/redis/pull/11963#discussion_r1147789811) feature) what's missing is:
* add `eventloop_duration_cron_sum` - total time spent in serverCron, and beforeSleep (excluding writes and aof)
agree, cron sum is useful, especially about the expire cycle and defrag cycle in serverCron.
* remove `eventloop_duration_max` for now.

IMO it could be nice to have two metrics for easy consumption from redis-cli (but not a must):
* `instantaneous_eventloop_cycles` - cycles per second in the last 1.6 seconds
* `instantaneous_eventloop_duration_avg` - average duration in the last 1.6 seconds
to clarify (since i didn't see a response) we can rename `trackInstantaneousMetric` to `trackInstantaneousRateMetric`, and add a similar `trackInstantaneousAvgMetric`. `getInstantaneousMetric` can remain as is, and i don't think any of that contradicts the ""instantaneous"" concept from the user's perspective.

@CharlesChen888 can you handle these changes or maybe want me to try to step in and help?"
1496323262,11963,CharlesChen888,2023-04-04T17:11:49Z,"@oranagra Sorry about the delay, I went on vacation last weekend and I will get back to this tomorrow."
1496982060,11963,oranagra,2023-04-05T06:26:33Z,no worries.. was just trying to see if i can help
1500870806,11963,oranagra,2023-04-08T11:21:54Z,"@CharlesChen888 I'd prefer you avoid force-pushes. It makes it harder to do incremental reviews. Since we're gonna squash-merge it (using the PR top comment AS a commit comment, these incremental commits are ok"
1514907515,11963,oranagra,2023-04-19T15:08:49Z,"we discussed this in a core-team meeting, specifically the [thread](https://github.com/redis/redis/pull/11963#discussion_r1162242078) about whether or not we should expose a breakdown of the composition of the event loop or not.

there were two main concerns:
1. the current code for measuring IO adds per-read overheads (not per event loop), and could slow down the throughput.
2. some of the metrics in the breakdown are hard to document and we may want to change them in the future which could be considered a breaking change.

the decisions we came up with:
1. drop the IO measurements. in theory, it could be possible for someone to compute the delta between the total and the sum of breakdowns and conclude that this might be IO.
2. keep the event loop cycle counter, duration_sum, and cmd_sum in the `stats` section, but chuck the other ones in some `dev`/`debug`/`experimental` section that's not displayed by default (not in even in `all`)

we didn't get to discuss the instantaneous ones. from the threads above, it seems we agree on keeping `instantaneous_eventloop_cycles_per_sec`.
we can name the other one `Instantaneous_eventloop_duration_usec` or drop it for now.

other than that, i do think we can follow up on many of Jim's cleanup suggestions, and then i'd hope we can merge this."
1518044309,11963,oranagra,2023-04-21T16:05:57Z,"another interesting metric could be the number of commands per event loop cycle, specifically the max.
considering that we now intend to introduce an experimental section, maybe we can add `eventloop_cmd_per_cycle_max` and maybe re-add `eventloop_duration_max` to have them temporarily until histograms are present?
@madolson WDYT? is the non-`default` (and non-`all`) ""experimental"" section hidden enough to add these temporarily, or you still rather avoid them for now?"
1537658518,11963,soloestoy,2023-05-08T02:41:32Z,"> i'm leaning towards keeping the simple metrics in INFO STATS (with the instantaneous but without the `max`), and exposing the histogram in LATENCY EVENTLOOP.
> 
> the histogram data itself in the event loop should probably be plain (similar to the LATENCY HISTOGRAM output), but maybe put under a ""histogram"" field in a map, so we can extend that map in the future.

@oranagra seems we forgot it? I think it's useful, since the metric in info may be lost, due to the issue of sampling frequency. It's bettor to record the historical data."
1537743849,11963,oranagra,2023-05-08T04:55:52Z,"@soloestoy forgot what? our plans to add a histogram?
we didn't forget it, just assumed that we won't have time to deal with it for 7.2.
if @CharlesChen888 or someone else has time we can try pushing it forward, but i'd rather have this one merged as a backup plan, even with some [temporary](https://github.com/redis/redis/pull/11963#discussion_r1174513519) metrics in the debug section that can later be removed."
1541432320,11963,CharlesChen888,2023-05-10T06:38:11Z,"So we now have these in `INFO STATS`
```
eventloop_cycles
eventloop_duration_sum
eventloop_duration_cmd_sum
instantaneous_eventloop_cycles_per_sec
instantaneous_eventloop_duration_usec
```
and these in the hidden `INFO DEBUG`
```
eventloop_duration_aof_sum
eventloop_duration_cron_sum
```
Do we really need `eventloop_cmd_per_cycle_max`? I don't see this metric very helpful.

If nothing else is to be modified in `INFO`, maybe we can merge this as a temporary conclusion and move to a new PR to handle histograms.

@oranagra"
1542674774,11963,oranagra,2023-05-10T19:11:07Z,"@CharlesChen888 we discussed this in a core-team meeting, and since we're leaving the histograms for a future PR, which might not make it to the next release, we'd like to add the two metrics (eventloop_cmd_per_cycle_max and eventloop_duration_max) to the debug section.

besides that, i'd love to have some (even basic) test coverage.
can you handle these?"
1545671854,11963,oranagra,2023-05-12T12:32:33Z,full CI: https://github.com/redis/redis/actions/runs/4958856487
1545718741,11963,oranagra,2023-05-12T13:07:07Z,"| before and after sleep 1                                                                                                                                                                                                                                                                                                             | idle                                                     | memtier_benchmark                                                        | --pipeline 100                                                             |   |
|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|--------------------------------------------------------------------------|----------------------------------------------------------------------------|---|
| eventloop_cycles - delta 1s apart<br>eventloop_duration_sum - delta 1s apart<br>eventloop_duration_cmd_sum - delta 1s apart<br>instantaneous_eventloop_cycles_per_sec<br>instantaneous_eventloop_duration_usec<br>eventloop_duration_aof_sum<br>eventloop_duration_cron_sum<br>eventloop_duration_max<br>eventloop_cmd_per_cycle_max | 13<br>1618<br>69<br>10<br>108<br>24<br>10043<br>247<br>1 | 1052<br>979175<br>237679<br>1058<br>932<br>478<br>209836<br>23137<br>199 | 133<br>1017352<br>294548<br>127<br>7841<br>646<br>292085<br>26621<br>18400 |   |
|                                                                                                                                                                                                                                                                                                                                      |                                                          |                                                                          |                                                                            |   |
|                                                                                                                                                                                                                                                                                                                                      |                                                          |                                                                          |                                                                            |   |"
1545738161,11963,oranagra,2023-05-12T13:21:36Z,"@CharlesChen888 please take a look at the test failures, some are thresholds that should be adjusted (take a safe distance, maybe twice of what's printed), and there's also some odd fortify compilation error that i didn't look into..
thanks."
1545945292,11963,oranagra,2023-05-12T15:44:43Z,"tests are now passing, and i also verified that we don't see that fortification error in unstable. trying to figure out how it is related to this PR.."
975183372,9822,oranagra,2021-11-22T06:57:05Z,"@ranshid thanks.
I like the idea of saving memory when there are many clients that don't actually need it.
but i don't like to add an extra config.
i would like to look into a possibility of making a better redis by default, without users needing to spot the problem and tune it.

in theory, we can realloc the client from time to time. i.e. create a small one initially, and make it grow when we see it normally consumes more than that first buffer.
but then i'm worried about a usage pattern that creates disposable clients.
i.e. client connects (created small), then runs some traffic, get resized, and then disconnect.
if we're flooded by such usage pattern, we could cause a significant regression.
@yoav-steinberg i'd like to hear your thoughts."
975276283,9822,ranshid,2021-11-22T08:39:49Z,"> @ranshid thanks. I like the idea of saving memory when there are many clients that don't actually need it. but i don't like to add an extra config. i would like to look into a possibility of making a better redis by default, without users needing to spot the problem and tune it.
> 
> in theory, we can realloc the client from time to time. i.e. create a small one initially, and make it grow when we see it normally consumes more than that first buffer. but then i'm worried about a usage pattern that creates disposable clients. i.e. client connects (created small), then runs some traffic, get resized, and then disconnect. if we're flooded by such usage pattern, we could cause a significant regression. @yoav-steinberg i'd like to hear your thoughts.

Thank you @oranagra for you quick reply!
I agree that adding a new config is restricting and an automatic adjustment mechanism would better serve the general case. 
I do think there is a variety of use cases and sometimes users would mostly like to use as much memory is the cost of performance and in some cases the other way around while identify the use case might sometimes be problematic. 
we can consider  introducing an occasional memory relocation for clients, but usually once the relevant memory is already allocated for user data, there can be little spare for clients resizing (unless evictions are made)
we can also come up with a formula to choose the optimal client size based on maxclients and maxmemory configurations, but again this might be restrictive and sub-optimal in some user setups.

given said all that I would be happy to followup and consider a better alternative :) "
975617335,9822,yoav-steinberg,2021-11-22T15:03:08Z,"Since the client output buffer is dynamically allocated with an initial static buffer of 16k and then a linked list of dynamic buffers, I'm guessing that any usage pattern that needs large buffers will cause dynamic allocations anyway so doing this dynamically doesn't make sense to me.

This should be either a compile time constant (like it is today) or a config (like proposed here). Both are good solutions.
I looked a bit at the history of this 16k size, I see it started out as 4k, then grew to ~7.5k and then grew to 16k. From the logs I couldn't really understand why. I think we should be willing to consider making this smaller (lets say 8k) to save memory."
975632865,9822,ranshid,2021-11-22T15:18:23Z,"> 

Thank you @yoav-steinberg !
I also agree regarding the dynamic reply allocations once that fixed size buffer is filled (which is why reducing the size will result in performance degradation)
I believe this change was first introduced by @antirez in this commit: https://github.com/redis/redis/commit/65330badb97206cd7cf0973d3f2267b0a523c05e
I can start an effort to test performance of different reply sizes and check the performance degradation, but as I said before - I suspect this is mainly intended for users to opt-in their Redis to specific use cases (which are not that common IMO) "
976339763,9822,oranagra,2021-11-23T09:44:14Z,"As you said, this buffer is need for performance (avoid allocations and also the indirection and possibly cache locality we'd have if it was a pointer).
As was pointed out, it was smaller in the past then then increased, so obviously decreasing it is not a wise idea.
I generally want to avoid adding configs for such tuning. too low level and i don't think users need to worry about it.
also i'd like to improve memory usage even for ones who didn't bother to dig in and realize what's eating their ram.

you can proceed to run some performance tests, maybe we'll realize something new.
but i'd also like to try to think of some automatic mechanism that can be put here instead of that config."
977220521,9822,yoav-steinberg,2021-11-23T22:17:08Z,"> but i'd also like to try to think of some automatic mechanism that can be put here instead of that config.

How about this: 
- Clients are allocated with some minimum fixed `buf` size (lets say 4K). 
- If this buffer is ever maxed out (meaning we added a dynamic allocation to the `reply` list) then we turn on some `CLIENT_BUF_WAS_FULL` in `flags` in the client struct. 
- In `clientsCron()` periodically we check if this flag is on. If it is we reallocate the client struct so its `buf` is doubled and replace it in the clients list (this means we can't have this client object referenced anywhere else). After reallocation we zero the flag.
- We need a maximum too (lets say 16K) and stop growing the client struct after we reach this max. So we need to keep track of the `buf` size in the client struct.
- Things to consider:
  - We need a shrinking mechanism to reallocate to a smaller size if between crons we don't use more than half the buffer. So we need another flag or perhaps the flag should always just mention half the buffer size?
  - This design might not be possible because of client struct references. An alternative can be to chuck the preallocated `buf` at the end of the client struct and just make sure we always have an initial entry in the `reply` list which we can manage with similar cron based growing and shrinking logic.
  - The cron intervals might not be good enough? Do we need to add some timestamps to the client struct signifying ""last grow/shrink time""?

"
977577900,9822,ranshid,2021-11-24T06:42:21Z,"> > but i'd also like to try to think of some automatic mechanism that can be put here instead of that config.
> 
> How about this:
> 
> * Clients are allocated with some minimum fixed `buf` size (lets say 4K).
> * If this buffer is ever maxed out (meaning we added a dynamic allocation to the `reply` list) then we turn on some `CLIENT_BUF_WAS_FULL` in `flags` in the client struct.
> * In `clientsCron()` periodically we check if this flag is on. If it is we reallocate the client struct so its `buf` is doubled and replace it in the clients list (this means we can't have this client object referenced anywhere else). After reallocation we zero the flag.
> * We need a maximum too (lets say 16K) and stop growing the client struct after we reach this max. So we need to keep track of the `buf` size in the client struct.
> * Things to consider:
>   
>   * We need a shrinking mechanism to reallocate to a smaller size if between crons we don't use more than half the buffer. So we need another flag or perhaps the flag should always just mention half the buffer size?
>   * This design might not be possible because of client struct references. An alternative can be to chuck the preallocated `buf` at the end of the client struct and just make sure we always have an initial entry in the `reply` list which we can manage with similar cron based growing and shrinking logic.
>   * The cron intervals might not be good enough? Do we need to add some timestamps to the client struct signifying ""last grow/shrink time""?

Thank you @yoav-steinberg!
I was thinking the same direction. we can keep track of cases were the buffer was full.
as you said we can check this counter on the clientsCron and enlarge the buffer when the counter crossed some boundary 
we can also establish some decay function on the counter of the buffer full occasions and on the same cronJob reduce the buffer size when this counter is below some boundary.
What I am more concerned about is the effect of making this a dynamic buffer in terms of cache. we can relocate the entire client struct but that would introduce much complexity.
I will now work to produce benchmark results for this buffer size with different payload sizes so that we can understand better and take correct actions before continue."
977585312,9822,yoav-steinberg,2021-11-24T06:57:25Z,"> we can keep track of cases were the buffer was full. as you said we can check this counter on the clientsCron and enlarge the 
buffer when the counter crossed some boundary...

I'm not sure a counter is the right direction here. When are we going to increase this counter? Different traffic patterns will produce different results but not necessarily meaning a higher value requires a larger buffer. My thought was that if the buffer was maxed out it means we will benefit from a larger buffer. This means a simple flag might be enough.

> What I am more concerned about is the effect of making this a dynamic buffer in terms of cache. we can relocate the entire client struct but that would introduce much complexity. 

I'm not worried about cache because my assumption is that we do the reallocation only periodically and this interval needs to be large enough to minimize performance implications which include both the cache invalidation and then `memcpy` itself that will happen during the reallocation. I think we just need large enough intervals, and in the case of redis between 0.1-1 is probably more than enough (these are the _in practice_ intervals you'll get with the `clientsCron` implementation).

> I will now work to produce benchmark...

Excellent! waiting for the results..


"
978911204,9822,ranshid,2021-11-25T07:35:33Z,"I have made several performance tests.
all tests where done on an m5.2xlarge instance on AWS.
I only wanted to measure get performance so I initially fill up the data with **512 byte**s size values over **3750000 keys**.
each test run for **60** seconds using **5 clients** and different pipeline and reply buffer sizes.

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:12.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{border-top:1.5pt solid windowtext;
	border-right:none;
	border-bottom:none;
	border-left:none;}
.xl66
	{border-top:1.5pt solid windowtext;
	border-right:1.5pt solid windowtext;
	border-bottom:none;
	border-left:none;}
.xl67
	{border-top:none;
	border-right:1.5pt solid windowtext;
	border-bottom:none;
	border-left:none;}
.xl68
	{border-top:none;
	border-right:none;
	border-bottom:1.5pt solid windowtext;
	border-left:none;}
.xl69
	{border-top:none;
	border-right:1.5pt solid windowtext;
	border-bottom:1.5pt solid windowtext;
	border-left:none;}
.xl70
	{border-top:1.5pt solid windowtext;
	border-right:none;
	border-bottom:none;
	border-left:1.5pt solid windowtext;}
.xl71
	{border-top:none;
	border-right:none;
	border-bottom:none;
	border-left:1.5pt solid windowtext;}
.xl72
	{border-top:none;
	border-right:none;
	border-bottom:1.5pt solid windowtext;
	border-left:1.5pt solid windowtext;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



  | tps | p50 | p95
-- | -- | -- | --
P=1 |   |   |  
1k | 111097 | 0.039 | 0.047
4k | 109655 | 0.039 | 0.047
8k | 113081 | 0.039 | 0.047
16k | 111977 | 0.039 | 0.047
32k | 111933 | 0.039 | 0.047
P=5 |   |   |  
1k | 229835 | 0.103 | 0.119
4k | 311151 | 0.071 | 0.087
8k | 314887 | 0.071 | 0.079
16k | 315922 | 0.071 | 0.079
32k | 313676 | 0.071 | 0.079
P=10 |   |   |  
1k | 359264 | 0.119 | 0.143
4k | 357449 | 0.119 | 0.143
8k | 418340 | 0.103 | 0.119
16k | 420073 | 0.103 | 0.119
32k | 417734 | 0.103 | 0.119
P=15 |   |   |  
1k | 422487 | 0.151 | 0.175
4k | 427107 | 0.151 | 0.175
8k | 482625 | 0.135 | 0.159
16k | 486129 | 0.135 | 0.151
32k | 488663 | 0.135 | 0.151



</body>

</html>
"
979197938,9822,oranagra,2021-11-25T13:05:53Z,"a few random notes:
1. regarding the benchmark, i think the objects are too small, 512 byte and pipeline of 15 only 7kb, which is why you can only see a difference of performance between buffer size of 4k and 8k. (btw, which benchmark tool did you use, and did you make sure to CPU is not saturated on either host in this test?)
2. we can't just watch the fact the reply list was populated since there are cases of Deferred reply, which will use the list even if the reply is small.
3. static buffer is faster than a pointer because of indirection indirection, we must keep a static buffer as part of the client struct and relocate the whole struct when needed, but we can maybe skip that if the client is listed in some list (like blocked, etc).
4. when resizing, we wanna avoid a spike of many clients resized at the same time, i.e. many clients connected at the same time and will reach the threshold on the same second.
5. keep it simple!, i think a boolean may be enough, or anyway, let's not make it too complicated."
980547357,9822,yoav-steinberg,2021-11-27T11:58:39Z,"I think the benchmark does show us how significant splitting the reply between the static buffer and the dynamic list is. Once the reply is split into two buffers we see a major performance impact. This is enough to convince us the original idea of having a big pre-allocated buffer is a good idea.
It might be a good idea to try the tests again, but change the code so the pre-allocated buffer is a dynamic buffer allocated in `createClient()` and not part of the `client` struct (in other words make `buf` a `char*`). This will help us asses how important it is to keep the static buffer implementation or which complicates things if there clients is referenced elsewhere."
981107074,9822,ranshid,2021-11-28T15:46:20Z,"> a few random notes:
> 
> 1. regarding the benchmark, i think the objects are too small, 512 byte and pipeline of 15 only 7kb, which is why you can only see a difference of performance between buffer size of 4k and 8k. (btw, which benchmark tool did you use, and did you make sure to CPU is not saturated on either host in this test?)
> 2. we can't just watch the fact the reply list was populated since there are cases of Deferred reply, which will use the list even if the reply is small.
> 3. static buffer is faster than a pointer because of indirection indirection, we must keep a static buffer as part of the client struct and relocate the whole struct when needed, but we can maybe skip that if the client is listed in some list (like blocked, etc).
> 4. when resizing, we wanna avoid a spike of many clients resized at the same time, i.e. many clients connected at the same time and will reach the threshold on the same second.
> 5. keep it simple!, i think a boolean may be enough, or anyway, let's not make it too complicated.

thank you @oranagra 
1/ I agree the value size has major effect on the results. I based the 512 since in our internal statistics this reflects the majority of the value size used. Also note that in my current implementation the reply list is still growing in at least 16K interval (which is also something to consider) So I did expect to only get a single performance drop point for each test case. I can repeat the test with much bigger value sizes, but I think the general concept should be clear by these results.
Per your question - I used redis-benchamrk -P <pipeline value> -c 5 -t get -r 3750000 -n 3750000 and an additional parameter test-duration which was introduced in the benchmark code to limit the test time. the cpu was operating at high rate, but I still think we can have a clear grasp of the performance degradation from these results. however I can perform more tests in order to satisfy our view of the potential degradation. 
2/ Also agree here the main trigger should consider the buffer state and not the existence of the reply list. when I get to detailed design I will address all the issues.
3 / That is also my callout previously regarding the fact that we should reallocate the client struct. however this might greatly complicate the implementation which might raise the question of profitability.
keeping logic for client is pointed by others will be very hard to maintain and will introduce risk of bugs in some scenarios.
I really support avoiding the indirection, but lets first establish the cost of it.
4+5/ 100% agree
"
981107949,9822,ranshid,2021-11-28T15:52:17Z,"> I think the benchmark does show us how significant splitting the reply between the static buffer and the dynamic list is. Once the reply is split into two buffers we see a major performance impact. This is enough to convince us the original idea of having a big pre-allocated buffer is a good idea. It might be a good idea to try the tests again, but change the code so the pre-allocated buffer is a dynamic buffer allocated in `createClient()` and not part of the `client` struct (in other words make `buf` a `char*`). This will help us asses how important it is to keep the static buffer implementation or which complicates things if there clients is referenced elsewhere.

I agree with you @yoav-steinberg . I did perform a comparative test of dynamic VS static buffer:
again with preallocated 512 values in the DB.
redis-benchmark  -P 30 -c 5 -t get -r 3750000 -n 3750000


<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:12.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



  | tps | p50 | p95
-- | -- | -- | --
Static |   |   |  
4k | 542377 | 0.239 | 0.271
16k | 586762 | 0.223 | 0.255
Dynamic |   |   |  
4k | 540891 | 0.239 | 0.271
16k | 585845 | 0.223 | 0.255



</body>

</html>

As you can see the results currently does not reflect any issue with the dynamic buffer (less than 1% TPS degradation)
But I might want to run a more ""realistic"" test with many client machines and 3K clients in order to better support this assumption"
981135467,9822,yoav-steinberg,2021-11-28T19:07:30Z,"> As you can see the results currently does not reflect any issue with the dynamic buffer (less than 1% TPS degradation)
But I might want to run a more ""realistic"" test with many client machines and 3K clients in order to better support this assumption

This is good news. This means we might be able to have a single preallocated `buf` which we can grow/shrink periodically without worrying about the client struct being referenced in other places. 

> when resizing, we wanna avoid a spike of many clients resized at the same time, i.e. many clients connected at the same time and will reach the threshold on the same second.

Regarding reallocating many `buf` at the same time, this will be mitigated by the fact that `clientsCron()` doesn't handle all clients at the same time, to avoid such potential spikes for any per-client cron work. And if this won't be enough we can limit the number of clients processed per-cron cycle even more. This should probably be enough.

"
981344467,9822,oranagra,2021-11-29T07:07:16Z,"I want to stress two points form my past discussions with Salvatore.
1. when i suggested to get rid of the static buffer and replace it with a pointer, IIRC he argued that this will degrade performance (i think he mentioned cache efficiency, which i'm not sure is right, but it might be the indirection). so my point is that if we go this way we need to really test this carefully, maybe even on a smaller replies, like a pipeline of PINGs.
2. IIRC (you can validate that in git blame), clientsCronResizeQueryBuffer has some limit not to try to resize buffers that are smaller than 4kb. this condition was added after complaints that this function caused latency issues (despite the fact we only process certain amount of clients per cron)."
981600569,9822,ranshid,2021-11-29T12:44:39Z,"> I want to stress two points form my past discussions with Salvatore.
> 
> 1. when i suggested to get rid of the static buffer and replace it with a pointer, IIRC he argued that this will degrade performance (i think he mentioned cache efficiency, which i'm not sure is right, but it might be the indirection). so my point is that if we go this way we need to really test this carefully, maybe even on a smaller replies, like a pipeline of PINGs.
> 2. IIRC (you can validate that in git blame), clientsCronResizeQueryBuffer has some limit not to try to resize buffers that are smaller than 4kb. this condition was added after complaints that this function caused latency issues (despite the fact we only process certain amount of clients per cron).

Thank you @oranagra! 
1/ I was originally also concerned regarding the TLB/cache effect of the indirection. I totally agree that in order to validate the cache/indirection effect of moving to dynamic pointer buffer will not introduce degradation.
In AWS we have specific benchmark setup which I already started to adjust for this testing purpose. I will circle back with the results ASAP
2/ ACK - maybe we will keep a lower limit of 4Kb. I will make sure to test small buffer sizes in the tests"
993788434,9822,ranshid,2021-12-14T17:03:28Z,"Hi @oranagra and @yoav-steinberg 
I am sorry for the long delay - I was held back due to some personal + work obligations.

In order to support the dynamic reply buffer I have implemented a very simple POC which does the folowing:
1. allocate an initial 1024 bytes buffer.
2. when the reply buffer is filled we set a flag to increase the buffer size x2 (done in cron job)
3. when the reply buffer is fully written in case the bytes written is up to 1/2 the buffer size we set a flag to shrink it by 1/2 (done in cron job)
4. when the client is idle for IDLE-TIME secs  we set a flag to shrink it by 1/2 (done in cron job)

I have performed some benchmark tests to validate the dynamic buffer overhead:
all tests performed on:
- m5.2xlarge instance 
- 3 io-threads (main+2 threads) 
- 468750 keys with 4K values
- 15 c5n.2xlarge client machines running single threaded redis-benchmark.
- each redis benchmark running 50 clients with only get requests and run for 900 seconds

 in the first test I compared 3 variations:
 1. STATIC - the current static buffer 
 2. NON-STATIC - same as current implementation only buffer is pre allocated during client creation
 3. DYNAMIC - POC version described above.
 

 <html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:12.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{font-weight:700;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



  | NON-STATIC | DYNAMIC | STATIC
-- | -- | -- | --
TPS | 163985 | 163945 | 162103
P90 Latency(ms) | 6.62 | 6.09 | 6.3



</body>

</html>

in the second test I run the STATIC vs DYNAMIC variation **AND used pipeline of 5 commands** 
and set IDLE-TIME to be one of {2,5,7,10}
the reason is to locate the optimal IDLE-TIME to shrink the buffer 
the results are:

<html xmlns:v=""urn:schemas-microsoft-com:vml""
xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:12.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl65
	{font-weight:700;}
.xl66
	{font-size:11.0pt;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



  | TPS | P90 Latency(ms)
-- | -- | --
STATIC | 253523 | 13.103
DYNAMIC/10 secs | 263910 | 13.96
DYNAMIC/7 secs | 247233 | 16.56
DYNAMIC/5 secs | 237149 | 17.103
DYNAMIC/2 secs | 231911 | 17.5



</body>

</html>

IMO the pipeline scenario illustrates a very intensive workload, however I think there is no special reason not to define a high idle time in order to shrink the buffer.
I think the results indicate the dynamic buffer will not introduce a performance degradation, however the dynamic shrinking might introduce some in specific workloads (large values + many clients).
I do not see any reason why the idle time should not be high, since the main purpose is to support long time idle connections.
 
 Please share your thoughts, and if we agree to continue I can introduce the more constructed version of my POC"
993834452,9822,yoav-steinberg,2021-12-14T17:56:36Z,"Thank you @ranshid, this looks promising. A few questions:
- I'm not sure I understand the logic behind the buffer shrinkage. Between cron intervals there might be many cases where we've written 1/2 the buffer but also many cases where we filled it completely. So you'll have both flags set. What do you do? I think what should be done is that if at the beginning of the cron interval the buffer isn't half full we can turn on a flag saying ""POTENTIAL_TO_SHRINK"", if during the interval we pass half the buffer then we turn off the flag. If by the next interval the flag is still on we can shrink.
- I didn't understand what you mean by ""IDLE_TIME""? Is this time of no activity on that client? If so how does this relate to your benchmarks, because surely there's some activity for all clients within the given intervals you checked (2-10sec). Why do we need an idle time shrinking mechanism if we can shrink based on half buffer usage anyway?"
994700518,9822,ranshid,2021-12-15T11:28:45Z,"> Thank you @ranshid, this looks promising. A few questions:
> 
> * I'm not sure I understand the logic behind the buffer shrinkage. Between cron intervals there might be many cases where we've written 1/2 the buffer but also many cases where we filled it completely. So you'll have both flags set. What do you do? I think what should be done is that if at the beginning of the cron interval the buffer isn't half full we can turn on a flag saying ""POTENTIAL_TO_SHRINK"", if during the interval we pass half the buffer then we turn off the flag. If by the next interval the flag is still on we can shrink.
>> [ranshid] @yoav-steinberg, currently for this POC I only allowed one flag to override the other. so in the case of this POC the last flag set before the cron run was serviced and cleared after the buffer adjustments. this was mainly for POC as I knew in my POC all key values were the same. it is true That a potential logic would be to always clear the shrink flag in case we write more than half the buffer (or maybe some other logic) 
> * I didn't understand what you mean by ""IDLE_TIME""? Is this time of no activity on that client? If so how does this relate to your benchmarks, because surely there's some activity for all clients within the given intervals you checked (2-10sec). Why do we need an idle time shrinking mechanism if we can shrink based on half buffer usage anyway?
>> [ranshid] well I think that shrinking the buffer in case of idle client is important. for example in case when the last write on the client used more than half the buffer. there will be no more writes and the buffer will not be shrinked. I think the query buffer acts the same after client is idle for 2 seconds.
also I have checked the statistics I added for the POC to monitor the number of expansions and shrinkage during test of IDLE-TIME=2 and IDLE-TIME=7:
for the IDLE-TIME=7:

> reply_buffer_shrink:132
> reply_buffer_idle_shrink:10
> reply_buffer_idle_expends:0

while for the IDLE-TIME=2:

> reply_buffer_shrink:314
> reply_buffer_idle_shrink:192
> reply_buffer_expends:186

I think though the numbers are not high, given the tests are identical in terms of setup and workload, we should expect no expansions. 

**BTW: I was wrong before claiming the initial buffer size is 1Kib. in my POC it is 16Kib...**




"
995522720,9822,yoav-steinberg,2021-12-16T07:49:56Z,"According to this design proposal:
> I think what should be done is that if at the beginning of the cron interval the buffer isn't half full we can turn on a flag saying 
""POTENTIAL_TO_SHRINK"", if during the interval we pass half the buffer then we turn off the flag. If by the next interval the flag is still on we can shrink.

You won't need the idle mechanism because shrinking will be handled in the cron even when there's no traffic. 

In any case I still don't understand why you had **any** idle time in your benchmarks. I also think that your benchmark might be shrinking the buffer too often in case both flags contradict. I might be wrong because I have a feeling I don't totally understand your logic in the POC. But since the results seem promising I suggest you create a (possibly draft)PR of your code so we can review the algorithm. @oranagra does this make sense?
"
998056588,9822,ranshid,2021-12-20T16:02:20Z,"@yoav-steinberg @oranagra - I pushed the latest POC implementation of the dynamic reply buf size.
the main changes are:
1. maintain per client buf_peak - I think that a better option than observe the last write position is to maintain a peak observed between cron runs. I reset the peak after each cron iteration on the client thus I am able to ignore the IDLE check.
2. the shrink/expend logic is maintained only in cron job according to the following logic:
   - in case the last observed peak was equal to the buffer size - double the buffer size
   - in case the last observed peak was less than half the buffer size - shrink the buffer in half.
   - otherwise do nothing
3. I added counters for the number of expends and shrinks (global counters) and added them in the info stats.
4. added some reply buffer info in the client list.

I have retested benchmark results in compare to the static buffer case:
<html xmlns:o=""urn:schemas-microsoft-com:office:office""
xmlns:x=""urn:schemas-microsoft-com:office:excel""
xmlns=""http://www.w3.org/TR/REC-html40"">

<head>

<meta name=ProgId content=Excel.Sheet>
<meta name=Generator content=""Microsoft Excel 15"">
<link id=Main-File rel=Main-File
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip.htm"">
<link rel=File-List
href=""file:///C:/Users/ranshid/AppData/Local/Temp/msohtmlclip1/01/clip_filelist.xml"">
<style>
<!--table
	{mso-displayed-decimal-separator:""\."";
	mso-displayed-thousand-separator:""\,"";}
@page
	{margin:.75in .7in .75in .7in;
	mso-header-margin:.3in;
	mso-footer-margin:.3in;}
tr
	{mso-height-source:auto;}
col
	{mso-width-source:auto;}
br
	{mso-data-placement:same-cell;}
td
	{padding-top:1px;
	padding-right:1px;
	padding-left:1px;
	mso-ignore:padding;
	color:black;
	font-size:12.0pt;
	font-weight:400;
	font-style:normal;
	text-decoration:none;
	font-family:Calibri, sans-serif;
	mso-font-charset:0;
	mso-number-format:General;
	text-align:general;
	vertical-align:bottom;
	border:none;
	mso-background-source:auto;
	mso-pattern:auto;
	mso-protection:locked visible;
	white-space:nowrap;
	mso-rotate:0;}
.xl63
	{font-weight:700;}
-->
</style>
</head>

<body link=""#0563C1"" vlink=""#954F72"">



  |   |  
-- | -- | --
  | DYNAMIC | STATIC
TPS | 164353 | 164667
P90 Latency(ms) | 6.1 | 6.1



</body>

</html>
"
999394391,9822,yoav-steinberg,2021-12-22T08:46:34Z,@ranshid Can you update the PR with your new design (instead of config option for buffer size) and update the title accordingly.
999539705,9822,ranshid,2021-12-22T12:26:17Z,"> @ranshid Can you update the PR with your new design (instead of config option for buffer size) and update the title accordingly.

@yoav-steinberg done.
Also made some cache misses analysis and added it to the PR.
Thank you for reviewing this!"
1027265682,9822,oranagra,2022-02-01T20:38:51Z,"Hi, i'm finally free to take another look at this.
I understand that the benchmark mentioned at the top done with pipeline of 1, right?
are you sure that in this case the bottleneck wasn't the network?
i.e. in order to measure the impact of the extra indirection, we need the CPU to be the bottleneck (loaded to 100%).
@filipecosta90 maybe you have additional feedback or a quick way to test this."
1027267996,9822,oranagra,2022-02-01T20:42:00Z,"The other concern in terms of benchmark is the effect of resizing.
i.e. an app that uses a lot of dormant connections that suddenly start loading the output buffer with a lot of data, and then goes silent (all connections are growing or shrinking at the same time).
this PR will probably introduce a regression for this case, which is expected since that's also the case where we save a lot of memory, but maybe we can limit the impact by throttling the growing and shrinking."
1027925450,9822,ranshid,2022-02-02T13:10:10Z,"> Hi, i'm finally free to take another look at this. I understand that the benchmark mentioned at the top done with pipeline of 1, right? are you sure that in this case the bottleneck wasn't the network? i.e. in order to measure the impact of the extra indirection, we need the CPU to be the bottleneck (loaded to 100%). @filipecosta90 maybe you have additional feedback or a quick way to test this.

Thank you @oranagra for taking the time to review this!

Indeed in this case I used a pipeline of 1.
I checked and during the test all engine and io-threads were running in 100% CPU and the network limit was verified by our tools to NOT reach the limits.
"
1027931897,9822,ranshid,2022-02-02T13:18:01Z,"> The other concern in terms of benchmark is the effect of resizing. i.e. an app that uses a lot of dormant connections that suddenly start loading the output buffer with a lot of data, and then goes silent (all connections are growing or shrinking at the same time). this PR will probably introduce a regression for this case, which is expected since that's also the case where we save a lot of memory, but maybe we can limit the impact by throttling the growing and shrinking.

I understand the concern here. I wonder if the result of throttling the rate of change (like the number of resizes per second) will not cause other type of degradation due to dynamic list allocations/deallocations. 
I can look into this or simply try to make cron job operate the resizing every n runs instead of each run. I actually tested some variation of it but I will have to repeat performance tests in order to be sure about the results.  "
1028913477,9822,oranagra,2022-02-03T11:51:48Z,"@ranshid FYI, i'd rather avoid force-pushes (amending commits and rebase-merges).
these are a bit harder to review and realize what's changed, and since we use squash-merge for most PRs, it doesn't really matter for the end result."
1028944047,9822,ranshid,2022-02-03T12:32:25Z,"> @ranshid FYI, i'd rather avoid force-pushes (amending commits and rebase-merges). these are a bit harder to review and realize what's changed, and since we use squash-merge for most PRs, it doesn't really matter for the end result.

@oranagra no problem - point taken."
1029272969,9822,oranagra,2022-02-03T18:22:30Z,"I'm having a hard time reviewing the recent changes to to the force push that mixed a rebase and other changes https://github.com/redis/redis/compare/9e16df520ce15a1d3702620850a20f407539cc13..873f0b87fdb653e3e58cea20c93aab1267ceafbe
Do you remember what you changed other than the cron timing? "
1031172064,9822,ranshid,2022-02-07T08:00:39Z,"> I'm having a hard time reviewing the recent changes to to the force push that mixed a rebase and other changes https://github.com/redis/redis/compare/9e16df520ce15a1d3702620850a20f407539cc13..873f0b87fdb653e3e58cea20c93aab1267ceafbe Do you remember what you changed other than the cron timing?

@oranagra I am very sorry for that!

my changes where:
1. fix the introspection test
2. fix the cron function to reset the peak every 5 seconds
3. align the cron boundary check on the  minimum/maximum resize threshold
4. fix the styling issues you raised"
1031791720,9822,ranshid,2022-02-07T18:37:35Z,"@oranagra / @yoav-steinberg thank you for this educating review (one of my first PRs so it might be a bit glitchy :) )
I might have located some race condition in case the cron will resize the buffer while the bufpos is not 0 (I suspected a potential issue there) I will start debugging it and circle back here.
Sorry if this is taking longer. "
1039918374,9822,oranagra,2022-02-15T06:52:23Z,Full CI: https://github.com/redis/redis/actions/runs/1845442677
1040453120,9822,oranagra,2022-02-15T15:58:07Z,re-run the CI https://github.com/redis/redis/actions/runs/1845442677
1040675854,9822,oranagra,2022-02-15T19:08:11Z,"summary of failures:
failed 6 times:
```
*** [err]: evict clients in right order (large to small) in tests/unit/client-eviction.tcl
Expected '1' to be equal to '0' (context: type eval line 54 cmd {assert_equal $count 0} proc ::test)
```
failed 2 times:
```
*** [err]: idle client buffer maximum shrink to 1kib in tests/unit/replybufsize.tcl
Expected '1032' to be equal to '1024' (context: type eval line 12 cmd {assert_equal $rbs 1024} proc ::test)
*** [err]: busy client buffer maximum enlarge to 16kib in tests/unit/replybufsize.tcl
Expected '1032' to be equal to '1024' (context: type eval line 14 cmd {assert_equal $rbs 1024} proc ::test)
```
failed once (might be unrelated):
```
16:33:31> Use takeover to bring slaves back: FAILED: caught an error in the test ERR You should send CLUSTER FAILOVER to a replica
ERR You should send CLUSTER FAILOVER to a replica
```
"
1046110181,9822,filipecosta90,2022-02-19T21:51:48Z,"@oranagra using the 14 benchmarks we have on the OSS benchmark spec we can:
- Detect a total of 2 improvements above the improvement water line.
   - 13.6% on the HGETALL test with hashes with 5 fields with 100B each field
   - 6.5% on LPOP RPOP benchmark with lists with 100B values on each element
- Detect a total of 9 stable tests between versions.
- Note that 2 benchmarks are unstable (high variance) on the unstable branch so we discard their numbers of the analysis.
```
redisbench-admin compare --triggering_env ci   --baseline-branch unstable --comparison-branch ranshid:oss-dynamic-client-size  --github_repo redis --github_org redis  --use_metric_context_path true --metric_name Ops/sec --testname_regex .+Totals
Effective log level set to INFO
2022-02-19 21:43:31,713 INFO Using: redisbench-admin 0.6.23
2022-02-19 21:43:33,056 INFO Using deployment_type=oss-standalone and deployment_name=oss-standalone for the analysis
2022-02-19 21:43:33,056 INFO Using a time-delta from 7 days ago to a second ago
2022-02-19 21:43:33,475 WARNING Based on test-cases set (key=ci.benchmarks.redislabs/ci/redis/redis:testcases_AND_metric_context_path) we have 14 comparison points. 
2022-02-19 21:43:43,392 INFO Printing differential analysis between branches
# Comparison between unstable and ranshid:oss-dynamic-client-size for metric: Ops/sec. Time Period from 7 days ago to a second ago
|                                   Test Case                                    |   Baseline (median obs. +- std.dev)    |Comparison (median obs. +- std.dev)|% change (higher-better)|             Note              |
|--------------------------------------------------------------------------------|----------------------------------------|-----------------------------------|------------------------|-------------------------------|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10:Totals| 312220 +- 20.6% UNSTABLE (5 datapoints)| 325352 +- 0.7%  (2 datapoints)    |4.2%                    |UNSTABLE (very high variance)  |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values:Totals            | 134161 +- 0.5%  (5 datapoints)         | 131942 +- 0.4%  (2 datapoints)    |-1.7%                   |-- no change --                |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10:Totals | 364426 +- 4.0%  (5 datapoints)         | 361472 +- 2.6%  (2 datapoints)    |-0.8%                   |-- no change --                |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values:Totals             | 143372 +- 4.3%  (5 datapoints)         | 134843 +- 7.5%  (2 datapoints)    |-5.9%                   |waterline=7.5%. -- no change --|
|memtier_benchmark-1Mkeys-100B-expire-use-case:Totals                            | 219014 +- 2.3%  (5 datapoints)         | 218677 +- 0.1%  (2 datapoints)    |-0.2%                   |-- no change --                |
|memtier_benchmark-1Mkeys-10B-expire-use-case:Totals                             | 217413 +- 0.4%  (5 datapoints)         | 218433 +- 1.2%  (2 datapoints)    |0.5%                    |-- no change --                |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case:Totals                            | 217806 +- 0.4%  (5 datapoints)         | 216338 +- 1.1%  (2 datapoints)    |-0.7%                   |-- no change --                |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case:Totals                            | 217860 +- 2.0%  (5 datapoints)         | 216596 +- 0.3%  (2 datapoints)    |-0.6%                   |-- no change --                |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values:Totals  | 169042 +- 1.9%  (5 datapoints)         | 191952 +- 4.6%  (2 datapoints)    |13.6%                   |IMPROVEMENT                    |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values:Totals                 | 165841 +- 4.1%  (5 datapoints)         | 176937 +- 5.6%  (2 datapoints)    |6.7%                    |waterline=5.6%. IMPROVEMENT    |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10:Totals| 140570 +- 0.3%  (5 datapoints)         | 138587 +- 2.8%  (2 datapoints)    |-1.4%                   |-- no change --                |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values:Totals            | 99571 +- 14.4% UNSTABLE (5 datapoints) | 98885 +- 1.5%  (2 datapoints)     |-0.7%                   |UNSTABLE (very high variance)  |
|memtier_benchmark-1Mkeys-load-list-with-100B-values:Totals                      | 150928 +- 1.8%  (5 datapoints)         | 148048 +- 4.8%  (2 datapoints)    |-1.9%                   |-- no change --                |
2022-02-19 21:43:43,464 INFO Detected a total of 9 stable tests between versions.
2022-02-19 21:43:43,464 WARNING Detected a total of 2 highly unstable benchmarks.
2022-02-19 21:43:43,464 INFO Detected a total of 2 improvements above the improvement water line.
```"
1046174356,9822,oranagra,2022-02-20T06:42:35Z,"> Detect a total of 2 improvements above the improvement water line.
13.6% on the HGETALL test with hashes with 5 fields with 100B each field
6.5% on LPOP RPOP benchmark with lists with 100B values on each element

interesting. i was not expecting any improvement in performance, only hoping for no regression.
@ranshid @yoav-steinberg do you have any idea what could cause this?

@ranshid looking at the changes this PR [brings](https://github.com/redis/redis/pull/9822/files) it looks like it attempts to revert many of the recent changes.
please fix."
1046183086,9822,ranshid,2022-02-20T07:55:26Z,"> > Detect a total of 2 improvements above the improvement water line.
> > 13.6% on the HGETALL test with hashes with 5 fields with 100B each field
> > 6.5% on LPOP RPOP benchmark with lists with 100B values on each element
> 
> interesting. i was not expecting any improvement in performance, only hoping for no regression. @ranshid @yoav-steinberg do you have any idea what could cause this?
> 
> @ranshid looking at the changes this PR [brings](https://github.com/redis/redis/pull/9822/files) it looks like it attempts to revert many of the recent changes. please fix.

@oranagra I fixed the PR commits.
regarding the performance tests: this is hard to tell as I was also hoping only for no performance degradation. 
@filipecosta90 do we have any system metrics to match with the benchmark (maybe some perf recordings which are operated in the background during the tests? syscalls/cache misses/TLB misses etc...)"
1046663335,9822,oranagra,2022-02-21T09:39:00Z,"@redis/core-team please approve.
there are some interface (introspection) changes, in addition of course to the new mechanism that has no interfaces, but is a major change with some potential for issues."
1842129847,12822,guybe7,2023-12-06T05:48:00Z,@vitarb can you please share your thoughts?
1845957570,12822,madolson,2023-12-07T19:15:06Z,"I guess I'm still not convinced that we need a separate entity that differentiates the DB from the Dict Array, to me they still seem one and the same. There is a lot of logic being silo'd out into the Dictarray that is still pretty heavily coupled to Redis, cursors and per-slot information for sampling. "
1862439640,12822,guybe7,2023-12-19T09:48:55Z,"Note that the merge commit [Merge remote-tracking branch 'origin/unstable' into dictarray](https://github.com/redis/redis/pull/12822/commits/22c7e0941fe4c4abb2180f2ac4955bb73feb4b83) inserts the logical change of moving the `resharding` list from the server struct into `dictarray`
i.e. in some way handling portion of #12848 in the reverse way."
1867954869,12822,hpatro,2023-12-22T18:14:05Z,"@guybe7 Thanks for working on this. 

Not right away evident to me, When does the code developer life becomes easier or avoids them making mistake with this refactoring? Could you highlight some API access or usage pattern? "
1868751199,12822,guybe7,2023-12-25T05:33:30Z,"hi @hpatro the main motivation is code cleanliness.
IMHO the dict-array code is self-contained enough to deserve its own file and API."
1873698807,12822,soloestoy,2024-01-02T07:42:59Z,"To be honest, based on the literal meaning, I prefer DB over DA."
1874246842,12822,oranagra,2024-01-02T16:24:36Z,"this PR was discussed in a core-team meeting and was conceptually approved.
some things remain to be argued about like:
1. the abbreviation of ""DA"" which we don't like.
2. the fact we would be maintaining the BIT data structure for shard_channels (may rarely be needed, e.g. for iteration, but on the other hand should have a low overhead).
3. i have a feeling i forgot one.

@madolson @soloestoy please comment."
1891285223,12822,guybe7,2024-01-15T04:30:30Z,"@madolson @soloestoy i think i've resolved most of the issues raised, can you please take a look?"
1920536317,12822,guybe7,2024-02-01T05:21:10Z,"@oranagra worth mentioning that due to the recent change the reply of DEBUG HTSTATS changed, in case of no keys were ever added to the db.

before:
```
127.0.0.1:6379> DEBUG htstats 9
[Dictionary HT]
Hash table 0 stats (main hash table):
No stats available for empty dictionaries
[Expires HT]
Hash table 0 stats (main hash table):
No stats available for empty dictionaries
```

after:
```
127.0.0.1:6379> DEBUG htstats 9
[Dictionary HT]
[Expires HT]
```"
1920572493,12822,oranagra,2024-02-01T06:00:51Z,"ok, i don't mind. but please mention it in the top comment."
1926409258,12822,oranagra,2024-02-05T07:57:37Z,"i'd like to merge the PR as is, and maybe address the last remaining comment later.
any objections?"
930424227,9564,hwware,2021-09-29T18:12:55Z,"@madolson  Could you please update https://github.com/redis/redis/issues/8948 on part: Human readable names for nodes to add this PR link, thanks"
932866870,9564,madolson,2021-10-03T05:35:01Z,@hwware Sure. Should we be propagating the cluster node name to other nodes?
934543383,9564,hwware,2021-10-05T16:01:16Z,"@madolson do you mean propogating nodename or hostname?
I have added the humanreadable nodename to the CLUSTER NODES commands shown below:
![image](https://user-images.githubusercontent.com/51993843/136056065-16dba800-09bd-4a7a-8f15-9054815b0ddc.png)
"
935265940,9564,madolson,2021-10-06T02:12:36Z,"Yes, sorry, I meant node name."
941528820,9564,hwware,2021-10-12T20:50:16Z,"Hey @madolson , is there anything that needs to added for this PR or is it fine?"
956614386,9564,hwware,2021-11-01T21:19:19Z,"Hey @madolson , I have updated the code and added the gossip functionality for the human readable name. And the node's name can be set manually by using the command: CLUSTER SETNAME. Please take a look.
Thank you."
1009378790,9564,madolson,2022-01-10T21:52:23Z,"@hwware Hey, now that the hostname extension framework is merged, do you want to follow up on this? I think it should be easy to add a new extension that is the nodename."
1010386774,9564,hwware,2022-01-11T21:44:51Z,"> @hwware Hey, now that the hostname extension framework is merged, do you want to follow up on this? I think it should be easy to add a new extension that is the nodename.

Thanks Madolson, I will start working on this PR. Thanks"
1031585342,9564,hwware,2022-02-07T15:21:07Z,"Hi @madolson , I agree that this can be used as debugging/human readable name that would be exposed in places like info and logging. This would be better than having a node ID when trying to debug any issue with a particular node."
1079956732,9564,zeekling,2022-03-27T15:41:28Z,"i think we can change the ip to hostname, and we can use hostname to identify the node.  no need add nodename 

![image](https://user-images.githubusercontent.com/17929800/160288781-232ca08e-ef23-4b53-b895-33838974af1c.png)
"
1080130046,9564,enjoy-binbin,2022-03-28T03:05:20Z,"@zeekling we cannot easily modify the output of the command (any commands), it will be a breaking change
and also node IP is useful information in any case i think, no reason to ""remove"" it"
1094064504,9564,zeekling,2022-04-09T15:12:51Z,"> @zeekling we cannot easily modify the output of the command (any commands), it will be a breaking change and also node IP is useful information in any case i think, no reason to ""remove"" it

ok, Another question, can you confirm the impact on large clusters.For example: the cluster has 300 masters and 300 slaves"
1099198048,9564,hwware,2022-04-14T13:39:45Z,"@madolson , I have added some tests, let me know if you think they are fine. Thanks!"
1504064988,9564,hwware,2023-04-11T20:44:08Z,"Hey @madolson , I have updated the code as per the comments. Please take a look.
Thank you."
1509288586,9564,madolson,2023-04-14T21:21:17Z,"@redis/core-team Another small change that is probably suitable for 7.2. It adds a human readable node-name that is available in logging for Redis clusters that is gossiped, so that node failures/failover authorizations can also include the logical name. The only ""major change"" part is the config and the fact it's exposed in `CLUSTER NODES`. "
1513364864,9564,hwware,2023-04-18T15:28:50Z,"> 44c0651a6ccff0dbb67f6219d4ee126754eb3a03 
> 127.0.0.1:7383@17383,third,nodename=XXXX, shard-id=0a01a28e96ae3c335b97eab13ad8757807f2a375 
> master - 0 1675886944444 2 connected



> comments just to be sure i understand it correctly. i.e. it's just a label of free text, right? and is only displayed in errors and the (admin oriented) CLUSTER NODES command..

Not really, although it is a label of free text, but it will display in the result of  CLUSTER NODES command, not only error happens. "
1550450247,9564,madolson,2023-05-16T22:41:24Z,"I thought I commented on this, we decided to merge this change, https://github.com/redis/redis/pull/12166, first and then we will merge this."
1572272931,9564,hwware,2023-06-01T15:28:58Z,"> I thought I commented on this, we decided to merge this change, #12166, first and then we will merge this.

Hello @madolson , #12166 is merged and I have also resolved all the conflicts. Kindly review."
1577882835,9564,madolson,2023-06-06T04:23:34Z,"@hwware We discussed this previously, but I'm going to double check no one else wants to review it, and then will merge it tomorrow."
1586036280,9564,oranagra,2023-06-11T06:22:47Z,"the top comment needs an update, right?
i.e. that's a new config that affects the generated config files, and some logging, but doesn't affect any commands at the moment, right?
please make sure the top comment / commit comment mentions that, as well as reasoning and future plans.
thanks."
1592304253,9564,madolson,2023-06-15T03:43:44Z,"Sorry for the delay, just got behind. Everything should be updated and consistent now."
1592364341,9564,madolson,2023-06-15T05:06:26Z,"@hwware Will you double check the top comment and look at my last commit. I tried to write a slightly better test, simplify the code, and updated the redis.conf text. "
1593288039,9564,hwware,2023-06-15T15:26:53Z,"> 

Thanks @madolson for updating them. It looks good , Sorry I missed the @oranagra comments."
1595947076,9564,madolson,2023-06-18T04:12:18Z,https://github.com/redis/redis/actions/runs/5301569607
1597330863,9564,oranagra,2023-06-19T14:54:20Z,"sporadic test failure:
```
*** [err]: Human nodenames are visible in log messages in tests/unit/cluster/human-announced-nodename.tcl
log message of '""*Node * (nodename-1) reported node * (nodename-0) as not reachable*""' not found in ./tests/tmp/server.6806.100/stdout after line: 0 till line: 42
```
https://github.com/redis/redis/actions/runs/5313162983/jobs/9618669303?pr=12159"
1598093285,9564,madolson,2023-06-20T04:20:38Z,I think this should fix it: https://github.com/redis/redis/pull/12330.
1125460355,10515,jhelbaum,2022-05-12T22:01:39Z,"Thanks for your comments. Sorry it's taken so long for me to get back to you. Responses inline:

> i think this approach of first tagging the arguments that are matched and then generating the hint for the reminder of the line based on these booleans is too limited.

Regarding the general approach, I'm open to suggestions. I haven't been able to come up with anything dramatically better. We have to match up arguments with input words in order to know which words and which arguments have been consumed and shouldn't be hinted. And we have to generate a hint describing any arguments which might still be matched. Some of the edge cases are tricky, though, and you've found a few of them.

(Nitpick: The matching isn't boolean, it's numeric, since one argument entry can match more than one input word, and an argument entry can be partially matched, such as a token without its parameter value.)

> for instance, if the last match is a token which should be followed by a mandatory argument, we're unable (and won't be able) to handle it properly. for instance typing `SET key value EX` should have hinted either only `<seconds>`. or adding `[NX | XX] [GET]`, but currently it hints the `seconds` last and suggests you add NX after EX. i'd argue that this is worse than the previous one which was easy to see that got broken and stopped responding)

Actually, I thought I was handling this scenario correctly. You seem to have found a bug. I'll investigate.

Your first and third examples are cases I believe I can handle. I'll try to fix them.

The second example is trickier, but it might also be doable. It's a bit problematic, though.

Consider the following partial input:

`ZADD k IN`

This is presumably the start of the token `INCR`, but it hasn't been matched yet. Do we want to guess this and suggest the completion of the word `INCR`? We currently only match whole words, not prefixes.

But `ZADD k 3` is presumably the start of a score, which we could know only by trying to validate the numeric data type - so far the code doesn't do any type checking.

And, of course, if the next mandatory argument was string-valued, we'd have no way to know whether `IN` was the start of `INCR` or the next mandatory string argument.


> * what exactly do we do now for older servers (i didn't bother to check yet).

The code currently falls back to the previous hinting algorithm if no arg specs are available.


> * do we wanna re-generate help.h so that we can do fancy things even with older servers?

Interesting. Is it that common that an upgraded CLI is run with an old server? If it's worth the effort, that can be left for a future task.
"
1126154958,10515,oranagra,2022-05-13T15:03:30Z,"regarding the mandatory arguments for tokens, i don't see how you could handle these, but please try to fix the bug and i'll review.
maybe that would be acceptable.

regarding optional arguments coming before mandatory ones, maybe we could have some code to realize we've hit a mandatory argument, and go mark all the optional ones before it as irrelevant (or keep a pointer to the first argument that's relevant going forward)."
1130728071,10515,jhelbaum,2022-05-18T23:28:21Z,"Okay, I think I've fixed the cases we discussed earlier. Let me know what you think of them, and the behavior in other situations.

I've also added a test suite for the hint mechanism. 

(I also seem to have messed up my branch of the repo, but I'll fix that later... sorry...)"
1133923080,10515,oranagra,2022-05-22T15:44:11Z,"haven't reviewed the code yet, but i did give it a quick test.
so first off, it does work **a lot** better! thank you!

here are a few places were i see room for improvement:

* ![image](https://user-images.githubusercontent.com/7045099/169703350-f2a76ebe-f088-4d25-8407-ac358402ea8a.png)
  this works good (typing `p` doesn't yet do anything)

* ![image](https://user-images.githubusercontent.com/7045099/169703397-066f9445-6362-4421-94a5-de3ade0ace70.png)
  but then, i add `x` and it hides everything (since it's matching `px`, but it could also be `pxat`, so i'd rather do the matching only after hitting space (i.e. exclude the last non-terminated word from the matching and add it only when the user presses space).

* ![image](https://user-images.githubusercontent.com/7045099/169703284-dd2333eb-7780-44b4-864f-3f290ce68204.png)
  for some reason, when i type an unrecognized keyword, it messes up the rest of the matching, maybe that's avoidable.

* ![image](https://user-images.githubusercontent.com/7045099/169703588-792cdd42-9b14-4ec2-8efb-49259a8937de.png)
  if i don't ""consume"" the `ANY` arg, it keeps hunting me like a ghost, even if i already moved on and it's no longer valid. 

* ![image](https://user-images.githubusercontent.com/7045099/169703702-0ff4f7fe-af26-4691-8aa6-55b35a12bf3f.png)
  if i type an incomplete keyword here (i.e. the `n`), it assume it's a score and hides all the optional args.

* ![image](https://user-images.githubusercontent.com/7045099/169703717-d72a3439-9d29-461b-bbb6-1d48ddc398fb.png)
  and then when i add the `x` of `nx` it recognizes it's not a score, and re-adds all the optional args.

* ![image](https://user-images.githubusercontent.com/7045099/169703478-d3b6bb84-d16a-4d30-ad67-2a8b79f7fe96.png)
  when i type the initial `score`, it suggests `member` next, but when when i type another score, it still suggests `[score member]`
ideally, it would suggest `member [score member]`. but that's petty, so just a small room for improvement, certainly not a must.

"
1134651797,10515,jhelbaum,2022-05-23T13:04:33Z,Thanks - those are some great examples. I'll go through them and see what can be improved. Will update you when there's news.
1138568102,10515,jhelbaum,2022-05-26T13:18:27Z,"> i'd rather do the matching only after hitting space

Do you mean in general? Only match the last word after the user hits space? That's not the behavior of the old hints. They appear as soon as the command name is complete, and then they match each word as soon as its first character is typed.

We can change that behavior in general, and maybe we should. But it would be a change from the current behavior."
1138651560,10515,oranagra,2022-05-26T14:40:38Z,"for positional arguments, maybe we can mark them as matched as soon as one character is typed.
but for optional args (specifically when there's more than one option), it could be a problem.
what bothered me specifically was when you type `PX`, it immediately hides all the options and shows ""milliseconds"", but it could still be PXAT.

i'm not certain, but i feel it might be better."
1140521054,10515,jhelbaum,2022-05-29T20:40:12Z,"> * ![image](https://user-images.githubusercontent.com/7045099/169703397-066f9445-6362-4421-94a5-de3ade0ace70.png)
>   but then, i add `x` and it hides everything (since it's matching `px`, but it could also be `pxat`, so i'd rather do the matching only after hitting space (i.e. exclude the last non-terminated word from the matching and add it only when the user presses space).

Most of the glitches you point out in this comment can be solved by deciding that we only match words after the space is typed. I've made that change and I prefer the behavior that way. I'm inclined to adopt that rule in general.

> * ![image](https://user-images.githubusercontent.com/7045099/169703284-dd2333eb-7780-44b4-864f-3f290ce68204.png)
>   for some reason, when i type an unrecognized keyword, it messes up the rest of the matching, maybe that's avoidable.

I'm not sure that's avoidable. The unrecognized keyword is not valid. I think the only solution would be to stop hinting if an unmatchable word is entered. 


> * ![image](https://user-images.githubusercontent.com/7045099/169703588-792cdd42-9b14-4ec2-8efb-49259a8937de.png)
>   if i don't ""consume"" the `ANY` arg, it keeps hunting me like a ghost, even if i already moved on and it's no longer valid.

This one is tricky. I need to think a bit more about how to fix it. 


>   when i type the initial `score`, it suggests `member` next, but when when i type another score, it still suggests `[score member]`
>   ideally, it would suggest `member [score member]`. but that's petty, so just a small room for improvement, certainly not a must.

I agree that it's not ideal, but I don't think it's easy to solve and I'm not inclined to spend time on it for now."
1148674803,10515,oranagra,2022-06-07T13:27:23Z,"@jhelbaum i took anther quick look (mostly at the tests, not the code).
AFAICT the main remaining issue that's hard to overlook is the problem with the ANY after COUNT in GEORADIUS.

regarding the other issues.
* i think that after an unrecognized input, we can simply stop hinting (might be better than showing irrelevant options).
* the problem i mentioned about ZADD is arguably even more annoying in BITFIELD after repeating the first action
"
1151708076,10515,jhelbaum,2022-06-09T23:25:14Z,"@oranagra Okay, I believe I've fixed all the issues we've discussed.

The code now handles:
* Repeated arguments
* Unmatched optional arguments ""orphaned"" by a subsequent matched argument
* Disabling hinting when argument matching fails
* Type validation of integers and doubles

I can't rule out the possibility that I've overlooked some edge cases, but I think it's pretty solid in most circumstances.

Let me know what you think."
1151709169,10515,jhelbaum,2022-06-09T23:27:32Z,"(I should note that changes to the command arg specs will break the unit tests for the hints, since the tests use the current command docs. This just happened with `BITFIELD_RO`.)"
1169668366,10515,jhelbaum,2022-06-29T08:10:27Z,"I'm not sure how soon I'll have the time to do more work on this. When I do, I guess I'll start with separating the commands structs."
1200428623,10515,jhelbaum,2022-07-31T13:45:46Z,"@oranagra I don't know when/if I'll have the time to do this refactoring properly. I'm not sure how best to proceed.

We ended up in this situation because of the desire to support pre-7.0 servers with the best possible hinting. That required static linking of the command argument specs in the CLI, which then either needs to duplicate the command tables for the client and server, or to refactor the command struct to separate the server-specific runtime parts from the shared syntax specs. That refactoring is nontrivial, since the command struct is used widely in the server code.

I started working on the refactoring but am now struggling with some memory bugs, and in any case it's going to be a big change. I can't promise to spend much more time on it.

So that's where this currently stands. I think the improved hinting would be a very nice feature, but the question is how much effort is it worth to support legacy servers, and how long it will take to get the rest of the work done."
1211821672,10515,jhelbaum,2022-08-11T10:43:06Z,@oranagra I think I've made all the changes you asked for except for sharing the arg structs. Let me know what you think when you have the chance.
1211827067,10515,guybe7,2022-08-11T10:49:04Z,@jhelbaum heads up: https://github.com/redis/redis/pull/11051
1211839096,10515,jhelbaum,2022-08-11T11:02:48Z,@guybe7 So I assume I should wait until #11051 is merged before moving forward with this?
1211844704,10515,guybe7,2022-08-11T11:08:50Z,"@jhelbaum i guess so
i'll try to get it merged tomorrow"
1212323313,10515,jhelbaum,2022-08-11T18:10:28Z,"@oranagra

I removed `key_specs_static` etc. I hope I did it right. The tests pass. "
1212391480,10515,jhelbaum,2022-08-11T19:19:05Z,"Okay, something is leaking from the changes I made to module.c... not sure what I did wrong."
1214646058,10515,oranagra,2022-08-15T05:57:22Z,ok. so what's next? are we conceptually done?
1214859569,10515,jhelbaum,2022-08-15T10:21:05Z,"As far as I'm concerned, it's done. The code is feature complete, and I think includes all the appropriate refactorings. I assume it could use to be reviewed more thoroughly, and people might want to interact with it and try to find more buggy edge cases."
1219415415,10515,guybe7,2022-08-18T12:10:57Z,@jhelbaum https://github.com/redis/redis/pull/11051 is merged
1221484614,10515,jhelbaum,2022-08-21T07:10:52Z,"> @jhelbaum #11051 is merged

Thanks. I updated this PR."
1252286023,10515,jhelbaum,2022-09-20T12:30:55Z,@oranagra Is there any plan for moving this change forward? 
1253362025,10515,oranagra,2022-09-21T08:16:24Z,"@jhelbaum my apologies, i do wanna merge it but currently don't have the time to review it properly.
i did review bits of it, and provided guidelines for the design decisions we made, so i'm quite confident these are ok.
maybe if we can find a way to split it, find another person with more free time to review the redis-cli.c part, and i will make sure to allocate time to review the other files?"
1463745467,10515,oranagra,2023-03-10T12:37:13Z,"Thanks for taking a look. At the time I did review all the parts outside redis-cli.c and did skim through redis-cli.c to provide guidance and advice. But I felt the code need an extra pair of eyes to review it aside from the author and was waiting for someone to step up. I agree the risk for issues is small but still don't like to merge a mass of unreviewed code.
Maybe the level of review you did is enough? 
I suppose there's no need to explicitly look for bugs, but I think we do need to make sure it is constructed in a readable manner and includes enough comments to be maintainable."
1463748310,10515,oranagra,2023-03-10T12:39:54Z,"If we can pick this up and close it within a couple of weeks, I think it can fit 7.2"
1463873689,10515,zuiderkwast,2023-03-10T14:21:39Z,"> I agree the risk for issues is small but still don't like to merge a mass of unreviewed code.
> Maybe the level of review you did is enough?
> I suppose there's no need to explicitly look for bugs, but I think we do need to make sure it is constructed in a readable manner and includes enough comments to be maintainable.

Yeah, I didn't execute the code in my head, but the code looks nice, seems to have enough comments, fairly short functions, comprehensive variable names, etc. I would say it's probably maintainable."
1463879943,10515,jhelbaum,2023-03-10T14:26:14Z,"Yes, thanks for taking a look at this. I'm unlikely to have much time to devote to this at the moment, however.

Regarding maintenance, I would just note that the test suite has a minor maintenance burden in that it is dependent on the current command specs. If some of the command argument specs change, the test cases for those commands will have to be updated accordingly (since the hint strings will change).

It might be ""better"" in some ideal world for the tests to be based on some static set of command specs, but that's not realistic to implement IMHO.

In any case, it would be nice to see this code merged. I think it's a handy feature."
1466206195,10515,oranagra,2023-03-13T14:04:41Z,"@zuiderkwast @jhelbaum i gave it some thought and also consulted Yossi and Itamar, i decided to take Viktor's advise and merge it without another reviewer, considering this is an isolated area, and we can fix things or improve readability later.
So now it'll just take someone to defrost this PR and bring it back to mergable state.
@zuiderkwast maybe you wanna have a go at that? (since Jason said he doesn't currently have the time)"
1466213727,10515,zuiderkwast,2023-03-13T14:08:52Z,"Defrost? :grin: Only fix merge conflicts and possibly address my own review comments, right?"
1466222596,10515,oranagra,2023-03-13T14:13:49Z,"yes, plus any additional cleanup you can do which results of a another pair of eyes reading this (like clarifying come comments)"
1466373984,10515,zuiderkwast,2023-03-13T15:32:32Z,"OK, I'll do it. :+1:"
1466447981,10515,oranagra,2023-03-13T16:11:20Z,"btw, for the technical part, maybe @jhelbaum can grant you access to his branch, or you can make PRs to it."
1490558468,10515,oranagra,2023-03-30T16:04:38Z,:tada: :tada: :tada: @jhelbaum merged thank you
1181929176,10966,sjpotter,2022-07-12T15:43:37Z,"should note, that I have a weirdness in my test module code for handling the error case, if I don't do that and just do a RM_ReplyWithCallReply for everything (i.e. errors or not) it dies with an address sanitizer error

<details>
  <summary>Sanitizer report</summary>

```
=================================================================
==164863==ERROR: AddressSanitizer: heap-buffer-overflow on address 0x611000002934 at pc 0x7f5d04cdd397 bp 0x7ffd9ae32930 sp 0x7ffd9ae320d8
READ of size 16384 at 0x611000002934 thread T0
    #0 0x7f5d04cdd396 in __interceptor_memcpy ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:827
    #1 0x55d8e152b156 in memcpy /usr/include/x86_64-linux-gnu/bits/string_fortified.h:29
    #2 0x55d8e152b156 in _addReplyToBuffer /home/spotter/CLionProjects/redis/src/networking.c:335
    #3 0x55d8e152b156 in _addReplyToBufferOrList /home/spotter/CLionProjects/redis/src/networking.c:395
    #4 0x55d8e1697e45 in RM_ReplyWithCallReply /home/spotter/CLionProjects/redis/src/module.c:3091
    #5 0x7f5d017a637b in call_with_acl /home/spotter/CLionProjects/redis/tests/modules/usercall.c:55
    #6 0x7f5d017a637b in call_with_acl /home/spotter/CLionProjects/redis/tests/modules/usercall.c:26
    #7 0x55d8e16a4785 in RedisModuleCommandDispatcher /home/spotter/CLionProjects/redis/src/module.c:793
    #8 0x55d8e14edb75 in call /home/spotter/CLionProjects/redis/src/server.c:3319
    #9 0x55d8e14f275c in processCommand /home/spotter/CLionProjects/redis/src/server.c:3948
    #10 0x55d8e15303d1 in processCommandAndResetClient /home/spotter/CLionProjects/redis/src/networking.c:2444
    #11 0x55d8e15303d1 in processInputBuffer /home/spotter/CLionProjects/redis/src/networking.c:2548
    #12 0x55d8e1539c2b in readQueryFromClient /home/spotter/CLionProjects/redis/src/networking.c:2684
    #13 0x55d8e171a37c in callHandler /home/spotter/CLionProjects/redis/src/connhelpers.h:79
    #14 0x55d8e171a37c in connSocketEventHandler /home/spotter/CLionProjects/redis/src/connection.c:310
    #15 0x55d8e14d7cc3 in aeProcessEvents /home/spotter/CLionProjects/redis/src/ae.c:436
    #16 0x55d8e14d8abc in aeMain /home/spotter/CLionProjects/redis/src/ae.c:496
    #17 0x55d8e14ca51a in main /home/spotter/CLionProjects/redis/src/server.c:7066
    #18 0x7f5d044d7d8f in __libc_start_call_main ../sysdeps/nptl/libc_start_call_main.h:58
    #19 0x7f5d044d7e3f in __libc_start_main_impl ../csu/libc-start.c:392
    #20 0x55d8e14cbfd4 in _start (/home/spotter/CLionProjects/redis/src/redis-server+0x103fd4)

0x611000002934 is located 0 bytes to the right of 244-byte region [0x611000002840,0x611000002934)
allocated by thread T0 here:
    #0 0x7f5d04d57c18 in __interceptor_realloc ../../../../src/libsanitizer/asan/asan_malloc_linux.cpp:164
    #1 0x55d8e150dadf in ztryrealloc_usable /home/spotter/CLionProjects/redis/src/zmalloc.c:243
    #2 0x55d8e150dbd3 in zrealloc_usable /home/spotter/CLionProjects/redis/src/zmalloc.c:287
    #3 0x55d8e15040ba in _sdsMakeRoomFor /home/spotter/CLionProjects/redis/src/sds.c:271
    #4 0x55d8e1507834 in sdsMakeRoomFor /home/spotter/CLionProjects/redis/src/sds.c:295
    #5 0x55d8e1507834 in sdscatfmt /home/spotter/CLionProjects/redis/src/sds.c:722
    #6 0x55d8e16b2dcd in validateACLS /home/spotter/CLionProjects/redis/src/module.c:5711
    #7 0x55d8e16b2dcd in CallWithUserInternal /home/spotter/CLionProjects/redis/src/module.c:5851
    #8 0x55d8e16b3775 in RM_CallWithUser /home/spotter/CLionProjects/redis/src/module.c:6089
    #9 0x7f5d017a6329 in call_with_acl /home/spotter/CLionProjects/redis/tests/modules/usercall.c:40
    #10 0x7f5d017a6329 in call_with_acl /home/spotter/CLionProjects/redis/tests/modules/usercall.c:26

SUMMARY: AddressSanitizer: heap-buffer-overflow ../../../../src/libsanitizer/sanitizer_common/sanitizer_common_interceptors.inc:827 in __interceptor_memcpy
Shadow bytes around the buggy address:
  0x0c227fff84d0: fd fd fd fd fd fd fd fa fa fa fa fa fa fa fa fa
  0x0c227fff84e0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0c227fff84f0: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
  0x0c227fff8500: fa fa fa fa fa fa fa fa 00 00 00 00 00 00 00 00
  0x0c227fff8510: 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00 00
=>0x0c227fff8520: 00 00 00 00 00 00[04]fa fa fa fa fa fa fa fa fa
  0x0c227fff8530: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c227fff8540: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c227fff8550: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c227fff8560: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
  0x0c227fff8570: fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa fa
Shadow byte legend (one shadow byte represents 8 application bytes):
  Addressable:           00
  Partially addressable: 01 02 03 04 05 06 07 
  Heap left redzone:       fa
  Freed heap region:       fd
  Stack left redzone:      f1
  Stack mid redzone:       f2
  Stack right redzone:     f3
  Stack after return:      f5
  Stack use after scope:   f8
  Global redzone:          f9
  Global init order:       f6
  Poisoned by user:        f7
  Container overflow:      fc
  Array cookie:            ac
  Intra object redzone:    bb
  ASan internal:           fe
  Left alloca redzone:     ca
  Right alloca redzone:    cb
  Shadow gap:              cc
==164863==ABORTING
```
</details>"
1182316936,10966,sjpotter,2022-07-12T19:12:50Z,"the above test / module implementation is problematic, as the docs for setting acl in redis module code is not quite accurate (as I learned when I tried to do more).  it says it uses the same syntax as acl setuser, but it only works on a single op at a time.  need to investigate how to use ACLMergeSelectorArguments and then loop over the args it builds.


"
1202184775,10966,sjpotter,2022-08-02T08:33:12Z,"Open Q: currently RM_CallWithUser will accept a NULL RedisModuleUser (and hence be equivalent to a plain RM_Call().  Should this be an error or is that behavior the desired behavior.

i.e. should we require users to use RM_Call() for running command as the ""root"" user or is it ok to enable that with RM_CallWithUser"
1214397312,10966,yossigo,2022-08-14T15:04:58Z,"@sjpotter If something can already be done without relying on `NULL` semantics, I lean towards doing that and require user to be non-NULL."
1214826911,10966,sjpotter,2022-08-15T09:41:58Z,@oranagra can you review this?
1223819716,10966,sjpotter,2022-08-23T09:39:45Z,"So in summary of above, I reverted all the refactor changed to RM_Call.  No More RM_CallWithUser.  now we have a RM_SetContextModuleUser.  If set (open Q above if it should be unset afte RM_Call), it will behave like the RM_CallWithUser (i.e. assign the user to the 'tempclient' allocated).

otherwise, its basically normal RM_Call, with a slight logic change for 'C' acl checking to use either the ctx->user if defined or ctx->client->user if not.

To enable it to distinguish ctx->user == NULL from the ""null"" user, created a NullUser const object that can be referenced if want to actually set it to the ""null/root"" user.  Now, unsure its totally necessary (just don't do an ACL check if if want the null user), but perhaps it makes code cleaner to have single paths with explicit mentioning of null user then to have 2 different paths if null or not."
1229433143,10966,sjpotter,2022-08-28T11:02:58Z,"sorry for the rebase, I was trying to squash to additional commits together and that rebased my whole branch by accident."
1229461011,10966,oranagra,2022-08-28T13:49:00Z,"@sjpotter so please let me know from which commit i should start reviewing the new content?
i understand the force push contains a fresh copy of unstable, and that the squashes you made are to meld new commits that i didn't see together right? so there should still be a commit that represents what i saw and after it ones that i didn't see..
or should i better just start over?"
1229562185,10966,sjpotter,2022-08-28T21:57:41Z,"yeah, I meant to just squash some local commits together before pushing, but muscle memory was rebase -i upstream/unstable (need to break that muscle memory). And then after realizing push failed, should have reset head, stashed the changes, reset commit to origin/branch and then stash popped. 

sorry about that.

I believe its only the last one or two commits that you haven't seen (probably just the last one)"
1250618202,10966,oranagra,2022-09-19T06:28:22Z,"@redis/core-team please approve or comment.
please notice the behavior change for RM_Call(""EVAL"") with the ""C"" flag, will now mean the script gets checked with ACL too (behavior change)."
1250751822,10966,sjpotter,2022-09-19T09:01:45Z,"In private conversation @MeirShpilraien wondered why we need the new `RM_SetContextModuleUser` API when we have `RM_AuthenticateClientWithUser`.  

It works by overwriting the user of `ctx->client->user` with whatever the caller passed in. 

So, I'm confused by how this works and what the side effects of it are. 

1) `RM_Call` seems to be designed to work even with `ctx->client` is NULL, i.e. when `RM_AuthenticateClientWithUser` would fail.  i.e. today if one calls `RM_Call` with the 'C' flag (i.e. for ACL checking), it explicitly fails, which makes me presume that this is an expected use case.

2) the `ctx->client` object is used at a number of other places within the code.

Examining the code, it seems that the ctx->client is allocated in normal use cases when the `RedisModuleCtx` is created.  I'm unsure if it can ever not be allocated?  What is the code path where it would be NULL?

and that leads to a follow up Q, if (true) every RedisModuleCtx has its own allocated `ctx->client`, why in `RM_Call` do we need to ""allocate"" a temp client to issue the `RM_Call` with?  why can't we use the client that is part of the context itself (no allocation, no resetting after execution....)"
1250825050,10966,tezc,2022-09-19T10:11:25Z,"> and that leads to a follow up Q, if (true) every RedisModuleCtx has its own allocated ctx->client, why in RM_Call do we need to ""allocate"" a temp client to issue the RM_Call with? why can't we use the client that is part of the context itself (no allocation, no resetting after execution....)

I think we still need to do some cleanup and call reset family functions, because we are making some changes to that client in `RM_Call()`, no? 

---- 

Just thinking out loud, can reentrant/concurrent calls of API functions cause problems? `RM_Call()` calls a module function, module function calls another API function which also uses `ctx->client`. (or makes another RM_Call()?, I don't know if this is allowed). So, if we are reusing same `ctx->client`, it might be in a partially modified state. I don't know if this is a valid/possible scenario. (my brain melts while trying to think these scenarios).

"
1250831884,10966,oranagra,2022-09-19T10:18:26Z,"RM_AuthenticateClientWithUser and RM_SetContextModuleUser are meant for completely different purposes.
The first one is meant to allow implementing commands such as AUTH and change the currently active user, and the second one is meant only to affect RM_Call.
I understand that maybe we can achieve what you want to do using the existing API and then undoing the change, but it feels wrong, and would have side effects.

i'm not sure about the NULL edge case you mentioned.

regarding the fake client, it is needed for many reasons, one of them is so that the caller of RM_Call would get the reply and not the real (socket) client."
1252346876,10966,oranagra,2022-09-20T13:21:25Z,"PR Approved in a core-team meeting.
Requires one final review and it can be merged."
590485393,6929,felipou,2020-02-24T18:41:12Z,"I started implementing this almost 5 years ago, so I decided to create a new PR. The old one is here: https://github.com/antirez/redis/pull/2664"
590489569,6929,felipou,2020-02-24T18:50:38Z,"I was in doubt about mainly 3 things:
- Tests: I just replicated all the RPOPLPUSH and BRPOPLPUSH tests. They add quite some testing time (only the blocking ones), but I think they add very little value, since the code is practically the same. But anyway, I felt that I should replicate then all, it's easier to just erase some now if that's the case.
- Comments: I didn't add any new comments, just modified the existing ones. Didn't really feel the need to, since the lists code is quite clean and easy to understand, and my changes don't make it more complicated.
- Coding style: I did my best to comply to overall coding style, although I haven't seen any guideline, just tried to look at the surrounding code to grasp it. But I'm a strong advocate of strict and consistent coding style, so I was confused because there are some bits of code that don't seem to comply with the overall style, like for example max line length. I tried to keep my code under 80 characters, but there were some long lines already, so when I modified these I just kept it as one-line. If anyone wants to point something that is not conformant with the project coding style, I'd be very grateful and eager to fix it."
590837475,6929,itamarhaber,2020-02-25T12:14:21Z,"> I started implementing this almost 5 years ago

+1 for being persistent :)"
648364189,6929,felipou,2020-06-23T19:12:50Z,"Did a rebase with unstable solving a minor conflict (wasn't an actual conflict, just a change very close to my changes flagged as a possible conflict). As a plus it solved the CI checks that were failing before (and wasn't my fault as expected)."
648700956,6929,antirez,2020-06-24T09:15:10Z,"Looks like a good coincise implementation @felipou, thanks. And since this is a plain extension of the original implementation, I guess the replication part should work as expected (didn't check the details). So indeed it is worth a careful review and a potential merge."
649771096,6929,felipou,2020-06-25T19:22:45Z,"Great, let me know if there is anything else I can do to help or improve the code!"
657140034,6929,itamarhaber,2020-07-11T22:30:26Z,"@yossigo @oranagra @madolson @soloestoy please review and consider for 6.0.6

@felipou would it be too much to ask you to help with the respective docs once approved? ;)"
657238006,6929,felipou,2020-07-12T15:32:45Z,"> @felipou would it be too much to ask you to help with the respective docs once approved? ;)

Not at all, please count me in for anything you need, it's a pleasure contributing to Redis!"
657239081,6929,felipou,2020-07-12T15:41:07Z,"@oranagra Thanks for your review, I'll go over everything right now. I may struggle a bit with the tests, since I've never worked with tcl, I just copied the existing tests for RPOPLPUSH, but I think I'll manage.

Just a quick question: what is the usual process here, do I overwrite my branch (`push --force`), or should I push multiple commits and afterwards we squash them (or maybe don't even squash, just merge as it is)? For now I'll just push new commits, but I'm asking because I like to keep my git history very clean, so I usually do a lot of overwrites on feature branches to keep them as tidy as possible before merging."
657239806,6929,oranagra,2020-07-12T15:46:25Z,"@felipou i like my git history clean too, we'll either squash it at the end, or if for some reason we want to keep some changes in separate commit, we'll do some cleanup.
for now you can either add more commits, or amend and push-f.

if you need help with the tests let me know, i think you'll just be able to use your clipboard and common sense.."
657253761,6929,felipou,2020-07-12T17:41:47Z,"@oranagra I just pushed some new commits addressing everything you commented. I just replied every thread instead of resolving the conversations, if you want I can resolve all the trivial ones (removing duplicate tests for example)."
657377376,6929,oranagra,2020-07-13T06:03:38Z,"@felipou thanks you. everything looks great.
hope to merge it soon (although not sure it can go into 6.0).

p.s. out of curiosity i run the list test.
original version took me 11 seconds
the one before my review comments took 26 seconds
and the last revision took just 7 seconds.
so adding tests and still runs faster."
657618917,6929,felipou,2020-07-13T15:12:38Z,"@oranagra Great, thanks!

Makes total sense that it runs faster too, the idea you gave of replacing the `after` calls with `wait_for_condition` is a very good optimization, no sense in waiting a full 1s for something that usually happens a lot faster."
658664956,6929,oranagra,2020-07-15T09:42:19Z,"@redis/core-team since this PR adds commands (which we can't later remove or rename), it requires more than lazy consensus.
There's no need to review the code again, but please approve the addition of these 6 new commands."
661624702,6929,madolson,2020-07-21T04:22:57Z,"I think the new commands are sensible. 

I want to bring up a separate point that I think we should rewrite the blocking command infrastructure. Today it works by marking the client as blocked on a key with a bunch of special variables, when that key is written to we used those stored special variables to complete the request. This has several issues:
1. We maintain a lot of duplicate code to process the requests.
2. The code as written is sort of spaghetti code. 
3. We may still have to re-block after the command is executed, which requires special handling in that case which is unnecessary. 
4. Explicit rewriting for replication.

Instead, I think we should merge the blocking framework with the mechanism that was outlined for the background threads. If a client is block from a non-existent key, it should just remove itself from the event handler and register that it is blocked on a key. When that key is touched, the blocked client will be unblocked and will attempt to re-execute the command that blocked it in the first place. This shouldn't introduce any major compatibility issues and should throw the same exceptions we do today. It can then inline the command it has within the replication system. It should hopefully remove a lot of the weird checks that we have to do that are included in the CR. 

I think this refactor is worth it either for this change or so that it is easier to make these sort of changes in the future. (If we agree it's worth it for this change, I'm okay with this specific PR getting merged and a separate one for the refactor)"
661687919,6929,guybe7,2020-07-21T07:32:32Z,@madolson great idea! we need to remember to unblock clients according to the order they were originally blocked (if two or more clients are blocked on the same key)
661689784,6929,oranagra,2020-07-21T07:36:46Z,"i agree that some major refactoring may be useful there, and it should certainly be in a separate PR.
Had this PR include such refactoring it would have been very hard to review."
661705107,6929,oranagra,2020-07-21T08:09:15Z,"@madolson @guybe7 @yossigo @soloestoy such a refactory (suggested by @madolson above) adds considerable risk, and should anyway be done after we merge this PR or any other PR that adds small features or fixes to the blocking code, otherwise it'll be one huge merge conflict.
however, since we can't push this PR into 6.0.x (due to adding new write commands and causing compatibility issues with AOF/Replicas), we need to carefully chose when to merge it to unstable (although the changes are not that heavy to create major cherry pick conflicts).
Then we need to decide when to start that major refactory (possibly for redis 7.0, but before adding any other features that will build on top of the refactored code).
Any thoughts?"
661794403,6929,yossigo,2020-07-21T11:15:56Z,"@oranagra I agree we should merge this closer to the time of next major release.

@madolson I think this is a great idea, do you have any suggestions how to push this forward? Maybe a dedicated issue and initiate a discussion around how the API for this can look like in more details?"
662160309,6929,madolson,2020-07-21T23:38:31Z,"Added a new issue: https://github.com/redis/redis/issues/7551 for immediately tracking the idea. I don't have the time right now, but I'll spend some time this week outlining all the touch points and how they'll get updated. 

@oranagra I think 7.0 is a good target, so I would propose sketching out the design and then implementing it when we find appropriate. 

"
663266186,6929,felipou,2020-07-23T22:38:34Z,"> There is also two comments in blocked.c that explicitly refers to BRPOPLPUSH that maybe should be renamed BxPOPxPUSH

I updated all references to BRPOPLPUSH in a new commit here: https://github.com/felipou/redis/commit/94019fb00b60130cb8366f0b144fa94ad786da49
Did it in a new commit pushed to a different branch for now, if it's ok I can just fast-forward this branch.
"
663332651,6929,oranagra,2020-07-24T03:34:47Z,Please go ahead and merge that to this branch. 
663334992,6929,felipou,2020-07-24T03:49:24Z,"> Please go ahead and merge that to this branch.

Done!"
667365345,6929,felipou,2020-07-31T21:23:53Z,Anything else needed before merging @oranagra ?
667465170,6929,oranagra,2020-08-01T04:08:54Z,"@felipou no, this PR is good to be merged, it's just that e can't make it part of 6.0.x (would break compatibility with old replicas), so we're not sure it should be put into `unstable` since it may cause issues with cherry-picks. So I'm postponing the merging for now. "
667738472,6929,felipou,2020-08-02T23:24:55Z,"Ok, great! 👍 "
673598145,6929,felipou,2020-08-13T17:08:54Z,"Just saw that there was a merge that generated some conflicts here. I'll push a new commit solving these conflicts soon, looks very simple to fix."
675182159,6929,felipou,2020-08-18T00:33:05Z,Done. Just two small simple conflicts.
675182973,6929,felipou,2020-08-18T00:35:59Z,"I noticed some tests were failing on the unstable branch on macOS (the OS I'm using), so I just checked if the exact same tests are failing on my branch. The tests added/modified by this PR are ok."
675277707,6929,guybe7,2020-08-18T06:17:19Z,"@felipou the tests are very repetitive - maybe they run in a loop-like manner? i.e. for cmd in {rpoprpush, rpoplpush lpoprpush, lpoplpush} ..."
675542399,6929,felipou,2020-08-18T15:18:01Z,"@guybe7 I agree, but I never worked with tcl before, so I'm not sure what is really possible in this case. I'll review the tests more carefully, because in some of them I believe it's just a matter of adding the loop and replacing the main command.

But it seems that in most cases, it's not only the main command that changes, but also the order of the values in the output variable for example, like in the ""*POP*PUSH base case"" tests. I'll try to think of a generic way to do those tests (for all poppush commands in a loop), but I'm afraid they could get too complicated. Perhaps the simplicity and clarity is enough to justify repeating instead of a loop?

Another possibility for these tests would be using some tcl features that I'm not aware of, or used to. If that's the case, could you provide some pointers, examples, documentation, or anything that could help me in doing that?"
680133252,6929,felipou,2020-08-25T16:29:53Z,"Replaced some repetitive tests with some foreach loops as suggested by @guybe7.

For the other tests, I came up with some options, but as I said, I'm not sure they are worth the sacrifice in readability, so I didn't commit them yet. Instead I created this gist: https://gist.github.com/felipou/acd4469dffbfe02740dfe7465ee60b72

These tests in the gist are still fairly simple, there are others that would require more logic/values in the loops. But if you think they are worth it, I'm more than willing to replace all of them!
"
684416598,6929,oranagra,2020-09-01T06:12:59Z,"@felipou sorry to bother you again with more requests.
I was about to merge this, but it then occurred to me that on another recent discussion (around adding `ZRANGESTORE [BYSCORE|BYLEX] [REV]` rather than `Z[REV]RANGEBYSCORESTORE`) we concluded that we want to to reduce the number of commands making them more flexible and later deprecate the old ones.

So I would like to suggest that instead of the 6 new commands you added, we'll introduce just two: `LMOVE` and `BLMOVE`, both take 4 arguments like:
`LMOVE <src> <dst> [LEFT|RIGHT] [LEFT|RIGHT]`
This would make today's BRPOPLPUSH an alias for `BLMOVE <src> <dst> RIGHT LEFT <timeout>`"
684924379,6929,felipou,2020-09-01T15:08:11Z,"Very interesting idea, I'll work on it! I'll try to have it finished by the weekend.

As for the command syntax, what about making the ""LEFT|RIGHT"" parameters be required instead of optional? And then we could move then to be like:
```LMOVE <src> <LEFT|RIGHT> <dst> <LEFT|RIGHT>```
The idea is to make it more ""readable"" like ""move from src left to dst right"". Makes sense?"
684926362,6929,oranagra,2020-09-01T15:11:04Z,"@felipou yes, certainly mandatory.. it was a typing error."
684927377,6929,oranagra,2020-09-01T15:12:29Z,i personally think that key names should come first (similarly to other commands like SMOVE).
685872597,6929,felipou,2020-09-02T17:06:56Z,"Makes sense, and consistency is certainly more important."
685927540,6929,itamarhaber,2020-09-02T18:45:11Z,"As a side bonus, this will make the docs simpler and less repetitive :)"
697175024,6929,oranagra,2020-09-23T07:02:12Z,"@felipou any news? if you don't think you can get to it, maybe one of us can step in.."
697709763,6929,felipou,2020-09-23T17:06:28Z,"Sorry for taking so long, I'll try to get something done by friday. But if anyone wishes to step in at anytime, I've no problem with that!"
703178596,6929,felipou,2020-10-03T23:52:51Z,"I did some work on a new branch today, it's basically done, but the replication test is failing and I didn't find out why yet.
https://github.com/felipou/redis/tree/poppush-generic-lmove

I'll try to fix that by tomorrow. But the if anyone can take a look, I had to change some things, mainly add more information to the ""blockingState"" since we lost the information about head/tail now that we have a single command."
703191253,6929,felipou,2020-10-04T02:34:54Z,"Done! Everything works, and all tests are passing. I now have just a few doubts:

- Is it ok to add more fields to the blockingState struct?
- I didn't add blmoveCommand to the redisServer struct because brpoplpushCommand wasn't present there, but I'm not sure why.
- I'm creating some strings for ""right"" and ""left"", but I'm not sure how the reference counts works in redis. Are they always required? Should I have made the string objs as shared objects? Should I have incremented their ref-counts after creating them? Also, should I even be creating these strings all the time? Could I have created just two global ""constant"" strings and reused them whenever need?
- Was using rewriteClientCommandVector a good solution? I wasn't sure if this is how it is supposed to be used.
- I kept the rpoplpush tests and created lmove tests where we had the other pop-push tests, but there were many tests just for rpoplpush. Now that we have a new command with a quite different name and new parameters, I thought that maybe we would like to change all rpoplpush tests to use lmove instead. Not sure if that is worth it (or even if there is any gain at all in doing that), so I just kept the rpoplpush tests as they were."
703239839,6929,oranagra,2020-10-04T11:14:12Z,"@felipou i'll try to sum up what i think is left (at least the important ones):
1. create lmoveGenericCommand and use it in all the command entry points instead of `rewriteClientCommandVector` or parsing arguments twice.
2. add some code to serveClientBlockedOnList to be able to generate RPOPLPUSH when the original command was BRPOPLPUSH, but keep generating LMOVE for the rest of the cases.
3. create static strings for `left` and `right` (just a minor performance optimization).
4. update the redis-doc PR."
703330240,6929,felipou,2020-10-04T23:17:53Z,"Ok, just a quick question: should I create the `left` and `right` static string as attributes of `shared` (in the `createSharedObjects` function)? If not, any suggestion to where should I put those?"
703332213,6929,felipou,2020-10-04T23:35:37Z,"On another topic, earlier today I started thinking more about the suggestions made by @guybe7 that we should try and use loops to avoid having repetitive tests for the poppush commands. Now that we have a single command, I thought it made even more sense, so I came up with this commit: https://github.com/felipou/redis/commit/6e542814cd7af5c9efe670194ec87eddc26ab888

But the same concern still plagues me, that maybe these aren't worth the extra complexity, so I didn't add them to this PR yet. What do you think?"
703427092,6929,oranagra,2020-10-05T06:30:19Z,"regarding the `left` and `right` strings, yes put them in `createSharedObjects`
same place you see these:
```c
    shared.plus = createObject(OBJ_STRING,sdsnew(""+""));
    shared.rpoplpush = createStringObject(""RPOPLPUSH"",9);
```

I skimmed though the test changes in https://github.com/felipou/redis/commit/6e542814cd7af5c9efe670194ec87eddc26ab888 they seem fine to me.
- although the test itself is more complicated to read now, it does reduce the overall line count in the tests by sharing code between the test and adding `if`s just where they differ (earlier it would be hard to see what exactly are the difference between these consecutive tests).
- it may be true that they also test shared code so they maybe it is enough to have just one test (e.g. just left to right), and no need to repeat it 4 times, but if they're quick, we don't have anything to lose, and it does add some additional coverage (may come in handy some day).
- it doesn't refactor existing tests, or remove RPOPLPUSH tests, just changes the new tests."
703999392,6929,felipou,2020-10-06T03:01:26Z,"Just pushed 3 commits with:
- The joined tests (I changed them a bit from the commit I had sent before, just to make it a little simpler by avoiding nested ifs)
- The left and right shared strings
- Replacement of rewriteClientCommandVector with more generic function that both commands can use
- Compatibility for rpoplpush/brpoplpush when propagating or rewriting

I'll now experiment with the wherefrom/whereto parameters of the **blockForKeys** function, and update the redis-doc PR."
704017847,6929,felipou,2020-10-06T04:19:39Z,"Option 1 - using LIST_NONE: https://github.com/felipou/redis/commit/073bb779a4a75c6ba27c860dc221de39b8dd9df9
Option 2 - using a struct: https://github.com/felipou/redis/commit/0cd3feb6f4919ff2029942e894ffbf0da3e79caf

Not sure if it was just me choosing terrible names, but option 2 doesn't look very good."
704568959,6929,felipou,2020-10-06T21:42:42Z,"Just pushed the commit to create the internal listPos struct as you suggested @oranagra, looks much better now, thanks!"
704569061,6929,felipou,2020-10-06T21:42:58Z,I'll take a look at the tests you mentioned.
704586142,6929,felipou,2020-10-06T22:27:07Z,"Ok, I think I managed. TCL is really new to me, so I didn't know much about what was possible. I created a new proc to avoid changing the file `tests/unit/introspection-2.tcl` too much."
705035238,6929,felipou,2020-10-07T16:02:00Z,"Ok, great, thanks for the orientation! This actually helped me understand something I had suspected but wasn't really sure: the propagated command is actually the non-blocking version (I had to fix that for the tests to pass).

That's how it worked in the rpoplpush command before my changes, so I just replicated the same idea (BLMOVE propagates LMOVE). Should we change that?"
705055626,6929,oranagra,2020-10-07T16:36:27Z,"@felipou change what?
you're right, the propagated command is the non-blocking one.
and with the changes we did, when the original command was [B]RPOPLPUSH we propagate an RPOPLPUSH, and when it's [B]LMOVE we propagate an LMOVE.
is that not what we did? i think the tests prove it.
so what did you suggest to change?"
705074569,6929,felipou,2020-10-07T17:10:22Z,"I thought maybe this wasn't the intended behaviour, and that perhaps we should propagate the same command (the blocking one) instead."
705084053,6929,oranagra,2020-10-07T17:28:03Z,"@redis/core-team please approve the addition of new LMOVE and BLMOVE commands (deprecating [B]RPOPLPUSH).

Note that when receiving a BRPOPLPUSH we'll still propagate an RPOPLPUSH (but on BLMOVE RIGHT LEFT we'll propagate an LMOVE)"
705340486,6929,oranagra,2020-10-08T05:34:03Z,merged.. @felipou thanks a lot for sticking that long and implementing all the feedback.
705636591,6929,felipou,2020-10-08T15:11:24Z,"Hey, thank you all very much for all the feedback and for taking this in, especially you @oranagra! It was my first significant contribution to an important open-source project, and I have to say, the experience was great. Looking forward to contribute more!"
832021954,8687,oranagra,2021-05-04T15:15:07Z,@redis/core-team please take a look at this new feature for redis 7.0 (details at the top)
856848263,8687,yoav-steinberg,2021-06-08T15:04:39Z,"@yossigo @oranagra There's the issue of tracking memory usage of watched keys:
- A `WATCH` command adds the key name to a **global** dict of watched keys. Each entry in the dict contains a list of clients watching that key. This means that this isn't a per-client memory consumption. So we need to think of a mechanism of limiting how much memory watched keys can consume. Another config?
- We also don't have any reporting of these global dicts. So mem overhead reporting should be updated accordingly.
- In addition, each client contains a list of pointers to all the keys it's watching. This can be accounted for per-client, reported in `CLIENT LIST` and used for client eviction. This is already implemented in my last commits.
Any thoughts?"
856896972,8687,yoav-steinberg,2021-06-08T16:01:23Z,"After talk with @oranagra about how to handle `io-threads-do-reads` we came up with following concept (to be tested):
To handle eviction buckets being global and update them when filling data per client in the read threads we can simply make sure all updates are either atomic decrement or increment (we need decrements when moving a client from one bucket to another). We can also check (and update) the total memory usage sum. If we pass `maxmemory-clients` we can stop processing the client or even abort the thread. When we're back in the main thread we can safely assume all sums in the buckets are valid because of eventual consistency. And at this point handle any client evictions if needed."
858448024,8687,oranagra,2021-06-10T09:03:01Z,"regarding the watched keys: i don't think the client eviction mechanism needs to be perfect and count all per-client overheads, it's ok that we solve the output buffer problem and other painful problems, and some edge cases remain unsolved (it's not a security feature).
So the things that are truly per client, and are easy to count, we'll count (no reason no to), but things that are shared between clients, we can skip.

we can however improve the total overhead reported in INFO MEMORY, and the detailed report in MEMORY STATS to include these WATCH, and maybe CSC (client side caching / tracking) overheads (for manual troubleshooting)."
859245515,8687,madolson,2021-06-11T03:59:53Z,"Regarding the memory usage, I agree with oran that right now a best effort will catch most of the issues for now. If we get t the point in the future we see issues, we can iterate on this solution."
860199635,8687,oranagra,2021-06-13T12:03:21Z,@madolson i didn't understand your comment about the `valid_fn` (for some reason i can't respond to that comment)
860339661,8687,madolson,2021-06-14T03:30:38Z,"@oranagra I'm not entirely sure how that comment ended up there, it was a response to a sundb comment but somehow got duplicated as its own comment. I can't respond to it either, so I deleted it."
1219066276,8687,oranagra,2022-08-18T05:52:49Z,"@yoav-steinberg i got some [failure](https://github.com/redis/redis/runs/7889776110?check_suite_focus=true) with valgrind. maybe you have time to look into it
```
*** [err]: avoid client eviction when client is freed by output buffer limit in tests/unit/client-eviction.tcl
Expected 'obuf-client1' to match 'no client named obuf-client1 found*' (context: type eval line 38 cmd {assert_match {no client named obuf-client1 found*} $e} proc ::test) 
```"
1219096789,8687,yoav-steinberg,2022-08-18T06:42:33Z,"Not sure, the test seems fine. If it recreates you can check the server logs to see why the two client's aren't being disconnected for reaching their output buffer."
586593229,6891,hwware,2020-02-15T14:03:36Z,"@itamarhaber Hi Itamar, thanks for your comment, the `INFO default all` return ""INFO all"" I was considering this since we need to return upperbound information that user needed. If user specifies ""all"" then we can simplely return every category. Thanks 
"
942654996,6891,hwware,2021-10-13T19:37:47Z,"@yossigo, Can you please take a look at this PR. This is a old PR but I think it is still useful and valid. I have rebased the code for this PR to remove any conflicts. Thanks"
958110259,6891,hwware,2021-11-02T19:40:57Z,"Hey @oranagra, I have updated the code for this PR and editted the top comment for description. Please take a look.
Thank you."
969289871,6891,hwware,2021-11-15T20:24:30Z,"Hey @oranagra, I agree with your suggesions. I have some questions about the implementation. 
1) The dictionary which is passed to the genRedisInfoString function, should we add `default` to it or all the subsections of default like server, memory, cpu ,etc.
2) Should we unify the two loops for argc? The first one is used to check if default is present and sets the has_def_sections flag to 1. This is used to avoid duplication in the case `info cpu default` is called. The first loop won't we needed if instead of `default`, we add the individual components (server, cpu, memory, ...) of default to the dict."
969933139,6891,oranagra,2021-11-16T07:00:01Z,"yes, the loop will add the individual default sections (`cpu`, `memory`, etc) to the dict when it sees the argument `default` mentioned, or when no arguments are passed.
the arguments we pass to genRedisInfoString will be: the dict, and the `all` and `everything` flags."
992537579,6891,hwware,2021-12-13T14:30:24Z,"> @hwware are you still working on this PR? It seems you where either unable to follow my advises or run into some problems that i didn't expect and can't figure out.
> 
> do you wanna proceed or want me to try pick it up on my own? after investing the effort so far, i think we should carry it though.

Hi Oran, I am still working on this. I will let you know when my codes update.  Thanks a lot."
998225091,6891,hwware,2021-12-20T19:55:00Z,"@oranagra  Hi Oran, I just finish the code refactor according to your suggestion. You could find the following changes:
1. create a new function named: genSectionDict, it is used to create a dictionary
2. in the modulesCollectInfo, i pass the whole dict as parameter.
Please take a look and let me know any concern.  Thanks a lot for your advises."
1010227443,6891,hwware,2022-01-11T18:08:27Z,"@oranagra  Hi Oran, I update the codes following your comments, and add some test cases. I think the codes are very close to our final goal.  Thanks and take a look. "
1026680016,6891,oranagra,2022-02-01T10:16:59Z,"@hwware it seemed that i'm unable to communicate my design suggestion, so i went ahead and just implemented it.
also resolved some bugs, and the rest of my unresolved comments.
merged recent unstable and made sure the tests are passing.
please review my changes and let me know if you see any problem.

p.s. i'm not sure the sentinel tests are good enough to detect any issues, so i suppose they better be improved.
also, i think we should not rely on the module tests alone, and add some tests to the normal redis test suite."
1027308723,6891,hwware,2022-02-01T21:32:54Z,"> @hwware it seemed that i'm unable to communicate my design suggestion, so i went ahead and just implemented it. also resolved some bugs, and the rest of my unresolved comments. merged recent unstable and made sure the tests are passing. please review my changes and let me know if you see any problem.
> 
> p.s. i'm not sure the sentinel tests are good enough to detect any issues, so i suppose they better be improved. also, i think we should not rely on the module tests alone, and add some tests to the normal redis test suite.

@oranagra As we discussed, I update the c->argv+2 and c->argc-2 to  c->argv+1 and c->argc-1 to fix the minor bug, now in sentinel mode, ""info"" command run well, and I add more test cases for ""info"", ""info all"", ""info default"" and ""info everything"" commands, and I also add some test cases for ""info + one subsection or multiple sections"" for redis server mode and sentinel mode.  Please take a look and Thanks for your great help. "
1028764265,6891,oranagra,2022-02-03T09:10:35Z,"@redis/core-team please approve, changing INFO to take multiple sections."
1030873444,6891,oranagra,2022-02-06T17:07:23Z,"I've benchmarked this PR against unstable, with the plain INFO command (no args).
even with the recent optimization of not creating the default dict every time, it's still about 10 times slower.
i've tried adding an `out_defaults` argument, and adding an `defaults ||` to all the sections `if`s (so we don't resort to `dictFind` on each one), but it didn't help.
it seems that the remaining calls to `dictFind` (e.g. for `commandstats` and `latencystats`) are causing the slowdown.
and it also seems that the latency is the same if we have just one of call to `dictFind` or several of them.
calling `dictExpand` didn't help."
1030896673,6891,oranagra,2022-02-06T19:21:06Z,"scrap all that, the regression was due to the accidental inclusion of the `latencystats` section by default (not the use of dict).

branch|throughput|latency
---|----|---
unstable|52994|0.873
w/o dict caching|47961|0.971
w/ dict caching|54347|0.851
avoid most dictFind|54854|0.842
with latencystats|6664|7.432"
1030900356,6891,oranagra,2022-02-06T19:43:43Z,"my final version:
args|throughput|latency
---|---|---
no args|53390|0.867
""default""|49701|0.936
""server""|137931|0.229
explicitly list all default sections|47596|0.979
""all""|6565|7.539

unstable:

args|throughput|latency
---|---|---
no args|49652|0.936
""default""|49212|0.942
""server""|140646|0.200
""all""|6876|7.192"
750859179,8242,oranagra,2020-12-24T11:45:03Z,"@panjf2000 can you please improve the description of the PR.
please explain what exactly are you improving, what are the benefits, or what are the problems in the old code that you solve."
750868703,8242,panjf2000,2020-12-24T12:26:12Z,"> @panjf2000 can you please improve the description of the PR.
> please explain what exactly are you improving, what are the benefits, or what are the problems in the old code that you solve.

First of all, the `epoll_create` call is an old version of `epoll_create1`, `epoll_create` was added to the kernel in version 2.6 and its `size` argument is ignored since 2.6.8 while the `epoll_create1` call is a new version to create an epoll instance, it's added to the kernel in version 2.6.27 which accepts flags like `EPOLL_CLOEXEC` to be essential in some multithreaded programs, see https://man7.org/linux/man-pages/man2/epoll_create1.2.html.

>    In the initial epoll_create() implementation, the size argument
       informed the kernel of the number of file descriptors that the
       caller expected to add to the epoll instance.  The kernel used
       this information as a hint for the amount of space to initially
       allocate in internal data structures describing events.  (If
       necessary, the kernel would allocate more space if the caller's
       usage exceeded the hint given in size.)  Nowadays, this hint is
       no longer required (the kernel dynamically sizes the required
       data structures without needing the hint), but size must still be
       greater than zero, in order to ensure backward compatibility when
       new epoll applications are run on older kernels.

In fact, calling `epoll_create1` with `EPOLL_CLOEXEC` flag first and falling back to `epoll_create` when the former
doesn't work on that kernel version is a more canonical way to create an epoll instance, which is also widely used among several world-renowned open-source projects: 
- [libuv](https://github.com/libuv/libuv/blob/30ff5bf2161257921f3a3ce5655804f7cb282aa9/src/unix/linux-core.c#L88-L98)
- [libevent](https://github.com/libevent/libevent/blob/852af060af7c7e439f0db97829ec74bedeba64d5/epoll.c#L179-L187)
- [go](https://github.com/golang/go/blob/0e85fd7561de869add933801c531bf25dee9561c/src/runtime/netpoll_epoll.go#L33-L41)
- [netty](https://github.com/netty/netty/blob/a63faa4fa1ae08efc391e9bcf461ab699a5d7efd/transport-native-epoll/src/main/c/netty_epoll_native.c#L196-L203)

@oranagra "
750871446,8242,oranagra,2020-12-24T12:38:57Z,"@panjf2000 i still fail to understand what does it do for redis.
other than being ""cleaner"" (code looks dirtier due to the fallback).
is redis performing better? has less bugs?

does redis suffer any bug due to not using EPOLL_CLOEXEC? in which case we can maybe enable CLOEXEC by other means with cleaner code.
p.s. if that's the case, we need to handle CLOEXEC anyway (even on old kernels where epoll_create1 is missing)"
750871809,8242,oranagra,2020-12-24T12:40:35Z,"p.s. the OSS libraries you mentioned are ""libraries"" (unlike redis). they may need to do things in a safe way since they don't know how they're gonna be used."
751220881,8242,panjf2000,2020-12-25T09:37:02Z,"> p.s. the OSS libraries you mentioned are ""libraries"" (unlike redis). they may need to do things in a safe way since they don't know how they're gonna be used.

Yes, but even if redis is not used as a library, this epoll fd may be misused in the future development, let's say that the epoll instance may get involved in a forked process, in which case some issues will occur, I think it's no harm to set the `close-on-exec` flag for epoll instance to prevent some potential issues and since you are uncomfortable with the current solution of `epoll_create1` + `epoll_create` fallback, how about removing the `epoll_create1` and just add a `cloexecFcntl` call？"
751446637,8242,oranagra,2020-12-27T09:48:48Z,"@panjf2000 FD_CLOEXEC is for `exec` not `fork`, am i missing something?
redis is a heavy user for `fork` and we have a few FDs that we close when doing that (see `closeClildUnusedResourceAfterFork`)

redis does use `execve` in one place (`sentinelRunPendingScripts`), but i wonder why this specific epoll fd deserves a different treating than all the other fds we have open?
if there's any specific concern for the epoll fd, maybe we should indeed merge this, but if this is a global issue, maybe we better handle it in sentinel, or in some other global way.

maybe you can look into that?"
751594088,8242,panjf2000,2020-12-28T06:19:36Z,"> @panjf2000 FD_CLOEXEC is for `exec` not `fork`, am i missing something?

Yes, it's indeed for `exec`, I was talking about the case that a parent process first `fork` a child process and then the child process `exec`.

> redis is a heavy user for `fork` and we have a few FDs that we close when doing that (see `closeClildUnusedResourceAfterFork`)
>
> redis does use `execve` in one place (`sentinelRunPendingScripts`), but i wonder why this specific epoll fd deserves a different treating than all the other fds we have open?
> if there's any specific concern for the epoll fd, maybe we should indeed merge this, but if this is a global issue, maybe we better handle it in sentinel, or in some other global way.
> 
> maybe you can look into that?

OK, I will take a look at that in the near few days.
"
753802964,8242,panjf2000,2021-01-04T07:16:58Z,"In fact, `restartServer` also calls `execve` to replace and start over the process:
https://github.com/redis/redis/blob/7896debe6c16ec212c2b8ce169227988ccd9a2d8/src/server.c#L2688-L2693

, which leaks the epoll/kqueue fd opened in `aeCreateEventLoop` --> `aeApiCreate`:
https://github.com/redis/redis/blob/7896debe6c16ec212c2b8ce169227988ccd9a2d8/src/server.c#L3018-L3024 

Thus, the `FD_CLOEXEC` is necessary here.

As for the `closeClildUnusedResourceAfterFork`, we can enable the `O_CLOEXEC` when opening `cluster_config_file_lock_fd` to eliminate the explicit close to it in `closeClildUnusedResourceAfterFork`, making code neat:
https://github.com/redis/redis/blob/7896debe6c16ec212c2b8ce169227988ccd9a2d8/src/server.c#L5261-L5264"
753805280,8242,panjf2000,2021-01-04T07:22:37Z,"Misunderstood the `closeClildUnusedResourceAfterFork` part, it didn't call `execve` after `fork`, will add the explicit close back."
753898838,8242,oranagra,2021-01-04T10:39:27Z,"@panjf2000 `restartServer` is only used by DEBUG RESTART, so only used by the test suite (and IIRC even that was recently dropped), so i don't care too much about it.
the only real usage of `exec` is in `sentinelRunPendingScripts`

but again, do we have any reason to believe that that the event loop fd deserves a different treatment from other fds? does it create a more severe problem than others? if not, then i still think we need to find a way to solve it systematically. just taking care of one fd isn't gonna matter much."
753916311,8242,panjf2000,2021-01-04T11:16:04Z,"> @panjf2000 `restartServer` is only used by DEBUG RESTART, so only used by the test suite (and IIRC even that was recently dropped), so i don't care too much about it.
> the only real usage of `exec` is in `sentinelRunPendingScripts`
> 
> but again, do we have any reason to believe that that the event loop fd deserves a different treatment from other fds? does it create a more severe problem than others? if not, then i still think we need to find a way to solve it systematically. just taking care of one fd isn't gonna matter much.

I still don't quite get it about handling it systematically, could you point it out?"
753980968,8242,oranagra,2021-01-04T13:41:30Z,"> I still don't quite get it about handling it systematically, could you point it out?

there are many other file descriptors being held open (pipes, sockets, files), e.g: `aof_fd`, and many many others.
is there any reason why we must fix the `exec` problem specifically for the epoll fd, and not for others?
would that file descriptor cause more damage than others if not closed properly?
if not, then we either need to fix the problem for all descriptors in some way, find another solution to the problem, or conclude there's no problem for some reason.
that's what i mean.."
754471741,8242,panjf2000,2021-01-05T07:58:09Z,"> there are many other file descriptors being held open (pipes, sockets, files), e.g: aof_fd, and many many others.
is there any reason why we must fix the exec problem specifically for the epoll fd, and not for others?

Now that the `execve` call is trivial in `restartServer` cuz the `restartServer` is only used for test and not an actual production feature, then the poll fds are not urgent to be set the flag `FD_CLOEXEC` (although I think it would be better to do that). On the other hand, the only `execve` call that matters is located at `sentinelRunPendingScripts` in `sentinel.c`, I think it's reasonable to set the `FD_CLOEXEC` on those opened socket fds to redis master servers.

Please share your opinion about this, thanks! @oranagra "
754535807,8242,oranagra,2021-01-05T10:02:03Z,"@panjf2000 yes, i suppose we need to set it to all our fds. (or if we can somehow conclude that this doesn't cause any problem in the case of `sentinelRunPendingScripts` we can skip it altogether.
either way, i don't see any reason to specifically handle just the epoll fd and not others."
754544046,8242,panjf2000,2021-01-05T10:16:34Z,"We don't have to enable `FD_CLOEXEC` for those opened fds whose processes don't call `execve`, but we should set this flag for those fds in processes with `execve`, `sentinelRunPendingScripts` calling `execve` may not cause some severe problems but it still results in fd leaks if we don't enable `FD_CLOEXEC`."
754567132,8242,oranagra,2021-01-05T11:03:29Z,"ohh, right, i forgot that sentinel doesn't mess with an AOF.
but still there are more FDs there than just the epoll one.
can you try to find all and handle them?"
754570917,8242,panjf2000,2021-01-05T11:10:51Z,"> ohh, right, i forgot that sentinel doesn't mess with an AOF.
> but still there are more FDs there than just the epoll one.
> can you try to find all and handle them?

Sorry but what did you mean about more than epoll fd?
I've remove the `FD_CLOEXEC` flag from the epoll/kqueue logic and now enabling it on the sentinel socket fds, I was talking about those socket fds (see the latest commit in this PR), were you saying that there are more fds we need to handle in sentinel.c? @oranagra "
754578262,8242,panjf2000,2021-01-05T11:26:05Z,"Oh, I've seen your point, a sentinel also starts with an epoll/kqueue fd and other fds like regular redis servers, so these fds still need to be handled with `FD_CLOEXEC`, right?"
754602777,8242,panjf2000,2021-01-05T12:21:36Z,"> ohh, right, i forgot that sentinel doesn't mess with an AOF.
> but still there are more FDs there than just the epoll one.
> can you try to find all and handle them?

Please take a look at the latest commit. @oranagra "
754614517,8242,panjf2000,2021-01-05T12:47:28Z,"Besides, should we return to the original fallback of epoll_create1? 
Leverage epoll_create1 to create an epoll instance with EPOLL_CLOEXEC in an atomic way:

```c
    state->epfd = epoll_create1(EPOLL_CLOEXEC);
    if (state->epfd == -1) {
        state->epfd = epoll_create(1024); /* 1024 is just a hint for the kernel */
        if (state->epfd == -1) {
            zfree(state->events);
            zfree(state);
            return -1;
        }
        cloexecFcntl(state->epfd);
    }
```"
754857959,8242,oranagra,2021-01-05T19:45:53Z,"I don't see a need for atomicity. 
If we could have just use epoll_create1 that would have been great, but since we have to use the fallback, I think a separate code to set FD_CLOEXEC would be cleaner. 

Another petty note about clean code, I don't think this short logic-less function deserves a file of its own (fileopt_unix) where the the copyright notice is longer than the actual code. I think we can find a place for it in another file. Maybe anet.c? 

Anyway, let's get to the important part, and I'm sorry I can't me more useful here. We need to find a way to locate all the file descriptors sentinel uses and make sure they're all set.
maybe an empiric approach is the right one?
Maybe run sentinel, and then using `strace` or dig into `/proc` will help us be sure we found them all? 
Or even better, maybe we can extend the sentinel test to validate that when sentinel is stopped, all its fds are set with the flag? This way we'll even be able to spot future regressions. 

I'm not too familiar with sentinel mysel. Maybe @hwware can chip in? "
754996150,8242,panjf2000,2021-01-06T00:59:17Z,"> I don't see a need for atomicity.
> If we could have just use epoll_create1 that would have been great, but since we have to use the fallback, I think a separate code to set FD_CLOEXEC would be cleaner.

Okay.

> Another petty note about clean code, I don't think this short logic-less function deserves a file of its own (fileopt_unix) where the the copyright notice is longer than the actual code. I think we can find a place for it in another file. Maybe anet.c?

My initial idea is to put the `cloexecFcntl` into anet.c but it just doesn't seem to be the right place for this function to me, considering anet.c is a place for network-related functions, so I created a new file for it and thought this new file could be a separate place for all file-option-related functions in the future? But it's up to you guys, if you think anet.c is a better place for `cloexecFcntl`, then we can move it to anet.c.

> Anyway, let's get to the important part, and I'm sorry I can't me more useful here. We need to find a way to locate all the file descriptors sentinel uses and make sure they're all set.
> maybe an empiric approach is the right one?
> Maybe run sentinel, and then using `strace` or dig into `/proc` will help us be sure we found them all?
> Or even better, maybe we can extend the sentinel test to validate that when sentinel is stopped, all its fds are set with the flag? This way we'll even be able to spot future regressions.
> 
> I'm not too familiar with sentinel mysel. Maybe @hwware can chip in?

I caught these fds in sentinel based on the `closeClildUnusedResourceAfterFork` which is invoked in `redisFork()`, right after a `fork()` system call, since the sentinel shares the underlying data structure `redisServer` with regular redis servers. Further more, including the particular socket fds of master redis servers in sentinel, I reckon that's all?

@oranagra @hwware 
"
755056053,8242,hwware,2021-01-06T03:34:49Z,"@oranagra  compared to redis server mode, sentinel maintains extra two connections(cmd and pubsub) for each monitored master/replica node and one connection(cmd) with other sentinels monitoring same master. I think @panjf2000 already locate the places where the socket fds was initialized..  ps maybe we can use `lsof ` to verify these opened fds if we want"
755938426,8242,panjf2000,2021-01-07T07:26:24Z,Any new comments on this? @oranagra 
755941111,8242,oranagra,2021-01-07T07:32:59Z,"@panjf2000 i'm sorry, i'm a bit busy these days, i'll probably only be able to look into it properly next week.

my only blind tip is that i think we should somehow empirically check that all sentinel FDs are handled, and that it would be nice if the test suite did that so that we make sure any future fds we create will be taken care of too."
755950824,8242,panjf2000,2021-01-07T07:55:49Z,"I believe that the current opened fds in sentinel are handled properly by this PR based on an empirical approach, see https://github.com/redis/redis/pull/8242#issuecomment-754996150 which has been confirmed by @hwware.

As for the verification in the redis test suite, since I'm not so familiar with it, I may need some hints about the test suite, is there any doc I can refer to? or we just submit this commit first to fix this fd leak and add tests in the next PR latter?

@oranagra "
758373527,8242,panjf2000,2021-01-12T03:36:38Z,"Please take a review at this PR when you are available this week, thanks~
@oranagra "
758432776,8242,oranagra,2021-01-12T06:18:35Z,"@panjf2000 i haven't forgotten you, it's just that i'm still busy (trying to close some gaps to release new version), and this PR requires some digging on my part, and isn't urgent.
i'll get to it later this week.
sorry."
758438050,8242,panjf2000,2021-01-12T06:31:25Z,"> @panjf2000 i haven't forgotten you, it's just that i'm still busy (trying to close some gaps to release new version), and this PR requires some digging on my part, and isn't urgent.
> i'll get to it later this week.
> sorry.

OK, take your time."
760258263,8242,oranagra,2021-01-14T15:10:58Z,"@panjf2000 i finally got to look into this PR, sorry for the delay.

i've added a few minor comments, but what's more important is to know that these changes cover all the FDs that sentinel uses.
as i said, there is really no reason to handle some if we don't handle all of them (doing so won't resolve the problem).

i don't know the sentinel well enough to judge, so i made some modifications to the test suite to see if this PR closes all of them, and i see that it doesn't (i.e. some sockets and pipes are still open).

as you can imagine by now, i'm a bit overloaded (need to respond to many other contributors and bug reports), and since this issue isn't an urgent one, I rather just provide guidance and hope that you can take it to completion.

so i'll push a commit now into your PR with my initial test suite changes.
i think it should be fairly easy to continue in that path and:
1. find the code responsible for the fds that aren't closed yet, and add the flag to these too.
2. change the test suite to error when a script is executed and it detects any open fds other than 0, 1, 2 (std streams).

i.e. you can change my script to append the output of `/proc/self/fd/ into some text file, and then when the tests exit, check that there are no surprises there. (TCL is not so fun, but it's not that complicated to guess your way though it and write these few lines)."
760261204,8242,oranagra,2021-01-14T15:15:35Z,"just to be clear, please run `./runtest-sentinel` on the branch with my modifications, and you'll see it prints a few open file descriptors other than 0,1,2."
760299608,8242,panjf2000,2021-01-14T16:15:45Z,"> just to be clear, please run `./runtest-sentinel` on the branch with my modifications, and you'll see it prints a few open file descriptors other than 0,1,2.

Is it supposed to print all leaked fds after the cleanup? I didn't spot any fds in the end of the output:
![image](https://user-images.githubusercontent.com/7496278/104617700-b7acca80-56c6-11eb-9482-1a08f39b2a9a.png)
Did I miss anything?
@oranagra "
760304938,8242,panjf2000,2021-01-14T16:24:08Z,"Never mind, I added some commands before listing leaked fds and I can see it now, working on those leaked fds."
760319000,8242,oranagra,2021-01-14T16:46:35Z,"yeah.. each time this script runs, it should not find any fds other than 0,1,2."
760432033,8242,panjf2000,2021-01-14T19:40:48Z,"I've eliminated the opened pipes in sentinel successfully, but there are still opened sockets residing, even though I've add some code attempting to close those sockets, it doesn't seem to be working, I am at my wit's end now.

Since @hwware is an expert on redis sentinel, could you come rescue me here? Thanks!!!

Before:
![image](https://user-images.githubusercontent.com/7496278/104637492-cf447d00-56df-11eb-8f10-4e94ae1bfaae.png)

After:
![image](https://user-images.githubusercontent.com/7496278/104636170-e5e9d480-56dd-11eb-9333-574b76d569d5.png)

"
761522846,8242,panjf2000,2021-01-16T07:41:40Z,"After a deep dive into the test scripts, it turns out that those remaining TCP sockets might be created and held by `tclsh`:

```shell
lrwx------ 1 root root 64 Jan 16 15:17 0 -> /dev/pts/2
lrwx------ 1 root root 64 Jan 16 15:17 1 -> /dev/pts/2
l-wx------ 1 root root 64 Jan 16 15:17 2 -> /data/dev/c/redis/tests/sentinel/tmp/sentinel_4/err.txt
lrwx------ 1 root root 64 Jan 16 15:17 21 -> socket:[384367382]
lrwx------ 1 root root 64 Jan 16 15:17 22 -> socket:[384371776]
lrwx------ 1 root root 64 Jan 16 15:17 23 -> socket:[384371777]
lrwx------ 1 root root 64 Jan 16 15:17 24 -> socket:[384367388]
lr-x------ 1 root root 64 Jan 16 15:17 3 -> /proc/31264/fd
lrwx------ 1 root root 64 Jan 16 15:17 8 -> socket:[384347306]
COMMAND     PID USER   FD   TYPE             DEVICE SIZE/OFF      NODE NAME
notify.sh 31252 root  cwd    DIR             252,17     4096   9961803 /data/dev/c/redis/tests/sentinel/tmp/sentinel_1
notify.sh 31252 root  rtd    DIR              252,1     4096         2 /
notify.sh 31252 root  txt    REG              252,1   922760     45210 /usr/bin/bash
notify.sh 31252 root  mem    REG              252,1  2172544     24989 /usr/lib64/libc-2.17.so
notify.sh 31252 root  mem    REG              252,1    19336     25105 /usr/lib64/libdl-2.17.so
notify.sh 31252 root  mem    REG              252,1   174520     25867 /usr/lib64/libtinfo.so.5.9
notify.sh 31252 root  mem    REG              252,1   163952      4899 /usr/lib64/ld-2.17.so
notify.sh 31252 root  mem    REG              252,1   217032      5616 /var/db/nscd/passwd
notify.sh 31252 root  mem    REG              252,1    42880     25620 /usr/lib64/libonion_security.so.1.0.19
notify.sh 31252 root    0u   CHR              136,2      0t0         5 /dev/pts/2
notify.sh 31252 root    1u   CHR              136,2      0t0         5 /dev/pts/2
notify.sh 31252 root    2w   REG             252,17        0   9961808 /data/dev/c/redis/tests/sentinel/tmp/sentinel_1/err.txt
notify.sh 31252 root    3u  unix 0xffff88034799f700      0t0 384395141 socket
notify.sh 31252 root   19u  IPv4          384368245      0t0       TCP VM-93-184-centos:microsan->VM-93-184-centos:50063 (ESTABLISHED)
notify.sh 31252 root   20u  IPv4          384372816      0t0       TCP VM-93-184-centos:microsan->VM-93-184-centos:50065 (ESTABLISHED)
notify.sh 31252 root   21u  IPv4          384367384      0t0       TCP VM-93-184-centos:microsan->VM-93-184-centos:50066 (ESTABLISHED)
notify.sh 31252 root   22u  IPv4          384364298      0t0       TCP VM-93-184-centos:microsan->VM-93-184-centos:49946 (ESTABLISHED)
notify.sh 31252 root   23u  IPv4          384367386      0t0       TCP VM-93-184-centos:microsan->VM-93-184-centos:50070 (ESTABLISHED)
notify.sh 31252 root  255r   REG             252,17      296   9834001 /data/dev/c/redis/tests/sentinel/tests/includes/notify.sh
COMMAND     PID USER   FD   TYPE             DEVICE SIZE/OFF      NODE NAME
notify.sh 31259 root  cwd    DIR             252,17     4096   9961811 /data/dev/c/redis/tests/sentinel/tmp/sentinel_4
notify.sh 31259 root  rtd    DIR              252,1     4096         2 /
notify.sh 31259 root  txt    REG              252,1   922760     45210 /usr/bin/bash
notify.sh 31259 root  mem    REG              252,1  2172544     24989 /usr/lib64/libc-2.17.so
notify.sh 31259 root  mem    REG              252,1    19336     25105 /usr/lib64/libdl-2.17.so
notify.sh 31259 root  mem    REG              252,1   174520     25867 /usr/lib64/libtinfo.so.5.9
notify.sh 31259 root  mem    REG              252,1   163952      4899 /usr/lib64/ld-2.17.so
notify.sh 31259 root  mem    REG              252,1   217032      5616 /var/db/nscd/passwd
notify.sh 31259 root  mem    REG              252,1    42880     25620 /usr/lib64/libonion_security.so.1.0.19
notify.sh 31259 root    0u   CHR              136,2      0t0         5 /dev/pts/2
notify.sh 31259 root    1u   CHR              136,2      0t0         5 /dev/pts/2
notify.sh 31259 root    2w   REG             252,17        0   9961820 /data/dev/c/redis/tests/sentinel/tmp/sentinel_4/err.txt
notify.sh 31259 root    3u  unix 0xffff88034799a300      0t0 384395149 socket
notify.sh 31259 root    8u  IPv4          384347306      0t0       TCP VM-93-184-centos:20004->VM-93-184-centos:41461 (ESTABLISHED)
notify.sh 31259 root   21u  IPv4          384367382      0t0       TCP VM-93-184-centos:20004->VM-93-184-centos:42458 (ESTABLISHED)
notify.sh 31259 root   22u  IPv4          384371776      0t0       TCP VM-93-184-centos:20004->VM-93-184-centos:42462 (ESTABLISHED)
notify.sh 31259 root   23u  IPv4          384371777      0t0       TCP VM-93-184-centos:20004->VM-93-184-centos:42463 (ESTABLISHED)
notify.sh 31259 root   24u  IPv4          384367388      0t0       TCP VM-93-184-centos:20004->VM-93-184-centos:42468 (ESTABLISHED)
notify.sh 31259 root  255r   REG             252,17      296   9834001 /data/dev/c/redis/tests/sentinel/tests/includes/notify.sh
```

```shell
lsof -i :41461            
                                                                                             
COMMAND     PID USER   FD   TYPE    DEVICE SIZE/OFF NODE NAME
tclsh8.5  27447 root    9u  IPv4 384342645      0t0  TCP VM-93-184-centos:41461->VM-93-184-centos:20004 (ESTABLISHED)
redis-sen 27484 root    8u  IPv4 384347306      0t0  TCP VM-93-184-centos:20004->VM-93-184-centos:41461 (ESTABLISHED)

nets 41461                                                                                  

tcp        0      0 127.0.0.1:20004         127.0.0.1:41461         ESTABLISHED 27484/../../../src/
tcp        0      0 127.0.0.1:41461         127.0.0.1:20004         ESTABLISHED 27447/tclsh8.5
```

These sockets are connected between `tclsh` and `redis-sentinel`:
![image](https://user-images.githubusercontent.com/7496278/104806074-c357d880-580f-11eb-864c-4b54bdacec8c.png)

Besides, I've tracked down the sentinel process searching every `socket` system call with TCP before `execve`, all TCP sockets are set with `FD_CLOEXEC` flag after being created:

```shell
...

18618 12:11:36.811761 socket(PF_INET, SOCK_STREAM, IPPROTO_TCP) = 22 <0.000028>
18618 12:11:36.811815 fcntl(22, F_GETFL) = 0x2 (flags O_RDWR) <0.000023>
18618 12:11:36.811871 fcntl(22, F_SETFL, O_RDWR|O_NONBLOCK) = 0 <0.000021>
18618 12:11:36.811943 connect(22, {sa_family=AF_INET, sin_port=htons(20001), sin_addr=inet_addr(""127.0.0.1"")}, 16) = -1 EINPROGRESS (Operation now in progress) <0.000046>
18618 12:11:36.812029 fcntl(22, F_GETFD) = 0 <0.000023>
18618 12:11:36.812086 fcntl(22, F_SETFD, FD_CLOEXEC) = 0 <0.000053>
18618 12:11:36.812183 epoll_ctl(5, EPOLL_CTL_ADD, 22, {EPOLLOUT, {u32=22, u64=22}}) = 0 <0.000024>

...

23636 12:11:37.606442 socket(PF_LOCAL, SOCK_DGRAM, 0) = 20 <0.000023>
23636 12:11:37.606502 fcntl(20, F_GETFL) = 0x2 (flags O_RDWR) <0.000019>
23636 12:11:37.606551 fcntl(20, F_SETFL, O_RDWR|O_NONBLOCK) = 0 <0.000018>
23636 12:11:37.606604 fcntl(20, F_GETFD) = 0 <0.000020>
23636 12:11:37.606658 fcntl(20, F_SETFD, FD_CLOEXEC) = 0 <0.000020>
23636 12:11:37.606712 sendto(20, ""\0\0\0\4\0\0\1D\0\36../../tests/includes/n""..., 324, 0, {sa_family=AF_LOCAL, sun_path=""/usr/local/sa/agent/log/agent_cmd.sock""}, 40) = 324 <0.000030>
23636 12:11:37.606784 close(20)         = 0 <0.000020>
23636 12:11:37.606842 execve(""../../tests/includes/notify.sh"", [""../../tests/includes/notify.sh"", ""-monitor"", ""master mymaster 127.0.0.1 30004""], [/* 41 vars */]) = 0 <0.000236>

...
```

Therefore I reckon that the result from the current test scripts could be false positive? 

How do you guys think of this? @oranagra @hwware "
761530502,8242,oranagra,2021-01-16T09:00:20Z,"unless i'm missing something, the tclsh can't open sockets to sentinel without sentinel cooperation.
aren't these simply the client connections, accepted by `anetGenericAccept`?"
761532282,8242,panjf2000,2021-01-16T09:14:54Z,"> unless i'm missing something, the tclsh can't open sockets to sentinel without sentinel cooperation.
> aren't these simply the client connections, accepted by `anetGenericAccept`?

You are right, these are indeed the client connections which should also be set with `FD_CLOEXEC` flag. I had mistakenly thought they were internal fds created by the sentinel, then I guess all leaked fds of sentinel are handled properly now."
761546941,8242,panjf2000,2021-01-16T11:19:46Z,"All fd leaks in sentinel are now fixed and the test scripts for detecting sentinel fd leaks have been added into the Redis test suite, please take a look. @oranagra "
761828249,8242,oranagra,2021-01-17T15:13:56Z,"ok. all seems good to me. let's wait for @hwware review and then we can merge it.
@panjf2000 thank you for bearing with me.."
761828911,8242,panjf2000,2021-01-17T15:19:11Z,"> ok. all seems good to me. let's wait for @hwware review and then we can merge it.
> @panjf2000 thank you for bearing with me..

Don't say that, it's all for keeping the Redis code refined.\(^o^)/
@oranagra "
762792456,8242,panjf2000,2021-01-19T11:49:46Z,any progress on this PR？
762863697,8242,oranagra,2021-01-19T14:11:15Z,"@panjf2000 i'll ping @hwware again, and set a reminder to merge this tomorrow either way."
762874513,8242,panjf2000,2021-01-19T14:27:53Z,"Okay, thanks~"
762877566,8242,hwware,2021-01-19T14:31:45Z,"sorry @panjf2000 @oranagra , got overwhelmed by the github notification emails and did not see your guys ping, i will review ASAP :("
762928883,8242,hwware,2021-01-19T15:45:34Z,"hi @panjf2000 , the code looks good to me, thank you very much for your time and good work...

one comments regarding the tests, i saw the way you are using for check fd leaks is for linux only, if you running in sentinel tests on mac or other os you may see a lot of warnings
 `ls: /proc/self/fd: No such file or directory`
maybe you can consider to add a if else check in the notify.sh using uname command to avoid running the fds leak check in other os environment. thanks. Also pinging @oranagra in case if i miss anything here . thanks"
762949630,8242,panjf2000,2021-01-19T16:14:48Z,"> hi @panjf2000 , the code looks good to me, thank you very much for your time and good work...
> 
> one comments regarding the tests, i saw the way you are using for check fd leaks is for linux only, if you running in sentinel tests on mac or other os you may see a lot of warnings
> `ls: /proc/self/fd: No such file or directory`
> maybe you can consider to add a if else check in the notify.sh using uname command to avoid running the fds leak check in other os environment. thanks. Also pinging @oranagra in case if i miss anything here . thanks

OK, working on it, will update the shell a few minutes later."
762962877,8242,panjf2000,2021-01-19T16:34:29Z,"Now the shell will play to the score on different OS's, please take a look, thank~
@oranagra @hwware "
762984592,8242,panjf2000,2021-01-19T17:07:34Z,"![image](https://user-images.githubusercontent.com/7496278/105068269-b65f1180-5abb-11eb-831d-7458f64bd9a6.png)

No idea why this is happening, the installation for tcl8.5 worked well before, any clues? @oranagra @hwware "
762990521,8242,hwware,2021-01-19T17:16:40Z,"Hi @panjf2000 , I don't think this is related to your changes, maybe the ci environment is broken for some reason, another pr also fails in the same case https://github.com/redis/redis/actions/runs/496514048"
762992548,8242,panjf2000,2021-01-19T17:19:39Z,"Okay, then could you please comment on the latest commit？:）
@hwware "
763136727,8242,oranagra,2021-01-19T20:57:57Z,"Thanks @panjf2000 , finally merged."
764494216,8242,oranagra,2021-01-21T09:14:08Z,"@panjf2000 this test caused many failures in the deily CI
https://github.com/redis/redis/runs/1738852836?check_suite_focus=true
maybe you have time to look into it?

i tried running it locally and it seems to normally pass, so i'm not sure what's the difference.

when i run it locally with TLS, i got these (which are probably easy to solve):
```
make distclean; make BUILD_TLS=yes
./utils/gen-test-certs.sh
./runtest-sentinel --tls
...
WARNING: sentinel test(s) failed, there are leaked fds in sentinel:
total 0
lrwx------ 1 oran oran 64 Jan 21 11:01 0 -> /dev/pts/2
l-wx------ 1 oran oran 64 Jan 21 11:01 1 -> pipe:[12265485]
l-wx------ 1 oran oran 64 Jan 21 11:01 2 -> /home/oran/work/redis/tests/sentinel/tmp/sentinel_0/err.txt
lr-x------ 1 oran oran 64 Jan 21 11:01 3 -> /dev/urandom
lr-x------ 1 oran oran 64 Jan 21 11:01 4 -> /dev/random
lr-x------ 1 oran oran 64 Jan 21 11:01 5 -> /proc/28956/fd
lr-x------ 1 oran oran 64 Jan 21 11:01 7 -> /dev/urandom
lr-x------ 1 oran oran 64 Jan 21 11:01 8 -> /dev/random
total 0
lrwx------ 1 oran oran 64 Jan 21 11:01 0 -> /dev/pts/2
l-wx------ 1 oran oran 64 Jan 21 11:01 1 -> pipe:[12259909]
l-wx------ 1 oran oran 64 Jan 21 11:01 2 -> /home/oran/work/redis/tests/sentinel/tmp/sentinel_0/err.txt
lr-x------ 1 oran oran 64 Jan 21 11:01 3 -> /dev/urandom
lr-x------ 1 oran oran 64 Jan 21 11:01 4 -> /dev/random
lr-x------ 1 oran oran 64 Jan 21 11:01 5 -> /proc/28957/fd
lr-x------ 1 oran oran 64 Jan 21 11:01 7 -> /dev/urandom
lr-x------ 1 oran oran 64 Jan 21 11:01 8 -> /dev/random
```"
764503423,8242,panjf2000,2021-01-21T09:29:40Z,So there are still fd leaks under the TLS?
764517125,8242,oranagra,2021-01-21T09:53:38Z,"yes, there's a problem under TLS (easily reproducible).
but according to the daily CI (https://github.com/redis/redis/runs/1738852836?check_suite_focus=true) there are also problems with non-tls runs."
764522505,8242,panjf2000,2021-01-21T10:02:47Z,Is there any difference between the environment of common CI and daily CI? I've seen all common CI's after this PR pass while daily CIs fail.
764527682,8242,oranagra,2021-01-21T10:11:39Z,"looks like the difference is that the normal CI doesn't run the sentinel tests at all (i suppose it was meant to be quick and shallow, and sentinel isn't changed frequently)
https://github.com/redis/redis/blob/unstable/.github/workflows/ci.yml
https://github.com/redis/redis/blob/unstable/.github/workflows/daily.yml"
764533653,8242,panjf2000,2021-01-21T10:21:58Z,"Well, everything is fine on my x86_64 linux server, odd...
Also, it seems that some unexpected fds pop up in the child process, like 
`lr-x------ 1 runner docker 64 Jan 21 01:28 45 -> /dev/urandom`
, which doesn't look like the inheritance from the parent process."
764536672,8242,panjf2000,2021-01-21T10:26:57Z,any chance that the root cause is because the test suite runs in docker?
764537471,8242,oranagra,2021-01-21T10:28:24Z,"i doubt it, but can you look into it?
the TLS issue i posted above was without docker."
764538579,8242,panjf2000,2021-01-21T10:30:18Z,"> i doubt it, but can you look into it?
> the TLS issue i posted above was without docker.

OK, I will try to figure out why this is happening in the next few days."
764548840,8242,panjf2000,2021-01-21T10:48:35Z,"```shell
./runtest-sentinel --tls

lrwx------ 1 root root 64 Jan 21 18:36 0 -> /dev/pts/5
lrwx------ 1 root root 64 Jan 21 18:36 1 -> /dev/pts/5
l-wx------ 1 root root 64 Jan 21 18:36 2 -> /data/dev/c/redis/tests/sentinel/tmp/sentinel_4/err.txt
lr-x------ 1 root root 64 Jan 21 18:36 3 -> /proc/29985/fd
lrwx------ 1 root root 64 Jan 21 18:36 9 -> socket:[403060267]
COMMAND   PID USER   FD   TYPE             DEVICE SIZE/OFF      NODE NAME
bash    29979 root  cwd    DIR             252,17     4096   9962834 /data/dev/c/redis/tests/sentinel/tmp/sentinel_4
bash    29979 root  rtd    DIR              252,1     4096         2 /
bash    29979 root  txt    REG              252,1   922760     45210 /usr/bin/bash
bash    29979 root  mem    REG              252,1  2172544     24989 /usr/lib64/libc-2.17.so
bash    29979 root  mem    REG              252,1    19336     25105 /usr/lib64/libdl-2.17.so
bash    29979 root  mem    REG              252,1   174520     25867 /usr/lib64/libtinfo.so.5.9
bash    29979 root  mem    REG              252,1   163952      4899 /usr/lib64/ld-2.17.so
bash    29979 root  mem    REG              252,1   217032      5616 /var/db/nscd/passwd
bash    29979 root  mem    REG              252,1    42880     25620 /usr/lib64/libonion_security.so.1.0.19
bash    29979 root    0u   CHR              136,5      0t0         8 /dev/pts/5
bash    29979 root    1u   CHR              136,5      0t0         8 /dev/pts/5
bash    29979 root    2w   REG             252,17        0   9962848 /data/dev/c/redis/tests/sentinel/tmp/sentinel_4/err.txt
bash    29979 root    3u  unix 0xffff880271895b00      0t0 403062992 socket
bash    29979 root    9u  sock                0,6      0t0 403060267 protocol: TCP
bash    29979 root  255r   REG             252,17      588   9833505 /data/dev/c/redis/tests/sentinel/tests/includes/notify.sh
```

seems like there are one or more sockets that are not listening nor connecting in the child process under TLS, any clue what is it for? where is it created?"
764551742,8242,oranagra,2021-01-21T10:53:49Z,@yossigo can you offer some advise?
764562665,8242,yossigo,2021-01-21T11:13:33Z,@panjf2000 Do you have any idea what's `/usr/lib64/libonion_security.so.1.0.19`? If it gets preloaded or otherwise injected into the process it could also be responsible for those additional fds.
764591298,8242,panjf2000,2021-01-21T11:51:53Z,"this file will be opened even the process is not under TLS:
![image](https://user-images.githubusercontent.com/7496278/105347422-d872a400-5c21-11eb-83c0-4aa7cec19330.png)

see https://github.com/redis/redis/pull/8242#issuecomment-761522846

so it may not be the root cause.
@yossigo "
764668170,8242,yossigo,2021-01-21T14:10:45Z,"@panjf2000 Looking into the TLS part, I think some cleanups are missing."
764724910,8242,yossigo,2021-01-21T15:32:17Z,"Pushed a fix, seems like a small race condition.

This solves the leaked socket, but it seems proper OpenSSL cleanup is also needed to prevent other potential leaks (e.g. `/dev/urandom`).

Need to take a closer look there because (as always) every version of OpenSSL is a bit different."
764729194,8242,panjf2000,2021-01-21T15:38:15Z,Well done!
764731072,8242,panjf2000,2021-01-21T15:41:00Z,Could you also take a look at #8376 ? It will help with debugging this issue. @yossigo 
764868536,8242,oranagra,2021-01-21T19:03:31Z,"@panjf2000 you can experiment and investigate this without merging that PR. 
You can copy parts of daily.yml into ci.yml or modify the trigger (`on`) and `if` (repository) of the daily.yml.

Then if you push it to a branch in your github repo, it'll run there (assuming actions are enabled in your repo). 
Or if you push a PR into our repo, you'll see these tests in the PR (and we'll revert the yaml changes before merging) "
765157265,8242,panjf2000,2021-01-22T06:10:49Z,"That PR is not only for this debugging, also for the future test, the current output file only shows the number and type of an fd, but with `lsof`, we can get more useful details, for instance, we can learn that the leaked sockets are not established, see https://github.com/redis/redis/pull/8242#issuecomment-764548840.

If a failure of this test occurs in the future, we can get details directly from the output of the github actions instead of doing some extra effort in the test suite and run the test suite again for more details."
765870805,8242,panjf2000,2021-01-23T05:29:37Z,"I've opened a new PR for the leftover of fd leaks, #8383, the `/dev/urandom` and pipe leaks are resolved now."
936083406,9601,oranagra,2021-10-06T11:49:20Z,"> https://godbolt.org/z/o7fej75xM

WOW, i'm always impressed by what compilers do (in this case both collapsing lots of reads and shifts into one, and also deleting a libc function call and replacing it with assembly).

Also, that's a great demonstration of how much the UB is in many cases BS, in all 3 cases the compiler still generates code that's still doing unaligned access. :smile: 

of course, had this code is used on an architecture that can't handle unaligned access, the compiler generate different results: https://godbolt.org/z/r58nTMMhP"
937659048,9601,tezc,2021-10-07T10:23:53Z,"@oranagra Just here my understanding overall 

I'm not a compiler dev by any means but I think undefined behavior something like ""almost"" impossible to reason about. Anything can happen. At some point, questions like if integer shift really causes an issue or if overflow would be fine in a specific case loses its meaning because it may not cause a problem with the current compiler version but next version. (or on different architecture). It may work for funcA() calls generally but for one of funcA() calls, compiler decides to inline the function and then decides to apply some optimization which takes advantage of undefined behavior. So, not easy to answer if any of these issues are actual problems :) 

More reasons to hate UBs: A jaw dropping [example](https://kristerw.blogspot.com/2017/09/why-undefined-behavior-may-call-never.html)

Good news, I believe almost all of the reasonable size C projects (if not all) have undefined behaviors (what a good news :) ). Redis has more than these sanitizer findings probably (A lot of things are UB if you follow the standard, makes you depressed each time you learn another thing is also UB). So, no need to be a paranoid and fix each issue. Question is whether we want help from sanitizers or not. If yes, we can follow a pragmatic approach, we can fix some of the issues, suppress ones we don't like.
"
937705880,9601,oranagra,2021-10-07T11:31:56Z,"> More reasons to hate UBs: A jaw dropping example

WOW, i'm not a compiler dev either, but IMHO that optimization shouldn't exist. (instead if C++ devs want speed, they should switch to C!).

anyway, i do agree we wanna avoid using undefined behavior, and that we want to use the sanitizer to help catch them.
i just think that if we can make sure to define the behavior, or only use it when we know it's defined (and fall back to less efficient implementation otherwise), that's better in some of these cases (the ones which [uglify](https://github.com/redis/redis/pull/8910) the code or make it less [efficient](https://github.com/redis/redis/pull/9601#discussion_r723284609)).

**but i don't want to rely on specific flags that exist only in some compilers, and now i'm also paranoid about the fact that a compiler will decide to erase all the files on my disk [see below].
so at this point i'm not sure what to do, it seems my two desires collide.**

[below]
i.e. because if we do something that's undefined, then erasing all the files on the disk is a possible outcome. (if on some theoretical platform that would be the behavior, why not apply it for all).
i know that's not the reason in the above link, but who knows what other interpretations of undefined behaviors exist..."
948077442,9601,tezc,2021-10-20T22:14:05Z,"@oranagra 

I tried to minimize the changes, please take another look at the PR. Also, realized that latest clang gives more warnings, so I fixed them as well and added clang build to the daily. Let me know if you're okay with that. (We can use GCC and/or Clang with UBSAN and ASAN). 

I suppressed unaligned load issues. To be honest, I don't think these would cause any performance issues in any environment but it's impossible to benchmark it anyway. So, suppressed them in a few place. "
964176873,9601,oranagra,2021-11-09T13:59:53Z,"@tezc i see all comments are addressed.. so are we good to merge this?

triggered a full CI to see there are no complaints by some esoteric platform: https://github.com/redis/redis/actions/runs/1439775340"
964193623,9601,tezc,2021-11-09T14:18:53Z,"@oranagra I was waiting Daily to be green to rebase and run full CI for this branch. You triggered that action for ""unstable"" right? Anyway, I also triggered CI for these PR + rebase here : https://github.com/tezc/redis/actions/runs/1439837027

Edit: Ok I see, you used github action from unstable. Sorry, I was confused for a moment. It's better to use github action from this branch so it runs sanitizer builds as well. CI build I triggered does that. Let's wait and see if any new issue is introduced after rebase. I'll ping you when it ends."
964204721,9601,oranagra,2021-11-09T14:30:40Z,"ohh, right. the action i triggered is silly:
1. it uses the old yaml files.
2. the branch wasn't rebased lately and unstable was a mess so it may contain bugs that are now fixed."
966239901,9601,tezc,2021-11-11T11:50:50Z,"@oranagra Rebased the Pr and added two new commits, please take a look"
1202464358,9601,oranagra,2022-08-02T12:47:23Z,"Gents, I don't remember if we discussed this, but how do we feel about changing the Makefile to default to `-O3` after this set of changes?"
1203762184,9601,tezc,2022-08-03T10:22:28Z,"@oranagra Sure, we can do it. In my experience, PGO is a bit more risky for UB. So, I think we can just switch to `-O3` now. 

Also, what about `-flto`? Usually, it’s more important. `-O3` might cause some more inlining but I guess `-flto` will inline functions a lot. (between compilation units). So, one downside is we might see less accurate stacktraces on a crash. Are we okay with that? Also, `-flto` will make build a bit slower. 

Another concern, these flags might make some scenarios slower. We need to run some benchmarks for it, hopefully we won’t see regressions. 

"
1203781693,9601,oranagra,2022-08-03T10:43:08Z,"@filipecosta90 already has some benchmarks indicating 5% improvement (with both `-O3` and `-flto` are used). but i wonder they cover a wide range of cases, and if there are any regressions too.
i'm still paranoid about bugs, but considering we're far away from the next release, i guess we can try it.
I also wonder if there were previous discussions on that subject and why we stayed with O2.
@yossigo are you aware of anything from the past?"
1204066019,9601,yossigo,2022-08-03T15:00:50Z,"@oranagra I'm not aware of anything, this is probably a good timing to try this."
1207873724,9601,tezc,2022-08-08T09:17:35Z,"@oranagra I was off last week, catching up.

> i'm still paranoid about bugs, but considering we're far away from the next release, i guess we can try it.

We won't know without trying and I guess sanitizer will help us with a small part of the possible bugs. Still, we might do something extra for it if you want. I think our best bet would be running tests with sanitizers on different architectures manually just to check if we see any failures/warnings. I'd pick these three builds: 32 bit (we can run on our local), armv8 (maybe we can ask someone with new Apple laptop or we run on the cloud) and s390x (we can run on qemu and try to ignore timing related failures. If there are many timing issues, maybe we just check if there is a sanitizer warning and ignore the rest of the failures).

Edit: I'll try these, let's see if we get any warnings.  "
1345511038,9601,oranagra,2022-12-11T10:08:22Z,"in the process of making a new 6.2.x release, the CI on Alpine fails due to bad BITFIELD overflow detection.
https://github.com/redis/redis/actions/runs/3667947707/jobs/6200741331
looks like in this case, the fix here is not just to silence a warning, it actually fixes a bug.
trying to decide between cherry picking this whole PR over there, ignoring the error and letting 6.2 remain as is, or just fix this specific issue with BITFIELD."
1345520019,9601,oranagra,2022-12-11T10:51:07Z,"I took parts of this PR to 6.2.8 see https://github.com/redis/redis/pull/11607/commits/4418cf166e025e7d0d2c965e75ad57c05ecff43f
and mentioned the BITFIELD issue in the release notes.
i'll also update the top comment here with a note about that issue.
let me know if you see something wrong or have more info."
2080252812,13209,tezc,2024-04-27T00:11:31Z,"@moticless I'm stuck adding hsetf to new setex api. It becomes too complex. I just realized I got setex api impl wrong for listpack even without hsetf (lost track of return values etc.). I started to feel like we would do better without an unified api (maybe it'd better to have some other smaller abstractions). 

I pushed hgetf, I'll prepare and add hsetf without using setex api. There will be duplicated code for these but otherwise I cannot make progress. After that, maybe we'll find a way to reuse some code or you may have a better idea how to combine it to setex api. Meanwhile, please feel free to push changes to integration branch. I won't be able to do my work quickly anyway. "
2099562622,13209,sundb,2024-05-08T01:26:02Z,"1. forget to handle the LISTPACK_EX encoding in rioWriteHashIteratorCursor(), dismissHashObject(). 
2. should we add LISTAPCK_EX support to `debug listpack`?"
2099591511,13209,tezc,2024-05-08T01:56:02Z,"As we discussed, I think we can address rioWriteHashIteratorCursor() and dismissHashObject() when we are dealing with AOF and RDB save. 

Added support for `debug listpack`. "
2100938912,13209,tezc,2024-05-08T16:17:44Z,@sundb @moticless @ronen-kalish Please approve if you don't have any additional comments. I'll merge it if daily run passes fine. 
1557027287,12209,judeng,2023-05-22T11:11:49Z,"CI failed again and seem that nothing to with this pr...
but I didn't find out why, @enjoy-binbin could you help me to have a look? thanks! refer https://github.com/redis/redis/actions/runs/5044212812/jobs/9046965683?pr=12209#step:6:1253
"
1557051233,12209,enjoy-binbin,2023-05-22T11:29:10Z,"don't worry, it looks like the failure has nothing to do with this PR. 
the test framework occasionally does this."
1560136776,12209,vitarb,2023-05-23T21:19:43Z,"Do we have any confirmation from benchmarks or profiling which show that this provides any performance benefits?
Also it looks like module code is already using something similar https://github.com/redis/redis/blob/unstable/src/module.c#L10895 should we try to unify our approach?"
1566665964,12209,judeng,2023-05-29T07:10:30Z,"> Do we have any confirmation from benchmarks or profiling which show that this provides any performance benefits?
Also it looks like module code is already using something similar https://github.com/redis/redis/blob/unstable/src/module.c#L10895 should we try to unify our approach?

@vitarb thank you and sorry for the late reply, we indeed need a benchmark and I will test it when all advises are fixed.
I'm not good at aboult module, IMO the code is reusable enough, both scan command and module use the `dictScan` function, but have a different callback function."
1569800114,12209,judeng,2023-05-31T09:09:00Z,"@vitarb @oranagra hi, here is the perfomance result:
1. scan keys which have expirtion time

scan  | match all | scan pattern and 100% unmatched | scan pattern and 50% unmatched | scan type and 100% matched | scan type and 100% unmatched | scan type and 50% matched
-- | -- | -- | -- | -- | -- | --
unstable | 24785 | 60916 | 33435 | 16693 | 26946 | 21249
this pr | 24925 | 169997 | 40288 | 22913 | 130979 | 42820

2. hscan key which use dict encoding

hscan | 100% matched | hscan pattern and 100% unmatched
-- | -- | --
unstable | 17949 | 31693
this pr | 18088 | 143322

3. sscan key wich use intset encoding

sscan intset | 100% matched | sscan pattern and 20% matched
-- | -- | --
unstable | 7529 | 10255
this pr | 7819 | 22122


4. zscan key which use listpack encoding

zscan listpack | 100% matched | zscan pattern and 10% matched
-- | -- | --
unstable | 18794 | 28605
this pr | 20004 | 81147


"
1594691416,12209,judeng,2023-06-16T13:38:00Z,"@oranagra I just finished it according to your last review and compare the benchmark performance bettwen before and after the lastest commit.

benchmark command: `scan 0 type string count 1000`
scan | 100% type matched |100% type unmatched
-- | -- | --
before  | 2432| 7811
after| 2491| 8537



benchmark command:`sscan set1 0 count 1000`,  set1 is a key with intset encoding.
benchmark command:`sscan hash1 0 count 1000`,  hash1 is a key with hash encoding, and every field is bigger the 44bytes(avoid 
 use emb str object).
command | sscan |hscan
-- | -- | --
before  | 18959| 114058 
after| 19099| 121752

result analysis:
1. when type string convert to interger, if the key 100% match the type, the performance bottleneck seems to be on the reply, and the performance increase is negligible. But when all key can't match the type, that is, reply is always empty, we gain a 9% increase.
2. Using the sds instread of robj, when filed is small that always used the emb string object, there is nothing different in performance. But when the robj is raw string at before, we gain a 7% increase."
1594859895,12209,oranagra,2023-06-16T15:18:00Z,@judeng did you use a pipeline in your benchmark? 
1598058084,12209,judeng,2023-06-20T03:31:23Z,@sundb thanks
1598215073,12209,oranagra,2023-06-20T06:55:28Z,"WOW. scan is now so much better (or so much less inefficient). fixed all the things that have been bothering me for years...
@redis/core-team please have a look at the behavior changes mentioned in the top comment and approve if they're ok for a minor version release."
1598216730,12209,oranagra,2023-06-20T06:57:02Z,@judeng maybe you can re-do some simple benchmark and mention some quick numbers in the top comment..
1598217679,12209,oranagra,2023-06-20T06:57:50Z,Full CI: https://github.com/redis/redis/actions/runs/5319477496
1599032384,12209,oranagra,2023-06-20T15:34:19Z,"@judeng the top comment says
> from now only the keys matched to the type / pattern filter will attempt to be lazy expired (deleted).

which is my edit, but looking at the code again, i see that it might not be correct.
before we could expire keys by either the `expireIfNeeded` call or the `lookupKeyReadWithFlags` call.
so i'm i'm not missing anything, it means that the behavior change in this PR is only for the TYPE matching.
the pattern MATCH feature already did skip the expiration attempt before this PR.

please ack that i'm right."
1599233953,12209,oranagra,2023-06-20T17:37:54Z,"while on that subject of breaking changes, i discussed this PR with the core-team earlier today and we concluded that we'd like to avoid the other behavior changes in 7.2 as well (due to be released in a month or so), since we're required by semver to avoid breaking changes in minor versions.
this leaves us with two options:
1. leave this PR out of 7.2 and merge it afterwards (so it lands in 8.0).
2. modify this PR (leaving some of the optimizations for a later one), so that it doesn't introduce behavior changes, and merge it now.

i prefer number 2, even though it's more work, rather than have roughly a year before people can enjoy the optimizations we made.
AFAIK this means we can keep the memcpy and malloc optimizations and also the early pattern MATCH changes, but that we have to give up the type name filter improvements.
i.e. we need to do the type matching by string and at a late stage, or we can keep doing it by integer comparison, but avoid the side effects (keep doing lookupKey, and make sure that unrecognized types don't match any key).
then soon after 7.2 is released we can merge another PR to apply the changes again (bring it to the current state of the PR).

WDYT?"
1600024053,12209,judeng,2023-06-21T03:34:57Z,"> it means that the behavior change in this PR is only for the TYPE matching.

yes, you are right, I confused them"
1600113605,12209,judeng,2023-06-21T05:34:55Z,"> 2. modify this PR (leaving some of the optimizations for a later one), so that it doesn't introduce behavior changes, and merge it now.

I also prefer number 2, I'll roll back the code asap. 
"
1600123145,12209,oranagra,2023-06-21T05:37:24Z,"thanks.
ideally, try to still avoid string compare, and just put the startup type check in a comment so we can revive it later.
i.e. try to keep the optimizations, but add some code or temporarily disable some code, so that we keep the old behavior temporarily."
1607438212,12209,oranagra,2023-06-26T13:09:00Z,"@judeng thanks for following all my requests (both the tips to incrementally further improve SCAN compared to what this PR initially meant to do), and also the rollback process, tests and comments.
i'm quite happy with the final result.

one last comment to handle IMHO is https://github.com/redis/redis/pull/12209#discussion_r1241100614
i.e. next to the currently disabled `SCAN unknown type` test, add a temporary one that does the opposite (prove that unknown types don't filter anything).

after that, please refresh the top comment, which i'll use as a squash-merge commit comment. if you can also include some raw benchmark numbers to give an idea of the impact of this PR on performance. and then i'll merge it."
1607600335,12209,judeng,2023-06-26T14:22:40Z,"Looks all set, I'm going to retest the performance impact tomorrow. "
1607802104,12209,oranagra,2023-06-26T16:11:44Z,"@judeng I updated the top comment. let me know if you see any problem.
i.e. the current version doesn't have any behavior changes (we moved them all to 8.0)"
1607889267,12209,oranagra,2023-06-26T17:10:45Z,"i checked out this PR, to do two things:
1. run coverage report, in which i found that the listDelNode after expireIfNeeded was uncovered (apparently all the tests for expiring keys used TYPE filter).
2. run all the new tests against the old version, to make sure we didn't have any unnoticed behavior change."
1607891894,12209,oranagra,2023-06-26T17:12:39Z,full CI: https://github.com/redis/redis/actions/runs/5381646678
1608002479,12209,oranagra,2023-06-26T18:25:55Z,"> * run coverage report, in which i found that the listDelNode after expireIfNeeded was uncovered (apparently all the tests for expiring keys used TYPE filter).

i now realize that this was already covered by expire.tcl (i only tested coverage by running scan.tcl).
but that test seems to be focused on MULTI-EXEC and replication, so i suppose we better keep the one i added."
1609222698,12209,judeng,2023-06-27T10:24:12Z,"@oranagra I have updated the performance test results in the top comment, it seems that except for the type filter, the other results have not changed much"
1609305404,12209,oranagra,2023-06-27T11:21:34Z,"Thanks.
is the first column, the one who says ""match all"" completely unfiltered?
i understand that none of these tests use volatile keys, right?"
1609398856,12209,judeng,2023-06-27T12:24:49Z,"> is the first column, the one who says ""match all"" completely unfiltered?

yes, I used the `match *`, actually it is eqaul with no any filters.

> i understand that none of these tests use volatile keys, right?

yes, I used the no-volatile keys to better display the perfermance improvement :-)"
1609539339,12209,oranagra,2023-06-27T13:45:34Z,"merged.
@judeng thank you for your dedicated work.
feel free to issue another PR to delete / uncomment lines, which i'll merge to unstable once 7.2 is out. (just so that we don't forget).
feel free to re-do some benchmark to show what additional improvements it brigs."
1616746616,12209,oranagra,2023-07-02T17:42:16Z,@judeng can you make a PR? i'm afraid to forget...
1617390410,12209,judeng,2023-07-03T05:38:51Z,"@oranagra Sorry, I didn't forget about it, just had a couple of nasty accidents last week so I didn't have time, I'll to work on it this week."
986062379,9890,zuiderkwast,2021-12-04T17:25:24Z,"""Sort out mess"" :+1: :grin:
"
986252280,9890,oranagra,2021-12-05T15:41:11Z,"@guybe7 thanks..  

Few requests while i review:
1. please start off the top comment to describe what was that mess and what complications it caused. i.e. some parts using ""also"" late propagation, others using an immediate one, causing edge cases, ugly / tacky code, and tendency for bugs?.
2. please look into the sanitizer timeout failure in `LATENCY of expire events are correctly collected`, i don't recall seeing it before."
986271621,9890,guybe7,2021-12-05T17:45:11Z,"@oranagra i can't figure it out, i ran it locally (build with SANITIZER=address and ran the tests) and it passed"
986279798,9890,oranagra,2021-12-05T18:41:58Z,"I've re triggered the tests and got the same outcome. 
Maybe it has something to do with a timing issue issue causing a hung, or maybe related to the gcc version being used? "
989667097,9890,oranagra,2021-12-09T09:27:52Z,@soloestoy i'll be happy if you can have a look and see if you can spot any issues with the new approach.
989682422,9890,soloestoy,2021-12-09T09:48:01Z,">@soloestoy i'll be happy if you can have a look and see if you can spot any issues with the new approach.

OK, I'll check in a day or two, too busy this week..."
997348251,9890,guybe7,2021-12-19T08:15:06Z,"@oranagra @MeirShpilraien please review:
https://github.com/redis/redis/pull/9890/files/f64b6c3ab6c7061ca73aa479e097ab89d36a9a53..74db1d538d1ef2d9a4e68bc195bae145cabdc738"
997802768,9890,oranagra,2021-12-20T10:37:06Z,"@redis/core-team this is not technically a major decision in the sense that it doesn't change any interfaces, but still it's a sensitive subject that you might wanna review, or at least read the top comment."
999594189,9890,oranagra,2021-12-22T13:54:55Z,"test failures are unrelated to this PR (a recent regression in unstable).
merging.."
999603032,9890,oranagra,2021-12-22T14:07:17Z,Full CI: https://github.com/redis/redis/actions/runs/1611637038
1000690581,9890,soloestoy,2021-12-24T07:08:13Z,"I don't like the way you call `preventCommandPropagation()` in `updateMaxmemory()`, the code is not easy to understand. And you didn't fix the bug thoroughly.

Actually it's not only the eviction's bug, it's very complex involve module system and redis-core propagation mechanism and notification system.

You give an example that the `propagate-test` module subscribes all keyspace events, and the `notify_callback` does a sync `RM_Call` with `!` that lead to `server.dirty` increasement.

Yes, it may let `config set maxmemory` be propagated, but to make matters worse, the wrong usage of module notification callback may lead to more bugs, for instance:

A read command like `GET` touch a stale key and triggers the lazy expire deletion, and a module's `notify_callback` call a write command and let `server.dirty` grows. After all in `call()` the read command `GET` will be propagated.

I believe you would not call `preventCommandPropagation()` in all read commands to fix it, I think we need refactor the propagation mechanism, the first idea comes to my mind is give the native redis commands a context to record what happened during the execution and recored what is cased by itself and what is cased by others(like modules). It's a big work we need design well at first."
1000915537,9890,oranagra,2021-12-24T18:47:59Z,"@soloestoy I'm sorry, I now realize you said you wanna review this (it was quite a while ago).. 
I'm not sure i understand all that you wrote (maybe it's the wine), so first let me ask two general questions:

do you agree with the general approach (of always queuing propagation ""later"", and not mixing it with immediate propagation), and just have concerns about some specific details? (which we can improve in a followup PR). 

Did you spot any regression or bug introduced here? or just that it didn't completely solve the problems? 

I.e. I'm trying to understand if your suggestion at the end is one that builds on top of what's done here, or completely replace it, and if you think it can me planned for some future day (we're running out of time for 7)? Or if you think we must change / improve things ASAP. "
1000921154,9890,guybe7,2021-12-24T19:35:08Z,"@soloestoy yes, I don't like it either.
AFAIC we can revert the preventPropagation change until we find a better solution for the damage potential caused by notifications+module.

But, in general, are you ok with the approach of this PR?"
1001553732,9890,soloestoy,2021-12-27T12:48:56Z,"@oranagra @guybe7 I agree we should sort out the mess around propagation, but I have to say the propagation is so complex... and I think we should clarify the current mechanism (before and after this PR) first, here I try to arrange the background and the relation between propagation and other systems:

# Introduction
Redis propagation mechanism has two categories in general, one is about single command and another is about some special features like transaction and modules etc.

## Single command
Redis propagation is so complex, the single command is the simplest part, but it still has three different ways:
1. The general way in `call()` function, check if `server.dirty` increases after command's `proc()` executed. If so redis will propagate the command (to AOF or replicas).
2. Some commands like `EXPIRE` `INCRBYFLOAT` rewrite themselves by using `rewriteClientCommandVector()` `replaceClientCommandVector()` and `rewriteClientCommandArgument()` to reach deterministic propagate.
3. Some commands like `SPOP key count` use `alsoPropagate()` and `preventCommandPropagation()` to reach deterministic propagate (and wrap with `MULTI/EXEC` if the array has more than one command, see #6615).

p.s. I think we can eliminate way 2, just keep way 3 is enough.

## Special features
I have to say because of these features, Redis propagation became very complex.
1. Transaction
    * Before 6.2, transaction emits `MULTI` by itself before the first write command in `execCommand()`, and does `server.dirty++` to let function `call()` propagate `EXEC` (which I think it's not easy to understand).
    * After 6.2 with #8097 and #8216 , function `propagate()` handles the `MULTI` emitting (by checking `server.in_exec` and `server.propagate_in_transaction`), but `EXEC` propagation still depends on `server.dirty++` in `execCommand()` (it's still hard to understand IMHO).
    * Now this PR introduce a new approach simplify `MULTI/EXEC` by using `server.in_nested_call`, I like this approach (but the new mechanism introduces some new edge cases and bugs...).
2. Lua
    * ~~The script verbatim replication has already removed in #9812.~~
    * Similar to transaction, before this PR Lua emits `MULTI` (in `luaRedisGenericCommand()`) and `EXEC` (in `evalCommand()`) both by itself.
        * Note: Lua in transaction would not lead to nested `MULTI/EXEC`.
    * This PR also eliminates the complexity about Lua.
3. Modules
    * Similar to transaction and Lua, `RM_Call()` with `!` emits `MULTI`, and `moduleFreeContext()` append `EXEC` (complex and hard to understand).
        * This PR eliminates the complexity.
    * Another API `RM_ReplicateVerbatim()` allow modules propagate the original module command, it calls both `alsoPropagate()` and `server.dirty++`, **but `server.dirty` doesn't work in `call()` on module command**. Because module cannot modify `server.dirty` directly (it's hard to understand).
    * **Some other callbacks can call `RM_Call()` and lead to bugs, like `notify_callback` this PR described (module system is too flexible)**.
4. Expire
    * Before this PR, expire deletion would call `propagate()` directly, and doesn't need to check if it's in transaction/Lua or not.
    * After this PR, expire deletion use `alsoPropagate()` mechanism 
        * **to make lazy expire deletion in transaction be wrapped in `MULTI/EXEC`, or the deletion would be propagated before `MULTI/EXEC` and lead to bug**.
        * **to make active expire deletion be *not* wrapped in `MULTI/EXEC`, set `server.propagate_no_multi` to 1 in `activeExpireCycle()`**
5. Eviction
    * Same as expire.
6. Block
    * Before this PR, the block system calls `propagate()` directly.
    * After this PR, the block system uses `alsoPropagate()` and needs call `afterCommand()` to execute propagate to avoid `MULTI/EXEC` wrapping.

# Other edge cases
In addition to the two general categories, there are also some other edge cases, especially about transaction, maybe modules.
## Before this PR in transaction:
1. Use `BGSAVE` in transaction, like `MULTI; SET foo bar; BGSAVE; SET foo bar2; EXEC`
    * After `BGSAVE` done, key `foo` in `RDB` file is the old value `bar`, which break transaction's atomicity IMHO.
2. Use `BGREWRITEAOF` in transaction, like `MULTI; SET foo bar; BGREWRITEAOF; SET foo bar2; EXEC`
    * After `BGREWRITEAOF` done, the base AOF file contains the transaction's first half `SET foo bar` without `MULTI` (because the child process dump all data to disk), and the inc AOF file contains the transaction's second half `SET foo bar2` without `EXEC`, **we lose the `MULTI`**.
3. Use `config set appendonly yes` can cause the same problem.

I don't think call **admin command** in transaction is a good choice, maybe we can disable them in transaction, but it's a breaking change, not sure.

## After this PR in transaction:
1. The `BGSAVE` problem remains.
2. The `BGREWRITEAOF` problem is more serious, `MULTI; LPUSH list a b c; BGREWRITEAOF; LPUSH list d e f; EXEC`
    * The base AOF contains single `SET foo bar`, but the inc AOF contains the whole transaction (since we use `alsoPropagate()`) and the first part `LPUSH list a b c` would be executed twice when loading, which lead to data inconsistency.
3. Use `config set appendonly yes` has the same problem.

## Modules
1. Use `RM_Call(write command)` with `!` in `notify_callback` may let some commands be propagated unexpected, 
    * `config set maxmemory` this PR described and fixed, but not easy to understand.
    * A read command like `GET` touch a stale key and triggers module's `notify_callback`.
2. **Modules may have more problems, not sure, someone may add supplement.**

# Summary
We can see that, this PR sorted out mess around **single command** scenario, but in **special features** and **edge cases** scenario, it introduced some new messes... and maybe more complex, like how and when to use `server.propagate_no_multi` and call `afterCommand()` is hard to handle.

And I'm worried maybe only @oranagra @guybe7 and me understand the new propagation (and I'm afraid even us may forget and write some bugs 😓 ), that's not what we want, I hope the propagation mechanism can be easy to understand and develop base on it, because a lot of systems needs propagate, and we hope more developers can join in."
1001643934,9890,guybe7,2021-12-27T16:25:29Z,"@soloestoy thanks for the thorough summary!

> p.s. I think we can eliminate way 2, just keep way 3 is enough.

I agree we should always use alsoPropagate() and preventCommandPropagation() to reach deterministic propagate and eliminate all ""command vector"" functions

> Another API RM_ReplicateVerbatim() allow modules propagate the original module command, it calls both alsoPropagate() and server.dirty++, but server.dirty doesn't work in call() on module command. Because module cannot modify server.dirty directly (it's hard to understand).

we should eliminate all `server.dirty++` in module.c, they are useless

> After this PR, the block system uses alsoPropagate() and needs call afterCommand() to execute propagate to avoid MULTI/EXEC wrapping.

just a side note, `afterCommand` was already there before my PR. the first version used to call `propagagtePendingCommands` just once, at the end of `handleClientsBlockedOnKeys` (with server.propagate_no_multi=1)

snapshot-creating commands in transactions: we have https://github.com/redis/redis/pull/10015 but it doesn't handle the `config set appendonly yes` case

modules: https://github.com/redis/redis/issues/10014


"
1001664550,9890,oranagra,2021-12-27T17:18:02Z," @soloestoy thanks for the summary of the background and history.
 with regards to your ending statement (the **Summary** section), i don't think we can make the propagation mechanism as simple as we would have hoped, and it'll always require some understanding of the details and corner cases, but IMHO the question is it didn't become less dangerous with this PR.
 i.e. the previous code may have looked simple in some places, but it had unexpected bugs due to two mechanisms working separately, and i think in the new mechanism it's safer.
 
 With regards to the bugs in the **edge cases** section, taking a snapshot (fork or foreground SAVE) inside a transaction was a bug before this PR. This PR changed the way that bug was manifested, and the solution should be just to block them: #10015 i don't see any other way around that, and i don't see a problem blocking them.
 
 With regards to the problems in the **edge cases** section about modules, this was also a bug before this PR.
 This PR attempted to solve some of them (and did), but you pointed out the solution is not complete and is ugly.
 I can live with fixing part of the problem without solving it completely, but i can also live with reverting the (ugly) fix since it's not complete and leaving it for some future day: #10013.
 if you have a suggestion of how to completely fix things in a clean manner, i'll be glad to hear. but since these are modules that do awkward things, i think we can just document that they shouldn't do that.
 
 With regards to the points in the **Special features** section, i'm not sure what's the problem.
 I see bullets 1 to 6. in some of them you stated that this PR changes things for the best. in others you stated that the solution seems complicated to you (in which case i return to my opening paragraph, which says that it's safer).
 I see two bullets are highlighted:
 **3 - Modules** - isn't this the same one that's discussed in **edge cases* which i responded to right above this paragraph?
 **4 - Expiry** (and **Eviction**) - i'll try to respond below.
 
 > **to make lazy expire deletion in transaction be wrapped in MULTI/EXEC, or the deletion would be propagated before MULTI/EXEC and lead to bug.**
 
 I'm not sure i understand this text. are you claiming that this PR introduced a bug? can you describe the scenario in more detail? (i assume / hope it's something that can be easily resolved).
 
 > **to make active expire deletion be not wrapped in MULTI/EXEC, set server.propagate_no_multi to 1 in activeExpireCycle()**
 
 Again, I'm not sure i understand. are you describing a bug, or complaining that you don't like the solution? (in which case i'll argue again, that it's safer)
 
 All in all your text is quite long and detailed, but i think in this case i'd appreciate less background (which some of us are aware of), and more details that are specific about:
 1. did this PR introduce a regression
 2. is there something missing that can be solved with additional work.
 3. if you think this PR is completely wrong and / or have a better suggestion.
 
 I really wish you would have joined this discussion sooner, i know you were busy, but this PR was outstanding for quite a while, begging for attention and eventually we decide to move on.
 So now we need to decide how to proceed, if we can build on top of what we have now? if we have a different plan which we can try to implement before the release? or if we revert it to go back to the old mechanism? (if the new one is more problematic, or if we're afraid of unexpected issues).

Bottom line, you might be right that there is more to understand in the new approach, but I think it's safer than the old approach, which seemed obvious but had many side effects. "
1001852489,9890,soloestoy,2021-12-28T03:41:33Z,"> but IMHO the question is it didn't become less dangerous with this PR.

@oranagra yes, that's the problem, actually I like the approach this PR proposed, but as you said it didn't become less dangerous, I think we can try to optimize some scenarios based on this PR to make it simple and clear, ping @guybe7 .

> I see bullets 1 to 6. in some of them you stated that this PR changes things for the best. in others you stated that the solution seems complicated to you

It's about bullets 4 to 6 (expiry eviction and block), `server.propagate_no_multi` is too implicit (I don't think it's a good idea that we break the `alsoPropagate()` mechanism, think about that this PR try to sort out mess around `MULTI/EXEC`, but make expiry/eviction/block become more complex), maybe just use `propagateNow()` here is clearer.

Module is too complex, I'm still thinking of it.

> I really wish you would have joined this discussion sooner, i know you were busy, but this PR was outstanding for quite a while, begging for attention and eventually we decide to move on.

My bad, this PR is very big, I was trying to point all problems, maybe next time I can leave my questions first before look deep into it."
1001935180,9890,oranagra,2021-12-28T08:26:46Z,"> > but IMHO the question is it didn't become less dangerous with this PR.

> @oranagra yes, that's the problem, actually I like the approach this PR proposed, but as you said it didn't become less dangerous,

i'm sorry, that was a typo. i meant to say:
> but IMHO the question is **if** it didn't become less dangerous with this PR.

i.e. i think it did become less dangerous.

> It's about bullets 4 to 6 (expiry eviction and block), server.propagate_no_multi is too implicit (I don't think it's a good idea that we break the alsoPropagate() mechanism, think about that this PR try to sort out mess around MULTI/EXEC, but make expiry/eviction/block become more complex), maybe just use propagateNow() here is clearer.

I don't think it really complicated these that much. and i certainly don't think we shifted the complication from one place to the other.
but anyway i'm ready to hear suggestions for further improvement to make it safer and / or clearer."
1002020920,9890,soloestoy,2021-12-28T11:05:50Z,"Another side effect about expiry is, when a write command delete stale key, the expire deletion and the write command would be wrapped in `MULTI/EXEC`, this doesn't lead to data inconsistency but could make AOF and replication stream too fat, it's easy to reproduce:
```
config set appendonly yes
debug SET-ACTIVE-EXPIRE 0
set a b
pexpire a 1
debug sleep 1
set a xx
```
And then AOF contains `MULTI; DEL a; set a xx; EXEC`
"
1002038992,9890,oranagra,2021-12-28T11:29:26Z,"that's intended, when changes happen atomically and in a certain order, they should be propagated like so.
if you have had two write commands (the second one does the expiration), it would be wrong to emit the DEL out of order.
i.e. it's not a side effect. that was on purpose."
1002042204,9890,soloestoy,2021-12-28T11:35:02Z,"> that's intended, when changes happen atomically and in a certain order, they should be propagated like so. if you have had two write commands (the second one does the expiration), it would be wrong to emit the DEL out of order. i.e. it's not a side effect. that was on purpose.

I don't think it's right and necessary, the expire deletion doesn't have relation with the write command, it's just because of the lazy-expire mechanism, and they are not atomic operation (AOF recovery and replica doesn't depend on the atomicity), what we should guarantee is only propagate the expire deletion before the write command."
1002113229,9890,oranagra,2021-12-28T13:45:37Z,"ohh, i think i misunderstood you (or got you and then lost you again). i think in my last argument i was referring to expiration that happens inside a transaction, and you were referring to a plain command that's executed (no transaction), and gets wrapped in transaction.

i think we could add some special case handling that will relax this in some cases (i.e. excluding EXEC, EVAL, and maybe modules). it'll be a bit ugly, but i could argue that if the other complicated paths are handled correctly by the generic mechsnism, and we just relex a specific case, then it's safer than doing it the other way around."
1038460166,10293,zuiderkwast,2022-02-13T22:40:56Z,"I think the slot ranges should be represented as two integers (as @madolson suggested in the issue) rather than a string on the form ""start-end"". This way we use resp for all structure and no extra parsing of a string is needed."
1038468750,10293,hpatro,2022-02-13T23:10:46Z,"> I think the slot ranges should be represented as two integers (as @madolson suggested in the issue) rather than a string on the form ""start-end"". This way we use resp for all structure and no extra parsing of a string is needed.

@zuiderkwast Actually Madelyn and I discussed this offline and thought with an extremely defragmented slot distribution, this would be cheaper for network out and has minimal parsing for the client. "
1038483200,10293,PingXie,2022-02-13T23:55:44Z,"> > I think the slot ranges should be represented as two integers (as @madolson suggested in the issue) rather than a string on the form ""start-end"". This way we use resp for all structure and no extra parsing of a string is needed.
> 
> @zuiderkwast Actually Madelyn and I discussed this offline and thought with an extremely defragmented slot distribution, this would be cheaper for network out and has minimal parsing for the client.

I think @zuiderkwast has a good point. Dealing with exceptions always adds complexity.

Based on my quick back-of-the-envelope calculation, I think we could achieve an overall similar footprint (if not better) using the original proposal with one trick, which is to encode the **length** of the slot range as opposed to the end slot number. Here is an example

1. Encode a slot range of [1000, 1000] using the special string with ""-""

$9\r\n1000-1000\r\n 

The total length is 15 bytes.

2.   Encode the same range using the original proposal but with the range length

*2\r\n:1000\r\n:1\r\n

The total length is also 15 bytes.

Note that a 4-digit slot number is where the two encoding schemes result in the same footprint. When the slot number is greater than 9999, the second encoding scheme yields a smaller footprint while, when the slot number is smaller than 1000, the first encoding scheme yields a smaller footprint. Therefore, in the most extreme case when no two slots are next to each other in the same shard, the second encoding scheme would actually yield an overall smaller footprint while remaining friendly to generic RESP parsing.

"
1038504146,10293,madolson,2022-02-14T00:48:36Z,"~~I like @PingXie's idea. It also a nice property of being easier to understand whether or not the boundary is inclusive.~~

Edit: After looking at the code some more, it seems inconsistent to have different mechanisms return with different values. The intention is this a ""replacement"" of sorts for cluster nodes, which actually gives slots in the form ""a-b"" or ""a"", so I think it's actually reasonable to re-use the code that generates the nodes output."
1038707175,10293,PingXie,2022-02-14T06:40:00Z,"> The intention is this a ""replacement"" of sorts for cluster nodes, which actually gives slots in the form ""a-b"" or ""a"", so I think it's actually reasonable to re-use the code that generates the nodes output.

@madolson did you actually mean a replacement for **CLUSTER SLOTS**? I thought one of the issues called out in #10168 was about the CLUSTER NODES output not being RESP compliant. Also, the output format implemented here is modeled after CLUSTER SLOTS than CLUSTER NODES. 

That said, the use of ""length"" instead of the ""end slot number"" does break away from the CLUSTER SLOTS norm. We are probably looking at an additional ~55KB (6,4K\*4 + 9K\*3 + .9K\*2) overhead in the most fragmented case if we were to go with full RESP compatibility. Assuming each node taking up 300B in the output, a 100-node cluster, and 15B per slot range with the ""-"" representation, the total output size is about 300B*100 + 15B * 16K = 270KB. 55KB is about 20% increase but again this is an extreme case. So here is what I see the tough call lies

1. how likely would one run into this extreme case? it is probably on a long tail if I could guess
2. if something has to give, would it be 
a. breaking away from the norm established by CLUSTER SLOTS and using ""slot range length""; or
b. using a non-RESP compliant representation (the ""-""), which by the way also diverges from the CLUSTER SLOTS norm, though in a different way
"
1038828014,10293,hpatro,2022-02-14T09:06:19Z,"@PingXie 

In case a node owns a single slot, the network output would be something like this for the dash `-` representation.
`$4\r\r1000\r\n`instead of `$9\r\n1000-1000\r\n`. Dash `-` comes into the picture once there are more than one contiguous slot owned by a node."
1038926784,10293,zuiderkwast,2022-02-14T10:39:18Z,"I think the difference of 5-10 bytes per slot range is not very important, even if the difference is 50K.

The real difference is when compared to CLUSTER SLOTS, where the complete nodes are repeated for each slot range. Assuming a maximally fragmented cluster with one master and two replicas per shard, assuming 100B per node info, CLUSTER SLOTS is 3\*100\*16K = 4.8M.

So, even if CLUSTER SHARDS is 100K, we save 98% compared to CLUSTER SLOTS.

Among the options we have:

|   | range | single slot |  |
|--|-----|----|--|
| 1 | `""1000-1340""` | `""1000""` | Yossi's suggestion |
| 2 | `1) 1000 2) 1034` | `1) 1000 2) 1000` | Madelyn's original suggestion |
| 3 | `1) 1000 2) 35` | `1) 1000 2) 1` | Ping's suggestion: start and length |

I think 1 and 2 are acceptable, but 3 is rather obscure."
1039399284,10293,PingXie,2022-02-14T18:13:00Z,"> I think the difference of 5-10 bytes per slot range is not very important, even if the difference is 50K.

+1

> I think 1 and 2 are acceptable, but 3 is rather obscure.

Agreed and 2) would be my preference.
"
1039419482,10293,hpatro,2022-02-14T18:33:20Z,"> > I think the difference of 5-10 bytes per slot range is not very important, even if the difference is 50K.
> 
> +1
> 
> > I think 1 and 2 are acceptable, but 3 is rather obscure.
> 
> Agreed and 2) would be my preference.

@zuiderkwast / @madolson Do you guys have any preference ?"
1039787610,10293,madolson,2022-02-15T02:33:52Z,"I'm fine with 1 or 2. I have a slight preference towards option 2 because it's the simplest for clients to implement.

@PingXie I did mean cluster nodes. Anyone that didn't like the cluster slots output switched to using `CLUSTER NODES`, so we were trying to provide a better output to get them back."
1042829830,10293,hpatro,2022-02-17T11:03:11Z,"> I haven't looked at the details yet, will do shortly, but I want to point out a potential ambiguity using the term ""shard"".
> 
> It has not been used by Redis Cluster until now, and in other contexts it has been used as either a synonym to ""cluster node"", or (as used in this PR), to describe a master and set of replicas sharing the same slots. To avoid confusion, I think we should avoid that using it here.

@yossigo With the new pubsub feature we decided to use ""Sharded PubSub"" as the terminology to describe a master/set of replica sharing the same slot. Shall we continue using the same definition here ?"
1042875608,10293,yossigo,2022-02-17T11:59:45Z,"@hpatro I think ""sharded"" and ""sharding"" is not a problem, my only concern is around the current ambiguity around ""shard"" and whether it refers to a single cluster node or a set of nodes with the same hash slots (master + replicas)."
1043205510,10293,zuiderkwast,2022-02-17T17:16:02Z,"@yossigo I think the term shard in this sense is very convenient. The result of this command is an array of info per shard. Do you prefer TOPOLOGY or do have another suggestion(s)? Then we can try to compare pros and cons...

Another possible term is ""slice"". The term ""network slicing"" is used in telecom with a similar meaning."
1043246865,10293,PingXie,2022-02-17T17:58:24Z,"The notion of ""shard"" was probably unofficially introduced already based on the [documentation](https://redis.io/topics/pubsub#sharded-pubsub):

> Sharded pubsub helps to scale the usage of pubsub in cluster mode. It restricts the propagation of message to be within the [**shard**](https://redis.io/topics/pubsub#sharded-pubsub) of a cluster.

I feel that it is very natural to go ahead and make it official. Besides, ""shard"" is a well established term in distributed systems and I don't see the use of ""shard"" in this context diverge from the norm. The other (less) popular term that I have seen is ""partition"" but that is quite foreign to Redis, to my best knowledge. I have not seen ""slice"" used in distributed storage/database systems."
1045107342,10293,PingXie,2022-02-18T20:01:27Z,"While we are here, can we also consider having the node's epoch included in the output as well? The client application can bounce between nodes at run time so there are times it'd receive conflicting ""shard"" topology. An embedded epoch value would go a long way to help the client application resolve the collisions. @zuiderkwast this is somewhat related to our discussion on your topology change pubsub proposal #10150."
1046158083,10293,madolson,2022-02-20T04:02:22Z,"Sorry for taking so long to weigh in, but I just also wanted voice that I think it's good to make the term ""shard"" more official. The only downside I can think of is it will become more complex if we ever want to introduce more flexible configurations. For example, consider the scenario if we wanted to allow masters to be replicas for other masters, so that on node failure we have the data replicated but all nodes are able to take writes."
1046164618,10293,madolson,2022-02-20T05:10:23Z,"@PingXie Just to make sure I understand the epoch case. You want to be able to call `CLUSTER SHARDS` on multiple different nodes, and resolve them with the epoch? It seems more like a best practice to keep calling it against one node since eventually all nodes will be consistent. I didn't read through all the conversation on pubsub subscribe slots, I'll try to read through that tomorrow."
1048588368,10293,yossigo,2022-02-23T09:26:21Z,"@madolson @PingXie My point is not that ""shard"" is an incorrect term, but that it's already used in different contexts as a synonym to ""cluster node"" (or `redis-server` process) and I'm concerned about overloading it and creating confusion."
1048597839,10293,zuiderkwast,2022-02-23T09:37:52Z,"@yossigo Where is ""shard"" used as a synonym to ""cluster node"" (or `redis-server` process)? Can't we changes those occurrences and clarify the terminology?"
1048612230,10293,PingXie,2022-02-23T09:56:12Z,"> @PingXie Just to make sure I understand the epoch case. You want to be able to call `CLUSTER SHARDS` on multiple different nodes, and resolve them with the epoch? It seems more like a best practice to keep calling it against one node since eventually all nodes will be consistent. I didn't read through all the conversation on pubsub subscribe slots, I'll try to read through that tomorrow.

@madolson, it is in my opinion that **not** requiring the client application stick to a single node for topology updates improves the client application's resilience to failures on the Redis side, such as when the Redis node is network partitioned from the majority of primaries or when the Redis node is overloaded by data requests. Therefore, I would imagine that an application maintain connections to all Redis nodes in the cluster and retrieve cluster topology from any ""good"" node (neither partitioned nor overloaded). My only concern here is that without knowing the epoch of the node associated with the slot, the app might accidentally end up working with stale routing information and worse still the (stale) owner node might've even left the cluster permanently. That being said, it is true that even with epochs it is not guaranteed that the topology received is always up to date but I'd assert that, by having the ability to accept **more** up to date topology only, we could reduce a lot the likelihood of using stale routing information."
1049133944,10293,PingXie,2022-02-23T19:26:06Z,"@yossigo @madolson @zuiderkwast 

I did a quick search on redis.io. I think now is a good time for a rigorous definition of shards in Redis - before the documentation diverges further.

Ones that I think mean nodes
> Cluster note: this command is especially beneficial for cluster-aware clients. Such clients must identify the names of keys in commands to route requests to the correct [shard](https://redis.io/commands/command#:~:text=Cluster%20note%3A%20this%20command%20is%20especially%20beneficial%20for%20cluster%2Daware%20clients.%20Such%20clients%20must%20identify%20the%20names%20of%20keys%20in%20commands%20to%20route%20requests%20to%20the%20correct%20shard)

>this option can be set to yes to allow reads from a node during the fail state, which is useful for applications that want to prioritize read availability but still want to prevent inconsistent writes. It can also be used for when using Redis Cluster with only one or two [shards](https://redis.io/topics/cluster-tutorial#:~:text=This%20option%20can%20be%20set%20to%20yes%20to%20allow%20reads%20from%20a%20node%20during%20the%20fail%20state%2C%20which%20is%20useful%20for%20applications%20that%20want%20to%20prioritize%20read%20availability%20but%20still%20want%20to%20prevent%20inconsistent%20writes.%20It%20can%20also%20be%20used%20for%20when%20using%20Redis%20Cluster%20with%20only%20one%20or%20two%20shards)

> Such clients must identify the names of keys in commands to route requests to the correct [shard](https://redis.io/commands/command#:~:text=Such%20clients%20must%20identify%20the%20names%20of%20keys%20in%20commands%20to%20route%20requests%20to%20the%20correct%20shard)

Ones that are more in line with my understanding of ""shard""
>The cluster makes sure the published shard messages are forwarded to all nodes in the [shard](https://redis.io/topics/cluster-spec#:~:text=The%20cluster%20makes%20sure%20the%20published%20shard%20messages%20are%20forwarded%20to%20all%20nodes%20in%20the%20shard), 
"
1049559245,10293,PingXie,2022-02-24T07:13:08Z,"Linking PR #10163 per @zuiderkwast's suggestion. 

The idea is to preserve a dead primary's slot range. I was hoping to return that information via ""CLUSTER NODES"" but there is a potential risk of breaking existing clients. I can port over #10163 on top of the new ""CLUSTER SHARDS"" command after this PR is merged."
1051004590,10293,madolson,2022-02-25T16:30:53Z,"> I did a quick search on redis.io. I think now is a good time for a rigorous definition of shards in Redis - before the documentation diverges further.

@PingXie I actually think all 4 of the places we mention shard are consistent with my understanding of the word. A command can be routed to any node in shard that is serving the slot, depending on the client preferences. 

> @madolson, it is in my opinion that not requiring the client application stick to a single node for topology updates improves the client application's resilience to failures on the Redis side, such as when the Redis node is network partitioned from the majority of primaries or when the Redis node is overloaded by data requests. Therefore, I would imagine that an application maintain connections to all Redis nodes in the cluster and retrieve cluster topology from any ""good"" node (neither partitioned nor overloaded). My only concern here is that without knowing the epoch of the node associated with the slot, the app might accidentally end up working with stale routing information and worse still the (stale) owner node might've even left the cluster permanently. That being said, it is true that even with epochs it is not guaranteed that the topology received is always up to date but I'd assert that, by having the ability to accept more up to date topology only, we could reduce a lot the likelihood of using stale routing information.

The main reason I don't want to expose epoch is that it's primarily focused around mastership generation. All other metadata associated with the cluster is not bumped with the epoch, so nodes may disagree and have the same epoch. I'm sure we can document all of this behavior, I just want to provide something that is easy to use for clients."
1051261038,10293,PingXie,2022-02-25T20:56:56Z,"
> @PingXie I actually think all 4 of the places we mention shard are consistent with my understanding of the word. A command can be routed to any node in shard that is serving the slot, depending on the client preferences.

Well this is even better :) it was not a super clear cut to me so I erred on the side of caution.

> The main reason I don't want to expose epoch is that it's primarily focused around mastership generation. All other metadata associated with the cluster is not bumped with the epoch, so nodes may disagree and have the same epoch. I'm sure we can document all of this behavior, I just want to provide something that is easy to use for clients.

I would like to push back on this a bit. The epoch (aka, mastership generation) is an important property of the shard IMO so if we are talking about a command that returns shard information, I think it is a correct and intuitive thing to include the epoch. To clarify, I am thinking about a change like below. The parsing is trivial and the client who doesn't care about the epoch can easily ignore it. Thoughts?

```
127.0.0.1:6379> CLUSTER SHARDS
1) 1) ""slots""
   2) ""10"" // <-- epoch
   3) 1) ""10923""
      2) ""11110""
      3) ""11113""
      4) ""16111"" 
   4) ""nodes""
2) 1) ""slots""
   2) ""11"" // <-- epoch
   3) 1) ""5461""
      2) ""10922""
   4) ""nodes""
3) 1) ""slots""
   2) ""12"" // <-- epoch
   3) 1) ""0""
      2) ""5460""
      3) ""11111""
      4) ""11112""
   4) ""nodes""
```

"
1053943167,10293,madolson,2022-02-28T06:50:17Z,"@PingXie It's worth also mentioning that a nodes perspective on another node may not necessarily align with any ""epoch"", since clients often explicitly override it with `CLUSTER SETSLOT`. Replicas can also disagree on what is the ""epoch""."
1053972930,10293,madolson,2022-02-28T07:39:23Z,"@redis/core-team PTAL. The top comment is updated with the new command which intended to provide a better interface for clients to get topology information. It should be a strict superset of `CLUSTER SLOTS`, and provides similar information to `CLUSTER NODES`."
1053991719,10293,PingXie,2022-02-28T08:07:29Z,"> @PingXie It's worth also mentioning that a nodes perspective on another node may not necessarily align with any ""epoch"", since clients often explicitly override it with `CLUSTER SETSLOT`. Replicas can also disagree on what is the ""epoch"".

Good callout on the consensus-less setslot. I agree this is a case where epoch can be non-deterministic. However, the conflict, which would require an automatic failover while completing slot migration, would be resolved quickly with the winner gaining a higher epoch so next `CLUSTER SHARDS` would quickly catch me up. In my situation, rolling upgrade is a much more common scenario than slot migration. My current solution to reduce the possibility of using stale routing information is to get the epoch from `CLUSTER NODES` and then get the topology from `CLUSTER SLOTS`, which is worse than the proposal here even if they are executed against the same node (which I am not sure we can guarantee), because the epoch can in theory change between the two commands. And then there is the additional parsing logic. Since Redis already outputs the epoch in `CLUSTER NODES`, the exposure of the disagreement is not a new issue to whoever needs this information."
1054103513,10293,zuiderkwast,2022-02-28T10:20:21Z,"Build failed: `E: Failed to fetch http://azure.archive.ubuntu.com/ubuntu/pool/universe/t/tclx8.4/tclx8.4_8.4.1-4_amd64.deb  Could not connect to azure.archive.ubuntu.com:80 (52.250.76.244), connection timed out`

Just restart the job? (someone who has permission to do that)"
1055136879,10293,madolson,2022-03-01T08:08:21Z,"@hpatro To summarize the next steps, we reviewed with the core group and wanted to update the following.
* Deprecate `CLUSTER SLOTS` indicating that this command could be used instead. `CLUSTER NODES` will not be deprecated since it contains extra debugging information.
* Start the documention for this new command, since there are some nuances.
* * Even though there is no way to distinguish shards today, we want to leave it open for that. Replicas should be able to replicate from masters even if they own no shards.
* * Note that both the shard information as well as the node information is extensible."
1056012730,10293,PingXie,2022-03-02T00:39:24Z,"> Replicas should be able to replicate from masters even if they own no shards.

+1.  

> * * Note that both the shard information as well as the node information is extensible.

@madolson what do you think about my justification for having epoch in the shard information? Also even though the new output uses a dict structure for extensibility, there is a slight concern of mine that some client might just look for a partial match of ""fail"" instead of the whole word. Clients like this can misinterpret ""pfail"" as ""fail"" in the future if pfail is introduced at a later time. 
"
1058424589,10293,zuiderkwast,2022-03-03T19:49:47Z,"> * Replicas should be able to replicate from masters even if they own no shards.

@madolson This is an interesting topic. If the last slot is migrated away from a master for the purpose of shutting it down, upgrading it, etc. it doesn't work as expected. It immediately turns itself into a replica of the new slot owner and starts replicating. The assumption in the comments below are wrong. (It is not always a failover causing this.)

```C
    /* If at least one slot was reassigned from a node to another node
     * with a greater configEpoch, it is possible that:
     * 1) We are a master left without slots. This means that we were
     *    failed over and we should turn into a replica of the new
     *    master.
     * 2) We are a slave and our master is left without slots. We need
     *    to replicate to the new slots owner. */
    if (newmaster && curmaster->numslots == 0 &&
            (server.cluster_allow_replica_migration ||
             sender_slots == migrated_our_slots)) {
        serverLog(LL_WARNING,
            ""Configuration change detected. Reconfiguring myself ""
            ""as a replica of %.40s"", sender->name);
        clusterSetMaster(sender);
```

Can we change this without breaking backwards compatibility? This is the source of #9223."
1061709582,10293,barshaul,2022-03-08T12:07:51Z,"Just want to make sure I understood it correctly - client that used to use the 'CLUSTER SLOTS' command to map the cluster slots/nodes, didn't retrieve nodes in 'fail' status with the SLOTS command. Failed nodes are only available through the CLUSTER NODES command (correct?).

So, when moving to the CLUSTER SHARDS command, failed nodes should be ignored when building the cluster mapping, right? 
"
1062131843,10293,PingXie,2022-03-08T19:35:33Z,"> So, when moving to the CLUSTER SHARDS command, failed nodes should be ignored when building the cluster mapping, right?

Yes. That is my understanding too.
"
1062159939,10293,madolson,2022-03-08T20:04:10Z,"@PingXie @barshaul Yes that is correct.

> @madolson This is an interesting topic. If the last slot is migrated away from a master for the purpose of shutting it down, upgrading it, etc. it doesn't work as expected. It immediately turns itself into a replica of the new slot owner and starts replicating. The assumption in the comments below are wrong. (It is not always a failover causing this.)

It doesn't! Most people are confused by this I agree. The conversation we had in the core group was just that ""logically"" the API should support it. I don't think we immediately plan on changing this though, that issue is interesting though, I'm going to read it next. 

> @madolson what do you think about my justification for having epoch in the shard information? Also even though the new output uses a dict structure for extensibility, there is a slight concern of mine that some client might just look for a partial match of ""fail"" instead of the whole word. Clients like this can misinterpret ""pfail"" as ""fail"" in the future if pfail is introduced at a later time.

We touched briefly on epoch, and no one had a strong opinion about it but me. We diverged briefly into a conversation about how we want to make config generations include all of the state information, so I think we settled on adding it in a future version once we have that. This would be independent of epoch though, so we don't want to proactively add it. I don't know we can defend against people matching against something incorrectly, they should be doing exact matches and not partial matches."
1062545901,10293,PingXie,2022-03-09T04:35:48Z,">We touched briefly on epoch, and no one had a strong opinion about it but me. We diverged briefly into a conversation about how we want to make config generations include all of the state information, so I think we settled on adding it in a future version once we have that. This would be independent of epoch though, so we don't want to proactively add it.

@madolson trying to get a better understanding of ""make config generations include all of the state information"". Are you talking about adding the remaining states like `ping-sent`, `pong-recv`, `link-state`,etc along with `config-epoch` in an all or none way? Or this is something else?

>I don't know we can defend against people matching against something incorrectly, they should be doing exact matches and not partial matches.

Sure. I don't have a strong opinion here and I haven't seen updates to #10195 either. Agreed we can revisit this later when there is a real use case.

> So, when moving to the CLUSTER SHARDS command, failed nodes should be ignored when building the cluster mapping, right?

Forgot to mention that I think we'd also need my changes (#10163) to ensure that failed nodes are included after a replica takes over the primary-ship in the shard. I can port it over after this PR is merged. "
1066273859,10293,madolson,2022-03-14T02:26:00Z,">@madolson trying to get a better understanding of ""make config generations include all of the state information"". Are you talking about adding the remaining states like ping-sent, pong-recv, link-state,etc along with config-epoch in an all or none way? Or this is something else?

@PingXie Include other metadata such as which replicas are serving data from which masters, updates for announced hostnames and IP addresses, any other future config state changes. "
1066314262,10293,madolson,2022-03-14T03:58:49Z,Added documentation: https://github.com/redis/redis-doc/pull/1815
1067351151,10293,PingXie,2022-03-14T22:09:34Z,">We touched briefly on epoch, and no one had a strong opinion about it but me. We diverged briefly into a conversation about how we want to make config generations include all of the state information, so I think we settled on adding it in a future version once we have that. This would be independent of epoch though, so we don't want to proactively add it.
> 
> @PingXie Include other metadata such as which replicas are serving data from which masters, updates for announced hostnames and IP addresses, any other future config state changes.

Thanks @madolson. I would like to clarify that there is a legit and real scenario for having epochs in `CLUSTER SHARD` as it further reduces the likelihood of getting stale topology. This information is currently available via `CLUSTER NODES`, whose replacement (#10168) is also one of the reasons why the new `CLUSTER SHARD` command is introduced. With `epoch`, applications do not need to invoke both `CLUSTER SHARD` and `CLUSTER NODES` any more. Happy to add the epoch along with the change to preserve failed nodes next.
"
2112598554,13243,moticless,2024-05-15T13:49:54Z,"@ronen-kalish , if it is not too much effort at this point in time, please consider to `rdbSave()` / `rdbLoad()` of  `RDB_TYPE_HASH_METADATA` tuples of type [ttl][field][value], instead of [field][value][ttl]. 

We will be able to optimize later on the allocation of `fields` such that from first place will be allocated with TTL support."
2115233061,13243,ronen-kalish,2024-05-16T13:21:52Z,"Full daily run: https://github.com/ronen-kalish/redis/actions/runs/9113052419
"
2115507512,13243,tezc,2024-05-16T15:07:46Z,"LGTM! 

If there are flaky tests in the daily, we may fix it later on the way (or you may comment out for now, or merge later with another PR after resolving the issue etc). It is up to you. "
2115519294,13243,ronen-kalish,2024-05-16T15:12:47Z,"@tezc, I'm not sure the flaky tests in the daily are related to this PR, unless we're sure they weren't flaky before. Is this indeed the case?"
2115536598,13243,tezc,2024-05-16T15:20:38Z,"hmm, I'm not sure. 

@sundb I see there is one hfe defrag and a corrupt/dump fuzzer failure. Maybe you can comment on that? Could you take a quick look? https://github.com/redis/redis/pull/13243#issuecomment-2115233061"
2115556235,13243,ronen-kalish,2024-05-16T15:30:05Z,"@tezc , @sundb , if anyone can approve this PR once done reviewing, It'll make @YaacovHazan very happy..."
2132049661,13243,sundb,2024-05-26T03:35:57Z,we need a document PR for this in [redis/redis-doc](https://github.com/redis/redis-doc).
2132124066,13243,ronen-kalish,2024-05-26T08:10:14Z,"> we need a document PR for this in [redis/redis-doc](https://github.com/redis/redis-doc).

Hi @sundb , can you please point to where RDB format is documented? Or is there another aspect that need to be documented? Thanks!"
2132169954,13243,sundb,2024-05-26T10:34:45Z,"@ronen-kalish  the RDB format doesn't need to be documented, just add document for `rdb_last_load_hash_fields_expired`."
2139020488,13243,ronen-kalish,2024-05-30T08:32:02Z,"@sundb, now that @moticless has removed hash field expiration on load, this specific field is gone and there is no need for documentation."
2139034095,13243,sundb,2024-05-30T08:36:01Z,"@ronen-kalish yes, i can't remove the needs-doc-pr tag.
i created a doc PR here: https://github.com/redis/redis-doc/pull/2718.
"
923904384,9530,zuiderkwast,2021-09-21T11:48:10Z,"Nice! If you ever get hit by a bus, it better be a cluster bus. :-)

I haven't looked at the code yet.

I think SNI verification between nodes might be useful, just as it is useful between client and cluster, for deploying a system in an untrusted network. We use mutual authentication instead though.

`cluster-prefer-hostnames` sounds good to me. If redirects use hostnames, that can already break clients, so if that's enabled, we can as well enable it for the first arg in CLUSTER SLOTS. But even better may be to let the client announce its capabilities (e.g. `HELLO 3 hostnames`).

If we ever want to add more fields to CLUSTER SLOTS, perhaps consider making the last argument a map. It may be secondary IP addresses (IPv6 and IPv4). A hostname can be resolved to multiple IP addresses though, if DNS is used, so it might not be needed for that use case."
925833600,9530,yossigo,2021-09-23T13:48:57Z,"@madolson great to see this making progress!

I didn't look at the implementation yet, but I suppose that any approach we take to support cluster bus upgrades should be flexible enough to support additional upgrades in the future. I'm sure we'll need that when we proceed with the ClusterV2 plans.

I support the `cluster-prefer-hostnames` all-or-nothing approach, so if we use hostnames we use them for everything. There will definitely be some client breakage but I think it's an opportunity to refresh them, and also migrate all to `CLUSTER SLOTS` while doing so.

Agree about not using DNS names for intra-node connectivity, and I think SNI validation is also not really that important there (adding other basic cert validation configuration is easier and just as good IMHO)."
925863787,9530,dmitrypol,2021-09-23T14:23:04Z,it would be nice if we also could use hostnames in create cluster process.  Right now you need to use IPs for that.  
930806557,9530,madolson,2021-09-30T05:53:52Z,"@yossigo 
> You mention this is semi-breaking change, but IIUC if one enables hostnames and delivers extensions to old nodes they'll just be ignored resulting with inconsistent behavior but no other breakage - right?

Yeah, I said it's breaking but it's really not as long as you're being deliberate. The danger here is that the extension is grouped with the ping/pong messages themselves, so that failure to parse the extensions means that the entire ping will also be rejected. 

>I think there's something a bit confusing about the way extensions are implemented. On one hand it's a generic mechanism with a packet-level flag and extensions count. On the other hand, extensions specifically extend the gossip section. Maybe we should consider going all the way to a more generic extensions mechanism?

Is there something specific you have in mind here that is useful? It is an extension, but it's meant to jump on the existing ping/pong structure that already exists to spread data around. The module interface is already extensible in that you can add new messages if you want. (We could implement hostnames that way as well) There is also no strong reason this mechanism couldn't be generalized to add arbitrary additional data to any of the other existing messages.

I'll also mention that I think long term this type of gossip isn't very efficient, and we probably want to figure out a better way to distribute this information in the cluster for cluster V2. 

>The argument for extensions vs. new commands is atomicity of updates, but IIUC that's not the case when a node joins - it will initially receive information about other nodes without hostnames, and only later have hostnames propagated to it directly from other nodes.

This is mostly right, but we do have atomicity because gossip data isn't that comprehensive. The gossiped information (IP, node name, flags, health information) is just enough so that nodes learning about a new node can reach out and ping it, it's not enough to know detailed information about the node. Specifically slots are missing, which disqualifies it from showing up in `CLUSTER SLOTS`. Once it has exchanged a single ping/pong message, it will then know all the information it needs to display it in cluster slots, which is where we can inject the new hostname. 

This is why I made a very specific point about `CLUSTER NODES` as well as SNI for intra-node communication. Cluster nodes requires very deliberate parsing to understand the state, which most clients don't do very well, but the node will show up immediately without the hostname. We also can't do SNI for intra-node based on the current implementation, since we reach out to the node *before* knowing it's hostname. There is no hard blocker for gossiping the hostname, just seems like extra data."
932833342,9530,dmitrypol,2021-10-02T23:25:39Z,@madolson - also any thoughts on that idea you and I discussed to create `cluster health` command so that users would not have to parse `cluster nodes` looking for `fail`?  
932860903,9530,madolson,2021-10-03T04:30:20Z,"@dmitrypol It's in one of the checkboxes ;) 

> There was a follow up ask to make a version of CLUSTER NODES that is more human readable, like CLUSTER HEALTH or CLUSTER STATUS. It should be able to show the hostnames, but not necessarily be used by clients.""

My thought was to decouple your ask from this specific PR. This is mostly code complete to my satisfaction for the core. (Also, I'll be out for a couple of weeks, so won't respond quickly)"
932970159,9530,dmitrypol,2021-10-03T15:08:21Z,"> @dmitrypol It's in one of the checkboxes ;)
> 
> > There was a follow up ask to make a version of CLUSTER NODES that is more human readable, like CLUSTER HEALTH or CLUSTER STATUS. It should be able to show the hostnames, but not necessarily be used by clients.""
> 
> My thought was to decouple your ask from this specific PR. This is mostly code complete to my satisfaction for the core. (Also, I'll be out for a couple of weeks, so won't respond quickly)

My mistake, did not notice"
936231921,9530,yossigo,2021-10-06T13:26:02Z,"@madolson

The only issue I had with the extensions is that it's a bit weird to have the flag and count at the `clusterMsg`, but still have to deal with extensions per `clusterMsgData`, but I suppose that's really the easiest way to maintain backwards compatible ping payloads. And I agree we'll probably want to move away from the gossip as it works right now anyway."
950985842,9530,yossigo,2021-10-25T14:26:46Z,"@madolson
Something that is related to this work and came up in a recent discussion: A primary use case for hostnames is to deal with network topologies where the cluster does not have good visibility into what addresses are exposed to clients, but assumes that a hostname will resolve to the right address on the client side.

If we stretch scenario further - the hostname itself may also not be known, or be dynamic and different for different clients. In that case, it could be useful to return something like `-MOVED ::<port>` (just an example) and expect a well behaved client to reuse the same address/hostname but just a different port.

There is an inherent assumption here that the client only uses ports to distinguish between cluster nodes, and that the hostname/address is identical - but I believe that is becoming the case with some network topologies that involve a service mesh proxy / load balancer / gateway / etc.

There's practically no work on the server side for this, it's only about setting a convention and communicating it to clients as part of the hostname support change. Any thoughts about this?"
951100898,9530,dmitrypol,2021-10-25T16:32:34Z,"@yossigo - you are absolutely correct, hostname can be different per node.  Cluster can be composed of server1.domain.com:6379, server2.domain.com:6379 and server3.domain.com:6379.  
"
951375187,9530,madolson,2021-10-25T22:16:04Z,"@yossigo That is a good insight. An alternative to what you proposed is we could add a client config so that a client can tell the cluster the hostname/IP that is should always respond with. I think that would require a bit less client changes, as they would more focus on sending an additional command on startup as opposed to changing how interpreting the Cluster Slots/redirects function. 

@dmitrypol Not sure I followed your comment, is having the server side not know the hostname a better solution for what we talked about?

I'm going to rebase and address my changes today in either case. We should be able to quickly add the changes outlined. Once this has general buy in, I'll close off on the tooling improvement."
951767733,9530,yossigo,2021-10-26T09:45:51Z,"@dmitrypol The example you provide is already part of this work, I was actually referring to something else. For example assume there are clients A and B behind different load balancers, both pointing to the same Redis Cluster. The clients may use different, locally known and locally resolved hostnames to reach those load balancers, but the cluster does not know where to redirect each client.

```
+-------------+              +-----------+          +----------------+               
|             | hostnameA    |           |--------->|                |               
|  Client A   |------------->|   LB A    |          |                |               
|             |              |           |          |                |               
+-------------+              +-----------+          |      Redis     |               
                                                    |     Cluster    |               
+-------------+              +-----------+          |                |               
|             | hostnameB    |           |          |                |               
|  Client B   |------------->|   LB B    |          |                |               
|             |              |           |--------->|                |               
+-------------+              +-----------+          +----------------+               
```

@madolson This is a good point, it involves less parsing changes. On the other hand, we're anyway introducing parsing changes, not just due to hostnames but potentially also pushing clients to finally move from `CLUSTER NODES` to `CLUSTER SLOTS` so we could try to get this all done together. I don't feel strongly either way though."
952318282,9530,dmitrypol,2021-10-26T20:55:37Z,thank you for clarifying @yossigo.  I misunderstood.  
965691807,9530,yossigo,2021-11-10T19:54:02Z,"@madolson Regarding the unknown hostname issue, I'm OK with both options and we should probably try to get some client author feedback to assess what makes more sense. I think the important bit is to bundle this along with hostname support and maybe a general migration from `CLUSTER NODES` to `CLUSTER SLOTS` because we don't get too many chances to realign clients."
976099179,9530,madolson,2021-11-23T02:00:22Z,"@yossigo Finally got time to update this, also pinged the client channel."
977795087,9530,yossigo,2021-11-24T11:37:36Z,"@madolson Great, so we need to agree on the open issues above:

> Do we support cluster nodes natively? Right now I am adding the hostname into the cluster nodes file, so that it's loaded on restart, but not considering making it terribly easy to parse. However, I know some clients try to use that to discover the topology, but I don't want to try to do anything special for them with regards to hostname support. Current bias is towards no.

I also tend to say no, move clients away from it to `CLUSTER SLOTS`.

> Is hostname really the right word? I might be nitpicking here but a ""hostname"" is usually just the left-most part of a FQDN, which comprises a domain and hostname. Most of the requests for this have been for ""hostname"" support though, so maybe the wording is fine.

I'm OK with hostname.

> Should the extension message be able to apply to any type of message? It definitely can, not sure that needs to be solved here unless there is a real use case.

I'm good with this implementation.

The last one is port based topology, how to handle it in a way that's easy for clients and whether or not it should be part of this PR. The benefit of pushing it together is we'll have a single PR to relate to when discussing Redis 7 Cluster Protocol changes."
986483911,9530,madolson,2021-12-06T06:42:23Z,"@yossigo While talking with the client folks, one of them mentioned that they didn't see the value in a custom protocol since clients could just do that anyways. They could add an argument to their clients that indicates they are talking to a NAT gateway, and just assume all redirects will go to the same port today. So I suppose the question is do we still think we need a client side change to support this functionality, or do we let clients implement it themselves?

@redis/core-team I suppose your approval is needed here now. Please refer to the top comment. NOTE: I will update the name of the test so it's in the right order later, there are several inflight cluster mode PRs and will just wait until this is ready to merge. "
986503134,9530,oranagra,2021-12-06T07:18:46Z,"I don't know enough about cluster to really approve or reject it, so LGTM (didn't review the code).
One question i have about upgrades, is whatever we can somehow prevent this from doing damage in case one of the nodes still doesn't support the new feature? (either block the config, or just avoid sending messages that would break something)."
991913376,9530,yossigo,2021-12-12T15:00:08Z,"@madolson I agree that technically clients could do that, but that assumes they're well aware of their network topology and configuration. I think in most cases we'd actually try to achieve the opposite - minimize the amount of configuration we need to feed the app down to some connection string."
992023275,9530,madolson,2021-12-13T01:15:31Z,"@yossigo Ack, let's close on this during on sync meeting, it's probably faster than gathering feedback here."
995479178,9530,madolson,2021-12-16T06:29:32Z,"@yossigo So, for the ""unknown hostname support"", I don't think we can necessarily use nil since for the default case for cluster slot will probably put (nil) as the new last argument. I'm going to propose we use ""?"" to indicate that there is no designated host for a node. So redirects will loo like `+MOVED 10000 ?:1234` and we can populate cluster slots with ""?"". Does that seem reasonable? If so, I'll create an issue documenting that and hopefully we can merge this PR."
996043565,9530,yossigo,2021-12-16T17:49:57Z,"@madolson What I had in mind is `-MOVED 111 :1234` for non-hostname redirects, and a cluster slots that contains nil (or an empty string) for *both* IP address and hostname in that case."
996170568,9530,madolson,2021-12-16T20:27:46Z,"@yossigo I'm not the biggest fan of the null/empty string, but I also don't feel that strongly about it. I think in the end this isn't a significant distinction, so I'll go create a doc PR for this."
997334314,9530,madolson,2021-12-19T05:53:02Z,@yossigo PTAL at the attached documentation and verify we aren't missing anything else.
997463167,9530,zuiderkwast,2021-12-19T21:20:43Z,"FWIW, in relative URIs and paths, omitted parts (URI scheme, host or path) are meant to be the same as the base (current or previous) URI or path. In this spirit, an omitted host as in `:1234` could be interpreted as the current host with a different port. If `?` is returned as the host, it doesn't give this idea to the user. (I didn't follow the discussion so I don't know what applies in this case.)"
998044596,9530,yossigo,2021-12-20T15:48:46Z,"@madolson I'm not crazy about that either, but I felt it's better than any other arbitrary convention (and thanks to @zuiderkwast I now know why).

Regarding the documentation, I'm not 100% sure it's clear what is the correct client behavior:
* Do we say a client must always use the secondary endpoint if listed, or is it discretionary?
* In case resolving fails, are clients allowed to fall back to the primary one?

I tend to think about it in terms of ""if the secondary is listed, use it and only it"". But then perhaps the term ""secondary endpoint"" is misleading?"
998306865,9530,madolson,2021-12-20T22:09:18Z,"@yossigo Definitely sounds like the syntax is ambiguous and not clear. How about this instead:
```
1) Preferred endpoint (Either IP, hostname, or NULL). This should be used by clients. 
2) port
3) Node ID
4) Always the IP if available. Intended to be informational, but could be optionally looked for.
5) Always the hostname if available. Intended to be informational, but could be optionally looked for.
```

I originally didn't like this since it's the most verbose, but it's definitely the clearest.

The reason I want to provide both, is we have a specific user at AWS that wants both IP and hostname. They like to directly connect to the IP but do SNI on the hostname. A note about AWS is that for TLS clusters we always show hostnames, and for TLS disabled we always show IP addresses."
998586854,9530,zuiderkwast,2021-12-21T08:52:03Z,What about multiple IP addresses (IPv4 and IPv6) and multiple host names?
998973255,9530,madolson,2021-12-21T17:43:06Z,"@zuiderkwast I'm not sure what to do about multiple-hostnames, that seems like it would be very difficult for clients to support. I suppose they could try them all, so I would be inclined to not try to support them.

IPv4 vs IPv6, do we want to support dual stack where you can talk to redis with either IPv4 or IPv6? It sounds like you could change the the announce IP (I think that supports IPV6, but I didn't check) to control which one you expose, but make either available."
998975286,9530,madolson,2021-12-21T17:46:17Z,"@zuiderkwast Another thought is we could do the same thing that you did for TLS. If you are requesting the command over the IPv6 address, we could return the known IPv6 address. "
998992282,9530,zuiderkwast,2021-12-21T18:14:08Z,"@madolson Maybe multiple hostnames are useless for Redis. Let's ignore it. :D

Regarding dual stack, your idea of serving IPv6 to IPv6 clients and IPv4 to IPv4 clients sounds like a good idea. I won't be surprised if we'll get requests sooner or later. Here is something related: #6358.

Multiple IP addresses could also be used for multi-homing, i.e. multiple network cards installed on the same Redis machine with redundant cables to different ISPs. (Something similar, SCTP with multi-homing, is used for high availability in telecom, between radio base stations and core network.)

Also related: Using DNS, a hostname can already be resolved to multiple IPv4 and IPv6 addresses."
998996964,9530,zuiderkwast,2021-12-21T18:21:33Z,"If we ever want to add more endpoints, I suppose they can be added after the other ones in the CLUSTER SLOTS array, unless we want to add something else there. We can simply keep that in mind when designing the protocol, that's all."
999604232,9530,yossigo,2021-12-22T14:08:58Z,"@madolson @zuiderkwast So, to summarize the last few comments:

* Multiple hostnames are not required.
* Multiple IPs are nice to have. In most cases they'd be covered by a single hostname or auto-sensing of IPv4/IPv6 but in other cases may still be useful.

I also don't like the verbosity of repeating a very-long-hostname-in-some-region twice. How about:

```
1) Preferred endpoint (Either IP, hostname, or NULL). This should be used by clients. 
2) port
3) Node ID
4..n) Additional Info
    1) Info-Type (e.g. `fqdn`, `addr`, etc.)
    2) Info-Data
```

And we can avoid repeating ourselves so if our preferred endpoint is an IP, we can choose to only include the hostname in the additional info, etc.

We could just drop those additional info fields arbitrarily, but that would be counting too much on the client to parse and make sense of them, and won't be very extensible in the future.

This makes `CLUSTER SLOTS` a bit more complex, but I think we're headed in that direction anyway if we want to optimize it for non-contiguous hash slot mapping to make it a viable alternative to `CLUSTER NODES` in those cases."
999688466,9530,madolson,2021-12-22T16:02:38Z,"@yossigo Sounds good to me. I'm also going to change that one config ""cluster-prefer-hostnames"" to ""cluster-preferred-endpoint-type"" and make it an enum, and that will control what shows up in the preferred field."
1002982006,9530,yossigo,2021-12-30T11:09:25Z,@madolson I just realized there was another extension topic to this - getting `CLUSTER SLOTS` to efficiently handle hash slot mapping that is not contiguous. Did you have any thoughts about that?
1003812385,9530,madolson,2022-01-03T00:57:01Z,"@yossigo My plan was to introduce a new command, something like `CLUSTER SHARDS` (alternatives are `CLUSTER TOPOLOGY` and `CLUSTER HEALTH`), which produces the same result as this command but organizes the nodes into shards and documents the slots associated with each shard. There was an ask to include additional information as well in the shards output. "
1003829466,9530,madolson,2022-01-03T01:46:26Z,"Doc PR: https://github.com/redis/redis-doc/pull/1715
Full test run: https://github.com/redis/redis/runs/4687132708"
1005297504,9530,liuchong,2022-01-05T01:11:28Z,"great! 🎉 
What is the release 7.0 schedule？:eyes:"
1008262060,9530,oranagra,2022-01-09T09:29:20Z,"@madolson please have a look
https://github.com/redis/redis/runs/4733590577?check_suite_focus=true
https://github.com/redis/redis/runs/4733591140?check_suite_focus=true
```
00:46:34> Verify the nodes configured with prefer hostname only show hostname for new nodes: FAILED: Expected 'e4f15fdae45cd214bbed9f01e6cf30354baf8917 127.0.0.1:30002@40002,shard-3.com master - 0 1641516394423 3 connected 10922-16383
```

"
1008263216,9530,oranagra,2022-01-09T09:38:14Z,"another one:
https://github.com/redis/redis/runs/4745050597?check_suite_focus=true"
1008439148,9530,madolson,2022-01-09T22:48:12Z,"Ack, will take a look."
1008461278,9530,madolson,2022-01-10T00:45:36Z,@oranagra Fix submitted: https://github.com/redis/redis/pull/10086
1068707524,9530,liuchong,2022-03-16T04:02:35Z,"This is on 7.0-rc2

```
root@redis-node-1:/data# redis-cli --cluster create redis-node-1:7001 redis-node-2:7002 redis-node-3:7003 redis-node-4:7004 redis-node-5:7005 redis-node-6:7006 --cluster-replicas 1
>>> Performing hash slots allocation on 6 nodes...
Master[0] -> Slots 0 - 5460
Master[1] -> Slots 5461 - 10922
Master[2] -> Slots 10923 - 16383
Adding replica redis-node-5:7005 to redis-node-1:7001
Adding replica redis-node-6:7006 to redis-node-2:7002
Adding replica redis-node-4:7004 to redis-node-3:7003
M: 8d0e1b09f6e6812c8e99e1eed82653d4d38a43e4 redis-node-1:7001
   slots:[0-5460] (5461 slots) master
M: fbc7c451d2681a24deefa06f78e4929b90634bf9 redis-node-2:7002
   slots:[5461-10922] (5462 slots) master
M: b4feae38993df2e1073cfc43d0e6f9ba5a014833 redis-node-3:7003
   slots:[10923-16383] (5461 slots) master
S: 4c3a58c2cd0217967e82f04ee3df187c78f5a84f redis-node-4:7004
   replicates b4feae38993df2e1073cfc43d0e6f9ba5a014833
S: 3a4ad7c47a3f07146581f1ad38fab7e648305865 redis-node-5:7005
   replicates 8d0e1b09f6e6812c8e99e1eed82653d4d38a43e4
S: 30379a24dd87f9b4016d3a25398a02150c1a6977 redis-node-6:7006
   replicates fbc7c451d2681a24deefa06f78e4929b90634bf9
Can I set the above configuration? (type 'yes' to accept): yes
>>> Nodes configuration updated
>>> Assign a different config epoch to each node
>>> Sending CLUSTER MEET messages to join the cluster
Node redis-node-2:7002 replied with error:
ERR Invalid node address specified: redis-node-1:7001
```

mention my self @liuchong for issue filter 👀 "
1068832634,9530,zuiderkwast,2022-03-16T07:52:13Z,@liuchong It seems as `CLUSTER MEET` does not accept a hostname. I guess we need to implement that.
1079782816,9530,FarhanSajid1,2022-03-26T22:00:40Z,"> This is on 7.0-rc2
> 
> ```
> root@redis-node-1:/data# redis-cli --cluster create redis-node-1:7001 redis-node-2:7002 redis-node-3:7003 redis-node-4:7004 redis-node-5:7005 redis-node-6:7006 --cluster-replicas 1
> >>> Performing hash slots allocation on 6 nodes...
> Master[0] -> Slots 0 - 5460
> Master[1] -> Slots 5461 - 10922
> Master[2] -> Slots 10923 - 16383
> Adding replica redis-node-5:7005 to redis-node-1:7001
> Adding replica redis-node-6:7006 to redis-node-2:7002
> Adding replica redis-node-4:7004 to redis-node-3:7003
> M: 8d0e1b09f6e6812c8e99e1eed82653d4d38a43e4 redis-node-1:7001
>    slots:[0-5460] (5461 slots) master
> M: fbc7c451d2681a24deefa06f78e4929b90634bf9 redis-node-2:7002
>    slots:[5461-10922] (5462 slots) master
> M: b4feae38993df2e1073cfc43d0e6f9ba5a014833 redis-node-3:7003
>    slots:[10923-16383] (5461 slots) master
> S: 4c3a58c2cd0217967e82f04ee3df187c78f5a84f redis-node-4:7004
>    replicates b4feae38993df2e1073cfc43d0e6f9ba5a014833
> S: 3a4ad7c47a3f07146581f1ad38fab7e648305865 redis-node-5:7005
>    replicates 8d0e1b09f6e6812c8e99e1eed82653d4d38a43e4
> S: 30379a24dd87f9b4016d3a25398a02150c1a6977 redis-node-6:7006
>    replicates fbc7c451d2681a24deefa06f78e4929b90634bf9
> Can I set the above configuration? (type 'yes' to accept): yes
> >>> Nodes configuration updated
> >>> Assign a different config epoch to each node
> >>> Sending CLUSTER MEET messages to join the cluster
> Node redis-node-2:7002 replied with error:
> ERR Invalid node address specified: redis-node-1:7001
> ```
> 
> mention my self @liuchong for issue filter 👀

Also seeing this"
1079876073,9530,oranagra,2022-03-27T07:56:16Z,"@zuiderkwast this is resolved by #10436, right?"
1080310775,9530,zuiderkwast,2022-03-28T07:47:44Z,@oranagra That's right. I created the issue #10433 to track it too.
1236261650,9530,oranagra,2022-09-04T05:13:10Z,"seen a failure in a test introduced here. i assume timing issue.
https://github.com/redis/redis-extra-ci/runs/8173232234?check_suite_focus=true
```
*** [err]: Verify the nodes configured with prefer hostname only show hostname for new nodes in tests/unit/cluster/hostnames.tcl
Expected '' to be equal to 'shard-2.com' (context: type eval line 39 cmd {assert_equal [lindex [get_slot_field $slot_result 0 2 3] 1] ""shard-2.com""} proc ::test)
```"
1704781331,12453,meiravgri,2023-09-04T07:50:12Z,"@oranagra 
> is the PR still a ""draft"" or can be marked as ready? 

Ready for review :)

>  did you run it against a full daily CI (all platforms)? 

yes

> please make sure the top comment is up to date (justification, design). please make sure to mention other changes we did (like SIGALRM, etc). 

Already done

> please add an example of how a crash log looks.

Will do

"
1763336576,12453,meiravgri,2023-10-15T09:41:39Z,"@yossigo @tezc @oranagra 
Regarding the async-signal-safe functions:
We were using unsafe functions before this PR.
`pthread_mutex_lock`, `serverLog` uses `vsnprintf` and the linux manual doesn't explictly mention 'backtrace()' 
though as for the latest, I found a StackOverflow thread that claims that it is safe to call it if `libgcc` is already loaded. I assume they rely on this note from [backtrace(3) — Linux manual page](https://man7.org/linux/man-pages/man3/backtrace.3.html)

> backtrace() and backtrace_symbols_fd() don't call malloc()
          explicitly, but they are part of libgcc, which gets loaded
          dynamically when first used.  Dynamic loading usually triggers
          a call to [malloc(3)](https://man7.org/linux/man-pages/man3/malloc.3.html).  If you need certain calls to these two
          functions to not allocate memory (in signal handlers, for
          example), you need to make sure libgcc is loaded beforehand.

* I can avoid using malloc for the **`threads_mngr` output arrays** (one array of output pointers, and one allocated by each thread for its backtrace). Allocating the backtraces array on the stack and assigning the address to a global variable will do the work. ✅ 
* The **semaphore** can be replaced with a busy wait. ✅ 
* Regarding `tids` list allocation - since we know the number of threads only at runtime, we can either:
** decide on a maximum hard-coded number of threads
** write the tids to a temporary file (write() and read() and safe) - i actually prefer this option.
* **opendir, readdir and closedir** : There is no other option to get the process's threads and the threads' mask during runtime. for this issue i have 2 suggestions:
** Introduce `register_thread` API: allows to register the thread to a global array upon creation. A`redisModule` function should be part of this API. should be taken into consideration: the **memory** to maintain this global structure and lots of **changes** in the existing code (replace each pthread_create). It is Important to mention that this option allows the threads_mngr to be supported on additional platforms, as we can use POSIX pthread API instead of linux syscalls. But, as i said, more complicated.
** Or - well, I can't see we call any of these functions in redis. Assuming they are unsafe since they use static variables, I think we can take the risk."
1763369563,12453,oranagra,2023-10-15T12:08:56Z,"* The pthread_mutex_lock calls are new, one is your recent work, and the other one is from #7700, i suppose we must find another way.
* As for serverLog, we have serverLogFromHandler that's intended for this use case, we need to convert the overlooked ones to use it.
* i suppose we should regard backtrace as ok, unless we find somewhere that explicitly says otherwise.
* as you said, the backgrace allocation can be moved the the calling thread's stack
* replacing the semaphore with e sleep-loop wait sounds ok to me.
* regarding the tids list, limiting a maximum number of threads seems reasonable 
* i've seen some advise in SO to call opendir in advance, and keep the DIR pointer for later, if we can't use that, then i suppose your thread registration API would be ok (although unoptimal). i wouldn't want to draw any guess work conclusions, or rely on us not calling these in other places in redis."
1763708858,12453,tezc,2023-10-16T04:32:02Z,"`register_thread()` API will also need `deregister_thread()` as well right? Modules can start/kill threads in between.
One caveat is if module uses a library and the library starts a thread, then you are going to miss it.


Regarding making existing impl safer, maybe we can replace these functions with signal safe versions: 
 - fopen(), fgets(), fclose() --------> open(), read(), close()
 - prctl()   --------> open(), read(), close() for  `/proc/self/task/<tid>/comm`
- snprintf(), atoi(), strtoul()  --------> maybe we can implement these in place or write a small version of these functions. If I'm not mistaken, we just use these for positive integers, probably we can get away with a few lines of code. 
 - opendir(), readdir(), closedir() ----> syscall(SYS_getdents):  I guess we can assume this syscall is async signal safe. Please, see this discussion and the code that shows how it parses: [link](https://lists.gnupg.org/pipermail/gnupg-devel/2017-September/033086.html)
  - I assume you are going to fix those explicit malloc calls and semaphore usage as well.
  
If we do these changes, we will be doing three things we are not supposed to do: `syscall(SYS_tgkill)` , `syscall(SYS_gettid)` and `syscall(SYS_getdents)`.  Probably, it is okay to assume these are safe to call in the signal handler. 
      "
1763964689,12453,meiravgri,2023-10-16T08:18:29Z,"https://github.com/redis/redis/pull/12655
This PR fixes the heap allocations.

Regarding the API, yes of course it should include a remove function. Let's see if we can fix the current implementation without introducing a new API.

* Saving the DIR pointer won't solve `readdir`. @tezc suggestion LGTM.
* atoi(), strtoul() are already implemented in src/util.c. 
* Instead of using snprintf() we can directly write to the log file. I suggest dividing serverLogFromHandler into 3 functions: One function `StartserverLogFromHandler` for preparations and writing the title (`<pid>:signal-handler (<time>)`) and `AddMessageServerLogFromHandler`to add the message. `EndServerLogFromHandler` to print the new line and close the file
sequential calls to MessageServerLogFromHandler can replace `snprintf` in most if not all cases.
```
void serverLogFromHandler(int level, const char *msg) {
    fd = StartserverLogFromHandler(level);
    err = MessageServerLogFromHandler(fd, msg);
    EndServerLogFromHandler(fd, err);
}
```

Additional callback that are unsafe:
logConfigDebugInfo uses malloc for the sds string, and serverLogRaw.
doFastMemoryTest:
* killThreads -> calls pthread_join(), pthread_cancel(), serverLog
* memtest_test_linux_anonymous_maps() -> fopen, fgets, strtoul, snprintf


"
1764234933,12453,oranagra,2023-10-16T11:06:12Z,"LGTM.
let's also change logConfigDebugInfo to print directly instead of using sds (add a comment as to why)
regarding doFastMemoryTest, let's ignore that crap."
1784060599,12453,meiravgri,2023-10-29T10:33:28Z,"@yossigo @oranagra @tezc 
Reagrding **`sprintf` and `serverLog`.** 
As I mentioned `serverLog` uses unsafe functions, among them sprintf.
Replacing `serverLog` with `serverLogFromHandler`, and any sprintf with sequential writes raise several issues:
1. The code is very long and messy. For example, this is how printing 
`62220:M 26 Oct 2023 14:39:04.526 # Redis 255.255.255 crashed by signal: 11, si_code: 2`
looks like
```
    /* ""Redis <REDIS_VERSION> crashed by signal: <sig>, si_code: <info->si_code>"", REDIS_VERSION */
    int fd = serverLogFromHandler_Start(LL_WARNING);
    while (1) {
        if (-1 == serverLogFromHandler_WriteMsg(fd, ""Redis "")) break;
        if (-1 == serverLogFromHandler_WriteMsg(fd, REDIS_VERSION)) break;
        if (-1 == serverLogFromHandler_WriteMsg(fd,""  crashed by signal: "")) break;
        ll2string(buf,sizeof(buf),sig);
        if (-1 == serverLogFromHandler_WriteMsg(fd, buf)) break;
        if (-1 == serverLogFromHandler_WriteMsg(fd,"", si_code: "")) break;
        ll2string(buf,sizeof(buf),info->si_code);
        if (-1 == serverLogFromHandler_WriteMsg(fd,buf)) break;
        serverLogFromHandler_End(fd); break;
    }
```
or path generation:
```
    char path_buff[PATH_MAX];
    memcpy(path_buff, proc_pid_task_path, PATH_MAX);
    redis_strlcat(path_buff, ""/"", PATH_MAX);
    redis_strlcat(path_buff, tid, PATH_MAX);
    redis_strlcat(path_buff, ""/status"", PATH_MAX);
```

2. `serverLog` begins with a different header than `serverLogFromHandler`:
`62220:M 26 Oct 2023 14:39:04.526 # `vs. `62220:signal-handler (1698331136)`

After discussing with Oran, I found out that we've been using `serverLog` in the signal handler ever since Redis exists and we should reconsider if changing it now is necessary. 

Another thing that I've noticed is that `serverLog` calls **file streaming functions** (fopen etc) which we also tried to avoid. 
After experimenting @tezc suggestion, **I'm unsure that `SYS_getdents` is preferred over `opendir`.** When I tried using it (check out https://github.com/redis/redis/pull/12658), it didn't recognize this syscall, but **SYS_getdents64** did work, on which I didn't find any good documentation. I ran into gnu's page regarding [getdents64](https://www.gnu.org/software/libc/manual/html_node/Low_002dlevel-Directory-Access.html), which is AS-Safe, but again, didn't find the formal definition of `struct dirent64`, I kinda guessed it. ([here](https://linux.die.net/man/2/getdents64) `d_type` is the last field, but I experimentally found out that it comes before `d_name`. and again, this is on my local machine and I can't guarantee that this is compatible with all platforms.

The bottom line is IMO if talking about **stdio functions**, the benefits of readability, maintainability, compatibility, and simplicity are more significant than the risk of being async-signal-unsafe.

"
1784186857,12453,oranagra,2023-10-29T18:12:06Z,"I'm guessing that considering we had this since forever, then maybe it's safe.
one observation i have is that `sigsegvHandler` (responsible for crashes that terminate the process) uses `serverLog` but `watchdogSignalHandler` that can be used on a live process without killing it, used `serverLogFromHandler` (which uses `open` instead of `fopen`, and doesn't use any malloc or sprintf)

so i'd suggest to stick to that. be more permissive in crashes, but be very strict in `sigalrmSignalHandler`.

@yossigo @tezc WDYT?"
1784217815,12453,tezc,2023-10-29T20:32:56Z,"If we are already calling non-async safe functions (like fopen which calls `malloc()`) and no one complains, maybe it is okay to use these new ones. Though, using more of these functions will increase the chance of happening something bad. (I guess a possible crash or a deadlock).

If you decide to do it by the book:
-  for snprintf, a common practice is writing a limited version like this: [link](https://github.com/twitter/twemcache/blob/8cfc4ca5e76ed936bd3786c8cc43ed47e7778c08/src/mc_util.c#L754)
So, you know it is signal safe. 

- @meiravgri I see using getdents is quite messy and requires making bunch of assumptions.  
Here is how cpython has done it: [link(1)](https://github.com/python/cpython/blob/b75186f69edcf54615910a5cd707996144163ef7/Modules/_posixsubprocess.c#L396C8-L396C22) , [link(2)](https://github.com/python/cpython/blob/b75186f69edcf54615910a5cd707996144163ef7/Modules/_posixsubprocess.c#L430)
In any case, it is quite ugly. "
1784638452,12453,meiravgri,2023-10-30T07:41:27Z,"@oranagra Well, most of the code i added is handling with printing the stacktrace so it will be called on both scenarios.

@tezc 
* I can try this one, thanks!
* Yep, looks similar to what i did. Anyway, i have this already implemented, and i also ran [Daily](https://github.com/meiravgri/redis/actions/runs/6534136257/job/17740697747) against it, seems to be OK, but as you said, i made some guesses...
@yossigo @oranagra @tezc let me know what you think.

In the meantime, you can take a look at the last commit I've pushed (WIP) to see how ugly it is 😄  
https://github.com/redis/redis/pull/12658/commits/82179c3e6e8d6095cdc1a182fd0aff984c860e84"
1785599947,12453,yossigo,2023-10-30T16:35:42Z,"The important bit here is to avoid regression, so we should aovid having a new unsafe function or an existing one in a new context (i.e. not right before exiting). I support @tezc's suggestion of using a safe `snprintf` subset, I think it's both practical and not ugly.
"
1797880986,12453,meiravgri,2023-11-07T06:12:00Z,"@oranagra @tezc @yossigo 
Hi, PR is ready
https://github.com/redis/redis/pull/12658

Please review :)"
1257424975,11290,sundb,2022-09-26T03:25:23Z,missing `set-listpack` in https://github.com/redis/redis/blob/1de675b3d52a369b88dc7a73f0bc359f7984ca44/src/redis-check-rdb.c#L100
1257665844,11290,zuiderkwast,2022-09-26T08:17:08Z,"> missing `set-listpack` in `redis-check-rdb.c`

Done."
1296326122,11290,zuiderkwast,2022-10-30T18:50:56Z,"Sorry for force-push. There was a typo in my last commit. I could have just added a separate fixup but, well, I amended.."
1302130605,11290,oranagra,2022-11-03T13:39:15Z,"trying to do a status check, i resolved some comments that seemed already handled, i can see 4 comments that are unresolved, but it seems they're all about optional future improvements.
maybe the one about lpBatchDelete is an exception (i.e. maybe we wanna handle it here).
please comment and let me know what you think the status is and what's missing if any."
1302361710,11290,zuiderkwast,2022-11-03T16:25:50Z,"> i can see 4 comments that are unresolved, but it seems they're all about optional future improvements.

Awesome. Yes, I think this is ready to merge. If you want me to solve any or all of the remaining comments, I can do as you want. Here's a summary:

1. Dict lookup using `char*` and `size_t` -- :x: leave out IMO. Sds creation just for dict lookup is a problem in many parts of redis, not only the sets type. It deserves a separate PR.
1. lpBatchDelete -- I can attempt it if you think it's strait-forward. Now I understand what you mean anyway. WDYT :question:
2. Intersection: initially large listpack allocation (lpNew with non-zero size), then scale it down at the end (lpShrinkToFit). -- It seems strait-forward now that you explain it like this. I can do it if you want. :question:
3. lpFindInteger -- :x: leave out.. or implement?"
1302455302,11290,oranagra,2022-11-03T17:38:04Z,"yeah, let's try to handle these two you marked with `?`, see how it goes, and leave the other two out.
p.s. i haven't reviewed the tests yet."
1305763701,11290,oranagra,2022-11-07T15:15:48Z,"@redis/core-team please approve:
1. new RDB version
2. new output for OBJECT ENCODING
3. new configs (see the top comment)

note that these default element count threshold is 512 (same as hash-max-listpack-entries, despite hash actually saving two listpack entries per element, so we could in theory set it to 1024). however, zset-max-listpack-entries is by default only 128."
1305770659,11290,oranagra,2022-11-07T15:21:03Z,@zuiderkwast is there anything still pending? maybe some benchmark attempt for the SINTERSTORE pre-allocation heuristics we added in the recent commit?
1305800509,11290,zuiderkwast,2022-11-07T15:43:41Z,"> * are all the new code paths covered by the test suite?

Now they are.

> * maybe run some simple benchmarks to see that this is actually beneficial, and we're not missing something that causes this commit to become a degradation?

Anything particular in mind? Is the github benchmark tag useful?

I tested SINTERSTORE to compare starting off with an intset vs listpack. I did this test in TCL. Do you want me to commit it under the `slow` tag?

The test starts with two identical sets of 100K elements, then adds some extra junk to make them non-intsets (hashtable) for the various cases. 

If the result is a hashtable, the time it takes is almost half (30ms vs 50ms) if we start with a listpack compared to if we start off with intset (which is later converted when a non-int is encountered).

If the result is an intset, the difference is negligible, i.e. no regression. 

"
1306033645,11290,oranagra,2022-11-07T18:42:11Z,"i don't think we need to commit this benchmark, just wanted to manually make sure our optimization pays off, or at least doesn't add a regression."
1308344988,11290,oranagra,2022-11-09T07:48:40Z,"this was conceptually approved in a core-team meeting, the only pending action is to change the default count threshold from 512 to 128 (similar to zsets).

@zuiderkwast please make that change and let me know if there's anything else missing before this can be merged.
thanks  a lot."
1308583347,11290,zuiderkwast,2022-11-09T11:05:05Z,"> this was conceptually approved in a core-team meeting, the only pending action is to change the default count threshold from 512 to 128 (similar to zsets).

Sure, I'll change to 128.

Just thinking that a zset listpack entry is two elements (key and score), while a set only has one element set element, so perhaps 256 is a better match?"
1308689320,11290,oranagra,2022-11-09T12:37:45Z,"logically you're right. as i said maybe even 1024 is right (double as what we have in hash), but as i noted [here](https://github.com/redis/redis/pull/11290#discussion_r1017545315) we don't know anything about how these defaults were created, and we'd rather the the conservative approach, give up some potential memory savings and avoid introducing a bigger performance regression.

btw. do we now need to re-do the benchmark at the top? or just mention it was done with a different config?"
1308901570,11290,zuiderkwast,2022-11-09T15:05:51Z,"> btw. do we now need to re-do the benchmark at the top? or just mention it was done with a different config?

The benchmark was done with ~50 elements per set, with some randomness. I don't think any of these sets got more than 128 elements so I don't think we need to redo the benchmark."
1309184937,11290,zuiderkwast,2022-11-09T18:22:25Z,"Thanks for another piece of good collaboration! It's been a pleasure, as always."
1316520907,11290,oranagra,2022-11-16T07:42:46Z,"@zuiderkwast recently the corrupt-dump-fuzzer test often hangs in SDIFF
`command caused test to hang? SDIFF _set`
https://github.com/redis/redis/actions/runs/3466227366/jobs/5789823598
`command caused test to hang? SDIFF _zset`
https://github.com/redis/redis/actions/runs/3475185032/jobs/5809132279
`command caused test to hang? SDIFF _setbig`
https://github.com/redis/redis/actions/runs/3475185032/jobs/5809133661
`command caused test to hang? SDIFF _set`
https://github.com/redis/redis/actions/runs/3475185032/jobs/5809133773

i suspect it has to do with this PR, can you look into it?
maybe improve the mechanism added by #8837 to print `$sent` instead of `$cmd`, to easily reproduce it."
1316535155,11290,enjoy-binbin,2022-11-16T07:51:16Z,"I also noticed, reproduced before, but it didn't stop there with `--stop` option
i tried adding something (something like --stop) early to see if I can reproduce it locally.
i am running a loop in my machine. let me see if I can get some useful information"
1316544754,11290,oranagra,2022-11-16T07:54:50Z,"IIRC the mechanism i implemented there kicks in only when the `--timeout` is reached and the whole thing is terminated.
`--stop` works when a test fails. i'm guessing that in order to reproduce this we need:
1. create a bash loop that runs many short sessions of this test (avoid using a long `--timeout`, and run with `--accurate` since that will cause us to detect the hung a lot after it happened).
2. print `$sent` instead of `$cmd`."
1321776911,11290,enjoy-binbin,2022-11-21T09:44:22Z,"@oranagra @zuiderkwast 
I successfully reproduced the problem, this payload:
```
restore key 0 ""\x14%%\x00\x00\x00\n\x00\x06\x01\x82_5\x03\x04\x01\x82_1\x03\x82_3\x03\x00\x01\x82_9\x03\x82_3\x03\b\x01\x02\x01\xff\x0b\x001\xbe}A\x01\x03[\xec"" replace

[root@binblog redis]# src/redis-cli smembers key
 1) ""6""
 2) ""_5""
 3) ""4""
 4) ""_1""
 5) ""_3""
 6) ""0""
 7) ""_9""
 8) ""_3""
 9) ""8""
10) ""2""
[root@binblog redis]# src/redis-cli sdiff key                                                                      
1) ""6""
2) ""_5""
3) ""4""
4) ""_1""
5) ""_3""
6) ""0""
7) ""_9""
8) ""8""
9) ""2""
[root@binblog redis]# src/redis-cli scard key                                                                      
(integer) 10
```

this payload actually created a set with repeating elements (-3, see the example), the sdiff except 10 elements, but got 9 in the last, so it hang

if we set `sanitize-dump-payload` to yes, we will be able to find the dup ele and report ""ERR Bad data format"" 

the fix for sdiff (avoid the hang), do you guys think of anything else that needs fixing?:
```diff
} else if (op == SET_OP_DIFF && sets[0] && diff_algo == 1) {
        /* DIFF Algorithm 1:
         *
         * We perform the diff by iterating all the elements of the first set,
         * and only adding it to the target set if the element does not exist
         * into all the other sets.
         *
         * This way we perform at max N*M operations, where N is the size of
         * the first set, and M the number of sets. */
        si = setTypeInitIterator(sets[0]);
        while ((encoding = setTypeNext(si, &str, &len, &llval)) != -1) {
            for (j = 1; j < setnum; j++) {
                if (!sets[j]) continue; /* no key is an empty set. */
                if (sets[j] == sets[0]) break; /* same set! */
                if (setTypeIsMemberAux(sets[j], str, len, llval,
                                       encoding == OBJ_ENCODING_HT))
                    break;
            }
            if (j == setnum) {
                /* There is no other set with this element. Add it. */
-                setTypeAddAux(dstset, str, len, llval, encoding == OBJ_ENCODING_HT);
-                cardinality++;
+                cardinality += setTypeAddAux(dstset, str, len, llval, encoding == OBJ_ENCODING_HT);
            }
        }
        setTypeReleaseIterator(si);
    } else if (op == SET_OP_DIFF && sets[0] && diff_algo == 2) {
```

The fix: #11530"
1321787327,11290,oranagra,2022-11-21T09:52:48Z,"nice! (generated a broken protocol and left the client hung).
i agree, this is the right fix, please make a PR.
if if you have any test suite improvements that you wrote to hunt that, please add them as well."
997457092,9938,oranagra,2021-12-19T20:36:22Z,@redis/core-team please approve
1000757867,9938,oranagra,2021-12-24T09:47:31Z,"> > FUNCTION RESTORE  [FLUSH|APPEND|REPLACE]
> 
> > APPEND: appends the restored functions to the existing functions. On collision, abort.
> 
> just one question, does it satisfy atomicity? the abort means rollback or just abort?

if any conflicts are detected, the operation is aborted without any modification to the server."
754725523,8288,oranagra,2021-01-05T15:58:08Z,"@zuiderkwast thanks for this PR, haven't reviewed the code yet, but i wanna mention my initial thoughts.

1. i'm a bit uncomfortable to merge an API for just ""add"" before we have a design for the rest of the stream APIs. my fear is that when we'll add the rest later, we'll have some realizations and will want to retroactively change something in an API that's already released.
2. i wonder if the ""add"" API is indeed useful on its own? i.e. if there are a lot of use cases for modules that would just like to write into a stream, and don't care much about reading (or in which performance of writing is more critical than reading).

pinging @MeirShpilraien and @gkorland in case they can share some thoughts on this."
754990555,8288,zuiderkwast,2021-01-06T00:41:24Z,"> i'm a bit uncomfortable to merge an API for just ""add"" before we have a design for the rest of the stream APIs.

Sure, leave it open until we have the rest of the design in place. I created this PR to initiate a discussion.

> i wonder if the ""add"" API is indeed useful on its own?

Yes, it is. Any module currently using RM_Call() will be faster with each native API function available. My associates gave me this wishlist with the following priority:

1. XADD ""Missing and very much needed, high overhead, called a lot.""
2. XREAD/XRANGE
3. XTRIM

XLEN is already supported as RedisModule_ValueLength().

> let's discuss the rest of the API and plans for the near future before we dive into the implementation details.

Sounds good.

I might start drafting a design for read, range and trim too as separate PRs (but I'd prefer to wait for @guybe7's ""Trim by MINID"" PR to be merged before I start implementing a trim API)."
755372091,8288,oranagra,2021-01-06T15:37:26Z,"I have a feeling that one Issue or PR would be better to discuss these API since changes in one may reflect the other, and they can share some flags / type.
Also, the code is not huge, being too large to review in one go.
The way i see it, the only reason to split them into separate PRs is if one is easier and more urgently needed than others, and we'd like to merge it before others are ready."
758762097,8288,zuiderkwast,2021-01-12T16:09:34Z,"PR updated to include adding, iterating and trimming. Please review mainly the API design."
762149910,8288,zuiderkwast,2021-01-18T10:25:54Z,"> i suppose the iterator can also be useful for single ""get"", but maybe we wanna expose a specific easy to use API for that?

Sure we can, but combined with your suggestion for iterating over the fields, what should ""get"" return?

> what about deletion?

If anyone needs it, it's always possible to use Call. But sure, I can add StreamDelete if you want. Or maybe as a flag to the iterator, deleting as we go?

> if we put aside consumer groups, i suppose with the addition of an explicit ""read"" (if we chose to add it), and a ""del"" we got streams covered, right?

AFAIK yes."
762159284,8288,oranagra,2021-01-18T10:41:21Z,"> Sure we can, but combined with your suggestion for iterating over the fields, what should ""get"" return?

good point. i guess an iterator is enough. come to think of it, the XREAD and XRANGE are also iterator based.
 
> If anyone needs it, it's always possible to use Call. But sure, I can add StreamDelete if you want. Or maybe as a flag to the iterator, deleting as we go?

Ideally, we'll have a delete API that takes ID, can can be used both without iteration, and as part of an iteration.
but IIRC the iterator is actually holding the current node, so we can't delete it before doing next.
this leaves us with two a bit ugly options.
1. tell the user that if he want's to delete during iteration, he need to remember the last ID, do a Next, and only then delete the previous ID.
2. add a separate API that does ""Delete and Step"". (in addition to a Next and a stand-alone Del).

is that right? or am i missing something (sorry busy with other things so i can't afford to dive into the code and plan right now)"
763708353,8288,oranagra,2021-01-20T15:25:48Z,@zuiderkwast seems good.. what else is missing before this is ready to be merged?
763806075,8288,zuiderkwast,2021-01-20T17:22:44Z,"> @zuiderkwast seems good.. what else is missing before this is ready to be merged?

TODO:

* [x] set errno **(EOF vs ERR with ENOENT for end-of-iterator TBD)**
* [x] RM_StreamIteratorDelete() &ndash; requires that the current stream ID is stored somewhere, probably in the key. (It can share space in the RedisModuleKey struct by making a `union { zset iterator stuff ; stream iterator stuff }`)
* [x] Out-of-bounds checking for RM_StreamIteratorNextField() &ndash; requires storing numfields and a field counter somewhere, in the key struct.
* [x] Optimization: Call signalKeyAsReady() on RM_CloseKey() instead of on every StreamAdd().

If you agree with the above..."
763913018,8288,oranagra,2021-01-20T20:30:07Z,"the above seem fine, but i need to look at the details / implementation to be able to judge better.
i posted a comment about signalKeyAsReady above which i think we also still need to discuss."
764979591,8288,zuiderkwast,2021-01-21T22:22:42Z,"> i really don't like all the change to zset.. but i guess there's no other way. 8-(

Well, if we allow anonymous unions and structs here (which are standard in C11) we can use a union without changing the reference to the zset fields..."
766304629,8288,oranagra,2021-01-24T07:22:41Z,"> Well, if we allow anonymous unions and structs here (which are standard in C11) we can use a union without changing the reference to the zset fields...

Sadly we can't assume C11 is supported. Redis 6.0 went that way, and we had to revert that in 6.2.
The other option is to create proprocessor replacements that will expand these member names, but that's ugly."
766397945,8288,oranagra,2021-01-24T17:22:16Z,"@redis/core-team please approve the creation of new module API for stream manipulation and iteration (handing anything other than consumer groups).
@zuiderkwast please make sure that the top comment is up to date (will be used as commit comment too).
please make sure to run the new tests with `--valgrind`."
769112981,8288,zuiderkwast,2021-01-28T14:39:02Z,"Thanks a lot, especially @oranagra, for your fast and useful responses! I makes me enjoy working with Redis. :-) See you in a different PR. Next, I'm considering picking something from the [Meta] Modules API issue."
769125070,8288,oranagra,2021-01-28T14:46:29Z,thank **you** Viktor for taking the initiative and doing all the work.
1780830081,12658,oranagra,2023-10-26T10:19:43Z,"@madolson one of the things we aim to solve here is to avoid memory allocations from within the signal handler.
apparently getConfigDebugInfo uses the config rewrite mechanism to generate an sds with the debug configs, but i wonder why you chose to use the config rewrite mechanism for this function and not just print the key and value?"
1819494881,12658,meiravgri,2023-11-20T17:19:05Z,"@oranagra @tezc @yossigo 
Basically done
daily is running
https://github.com/meiravgri/redis/actions/runs/6933529470

tomorrow I'll edit the top description to include all the latest changes."
1819753115,12658,tezc,2023-11-20T20:25:11Z,"@meiravgri Out of curiosity, I've complied this branch and sent SIGALRM. This is the output: 

```
12729:signal-handler (1700510847) 
Received SIGALRM

------ STACK TRACE ------
EIP:
/lib/x86_64-linux-gnu/libc.so.6(epoll_wait+0x5e)[0x7f7354b2601e]
12729:signal-handler (1700510847) writeStacktraces(): Failed to get the process's threads.

0/0 expected stacktraces.
12729:signal-handler (1700510847) --------
```

Maybe, we broke something on the way. This is not the expected output right?
"
1820288428,12658,meiravgri,2023-11-21T05:57:56Z,"@tezc 
no this is not
Doesn't happen to me locally + all the test passed + the test includes checking that we don't get this specific line so it doesn't happen on any system we support

what os do you have?
"
1820412744,12658,tezc,2023-11-21T08:02:20Z,"```
ozan@ozan-x1:~/Desktop/redis/src$ uname -a
Linux ozan-x1 6.2.0-36-generic #37~22.04.1-Ubuntu SMP PREEMPT_DYNAMIC Mon Oct  9 15:34:04 UTC 2 x86_64 x86_64 x86_64 GNU/Linux
ozan@ozan-x1:~/Desktop/redis/src$ cat /etc/lsb-release 
DISTRIB_ID=Ubuntu
DISTRIB_RELEASE=22.04
DISTRIB_CODENAME=jammy
DISTRIB_DESCRIPTION=""Ubuntu 22.04.3 LTS""

```

I see it doesn't work with current unstable as well on my local. Maybe there is some sort of security enabled on my system (I guess I should be happy) "
1820623546,12658,oranagra,2023-11-21T10:18:13Z,"@tezc does DEBUG SEGFAULT or DEBUG ASSERT works for you (before this PR)?
maybe we can at least improve the failure to print the relevant `errno`?"
1820627256,12658,meiravgri,2023-11-21T10:20:19Z,"@oranagra apprently the condition o the signal mask wasn't correct
fixed it in the last commit

@tezc please confirm it fixed the problem"
1820637183,12658,tezc,2023-11-21T10:25:28Z,"Thanks, it works now! 
Btw, you may want to mention it in the top comment. The check was not correct before this PR."
1823258332,12658,oranagra,2023-11-22T18:17:12Z,"@meiravgri i think we're done here, please make sure the top comment is up to date, and the tests pass on the various platforms."
1823843910,12658,meiravgri,2023-11-23T05:39:04Z,"@oranagra 👏 👏 
It's updated

daily with `--single integration/logging` is green
https://github.com/meiravgri/redis/actions/runs/6959290055
now the full routine is running
https://github.com/meiravgri/redis/actions/runs/6966006124"
973983952,9812,soloestoy,2021-11-19T11:15:30Z,"Good work! ping @redis/core-team to review.

About scripts persisting, I prefer NO, since we never guarantee scripts could be persisted, and as antirez said, it can make replication clearer.

And I think we can allow eviction in lua now, fix issue #8478."
974021428,9812,oranagra,2021-11-19T12:15:52Z,"@zhugezy thank you and welcome.
@soloestoy i was under the impression we agreed this task is assigned to @yoav-steinberg .
in any case, i think this was done too early, and will need to be re-done once #9780 is merged.
In the meanwhile, we can start discussing the details here (instead of the other 3 existing open issues / PRs on the same subject).
There are currently arguments for both keeping scripts deterministic (i'll let @yossigo post them), as well as keeping the persistence (which you argued for).

Regarding the persistence, the EVAL design was very clear from the get go that clients can't assume the script is cached, and the EVALSHA is an optimization that the client should be prepared for any case the script is not cached.
I know users kept arguing that it's buggy, without reading the theory and design behind it (which is that the script is part of the client application, that's executed in the server, and not at all the responsibility of the server, which is why it was not named and not versioned)

In redis 4.0, because of PSYNC2, these cases where the script is missing got rarer, so users may have got accustomed to it.
and now that we remove EVAL propagation, in theory we can go back on that, and never replicate or persist the EVAL scripts.
This will make a the EVAL scripts design and the differences from Redis Functions clearer, and force users to realize that.
but indeed some users will see it as a step backwards."
974031344,9812,soloestoy,2021-11-19T12:29:43Z,">@soloestoy i was under the impression we agreed this task is assigned to @yoav-steinberg .

Oh I didn't notice it, maybe you mean this approach https://github.com/redis/redis/issues/8478#issuecomment-799757436? But I think after this PR we can allow eviction in lua just like `MULTI/EXEC`, so the problem can be fixed smoothly and no need break atomic."
974036907,9812,oranagra,2021-11-19T12:38:25Z,"I thought we agreed that both of these will be assigned to Yoav, but maybe don't recall something. 
Anyway, it's too early to code it, but not too early to discuss and decide... 

Regarding the OOM, even if we allow eviction, the problem still exists since the server might be configured not to allow eviction. And also, there's the other discussion to make about deterministic execution (which I mentioned above but left for Yossi to present), which may lead to concluding that we still can't evict inside a script.
But anyway, let's not mix these two topics, and discuss each of them in a separate PR. "
974041507,9812,oranagra,2021-11-19T12:45:30Z,"
Let's first debate about propagation and persistence. 
I remember Salvatore vocally mentioning that users should not assume scripts are cached, possibly even if the same connection that loaded them is still alive. 
I don't remember where it was and what was the argument, but the thing is that the design is that the script and its exact content are actually part of the caller app (not the server's responsibility), and it should always be prepared to reload it. 
The main feature is EVAL, and the SHA part is just an optimization. "
974804563,9812,yossigo,2021-11-21T12:06:05Z,"Persistence and replication: the actual guarantee as documented was never very clear and at some point (https://github.com/redis/redis/issues/5292#issuecomment-416968162) it was limited to the scope of a single connection (although `SCRIPT FLUSH` can break that as well).

Redis 7 and Redis Functions is a good opportunity to clean this up and re-set the client expectations, so I think it makes sense to completely drop script replication.

Eviction: I'm not in favor of that. While it's not strictly needed for consistency any longer, I think guaranteeing atomicity is still good practice, especially given that scripts are not designed to be long running. Having scripts suddenly see keys vanishing keys during execution in Redis 7 seems like a problem to me.

Deterministic execution: This too is not strictly needed but I think not being able to enforce it, at least optionally, is a potential regression. I think it's reasonable to expect identical results if you run the same script on multiple instances with the same dataset (e.g. running tests scenarios on different envs, prod vs. staging/dev, etc.)."
982448378,9812,zhugezy,2021-11-30T09:29:18Z,"Stage summary: looks like we have reached a consensus that script persistence/replication can & should be dropped, and haven't seen any objections in the past few days. If it's done, then we can move onto the next topic.

For deterministic execution, I think non-deterministic results are consistent with users' expectations when they put ""random"" commands in scripts/functions and send them to multiple instances, while deterministic may not. So why bother making the result deterministic? Anyway, it would be better if someone could cite their examples about it."
982468917,9812,oranagra,2021-11-30T09:52:57Z,"@zhugezy we had a discussion about this these topics in a core-team meeting today.
At first, we concluded that we better keep the feature in redis that allows scripts to be deterministic, but maybe let the user control if it's enabled or disabled.
we considered to either:
1. if we intend to keep this feature forever, then have it disabled in functions but default, and keep it enabled in eval by default (so we don't break existing scripts).
2. or if we intend delete it some day, we can start by making it disabled by default for both functions and eval (so users can enable it if the run into trouble), and then consider deleting the code at a later stage (kinda how the lua_replicate_commands feature was introduced in baby steps too).

but then we realized that since `lua_replicate_commands` was changed to default of true, it also meant that scripts where allowed to be non-deterministic by default, and we are not aware of anyone who complained.
so in fact we're already half way through on our way to delete that feature, and we can take the next step now (actually delete it).

regarding script persistence and replication, as was said before, it was never meant to be persisted and replicated, and even if some users started getting accustomed to it since PSYNC2, it's actually likely that it'll cause them bugs, since the scripts are sometimes replicated, but they're never persisted. we wanna proceed and delete that feature completely.

so i think the next step is to wait for the functions PR to be merged, and then re-do this deletion work."
982478114,9812,zhugezy,2021-11-30T10:03:32Z,"@oranagra Totally agreed.
BTW should I set this PR as a draft, or close it now and make a new PR after Function merged?"
982497760,9812,oranagra,2021-11-30T10:24:52Z,"i think you can just force-push the new version into it when it's ready.
no sense in opening another thread when we have the discussion here."
983312497,9812,oranagra,2021-12-01T05:53:39Z,"btw, i see this PR doesn't remove the `MASTER and SLAVE consistency with EVALSHA replication` test, i guess it should.
this test keeps hanging every day in the freebsd CI (i guess for being slow).
not sure what was the process to decide which tests to delete, was it just ones who failed?"
983332687,9812,zhugezy,2021-12-01T06:33:44Z,"@oranagra Oh, I didn't notice this test... I just checked all tests in scripting.tcl, tests with keywords like ""lua-replicate-commands"",""redis.replicate_commands"" or else and other failed tests, which I thought are all of them. I'll check other test files later."
986180121,9812,oranagra,2021-12-05T07:21:51Z,"@zhugezy we discussed script persistence and propagation in a core-team meeting and concluded we want them removed.
in any case i don't see a reason to keep script persistence if we trimmed the propagation part.

also, we'll need to remove the excessive command flags soon, but let's do it after #9656 is merged.

i added these to a `todo` section in the top comment."
986485256,9812,oranagra,2021-12-06T06:45:40Z,"@zhugezy i remind you about `MASTER and SLAVE consistency with EVALSHA replication` and possibly other tests that make no sense now.
on one side, as long as they pass, maybe they still have value and increase coverage, but on the other hand some may not make any sense.
specifically for the one mentioned above, i'd like to see it gone since it keeps [hanging](https://github.com/redis/redis/runs/4424758278?check_suite_focus=true#step:4:5663) on freebsd CI."
986488540,9812,zhugezy,2021-12-06T06:52:49Z,"> @zhugezy i remind you about `MASTER and SLAVE consistency with EVALSHA replication` and possibly other tests that make no sense now.
> on one side, as long as they pass, maybe they still have value and increase coverage, but on the other hand some may not make any sense.
> specifically for the one mentioned above, i'd like to see it gone since it keeps [hanging](https://github.com/redis/redis/runs/4424758278?check_suite_focus=true#step:4:5663) on freebsd CI.

Discussed about it with @soloestoy last week, maybe this test is not totally useless, though replication cache has gone and it is effects replication now? Or you mean it is duplicated with other tests so we have no need keeping it? Just some confirmation."
986507844,9812,oranagra,2021-12-06T07:27:24Z,"well, as i mentioned, we don't wanna blindly delete all the tests that where aiming to test the EVAL propagation, since (as long as they don't fail) they may still have value and increase coverage.
but i do think we wanna evaluate them one by one and try to conclude which ones can be removed and which ones should be kept (maybe slightly changing their code and title, so it makes sense in the new reality).

regarding this specific test, do you have any reason to believe it provides some value that's not covered by many other tests?
what specific value does it add.
I imagine that anything it tests (propagation of EVAL) has other tests that cover it, and the only thing that's special about this one is that it was aiming to exhaust the script cache.

at the very least, we should modify this line:
```
set numops 20000 ;# Enough to trigger the Script Cache LRU eviction
```
i.e. first, the comment is completely outdated, and secondly if we now change `numops` to 2 rather than 20k, maybe the freebsd tests will stop hanging every day."
986518094,9812,zhugezy,2021-12-06T07:48:08Z,"> I imagine that anything it tests (propagation of EVAL) has other tests that cover it, and the only thing that's special about this one is that it was aiming to exhaust the script cache.

I guess there isn't any other 'unique' points that this test covers, but I was not so sure about it(I don't like 'guess'). 
Going to remove this test rather than erasing '0's, for simplicity. "
986544493,9812,oranagra,2021-12-06T08:25:56Z,"@zhugezy please invest a few minutes to look for other tests that may no longer be needed (e.g. this one specifically mentions the ""Script Cache""), others may switch between EVAL and EVALSHA, add and remove replicas, etc.
Lets list them here and try to reach a conclusion if they should be trimmed, or just slightly modified / re-labeled."
988563862,9812,zhugezy,2021-12-08T07:25:29Z,"I took some time looking into tests with keyword ""eval""/""script""/""replication cache""/... and re-evaluating them. Too many that I can't list them all, so some of them which I think may draw your attention are listed below:

Tests I think should be kept without modifying are marked with ✅.
Ones I am not sure are marked with ❓.
Last ones with ❌ should be removed.

Still there may be some missing tests, but I think they are quite few.


## Integration/psync2

PSYNC2: Replica RDB restart with EVALSHA in backlog issue #4483  ❓

## Integration/replication

348 Master stream is correctly processed while the replica has a script in -BUSY state ❓

## Integration/replication-3

147 SLAVE can reload ""lua"" AUX RDB fields of duplicated scripts ❌ 	Looks like it's useless now?

## cluster/04-resharding

Cluster consistency during live resharding ❓

## cluster/10-manual-failover

Send CLUSTER FAILOVER to #5, during load ❓

## unit/acl

ACL can log errors in the context of Lua scripting ✅   keep it

## unit/multi

MULTI propagation of SCRIPT LOAD ✅  keep it

MULTI propagation of EVAL ✅ already modified in previous commits

## unit/scripting

EVAL processes writes from AOF in read-only slaves❓

We can call scripts rewriting client->argv from Lua ❓

### 765 tags {""scripting repl needs:debug external:skip""}

❓ Need detailed analyzing and judgement

### 862 tags {""scripting repl external:skip""}

✅  should keep them all"
992312419,9812,oranagra,2021-12-13T10:18:19Z,"> Integration/psync2
PSYNC2: Replica RDB restart with EVALSHA in backlog issue #4483  ❓ 

**delete it**

> Integration/replication
Master stream is correctly processed while the replica has a script in -BUSY state ❓ - 

**keep it**, the script is runs on the slave

> Integration/replication-3
SLAVE can reload ""lua"" AUX RDB fields of duplicated scripts ❌ 	Looks like it's useless now?

**yes, delete it** together with the test above it and the entire server block.

> cluster/04-resharding
Cluster consistency during live resharding ❓

**keep**, it has nothing to do with script propagation AFAICT

> cluster/10-manual-failover
Send CLUSTER FAILOVER to #5, during load ❓

**keep**, it has nothing to do with script propagation AFAICT

> unit/acl
ACL can log errors in the context of Lua scripting ✅   keep it

**yes, keep it**

> unit/multi
MULTI propagation of SCRIPT LOAD ✅  keep it

i guess now is the time to also avoid propagating SCRIPT LOAD (since we avoid propagating EVAL, and avoid saving scripts to the RDB).
in which case we can keep that test, but negate it.
(and mention this change in the top comment, i.e the fact that SCRIPT LOAD is no longer propagated)

> MULTI propagation of EVAL ✅ already modified in previous commits

good.

> unit/scripting
EVAL processes writes from AOF in read-only slaves❓

we need to change this test to create the AOF manually, like you did for `EVAL timeout with slow verbatim Lua script from AOF` (and maybe move it to aof.tcl too)

> We can call scripts rewriting client->argv from Lua ❓ 

WTF? this test isn't doing what it says it does, and never did (76c31d425e797ddf5daedd29d893d3fc9c7cfc19).
i think we should fix the test to do what it says it aims to do.
@yoav-steinberg do you have any idea what's going on with this commit?

> 765 tags {""scripting repl needs:debug external:skip""}
❓ Need detailed analyzing and judgement

i don't understand what you're referring to.

> 862 tags {""scripting repl external:skip""}
✅  should keep them all

i don't understand what you're referring to."
992335041,9812,zhugezy,2021-12-13T10:45:59Z,"@oranagra I meant the tests in line 765-860 and in line 862-971 in scripting.tcl. Didn't have time looking deep into them, but now I think they should all be kept."
992345255,9812,oranagra,2021-12-13T11:00:18Z,"ok, i looked at them, and some are a little bit out of context (e.g. `Before the replica connects we issue two EVAL commands`), but let's keep them"
993238401,9812,zhugezy,2021-12-14T07:32:40Z,"Two quick notes:
@oranagra everything's ready except the ""We can call scripts rewriting client->argv from Lua"" thing, I have no idea what it is doing. Maybe we should wrap them with eval?...
And ""MULTI propagation of SCRIPT LOAD"" I made some tiny changes on this case, instead of removing it.


@MeirShpilraien Please take some time looking at the case in aof.tcl in commit 0ee9dfb (which is just above this comment).
I spent some time testing about this case. The result is:
Before Redis Function PR, the write eval commands in AOF can be loaded into a ro-slave with command ""debug loadaof"" successfully (which means the result is 102).
However after the PR, this cannot be done (which means the result is nil and I got an '-READONLY' err). Is this change intended or it's out of expectation?
<del>================TEMPORARY DEBUGGING NOTES HERE=============================
UPD: Same problem with loading AOF when starting server. I'm sure there's something not going well with the func `scriptVerifyWriteCommandAllow` in `scriptCall`.</del>

<del>UPD2: looks like the bug occurs when the `rctx = luaGetFromRegistry(...)` in function `luaRedisGenericCommand` is called. The returned ctx `rctx->original_client` should be that AOF value, but it's not. @MeirShpilraien @oranagra I'm not familiar with lapi.c, they are just too hard to read. Could you explain what happens in this `luaGetFromRegistry` function (it's in script_lua.c:79) for me? Much thanks to you!</del>
UPD3:No need to explain it anymore."
993460131,9812,MeirShpilraien,2021-12-14T11:46:17Z,@zhugezy I will take a look and update ASAP.
995703093,9812,guybe7,2021-12-16T11:25:31Z,"@oranagra this is the PR that should move ""sort_for_script"" and ""random"" to be doc-flags right? just making sure it's not overlooked

also, maybe rename sort_for_script to ""random-order"" or something more explicit"
995708757,9812,zhugezy,2021-12-16T11:32:56Z,"@guybe7 if you mean mentioning it, yes. If you mean doing the moving work, no.
https://github.com/redis/redis/pull/9812#discussion_r764917507 
I didn't track it further, what I can make sure is that discussions in this thread is all of the discussions about these 'flags' works(at least in this PR)."
995709978,9812,oranagra,2021-12-16T11:34:35Z,"@guybe7 no, this is gonna be the responsibility of #9876"
995712132,9812,oranagra,2021-12-16T11:37:25Z,"I already copied our conclusions to the other issue, there's nothing more to do about these in this PR.
all it does is delete all the code that uses these hints, and the hints will be refactored (and discussed) in another PR."
996481524,9812,oranagra,2021-12-17T06:55:20Z,@redis/core-team please approve (see top comment for a list of what's included and excluded) 
998241662,9812,oranagra,2021-12-20T20:20:07Z,Full CI: https://github.com/redis/redis/actions/runs/1603831303
1954562053,12826,oranagra,2024-02-20T16:15:15Z,@redis/redis-committers maybe one of you has time to review this one?
1964523855,12826,zuiderkwast,2024-02-26T16:06:55Z,"I tried this feature briefely on some small dummy data. I think the feature can be useful, but probably the output and other user experience can be improved. (I haven't thought about it much yet.) Here is a text-based ""screenshot"" for the record:

```
$ ./redis-cli --keystats

# Scanning the entire keyspace to find the biggest keys and distribution information.
# Use -i 0.1 to sleep 0.1 sec per 100 SCAN commands (not usually needed).
# Use --cursor <n> to start the scan at the cursor <n> (usually after a Ctrl-C).
# Use --top <n> to display <n> top key sizes (default is 10).
# Ctrl-C to stop the scan.

100.00% ||||||||||||||||||||||||||||||||||||||||||||||||||||||||||||
Keys sampled: 9
Keys size:    520B

--- TOP 10 KEY SIZES
  1      88B hash       ""h1""
  2      72B list       ""l1""
  3      56B string     ""x""
  4      56B string     ""\xc3\xa4""
  5      56B string     ""z""
  6      48B string     ""5""
  7      48B string     ""1""
  8      48B string     ""7""
  9      48B string     ""3""

--- TOP SIZE PER TYPE
list       ""l1"" is 72B
hash       ""h1"" is 88B
string     ""x"" is 56B

--- TOP LENGTH AND CARDINALITY PER TYPE
list       ""l1"" has 5 items
hash       ""h1"" has 4 fields
string     ""\xc3\xa4"" has 3B

--- Key Size  Percentile   Total Keys
         48B     0.0000%            4
         56B    50.0000%            7
         72B    87.5000%            8
         88B   100.0000%            9
Note: 0.01% size precision, Mean: 57B, StdDeviation: 12B

--- Key length  Percentile  Total keys
            2B   100.0000%           9
Total key length is 12B (1B avg)

--- Type |  Total keys| Keys %|Tot size|Avg size| Total length/card|Avg ln/card
list                 1  11.11%      72B      72B            5 items        5.00
hash                 1  11.11%      88B      88B           4 fields        4.00
string               7  77.78%     360B      51B                10B          1B
```

Then, I tried it again after populating 1M keys (using `DEBUG POPULATE 1000000`). It does a scan over the entire keyspace. It suppose it can be quite costly. The comment ""Use -i 0.1 to sleep 0.1 sec per 100 SCAN commands (not usually needed)"" worries me a little, [Edit] but `--bigkeys` does this too, so I suppose it's fine.

I've seen an oddity when aborting the scan with Ctrl+C. It prints the results of the keys scanned, along with a message like

```
Scan interrupted:
Use 'redis-cli --keystats --cursor 784131' to restart from the last cursor.
```

During the scan, there is a nice progress bar displayed during the scan. If I abort the scan after 75% and then resume using `redis-cli --keystats --cursor 784131`, the progress bar then goes from 0 to 25% and then it stops at 25%, which looks odd. I would expect it to continue from 75% and scan to reach 100%, but obviously it doesn't know that the cursor is at 75% of the scan."
1967330078,12826,zuiderkwast,2024-02-27T18:14:39Z,"The interactive progress bar and the possibility to resume if interrupted is a nice experience. Can we add those to `--bigkeys` and `--hotkeys` too? Similar features should have a similar user experience IMO.

Maybe even `--scan` can benefit from interrupt and resume using `--cursor`, but I guess it makes sense only if stdin is a TTY but stdout is not a TTY, i.e. if it's invoked like `redis-cli --scan > file`. Possibly we can display the progress bar and message on the TTY in that case, where it isn't mixed with the returned keys. Does it make sense?

The design of the output (headings and tables) is not similar to the output of the other features like `--bigkeys`, `--memkeys` and `--stat`. I think we should use a similar design as the existing features."
1968321433,12826,yveslb,2024-02-28T06:32:03Z,"@zuiderkwast

You are correct about restarting at a cursor other than 0. It will be nicer to restart at the last percentage, but as you mentioned, I am not sure we can find a way to do so. I guess the user should be ok knowing that using `--cursor` will restart from 0% and will probably not reach 100%. In addition, between the scans, we might have added or remove some keys.

`--keystats` should have all the information from `--memkeys` and `--bigkeys`. To keep  the existing code behavior and output, I did not want to change it too much (except to fix the issue with `--memkeys-samples 0`). The worry being to break any automations that rely on the output of existing commands. I am not sure the output of `--memkeys`, `--bigkeys`, and `--hotkeys` is designed to be parsed, but some users are creative.
If we think we should change the output of existing commands to be more consistent, we could do it in another pull request.

For the Redis Data Type summary difference, I did the change for readeability purpose.

```
--- Type |  Total keys| Keys %|Tot size|Avg size| Total length/card|Avg ln/card
list             10001  20.00%    2.25G  235.39K       188538 items       18.85
string           10003  20.00%   12.73M    1.30K             11.05M       1.13K
MBbloomCF            1   0.00%    1.11K    1.11K                 -           -
hash             10000  19.99%  127.31M   13.04K       47160 fields        4.72
TopK-TYPE            1   0.00%  112.62K  112.62K                 -           -
TDIS-TYPE            1   0.00%    9.58K    9.58K                 -           -
TSDB-TYPE            1   0.00%    4.14K    4.14K                 -           -
set              10000  19.99%   70.63M    7.23K      47390 members        4.74
CMSk-TYPE            1   0.00%  140.67K  140.67K                 -           -
zset             10000  19.99%   65.48M    6.70K      47098 members        4.71
MBbloom--            1   0.00%     280B     280B                 -           -
ReJSON-RL            4   0.01%    1.45K     370B                 -           -
```

vs

```
10001 lists with 2410602722 bytes (20.00% of keys, avg size 241036.17)
10003 strings with 13345720 bytes (20.00% of keys, avg size 1334.17)
1 MBbloomCFs with 1136 bytes (00.00% of keys, avg size 1136.00)
10000 hashs with 133490960 bytes (19.99% of keys, avg size 13349.10)
1 TopK-TYPEs with 115328 bytes (00.00% of keys, avg size 115328.00)
0 streams with 0 bytes (00.00% of keys, avg size 0.00)
1 TDIS-TYPEs with 9808 bytes (00.00% of keys, avg size 9808.00)
1 TSDB-TYPEs with 4240 bytes (00.00% of keys, avg size 4240.00)
10000 sets with 74061054 bytes (19.99% of keys, avg size 7406.11)
1 CMSk-TYPEs with 144048 bytes (00.00% of keys, avg size 144048.00)
10000 zsets with 68656760 bytes (19.99% of keys, avg size 6865.68)
1 MBbloom--s with 280 bytes (00.00% of keys, avg size 280.00)
4 ReJSON-RLs with 1483 bytes (00.01% of keys, avg size 370.75)
```

`--keystats` has more data to present as we combine `--memkeys` and `--bigkeys`.
I think it is more readeable using a table, but the inconsistency comment is valid, if we think consistency is more important than readeability.
I would say that most data presentation in `--keystats` breaks from `--memkeys` and `--bigkeys`. "
1968882480,12826,zuiderkwast,2024-02-28T12:29:27Z,"> You are correct about restarting at a cursor other than 0. It will be nicer to restart at the last percentage, but as you mentioned, I am not sure we can find a way to do so. I guess the user should be ok knowing that using `--cursor` will restart from 0% and will probably not reach 100%. In addition, between the scans, we might have added or remove some keys.

I guess you're right. I don't have a better idea, so I think this is fine.

> The worry being to break any automations that rely on the output of existing commands. I am not sure the output of `--memkeys`, `--bigkeys`, and `--hotkeys` is designed to be parsed, but some users are creative.

I think we cannot break the non-interactive output which the creative user can use in scripts, but in interactive mode it's OK to change things like progress bar.

> If we think we should change the output of existing commands to be more consistent, we could do it in another pull request.

Yes, but my experience says if we postpone it, it will not be done at all. Bigkeys is faster than keystats when I tried, so some users may still want to continue to use it.

> For the Redis Data Type summary difference, I did the change for readeability purpose.

Of course. I like the table, but it just the design of the heading and table header row (start with three dashes, pipe between columns) which is not a style we used elsewhere. I think it looks like some kind of markup/down syntax.

How about this style? (more graphic, less like a markup syntax)

```
Type        Total keys  Keys % Tot size Avg size  Total length/card Avg ln/card
--------- ------------ ------- -------- -------- ------------------ -----------
list             10001  20.00%    2.25G  235.39K       188538 items       18.85
string           10003  20.00%   12.73M    1.30K             11.05M       1.13K
MBbloomCF            1   0.00%    1.11K    1.11K                 -           -
...
```

For headings, instead of three dashes in the beginning of the line (like `--- TOP 10 KEY SIZES`), how about dashes in the beginning and in the end of the line (like `--- TOP 10 KEY SIZES ---`)? It is more similar to the heading in bigkeys (`------- summary -------`)."
1970524597,12826,yveslb,2024-02-29T06:58:22Z,"Your table presentation and trailing dashes are indeed looking better. Let me know if the output below is ok, and I can push a commit for that.

```
--- Top 10 key sizes ---
  1    2.04G list       ""K2G""
...

--- Top size per type ---
list       ""K2G"" is 2.04G
...

--- Top length and cardinality per type ---
list       ""list7659"" has 50 items
...

Key size Percentile Total keys
-------- ---------- -----------
   2.04G  100.0000%       50014
...
Note: 0.01% size precision, Mean: 51.98K, StdDeviation: 9.32M

Key length     Percentile Total keys
-------------- ---------- -----------
           22B  100.0000%       50014
...
Total key length is 395.09K (8B avg)

Type        Total keys  Keys % Tot size Avg size  Total length/card Avg ln/card
--------- ------------ ------- -------- -------- ------------------ -----------
list             10001  20.00%    2.25G  235.39K       188538 items       18.85
...
```

To generalize the usage of the progress bar and allowing to use `--cursor` when interrupted, I can start implementing it for `--keystats`, `--memkeys`, `--bigkeys`, `--scan`, and `--hotkeys`.
Not to break the non-interactive output, we need to only display the progress bar and cursor information in TTY, and make sure we keep the current output when redirecting to a file.

> if it's invoked like `redis-cli --scan > file`. Possibly we can display the progress bar and message on the TTY in that case, where it isn't mixed with the returned keys.

When redirecting to a file, you would like to still keep some information on the TTY.
If we take `redis-cli --memkeys > file` for instance, we will see in the terminal:

```
# Scanning the entire keyspace to find biggest keys as well as
# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec
# per 100 SCAN commands (not usually needed).

31.05% |||||||||||||||||||-----------------------------------------

Scan interrupted:
Use 'redis-cli --memkeys --cursor 11274' to restart from the last cursor.
```

And in the file:

```
# Scanning the entire keyspace to find biggest keys as well as
# average sizes per key type.  You can use -i 0.1 to sleep 0.1 sec
# per 100 SCAN commands (not usually needed).

[00.00%] Biggest zset   found so far ""zadd5838"" with 3240 bytes
[00.00%] Biggest zset   found so far ""zadd7204"" with 11419 bytes
[00.00%] Biggest list   found so far ""list7659"" with 67732 bytes
[30.50%] Biggest string found so far ""string7372"" with 72 bytes
^C
-------- summary -------

[51.77%] Sampled 25905 keys in the keyspace!
Total key length in bytes is 209634 (avg len 8.09)

Biggest   list found ""K2G"" has 2185363872 bytes
Biggest string found ""string5150"" has 6712 bytes
Biggest   hash found ""hash6292"" has 43404 bytes
...
```

If this is what you meant, I need to find out how to do that.
"
1971108689,12826,zuiderkwast,2024-02-29T13:10:55Z,"> Your table presentation and trailing dashes are indeed looking better. Let me know if the output below is ok, and I can push a commit for that.

Yes, that looks good. I'm glad you agree. :)

> When redirecting to a file, you would like to still keep some information on the TTY.
If we take redis-cli --memkeys > file for instance, we will see in the terminal

Well, probably only for `--scan`. Here the output can be a very large list of keys, which the user probably wants to parse or use for something, while it can be nice to have some progress indicator if we are still on a TTY. It's different to the bigkeys/memkeys/hotkeys feature, and I'm not even sure it's a good idea, so I guess you're right that we should to leave this part for another PR. (An idea is `if (isatty(STDERR_FILENO) && !isatty(STDOUT_FILENO)) { /* Show progress bar on stderr and print data on stdout. */ }`.)

For bigkeys/memkeys/hotkeys, I think we can simply use similar logic as you did for keystats, i.e. show the progressbar if STDOUT is a TTY; otherwise hide it.

I see you're checking `config.output == OUTPUT_STANDARD` to determine if the progress bar animation should be displayed or not. This can be messed up by `redis-cli --keystats --no-raw > file` (progressbar updates stored inside the file, which becomes huge). So I think we should instead check `isatty(STDOUT_FILENO) || getenv(""FAKETTY"")`."
1972590943,12826,yveslb,2024-03-01T06:29:27Z,"I have pushed the new format for the tables and titles.

You are right, when redirecting to a file we only need the percentage and not the progress bar. I actually show the progress bar in the file. I will remove it.

Good catch on `redis-cli --keystats --no-raw > file`, I did not think about `--no-raw`. Refreshing the output requires to be more careful. Thank you for the help, `isatty(STDOUT_FILENO) || getenv(""FAKETTY"")` works like a charm. I will test some more and check where to add the progress bar to `--memkeys`, `--bigkeys`, and `--hotkeys`, only if STDOUT is a TTY.
"
1987689440,12826,yveslb,2024-03-11T06:03:26Z,"@zuiderkwast
I updated `findBigKeys()` and `findHotKeys()` to have the progress bar in TTY and the previous output when redirecting to a file."
1990226834,12826,yveslb,2024-03-12T04:13:06Z,"
@enjoy-binbin raised two points:

> Should we check that `config.memkeys_samples` is not negative?

We should. I added some checks after using `strtoll()`.

> Do we need to check `hdr_record_value()` return value?

The function returns false if the value we are trying to insert is is negative or larger than the highest_trackable_value.
We are inserting key sizes. We use `unsigned long long` for a key size and therefore cannot be negative.
In Keystats, the highest_trackable_value is set to 1 TB. It is unlikely we will have a key that large but we rather know if we cannot insert a key size.

Speaking of not checking a return value, I realized that I do not need to check the return value of `zmalloc()` and I did it... let me know if I should remove the check.
"
1991986974,12826,yveslb,2024-03-12T15:52:18Z,"@enjoy-binbin @zuiderkwast 
I have pushed a new commit to remove the unnecessary check of zmalloc().
Let me know if there is anything else I should do.
Thank you for your help, I appreciate it."
2016979534,12826,CLAassistant,2024-03-24T23:04:45Z,[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/redis/redis?pullRequest=12826) <br/>All committers have signed the CLA.
2044116818,12826,sundb,2024-04-09T04:12:14Z,"@yveslb I submit a code style commit, please have a look."
2044148217,12826,yveslb,2024-04-09T04:57:22Z,"> @yveslb I submit a code style commit, please have a look.

Thanks for all the fix up!"
2048996010,12826,yveslb,2024-04-11T06:18:41Z,"@sundb let me know if you have more suggestions.
One comment. As suggested previously, the progress bar from `keyStats()` was added to `findBigKeys()` and `findHotKeys()`. As we did not want to break some potential automations created by the users, we kept the original output when redirecting to a file. However, this makes `findBigKeys()` and `findHotKeys()` a bit messy. I guess that's ok as we should see it from the user point of view."
2049063856,12826,sundb,2024-04-11T07:14:16Z,"@yveslb thanks, I'm now busy in other place, come back later."
862064585,8974,oranagra,2021-06-16T05:53:22Z,"@ShooterIT i must say i expected better results.
I see your table contains the value size, total db used memory, and peak COW with and without the commit.
can you add these measurements:
* the final COW
* the average COW
* duration of the fork
* ops / sec of the write traffic

to save time, let's just do that for one of the rows, not all.

Some random thoughts:
* I suspect that the fork duration is long, and the ops/sec is very high too, so we reach the peak right after the fork starts (before serializing much data).
* Maybe it is better to test this when both the master and replica use diskless replication (so that the fork is not disk-bound)
* Maybe the ops / sec throughput is not realistic and we'll get far better results with a more realistic (slower) write throughput.
* Which tool did you use to generate the traffic? was it using uniform random access or sequential? maybe memtier_benchmark with --key-pattern=G:G (Gaussian standard distribution is more correct)"
862975856,8974,ShooterIT,2021-06-17T06:47:37Z,"Yes, I agree, I think in usual realistic cases, write traffic is not always so high,  I used `redis-benchmark` to write redis, it makes keys changed quickly.

In my tests, I didn't find this commit increase dump time on disk, maybe my disk is slow, as you said, i also need to test for diskless replication.

For traffic, I will try to use `memtier_benchmark`, thanks."
877840186,8974,oranagra,2021-07-11T18:07:15Z,"I've experimented with this branch, using memtier_benchmark and bgsave (on NVMe).
* i tested payloads of 512b, obviously there was no impact (can't free a single full page).
* I tested payloads of 4096, impact was low since they allocate 5k chunks which which jemalloc stores in a run of 4 in 5 pages, so 2 out of 4 chunks can't release a single full page.
* i tested using `free` in hope the allocator will be able to release full pages eventually. it did do some good, but considerably slowed down the bgsave.
* i tested libc malloc which uses `sbrk`, on which there was no impact either.

So only posting the interesting results, for 4,000 bytes payload (which are more in line of what i expected).
The test created 10GB worth of data (some 2.5M keys), then called BGSAVE while running write-only traffic that overrides the existing keys. CoW in MB.
| test | final COW | peak COW | avg COW | bgsave time | ops/sec | notes |
|-------|----------------|----------------|---------------|-----------------|-----------------------------|:---|
| default before | 3547 | 3547 | 1849 | 10994ms | 100764| 200 clients \ 
| default after | 155 | 1433 | 718 |15264ms|102836 | doing writes|
|slow client before| 1568|1568|791|10521ms|37489 | one client \
|slow client after|152|636|422 |15337ms|31312|doing writes
|fast clients before |5079|5079|2498|10488ms|183474|200 clients with \
|fast clients after |160|2060|1240|14446ms|183002|pipeline of 10"
878368163,8974,oranagra,2021-07-12T15:19:09Z,"i can't find a way to accelerate the bgsave slowdown when using MADV_DONTNEED.
but on the bright side, it appears that the slowdown only happens when it makes a positive impact.
i.e:
1. calling `madvise` on all keys, doesn't slow down the bgsave if there's no write traffic modifying the pages on the parent process.
2. obviously this mechanism isn't causing any slowdown when the allocations are less than a page or not page aligned (i.e. when it doesn't end up calling `madvise`)"
879561891,8974,ShooterIT,2021-07-14T03:43:25Z,"The difference between Oran's tests mine is caused by our different environments, my disk is SATA that is very slow. We will have less earnings if disk is slow or network(diskless replication) is not fast, because, on heavy writing traffic, child process doesn't have enough time to release pages but all keys may be changed and CoW already happened."
882543563,8974,oranagra,2021-07-19T13:22:10Z,"I consulted @yossigo about a few concerns and here's what we concluded:
1. we think it is likely that the big savings will mostly be on string type, so a second iteration on let's say all quicklist nodes, or hash fields (dictIterator) can be wasteful (waste of time and no gain), on the other hand putting the madvise code into the rdb serialization code is ugly. one idea that came up is to add some heuristics like measuring the destination file offset before and after serializing the key, and doing that extra iteration and madvise only in case we estimate that it had potential for gain (good ratio between dictSize and the size that was serialized)
2. my paranoia about not iterating on all clients and their output buffers may be wrong (maybe it's just a few milliseconds), let's benchmark that on an extreme case (30k clients?) and see how much time it takes. even if it does take time, we can maybe apply some heuristics of looking into some global server metric to decide if that iteration is worthwhile (likely to release memory). note that unlike advising on keys / fields, which can be a waste of time if the clients aren't doing any writes, advising on client output buffers is probably worthwhile, since the clients are likely to consume these buffers while the fork is active (unless the clients are dead)"
883409996,8974,ShooterIT,2021-07-20T13:50:22Z,"> one idea that came up is to add some heuristics like measuring the destination file offset before and after serializing the key.

it seems a good idea, but there is a trouble, we can't know real size of key/value before compressing, generally, we always use lzf compression algorithms. It is safe to do extra iteration and madvise only when average field/member size is more than page size.

Client has 16 default static buffer, we will change this buffer even if there only are reading requests, and we also easily change some fields of client struct, that also triggers CoW."
883609822,8974,oranagra,2021-07-20T18:37:20Z,"regarding LZF, i know it'll not be mathematically correct, we care about the used memory, not the serialized size, but since this is just heuristics, i think it's ok.

regarding clients, as i said in my last post, it could be that i was just paranoid over nothing. let's benchmark it, we will probably find that even in extreme cases, it only slows us down by a couple of milliseconds."
885599021,8974,ShooterIT,2021-07-23T12:18:51Z,"Should we add some tcl tests for this commit? and how to check? On my machine, i added some debug logs for dismissing six data types, and make sure child process dumping RDB is right."
885878561,8974,oranagra,2021-07-23T19:51:04Z,"I don't think a test is needed to make sure the memory is released and cow is low, or test performance. For both of these we need to compare the results to a version with the advise disabled... 

The test we may wanna add is one that's makes sure all the new code is covered (reachable). Usually, in our other tests, the allocations are small, so the efficiency ifs we added cause much of our code to be skipped.. "
887286255,8974,ShooterIT,2021-07-27T07:39:50Z,"server.c and object.c lcov
[some_cover.zip](https://github.com/redis/redis/files/6883584/some_cover.zip)
"
887431009,8974,oranagra,2021-07-27T11:23:32Z,"let's add a top comment in the new test file that explains that all it does is aim to get coverage on all the ""dismiss"" methods, and check for crashes, and dump file inconsistencies.
i.e. there are not many assertions in that file, so it's not clear what it tests.

are there any other things missing from this PR?
if not, then i suppose what's left is a cleanup / code clarity review, to improve the readability of our work..
maybe some big comment above all the ""dismiss"" functions that explains what that that whole thing does and why."
888198339,8974,ShooterIT,2021-07-28T10:25:24Z,"Currently i have no new things i want to add.
I add some comments, please review, feel free to ask me to modify or supplement somethings, or you change directly."
889852049,8974,oranagra,2021-07-30T12:13:56Z,triggered full daily CI: https://github.com/redis/redis/actions/runs/1082105137
889872065,8974,oranagra,2021-07-30T12:52:38Z,"@redis/core-team please approve..
interface wise, there's only one change here, a new info metric.
please see the top comment for details, and the link it has to one of the other comments with a benchmark.
as far as we can tell, this should not slow down the BGSAVE unless it is able to release COW."
889872958,8974,oranagra,2021-07-30T12:54:20Z,"@ShooterIT can you please run another benchamrk on the latest using some complex data type, once with big members, and once with small members?"
890377092,8974,ShooterIT,2021-07-31T17:11:59Z,"I modified redis-benchmark, for every hash, just set one field
```
if (test_is_selected(""hset"")) {
    len = redisFormatCommand(&cmd,
        ""HSET myhash:__rand_int__ element %s"",data);
    benchmark(""HSET"",cmd,len);
    free(cmd);
}
```
Now my testing machine has SSD disk. redis would `bgsave` since of default save config.

For big member size, i used `./src/redis-benchmark -t hset -d 8000 -r 1000000 -n 30000000` to write redis, The result is similar with `string` type, current solution can reduce CoW without increasing much time.
|  Test | Final CoW| Peak CoW | Average CoW |Bgsave Time | QPS
| ---- | ---- | ----|---- | ---- | ----|
| Release | 71 MB | 2726 MB|972 MB| 44s | 58632|
| Not Release | 7048 MB | 7048 MB|4647 MB| 43s |55952|

For small member size, i used `./src/redis-benchmark -t hset -d 1000 -r 8000000 -n 50000000` to write redis, there is no  obvious difference since of no release actually.
|  Test | Final CoW| Peak CoW | Average CoW |Bgsave Time | QPS
| ---- | ---- | ----|---- | ---- | ----|
| Release | 7530 MB | 7530 MB| 5021 MB| 57s | 77370|
| Not Release | 7584 MB | 7584 MB|4915 MB| 57s |76648|"
890998197,8974,ShooterIT,2021-08-02T12:49:55Z,"> Do we want an option to disable this? With the latest optimization it's harder to see where it may introduce a regression but that's still possible (e.g. systems where madvise is particularly slow, etc.).

I am not sure, AFAIK, we prefer not to add more configs if possible. If systems where madvise is particularly slow, adding a config makes sense."
891001971,8974,oranagra,2021-08-02T12:55:51Z,"> Do we want an option to disable this? With the latest optimization it's harder to see where it may introduce a regression but that's still possible (e.g. systems where madvise is particularly slow, etc.).

i think releasing it in a major version with all the release candidates and careful upgrades is enough. it's completely non-user visible, and unlike `sanitize-dump-payload` we're not aware of any significant regression."
891474678,8974,ShooterIT,2021-08-03T02:50:04Z,"It absolutely is useful for dismissing replication backlog and client buffer even there only are reading traffic. For dismiss object, that really depend on the data traffic of users, it is hard to evaluate earnings. Our operational system usually found big string, or big fields/members, but the possibility is small since we may ask them to optimize big key. In current PR, it won't cost time to iterate if there is no big members of complex data type. For list type, maybe it is also helpfully, `list-max-ziplist-size` is -2 by default, means max size is 8k of internal list node, and page size is 4k generally."
891548695,8974,madolson,2021-08-03T05:45:33Z,"@ShooterIT Sure, I suppose I should rephrase my comment as there is only upside with this change. I think a lot of workloads won't see a big improvement, but it's a great improvement none the less."
892938024,8974,oranagra,2021-08-04T20:05:23Z,"merged :tada:
@ShooterIT thank you for implementing and testing it (so many years after i had this idea it became a reality)."
893121452,8974,ShooterIT,2021-08-05T02:49:58Z,"@oranagra Welcome, it is my pleasure, truly thank you for bringing this idea, and we also did more optimizations in our extensive discussion. Actually, for me, one of the biggest problems of redis always is **memory**, redis suddenly each too much memory in some cases such as memory fragmentation, CoW, client output buffer, dict rehash, we must reserve more memory to guarantee stability of service that is expensive. Now we gradually optimize them or mitigate influence of them one by one, i am really really excited, cheers 🎉"
893122068,8974,ShooterIT,2021-08-05T02:51:43Z,"BTW, less average CoW and short duration time of peak CoW also make senses, i think, since that may reduce the performance loss duration time if we enable swap/zram."
893171120,8974,oranagra,2021-08-05T05:15:10Z,"@ShooterIT i don't think i understand your last post, can you explain with more detail?"
893201944,8974,ShooterIT,2021-08-05T06:30:17Z,"Although we already reserver more than `maxmemory` memory for redis,  in some cases, we still may enable swap/zram mechanism of OS to avoid OOM since redis may suddenly cost enormous memory. In our current solution, the fork child used private memory increases first and then decreases, and peak memory duration time is short, so i think, OS would gradually reduce SWAP memory when child starts to reduce used private memory. Moreover, if there are several redis instance on one physical machine, they share the total memory, less average CoW and short duration time of peak CoW may would lessen the possibility that peak CoW memory of them happen at the same time.
@oranagra my thoughts"
893229530,8974,oranagra,2021-08-05T07:23:33Z,"ohh, OK.. 
so basically saying that this PR is great!. 8-)
and also that measuring the peak is not enough, and we can also measure the peak time, but that's hard to define, so maybe we need to add the average."
968671192,9780,oranagra,2021-11-15T09:00:02Z,"@redis/core-team please review / approve and state what's missing or needs a change.
there are a few additional minor changes that should be done on this PR before being merged, but i think the majority of the additional changes can come in a few followup PRs."
970207096,9780,eduardobr,2021-11-16T12:06:58Z,"@MeirShpilraien I noticed the functionsCtxGetCurrent gets a global value. Does it make sense that it could live within the redisDb struct? Same style as slots_to_keys, that was moved there recently.
Reason for that is that passing around, initializing, swapping and destroying server.db and tempDb + related objects is cleaner and more explicit when the related objects come together (at least things that share same life cycle).
Methods like functionsCtxSwapWithCurrent would be called or have the logic in the main function that swaps whole db."
970219703,9780,oranagra,2021-11-16T12:23:22Z,"@eduardobr the problem is that functions are global (not per db, in a multi-db server).
unlike the cluster slots mapping, which is global, but only valid in a single db configuration.
that's why we had to create `rdbLoadingCtx`, if we had that one before, maybe we would have put the slots mapping there too.

Note, i did review this code before being posted publicly, and did attempt to follow the cluster slots footsteps as much as i could.
if you still think there's a possibility for an additional cleanup, please suggest it."
977713963,9780,MeirShpilraien,2021-11-24T09:54:21Z,"@madolson thanks for the review. Fixed most of the comments (pushed a new commit) and I added comments where I needed more input about how to proceed. 

Regarding pre-load of functions (not from persistence). This is listed as first task to handle on following PR's. I am working on the design and will share in a few days. Once you (the core team) will approve the design I will work on the implementation.

Regarding this https://github.com/redis/redis/issues/8478, I have another design coming to allow better code sharing and I hope to address it (or at least suggest a possible solution). Will share soon."
980440116,9780,madolson,2021-11-26T21:23:46Z,"@MeirShpilraien My point is that I'm not convinced RDB is the right place to be storing functions, or at least we should take a deeper look into the RDB storage to think about how it interacts with other features. One example is cluster mode, when you want to split a cluster, you add N new nodes with the same configs of the other nodes and then migrate the keyspace data off for some slices. This won't include functions until we add them to configs, but then you run into the issue where they need to only be loaded on primaries and replicated (or perhaps we allow primaries and replicas to diverge?).

That's why I want to understand why it's not being built more analogous to the config file or to ACLs. The only real benefit here is that storing it in the RDB is actually really convenient (Since you can just restore the RDB to another node). But it would also be convenient for ACLs, so maybe the right answer is we should be storing them both in the RDB. (Also, I didn't actually review the code but I assume we replicate by the effects of the script and not the script itself. If that isn't the case, then having replica and primary agree on the state of a function matter more)

I suppose I would want an answer to this before I would signoff on it."
981049396,9780,MeirShpilraien,2021-11-28T09:05:19Z,"@madolson the entire point of functions is that they are replicated and persisted so you will have them on fail-overs for example (this is also how we defined it in the issue https://github.com/redis/redis/issues/8693). I do agree though that we need a way to allow users to also load them from config in case the user runs without persistence for example. Hope to publish my design for that today/tomorrow and we will be able to discuss it.

> (Also, I didn't actually review the code but I assume we replicate by the effects of the script and not the script itself. If that isn't the case, then having replica and primary agree on the state of a function matter more)

Yes, on functions we replicate only by effect."
981115848,9780,madolson,2021-11-28T16:44:29Z,"@MeirShpilraien Agree about having them available on failover, if they were stored in a config file they would also be available though."
982620284,9780,oranagra,2021-11-30T13:08:34Z,"For the record, we discussed the persistence issue in a core-team meeting today, we concluded that unlike ACL (which is more an admin related feature), Functions are an application related feature, and should be persisted together with the data.
We do need to resolve the problem with redis cluster in some way (possibly propagating them on the cluster bus, or adding some tools that will allow cluster admins to easily load them on all nodes), we will look into that in a followup PR."
982921969,9780,madolson,2021-11-30T18:53:09Z,"There was one other point in the meeting last night that was mentioned that I forgot to circle back to, which was the ephemeral use cases. Like, spinning up a cache but not using replication, we should make it easy for functions to be available there. It was mentioned that this is not common, but I disagree and think that use case is extremely common. 

I also think clusterbus propagation of functions isn't a Redis 7 feature, so we should aim for something that solves cluster + ephemeral workloads, which is probably just passing in a startup config and maybe some rewrite functionality (or listing all the functions with their descriptions) so they can be loaded on a new cluster node. 

We don't have to implement this other mechanism, but I would like to understand how it will work before we merge this PR."
984104896,9780,MeirShpilraien,2021-12-01T22:19:16Z,"Squashed all reviews commits to their relevant location, kept only the original 5 commits."
984430577,9780,oranagra,2021-12-02T09:07:57Z,triggered full CI https://github.com/redis/redis/actions/runs/1529820590
986295969,9780,MeirShpilraien,2021-12-05T20:37:19Z,Loading Functions on cluster and/or startup time is discussed here: https://github.com/redis/redis/issues/9899
987068867,9780,MeirShpilraien,2021-12-06T18:54:24Z,Code sharing on functions is discussed here: https://github.com/redis/redis/issues/9906
895924782,9309,YaacovHazan,2021-08-10T10:44:09Z,"@yossigo @oranagra @MeirShpilraien RedisModuleUserId added.
Another Q, do we need to log ACL errors (addACLLogEntry) in the context of a module (the 3 new API's and RM_Call with 'C')?"
897502002,9309,oranagra,2021-08-12T09:51:47Z,"> @yossigo @oranagra @MeirShpilraien RedisModuleUserId added.
> Another Q, do we need to log ACL errors (addACLLogEntry) in the context of a module (the 3 new API's and RM_Call with 'C')?

I think that maybe it's the responsibility of the module to log these explicitly (i.e. we need to add an API).
maybe in case the module pass a `C` flag to RM_Call, it could be done automatically, but the other 3 APIs shouldn't do that automatically, and instead the module may decide to call a 4th API.
@yossigo @madolson WDYT?"
897577036,9309,yossigo,2021-08-12T11:53:32Z,"@oranagra I agree, ACLs in modules are already explicit so logging should be the same. I think it makes sense to make the `C` flag an exception, as this already serves as a simplified alternative. "
913124665,9309,oranagra,2021-09-05T10:19:48Z,@redis/core-team please approve the new module APIs.
913910691,9309,madolson,2021-09-07T00:26:52Z,"I assume the ACL code all still works, and not really going to look into the refactoring that much. I looked through all the APIs, and had a couple of comments. "
917889966,9309,oranagra,2021-09-13T06:46:43Z,"@yossigo that's an an interesting point, but i'm not certain it's a problem or what's the solution.
the design is that as soon as ACL allows the module command to run, it can do whatever it wants, and we added these APIs for modules that do complex things (like doing a series of RM_Calls evaluating a script?)
so if a module is implementing a certain command that uses RM_Open (not RM_Call), which command name should it pass to such a check?

maybe a valid use case for that is this.
imagine a module with a command like `MOD.COMPUTE` that takes a single argument like `key1+key2/key3`.
the module command then breaks that single argument, and extracts key names from it, so it can't support the RM_IsKeysPositionRequest thing.
Then imagine an ACL rule that says that a certain user can use the MOD.COMPUTE command but only on certain key names.
then using RM_ACLCheckKeyPermissions with MOD.COMPUTE as command name makes sense?"
918644152,9309,madolson,2021-09-13T22:59:24Z,"A little bit of a tangent, but reading this conversation makes me wonder if the direction of ACL v2 is maybe not right. The initial version of ACLs gave a user access to commands and access to keys, and a user needed access to both to execute a command. This is limiting. I've heard several of our customers say that they want ""read access to ~foo* and write access to ~bar*"". read/write has nothing really to do with the underlying commands a user can execute though, what they are really asking for is control over how the data can be accessed.

So maybe what we really want to do is decompose access to keys into read/write. To use `GET FOO`, you need read access to FOO. To call `SET FOO BAR`, you need write foo. To call `APPEND FOO BAR`, you need read and write access to foo. This fits more cleanly into the module API context being described. A module wants to ask the question, ""Can the user calling me read from key X and then write to key Y"", and if so, they can go and execute the command. "
918859221,9309,oranagra,2021-09-14T06:47:32Z,"this is an interesting observation. good thing you caught it before we merged both of these PRs.
it actually sits nicely with the read and write flags we intend to add to the key-specs: https://github.com/redis/redis/pull/8324#issuecomment-768077667

so this is really a last minute change.
to adjust the API in this PR for this idea, we only need to add a read/write flag to RM_ACLCheckKeyPermissions (instead of the command name i suggested to add in my previous comment).
however, adjusting the ACL roles PR will be a lot of work, right?"
919941645,9309,yossigo,2021-09-15T11:38:16Z,"@madolson Are you suggesting to completely drop command reference from ACLs?

The classic access control model deals with subjects, objects and operations. To me, using read/write is just another way simplified way to describe operations, which can be considered complementary to commands.

I definitely see how it's going to be super useful, but I'm not sure it can substitute commands for a few reasons:
* Backwards compatibility
* Restriction of administrative commands (e.g. `FLUSHALL`)
* Other nuances - e.g. allow write but disallow `DEL`, append-only lists, etc."
920078122,9309,oranagra,2021-09-15T14:36:29Z,"since this PR is applied on what's currently in unstable, i suggest to keep that API as is (no command name), and merge it.
i.e. that's the right API for ACLv1.
then in the PR that adds roles, we can modify that API and add another argument (since it wasn't released yet).

i.e. we'll discuss roles / ACLv2 in the other PR, and do any necessary adjustments."
920333589,9309,madolson,2021-09-15T19:58:16Z," I agree with Oran that we should close on the ACL V2 path, and we can decide if this API doesn't make sense at that point.

@yossigo Going to respond here just for continuity, but also going to paste the same comment on the main ACL thread. I'm not saying we should drop commands, I'm saying that instead of using policies/complex access controls we instead make more complex ways to describe access to keys. An ACL V2 access control might instead look like:
```
USER +@all -@dangerous (~readonly::* )~writeonly::* ~::readwrite*   
```

Where (~ describes the key spaces that have readonly access, )~ describes key spaces that have write only access, and ~ describes keyspaces that have both read and write access.

In this model, a user would be able to execute:
```
GET readonly::foo
SET writeonly::foo bar
APPEND readwrite::foo
```

but not
```
GET writeonly::foo
SET readonly::foo bar
```

The thinking is that it might be easier for end users to reason about key access as opposed to what operations can be accessed on which keys. "
922480414,9309,YaacovHazan,2021-09-19T14:12:58Z,"Following the comment of @madolson (thanks) about that the new API's will fail against users created by a Module (RM_CreateModuleUser()), I had a discussion with Oran and Yossi and we decide to get rid of the RedisModuleUserID.

All the new API's will get a RedisModuleUser as an input.
For a user that the Module created, the RedisModuleUser already exists as a return value of RM_CreateModuleUser. But for a general ACL user, the Module will obtain it by two new API's:
RM_GetCurrentUserName - Retrieve the user name of the client connection behind the current context.
RM_GetModuleUserFromUserName - Get a RedisModuleUser from a user name

In the case of general ACL user (RM_GetModuleUserFromUserName), the underlying user pointer in RedisModuleUser, is a reference for an existing ACL user from the global ACL users list (Users), and thus can be used only in the context where the RedisModuleUser obtained (the ACL user can be deleted at any time).

Another point is that, now that a Module can get RedisModuleUser also for a general ACL user, he can use it with the already existing API RM_SetModuleUserACL(), to change the rules of such user.

@madolson @sundb with all these changes, please take a second look"
922644519,9309,oranagra,2021-09-20T05:19:26Z,triggered daily CI on unit/acl and moduleapi: https://github.com/redis/redis/actions/runs/1252369957 (passed)
922984054,9309,oranagra,2021-09-20T14:31:57Z,"I just realized that we already have 
```c
RedisModuleString *RM_GetClientUserNameById(RedisModuleCtx *ctx, uint64_t id);
unsigned long long RM_GetClientId(RedisModuleCtx *ctx) ;
```

So I guess this makes `RM_GetCurrentUserName` excessive. "
924360230,9309,madolson,2021-09-21T20:29:59Z,"Having RM_GetCurrentUserName seems fine to me, it seems like enough of convenience function and it's a primitive we want to expose."
993295537,9940,tezc,2021-12-14T08:32:10Z," ```c
#define _XOPEN_SOURCE 600
#define REDISMODULE_EXPERIMENTAL_API
#include ""../redismodule.h""
#include <stdio.h>
#include <stdlib.h>
#include <pthread.h>
#include <unistd.h>
#include <string.h>

pthread_t tid;
pthread_mutex_t mtx = PTHREAD_MUTEX_INITIALIZER;
RedisModuleBlockedClient *clients[1024];
int clientCount;

#define MAX_CLIENTS (sizeof(clients) / sizeof(clients[0]))


void *HelloBlockPerf_ThreadMain(void *arg) {
    (void) arg;
    int count = 0;
    RedisModuleBlockedClient *bc;
    RedisModuleBlockedClient *cl[1024];

    while (1) {
        pthread_mutex_lock(&mtx);
        if (clientCount == 0) {
            pthread_mutex_unlock(&mtx);
            usleep(1);
            continue;
        }

        count = clientCount;
        memcpy(cl, clients, sizeof(clients[0]) * clientCount);
        clientCount = 0;
        pthread_mutex_unlock(&mtx);

        for (int i = 0; i < count; i++) {
            bc = cl[i];
            RedisModuleCtx *ctx = RedisModule_GetThreadSafeContext(bc);
            RedisModule_ReplyWithSimpleString(ctx, ""HELLO"");
            RedisModule_FreeThreadSafeContext(ctx);
            RedisModule_UnblockClient(bc, NULL);
        }

    }

    return NULL;
}


int HelloBlockPerf_Command(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {
    (void) argv;
    if (argc != 1) return RedisModule_WrongArity(ctx);

    RedisModuleBlockedClient *bc = RedisModule_BlockClient(ctx,NULL,NULL,NULL,0);
retry:
    pthread_mutex_lock(&mtx);
    if (clientCount == MAX_CLIENTS) {
        pthread_mutex_unlock(&mtx);
        goto retry;
    }
    clients[clientCount++] = bc;
    pthread_mutex_unlock(&mtx);

    return REDISMODULE_OK;
}


int RedisModule_OnLoad(RedisModuleCtx *ctx, RedisModuleString **argv, int argc) {
    REDISMODULE_NOT_USED(argv);
    REDISMODULE_NOT_USED(argc);

    if (RedisModule_Init(ctx,""helloblockperf"",1,REDISMODULE_APIVER_1)
                         == REDISMODULE_ERR) {
        return REDISMODULE_ERR;
    }

    if (RedisModule_CreateCommand(ctx,""helloblockperf.hello"", HelloBlockPerf_Command,
                                  """",0,0,0) == REDISMODULE_ERR) {
        return REDISMODULE_ERR;
    }

    if (pthread_create(&tid,NULL,HelloBlockPerf_ThreadMain,NULL) != 0) {
        return REDISMODULE_ERR;
    }

    return REDISMODULE_OK;
}
```

Using the module above, quick bench on my local : 
memtier_benchmark -t 2 --command=helloblockperf.hello

Before PR : 193187 ops/sec, 0.52 msec latency
After PR : 245091 ops/sec, 0.41 msec latency"
997641908,9940,tezc,2021-12-20T06:45:43Z,"@oranagra @yossigo Regarding your comments, created a generic function `clearClient()`  to clear the client state. Hopefully it does it properly, please take a look. 

Also, we have couple of other temp client usages in module.c (`moduleFreeContextReusedClient` and `server.module_client`) . I've changed these to use same temp client cache. Pros : single place to handle temp clients, simplifies code a bit, probably more secure as we clear it properly. Cons: a bit more work to alloc/release temp client compared to current version. Let me know if this makes sense or if you think no need for that? "
1000112782,9940,MeirShpilraien,2021-12-23T07:55:43Z,"Maybe I am wrong but from what I understand we need the mutex on the client pool only because it is used from within the RM_GetThreadSafeContext which can be called without the Redis GIL. Maybe we can set the client on the RedisModuleBlockClient when we create it (which must be when Redis GIL is takes) and then just use it when we create the ThreadSafeCtx. In case the ThreadSafeCtx is created without BlockedClient we can just create a client for it (and not use the pool), I guess that in such case the ThreadSafeCtx can be (and should be) reused by the module anyway no?
Let me know what you think."
1000122945,9940,tezc,2021-12-23T08:15:46Z,"@MeirShpilraien IIUC, you are suggesting allocating another client instance on `RedisModuleBlockClient()` to be used for a potential `RM_GetThreadSafeContext()` call.  I asked this but looks like `RM_GetThreadSafeContext()` can be called multiple times. At least, this is not strictly prohibited? I don't know if anybody is doing this though."
1000128740,9940,MeirShpilraien,2021-12-23T08:26:28Z,"I believe that if its created multiple time than we can allocate the client (and not use the pool), I guess we want to optimise the common use-case and I believe that creating multiple ThreadSafeCtx from the same BlockedClient is supper rare (maybe not even exits?)"
1000209433,9940,tezc,2021-12-23T10:44:01Z,"@MeirShpilraien  We'll need an atomic variable to detect multiple ThreadSafeCtxs if I'm not mistaken. I feel like current cache with mutex will be uncontended and should perform similar to this change. (Maybe I'm wrong but I don't expect contention on the mutex). 

Just another question, similar to your suggestion or same  :

https://github.com/redis/redis/blob/63f606d3e3fa8196f98627a1c3924179811d07a1/src/module.c#L1848

For ThreadSafeCtxs of blocked clients, we are always returning `ctx->blocked_client->reply_client`. So, we can assume one cannot create multiple ThreadSafeCtxs and use them in different threads etc. (Sorry my previous reply says the opposite). It can be created multiple times in the same thread, it is okay. 

```c
RedisModuleCtx *RM_GetThreadSafeContext(RedisModuleBlockedClient *bc) {
    RedisModuleCtx *ctx = zmalloc(sizeof(*ctx));
    RedisModule *module = bc ? bc->module : NULL;
    moduleCreateContext(ctx, module, REDISMODULE_CTX_THREAD_SAFE);

    /* Even when the context is associated with a blocked client, we can't
     * access it safely from another thread, so we create a fake client here
     * in order to keep things like the currently selected database and similar
     * things. */

    if (bc) {
        ctx->blocked_client = bc;
        ctx->client = bc->client;   // Use same client, no need for a new one? 
    } else {
        ctx->blocked_client = NULL;
        ctx->client = createClient(NULL);
    }
    return ctx;
}
```
We can make this change. If bc is not null, we can use `ctx->blocked_client` as `ctx->client`, no need for a new client as ReplyWithXXX api always return bc->client anyway. In this way, client object cache will be local to redis main thread, no need for mutexes. 

Is this correct? 
"
1000217377,9940,MeirShpilraien,2021-12-23T10:58:56Z,"So IIUC `bc->client` is the actual client that sends the command so with your change, if inside the thread we will use `RedisModule_SelectDb` with the `threadSafeCtx` generated with the `BlockClient`, we will change the db the original client works with (this is not what would happened today so I guess it can be considered a breaking change?).
But I do agree with your claim that creating ThreadSafeCtx from 2 threads simultaneously is not safe even today so my opinion is that it should not be done and then we can go with the original suggestion? (putting another client on the BlockedClient and use it?)
@yossigo @oranagra WDYT?"
1000232369,9940,tezc,2021-12-23T11:15:53Z,"Hmm, so people can call functions other than `ReplyWith..` :(
Yeah, your suggestion is the best option if we can assume `GetThreadSafeContext()` is not multi thread-safe. It can be called/used with a single thread only, not many. "
1000270684,9940,oranagra,2021-12-23T12:26:26Z,"haven't followed this discussion, but had a short talk to Meir for an update.
what i understand is:
1. GetThreadSafeContext can be used by multiple threads in parallel, and it's likely to assume each thread will create just one, or maybe a few at startup, so their creation may not be expensive compared to the creation of the thread (so maybe there's no need for the pool here?).
2. GetThreadSafeContext on a specific blocked client, is only allowed from one thread (per blocked client)
3. blocked client can be used without a thread (replying from the unblocked callback after being released by another command.
4. we don't wanna lock another mutex in the main thread (while the GIL is locked) when we do RM_Call.
5. maybe the way around it is that we pre-""allocate"" a fake client (from the pool) when we create the blocked client (rather than do that when we create the thread safe context, this way we always take things from the pool when the GIL is locked, so there's no need for that mutex.

did i get this right?"
1000372648,9940,tezc,2021-12-23T15:21:04Z,"Yeah, seems correct. The question is case 2. User can call GetThreadSafeContext() from different threads for the same blocked client. It just won't work as there is no proper synchronization for these threads (All threads will use same client object without a mutex). So, currently, assumption is GetThreadSafeContext() should not be called from the different threads for the same blocked client. So, if we are okay with this assumption, we can implement case 5. "
1000421467,9940,oranagra,2021-12-23T16:51:27Z,"i think we're ok with this assumption, and we can clear that up in documentation if we want..
modules can do worse thing if they want, we don't attempt to block them from doing silly things, except for cases that are really slippery. (where common sense will lead you the wrong way)"
1000430387,9940,tezc,2021-12-23T17:09:28Z,"Ok, I'll update the PR"
1001098733,9940,tezc,2021-12-26T02:38:28Z,"Rebased and updated the PR, RM_GetThreadSafeCtx() will now use a temp client from blocked client. "
1009759291,9940,tezc,2022-01-11T09:31:40Z,"@oranagra Changed code a bit to free clients faster(I'm open to suggestions if you have a better idea). There is now `moduleTempClientMinCount` which is the number of unused clients in the pool for a cron period. Still, even all these clients are unused, I'd like to keep at least a few clients in the pool if this is okay. If requests are coming with an interval, we'll need to allocate clients, it may hurt latency. Currently, min count is 16, I can lower it to 8 or 4 if this is too many. "
1239762568,11248,yossigo,2022-09-07T18:56:13Z,@uvletter Did you consider the alternative of using `O_DIRECT` so written data doesn't occupy the buffer cache in the first place? 
1240986256,11248,uvletter,2022-09-08T17:06:12Z,"@yossigo Yes I discussed this problem with the guys in my company before the PR, our thoughts is direct IO is somehow more complex for use, you must align the buffer and file offset, and fill the buffer to the multiple of sector size, but the performance may be even poorer than buffered IO, for eliminating the page cache `fadvise` may be a more easy way. I also find the default option for mysql innodb and rocksdb to write log file is also with plain buffered write. I simply tested the two modes in my dev ec with fio(but not garantee the parameters matching the workload accurately since I dont have much time to dive into it...)
```
# DIRECT IO
$ fio --filename=/home/admin/tmp --size=10m --ioengine=sync --iodepth=32 --numjobs=1 --rw=write --buffered=0 --direct=1 --fsync=1 --bs=4k --name=job1
job1: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=sync, iodepth=32
fio-3.25
Starting 1 process
job1: Laying out IO file (1 file / 10MiB)
Jobs: 1 (f=1): [W(1)][100.0%][w=1604KiB/s][w=401 IOPS][eta 00m:00s]
job1: (groupid=0, jobs=1): err= 0: pid=66679: Thu Sep  8 16:32:47 2022
  write: IOPS=391, BW=1567KiB/s (1605kB/s)(10.0MiB/6534msec); 0 zone resets
    clat (usec): min=484, max=8044, avg=738.23, stdev=215.15
     lat (usec): min=485, max=8045, avg=739.21, stdev=215.16
    clat percentiles (usec):
     |  1.00th=[  545],  5.00th=[  578], 10.00th=[  594], 20.00th=[  627],
     | 30.00th=[  660], 40.00th=[  685], 50.00th=[  717], 60.00th=[  750],
     | 70.00th=[  783], 80.00th=[  816], 90.00th=[  865], 95.00th=[  930],
     | 99.00th=[ 1237], 99.50th=[ 1418], 99.90th=[ 2278], 99.95th=[ 4424],
     | 99.99th=[ 8029]
   bw (  KiB/s): min= 1504, max= 1640, per=100.00%, avg=1568.00, stdev=37.24, samples=13
   iops        : min=  376, max=  410, avg=392.00, stdev= 9.31, samples=13
  lat (usec)   : 500=0.04%, 750=60.74%, 1000=36.05%
  lat (msec)   : 2=2.89%, 4=0.20%, 10=0.08%
  fsync/fdatasync/sync_file_range:
    sync (usec): min=1234, max=10963, avg=1806.15, stdev=456.71
    sync percentiles (usec):
     |  1.00th=[ 1336],  5.00th=[ 1418], 10.00th=[ 1467], 20.00th=[ 1565],
     | 30.00th=[ 1631], 40.00th=[ 1696], 50.00th=[ 1778], 60.00th=[ 1844],
     | 70.00th=[ 1909], 80.00th=[ 1975], 90.00th=[ 2073], 95.00th=[ 2180],
     | 99.00th=[ 3130], 99.50th=[ 3884], 99.90th=[ 8717], 99.95th=[ 9634],
     | 99.99th=[10945]
  cpu          : usr=0.52%, sys=1.42%, ctx=7686, majf=0, minf=12
  IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2560,0,2559 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=1567KiB/s (1605kB/s), 1567KiB/s-1567KiB/s (1605kB/s-1605kB/s), io=10.0MiB (10.5MB), run=6534-6534msec

Disk stats (read/write):
  xvda: ios=0/7514, merge=0/2510, ticks=0/6248, in_queue=6248, util=98.54%

# BUFFERED IO
$ fio --filename=/home/admin/tmp --size=10m --ioengine=sync --iodepth=32 --numjobs=1 --rw=write --buffered=1 --direct=0 --fsync=1 --bs=4k --name=job2
job2: (g=0): rw=write, bs=(R) 4096B-4096B, (W) 4096B-4096B, (T) 4096B-4096B, ioengine=sync, iodepth=32
fio-3.25
Starting 1 process
Jobs: 1 (f=1): [W(1)][100.0%][w=2706KiB/s][w=676 IOPS][eta 00m:00s]
job2: (groupid=0, jobs=1): err= 0: pid=66748: Thu Sep  8 16:35:29 2022
  write: IOPS=696, BW=2787KiB/s (2854kB/s)(10.0MiB/3674msec); 0 zone resets
    clat (nsec): min=5233, max=43646, avg=9043.62, stdev=2810.80
     lat (nsec): min=5994, max=44574, avg=9989.31, stdev=2948.80
    clat percentiles (nsec):
     |  1.00th=[ 5536],  5.00th=[ 6048], 10.00th=[ 6304], 20.00th=[ 7008],
     | 30.00th=[ 7648], 40.00th=[ 8384], 50.00th=[ 8768], 60.00th=[ 9152],
     | 70.00th=[ 9536], 80.00th=[10304], 90.00th=[11456], 95.00th=[13504],
     | 99.00th=[20352], 99.50th=[23168], 99.90th=[31872], 99.95th=[36608],
     | 99.99th=[43776]
   bw (  KiB/s): min= 2632, max= 3056, per=100.00%, avg=2812.57, stdev=177.62, samples=7
   iops        : min=  658, max=  764, avg=703.14, stdev=44.41, samples=7
  lat (usec)   : 10=76.05%, 20=22.81%, 50=1.13%
  fsync/fdatasync/sync_file_range:
    sync (usec): min=497, max=9211, avg=1419.31, stdev=932.26
    sync percentiles (usec):
     |  1.00th=[  545],  5.00th=[  586], 10.00th=[  619], 20.00th=[  668],
     | 30.00th=[  725], 40.00th=[  791], 50.00th=[  857], 60.00th=[ 1188],
     | 70.00th=[ 2245], 80.00th=[ 2442], 90.00th=[ 2671], 95.00th=[ 2802],
     | 99.00th=[ 3326], 99.50th=[ 4490], 99.90th=[ 7963], 99.95th=[ 8848],
     | 99.99th=[ 9241]
  cpu          : usr=0.76%, sys=2.12%, ctx=4388, majf=0, minf=13
  IO depths    : 1=200.0%, 2=0.0%, 4=0.0%, 8=0.0%, 16=0.0%, 32=0.0%, >=64=0.0%
     submit    : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     complete  : 0=0.0%, 4=100.0%, 8=0.0%, 16=0.0%, 32=0.0%, 64=0.0%, >=64=0.0%
     issued rwts: total=0,2560,0,2559 short=0,0,0,0 dropped=0,0,0,0
     latency   : target=0, window=0, percentile=100.00%, depth=32

Run status group 0 (all jobs):
  WRITE: bw=2787KiB/s (2854kB/s), 2787KiB/s-2787KiB/s (2854kB/s-2854kB/s), io=10.0MiB (10.5MB), run=3674-3674msec

Disk stats (read/write):
  xvda: ios=0/4356, merge=0/906, ticks=0/3553, in_queue=3552, util=97.46%
```
The result shows the buffered write is more faster."
1246139695,11248,pizhenwei,2022-09-14T02:09:31Z,"> @uvletter Did you consider the alternative of using `O_DIRECT` so written data doesn't occupy the buffer cache in the first place?

Hi @yossigo @uvletter 
I'm quite familiar with the Linux I/O subsystem. In my experience, `O_DIRECT` is not suitable for Redis:
* During dumping into db file, Redis iterates all the KV and write the KV into db file one by one, this means Redis call lots of write syscall. I also tested this by `strace -ff -p PID` during `BGSAVE`, the test result also confirms this.
* The typical IOPS of 4K random write: HDD 150(avg lat 10ms), SSD 50K(agv lat 200us), NVMe 100K(agv lat 20us), This means that direct IO slows down the speed of writing db file, especially on a HDD.
* To improve the performance of this scenario, using buffered IO(page cache, but not buffer cache, they are different on linux) is suitable. Also to reduce the page cache by `fadvise` is a good choice, this is better than kernel reclaims. Because a user process knows which file(or part of file) will not be used recently, but the kernel has to scan all the pages of cache by LRU. So from the point of my view, this patch is reasonable."
1246261074,11248,uvletter,2022-09-14T05:38:31Z,"@pizhenwei Thank you for your informations! I have seen your post in the forum of ByteDance, and heard your speech about 
our works on `htop`, I'm a big fan of you aha."
1246282955,11248,uvletter,2022-09-14T06:03:17Z,"About your comments I also want to add some solution/ideas:
- we can buffer the KV into in the user space buffer, and `write` data to kernel when buffer is full, with this we could reduce the frequency of `write`, But I'm suspicious even so the direct IO is slower than the normal IO as the `fio` test shows.
- RDB(or AOF) need to guarantee the duration of the data, so a `fsync` is needed with either direct IO or buffered IO, so the cost flushing data to disk maybe is unavoidable...
- I can't agree more, this is the main reason of the patch. We found a performance thrash when the memory is scarce and kernel reclaim thread start to work in out production environment(a bare metal machine with tens of redis instances deployed on it). I don't figure out the details why kernel reclaiming memory would slow down the service, but I think with this patch we could reduce the memory kernel will scan and reclaim."
1246367531,11248,pizhenwei,2022-09-14T07:38:19Z,"> 



> About your comments I also want to add some solution/ideas:
> 
> * we can buffer the KV into in the user space buffer, and `write` data to kernel when buffer is full, with this we could reduce the frequency of `write`, But I'm suspicious even so the direct IO is slower than the normal IO as the `fio` test shows.
> * RDB(or AOF) need to guarantee the duration of the data, so a `fsync` is needed with either direct IO or buffered IO, so the cost flushing data to disk maybe is unavoidable...
> * I can't agree more, this is the main reason of the patch. We found a performance thrash when the memory is scarce and kernel reclaim thread start to work in out production environment(a bare metal machine with tens of redis instances deployed on it). I don't figure out the details why kernel reclaiming memory would slow down the service, but I think with this patch we could reduce the memory kernel will scan and reclaim.

Because the kernel maintains memory zones, and there is a `watermark` of each memory zone, When the *free memory* gets less than the watermark, the memory allocation on this zone needs to wait reclaim done. Once a Redis process hits page fault and  allocates page, it has to be uninterruptable state(D state in `ps aux`), wait the memory reclaim by kswapd, then continue to run. This is the reason of latency spike.
Using `fadivse` to reclaim page cache, this syscall gives a hint to kernel to drop some page cache. It's helpful to keep the free memory more than watermark to avoid kernel memory reclaim."
1350418697,11248,uvletter,2022-12-14T05:27:41Z,"Add more context. The page cache issues may happen in old kernel like 3.10, before which [watermark_scale_factor](https://lore.kernel.org/lkml/1455813719-2395-1-git-send-email-hannes@cmpxchg.org/) is not involved. As the space between low watermark and min watermark is quite small, usually kswapd is too late to wake up before trigger a direct reclaim, which would stall the whole memory allocating process. This can be observe more obviously via /proc/vmstat, `allocstall_normal` has a significant increment.

As for the more modern kernel, like 4.18, this will not be a problem. low watermark is far away from min watermark so kswapd can work as expect to avoid direct reclaim.

We find this problem when we try to upgrade the redis cluster, with the RDB generated, transformed and persisted, which occupy lots of page cache, the `MemFree` exhausts.

This pr could not resolve the direct reclaim issue ultimately in old kernel, it only slows down the process. A side effect is it can make `MemFree` looks more,  and offload the work of kswapd.

If this proposal is adopted, maybe AOF cache should also be addressed, which this pr doesn't cover."
1351785199,11248,uvletter,2022-12-14T17:08:16Z,"> did you mean that this has no value in newer kernels? 

Yes, we noticed that in newer kernels the stall involved by direct reclaim doesn't occur in our production environment,  As [watermark_scale_factor](https://lore.kernel.org/lkml/1455813719-2395-1-git-send-email-hannes@cmpxchg.org/) would guarantee kswapd begin to work before the free memory reaching low watermark and direct reclaim. But I think it still have some values in newer kernels, firstly the program knows better which cache should be evicted, in contrary kswapd maintains two LRU lists for all pages, and scan them to find out which page could be reclaimed, so active reclaim is more efficient than passive reclaim in theory. Secondly, it can make more `free` and less `cache` in memory stats like `free` command or `/proc/meminfo`(though I'm not sure it's value..)

> or just that it doesn't fully solve the problem on old ones?
In my test `fadvise` cannot completely guarantee the cache backed by file is evicted, it's a hint to kernel. Besides, there may be other processes in the host machine which produce cache, when cache grow up and free memory exhausts, the stall still will happen finally."
1352449148,11248,pizhenwei,2022-12-15T01:44:43Z,"> > did you mean that this has no value in newer kernels?
> 
> Yes, we noticed that in newer kernels the stall involved by direct reclaim doesn't occur in our production environment, As [watermark_scale_factor](https://lore.kernel.org/lkml/1455813719-2395-1-git-send-email-hannes@cmpxchg.org/) would guarantee kswapd begin to work before the free memory reaching low watermark and direct reclaim. But I think it still have some values in newer kernels, firstly the program knows better which cache should be evicted, in contrary kswapd maintains two LRU lists for all pages, and scan them to find out which page could be reclaimed, so active reclaim is more efficient than passive reclaim in theory. Secondly, it can make more `free` and less `cache` in memory stats like `free` command or `/proc/meminfo`(though I'm not sure it's value..)
> 
> > or just that it doesn't fully solve the problem on old ones?
> > In my test `fadvise` cannot completely guarantee the cache backed by file is evicted, it's a hint to kernel. Besides, there may be other processes in the host machine which produce cache, when cache grow up and free memory exhausts, the stall still will happen finally.

I suppose that this PR has no dependence on `watermark_scale_factor`(but yes, this kernel parameter help to keep more free memory), so let's focus on `fadvise` only. `fadvise` is a hint(not a command), we can tell the kernel to try to reclaim the specific page cache(because we know which file we will *not* access recently) rather than kernel scans the full LRU blendly. This leads more effective memory reclaim."
1352632338,11248,oranagra,2022-12-15T06:52:36Z,"so in that light. what do you think about my comments to incrementally advise (free pages) as we go rather than at the end?

i remember an old project i worked on (kernel 2.6.17 with not a lot of RAM), i had to fadvise incrementally while copying large files in order to avoid disruptions (executable code being evicted from page cache).

but maybe if in our case, the excess system calls would cause more harm than good."
1352663805,11248,pizhenwei,2022-12-15T07:30:51Z,"> so in that light. what do you think about my comments to incrementally advise (free pages) as we go rather than at the end?
> 
> i remember an old project i worked on (kernel 2.6.17 with not a lot of RAM), i had to fadvise incrementally while copying large files in order to avoid disruptions (executable code being evicted from page cache).
> 
> but maybe if in our case, the excess system calls would cause more harm than good.

From the point of my view, your suggestion incrementally advise (free pages) seems better.
In my experience, I optimized a log library several years ago: use fadvise to reclaim page cache every 1M, it works fine until today. If so, don't worry about the performance of the handful additional syscalls. (A single CPU can do 1M+ syscalls per second on a modern x86 server)"
1353313922,11248,uvletter,2022-12-15T15:58:58Z,"I also agree the incremental fadvise, in my implementation I found the fadvise in one go may block for some time if the file is huge enough, so I put it into bio, but with incremental fadvise we could amortize the work to many fadvise, so no background fadvise is needed, it's pretty cool!

I tried to find some inspiration in RocksDB just now, RocksDB also range_sync and invalidate cache every 1MB. I find the way sync_file_range used in RocksDB seems more clear and proper over the one in our current implementation, so I wanna port it in next pr as a following of #11150"
1353394970,11248,uvletter,2022-12-15T16:53:16Z,"Oops... it seems like background fadvise is required. The incremental reclaim is feasible for write cache, but a little messy for read cache, so I would rather do it in one go in the background.

BTW I have solved all the comments, thanks for the suggestions! @oranagra "
1402311535,11248,uvletter,2023-01-24T17:22:38Z,"> We should also consider adding at least some basic smoke tests to validate this mechanism.

@yossigo I think the suggestion is fair, and I add a github CI action to test it, but I'm not familiar with it, so please help me have a review!
"
1404868413,11248,yossigo,2023-01-26T11:21:04Z,"@uvletter I'm sorry if I wasn't clear, I don't think we need a test for the `fadvise` detection, but for the actual implementation."
1407592178,11248,oranagra,2023-01-29T08:00:33Z,"@uvletter please avoid force-pushes, it makes it harder to do incremental review.
since we're gonna squash-merge this into one command, it's ok to just add more and more commits instead of amending existing ones.

it also looks like github doesn't wanna tell me what's the last revision i reviewed, can you please tell me what's new?"
1407606369,11248,uvletter,2023-01-29T09:11:05Z,"> @uvletter please avoid force-pushes, it makes it harder to do incremental review. since we're gonna squash-merge this into one command, it's ok to just add more and more commits instead of amending existing ones.
> 
> it also looks like github doesn't wanna tell me what's the last revision i reviewed, can you please tell me what's new?

I didn't rewrite the historical commit, I just `git rebase origin/unstable` to solve the conflict between this branch and the trunk, but github would rewrite the timeline and regard all commits as newly commited ones... [Revert ""add ci for compile-time detection""](https://github.com/redis/redis/pull/11248/commits/f1600632a674769a0b37a8542bbaf262aec6bb4a) is the new one."
1411834381,11248,enjoy-binbin,2023-02-01T10:33:17Z,"> I didn't rewrite the historical commit, I just git rebase origin/unstable to solve the conflict between this branch and the trunk, but github would rewrite the timeline and regard all commits as newly commited ones..

@uvletter yes, rebase always requires a force-push, in this case, you should use merge, like `git merge origin/unstable`"
1411845076,11248,oranagra,2023-02-01T10:41:44Z,"@uvletter in your redis fork, you should have an ""actions"" tab where you'll be able to ""dispatch"" the daily workflow with some filters.
the other alternative, is to just modify CI.yml temporarily (it has the `push` trigger), push into a temporary branch and watch the actions there.
let me know if you have some difficulties with this and i'll help."
1420725762,11248,uvletter,2023-02-07T12:54:55Z,"@enjoy-binbin thanks for you explanation. `git rebase` is the convention in our company, but seems it's not compatible to github's pull.

@oranagra I finally figure out the test in github action, it's quite hard to debug... after tens of fix-gitpush-trigger-checklog circles, I finally got a relatively stable version. It involves three cases: 1. rdb save 2. replication 3. restart, and check the cache backed by rdb files, as well as the overall cache in the host. By now I generate about 1GB data, and the check threshold for overall cache is 500KB."
1422181231,11248,oranagra,2023-02-08T07:57:59Z,"@uvletter can you please update the top comment with details on everything we ended up doing (i.e. in which cases we reclaim gradually, in which we reclaim at the end).
and also any interesting things we can mention about the implementation and certain concerns (like platform support, logging, don't remember what else).."
1435950573,11248,oranagra,2023-02-19T10:28:56Z,"@uvletter i made a PR to stabilize this test, please take a look
https://github.com/redis/redis/pull/11818"
776367951,8474,madolson,2021-02-10T01:37:32Z,"@redis/core-team Ok, probably time for a wider review. Nan outlined the two major additions in the top comment. The first one is more should be mostly agreed upon. The second item is just a mechanism to test that the absolute timestamp is happening correctly, the current implementation minimizes the amount of code."
776921884,8474,oranagra,2021-02-10T18:34:41Z,"i'm sorry, i didn't follow the discussion in this PR and issue (wasn't aware of them).
since this goes against past decision, i would like to study both the correspondence here (didn't read it yet), and the past discussions in other issues and PRs (where Salvatore responded).
however i'm currently a bit short in time, and and in any case i'm not sure if we wanna merge such a change to 6.2 (already past it's last RC). since this is not an urgent change, i wanna propose that we'll hold this for a future version.
does this make sense? i might be overthinking of this, but why take the risk?"
777145941,8474,madolson,2021-02-11T01:10:31Z,"@oranagra, I was under the impression this would not be in 6.2 given that we said only bug fixes for the last RC. You can take as much time as you need to think it through :)"
797827051,8474,QuChen88,2021-03-13T00:15:45Z,"@oranagra I just read through the other related issues on this topic #8433 and #5171. I am not entirely convinced about the original argument that Salvatore was making that the TTL should be the relative time since the moment key was written onto the redis server which is subject to the replication delay on the replica. From the point of view of the Redis user, I am writing a key with an TTL onto the master, and the expectation is that the key should be expired after that time duration from this moment on. So I also lean towards having the replica attempt to expire the item at the same point in time as the master regardless of the replication lag between master and replica. 

I just looked at this PR from @ny0312 and I think the implementation looks good mostly. I raised a few minor comments for him namely around introducing the new `expiretime` command. Please take a look. Thanks!"
800922841,8474,oranagra,2021-03-17T09:12:40Z,"Sorry for the delay, also sorry to be the last to join the party.

TL:DR i'm voting in favor of this PR (didn't read the code yet).

The main reason is that we're trying to fight two contradicting issues here:
1. a significant clock de-sync between master and replica. i'm not sure such a thing really exists, and if there is, there are ways to fix it (NTP and others).
2. significant replication delays (either due to PSYNC reading from the backlog, or due to large replica-buffers accumulated during full sync). these are very real, and there's no easy way to mitigate them.

Item 2 can also be said to be affected by trivial network latency, but that's not in the same ballpark of the other two replication delay issues.

On top of that there's the concern of code simplicity and consistency, and it would certainly be better to replicate the same to AOF and replicas, and have the same type of (absolute) value in full-sync and replication command stream.

I don't agree with some of the arguments that were posted, but the bottom line is that these two concerns contradict each other, and one of these concerns can be eliminated while the other can't (at least not easily).

Now that we seem to agree about this change we need to decide when it's safe to merge. if we conclude that we don't wanna release this change in 6.2.x, it would be better to leave it out of unstable for a while, so that unstable doesn't diverge too far from 6.2, making it harder to cherry pick bug fixes.

------------
Footnotes:

I think what matters here is actually the wall-clock of the client machine. Requesting a key to expire in 3 minutes means 3 minutes since the command was sent, or since the time it was processed by the master (the client machine's wall-clock at that time).

And as Salvattore mentioned if the replica clock is 2 minutes ahead of master's clock, and the client is using the replica to accelerate reads, from the client's perspective, the key will appear to expire on the replica after one minute (and remain in the master for another 2).
So putting aside full-sync, psync, and failovers and other complications, this is what we break with this PR.
But on the bright side, i think clock offsets can and should be resolved with tech, so this should not be a real problem.

If we really wanted to solve it anyway, what we can do with quite a lot of extra complexity is:
1. when the replica first connects to the master, it sends a TIME command before it attempts to sync (this response will be processed right away, no backlog or replica-buffers delays). this way it knows both the clock difference between them, and also the network latency. 
2. when master replicates relative TTLs to replicas (goes into the backlog and replica buffers which can be received with a huge delay) it replicates both the relative TTL and its current clock.

this way the replica knows how to respect the client's intent (imagine it knows the client machine's wall-clock at the time the command was processed by the master).
The replica will need to translate both the absolute TTLs it gets from RDB or command stream, and also the relative ones (which come together with the master's clock) with it's own clock diff (add 2 mins in our example)

I don't think we wanna this way..."
801472429,8474,ny0312,2021-03-17T22:04:03Z,"@oranagra Appreciate the thoughtful comments.

I agree. Two things:

1. Re: When it's safe to merge. I understand your argument. The counter-argument is that merging it sooner gives us a longer runway for its release - it gives us more time/opportunity to see how it meshes with the rest of Redis and see if it has bugs or unforeseen consequences.

2. Re: Out-of-sync clock on replica, the issue you described in ""footnotes"". This is a good point. I agree this is what this PR ""breaks"". If we do care about such issues, I see at least two options that could help:

(1) Stop replicas from expiring(hiding) keys independently based on local clock. This is basically what Madelyn was referring to in https://github.com/redis/redis/issues/8433 with `makes expire ""linearizable"" across the cluster`. So basically, all expiration are entirely driven by masters. Replicas would keep serving TTL keys until it receives the `DEL` command for them from master.

With this scheme, TTL will essentially become a logical-time-based concept - A TTL key’s life span is measured in terms of replication offsets. Its starting offset is when a master first received the TTL. Its end offset is when a master expires it. The key lives for the same span of offsets on every node, regardless how much wall time it takes each node to replicate/traverse through that span. 

(2) Another option is like what you suggested - replicate master's local time. E.g. master would periodically send its current local time to its replicas via replication stream. Upon receiving these timestamps, replicas would reset their local clock accordingly. This way, as long as time's ""velocity"" is roughly the same between master and replicas, TTL keys should live relatively the same lifetime on them.

Either way, even though it is related, I think it's better if we tackle this issue separately and leave it out of scope for this particular PR. I would love to hear what you think."
850949243,8474,oranagra,2021-05-30T06:22:25Z,"finally merged (not sure why it was waiting for in the last month).
@ny0312 would you care to create a [doc PR](https://github.com/redis/redis-doc/pulls)?"
851029958,8474,ny0312,2021-05-30T16:58:51Z,"Nice. I will create a doc PR this week and reference the link here.

Thanks again for the reviews."
851209486,8474,oranagra,2021-05-31T06:07:11Z,FYI: [valgrind and our freebsd CI](https://github.com/redis/redis/runs/2706636345?check_suite_focus=true) is so slow that more than 10 seconds passed from the time the command was sent to the time it was executed: #9010
851719932,8474,ny0312,2021-05-31T23:50:33Z,"@madolson @oranagra Follow-up PR for API documentation on `EXPIRETIME` and `PEXPIRETIME` commands: https://github.com/redis/redis-doc/pull/1582

Please review. Thanks."
852921025,8474,oranagra,2021-06-02T10:42:27Z,"i see the valgrind run [still fails](https://github.com/redis/redis/runs/2723626957?check_suite_focus=true) despite my [fix](https://github.com/redis/redis/pull/9010) (to change from 10s to 100s).
but more interestingly, note the timestamps (it's not as slow as i thought it is):
```
2021-06-02T00:40:04.1835934Z [ok]: All time-to-live(TTL) in commands are propagated as absolute timestamp in milliseconds in AOF
2021-06-02T00:40:04.8603956Z [err]: All TTL in commands are propagated as absolute timestamp in replication stream in tests/unit/expire.tcl
2021-06-02T00:40:04.8608919Z Expected 'del a' to match 'set foo1 bar PXAT *' (context: type source line 778 file /home/runner/work/redis/redis/tests/test_helper.tcl cmd {assert_match [lindex $patterns $j] [read_from_replication_stream $s]} proc ::assert_replication_stream level 1)
```
@ny0312 maybe you can please look into that?"
855346434,8474,oranagra,2021-06-06T06:21:36Z,"happened again:
https://github.com/redis/redis/runs/2741999584?check_suite_focus=true
https://github.com/redis/redis/runs/2755028920?check_suite_focus=true"
856474299,8474,eliblight,2021-06-08T06:06:43Z,@oranagra I am taking a look on this
859104257,8474,ny0312,2021-06-10T21:55:02Z,"Found the root cause and fixed: https://github.com/redis/redis/pull/9069

@oranagra please take a look"
749212826,8217,madolson,2020-12-21T21:43:44Z,"Also related to Oran's top level comment, we don't always use ""addReplyError*"" for all of the errors, we need to go through and audit them."
749305811,8217,filipecosta90,2020-12-22T02:44:08Z,"> do we intend to cover only the first group? or also the second?
I believe we can see value out of both groups right? meaning the more errors we can keep track without paying a penalty the better.

> If we intend to extend this feature to match other types of errors, it's probably wrong to do an if-else chain of calls to `strncmp`, and then a lookup in a dict (or an enum lookup into an array).
> maybe we should have a rax in which we can search by prefix, and implicitly support all type of errors.
> i think it would be nice to examine this feature after adding a few more errors as an example (like OOM, WRONGTYPE, NOSCRIPT, LOADING, BUSY, BUSYKEY.

@oranagra @madolson, following what you've mentioned I've updated the code to use the Error Prefix ( The first word after the ""-"", up to the first space ) as the rax key (lowercased). In that manner we have the desired implicit support for all errors. Can you guys check the new commit. 

Sample output of errorstats now:
```
127.0.0.1:6379> auth a
(error) ERR AUTH <password> called without any password configured for the default user. Are you sure your configuration is correct?
127.0.0.1:6379> set k
(error) ERR wrong number of arguments for 'set' command
127.0.0.1:6379> info errorstats
# Errorstats
errorstat_auth:count=1
errorstat_wrong:count=1
```
"
749625679,8217,oranagra,2020-12-22T16:12:43Z,"@filipecosta90 looking at the example in your last post (didn't read the code yet), i think these two cases should fall into the standard `err` since the error code is `-ERR` (""wrong"" and ""AUTH"" are just part of the text).  
i.e.
```
errorstat_err:count=2
``` 
not
```
errorstat_auth:count=1
errorstat_wrong:count=1
```

if we think it is important for redis to distinguish between them, then it's also important for client libraries, and we should change the code and promote them to have their own error code."
749834459,8217,filipecosta90,2020-12-22T23:28:53Z,"> we do not want to **optionally** remove the spurious dash, this is in fact an indication that this method (`afterErrorReply`) is called in the wrong place, and given the wrong input (incomplete error message).
> if we do that, the dash will always be present (and we can remove it, and even assert that it's there).

@oranagra agree. In the latest code push I assume that if no `-` is present than the error is the default `ERR` (same as done in `addReplyErrorLength`), otherwise we use the first after the `-` until the first space. 

I've also removed the error name from the error struct and I'm using the error prefix as is ( meaning no lowercase ). 

I believe all comments have been address. wdyt?"
750571563,8217,oranagra,2020-12-23T23:36:41Z,"@filipecosta90 it looks like you added a bunch of commits, and a rebase/merge from unstable, and then squashed it all, and i can't seem to be able to review the changes easily.
please avoid it in the future.

i'm ok with amending a commit and force pushing it (github let's me view the diffs), or to add more and more commits which will then be squashed when the PR is merged.
rebase from unstable or squashing your commits should be done when no one is still looking at the PR, and if you have to grab fresh code from unstable, do it with merge and no squashing, so i can skip that commit."
750579077,8217,madolson,2020-12-23T23:42:26Z,@oranagra Did you look at the code where we are temporarily attaching a flag to a command? That doesn't seem like a very maintainable assumption to make.
750587204,8217,oranagra,2020-12-23T23:48:58Z,@madolson i agree. i'm still editing my CR post when i'm suggesting a better alternative.
751443558,8217,oranagra,2020-12-27T09:11:10Z,"@redis/core-team please approve new info fields:
a per command stat `rejected_calls` and `failed_calls`
a per-error type counters (new info section)
and a new stat for total error count.
details at the top.

@filipecosta90 can you please make a redis-doc PR? thank you!"
751507702,8217,filipecosta90,2020-12-27T19:37:02Z,"> @filipecosta90 can you please make a redis-doc PR? thank you!

@oranagra @itamarhaber with regards to the redis-doc PR can you guys check https://github.com/redis/redis-doc/pull/1472 ?

"
752168654,8217,filipecosta90,2020-12-29T17:16:38Z,"> My approval is for the new API syntax, I would still like to fix some of the minor comments.

@madolson @oranagra addressed the comments in the new commit. "
752802742,8217,filipecosta90,2020-12-31T00:40:40Z,"> That said, I doubt that performance is really an issue here (@filipecosta90 you can probably verify that in a breeze). Everything else is a matter of personal taste and I don't feel that strongly about it - so +1.

@oranagra @yossigo @itamarhaber @madolson  I was getting the numbers for the standalone and cluster variations and noticed the following:
- performance is not affected here as you can check bellow
- one thing I believe we need to fix prior merge is that MOVED/CLUSTERDOWN/CROSSSLOT/... ( all error replies caused within `clusterRedirectClient()` are being accounted on the errorstats section, but not on the commandstats ( given we're not incrementing the rejected_calls there ). I believe we should... . If you agree tomorrow I'll fix it and include tests to verify it...

As a quick note @oranagra , to test the OSS Cluster variation I needed to update locally the branch to include the already merged changes by #8223.

----- 
## standalone numbers of an OOO error test

Used command and error replies:
```
$ redis-benchmark --threads 4 -h 10.3.0.110 -e -n 10000000 -t set
(...)
(...)
Error from server: OOM command not allowed when used memory > 'maxmemory'.
Error from server: OOM command not allowed when used memory > 'maxmemory'.
Error from server: OOM command not allowed when used memory > 'maxmemory'.
(...)
(...)
```

Without errorstats:
```
Summary:
  throughput summary: 204611.95 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.225     0.048     0.223     0.303     0.335     1.487
```

With errorstats:
```
Summary:
  throughput summary: 198503.30 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.232     0.048     0.231     0.311     0.343     3.319

$ $ redis-cli -h 10.3.0.110
10.3.0.110:6379> info errorstats
# Errorstats
errorstat_OOM:count=10000000
10.3.0.110:6379> info commandstats
# Commandstats
cmdstat_info:calls=2,usec=61,usec_per_call=30.50,rejected_calls=0,failed_calls=0
cmdstat_command:calls=2,usec=801,usec_per_call=400.50,rejected_calls=0,failed_calls=0
cmdstat_config:calls=5,usec=56,usec_per_call=11.20,rejected_calls=0,failed_calls=0
cmdstat_set:calls=348459,usec=106708,usec_per_call=0.31,rejected_calls=10000000,failed_calls=0
cmdstat_flushall:calls=1,usec=20,usec_per_call=20.00,rejected_calls=0,failed_calls=0
```


----- 
## OSS cluster numbers of an MOVED errors worst case scenario ( all moved )

Used command and error replies:
```
$ redis-benchmark --threads 4 -h 10.3.0.110 -p 30001 -e -n 10000000 -t set
(...)
(...)
Error from server: MOVED 13782 127.0.0.1:300038 (overall: 0.261).261)
Error from server: MOVED 13782 127.0.0.1:300030.407 (overall: 0.257))
Error from server: MOVED 13782 127.0.0.1:300032 (overall: 0.256).256)
(...)
(...)
```

Without errorstats:
```
Summary:
  throughput summary: 183874.23 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.252     0.048     0.255     0.327     0.359    18.207
```

With errorstats ( notice errorstats is populated with info but not commandstats ):
```
Summary:
  throughput summary: 184723.38 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.250     0.048     0.247     0.327     0.351    14.719

ubuntu@ip-10-3-0-195:~$ redis-cli -h 10.3.0.110 -p 30001 
10.3.0.110:30001> info errorstats
# Errorstats
errorstat_MOVED:count=10000000
10.3.0.110:30001> info commandstats
# Commandstats
cmdstat_psync:calls=1,usec=314,usec_per_call=314.00,rejected_calls=0,failed_calls=0
cmdstat_command:calls=1,usec=414,usec_per_call=414.00,rejected_calls=0,failed_calls=0
cmdstat_info:calls=4,usec=135,usec_per_call=33.75,rejected_calls=0,failed_calls=0
cmdstat_ping:calls=1,usec=0,usec_per_call=0.00,rejected_calls=0,failed_calls=0
cmdstat_cluster:calls=8,usec=1059,usec_per_call=132.38,rejected_calls=0,failed_calls=0
cmdstat_replconf:calls=204,usec=129,usec_per_call=0.63,rejected_calls=0,failed_calls=0
cmdstat_config:calls=2,usec=32,usec_per_call=16.00,rejected_calls=0,failed_calls=0

```"
752878876,8217,oranagra,2020-12-31T07:46:46Z,"you meant #8226, yes, you should probably rebase this PR (can be a simple merge from unstable, since we're gonna squash this PR later)

regarding clusterRedirectClient I think it's a deeper problem, created: #8274 to discuss that.
meanwhile, i suggest you'll explicitly increment cmd->rejected_calls in processCommand after the call to clusterRedirectClient. if my PR will be merged, we'll get a merge conflict and resolve it properly."
1976159196,8217,enjoy-binbin,2024-03-04T09:48:37Z,"We have encountered such a problem here. The user used such lua code:
```
return redis.error_reply(string.format('last:%s, now:%s....)
```

We can see `last:%s`, which will format the parameters. In errorstats, it will be used as an error code, causing more and more data saved in `info Errorstats`, and then when using INFO, the server will block since there are too many errorstats.
```
#Errorstats
errorstat_ERR:count=27
errorstat_last_1709544349,:count=1
errorstat_last_1709544354,:count=1
errorstat_last_1709544359,:count=1
errorstat_last_1709544360,:count=1
errorstat_last_1709544371,:count=1
errorstat_last_1709544385,:count=1
errorstat_last_1709544390,:count=1
errorstat_last_1709544395,:count=1
errorstat_last_1709544396,:count=1
errorstat_last_1709544408,:count=1
errorstat_last_1709544413,:count=2
errorstat_last_1709544414,:count=2
errorstat_last_1709544432,:count=1
errorstat_last_1709544442,:count=1
errorstat_last_1709544450,:count=1
```

Is there any way we can control this incorrect usage?"
1976249979,8217,oranagra,2024-03-04T10:23:48Z,"i'm not sure there's anything we can do.
the API is clearly documented that the first word is the error code, and i don't think we have any way to distinguish between a misuse and a valid one.
the only thing we can do is maybe put some maximum cap on the extent of damage it does, and disable the error stats mechanism completely if there are more than say 1000 different errors.
i.e. print them to the log file, empty the dict, and set a flag to avoid populating it from now on."
1976270768,8217,enjoy-binbin,2024-03-04T10:34:36Z,"> the only thing we can do is maybe put some maximum cap on the extent of damage it does, and disable the error stats mechanism completely if there are more than say 1000 different errors.

yean, the only way i thought of is the same. in info errorstats, use raxSize and only part of the data is output to break out of the loop. Note that it may also potentially consume some memory (if it has been accumulated)

Or set a parameter to limit the number of Errorstats we save"
1976285851,8217,oranagra,2024-03-04T10:42:28Z,"if we do that, i'd rather stop collecting them, release the memory and permanently disable error stats. not just filter them at reporting time."
1976297995,8217,enjoy-binbin,2024-03-04T10:48:59Z,"yes, i think so too, just.. i can’t describe it very well.

ok, what i think is similar to only saving up to 1000 errorstats, and deleting some of them when it exceeds, similar to our earliest repl_scriptcache_fifo, something like that

but just some random ideas, just to throw the discussion out there first"
1976311888,8217,oranagra,2024-03-04T10:56:33Z,"i don't think we'd want to delete some. i think we can detect a misuse and completely disable this feature preventing the damage it can cause, and losing the functionality.
to help users figure out what happened, i'd print the existing values to the log file, add a single entry to replace them that mentioned it's disabled due to misuse, and prevent any further accumulation."
1006284160,10061,chenyang8094,2022-01-06T04:54:19Z,PAT @yoav-steinberg 
1006348472,10061,oranagra,2022-01-06T07:42:16Z,"@chenyang8094 i didn't review the code yet, but i'd like to respond to the top comment.
1. i think redis-check-aof should still support checking old aof files (preamble or non-preamble files without manifest). i think someone may be using a new redis-check-aof with on an AOF file from an old server. (if this complicates the code too much, maybe we can drop that, but i'd like to try supporting it).
2. i think your reasoning above about not truncating or fixing INCR files other than the last one, are acceptable. but i think we need to print a message to a user trying to use `--truncate-to-timestamp`, telling him what to do (edit the manifest file manually). i.e. if we find the timestamp in a non-last INCR file, we should tell him in which file we found it, and instruct him to edit the manifest and re-run."
1006349290,10061,yoav-steinberg,2022-01-06T07:44:16Z,"Before I review here are some questions:
1. 
> If we want to support truncate any file, we need to add very complicated code to support the atomic modification of multiple file deletion and update manifest, I think this is unnecessary

I don't think the tool needs to be atomic. If we have BASE+INCR1+INCR2 and the specified timestamp is in the middle of INCR1 then we need to delete to truncate INCR1, delete INCR2 and update the manifest. I think it's OK not to be atomic here. The user should always backup the directory before running this tool.

2.  Don't we still support a **non** RDB BASE? If so then the tool can also truncate the base file in such a case.

3. Is the tool still backwards compatible with the old (non multi-part) AOF file?"
1006355372,10061,oranagra,2022-01-06T07:57:03Z,"> Don't we still support a non RDB BASE? If so then the tool can also truncate the base file in such a case.

Ahe AOF file produced by rewrite won't have timestamps.

And note that corruptions are only expected at the last INCR file (due to fsync or bad mount options)"
1006355970,10061,chenyang8094,2022-01-06T07:58:17Z,"> 1. i think redis-check-aof should still support checking old aof files (preamble or non-preamble files without manifest). i think someone may be using a new redis-check-aof with on an AOF file from an old server. (if this complicates the code too much, maybe we can drop that, but i'd like to try supporting it).

Unless we add special parameters, I can't think of any effective way to identify whether this file is an old AOF or a manifest. It is not reliable to rely on the file name alone.

> 2. i think your reasoning above about not truncating or fixing INCR files other than the last one, are acceptable. but i think we need to print a message to a user trying to use `--truncate-to-timestamp`, telling him what to do (edit the manifest file manually). i.e. if we find the timestamp in a non-last INCR file, we should tell him in which file we found it, and instruct him to edit the manifest and re-run.

I thought, in this case, the user will see the following message:
```
Failed to truncate AOF appendonly.aof.1.incr.aof to timestamp 1628217471 because it is not the last file.
If you insist, please delete all files after this file according to the manifest file and delete the corresponding records in manifest file manually. Then re-run redis-check-aof.

```

"
1006360623,10061,oranagra,2022-01-06T08:07:10Z,"regarding the detection of an old AOF file, or a manifest i'm not sure it's that bad to rely on the file name (we do control the name of the manifest file, and it's unlikely that users will name their old AOF files with a ""manifest"" suffix).
however, i think it's not that complicated to detect automatically by the content.
either we try to match the first word we know must be on the first line of the manifest, or even add some new unique header to our manifest files (it's not too late).

regarding the truncation message, that's great.
two suggestions for improvements:
1. we can take this opportunity to mention the offset we found in that file (so user can use other tools to manually truncate it, rather than go though the slow AOF file processing again)
2. do we really have to instruct them to delete the files? isn't it enough to just modify the manifest?"
1006360916,10061,chenyang8094,2022-01-06T08:07:42Z,"
> I don't think the tool needs to be atomic. If we have BASE+INCR1+INCR2 and the specified timestamp is in the middle of INCR1 then we need to delete to truncate INCR1, delete INCR2 and update the manifest. I think it's OK not to be atomic here. The user should always backup the directory before running this tool.

We only recommend that users back up but it is not mandatory. Without the guarantee of atomicity, I don't want to destroy the original data. In addition, if it crashes due to some reason in the middle, and we have no chance to print out enough log, may cause the manifest and AOF to be incompatible, the user will not know whether the problem is caused by the `redis-check-aof` tool itself or the original data error. This will be confusing.

> 2. Don't we still support a **non** RDB BASE? If so then the tool can also truncate the base file in such a case.

   Of course  we can (if BASE is the last file).

> 3. Is the tool still backwards compatible with the old (non multi-part) AOF file?

    No, we are discussing this."
1006362693,10061,chenyang8094,2022-01-06T08:11:06Z,"The manifest must start with `file` ， like：
`file appendonly.aof.1.incr.aof seq 1 type i`

So, we can do like this:
```
if (file start  with REDIS) {
   check rdb file
} else if (file start with file) {
  check multi part aof
} else {
  check old aof
}
```


> 2. do we really have to instruct them to delete the files? isn't it enough to just modify the manifest?

Yes, only modifying the manifest is enough, but a better suggestion is to delete the files together, otherwise these files will never be seen by redis and will not be recycled by the GC mechanism.

"
1006367954,10061,oranagra,2022-01-06T08:21:13Z,"ok, so i think we can agree on:
1. no need to automatically handle truncation of non-last files (for both atomicity and complexity concerns)
2. detect old aof files vs manifest files like you just described.
3. keep the message about delete (or move) files when you manually remove them from the manifest.
4. let's add the offset inside the file where the truncation is needed in the log message."
1006397988,10061,yoav-steinberg,2022-01-06T09:11:39Z,"For the record I think a recovery tool shouldn't worry about atomicity. For two main reasons:
1. When dealing with production issues that require manual fixes to get back up online we always perform non-atomic operations. We don't have the luxury to stop and consider what happens if we have another failure (causing atomicity issues) while performing the recovery process. This is simply in order to fix things and get back online as soon as possible because developing an atomic custom recovery plan during a crisis is never practical.
2. To mitigate the risk in _1._  we backup our data or any configuration before performing a risky operation. Sometimes the cost of this is large (copying a 500GB AOF file is expensive) but there's no practical way around it.
I think this tool should in general disregard atomicity concerns but should definitely warn the user to **backup** the files before applying any fixes to them.

I'm also fine with not editing the manifest file at this point to keep the tool simple (in 99% of the cases we won't need to truncate a non last INCR file), but I don't think atomicity has anything to do with it.

"
1006438839,10061,chenyang8094,2022-01-06T10:10:17Z,"@yoav-steinberg  Aside from atomicity, but it does bring a lot of complexity. Thinking from another perspective, if the user really wants to truncate the non-last INCR file, we print a friendly message and ask him to manually delete these files (this time he may realize that these files should be backed up first). I think this is a safe practice, and deleting files  is a very easy thing for user, and the probability of this happening is very small."
1006439737,10061,chenyang8094,2022-01-06T10:11:20Z,@oranagra 1，2，3，4 all done.
1046318404,10061,oranagra,2022-02-20T20:51:01Z,"@chenyang8094 please take a look at this failure 
https://github.com/redis/redis/runs/5266360953?check_suite_focus=true#step:5:1732
```
child process exited abnormally
    while executing
""exec src/redis-check-aof --truncate-to-timestamp 1628217470 $aof_manifest_file""
    (""uplevel"" body line 23)
    invoked from within
""uplevel 1 $code""
    (procedure ""test"" line 51)
    invoked from within
""test {Truncate AOF to specific timestamp} {
```"
1046961796,10061,oranagra,2022-02-21T14:54:47Z,"another similar error:
https://github.com/redis/redis/runs/5273856527?check_suite_focus=true#step:5:1456
```
[err]: Short read: Utility should confirm the AOF is not valid in tests/integration/aof.tcl
Expected 'Start checking Multi Part AOF
Start to check INCR files.
Cannot open file: appendonly.aof.1.incr.aof, aborting...
child process exited abnormally' to match '*not valid*' (context: type eval line 5 cmd {assert_match ""*not valid*"" $result} proc ::test) 
```
i don't understand how we could get ""aborting..."", if we didn't pass the `--fix` argument."
1047369752,10061,chenyang8094,2022-02-22T02:36:42Z,"@oranagra GOT,  I have added more logs (printf the `errno` and AOF abs path) in my branch and re-run the workflow, but so far it has not reappeared."
1047395750,10061,chenyang8094,2022-02-22T03:40:45Z,"finally reappeared.

It looks like we are getting a wrong path, is there a bug in the dirname function? 
```
[exception]: Executing test client: Start checking Multi Part AOF
Start to check INCR files.
Cannot open file ./tests/tmp/server.aof.2565.11/appendonlydir/appendonly.aof.manifest/appendonly.aof.1.incr.aof: Not a directory, aborting...
child process exited abnormally.
Start checking Multi Part AOF
```"
1047460271,10061,chenyang8094,2022-02-22T06:14:06Z,"I add this log:
![image](https://user-images.githubusercontent.com/13696140/155073455-8b4aa8dc-4ecb-4318-83d1-5bc16abbac85.png)
and finally reproduced it (https://github.com/chenyang8094/redis/runs/5283813036?check_suite_focus=true):
![image](https://user-images.githubusercontent.com/13696140/155073512-b1e2a32d-2930-45db-89af-6f39ac3a9eba.png)

So it's `dirname` that is returning us the wrong path, I'm not sure that `dirname` will have some kind of exception under `SANITIZER=address`, at least I haven't found a similar answer yet.

Copy a glibc implementation:
```
char *dirname(char *path) {
    static const char dot[] = ""."";
    char *last_slash;
    /* Find last '/'.  */
    last_slash = path != NULL ? strrchr(path, '/') : NULL;
    if (last_slash != NULL && last_slash != path && last_slash[1] == '\0') {
        /* Determine whether all remaining characters are slashes.  */
        char *runp;
        for (runp = last_slash; runp != path; --runp)
            if (runp[-1] != '/') break;
        /* The '/' is the last character, we have to look further.  */
        if (runp != path) last_slash = __memrchr(path, '/', runp - path);
    }
    if (last_slash != NULL) {
        /* Determine whether all remaining characters are slashes.  */
        char *runp;
        for (runp = last_slash; runp != path; --runp)
            if (runp[-1] != '/') break;
        /* Terminate the path.  */
        if (runp == path) {
            /* The last slash is the first character in the string.  We have to
               return ""/"".  As a special case we have to return ""//"" if there
               are exactly two slashes at the beginning of the string.  See
               XBD 4.10 Path Name Resolution for more information.  */
            if (last_slash == path + 1)
                ++last_slash;
            else
                last_slash = path + 1;
        } else
            last_slash = runp;
        last_slash[0] = '\0';
    } else
        /* This assignment is ill-designed but the XPG specs require to
           return a string containing ""."" in any case no directory part is
           found and so a static and constant string is required.  */
        path = (char *)dot;
    return path;
}
```"
1047478008,10061,chenyang8094,2022-02-22T06:51:12Z,@yossigo @oranagra Do you have any relevant insights on this?
1047507777,10061,chenyang8094,2022-02-22T07:41:00Z,"finally found the bug:

```
    char temp_filepath[PATH_MAX + 1];
    memcpy(temp_filepath, filepath, strlen(filepath));
    dirpath = dirname(temp_filepath);
```

Since `temp_filepath` is allocated on the stack and not initialized,   and we did not copy `'\0'` when executing `memcpy`, so when a lot of `'\' `are left on the stack, `dirname` may get a wrong result."
1016786314,10127,oranagra,2022-01-19T19:15:08Z,"@redis/core-team listing sub-commands in ACL CAT and COMMAND LIST.
complementary act for the sub-command work in 7.0.
if you see any reason not to do that, please speak. "
1017726807,10127,guybe7,2022-01-20T17:09:20Z,@enjoy-binbin i'm done reviewing the lat batch of changes
1017740532,10127,enjoy-binbin,2022-01-20T17:22:58Z,"@guybe7 thank you very much for the review.. sorry for not doing it perfectly, i will take a deep look tomorrow"
1018724137,10127,enjoy-binbin,2022-01-21T17:40:05Z,trigger a full CI in this branch: https://github.com/enjoy-binbin/redis/actions/runs/1729975896
1019258215,10127,oranagra,2022-01-22T13:16:02Z,"@enjoy-binbin gave it another top to bottom review, and last round of comments.
there's one real issue here, one that existed before your PR, but i just found it now (iterating on the subcommands list instead of dict).
all other comments are just some small suggestions for improvements.
i also added some details to the top comment.
please also go over all the earlier unresolved comments make sure they're resolved and mark them as such.
then let's merge this."
1019314815,10127,enjoy-binbin,2022-01-22T17:31:20Z,"@oranagra thanks for the review / patience. I checked all the comments.
I admit this PR getting bigger and bigger with each small change, with too much cleanup mixed in...
will avoid doing this in the future, or at least split the PR (avoid irrelevant changes)"
1019326939,10127,oranagra,2022-01-22T18:14:31Z,"a little unrelated cleanup is ok.. and in this case much of it is NOT unrelated.
in retrospect, i'm not sure we should have made all the error messages uniform (inducing many changes in the tests)

we could even consider not to rename `name` to `fullname`, but i figured it could be dangerous for some merge branches / forks, and once we did that, many of these lines where already modified anyway..."
897733068,9356,madolson,2021-08-12T15:25:16Z,"I didn't initially think of this, but I think we also need a change in defrag. Here https://github.com/redis/redis/blob/1221f7cd5eea8877a3df766f5c0a67255a894295/src/defrag.c#L898 we might reallocate the dictEntry, but the clustermeta data pointers will still be pointing to the previous invalid locations."
898183836,9356,madolson,2021-08-13T04:30:11Z,"@redis/core-team Probably worth pinging the group about this, as it's a rather intrusive change and I can see the argument the complexity is not worthwhile.

@zuiderkwast If you get a chance, I also would expect this to perform better as well. If you don't get a chance I can try running some benchmarks tomorrow."
899461061,9356,zuiderkwast,2021-08-16T12:12:11Z,"## Benchmarks

Cluster of 3 nodes, no replicas, running locally on laptop.

### Latency

`redis-benchmark -p 30001 --cluster -t set,get -n 1000000 --threads 8 -P 100 -q`

Result: No significant difference compared to `unstable`.

**[Edit] I forgot to use -r here, so only one key was created. :-/ See new benchmark below!**

### Memory

`DEBUG POPULATE 1000000` on one of the cluster nodes. Then `INFO MEMORY`.

| Result | `used_memory_human` |
|------|-----|
| Unstable | 116.37M |
| This PR | 93.51M |
| Difference | 22.86M (20% save) |

The keys generated by `DEBUG POPULATE` are on the form `key:NNNNNN`. With a longer common key prefix (10 bytes), the memory saving is around 18%. For long keys without a shared prefix, the saving is expected to be even higher but I haven't written any script for generating such data."
900016749,9356,madolson,2021-08-17T06:02:46Z,@yossigo @itamarhaber @soloestoy ?
904543611,9356,itamarhaber,2021-08-24T11:07:21Z,"Haven't done a CR, but this looks good as long as there's no regression in performance - the rax was put there to replace a skip list that didn't perform well (https://github.com/redis/redis/commit/1409c545da7861912acef4f42c4932f6c23e9937). "
904935073,9356,zuiderkwast,2021-08-24T20:01:46Z,"Regarding the performance regression concerns, I increased the numbers and ran some more benchmarks.

Benchmark: Set and get
Number of requests (-n): 10,000,000
Keyspace length (-r): 1G, 1M, 1K (see table)
Cluster: 3 masters, 0 replicas
Command: `redis-benchmark -p 30001 --cluster -t set,get -n 10000000 --threads 8 -P 100 -q -r $r`
The database was flushed on all nodes between each run.

| Keyspace length |     | unstable                                       | This PR  |
|-----------------|-----|------------------------------------------------|----------|
| r=1,000,000,000 | SET | 1327140.00 requests per second, p50=3.351 msec | 1990049.75 requests per second, p50=2.239 msec |
|                 | GET | 3619254.50 requests per second, p50=1.159 msec | 3317850.00 requests per second, p50=1.239 msec |
| r=1,000,000     | SET | 1473839.38 requests per second, p50=2.167 msec | 2094240.75 requests per second, p50=2.167 msec |
|                 | GET | 3620564.75 requests per second, p50=1.263 msec | 3320053.00 requests per second, p50=1.367 msec |
| r=1,000         | SET | 2844950.25 requests per second, p50=1.623 msec | 2840909.00 requests per second, p50=1.647 msec |
|                 | GET | 4424779.00 requests per second, p50=0.879 msec | 4422822.00 requests per second, p50=0.895 msec |"
905169463,9356,madolson,2021-08-25T04:17:25Z,"Ooh, that is more like what I was expecting.

So it looks like we see about a ~40/50% speedup for writes (which is great) at the cost of about a 10% get performance regression. For the R=1000, almost everything is probably in L1/L2 CPU cache, so that likely explains why their wasn't much difference there.

I think that 10% regression is acceptable for the memory savings and the write speed improvement. @redis/core-team Going to ping everyone again since this is definitely something to discuss."
905179213,9356,QuChen88,2021-08-25T04:45:57Z,"The read performance degradation is marginal and not a major concern IMO. 

On the other hand, the current approach of using a RAX to store the slot to keys structure ensures that the keys in the slot are strictly ordered in lexicographic order. With this approach that property is foregone. There is no longer any ordering that can be done given a specific key which makes async iteration on the keys in a slot to be harder when we need to resume the operation later on after the slot is modified. Looked through `db.c` but can't seem to find a good place where redis is currently relying on that today though. 

More specifically, I was thinking of a use-case when we are iterating the keys in the slot, and we do that in an incremental manner with an iterator. When the iterator is somewhere in the middle of the slot, is there an efficient way to determine if a given item comes ""before"" or ""after"" the iterator? With RAX when the keys are ordered, we can simply do a `>` comparator on the key name with the iterator key name, which I think is kind of nice. Also today, the `raxIterator` is able to store the current key name so that even if the underlying slot_to_keys changed across iterator runs, we can do a quick re-seek the next time around and resume iteration. I don't think we will have the same safe iteration mechanism available on the double linked list with this change. 

Does anyone know why `slot_to_keys` is ordered today? There might be a reason for it that I am not aware of. I wonder if there can be future potential use-cases for keeping the keys in the slot ordered which will be harder to support after this change. "
905235868,9356,oranagra,2021-08-25T06:58:32Z,"@madolson i agree that considering the memory improvement and the write speed improvement we can accept the read degradation, but maybe we should give it a quick optimization attempt to see if we can solve that too.
i.e. invest a small effort rather than give up without trying.
it could be an extra indirection that's unavoidable (although RAX had an abundance of these), but maybe there's something simple that can be done to make it more cache friendly, or do some function inlining tip / macro.
@zuiderkwast can you do some profiling to see if you happen to find something."
905424401,9356,QuChen88,2021-08-25T11:42:24Z,"If you do look into performance degradation of the read operations, I would suggest finding out where the additional latency comes from. Looking at this CR there is nothing that changed directly in the `lookupKeyRead` code path apart from the additional metadata in the `dictEntry` so I am curious about it. It could potentially be coming from the way benchmark was set up but I am not sure assuming you are keeping all other factors the same in your test run besides the redis-server binary. 

Thinking about this more, the key range of 1000 where everything fits into L1/L2 cache is a very small redis instance and quite unusual usage scenario nowadays especially given the increase in the memory capacity over time from various service offerings. It is very likely that people would store much larger number of keys in Redis in the range of millions, where they will see this 10% slow down in the read path. "
905540954,9356,zuiderkwast,2021-08-25T14:16:45Z,"> @zuiderkwast can you do some profiling to see if you happen to find something.

@oranagra I can try, but I can't think of any obvious optimization. My guess is reads are slower because dictEntry is larger, which means more cache misses for reads. The slot-to-key mapping is not used for reads.

> It could potentially be coming from the way benchmark was set up but I am not sure assuming you are keeping all other factors the same in your test run besides the redis-server binary.

@QuChen88 Well, I ran the benchmarks on a laptop, which is not very good in terms of stable CPU frequency, other background processes, etc. If you have a better environment to run tests, I'd be happy if you repeat them. It's very easy to set up the cluster using the utils/create-cluster script and redis-benchmark."
905621187,9356,QuChen88,2021-08-25T15:37:24Z,"@zuiderkwast Maybe you can consider doing the benchmark by launching redis instances on AWS EC2 instances, and generate benchmark traffic from another client EC2 instance? That way you have dedicated resources to run the redis server and benchmark processes with sufficient amount of CPU/memory. I think we should repeat the experiment and look at data sets that consists of a million keys and 10 million keys. "
905834691,9356,zuiderkwast,2021-08-25T20:03:28Z,"@QuChen88 I repeated the tests a few times on laptop and they show consistent enough results of +50% write and -10% read performance.

I don't have directly available access to AWS, but if you have, maybe you want to run them on EC2 instances?

I think pref and flamegraph would be better to pinpoint where the CPU time is spent."
905912942,9356,QuChen88,2021-08-25T22:15:29Z,+1 on flamegraph. 
906024601,9356,JimB123,2021-08-26T02:22:20Z,"Several of us were discussing this yesterday.  We theorize that the slight degradation is how the dict entries fit into cache.  The old dictEntry was 24 bytes.  A cache line is 64 bytes.  So there are likely to be 2 dictEntries (and part of a 3rd) in every cache line.  The new dictEntry is 40 bytes.  So only 1 dictEntry can fit in a cache line.  If the alignment is just right, you can fit the first DE (40 bytes) + the first 24 bytes of the next entry (the main bytes of the dictEntry) into a single cache line - so, in some cases we still get 2.  But usually not the case.

Given this, we would expect less performance degradation with a larger database.  The larger the database, the less likely a random item is to already be in cache.

Also, note that this is partly related to the test itself.  If the only 24 byte allocations are dictEntries, we are very likely to have multiple dictEntries in a cache line.  However, if values and/or keys require 24 byte blocks, we would be less likely that the cache line would have 2 dictentries."
906124362,9356,ShooterIT,2021-08-26T06:12:01Z,"Very interesting! @JimB123 @zuiderkwast i have one thought, want to share with you, but without test, i am sure it is effective. Now entry size 40, according to jemalloc bin size, actually allocs 48 byte, old dictEntry was 24 bytes, allocs 32byte, cache line could contain 2 entries. Based on this idea, i come up a idea, we separate real metadata with its pointer, that is to say, in dict entry, we only store a metadata pointer, and alloc another memory to store metadata. so new dict entry is 32 bytes(jemalloc has 32bytes bin), metadata is 16bytes(jemalloc also has 16bytes bin). This solution doesn't cost extra memory compared with current PR, but it brings one extra memory allocation when creating new keys and extra one free when deleting keys."
906128538,9356,oranagra,2021-08-26T06:20:43Z,"@ShooterIT we do have a bin for 40 bytes, and also one for 24 bytes (#2510)
also, that extra pointer **will** consume extra memory (reducing the savings), and also it'll add another indirection, probably slowing down the cases where this data structure is used."
906135136,9356,ShooterIT,2021-08-26T06:35:14Z,"Oh, thanks @oranagra  i miss that, please ignore above thoughts"
906442803,9356,zuiderkwast,2021-08-26T14:03:32Z,"Flame graphs, 10M keys.

<details>
  <summary>Unstable, SET</summary>

![unstable-set](https://user-images.githubusercontent.com/273886/130974591-a668d864-7d84-4b01-b13b-43e950bb8683.png)
</details>

<details>
  <summary>This PR, SET</summary>

![metadata-set](https://user-images.githubusercontent.com/273886/130974918-93583a35-03a0-47ef-99ea-9416c46b7e05.png)
</details>

<details>
  <summary>Diff, SET</summary>

![diff-set](https://user-images.githubusercontent.com/273886/130974971-a53372db-58d3-4ece-b024-4eca7238f0e7.png)
</details>

<details>
  <summary>Unstable, GET</summary>

![unstable-get](https://user-images.githubusercontent.com/273886/130975111-9781eb19-9e53-4852-88fa-50c378cd7b74.png)
</details>

<details>
  <summary>This PR, GET</summary>

![metadata-get](https://user-images.githubusercontent.com/273886/130975141-91a33c21-4629-4823-b88d-548ae4034093.png)
</details>

<details>
  <summary>Diff, GET</summary>

![diff-get](https://user-images.githubusercontent.com/273886/130975239-48c22311-9a84-463b-b14b-b2b2cdf3f443.png)
</details>

<details>
  <summary>How to reproduce</summary>

* Compile Redis with `CFLAGS=-fno-omit-frame-pointer`.
* Start a cluster of 3 masters, no replicas.
* Start `perf record -F 99 -a -g -p $PID` where PID is the pid of one of the Redis nodes (as root if you have to).
* In another terminal, run benchmark `redis-benchmark -p 30001 --cluster -t $TEST -n 10000000 --threads 8 -P 100 -r 1000000000` where TEST is either `set` or `get`. Run `set` first to populate the database and later `get` on that data.
* Stop `perf` using Ctrl+C. Perf writes `out.data`.
* See https://www.brendangregg.com/FlameGraphs/cpuflamegraphs.html#Instructions on how to generate the FlameGraph from this.
* See https://www.brendangregg.com/blog/2014-11-09/differential-flame-graphs.html on how to generate a diff flame graph.
</details>
"
906811065,9356,zuiderkwast,2021-08-26T23:32:13Z,">  If the only 24 byte allocations are dictEntries, we are very likely to have multiple dictEntries in a cache line. However, if values and/or keys require 24 byte blocks, we would be less likely that the cache line would have 2 dictentries.

@JimB123 The keys in the test are 22 bytes (`key:{sss}:nnnnnnnnnnnn`) and the values 3 bytes, so for each key, we have the following allocations:

| What | Details | Size | Alloc size class |
|------|-------|-----|-------|
| Key | sdshdr5 (1) + key bytes (22) + nullterm (1) | 24 | 24 |
| Value | robj (16) + embedded sdshdr8 (3) + value bytes (3) + nullterm (1) | 23 | 24 |
| dictEntry |  | 40 | 40 |
| dictEntry* | in db->dict.ht[0] per key (one large allocation) | 8(+) | 8(+) |
| Total |     | 95 | 96 |

On one of the Redis nodes after running the SET benchmark, INFO gives `used_memory=320489744` and `db0:keys=3241757` (1/3 of the 10M keys), which implies 99 bytes per key. Only 3 bytes overhead left to other stuff or I missed something. Anyway, assuming each key-value pair takes 100 bytes is not too far-off.

Given a random key of 10M keys, I estimate its dictEntry is almost never in the cache line.

The flame graph ""diff, get"" shows dictSdsKeyCompare in red, meaning it takes more time than before. This is where the key is loaded from RAM during lookup, so there can be cache misses here too.

Let's calculate the probability that the entry (or parts of) is in the L3 cache then. My i9 CPU has 12MB of L3 cache. Since I was running redis-benchmark on the same machine, let's say I had 10MB of L3 for the Redis nodes.

Given 100 bytes per key, we can fit 100k keys in 10MB of L3 cache, so we have a probability of 0.01 that a random key of 10M keys already sits in the L3 cache. With 16 bytes less per dictEntry (i.e. without this PR), that probability is 0.012. This difference still can't explain the 10% higher latency.

If we had a way to simulate real traffic with a mix of SET, GET, DEL and other commands, based on statistics from a real production system, that would be pretty nice."
906811802,9356,filipecosta90,2021-08-26T23:34:13Z,"@zuiderkwast @madolson @QuChen88 , got some further interesting data on the GET command top on-cpu consumers by function and LOC. 
( Using 3 primaries, 0 replicas, 100M keys populated with memtier via: `taskset -c 4-11 memtier_benchmark -d 3 --threads 8  -c 20 --key-pattern=P:P --ratio 1:0 --pipeline 100 --cluster-mode -p 30001 --hide-histogram -n allkeys --key-maximum 100000000` ). The 3 redis process are pinned to individual cores 0,1,2. 
Using commit 4e1ea907c3cb5ccf7c73cdc3adcaf958d856a150 of https://github.com/zuiderkwast/redis/commit/4e1ea907c3cb5ccf7c73cdc3adcaf958d856a150

## Top on-cpu consumers call graph by LOC 

Focusing specifically on call stacks that represent > 1% on cpu
- 1) we see that dictFetchValue->lookupKey->dictFind is the top stack. `dictFind` line 522: `if (key==he->key || dictCompareKeys(d, key, he->key))` takes 12.21%. Basically the dictCompareKeys is the most expensive call in terms of cpu time.
- Following up on the above point, within dictSdsKeyCompare, the line src/sds.h:88 `l2 = sdslen((sds)key2);` takes 9.50% cpu time. If we can preserve the sdslen value on the object it would improve ~10% the cpu time.

```
----------------------------------------------------------+-------------
      flat  flat%   sum%        cum   cum%   calls calls% + context 	 	 
----------------------------------------------------------+-------------
                                       26041246824 86.66% |   lookupKey /root/redis/src/db.c:63
                                        4008158552 13.34% |   dictFetchValue /root/redis/src/dict.c:534
24430952517 12.21% 12.21% 30049405376 15.02%                | dictFind /root/redis/src/dict.c:522
                                         248748091  0.83% |   [redis-server]
----------------------------------------------------------+-------------
                                       18299809561 96.21% |   dictSdsKeyCompare /root/redis/src/server.c:1289 (inline)
                                         286078193  1.50% |   processInputBuffer /root/redis/src/networking.c:2104 (inline)
                                         248409523  1.31% |   callHandler /root/redis/src/connhelpers.h:79 (inline)
19004608082  9.50% 21.70% 19021562362  9.50%                | sdslen /root/redis/src/sds.h:88
----------------------------------------------------------+-------------
                                       14283739341 99.16% |   lookupKey /root/redis/src/db.c:63
14400803702  7.20% 28.90% 14404227604  7.20%                | dictFind /root/redis/src/dict.c:520
----------------------------------------------------------+-------------
                                        8167902687 63.02% |   __GI___clock_gettime /build/glibc-S9d2JN/glibc-2.27/sysdeps/unix/clock_gettime.c:115
                                        4229778227 32.63% |   ustime /root/redis/src/server.c:1241
12951564730  6.47% 35.37% 12961293678  6.48%                | [[vdso]]
----------------------------------------------------------+-------------
                                       10048350456 69.72% |   __libc_write /build/glibc-S9d2JN/glibc-2.27/sysdeps/unix/sysv/linux/write.c:27
                                        3479184165 24.14% |   __libc_read /build/glibc-S9d2JN/glibc-2.27/sysdeps/unix/sysv/linux/read.c:27
12787574214  6.39% 41.76% 14412705731  7.20%                | [[kernel.kallsyms]]
                                        1038598602  7.21% |   ipv4_conntrack_local ??:?
----------------------------------------------------------+-------------
                                        9457121329   100% |   getNodeByQuery /root/redis/src/cluster.c:5858
9446994814  4.72% 46.48% 9457121329  4.73%                | crc16 /root/redis/src/crc16.c:86
----------------------------------------------------------+-------------
                                        5341735489   100% |   processCommand /root/redis/src/server.c:4076
5338663420  2.67% 49.15% 5341735489  2.67%                | getNodeByQuery /root/redis/src/cluster.c:5852
----------------------------------------------------------+-------------
                                        1336166613 28.09% |   getNodeByQuery /root/redis/src/cluster.c:5859 (inline)
                                        1057060667 22.22% |   dictSdsKeyCompare /root/redis/src/server.c:1289 (inline)
                                         855134838 17.97% |   processInputBuffer /root/redis/src/networking.c:2104 (inline)
                                         454691416  9.56% |   addReply.part.0 /root/redis/src/networking.c:351 (inline)
                                         415611017  8.74% |   lookupKey /root/redis/src/db.c:63 (inline)
                                         272520302  5.73% |   addReplyBulk /root/redis/src/networking.c:826 (inline)
                                         225547372  4.74% |   callHandler /root/redis/src/connhelpers.h:79 (inline)
4757417237  2.38% 51.53% 4757417237  2.38%                | sdslen /root/redis/src/sds.h:89
----------------------------------------------------------+-------------
                                        4074824248   100% |   zfree /root/redis/src/zmalloc.c:290 (inline)
4071521519  2.03% 53.56% 4074824248  2.04%                | zfree /root/redis/src/zmalloc.c:298
----------------------------------------------------------+-------------
                                        2734987566 67.15% |   createEmbeddedStringObject /root/redis/src/object.c:85 (inline)
                                        1328220477 32.61% |   processMultibulkBuffer /root/redis/src/networking.c:1888 (inline)
4069748453  2.03% 55.60% 4073127307  2.04%                | ztrymalloc_usable /root/redis/src/zmalloc.c:113
----------------------------------------------------------+-------------
                                        2228192339   100% |   processCommand /root/redis/src/server.c:4076
2228192339  1.11% 56.71% 2228192339  1.11%                | getNodeByQuery /root/redis/src/cluster.c:5866
----------------------------------------------------------+-------------

```"
906857629,9356,zuiderkwast,2021-08-27T01:45:53Z,"Nice @filipecosta90! The call to sdslen (in dictSdsKeyCompare) is the first access, so it's what loads the key from memory (unless it's already in the cache).

```C
static inline size_t sdslen(const sds s) {
    unsigned char flags = s[-1];
    switch(flags&SDS_TYPE_MASK) {
```

When a key is added to db, the dictEntry is allocated in dictAddRaw just after the sds key in sdsdup.

```C
void dbAdd(redisDb *db, robj *key, robj *val) {
    sds copy = sdsdup(key->ptr);
    dictEntry *de = dictAddRaw(db->dict, copy, NULL);
```

... so if they're in the same cache line, the sds key is already loaded when the dictEntry is loaded. If they're in the same bin size, they're more likely to be allocated next to each other, thus to be in the same cache line.

If this hypothesis is right, this branch should beat unstable on reads for key lengths 31-36 bytes (the 40 bytes bin when sds overhead is added). We can use #9394 to test it, or `--key-prefix` for memtier."
907000670,9356,oranagra,2021-08-27T07:47:04Z,"It's possible that we're focusing on the slowest thing we see, but that's also as slow in the benchmark on unstable, and thus maybe we need to run a similar proofing on unstable and compare them.

If I read the calculations above correctly, the chances of a dict entry or sds to be in the cache in that Benchmark are slim, right?"
907073622,9356,zuiderkwast,2021-08-27T09:45:33Z,"I repeated the benchmark with 33 bytes keys and voilà the large dictSdsKeyCompare is gone again, as predicted.

<details>
  <summary>FlameGraph GET with 33 bytes keys</summary>

![metadata-get-33B-keys](https://user-images.githubusercontent.com/273886/131104753-2f30685f-5bcf-4efe-8f62-69467b6ce1dd.png)
</details>

<details>
  <summary>Benchmark details for the above graph</summary>

Preparation: `redis-benchmark -p 30001 --cluster -n 10000000 --threads 8 -P 100 -r 1000000000 set 'key:{tag}:__rand_int__:0123456789' 'abc'`

Benchmark: `redis-benchmark -p 30001 --cluster -n 10000000 --threads 8 -P 100 -r 1000000000 get 'key:{tag}:__rand_int__:0123456789'`

```
Summary:
  throughput summary: 3057169.25 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        1.391     0.112     1.455     1.919     2.215     3.799
```

The same benchmark on unstable:

```
Summary:
  throughput summary: 2840102.25 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        1.444     0.168     1.535     1.967     2.151     3.535
```
</details>

> It's possible that we're focusing on the slowest thing we see, but that's also as slow in the benchmark on unstable, and thus maybe we need to run a similar proofing on unstable and compare them.

@oranagra No, the same profiling was done on unstable already. We were focusing on the regression specifically. See the flame graph for ""unstable, GET"" above and ""diff, GET"", specifically the block ""dictSdsKeyCompare"" in the middle, it almost invisible in the unstable diagram (next to dictFind in the middle) and it sticks out in red in the diff diagram.

> If I read the calculations above correctly, the chances of a dict entry or sds to be in the cache in that Benchmark are slim, right?

The changes the key is in the cache is very high for keys of the same bin size as the dictEntry, I would say, since they're allocated just after each other in dbAdd by the same slab allocator. Even if they're not always in the same cache line, the sds key is very likely to be in the L2 and L3 caches already once the dictEntry has been accessed.

Kudos Jim for the cache line idea and @ShooterIT for bringing up the bin sizes and everyone else for the motivation to investigate this! :clap:

If we'd want to optimize this regardless of key length, I can see one possibility: We can embed the key in the dictEntry allocation. :-) It's an obscure idea, but it would increase the changes it's in the same cache line as the dictEntry."
907080328,9356,zuiderkwast,2021-08-27T09:56:22Z,Btw: I believe keys of length 15-22 (the 24 bytes bin) are more common than keys in the 40 bytes bin.
907098302,9356,ShooterIT,2021-08-27T10:24:59Z,"@zuiderkwast another idea hits me, could we store expire time into dict entry metadata, expire time dict cost too much memory, especially, rehashing on big hash table.

the sum of `expire time, pre pointer, next pointer` is 24 byte, you designed new entry is 40, total size is 64(one cache line).
and that may accelerate accessing key expire time. "
907118027,9356,zuiderkwast,2021-08-27T11:00:42Z," @ShooterIT Expire is only used for keys with TTL. If you don't use TTL, this is wasted space."
907148365,9356,ShooterIT,2021-08-27T11:58:48Z,"@zuiderkwast Yes, I initially think we have different dict entry for keys with TTL or keys without TTL, we allocate new dict entry(with TTL metadata) when setting expire time,  but  i am not sure there is a clear way to implement."
907297349,9356,madolson,2021-08-27T15:45:51Z,"@zuiderkwast I think trying to poke around to see if we can embed the key into the dict entry efficiently is perhaps a good idea. We prototyped it at AWS few years ago but didn't like the complexity at the time. The counter idea was to try to prefetch the key into the CPU cache using GCC builtins. 

As an aside, within AWS we've found that the average key size is about 17 bytes, so that sort of lines up and is what we do benchmarking on"
907299003,9356,madolson,2021-08-27T15:48:34Z,"I'm happy merging this given the above information, we can always do follow up to continue diving into key specific performance optimizations, but that doesn't seem like an ideal quick win that Redis likes to focus on :)"
908105175,9356,zuiderkwast,2021-08-30T07:31:08Z,Rebased and squashed to two commits (see top comment). Don't know why Mac build failed.
908669093,9356,madolson,2021-08-30T20:26:42Z,"I kicked off the runs again, I can't tell what went on with the mac build either."
908786419,9356,QuChen88,2021-08-30T23:55:37Z,"I just ran the same benchmark on AWS EC2 with a server instance that has dedicated CPU/memory resources. Here is my set-up:

1. Server: One EC2 r5.2xlarge instance in VPC, cluster mode enabled, running redis-server code before and after this change. 
2. Client: EC2 c5n.9xlarge in the same VPC but a different availability zone, runs the same redis-benchmark command as above, mainly focus on testing for 1 million keys and 10 million keys range. 

Below are my performance run findings:

1. If I run the benchmark only once (which takes only ~10 seconds to complete), I am seeing similar performance characteristics as what @zuiderkwast  reported - SET throughput increased by 25-30%, and GET throughput dropped by ~7%. Please note that in this set-up, the SET operations are mostly adding new items into Redis here given that we only run it for a very short period of time, so ADD operations are predominate here. 
2. Now I run the same benchmark in a loop (i.e. with `-l` option) to simulate the steady state read/write workload when the Redis server has already been populated with data, here both SET and GET shows a ~6-7% drop in TPS. Please note that in this set-up, the SET operations are mostly doing an UPDATE operation instead of an ADD operation. I think this mostly comes from the fact that UPDATE of an existing item in Redis doesn't need to update `key_to_slots` data structure like what adding a new item does. 

After discussion, @madolson brought up to me the theory that the read performance drop mostly comes from the key size being 16 bytes which makes the key and dictEntry fit on the same cache line without this change... So I did another run with the same benchmark testing with longer key sizes (i.e. from 16 Byte to 26 Byte), still the results are the same in terms of read performance. 

In summary, I'd like to call out the fact that the performance gain only applies to adding new items into Redis, which typically happens during the initial phase when the Redis server is getting warmed up with frequently accessed data. When steady state is reached, even though Redis user can still add new items into Redis (i.e. when they use expiry), I think that still has a performance penalty due to cache miss, and a well tuned redis application would try to avoid that (e.g. something like 80% read and 20% write is a typical scenario) **Given that Redis users are a lot more likely to be doing read and update operations instead of adding new items into the cache in steady state scenarios and experience performance drop, I am not in favor of merging this change.** 

More details on the steady state performance results below (Sorry the redis-benchmark I ran didn't give a better precision in terms of latency numbers for p50)

Keyspace length |   | unstable | This PR
-- | -- | -- | --
r=1,000,000,000 | SET | 645K requests per second, p50 = 7 ms | 610K requests per second, p50 = 7 ms
  | GET | 810K requests per second, p50 = 5 ms | 760K requests per second, p50 = 5 ms
r=1,000,000 | SET | 710K requests per second, p50=6 msec | 660K requests per second, p50=6 msec
  | GET | 880K requests per second, p50=5 msec | 804K requests per second, p50=5 msec


"
909056594,9356,zuiderkwast,2021-08-31T09:18:41Z,"@QuChen88 That's a very interesting read! Thanks for benchmarking in a proper environment.

I agree, it's the ADD operation that is optimized, but also DEL, though we didn't test that. (Deletions happen also when accessing an expired key.)

It's also a memory save, though I think the benchmark is biased here too, since we use really small values, so the keys make up a higher percentage of the memory.

> So I did another run with the same benchmark testing with longer key sizes (i.e. from 16 Byte to 26 Byte), still the results are the same in terms of read performance.

These keys would go in the 32 bytes allocator bins. They need to go in the 40 bytes bin (the same as the new dictEntry) to benefit from this, so try 31-36 bytes keys. That's what I did in https://github.com/redis/redis/pull/9356#issuecomment-907073622. That's an unusual key size, but it backs the analysis.

I would like to attempt a follow-up PR to ""embed small keys in dictEntry metadata"" (AKA ""coallocate""). That should eliminate the cache misses that cause the regression here. I hope it can be made simple enough.

Regarding the typical 80%/20% reads/writes ratio, I want to share a different use case. In Ericsson, we have systems that have almost only writes (though the majority are updates, not adds). Most of the user/session data is cached in the application itself and Redis Cluster is used more like a backup. Traffic for the same user is routed to the same application instance to make sure the in-application cache is utilized. Reads are done mainly when users are routed to a different application instances, e.g. when scaling, updating, etc. Mobile networks require really low latency and this design works for that."
909393440,9356,madolson,2021-08-31T16:29:50Z,"@zuiderkwast I did testing and validated the claim that DEL are also improved, along with inserts. Qu and I were chatting about it yesterday, and he did convince me that the follow up of trying to optimize small keys (I suppose up to 32bytes, to keep it in a 64 byte cache line) could be embedded into the dict entry. Let's move any future discussion to a new issue, but I'll assume you'll dive into it then :)"
909941989,9356,oranagra,2021-09-01T06:23:24Z,"meanwhile, can you please update the summary at the top (will be linked in the release notes), with a summary of what was done, what are the implications, and why we decided to accept them."
910092746,9356,zuiderkwast,2021-09-01T09:11:02Z,"Note that a UUID is 36 bytes (on hexadecimal form). It goes in the 40 bytes bin. Since they lack a common prefix, there might be even more than 20% memory savings for these. Those who use UUIDs are likely the real winners here."
910098113,9356,zuiderkwast,2021-09-01T09:18:19Z,"> meanwhile, can you please update the summary at the top (will be linked in the release notes), with a summary of what was done, what are the implications, and why we decided to accept them.

I've updated it. Does it clarify why you decided to accept them? If not, please update."
981700502,9356,oranagra,2021-11-29T14:44:38Z,"For the record, i just realized this this change also fixes an issue.
in the past, defrag.c wasn't defragging the slot-to-keys, so these allocations could have prevented the allocator from releasing pages and result in a defragger that's consuming a lot of CPU and sin't gaining anything, see #9773.
Now since they're part of the dictEntry, they'll get implicitly defragged with it, so problem solved!
i'll update the top comment for the sake of release notes."
981745414,9356,zuiderkwast,2021-11-29T15:30:37Z,Nice! Maybe that's why my colleagues didn't see any benefit of defrag and turned it off. (They also wanted to turn off the allocator statistics because of the costs to keep them updated at every alloc and free. I think I mentioned this once.)
981870043,9356,madolson,2021-11-29T17:49:52Z,"That's awesome! We've also seen limited benefit in the past, good to know this might have been part of the problem."
981969427,9356,oranagra,2021-11-29T20:01:03Z,"Can someone have a look at the above mentioned ticket and confirm it makes sense that this is due the cluster slots mapping? IIRC it's redis 4, which used to use rax, and the fragmentation was mainly in the 24 byte bin. "
1084130365,9356,funny-falcon,2022-03-31T06:05:41Z,"Excuse me for being late...
Wasn't it simpler to have 16384 dicts (in worst case) - one dict for cluster slot? Empty slot could live without dict."
1084228386,9356,zuiderkwast,2022-03-31T07:57:37Z,"@funny-falcon Maybe it is simpler but now the other solution is already done. :-) Do you want to investigate the dicts idea and benchmark it?

I think 16384 dicts (or 5400 dicts, a 3rd of the slots) is a lot of memory allocations, especially if there is only one key in each slot."
1084742830,9356,JimB123,2022-03-31T15:23:44Z,"> Wasn't it simpler to have 16384 dicts (in worst case) - one dict for cluster slot? Empty slot could live without dict.

The question this solves is: ""give me all the keys in slot X"".

Mapping slot-to-key is only used rarely - for things like slot-migration.  Normally, we look up keys without regard to slot.

Having 16k dictionaries (in place of the main dictionary) would increase lookup time as we'd need to find the correct dictionary first.  It would also be fairly disruptive to the code base as there are a lot of operations which use the main dictionary which would have to be redesigned to hash to slot first and then lookup.  Anything that iterates on the main dictionary would have to be redesigned to iterate over 16k dictionaries (or the portion of 16k which are on a given node).

Having 16k dicts in addition to the main dictionary is much more memory than the linked list solution and the direct item access isn't needed for the problem being solved.

IMO stringing a linked list through the existing dictionary is a good solution.  

And, of course, this is FAR more efficient (both time & space) than the old RAX table implementation.
"
1084768437,9356,funny-falcon,2022-03-31T15:45:58Z,"Sorry, I had deal with Redis instances with 6-10GB per instance - ~10M keys. And ~500 slots per instance (yes, 33 partitions).
Doubtfully linked list links (16*10M=160M) consumes less memory than per-slot dictionary."
1084831332,9356,zuiderkwast,2022-03-31T16:42:47Z,"If we replace the db dict with these 16K dicts, then I can agree it doesn't use more memory. I assumed you wanted to use separate dicts just as to reference the set of keys in each slot.

If we want a two-level hash table anyway to avoid huge allocations, this could be a possible way forward. It is similar to #9517. However, in the extreme case that a node has only one slot (16K shards) there will be only one dict again."
1100451567,9356,madolson,2022-04-15T22:45:20Z,"@funny-falcon I'm going to promote this to it's own issue, I think it's a good idea, https://github.com/redis/redis/issues/10589."
1285244511,9356,judeng,2022-10-20T09:47:50Z,"finally followed.
we could fundamentally consider the application scenario of `slottokey`, basically in the scenario of cluster rehash and this function is used infrequent(In our practical experience, 90% of the clusters do not use the rehash once a year.). 
we can make a tradeoff and use the new migraite mechanism to completely remove the storage and performance overhead of the `slottokey`
This is a preliminary design:
1. fork a child
2. scan the keys in child process, find the slot key and write to dest node
3. Synchronize the incremental data of this slot 

This solution will increase the CPU overhead of the child process in step 2, but considering the overall ROI, compared with the performance and storage that can be improved in the main process in most of the time, this overhead is worth it."
1297099585,9356,uvletter,2022-10-31T13:33:49Z,"@judeng your idea is somewhat like what #10933 proposes, or the implementation  in Tencent or Bytedance' internal forks, which regards migration as a special full-sync replication"
1305559018,9356,judeng,2022-11-07T12:45:54Z,"@uvletter cool job! If convenient, can you tell me about the memory savings after `slottokey` be deleted? :-)"
1305635008,9356,uvletter,2022-11-07T13:42:05Z,"I don't know… migrating with full replication is mainly for mitigating the impact involved by original migration like blocking with big key, or resource contention between business and operations. Removing `slottokey` is not the main concern, but of course `slottokey` can be removed in that way depending on if you concern memory or CPU/time resumption more."
1306018093,9356,madolson,2022-11-07T18:27:10Z,"@uvletter I didn't see any comments from you on https://github.com/redis/redis/issues/10933, did you have any additional insights you wanted to provide?"
1306551415,9356,uvletter,2022-11-08T02:53:54Z,"> @uvletter I didn't see any comments from you on #10933, did you have any additional insights you wanted to provide?

Yes, I only roughly viewed the raw proposal before as when I look at it the discussion list is quite long... I think the general direction and HLD is pretty well(silence is also an attitude I guess), looking forward the next moving. I will look into the whole discussions when I have some time."
1288988374,11193,oranagra,2022-10-24T12:52:09Z,"i think i'd like a test that uses UNLINK too (and mention both FLUSHALL ASYNC and UNLINK in the top comment, for clarity)"
1308281792,11193,oranagra,2022-11-09T06:39:42Z,"@moticless i realized we're pausing write commands coming from, but in fact we should pause DENYOOM commands.
i.e. we want to pause clients that can increase the memory usage, but there's no need to pause DEL and UNLINK"
1308308507,11193,moticless,2022-11-09T07:09:38Z,"@oranagra
Cool. I will add another `PAUSE_ACTION_CLIENT_DENYOOM` action to `pauseActions()` (Along with `PAUSE_ACTION_CLIENT_WRITE`) and refine `ClientWritePauseDuringOOM()`. 
"
1308357358,11193,guybe7,2022-11-09T08:02:09Z,"after a discussion with @oranagra:
i think that this PR attempts to solve a theoretical problem (no one complained about FLUSHALL ASYNC causing eviction/OOM) and creates a new semi-theoretical problem (clients using pipeline during FLUSHASYNC are more likely to cause eviction/OOM due to the large query/output buffers after this PR)
i don’t think we will find an air-tight solution, but i think we should at least try to further minimize the impact by removing the read event of rouge clients. we may still reach OOM/eviction due to a lot of medium-sized buffers if many clients are using reasonably-sized pipelines. (it makes little sense to solve one theoretical issue but leave another unsolved, if we solve theoretical issues, let’s solve both)"
1308639871,11193,soloestoy,2022-11-09T11:56:27Z,"I'm still worry about the rtt problem caused by this PR, it's hard to trouble shoot the timeout reason(write command paused by OOM with pending lazyfree).

the client client pause mechanism is used in `client pause` command and failover, they are all active behavior, but oom is passive behavior. If the oom client pause lead to client timeout, how to quickly locate the root cause and measure the paused (timeout) time?"
1308819033,11193,moticless,2022-11-09T14:07:38Z,"@soloestoy,
IMHO, we need some flow control mechanism when clients are overloading Redis with I/O data.  Especially when Redis reach OOM. It is better to delay the client or even to make it disconnect (like we do today when client's query-buf reach some limit) rather than aggregate too much data that is more than Redis can handle.
"
1308858228,11193,oranagra,2022-11-09T14:35:06Z,"how about adding a latency monitor event?
i.e. call `latencyStartMonitor` when we initiate pause, and call `latencyEndMonitor`. it doesn't really reflect the average impact on latency since a similar impact can be maybe caused buy one long pause, and also many small ones, but in some sense LATENCY HISTORY can track these, and maybe we can even add a LATENCY STATS command which will give you some average latency over time.

another idea we can replicate is the metrics added in #9031 and #9377.
maybe these can help identify the problem, specifically for people who know where to look.. so maybe the next thing is to add something to the dreadful LATENCY DOCTOR."
1309700249,11193,soloestoy,2022-11-10T02:58:27Z,"flow control is needed and interesting, but I have to say the current client pause mode by blocking client cannot work well. it only postpone command processing, but cannot control the real input and output flow. if client is pipelining, it will continue eat lots of querybuf memory, and if we keep lazyfree memory (unlink command or lazyfree expire), then the waiting would be infinite in theory."
1309829562,11193,soloestoy,2022-11-10T06:22:35Z,"and about the monitor, I think the info metrics is better, always latency means the server's block or execution time, but client pause doesn't eat server's time."
1310034961,11193,moticless,2022-11-10T09:56:46Z,"Ok, maybe I will add for now `total_time_client_paused_during_oom`.

Limiting out-buffers: What about simply to mark all the commands that we can safely execute during OOM (say, CMD_OOM)  and drop the word 'write' from the terminology of this feature/fix. 

Limiting in-buffers: As for the challenge to control query-buffers fill-up (that is masking somehow kqueue/kevent), we need to decide whether it should be part of this PR, or in another one."
1337876672,11193,JimB123,2022-12-05T18:08:00Z,"The concept for this PR raises some questions for me.  Is this really a problem?  Is this the right ""solution""?  Here's what I'm thinking:
* In general we should expect that freeing an item will be faster than allocating an item AND filling it.  In the case of freeing, we're  avoiding the address dereference and memory access (as we don't need to actually read/write the block we are freeing).
* In the scenario given in the initial statement, we try to create a worst-case freeing situation (lots of little blocks) with a best case allocation scenario (fewer large blocks).  It seems that this would be a rather unusual scenario as we would expect that typically, when flushing and reloading data, that the new data would be somewhat similar in nature to the old data.
* What's missing in this analysis is any discussion of fragmentation.  While zmalloc's `used_memory` is being adjusted down by frees and up by allocations, REAL memory is behaving a bit differently.  Given jemalloc is a slab allocator, new pages are being allocated for the new large items, while old pages remain as the small items are being randomly deleted.  This means that no physical memory is released until very near the end of the async flush.  Even though `used_memory` may remain constant, we can easily generate swap usage in this case.  `used_memory` is a poor proxy for actual memory.

And what's the worst that can happen here?  In an extreme case, where the fundamental nature of the data is being dramatically changed AND we are aggressively refilling the DB after an async flush, some items which have been identified by the user as evictable are evicted.  That doesn't seem unreasonable to me."
1339188781,11193,moticless,2022-12-06T11:36:06Z,"Hi @JimB123 ,
You are right that it is a very unusual scenario and most probably old data in the common case will look similar to the new data and therefore removal will be faster than insertion. Yet, there might be a user out there that might have a DB with two big hashes such that the first one holds many small keys and the other hash holds very big keys and now database reconstruction might lead to this improbable scenario. Etc. On the other hand the price is not too high for (selective) blocking clients in case of OOM+Lazyfree. 

Regarding your comment about `used_memory`, I am far from having an in-depth understanding in jemalloc integration into Redis, but as I get it, the current approach attempts to tackle the memory issue at two distinct levels. Memory allocation vs. Redis keyspace. At Redis keyspace it cannot do much but to count memory without being aware to paging and fragmentation. Obviously `maxmemory` is not accurate value at OS level, but it is a decent approximation, constant and deterministic. So the user must keep a margin of safety."
1339714251,11193,JimB123,2022-12-06T17:19:16Z,"@moticless Regarding `used_memory` and jmalloc, for the scenario constructed here `used_memory` is not ""a decent approximation"".

For discussion, let's assume the original data used 8-byte blocks and the new data uses 64-byte blocks.

In jmalloc, all of the 8-byte blocks are placed into the same 4k pages.  Each 4k page is like an array of 8-byte values.  Similarly, the new 64-byte blocks are placed in pages dedicated to 64-byte values.  Each 4k page is like an array of 64-byte values.

Each time Redis frees an 8-byte block, it frees space on one of the pages for 8-byte values.  You can allocate a new 8-byte value into that space, but NEVER a 64 byte value (even if multiple consecutive 8-byte values have been freed).

Now consider that we have 1000s of pages for 8-byte values - and we start randomly freeing.  We would expect that all of the pages would be decreasing in fullness at about the same rate.  So, at the point that half of the values have been deleted:
* all of our pages are about half full
* `used_memory` has decreased by 50%
* **not a single page of memory can be released/reused**

We need a page to be completely empty before it can be released or reused.

For the given scenario, ACTUAL memory will increase as new key/values are loaded - quite possibly driving the system into swap.  Near the very end of the async flush, lots of memory will suddenly be released.

The result is that this proposed mechanism solves an ARTIFICIAL problem.  It keeps the `used_memory` statistic in check, while doing nothing to address the real problem of growing memory usage.  This possibly prevents a few evictions of the new values (further increasing swap).  However, the scenario seems so contrived that I question the value of trying to prevent such evictions."
1340584060,11193,oranagra,2022-12-07T08:33:19Z,"@JimB123 so before this PR, in some case that use FLUSHDB ASYNC would risk OOM kill, and others (if there's sufficient memory on the machine, or swap configured :nauseated_face:), would only risk eviction or OOM errors (which IMHO is a logical bug that we can fix).
Swap trades OOM kill with performance degradation, but still risking eviction and OOM.

> And what's the worst that can happen here? In an extreme case, where the fundamental nature of the data is being dramatically changed AND we are aggressively refilling the DB after an async flush, some items which have been identified by the user as evictable are evicted. That doesn't seem unreasonable to me.

i do think that's unreasonable, if the new data is ""evictable"", then it could be that after population the user can see half of the new data evicted, but he's well blow the memory limit. but maybe the more severe effect is OOM (possibly terminating the workload, unless the user implements a retry on OOM error).

but anyway, we have 3 options:
1. leave it as it was
2. keep this PR to fix one problem without the other (accepting some complexity).
3. extend our efforts to improve the solution to solve the RSS problem as well.

are you promoting 1 or 3?

p.s. on some orchestrations, there could be an external daemon monitoring the machine's free memory and adjusting maxmemory accordingly. in these, the current form of the PR won't have the problem you mentioned."
1343182145,11193,madolson,2022-12-08T18:49:33Z,"Reading through this thread again, I'm really struggling to justify the existence of this feature. We still don't believe it's a real workload, just a theoretical one. 

> i do think that's unreasonable, if the new data is ""evictable"", then it could be that after population the user can see half of the new data evicted, but he's well blow the memory limit. but maybe the more severe effect is OOM (possibly terminating the workload, unless the user implements a retry on OOM error).

I think we are placing too much weight into the argument that OOM/evictions are logical errors. If you are ""loading"" data that is evict-able, data might be evicted. I also think it's possibly just as likely users will complain about elevated write calls stalling out their client and triggering reconnects.

> leave it as it was

I think we should do this."
1348011231,11193,moticless,2022-12-13T09:17:40Z,"@madolson, 
I thought we already agreed, at corresponding [issue](https://github.com/redis/redis/issues/10907), that this is not theoretical race but reproducable scenario. 

This step forward to backpressure in case of OOM and data that soon be released along the pipeline is rather well known pattern in networking. That is, momentary pause and backpressure in order not to overload the receiver. This our best effort to avoid redundant evictions.

Actually it surprised me that redis only has single limitation on the query-buffer which is `client-query-buffer-limit` (default 1GB per client. And behind it, it simply closes the client). And until that limit, querybuf is being filled up withtout any control. "
1350240642,11193,madolson,2022-12-14T01:58:34Z,"> I thought we already agreed, at corresponding https://github.com/redis/redis/issues/10907, that this is not theoretical race but reproducable scenario.

Just because something is ""reproducible"" doesn't imply that it is actually a concern. On that issue you outlined a synthetic workload. I did agree to it then, but Jim also brought up the fact that it doesn't really solve the memory issue, it's just shifting around the problem because of how the underlying jemalloc bins are constructed.

> This step forward to backpressure in case of OOM and data that soon be released along the pipeline is rather well known pattern in networking. That is, momentary pause and backpressure in order not to overload the receiver. This our best effort to avoid redundant evictions.

Don't conflate two disjoint problems. Backpressure is a legitimate form of a flow control, and I buy into that notion, but I'm saying I don't think you can fairly say that it applies here. Getting an OOM message is also applying back pressure to the application to try again."
1387154994,11193,oranagra,2023-01-18T14:22:30Z,removing this PR from 7.2 project to sit stale until the day we'll have a reason to reconsider.
2016982293,11193,CLAassistant,2024-03-24T23:12:30Z,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/redis/redis?pullRequest=11193) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/redis/redis?pullRequest=11193) before we can accept your contribution.<br/><hr/>**moticless** seems not to be a GitHub user. You need a GitHub account to be able to sign the CLA. If you have already a GitHub account, please [add the email address used for this commit to your account](https://help.github.com/articles/why-are-my-commits-linked-to-the-wrong-user/#commits-are-not-linked-to-any-user).<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/redis/redis?pullRequest=11193) it.</sub>"
999797150,9963,MeirShpilraien,2021-12-22T19:00:07Z,"Suggestion, why not using the script timeout mechanism here:  https://github.com/redis/redis/blob/e33e0295bb18be14eee1f3ff162dff9522e37e20/src/script.c#L71?
We can create `scriptRunCtx` and set it on RedisModuleCtx, then we will be able to call this API and share the code, WDYT?

In addition, I believe we should consider whether or not this can be called from a thread safe context (I believe that we should support it, many of our modules needs to call if from a thread). And we should probably add tests that uses it from a thread safe context if we decide that its supported."
999837183,9963,oranagra,2021-12-22T20:15:00Z,"@MeirShpilraien if feels to me like trying to tie two things that are not really related, and i suspect it'll cause complications in the future.
maybe i'm missing something, but if it's just about replicating 10 lines of code that don't contain any very sensitive code, we better go with the cloning approach."
999898580,9963,guybe7,2021-12-22T21:43:18Z,"maybe i'm missing something but if we have a module command that does something heavy, shouldn't it just block the client and do the heavy lifting in a thread?
or maybe we're talking about a case where you deliberately want to prevent the server from executing most of the commands while your module command is doing something? is there a concrete example for such a case?"
1000086683,9963,MeirShpilraien,2021-12-23T06:55:43Z,"> @MeirShpilraien if feels to me like trying to tie two things that are not really related, and i suspect it'll cause complications in the future.
maybe i'm missing something, but if it's just about replicating 10 lines of code that don't contain any very sensitive code, we better go with the cloning approach.

@oranagra I can see how it can be useful on more places, for example `RM_Call` and `scriptCall` has a huge code duplication of basically invoking commands on Redis.

I agree though that it will probably require more effort to match this code to what we need so we can leave it for the future I guess.

> maybe i'm missing something but if we have a module command that does something heavy, shouldn't it just block the client and do the heavy lifting in a thread?

@guybe7 the use-case is that when a module need to do a long operation which need to be atomic and then it needs to lock the GIL for a long time."
1005730719,9963,oranagra,2022-01-05T14:26:12Z,@MeirShpilraien do we wanna add a test using thread safe context? and maybe one that calls this from the rdb_load callback? i guess we do..
1008579065,9963,oranagra,2022-01-10T06:38:27Z,@redis/core-team please approve
1009686071,9963,madolson,2022-01-11T07:59:29Z,"It does like we processes event loop file events:
https://github.com/redis/redis/blob/a1ae260e8addad04e1a73348c8f7bfeab398c2a9/src/networking.c#L3719. Which should include cluster events. I still believe the function should be able to determine what types of ""processing"" is done when it yields, since a failover could happen for example."
1013135901,9963,oranagra,2022-01-14T13:52:45Z,"@madolson do you mean that the module will provide some flags to tell redis what is ok to do inside that yield and what's not?
i think this kind of interface is too complex and will be hard to support when future changes are done.

also, isn't it already a bug that cluster failover can happen during busy scripts? in which case, we don't need to provide any flags for the module, but rather fix cluster.c or something like that."
1013800558,9963,madolson,2022-01-16T03:01:53Z,"My suggestion for a flag was more to differentiate two different cases.
1. Let the module yield to allow cluster events to happen, so it isn't marked as dead by other nodes. The flag would indicate the command is making progress, but isn't done yet.
2. Let the module yield to allow a client to potentially kill it. This will also allow cluster events to happen, but the main purpose is to allow us client commands to execute. This would also start throwing errors to clients, which is the behavior I would prefer to avoid.

I also think something like extending the client pause blocking mechanism, the one that defers command execution based on what command was sent, for commands that can't be run during the yield context might also work. A newly established connection could send a kill command whenever it wanted, but all other clients would be silently blocked.

For cluster failover, we should probably defer the failover till a later point. "
1013826200,9963,oranagra,2022-01-16T07:33:52Z,"@madolson AFAICT all of that is applicable for scripts too.
* the fact that we reject commands with BUSY error rather than postpone them for later.
* the fact we can process cluster events (and PING) earlier so that the server isn't marked as dead.
* the fact we should avoid triggering failover in the wrong state.

so these are all old issues that we should someday improve, and the question is if it should affect the module API.
considering the module doesn't know it's hung (if it did, it would have stopped), it just knows the operation can take time, and wants to allow the operation to be monitored and killed.

let me make sure i understand, you're suggestion a flag to enable processing of the kill command like so (the new process_commands arg):
```
/* This API allows modules to let Redis process life checks and some commands during long
 * blocking execution of a module command. The module can call this API
 * periodically, if process_commands is true, after the time defined by the `script-time-limit` config,
 * Redis will start rejecting most commands with `-BUSY` error, but allow the
 * ones marked with the `allow-busy` flag to be executed. This API can also be
 * used in thread safe context (while locked), and during loading (in the
 * rdb_load, in which case it'll reject commands with -LOADING error) */
void RM_Yield(RedisModuleCtx *ctx, int process_commands, const char *busy_reply) {
```"
1015233235,9963,oranagra,2022-01-18T09:39:40Z,"i added commits to handle what's decided in the core-team meeting
* improve documentation around the renamed config
* add a flag to RM_Yield to let modules yield to redis tasks, but not to client commands.

i still didn't implement the logic behind the later, for now pushed the API change to see that we're all good with it."
1015881662,9963,oranagra,2022-01-18T22:13:28Z,"@madolson i'd appreciate a review of my last commit.
maybe we should rename `BLOCKED_PAUSE` to `BLOCKED_POSTPONED` since i think it just means that processCommand tried to process it and decided to postpone it.
or, if you find bugs in my code, maybe i need to create a dedicated `BLOCKED_YIELD` flag."
1017147005,9963,sundb,2022-01-20T06:01:44Z,"It seems that `replconfCommand` and `quitCommand` missing `CMD_ALLOW_BUSY`.
```c
    if (scriptIsTimedout() &&
          c->cmd->proc != authCommand &&
          c->cmd->proc != helloCommand &&
          c->cmd->proc != replconfCommand &&        <- missing
          c->cmd->proc != multiCommand &&
          c->cmd->proc != discardCommand &&
          c->cmd->proc != watchCommand &&
          c->cmd->proc != unwatchCommand &&
          c->cmd->proc != quitCommand &&            <- missing
          c->cmd->proc != resetCommand &&
          c->cmd->proc != shutdownCommand && /* more checks in shutdownCommand */
        !(c->cmd->proc == scriptCommand &&
```"
1017171113,9963,oranagra,2022-01-20T06:52:47Z,"> It seems that replconfCommand and quitCommand missing CMD_ALLOW_BUSY.


good catch.. i guess it was a bad merge conflict resolution."
1831181220,12817,sundb,2023-11-29T04:02:12Z,"`RM_UnblockClient`  is also not thread-safe.
`RM_UnblockClient()` -> `moduleBlockedClientTimedOut` -> `updateStatsOnUnblock()`(not thread-safe)
FYI @MeirShpilraien

```
WARNING: ThreadSanitizer: data race (pid=29808)
  Read of size 8 at 0x000104940040 by main thread (mutexes: write M0):
    #0 serverCron server.c:1376 (redis-server:arm64+0x1000214e4)
    #1 processTimeEvents ae.c:331 (redis-server:arm64+0x1000100b8)
    #2 aeProcessEvents ae.c:466 (redis-server:arm64+0x10000f614)
    #3 aeMain ae.c:496 (redis-server:arm64+0x1000103bc)
    #4 main server.c:7212 (redis-server:arm64+0x10003e76c)

  Previous write of size 8 at 0x000104940040 by thread T4:
    #0 updateStatsOnUnblock blocked.c:110 (redis-server:arm64+0x10016a564)
    #1 moduleBlockedClientTimedOut module.c:8375 (redis-server:arm64+0x10019cf8c)
    #2 RM_UnblockClient module.c:8205 (redis-server:arm64+0x10019cd98)
    #3 worker <null>:87148612 (blockonkeys.so:arm64+0x2740)
```

patch:
```diff
diff --git a/tests/modules/blockonkeys.c b/tests/modules/blockonkeys.c
index 94bb36123..b978f698e 100644
--- a/tests/modules/blockonkeys.c
+++ b/tests/modules/blockonkeys.c
@@ -4,6 +4,7 @@
 #include <strings.h>
 #include <assert.h>
 #include <unistd.h>
+#include <pthread.h>
 
 #define UNUSED(V) ((void) V)
 
@@ -442,6 +443,16 @@ int blockonkeys_popall_timeout_callback(RedisModuleCtx *ctx, RedisModuleString *
     return RedisModule_ReplyWithError(ctx, ""ERR Timeout"");
 }
 
+void *worker(void *arg) {
+    // Retrieve blocked client
+    RedisModuleBlockedClient *bc = (RedisModuleBlockedClient *)arg;
+
+    // Unblock client
+    RedisModule_UnblockClient(bc, NULL);
+
+    return NULL;
+}
+
 /* BLOCKONKEYS.POPALL key
  *
  * Blocks on an empty key for up to 3 seconds. When unblocked by a list
@@ -452,13 +463,20 @@ int blockonkeys_popall(RedisModuleCtx *ctx, RedisModuleString **argv, int argc)
         return RedisModule_WrongArity(ctx);
 
     RedisModuleKey *key = RedisModule_OpenKey(ctx, argv[1], REDISMODULE_READ);
+    RedisModuleBlockedClient *bc;
     if (RedisModule_KeyType(key) == REDISMODULE_KEYTYPE_EMPTY) {
-        RedisModule_BlockClientOnKeys(ctx, blockonkeys_popall_reply_callback,
+        bc = RedisModule_BlockClientOnKeys(ctx, blockonkeys_popall_reply_callback,
                                       blockonkeys_popall_timeout_callback,
                                       NULL, 3000, &argv[1], 1, NULL);
     } else {
         RedisModule_ReplyWithError(ctx, ""ERR Key not empty"");
+        return REDISMODULE_OK;
     }
+
+    pthread_t tid;
+    int res = pthread_create(&tid, NULL, worker, bc);
+    assert(res == 0);
+    
     RedisModule_CloseKey(key);
     return REDISMODULE_OK;
 }

```

command: `blockonkeys.popall k`"
1848961783,12817,oranagra,2023-12-10T13:10:55Z,"i lost track (or never had it), with all the comment threads here.
can you close the ones that are resolved (and list them in the top comment), so we can focus on what's not resolved?
or maybe it's too early, and you need more time to figure things out?"
1851899163,12817,sundb,2023-12-12T11:59:00Z,"introduced: https://github.com/redis/redis/commit/9b01b64430fbc1487429144d2e4e72a4a7fd9db2
reason: adding fake client to server.clients_pending_write
```c
src/redis-server 127.0.0.1:21111(updateClientMemUsageAndBucket+0x5b)[0x55bed1f9efcf]
src/redis-server 127.0.0.1:21111(writeToClient+0x21a)[0x55bed1fc2cd9]
src/redis-server 127.0.0.1:21111(handleClientsWithPendingWrites+0xb8)[0x55bed1fc2de0]
src/redis-server 127.0.0.1:21111(handleClientsWithPendingWritesUsingThreads+0x52)[0x55bed1fc9bda]
src/redis-server 127.0.0.1:21111(beforeSleep+0x271)[0x55bed1fa094e]
src/redis-server 127.0.0.1:21111(aeProcessEvents+0x9e)[0x55bed1f9671b]
src/redis-server 127.0.0.1:21111(aeMain+0x2e)[0x55bed1f96b03]
src/redis-server 127.0.0.1:21111(main+0xd21)[0x55bed1fb084f]
/lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7ffa6ba29d90]
/lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7ffa6ba29e40]
src/redis-server 127.0.0.1:21111(_start+0x25)[0x55bed1f908d5]
```"
1859996236,12817,sundb,2023-12-18T10:05:50Z,"@oranagra `moduleOwnsGIL()` isn't really right, `pthread_id` may not be numeric type or it may be struct, not sure if you've seen the earliest use of `__thread` by me to modify `ProcessingEventsWhileBlocked`, but `__thread` isn't a c99 standard, and I'm not sure if I should use it or not, but it's a good choice."
1860068582,12817,oranagra,2023-12-18T10:35:47Z,"@sundb is the last comment in the context of the `el_poll_mutex` or a new discussion?
i'd rather start discussions in threads so we can focus on each one.
p.s. i think maybe we can compromise on _thread and have certain capabilities of redis require that. but the problem with the pthread_id is probably an issue."
1864040391,12817,sundb,2023-12-20T08:13:29Z,"@oranagra @MeirShpilraien I've updated the top comment, all the issues have been handled, please take a look."
1864953185,12817,oranagra,2023-12-20T18:32:14Z,"@sundb thanks a lot.
can you try to add some comment about severity on each of these?
i.e. some are rather harmless, and others can cause a crash.
also, please copy the statement about when each of them was introduced (version or PR)"
1866171873,12817,sundb,2023-12-21T12:33:47Z,@oranagra I've updated top comment.
1869457140,12817,sundb,2023-12-26T10:56:00Z,"> do we wanna run the threads sanitizer in CI (to prevent regressions in this area)?

@oranagra It's needed, but right now some other warnings are not related to race conditions, like thread leaks, as this PR needs to be backported, and I'd like them to be handled in other PR."
1869470594,12817,oranagra,2023-12-26T11:20:44Z,"considering the size of this PR, i think we may choose to only backport some (safe, or high risk) parts of it, and not the entire thing.
still, we can leave these other aspects for a later PR."
1880618083,12817,oranagra,2024-01-08T09:11:04Z,don't forget to remove the RM_Yield part from the top comment (maybe just add a quick reference to the other PR as a one liner at the bottom of it)
1882454289,12817,sundb,2024-01-09T05:55:07Z,"race condition:
```
WARNING: ThreadSanitizer: data race (pid=53103)
  Read of size 8 at 0x000100ac83e0 by main thread (mutexes: write M0):
    #0 RM_BlockedClientMeasureTimeEnd <null>:16584324 (redis-server:arm64+0x10017cf44)
    #1 HelloBlock_Timeout <null>:16584324 (blockonbackground.so:arm64+0x2a80)
    #2 beforeSleep <null>:16584324 (redis-server:arm64+0x100082098)
    #3 beforeSleep <null>:16584324 (redis-server:arm64+0x100082098)
    #4 beforeSleep <null>:16584324 (redis-server:arm64+0x100082098)
    #5 aeProcessEvents <null>:16584324 (redis-server:arm64+0x100070830)
    #6 aeProcessEvents <null>:16584324 (redis-server:arm64+0x100070830)
    #7 aeProcessEvents <null>:16584324 (redis-server:arm64+0x100070830)
    #8 main <null>:16584324 (redis-server:arm64+0x10009d404)
    #9 <null> <null> (0x0001860550e0)
    #10 <null> <null> (0x0001860550e0)

  Previous write of size 8 at 0x000100ac83e0 by thread T4:
    #0 RM_BlockedClientMeasureTimeStart <null>:16584324 (redis-server:arm64+0x10017cf04)
    #1 BlockDebug_ThreadMain <null>:16584324 (blockonbackground.so:arm64+0x2afc)
```

@oranagra In the commit https://github.com/redis/redis/pull/12817/commits/d272368512bb62c44e241d263befbf4f3771dd12, I put `RM_BlockedClientMeasureTimeStart( )` and `RM_BlockedClientMeasureTimeEnd()` in the GIL, and it's constantly lock and unlock, otherwise the timeout callback wouldn't be triggered on sleep.
@filipecosta90 Can you have a better way to handle it?"
1883589732,12817,oranagra,2024-01-09T18:40:54Z,"I don't think it's reasonable to ask the module to lock the mutex before starting / stopping the time measurement.
maybe the timeout callback can somehow terminate the thread before it messes with the timer? or maybe we should have let the module track the time on it's own, and then just report it to redis at the end?
@MeirShpilraien WDYT?"
1884452198,12817,sundb,2024-01-10T09:09:58Z,"> maybe the timeout callback can somehow terminate the thread before it messes with the timer? 

force terminating threads is not good practice, are these tests misleading module developers?
and if the module thread is already in `RM_UnblockClient()`, will there be unpredictable behavior if we terminate it.

> or maybe we should have let the module track the time on it's own, and then just report it to redis at the end?

But the purpose of this test is to reply to the user in the middle of blocking, not to wait for the thread to end before replying."
1884477098,12817,oranagra,2024-01-10T09:25:38Z,"> > maybe the timeout callback can somehow terminate the thread before it messes with the timer?
> 
> force terminating threads is not good practice, are these tests misleading module developers? and if the module thread is already in `RM_UnblockClient()`, will there be unpredictable behavior if we terminate it.

i see that's what you ended up implementing.

> > or maybe we should have let the module track the time on it's own, and then just report it to redis at the end?
> 
> But the purpose of this test is to reply to the user in the middle of blocking, not to wait for the thread to end before replying.

the replying can be done during.. but reporting how much time was used to process the request can be reported to redis at the end (when you unblock the client, and the total time in that timer is added to the command stats)"
1900233105,12817,oranagra,2024-01-19T11:25:27Z,full CI: https://github.com/redis/redis/actions/runs/7583134243
1910269722,12817,PhanSon95,2024-01-25T13:56:55Z,We do not release a new version for this PR right ?
1910309500,12817,oranagra,2024-01-25T14:20:07Z,not yet. but we might backport some of these fixes to the next release of 7.2 (see the bottom of the top comment)
964218946,9748,zuiderkwast,2021-11-09T14:46:23Z,@bjosv 
976198482,9748,oranagra,2021-11-23T06:30:17Z,@redis/core-team please approve
983395559,9748,oranagra,2021-12-01T08:15:27Z,@yoav-steinberg please don't forget to make a redis-doc PR.
1120481244,9748,YaacovHazan,2022-05-08T20:19:26Z,"This PR introduces a breaking change.
I encounter it when using the TLS configurations with files path and the user overrides the files in place.
Example:
1. Generate certificate files (`tls-key`, `tls-cert`, `tls-ca`).
2. Run the server and connect the client using these certificates.
3. Override this certificates files with new ones (e.g. certificates rotation)
4. Using the connected client to `CONFIG SET tls-cert` with the same file name (the file overridden), and assuming that once one configuration is changed the Redis will reconfigure all TLS configuration (`tlsConfigure`).
5. Before Redis 7, the TLS context was re-configured with the new certificate files, but after this PR it will not run the `apply` function as the `prev` and `new` values are identical. And the user will not succeed to connect with the new certificate files although the `CONFIG SET` returned `OK`.

Code `config.c:stringConfigSet`:
`if (new != prev && (new == NULL || prev == NULL || strcmp(prev, new))) {`

@yoav-steinberg @yossigo WDYT?
Maybe we should add extra param to configs to enforce applying even if the values are identical.
I'm not sure if this is a common usage of TLS certificate files, where the user just overrides the exiting files and does not create new files with new names/paths.

Also in such a configuration, the `rolling back` mechanism not going to work as we don't save the internal state of the server but only the old configuration values.
"
1120761736,9748,yoav-steinberg,2022-05-09T07:49:45Z,"@YaacovHazan thanks for pointing that out.

I'm OK with the idea of having a flag marking some configs as ""apply always"", but I'd try to avoid it if possible, just to reduce complexity. 

Specifically in this case the problem is that what we're storing isn't the configuration but a ""point to the configuration"" (the file name being the pointer). This is contrary to all other config data in redis and therefore I'd aim for deprecating the mutability of the tls file related configs. And instead we should set the values directly:
`config set tls-key-data ""-----BEGIN RSA PRIVATE KEY-----....""`. This will resolve these issues and make the key/cert/ca data inline with how Redis stores all other configs. In other words store the config itself and not a file name.

Please note that a failure to apply these configs couldn't have rolled back previously either for the same reason. When implementing this I was aware of this and initially panicked the server. Later @yossigo suggested panicking was extreme so I changed the code to:
```c
serverLog(LL_WARNING, ""Failed applying restored failed CONFIG SET command: %s"", errstr);
```

As a workaround which seems also like the logical thing to do, I'd just use a different file name, this will also support rolling back in case of failure."
1120908388,9748,yossigo,2022-05-09T10:12:30Z,"@yoav-steinberg @YaacovHazan
Using a different file should be considered better practice that solves both problems. But for backwards compatibility, I'd consider a ""volatile config"" flag that indicates the value is only a pointer so we should always apply and either never attempt rollback or ignore failures."
1001209245,10004,oranagra,2021-12-26T16:32:27Z,"@MeirShpilraien i've set this to resolve the issue that describes it, assuming that all the ideas specified there are handled here.
please skim over the description and discussion there, and list here (maybe in todo section of the top comment) if there's something not handled yet."
1001212633,10004,MeirShpilraien,2021-12-26T16:58:21Z,"@oranagra function flags was missing, added it to todo list."
1001974790,10004,MeirShpilraien,2021-12-28T09:55:08Z,"@oranagra fixed the comments.
> One concern i have about naming, is the use of the term ""library"" without a context.
in the past i argued the same about ""engine"".
the term ""function"" is a lost cause (in the context of redis, a ""function"" is a ""script function"".
but maybe ""library"" should be called ""functions library"" (or ""functionsLib"")?
same as we called ""engine"" a ""script engine""

I think its ok to call it just library, but I also thought its ok to just say engine so it make sense we disagree :)
that said, I do not mind renaming. How about:
* libraryInfo -> functionLibInfo
* librariesCtx -> functionLibsCtx

Let me know if you are OK with it."
1002118432,10004,oranagra,2021-12-28T13:58:32Z,"@MeirShpilraien i'm not sure if just renaming these two is enough. maybe there are other functions (procedures) that should be renamed, comments that should be updated, variable names or command arguments, etc.

i suppose that in the context of a FUNCTION command, the term ""library"" is ok.
and same goes for anything in functions.c.
if you wanna consider this change, please grep the diff in this pr for the word ""library"". skip functions.c and all the FUNCTION json files, and see what you're left with."
1002427968,10004,MeirShpilraien,2021-12-29T07:08:09Z,@oranagra let me know what you think on the last PR (renaming)
1002431273,10004,oranagra,2021-12-29T07:18:26Z,"LGTM (only looked at the commit comment)
i assume you used this method to spot them:
> if you wanna consider this change, please grep the diff in this pr for the word ""library"". skip functions.c and all the FUNCTION json files, and see what you're left with.

"
1002432792,10004,MeirShpilraien,2021-12-29T07:23:39Z,@oranagra I actually look everywhere where any function API is used and then find out what need to be renamed. Will do another round to make sure I did not miss anything.
1002456349,10004,oranagra,2021-12-29T08:30:35Z,@redis/core-team this is ready for merge. please read the top comment and approve
1002478268,10004,MeirShpilraien,2021-12-29T09:22:24Z,Redis functions flags is discussed on https://github.com/redis/redis/issues/10025
1003030093,10004,MeirShpilraien,2021-12-30T13:30:09Z,"@yossigo there are API's on `redis` object that we do allow on `FUNCTION LOAD` context, for example `redis.log`. Do you think we should remove it?"
1003033369,10004,oranagra,2021-12-30T13:39:06Z,we'll soon probably wanna add `redis.version()` so the lib can register different functions / flags depending on the version.
1003128132,10004,yossigo,2021-12-30T17:56:58Z,"@MeirShpilraien I think we should opt-in on what we provide and not opt-out, and using a different name might make better distinction about these two run modes."
1003598885,10004,MeirShpilraien,2022-01-01T18:38:50Z,"@yossigo I see your point and I agree, so I guess we can introduce a new API, `library`, that will be available only on the load run and we can put there everything that is available to the load run (`library.register_function`, `library.log`, ...). WDYT?"
1004122661,10004,MeirShpilraien,2022-01-03T14:17:15Z,"@oranagra @yossigo , added `library` API and update top comment, let me know what you think."
1004240568,10004,MeirShpilraien,2022-01-03T17:24:06Z,"@yossigo @oranagra @itamarhaber @madolson 
Please notice that I added another commit (the last commit) that add `DENYOOM` flag to `FUNCTION LOAD` and `FUNCTION RESTORE`. Because we consider functions as data and those commands potentially will add new functions (data), I believe it make sense. Let me know what you think."
1006486767,10004,MeirShpilraien,2022-01-06T11:10:57Z,"@oranagra rename `library -> redis`, updated top comment."
1238045854,11199,MeirShpilraien,2022-09-06T11:56:13Z,"Thanks @oranagra, I have fixed your comments / replied to them. Waiting for more comments from the community."
1257170703,11199,MeirShpilraien,2022-09-25T11:14:06Z,"@oranagra @guybe7 I have fixed/replies to your comments. Summarising the open questions that remains:
1. Whether or not its OK to apply some of the suggested changes on a followup PR and keep this PR safer for cherry pick. Related comments:
   * https://github.com/redis/redis/pull/11199#discussion_r977863570
   * https://github.com/redis/redis/pull/11199#discussion_r979378474
   * https://github.com/redis/redis/pull/11199#discussion_r979380993
2. API generalisation where else can this API can/should be used, Related comments:
   * https://github.com/redis/redis/pull/11199#discussion_r979379652
   * https://github.com/redis/redis/pull/11199#discussion_r979380208

Notice that I am not resolving the comments so you can go over them and verify the fixes. Please resolve when you approve or let me know if you prefer that I will resolve them myself."
1279894546,11199,oranagra,2022-10-16T05:18:31Z,"@redis/core-team please take a look.
please note the new ""execution unit"" terminology."
1282695930,11199,oranagra,2022-10-18T16:47:41Z,conceptually approved in a core-team meeting. @yossigo please take a quick look to be on the safe side.
1308384343,11199,oranagra,2022-11-09T08:26:15Z,"PR was conceptually approved in a core-team meeting, with the exception of https://github.com/redis/redis/pull/11199#discussion_r1017596946.
it was also approved to be eventually backported to 7.0 in due time."
759273128,8315,madolson,2021-01-13T07:55:06Z,"@oranagra @guybe7 If you want to take another pass, I think everything is fixed now. Some open questions I still have:

1. Failoverto adds a fixed 5 second to the timeout if it detects a replica is caught up with it, regardless of what value is passed in. This is probably too high, should be configurable somehow.
2. Both nodes can end up as masters if the original master gives up after if it has sent the psync failover to the target replica. This seems like an edge case that should probably be handled, but isn't currently in this draft of the PR. 
3. Related to a previous comment about protocol version. If the primary is on the latest version and the replica isn't, the primary will demote itself but fail to handoff, so there should be no harm. We should also never send failover in that field for any reason.
4. Do we want a failover abort option? I think that was originally a concern, I'm not sure it's that necessary.
5. The pause stays in place until the the full-transfer has started/psync has succeeded. This is the most defensive option, but could also unpause the moment we demote the master. "
762467496,8315,yossigo,2021-01-18T20:57:36Z,"To me the main motivation to implement this as part of Redis is to provide a standard and safe way to handle manual failovers.

I think this approach can be slightly modified to make it safer and not necessarily less useful, mainly thanks to having the client read-only pause option:

* Initiate client read-only pause as soon as failover begins.
* Automatically unpause only on the happy path, i.e. `PSYNC FAILOVER` completed successfully.
* Not sure if `TIMEOUT` is really needed, but if we think it is - apply it only while waiting for offsets to sync but not beyond.
* If `PSYNC` fails we may retry until successful or until the user aborts (and unpause)."
763179586,8315,madolson,2021-01-19T22:22:12Z,"@oranagra @yossigo Looks like Allen is busy, so I will work to update the PR. To summarize our conversation:
* Rename it from failoverto to just FAILOVER. It will now look like FAILOVER [ABORT] [FORCE] [TIMEOUT X] [TO [HOSTNAME IP][ANY ONE]].
* The timeout only applies to waiting until we have synced up with the replica, at that point the timeout is no longer used. We want to prevent multi-primary when possible.
* The previous master will unpause whenever failover ""ends"" which will be either on the replica rejecting or accepting the command. All other scenarios will leave the previous master in a paused state.
"
764396907,8315,madolson,2021-01-21T05:48:54Z,See top comment again for overview.
767025874,8315,madolson,2021-01-25T18:36:04Z,"ACK, will move to optional TO. You're right that the most common use case will be people with a single replica."
767154674,8315,madolson,2021-01-25T22:27:25Z,"@oranagra @yossigo Look good?
@itamarhaber @soloestoy Do you guys have any thoughts?"
767882636,8315,madolson,2021-01-26T22:57:50Z,Failover doc: https://github.com/redis/redis-doc/pull/1502
1002889590,8315,vincent-163,2021-12-30T06:20:17Z,"Hello, I'm looking for a way to trigger atomic failover without data loss. I'm happy to find that this command has been implemented recently. However, it seems like the clients have to be reconfigured manually to use the new master, which requires a restart of all the clients.
Currently I'm using a single-master Redis cluster (I did this by manually assigning slots 0-16383 to a freshly created Redis instance, copy the nodes.conf to the relevant Redis server, and then restart that Redis server with cluster support enabled). Then I am able to use clients that support Redis Cluster and trigger manual failover with the `CLUSTER FAILOVER` command. Then the clients will automatically be redirected to the new master, which is good. The drawback is that cross-slot transactions are disabled, and I'm planning to update my app to prefix every key with a fixed hashslot to mitigate that. Will such a usage scenario get better support in the future? Like better documentation and tools for single-master clusters, config option to allow cross-slot transactions in single-master clusters, etc."
745764498,8170,madolson,2020-12-16T04:58:52Z,"Noted the individual comments, I made more progress today, hoping to update the PR tomorrow with tests. "
750604763,8170,oranagra,2020-12-24T00:03:08Z,"@madolson i think the only thing that's not covered is lazy expiry, did i miss anything about that? (or other things that's not covered yet?)"
750715299,8170,madolson,2020-12-24T02:54:40Z,"@oranagra Yes, I somehow missed that from your original comment."
754307009,8170,madolson,2021-01-05T00:25:10Z,"@oranagra Ok, now I think everything is updated."
754594720,8170,oranagra,2021-01-05T12:03:38Z,@madolson it looks like you pused a one force-push that includes both a rebase and new code. i'm having hard time to know what i already reviewed. (i see 11 commits but all have today's date).
754601907,8170,oranagra,2021-01-05T12:19:38Z,"i think i manged to figure it out, the new commits are the last 3"
755960038,8170,oranagra,2021-01-07T08:15:58Z,@redis/core-team please approve (see top comment)
1004061006,10043,oranagra,2022-01-03T12:30:49Z,"thanks for your PR.. started looking at it..
i now realize that deleting help.h would mean that new redis-cli will be unable to provide any hints when connected to old servers (in the past it just used to provide the wrong hings).
one alternative is to keep the old help.h just for use in old servers
but that would probably complicate the code, and also, i suppose it'll mean we'll have to keep the old help.h forever.
anyone has any other ideas before we embrace that ""limitation""?"
1004100235,10043,jhelbaum,2022-01-03T13:41:09Z,"Right - that would mean keeping the existing help.h mechanism as a fallback for when COMMAND returns the old format. Basically, keep all the code this PR deletes, behind a conditional. We could keep it for a predefined transition period, until the old server version is no longer supported. But sure, it would complicate the code."
1004160026,10043,oranagra,2022-01-03T15:14:50Z,ok. i don't like it.. but just wanted to raise it.
1004922181,10043,guybe7,2022-01-04T15:47:42Z,"for the record, i guess it's not so bad if a new redis-cli doesn't show hints when running with an old redis server (I would imagine the more common case is an old redis-cli with an new redis-server)"
1004923721,10043,oranagra,2022-01-04T15:49:40Z,"i'm not sure that's right.. someone managing many redis servers, will maybe update his redis-cli but keep using it for both old and new servers."
1010713661,10043,oranagra,2022-01-12T07:15:40Z,"@jhelbaum FYI, #10056 was merged."
1011344731,10043,jhelbaum,2022-01-12T18:42:41Z,"Can redis-cli assume that COMMAND INFO and COMMAND DOCS return the commands in the same order? It would simplify the code.
"
1011366668,10043,oranagra,2022-01-12T19:11:14Z,"currently that's the case, and i can't think of any reason why the implementation could be changed in a way that'll break that.
i.e. both commands recursively run on the same dict.
that said, it seems like an invalid assumption to make (each command has it's own documented semantics, but they have nothing to do with each other, formally)"
1012657262,10043,jhelbaum,2022-01-14T01:06:29Z,"COMMAND INFO is not necessary for this purpose. The code now only calls COMMAND DOCS, which has all the relevant information for the help strings."
1021297424,10043,oranagra,2022-01-25T15:19:13Z,"@jhelbaum forgive me for the luck of focus. what's the status here?
ready for merge?"
1021337997,10043,jhelbaum,2022-01-25T15:58:30Z,"As far as I'm concerned, it's ready. I don't get the sense that it's had a
thorough code review, though.

I also don't know if there are any tests that are usually run on redis-cli?

On Tue, Jan 25, 2022 at 5:19 PM Oran Agra ***@***.***> wrote:

> @jhelbaum <https://github.com/jhelbaum> forgive me for the luck of focus.
> what's the status here?
> ready for merge?
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/redis/redis/pull/10043#issuecomment-1021297424>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AH5PZMMIUZWP4QOM7NY3XBDUX25QBANCNFSM5LE7HCOQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
"
1021386840,10043,oranagra,2022-01-25T16:41:37Z,"I'll give it another review.. just wasn't sure there are still changes pending or it's ready from your side.
there are some tests in `tests/integration/redis-cli.tcl` but I i'm not sure if the infrastructure can serve for our purpose here.
and also, i suppose manual testing is enough for this feature, not sure we need regression tests."
1021992039,10043,jhelbaum,2022-01-26T08:56:55Z,"Another thought: Presumably in a separate PR, would anyone object to breaking this up into multiple files? 8800 lines is a bit much much for one source file."
1022019836,10043,oranagra,2022-01-26T09:31:11Z,"i agree. but i don't see a point in just breaking it up in a PR that does only that..
we should do some major revamp in it some day, maybe even split it into multiple tools, or multiple units bundled in one tool.
i don't have a concrete idea. just agree that what we currently have is a mess."
1022606382,10043,jhelbaum,2022-01-26T21:11:41Z,"Okay, I added support for help entries for command group names. I've also done a bit of code cleanup, and I hope I caught all the style fixes. Please take another look.
"
1022648338,10043,jhelbaum,2022-01-26T22:08:35Z,I don't understand why these tests are timing out. Any leads? 
1025880833,10043,oranagra,2022-01-31T15:20:47Z,"@yossigo @jhelbaum what's the next move?
the options i see are:
1. drop help.h, so that redis-cli is not showing any tips for old servers.
2. keep help.h completely static (representing v6.2 content), and used only when COMMAND DOCS is unsupported.
3. somehow (and for some reason), use both (merge data from COMMAND DOCS with the static one from help.h)

my thoughts
1. this PR currently implements 1.
2. i really wanna avoid maintaining this help.h file, so keeping it static with the contents from redis 6.2 seems ok.
3. if for some reason we want to let people skip issuing COMMAND DOCS on startup (as an optimization), and still get up to date tips, we'll need to take option 3, but i don't think it's a must, so i rather not."
1026555114,10043,oranagra,2022-02-01T07:45:00Z,"@jhelbaum we discussed this in a core-team meeting, and concluded we don't wanna introduce a regression when new redis-cli talks to an old server.
so we'd like to pursue option 2 above.
can you handle it?"
1026632725,10043,jhelbaum,2022-02-01T09:24:45Z,"So, maintain the old behavior as a fallback? Including retrieving additional commands from `COMMAND`?

It's certainly doable. I'll see what I can do. Is there a timeline for this?"
1026687702,10043,oranagra,2022-02-01T10:24:06Z,"next release candidate will be in about 3 week, would be nice to finish this campaign by then.

retrieving additional info from COMMAND would only be useful for module commands, right?"
1026716824,10043,guybe7,2022-02-01T10:58:56Z,@oranagra will we ever the static help.h?
1026740923,10043,oranagra,2022-02-01T11:25:21Z,@guybe7 maybe in a few years (10?) we'll be able to delete it. we can discuss this again in the future and decide.
1026749096,10043,jhelbaum,2022-02-01T11:35:40Z,"Yes, for module commands."
1029533771,10043,jhelbaum,2022-02-04T00:41:31Z,"Okay, I've reinstated the static help.h file for use with pre-7.0 servers, along with the code that generates the help tables from it. 

I still can't figure out why some of the tests are timing out. Any leads in investigating the problem would be appreciated.

```
Testing integration/redis-cli
[TIMEOUT]: clients state report follows.
sock55d1e5c59510 => (IN PROGRESS) Interactive CLI: INFO response should be printed raw
```"
1029849050,10043,oranagra,2022-02-04T10:35:54Z,"I can't figure out why the test hangs on GH actions (it passes locally).
@yossigo maybe you can figure it out?"
1029923749,10043,jhelbaum,2022-02-04T11:59:56Z,It's been broken since at least 896dad5
1030038260,10043,oranagra,2022-02-04T14:27:57Z,"it was actually easy to reproduce locally if using the same `make` line as the one the CI uses.
the test hang because redis-cli crashed on SIGABRT because realloc was called with a bad address."
1030047448,10043,oranagra,2022-02-04T14:38:34Z,"p.s. `zrealloc` uses `realloc`, but has this in smalloc.c:
```
#define realloc(ptr,size) je_realloc(ptr,size)
```
so when you call `realloc` from redis-cli.c, it uses libc allocator.
but if you debug this with `make valgrind` or any system that doesn't use jemalloc by default, then both use libc, which is why it didn't reproduce locally for you."
1030049875,10043,jhelbaum,2022-02-04T14:41:22Z,"Okay, I've learned something. Thanks.

On Fri, Feb 4, 2022, 16:38 Oran Agra ***@***.***> wrote:

> p.s. zrealloc uses realloc, but has this in smalloc.c:
>
> #define realloc(ptr,size) je_realloc(ptr,size)
>
> so when you call realloc from redis-cli.c, it uses libc allocator.
> but if you debug this with make valgrind or any system that doesn't use
> jemalloc by default, then both use libc, which is why it didn't reproduce
> locally for you.
>
> —
> Reply to this email directly, view it on GitHub
> <https://github.com/redis/redis/pull/10043#issuecomment-1030047448>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AH5PZMOG2ZROREBE3VS4F63UZPQHNANCNFSM5LE7HCOQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
"
1030598276,10043,oranagra,2022-02-05T10:43:42Z,"Maybe we should automatically hide suggestions based on version? 
I.e. Send HELLO or INFO at startup, and use the version to hide irrelevant suggestions? 
or maybe we should remove commands and args that are newer than 6.2 from help.h when generating it? 
The first option is nicer, but not sure how much effort we wanna invest in old version support (since in  versions newer than 7, it'll happen automatically since we use command DOCS) "
761061836,8324,oranagra,2021-01-15T17:02:08Z,"@guybe7 for the 3 bugfixes you found (ZDIFFSTORE etc, mentioned above), i think it would be a good idea to issue a separate PR, so we can cherry pick that fix to 6.0."
761763828,8324,oranagra,2021-01-17T09:56:10Z,"@redis/core-team please approve the new COMMAND output fields, see example in the issue, and the new module API"
765541062,8324,guybe7,2021-01-22T16:43:22Z,"last commit handles the above comments by @yossigo + add the ""startfrom"" arg to the keyword spec(see top comment)"
766143117,8324,oranagra,2021-01-23T17:12:17Z,"@redis/core-team note the changes:
1. a new argument for the `keyword` spec, named `startfrom`
1. a new keyspec flag named `incomplete`.
see the top comment about MIGRATE command."
768077667,8324,oranagra,2021-01-27T06:57:56Z,"@madolson i don't have a clear use case for the read/write flags for the keyspecs, but i think we should make that distinction, i'm sure it'll come in handy, even inside redis one day.
this is in part related to the lookupKeyRead vs lookupKeyWrite saga, and in part lesson learned from Redis On Flash.
is is good to know which key is gonna be only read from, which one will be just overwritten, and which one will be modified (both read and write).

maintaining this info in the command table is not a huge overhead, and since for each command we always know what each key argument is gonna be used for, there's never any doubt as to what to fill in that field."
768103411,8324,madolson,2021-01-27T07:50:51Z,"Sure, I guess my concern is we're optimistically building out this functionality without a clear understanding of how it will be used. The key location use case, which is well understand for cluster mode and ACLs, doesn't really need it.

The one way decision is exposing this to clients. Even if we have use cases inside Redis, it may not be relevant to clients. The API is solid though, we could always introduce new specs/deprecate old ones. The risk is small is small in the end, I'm just not convinced we should release in for 6.2."
768932060,8324,yossigo,2021-01-28T09:47:13Z,"@oranagra @guybe7 and myself got to discuss this a bit more. In a nutshell, we conclude that:

* We can go through another round of improvements to the keyspec proposal above to make it much more flexible and support more command patterns, without increasing complexity much
* Considering all the different modules out there, we won't get 100% coverage unless we come up with full command grammar. This will require full parsing - much more complex
* There is still some uncertainty about the value of this,  we need to hear clearer voices from the client ecosystem about how useful this is going to be

Calling @mgravell @andymccurdy @sazzad16 @mp911de @luin and other Redis client library developers to provide their perspective. TL;DR the idea is to extend `COMMAND INFO` significantly to describe more/most/all command patterns so clients could extract key names from command arguments autonomously, and we need to iterate the design and get some validation of the value in doing that."
768959882,8324,mp911de,2021-01-28T10:35:23Z,Lettuce doesn't use `COMMAND INFO` for key/argument patterns at all. We actually use it only to determine which commands are available (command names). The key/argument pattern is so complicated that it's easier for us to just declare methods statically instead of assembling commands from `COMMAND INFO`.
768980849,8324,oranagra,2021-01-28T11:12:18Z,"@mp911de thanks.
1. so that means that each time a new command is added you have to implement a to extract the keyname arguments?
2. forgive me for my ignorance (and laziness), does Lettuce supports running arbitrary commands (like execute_command in redis-py)?
if that's the case, then you want to let users use commands that Lettuce doesn't support yet (or module commands), so using COMMAND GETKEYS or the new keyspecs idea can solve that.
3. would you consider using the new keyspecs when available? do they look usable to you?
it can simplify your work, but you'll need to keep the old code in case the redis server that's used doesn't support them."
769035237,8324,mp911de,2021-01-28T12:58:23Z,"> 1. so that means that each time a new command is added you have to implement a to extract the keyname arguments?

Exactly. This more due to the fact that with Java you kind of expect a method declaration to assist users. Aside from that, we have a method to dispatch plain commands (`dispatch(""SET"", ""KEY"", ""VALUE"")`-style). Based on that, the user decides what arguments come into a command and we don't have the immediate need to know which argument is a key or a keyword.

> 3. would you consider using the new keyspecs when available? do they look usable to you?

We have a parser for the `COMMAND` response that returns a value object to the caller. We would certainly add the keyspec there for the sake of having complete support for the response but we don't have any use for it.
"
769038148,8324,oranagra,2021-01-28T13:03:35Z,@mp911de so with `dispatch` (which would be needed to use new command or module commands) how do you know which cluster node to forward the command? (using either COMMAND GETKEYS or relying on the `-MOVED` would be an overhead).
769040347,8324,mp911de,2021-01-28T13:07:18Z,"Excuse my oversimplified representation of our API. The actual code to use `dispatch` is:

```java
dispatch(MyCommands.SET, new StatusOutput<>(StringCodec.UTF8), new CommandArgs<>(StringCodec.UTF8).addKey(key).addValue(value));
```

`CommandArgs` captures which argument is a key and which one is a value, mostly to apply the correct encoding. That also allows us to reason about the command argument ordering."
909971769,8324,guybe7,2021-09-01T06:52:23Z,"> can you remind me what tests did we perform to see that the specs we filled are correct?
we can match the old output of COMMAND to the new one and see that the legacy spec is ok.

i did

> but maybe there's something else we can do? by matching the output of GETKEYS to what this extracts?
i.e. write some test that implements what we expect a client to implement based on the specs, then somehow compare the result of that with GETKEYS on either a hard coded list of commands, or some fuzzer?

not sure a fuzzer will be beneficial (especially for commands that have a keyword spec)

WDYM by the GETKEYS suggestion? to actually write the logic that retrieves the keys when given argv + COMMAND's output? and then tests it against a predefined group of argv arrays and compare to COMMAND GETKEYS?"
911943762,8324,oranagra,2021-09-02T18:21:42Z,"@redis/core-team please approve again. quite a lot have changed since last time.
i think the main difference is that we've split the specs into two steps (start_search and find_keys), and changed the module apis. the top comment was updated so please have a look at it.

i suggest to focus on reviewing the module api, and the output of COMMAND command.

note that the changes to COMMAND command are gonna be reverted soon (that's why we don't document it yet), and instead will be probably put into a new COMMAND**S** command (the old command will only be kept for backwards compatibility, and the one one will have info on sub-commands).
also note that the command table is gonna be changed soon to a json file, so you can ignore the format there."
916086925,8324,itamarhaber,2021-09-09T13:22:17Z,Approving the concept and APIs.
918924275,8324,oranagra,2021-09-14T08:19:16Z,running daily CI before merging: https://github.com/redis/redis/actions/runs/1232825354
919040031,8324,oranagra,2021-09-14T10:54:37Z,"@guybe7 valgrind found some issues in the module API: https://github.com/redis/redis/runs/3596355820?check_suite_focus=true
and some test failure on alpine linux: https://github.com/redis/redis/runs/3596356192?check_suite_focus=true"
1897638878,12913,soloestoy,2024-01-18T01:55:59Z,">i'd like another metric to count the overhead of the rehashing (i.e. the total memory used by ht[0] in dicts that also have ht[1])

I like this idea, but one concern is that cluster with 16384 slots, in the worst case scenario, we would need to iterate through all the slots to obtain complete information about ht[0] and ht[1]. Perhaps we need to use `rehashingStarted` and `rehashingCompleted` to optimize this process."
1898080094,12913,oranagra,2024-01-18T09:12:31Z,"yes, let's do that (keep track of it rather than calculate it on the spot)"
1951636796,12913,CharlesChen888,2024-02-19T03:38:56Z,"Now that ""kvstore"" is merged, I have to rewrite a lot of things.

`mem_overhead_hashtable_rehashing` and `mem_overhead_hashtable_lut` are now maintained in `kvstoreDictRehashingStarted` and `kvstoreDictRehashingCompleted`, and therefore it is inconvenient to tell whether the rehashing dict is from DBs or from PUBSUB dicts. But I guess including the overhead of pubsub is somehow OK? Sometimes pubsub also takes a lot of memory and we need to check it.

And since `databases_rehashing_dict_count` may not be so useful and now we need to iterate all DBs to get the exact count, I temporarily dropped it.

Or we can maintain the overhead inside kvstore structure and when `info` is called, we gather the overhead from all DBs' kvstore. And since in this case we have to iterate all DBs, we can keep the `databases_rehashing_dict_count`."
1954557025,12913,oranagra,2024-02-20T16:12:49Z,"discussed and approved this in a core-team meeting.
one metric in INFO, and all 3 in MEMORY STATS.

i haven't reviewed the recent changes, please ping me when this is ready."
1955986676,12913,oranagra,2024-02-21T06:36:10Z,"i realize we need to update the reply schema, see https://github.com/redis/redis/pull/13076"
1958530760,12913,CharlesChen888,2024-02-22T02:08:18Z,"Just realized that in test ""Redis can resize empty dict"" in other.tcl, it is assumed that `memory stats` shows only the overhead of non-empty DBs. But now all DBs' overhead is displayed and so we need to change the test."
1959069841,12913,CharlesChen888,2024-02-22T09:47:29Z,"> i see the test passes, what do you mean?

@oranagra

The test should read `overhead.hashtable.main` of DB9 (the DB whose dict expanded and then emptied). In the test, it use to be that only DB9 has a key (`r set a b`) and so `memory stats` shows only DB9's overhead. In this case we only need to find the first `overhead.hashtable.main` field and read its value.

However in this PR `getMemoryOverheadData` is modified and `memory stats` shows not only non-empty DBs but all DBs, (https://github.com/redis/redis/pull/12913#discussion_r1495198272), so we need to specify it is the DB9's overhead that we need to read.

The test would also pass if it reads other DB's overhead, but that's meaningless."
1959115603,12913,oranagra,2024-02-22T10:11:43Z,"ohh, you mean that so far it just looked for any `overhead.hashtable.main`, and you now modified it to be `db.9 overhead.hashtable.main`.
ok, so that's solved then.

let's wait to see if @soloestoy has anything else to argue and then merge it."
1963857561,12913,CharlesChen888,2024-02-26T10:58:07Z,@soloestoy Any more problems?
1970701995,12913,soloestoy,2024-02-29T09:05:34Z,"My OCD is kicking... I feel it's best to add ""db"" to the names of these metrics to avoid the misunderstanding that they refer to all hash data types:

INFO MEMORY
* `mem_overhead_hashtable_rehashing` -> `mem_overhead_db_hashtable_rehashing`

MEMORY STATS
* `overhead.hashtable.lut` ->  `overhead.db.hashtable.lut`
* `overhead.hashtable.rehashing` -> `overhead.db.hashtable.rehashing`

@oranagra @madolson WDYT?

about the db prefix error, @CharlesChen888  could you change `database.dict.rehashing.count` to `db.dict.rehashing.count` and modify the regular expression in memory-stats.json to `""^db\\.\\d+$""`, then try again?
"
1973716956,12913,madolson,2024-03-01T18:28:45Z,Just retrospectively acknowledging that the updating wording seems better to me as well.
1382246450,11708,madolson,2023-01-13T18:49:33Z,@redis/core-team Hey team. This is a new module API that allows module commands to register themselves to the built-in ACL categories.
1399128863,11708,roshkhatri,2023-01-21T01:25:35Z,"Hi team, I have pushed the code with the necessary changes. Please take a look and review them.
Thanks!"
1399895153,11708,oranagra,2023-01-23T07:05:36Z,"we need to consider how this impacts users on upgrade.
i.e. a command is added to a category implicitly exposes it (without user's intention), or blocks it (breaking an app).

i suppose this potential for breakage is not about upgrading redis (to include this API), but rather about upgrading the module. and not necessarily when the module starts using this API, but any time it changes the categories (i.e. even 5 years from now, when a module upgrade adds a command to a category).
and in fact it's arguably no different than cases were redis [changes](https://github.com/redis/redis/pull/9208) categories.

i guess it ultimately depends on the module developers, not redis. and like many other breaking changes, we just have a choice between keeping something bad (inconsistent, confusing or hard to use) forever, or fixing it and accepting the uncomfortable transition. still, i'd like to hear opinions and suggestions on how we can mitigate this."
1400677483,11708,madolson,2023-01-23T17:02:14Z,"> i guess it ultimately depends on the module developers, not redis. and like many other breaking changes, we just have a choice between keeping something bad (inconsistent, confusing or hard to use) forever, or fixing it and accepting the uncomfortable transition. still, i'd like to hear opinions and suggestions on how we can mitigate this.

I think the current choice is the right tradeoff, but also interested in other options. Given the security implications, I feel pretty strongly that we shouldn't just start opt'ing in modules into ACL categories on an engine upgrade, it makes more sense on a module upgrade. I think the changes we made to ACL categories before were all very tactical and low risk generally, and they still made me feel uncomfortable. I also would like to give modules the flexibility to choose what categories they want to be a part of. "
1422189542,11708,oranagra,2023-02-08T08:05:20Z,"this PR was conceptually approved in a core-team meeting.
can be merged after the last batch of comments are addressed
we are ok with blocking the registration of command, configs, and data type outside the onload (must be mentioned in the top comment as a breaking change).

one other concern that was raised is if the API would be convenient. considering that all modules are gonna want to use this, but they must remain compatible with older versions of redis, they need to do some filtering of the categories from the flag string they pass.
the alternative is to add this feature to either a new API or the existing `RM_SetCommandInfo`, so the module can conditionally add this info without filtering of the hard coded flags string.
we would like to explore these alternative (implement each in some branch) and see which one feels better.

@roshkhatri can you handle all of that in the coming couple of weeks?"
1422271248,11708,roshkhatri,2023-02-08T09:13:19Z,"Thanks, that sounds good, yes I can handle these changes in couple of weeks. Will keep you posted on the PR. Will implement the alternative and put up another PR to see which one makes more sense."
1466983268,11708,roshkhatri,2023-03-13T21:23:48Z,"Hi Team, I have updated this PR and have addressed the comments. I have also updated the PR with the implementation of blocking the registration of command, registering the configs, and creating the data type outside the OnLoad function. Please review the changes and let me know if there are any questions. @oranagra @madolson "
1467292929,11708,madolson,2023-03-14T03:22:29Z,"For the conversation about RedisModule_SetCommandInfo(). I feel like there are three options:
1. We leave the API as is. Module developers will need to implement a check for Redis version and determine flags as necessary.
2. We extend RedisModule_SetCommandInfo(). We will need a new version, and module developers will still need to check for Redis version for setting the fields.
3. We implement a third API. RedisModule_SetCommandCategories(), which a module developer will need to check the version to know if they are allowed to call. 

\1 and 2\ seem equally bad, as we basically are required to have a Redis version check for the API. 3\ seems theoretically better, as we could check if that command exists, and only call it if it does. Implementation wise they are all pretty similar now, as we just need to go and set all the bits and flags and call ""recompute"" like we already do.

In the fullness of time, I still think 1 is the clearest and most sensible approach. I would still be inclined to suggest we merge what is currently implemented. @yossigo Have you thought much more about this."
1467644767,11708,oranagra,2023-03-14T08:40:08Z,"> \1 and 2\ seem equally bad, as we basically are required to have a Redis version check for the API. 3\ seems theoretically better, as we could check if that command exists, and only call it if it does. Implementation wise they are all pretty similar now, as we just need to go and set all the bits and flags and call ""recompute"" like we already do.

IIRC the concern wasn't about detecting the redis version, but rather about the string manipulation the module would need to do (either concatenate strings, or filter them), which would make the boilerplate code of registering commands rather ugly.
so if that's the case, and we take the categories to another string (either SetCommandInfo or SetCommandCategories), it resolves that problem.
i agree that in the fullness of time this concern is gonna be gone, but in the short term (next 4 years), all modules that wanna use it are gonna suffer from this complication.

am i missing something?"
1469316264,11708,madolson,2023-03-15T04:36:16Z,"Core meeting update. We decided that we should probably implement the new API, RM_SetcommandCategories(), since that was probably the most module developer friendly thing to do. Other than that, the new APIs are approved."
1476818941,11708,roshkhatri,2023-03-20T19:29:40Z,"Hi Team, I have pushed the changes to this PR and implemented the new API, RM_SetcommandCategories(). Please review the changes and let me know if there are any questions. @oranagra  @madolson "
1477128527,11708,roshkhatri,2023-03-21T00:32:05Z,I have also updated the top comment with the updated implementation.
1477138688,11708,madolson,2023-03-21T00:53:18Z,"@oranagra @yossigo All updates, PTAL and validate the new name looks good at the very least."
1552921391,12192,oranagra,2023-05-18T11:30:38Z,"generally LGTM.

I think the -1 in the slot number is acceptable, i suppose that for many clients will find it easier than supporting a new reply, and i hope that for some it'll even be implicitly supported in the current code, but let's validate that with client library maintainers.

not sure regarding timing (fitting this into 7.2), don't want to rush it and then regret."
1553336010,12192,madolson,2023-05-18T16:50:37Z,"> reuse MOVED or add a new reply REDIRECT, I see MOVED has slot arg, now I set -1, maybe REDIRECT without slot is better?

I have a strong preference to keep MOVED. It still seems easier long term for clients to just understand one error. 

> do we need redirect the pubsub commands? they are not read or write

We currently don't force pubsub commands to primaries, but maybe we should, I think that's a different conversation though.

> the TLS problem, master and replica doesn't know the real work port each other

Not sure what to say about that."
1553336287,12192,madolson,2023-05-18T16:50:53Z,How is this a breaking change?
1554041223,12192,soloestoy,2023-05-19T05:46:08Z,"> How is this a breaking change?

`readonly` and `readwrite` cannot be executed in standalone mode before this, I'm not sure if it will really have an impact, but to be safe I labeled breaking change."
1554072948,12192,yangbodong22011,2023-05-19T06:28:25Z,"I vote for MOVED, which will not bring new understanding costs."
1554788867,12192,madolson,2023-05-19T15:59:00Z,"> readonly and readwrite cannot be executed in standalone mode before this, I'm not sure if it will really have an impact, but to be safe I labeled breaking change.

Usually this is just a major decision. I don't think we would expect clients to be relying on this functionality for anything? I'm going to remove the tag."
1559734069,12192,oranagra,2023-05-23T16:01:34Z,"we discussed this in a core-team meeting, we feel we'd like to completely design this feature, before implementing and committing to parts of it.

two concerns that were raised are:
1. maybe MOVED response will need to carry more info (like protocol, TLS, or others)
2. we think we'd like all multi-instance deployments (even a master+replica set) to look like a cluster, that means from topology discovery concerns, we might wanna present them as such to clients.
3. the above discussion about instances in containers or behind NAT, and `master-announce-*`, etc. (including TLS), all of that should be designed before we take the first step.

further discussions are needed.."
1560421771,12192,soloestoy,2023-05-24T03:53:26Z,"it's ok we can design this feature completely and raise all concerns, this is a good way in scheme design stage.

and now this PR can work well when users deploy Redis on simple hosts, I think we have reached a consensus on this point. and the implementation now is forward compatible, .

about the concerns:
1. extend the ability of MOVE is a good idea, but I think this is out of this PR. MOVE is a general redirection reply that affects not only standalone but also cluster, we can do it in 8.0
2. I don't get this point, clients can use `info replication` and `role` to get the master-replicas topology, and get the replica-subreplicas topology recursively.
3. this is a practical problem, but the current `*-announce-*` implement are not perfect either, sometimes it can be confusing.
    * For example when we config the announced ip:port, they will override the real ip:port for communication between Redis noeds, but users may cannot access the announced ip:port. In this case they need use `cluster-announce-hostname`, but IMHO the `cluster-announce-hostname` and `cluster-announce-ip` are confusing, users cannot distinguish which one is for server side and which one is for client side. Moreover, `cluster-announce-hostname` may need `cluster-announce-hostname-port` and `cluster-announce-hostname-tls-port` to work.
        ![image](https://github.com/redis/redis/assets/24804835/5fe6979b-ea8f-4946-8e3c-2d8cbb6ff380)
    * Another situation is Redis nodes and client in different subnets as described in #7460, if users config `cluster-announce-ip:port` then the Redis noes cannot reach each other via the announced ip:port
        ![image](https://github.com/redis/redis/assets/24804835/d67673f9-8f62-49c6-95d3-2e5c6fc18af8)
    * There are also many issues with the current announced ip:port mechanism, and we need to solve the problem before applying it here. I don't want this PR to become too complicated, I suggest we put these things aside for now and focus on satisfying the users of Simple Host first.

all in all, my opinion is we can design completely but implement step by step."
1561542375,12192,monkey92t,2023-05-24T16:31:10Z,"First, it is necessary to clarify the meaning of ""MOVE"" in the context mentioned. Does it represent 301 or 302? Perhaps more people believe that ""MOVE"" represents 301, but in the case of redis-cluster, it has always been possible to have A -> MOVE -> B, which makes it look more like 302.

I think perhaps it should be called ""REPLACER,"" which would indicate that the new address permanently replaces all the functionalities of the current node, rather than just being a redirection.

Regarding the seamless switch problem in Redis, I propose adding a ""retired"" status to the master node. If the current node is not the master node or there are no slave nodes available, an error will be returned..

When the master node is in the ""retired"" status, it can still accept read and write commands without the client being aware of this change. In this state, the master node handles read commands as usual, but forwards write commands to the slave node(s) (while keeping track of keys that the slave node(s) have not yet synchronized).

Once the slave node(s) reach an identical state with the master node, the master node responds to any command with ""REPLACER"". When the client receives the ""REPLACER"" status, it switches to the slave node, which provides all the functionalities of the original master node."
1561942045,12192,madolson,2023-05-24T21:20:37Z,"I will say, I think the incremental delivery we made this happen in makes me wish we had thought this through more holistically :D.

So, is our ideal cluster setup:

These configs are set as a block to indicate there is a different way to reach this node for replication and cluster. By default we assume a flat topology for all nodes, so this would also apply to clients, however we definitely assume the topology is flat for the nodes in the cluster. 

```
cluster-announce-ip
cluster-announce-port
cluster-announce-tls-port
cluster-announce-bus-port
```

We also need a second set of configs, which are used when the route from the client to the cluster may not be the same as the route between nodes in the cluster. 

```
cluster-announce-hostname: The client visible name displayed in redirects and topology.
<new> cluster-announced-client-ip: We're going to assume there is a fixed IP for both TLS and TCP.
<new> cluster-announced-client-tls-port: This is the port to show in clients in redirects and topology for tls clients.
<new> cluster-announce-client-port: This is the port to show to clients in redirects and topology.
```

I think in the fullness of time we want all of this configuration for cluster mode disabled as well, so I'm not clear why we wouldn't just copy the interface we are currently using. 

> I don't get this point, clients can use info replication and role to get the master-replicas topology, and get the replica-subreplicas topology recursively.

Do we want clients to be doing the `replication info`? I think we don't, I think we want them to call something like `cluster shards` to learn about the topology. It's an important call out here that cluster mode enabled doesn't have sub-replicas, so what is the right way to expose that. I see a few approaches:
1. We have all nodes know the topology, through gossip or some other means.
2. We push the clients to call multiple discovery commands or force it to a specific node.

I think we want to align on 1 to be our long term solution. 

> all in all, my opinion is we can design completely but implement step by step.

I think this is very much an engineering mindset, but it's not very coherent from project mindset. For performance improvements, that incremental approach makes more sense since, but when releasing a coherent feature, we should strive to build something that is consistent over a longer time frame."
1562187636,12192,soloestoy,2023-05-25T02:58:31Z,">These configs are set as a block to indicate there is a different way to reach this node for replication and cluster. By default we assume a flat topology for all nodes, so this would also apply to clients, however we definitely assume the topology is flat for the nodes in the cluster.
>```
>cluster-announce-ip
>cluster-announce-port
>cluster-announce-tls-port
>cluster-announce-bus-port
>```
>We also need a second set of configs, which are used when the route from the client to the cluster may not be the same as the route between nodes in the cluster.
>```
>cluster-announce-hostname: The client visible name displayed in redirects and topology.
><new> cluster-announced-client-ip: We're going to assume there is a fixed IP for both TLS and TCP.
><new> cluster-announced-client-tls-port: This is the port to show in clients in redirects and topology for tls clients.
><new> cluster-announce-client-port: This is the port to show to clients in redirects and topology.
>```

This makes sense, but there is a problem how to identify whether this connection is from a client or a node, some admin service may need get the announce topology not the announce client topology. I know some virtual network tech can do that, but there is currently no universal standardized technology to achieve this. Maybe we need some ways like a new command `CLIENT COME-FROM`, but it's a bit ugly, and command can lie.

>I think in the fullness of time we want all of this configuration for cluster mode disabled as well, so I'm not clear why we wouldn't just copy the interface we are currently using.

Yes, I agree these configs can be used for more than just cluster, maybe in some day we can remove the `cluster` prefix.

I didn't understand the words `copy the interface we are currently using`, you mean adding `master-announce-ip:port`? I think it's unnecessary, master-replica is different with cluster-nodes, master doesn't need connect to replica, the `REPLICAOF` must use the NAT ip:port to connect master, so the `-MOVED` must reply with the NAT master ip:port.

>Do we want clients to be doing the replication info? I think we don't

But now like sentinel is using these to get the topology, and in Redis perspective Sentinel is just a normal client.

>I think this is very much an engineering mindset, but it's not very coherent from project mindset. For performance improvements, that incremental approach makes more sense since, but when releasing a coherent feature, we should strive to build something that is consistent over a longer time frame.

This feature is forward compatible, if we add some new configs or change internal mechanism, it can still work well for users, that's why I think we can do step by step."
1569378813,12192,soloestoy,2023-05-31T01:59:29Z,"ping @redis/core-team @zuiderkwast,  need your feedbacks"
1598101952,12192,madolson,2023-06-20T04:34:30Z,"> Yes, I agree these configs can be used for more than just cluster, maybe in some day we can remove the cluster prefix.

I think it makes sense to keep cluster. While talking with our AWS customers, people seem to resonate calling any collection of nodes a cluster. The configs only make sense for more than 1 node.

> I didn't understand the words copy the interface we are currently using, you mean adding master-announce-ip:port? I think it's unnecessary, master-replica is different with cluster-nodes, master doesn't need connect to replica, the REPLICAOF must use the NAT ip:port to connect master, so the -MOVED must reply with the NAT master ip:port.

I would really like to just use the current cluster announce configs, and only add a single new config which describes an unsharded cluster (in addition to the other configs I mentioned). I want to merge our two deployment configurations, cluster-enabled and disabled, as much as possible. From an end-user perspective the result is basically the same, but from a management and maintenance perspective, I think it will be good for us long term.

> This feature is forward compatible, if we add some new configs or change internal mechanism, it can still work well for users, that's why I think we can do step by step.

I agree with this, but I just think it's short sighted. I don't want end users configuring stuff that stops being relevant quickly."
1598156100,12192,soloestoy,2023-06-20T05:51:20Z,">I think it makes sense to keep cluster. While talking with our AWS customers, people seem to resonate calling any collection of nodes a cluster. They configs only make sense for more than 1 node.

Interesting, on the contrary, our users at Alibaba Cloud prefer to clearly distinguish between standalone and cluster because these two usage modes are very different.

I have experience with AWS ElastiCache and have consulted with some users. I find it misleading to call a standalone mode with a primary node and multiple secondary nodes a ""cluster"". I do not like this design.

>I would really like to just use the current cluster announce configs, and only add a single new config which describes an unsharded cluster (in addition to the other configs I mentioned). I want to merge our two deployment configurations, cluster-enabled and disabled, as much as possible. From an end-user perspective the result is basically the same, but from a management and maintenance perspective, I think it will be good for us long term.

I don't want to mix up standalone and cluster. If you want to merge the configuration options for these two different modes, I still believe that removing the ""cluster"" prefix is a good approach because the announced IP port doesn't need to distinguish between deployment modes.

>I don't want end users configuring stuff that stops being relevant quickly.

Yes, that's why I don't want to add too much configuration. The current approach is already sufficient for users who deploy directly on physical hosts and for those who deploy in NAT mode using replica-announce-ip. In my opinion, before we have figured out the correct and optimal use of announced IP:Port, it's better to maintain a relatively pure state and this is the correct way to avoid short-sighted."
1600240618,12192,oranagra,2023-06-21T06:09:21Z,"it's a little hard for me to follow the discussion.
IIRC we argued that we should design that topic fully before making any changes, and that one of the concerns is topology discovery commands, but i don't see a proposal around that topic. did we conclude something to change that?

regarding the redirect configurations, i'm also lost in the discussion, is the current implementation in the PR the most up to date proposal (which we can approve, reject or discuss it's shortcomings)?
i.e. is the debate about whether or not we should rename / add alias to the cluster-* configs, or simply reuse them for non cluster, or are there any other issues around NAT deployments that are not solved by the current set of configs in the PR?"
1600498216,12192,soloestoy,2023-06-21T09:24:01Z,"1. About  topology discovery, I think the ROLE command and the replication section in the INFO command are sufficient, sentinel also uses these two commands to obtain the topology.

2. Regarding the name of the configuration item, I changed to ""replica-enable-redirect"" because I want to retain enough flexibility. Currently, this PR does not redirect pubsub commands, but this may change in the future. If we use the name ""replica-redirect-read-write"", it will be difficult to change it later.

3. In the matter of NAT deployments, I think there is no problem. Currently, for standalone(non-cluster) mode, if NAT deployment is used, the ""replica-announce-ip"" must be configured, and the IP specified in the REPLICAOF command must also be the master's NAT IP(equivalent to ""master-announce-ip""), otherwise the replica cannot connect to the master. So we don't need to add the ""master-announce-ip"" config, the replica can already return ""-MOVED -1 master-NAT-ip:port"", everything can work well.

4. My argument with madelyn is reuse or rename the cluster-* configs IIUC. Actually, I don't want to bring cluster-* configs into the discussion of this PR, using ""replica-announce-ip"" can work well for standalone(non-cluster) mode."
1600523364,12192,oranagra,2023-06-21T09:42:04Z,"ok, i'm not certain about the topology discovery, i think someone argued that client libraries are using CLUSTER commands and that we should think about exposing that, but i admit i don't know anything about that topic.

regarding NAT, i'm confused, you said there's no problem, but also suggested to add `master-announce-ip`?
maybe add it to this PR?

regarding the `cluster-*` configs, maybe adding an alias can make it nice? again, if it does, let's add them to the PR, so we'll have something to approve or complain about, maybe in several threads, which will be easier to track."
1600535202,12192,soloestoy,2023-06-21T09:50:17Z,">regarding NAT, i'm confused, you said there's no problem, but also suggested to add master-announce-ip?
maybe add it to this PR?

no no no, I mean replica can reply with master's NAT IP without master-announce-ip, since the ip used in REPLICAOF command is already master's NAT IP. we don't need add master-announce-ip.

>regarding the cluster-* configs, maybe adding an alias can make it nice? again, if it does, let's add them to the PR, so we'll have something to approve or complain about, maybe in several threads, which will be easier to track.

I prefer just keep the current configs, I don't want things to become too complicated, alias may make it more difficult for users to understand"
1603439779,12192,madolson,2023-06-22T23:38:51Z,"> ok, i'm not certain about the topology discovery, i think someone argued that client libraries are using CLUSTER commands and that we should think about exposing that, but i admit i don't know anything about that topic.

I think this gets to the heart of my main point, and why I think this entire PR is only focused on the short term. I want all client configurations to execute basically the same set of commands and handle topology the same way. I don't want clients to implement two different pathways (info replication and cluster shards). We are at the point where we can try to unify them, and I don't want us to take the short route. 

There are two decisions we need to make which are important:
1. How clients see the commands. I think this should be `CLUSTER *`, I feel extremely strongly about this. I think any decision that is not this is short sighted. I think we already have consensus about the moved stuff.
2. The configuration that is used. Today, non-clustered nodes aren't gossiping with each other, so the configuration is the inverse of the cluster mode flags. Instead of nodes being configured to advertise their configuration, they are configured to advertise their master configuration. We can't alias the configs because they don't do the same thing, releasing these configs is a one way door.

I don't feel as strongly about two, my argument is more that I don't think it's really aligned with our end state of trying to merge non-clustered vs clustered."
1603950761,12192,soloestoy,2023-06-23T08:51:15Z,"No offend, but you seem to be promoting AWS design and forcing everyone to accept it, which makes me uncomfortable. In this PR, I just want to focus on solving the redirect method in standalone mode, I strongly oppose imposing cluster commands on standalone mode in this PR. If you insist on doing so, you can implement it in so-called Cluster v2."
1604010079,12192,oranagra,2023-06-23T09:35:03Z,"> 1. How clients see the commands. I think this should be `CLUSTER *`

I think that in cases redis isn't using the cluster subsystem / architecture, we shouldn't use ""cluster"" interface.
e.g. in retrospect some of these `cluster-*` configs should have been without the `cluster` prefix, and we can fix that with an alias. same goes with the CLUSTER commands. we should imagine what we want in the end, and if the interface applies to both we can expose a new one and keep backwards compatibility.
i agree that it can be confusing in the short term, but maybe better in the long term.
i suppose it's similar to the changes when we renamed ""lua"" to ""script"" recently."
1605075134,12192,madolson,2023-06-23T22:42:55Z,"> I think that in cases redis isn't using the cluster subsystem / architecture, we shouldn't use ""cluster"" interface.

Can you elaborate? I'm arguing that in the fullness of time everyone should be running on the clusterbus system, so in that world it would make sense. Changing the API later adds a bunch of unnecessary churn in the meantime. I'm okay with changing to another topology API, but that also seems rather pointless.

> i suppose it's similar to the changes when we renamed ""lua"" to ""script"" recently.

I don't think this has any relevance. The naming change you are proposing is purely semantic. I never called them anything besides scripts, it was mostly organizational and allowed us to generalize some configs. The issue here is that the configs are structurally different, they can't just be solved by aliasing AFAIK. "
1605084788,12192,madolson,2023-06-23T22:55:26Z,"> No offend, but you seem to be promoting AWS design and forcing everyone to accept it, which makes me uncomfortable. In this PR, I just want to focus on solving the redirect method in standalone mode, I strongly oppose imposing cluster commands on standalone mode in this PR. 

I'm sorry I'm making you uncomfortable, that was not my intention. I just don't want to just solve a tactical problem. In the past we've talked about trying to move everything to be more similar to cluster mode enabled, and I thought we were all more in agreement on that. Everything we release is something we have to maintain, so I really want to make sure we understand the vision that we're building towards."
1605917447,12192,oranagra,2023-06-25T07:49:54Z,"> Can you elaborate? I'm arguing that in the fullness of time everyone should be running on the clusterbus system

I don't think we're aiming there.
we **are** aiming to get rid of Sentinel, and allow for unsharded (possibly multi-db) redis-cluster. but i think we still want to support standalone.
i.e. a plain set of one master + replicas and someone managing them from outside without all the complexity of cluster.
i think that when someone's using redis this way, it should become his responsibility (the deployment scripts) to manage all the configurations correctly, but from the client's perspective it should look the same."
1606153079,12192,madolson,2023-06-25T16:40:38Z,"> I don't think we're aiming there.
we are aiming to get rid of Sentinel, and allow for unsharded (possibly multi-db) redis-cluster.

Ok, assuming the ""we"" is you and Zhao, I think that is the misalignment. I don't really see the benefit of removing sentinel by pushing a bunch of configuration user side (which is what in practice you mean be scripts). I want all configurations to be passed through something like the clusterbus, so that nodes are talking with each other and management is simpler. "
1606722093,12192,oranagra,2023-06-26T06:16:31Z,"by ""we"" i meant that it was something all of us agreed on long ago (not specifically this thread).
i.e. in order to retire sentinel, we wanna let cluster handle the sentinel use cases that are currently unsupported (un-sharded, with voting replicas or any other way we can support a cluster of a single shard. and since it's un-sharded, it might as well also allow multi-db). in that case no script will be required, it'll be a proper cluster (cluster bus and all).
the main gain from this IMHO is that we won't need to keep maintaining both architectures (cluster and sentinel) at the same time.

anyway, completely detached from the above paragraph, i think we should still support the standalone mode, where there's no cluster bus, and instances don't talk to each other to make decisions, instead everything is controlled from outside by the orchestration scripts users write (without the cluster mode subsystem being active).

i'm arguing that from operations (deployment / configuration) point of view these two architectures are completely different, but the client / app interfaces should be the same."
1607016077,12192,zuiderkwast,2023-06-26T08:57:28Z,"FWIW, I thought it was surprising when I learned that replicas don't automatically take over in case of master failure. The different modes were confusing to me as a newcomer. In a good distributed database, everything should just work.

Cluster is the only mode where failover is automatic and clients automatically and quickly learn about it without reconnecting. I hope we can make it usable for all, by minimizing the negative aspects such as cluster bus network overhead, memory overhead, etc., which I think we can do with unsharded cluster, voting replicas, the one-dict-per-slot feature (eliminates the linear memory overhead) and whatever else. Deprecate multiple DBs in all modes or support it in all modes.

User-written orchestration scripts are risky. There's always a risk you don't get things right. I don't think we should recommend it for failover handling and such. Single standalone nodes can be useful by themselves though. But whenever two nodes talk to each other, why not enable the cluster bus?"
1611158355,12192,yossigo,2023-06-28T10:28:22Z,"> Cluster is the only mode where failover is automatic and clients automatically and quickly learn about it without reconnecting.

I strongly agree with this.

Standalone clients rely on the user providing the server address, and they can assume all connections always go to the same endpoint. Redis Cluster clients must deal with multiple, changing endpoints - which naturally makes everything more complex.

The main concern I have with this PR is that we're inadvertently pushing this complexity into standalone Redis clients. Why should we do it when we already have Redis Cluster clients that have done that?

We should leave standalone clients as they are and drive more adoption of Redis Cluster clients by treating the Redis Cluster spec as the universal protocol for clients to talk to Redis, particularly any deployment with multiple instances."
1611184468,12192,soloestoy,2023-06-28T10:50:18Z,"@yossigo finally waiting for your reply. But I don't agree with you. Standalone mode has been widely used, and I believe it will continue to be used for a long time.

>Standalone clients rely on the user providing the server address, and they can assume all connections always go to the same endpoint. Redis Cluster clients must deal with multiple, changing endpoints - which naturally makes everything more complex.

I'm not sure how you came to this conclusion, but the standalone mode also uses different endpoints for the master and replicas. Moreover, as far as I know, many users also achieve read-write separation in standalone mode by executing read commands on replicas.

>The main concern I have with this PR is that we're inadvertently pushing this complexity into standalone Redis clients. Why should we do it when we already have Redis Cluster clients that have done that?

I don't understand, we didn't introduce any complexity to standalone, if users don't want to receive -MOVED they can disable the config `replica-enable-redirect`. If they enable it, it means that they want to obtain the ability of automatic switch-over.

>We should leave standalone clients as they are and drive more adoption of Redis Cluster clients by treating the Redis Cluster spec as the universal protocol for clients to talk to Redis, particularly any deployment with multiple instances.

We can promote the cluster mode, but it's not a reason to give up on optimizing standalone mode. From my point of view, suppressing B in order to promote A is a very wrong approach."
1611278968,12192,soloestoy,2023-06-28T12:04:00Z,"The following are my opinions and responses about the matters we discussed during our core-team meeting yesterday:
1. Firstly, I believe we should continue to support standalone mode for a long time, and there may be four deployment modes in the future: standalone, sharded-cluster, unsharded-cluster, and sentinel.
2. It's very necessary to enable standalone mode to have the ability of redirection, clients can automatically failover according to `-MOVED` without causing errors in users business. And this will not break anything, including sharded-cluster, unsharded-cluster, and sentinel. I don't understand why this optimization is not being made.
3. As for `-MOVED -1 ip:port`, the `-1` indicates that redis server is currently no split deployment according to slots. Therefore, this interpretation is reasonable for standalone, sharded-cluster, and unsharded-cluster, without any ambiguity.
4. Regarding clients, as far as I know, clients also have their own working modes. They have explicit standalone mode (which is usually not explicitly written in the code) and cluster mode. Their work processes are different, and they have different paths when processing the `-MOVED`. This will not cause any ambiguity. For examples:
    * https://github.com/redis/jedis/blob/master/src/main/java/redis/clients/jedis/executors/ClusterCommandExecutor.java
    * https://github.com/redis/jedis/blob/master/src/main/java/redis/clients/jedis/executors/DefaultCommandExecutor.java
5. Speaking of how clients handle `-MOVED` in more detail, you may argue that when cluster mode receives `-MOVED`, it needs to execute `CLUSTER SLOTS` or other commands to update the local routing table. However, there are two meanings behind `-MOVED` in cluster mode:
    * one is ""Slot migration occurred""
    * another one is ""The master-replica role has changed"".

    Actually, it is unnecessary to re-pull the routing table when the master-replica role changes. The client just needs to replace the current IP with the new IP in -MOVED. Only when the slot migration occurs, it is necessary to re-pull the table to avoid continuous redirection of subsequent commands. Here I think you can get the point that if the client works in standalone mode, there is no need to pull the table because there will be no slot migration events.
6. As for sentinel mode, this is even easier to solve. After receiving `-MOVED`, if clients want to be more real-time, just automatically redirect to the new address. If clients want to keep using the previous way, then just send `sentinel get-master-addr-by-name` to sentinel again to get the new master address. `-MOVED` enhances the real-time performance of failover processing and does not break the current working mode of sentinel clients."
1611769052,12192,yossigo,2023-06-28T16:51:43Z,"> Firstly, I believe we should continue to support standalone mode for a long time, and there may be four deployment modes in the future: standalone, sharded-cluster, unsharded-cluster, and sentinel.

I agree with that, and I also think `unsharded-cluster` and `sentinel` should eventually merge, leaving us with a single server-side implementation, even if we continue to expose the Sentinel API for some time for backward compatibility.

I'm not sure we mean the same thing by `standalone`. To me, this means a single `redis-server` instance. It may be replicated for various reasons, but we don't consider this deployment a distributed system. It's not meant to provide H/A, for example, and we expect users who want that to use `unsharded-cluster` instead and not try to roll their own home-grown scripting solution (as @zuiderkwast pointed out).

> It's very necessary to enable standalone mode to have the ability of redirection, clients can automatically failover according to -MOVED without causing errors in users business. And this will not break anything, including sharded-cluster, unsharded-cluster, and sentinel. I don't understand why this optimization is not being made.

Because from my point of view, this brings to standalone-mode clients complexities that we already solve in cluster-mode clients. If we solve that problem for cluster-mode and support unsharded-cluster, why not pick a cluster-mode client if we need redirects, etc.?

We need to figure out two things here:
1) Does this introduce significant complexity to clients, as I fear? Maybe we need to get some perspective about this from some of the client maintainers.
2) In a future world where we have an `unsharded-cluster` option, based on Cluster V2 or even more advanced work that removes existing cluster or cluster bus complexities, what is the value of using `standalone+redirects` and not `unsharded cluster`?

> As for -MOVED -1 ip:port, the -1 indicates that redis server is currently no split deployment according to slots. Therefore, this interpretation is reasonable for standalone, sharded-cluster, and unsharded-cluster, without any ambiguity.

For clients that already handle `-MOVED`, I agree this is simple and clear. But as stated above, my concern is that now we're requiring *all* clients to handle `-MOVED` which has a lot of potentially sharp edges:

* Does the redirect apply to *all* connections or just the one we received the reply for?
* If we have a connection pool, do we need to drop all connections and reconnect?
* What happens if the `-MOVED` target fails to resolve / connect / authenticate? Do we have to remember the old address and go back to it again, hoping we'll get better luck or at least `-MOVED` to a better endpoint? Otherwise, it's a dead end.

> Actually, it is unnecessary to re-pull the routing table when the master-replica role changes. The client just needs to replace the current IP with the new IP in -MOVED. Only when the slot migration occurs, it is necessary to re-pull the table to avoid continuous redirection of subsequent commands. Here I think you can get the point that if the client works in standalone mode, there is no need to pull the table because there will be no slot migration events.

I know that different clients handle this differently, and I think we should have done a better job creating a Redis Cluster Client Spec that describes exactly how clients need to handle the different cases.

But one common thing to cluster-mode clients is they maintain some view of the server-side topology - what nodes exist, etc. They may update it incrementally based on `-MOVED` or use `-MOVED` as a trigger to refresh everything, but this is inherent to having cluster support. Standalone clients don't have any of this as a common baseline. This also means that even if we define `-MOVED` as a valid reply in standalone mode, we may discover that adoption is limited.

> As for sentinel mode, this is even easier to solve. After receiving -MOVED, if clients want to be more real-time, just automatically redirect to the new address. If clients want to keep using the previous way, then just send sentinel get-master-addr-by-name to sentinel again to get the new master address. -MOVED enhances the real-time performance of failover processing and does not break the current working mode of sentinel clients.

I agree, and that's exactly the point I've tried to convey. Sentinel clients, like cluster clients, and unlike standalone clients, are already aware of topology and are part of a distributed system."
1612048951,12192,zuiderkwast,2023-06-28T20:21:02Z,"@soloestoy You have a point that `unsharded-cluster` also may need a MOVED-redirect without slot.

From a client POV, perhaps `standalone-with-redirect` and `unsharded-cluster` are identical? If that is so, I have no objection to a `standalone-with-redirect` mode, i.e. if there is no extra complexity for clients to care about.

I hope it will be possible to scale an unsharded cluster to a sharded cluster in runtime, or at least switch between these modes using manual failovers, so a good client should handle both.

> Speaking of how clients handle `-MOVED` in more detail, you may argue that when cluster mode receives `-MOVED`, it needs to execute `CLUSTER SLOTS` or other commands to update the local routing table. However, there are two meanings behind `-MOVED` in cluster mode:
>
> * one is ""Slot migration occurred""
>
> * another one is ""The master-replica role has changed"".
>
> Actually, it is unnecessary to re-pull the routing table when the master-replica role changes. The client just needs to replace the current IP with the new IP in -MOVED. Only when the slot migration occurs, it is necessary to re-pull the table to avoid continuous redirection of subsequent commands. Here I think you can get the point that if the client works in standalone mode, there is no need to pull the table because there will be no slot migration events.

That is a very good point! :bulb: I haven't thought about that before nor seen it implemented. A client can easy tell the difference: if the MOVED-targed was a replica before, it means there was a failover. Let's explain this in the cluster spec and I'll consider implementing this logic in the clients I'm involved in (hiredis-cluster in C and some client in Erlang).

These are also exactly the two events cluster clients may want to subscribe to, so they can eagerly update their slot mapping before they get a single MOVED. (I implemented one of them in #10358.)

Btw, I think it's good that clients are allow some amount of freedom in how they handle these things."
1612077205,12192,madolson,2023-06-28T20:41:13Z,">if the MOVED-targed was a replica before, it means there was a failover.

These types of optimizations are risky to document and suggest clients make, since it limits our ability to make changes server side. If in the future we support nodes which might act as a primary to some slots and a replica to others, think an all primary cluster where if a node fails one of the other primaries takes the slots, we run into a situation where your assertion no longer holds. There are also extreme cases where a replica migration and slot migration might occur, so the clients stale data might think something was a failover that was really a migration. This can't happen in the standalone case."
1612395943,12192,yangbodong22011,2023-06-29T03:55:53Z,"EDIT
-----
unsharded-cluster will try its best to be compatible with standalone, so from the perspective of the client, it will be accessed in standalone mode.

old response: 
---------
As a Jedis Reviewer, I evaluated that if the replica redirect function is implemented in Jedis standalone mode, the work will not be too much (1 day). 

As a Contributor of Redis, my opinion on this function is: Redis has been developed for 14 years, and the standalone mode is simple and out-of-the-box. And most clients have (or default) to support the standalone mode, in the foreseeable future, it will not be replaced. Even if we support `unsharded-cluster`, the access protocol is also in `Cluster` mode, that is, the open source client must support `unsharded-cluster` on the cluster code, not in standalone mode, so I think optimizing the standalone experience (whether from redis or client), are meaningful and will not be deprecated in the future."
1612458256,12192,soloestoy,2023-06-29T05:36:38Z,">From a client POV, perhaps standalone-with-redirect and unsharded-cluster are identical? If that is so, I have no objection to a standalone-with-redirect mode, i.e. if there is no extra complexity for clients to care about.

@zuiderkwast yes, you are right!

>I hope it will be possible to scale an unsharded cluster to a sharded cluster in runtime, or at least switch between these modes using manual failovers, so a good client should handle both.

That is a beautiful wish, and I also hope we can achieve it. However, unfortunately, it is not possible, or rather, it is unlikely for a long time. Even if we have implemented unsharded-cluster and also use the same client (cluster mode?) to access both unsharded-cluster and sharded-cluster, there are still significant differences for users' businesses. When scale from unsharded-cluster to sharded-cluster, many problems will arise, such as the inability to execute select commands, inability to execute sort by, inability to execute cross-slot data commands, and so on."
1612464057,12192,soloestoy,2023-06-29T05:46:12Z,">* Does the redirect apply to all connections or just the one we received the reply for?
>* If we have a connection pool, do we need to drop all connections and reconnect?
>* What happens if the -MOVED target fails to resolve / connect / authenticate? Do we have to remember the old address and go back to it again, hoping we'll get better luck or at least -MOVED to a better endpoint? Otherwise, it's a dead end.

@yossigo Wouldn't cluster mode have these issues as well? These problems are not unique to standalone mode, and cluster mode will also encounter them. Moreover, these problems have already been solved in cluster mode and can be easily applied to standalone mode. As @yangbodong22011 said, this is not a difficult task."
1614010722,12192,soloestoy,2023-06-30T02:01:03Z,"It looks like we all agree that standalone should be long-term supported, I think we can merge it, @redis/core-team any other questions?"
1617454544,12192,yossigo,2023-07-03T06:40:33Z,"@soloestoy We agree about standalone mode being supported in the future, but I think merging this PR (as is or with modifications) requires consensus on other topics where we still don't have it.

I think we should at least be aligned about the answers to these questions:
1. Should the standalone mode support high availability, or can we assume we have a non-sharded cluster for that?
2. If high availability in standalone mode is supported, what does that look like on the protocol side? Arbitrary extensions, a subset of Redis Cluster Spec, or a full Redis Cluster Spec?
3. Do we expect all standalone clients to support high availability (unlike in the current state where clients need to support Sentinel or Cluster mode explicitly)? If we do, and we adopt a full or partial cluster spec for standalone high availability, doesn't this mean we effectively want to move all clients to cluster mode?"
1617692368,12192,soloestoy,2023-07-03T09:14:08Z,"@yossigo I couldn't fully understand what you were saying, I'm trying to provide some answers based on my understanding:

Firstly, of course, standalone should support high availability, which is unrelated to the Redis mode (standalone or cluster) and even Redis. All databases should support high availability.

Regarding Redis, it's not true that returning `-MOVED` in cluster mode means high availability. The `-MOVED` in cluster mode is essentially to indicate the change of the routing table, which includes failover.

Cluster supported self-failover from the beginning, which I think is a wise choice. However, Cluster also allows disabling automatic failover by setting `cluster-replica-no-failover`. In this case, is it not high availability if manual failover is performed through an external system?

As far as I know, many users build their own high availability systems, even without using Sentinel and Cluster's auto failover. These self-built high availability systems ensure their running Redis, whether standalone or cluster. I guess AWS is doing the same.

In addition, `-MOVED` is only a supplement to the high availability of the server-side master-replica role switch, or it is an optimization of Redis high availability switch. The `-MOVED` reply in Cluster can help clients better handle server-side switch. Standalone switch is still happening without `-MOVED`, and having `-MOVED` doesn't mean it becomes Cluster.

I still need to emphasize one point: `-MOVED` has two meanings, ""Slot migration occurred"" and ""The master-replica role has changed"". If stick to its literal meaning, `-MOVED` should only be used to describe slot migration, and master-replica role switch should be described by other words, such as `-REDIRECT`, even the switch in the cluster mode should use `-REDIRECT`. But I don't recommend doing so, as it would break existing systems and has no meaning."
1627674167,12192,yossigo,2023-07-09T10:32:47Z,"> @yossigo I couldn't fully understand what you were saying, I'm trying to provide some answers based on my understanding:

Sorry about that. Please refer to specific unclear questions so I can try to explain myself more clearly.

> Firstly, of course, standalone should support high availability, which is unrelated to the Redis mode (standalone or cluster) and even Redis. All databases should support high availability.

We can question that. Why? Today, standalone Redis does not support high availability. You need to use either Sentinel, which is compatible with standalone Redis, or Redis Cluster, which is incompatible. But if we had a non-sharded cluster, why would anyone who wants high availability choose standalone?

> Cluster supported self-failover from the beginning, which I think is a wise choice. However, Cluster also allows disabling automatic failover by setting cluster-replica-no-failover. In this case, is it not high availability if manual failover is performed through an external system?

It's still highly available because the Redis Cluster Specification clearly defines how clients are expected to learn about and deal with failovers. Redis standalone doesn't have this capability today. Yes, we can add it - but that's my point: why should we do it if we already have it in Redis Cluster, and we're in consensus about creating a non-sharded cluster in the future?

> As far as I know, many users build their own high availability systems, even without using Sentinel and Cluster's auto failover. These self-built high availability systems ensure their running Redis, whether standalone or cluster. I guess AWS is doing the same.

But if we had a much better Redis Cluster supporting non-sharded mode, why would those users still build their high-availability systems?

> I still need to emphasize one point: -MOVED has two meanings, ""Slot migration occurred"" and ""The master-replica role has changed"". If stick to its literal meaning, -MOVED should only be used to describe slot migration, and master-replica role switch should be described by other words, such as -REDIRECT, even the switch in the cluster mode should use -REDIRECT. But I don't recommend doing so, as it would break existing systems and has no meaning.

If we agree that there are valid use cases for redirecting in standalone mode, using a different, dedicated reply makes sense."
1628086269,12192,soloestoy,2023-07-10T03:56:51Z,"@yossigo TBH, I'm afraid I don't totally agree with your definition of high availability, but I can follow your opinion and discuss it further.

>It's still highly available because the Redis Cluster Specification clearly defines how clients are expected to learn about and deal with failovers.

From these words, in my understanding you are suggesting that high availability means that the server can provide switch information to allow the client to smoothly complete the switch, right? And then in this case, we can assume that high availability is unrelated to whether the server can detect and perform failover on its own, i.e. it is unrelated to whether cluster is configured with `cluster-replica-no-failover`, whether standalone uses Sentinel, and whether Failover Coordinator is used in Flotilla (IMHO, FC in Flotilla is like Sentinel in standalone. If unsharded-cluster is part of Flotilla, how does it perform failover?).

So the key point in high availability is whether Redis server can return switch information to the client, e.g. `-MOVED`. We can continue the discussion based on this POV. My opinion is that every form of Redis needs to support high availability, including standalone, cluster, and unsharded-cluster.

>But if we had a non-sharded cluster, why would anyone who wants high availability choose standalone?

Users have the right to choose, and IIRC we have already reached a consensus that standalone needs to be supported for a long time, even if unsharded-cluster appears in the future. I still don't understand why we can't make standalone support high availability. Are we planning to abandon users who use standalone mode? @oranagra I also need your opinion on this issue.

However, please note that unsharded-cluster is only a concept, it has not started development and there are still many details that have not been touched upon, and it should be noted that there is a big difference between concepts and implementation. And it is uncertain when it can be implemented, considering this, I also hope that we can support high availability for standalone users as soon as possible."
1628235397,12192,oranagra,2023-07-10T05:28:55Z,"we are certainly not gonna abandon standalone. the key point in my opinion remains that although there are several ways to manage instances, the clients should have just one interface, and that's what we need to fully design before we made any changes."
1628265773,12192,soloestoy,2023-07-10T05:46:00Z,"@oranagra can you elaborate on it? We agree standalone is long term supported, then high availability is necessary. I don't see any conflict with ""the clients should have just one interface""."
1628592567,12192,yossigo,2023-07-10T09:36:06Z,"@soloestoy
> From these words, in my understanding you are suggesting that high availability means that the server can provide switch information to allow the client to smoothly complete the switch, right?

A highly available Redis deployment provides all the moving parts required to handle failover end-to-end.

> So the key point in high availability is whether Redis server can return switch information to the client, e.g. -MOVED. We can continue the discussion based on this POV.

The ability to provide clients with the information necessary to handle failover is a key element in a highly available deployment - I agree with that. But server-side support is only one part. The other part is that clients need to properly support that. Currently, some clients do (those that support Sentinel or Cluster), and some clients don't (the rest).

>  My opinion is that every form of Redis needs to support high availability, including standalone, cluster, and unsharded-cluster.

In a world with unsharded-cluster and standalone server modes, why would we need to work out a way to deploy highly available standalone-mode Redis?

Yes, someone can build their own system based on standalone Redis and handle failovers. But why should they do that? And why should we ask clients that only support cluster or Sentinel to support yet another kind of deployment? After all, if someone builds their own H/A system based on standalone Redis and custom components, they could also use a load balancer or DNS or other network tricks to handle those redirects.

> Users have the right to choose, and IIRC we have already reached a consensus that standalone needs to be supported for a long time, even if unsharded-cluster appears in the future. I still don't understand why we can't make standalone support high availability. Are we planning to abandon users who use standalone mode? @oranagra I also need your opinion on this issue.

Yes, they can choose:
1. Run standalone Redis which, like today, doesn't support high availability *out of the box*.
2. Run cluster mode Redis, which supports high availability and shading - but is not compatible with standalone Redis.
3. Run a future non-sharded cluster Redis, which supports high availability but no sharding and is 100% compatible with standalone Redis.

You're proposing option ""4"". But why? And how exactly is it supposed to work? The way I understand it, it's a home-grown system and, as such, it can either be built not to require any client support (VIPs, DNS, etc.) or communicate to clients using the cluster protocol.

> However, please note that unsharded-cluster is only a concept, it has not started development and there are still many details that have not been touched upon, and it should be noted that there is a big difference between concepts and implementation. 

I agree! And based on this discussion, I think it's important to focus our efforts there so it's no longer just a concept. "
1628675533,12192,zuiderkwast,2023-07-10T10:33:16Z,"> > Cluster supported self-failover from the beginning, which I think is a wise choice. However, Cluster also allows disabling automatic failover by setting cluster-replica-no-failover. In this case, is it not high availability if manual failover is performed through an external system?
> 
> It's still highly available because the Redis Cluster Specification clearly defines how clients are expected to learn about and deal with failovers. Redis standalone doesn't have this capability today. Yes, we can add it - but that's my point: why should we do it if we already have it in Redis Cluster, and we're in consensus about creating a non-sharded cluster in the future?

@yossigo I disagree with this answer. If the cluster can't do failovers, it's not highly available. If all replicas are configured with `cluster-replica-no-failover`, or if there are no replicas at all, the cluster is not highly available by itself. (Edit: I see now that the question does include an external system that performs manual failover, so the answer does make sense.)

I think we need to make a difference between ""support HA"" and ""be HA"". Standalone is not HA by itself, but it can be HA with help of sentinel or other sentinel-like tools. In the same way, maybe a cluster with `cluster-replica-no-failover` can be HA with help of some kind of daemon, K8s operator or such. But it's not HA by itself.

Why do we want to deprecate sentinel? I believe the reason is that it's not very elegant to require an external tool. A system that can be HA by itself is more elegant and user-friently. We can't forbid external sentinel-like tools but we don't need to actively support them."
1628715636,12192,oranagra,2023-07-10T10:59:26Z,"> Why do we want to deprecate sentinel?

We wanna deprecate Sentinel so that we don't have to keep maintaining that code (side by side with cluster).
ideally, the unsharded-cluster can fill that spot, maybe together with some adapter layer to provide sentinel interface to clients."
1628819815,12192,soloestoy,2023-07-10T11:58:50Z,"> Yes, they can choose:
>
>1. Run standalone Redis which, like today, doesn't support high availability out of the box.
>2. Run cluster mode Redis, which supports high availability and shading - but is not compatible with standalone Redis.
>3. Run a future non-sharded cluster Redis, which supports high availability but no sharding and is 100% compatible with standalone Redis.

What users can choose should be:
1. Run standalone Redis which, optimized, **support** high availability.
2. Run cluster mode Redis, which supports high availability and shading - but is not compatible with standalone Redis.
3. Run a future non-sharded cluster Redis, which supports high availability but no sharding and is 100% compatible with standalone Redis.

And about point 3, I don't think unsharded cluster can be 100% compatible with standalone, like if a server in standalone mode does not support redirection, then the client in standalone mode will not be able to complete failover. As a result, accessing a server in unsharded cluster mode with a client in standalone mode cannot guarantee high availability.

>it can either be built not to require any client support (VIPs, DNS, etc.)

VIP and DNS cannot support high availability in real-time. Since DNS cannot guarantee real-time updates, and there may be existing connections pointing to the backend after the VIP is updated, these will result in those old connections accessing the replica (master before failover). This is the problem that this PR aims to solve, allowing those old connections to be automatically switched to the new master (replica before failover), i.e. automatically routed to the new master's DNS, establishing new connections on the VIP will also access the new master."
1630579289,12192,yossigo,2023-07-11T10:33:12Z,"> And about point 3, I don't think unsharded cluster can be 100% compatible with standalone, like if a server in standalone mode does not support redirection, then the client in standalone mode will not be able to complete failover. As a result, accessing a server in unsharded cluster mode with a client in standalone mode cannot guarantee high availability.

Why do you think there's no way to get unsharded cluster to be 100% compatible with standalone Redis? I think this is the key here.

If we do manage to achieve that, users will be able to connect to it using standalone clients for maximum compatibility but no failover support, or using cluster clients if they also wish to have failover support.

> VIP and DNS cannot support high availability in real-time. Since DNS cannot guarantee real-time updates, and there may be existing connections pointing to the backend after the VIP is updated, these will result in those old connections accessing the replica (master before failover). This is the problem that this PR aims to solve, allowing those old connections to be automatically switched to the new master (replica before failover), i.e. automatically routed to the new master's DNS, establishing new connections on the VIP will also access the new master.

I was only providing this as an example of how someone who builds their own high availability can address that, without client support."
1631813459,12192,yangbodong22011,2023-07-12T04:01:39Z,"> If we do manage to achieve that, users will be able to connect to it using standalone clients for maximum compatibility but no failover support, or using cluster clients if they also wish to have failover support.

@yossigo Do you mean that we can use the client's cluster mode or standalone mode to access unsharded-cluster?

This may have the following details:
1. Clients in cluster mode usually need to call `cluster slots` or `cluster nodes` to obtain and resolve the routing table. But I think unsharded-cluster does not need it, because it holds 16384 slots and does not need routing tables to inform.
2. Clients in cluster mode usually restrict the execution of cross-slot commands (most clients check the key by themselves and report an error in advance, e.g. [jedis](https://github.com/redis/jedis/blob/master/src/main/java/redis/clients/jedis/ClusterCommandArguments.java#L20C19-L20C19)), but standalone does not, and neither does unsharded-cluster.
3. Clients in cluster mode usually restrict `select` and `swapdb` commands, but neither are standalone and unsharded-cluster.

So I think it is the right way to use the client's standalone mode to access unsharded-cluster (the only thing we need to do is to support the `moved` protocol for standalone, so that high availability can be achieved)"
1632265277,12192,zuiderkwast,2023-07-12T10:38:33Z,"There is an alternative to redirects that doesn't need any modification to clients: The server can close the connection after failover. If the client reconnects, it can be NAT-routed to the new master. If failover is triggered manually, cloing clients can also be done manually using client kill. Too brutal?

Regarding unsharded cluster, I opened a ticket to discuss it: #12408"
1635182255,12192,soloestoy,2023-07-14T02:39:27Z,">users will be able to connect to it using standalone clients for maximum compatibility but no failover support

@yossigo why? In my opinion, this is an irresponsible approach for users. Clients using standalone mode also need to support high availability, which is a very necessary feature."
2016980659,12192,CLAassistant,2024-03-24T23:08:04Z,"[![CLA assistant check](https://cla-assistant.io/pull/badge/not_signed)](https://cla-assistant.io/redis/redis?pullRequest=12192) <br/>Thank you for your submission! We really appreciate it. Like many open source projects, we ask that you sign our [Contributor License Agreement](https://cla-assistant.io/redis/redis?pullRequest=12192) before we can accept your contribution.<br/><sub>You have signed the CLA already but the status is still pending? Let us [recheck](https://cla-assistant.io/check/redis/redis?pullRequest=12192) it.</sub>"
762373301,8327,madolson,2021-01-18T17:06:16Z,"@redis/core-team

Hey, this should be ready for review. I looked through the command code, but would appreciate another pair of eyes at least since that is not the code I'm more comfortable with."
762720282,8327,soloestoy,2021-01-19T09:34:24Z,"IMHO `MULTI/EXEC` and lua script can do it better, but if users really wanna it, I prefer some clear and specific commands like GETEX/PGETEX/GETDEL as mentioned in the original issue, it's not easy to understand and use GETEX with so much flags, especially these flags have many conflict."
762801080,8327,oranagra,2021-01-19T12:08:24Z,"@soloestoy indeed it can be achieved by MULTI exec, but this will make it more convenient for users, and AFAIK it was requested a lot.

The problem with splitting it to many commands is that whenever we wanna add a new option, we need to add it to all of them, or add another variant of the command.
Redis did abandon SETEX, SETNX, SETXX a long time ago in favor of SET with various arguments.
and we recently abandoned GETSET (to avoid adding an EX argument to it) in favor of SET with GET argument, and did the same to many many other commands (Z*RANGE*, GEORADIUS*, *PUP*PUSH).

so as long as the GETEX command is always a write command and always refers to one key, and always returns the value, i think we should stick with one command rather than several."
763561854,8327,oranagra,2021-01-20T12:09:50Z,"@nmvk please avoid pushing a rebase and an actual change (editing old commits) in the same force-push, it's very hard for me to find your changes and review them."
763801688,8327,oranagra,2021-01-20T17:17:05Z,"@nmvk thank you.
can you please update the PR top comment, so that we can use it as a commit comment.
KEEPTTL needs to be removed, but maybe a few other updates too.
also maybe note some non-obvious things about the code (like why `setexCommand` no longer needs to be handled in feedAppendOnlyFile).
if you can, please also make a PR https://github.com/redis/redis-doc"
763802694,8327,oranagra,2021-01-20T17:18:24Z,"@redis/core-team please approve a new GETEX command, and new arguments to the SET command (see top comment)"
763813642,8327,nmvk,2021-01-20T17:35:15Z,@oranagra Thank you so much for the review. I have updated the PR top comment with the new details. I will follow up with doc change soon and make a PR once it is ready.
764456206,8327,soloestoy,2021-01-21T08:03:01Z,"Just a little suggestion, could we rename the `GETEX` command for example `GETX`? Like `XML` means e**X**tensible **M**arkup **L**anguage. I know `GETEX` means **get extended**, but users may be confused, it looks so much like `SETEX`."
764481198,8327,oranagra,2021-01-21T08:51:17Z,"I agree GetEx is not very good, but i'm also not sure GetX is much better.
maybe someone has additional suggestions?
@itamarhaber?"
764852956,8327,madolson,2021-01-21T18:36:08Z,"I think I like getx slightly more than getex. The other one I thought about was getopt, for get options, but thought it was slightly clunky. "
764943677,8327,nmvk,2021-01-21T21:10:12Z,"I do not have a strong opinion, but may be `GETX` can help clear some confusion due to `SETEX`.  I can revise the PR once there is an agreement."
766349921,8327,oranagra,2021-01-24T13:40:27Z,"I was in favor of GETX but i wanna change my mind and go 180° backwards.
* GETX may be a good name for an ""extended"" command, but it's not consistent with SETEX, so we may one day wanna add SETX too (it looks odd to get GETX and SETEX)
* on the other hand, EX may stand for ""expiry"" in SET and it indeed looks odd to get a DEL argument there.

Then thinking of the INFO COMMANDSTATS, I think that DEL is inherently a different action than changing expiry, and i would rather see GETDEL distinctively there rather than be mixed with GETEX.

What these two command have in common is that they're a write command that doesn't take a new value, but that doesn't mean they have to be one command.

we do have a GET argument for SET, but that doesn't really changes what the command does to db.
but a DEL argument might be wrong.

p.s. using an old EXAT argument it is also possible to use GETEX as a GETDEL, but that's a side effect i would like to ignore.

Bottom line, i now vote for splitting this command into two: GETEX and GETDEL.
@redis/core-team please share your thoughts or approve before we ask for changes to be implemented in the PR."
766600370,8327,madolson,2021-01-25T07:10:56Z,"I like your proposal better. I suppose we've been on a trend to have fewer commands, but it's probably a good pattern to split commands by what they are intended to do.

Two minor comments if we go down this path:
1. GETEX should behave more similarly to SETEX, the syntax should probably be ```GETEX KEY <time> [PX|PXAT|EXAT]``` so that in the future we can extend setex with the same functionality.
2. Instead of GETDEL, I would propose just ""pop"", since that is fairly consistent for ""remove and return"" across Redis. This sounds good in my head, but it may be a bad idea."
766605386,8327,oranagra,2021-01-25T07:20:58Z,"@madolson i don't agree with these proposals.
1. we never wanna extend SETEX with more functionality, it's a deprecated command which is replaced by adding these functionalities to the SET command. making GETEX look like SETEX would be a mistake, it should look like SET.
This is quite clear that putting the `<time>` as a mandatory positional argument would conflict with the PERSIST option.
2. I think POP is confusing (hints this is a list or a stack), i think GETDEL reflects what it does better and is more consistent with the rest of the string commands."
766608476,8327,madolson,2021-01-25T07:27:15Z,"1. Than I would keep it as GETX, to not confuse anyone with SETEX.
2. Eh, I don't like GETDEL, but I don't feel strongly about it. EDIT: The other proposal was take. "
766612542,8327,oranagra,2021-01-25T07:35:03Z,"i think the GETEX refers to ""GET with EXpiry"", it's similar to ""GET EX"" and IMHO doesn't need to be syntax compatible with SETEX.
What i don't like about GETX (referring to eXtended) is that we add a new convention to redis commands. tomorrow we'll have BRPOPX, and others. but in fact, after removing the DEL argument, this is a GET with EXpiry.

I rather avoid the GETX, and i think that once we removed the DEL argument, GETEX is a good name (just don't try to match it to the bad syntax of SETEX).

I still don't like POP and TAKE, these are fancy names, but GETDEL it clearer and easier to understand it refers to a string object."
766622715,8327,madolson,2021-01-25T07:54:26Z,"Why do we want to limit only to expire? That is what get's us into these constrained situations in the first place. Right now we are just adding flags optimistically, I honestly think the PERSIST flag is probably worthless, and ideally we would probably want to more deeply re-think how Redis commands are structured so we don't have all these weird name collisions.

I guess I really don't feel that strongly: I have a weak preference for GETX + GETDEL then."
766676621,8327,itamarhaber,2021-01-25T09:23:51Z,I'm aligned with @oranagra's opinion on this.
766776489,8327,yossigo,2021-01-25T12:17:05Z,I also agree with @oranagra. 
766791220,8327,oranagra,2021-01-25T12:45:07Z,"@itamarhaber @yossigo since there's a lot of text here, please be a bit more explicit.
i assume you mean this post: https://github.com/redis/redis/pull/8327#issuecomment-766349921
i.e:
```
GETEX <key> [PERSIST][EX seconds][PX milliseconds] [EXAT seconds-timestamp][PXAT milliseconds-timestamp]
GETDEL <key>
```
maybe it would help if you say why you don't like the alternatives (mixing GET as an argument for the first command, or naming them TAKE, POP, GETX)"
767161397,8327,yossigo,2021-01-25T22:43:30Z,"Okay the long version then :)

* `GETX` seems confusing, creates yet another command notation which I don't think we want at this point.
* `POP` implies an operation on a collection, I think it's confusing.
* `TAKE` is a bit awkward for me as it doesn't seem related in any way to `GET`.
* `GETEX` is not perfect, it's a bit ambiguous as it is more easy to interpret as ""expire"" rather than ""extended"", but I can't come up with something better. At some point I did toy with `GETMUT` (mutable get) but:
  * It would make more sense if we also give up `GETDEL` and use a `DEL` argument, which is not good introspection-wise
  * Is also awkward
  * Steers this discussion further from a conclusion"
767309401,8327,oranagra,2021-01-26T05:39:48Z,"Ok, next. Who wants to make a redis-doc PR? "
767315064,8327,nmvk,2021-01-26T05:57:29Z,@oranagra I will follow up with a doc soon.
767737351,8327,nmvk,2021-01-26T18:25:11Z,Document PR - https://github.com/redis/redis-doc/pull/1501
1341342115,11595,zuiderkwast,2022-12-07T17:47:43Z,@madolson This trick can perhaps be used for the main keyspace db later if we move the key to robj as you suggested in #10802. (Open addressing makes scan complicated. This is simpler.)
1341348246,11595,zuiderkwast,2022-12-07T17:53:02Z,"Reviewers: The first 6 commits are from ""Make dictEntry opaque"" and have already been reviewed. It's enough to look at ""Store keys without dictEntry in dict for sets"" (and any commits added in the future)."
1341499526,11595,vitarb,2022-12-07T19:43:36Z,"This looks like a good idea for improving memory footprint of sets. At the same time, as part of https://github.com/redis/redis/issues/10802 we've been thinking about making memory improvements for the main dictionary that could also benefit sets. Our main idea was to embed entry metadata and keys into the byte array inside of the dict entry (similar to rax entry). This would allow optional values for sets and embedded keys without pointers, achieving similar (perhaps exactly same) memory improvements as you did here. One way these two changes could co-exist, is if we focus only on the main dictionary in https://github.com/redis/redis/issues/10802 making a copy of the dict type, and keeping legacy layout (with your improvement) for sets, while iterating on top of the entry with embedded content for the main dictionary. This might be a reasonable approach, given that embedding all parts of the dict entry has more risks and would take more time to get done. If embedded entry design proves successful, we could migrate sets to it later too."
1341922562,11595,madolson,2022-12-08T03:09:15Z,I don't think the two ideas are necessarily exclusive. This approach still looks  has to pay 24 bytes when we need a collision resolution. Having a way to reduce that to 16 bytes by omitting the key pointer would still save some extra bytes.
1345483735,11595,oranagra,2022-12-11T07:56:47Z,"i'm not sure i'm aware of the related discussions, for forgive me if i'm causing a mess.

i think that previously i thought this optimization will use a special dictEntry with no value (key and next), and save 8 byte.
it's nice that we save the next entry too, but:
1. maybe there's no reason for a full 24 byte dictEntry in case of collision? maybe we'll store the key directly in the table when there's no collision, and a pointer to a struct with key and next when there is a collition.
2. maybe we can avoid the next pointer on the main dict and hash/zset too, by using that MSB and a struct of 16 bytes?

i.e. this PR does two unrelated optimizations in a certain case, and leaves cases that can't use both, completely optimized.
considering the refactoring of #11465 it seems we can easily achieve either separately too."
1346198980,11595,zuiderkwast,2022-12-12T10:01:55Z,"@oranagra Of course we can do your suggested optimizations too. They are almost orthogonal to this PR. The reason I didn't implement ""omit next pointer"" yet is that dictEntry metadata complicates things and there are plans to get rid of the metadata (#10589).

The possible dictEntry optimizations I've considered so far:

1. Omit the entire dictEntry when possible (this PR)
2. Omit next pointer in dictEntry
3. Omit value in dictEntry
4. Put multiple key-value pairs in the same dictEntry allocation (AKA bulk chaining)

Do you want me to put multiple optimizations in this PR or to make separate PRs?"
1346292411,11595,oranagra,2022-12-12T11:11:39Z,"you're right, let's not mix things, but let's see what's logical to combine (added numbers to your post)

1. is only relevant to sets, not for the global dict or any other type
2. a generic dict optimization, whenever there are no collisions
3. only relevant for sets again same as 1
4. a generic dict optimization, same as 2 (for when we **do** have collisions)

so 1 and 3 are both relevant when we don't have a value, and also when we don't depend on the metadata feature.
they're both about sets, and i could argue that 1 is an evolution of 3.
so maybe handle these two here. and then a separate PR for each of the other two?"
1346500761,11595,zuiderkwast,2022-12-12T13:29:06Z,"OK, I can try adding a dictEntry without value (with key and next) in this PR. I guess we should come up with a scheme for pointers, also for the other things we want in the future. How about this?

    LSB  dictEntry* actually points to
    ---  -----------------------------
    xx1  Key
    000  Normal dictEntry {key, value, next, (metadata)}
    010  dictEntry_no_value {key, next}
    100  Reserved for dictEntry_no_next {key, value, (metadata)}
    110  Reserved for future use, maybe bulk entry, maybe something else
"
1346687943,11595,oranagra,2022-12-12T15:19:57Z,"note that in 32bit you only have 2 LSBs.
maybe we'll have to have some compromises, or let each instance of the dict designate the bits for different purposes. i.e. some dicts will need one optimization and know which bucket uses it, and for others that optimization is irrelevant and can use that bit can be used to distinguish the bucket type for another optimization."
1346822668,11595,zuiderkwast,2022-12-12T16:20:30Z,"I believe we have 3 LSBs on 23bit too, as long as the allocation is at least 8 bytes. Malloc returns memory ""suitably aligned for any built-in type"" according to `man malloc`. Something similar is written in POSIX and the C standards too. Built-in types include double and int64_t IIUC.

When jemalloc is used, lg-quantum = 3 also for 32-bit. It's not even possible to set lg-quantum lower than 3."
1346839378,11595,zuiderkwast,2022-12-12T16:32:23Z,"I guess you're right. There is one case where we don't have 8 bytes alignment: If we don't HAVE_MALLOC_SIZE, we `return (char*)ptr+PREFIX_SIZE` in ztrymalloc_usable, where `PREFIX_SIZE` is `sizeof(size_t)` which is 4 on 32-bit. So zmalloc is not fully malloc compatible... Can we change PREFIX_SIZE to a minimum of 8?"
1347039943,11595,oranagra,2022-12-12T18:35:10Z,"you're right (twice), but also sds pointers can easily get non-8 byte aligned (due to the sds header, that can even by one byte), i.e. considering that we use that bit to distinguish between dictEntry and sds, we need both to avoid using that bit.
am i missing anything?

in addition to that, if that dict thing is a generic feature, it also collides with the ability to store other (non pointer) values (like long), but i guess we can put that aside and enable that optimizations only on dicts that only save pointers."
1347320439,11595,zuiderkwast,2022-12-12T21:00:54Z,"> am i missing anything?

Yes, so it seems. :hand_over_mouth: The sds header is either 1, 3, 5 or 9 bytes so all sds pointers are odd. This optimization actually relies on the least significant bit being 1 (i.e. an odd value) and doesn't encode it further. This is what distinguishes it when it is used instead of a dictEntry.

If a 'no_value' dict is fed a key that is not an odd pointer, it falls back to using a dictEntry, so it's fine to use `long` where some are odd and some are even and get the optimization half of the time. But you could say this trick is biased towards odd numbers and sds keys. If we want a dict even numbers or normally aligned pointers (e.g. robj) as keys, the caller can flip the least significant bit to enable this optimization manually. Or we can control it using a bit in the dictType. That's not something we need at the moment though.

"
1348140524,11595,oranagra,2022-12-13T10:23:00Z,"ohh, so you actually rely on the fact sds headers give you odd pointers (and it'll break if we some day change sds).
i certainly missed that.
actually i don't think i bothered to look at the code yet, just the description (which probably should have had that detail).

well, i don't like it very much, if we can't find another way, maybe indeed dictType should be able to control that.
need to think if we have better options."
1348176460,11595,zuiderkwast,2022-12-13T10:50:50Z,"The top comment start like this: ""If a key has LSB=1 (e.g. sds strings)"".

Well, dict type can have flag that makes turns the bias towards even keys and then we set the bit internally and clear it again before it's returned to the dict user. We don't have this need now though, so I didn't implement it. If we want to use any bits in sds keys, this bit is actually the only thing we can rely on right now."
1348771673,11595,sundb,2022-12-13T15:12:02Z,"@zuiderkwast IMHO, i don’t think i like the implementation that makes me feel less readable.
Why not extend dictType to support operations with different entries (create, set value, get value, or other)?"
1348870100,11595,zuiderkwast,2022-12-13T15:52:45Z,"> Why not extend dictType to support operations with different entries (create, set value, get value, or other)?

@sundb I don't understand exactly. Can you explain more or give an example?"
1348878226,11595,oranagra,2022-12-13T15:56:39Z,"> The top comment start like this: ""If a key has LSB=1 (e.g. sds strings)"".

sorry i missed that.
it would be better to also explain that sds strings always have the LSB set and why. so it's clear we're counting on that."
1348883785,11595,zuiderkwast,2022-12-13T16:00:01Z,"> it would be better to also explain that sds strings always have the LSB set and why. so it's clear we're counting on that.

@oranagra Yes, you're right it wasn't very clearly explained. :)

If you like me to continue this at all, then how about adding two bits to dictType: `keys_are_odd:1` and `keys_are_even:1` and we can assert on that when adding keys and only use the key-as-dictType optimization if one of these are set. No fallback. Is it better?"
1349092632,11595,oranagra,2022-12-13T17:11:38Z,"it's hard to dismiss such a big memory saving.
but on the other hand this campaign starts to feel like the code is gonna be ugly and complicated.

i was hoping someone comes up with a winning design.

let me try to recap, and see if it's not that dirty after all.
out of the 4 different optimizations we listed above, the only one that doesn't store a dictEntry pointer is this one, which stores an sds.
the other 3 will store a dict entry, which in both 32bit and 64bit has 8 byte alignment, unless we don't HAVE_MALLOC_SIZE (which i'm willing to overlook for a moment, we can skip this optimization for that config).
so we can actually rely on the bit scheme you listed here https://github.com/redis/redis/pull/11595#issuecomment-1346500761
and if we want it more generic, we can take the `keys_are_odd` flag you proposed earlier (i suggest not to implement it yet but document that idea next to the current limitation).

maybe with some effort and some comments it'll not end up too bad.
WDYT?"
1350845151,11595,zuiderkwast,2022-12-14T10:36:20Z,"> maybe with some effort and some comments it'll not end up too bad.
> WDYT?

That's what I'd like to think. With some great reviewing we can get some nice result. :wink:

Of course, with pointer tricks, we do need casting to/from uintptr_t and flipping bits, etc. which adds complexity but we can isolate that stuff in functions.

I thought about defining dictEntry as a `union { entry with next; entry without next; etc. }` but it doesn't help much since we need to cast the pointer anyway and can as well cast to different structs depending on bits.

I believe the reason for the sun/sparc definition of HAVE_MALLOC_SIZE is precisely to make it 8 byte aligned in 32-bit builds. That architecture seems to require that though, which x86 doesn't, but what if we enable this on all 32-bit builds? Perhaps the dict memory saving covers the alignment penalty. We should probably measure this.

```diff
 #ifdef HAVE_MALLOC_SIZE
 #define PREFIX_SIZE (0)
 #else
-#if defined(__sun) || defined(__sparc) || defined(__sparc__)
-#define PREFIX_SIZE (sizeof(long long))
+/* Use at least 8 bits alignment on all systems. */
+#if SIZE_MAX < 0xffffffffffffffffull
+#define PREFIX_SIZE 8
 #else
 #define PREFIX_SIZE (sizeof(size_t))
 #endif
```

----

@sundb In #9464, I added a createEntry callback in dictType, which replaces the metadata feature and lets the caller allocate the entry and embed whatever they want in the entry, such as key and value. Is this what you have in mind? I don't particularily like this though, since it makes the dict API more complex.

This PR tries to keep the dict API simple and keep the dict entry opaque, which I think is a good thing. The caller doesn't need to worry about the internals of the dict entry."
1350937391,11595,oranagra,2022-12-14T11:00:05Z,"regarding PREFIX_SIZE, i don't think the dict optimizations (specifically for deployment who don't use sets) can overcome the overhead of increasing it, but i'm not sure i care much about the case when we're both in 32bit **and** don't HAVE_MALLOC_SIZE. either one of these on it's own will be ok, and when they're combined people will have to pay that price.
bottom line, so i'd accept the change you suggested.

"
1351327304,11595,sundb,2022-12-14T13:09:36Z,"> @sundb In #9464, I added a createEntry callback in dictType, which replaces the metadata feature and lets the caller allocate the entry and embed whatever they want in the entry, such as key and value. Is this what you have in mind? I don't particularily like this though, since it makes the dict API more complex.
> 
> This PR tries to keep the dict API simple and keep the dict entry opaque, which I think is a good thing. The caller doesn't need to worry about the internals of the dict entry.

@zuiderkwast  Yes, It is exactly what I imagined it to be, it is more elegant and abstract.
"
1363092753,11595,zuiderkwast,2022-12-22T16:51:38Z,"@oranagra I have implemented dictEntry without 'next' pointer on top of this PR in a separate branch [dict-no-next](https://github.com/zuiderkwast/redis/tree/dict-no-next). (Maybe just look at [dict.c](https://github.com/zuiderkwast/redis/blob/dict-no-next/src/dict.c).) If we decide to go this way, we can as well include it in this PR since it's almost the same trick. WDYT?

@sundb @vitarb If we want the caller to put stuff inside the dict entry, we can either use metadata (already available) or let the caller use `sizeof(dictEntry)` and allocate the entry, in which case we can't make it opaque. I don't like either of these options very much. IMO it's a better abstraction to isolate the optimizations inside dict and make the entry opaque.

The point of putting stuff in the same allocation as the dict entry is to avoid one cache miss and one pointer, right? Key-as-entry can accomplish that too by skipping the dict entry for no-value dicts (except for collisions but that's a minority). (If we put key in robj and use the whole robj as the dict key, we can make the main db dict a no-value dict. We just need a special keyCompare function.)

<details>
<summary>Some ascii diagrams regarding main db dict</summary>

```
Key included in robj, key as entry, no collision

 +-----------+      +----------------------+
 | ht_table  |   ,->| robj                 |
 +-----------+  /   +----------------------+
 | ...       | /    | key               -----> (sds) the actual key
 | dict key --'     | type, encoding, lru, |
 | ...       |      | refcount, expire(?)  |
 +-----------+      | ptr               ----> (string, set, hash, etc.)
                    +----------------------+

... with collision

 +-----------+      +-------+     +----------------------+
 | ht_table  |   ,->| entry |  ,->| robj                 |
 +-----------+  /   +-------+ /   +----------------------+
 | ...       | /    | key  --'    | key               -----> (sds)
 | entry -----'     | next  |     | type, encoding, lru, |
 | ...       |      +-------+     | refcount, expire(?)  |
 +-----------+                    | ptr                  |
                                  +----------------------+
```
</details>"
1364686640,11595,oranagra,2022-12-25T13:58:51Z,"i skimmed through both this PR (the delta from the previous one), and the last commit in the branch you referred to.
i didn't do an in-depth review of each line yet.
to be honest, the part i liked the least, was `dictMemUsage`.

I think the complexity here is acceptable (considering it's all inside dict.c, and consider the memory savings it provides).
the other two things that we should validate before proceeding IMHO are:
1. be confident that this direction doesn't block other things we plan to do soon.
2. do some benchmark and make sure this doesn't have any negative impact (on either hashes, or sets)"
1366160372,11595,zuiderkwast,2022-12-27T20:30:31Z,"@oranagra Thanks for looking.

> to be honest, the part i liked the least, was `dictMemUsage`.

Well, I'd had to come up with something. :) Do you have a better idea? It's an estimate but in many cases we sample keys and values, so the total size is an estimate anyway. I just hope my statistical calculations make sense.

> I think the complexity here is acceptable (considering it's all inside dict.c, and consider the memory savings it provides). the other two things that we should validate before proceeding IMHO are:
> 
> 1. be confident that this direction doesn't block other things we plan to do soon.

Sure, what other things do you have in mind?

> 2. do some benchmark and make sure this doesn't have any negative impact (on either hashes, or sets)

Of course. I'm hoping the tag action:run-benchmark tag can help with this.

Assuming none of the above is a blocker, there are still some more things to solve until this is ready.

* The defrag code still uses dictSetKey which asserts for sets. (It's possible to implement dictSetKey for keys without entry but it'd fallback to hashing the key. Alternatively, we can let dictScanDefrag take a struct of `{defrag-alloc, defrag-alloc-key, defrag-alloc-value}` callbacks and let the dict code handle defragging the keys and values as well. Any preference?)
* In the no-next branch, the tests crash in various places. I'm trying to debug it."
1366412032,11595,oranagra,2022-12-28T06:49:53Z,"> > 1. be confident that this direction doesn't block other things we plan to do soon.
> 
> Sure, what other things do you have in mind?

I'm not certain, i thought you know that better than me. @madolson feel free to jump in.

> > 2. do some benchmark and make sure this doesn't have any negative impact (on either hashes, or sets)
> 
> Of course. I'm hoping the tag action:run-benchmark tag can help with this.

i'm not certain the tag by itself is enough, we can start with that, but maybe we'll need some specific workflows to hit specific scenarios? please ping Filipe if needed.

> The defrag code still uses dictSetKey which asserts for sets. (It's possible to implement dictSetKey for keys without entry but it'd fallback to hashing the key. Alternatively, we can let dictScanDefrag take a struct of {defrag-alloc, defrag-alloc-key, defrag-alloc-value} callbacks and let the dict code handle defragging the keys and values as well. Any preference?)

i'm not certain i follow you, but i think we want to avoid re-hashing keys, so the other (slightly uglier) option is preferred. "
1378351402,11595,oranagra,2023-01-11T07:44:39Z,"we discussed this PR in a core-team meeting and we would like to proceed.
we realized that we might wanna change some things later (give up one optimization in favor of another), but since dict will be abstracted, it will be easy to do even after this is merged, or even in a later version (no compatibility issues)
@zuiderkwast please update on the status and next steps."
1378911357,11595,zuiderkwast,2023-01-11T15:11:38Z,"@oranagra This PR is up to date and working. I think it can be merged. I've updated the top comment slightly.

(We also discussed omitting the 'next' pointer in all dicts. I have an implementation in a separate branch but it's not stable.)"
1378925665,11595,oranagra,2023-01-11T15:17:56Z,"OK. I'll need to go through a detailed review. But also I remember we were missing some things. One was benchmark, not sure what else is any.."
1378951308,11595,zuiderkwast,2023-01-11T15:29:16Z,"Benchmark has run now. It looks OK to me.

![image](https://user-images.githubusercontent.com/273886/211846484-89b7afcf-9ec9-4972-ab19-905fcaca0290.png)

See https://benchmarksredisio.grafana.net/d/1fWbtb7nz/experimental-oss-spec-benchmarks?orgId=1"
1380533813,11595,oranagra,2023-01-12T15:10:46Z,Daily CI (for valgrind) https://github.com/redis/redis/actions/runs/3903508169
1380650231,11595,oranagra,2023-01-12T16:15:15Z,"looks like there some assertion in the module tests with 32bit build, PTAL"
1380703148,11595,filipecosta90,2023-01-12T16:43:38Z,"### Automated performance analysis summary

This comment was automatically generated given there is performance data available.

Using platform named: intel64-ubuntu20.04-biredis to do the comparison.

In summary:
- Detected a total of 54 stable tests between versions.



### Comparison between unstable and zuiderkwast:key-as-dictEntry.

Time Period from a month ago. (environment used: oss-standalone)

|                                Test Case                                 |Baseline unstable (median obs. +- std.dev)|Comparison zuiderkwast:key-as-dictEntry (median obs. +- std.dev)|% change (higher-better)|        Note        |
|--------------------------------------------------------------------------|------------------------------------------|----------------------------------------------------------------|------------------------|--------------------|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-1000B-values            | 120948 +- 0.3%  (3 datapoints)           | 120048 +- nan%  (1 datapoints)                                 |-0.7%                   |-- no change --     |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10| 173252 +- 0.1%  (3 datapoints)           | 171867 +- nan%  (1 datapoints)                                 |-0.8%                   |-- no change --     |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values             | 170524 +- 0.2%  (3 datapoints)           | 168662 +- nan%  (1 datapoints)                                 |-1.1%                   |-- no change --     |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10 | 481923 +- 0.4%  (3 datapoints)           | 464881 +- nan%  (1 datapoints)                                 |-3.5%                   |potential REGRESSION|
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values              | 202098 +- 0.4%  (3 datapoints)           | 203361 +- nan%  (1 datapoints)                                 |0.6%                    |-- no change --     |
|memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10  | 598786 +- 0.1%  (3 datapoints)           | 605342 +- nan%  (1 datapoints)                                 |1.1%                    |-- no change --     |
|memtier_benchmark-1Mkeys-100B-expire-use-case                             | 259532 +- 0.1%  (3 datapoints)           | 260207 +- nan%  (1 datapoints)                                 |0.3%                    |-- no change --     |
|memtier_benchmark-1Mkeys-10B-expire-use-case                              | 259175 +- 0.3%  (3 datapoints)           | 259582 +- nan%  (1 datapoints)                                 |0.2%                    |-- no change --     |
|memtier_benchmark-1Mkeys-1KiB-expire-use-case                             | 258416 +- 0.7%  (3 datapoints)           | 260832 +- nan%  (1 datapoints)                                 |0.9%                    |-- no change --     |
|memtier_benchmark-1Mkeys-4KiB-expire-use-case                             | 259649 +- 0.4%  (3 datapoints)           | 259729 +- nan%  (1 datapoints)                                 |0.0%                    |-- no change --     |
|memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values   | 274890 +- 0.4%  (3 datapoints)           | 276307 +- nan%  (1 datapoints)                                 |0.5%                    |-- no change --     |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values                  | 276134 +- 0.6%  (3 datapoints)           | 276369 +- nan%  (1 datapoints)                                 |0.1%                    |-- no change --     |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-10B-values                   | 277877 +- 0.3%  (3 datapoints)           | 279045 +- nan%  (1 datapoints)                                 |0.4%                    |-- no change --     |
|memtier_benchmark-1Mkeys-list-lpop-rpop-with-1KiB-values                  | 271764 +- 0.2%  (3 datapoints)           | 270485 +- nan%  (1 datapoints)                                 |-0.5%                   |-- no change --     |
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values             | 136116 +- 0.3%  (3 datapoints)           | 131817 +- nan%  (1 datapoints)                                 |-3.2%                   |potential REGRESSION|
|memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10 | 180910 +- 1.0%  (3 datapoints)           | 179835 +- nan%  (1 datapoints)                                 |-0.6%                   |-- no change --     |
|memtier_benchmark-1Mkeys-load-list-with-100B-values                       | 200173 +- 0.9%  (3 datapoints)           | 201852 +- nan%  (1 datapoints)                                 |0.8%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-list-with-10B-values                        | 233292 +- 0.2%  (3 datapoints)           | 235273 +- nan%  (1 datapoints)                                 |0.8%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-list-with-1KiB-values                       | 143119 +- 0.3%  (3 datapoints)           | 144046 +- nan%  (1 datapoints)                                 |0.6%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-stream-1-fields-with-100B-values            | 189935 +- 0.9%  (3 datapoints)           | 189733 +- nan%  (1 datapoints)                                 |-0.1%                   |-- no change --     |
|memtier_benchmark-1Mkeys-load-stream-1-fields-with-100B-values-pipeline-10| 541997 +- 0.4%  (3 datapoints)           | 543571 +- nan%  (1 datapoints)                                 |0.3%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-stream-5-fields-with-100B-values            | 151407 +- 0.7%  (3 datapoints)           | 150564 +- nan%  (1 datapoints)                                 |-0.6%                   |-- no change --     |
|memtier_benchmark-1Mkeys-load-stream-5-fields-with-100B-values-pipeline-10| 337467 +- 1.3%  (3 datapoints)           | 344005 +- nan%  (1 datapoints)                                 |1.9%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-string-with-100B-values                     | 235254 +- 0.6%  (3 datapoints)           | 236943 +- nan%  (1 datapoints)                                 |0.7%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-string-with-100B-values-pipeline-10         | 805071 +- 1.6%  (3 datapoints)           | 821425 +- nan%  (1 datapoints)                                 |2.0%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-string-with-10B-values                      | 252614 +- 0.2%  (3 datapoints)           | 253519 +- nan%  (1 datapoints)                                 |0.4%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-string-with-10B-values-pipeline-10          | 1043960 +- 0.1%  (3 datapoints)          | 1037922 +- nan%  (1 datapoints)                                |-0.6%                   |-- no change --     |
|memtier_benchmark-1Mkeys-load-string-with-1KiB-values                     | 215486 +- 0.1%  (3 datapoints)           | 216643 +- nan%  (1 datapoints)                                 |0.5%                    |-- no change --     |
|memtier_benchmark-1Mkeys-load-zset-with-10-elements-double-score          | 133177 +- 0.4%  (3 datapoints)           | 131313 +- nan%  (1 datapoints)                                 |-1.4%                   |-- no change --     |
|memtier_benchmark-1Mkeys-load-zset-with-10-elements-int-score             | 168669 +- 0.5%  (3 datapoints)           | 167078 +- nan%  (1 datapoints)                                 |-0.9%                   |-- no change --     |
|memtier_benchmark-1Mkeys-string-get-100B                                  | 270030 +- 0.5%  (3 datapoints)           | 271417 +- nan%  (1 datapoints)                                 |0.5%                    |-- no change --     |
|memtier_benchmark-1Mkeys-string-get-100B-pipeline-10                      | 1427914 +- 0.3%  (3 datapoints)          | 1430684 +- nan%  (1 datapoints)                                |0.2%                    |-- no change --     |
|memtier_benchmark-1Mkeys-string-get-10B                                   | 271456 +- 0.5%  (3 datapoints)           | 271334 +- nan%  (1 datapoints)                                 |-0.0%                   |-- no change --     |
|memtier_benchmark-1Mkeys-string-get-10B-pipeline-10                       | 1427805 +- 0.7%  (3 datapoints)          | 1434849 +- nan%  (1 datapoints)                                |0.5%                    |-- no change --     |
|memtier_benchmark-1Mkeys-string-get-1KiB                                  | 271239 +- 0.5%  (3 datapoints)           | 270881 +- nan%  (1 datapoints)                                 |-0.1%                   |-- no change --     |
|memtier_benchmark-1Mkeys-string-get-1KiB-pipeline-10                      | 1422791 +- 0.2%  (3 datapoints)          | 1436788 +- nan%  (1 datapoints)                                |1.0%                    |-- no change --     |
|memtier_benchmark-1key-list-10-elements-lrange-all-elements               | 252518 +- 0.2%  (3 datapoints)           | 254024 +- nan%  (1 datapoints)                                 |0.6%                    |-- no change --     |
|memtier_benchmark-1key-list-100-elements-lrange-all-elements              | 155628 +- 0.4%  (3 datapoints)           | 155317 +- nan%  (1 datapoints)                                 |-0.2%                   |-- no change --     |
|memtier_benchmark-1key-list-1K-elements-lrange-all-elements               | 28348 +- 0.2%  (3 datapoints)            | 28447 +- nan%  (1 datapoints)                                  |0.4%                    |-- no change --     |
|memtier_benchmark-1key-set-10-elements-smembers                           | 208332 +- 0.8%  (3 datapoints)           | 205808 +- nan%  (1 datapoints)                                 |-1.2%                   |-- no change --     |
|memtier_benchmark-1key-set-10-elements-smembers-pipeline-10               | 700134 +- 3.7%  (3 datapoints)           | 716401 +- nan%  (1 datapoints)                                 |2.3%                    |-- no change --     |
|memtier_benchmark-1key-set-10-elements-smismember                         | 267170 +- 0.4%  (3 datapoints)           | 271167 +- nan%  (1 datapoints)                                 |1.5%                    |-- no change --     |
|memtier_benchmark-1key-set-100-elements-smembers                          | 120970 +- 0.1%  (3 datapoints)           | 122871 +- nan%  (1 datapoints)                                 |1.6%                    |-- no change --     |
|memtier_benchmark-1key-set-100-elements-smismember                        | 239807 +- 0.6%  (3 datapoints)           | 242402 +- nan%  (1 datapoints)                                 |1.1%                    |-- no change --     |
|memtier_benchmark-1key-set-1K-elements-smembers                           | 20122 +- 0.5%  (3 datapoints)            | 19573 +- nan%  (1 datapoints)                                  |-2.7%                   |-- no change --     |
|memtier_benchmark-1key-zset-10-elements-zrange-all-elements               | 124358 +- 0.2%  (3 datapoints)           | 124843 +- nan%  (1 datapoints)                                 |0.4%                    |-- no change --     |
|memtier_benchmark-1key-zset-100-elements-zrange-all-elements              | 29350 +- 0.1%  (3 datapoints)            | 29749 +- nan%  (1 datapoints)                                  |1.4%                    |-- no change --     |
|memtier_benchmark-1key-zset-1K-elements-zrange-all-elements               | 5396 +- 0.7%  (3 datapoints)             | 5384 +- nan%  (1 datapoints)                                   |-0.2%                   |-- no change --     |
|memtier_benchmark-1key-zset-1M-elements-zrevrange-5-elements              | 257408 +- 1.3%  (3 datapoints)           | 261668 +- nan%  (1 datapoints)                                 |1.7%                    |-- no change --     |
|memtier_benchmark-2keys-set-10-100-elements-sdiff                         | 27577 +- 0.3%  (3 datapoints)            | 27512 +- nan%  (1 datapoints)                                  |-0.2%                   |-- no change --     |
|memtier_benchmark-2keys-set-10-100-elements-sinter                        | 134830 +- 1.1%  (3 datapoints)           | 134634 +- nan%  (1 datapoints)                                 |-0.1%                   |-- no change --     |
|memtier_benchmark-2keys-set-10-100-elements-sunion                        | 26448 +- 0.2%  (3 datapoints)            | 26523 +- nan%  (1 datapoints)                                  |0.3%                    |-- no change --     |
|memtier_benchmark-2keys-stream-5-entries-xread-all-entries                | 118346 +- 0.1%  (3 datapoints)           | 118037 +- nan%  (1 datapoints)                                 |-0.3%                   |-- no change --     |
|memtier_benchmark-2keys-stream-5-entries-xread-all-entries-pipeline-10    | 189946 +- 0.8%  (3 datapoints)           | 192854 +- nan%  (1 datapoints)                                 |1.5%                    |-- no change --     |
"
1380712388,11595,filipecosta90,2023-01-12T16:50:59Z,"@oranagra and @zuiderkwast the above report was generated with the reporting tool. 
Even tough it seems we have stable numbers there are 2 variations on large hashes.



Test Case | Baseline unstable (median obs. +- std.dev) | Comparison zuiderkwast:key-as-dictEntry (median obs. +- std.dev) | % change (higher-better) | Note
-- | -- | -- | -- | --
memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values | 136116 +- 0.3%  (3 datapoints) | 131817 +- nan%  (1 datapoints) | -3.2% | potential REGRESSION
memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values | 136116 +- 0.3%  (3 datapoints) | 131817 +- nan%  (1 datapoints) | -3.2% | potential REGRESSION

Notice the use machine is on Intel Lab and fully devoted to this. There should be small error on the numbers -- notice the variance of data of same version is minimal. 

@zuiderkwast please use biredis as platform to look for more stable data when going over the dashboard: https://benchmarksredisio.grafana.net/d/1fWbtb7nz/experimental-oss-spec-benchmarks?orgId=1&var-platforms=intel64-ubuntu20.04-biredis&var-branch=unstable&var-branch=zuiderkwast:key-as-dictEntry&var-branch=refs%2Fheads%2Funstable&var-version=All&var-test_suite=All&var-build_variants=gcc:8.5.0-amd64-debian-buster-default&var-platform_description=

"
1382418419,11595,zuiderkwast,2023-01-13T21:34:45Z,"> looks like there some assertion in the module tests with 32bit build, PTAL

I don't understand why we got assertion on entryHasValue in the 32-bit build https://github.com/redis/redis/actions/runs/3903508169/jobs/6667972903. That dict has values so all entries in it should have values. I've added another assert after zmalloc to check that we get 8 bytes alignment. Maybe bad alignment why it was accidentally tagged wrong in the LSB bits. I have no other ideas why it could happen."
1382715854,11595,oranagra,2023-01-14T11:12:17Z,"> > looks like there some assertion in the module tests with 32bit build, PTAL
> 
> I don't understand why we got assertion on entryHasValue in the 32-bit build https://github.com/redis/redis/actions/runs/3903508169/jobs/6667972903. That dict has values so all entries in it should have values. I've added another assert after zmalloc to check that we get 8 bytes alignment. Maybe bad alignment why it was accidentally tagged wrong in the LSB bits. I have no other ideas why it could happen.

maybe for small allocations it can do that?
i.e. allocations of less than 8 bytes obviously don't have to be 8 byte aligned. i suppose that's true for allocations of 12 bytes?
although that would be odd that the only test that fails on this is a module config test (specifically the ones in which dictReplace ends up doing a replacement)

note that in this case it's 32bit jemalloc, and we don't seem to have any coverage for 32it glibc malloc.
maybe run some specific tests on 32 bit malloc (both jemalloc and glibc) to learn more what it does for a few allocation sizes (4 and 12)?

i see this is in dictReplace when trying to free the previous value, which is strange since i see the same assertion after allocating the entry in dictInsertIntoBucket doesn't fail."
1386926036,11595,oranagra,2023-01-18T11:47:59Z,"> i'm not sure what that benchmark does and if it's sufficient. maybe we have to do some benchmarks manually.
> 
> * impact on string keys
> * impact on hashs (large ones that are dict encoded)
> * impact on sets (large ones that are dict encoded)
> * would be nice to see both a case of a relatively constant set, vs one that keeps growing and growing.

@filipecosta90 i assume the benchmarks you shared cover the first 3 bullets (right?), what about the last one?
@zuiderkwast maybe for that last one we need to run some local / manual benchmark? "
1387096085,11595,oranagra,2023-01-18T13:43:34Z,"> > > looks like there some assertion in the module tests with 32bit build, PTAL
> > 
> > 
> > I don't understand why we got assertion on entryHasValue in the 32-bit build https://github.com/redis/redis/actions/runs/3903508169/jobs/6667972903. That dict has values so all entries in it should have values. I've added another assert after zmalloc to check that we get 8 bytes alignment. Maybe bad alignment why it was accidentally tagged wrong in the LSB bits. I have no other ideas why it could happen.
> 
> maybe for small allocations it can do that? i.e. allocations of less than 8 bytes obviously don't have to be 8 byte aligned. i suppose that's true for allocations of 12 bytes? although that would be odd that the only test that fails on this is a module config test (specifically the ones in which dictReplace ends up doing a replacement)
> 
> note that in this case it's 32bit jemalloc, and we don't seem to have any coverage for 32it glibc malloc. maybe run some specific tests on 32 bit malloc (both jemalloc and glibc) to learn more what it does for a few allocation sizes (4 and 12)?
> 
> i see this is in dictReplace when trying to free the previous value, which is strange since i see the same assertion after allocating the entry in dictInsertIntoBucket doesn't fail.

i had some time so i looked into it.
first, i verified that both glibc and jemalloc, in a 32bit build, always return addresses that are 8 bytes aligned (even for small allocations).

secondly, i found the problem:
```c
dictFreeVal(d, &auxentry);
```

we call dictFreeVal on a stack allocated dictEntry (not a heap one).
i suppose that since dictEntry is opaque, this is the only place we're at risk of doing such a thing. "
1396233006,11595,zuiderkwast,2023-01-18T23:35:36Z,"## Benchmark

    redis-server --save ''

Warmup: Populate with random elements in the range 0..200k. The set soon reaches a more or less constant size near 200k.

    redis-benchmark -P 10 --threads 2 -n 10000000 -r 200000 sadd myset __rand_int__

Benchmark 1: A relatively constant set. Run the warmup again. Most elements already exist.

    redis-benchmark -P 10 --threads 2 -n 10000000 -r 200000 sadd myset __rand_int__

Benchmark 2: A set that keeps growing and growing.

    redis-cli flushdb
    redis-benchmark -P 10 --threads 2 -n 10000000 -r 100000000 sadd myset __rand_int__

### key-as-dictEntry (This PR)

1.

```
Summary:
  throughput summary: 1109631.62 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.408     0.088     0.399     0.551     0.695     1.319
```

2.

```
Summary:
  throughput summary: 887862.94 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.509     0.096     0.503     0.695     0.895     3.303
```

### Unstable

1.

```
Summary:
  throughput summary: 1051414.12 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.421     0.080     0.415     0.575     0.679     1.631
```

2.

```
Summary:
  throughput summary: 784006.25 requests per second
  latency summary (msec):
          avg       min       p50       p95       p99       max
        0.578     0.096     0.567     0.847     1.191     3.767
```

### Compare results

Thoughput

|   | This PR    | Unstable   | Diff        |
|---|------------|------------|-------------|
| 1 | 1109631.62 | 1051414.12 | +5.54%      |
| 2 | 887862.94  | 784006.25  | +13.2%      |

Latency p50

|   | This PR    | Unstable   | Diff        |
|---|------------|------------|-------------|
| 1 | 0.399      | 0.415      | -3.85%      |
| 2 | 0.503      | 0.567      | -11.3%      |
"
1396577966,11595,oranagra,2023-01-19T08:01:48Z,"great (maybe mention it in the top comment), can you have a look at the possible regressions Filipe posted, see if you can reproduce them or dismiss them.
anything else missing before we can merge this?"
1396698804,11595,filipecosta90,2023-01-19T09:47:27Z,"> great (maybe mention it in the top comment), can you have a look at the possible regressions Filipe posted, see if you can reproduce them or dismiss them. anything else missing before we can merge this?

@zuiderkwast and @oranagra will these as 2 new tests to the SPEC :)"
1396897280,11595,oranagra,2023-01-19T12:23:00Z,"so the only thing that left is the two possible regressions, right?
it could be due to some assertions added, maybe we can drop them.."
1397096758,11595,zuiderkwast,2023-01-19T14:51:07Z,"I tried memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values locally. I got the exact same numbers for p50 latency and 1% regression in throughput.

<details>
<summary>memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values</summary>

`memtier_benchmark --test-time 180 ""--pipeline"" ""10"" ""--data-size"" ""1000"" --command ""HSET __key__ field1 __data__ field2 __data__ field3 __data__ field4 __data__ field5 __data__"" --command-key-pattern=""P"" --key-minimum=1 --key-maximum 1000000 -c 50 -t 4 --hide-histogram`

The command was taken from [this yaml file](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10.yml).


Unstable:

```
ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Hsets      169820.42        11.77444        12.15900        18.94300        23.80700    853229.76 
Totals     169820.42        11.77444        12.15900        18.94300        23.80700    853229.76 
```

This PR:

```
ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Hsets      168060.35        11.89821        12.15900        18.81500        23.42300    844386.56 
Totals     168060.35        11.89821        12.15900        18.81500        23.42300    844386.56 
```
</details>

Looking at the graphs on the grafana site (screenshot below), it seems this branch (the short brown line with fewer data points) follow the same pattern as unstable (the longer yellow line).

![image](https://user-images.githubusercontent.com/273886/213472569-23d0282b-6160-49fb-8e45-22233ae3f6de.png)

Thus, I would guess it's not a real regression, just coincidence. WDYT?
"
1397134402,11595,oranagra,2023-01-19T15:16:23Z,"@filipecosta90 could that have been some environmental issue?
in any case, if we can't reproduce it locally, i'm guessing it's nothing.
anything else?"
1397141793,11595,zuiderkwast,2023-01-19T15:20:41Z,"@oranagra do you want to start the daily again just to be sure it's stable? Other than that, I believe it's ready to merge.

If you have a lot of time, perhaps you want to find the problem in the dict-no-next branch too? :-) I have rebased it on top of this branch' last changes. When that branch is working well, it can bring a lot of benefit since it saves memory in all dicts."
1397187277,11595,oranagra,2023-01-19T15:44:44Z,fully CI https://github.com/redis/redis/actions/runs/3960045983
1398330389,11595,filipecosta90,2023-01-20T12:45:38Z,"> @filipecosta90 could that have been some environmental issue? in any case, if we can't reproduce it locally, i'm guessing it's nothing. anything else?

@oranagra and @zuiderkwast confirmed that locally ( on a stable cascade lake system there is no variance )
```
unstable b4123663c31aaf1e97f4dbd630cbd7b8d0e91e31
4         Threads
50        Connections per thread
180       Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Hsets      165266.85        12.10056        12.35100        17.79100        22.65500    830349.23 
Totals     165266.85        12.10056        12.35100        17.79100        22.65500    830349.23 
Json file closed.

zuiderkwast/key-as-dictEntry

4         Threads
50        Connections per thread
180       Seconds


ALL STATS
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Hsets      165975.12        12.04894        12.28700        17.66300        21.63100    833907.75 
Totals     165975.12        12.04894        12.28700        17.66300        21.63100    833907.75 
```

sorry for the delay on this but wanted to check manually. All set performance wise :)
PS: as stated, we're adding 2 extra benchmarks to track SET ingestion over time to https://github.com/redis/redis-benchmarks-specification/pull/198"
63564549,2143,badboy,2014-11-18T23:19:03Z,"Sir, this is one hell of a Pull Request. I will definitely read the code and test it tomorrow while flying 35000ft above the ground.

![respect](http://media.giphy.com/media/yCAoGdVUCW5LW/giphy.gif)
"
63770677,2143,badboy,2014-11-20T07:30:23Z,"Impressive work. I read the code all in once and it was quite clear and concise. I saw no immediate bugs. Also great you have extensive test code in there (nearly half the new code is tests).

I'm sure we can get this into unstable soon. :airplane:
"
63849239,2143,mattsta,2014-11-20T17:49:00Z,"Thanks for the review!  I incorporated your fixes (plus a few more cleanups) into the existing commit.  The diff is below since it's difficult to see changes between rebased forced pushes.

I don't see any warnings against clang or gcc-4.9.2 anymore, but feel free to try more compilers and tests.  The more problems we find now the fewer cleanup commits we have to apply later.  :)

``` diff
matt@ununoctium:~/repos/redis/src% git diff
diff --git a/src/quicklist.c b/src/quicklist.c
index bb29121..081296d 100644
--- a/src/quicklist.c
+++ b/src/quicklist.c
@@ -28,12 +28,15 @@
  * POSSIBILITY OF SUCH DAMAGE.
  */

-#include <stdlib.h>
-#include <stdio.h>  /* for snprintf */
 #include <string.h> /* for memcpy */
 #include ""quicklist.h""
 #include ""zmalloc.h""
 #include ""ziplist.h""
+#include ""util.h"" /* for ll2string */
+
+#if defined(REDIS_TEST) || defined(REDIS_TEST_VERBOSE)
+#include <stdio.h> /* for printf (debug printing), snprintf (genstr) */
+#endif

 /* If not verbose testing, remove all debug printing. */
 #ifndef REDIS_TEST_VERBOSE
@@ -323,15 +326,14 @@ static quicklistNode *_quicklistZiplistMerge(quicklist *quicklist,
       target->count);

     int where;
-
     unsigned char *p = NULL;
     if (target == a) {
         /* If target is node a, we append node b to node a, in-order */
         where = ZIPLIST_TAIL;
         p = ziplistIndex(b->zl, 0);
         D(""WILL TRAVERSE B WITH LENGTH: %u, %u"", b->count, ziplistLen(b->zl));
-    } else if (target == b) {
-        /* If target b, we pre-pend node a to node b, in reverse order of a */
+    } else {
+        /* If target b, we prepend node a to node b, in reverse order of a */
         where = ZIPLIST_HEAD;
         p = ziplistIndex(a->zl, -1);
         D(""WILL TRAVERSE A WITH LENGTH: %u, %u"", a->count, ziplistLen(a->zl));
@@ -347,7 +349,7 @@ static quicklistNode *_quicklistZiplistMerge(quicklist *quicklist,
      * complex than using the existing ziplist API to read/push as below. */
     while (ziplistGet(p, &val, &sz, &longval)) {
         if (!val) {
-            sz = snprintf(lv, sizeof(lv), ""%lld"", longval);
+            sz = ll2string(lv, sizeof(lv), longval);
             val = (unsigned char *)lv;
         }
         target->zl = ziplistPush(target->zl, val, sz, where);
@@ -747,7 +749,7 @@ int quicklistNext(quicklistIter *iter, quicklistEntry *entry) {
         return 0;
     }

-    unsigned char *(*nextFn)(unsigned char *, unsigned char*) = NULL;
+    unsigned char *(*nextFn)(unsigned char *, unsigned char *) = NULL;
     int offset_update = 0;

     if (!iter->zi) {
@@ -842,7 +844,7 @@ int quicklistIndex(const quicklist *quicklist, const long long idx,
     quicklistNode *n;
     unsigned long long accum = 0;
     unsigned long long index;
-    int forward = idx < 0 ? 0 : 1; /* < 0 -> reverse, positive -> forward */
+    int forward = idx < 0 ? 0 : 1; /* < 0 -> reverse, 0+ -> forward */

     initEntry(entry);
     entry->quicklist = quicklist;
@@ -862,7 +864,8 @@ int quicklistIndex(const quicklist *quicklist, const long long idx,
         if ((accum + n->count) > index) {
             break;
         } else {
-            D(""Skipping over (%p) %u at accum %lld"", n, n->count, accum);
+            D(""Skipping over (%p) %u at accum %lld"", (void *)n, n->count,
+              accum);
             accum += n->count;
             n = forward ? n->next : n->prev;
         }
@@ -871,8 +874,8 @@ int quicklistIndex(const quicklist *quicklist, const long long idx,
     if (!n)
         return 0;

-    D(""Found node: %p at accum %llu, idx %llu, sub+ %llu, sub- %llu"", n, accum,
-      index, index - accum, (-index) - 1 + accum);
+    D(""Found node: %p at accum %llu, idx %llu, sub+ %llu, sub- %llu"", (void *)n,
+      accum, index, index - accum, (-index) - 1 + accum);

     entry->node = n;
     if (forward) {
@@ -906,7 +909,7 @@ void quicklistRotate(quicklist *quicklist, const size_t fill) {
     /* If value found is NULL, then ziplistGet populated longval instead */
     if (!value) {
         /* Write the longval as a string so we can re-add it */
-        int wrote = snprintf(longstr, sizeof(longstr), ""%lld"", longval);
+        int wrote = ll2string(longstr, sizeof(longstr), longval);
         value = (unsigned char *)longval;
         sz = wrote;
     }
@@ -1017,15 +1020,16 @@ void quicklistPush(quicklist *quicklist, const size_t fill, void *value,

 /* The rest of this file is test cases and test helpers. */
 #ifdef REDIS_TEST
-#include <stdio.h>
 #include <stdint.h>

 #define assert(_e)                                                             \
-    ((_e) ? (void)0 : (_assert(#_e, __FILE__, __LINE__), exit(1)))
-static void _assert(char *estr, char *file, int line) {
-    printf(""\n\n=== ASSERTION FAILED ===\n"");
-    printf(""==> %s:%d '%s' is not true\n"", file, line, estr);
-}
+    do {                                                                       \
+        if (!(_e)) {                                                           \
+            printf(""\n\n=== ASSERTION FAILED ===\n"");                          \
+            printf(""==> %s:%d '%s' is not true\n"", __FILE__, __LINE__, #_e);   \
+            err++;                                                             \
+        }                                                                      \
+    } while (0)

 #define yell(str, ...) printf(""ERROR! "" str ""\n\n"", __VA_ARGS__)

@@ -1718,7 +1722,7 @@ int quicklistTest(int argc, char *argv[]) {
         long long nums[5000];
         for (int i = 0; i < 5000; i++) {
             nums[i] = -5157318210846258176 + i;
-            int sz = snprintf(num, sizeof(num), ""%lld"", nums[i]);
+            int sz = ll2string(num, sizeof(num), nums[i]);
             quicklistPushTail(ql, F, num, sz);
         }
         quicklistPushTail(ql, F, ""xxxxxxxxxxxxxxxxxxxx"", 20);
@@ -1876,7 +1880,7 @@ int quicklistTest(int argc, char *argv[]) {
             long long nums[5000];
             for (int i = 0; i < 760; i++) {
                 nums[i] = -5157318210846258176 + i;
-                int sz = snprintf(num, sizeof(num), ""%lld"", nums[i]);
+                int sz = ll2string(num, sizeof(num), nums[i]);
                 quicklistPushTail(ql, f, num, sz);
             }

@@ -1901,7 +1905,7 @@ int quicklistTest(int argc, char *argv[]) {
             long long nums[5000];
             for (int i = 0; i < 32; i++) {
                 nums[i] = -5157318210846258176 + i;
-                int sz = snprintf(num, sizeof(num), ""%lld"", nums[i]);
+                int sz = ll2string(num, sizeof(num), nums[i]);
                 quicklistPushTail(ql, f, num, sz);
             }
             if (f == 32)
@@ -1929,7 +1933,7 @@ int quicklistTest(int argc, char *argv[]) {
             long long nums[5000];
             for (int i = 0; i < 33; i++) {
                 nums[i] = i;
-                int sz = snprintf(num, sizeof(num), ""%lld"", nums[i]);
+                int sz = ll2string(num, sizeof(num), nums[i]);
                 quicklistPushTail(ql, f, num, sz);
             }
             if (f == 32)
@@ -1973,7 +1977,7 @@ int quicklistTest(int argc, char *argv[]) {
             long long nums[5000];
             for (int i = 0; i < 33; i++) {
                 nums[i] = -5157318210846258176 + i;
-                int sz = snprintf(num, sizeof(num), ""%lld"", nums[i]);
+                int sz = ll2string(num, sizeof(num), nums[i]);
                 quicklistPushTail(ql, f, num, sz);
             }
             if (f == 32)
@@ -1999,7 +2003,7 @@ int quicklistTest(int argc, char *argv[]) {
             long long nums[5000];
             for (int i = 0; i < 33; i++) {
                 nums[i] = -5157318210846258176 + i;
-                int sz = snprintf(num, sizeof(num), ""%lld"", nums[i]);
+                int sz = ll2string(num, sizeof(num), nums[i]);
                 quicklistPushTail(ql, f, num, sz);
             }
             if (f == 32)
```
"
64084254,2143,mattsta,2014-11-22T15:44:32Z,"Updated with below diff thanks to sunheehnus at https://github.com/antirez/redis/pull/2143#discussion_r20757598!

The fix also caught an invalid check in a test case.  Now that we're merging quicklist nodes more properly, we use fewer nodes in a test (26 -> 25) and that's a good thing.

``` diff
diff --git a/src/quicklist.c b/src/quicklist.c
index dbb2dae..c27827b 100644
--- a/src/quicklist.c
+++ b/src/quicklist.c
@@ -380,6 +380,9 @@ static void _quicklistMergeNodes(quicklist *quicklist, const size_t fill,
     if (center->prev && (center->count + center->prev->count) <= fill) {
         target = _quicklistZiplistMerge(quicklist, center->prev, center);
         center = NULL; /* center could have been deleted, invalidate it. */
+    } else {
+        /* If can't merge here, target still needs to be valid for below. */
+        target = center;
     }

     /* Use result of center merge to try and merge result with next node. */
@@ -1467,7 +1470,7 @@ int quicklistTest(int argc, char *argv[]) {
                 quicklistInsertBefore(ql, f, &entry, genstr(""abc"", i), 32);
             }
             if (f == 32)
-                ql_verify(ql, 26, 750, 32, 20);
+                ql_verify(ql, 25, 750, 32, 20);
             quicklistRelease(ql);
         }
     }
```
"
64102223,2143,antirez,2014-11-23T01:12:56Z,"Love the feature and that there are even reviews here. Thank you. Tomorrow I'll be back in Sicily from Bracelona, and Monday I'll review the code as well. 
"
64106518,2143,mattsta,2014-11-23T04:34:58Z,"Updated:

``` diff
diff --git a/src/quicklist.c b/src/quicklist.c
index 664a6c6..7a2490c 100644
--- a/src/quicklist.c
+++ b/src/quicklist.c
@@ -393,10 +393,22 @@ static void _quicklistMergeNodes(quicklist *quicklist, const size_t fill,
     }
 }

-/* Split 'node' at 'offset' into two parts.
+/* Split 'node' into two parts, parameterized by 'offset' and 'after'.
+ *
+ * The 'after' argument controls which quicklistNode gets returned.
+ * If 'after'==1, returned node has elements after 'offset'.
+ *                input node keeps elements up to 'offset', including 'offset'.
+ * If 'after'==0, returned node has elements up to 'offset', including 'offset'.
+ *                input node keeps elements after 'offset'.
+ *
+ * If 'after'==1, returned node will have elements _after_ 'offset'.
+ *                The returned node will have elements [OFFSET+1, END].
+ *                The input node keeps elements [0, OFFSET].
+ *
+ * If 'after'==0, returned node will keep elements up to and including 'offset'.
+ *                The returned node will have elements [0, OFFSET].
+ *                The input node keeps elements [OFFSET+1, END].
  *
- * If after==1, then the returned node has elements [OFFSET, END].
- * Otherwise if after==0, then the new node has [0, OFFSET)
  * The input node keeps all elements not taken by the returned node.
  *
  * Returns newly created node or NULL if split not possible. */
@@ -409,8 +421,10 @@ static quicklistNode *_quicklistSplitNode(quicklistNode *node, int offset,
         return NULL;

     new_node->zl = zmalloc(zl_sz);
-    if (!new_node->zl)
+    if (!new_node->zl) {
+        zfree(new_node);
         return NULL;
+    }

     /* Copy original ziplist so we can split it */
     memcpy(new_node->zl, node->zl, zl_sz);
```

Thanks for the ongoing reviews!
"
64198291,2143,sunheehnus,2014-11-24T14:10:18Z,"Finish my review.
You did impressive work. Learn a lot from you. :) :+1:  :+1:  :+1:  
"
64204019,2143,badboy,2014-11-24T14:52:15Z,"Wow, @sunheehnus, your work is great as well. 
"
64204246,2143,antirez,2014-11-24T14:53:51Z,"Thank you, after Matt changes the code according to the reviews I'll do mine as well... waiting because there is already too many good things ongoing here :-)
"
64265674,2143,mattsta,2014-11-24T21:14:09Z,"Update: `3 files changed, 124 insertions(+), 43 deletions(-)`

New changes:
- Imported ziplists now get appended element-by-element instead of just added as a single block of elements (see `quicklistAppendValuesFromZiplist()` and wrapper for it `quicklistCreateFromZiplist()`).
- Fixed incorrect iterator updating during a delete; turned that section of code inside out
- Incorporated suggested fixes from https://github.com/mattsta/redis/pull/5
  - delete range fixes when given crazy arguments + tests for the crazy arguments
  - use proper string value instead of casting a long long to a char\* incorrectly
  - during rotate, update ziplist index pointer during when appropriate
- added tests to trigger some previously untraversed code paths (see the number pushes in test `rotate 500 val 5000` — now triggers the longval segfault and the head-ziplist-moves-itself segfault (before they were fixed)). 
- improved test cases to accumulate errors when `ql_verify` fails instead of just printing a failure message in the middle of 10,000 line test output.
- changed RDB ziplist import to use `quicklistCreateFromZiplist()`

``` diff
diff --git a/src/quicklist.c b/src/quicklist.c
index 7a2490c..6d89927 100644
--- a/src/quicklist.c
+++ b/src/quicklist.c
@@ -194,18 +194,39 @@ quicklist *quicklistPushTail(quicklist *quicklist, const size_t fill,
     return quicklist;
 }

-/* Create new node consisting of an entire pre-formed ziplist.
- * Used for loading old RDBs where entire ziplists have been stored
- * to be restored later. */
-void quicklistPushTailZiplist(quicklist *quicklist, unsigned char *zl) {
-    unsigned int sz = ziplistLen(zl);
-
-    quicklistNode *node = quicklistCreateNode();
-    _quicklistInsertNodeAfter(quicklist, quicklist->tail, node);
-
-    node->zl = zl;
-    node->count = sz;
-    quicklist->count += sz;
+/* Append all values of ziplist 'zl' individually into 'quicklist'.
+ *
+ * This allows us to restore old RDB ziplists into new quicklists
+ * with smaller ziplist sizes than the saved RDB ziplist.
+ *
+ * Returns 'quicklist' argument. Frees passed-in ziplist 'zl' */
+quicklist *quicklistAppendValuesFromZiplist(quicklist *quicklist,
+                                            const size_t fill,
+                                            unsigned char *zl) {
+    unsigned char *value;
+    unsigned int sz;
+    long long longval;
+    char longstr[32] = { 0 };
+
+    unsigned char *p = ziplistIndex(zl, 0);
+    while (ziplistGet(p, &value, &sz, &longval)) {
+        if (!value) {
+            /* Write the longval as a string so we can re-add it */
+            sz = ll2string(longstr, sizeof(longstr), longval);
+            value = (unsigned char *)longstr;
+        }
+        quicklistPushTail(quicklist, fill, value, sz);
+        p = ziplistNext(zl, p);
+    }
+    zfree(zl);
+    return quicklist;
+}
+
+/* Create new (potentially multi-node) quicklist from a single existing ziplist.
+ *
+ * Returns new quicklist.  Frees passed-in ziplist 'zl'. */
+quicklist *quicklistCreateFromZiplist(size_t fill, unsigned char *zl) {
+    return quicklistAppendValuesFromZiplist(quicklistCreate(), fill, zl);
 }

 #define quicklistDeleteIfEmpty(ql, n)                                          \
@@ -265,27 +286,24 @@ void quicklistDelEntry(quicklistIter *iter, quicklistEntry *entry) {
     /* after delete, the zi is now invalid for any future usage. */
     iter->zi = NULL;

-    if (iter->direction == AL_START_HEAD) {
-        if (deleted_node) {
-            /* Current node was deleted.  Assign saved next to current. */
-            /* Also re-init zi to the first position in new current. */
+    /* If current node is deleted, we must update iterator node and offset. */
+    if (deleted_node) {
+        if (iter->direction == AL_START_HEAD) {
             iter->current = next;
             iter->offset = 0;
-        } else {
-            /* Current node remains.  Replace iterator zi with next zi. */
-            iter->zi = entry->zi;
-            iter->offset++;
-        }
-    } else if (iter->direction == AL_START_TAIL) {
-        if (deleted_node) {
-            /* Current node was deleted.  Assign saved prev to current. */
-            /* Also re-init zi to the last position in new current. */
+        } else if (iter->direction == AL_START_TAIL) {
             iter->current = prev;
             iter->offset = -1;
-        } else {
-            /* Current node still exists. */
         }
     }
+    /* else if (!deleted_node), no changes needed.
+     * we already reset iter->zi above, and the existing iter->offset
+     * doesn't move again because:
+     *   - [1, 2, 3] => delete offset 1 => [1, 3]: next element still offset 1
+     *   - [1, 2, 3] => delete offset 0 => [2, 3]: next element still offset 0
+     *  if we deleted the last element at offet N and now
+     *  length of this ziplist is N-1, the next call into
+     *  quicklistNext() will jump to the next node. */
 }

 /* Replace quicklist entry at offset 'index' by 'data' with length 'sz'.
@@ -573,8 +591,8 @@ int quicklistDelRange(quicklist *quicklist, const long start,

     if (start >= 0 && extent > (quicklist->count - start)) {
         /* if requesting delete more elements than exist, limit to list size. */
-        extent = quicklist->count;
-    } else if (start < 0 && extent > (quicklist->count - (-start) + 1)) {
+        extent = quicklist->count - start;
+    } else if (start < 0 && extent > (unsigned long)(-start)) {
         /* else, if at negative offset, limit max size to rest of list. */
         extent = -start; /* c.f. LREM -29 29; just delete until end. */
     }
@@ -598,7 +616,7 @@ int quicklistDelRange(quicklist *quicklist, const long start,
              * can just delete the entire node without ziplist math. */
             delete_entire_node = 1;
             del = node->count;
-        } else if (entry.offset >= 0 && extent > node->count) {
+        } else if (entry.offset >= 0 && extent >= node->count) {
             /* If deleting more nodes after this one, calculate delete based
              * on size of current node. */
             del = node->count - entry.offset;
@@ -890,14 +908,19 @@ void quicklistRotate(quicklist *quicklist, const size_t fill) {
     /* If value found is NULL, then ziplistGet populated longval instead */
     if (!value) {
         /* Write the longval as a string so we can re-add it */
-        int wrote = ll2string(longstr, sizeof(longstr), longval);
-        value = (unsigned char *)longval;
-        sz = wrote;
+        sz = ll2string(longstr, sizeof(longstr), longval);
+        value = (unsigned char *)longstr;
     }

     /* Add tail entry to head (must happen before tail is deleted). */
     quicklistPushHead(quicklist, fill, value, sz);

+    /* If quicklist has only one node, the head ziplist is also the
+     * tail ziplist and PushHead() could have reallocated our single ziplist,
+     * which would make our pre-existing 'p' unusable. */
+    if (quicklist->len == 1)
+        p = ziplistIndex(tail->zl, -1);
+
     /* Remove tail entry. */
     quicklistDelIndex(quicklist, tail, &p);
 }
@@ -1083,8 +1106,13 @@ static int itrprintr_rev(quicklist *ql, int print) {
     return _itrprintr(ql, print, 0);
 }

+#define ql_verify(a, b, c, d, e)                                               \
+    do {                                                                       \
+        err += _ql_verify((a), (b), (c), (d), (e));                            \
+    } while (0)
+
 /* Verify list metadata matches physical list contents. */
-static void ql_verify(quicklist *ql, uint32_t len, uint32_t count,
+static int _ql_verify(quicklist *ql, uint32_t len, uint32_t count,
                       uint32_t head_count, uint32_t tail_count) {
     int ok = 1;

@@ -1117,7 +1145,7 @@ static void ql_verify(quicklist *ql, uint32_t len, uint32_t count,

     if (ql->len == 0 && ok) {
         OK;
-        return;
+        return !ok;
     }

     if (head_count != ql->head->count &&
@@ -1137,6 +1165,7 @@ static void ql_verify(quicklist *ql, uint32_t len, uint32_t count,
     }
     if (ok)
         OK;
+    return !ok;
 }

 /* Generate new string concatenating integer i against string 'prefix' */
@@ -1150,7 +1179,7 @@ static char *genstr(char *prefix, int i) {
 #define F 32
 /* main test, but callable from other files */
 int quicklistTest(int argc, char *argv[]) {
-    int err = 0;
+    unsigned int err = 0;

     UNUSED(argc);
     UNUSED(argv);
@@ -1249,6 +1278,10 @@ int quicklistTest(int argc, char *argv[]) {
     for (size_t f = 0; f < 1024; f++) {
         TEST_DESC(""rotate 500 val 5000 times at fill %lu"", f) {
             quicklist *ql = quicklistCreate();
+            quicklistPushHead(ql, f, ""900"", 3);
+            quicklistPushHead(ql, f, ""7000"", 4);
+            quicklistPushHead(ql, f, ""-1200"", 5);
+            quicklistPushHead(ql, f, ""42"", 2);
             for (int i = 0; i < 500; i++)
                 quicklistPushHead(ql, f, genstr(""hello"", i), 32);
             ql_info(ql);
@@ -1256,8 +1289,12 @@ int quicklistTest(int argc, char *argv[]) {
                 ql_info(ql);
                 quicklistRotate(ql, f);
             }
-            if (f == 32)
-                ql_verify(ql, 16, 500, 28, 24);
+            if (f == 1)
+                ql_verify(ql, 504, 504, 1, 1);
+            else if (f == 2)
+                ql_verify(ql, 252, 504, 2, 2);
+            else if (f == 32)
+                ql_verify(ql, 16, 504, 32, 24);
             quicklistRelease(ql);
         }
     }
@@ -1622,6 +1659,15 @@ int quicklistTest(int argc, char *argv[]) {
         quicklistRelease(ql);
     }

+    TEST(""delete range of entire node with overflow counts"") {
+        quicklist *ql = quicklistCreate();
+        for (int i = 0; i < 32; i++)
+            quicklistPushHead(ql, F, genstr(""hello"", i), 32);
+        quicklistDelRange(ql, 0, 128);
+        ql_verify(ql, 0, 0, 0, 0);
+        quicklistRelease(ql);
+    }
+
     TEST(""delete middle 100 of 500 list"") {
         quicklist *ql = quicklistCreate();
         for (int i = 0; i < 500; i++)
@@ -1640,6 +1686,15 @@ int quicklistTest(int argc, char *argv[]) {
         quicklistRelease(ql);
     }

+    TEST(""delete negative 1 from 500 list with overflow counts"") {
+        quicklist *ql = quicklistCreate();
+        for (int i = 0; i < 500; i++)
+            quicklistPushTail(ql, F, genstr(""hello"", i + 1), 32);
+        quicklistDelRange(ql, -1, 128);
+        ql_verify(ql, 16, 499, 32, 19);
+        quicklistRelease(ql);
+    }
+
     TEST(""delete negative 100 from 500 list"") {
         quicklist *ql = quicklistCreate();
         for (int i = 0; i < 500; i++)
@@ -1997,6 +2052,30 @@ int quicklistTest(int argc, char *argv[]) {
         }
     }

+    for (size_t f = 0; f < 72; f++) {
+        TEST_DESC(""create quicklist from ziplist at fill %lu"", f) {
+            unsigned char *zl = ziplistNew();
+            long long nums[32];
+            char num[32];
+            for (int i = 0; i < 33; i++) {
+                nums[i] = -5157318210846258176 + i;
+                int sz = ll2string(num, sizeof(num), nums[i]);
+                zl = ziplistPush(zl, (unsigned char *)num, sz, ZIPLIST_TAIL);
+            }
+            for (int i = 0; i < 33; i++) {
+                zl = ziplistPush(zl, (unsigned char *)genstr(""hello"", i), 32,
+                                 ZIPLIST_TAIL);
+            }
+            quicklist *ql = quicklistCreateFromZiplist(f, zl);
+            if (f == 1)
+                ql_verify(ql, 66, 66, 1, 1);
+            else if (f == 32)
+                ql_verify(ql, 3, 66, 32, 2);
+            else if (f == 66)
+                ql_verify(ql, 1, 66, 66, 66);
+            quicklistRelease(ql);
+        }
+    }
     if (!err)
         printf(""ALL TESTS PASSED!\n"");
     else
diff --git a/src/t_list.c b/src/t_list.c
index a7201fd..e740524 100644
--- a/src/t_list.c
+++ b/src/t_list.c
@@ -185,11 +185,10 @@ void listTypeConvert(robj *subject, int enc) {
     redisAssertWithInfo(NULL,subject,subject->encoding==REDIS_ENCODING_ZIPLIST);

     if (enc == REDIS_ENCODING_QUICKLIST) {
-        quicklist *ql = quicklistCreate();
-        quicklistPushTailZiplist(ql, subject->ptr);
+        size_t zlen = server.list_max_ziplist_entries;

         subject->encoding = REDIS_ENCODING_QUICKLIST;
-        subject->ptr = ql;
+        subject->ptr = quicklistCreateFromZiplist(zlen, subject->ptr);
     } else {
         redisPanic(""Unsupported list conversion"");
     }
```
"
64265858,2143,antirez,2014-11-24T21:15:40Z,"Great :-) my review starts tomorrow morning.
"
64422310,2143,antirez,2014-11-25T16:00:19Z,"Finally... Initial feedback, based on an initial review :-)
1. Great work, very appreciated: this can make a difference for many users, and may also turn the list type into a more interesting type for Redis, because of the memory benefits not easily applicable to zsets, for example, that have certain use cases overlaps with lists. Thank you.
2. Impressive amount of tests: +1.
3. t_list.c was simplified by the changes because of the single encoding in many parts, which is great.
4. We should allow list-max-ziplist-value option to be processed, just ignored.
5. The way lists are stored into RDBs now is a non trivial speed and size regression for RDBs for data sets composed of many small lists. I think we should introduce a new RDB type for quicklists where each ziplist is saved and loaded as a blob. ziplists are internally architecture agonistic so this is pretty simple to do. The result is huge speedup in loading/saving times for big lists compared to pre-quicklist Redis. I can take care of implementing that.
6. There are zmalloc() checks around the code, these can safely be removed, since the Redis memory management way is: abort on OOM since recovery is futile.
7. Remove all the “static”: no symbols on Redis crashes stack traces otherwise.
8. This is not a problem, but I noted that quicklistDelRange() is a lot smarter it needs to be for LTRIM semantics. In theory complexity could be reduced.
9. We may remove the listType\* wrappers, there is only a single type. However the current code is more adapt to change in the future. Tradeoffs… Better to take the wrappers probably. 
10. Quicklists will probably stress the allocator more than lists on long-lasting workloads that remove/add elements to lists. I bet users will complain about this soon or later. However because of the memory savings is a neat win: If you use 100MB x 1.4, it is a lot better than using 1GB x 1.02. We should just remember to ignore complains sometimes… ;-)
11. Maybe we should get rid also of the number of items parameter? From all your graphs, there is a soft spot at around ~80 items per ziplist where we have all the advantages. But also would be interesting to see the speed charts.
12. In your blog post I didn’t saw much about speed, did you tested a few workloads maybe?

Ok that’s the initial impressions. I’ll review more carefully and test the code more tomorrow to provide more feedbacks.
Since this is a lot of new code, it may be wise to merge just into unstable and wait for it to hit a stable release naturally (Redis 3.2).
Because of this, also, that’s the right moment to add the new RDB format for quicklists as well… however it is important to be able to read old RDB formats, so some code (already present in your quicklist implementation) to handle the old list formats should be retained.

More comments ASAP, I really appreciated this effort, and also thank you for the reviews of Matt’s code.
"
64661045,2143,antirez,2014-11-26T15:23:30Z,"Today I checked the implementation better, and ran a few fuzzy tests for a long time, trying to break it from different point of views. I also did some performance profiling. TLDR:
1. It does not break easily... I was not able to crash it a single time using complex access patters, inserts, deletions, popping, with many clients at the same time in long lists, with op set designed to stress merge operations.
2. It is FAAAAST. Much faster tha common lists. LRANGE is impressive but the real killer for me was `DEL`. Now DEL is proportional to the number of ziplists... so it's O(N/BIG_CONSTANT).

The only really enhancement that we need is to serialize it as ziplists, because of the stuff expressed above, but there is actually another thing that I forgot to mention, which is MIGRATE. Quicklists + RDB serialization as ziplists = moving big lists in a trivial amount of time.

Another tiny feature request is to see the actual number of ziplist nodes in ""DEBUG OBJECT"" output.

I'm really enthusiastic about this PR, it is a complex thing, well executed, with big impact.
"
64667492,2143,lovelle,2014-11-26T16:02:14Z,"Wow, if is more fast than common lists, this will be great. I'm learning a lot from all of you, very awesome. :wave:
"
65842924,2143,mattsta,2014-12-05T19:40:16Z,"Warning: big follow-up reply below!

> We should allow list-max-ziplist-value option to be processed, just ignored.

Yeah, as far as I know, there aren't any other options we accept but ignore.  We could just keep a list of options to ignore to loop over if we remove things in the future.

> I think we should introduce a new RDB type for quicklists where each ziplist is saved and loaded as a blob.

That is possible, but for loading saved ziplists, they should be re-added element-by-element instead of just restored in-place.

Re-adding ziplists element-by-element cleans up any internal node fragmentation from users inserting/deleting elements in the middle of the quicklist and breaking the internal maximum fill levels.

> There are zmalloc() checks around the code, these can safely be removed, since the Redis memory management way is: abort on OOM since recovery is futile.

Yeah, there are, but there are also zmalloc return value checks in other places in the Redis code, so I just kept the pattern alive (e.g. adlist.c).

It's written to be a generic data structure, so if people redefine zmalloc to just malloc, it should still do the right thing.

At some point I'm going to make all the Redis data structures a reusable external library.  :)

> Remove all the “static”: no symbols on Redis crashes stack traces otherwise.

That's a new one to me.  I'd prefer to leave some static signal because compilers can generate better code when they know certain functions don't leave the file, and it's made to be reusable-outside-of-Redis.  We could label functions as REDIS_STATIC then define REDIS_STATIC to nothing for Redis builds.

We get a lot of crash reports with no symbols anyway (packagers manually stripping the binary?).

> I noted that quicklistDelRange() is a lot smarter it needs to be for LTRIM semantics. In theory complexity could be reduced.

Everything is made to be generic and reusable, even if Redis doesn't need some of the operations.

If there's a shortcut to make LTRIM usage quicker, we should certainly add it.  :)

> We may remove the listType\* wrappers, there is only a single type.

Yeah, that's just a design decision.  I left them there for redundant type checking, but we could remove them completely.

> Quicklists will probably stress the allocator more than lists on long-lasting workloads that remove/add elements to lists.

Difficult to say.  I could see it happening either way.  It probably depends on usage patterns (append-only workload with heavy reads?  best memory usage).  There are more optimizations we could make to reduce fragmentation (pre-allocating ziplists up to 4k or 8k), but they may not be useful for everybody.

> Maybe we should get rid also of the number of items parameter? From all your graphs, there is a soft spot at around ~80 items per ziplist where we have all the advantages.

Removing the number of items _is_ possible now, but for another reason.  The number of _items_ is actually a really bad measurement for ziplist performance.  If we set a fixed limit of 80 items and people are inserting 5 MB images into a list, that's _really_ bad.  If people are inserting only integers, that's also bad because we could still store more integers in one ziplist without impacting performance.

Commit https://github.com/mattsta/redis/commit/dab04ed23ef0a38458da5790a0bc2474d79aabe7 adds the option to use dynamic ziplist length based on total ziplist _size_ instead of a maximum length.  Using a maximum size gives us _optimal_ pointer overhead reduction while also giving us _optimal_ speed due to ziplist reallocations.  (The slowdown in large ziplists is because of memory reallocation, so forcing ziplists to remain small gives us better bounds on ziplist performance.  Also see [the before and after graphs](https://matt.sh/redis-quicklist-adaptive#_results).)

So, we could modify the option in three ways: completely remove the option and just set a global default of 4k or 8k ziplist sizes, change the option to only allow selecting 4k, 8k, or 16k ziplist sizes, or leave it as-is where we can select ziplist sizes or ziplist length.  It just depends on if we think letting people tune their sizes has any benefit.  (We should probably rename the option too?)

Setting ziplist limits based on size instead of length _also_ protects us from users inserting large elements into a ziplist and causing the ziplist to grow too big.  Now, if someone inserts a large element, that element gets its own ziplist assigned and no other elements will insert to that ziplist.  We automatically get protected from bad performance due to big elements.

> In your blog post I didn’t saw much about speed, did you tested a few workloads maybe?

Very observant!  The first post had no performance testing.  Insert/delete performance testing is at https://matt.sh/redis-quicklist-adaptive#_results.  I haven't done performance testing of just reads yet, but those should be very close to the linked list version.

> may be wise to merge just into unstable

That certainly works!

[larger sidenote: We should probably end up using a slightly better workflow.  We currently have `unstable -> stable`, but we really need a few more steps [[this can also help fix the ""too many Redis point releases"" or ""major new features in minor version numbers"" problem if we push proper maintenance releases (2.8.18.1, 2.8.18.2, etc) versus others]].  Git workflows for public projects are solved problems we don't have to re-invent.  See things like http://git-scm.com/docs/gitworkflows#_graduation — I have an entire local 'pu' branch (not public yet, but soon) that has about 40 commits from public PRs waiting for official approval.  we have too many proposed things that nobody gets around to testing/using, and since nobody tests or uses them, they never get approved, so it's a cycle.  ""unstable"" is actually _too_ stable!  :)]

> add the new RDB format

Pluggable, key/value metadata RDB format, right?  :)

> It does not break easily... I was not able to crash it a single time using complex access patters, inserts, deletions, popping, with many clients at the same time in long lists, with op set designed to stress merge operations.

Great to hear!  It sure broke a lot while being written.  Most of the code tests run in a loop covering all the different edge cases for ziplist sizes too which helps a lot.

> It is FAAAAST. Much faster tha common lists. LRANGE is impressive but the real killer for me was DEL. Now DEL is proportional to the number of ziplists... so it's O(N/BIG_CONSTANT).

Yay!  Deleting in the middle of large lists is probably the biggest speed-up compared to linked list.

> which is MIGRATE. Quicklists + RDB serialization as ziplists = moving big lists in a trivial amount of time.

Oh, I hadn't even considered that.  It'll help a lot with large lists in clusters during slot moving.

> Another tiny feature request is to see the actual number of ziplist nodes in ""DEBUG OBJECT"" output.

Good point.  I'll look into adding it (the quicklist node count is just quicklist->len; we could also dump the individual ziplist info (byte size and element count) in DEBUG output too).

Thanks for all the notes!

So, what's remaining:
- RDB format upgrade
  - Save QL as series of ZLs
  - Restore QL by appending ZL elements (`quicklistAppendValuesFromZiplist()`)
- Add QL-specific DEBUG info
- Move existing list `list-max-ziplist-value` option to new ""ignore"" section of config parsing
- Maybe rename `list-max-ziplist-entries` (or remove it entirely and just set default ziplist size to 4k or 8k)
- Investigate if `quicklistDelRange()` can take a faster LTRIM shortcut.
- Maybe rename static functions to REDIS_STATIC and set REDIS_STATIC="""" during build
"
66252605,2143,antirez,2014-12-09T09:04:29Z,"Quick reply to move forward ASAP:
1. Ignored options: we had it in the past at some point, for a similarly popular option. Instead of having a list, given that's a single option, for now just removing the option entirely from config SET/GET and processing it in redis.conf as startup emitting a warning may do. We'll deprecate it entirely at some point in the future.
2. You write ""Re-adding ziplists element-by-element cleans up any internal node fragmentation from users inserting/deleting elements in the middle of the quicklist and breaking the internal maximum fill levels."". I don't think this is a good idea for a simple reason: if you have an issue with fragmentation while the system is running, then you have a general problem. If you don't have a general problem, then you should not care about restoring the same setup when the list is reloaded. Otherwise semantically is like if restarts are needed from time to time. Which of the two is true? :-)
3. Malloc return value check: the old checks are there because those libs were imported form old code that did not ignored the malloc return value (generic libs indeed). We may do this: remove checks inside the Redis core anyway. Retain them when possible for code implemented already in form of a quasi-standalone-library in order to make reuse simpler.
4. Symbols: stack traces in issues or emitted by running tests that I was no longer able to reproduce saved me a lot of times. The hypothetical speed gain, we don't even now if there is. Given that fixing issues with little efforts is critical to the project, removing the statics is fundamental. Not blocking to get merged since I can do it myself easily, but would appreciate if PRs follow the existing code style to avoid useless friction for useful code to get inserted.
5. About the workflow, I agree, and don't agree. Larger discussion in the ML, but my fear is that our current core team of two is so tiny that with a more sane development model may end reducing our output.

TL;DR: I want to merge this code, there are just small things left. The only blocking one is RDB format / fragmentation on load issue. If this gets fixed I'll merge it ASAP.

About RDB, if you want to act in order for this work be merged:
1. You can break the RDB format compatibility, upgrading the number. However if you start your work in this direction, please ping me, I want to break RDB format for another issue, better that we break it a single time.
2. Please make the new code backward compatible with old RDB files (but not the contrary..., not needed).
3. If you really really think the defragmentation stuff is a good idea, please follow up here and let's implement this as a single pass operation that can be performed after loading the object: often it may not be needed _at all_, or we can just have thresholds that modify nodes when they are too far from optimal sizes.
4. I can help implementing this if needed (I don't think you need any help but you may be already up to something else).

Let's work together to let this code enter the code ASAP so that users can stress test it.
"
66360123,2143,mattsta,2014-12-09T21:23:11Z,"(just replying to this one thing right now)

> You write ""Re-adding ziplists element-by-element cleans up any internal node fragmentation from users inserting/deleting elements in the middle of the quicklist and breaking the internal maximum fill levels."". I don't think this is a good idea for a simple reason: if you have an issue with fragmentation while the system is running, then you have a general problem.

Example of ziplists in a quicklist:

``` haskell
[32 elements] <-> [32 elements] <-> [32 elements] <-> [32 elements] ...
```

If someone inserts at position 40, we now have:

``` haskell
[32 elements] <-> [8 elements] <-> [25 elements] <-> [32 elements] <-> [32 elements] ...
```

The ""fragmentation"" is just the second node now has 8 elements instead of the maximum allowed.  If you re-create the quicklist element-by-element, all internal nodes will return to their maximum fill.

The only way to ""fix"" it during runtime is to always re-create the entire list after any mid-list insert or delete, which isn't reasonable.  (If multiple adjacent nodes end up below the allowed fill level, they will get merged.  It's only individual inserts or deletes that create not-maximum-fill nodes.)

So, there isn't actually a problem here.  But, if we re-add elements individually at RDB restore time, then we aren't persisting the 'fragmentation' across restarts.
"
66372340,2143,antirez,2014-12-09T22:42:09Z,"@mattsta persisting fragmentation is not a problem per se: is what happens if the system keeps running instead of being restarted, which is the optimum, so we should not try to optimized for the wrong case. Instead the difference in loading time between how it is now, and how I think it should be, is 40x (test it, it's just a bet, but I think is more than 1 order of magnitude for sure).

So what you want more, 40x faster any-size lists loading, or persisting fragmentation which is what happens anyway in the optimal case?
"
66374118,2143,mattsta,2014-12-09T22:54:37Z,"> persisting fragmentation is not a problem per se

persisting fragmentation isn't the problem — _restoring_ fragmentation is what we're trying to avoid.  :)

Restoring lists element-by-element (instead of entire ziplists at once) could be fast enough (*untested) since it's just sequential append operations.
"
66374585,2143,antirez,2014-12-09T22:58:01Z,">  restoring fragmentation is what we're trying to avoid. :)

Yep but it is what happens when the system is not restated, which is what we hope in the first instance, so I mean, an optimization that you can't control as a developer of the feature, that may not happen at all, is IMHO futile (unless it has zero drawbacks). The list fragmentation is going to be, anyway, bound fortunately. And here we have a tradeoff, an optimization you can't control, VS speed in critical moments (restarts / BGSAVEs).
"
66527543,2143,mattsta,2014-12-10T21:30:50Z,"Freshly rebased quicklist branch is pushed here.  All the tests still pass, though I did have to update one test: https://twitter.com/mattsta/status/542785402751692800

> About RDB, if you want to act in order for this work be merged:
> 1. You can break the RDB format compatibility, upgrading the number. However if you start your work in this direction, please ping me, I want to break RDB format for another issue, better that we break it a single time.

  Done!  Let me know what else you want to change with this RDB version increase (6->7).  I'll get it added to this branch.

> 1. Please make the new code backward compatible with old RDB files (but not the contrary..., not needed).

  Done!  The RDB loader can still restore old lists from linked lists or individual ziplists.

> 1. If you really really think the defragmentation stuff is a good idea, please follow up here and let's implement this as a single pass operation that can be performed after loading the object: often it may not be needed at all, or we can just have thresholds that modify nodes when they are too far from optimal sizes.

  Not done!  If we want to test it, it's a one line change.  We would just change `quicklistAppendZiplist()` to `quicklistAppendValuesFromZiplist()` in the RDB loading code.

> 1. I can help implementing this if needed (I don't think you need any help but you may be already up to something else).

  I figured it out mostly okay.  One of the worst parts was figuring out tests were failing because this thing had to be updated too (there weren't notes anywhere.  I added big all-caps notes for future editors.):

``` haskell
#define rdbIsObjectType(t) ((t >= 0 && t <= 4) || (t >= 9 && t <= 13))
```

### Remaining

I'll work on the rest of these today/tomorrow:
- Add QL-specific DEBUG info
- Ignore existing list list-max-ziplist-value option
- Rename list-max-ziplist-entries, ignore old option
- Investigate if quicklistDelRange() can take a faster LTRIM shortcut.
- remove static
  - though, all I see in the redis codebase now are 'static' declarations everywhere.  :)
  - `~/repos/redis/src% grep ^static *.c |wc -l` = `292`  :dancer: 
"
66528988,2143,antirez,2014-12-10T21:40:07Z,"Cool, thank you. This will get merged ASAP.
"
66577725,2143,mattsta,2014-12-11T06:13:56Z,"I've found a few more improvements to make, so let's hold off until attempting to merge this until next week.  I want to run a few more tests to compare performance differences too.  :)
"
67613852,2143,antirez,2014-12-19T09:06:49Z,"Ping! I'll blocking merging unstable into testing until this is ready, we can't miss this from next testing release.
"
67656244,2143,mattsta,2014-12-19T15:57:57Z,"Update imminent.  Adding a few more notes to the accompanying benchmarticle.  Will push within an hour hopefully. :gift: 
"
67668408,2143,mattsta,2014-12-19T17:24:31Z,"Things to review:
- I enabled a [commented out assembly function](https://github.com/mattsta/redis/commit/cc954a209f338bda87878c08523170076148bc36) in LZF decompression
- The new `redis-server test quicklist` test runs over 200,000 test iterations and takes between 20 seconds and 45 seconds depending on your test machine.
- ~~I haven't valgrind'd the new changes yet~~
  - have run valgrind now (takes 45 minutes for the test to complete under valgrind!) — `==16396== All heap blocks were freed -- no leaks are possible`
- I think I [added something useful](https://github.com/mattsta/redis/commit/a99ff667572d61952a4ead06b8406c0ba1e3e8e4) to sds, but I'm really bad at knowing what I can add there.  :)
- We allow [old list ziplist options](https://github.com/mattsta/redis/commit/ac54ea53591f63a08ff53c11113672c8f44e8854#diff-75218e1e64b8201a55c2ac2203e8970cL399), but don't print warnings about old options yet.
"
67955281,2143,antirez,2014-12-23T14:23:51Z,"Hello Matt, thank you for all the work put on this! I've yet to finish my review but I found two things to report ASAP. Maybe I'll comment more later.
1. There are memory leaks apparently, if you run `make test` on your latest commit branch on osx, the `leaks` utility will detect a number of lost blocks. You can get more info by running the whole test over valgrind, like this:

```
make distclean
make OPT=-O0 MALLOC=libc
./runtest --valgrind --clients 1
```

If you don't want to run the full test to start debugging, just run selected units with --single.
There are leaks in the following units: type/set, unit/scripting, integration/aof. I did not debugging at all so no clues, sorry. However the leaks are reproducible at every run, you'll track it in no time.
1. LZF macro for asm is probably a bad idea, from the LZF Changelog of a version that is newer than the one we run here: _\- finally disable rep movsb - it's a big loss on modern intel cpus, and only a small win on amd cpus._.

I suggest reverting the LZF change and opening a new issue where we understand if we want to switch from 3.5 to 3.6, where LZF claims some speed improvement.

I hope to have more in the next hours, however since I'll take a few days away from work I may be silent for a few days. Thanks.
"
67956581,2143,mattsta,2014-12-23T14:36:53Z,"> There are memory leaks apparently,

Those could have been because it got rebased against unstable when unstable had the bad merge with set operations.  (Also related to that: the CI didn't detect all those errors _and_ the CI doesn't alert anybody when there are errors.  Is there an easy fix?  Maybe have the CI email the redis-dev list when a valgrind test fails on a public branch?  TravisCI could do that (and more) automatically too.)

Updated: Re-based to current unstable.  Removed LZF ASM thing too.  Though, what is the ""LZF Changelog?""  I didn't see a reference to where the implementation came from (version 3.5?  version 3.6?  from where?) so I couldn't check versions myself.
"
67957886,2143,antirez,2014-12-23T14:50:02Z,"@mattsta Not sure what is the problem with unstable, I guess spopCommand() was broken and later fixed into unstable but your code is still rebased with a broken version? So this would mean -> we have no problems with quicklist at all, hopefully.

About the CI, it should definitely detect it, and it can be configured to send emails, currently is disabled because it is a bit annoying... since there are many false positives when you run the test _so_ many times. However I can improve recidiv in order to send only if there is a given pattern (valgrind for example).

However normally I go to check the CI manually very often, but since I'm in my home town, this is what happened: I'm here without access to the box, and the kind person that helps us to keep our house clean apparently powered off the computer for error... :-) No longer CI until I return back first days of January.

About LZF:
1. Changelog of LZF is at their official site: just download the latest tar.gz and read the Changelog file. Website is here: http://oldhome.schmorp.de/marc/liblzf.html
2. About our version, I believe we are just one version behind:

```
3.6  Mon Feb  7 17:37:31 CET 2011
        - fixed hash calculation in C♯ version (Tiago Freitas Leal).
        - unroll copy for small sizes, use memcpy for larger sizes,
          greatly speeding up decompression in most cases.
        - finally disable rep movsb - it's a big loss on modern intel cpus,
          and only a small win on amd cpus.
        - improve C++ compatibility of the code.
        - slightly improve compressor speed.
        - halved memory requirements for compressor on 64 bit architectures,
          which can improve the speed quite a bit on older cpus.

3.5  Fri May  1 02:28:42 CEST 2009
        - lzf_compress did sometimes write one octet past the given output
          buffer (analyzed and nice testcase by Salvatore Sanfilippo).
```

3.5 mentions my bug report, so I'm pretty sure I updated as soon as the fix was released.
However 3.6 uses less memory and more speed, so may be worth it assuming it's stable.
"
67960461,2143,mattsta,2014-12-23T15:15:44Z,"> I believe we are just one version behind:

Not anymore!  I've updated this branch to include the new version.  All my tests pass with no crashes or memory leaks.  YMMV.  :)

:christmas_tree: 
"
68068376,2143,antirez,2014-12-24T18:29:20Z,"Thanks for the update of LZF, the only thing to check is if the previous version was actually not modified compared to the original sources, I totally don't recall but that's easy to check.

About the more interesting problem of sdsnative(), actually I see what you are doing here (cit.). I think there is an even better solution that avoids the double-copy and reallocation, I'm sure you thought about it but avoided to implement it to don't put too much stuff into the same PR.

So basically I'm working at a branch (originated from the quicklist branch) that refactors rdb.c in order to have functions that return plain zmalloc allocated objects instead of robjects. It was not a huge change at all and should improve loading times. At the same time I'm reviewing more code from quicklist. In the first days of January at max everything will get merged: in order to merge I've also to do my RDB changes otherwise I need to re-increment the RDB number again, or risk that users have RDB files with same version but different format.

Thanks again! :christmas_tree: 
"
68996956,2143,antirez,2015-01-07T09:24:25Z,"Hello Matt, in order to merge I'm doing my changes to RDB here:

https://github.com/antirez/redis/commits/rdbchanges

I see that we diverged a bit, you mostly changed spacing and did a force update: trivial to rebase my changes upon yours, but please for future change add commits instead of force-updating so that it will be simpler to merge. Thx! 
"
69151560,2143,antirez,2015-01-08T08:52:14Z,"Merged! Applying my commits from the other branch to master as well, and RDB v7 is done...
"
69353843,2143,mattsta,2015-01-09T15:59:32Z,"> I see that we diverged a bit,

Sorry for the confusion!  Thanks for getting this added (_and_ thanks for the new K/V RDB format)!

> trivial to rebase my changes upon yours, but please for future change add commits instead of force-updating so that it will be simpler to merge.

That's the problem with distributed revision control (and github PRs)... the code lives in the author's repo and can be updated at any time.

If I _did_ only add new commits after this PR was first created, it would have 100 cleanup commits sitting here.  :)

The _real_ problem is git has a bad default interface.  The **default** should be `git pull -r` to automatically fix any upstream changes.  Without `git pull -r`, git thinks everything changed on top of existing changes, and it can't deal with it.

So, if `git pull` freaks out, run `git merge --abort` then `git pull -r`.

Or, worst case, `git checkout unstable; git branch -D [messed up branch]; git checkout [clean branch]` (assuming the branch is only the PR branch without any local changes on it).

:shipit: 
"
163983411,2143,hey-jude,2015-12-11T16:31:44Z,":+1: 
Which version can I use this feature?
"
163992222,2143,badboy,2015-12-11T17:06:05Z,"It's currently in the `testing` branch and will thus end up in the upcoming 3.2 release.
"
1641921039,12416,oranagra,2023-07-19T11:36:13Z,"@zh1029 i rather you avoid amend and force-push, it's harder to do incremental review this way.
the PR is gonna be squash-merged eventually anyway. "
1764113224,12416,zh1029,2023-10-16T09:49:04Z,"Sorry, was busy in other working. While going through the latest comments. I see you guys are prefer to have separated APIs. Personal I agree with it as from user perspective I'd like have explicit API name to help understanding the exact operation. I reverted to the earlier version I pushed to have separated APIs. Please let's initiate reviewing from this version."
1820109032,12416,zh1029,2023-11-21T02:20:55Z,Is the code mature enough to be candidate to Redis 8.0? 
1820397244,12416,zuiderkwast,2023-11-21T07:50:15Z,"> Is the code mature enough to be candidate to Redis 8.0?

@zh1029 Yes, I hope so. Can you update the top comment to describe the API correctly? Then we can check that everyone agrees about this API. Later, when the PR is merged, the top comment will be used as the commit message."
1863728757,12416,zh1029,2023-12-20T01:56:45Z,Sorry for the bother. Let's follow the PR #12874 instead to follow company's contributing to open source policy. I shall close this PR.
925487387,9511,panjf2000,2021-09-23T03:50:30Z,"Hi @oranagra , got some time to take a look at this?"
925546511,9511,oranagra,2021-09-23T06:49:36Z,already on my list.. will get to it when i'm done with other things.
925552824,9511,panjf2000,2021-09-23T07:02:49Z,"got it.

On Thu, Sep 23, 2021 at 14:49 Oran Agra ***@***.***> wrote:

> already on my list.. will get to it when i'm done with other things.
>
> —
> You are receiving this because you authored the thread.
> Reply to this email directly, view it on GitHub
> <https://github.com/redis/redis/pull/9511#issuecomment-925546511>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/ABZGEVQVU4OWSJSJNBSNBMDUDLEYZANCNFSM5EEGOVGA>
> .
>
-- 

Best Regards,
Andy Pan
"
932694327,9511,panjf2000,2021-10-02T06:39:54Z,Ping @oranagra 
932871515,9511,oranagra,2021-10-03T06:16:52Z,"@panjf2000 sorry, it's still on my list (haven't forgotten it), but i have lots of other things to handle first."
1735009019,12611,sundb,2023-09-26T07:48:02Z,"seems that this fix is much better than #12601.
But you still need to fix the other places with `defined(MAC_OS_X_VERSION_10_6)`"
1735236279,12611,sundb,2023-09-26T10:09:55Z,@ygcaicn Please also handle these (https://github.com/redis/redis/pull/12611#issuecomment-1735009019).
1735265390,12611,sundb,2023-09-26T10:29:47Z,@ygcaicn there are three more in debug.c
1735296938,12611,sundb,2023-09-26T10:52:55Z,@ygcaicn Please temporarily apply this commit(https://github.com/redis/redis/pull/12601/commits/aa40f1b01b749e9adc65ca10616733475e1ec5c5) to this and i will help you to run CI to verify it.
1735313113,12611,sundb,2023-09-26T11:04:15Z,CI only for macOS with commit (https://github.com/redis/redis/pull/12611/commits/c64040ba7c34470ad3ef7ca51b8099bf587e550e): https://github.com/sundb/redis/actions/runs/6311722587
1735721819,12611,ygcaicn,2023-09-26T14:59:39Z,"> CI only for macOS with commit ([c64040b](https://github.com/redis/redis/commit/c64040ba7c34470ad3ef7ca51b8099bf587e550e)): https://github.com/sundb/redis/actions/runs/6311722587

@sundb It seems that the CI has failed. Could you please confirm what the issue is?"
1735724232,12611,sundb,2023-09-26T15:00:49Z,"@ygcaicn it doesn't relate to this PR, please ignore it."
1738767563,12611,sundb,2023-09-28T09:12:10Z,@ygcaicn Please also fix `#if (defined __APPLE__ && defined(MAC_OS_X_VERSION_10_7))` in `config.h`.
1746240754,12611,hoangnx30,2023-10-04T06:47:59Z,"Hi guys, when this pull request can be merged? I can not install redis-memory-server on MAC OS sonoma now. "
1751808369,12611,melishev,2023-10-07T20:11:40Z,@enjoy-binbin any news on the review?
1751917224,12611,0xtrou,2023-10-08T04:30:11Z,need this pr to be merged asap :(
1752520056,12611,hoangnx30,2023-10-09T08:04:18Z,Please let us know when you release a new version. We are looking forward to it. 
1754602765,12611,oranagra,2023-10-10T07:45:55Z,"we didn't have any plans for an immediate release, is it blocking, feel free to provide context."
1755744087,12611,mhassan1,2023-10-10T15:56:44Z,"> we didn't have any plans for an immediate release, is it blocking, feel free to provide context.

Users of the [redis-memory-server](https://www.npmjs.com/package/redis-memory-server) NPM package are having [trouble](https://github.com/mhassan1/redis-memory-server/issues/22) installing Redis Stable (7.2), which is the default. We are able to specify an old version as a workaround, but some users are unhappy with that. We are also considering applying the [patch](https://github.com/redis/redis/issues/12585#issuecomment-1729243412) programmatically, but this is risky."
1757798470,12611,oranagra,2023-10-11T14:19:47Z,"why does an old version (which?) work? shouldn't it suffer from the same problem?
we plan on publishing a set of releases next week."
1757883955,12611,mhassan1,2023-10-11T14:57:38Z,"On my machine (OS X 13.6), all of Redis 6.0, 6.2, and 7.0 build successfully, whereas 7.2 fails with this error:
```txt
replication.c:1666:21: error: 'fstat64' is deprecated: first deprecated in macOS 10.6 [-Werror,-Wdeprecated-declarations]
                    redis_fstat(slave->repldbfd,&buf) == -1) {
                    ^
./config.h:45:21: note: expanded from macro 'redis_fstat'
#define redis_fstat fstat64
                    ^
/Library/Developer/CommandLineTools/SDKs/MacOSX.sdk/usr/include/sys/stat.h:430:9: note: 'fstat64' has been explicitly marked deprecated here
int     fstat64(int, struct stat64 *) __OSX_AVAILABLE_BUT_DEPRECATED(__MAC_10_5, __MAC_10_6, __IPHONE_NA, __IPHONE_NA);
        ^
1 error generated.
```"
1759064103,12611,oranagra,2023-10-12T07:16:29Z,"i don't understand why 7.2 would be different than 7.0 in that respect. they both have the same code and conditions around fstat.
@sundb do you understand that?"
1759076404,12611,sundb,2023-10-12T07:25:40Z,"@mhassan1 Did you compile these versions(6.0, 6.2, 7.0) after upgrading to 13.6?
They all compile with the same error on my local macos."
1759629090,12611,mhassan1,2023-10-12T13:36:18Z,"I see what's going on. It looks like we added `-Werror=deprecated-declarations` in 7.2 ([link](https://github.com/redis/redis/pull/10932/files#diff-3e2513390df543315686d7c85bd901ca9256268970032298815d2f893a9f0685R34)). On my machine, 7.0 and earlier would emit `'fstat64' is deprecated` as a warning, but the build would still complete; with 7.2, it became an error. So the issue I'm having is not the same as the original issue, but the fix is the same.

I believe the original reporters may have had M1 Macs (I don't); maybe that explains the difference in behavior. I know some users reported that the old version workaround worked, while others said it didn't, so maybe the difference is machine-specific."
1760699670,12611,sundb,2023-10-13T03:15:25Z,"@mhassan1 Thanks, you are right.
Test on i7 macos, these (5.0, 6.0, 7.0) are compiled successfully, but with warnings.
"
1762066928,12611,elovin,2023-10-13T19:19:37Z,"FYI, I have an M1 pro with MacOS 13.6 and I get the same error with `redis-memory-server`."
1762855745,12611,melishev,2023-10-14T12:12:58Z,"Yes, we hope you will release an updated release as soon as possible so that we can use it in the `redis-memory-server` package"
1768915921,12611,hoangnx30,2023-10-18T16:29:48Z,"FYI, I have an M1 with MacOS 14.0 and I get the same error with redis-memory-server."
1770527821,12611,oranagra,2023-10-19T10:29:14Z,a new version of redis with this fixed was released yesterday. so i assume you're using an older version.
1771197884,12611,hoangnx30,2023-10-19T15:14:53Z,Sorry. I forgot to remove the cache. 
1783826447,12611,KFoxder,2023-10-28T14:18:17Z,"@oranagra Do backport [release notes](https://github.com/redis/redis/releases/tag/6.2.14) not call out these type of fixes? I was going to ask when it was going to be released in a 6.2.X release and assumed it wasn't until I looked at the comparison here: 
https://github.com/redis/redis/compare/6.2.13...6.2.14"
1783831935,12611,oranagra,2023-10-28T14:35:48Z,"@KFoxder the reason I didn't list these in the 6.x and 7.0 release notes is that it was stated above that in these branches it doesn't fail the build, only in 7.2 it fails the build."
1783862210,12611,mhassan1,2023-10-28T16:23:31Z,@oranagra I believe builds were failing for all unpatched versions on M1 Macs.
1783887916,12611,KFoxder,2023-10-28T18:11:40Z,@oranagra agreed with @mhassan1 that this was failing for us and others on M1/M2 Macs on all versions. 
1783888287,12611,oranagra,2023-10-28T18:13:16Z,Ok. Sorry.. so anyway I did include the fix in all of them.
967733725,9774,ny0312,2021-11-12T23:55:19Z,"This is a new PR that replaces old PR: https://github.com/ny0312/redis/pull/1

There were some comments left on the old PR that are now addressed in this new PR."
980825077,9774,madolson,2021-11-28T02:32:38Z,"@redis/core-team Please review, there are two core team decisions. The first is the config to limit maximum amount of outbound data, the second is the command to describe the information for the cluster links. "
987680690,9774,madolson,2021-12-07T08:23:27Z,"@ny0312 The decision was that we will set the default to 0, implying infinite today, and we'll evaluate how it works. We'll document that you can do something about it."
987686811,9774,bashanyy,2021-12-07T08:32:11Z,谢谢
993061410,9774,ny0312,2021-12-14T01:10:36Z,"Accompanying doc PR: https://github.com/redis/redis-doc/pull/1710

Thanks."
993206538,9774,oranagra,2021-12-14T06:38:47Z,The top comment of the PR is outdated too.
995400056,9774,madolson,2021-12-16T03:13:07Z,"@ny0312 Sorry to ask more from you, but we moved to a new command definition system:
```
If you had any PR that modifies the command table, you'll need to rebase, modify the json files, and run the python script that generates commands.c (and commit that too).
Same thing about modifying some things about the command (like argument, response, etc. they're all documented there now).

If you have any redis-doc PR that changes commands.json, make sure that the change is reflected in the json files in the redis repo, the command.json in redis-doc is now in ""code"" freeze (for now you can keep the change in both repos, but make sure they're ""Identical""), we'll soon delete the json file from redis-doc and take the data from redis in some way.
```

You can transfer the contents of the doc PR over to this CR now. 

@yossigo Can you approve the major changes if you have time, so we can get this merged?"
996041072,9774,oranagra,2021-12-16T17:46:31Z,"@ny0312 FYI, i'd prefer to avoid rebase, amend, and force-push, it's harder to review what changed.
since we're merging most PRs with a squash-merge, it doesn't matter how many incremental commits they contain.
in some cases we have PRs that we plan to merge without squash (if they contain several topics, or some mass refactory in a separate commit), and in these cases, when we rebase, we must do a separate force-push (not mixed with any actual change), so we can let GH show us the diff that each force-push introduced.

anyway, long story short, does the last push contain any actual changes? or just a rebase?"
996053465,9774,madolson,2021-12-16T18:03:39Z,@oranagra This is just a rebase to handle the merge conflicts because there is the new command definitions.
996057719,9774,oranagra,2021-12-16T18:09:35Z,"the new CLUSTER LINKS command will be implicitly supported in async-loading when this PR and #9878 are both individually applied, right? (no need to make any special changes in one for the benefit of the other)

p.s. don't forget to make a redis-doc PR."
996098789,9774,ny0312,2021-12-16T19:07:18Z,"@oranagra My apologies. I'm clumsy with github PRs. I simply did not know that `we're merging most PRs with a squash-merge`. 

To answer your question, my last force-push contained:
1. rebase
2. Added a new `cluster-links.json` file under `src/commands/` folder.
3. Added a line to `commands.c` for `cluster links`, manually. Later I realize that `commands.c` is supposed to be auto-generated. So my next revision will remove my manual change to `commands.c` and replace it with an auto-generated change."
996102580,9774,ny0312,2021-12-16T19:12:41Z,"@oranagra Yes, running `CLUSTER LINKS` commands during async loading should be fine. I gave it the same flags that `CLUSTER NODES` have.

The accompanying redis-doc PR is here: https://github.com/redis/redis-doc/pull/1710. I dont see a commit on the `redis-doc` repo that removes all existing json files. I dont want to start making `redis-doc/commands/*.json` and `redis/src/commands/*json` to diverge, so im stilll keeping the `cluster-links.json` file in my `redis-doc` PR."
996122857,9774,oranagra,2021-12-16T19:38:48Z,"For now we didn't yet get rid of commands.json in redis-doc, so we need to be very careful and make sure we do the same changes in both places, so we we eventually delete it, no data is lost"
996166354,9774,madolson,2021-12-16T20:21:02Z,@oranagra Can you verify that commands.c and the json file were implemented correctly? They look right to me but I haven't looked that much. Besides that I think this is ready to merge along with the doc PR.
996808520,9774,sundb,2021-12-17T15:26:16Z,"`Disconnect link when send buffer limit reached` test failed in my daily CI.
https://github.com/sundb/redis/runs/4560035116?check_suite_focus=true
https://github.com/sundb/redis/runs/4560034978?check_suite_focus=true"
1002470005,9774,oranagra,2021-12-29T09:02:27Z,"@ny0312 can you please look into these failures.
see also
https://github.com/redis/redis/runs/4654435937?check_suite_focus=true#step:10:1262
```
01:17:54> Each node has two links with each peer: FAILED: Expected 19*2 eq 37 (context: type eval line 11 cmd {assert {$num_peers*2 eq $num_links}} proc ::foreach_instance_id)
```"
1002617971,9774,madolson,2021-12-29T14:21:28Z,"@oranagra @ny0312 that issue specifically is caused because the cluster tests do hard resets in between tests, and the nodes may not have re-established all links, especially for slower tests. I was looking into the other failures a bit last night, and don't have any other suggested fixes at the moment."
1003667959,9774,oranagra,2022-01-02T05:49:06Z,"some failure in the FreeBSD CI in `Disconnect link when send buffer limit reached`
https://github.com/redis/redis/runs/4681750680?check_suite_focus=true#step:4:7614
This may be an issue with the CI itself (it't not stable), but also, maybe there's some timing issue in the test."
1003668059,9774,madolson,2022-01-02T05:50:12Z,"I'll ping @ny0312 on our AWS slack, I'm not sure he's noticed this."
1003668206,9774,oranagra,2022-01-02T05:51:59Z,@madolson a few days ago he told me he's on vacation 8-)
1003668724,9774,madolson,2022-01-02T05:59:12Z,"I probably should have known that :D.

I was able to slowly validate it was this commit that is causing both the crashes on sanitizer and the buffer disconnect issues. I have no idea why though, it might just be we're dumping a bunch of memory into the test which doesn't seem to be done elsewhere in the cluster test."
1004236189,9774,ny0312,2022-01-03T17:16:49Z,I'm back. I will sync up with Maddy and investigate and fix what is going on. Sorry and thanks. 
1008262857,9774,oranagra,2022-01-09T09:35:19Z,"any news?
i must admit i didn't notice it fail lately, but got this one now:
https://github.com/redis/redis/runs/4733591841?check_suite_focus=true
```
00:47:22> Disconnect link when send buffer limit reached: error writing ""sock802fbc590"": broken pipe
```

it could be just a random failure of the slow freebsd CI (we see broken pipes in random places).
note that in the past we saw:
```
00:44:36> Disconnect link when send buffer limit reached: error writing ""sock80264c2d0"": connection reset by peer
```
as well as other types of errors, on **non**-freebsd CI.
* a hung triggering a SIGSEGV kill by the test framework on a sanitizer run.
* a the following failure on centos+tls:
```
01:17:54> Each node has two links with each peer: FAILED: Expected 19*2 eq 37 (context: type eval line 11 cmd {assert {$num_peers*2 eq $num_links}} proc ::foreach_instance_id)
```"
1012652019,9774,ny0312,2022-01-14T00:54:21Z,"Hey sorry for the late response. But I have been trying to reproduce this test failure, but couldn't.

This is the command I run (I copied it from https://github.com/redis/redis/runs/4654435988?check_suite_focus=true):

```
gmake || exit 1 ; if echo """" | grep -vq redis ; then ./runtest --verbose --timeout 2400 --no-latency --dump-logs  || exit 1 ; fi ; if echo """" | grep -vq modules ; then MAKE=gmake ./runtest-moduleapi --verbose --timeout 2400 --no-latency --dump-logs  || exit 1 ; fi ; if echo """" | grep -vq sentinel ; then ./runtest-sentinel  || exit 1 ; fi ; if echo """" | grep -vq cluster ; then ./runtest-cluster  || exit 1 ; fi ;
```

I've run it numerous times locally. I've run it numerous times on a FreeBSD 64 bit EC2. All with the latest `unstable` branch. Never able to reproduce the failure.

From the error message, it looks like Redis crashed during my test. The crash must be flaky and non-deterministic.

Any ideas that could help reproduce it?

Another question - how do I preserve redis logs from these test runs?"
1012679427,9774,sundb,2022-01-14T01:57:49Z,"@ny0312 It's bound to happen every time on my local freebsd vm, I've only allocated 1 core and 256M RAM to it, and since I don't know about clusters, there's no way to solve this problem directly, maybe I can package the logs and send them to you.

I use the following to reproduce.
```
./runtest-cluster --single 24-links --dont-clean
```"
1012802321,9774,sundb,2022-01-14T06:08:42Z,"When memory is exhausted, this test is bound to fail, and here are some logs after enabling `loglevel debug`.

```
16316:M 14 Jan 2022 12:48:51.257 . I/O error reading from node link: connection closed
16316:M 14 Jan 2022 12:48:51.315 . I/O error reading from node link: connection closed
16316:M 14 Jan 2022 12:48:51.315 . I/O error reading from node link: connection closed
16316:M 14 Jan 2022 12:48:51.318 . Unable to connect to Cluster Node [127.0.0.1]:40000 -> accept: Resource temporarily unavailable
16316:M 14 Jan 2022 12:48:51.318 . Unable to connect to Cluster Node [127.0.0.1]:40002 -> accept: Resource temporarily unavailable
16316:M 14 Jan 2022 12:48:51.318 . Unable to connect to Cluster Node [127.0.0.1]:40001 -> accept: Resource temporarily unavailable
```

```
16314:M 14 Jan 2022 12:48:51.254 . I/O error reading from node link: Connection reset by peer
16314:M 14 Jan 2022 12:48:51.257 . I/O error reading from node link: connection closed
```"
1012814027,9774,ny0312,2022-01-14T06:39:17Z,"Huh, looks like file descriptors are exhausted?

@sundb ,

You mentioned you reproduced this on a FreeBSD vm. Could you please share what exactly is your setup? Which VirtualMachine?

In the meanwhile, I will try to simulate your setup with a EC2 to reproduce. i.e. I will provision a EC2 with FreeBSD, 1 core and 256MB memory.

Also, if you could share the full Redis logs for the failed test, that'd be great. (For example you could attach a file here, or use something like https://justpaste.it/)

Thanks a lot!

"
1012834938,9774,sundb,2022-01-14T07:11:08Z,"@ny0312 justpaste.it can't upload files, so I created a repository to hold the logs(https://github.com/sundb/log).

* system: `Freebsd 12 64bit`
* vm: VMware

* ulimit
```
-t: cpu time (seconds)              unlimited
-f: file size (blocks)              unlimited
-d: data seg size (kbytes)          33554432
-s: stack size (kbytes)             524288
-c: core file size (blocks)         unlimited
-m: resident set size (kbytes)      unlimited
-l: locked-in-memory size (kbytes)  unlimited
-u: processes                       10240
-n: file descriptors                10240
-b: socket buffer size (bytes)      unlimited
-v: virtual memory size (kbytes)    unlimited
-p: pseudo-terminals                unlimited
-w: swap size (kbytes)              unlimited
-k: kqueues                         unlimited
-o: umtx shared locks               unlimited
```

* change loglevel
```diff
diff --git a/tests/cluster/run.tcl b/tests/cluster/run.tcl
index c81d8f39d..ddf9986aa 100644
--- a/tests/cluster/run.tcl
+++ b/tests/cluster/run.tcl
@@ -17,6 +17,7 @@ proc main {} {
         ""appendonly yes""
         ""enable-protected-configs yes""
         ""enable-debug-command yes""
+        ""loglevel debug""
     }
     run_tests
     cleanup
```"
1015076735,9774,ny0312,2022-01-18T05:11:41Z,"OK. I am able to reproduce the test failure with 1 core, 512MB RAM 64bit FreeBSD.

The failure is Redis getting OOM killed by kernel due to out of swap. In the test, I'm allowing cluster link buffers to grow up to 32MB. That proved to be too much for the FreeBSD test environment used by the daily runs.

I'm currently experimenting with a new cluster link buffer limit in my test. The challenge is that in order to trigger the condition under test, which is a cluster bus link being freed once over the limit, I need to fill up the cluster link buffer. But to fill up the link buffer, I need to first fill up the TCP write buffer in the kernel level, which varies from OS to OS.

I will post a fix PR as soon as I can get a good stable buffer limit."
1015092611,9774,oranagra,2022-01-18T05:53:21Z,"note that in the ""other"" test suite, we have the `--large-memory` which enables us to run certain tests only on systems with a lot of memory (currently only manually).

also, what about the test failures in the sanitizer runs, like this one:
https://github.com/redis/redis/runs/4829401183?check_suite_focus=true#step:9:630

not sure if this one is related:
https://github.com/redis/redis/runs/4810931899?check_suite_focus=true#step:9:466"
1015978344,9774,ny0312,2022-01-19T01:04:23Z,"
> also, what about the test failures in the sanitizer runs, like this one:
> https://github.com/redis/redis/runs/4829401183?check_suite_focus=true#step:9:630

This has a different root cause. In the test, I'm assuming as soon as I send a large PUBLISH command to fill up a cluster link, the link will be freed. But in reality the link will only get freed in the next `clusterCron` run whenever that happens. My test is not accounting for this race condition. I will fix it too.


>not sure if this one is related:
> https://github.com/redis/redis/runs/4810931899?check_suite_focus=true#step:9:466

This link has two failures. `test-sanitizer-address (clang) ` and `test-freebsd`

`test-sanitizer-address (clang) `  is not related to my test. `test-freebsd` is related to my test and it should be the same OOM root cause."
1016128938,9774,oranagra,2022-01-19T06:34:51Z,"i didn't look at the test, but i wanna mention that there are other tests in which we struggled to fill output buffers and had to deal with the OS socket buffers swallowing our bytes.

i think these are in:
tests/unit/client-eviction.tcl
tests/unit/maxmemory.tcl
tests/integration/replication-buffer.tcl

IIRC the best approach we had was to pause the destination process, and then gradually fill more and more data until we see it starts piling up, then stop."
1017951216,9774,ny0312,2022-01-20T21:39:25Z,"@madolson @oranagra @sundb 

PR to fix flaky test ""Disconnect link when send buffer limit reached"" - https://github.com/redis/redis/pull/10157

Sorry for all the inconvenience. And thanks for the help and advice.

"
1018239089,9774,tezc,2022-01-21T07:06:56Z,"@ny0312 
Test failed : https://github.com/redis/redis/runs/4890129923?check_suite_focus=true#step:9:627

```
Testing unit: 24-links.tcl
00:39:15> (init) Restart killed instances: OK
00:39:15> Cluster nodes are reachable: OK
00:39:15> Cluster nodes hard reset: OK
00:39:15> Cluster Join and auto-discovery test: OK
00:39:17> Before slots allocation, all nodes report cluster failure: OK
00:39:17> Create a cluster with two single-node shards: OK
00:39:21> Cluster should start ok: OK
00:39:21> Each node has two links with each peer: FAILED: Expected 19*2 eq 37 (context: type eval line 11 cmd {assert {$num_peers*2 eq $num_links}} proc ::foreach_instance_id)
(Jumping to next unit after error)
```
Don't know about this test but might be related with timing. I increase ""cluster-node-timeout"" for something else on my branch, I see this test fails here. Just fyi. "
1018725555,9774,ny0312,2022-01-21T17:42:10Z,@tezc Thanks for reporting the failure. Sorry for the inconvenience. I updated my PR(https://github.com/redis/redis/pull/10157) to fix this test as well.
2413102986,13592,fcostaoliveira,2024-10-15T07:27:56Z,"### CE Performance Automation : step 1 of 2 (build) DONE.

This comment was automatically generated given a benchmark was triggered.
Started building at 2024-10-29 07:48:16.872643 and took 64 seconds.
You can check each build/benchmark progress in grafana:
   - git hash: 832a7ad84f913fa0e0aadc91e6953f68f839f41e
   - git branch: moticless:info-keysizes
   - commit date and time: n/a
   - commit summary: n/a
   - test filters:
       - command priority lower limit: 0
       - command priority upper limit: 10000
       - test name regex: .*
       - command group regex: .*

You can check a comparison in detail via the [grafana link](https://benchmarksredisio.grafana.net/d/edsxdsrbexhc0f/ce-benchmark-run-status?orgId=1&var-benchmark_work_stream=1730188162072-0)"
2413104933,13592,fcostaoliveira,2024-10-15T07:28:59Z,"### CE Performance Automation : step 2 of 2 (benchmark) RUNNING...

This comment was automatically generated given a benchmark was triggered.

Started benchmark suite at 2024-12-19 08:07:30.023779 and took 7866.556751 seconds up until now.
Status: [################################------------------------------------------------] 39.48% completed.

In total will run 271 benchmarks.
    - 164 pending.
    - 107 completed:
      - 0 successful.
      - 107 failed.
You can check a the status in detail via the [grafana link](https://benchmarksredisio.grafana.net/d/edsxdsrbexhc0f/ce-benchmark-run-status?orgId=1&var-benchmark_work_stream=1728977339189-0)"
2413111949,13592,fcostaoliveira,2024-10-15T07:32:46Z,"### Automated performance analysis summary

This comment was automatically generated given there is performance data available.

Using platform named: intel64-ubuntu22.04-redis-clx1 to do the comparison.

In summary:
- Detected a total of 130 stable tests between versions.
- Detected a total of 5 regressions bellow the regression water line 10.0.
   - Median/Common-Case regression was -17.9% and ranged from [-84.2%,-13.6%].

You can check a comparison in detail via the [grafana link](https://benchmarksredisio.grafana.net/d/1fWbtb7nz/experimental-oss-spec-benchmarks/?var-branch=unstable&var-branch=moticless:info-keysizes)

### Comparison between unstable and moticless:info-keysizes.

Time Period from 5 months ago. (environment used: oss-standalone)

#### Regressions Table

|                                                                                                                   Test Case                                                                                                                   |Baseline redis/redis unstable (median obs. +- std.dev)|Comparison redis/redis moticless:info-keysizes (median obs. +- std.dev)|% change (higher-better)|   Note   |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------:|-----------------------------------------------------------------------|------------------------|----------|
|[memtier_benchmark-1key-100M-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-100M-bits-bitmap-bitcount.yml)                          |                                                 19127| 3019 +- 0.2% (9 datapoints)                                           |-84.2%                  |REGRESSION|
|[memtier_benchmark-1key-1Billion-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-1Billion-bits-bitmap-bitcount.yml)                  |                                                  1225| 292 +- 0.3% (10 datapoints)                                           |-76.2%                  |REGRESSION|
|[memtier_benchmark-1key-list-10K-elements-linsert-lrem-integer](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-linsert-lrem-integer.yml)|                                                  4292| 3644 +- 3.4% (9 datapoints)                                           |-15.1%                  |REGRESSION|
|[memtier_benchmark-1key-list-10K-elements-lpos-integer](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-lpos-integer.yml)                |                                                  4237| 3660 +- 3.1% (10 datapoints)                                          |-13.6%                  |REGRESSION|
|[memtier_benchmark-1key-list-10K-elements-lpos-string](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-lpos-string.yml)                  |                                                  5676| 4660 +- 2.5% (9 datapoints)                                           |-17.9%                  |REGRESSION|


Regressions test regexp names: memtier_benchmark-1key-100M-bits-bitmap-bitcount|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|memtier_benchmark-1key-list-10K-elements-linsert-lrem-integer|memtier_benchmark-1key-list-10K-elements-lpos-integer|memtier_benchmark-1key-list-10K-elements-lpos-string

<details>
  <summary>Full Results table:</summary>

|                                                                                                                                     Test Case                                                                                                                                     |Baseline redis/redis unstable (median obs. +- std.dev)|Comparison redis/redis moticless:info-keysizes (median obs. +- std.dev)|% change (higher-better)|        Note         |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------:|-----------------------------------------------------------------------|------------------------|---------------------|
|[memtier_benchmark-100Kkeys-hash-hgetall-50-fields-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-100Kkeys-hash-hgetall-50-fields-100B-values.yml)                                    |                                                103487| 102072 +- 1.1% (8 datapoints)                                         |-1.4%                   |No Change            |
|[memtier_benchmark-100Kkeys-load-hash-50-fields-with-1000B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-100Kkeys-load-hash-50-fields-with-1000B-values.yml)                              |                                                 10640| 10671 +- 0.4% (9 datapoints)                                          |0.3%                    |No Change            |
|[memtier_benchmark-100Kkeys-load-hash-50-fields-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-100Kkeys-load-hash-50-fields-with-100B-values.yml)                                |                                                 28449| 28868 +- 1.2% (12 datapoints)                                         |1.5%                    |No Change            |
|[memtier_benchmark-100Kkeys-load-hash-50-fields-with-10B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-100Kkeys-load-hash-50-fields-with-10B-values.yml)                                  |                                                 27237| 25882 +- 1.9% (10 datapoints)                                         |-5.0%                   |potential REGRESSION |
|[memtier_benchmark-10Kkeys-load-hash-50-fields-with-10000B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-10Kkeys-load-hash-50-fields-with-10000B-values.yml)                              |                                                  1913| 1926 +- 0.8% (12 datapoints)                                          |0.7%                    |No Change            |
|[memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values.yml)                                    |                                                 77195| 77508 +- 0.7% (10 datapoints)                                         |0.4%                    |No Change            |
|[memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-10Mkeys-load-hash-5-fields-with-100B-values-pipeline-10.yml)            |                                                222824| 228231 +- 0.8% (8 datapoints)                                         |2.4%                    |No Change            |
|[memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values.yml)                                      |                                                 86269| 85517 +- 0.8% (9 datapoints)                                          |-0.9%                   |No Change            |
|[memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-10Mkeys-load-hash-5-fields-with-10B-values-pipeline-10.yml)              |                                                302135| 298168 +- 0.9% (8 datapoints)                                         |-1.3%                   |No Change            |
|[memtier_benchmark-1Mkeys-100B-expire-use-case](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-100B-expire-use-case.yml)                                                                    |                                                117600| 116512 +- 0.4% (8 datapoints)                                         |-0.9%                   |No Change            |
|[memtier_benchmark-1Mkeys-10B-expire-use-case](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-10B-expire-use-case.yml)                                                                      |                                                118128| 117731 +- 0.5% (11 datapoints)                                        |-0.3%                   |No Change            |
|[memtier_benchmark-1Mkeys-1KiB-expire-use-case](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-1KiB-expire-use-case.yml)                                                                    |                                                114750| 114716 +- 0.6% (10 datapoints)                                        |-0.0%                   |No Change            |
|[memtier_benchmark-1Mkeys-4KiB-expire-use-case](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-4KiB-expire-use-case.yml)                                                                    |                                                109654| 110024 +- 0.5% (12 datapoints)                                        |0.3%                    |No Change            |
|[memtier_benchmark-1Mkeys-bitmap-getbit-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-bitmap-getbit-pipeline-10.yml)                                                          |                                                671278| 669725 +- 1.1% (9 datapoints)                                         |-0.2%                   |No Change            |
|[memtier_benchmark-1Mkeys-generic-exists-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-generic-exists-pipeline-10.yml)                                                        |                                                724807| 715804 +- 1.3% (10 datapoints)                                        |-1.2%                   |No Change            |
|[memtier_benchmark-1Mkeys-generic-expire-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-generic-expire-pipeline-10.yml)                                                        |                                                661298| 655123 +- 0.3% (9 datapoints)                                         |-0.9%                   |No Change            |
|[memtier_benchmark-1Mkeys-generic-expireat-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-generic-expireat-pipeline-10.yml)                                                    |                                                653748| 643110 +- 0.8% (10 datapoints)                                        |-1.6%                   |No Change            |
|[memtier_benchmark-1Mkeys-generic-pexpire-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-generic-pexpire-pipeline-10.yml)                                                      |                                                658001| 647711 +- 0.6% (10 datapoints)                                        |-1.6%                   |No Change            |
|[memtier_benchmark-1Mkeys-generic-scan-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-generic-scan-pipeline-10.yml)                                                            |                                                374937| 372021 +- 3.9% (12 datapoints)                                        |-0.8%                   |No Change            |
|[memtier_benchmark-1Mkeys-generic-touch-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-generic-touch-pipeline-10.yml)                                                          |                                                737184| 715528 +- 2.0% (10 datapoints)                                        |-2.9%                   |No Change            |
|[memtier_benchmark-1Mkeys-generic-ttl-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-generic-ttl-pipeline-10.yml)                                                              |                                                718197| 706440 +- 1.2% (12 datapoints)                                        |-1.6%                   |No Change            |
|[memtier_benchmark-1Mkeys-hash-hexists](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-hash-hexists.yml)                                                                                    |                                                109547| 108667 +- 0.5% (9 datapoints)                                         |-0.8%                   |No Change            |
|[memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-hash-hget-hgetall-hkeys-hvals-with-100B-values.yml)                |                                                113532| 111379 +- 0.7% (11 datapoints)                                        |-1.9%                   |No Change            |
|[memtier_benchmark-1Mkeys-hash-hgetall-50-fields-10B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-hash-hgetall-50-fields-10B-values.yml)                                          |                                                107946| 106369 +- 0.8% (9 datapoints)                                         |-1.5%                   |No Change            |
|[memtier_benchmark-1Mkeys-hash-hincrby](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-hash-hincrby.yml)                                                                                    |                                                118455| 116899 +- 0.5% (8 datapoints)                                         |-1.3%                   |No Change            |
|[memtier_benchmark-1Mkeys-hash-hmget-5-fields-with-100B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-hash-hmget-5-fields-with-100B-values-pipeline-10.yml)            |                                                540261| 530660 +- 0.7% (9 datapoints)                                         |-1.8%                   |No Change            |
|[memtier_benchmark-1Mkeys-hash-transactions-multi-exec-pipeline-20](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-hash-transactions-multi-exec-pipeline-20.yml)                            |                                                829095| 825754 +- 0.6% (10 datapoints)                                        |-0.4%                   |No Change            |
|[memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-list-lpop-rpop-with-100B-values.yml)                                              |                                                112361| 112744 +- 0.7% (8 datapoints)                                         |0.3%                    |No Change            |
|[memtier_benchmark-1Mkeys-list-lpop-rpop-with-10B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-list-lpop-rpop-with-10B-values.yml)                                                |                                                113895| 112619 +- 2.1% (9 datapoints)                                         |-1.1%                   |No Change            |
|[memtier_benchmark-1Mkeys-list-lpop-rpop-with-1KiB-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-list-lpop-rpop-with-1KiB-values.yml)                                              |                                                112523| 111720 +- 2.0% (10 datapoints)                                        |-0.7%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values.yml)                                    |                                                 65095| 64894 +- 0.5% (9 datapoints)                                          |-0.3%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-hash-5-fields-with-1000B-values-pipeline-10.yml)            |                                                103882| 104842 +- 0.4% (8 datapoints)                                         |0.9%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-hash-hmset-5-fields-with-1000B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-hash-hmset-5-fields-with-1000B-values.yml)                        |                                                 64967| 64969 +- 0.5% (12 datapoints)                                         |0.0%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-list-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-list-with-100B-values.yml)                                                        |                                                 94760| 93700 +- 1.4% (10 datapoints)                                         |-1.1%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-list-with-10B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-list-with-10B-values.yml)                                                          |                                                102915| 100944 +- 1.8% (12 datapoints)                                        |-1.9%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-list-with-1KiB-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-list-with-1KiB-values.yml)                                                        |                                                 72172| 72297 +- 1.0% (8 datapoints)                                          |0.2%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-set-intset-with-100-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-set-intset-with-100-elements.yml)                                          |                                                 46316| 47190 +- 0.9% (10 datapoints)                                         |1.9%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-set-intset-with-100-elements-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-set-intset-with-100-elements-pipeline-10.yml)                  |                                                 74627| 76891 +- 1.6% (10 datapoints)                                         |3.0%                    |potential IMPROVEMENT|
|[memtier_benchmark-1Mkeys-load-stream-1-fields-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-stream-1-fields-with-100B-values.yml)                                  |                                                 82032| 82007 +- 0.6% (8 datapoints)                                          |-0.0%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-stream-1-fields-with-100B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-stream-1-fields-with-100B-values-pipeline-10.yml)          |                                                265326| 266894 +- 0.7% (9 datapoints)                                         |0.6%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-stream-5-fields-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-stream-5-fields-with-100B-values.yml)                                  |                                                 70693| 69650 +- 0.5% (8 datapoints)                                          |-1.5%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-stream-5-fields-with-100B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-stream-5-fields-with-100B-values-pipeline-10.yml)          |                                                160951| 160688 +- 0.4% (11 datapoints)                                        |-0.2%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-string-with-100B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-string-with-100B-values.yml)                                                    |                                                103830| 103227 +- 0.6% (10 datapoints)                                        |-0.6%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-string-with-100B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-string-with-100B-values-pipeline-10.yml)                            |                                                488217| 490434 +- 1.0% (12 datapoints)                                        |0.5%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-string-with-10B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-string-with-10B-values.yml)                                                      |                                                108232| 106477 +- 2.2% (12 datapoints)                                        |-1.6%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-string-with-10B-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-string-with-10B-values-pipeline-10.yml)                              |                                                580359| 569525 +- 0.6% (9 datapoints)                                         |-1.9%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-string-with-1KiB-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-string-with-1KiB-values.yml)                                                    |                                                 98369| 97173 +- 0.5% (9 datapoints)                                          |-1.2%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-string-with-20KiB-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-string-with-20KiB-values.yml)                                                  |                                                 39932| 40157 +- 0.5% (12 datapoints)                                         |0.6%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-zset-listpack-with-100-elements-double-score](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-zset-listpack-with-100-elements-double-score.yml)          |                                                  2596| 2615 +- 1.3% (12 datapoints)                                          |0.7%                    |No Change            |
|[memtier_benchmark-1Mkeys-load-zset-with-10-elements-double-score](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-zset-with-10-elements-double-score.yml)                              |                                                 72054| 70642 +- 0.6% (10 datapoints)                                         |-2.0%                   |No Change            |
|[memtier_benchmark-1Mkeys-load-zset-with-10-elements-int-score](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-load-zset-with-10-elements-int-score.yml)                                    |                                                 77519| 75907 +- 0.6% (9 datapoints)                                          |-2.1%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-append-1-100B](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-append-1-100B.yml)                                                                    |                                                108760| 108026 +- 0.7% (9 datapoints)                                         |-0.7%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-append-1-100B-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-append-1-100B-pipeline-10.yml)                                            |                                                619102| 603858 +- 0.6% (9 datapoints)                                         |-2.5%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-decr](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-decr.yml)                                                                                      |                                                109236| 108831 +- 0.6% (9 datapoints)                                         |-0.4%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-get-100B](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-get-100B.yml)                                                                              |                                                117663| 116320 +- 1.1% (9 datapoints)                                         |-1.1%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-get-100B-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-get-100B-pipeline-10.yml)                                                      |                                                714333| 703603 +- 1.2% (9 datapoints)                                         |-1.5%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-get-10B](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-get-10B.yml)                                                                                |                                                117990| 116098 +- 0.3% (8 datapoints)                                         |-1.6%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-get-10B-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-get-10B-pipeline-10.yml)                                                        |                                                727751| 713079 +- 1.3% (10 datapoints)                                        |-2.0%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-get-1KiB](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-get-1KiB.yml)                                                                              |                                                117186| 115715 +- 0.4% (10 datapoints)                                        |-1.3%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-get-1KiB-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-get-1KiB-pipeline-10.yml)                                                      |                                                675072| 660299 +- 1.0% (10 datapoints)                                        |-2.2%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-incrby](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-incrby.yml)                                                                                  |                                                112548| 111705 +- 0.6% (12 datapoints)                                        |-0.7%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-incrby-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-incrby-pipeline-10.yml)                                                          |                                                634572| 628918 +- 0.6% (9 datapoints)                                         |-0.9%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-incrbyfloat](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-incrbyfloat.yml)                                                                        |                                                104082| 96494 +- 0.5% (10 datapoints)                                         |-7.3%                   |potential REGRESSION |
|[memtier_benchmark-1Mkeys-string-incrbyfloat-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-incrbyfloat-pipeline-10.yml)                                                |                                                390760| 365249 +- 0.8% (8 datapoints)                                         |-6.5%                   |potential REGRESSION |
|[memtier_benchmark-1Mkeys-string-mget-1KiB](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-mget-1KiB.yml)                                                                            |                                                 77115| 75893 +- 0.4% (8 datapoints)                                          |-1.6%                   |No Change            |
|[memtier_benchmark-1Mkeys-string-setex-100B-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-setex-100B-pipeline-10.yml)                                                  |                                                520259| 521054 +- 0.8% (8 datapoints)                                         |0.2%                    |No Change            |
|[memtier_benchmark-1Mkeys-string-setrange-100B](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-setrange-100B.yml)                                                                    |                                                109608| 110081 +- 1.6% (10 datapoints)                                        |0.4%                    |No Change            |
|[memtier_benchmark-1Mkeys-string-setrange-100B-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-string-setrange-100B-pipeline-10.yml)                                            |                                                626186| 625137 +- 0.8% (10 datapoints)                                        |-0.2%                   |No Change            |
|[memtier_benchmark-1key-100M-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-100M-bits-bitmap-bitcount.yml)                                                              |                                                 19127| 3019 +- 0.2% (9 datapoints)                                           |-84.2%                  |REGRESSION           |
|[memtier_benchmark-1key-1Billion-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-1Billion-bits-bitmap-bitcount.yml)                                                      |                                                  1225| 292 +- 0.3% (10 datapoints)                                           |-76.2%                  |REGRESSION           |
|[memtier_benchmark-1key-geo-2-elements-geopos](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-2-elements-geopos.yml)                                                                      |                                                 98266| 96576 +- 0.8% (9 datapoints)                                          |-1.7%                   |No Change            |
|[memtier_benchmark-1key-geo-2-elements-geosearch-fromlonlat-withcoord](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-2-elements-geosearch-fromlonlat-withcoord.yml)                      |                                                 65363| 64862 +- 0.9% (9 datapoints)                                          |-0.8%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geodist](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geodist.yml)                                                                |                                                113698| 113332 +- 0.6% (9 datapoints)                                         |-0.3%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geodist-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geodist-pipeline-10.yml)                                        |                                                716741| 707237 +- 1.4% (10 datapoints)                                        |-1.3%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geohash](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geohash.yml)                                                                |                                                115730| 114425 +- 1.8% (12 datapoints)                                        |-1.1%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geohash-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geohash-pipeline-10.yml)                                        |                                                752452| 735349 +- 1.3% (10 datapoints)                                        |-2.3%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geopos](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geopos.yml)                                                                  |                                                114621| 114417 +- 0.5% (8 datapoints)                                         |-0.2%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geopos-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geopos-pipeline-10.yml)                                          |                                                745365| 734078 +- 0.9% (9 datapoints)                                         |-1.5%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geosearch-fromlonlat](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geosearch-fromlonlat.yml)                                      |                                                 98723| 98560 +- 0.2% (9 datapoints)                                          |-0.2%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geosearch-fromlonlat-bybox](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geosearch-fromlonlat-bybox.yml)                          |                                                 97631| 97413 +- 0.5% (12 datapoints)                                         |-0.2%                   |No Change            |
|[memtier_benchmark-1key-geo-60M-elements-geosearch-fromlonlat-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-geo-60M-elements-geosearch-fromlonlat-pipeline-10.yml)              |                                                451254| 455171 +- 1.7% (11 datapoints)                                        |0.9%                    |No Change            |
|[memtier_benchmark-1key-hash-hscan-50-fields-10B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-hash-hscan-50-fields-10B-values.yml)                                                  |                                                 74076| 74155 +- 1.1% (10 datapoints)                                         |0.1%                    |No Change            |
|[memtier_benchmark-1key-list-10-elements-lrange-all-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10-elements-lrange-all-elements.yml)                                        |                                                107144| 104590 +- 1.7% (10 datapoints)                                        |-2.4%                   |No Change            |
|[memtier_benchmark-1key-list-10-elements-lrange-all-elements-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10-elements-lrange-all-elements-pipeline-10.yml)                |                                                505841| 486350 +- 1.0% (10 datapoints)                                        |-3.9%                   |potential REGRESSION |
|[memtier_benchmark-1key-list-100-elements-lrange-all-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-100-elements-lrange-all-elements.yml)                                      |                                                 77780| 74513 +- 1.3% (12 datapoints)                                         |-4.2%                   |potential REGRESSION |
|[memtier_benchmark-1key-list-100-elements-lrange-all-elements-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-100-elements-lrange-all-elements-pipeline-10.yml)              |                                                131289| 122377 +- 1.9% (9 datapoints)                                         |-6.8%                   |potential REGRESSION |
|[memtier_benchmark-1key-list-10K-elements-lindex-integer](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-lindex-integer.yml)                                                |                                                 91531| 89939 +- 1.0% (12 datapoints)                                         |-1.7%                   |No Change            |
|[memtier_benchmark-1key-list-10K-elements-lindex-string](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-lindex-string.yml)                                                  |                                                 77637| 77669 +- 1.2% (8 datapoints)                                          |0.0%                    |No Change            |
|[memtier_benchmark-1key-list-10K-elements-linsert-lrem-integer](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-linsert-lrem-integer.yml)                                    |                                                  4292| 3644 +- 3.4% (9 datapoints)                                           |-15.1%                  |REGRESSION           |
|[memtier_benchmark-1key-list-10K-elements-linsert-lrem-string](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-linsert-lrem-string.yml)                                      |                                                  5267| 4934 +- 3.1% (10 datapoints)                                          |-6.3%                   |potential REGRESSION |
|[memtier_benchmark-1key-list-10K-elements-lpos-integer](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-lpos-integer.yml)                                                    |                                                  4237| 3660 +- 3.1% (10 datapoints)                                          |-13.6%                  |REGRESSION           |
|[memtier_benchmark-1key-list-10K-elements-lpos-string](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-10K-elements-lpos-string.yml)                                                      |                                                  5676| 4660 +- 2.5% (9 datapoints)                                           |-17.9%                  |REGRESSION           |
|[memtier_benchmark-1key-list-1K-elements-lrange-all-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-1K-elements-lrange-all-elements.yml)                                        |                                                 13768| 13382 +- 2.6% (10 datapoints)                                         |-2.8%                   |No Change            |
|[memtier_benchmark-1key-list-1K-elements-lrange-all-elements-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-1K-elements-lrange-all-elements-pipeline-10.yml)                |                                                 13360| 12885 +- 2.1% (11 datapoints)                                         |-3.6%                   |potential REGRESSION |
|[memtier_benchmark-1key-list-2K-elements-quicklist-lrange-all-elements-longs](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-list-2K-elements-quicklist-lrange-all-elements-longs.yml)        |                                                  4891| 5136 +- 1.7% (10 datapoints)                                          |5.0%                    |potential IMPROVEMENT|
|[memtier_benchmark-1key-pfadd-4KB-values-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-pfadd-4KB-values-pipeline-10.yml)                                                        |                                                202629| 202655 +- 0.7% (9 datapoints)                                         |0.0%                    |No Change            |
|[memtier_benchmark-1key-set-10-elements-smembers](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-10-elements-smembers.yml)                                                                |                                                106148| 104905 +- 0.5% (9 datapoints)                                         |-1.2%                   |No Change            |
|[memtier_benchmark-1key-set-10-elements-smembers-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-10-elements-smembers-pipeline-10.yml)                                        |                                                513474| 502341 +- 0.7% (9 datapoints)                                         |-2.2%                   |No Change            |
|[memtier_benchmark-1key-set-10-elements-smismember](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-10-elements-smismember.yml)                                                            |                                                110541| 110760 +- 0.5% (12 datapoints)                                        |0.2%                    |No Change            |
|[memtier_benchmark-1key-set-100-elements-sismember-is-a-member](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-100-elements-sismember-is-a-member.yml)                                    |                                                111039| 109246 +- 0.6% (11 datapoints)                                        |-1.6%                   |No Change            |
|[memtier_benchmark-1key-set-100-elements-sismember-not-a-member](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-100-elements-sismember-not-a-member.yml)                                  |                                                107293| 104700 +- 1.9% (9 datapoints)                                         |-2.4%                   |No Change            |
|[memtier_benchmark-1key-set-100-elements-smembers](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-100-elements-smembers.yml)                                                              |                                                 71411| 70971 +- 0.9% (10 datapoints)                                         |-0.6%                   |No Change            |
|[memtier_benchmark-1key-set-100-elements-smismember](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-100-elements-smismember.yml)                                                          |                                                107047| 100133 +- 0.6% (12 datapoints)                                        |-6.5%                   |potential REGRESSION |
|[memtier_benchmark-1key-set-100-elements-sscan](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-100-elements-sscan.yml)                                                                    |                                                 70418| 68937 +- 1.2% (12 datapoints)                                         |-2.1%                   |No Change            |
|[memtier_benchmark-1key-set-10M-elements-sismember-50pct-chance](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-10M-elements-sismember-50pct-chance.yml)                                  |                                                109889| 110335 +- 0.3% (10 datapoints)                                        |0.4%                    |No Change            |
|[memtier_benchmark-1key-set-1K-elements-smembers](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-1K-elements-smembers.yml)                                                                |                                                 13176| 12936 +- 1.0% (10 datapoints)                                         |-1.8%                   |No Change            |
|[memtier_benchmark-1key-set-1M-elements-sismember-50pct-chance](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-1M-elements-sismember-50pct-chance.yml)                                    |                                                112344| 112139 +- 1.9% (10 datapoints)                                        |-0.2%                   |No Change            |
|[memtier_benchmark-1key-set-200K-elements-sadd-constant](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-200K-elements-sadd-constant.yml)                                                  |                                                112868| 112082 +- 0.6% (10 datapoints)                                        |-0.7%                   |No Change            |
|[memtier_benchmark-1key-set-2M-elements-sadd-increasing](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-set-2M-elements-sadd-increasing.yml)                                                  |                                                109917| 108590 +- 0.4% (10 datapoints)                                        |-1.2%                   |No Change            |
|[memtier_benchmark-1key-zincrby-1M-elements-pipeline-1](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zincrby-1M-elements-pipeline-1.yml)                                                    |                                                 27594| 27511 +- 1.0% (11 datapoints)                                         |-0.3%                   |No Change            |
|[memtier_benchmark-1key-zrank-1M-elements-pipeline-1](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zrank-1M-elements-pipeline-1.yml)                                                        |                                                 29341| 28917 +- 0.7% (10 datapoints)                                         |-1.4%                   |No Change            |
|[memtier_benchmark-1key-zrem-5M-elements-pipeline-1](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zrem-5M-elements-pipeline-1.yml)                                                          |                                                 28462| 28500 +- 0.6% (8 datapoints)                                          |0.1%                    |No Change            |
|[memtier_benchmark-1key-zrevrangebyscore-256K-elements-pipeline-1](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zrevrangebyscore-256K-elements-pipeline-1.yml)                              |                                                 64642| 65389 +- 0.7% (9 datapoints)                                          |1.2%                    |No Change            |
|[memtier_benchmark-1key-zrevrank-1M-elements-pipeline-1](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zrevrank-1M-elements-pipeline-1.yml)                                                  |                                                 29244| 28810 +- 0.6% (10 datapoints)                                         |-1.5%                   |No Change            |
|[memtier_benchmark-1key-zset-10-elements-zrange-all-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-10-elements-zrange-all-elements.yml)                                        |                                                 65493| 64841 +- 0.9% (10 datapoints)                                         |-1.0%                   |No Change            |
|[memtier_benchmark-1key-zset-10-elements-zrange-all-elements-long-scores](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-10-elements-zrange-all-elements-long-scores.yml)                |                                                 81764| 81104 +- 0.5% (10 datapoints)                                         |-0.8%                   |No Change            |
|[memtier_benchmark-1key-zset-100-elements-zrange-all-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-100-elements-zrange-all-elements.yml)                                      |                                                 17921| 17569 +- 0.3% (10 datapoints)                                         |-2.0%                   |No Change            |
|[memtier_benchmark-1key-zset-100-elements-zrangebyscore-all-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-100-elements-zrangebyscore-all-elements.yml)                        |                                                 17942| 17578 +- 0.1% (9 datapoints)                                          |-2.0%                   |No Change            |
|[memtier_benchmark-1key-zset-100-elements-zrangebyscore-all-elements-long-scores](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-100-elements-zrangebyscore-all-elements-long-scores.yml)|                                                 39330| 38568 +- 2.3% (10 datapoints)                                         |-1.9%                   |No Change            |
|[memtier_benchmark-1key-zset-100-elements-zscan](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-100-elements-zscan.yml)                                                                  |                                                 51038| 52711 +- 1.7% (10 datapoints)                                         |3.3%                    |potential IMPROVEMENT|
|[memtier_benchmark-1key-zset-1K-elements-zrange-all-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-1K-elements-zrange-all-elements.yml)                                        |                                                  2636| 2629 +- 0.7% (10 datapoints)                                          |-0.3%                   |No Change            |
|[memtier_benchmark-1key-zset-1M-elements-zcard-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-1M-elements-zcard-pipeline-10.yml)                                            |                                                752988| 735102 +- 0.7% (12 datapoints)                                        |-2.4%                   |No Change            |
|[memtier_benchmark-1key-zset-1M-elements-zrevrange-5-elements](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-1M-elements-zrevrange-5-elements.yml)                                      |                                                108128| 106792 +- 1.0% (9 datapoints)                                         |-1.2%                   |No Change            |
|[memtier_benchmark-1key-zset-1M-elements-zscore-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-zset-1M-elements-zscore-pipeline-10.yml)                                          |                                                644473| 636704 +- 1.0% (12 datapoints)                                        |-1.2%                   |No Change            |
|[memtier_benchmark-2keys-lua-eval-hset-expire](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-lua-eval-hset-expire.yml)                                                                      |                                                 63751| 63906 +- 1.3% (9 datapoints)                                          |0.2%                    |No Change            |
|[memtier_benchmark-2keys-lua-evalsha-hset-expire](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-lua-evalsha-hset-expire.yml)                                                                |                                                 72878| 71972 +- 1.5% (10 datapoints)                                         |-1.2%                   |No Change            |
|[memtier_benchmark-2keys-set-10-100-elements-sdiff](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-set-10-100-elements-sdiff.yml)                                                            |                                                 20601| 20038 +- 1.0% (10 datapoints)                                         |-2.7%                   |No Change            |
|[memtier_benchmark-2keys-set-10-100-elements-sinter](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-set-10-100-elements-sinter.yml)                                                          |                                                 61584| 58869 +- 0.6% (8 datapoints)                                          |-4.4%                   |potential REGRESSION |
|[memtier_benchmark-2keys-set-10-100-elements-sunion](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-set-10-100-elements-sunion.yml)                                                          |                                                 25841| 25759 +- 0.8% (8 datapoints)                                          |-0.3%                   |No Change            |
|[memtier_benchmark-2keys-stream-5-entries-xread-all-entries](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-stream-5-entries-xread-all-entries.yml)                                          |                                                 56887| 56701 +- 0.6% (9 datapoints)                                          |-0.3%                   |No Change            |
|[memtier_benchmark-2keys-stream-5-entries-xread-all-entries-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-stream-5-entries-xread-all-entries-pipeline-10.yml)                  |                                                102470| 103494 +- 0.6% (8 datapoints)                                         |1.0%                    |No Change            |
|[memtier_benchmark-2keys-zset-300-elements-skiplist-encoded-zunion](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-zset-300-elements-skiplist-encoded-zunion.yml)                            |                                                  2951| 2950 +- 1.1% (12 datapoints)                                          |-0.0%                   |No Change            |
|[memtier_benchmark-2keys-zset-300-elements-skiplist-encoded-zunionstore](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-2keys-zset-300-elements-skiplist-encoded-zunionstore.yml)                  |                                                  3467| 3385 +- 1.6% (9 datapoints)                                           |-2.4%                   |No Change            |
|[memtier_benchmark-3Mkeys-load-string-with-512B-values](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-3Mkeys-load-string-with-512B-values.yml)                                                    |                                                108689| 108250 +- 1.3% (8 datapoints)                                         |-0.4%                   |No Change            |
|[memtier_benchmark-connection-hello](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-connection-hello.yml)                                                                                          |                                                109803| 107841 +- 1.2% (9 datapoints)                                         |-1.8%                   |No Change            |

</details>
"
2427131932,13592,moticless,2024-10-21T16:16:46Z,"> I remember you implemented `sflush` command, do you update db keysizes info after flushing specific slot dict?

@ShooterIT ,
Nice catch! Actually there is no need to special care because **currently** sflush is degenerated that only if it covers all the slots it is applied.
"
850821078,9003,oranagra,2021-05-29T11:59:34Z,"@sundb AFAIR you started to investigate that to solve the `replica buffer don't induce eviction` test, but i see it fails in this PR.
is that related or unrelated to this test?"
850822657,9003,sundb,2021-05-29T12:12:42Z,"@oranagra This pr solves the problem of ``replica buffer don't induce eviction`` test, but now the test fails because my modification causes the memory to cause the slave's querybuf to no longer be shrunk, and when the slave is killed, ``delta_no_repl`` does not count the memory added by the querybuf, I'm still testing."
850968312,9003,oranagra,2021-05-30T09:18:33Z,"Let me see if i get the the full story here.

by default when we read a query we use:
```c
    readlen = PROTO_IOBUF_LEN;
....
    c->querybuf = sdsMakeRoomFor(c->querybuf, readlen);
```
PROTO_IOBUF_LEN is 16k, and sdsMakeRoomFor being greedy doubles that.
so that, together with the (not so recent) change in ~~#7864~~ #7875. caused the sds size to be 48k (instead of 32k which it was before)

so when it was combined with this check (PROTO_MBULK_BIG_ARG is set to 32k, and using `>`) and avoid shrinking the default buffer size.
```c
    if (querybuf_size > PROTO_MBULK_BIG_ARG &&
```

so in fact this is a ""regression"" from ~~#7864~~ #7875."
850969819,9003,sundb,2021-05-30T09:30:38Z,"@oranagra It's introduced by #7875.
sdsMakeRoomFor will change sds size to be 40k(http://jemalloc.net/jemalloc.3.html#size_classes)."
850973668,9003,oranagra,2021-05-30T10:03:04Z,"ohh, yeah, that's the PR i meant (took the wrong one since they had a similar title)"
853744156,9003,oranagra,2021-06-03T09:54:05Z,"i take it back, this is not a regression from #7875.
before 7875, we used to ask for 16k, and the greenness of sdsMakeRoomFor gave us 32k, but since serverCron uses sdsAllocSize rather than sdsalloc (what #5013 wants to solve), it would readk 32k+6, so the fact the code there uses `>` and not `>=` doesn't help.

so even before 7875 we would have shrieked the query buff right after it was allocated."
853751174,9003,sundb,2021-06-03T10:05:03Z,):8 It's been around for 9 years.
860165557,9003,oranagra,2021-06-13T07:14:52Z,@sundb @yoav-steinberg let me know if we're good to merge this one.
861096289,9003,sundb,2021-06-15T01:09:53Z,"@oranagra Sorry, I can't be in front of the PC these days.
I make the following changes."
861389835,9003,sundb,2021-06-15T10:37:26Z,@oranagra Yes.
1521198333,11907,vitarb,2023-04-25T06:04:45Z,"I have no objections to the code, however I find API design a bit convoluted.
Can we get rid of `GLOBAL|SHARD|PATTERN channel` part and use something like:
```
pubsub subscribers PATTERN
```
The only place where you are using `type` field is when determining dictionary to search, can't we search all of them and include  type of the subscription into response?"
1522172603,11907,hpatro,2023-04-25T17:40:54Z,"> I have no objections to the code, however I find API design a bit convoluted. Can we get rid of `GLOBAL|SHARD|PATTERN channel` part and use something like:
> 
> ```
> pubsub subscribers PATTERN
> ```
> 
> The only place where you are using `type` field is when determining dictionary to search, can't we search all of them and include type of the subscription into response?

Filtering on the server side would be beneficial for the clients as it would lead to smaller payload (less network I/O) as well as less work on the client to determine the type of subscription. 
The client could always run multiple API call to get the output which you're suggesting.

```
MULTI
PUBSUB SUBSCRIBERS GLOBAL
PUBSUB SUBSCRIBERS SHARD
PUBSUB SUBSCRIBERS PATTERN
EXEC
```"
1535336485,11907,madolson,2023-05-04T19:57:55Z,"> The only place where you are using type field is when determining dictionary to search, can't we search all of them and include type of the subscription into response?

I think I agree with this. We could just have three separate fields in the response. We don't care that much about network since this is a debugging command."
1535566747,11907,hpatro,2023-05-05T00:52:29Z,"From our internal discussion:

1. Have pagination (cursor/count) to avoid returning all of the response at once. There is still a case of too many clients connected to a single channel/pattern. It's a nested response, so won't be able to add pagination on it.
2. Exact matching would be difficult to figure out if the application subscription is not already determined. Have glob based matching pattern as an optional parameter.

New structure (All parameters are optional):
```
PUBSUB SUBSCRIBERS [CURSOR cursor] [COUNT count] [MATCH pattern] [TYPE CHANNEL|PATTERN|SHARD-CHANNEL]
```

Default parameter value:
1. Type: `channel`
2. Match: `*`
3. Count: `10`
4. Cursor: `0`

Response:

```
127.0.0.1:6379> PUBSUB SUBSCRIBERS
1) (integer) 0
2) 1) ""ch1""
   2) 1) ""subscribed-clients""
      2) 1) 1) ""name""
            2) (nil)
            3) ""id""
            4) ""5""
            5) ""addr""
            6) ""127.0.0.1:52728""
            7) ""subscription_type""
            8) ""channel""
```"
1535567494,11907,hpatro,2023-05-05T00:54:10Z,"@vitarb @madolson Please have a look at the new update. 

> I think I agree with this. We could just have three separate fields in the response. We don't care that much about network since this is a debugging command.

Responding with all three subscription type together would cause confusion for cursor/count. Let me know what you think of the above solution."
1542555382,11907,hpatro,2023-05-10T17:21:10Z,@madolson Could you give it another look ?
1542563200,11907,hpatro,2023-05-10T17:27:32Z,@guybe7 reply-schemas-linter is failing with no error message. Ran it locally don't see any issue. Could you please take a look?
1557013851,11907,oranagra,2023-05-22T11:02:57Z,"i'm not certain the COUNT feature is very useful (considering it doesn't allow gradual iteration, just a sneak peek)"
1557038712,11907,soloestoy,2023-05-22T11:19:36Z,"> Code looks good to me now. @redis/core-team Here is a relatively small extension to pubsub that makes it easier to figure out what clients are registered. This came out of an issue we saw in AWS were a user wasn't sure why pubsub was taking so long and they only had the raw counts and weren't sure which clients were listening. Please review.

Some users in Alibaba Cloud have the problem too, this feature is very useful."
1558013330,11907,madolson,2023-05-22T21:11:42Z,"> I'm not certain the COUNT feature is very useful (considering it doesn't allow gradual iteration, just a sneak peek)

We had some internal conversation about this. The main goal was to prevent accidentally dumping like 10 million records for highly used clusters. Adding a cursor is complex and once you have a rough count, you can follow up on individual channels, which are more important for high use cases which is what we were looking for. The API we designed specifically solves the case we saw in AWS, but if there are other cases we could try to handle them."
1560247548,11907,madolson,2023-05-23T23:17:57Z,"Discussed in core team, Yossi will give a second review."
1572406656,11907,hpatro,2023-06-01T16:48:36Z,"Hello @yossigo,

Would you be able to take a look ?"
1577194007,11907,hpatro,2023-06-05T17:29:18Z,"Hi @yossigo, Thanks for taking a look. 

I did think about this from `CLIENT LIST` perspective. Here are the few things due to which I made it as part of `PUBSUB`.

* Through the `CLIENT LIST` we could expose the pubsub channel(s) which an individual client has subscribed to. Further an application would need to collate the result(s) to understand the activity of a given channel. 
With the above proposed API, it would be rather straight forward to gather all the subscribers for a given channel. 

* `CLIENT LIST` is non RESP friendly and didn't want to bring in more complexity to it by adding another level of nested elements int it.

* `PUBSUB SUBSCRIBERS` currently only exposes three information about a client. But if the need arises we could always extend it with more information. It's a two way door. (similar to `SLOWLOG GET`)."
1589616666,11907,madolson,2023-06-13T16:09:36Z,"We want to consider a new variant of `CLIENT LIST` that provides better introspection, see https://github.com/redis/redis/issues/12311. "
1594990879,11907,madolson,2023-06-16T17:04:22Z,"@yossigo I also want to follow up on a quick conversation we had during the meeting. Basically the client list filter is still not sufficient for deep introspection, since it doesn't help find the channels with a large number of connected clients. Do you have thoughts about simplifying this command so that is just prints out the list of channels with the number of subscribes to each? "
1596085563,11907,yossigo,2023-06-18T10:14:45Z,"@madolson I think counting clients per *something* is a generic troubleshooting use case and not specific to channels. So, it could make sense to handle it as a `CLIENT` command, which would resemble something like `CLIENT COUNT [BY-CHANNEL | BY-USER | BY-ADDR]`. Is that generic and common enough to be handled server-side? Not sure."
1597799611,11907,madolson,2023-06-19T21:51:10Z,"@yossigo That assumes you know *WHAT* channel you are looking for, which was not the use case mentioned here. The solution you are suggesting be asking for would like a join, since you want to find the top channels, then find the clients that are listening to them top channels."
1598320308,11907,yossigo,2023-06-20T08:12:33Z,"@madolson Not necessarily, I was pointing out that we could extend `CLIENT` to handle grouping and aggregation on the server side, and theoretically apply that to more than the channels use case."
1598854199,11907,madolson,2023-06-20T14:00:00Z,@yossigo That sounds so much more complex. I suppose in either case let's walk through the implementation and see if we can implement it so that it is simple. Which I started documenting here: https://github.com/redis/redis/issues/12311
1631183063,11907,hpatro,2023-07-11T17:06:23Z,The current inclination of the @redis/core-team is to have a singular command i.e. `CLIENT` command to provide tooling for admin to debug. Marking this as closed. Further updates will be discussed in #12311
1820687187,12742,oranagra,2023-11-21T10:55:46Z,"@jhershberg does the last force-push contains only rebase, or any other changes?"
1821279923,12742,madolson,2023-11-21T16:41:30Z,Full cluster tests: https://github.com/redis/redis/actions/runs/6946918547
1822141663,12742,oranagra,2023-11-22T05:33:03Z,Full CI: https://github.com/redis/redis/actions/runs/6953319248
1822354103,12742,oranagra,2023-11-22T08:58:18Z,"i think we should make an effort to protect clusterState and clusterNode etc from being used outside cluter_legacy.c, so that people won't try to access them from other C files.
e.g. see [this](https://github.com/redis/redis/pull/12773/commits/63dd560fea2ae591242be0f1e7f5bc90fe251f5d) quick ""fix"" for [this](https://github.com/redis/redis/actions/runs/6954666820/job/18921924091) rebase issue.

the two options i see are:
1. fold cluster_legacy.h into the top of cluster_legacy.c
2. add some `#error` in cluster_legacy.h when it's included from any other file.

the main difference between the two, is that if we'll ever want to split cluster_legacy.c into several files, or for some reason access it's internals from another file, we need to take the second solution.

@madolson WDYT?"
1839818033,12742,madolson,2023-12-05T00:57:10Z,"@oranagra I think your option (1), making them opaque by moving the type definition into cluster_legacy.c, is a good idea. I've never cared about how large a file is as long as it's logically coherent."
1840185250,12742,oranagra,2023-12-05T07:51:24Z,@jhershberg can you make a PR for that? (move cluster_legacy.h into the c file)
1123682772,10636,oranagra,2022-05-11T12:19:35Z,@redis/core-team please approve. see top comment.
1127256092,10636,oranagra,2022-05-16T06:07:30Z,approved in a core-team meeting.
1183629564,10969,oranagra,2022-07-13T20:08:20Z,"@MeirShpilraien didn't we agree to fix spop in a breaking manner?
or did we say that we'll first fix it in a non-breaking manner, and then break it in a later version?"
1183638108,10969,MeirShpilraien,2022-07-13T20:18:45Z,"@oranagra we said that he will first fix it without breaking so we can cherrypick this to 7.0 if we want to. We will fix it proparly after, in anothet RP."
1184644132,10969,oranagra,2022-07-14T16:25:08Z,"Ohh, I forgot. Better outline that plan in the top comment. 
@yossigo do you think we can put this in 7.0?"
1198156372,10969,oranagra,2022-07-28T13:39:02Z,"@yossigo please take another look.
we attempted to include only the bugfix in this PR without fixing other things that can be considered a breaking change.
along the way a few other improvements were made and some mechanism was introduced.
we need to decide if it's safe to backport to 7.0, maybe after a few weeks in unstable.
maybe try to make a review sweep that ignores all the comments and tests, and just consider the actual changes that have a risk of breaking things..."
1211806014,10969,oranagra,2022-08-11T10:26:53Z,"@redis/core-team not really a major-decision since there's no interface change, but it's a sensitive area so maybe one of you wants to review it too.
besides merging it to unstable, i'd also like to consider backporting it to 7.0, so that's probably more of a major-decision, so if you have any opinion, please comment.
either way, we better merge it to unstable to let it age a bit there, so i'd like to merge it soon."
1214391528,10969,oranagra,2022-08-14T14:37:55Z,"the PR **doesn't** include a breaking change (just a bug fix).
unless someone wrote code that actually relies on the bug (but that can be true for any bug fix).
it is however sensitive area, so maybe it introduces a new bug."
1223771006,10969,MeirShpilraien,2022-08-23T08:59:33Z,"Notice that we have an occasional test failure because of the changes on this PR: https://github.com/redis/redis/runs/7927670765?check_suite_focus=true

The issue is that if there is a lazy expire during the command invocation, the `del` command is added to the replication stream after the command placeholder. So the logical order on the primary is:
* Delete the key (lazy expiration)
* Command invocation

But the replication stream gets it the other way around:
* Command invocation (because the command is written into the placeholder)
* Delete the key (lazy expiration)

So if the command write to the key that was just lazy expired we will get inconsistency between primary and replica.

The following test can constantly reproduce the issue:
```
start_server {tags {""repl external:skip""}} {
    set master [srv 0 client]
    set master_host [srv 0 host]
    set master_port [srv 0 port]
    $master debug SET-ACTIVE-EXPIRE 0
    start_server {} {
        set slave [srv 0 client]
        $slave debug SET-ACTIVE-EXPIRE 0
        $slave slaveof $master_host $master_port

        test ""Tes replication with lazy expire"" {
            # wait for replication to be in sync
            wait_for_condition 50 100 {
                [lindex [$slave role] 0] eq {slave} &&
                [string match {*master_link_status:up*} [$slave info replication]]
            } else {
                fail ""Can't turn the instance into a replica""
            }

            $master sadd s foo
            $master expire s 1
            after 1000
            $master sadd s foo
            assert_equal 1 [$master wait 1 1]

            assert_equal ""set"" [$master type s]
            assert_equal ""set"" [$slave type s]
        }
    }
}
```

One solution we considered is to add another lazy expire replication stream and write all the lazy expire there. Then when replicating, we will replicate the lazy expire replication stream first. This will solve this specific test failure but we realise that the issues does not ends here and the more we dig the more problems we find. One of the example we thought about (that can actually crashes Redis) is as follow:
1. User perform `SINTERSTORE`
2. When Redis tries to fetch the second input key it triggers lazy expire
3. The lazy expire trigger a module logic that deletes the first input key
4. Now Redis hold the `robj` of the first input key that was actually freed

We believe we took the wrong approach here and we would like to suggested a new approach:
1. **Modules should not perform any writes during key space notifications**, we should document it and make it clear to any module writer.
2. Module might still want to perform writes as a result of a key space notification, and the requirement is that it will be atomic along side the command that triggered the notification, for this we will introduce a new post command hook that will allow the module to run whatever logic it wants (including writes) **after** the command was finished. The logical order and the replication order will be the same because any writes logic was invoke **after** the command was finished and there is not risk in changing the key space while the command is running. A module will be able to register to key space notifications, collect them, and only react on them on the post command hook.

We are planing to revert this PR and implements this post command hook idea on another PR."
1131214187,10747,guybe7,2022-05-19T05:23:52Z,Why would an application execute these commands on a non existing group? Sounds like a bug in the application
1132719081,10747,maor-rozenfeld,2022-05-20T10:02:36Z,"> Why would an application execute these commands on a non existing group? Sounds like a bug in the application

I'm not familiar with redis guidelines on automatic creation of resources, but at least in streams we can see that things are created automatically. For example, you can use [XADD](https://redis.io/commands/xadd/) to add a message to a stream even if said stream doesn't exist. Is that a bug in the application? Of course not.

So a stream is automatically created on some of the stream related commands. A consumer is also created automatically on [XREADGROUP](https://redis.io/commands/xreadgroup/). I'd expect the same behavior to apply to consumer groups.

This allows the application to avoid having to write recovery code to deal with a cluster reset (i.e. recreate all the consumer group), and to not worry about creating the groups on startup if they don't exist.
"
1133837078,10747,oranagra,2022-05-22T07:35:54Z,"the fact that XADD creates a key is consistent with the rest of redis (same as LPUSH, HSET, etc).
looking at all the stream commands, it seems that consumers are created implicitly and groups must be created explicitly.
i'm not sure i understand why maybe @itamarhaber or @guybe7 do.
in any case, maybe if we want to change that, in a backwards compatible way, we need to add an MKGOUP argument (like the MKSTREADM arg we have in XGROUP CREATE"
1136253763,10747,hwware,2022-05-24T17:43:17Z,"> the fact that XADD creates a key is consistent with the rest of redis (same as LPUSH, HSET, etc). looking at all the stream commands, it seems that consumers are created implicitly and groups must be created explicitly. i'm not sure i understand why maybe @itamarhaber or @guybe7 do. in any case, maybe if we want to change that, in a backwards compatible way, we need to add an MKGOUP argument (like the MKSTREADM arg we have in XGROUP CREATE

@maor-rosenfeld I agree with what Oran suggests, we could add an MKGOUP argument for the XREADGROUP.  Of course, we could add another standlone command to check if the group exists. How do you think? Thanks"
1136889256,10747,guybe7,2022-05-25T07:28:10Z,"IMHO it makes sense for `XADD` and `XGROUP CREATE` to have the ability to create a stream because in their nature they are write-commands.
it feels a bit weird for a read-command such as `XPENDING` or `XREADGROUP` to create a group (also, where does it stop? why not `XCLAIM`, XAUTOCLAIM`, etc.?)"
1136891933,10747,guybe7,2022-05-25T07:31:08Z,"having said that, if the fact that some stream ""read"" commands don't create consumer groups makes it difficult for users we can go ahead with the optional MKSTREAM, but let's do it for all groups-related commands and in the future, we will make sure to always create the group if needed (i.e. get Redis will not reply with ""no such group"" in all future commands)"
1136933872,10747,maor-rozenfeld,2022-05-25T08:15:15Z,"MKGROUP sounds good to me. Creating a group on read might look weird, but a consumer group is essentially a read entity by nature. So it's only natural that the groups will be created automatically on read commands."
1138387479,10747,oranagra,2022-05-26T10:27:12Z,ok. let's do that for 7.2
1138590740,10747,hwware,2022-05-26T13:39:02Z,"> MKGROUP sounds good to me. Creating a group on read might look weird, but a consumer group is essentially a read entity by nature. So it's only natural that the groups will be created automatically on read commands.

I will update this PR for adding an extra optional parameter MKGROUP for XPENGDING and XREADGROUP commands, Thanks"
1142254567,10747,hwware,2022-05-31T15:03:50Z,"@guybe7 @maor-rosenfeld @oranagra 
I already finsih the code part for the new command option, please take a look and later I will add test cases for them, Thanks"
1145812662,10747,maor-rozenfeld,2022-06-03T10:12:20Z,"> @guybe7 @maor-rosenfeld @oranagra I already finsih the code part for the new command option, please take a look and later I will add test cases for them, Thanks

I'm not familiar with redis code but it looks good to me. Do you think the same should happen with XCLAIM and XAUTOCLAIM?"
1147521528,10747,hwware,2022-06-06T14:37:27Z,"> XAUTOCLAIM

@guybe7 @maor-rosenfeld When I read the doc of command XAUTOCLAIM , I notice it says that Conceptually, XAUTOCLAIM is equivalent to calling [XPENDING] and then [XCLAIM].
Personally,  I think it makes sense adding similar function which create consumer group if it doesn't exists.

Beside this, I have another idea that: 
add a new command:  XEXIST key group -- which indicates if there is one consumer group for one specific key, return 1 if it exists and return 0 if not. 
Thus, it provides client side has another option to run the command: XPENDING, XREADGROUP, XCLAIM, XAUTOCLAIM without adding the MKGROUP option.  Thanks

How do you think?

"
1148484653,10747,maor-rozenfeld,2022-06-07T10:27:00Z,"> > XAUTOCLAIM
> 
> @guybe7 @maor-rosenfeld When I read the doc of command XAUTOCLAIM , I notice it says that Conceptually, XAUTOCLAIM is equivalent to calling [XPENDING] and then [XCLAIM]. Personally, I think it makes sense adding similar function which create consumer group if it doesn't exists.
> 
> Beside this, I have another idea that: add a new command: XEXIST key group -- which indicates if there is one consumer group for one specific key, return 1 if it exists and return 0 if not. Thus, it provides client side has another option to run the command: XPENDING, XREADGROUP, XCLAIM, XAUTOCLAIM without adding the MKGROUP option. Thanks
> 
> How do you think?

The problem with commands like XEXIST is that they're begging you to implement an unsafe operation in your code (XEXIST and then XGROUP CREATE). Such a command will be useless without a distributed lock which not everyone are using.

I agree that both XCLAIM and XAUTOCLAIM should create a group using the same MKGROUP flag."
1148685636,10747,hwware,2022-06-07T13:35:21Z,"> > > XAUTOCLAIM
> > 
> > 
> > @guybe7 @maor-rosenfeld When I read the doc of command XAUTOCLAIM , I notice it says that Conceptually, XAUTOCLAIM is equivalent to calling [XPENDING] and then [XCLAIM]. Personally, I think it makes sense adding similar function which create consumer group if it doesn't exists.
> > Beside this, I have another idea that: add a new command: XEXIST key group -- which indicates if there is one consumer group for one specific key, return 1 if it exists and return 0 if not. Thus, it provides client side has another option to run the command: XPENDING, XREADGROUP, XCLAIM, XAUTOCLAIM without adding the MKGROUP option. Thanks
> > How do you think?
> 
> The problem with commands like XEXIST is that they're begging you to implement an unsafe operation in your code (XEXIST and then XGROUP CREATE). Such a command will be useless without a distributed lock which not everyone are using.
> 
> I agree that both XCLAIM and XAUTOCLAIM should create a group using the same MKGROUP flag.

I got your point, I will implement this parameter in XCLAIM and XAUTOCLAIM command, Thanks for your explanation!"
1151512225,10747,hwware,2022-06-09T19:13:12Z,"@oranagra @guybe7 All codes and test cases are finished, please take a look Thanks a lot. "
1153085601,10747,oranagra,2022-06-12T06:47:07Z,"@hwware thanks.
we'll have to revisit that when approaching 7.2 (can't make that change in 7.0)"
1153936143,10747,hwware,2022-06-13T13:46:04Z,"> @hwware thanks. we'll have to revisit that when approaching 7.2 (can't make that change in 7.0)

No problem, Thanks Oran"
1284481726,10747,hwware,2022-10-19T19:34:49Z,"@oranagra Code is updated, please check them, Thanks"
1397129368,10747,hwware,2023-01-19T15:13:12Z,"@oranagra Hi Oran, could you please take sometime to check my latest update?  Thanks a lot"
1596653282,10747,oranagra,2023-06-19T07:26:23Z,"@hwware please avoid rebase and force-push, it makes it hard to know what changed and do incremental reviews.
please let me know which commits represent new content since my last review, and please go over the unresolved comments and either respond or resolve them."
1839175717,10747,hoyhoy,2023-12-04T17:55:38Z,"We need this for testing our app.  Our unit tests rely on flushed database and without a MKGROUP, this causes them all to fail.  As it is now, we have to FLUSH and then issue five CREATE GROUPs.  It would be good for production as well as it would make initialization more fault-tolerant.  

It is very odd indeed that STREAMs are the only data type that require you to create a GROUP before issuing the read commands -- SETs, ZSETs, LISTs, and HASHes don't require anything like this."
1841603850,10747,hwware,2023-12-05T21:00:27Z,"Hi Oran,

I have updated the code and also addressed the comments and replied. And I have even updated json file for 8.0.
here are the commit details which had covered the comments.
https://github.com/redis/redis/pull/10747/commits/1693d83dc45245d49a0ac42fd621361d6e0d6e85
https://github.com/redis/redis/pull/10747/commits/5ca566454cbb87d749e3d23b4773642ab1c87429
https://github.com/redis/redis/pull/10747/commits/765129339487770b95823d8d9499eed9be0cef74"
850952157,8999,oranagra,2021-05-30T06:53:22Z,"@chenyang8094 I really don't like the (ab)use of server event here, this is not an event (like client disconnecting or role change, etc).

I certainly agree that we need to solve this though, and provide the information to the module in some way.
i think it's relatively easy to add add that to AOF and RDB  saving / loading etc, by following the footsteps of `RM_GetKeyNameFromIO`, and also for some other cases by following the footsteps of `RM_GetKeyNameFromModuleKey`.

I suppose that for `digest` and `defrag` it may not be critical, but since they both take some kind of a context, we can create a similar mechanism.

Sadly, i don't currently have an idea of how to solve it for the `free`, `unlink` and `free_effort`, `copy`, `mem_usage` etc.
Pity we didn't think of that when we recently created `unlink`, `free_effort`, and `copy` (when we had the idea / concern of adding the key name in them we should have added dbid too).

BTW, ironically, IIRC when we created the `unlink` you argued against adding the keyname. 8-)

@redis/core-team please respond if you have any idea how to better resolve this concern, or state what's your preference."
851084409,8999,chenyang8094,2021-05-31T00:06:17Z,"> @redis/core-team @oranagra 

Yes, I very much agree. I also think the current implementation is ugly. Adding an api similar to RedisModule_GetKeyNameFromIO was also what I first thought of. In fact, I first modified it in this way, on another branch, but because I couldn’t solve` free, unlink and free_effort, copy, mem_usage` etc., so I Give up that plan.

I also express a pity that the `unlink` did not directly bring in dbid or use RedisModuleKey. There are many usage scenarios for modules, but our module api was designed without too much consideration for compatibility and scalability.In other words, it is really difficult to design the APIs correctly at the beginning.

Since these APIs such as `free, unlink, free_effort, copy, mem_usage` do not have specific ctx parameters, apart from using event or we separately expose an API such as `getCurrentUnlinkCtx`, I can't think of other solutions at present. This is really a thorny issue, and it has bothered me for a long time.
"
851196814,8999,oranagra,2021-05-31T05:46:27Z,"Maybe one brave alternative would be to completely deprecate the existing `RM_CreateDataType` in favor of a new API with a better set of callbacks.
Or maybe slightly better idea is to add new overlapping callbacks to the exiting API.
i.e. in addition to the old `free` callback that takes only a pointer, one would be able to register a `free2` callback that takes an additional key name and dbid.
A module in that case would be able to create a v4 struct, with both `free` and `free2` pointers, so that old redis versions would use the `free` pointer, and new ones will use `free2`.

@guybe7 and @MeirShpilraien please also share your thoughts / preference and if you happen to have a better idea."
851279236,8999,MeirShpilraien,2021-05-31T07:45:29Z,"@oranagra I think that the free2 idea is good but I would not lock the callback signature just to key name and dbid. Instead, I would give an opaque object and give an API to extract information from this opaque object such as dbid or key name. This way, if in the future, we will want to add more information to those callbacks we will not have o break the API, WDYT?"
852057965,8999,guybe7,2021-06-01T11:44:10Z,"i agree that it's about time to solve the missing-info-in-callbacks and having a new structure to hold the new callbacks is probably the way to go. we need to dedicate some thought to how we want the callbacks signatures to look like. generally speaking we can always provide some opaque structure + functions to extract information from it. in that case this struct could be the only argument for all callbacks.
another option is to have multiple args to each callback, but all of them should be opaque. e.g. `free` will take RedisModuleCtx* and RedisModuleKey*.
i tend to prefer the first approach because it's more flexible. on the other hand it's a bit less intuitive (the user has to be familiar with all data extraction functions + it's uncertain if each callback can support all of them) and it's more work."
853454307,8999,chenyang8094,2021-06-02T23:49:39Z,"@oranagra @MeirShpilraien @guybe7  Thank you for your review and very good suggestions. The following are the prototypes of all current callbacks:
```
typedef void *(*moduleTypeLoadFunc)(struct RedisModuleIO *io, int encver);
typedef void (*moduleTypeSaveFunc)(struct RedisModuleIO *io, void *value);
typedef int (*moduleTypeAuxLoadFunc)(struct RedisModuleIO *rdb, int encver, int when);
typedef void (*moduleTypeAuxSaveFunc)(struct RedisModuleIO *rdb, int when);
typedef void (*moduleTypeRewriteFunc)(struct RedisModuleIO *io, struct redisObject *key, void *value);
typedef void (*moduleTypeDigestFunc)(struct RedisModuleDigest *digest, void *value);
typedef size_t (*moduleTypeMemUsageFunc)(const void *value);
typedef void (*moduleTypeFreeFunc)(void *value);
typedef size_t (*moduleTypeFreeEffortFunc)(struct redisObject *key, const void *value);
typedef void (*moduleTypeUnlinkFunc)(struct redisObject *key, void *value);
typedef void *(*moduleTypeCopyFunc)(struct redisObject *fromkey, struct redisObject *tokey, const void *value);
typedef int (*moduleTypeDefragFunc)(struct RedisModuleDefragCtx *ctx, struct redisObject *key, void **value);
```
Among them, the callbacks without the opaque structure parameter are:
```
typedef size_t (*moduleTypeMemUsageFunc)(const void *value);
typedef void (*moduleTypeFreeFunc)(void *value);
typedef size_t (*moduleTypeFreeEffortFunc)(struct redisObject *key, const void *value);
typedef void (*moduleTypeUnlinkFunc)(struct redisObject *key, void *value);
typedef void *(*moduleTypeCopyFunc)(struct redisObject *fromkey, struct redisObject *tokey, const void *value);
```
I want to add an opaque structure parameter named RedisModuleXxxCtx to these callbacks, such as the following:
```
typedef size_t (*moduleTypeMemUsageFunc)(struct RedisModuleMemUsageCtx *ctx, const void *value);
typedef void (*moduleTypeFreeFunc)(struct RedisModuleFreeCtx *ctx, void *value);
typedef size_t (*moduleTypeFreeEffortFunc)(struct RedisModuleFreeEffortCtx *ctx, struct redisObject *key, const void *value);
typedef void (*moduleTypeUnlinkFunc)(struct RedisModuleUnlinkCtx *ctx, void *value);
typedef void *(*moduleTypeCopyFunc)(struct RedisModuleCopyCtx *ctx, struct redisObject *fromkey, struct redisObject *tokey, const void *value);
```
I wonder if this is enough?"
853579307,8999,oranagra,2021-06-03T05:39:09Z,"i think that probably `free`, `freeEffort`, and `unlink` can share the same type of context (they're all different stages of the same operation).
and maybe we can even go further and say that all of these (5 callbacks) can share the same type since they're all operations on a key. i.e. something like `RedisModuleKeyOpCtx`, then one can wonder why we don't have that type as argument for the other callbacks (like defrag, and save), in theory we can, but since we don't have to, we may prefer not to change these."
853876358,8999,chenyang8094,2021-06-03T13:38:39Z,"Well, let me spend some time to modify this PR according to this plan."
854439605,8999,chenyang8094,2021-06-04T07:29:58Z,"@oranagra @MeirShpilraien @guybe7 @soloestoy   Please review this PR again, thanks :). 

```
    mem_usage -> mem_usage2;
    free_effort ->  free_effort2;
    unlink -> unlink2;
    copy -> copy2;
```
I did not refactor the `free` callback because it is too special. First of all, because `free` may be called in bio thread, we cannot get the **key** and **dbid** information in this context. Secondly, all the meta-information that free wants is available in `unlink2` now."
855357859,8999,oranagra,2021-06-06T08:06:33Z,"@chenyang8094 i agree, `free` should remain as is.
that's exactly why we added `unlink`, since that's the actual / logical point in time were the value is detached from redis.
`free` should only release memory."
856405519,8999,chenyang8094,2021-06-08T03:15:24Z,"@oranagra Please review my changes again, I have basically solved the problems you mentioned above. The major change is to rewrite the test module. In order to fully reflect and use these enhanced calllbacks, I constructed a very simple memory allocator. After all, if you want to construct db-related and key-related resources, the memory is one Very typical example."
859974014,8999,chenyang8094,2021-06-12T01:03:00Z,Resolve conflicts with the unsatble branch @oranagra @soloestoy 
862956397,8999,oranagra,2021-06-17T06:09:50Z,"@chenyang8094 we have failures in your test modules in daily CI, can you please look into it?

specifically in: libc malloc, 32 bit, alpine linux, and macos.
i.e. anything that's no jemalloc + 32bit.
https://github.com/redis/redis/runs/2844342808?check_suite_focus=true

(the errors in the valgrind run are unrelated, but they aborted the test before your new test got to run, so please try to run it with valgrind, so we don't need to wait for next daily to see)"
863426051,8999,oranagra,2021-06-17T17:27:32Z,should be fixed by #9102
887242778,8999,oranagra,2021-07-27T06:18:48Z,"we have another issue with a test that's introduced by this PR.
`datatype2: test rdb save and load` hang with valgrind.
https://github.com/redis/redis/runs/3166938069?check_suite_focus=true
i suppose this is some very rare timing issue.
usually these tests take 5 minutes with valgrind (10 seconds without), but in this run it was killed after some 25 minutes.
@chenyang8094 can you please look into it if you have time?
"
1001140171,8999,enjoy-binbin,2021-12-26T09:52:34Z,"```
# https://github.com/redis/redis/runs/4633836706?check_suite_focus=true#step:7:647
*** [err]: datatype2: test memusage in tests/unit/moduleapi/datatype2.tcl
Expected '12384' to be equal to '12400' (context: type eval line 14 cmd {assert_equal [r memory usage k1] [r memory usage k2] } proc ::test)
```

> test ""datatype2: test memusage"" {

This has failed several times recently (i am not able to create a review comment..., so i comment in here)
![image](https://user-images.githubusercontent.com/22811481/147404691-1fb5b6a9-a4bc-48c0-a2f8-f98a098a7c6a.png)
"
1001162852,8999,oranagra,2021-12-26T11:45:06Z,"yes, i've seen that test fail 3 times in the past week. i wonder what changed.
can someone look into it and debug that failure?"
1001169724,8999,sundb,2021-12-26T12:25:06Z,"I reverted to its first commit and it failed just as often, but it's been occurring particularly frequently lately.
When add `save ""` config, it no longer failed.
The main reason is that when creating k2 sds, zmalloc_size is normally 24, but sometimes it can be 40.
Not sure if this is in the form of memory fragmentation."
1001177837,8999,oranagra,2021-12-26T13:07:02Z,"since we don't count external fragmentation, and since the internal one should be completely predictable and consistent, i don't think that's it.. we probably need deeper debugging to figure it out."
1001304953,8999,sundb,2021-12-27T03:04:02Z,"@oranagra I don't mean that zmalloc_size contains fragmentation, but that malloc allocates more memory than expected when there is a lot of internal memory fragmentation.
BTW, this test only occurs when malloc in libc."
1001358250,8999,chenyang8094,2021-12-27T06:15:33Z,"I think this error is very strange, it is impossible from a code point of view.

`Expected '12384' to be equal to '12400' `  means that the memory occupied by the two keys differs by 16 bytes （They should actually be equal).

```
typedef struct MemAllocObject {
    long long size;
    long long used;
    uint64_t mask;
} MemAllocObject;

#define BLOCK_SIZE 4096
struct MemBlock {
    char block[BLOCK_SIZE];
    struct MemBlock *next;
};
```
The following calculation logic is impossible to have a difference of 16 bytes.
```
 size += sizeof(*o);
 size += o->size * sizeof(struct MemBlock);
```"
1001359802,8999,sundb,2021-12-27T06:19:33Z,"@chenyang8094 The 16-byte difference is not in the MemBlock, but in the two keys k1 k2.
There is a very small chance that the actual memory allocated will be 40 bytes, and we should expect 24 bytes.
BTW. The minimum allocation of malloc is 24, then 40.
```
key->ptr: k1
hdrlen+initlen+1: 4
usable: 40, 0x55dde15e1570        <- k1 is actually allocated 40 bytes
key->ptr: k2
hdrlen+initlen+1: 4
usable: 24, 0x55dde1603020        <- k2 is actually allocated 24 bytes
```"
1001361688,8999,chenyang8094,2021-12-27T06:24:28Z,"Okay, it doesn't seem to have anything to do with the datatype2 code. So we need to look at what PRs related to memory allocation have been merged recently?
```
  size_t usage = objectComputeSize(c->argv[2],dictGetVal(de),samples,c->db->id);
  usage += sdsZmallocSize(dictGetKey(de)); // here error 
  usage += sizeof(dictEntry);
```"
1001362125,8999,oranagra,2021-12-27T06:25:30Z,"> BTW, this test only occurs when malloc in libc.

ohh, i didn't realize that the sanitizer build is using libc malloc.
I don't know much about the internals of this allocator.
if it can return a different size of allocation on the same requested size, maybe our only way out is to exclude this test assertion when not using jemalloc."
1001362531,8999,oranagra,2021-12-27T06:26:42Z,"the only thing that was done recently in that respect is the introduction of the sanitizer CI, but even that was quite a while ago, and this test only started to fail a week ago AFAIK"
1001428093,8999,chenyang8094,2021-12-27T08:25:56Z,"I can reproduce this phenomenon with the following code:
```
#include <stdio.h>
#include <stdlib.h>
#include <malloc.h>
#include <time.h>

#define LOOP_CNT 10000000
#define MALLOC_SIZE 2
#define MIN_SIZE 1
#define MAX_SIZE 1024

int main(int argv, char **argc) {
    int normal_cnt = 0, strange_cnt = 0;
    srand((unsigned) time(NULL));
    for (int i = 0; i < LOOP_CNT; i++) {
        int rand_size = rand() % MAX_SIZE + MIN_SIZE;
        char *buf1 = malloc(rand_size);
        char *buf2 = malloc(MALLOC_SIZE);
        size_t bufsize = malloc_usable_size((void *) buf2);
        if (bufsize != 24) {
            strange_cnt++;
        } else {
            normal_cnt++;
        }
        free(buf2);
        free(buf1);
    }
    printf(""total: %d, normal_cnt: %d, strange_cnt: %d\n"", LOOP_CNT, normal_cnt, strange_cnt);
    return 0;
}
```

build and run:
```
$./malloc_test
total: 10000000, normal_cnt: 9999961, strange_cnt: 39
```"
1001431383,8999,oranagra,2021-12-27T08:31:08Z,i think we should just skip this check when `[s mem_allocator]` doesn't match `jemalloc*`
1001433143,8999,chenyang8094,2021-12-27T08:34:47Z,"OK, i will add it."
1001434979,8999,enjoy-binbin,2021-12-27T08:37:52Z,"```
        # with a right comment explain it
        if {[string match {*jemalloc*} [s mem_allocator]]} { assert_equal [r memory usage k1] [r memory usage k2] }
```


@chenyang8094 please also do some cleanups...
like:
```
line 129, extra line break
        assert_equal $data [r mem.read k2 0]

       
        r select 1

line 70/105 extra space
```
> I saw it when I was investigating the problem, and I was a little obsessive-compulsive.
> you may need to check the file again. "
1247122545,8999,JimB123,2022-09-14T18:01:30Z,"I know I'm commenting late on this update.  But as a whole, I find this update disturbing.

The problem is that fundamental APIs are being altered, and not necessarily in a good (planned) way.

As an example, consider `lazyfreeGetFreeEffort()`.

Originally, this function was defined as:
```C
size_t lazyfreeGetFreeEffort(robj *obj)
```
The purpose of this function is (as described in the comment) ""Return the amount of work needed in order to free an object"".  There is no implication that the object is a current entry in the database.  There is no implication that the object has an associated key.

Later, in this update ( https://github.com/redis/redis/pull/7912 ) the function is defined as:
```C
size_t lazyfreeGetFreeEffort(robj *key, robj *obj)
```
This update is logically a bug.  By adding a key, we are implying that they object is a current entry in the DB (which wasn't implied before).  The reason for this addition is solely for use by modules and implies the possibility of additional data (not included in the robj) which must be memory managed (freed) along with the robj.  But the key alone isn't enough to define an entry in the DB.  A DBID/KEY pair is needed.  So the function is redefined (again) in this update as:
```C
size_t lazyfreeGetFreeEffort(robj *key, robj *obj, int dbid)
```

This new function works in 2 ways.
* For everything BUT modules, only the `*obj` is used
* For modules, all 3 parameters are used

The API signature itself is weird.  `dbid` and `key` are associated, and should fundamentally be presented as a pair.  But `key` was added at the beginning, and `dbid` added at the end.  I would personally list `dbid` first, because `key` is a further subdivision after DB.  Note that if dbid/key were consistently referred to as a pair, it might have avoided the original logical bug where only the key was provided.

DBID/KEY implies a ROBJ.  So this API could really be an either/or type interface - and broken into 2 APIs.  EITHER dbid/key OR robj.  (It might be useful to provide an optional robj in the key case to prevent having to look it up again if the value is already available.)

Adding these (dbid/key) parameters necessitated changes in multiple layers of code to pass these values only for the module case.  In general, if parameters are added to a function only for the purpose of calling another function, this could be an indication that there is a design issue.

I would assert that since modules are intended to be modular, it could/should be up to the module whether async free is desired or not.  I suggest that a more ""modular"" mechanism for this would be to not even attempt to determine ""free effort"" for module items.  It shouldn't be up to the Redis engine to decide if a module item should be freed in a sync or async manner.  When it comes time to free a module item, the Redis core should always call the module's free function (synchronously) and let the module decide if it should perform the free in a sync or async manner.

A similar, change is made in `objectComputeSize()`.  Luckily, this function is only used in 1 place.  But it does the same things:
* Adds key and DBID to an existing function, only for the support of modules.  It would be better to have a separate function just for modules.
* DBID/KEY is a logical pair.  However, they are split in the parameter list with key at the beginning, and dbid at the end.
* As before, the dbid/key are only provided to be passed to another function.
"
1247488259,8999,chenyang8094,2022-09-15T02:15:01Z,"@JimB123 Essentially you are right, I think we can do better if we design all APIs from the ground up. But the problem is that the entire module api is very simple at the beginning of the design. For a data type, we often care about not only its value, but also its `keyname` and `dbid` meta-information (I believe a slightly more complex module need them). And these, in the previous API are missing state.
We put a lot of effort into patching these APIs (like the two PRs you mentioned, and things like #3650 ), even though they seem to make changes to the original internal APIs, but I think it's worth it. Even if the API is completely redesigned, we will find ways to pass this information to the module.

> I would personally list dbid first, because key is a further subdivision after DB. 

Regarding the problem that dbid is located after the key, I think one reason is because dbid is an appended parameter, and the other reason is the existing style of the redis source code. You can see similar usage in functions such as `notifyKeyspaceEvent`, `moduleNotifyKeyspaceEvent`, and `streamCreateConsumer`. I don't think there's anything wrong with that. A possible explanation is that the redis believes that the key is the most important information, and the dbid is the auxiliary information, which is meaningful only when it is needed.
"
2126243763,13285,sundb,2024-05-23T05:08:54Z,@moticless should the TODO in https://github.com/redis/redis/blob/a25b15392c3f50909ab2dafbcbdb3794ada29620/src/rdb.c#L2312:L2315 be handled here?
2126923630,13285,moticless,2024-05-23T11:54:15Z,"> @moticless should the TODO in https://github.com/redis/redis/blob/a25b15392c3f50909ab2dafbcbdb3794ada29620/src/rdb.c#L2312:L2315 be handled here?

I will handle it on the next commit. "
1489855209,11982,sundb,2023-03-30T07:53:32Z,"Testing the performance impact of `extend_to_usable()` with sds.

Test environment: Ubuntu 22.04, 8 Core, 16G mem.

Test code:
```c
    unsigned long long start = usec();
    for (int i = 0; i < 1000000000; i++) {
        sds s = sdsnew(""fooboo"");
        sdsfree(s);
    }
    printf(""Done. usec=%lld\n"", usec()-start);
```

Result:
```
# Unstable
Done. usec=22888887
```

```
# Test code with this PR:
Done. usec=27105710  (-18%)
```"
1489862975,11982,sundb,2023-03-30T07:59:04Z,"Test environment: Ubuntu 22.04, 8 Core, 16G mem.
benchmark command: memtier_benchmark --command=""lpush l fooboo"" --hide-histogram --test-time 30 -x 5 -c 1000 --pipeline=10

~~Although `extend_to_usable()` has 18% degration impact on the performance of `sdsnew()`, no significant impact is seen on real scenarios.~~ [Wrong result, please see the following]
"
1490577689,11982,siddhesh,2023-03-30T16:18:12Z,"As far as the block extension hint for gcc/clang is concerned, I think this looks OK; I can't comment on the coding conventions/style (edit: because I don't have experience with the redis code base).

I wonder what the overhead would be if the usable_size functions didn't return any additional memory, i.e. they returned the exact same size that was requested. Most allocators are quite efficient with resizing upwards if the new size is not too different from the earlier one. ISTM that the difference would be one allocator call vs usable_size call but not as a 1:1 replacement; there ought to be less allocator calls than usable_size calls."
1490623774,11982,oranagra,2023-03-30T16:53:01Z,"we actually had a recent change about using je_nallocx to avoid an excessive call to realloc for shrinking an allocation, in case it's bound to fall into the same allocation bin, see #11766.
not sure what's the reason, but that's an indication that a NO-OP realloc could still be expensive. "
1490631390,11982,oranagra,2023-03-30T16:59:22Z,"@sundb please describe at the top comment the implications of the problem (compilers, specifically new ones), would SIGABRT due to fortification checks.
if we can somehow narrow it down to specific redis versions and specific set of compilers that were vulnerable that would be nice (but i suppose we can't)."
1490648552,11982,siddhesh,2023-03-30T17:11:05Z,"> if we can somehow narrow it down to specific redis versions and specific set of compilers that were vulnerable that would be nice (but i suppose we can't).

I can answer the compilers part; it'll be most likely at `_FORTIFY_SOURCE=3` with all compilers that support it, i.e. clang 9.0 or later and gcc 12 or later alongside glibc 2.33 or later."
1490650107,11982,siddhesh,2023-03-30T17:12:12Z,"> we actually had a recent change about using je_nallocx to avoid an excessive call to realloc for shrinking an allocation, in case it's bound to fall into the same allocation bin, see #11766. not sure what's the reason, but that's an indication that a NO-OP realloc could still be expensive.

That PR seems to indicate a performance issue with jemalloc realloc, so maybe it's an unrelated issue?"
1491232292,11982,sundb,2023-03-31T03:16:08Z,"Updating the latest benchmarking results, it seems that the result of https://github.com/redis/redis/pull/11982#issuecomment-1489862975 is wrong.
As can be seen from the results, the overall performance degradation is ~1.7%.

memtier_benchmark --command=""lpush l fooboo"" --hide-histogram --test-time 600 -x 1 -c 1000 --pipeline=10

```
4         Threads
1000      Connections per thread
600       Seconds
```

```
# Unstable
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lpushs     627502.70        63.73494        53.50300       203.77500       261.11900     28007.77 
Totals     627502.70        63.73494        53.50300       203.77500       261.11900     28007.77 
```

```
# This PR (-1.66%)
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lpushs     617078.43        64.80224        55.55100       209.91900       264.19100     27539.51 
Totals     617078.43        64.80224        55.55100       209.91900       264.19100     27539.51
```

memtier_benchmark --command=""lpush l fooboo"" --hide-histogram --test-time 600 -x 1 -c 1000 --pipeline=100

```
# Unstable
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lpushs    1655179.95       241.59559       242.68700       415.74300       468.99100     74203.40 
Totals    1655179.95       241.59559       242.68700       415.74300       468.99100     74203.40 
```


```
# This PR (- 1.77%)
==================================================================================================
Type         Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
--------------------------------------------------------------------------------------------------
Lpushs    1625763.51       245.60826       244.73500       423.93500       479.23100     72857.61 
Totals    1625763.51       245.60826       244.73500       423.93500       479.23100     72857.61
```"
1491809190,11982,sundb,2023-03-31T11:50:05Z,"> @sundb please describe at the top comment the implications of the problem (compilers, specifically new ones), would SIGABRT due to fortification checks. if we can somehow narrow it down to specific redis versions and specific set of compilers that were vulnerable that would be nice (but i suppose we can't).

@oranagra This issue only affects after 7.0.3 (#11196).
we didn't add the `alloc_size` attribute before this, so GCC couldn't see the real memory size, which was confirmed by disassembling the binary of version 6.2.8.
@siddhesh Please help to confirm if my verification is correct.

```
_addReplyProtoToList() {
...
        tail = zmalloc(size + sizeof(clientReplyBlock));
        /* take over the allocation's internal fragmentation */
        tail->size = zmalloc_usable_size(tail) - sizeof(clientReplyBlock);
        tail->used = len;
        memcpy(tail->buf, s, len);
...
}
```

Assembly code:
```
   0x00000000004611c0 <+112>:   call   0x44f330 <zmalloc>
   0x00000000004611c5 <+117>:   mov    %rax,%rdi
   0x00000000004611c8 <+120>:   mov    %rax,%r13
   0x00000000004611cb <+123>:   call   0x55c4e0 <je_malloc_usable_size>
   0x00000000004611d0 <+128>:   mov    %rbx,0x8(%r13)
   0x00000000004611d4 <+132>:   lea    0x10(%r13),%rdi
   0x00000000004611d8 <+136>:   mov    %rbx,%rdx
   0x00000000004611db <+139>:   sub    $0x10,%rax
   0x00000000004611df <+143>:   mov    %r12,%rsi
   0x00000000004611e2 <+146>:   mov    %rax,0x0(%r13)
   0x00000000004611e6 <+150>:   call   0x434e00 <memcpy@plt>
```

Add `__attribute__((malloc)) __attribute__((alloc_size(1))) ` for zmalloc()
```
   0x0000000000461194 <+116>:   call   0x44f2c0 <zmalloc>
   0x0000000000461199 <+121>:   mov    %rax,%rdi
   0x000000000046119c <+124>:   mov    %rax,%r14
   0x000000000046119f <+127>:   call   0x55ca40 <je_malloc_usable_size>
   0x00000000004611a4 <+132>:   mov    %rbx,0x8(%r14)
   0x00000000004611a8 <+136>:   lea    0x10(%r14),%rdi
   0x00000000004611ac <+140>:   mov    %rbx,%rdx
   0x00000000004611af <+143>:   sub    $0x10,%rax
   0x00000000004611b3 <+147>:   mov    %r12,%rsi
   0x00000000004611b6 <+150>:   mov    %rax,(%r14)
   0x00000000004611b9 <+153>:   mov    $0x10,%eax
   0x00000000004611be <+158>:   cmp    %rax,%r13
   0x00000000004611c1 <+161>:   cmovae %r13,%rax
   0x00000000004611c5 <+165>:   mov    %rax,%rcx
   0x00000000004611c8 <+168>:   sub    $0x10,%rcx
   0x00000000004611cc <+172>:   call   0x434450 <__memcpy_chk@plt>
```"
1491816018,11982,siddhesh,2023-03-31T11:56:16Z,"> @oranagra This issue only affects after 7.0.3 (#11196). we didn't add the `alloc_size` attribute before this, so GCC couldn't see the real memory size, which was confirmed by disassembling the binary of version 6.2.8. @siddhesh Please help to confirm if my verification is correct.

Without `alloc_size` the compiler won't see the `zmalloc` functions as being allocators and hence will not add the fortification checks.  Note however that the other allocator functions (jemalloc, glibc malloc, etc.) do have the `alloc_size` attribute and if those get called (and their corresponding malloc_usable_size) then the compiler will see the allocation size and add fortification.

So the problem won't be gone, only papered over until a code change exposes it years later."
1492337118,11982,oranagra,2023-03-31T17:32:38Z,"right, even before 7.0.3, the compiler can see that zmalloc calls malloc, and be able to track the size of the allocation.
and we do use the memory reported by malloc_usable_size in many places for quite some time.. so it doesn't look like anything dramatic was changed in #11666 (which was what uncovered this).
i.e. the problem could be hiding in 6.2.0, and even before as well."
1493595798,11982,siddhesh,2023-04-03T03:36:51Z,"> @siddhesh regarding your comment about RM_Alloc, these functions use functions that do have the attribute down the line, so in theory the compiler could have still kept track of the allocation size.
> so what's preventing that is the fact that it's used in a different module, right?

Correct, it's because the compiler can't see it even with LTO.

> i.e. if redis would have used RM_Alloc internally it will be vulnerable.

Yes, in that case `RM_MallocUsableSize` would also need a call to `extend_to_usable` following the `usable_size` call to ensure that code that does see it will be able to see the additional available size too."
1493816569,11982,sundb,2023-04-03T07:27:12Z,"> right, even before 7.0.3, the compiler can see that zmalloc calls malloc, and be able to track the size of the allocation. and we do use the memory reported by malloc_usable_size in many places for quite some time.. so it doesn't look like anything dramatic was changed in #11666 (which was what uncovered this). i.e. the problem could be hiding in 6.2.0, and even before as well.

This issue starts from 5.0.0 that zmalloc_usable() was introduced first."
1495244734,11982,sundb,2023-04-04T02:19:24Z,@oranagra This issue was triggered on the latest ubuntu 23.04 (default gcc 12).
1495359669,11982,oranagra,2023-04-04T05:18:46Z,"@sundb do you mean you did this manually? (i see GH ubuntu-latest is 22.04)
maybe we can add some daily CI that uses a bleeding edge toolchain.

I would like to do some additional manual testing if you can:
* please run some performance smoke test again on the last version to see that we didn't get any noticeable hit.
* please do some similar tests (old and new toolchains) without LTO (or better yet, try to backport this to 7.0)
* please also try building it with `MALLOC=libc` (again with and without LTO, or redis 7.0, and various toolchains)
* maybe run a quick smoke test using `CFLAGS=-DNO_MALLOC_USABLE_SIZE` (should be unaffected, right?)"
1495363106,11982,sundb,2023-04-04T05:22:45Z,"@oranagra Yes, I'm testing in a VM.
I am also working on the daily CI."
1498700178,11982,sundb,2023-04-06T08:43:27Z,"@oranagra In the last commit I fixed the compile warnings and errors under clang with fortify, I'm not sure if they should be fixed in this PR, but it looks like they can be backported to 7.0 easily.
1) Fix jemalloc.c
```
src/jemalloc.c:1028:7: warning: suspicious concatenation of string literals in an array initialization; did you mean to separate the elements with a comma? [-Wstring-concatenation]
                    ""/etc/malloc.conf"",
```
2) Fix config.h
They need to be defined before include <features.h>
```
In file included from quicklist.c:36:
./config.h:317:5: error: conflicting types for '__builtin___sprintf_chk'
int sprintf(char *str, const char *format, ...) __attribute__((deprecated(""please avoid use of unsafe C functions. prefer use of snprintf instead"")));
```
2) Fix redis-cli.c and redis-benchmark.c
```
warning: embedding a directive within macro arguments has undefined behavior [-Wembedded-directive]
```"
1499891338,11982,sundb,2023-04-07T03:22:26Z,"the daily ci yml with fortify(in the bottom): https://github.com/sundb/redis/blob/zmalloc_usable_ci/.github/workflows/daily.yml
unstable with commit https://github.com/redis/redis/pull/11982/commits/573c9f4f66eeb6b66a364b4a7cc33f7a9bee9730
gcc-12 (failed): https://github.com/sundb/redis/actions/runs/4627189801
clang (failed): https://github.com/sundb/redis/actions/runs/4627195804
gcc-12 without lto (failed):  https://github.com/sundb/redis/actions/runs/4628798545
clang without lto (failed): https://github.com/sundb/redis/actions/runs/4628803915

this PR:
gcc-12 (passed): https://github.com/sundb/redis/actions/runs/4606207843
gcc-11 (no effect): https://github.com/sundb/redis/actions/runs/4606214521
gcc-12 without lto (passed):  https://github.com/sundb/redis/actions/runs/4606820338
gcc-11 without lto (no effect): https://github.com/sundb/redis/actions/runs/4606826151
clang (passed): https://github.com/sundb/redis/actions/runs/4616206027
clang without lto (passed): https://github.com/sundb/redis/actions/runs/4635166757
"
1499966449,11982,sundb,2023-04-07T05:58:28Z,"@oranagra Now there are still some compile warnings under non-FORTIFY, but it is certain that some of them are due to gcc bugs, maybe we can wait for the next GCC release to fix them.

.e.g https://gcc.gnu.org/bugzilla/show_bug.cgi?id=98512
This will cause the warning disabled in #11538 to appear again.
"
1500247341,11982,sundb,2023-04-07T12:29:50Z,"Benchmarking under GCC and clang with jemalloc.
environment: Ubuntu 22.04, 12 cores, 16G mem
compiler: GCC 12.1,  clang 14
start redis: `taskset -c 0-1 ./src/redis-server --save """"`
benchmark command: `taskset -c 4-11 memtier_benchmark --hide-histogram --test-time 60 -x 10 -c 100 --pipeline=10 -t 10`
```
10        Threads
100       Connections per thread
60        Seconds
```

1) GCC

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        82423.96          ---          ---        11.02720        10.81500        21.37500        26.75100      6349.53 
Gets       824155.71       179.32    823976.40        11.02601        10.81500        21.37500        26.75100     32109.08 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     906579.67       179.32    823976.40        11.02612        10.81500        21.37500        26.75100     38458.61 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        78060.60          ---          ---        11.64397        10.94300        25.59900        31.35900      6013.42 
Gets       780523.11       333.25    780189.86        11.64185        10.94300        25.47100        31.35900     30414.65 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     858583.71       333.25    780189.86        11.64204        10.94300        25.59900        31.35900     36428.07 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        79744.84          ---          ---        11.39778        10.87900        23.67900        30.46300      6143.13 
Gets       797365.80       322.81    797042.98        11.39582        10.87900        23.55100        30.46300     31070.29 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     877110.63       322.81    797042.98        11.39600        10.87900        23.55100        30.46300     37213.42
```

* PR (-2%) [need check]
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        79302.77          ---          ---        11.46250        10.94300        24.83100        30.71900      6109.08 
Gets       792946.78       334.43    792612.35        11.46067        10.94300        24.70300        30.71900     30898.51 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     872249.56       334.43    792612.35        11.46084        10.94300        24.83100        30.71900     37007.59 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        76603.79          ---          ---        11.86561        11.39100        24.83100        31.87100      5901.22 
Gets       765954.81       333.30    765621.51        11.86437        11.39100        24.95900        31.74300     29847.47 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     842558.59       333.30    765621.51        11.86448        11.39100        24.95900        31.74300     35748.70 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        78142.64          ---          ---        11.63040        11.13500        24.44700        31.10300      6019.74 
Gets       781343.21       316.24    781026.97        11.62923        11.13500        24.31900        31.10300     30446.08 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     859485.84       316.24    781026.97        11.62933        11.13500        24.31900        31.10300     36465.82
```

2) GCC without LTO

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        77158.62          ---          ---        11.77982        11.58300        22.78300        28.92700      5943.95 
Gets       771502.21       152.82    771349.39        11.77878        11.58300        22.78300        28.92700     30057.46 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     848660.83       152.82    771349.39        11.77887        11.58300        22.78300        28.92700     36001.41 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        73767.86          ---          ---        12.32140        11.71100        25.98300        32.25500      5682.79 
Gets       737597.62       314.50    737283.12        12.31948        11.71100        25.98300        32.12700     28742.19 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     811365.49       314.50    737283.12        12.31966        11.71100        25.98300        32.25500     34424.98 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        75148.63          ---          ---        12.09486        11.58300        25.34300        31.48700      5789.14 
Gets       751402.84       299.79    751103.06        12.09315        11.58300        25.34300        31.48700     29279.52 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     826551.47       299.79    751103.06        12.09331        11.58300        25.34300        31.48700     35068.66 
```

* PR (-0.47%)
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        77694.24          ---          ---        11.69704        11.51900        22.91100        29.95100      5985.21 
Gets       776862.91       157.40    776705.51        11.69780        11.51900        22.91100        29.95100     30266.33 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     854557.15       157.40    776705.51        11.69773        11.51900        22.91100        29.95100     36251.54 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        73077.97          ---          ---        12.43944        11.71100        25.21500        32.76700      5629.64 
Gets       730697.77       305.63    730392.15        12.43639        11.71100        25.21500        32.76700     28473.13 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     803775.75       305.63    730392.15        12.43667        11.71100        25.21500        32.76700     34102.77 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        74789.41          ---          ---        12.15357        11.58300        24.95900        32.12700      5761.47 
Gets       747811.03       298.63    747512.40        12.15128        11.58300        24.83100        32.12700     29139.54 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     822600.44       298.63    747512.40        12.15149        11.58300        24.83100        32.12700     34901.02
```

3) clang

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        83798.95          ---          ---        10.84649        10.68700        21.24700        30.46300      6455.48 
Gets       837909.39       179.65    837729.74        10.84480        10.68700        21.24700        30.46300     32644.80 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     921708.34       179.65    837729.74        10.84496        10.68700        21.24700        30.46300     39100.29 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        77279.52          ---          ---        11.75883        11.13500        24.31900        32.63900      5953.27 
Gets       772713.56       333.23    772380.33        11.75750        11.13500        24.31900        32.63900     30110.64 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     849993.08       333.23    772380.33        11.75763        11.13500        24.31900        32.63900     36063.91 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        80150.39          ---          ---        11.34004        10.81500        24.06300        31.48700      6174.39 
Gets       801421.52       324.67    801096.84        11.33786        10.81500        24.06300        31.35900     31228.36 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     881571.90       324.67    801096.84        11.33805        10.81500        24.06300        31.35900     37402.75
```

* PR (+1.6%) [need check]
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        84795.73          ---          ---        10.71638        10.62300        21.11900        29.05500      6532.29 
Gets       847872.17       194.45    847677.72        10.71493        10.62300        21.11900        28.92700     33033.34 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     932667.90       194.45    847677.72        10.71506        10.62300        21.11900        28.92700     39565.63 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        80096.46          ---          ---        11.34857        10.75100        25.34300        30.71900      6170.20 
Gets       800881.01       344.01    800537.00        11.34677        10.75100        25.21500        30.71900     31207.91 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     880977.47       344.01    800537.00        11.34694        10.75100        25.21500        30.71900     37378.11 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        81464.02          ---          ---        11.15732        10.75100        23.42300        29.95100      6275.57 
Gets       814556.95       333.15    814223.81        11.15551        10.75100        23.42300        29.95100     31740.26 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     896020.97       333.15    814223.81        11.15568        10.75100        23.42300        29.95100     38015.83
```

4) clang without LTO

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        84819.04          ---          ---        10.71485        10.68700        20.86300        23.55100      6534.09 
Gets       848107.85       191.17    847916.69        10.71449        10.68700        20.86300        23.42300     33042.43 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     932926.90       191.17    847916.69        10.71452        10.68700        20.86300        23.42300     39576.52 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        80161.55          ---          ---        11.33877        10.75100        24.83100        30.59100      6175.21 
Gets       801532.33       344.89    801187.44        11.33669        10.75100        24.83100        30.59100     31233.31 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     881693.88       344.89    801187.44        11.33688        10.75100        24.83100        30.59100     37408.52 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        81124.07          ---          ---        11.20419        10.75100        23.29500        30.07900      6249.38 
Gets       811157.24       332.25    810824.99        11.20209        10.75100        23.16700        30.07900     31607.81 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     892281.31       332.25    810824.99        11.20228        10.75100        23.29500        30.07900     37857.18
```

* PR (-0.36%)
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        84394.84          ---          ---        10.76938        10.75100        20.73500        23.03900      6501.39 
Gets       843866.71       185.53    843681.17        10.76808        10.75100        20.73500        23.03900     32877.00 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     928261.54       185.53    843681.17        10.76820        10.75100        20.73500        23.03900     39378.40 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        79590.08          ---          ---        11.41765        10.81500        25.47100        32.12700      6131.20 
Gets       795819.20       336.58    795482.61        11.41638        10.81500        25.47100        31.99900     31010.45 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     875409.27       336.58    795482.61        11.41649        10.81500        25.47100        31.99900     37141.65 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        80828.32          ---          ---        11.25250        10.75100        23.42300        30.59100      6226.59 
Gets       808199.48       328.06    807871.43        11.25047        10.75100        23.29500        30.59100     31492.44 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     889027.80       328.06    807871.43        11.25065        10.75100        23.29500        30.59100     37719.03
```
"
1500252913,11982,sundb,2023-04-07T12:38:44Z,"Benchmarking under GCC and clang with libc (MALLOC=libc).
environment: Ubuntu 22.04, 12 cores, 16G mem
compiler: GCC 12.1,  clang 14
start redis: taskset -c 0-1 ./src/redis-server --save """"
benchmark command: taskset -c 4-11 memtier_benchmark --hide-histogram --test-time 60 -x 10 -c 100 --pipeline=10 -t 10
```
10        Threads
100       Connections per thread
60        Seconds
```

1) GCC

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        78337.78          ---          ---        11.60000        11.07100        24.83100        30.46300      6034.77 
Gets       783293.07       316.56    782976.50        11.59898        11.07100        24.83100        30.46300     30521.97 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     861630.85       316.56    782976.50        11.59908        11.07100        24.83100        30.46300     36556.75 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        74742.68          ---          ---        12.16229        11.39100        24.83100        33.53500      5757.89 
Gets       747342.23       316.46    747025.77        12.15983        11.39100        24.83100        33.53500     29121.94 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     822084.91       316.46    747025.77        12.16005        11.39100        24.83100        33.53500     34879.83 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        76562.91          ---          ---        11.87106        11.26300        24.83100        31.61500      5898.07 
Gets       765545.19       300.04    765245.15        11.86912        11.26300        24.83100        31.61500     29830.32 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     842108.10       300.04    765245.15        11.86929        11.26300        24.83100        31.61500     35728.40
```

* PR (+0.7%)
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        80019.40          ---          ---        11.35984        11.32700        22.14300        26.75100      6164.27 
Gets       800112.44       168.09    799944.35        11.35764        11.32700        22.14300        26.62300     31172.13 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     880131.84       168.09    799944.35        11.35784        11.32700        22.14300        26.62300     37336.40 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        75678.52          ---          ---        12.01142        11.45500        25.85500        31.87100      5829.94 
Gets       756701.10       333.27    756367.83        12.00879        11.45500        25.72700        31.87100     29487.03 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     832379.62       333.27    756367.83        12.00903        11.45500        25.72700        31.87100     35316.97 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        77107.69          ---          ---        11.78796        11.32700        24.70300        31.10300      5940.03 
Gets       770994.15       316.75    770677.41        11.78593        11.32700        24.57500        30.97500     30043.14 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     848101.84       316.75    770677.41        11.78611        11.32700        24.70300        30.97500     35983.17
```

2) GCC without LTO

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        78410.18          ---          ---        11.59081        11.64700        22.52700        24.83100      6040.34 
Gets       784017.99       153.25    783864.73        11.59088        11.64700        22.52700        24.83100     30544.82 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     862428.17       153.25    783864.73        11.59087        11.64700        22.52700        24.83100     36585.16 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        72183.19          ---          ---        12.59385        11.83900        26.11100        32.76700      5560.69 
Gets       721749.46       299.72    721449.75        12.59049        11.83900        26.11100        32.63900     28124.53 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     793932.65       299.72    721449.75        12.59080        11.83900        26.11100        32.63900     33685.21 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        74465.73          ---          ---        12.20652        11.71100        25.21500        31.23100      5736.54 
Gets       744573.27       297.64    744275.63        12.20475        11.71100        25.08700        31.23100     29013.38 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     819039.00       297.64    744275.63        12.20491        11.71100        25.08700        31.23100     34749.92
```

* PR (-0.25%)
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        77594.11          ---          ---        11.71346        11.64700        22.91100        29.05500      5977.50 
Gets       775856.86       157.37    775699.50        11.71302        11.64700        22.91100        29.05500     30227.16 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     853450.97       157.37    775699.50        11.71306        11.64700        22.91100        29.05500     36204.66 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        72816.93          ---          ---        12.48289        11.77500        27.00700        31.74300      5609.52 
Gets       728085.23       302.80    727782.43        12.48167        11.77500        27.00700        31.61500     28371.31 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     800902.16       302.80    727782.43        12.48178        11.77500        27.00700        31.61500     33980.83 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        74278.93          ---          ---        12.23763        11.71100        25.34300        31.23100      5722.16 
Gets       742705.63       296.68    742408.95        12.23541        11.71100        25.21500        31.23100     28940.60 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     816984.56       296.68    742408.95        12.23561        11.71100        25.34300        31.23100     34662.76
```

3) clang
* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        84077.64          ---          ---        10.81082        10.81500        20.99100        23.42300      6476.95 
Gets       840694.60       179.95    840514.65        10.80909        10.81500        20.99100        23.29500     32753.29 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     924772.24       179.95    840514.65        10.80924        10.81500        20.99100        23.29500     39230.24 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        78464.20          ---          ---        11.58433        10.94300        25.34300        31.35900      6044.51 
Gets       784558.95       333.30    784225.65        11.58265        10.94300        25.34300        31.35900     30571.83 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     863023.15       333.30    784225.65        11.58280        10.94300        25.34300        31.35900     36616.34 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        80060.77          ---          ---        11.35243        10.87900        23.55100        30.33500      6167.47 
Gets       800523.87       324.07    800199.81        11.35097        10.87900        23.42300        30.20700     31193.35 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     880584.65       324.07    800199.81        11.35110        10.87900        23.42300        30.20700     37360.82
```

* PR (+0.55%)
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        83839.73          ---          ---        10.84032        10.81500        20.99100        24.95900      6458.62 
Gets       838313.32       186.51    838126.80        10.83938        10.81500        20.99100        24.95900     32660.74 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     922153.05       186.51    838126.80        10.83947        10.81500        20.99100        24.95900     39119.36 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        77961.36          ---          ---        11.65835        11.00700        26.49500        31.35900      6005.78 
Gets       779529.62       333.28    779196.34        11.65701        11.00700        26.36700        31.35900     30375.98 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     857490.98       333.28    779196.34        11.65713        11.00700        26.36700        31.35900     36381.76 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        80503.69          ---          ---        11.29014        10.81500        22.91100        30.20700      6201.60 
Gets       804953.49       327.67    804625.82        11.28839        10.81500        22.78300        30.20700     31366.02 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     885457.18       327.67    804625.82        11.28855        10.81500        22.78300        30.20700     37567.62
```

4) clang without LTO

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        75728.14          ---          ---        12.00450        11.90300        23.29500        30.07900      5833.79 
Gets       757196.56       153.12    757043.44        12.00164        11.90300        23.29500        29.95100     29500.26 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     832924.69       153.12    757043.44        12.00190        11.90300        23.29500        29.95100     35334.04 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        72099.29          ---          ---        12.60783        11.90300        27.64700        34.04700      5554.22 
Gets       720908.26       316.21    720592.05        12.60507        11.90300        27.51900        34.04700     28092.31 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     793007.55       316.21    720592.05        12.60532        11.90300        27.51900        34.04700     33646.54 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        73068.00          ---          ---        12.43989        11.90300        26.11100        31.87100      5628.86 
Gets       730595.95       304.23    730291.71        12.43822        11.83900        25.98300        31.74300     28469.18 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     803663.95       304.23    730291.71        12.43837        11.83900        25.98300        31.74300     34098.04
```

* PR (~0%)
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        73907.53          ---          ---        12.29716        11.90300        26.49500        32.12700      5693.55 
Gets       738992.75       315.27    738677.48        12.29650        11.90300        26.49500        32.12700     28796.57 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     812900.28       315.27    738677.48        12.29656        11.90300        26.49500        32.12700     34490.11 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        71554.06          ---          ---        12.70368        12.15900        27.26300        32.63900      5512.21 
Gets       715460.52       298.18    715162.34        12.70167        12.15900        27.13500        32.51100     27879.56 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     787014.58       298.18    715162.34        12.70185        12.15900        27.13500        32.51100     33391.76 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        73082.16          ---          ---        12.45171        11.90300        25.85500        32.25500      5629.95 
Gets       730738.44       290.86    730447.58        12.45013        11.90300        25.85500        32.25500     28474.28 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     803820.60       290.86    730447.58        12.45028        11.90300        25.85500        32.25500     34104.23
```
"
1500261327,11982,sundb,2023-04-07T12:51:08Z,"As we can see from the above results, this PR does not result in any significant performance degradation.
Note that the performance degradation in https://github.com/redis/redis/pull/11982#issuecomment-1491232292
was due to my use of `LPUSH`, but now that the listpack fix is in place with https://github.com/redis/redis/pull/11982/commits/24c901f76d56c88731863329d83fd7792b4437c2, my initial tests also showed no
significant performance degradation, and further testing is needed."
1500310222,11982,sundb,2023-04-07T13:53:43Z,"Benchmarking under GCC with NO_MALLOC_USABLE_SIZE.
environment: Ubuntu 22.04, 12 cores, 16G mem
compiler: GCC 12.1
start redis: taskset -c 0-1 ./src/redis-server --save """"
benchmark command: taskset -c 4-11 memtier_benchmark --hide-histogram --test-time 60 -x 10 -c 100 --pipeline=10 -t 10

* Unstable
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        58880.21          ---          ---        15.43884        15.03900        29.95100        38.65500      4536.07 
Gets       588720.05        66.66    588653.40        15.43708        15.03900        29.95100        38.65500     22934.40 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     647600.27        66.66    588653.40        15.43724        15.03900        29.95100        38.65500     27470.47 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        56597.50          ---          ---        16.05901        15.35900        30.84700        39.67900      4360.16 
Gets       565893.26       182.43    565710.83        16.05699        15.35900        30.84700        39.67900     22049.02 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     622490.76       182.43    565710.83        16.05718        15.35900        30.84700        39.67900     26409.18 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        57533.92          ---          ---        15.79895        15.23100        30.84700        38.39900      4432.31 
Gets       575255.82       171.48    575084.34        15.79717        15.23100        30.84700        38.39900     22413.36 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     632789.74       171.48    575084.34        15.79734        15.23100        30.84700        38.39900     26845.67
```

* PR (+0.35%)
```
BEST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        60134.09          ---          ---        15.11771        14.91100        29.82300        37.11900      4632.67 
Gets       601259.08        75.86    601183.22        15.11541        14.91100        29.82300        37.11900     23423.26 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     661393.17        75.86    601183.22        15.11562        14.91100        29.82300        37.11900     28055.92 


WORST RUN RESULTS
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        56883.66          ---          ---        15.98377        15.23100        30.84700        40.19100      4382.20 
Gets       568754.73       183.06    568571.67        15.97802        15.23100        30.71900        40.19100     22160.50 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     625638.38       183.06    568571.67        15.97854        15.23100        30.71900        40.19100     26542.70 


AGGREGATED AVERAGE RESULTS (10 runs)
============================================================================================================================
Type         Ops/sec     Hits/sec   Misses/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------------------------------
Sets        57737.27          ---          ---        15.74852        15.10300        30.59100        38.65500      4447.97 
Gets       577289.54       172.56    577116.98        15.74662        15.10300        30.59100        38.65500     22492.63 
Waits           0.00          ---          ---             ---             ---             ---             ---          --- 
Totals     635026.81       172.56    577116.98        15.74679        15.10300        30.59100        38.65500     26940.60
```"
1500559909,11982,oranagra,2023-04-07T19:09:18Z,"> As we can see from the above results, this PR does not result in any significant performance degradation. Note that the performance degradation in [#11982 (comment)](https://github.com/redis/redis/pull/11982#issuecomment-1491232292) was due to my use of `LPUSH`, but now that the listpack fix is in place with [24c901f](https://github.com/redis/redis/commit/24c901f76d56c88731863329d83fd7792b4437c2), my initial tests also showed no significant performance degradation, and further testing is needed.

I'm not sure i understand, is that regression gone now? how did that commit affected it?

from the plain SET/GET benchmarks you made (who does rely on usable_size features of sds and the output buffer), we see there's no meaningful impact, so i guess we can proceed to merge it. (please update the top comment).

Regarding all the compilation warnings / error changes, and additional CI jobs to use bleeding edge toolchains, i suggest to revert these changes from this PR and introduce them in a followup PR. we can backport both to 7.0 later, but i think it'll be clearer to have one deal with the malloc fortification, and another with compilation errors and CI."
1501507917,11982,sundb,2023-04-10T07:41:04Z,"@oranagra After a lot of repeating the tests in https://github.com/redis/redis/pull/11982#issuecomment-1491232292 (using RPUSH instead of LPUSH to avoid memory copies), I believe this PR does not have any impact.
And reset to 2f547caf27149406ab8ce91329ffc038f70377ee did not find any impact, I'm not sure where I'm going wrong."
1501538167,11982,sundb,2023-04-10T08:13:36Z,@oranagra Reverted the last commit and completed the top comment.
1506677467,11982,oranagra,2023-04-13T09:51:13Z,@sundb please don't forget to make a PR with the compilation fixes and the CI coverage.
1506686124,11982,sundb,2023-04-13T09:57:27Z,@oranagra #12035 is in progress.
1506866570,11982,oranagra,2023-04-13T12:18:27Z,"Note, after discussing it with Yossi, we decided not to take this change to 6.x releases, only to 7.0.
and even there, in the 7.0 backport we don't declare malloc_size attributes in
zmalloc.h so that we don't take the risk of inducing any crashes in a
bugfix release, so will only have effect if LTO was enforced from outside.
"
2182471920,13359,sundb,2024-06-21T10:24:04Z,"@hanhui365 thanks, could you make a benchmark for this PR?"
2182691402,13359,collinfunk,2024-06-21T12:48:26Z,"I think some ARM and RISC-V extensions have popcount instructions too.

Maybe it would be better to use __builtin_popcountll [1]? It would help other architectures assuming their compiler supports it. It appears that intrinsic is just an inline call to that anyways.

```
$ grep  'popcntq' /usr/lib/gcc/x86_64-linux-gnu/14/include/ia32intrin.h -A3
__popcntq (unsigned long long __X)
{
  return __builtin_popcountll (__X);
}
--
#define _popcnt64(a)		__popcntq(a)
```

[1] https://gcc.gnu.org/onlinedocs/gcc/Other-Builtins.html#index-_005f_005fbuiltin_005fpopcountll"
2182701696,13359,sundb,2024-06-21T12:55:12Z,"@collinfunk yes, `__builtin_popcountll` is a good choice."
2185583511,13359,hanhui365,2024-06-24T04:30:11Z,"@sundb @collinfunk How about just use __GNUC__ &&  __clang__ macro. I will assume __builtin_popcountll will use the best choice.   

Here is the benchmark using __builtin_popcountll:
The unit of input string LEN is bytes, and unit of execution time is us:

- CPU: Hygon C86-4G

- OS: Ubuntu 22.04

- Kernel: 5.15.0

- GCC: 11.4

LEN=256   before optimize = 0.195393   after optimize= 0.106510
LEN=512   before optimize = 0.390815   after optimize= 0.216116
LEN=1024 before optimize = 0.779776   after optimize= 0.424560
LEN=2048 before optimize = 1.508710   after optimize= 0.844279
LEN=4096 before optimize = 3.015011   after optimize= 1.676645"
2185586807,13359,sundb,2024-06-24T04:34:21Z,@hanhui365 can you make a benchmark for `bitcount` command?
2185831367,13359,hanhui365,2024-06-24T07:46:07Z,"@sundb here is benchmark for bitcount command. The total time is separated to three parts: client-->server, server processing command, server->client. It seems bitcount processing occupation is not high. The benchmark shows about 10% performance boost.
the platform used just as above:
LEN     before optimize         after optimize
1M         0.000342                      0.000312
2M         0.000632                      0.000571
4M         0.001212                      0.001093                      
8M         0.002376                      0.002147                      
16M       0.004822                      0.004420    

the python code is for your reference:

import time
import redis
import random
import string

def random_string_generator(str_size, allowed_chars):
    return ''.join(random.choice(allowed_chars) for x in range(str_size))
chars = string.ascii_letters + string.punctuation
size = 1024 * 1024 * 16
setkeycmd = 'set ' + 'key1 ' + random_string_generator(size, chars)

def get_command_execution_time(command):
    start_time = time.time()
    for i in range(1000):
        result = redis_client.execute_command(command)
    end_time = time.time()
    execution_time = (end_time - start_time)/1000
    return execution_time, result

redis_client = redis.Redis(host='localhost', port=6379)

redis_client.execute_command(setkeycmd)

execution_time, result = get_command_execution_time('bitcount key1')
print(""Execution time: %.6f seconds"" % execution_time)"
2185907035,13359,sundb,2024-06-24T08:25:19Z,@hanhui365 please have a look the failue in the CIs.
2262706232,13359,sundb,2024-08-01T10:32:50Z,"@hanhui365 i realize we are in the wrong way, please have a look https://github.com/redis/redis/commit/7c34643f154f543e1eef7c9855fb8d657146c646
we still need to prove this PR will be faster, including large string, .e.g 512MB"
2335157920,13359,fcostaoliveira,2024-09-07T11:26:27Z,"WRT 
https://github.com/redis/redis/pull/13359#issuecomment-2262706232

@sundb I've added 2 benchmarks for bitcount on bitmaps of 100M(14MB) and 1Billion entries (140MB). The benchmark varies the BITCOUNT start and always goes to the end of bitmap.  Sample command: `""BITCOUNT"" ""users"" ""243001"" ""-1""`
For now I don't see any improvement on the results. I'll profile the use-cases and reply back. 


To reproduce (assuming redis is available at port 6379 and pinned to core 0 ) :
```
taskset -c 0 ./src/redis-server --save '' --protected-mode no --daemonize yes
pip3 install redis-benchmarks-specification==0.1.334
redis-benchmarks-spec-client-runner --tests-regexp "".*bitcont.*"" --flushall_on_every_test_start --flushall_on_every_test_end  --cpuset_start_pos 2 --override-memtier-test-time 60
```

We get:

## Baseline unstable (  31227f4faf8c3bfce92ec458f37485d6bdb2dc62 ) :

|                     Test Name                      |                 Metric JSON Path                 |Metric Value|
|----------------------------------------------------|--------------------------------------------------|-----------:|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Ops/sec""                      |     347.770|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Latency""                      |     570.798|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Misses/sec""                   |       0.000|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Percentile Latencies"".""p50.00""|     561.151|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Ops/sec""                      |    3506.040|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Latency""                      |      57.022|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Misses/sec""                   |       0.000|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Percentile Latencies"".""p50.00""|      52.991|

## Comparison this branch ( f48e329b81038701b8480893cd98de79fb692273 ) : 

|                     Test Name                      |                 Metric JSON Path                 |Metric Value|
|----------------------------------------------------|--------------------------------------------------|-----------:|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Ops/sec""                      |     334.050|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Latency""                      |     589.658|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Misses/sec""                   |       0.000|
|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount|""ALL STATS"".Totals.""Percentile Latencies"".""p50.00""|     589.823|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Ops/sec""                      |    3364.460|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Latency""                      |      59.422|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Misses/sec""                   |       0.000|
|memtier_benchmark-1key-100M-bits-bitmap-bitcount    |""ALL STATS"".Totals.""Percentile Latencies"".""p50.00""|      55.039|

"
2357781416,13359,hanhui365,2024-09-18T08:04:26Z,"@sundb @fcostaoliveira 
I test 100M/256M/512M with python above, the performance boost is obvious.
CPU: Hygon C86-4G
OS: Red Hat Enterprise Linux release 9.4
Kernel: 5.14.0
GCC: 11.4.1
LEN 	      original(s)	 optimized(s)
100M     0.044603	0.010833
256M     0.112749	0.026016
512M     0.224174	0.051313

Actually i noticed there is no improvement in certain circumstances, when i use gcc version = 7.3.0
I objdump bitops.o and find there is no popcnt comparing using gcc version = 11.4.1
<img width=""416"" alt=""1726646483165"" src=""https://github.com/user-attachments/assets/300f42d6-f6c8-4893-b979-48bd44e6e19e"">
<img width=""418"" alt=""1726646539102"" src=""https://github.com/user-attachments/assets/c84baa3a-fe68-4c23-943e-13ae32d619e8"">

Could you pls check your gcc version and bitops.o if popcnt is compiled or not?




"
2357861744,13359,hanhui365,2024-09-18T08:43:47Z,"@fcostaoliveira 
adding   #pragma GCC target (""popcnt"")   will help gcc to compile __builtin_popcountll() to popcnt
maybe you can try add this in bitops.c"
2357941207,13359,sundb,2024-09-18T09:20:20Z,"@hanhui365 a second check, still can't see any benefit, am i missing something?

```asm
   0x000000000016f9e0 <+0>:     endbr64
   0x000000000016f9e4 <+4>:     push   %rbp
   0x000000000016f9e5 <+5>:     mov    %rsp,%rbp
   0x000000000016f9e8 <+8>:     push   %r15
   0x000000000016f9ea <+10>:    mov    %rsi,%r15
   0x000000000016f9ed <+13>:    push   %r14
   0x000000000016f9ef <+15>:    push   %r13
   0x000000000016f9f1 <+17>:    mov    %rdi,%r13
   0x000000000016f9f4 <+20>:    push   %r12
   0x000000000016f9f6 <+22>:    push   %rbx
   0x000000000016f9f7 <+23>:    sub    $0x18,%rsp
   0x000000000016f9fb <+27>:    cmp    $0x1f,%rsi
   0x000000000016f9ff <+31>:    jle    0x16faa8 <redisPopcount+200>
   0x000000000016fa05 <+37>:    lea    -0x20(%rsi),%rax
   0x000000000016fa09 <+41>:    mov    %rdi,%rbx
   0x000000000016fa0c <+44>:    xor    %r12d,%r12d
   0x000000000016fa0f <+47>:    mov    %rax,-0x38(%rbp)
   0x000000000016fa13 <+51>:    and    $0xffffffffffffffe0,%rax
   0x000000000016fa17 <+55>:    lea    0x20(%rdi,%rax,1),%r14
   0x000000000016fa1c <+60>:    nopl   0x0(%rax)
   0x000000000016fa20 <+64>:    mov    (%rbx),%rdi
   0x000000000016fa23 <+67>:    add    $0x20,%rbx
   0x000000000016fa27 <+71>:    call   0x2ceab0 <__popcountdi2>
   0x000000000016fa2c <+76>:    mov    -0x18(%rbx),%rdi
   0x000000000016fa30 <+80>:    cltq
   0x000000000016fa32 <+82>:    add    %rax,%r12
   0x000000000016fa35 <+85>:    call   0x2ceab0 <__popcountdi2>
   0x000000000016fa3a <+90>:    mov    -0x10(%rbx),%rdi
   0x000000000016fa3e <+94>:    cltq
   0x000000000016fa40 <+96>:    add    %rax,%r12
   0x000000000016fa43 <+99>:    call   0x2ceab0 <__popcountdi2>
   0x000000000016fa48 <+104>:   mov    -0x8(%rbx),%rdi
   0x000000000016fa4c <+108>:   cltq
   0x000000000016fa4e <+110>:   add    %rax,%r12
   0x000000000016fa51 <+113>:   call   0x2ceab0 <__popcountdi2>
   0x000000000016fa56 <+118>:   cltq
   0x000000000016fa58 <+120>:   add    %rax,%r12
   0x000000000016fa5b <+123>:   cmp    %rbx,%r14
   0x000000000016fa5e <+126>:   jne    0x16fa20 <redisPopcount+64>
   0x000000000016fa60 <+128>:   mov    -0x38(%rbp),%rax
   0x000000000016fa64 <+132>:   and    $0xffffffffffffffe0,%rax
   0x000000000016fa68 <+136>:   add    $0x20,%rax
   0x000000000016fa6c <+140>:   cmp    %rax,%r15
   0x000000000016fa6f <+143>:   jle    0x16fa93 <redisPopcount+179>
   0x000000000016fa71 <+145>:   add    %r13,%rax
   0x000000000016fa74 <+148>:   lea    0x164085(%rip),%rcx        # 0x2d3b00 <bitsinbyte.0>
   0x000000000016fa7b <+155>:   add    %r15,%r13
   0x000000000016fa7e <+158>:   xchg   %ax,%ax
   0x000000000016fa80 <+160>:   movzbl (%rax),%edx
   0x000000000016fa83 <+163>:   add    $0x1,%rax
   0x000000000016fa87 <+167>:   movzbl (%rcx,%rdx,1),%edx
   0x000000000016fa8b <+171>:   add    %rdx,%r12
   0x000000000016fa8e <+174>:   cmp    %rax,%r13
   0x000000000016fa91 <+177>:   jne    0x16fa80 <redisPopcount+160>
   0x000000000016fa93 <+179>:   add    $0x18,%rsp
   0x000000000016fa97 <+183>:   mov    %r12,%rax
   0x000000000016fa9a <+186>:   pop    %rbx
   0x000000000016fa9b <+187>:   pop    %r12
   0x000000000016fa9d <+189>:   pop    %r13
   0x000000000016fa9f <+191>:   pop    %r14
   0x000000000016faa1 <+193>:   pop    %r15
   0x000000000016faa3 <+195>:   pop    %rbp
   0x000000000016faa4 <+196>:   ret
   0x000000000016faa5 <+197>:   nopl   (%rax)
   0x000000000016faa8 <+200>:   xor    %eax,%eax
   0x000000000016faaa <+202>:   xor    %r12d,%r12d
   0x000000000016faad <+205>:   jmp    0x16fa6c <redisPopcount+140>
```

benchmark
data prepration:
```
import redis
import os

r = redis.Redis(host='localhost', port=6379, db=0)
random_string = os.urandom(100 * 1024 * 1024)
r.set('key', random_string)
```

```
taskset -c 16-20 memtier_benchmark --hide-histogram --test-time 180 --command=""bitcount key""
```

unstable:
```
ALL STATS
====================================================================================================
Type           Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------
Bitcounts        63.55       765.75827       876.54300       876.54300       991.23100         2.42 
Totals           63.55       765.75827       876.54300       876.54300       991.23100         4.84 
```


this PR:
```
ALL STATS
====================================================================================================
Type           Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------
Bitcounts        62.60       807.47520       921.59900       925.69500      1036.28700         2.38 
```"
2357998472,13359,hanhui365,2024-09-18T09:44:53Z,"@sundb From the assembly code, I am afraid pocnt IS NOT used by your gcc. you can add #pragma GCC target (""popcnt"") in bitops.c or upgrade you gcc version."
2358250201,13359,sundb,2024-09-18T11:46:28Z,"@hanhui365 thanks, i made a commit to tell compiler to use popcnt(), let me wait for the benchmark report."
2358613060,13359,fcostaoliveira,2024-09-18T14:20:30Z,"### Automated performance analysis summary

This comment was automatically generated given there is performance data available.

Using platform named: intel64-ubuntu22.04-redis-icx1 to do the comparison.

In summary:
- Detected a total of 1 stable tests between versions.
- Detected a total of 2 improvements above the improvement water line.
   - Median/Common-Case improvement was 291.5% and ranged from [274.9%,308.1%].

You can check a comparison in detail via the [grafana link](https://benchmarksredisio.grafana.net/d/1fWbtb7nz/experimental-oss-spec-benchmarks/?var-branch=unstable&var-branch=bitcount)

### Comparison between unstable and bitcount.

Time Period from 5 months ago. (environment used: oss-standalone)

#### Improvements Table

|                                                                                                          Test Case                                                                                                          |Baseline redis/redis unstable (median obs. +- std.dev)|Comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)|   Note    |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------|
|[memtier_benchmark-1key-100M-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-100M-bits-bitmap-bitcount.yml)        | 3420 +- 1.0% (3 datapoints)                          |                                                       13958|308.1%                  |IMPROVEMENT|
|[memtier_benchmark-1key-1Billion-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-1Billion-bits-bitmap-bitcount.yml)| 331 +- 0.4% (3 datapoints)                           |                                                        1243|274.9%                  |IMPROVEMENT|


Improvements test regexp names: memtier_benchmark-1key-100M-bits-bitmap-bitcount|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount

<details>
  <summary>Full Results table:</summary>

|                                                                                                          Test Case                                                                                                          |Baseline redis/redis unstable (median obs. +- std.dev)|Comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)|   Note    |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------|
|[memtier_benchmark-1Mkeys-bitmap-getbit-pipeline-10](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1Mkeys-bitmap-getbit-pipeline-10.yml)    | 938168 +- 3.2% (3 datapoints)                        |                                                      937498|-0.1%                   |No Change  |
|[memtier_benchmark-1key-100M-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-100M-bits-bitmap-bitcount.yml)        | 3420 +- 1.0% (3 datapoints)                          |                                                       13958|308.1%                  |IMPROVEMENT|
|[memtier_benchmark-1key-1Billion-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-1Billion-bits-bitmap-bitcount.yml)| 331 +- 0.4% (3 datapoints)                           |                                                        1243|274.9%                  |IMPROVEMENT|

</details>
"
2359835248,13359,sundb,2024-09-19T02:09:06Z,"@hanhui365 we still need to use `__builtin_cpu_supports` to see whether the cup support `popcnt` instruction, otherwise, it could result in an illegal instruction.
1. we can add new redisPopcount method to support `popcnt`, and leave the old one there.
2. use `__builtin_cpu_supports()` to determine which is used. "
2359860889,13359,hanhui365,2024-09-19T02:37:23Z,"Sure, like:
if ( __builtin_cpu_supports (""popcnt"") )
  {
     redisPopcountHw(); //use popcnt instruction
  }
else
  {
     redisPopcountSW(); //generic implementation
  }"
2361455777,13359,collinfunk,2024-09-19T16:17:06Z,"Where did you see that `__builtin_cpu_supports` is required? I was under the impression that GCC would use the optimal method for the specific target, regardless if it supports a popcnt instruction or not."
2362499450,13359,sundb,2024-09-20T00:51:51Z,"> Where did you see that `__builtin_cpu_supports` is required? I was under the impression that GCC would use the optimal method for the specific target, regardless if it supports a popcnt instruction or not.

what if we compile with popcnt instruction using gcc, but execute it on another cpu that doesn't support popcnt?
i found a similar issue: https://github.com/simdjson/simdjson/issues/34

and the document in https://gcc.gnu.org/onlinedocs/gcc/x86-Built-in-Functions.html
```
Built-in Function: void __builtin_cpu_init (void)
This function runs the CPU detection code to check the type of CPU and the features supported. This built-in function needs to be invoked along with the built-in functions to check CPU type and features, __builtin_cpu_is and __builtin_cpu_supports, only when used in a function that is executed before any constructors are called. The CPU detection code is automatically executed in a very high priority constructor.

For example, this function has to be used in ifunc resolvers that check for CPU type using the built-in functions __builtin_cpu_is and __builtin_cpu_supports, or in constructors on targets that don’t support constructor priority.
```"
2362501740,13359,sundb,2024-09-20T00:54:31Z,"@hanhui365 sorry for my bad, i fixed the complaints."
2362553152,13359,collinfunk,2024-09-20T01:56:13Z,"> what if we compile with popcnt instruction using gcc, but execute it on another cpu that doesn't support popcnt? i found a similar issue: [simdjson/simdjson#34](https://github.com/simdjson/simdjson/issues/34)

This seems like an issue where the compiler supports more modern CPU instructions than the host machine. So the code used AVX2 instructions that the CPU didn't support, if I understand correctly.

In Gnulib we just use `__builtin_popcount` if the compiler supports it (based on `__has_builtin` or version checks since that macro is newer) [1].

Also, in GCC I see this comment, which makes me think it isn't required [2]:

```
      /* Even though is_inexpensive_builtin might say true, we will get a
	 library call for popcount when backend does not have an instruction
	 to do so.  We consider this to be expensive and generate
	 __builtin_popcount only when backend defines it.  */
```

So, I'm thinking for most machines a simple `popcnt` instruction is emitted by the compiler. For systems without it, a suitable replacement is emitted.

[1] https://github.com/coreutils/gnulib/blob/4d57a8df99bc20ed6155c8cbd307c7c5245093b0/lib/stdbit.in.h#L112
[2] https://github.com/gcc-mirror/gcc/blob/0135a90de5a99b51001b6152d8b548151ebfa1c3/gcc/tree-scalar-evolution.cc#L3431C1-L3434C55


"
2362572563,13359,sundb,2024-09-20T02:16:15Z,"> > what if we compile with popcnt instruction using gcc, but execute it on another cpu that doesn't support popcnt? i found a similar issue: [simdjson/simdjson#34](https://github.com/simdjson/simdjson/issues/34)
> 
> This seems like an issue where the compiler supports more modern CPU instructions than the host machine. So the code used AVX2 instructions that the CPU didn't support, if I understand correctly.
> 
> In Gnulib we just use `__builtin_popcount` if the compiler supports it (based on `__has_builtin` or version checks since that macro is newer) [1].

yes, but this is all at compile phase, not runtime.
`__has_builtin` is only used to check whether the compiler supports `__builtin_popcount`, and has nothing to do with the popcnt instruction.

> 
> Also, in GCC I see this comment, which makes me think it isn't required [2]:
> 
> ```
>       /* Even though is_inexpensive_builtin might say true, we will get a
> 	 library call for popcount when backend does not have an instruction
> 	 to do so.  We consider this to be expensive and generate
> 	 __builtin_popcount only when backend defines it.  */

but we now enforce the popcnt target, telling the compiler to use `popcnt` anyway instead of the expensive builtin.

> ```
> 
> So, I'm thinking for most machines a simple `popcnt` instruction is emitted by the compiler. For systems without it, a suitable replacement is emitted.

🥲 at runtime, the cpu can no longer do the replacement operation, maybe we can test it by using `qemu`.

> 
> [1] [coreutils/gnulib@`4d57a8d`/lib/stdbit.in.h#L112](https://github.com/coreutils/gnulib/blob/4d57a8df99bc20ed6155c8cbd307c7c5245093b0/lib/stdbit.in.h#L112) [2] [gcc-mirror/gcc@`0135a90`/gcc/tree-scalar-evolution.cc#L3431C1-L3434C55](https://github.com/gcc-mirror/gcc/blob/0135a90de5a99b51001b6152d8b548151ebfa1c3/gcc/tree-scalar-evolution.cc#L3431C1-L3434C55)

"
2362650589,13359,collinfunk,2024-09-20T02:51:35Z,"> yes, but this is all at compile phase, not runtime. `__has_builtin` is only used to check whether the compiler supports `__builtin_popcount`, and has nothing to do with the popcnt instruction.

GCC should preform feature checks and compile for the host machine, unless you use something like `gcc -march=SOME-OTHER-CPU` [1]. Or in this case `gcc -mpopcnt` on a CPU that doesn't support the instruction.

> 🥲 at runtime, the cpu can no longer do the replacement operation, maybe we can test it by using `qemu`.

I thought about it, but I forget what CPU's don't support it anymore. Any recent x86 should have it, atleast I think. :cry:

[1] https://gcc.gnu.org/onlinedocs/gcc/x86-Options.html"
2362707944,13359,sundb,2024-09-20T03:55:04Z,"> I thought about it, but I forget what CPU's don't support it anymore. Any recent x86 should have it, atleast I think. 😢

the only scenario i can think of is compiling on model x86 cpu and running on older cup, but that seems to be rarely.
so i think we can leave it there, wait for others' opinions.

ping @oranagra "
2363590030,13359,sundb,2024-09-20T12:16:02Z,@Nugine can you also take a look? thanks.
2365205516,13359,sundb,2024-09-21T14:18:44Z,"> LGTM.
> 
> See also [WojciechMula/sse-popcount](https://github.com/WojciechMula/sse-popcount/) The repo has [tested different popcount implementations](https://github.com/WojciechMula/sse-popcount/blob/master/README.rst?rgh-link-date=2024-09-21T07%3A59%3A27Z#performance-results). There is [an interesting trick](https://github.com/WojciechMula/sse-popcount/blob/138c91e21c3e6dab7875521b5d33b995e0e4c85e/popcnt-builtin.cpp#L49-L67) in it.

nice!! i test in my local and found that the trick can bring a boost.
@hanhui365 please take a look."
2366785736,13359,sundb,2024-09-22T13:18:50Z,"benchmark after the last change:
use the same test data in https://github.com/redis/redis/pull/13359#issuecomment-2357941207

unstable
```
ALL STATS
====================================================================================================
Type           Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------
Bitcounts        63.26       793.60000       884.73500       884.73500       884.73500         2.41 
Totals           63.26       793.60000       884.73500       884.73500       884.73500         4.82 
```

this PR (7x):
```
ALL STATS
====================================================================================================
Type           Ops/sec    Avg. Latency     p50 Latency     p99 Latency   p99.9 Latency       KB/sec 
----------------------------------------------------------------------------------------------------
Bitcounts       478.26       416.79739       411.64700       761.85500       798.71900        18.21 
Totals          478.26       416.79739       411.64700       761.85500       798.71900        36.43 
```

@hanhui365 thanks a lot, please don't mind i made some many changes, just want to verify early."
2366938210,13359,fcostaoliveira,2024-09-22T19:55:43Z,"@sundb using gcc-8.5, with the latest change (or anything after https://github.com/redis/redis/pull/13359/commits/2c1c1d5b674ee1e9e2b73e489ddf1bbc1dbf421a ) we don't see a variance vs unstable. 

In summary:
- Detected a total of 2 stable tests between versions.

You can check a comparison in detail via the [grafana link](https://benchmarksredisio.grafana.net/d/1fWbtb7nz/experimental-oss-spec-benchmarks/)

### Comparison between unstable and 92e6523e2d4648bf55dd96bf0e604f288bde1f28.

Time Period from 5 months ago. (environment used: oss-standalone)

  <summary>Full Results table:</summary>

|                                                                                                          Test Case                                                                                                          |Baseline redis/redis unstable (median obs. +- std.dev)|Comparison hanhui365/redis 92e6523e2d4648bf55dd96bf0e604f288bde1f28 (median obs. +- std.dev)|% change (higher-better)|  Note   |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-------------------------------------------------------------------------------------------:|------------------------|---------|
|[memtier_benchmark-1key-100M-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-100M-bits-bitmap-bitcount.yml)        | 3424 +- 0.1% (3 datapoints)                          |                                                                                        3427|0.1%                    |No Change|
|[memtier_benchmark-1key-1Billion-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-1Billion-bits-bitmap-bitcount.yml)| 331 +- 0.2% (3 datapoints)                           |                                                                                         331|-0.0%                   |No Change|

"
2367122923,13359,sundb,2024-09-23T02:16:14Z,"@fcostaoliveira since __has_builtin was supported since gcc 10, please do the benchmark again, thx."
2367406691,13359,oranagra,2024-09-23T07:19:09Z,"> > > I thought about it, but I forget what CPU's don't support it anymore. Any recent x86 should have it, atleast I think. 😢
> 
> > the only scenario i can think of is compiling on model x86 cpu and running on older cup, but that seems to be rarely.
> > so i think we can leave it there, wait for others' opinions.
> 
> not sure i understand the question. are you considering if we need any compile time check or runtime check? i don't think we should assume anything, i prefer to keep supporting old compilers and hardware.

@sundb i'm still unsure what feedback you wanted from me and if you got what you needed. if you did please ack."
2367413779,13359,sundb,2024-09-23T07:23:24Z,"> @sundb i'm still unsure what feedback you wanted from me and if you got what you needed. if you did please ack.

@oranagra 😄 already got answer from you, thanks."
2367415657,13359,sundb,2024-09-23T07:24:26Z,fully CI: https://github.com/sundb/redis/actions/runs/10989185498
2367618166,13359,fcostaoliveira,2024-09-23T09:03:01Z,"### Automated performance analysis summary

This comment was automatically generated given there is performance data available.

Using platform named: intel64-ubuntu22.04-redis-icx1 to do the comparison.

In summary:
- Detected a total of 2 improvements above the improvement water line.
   - Median/Common-Case improvement was 442.7% and ranged from [368.0%,517.4%].

You can check a comparison in detail via the [grafana link](https://benchmarksredisio.grafana.net/d/1fWbtb7nz/experimental-oss-spec-benchmarks/?var-branch=unstable&var-branch=bitcount)

### Comparison between unstable and bitcount.

Time Period from 5 months ago. (environment used: oss-standalone)

#### Improvements Table

|                                                                                                          Test Case                                                                                                          |Baseline redis/redis unstable (median obs. +- std.dev)|Comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)|   Note    |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------|
|[memtier_benchmark-1key-100M-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-100M-bits-bitmap-bitcount.yml)        | 3424 +- 0.1% (3 datapoints)                          |                                                       21141|517.4%                  |IMPROVEMENT|
|[memtier_benchmark-1key-1Billion-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-1Billion-bits-bitmap-bitcount.yml)| 331 +- 0.2% (3 datapoints)                           |                                                        1550|368.0%                  |IMPROVEMENT|


Improvements test regexp names: memtier_benchmark-1key-100M-bits-bitmap-bitcount|memtier_benchmark-1key-1Billion-bits-bitmap-bitcount

<details>
  <summary>Full Results table:</summary>

|                                                                                                          Test Case                                                                                                          |Baseline redis/redis unstable (median obs. +- std.dev)|Comparison hanhui365/redis bitcount (median obs. +- std.dev)|% change (higher-better)|   Note    |
|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|------------------------------------------------------|-----------------------------------------------------------:|------------------------|-----------|
|[memtier_benchmark-1key-100M-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-100M-bits-bitmap-bitcount.yml)        | 3424 +- 0.1% (3 datapoints)                          |                                                       21141|517.4%                  |IMPROVEMENT|
|[memtier_benchmark-1key-1Billion-bits-bitmap-bitcount](https://github.com/redis/redis-benchmarks-specification/blob/main/redis_benchmarks_specification/test-suites/memtier_benchmark-1key-1Billion-bits-bitmap-bitcount.yml)| 331 +- 0.2% (3 datapoints)                           |                                                        1550|368.0%                  |IMPROVEMENT|

</details>
"
2367627686,13359,sundb,2024-09-23T09:07:10Z,"@hanhui365 please take a look at the last CI failure, it looks like we still need to determine if it's x86, thanks."
2398826565,13359,sundb,2024-10-08T04:47:24Z,@hanhui365 is it ready to merge?
2409621205,13359,hanhui365,2024-10-14T01:28:44Z,"@sundb Yes, it is ok."
2427881665,13359,fcostaoliveira,2024-10-21T22:52:48Z,"### CE Performance Automation : step 2 of 2 (benchmark) RUNNING...

This comment was automatically generated given a benchmark was triggered.

Started benchmark suite at 2024-10-22 00:26:58.369905 and took 1.177628 seconds up until now.
Status: [###########################-----------------------------------------------------] 33.33% completed.

In total will run 3 benchmarks.
    - 2 pending.
    - 1 completed:
      - 0 successful.
      - 1 failed.
You can check a the status in detail via the [grafana link](https://benchmarksredisio.grafana.net/d/edsxdsrbexhc0f/ce-benchmark-run-status?orgId=1&var-benchmark_work_stream=1726667214010-0)"
739640315,8094,yangbodong22011,2020-12-07T03:25:52Z,"@oranagra Thanks for your review, follow this tcl case,  my original idea is as follows:

![image](https://user-images.githubusercontent.com/13137470/101305387-89dd9600-387d-11eb-9dad-8669c2f0c926.png)


- The circle is the actual search area of 200 km byradius.
- The rectangle is the actual search area of 200 km bybox.

Q: How to determine that a point is within the axis-aligned rectangle?

If the coordinates of this point are greater than the minimum latitude and longitude and less than the maximum latitude and longitude.

```
// x2,y2 is the point, x1, y1 is the search center point. so use x2, y2 and bounds to judge.

int geohashGetDistanceIfInRectangle(double *bounds, double x1, double y1,
                                     double x2, double y2, double *distance) {
     if (x2 <bounds[0] || x2> bounds[2] || y2 <bounds[1] || y2> bounds[3]) return 0;
     *distance = geohashGetDistance(x1, y1, x2, y2);
     return 1;
}
```

After you suggest that you should use height/2, width/2, I think your opinion is correct and logical.

So in order to reach the search range of the above pictures, the new parameters are as follows:
```
geosearch Sicily fromloc 15 37 bybox 400 400 km asc
```
Is that right?

"
739718343,8094,oranagra,2020-12-07T07:08:13Z,"@yangbodong22011 yes, width and height of that box are 400. i'm not aware of a word in English that describes half the width (other than saying `half_width`), so i guess we need to stick to the terminology of the command syntax, and adjust the code."
739865317,8094,yangbodong22011,2020-12-07T11:41:11Z,"updated:

- Modify the position of the `bounds` variable and add comments to GeoShape.
- Modify the calculation method of radius_meters, remove sqrt, ues the larger of height/2 and width/2.
- GEOSEARCH with STORE|STOREDIST and GEOSEARCHSTORE with STORE means syntax error.
- Add extractBoxOrReply() for parse box
-  refactor some function name and modify comment

@oranagra I will close some of the comments in your previous comments, please review again when you have time."
740527234,8094,oranagra,2020-12-08T10:17:15Z,@redis/core-team please approve the new commands and their syntax.
742272136,8094,oranagra,2020-12-10T06:27:49Z,@yangbodong22011 can you please also make a redis-doc PR for this?
742276535,8094,yangbodong22011,2020-12-10T06:38:29Z,"> @yangbodong22011 can you please also make a redis-doc PR for this?

sure, I will do it."
2017135489,13169,CLAassistant,2024-03-25T03:15:25Z,[![CLA assistant check](https://cla-assistant.io/pull/badge/signed)](https://cla-assistant.io/redis/redis?pullRequest=13169) <br/>All committers have signed the CLA.
2017192313,13169,alyahmedaly,2024-03-25T04:34:00Z,LGTM 🚀
2017303094,13169,chenyang8094,2024-03-25T06:17:02Z,:)
2017888202,13169,kova1max,2024-03-25T12:25:12Z,Sounds reasonable. 💪
2019002443,13169,alexsuter,2024-03-25T22:06:27Z,go ahead 🤝
2019102755,13169,thrawn01,2024-03-25T23:31:25Z,"Roses are red, violets are blue, the license is back, as good as new.
"
2019204790,13169,StevenMatchett,2024-03-26T01:06:31Z,https://github.com/redis/redis/pull/13168
2019224196,13169,PatrickJS,2024-03-26T01:32:28Z,@chenyang8094 thanks for assigning the task to yourself 😁 hopefully we can get more approvals 
2019232662,13169,jacksonlevine,2024-03-26T01:42:42Z,This seems like a quality Pull Request 
2019244660,13169,JQuinzell,2024-03-26T01:53:37Z,LGTM
2019291577,13169,LazyClicks,2024-03-26T02:54:43Z,is redis development dead? clearly this fix is ready to be merged what they waiting for tsk tsk tsk
2019323743,13169,polaris-alioth,2024-03-26T03:37:11Z,LGTM
2019333827,13169,bimsonz,2024-03-26T03:51:59Z,Makes sense! Let get it merged 
2019371978,13169,dtarellano,2024-03-26T04:36:05Z,LGTM
2019687885,13169,rdesgroppes,2024-03-26T07:55:14Z,LGTM
2019958811,13169,SaltyAom,2024-03-26T09:43:17Z,"LGTM 👍
Make Redis great again."
2020038484,13169,tiagorangel1,2024-03-26T10:18:51Z,+1
2020418145,13169,bubbakk,2024-03-26T13:22:13Z,LGTM
2020428474,13169,AbhinavAchha,2024-03-26T13:26:33Z,LGTM!
2020691156,13169,hopeseekr,2024-03-26T15:07:52Z,"What's crazy is that this same PR could be applied to any future version of Redis, no matter what their licenses, and probably be OK in a court of law, on the grounds of ""correcting damages"" of the previous crime: Violating the copyrights of all the contributors who did not consent to the license change."
2020693531,13169,nawaphonOHM,2024-03-26T15:08:44Z,Looks good to me!
2020705868,13169,terminalcommandnewsletter,2024-03-26T15:13:30Z,LGTM
2021009024,13169,XtremeOwnageDotCom,2024-03-26T17:08:34Z,"@K-Jo 

Good luck on your future business adventures. The community has spoken.

You didn't listen.

See you over at https://redict.io/"
2021245310,13169,PatrickJS,2024-03-26T18:58:19Z,"oh RIP.
the community moved to the fork (lead by maintainers not working at redis).
community fork: [ValKey](https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community) (previously placeholderkv) by Linux Foundation
"
2026449203,13169,zackarychapple,2024-03-29T01:38:06Z,@PatrickJS Linux Foundation created [ValKey](https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community) today.
2027068950,13169,rdesgroppes,2024-03-29T10:52:58Z,"> @PatrickJS Linux Foundation created [ValKey](https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community) today.

I think he's well aware of that: https://github.com/valkey-io/valkey/pull/62/files"
2027670134,13169,LazyClicks,2024-03-29T19:45:33Z,don't think this change is a breaking change why refuse the PR 
2027679633,13169,XtremeOwnageDotCom,2024-03-29T19:56:54Z,"> @PatrickJS Linux Foundation created [ValKey](https://www.linuxfoundation.org/press/linux-foundation-launches-open-source-valkey-community) today.

valkey IS placeholderkv. (They gave it a name)"
2027708251,13169,PatrickJS,2024-03-29T20:31:09Z,"thanks, I updated my comment"
2050466476,13169,Sven65,2024-04-11T20:24:08Z,LGTM
2234860048,13169,fgclue,2024-07-18T00:16:47Z,LGTM
147048935,2795,ptaoussanis,2015-10-10T07:31:04Z,"+1, this would be handy
"
880650483,2795,yoav-steinberg,2021-07-15T12:23:00Z,"@sunng87 like this. Can you rebase and add a test.
@oranagra @itamarhaber WDYT?"
880659063,2795,itamarhaber,2021-07-15T12:37:06Z,"Definitely useful for saving a 7-liner scriprt - I'm for it.

My only beef is the naming - `NX` means ""if not exists"" elsewhere in Redis, whereas here we're talking about ""if no TTL/expiry"". So, mebbe `NE` or `NT`?"
882006808,2795,oranagra,2021-07-18T06:29:02Z,"I agree this would be nice to have. 
In addition to the NX feature, maybe it would also be useful to gave GT and LT (grater than / less than) feature. 
My other concern is that as soon as e add these argument flags, it would mean that we can't change the command to be variadic in the future (taking multiple key names).
Also, looking at the linked issue, there was a request to return the TTL rather than 0 / 1. 
@redis/core-team please share your thoughts. "
882014843,2795,sunng87,2021-07-18T07:44:34Z,"> Can you rebase and add a test.

No problem. Let me catch up "
882053852,2795,sunng87,2021-07-18T13:06:53Z,"> My only beef is the naming - NX means ""if not exists"" elsewhere in Redis, whereas here we're talking about ""if no TTL/expiry"". So, mebbe NE or NT?

`NX` here means if a TTL not exists. I just followed previous attempts of this feature for naming. And I'm pretty open for other suggestions.

>  In addition to the NX feature, maybe it would also be useful to gave GT and LT (grater than / less than) feature.

~~For now I don't have a use case for `GT` and `LT` but the code base is open for adding more flags here.~~ I just realized `LT` is to fix same issue with my `NX`. 

> My other concern is that as soon as e add these argument flags, it would mean that we can't change the command to be variadic in the future (taking multiple key names). Also, looking at the linked issue, there was a request to return the TTL rather than 0 / 1.

These two tasks can be done via pipeline as a workaround (pipelining multiple `expire` and `ttl` command). So for now I think it's not very urgent."
882063490,2795,oranagra,2021-07-18T14:15:48Z,"@sunng87 you mean that LT would fix your problem the same way that NX does, but there are probably use cases that will benefit from one and not from the other.
In fact, now that i think of it, there are probably also use cases for XX (modify only if already volatile)."
882072395,2795,sunng87,2021-07-18T15:15:42Z,I agree and there are always possibilities. But before we have real scenario I would suggest to keep the code simple and open for new options. 
882073991,2795,yossigo,2021-07-18T15:25:07Z,"@oranagra I think that having smarter `EXPIRE` is more useful than having it variadic, which I believe has very little value over pipelining."
882181742,2795,sunng87,2021-07-19T02:19:24Z,The patch is ready for review.
882283107,2795,oranagra,2021-07-19T06:41:05Z,"I think we wanna take this opportunity to add all the other useful options.
and we still need to conclude if we can / want to change the response type when any of these options are used (as requested in the linked issue).
maybe that's valid to do similarly as how SPOP changes return value when COUNT is added, or maybe we can add a GETTTL argument.
let's wait for further feedback..
"
883127992,2795,yoav-steinberg,2021-07-20T06:41:43Z,"`LT`/`GT`/`XX` sound good to me.
Regarding return value, if I'm correct 0 can still be a valid value telling us the key expired or never existed while anything larger than 0 will give us the current TTL (this will need to be adjusted to unixtime for `AT` variants and millisecs for `P` variants). This seems like a relatively small compatibility break, so I'm for it."
886203158,2795,sunng87,2021-07-25T13:35:36Z,@oranagra All issues resolved. Thanks for your patient and great suggestions. 
886210488,2795,oranagra,2021-07-25T14:30:45Z,"@sunng87 thanks..
can you also make a PR for https://github.com/redis/redis-doc/pulls ?"
886430306,2795,oranagra,2021-07-26T06:58:13Z,"@sunng87 please merge my indentation fixes suggestions (it seems this PR doesn't let me edit the code)
@redis/core-team please approve the new flags for the EXPIRE group commands."
886834564,2795,sunng87,2021-07-26T16:08:14Z,Doc changes created at https://github.com/redis/redis-doc/pull/1613
717120000,7953,oranagra,2020-10-27T09:49:06Z,"just mentioning that i didn't go over the code yet (just took a quick look at the comments @madolson gave). 

p.s. maybe in a followup PR, you can add module API callback so that modules can provide cloning functionality. but let's put that aside for now."
718262786,7953,swamp0407,2020-10-28T23:18:38Z,"thanks @madolson @oranagra @guybe7 !
I fixed the code, but I wasn't sure how to fix things like removing the fast flag and keeping the LRU.
Please review."
718812665,7953,oranagra,2020-10-29T15:02:33Z,"@swamp0407 you did remove the `fast` flag (added `use-memory` instead).
and i think the conclusion from the discussion was that we want it to behave like RESTORE, so we we use dbDelete and dbAdd (don't keep the LRU).
both of these aspects are fine in the last version of the code in that respect (unless we hear other concerns / opinions that support changing that).
I see you changed to lookupKeyRead, which is wrong IMHO, let's wait for @madolson to respond in that discussion."
721899433,7953,madolson,2020-11-04T18:28:51Z,"The lookupKeyRead behavior is weird, but it looks like it's well established in other places, so I guess we should keep that consistent. Still a couple of other open comments. @swamp0407 "
723064720,7953,swamp0407,2020-11-06T12:51:38Z,"thanks @oranagra @madolson 
In addition to the areas you suggested I should fix, I have also changed the code that copies the inset object.

Should I also do a fix to include the consumer group information in the digest of the stream key? I'm not sure I can implement it correctly.
"
723522346,7953,swamp0407,2020-11-08T02:41:12Z,"Thank you for reviewing my code. @oranagra 

Please check it out as I have corrected it.
I have also fixed my mistake that I found in the dupStream function.

And, I don't need to use the stream consumer group's digest, so I won't touch it."
723576576,7953,swamp0407,2020-11-08T13:21:31Z,"thanks @oranagra 
I have corrected my codes.

I wrote the code in reference to streamReplyWithRange in t_stream.c, but I didn't understand the streamReplyWithRange function correctly, so the part about adding nack to the group and consumer pel got complicated.

After reading the code again, I thought that the same key nack would never go into multiple people's pel.

I also fixed the part about creating a new consumer.
Let me know if there is  any problems with this fix.
"
723578987,7953,oranagra,2020-11-08T13:42:15Z,"@swamp0407 looks like i [responded](https://github.com/redis/redis/pull/7953#discussion_r519395397) to your last push before you submitted your last message.
AFAIK the entries in the PEL of the group and consumers point to the same memory (see rdb loading)."
724728192,7953,itamarhaber,2020-11-10T14:14:09Z,"Before we seal this one, I have a couple of notes:

* Should this be a multi-key command? I'm torn between liking the simplicity (e.g. `MOVE`), my general dislike of multi-key commands but acknowledging there may be a future (see `MIGRATE`).
* Should this support a `DELETE` (or similar) option to delete the source key? This would make this into an uber-command that could eventually replace/deprecate `MOVE` and `RENAME`
* The current implementation copies the source's TTL, if set. I agree with this default, but perhaps we'd like to add a `NOTTL` (or similar) option."
724738849,7953,oranagra,2020-11-10T14:31:50Z,"1. it must be a multi-key command, unless we only want to let it copy a key to another db (keeping it's name), which i think is a big miss
2. i would have considered it a good idea to add a DELETE argument if they where sharing code, but the fact is that if we do that, it'll just be a shortcut to the MOVE/RENAME implementation which is a `fast` command. so i'm not sure that's wise. but anyway, we can always add more arguments in the future.
3. i guess we can add `NOTTL`, `EX <ttl>`, and even `KEEPTTL`. not sure if that's needed, but either way we can add later."
724741679,7953,itamarhaber,2020-11-10T14:36:22Z,"WRT 1, I guess I meant variadic keys, something like `COPY SOURCE key [...] DEST key [...]`"
724744439,7953,oranagra,2020-11-10T14:40:53Z,"ohh... 
i don't see the value of that (just sending it inside a MULTI will do the same thing, no disadvantages).
has anyone ever requested such a thing for MOVE or RENAME?"
724867215,7953,madolson,2020-11-10T17:57:07Z,"I don't like the multi-key approach when there are flags involved, it really complicates the command. As oran said you can always multi-exec them. I also agree that we can add flags later as needed. 

From my understanding the follows up here are:
1. Reorganize the functions so that they are located in their respective types.
2. Extend the debug digest functionality so that it also includes consumer groups.

Since neither of these require a major consensus, I am OK with the current iteration. @redis/core-team conensus?"
728823710,7953,oranagra,2020-11-17T10:03:25Z,@swamp0407 thank you. merged.
712811430,7912,soloestoy,2020-10-20T12:23:58Z,"It's a very useful feature and already worked in our product env. I think is's safe to merge after @yossigo suggestion : )

BTW, this PR contains another change, record counts of object freed in bio, info reply modification needs a major decision too."
714394192,7912,yossigo,2020-10-22T10:22:44Z,"@soloestoy Thanks for the clarification!
@chenyang8094 chenyang8094 Please consider updating the PR description to more clearly indicate it includes both aspects (API change and INFO addition)."
714872366,7912,chenyang8094,2020-10-23T02:43:13Z,"> @soloestoy Thanks for the clarification!
> @chenyang8094 chenyang8094 Please consider updating the PR description to more clearly indicate it includes both aspects (API change and INFO addition).

Ok, I have updated it"
715368424,7912,oranagra,2020-10-23T14:13:45Z,"I would like to use this opportunity to add a few more changes in that area.

First, in the spirit of #7865, it would be nice if a module can get the compiled version of REDISMODULE_TYPE_METHOD_VERSION so that when registering the callbacks it knows which of them are gonna be used and which will be ignored.

Secondly, I know a few complicated modules also keep some global information about their keys (outside redis's dict), while this is a bit of a violation, they get away with it since redis is not able to do anything with that pointer without the module's assistance (not even free it), but now that redis can detach that key from the keyspace in the main thread, and free it in a background thread, i think we need to give the module a little bit more help.

What i think would solve it is to add another `detach` callback which will also carry the keyname being detached. this will solve two things.
1) In some say it covers for the missing keyname argument from the free callback (i know some modules really miss it).
2) It tells the module that this pointer is no longer part of the database, and also that it will be soon freed by a thread.

I would like to get some feedback from @MeirShpilraien, @swilly22, @guybe7.
If we agree on the details, can ask @chenyang8094 to extend this PR, or one of us can make a followup PR, but we probably don't wanna release that without these (and more?) additions (due to the version bump).

P.S. i vaguely remember there were a few other things we wanna do by iterating on module data structure (inside its value pointer), which would be nice to add soon too, but i don't recall what.
i.e. we already have `mem_usage` and `digest`, we're missing a `defrag` and i think something else, but i don't recall.
maybe CLONE support for #6599?"
715591510,7912,MeirShpilraien,2020-10-23T21:10:13Z,"@oranagra I agree we need the `detach` callback so we can also detach the object from globals and just free the memory on the free callback. I am not sure if the `detach` and `free_effort` can somehow combine, do we use the `free_effort` on more places other then free the key?"
715710513,7912,oranagra,2020-10-24T05:15:57Z,"@MeirShpilraien i meant that the `detach` callback should be called before a normal `free` too. 
This way the detach is always called when detaching a value, from the db, and the free callback is just to release the memory. 
Also, it mans you kinda have the key name for the non-lazy free. 
P.s it might be a good idea to add an `attach` callback too, so on `rdbload` you don't assume it was added to the db. This way we can some day do the deserialization in a thread, and the `attach` in the main thread, and modules that do funny things will do them in the attach and wont suffer from multi threading issues. "
716035230,7912,MeirShpilraien,2020-10-24T18:24:53Z,"@oranagra I get the idea of the `detach`, I was just wondering if it makes sense to combine the `detach` and the `free_effort` to make the API less verbose (for example, maybe the `detach` can return the free effort, and then Redis can choose if he wants to free the memory immediately or pass it to a background thread). Not sure if it's a good idea, just a thought, what do you think?

Regarding the `attach`, today we have the `loaded` keyspace notification for modules only, maybe we can somehow combine it there? Again to make the API less verbose?"
716107878,7912,oranagra,2020-10-25T07:52:15Z,"@MeirShpilraien i'm not sure about combining free_effort and detach. usually redis knows if it's willing to consider doing a lazy free or not, so combining these will force the module to ""compute"" the free effort even when there's no need.
on the other hand, this computation is meant to be quick (and doesn't have to be accurate).

regarding `loaded` keyspace notification, and attach, i don't think these two are related.
the keyspace notification is received for all keys types, and the attach would happen only for the module type.
in addition the attach can carry the actual module data pointer with it (possibly saving the module an RM_OpenKey).

@danni-m FYI - i think you also had trouble with `free` not providing the key name."
716363515,7912,yossigo,2020-10-26T07:29:18Z,"@oranagra I think all of this calls for a more careful design, and we don't necessarily have to address it all at once or as part of this PR.

One issue is exposing the two-step lazy free to modules, so they also get an `unlink` callback. This makes sense to me as a direct extension to this PR. It will require considering what is the expected outcome depending on what callbacks are/aren't implemented, and how this affects backwards compatibility.

The issue of the opposite `link` callback seems to me unrelated, and I'm not even sure it represents a problem at present time. Note that `rdbload` already does not imply the key gets linked (e.g. with `RM_LoadDataTypeFromString()`) and this is indicated by not having a key name associated with the `RedisModuleIO` context."
716499195,7912,oranagra,2020-10-26T11:54:28Z,"@yossigo i agree much of what i raised is not directly related and can be handled separately (just felt that this is an opportunity to raise it).

The detach / unlink thing though is a bit related, at least in the sense that we should avoid releasing the API in this PR until we finished designing that part too since we might wanna change that API.

p.s. even if the additional API doesn't change the API of this PR, i rather not increment REDISMODULE_TYPE_METHOD_VERSION twice.

regarding backwards compatibility, if the detach and free effort come in the same version, then i see no problem, old modules that don't implement, will always be freed from the main thread.

So what do you suggest? wanna call the core-team to approve it and merge it as is, and then open another PR to add / modify it?
Or try to at least integrate the detach / unlink callback into this PR?"
717656276,7912,chenyang8094,2020-10-28T02:33:16Z,"> @yossigo i agree much of what i raised is not directly related and can be handled separately (just felt that this is an opportunity to raise it).
> 
> The detach / unlink thing though is a bit related, at least in the sense that we should avoid releasing the API in this PR until we finished designing that part too since we might wanna change that API.
> 
> p.s. even if the additional API doesn't change the API of this PR, i rather not increment REDISMODULE_TYPE_METHOD_VERSION twice.
> 
> regarding backwards compatibility, if the detach and free effort come in the same version, then i see no problem, old modules that don't implement, will always be freed from the main thread.
> 
> So what do you suggest? wanna call the core-team to approve it and merge it as is, and then open another PR to add / modify it?
> Or try to at least integrate the detach / unlink callback into this PR?

I can understand what you mean very much. `detach` mainly solves the problem that `free` callback does not provide parameter key. When I was writing a module product, I did save the key separately in the index of the module, but it was really difficult for me to get this information when the key was deleted. I think `detach` is useful for `free/lazyfree`. If possible, I can extend this PR and add `detach` and `free_effort` to the v3 version."
717737266,7912,oranagra,2020-10-28T06:49:29Z,"@chenyang8094 i think that we all agree that `unlink` (better fits redis terminology) makes sense, and probably is a must-have feature in order to push the lazy free feature in.
We need to make sure this can work in a backwards compatible manner for old modules that are not aware of it, but i actually don't see any problem (seems it'll work good already with the trivial implementation).
Please go ahead and add this change to this PR, it'll be easier to discuss when we see the code."
718391465,7912,chenyang8094,2020-10-29T06:30:46Z,"> @chenyang8094 i think that we all agree that `unlink` (better fits redis terminology) makes sense, and probably is a must-have feature in order to push the lazy free feature in.
> We need to make sure this can work in a backwards compatible manner for old modules that are not aware of it, but i actually don't see any problem (seems it'll work good already with the trivial implementation).
> Please go ahead and add this change to this PR, it'll be easier to discuss when we see the code.

@yossigo @soloestoy @oranagra @MeirShpilraien I have tried to modify the code according to my own understanding, please review it again, thank you."
720560931,7912,oranagra,2020-11-02T15:58:58Z,"@chenyang8094 thank you.
one more thing i need to ask: we need a better commit comment that describes the changes.
one way is to edit / update the top comment in this PR, which i can use when squash-merging the PR (will create one commit in unstable).
Or if we want to keep it two separate commits, you need to edit the commit comments and force-push.

@redis/core-team please approve:
- new RM_GetTypeMethodVersion
- new module type callback for lazy free effort
- new module type callback for detaching a key from the database prior to freeing it (comes with key name and value)
- new lazyfreed_objects info field"
721079274,7912,chenyang8094,2020-11-03T12:11:33Z,"> @chenyang8094 thank you.
> one more thing i need to ask: we need a better commit comment that describes the changes.
> one way is to edit / update the top comment in this PR, which i can use when squash-merging the PR (will create one commit in unstable).
> Or if we want to keep it two separate commits, you need to edit the commit comments and force-push.
> 
> @redis/core-team please approve:
> 
> * new RM_GetTypeMethodVersion
> * new module type callback for lazy free effort
> * new module type callback for detaching a key from the database prior to freeing it (comes with key name and value)
> * new lazyfreed_objects info field

I have used rebase to merge the two commits into one, and re-edited the commit message. Please check again if there are any problems, thank you."
721589180,7912,soloestoy,2020-11-04T08:33:05Z,"I read the codes and discussions above, if I understand right the `unlink` callback in this PR is a notify way to tell module the specific key-value is unlinked from redis db's dict, it's OK to me.

And seems we don't implement the `detach`(I think this func means the move semantics) and `attach` yet, maybe need another PR right? @oranagra "
721604628,7912,oranagra,2020-11-04T09:03:35Z,"@soloestoy the `detach` is the `unlink` same thing.
for now we left the `attach` for some future PR."
721862927,7912,guybe7,2020-11-04T17:16:40Z,"(i have only read the code, not the correspondence)

why won't the unlink and free_effort functions take a RedisModuleKey* instead of just the value (and keyname)
inside the RedisModuleKey we can access the keyname, the value and the RedisModuleCtx"
721891301,7912,oranagra,2020-11-04T18:12:10Z,"@guybe7 it was discussed here:
https://github.com/redis/redis/pull/7912#discussion_r514304716
Please take a look and if you can, It would help if you can provide reasoning for changing it. 
I.E. What are the limitations of the current code. 
Thanks. "
721911470,7912,guybe7,2020-11-04T18:53:37Z,"@oranagra well, there are many things that are impossible to do without a Redis context.. like opening a key. logging is possible but it won't know which module logged it.
generally speaking, this is an API that we can't change later on, why not give as much information as possible?
i think everybody here knows how painful and annoying it is that the `free` CB doesn't have a keyname..."
721943349,7912,yossigo,2020-11-04T19:57:33Z,"@guybe7 Logging is no longer an issue, as there is a way to create a detached thread safe context from a regular context so the module identity is preserved.

I understand your concern but I also see the other side of giving modules too much rope, especially in such low-level callbacks that can be triggered by many unexpected code paths (just think about the re-entrancy options here!).

I think that the downside of creating and providing a RedisModuleKey for every data type callback (or at least the new ones we add here) is greater than the upside. Having an additional (optional) name parameter makes more sense to me, and based on past experience it may also be all that modules really need."
722276604,7912,guybe7,2020-11-05T10:06:20Z,@yossigo ok i'm convinced - but let's at least add the keyname to the free_effort function?
722298495,7912,oranagra,2020-11-05T10:47:46Z,"seems legitimate to me.. (adding key name to free_effort too).
anyone has any objection? or can we ask @chenyang8094 to add it and merge this monster (43 posts)?"
722385748,7912,chenyang8094,2020-11-05T13:42:45Z,"> seems legitimate to me.. (adding key name to free_effort too).
> anyone has any objection? or can we ask @chenyang8094 to add it and merge this monster (43 posts)?

@oranagra @guybe7  I don’t think this is really necessary. Let’s revisit the function of the `free_effort` callback. It is used to return the amount of work needed in order to free an module **value**, not a key. The release of the key is always synchronous. Therefore, `free_effort` only needs to pass the value and let the module return the corresponding effort based on this value. Its work is so simple and clear, which is why the `lazyfreeGetFreeEffort` function has only one parameter `value`. I don't understand the practical significance of adding a key parameter to `free_effort`. Or I did not fully understand what you mean, maybe you can explain it clearly, thank you."
722388151,7912,guybe7,2020-11-05T13:47:12Z,"@chenyang8094 well, let's say you have a bug in your free_effort function and you want to investigate it, one might want to add some logs, which will not be very informative without the keyname"
722401988,7912,chenyang8094,2020-11-05T14:10:45Z,"> @chenyang8094 well, let's say you have a bug in your free_effort function and you want to investigate it, one might want to add some logs, which will not be very informative without the keyname

I think we should consider the functions to be implemented when designing the API, rather than for the purpose of debugging. If this is the case, then many APIs have to be designed very bloated (such as rdb_save/rdb_load/mem_usage, etc.). The scenario you mentioned is more likely to appear in `free` callback. If you have to know which key is to be freed, because `unlink` must be the first to be called back, it is possible to use a global variable  to record, and so is `free_effort`. . In short, this is just a debugging method, the same as using GDB. This is not its job.This is my understanding."
722407232,7912,guybe7,2020-11-05T14:19:10Z,"@chenyang8094 yes, it's just for debugging/logging purposes AFAIU - but in that case, what's the justification of adding keyname to the `unlink` function? it's also for logging/debugging, no?"
722414904,7912,MeirShpilraien,2020-11-05T14:32:07Z,"I agree with @guybe7. I see more use-cases to get the key other than debugging. We say we pass the key to the unlink function because modules might save information out of the keyspace (in some global memory) and they want to ""unlink"" this memory also. In the same manner, modules might want to report the free_effort of this global memory and they might need the key name to find it."
722759284,7912,chenyang8094,2020-11-06T01:57:21Z,"> I agree with @guybe7. I see more use-cases to get the key other than debugging. We say we pass the key to the unlink function because modules might save information out of the keyspace (in some global memory) and they want to ""unlink"" this memory also. In the same manner, modules might want to report the free_effort of this global memory and they might need the key name to find it.

If there is a global memory related to this key that needs to be freed, then I think `unlink` can already provide the name of the key (because `unlink` will definitely be called before `free_effort`), which is also our original intention to add `unlink` callback. If must add the parameter **key** to `free_effort`, then the corresponding `lazyfreeGetFreeEffort` must also add too. This will lead to several linkage modifications. @soloestoy @yossigo @oranagra  Please see if this is necessary."
722875947,7912,chenyang8094,2020-11-06T06:12:22Z,"> @chenyang8094 yes, it's just for debugging/logging purposes AFAIU - but in that case, what's the justification of adding keyname to the `unlink` function? it's also for logging/debugging, no?

The purpose of `unlink` is not for debugging/logging, you can see the discussion above."
723447668,7912,oranagra,2020-11-07T13:40:42Z,"@chenyang8094 maybe i'm missing the point of your last post, but i think what @guybe7 meant is that in case of error, he'd like to log the key name (on both unlink and free_effort callbacks).

p.s. regarding your previous post, we currently call the `free_effort` before `unlink` (not after it).
```c
        robj *val = dictGetVal(de);
        size_t free_effort = lazyfreeGetFreeEffort(val);	        size_t free_effort = lazyfreeGetFreeEffort(val);


        /* Tells the module that the key has been unlinked from the database */
        moduleNotifyKeyUnlink(key,val);
```
we can probably change that, but i don't like modules to rely on the order of these.

at first i thought that modules should not include the global data in their estimated free_effort, since they must release that global data in the `unlink` (not in the `free` callback, since it can be called form a thread), but maybe some module would like to detach data from a global structure and attach it to the object, and that the freeing of that data will be done from the thread.

in such case the effort of the unlink remains small, and the one in free becomes bigger.

since we can't predict what some crazy modules will do or need, i think we should have the key in both callbacks. and i think we can probably change the order of these calls (just makes a little bit more sense).

@chenyang8094 i don't understand what you mean by ""This will lead to several linkage modifications""."
723563188,7912,chenyang8094,2020-11-08T11:20:17Z,"> @chenyang8094 maybe i'm missing the point of your last post, but i think what @guybe7 meant is that in case of error, he'd like to log the key name (on both unlink and free_effort callbacks).
> 
> p.s. regarding your previous post, we currently call the `free_effort` before `unlink` (not after it).
> 
> ```c
>         robj *val = dictGetVal(de);
>         size_t free_effort = lazyfreeGetFreeEffort(val);	        size_t free_effort = lazyfreeGetFreeEffort(val);
> 
> 
>         /* Tells the module that the key has been unlinked from the database */
>         moduleNotifyKeyUnlink(key,val);
> ```
> 
> we can probably change that, but i don't like modules to rely on the order of these.
> 
> at first i thought that modules should not include the global data in their estimated free_effort, since they must release that global data in the `unlink` (not in the `free` callback, since it can be called form a thread), but maybe some module would like to detach data from a global structure and attach it to the object, and that the freeing of that data will be done from the thread.
> 
> in such case the effort of the unlink remains small, and the one in free becomes bigger.
> 
> since we can't predict what some crazy modules will do or need, i think we should have the key in both callbacks. and i think we can probably change the order of these calls (just makes a little bit more sense).
> 
> @chenyang8094 i don't understand what you mean by ""This will lead to several linkage modifications"".

Yes, the `free_effort` order is indeed wrong, I have modified it. I have added the parameter **key** to `free_effort`, which also led to the modification of the `freeObjAsync` function. Please review it, thank you."
724951181,7912,oranagra,2020-11-10T20:34:19Z,"so in my eyes, only thing that's left is a [call](https://github.com/redis/redis/pull/7912#discussion_r519400486) to `moduleNotifyKeyUnlink` in `dbOverwrite`
@chenyang8094 can you take care of this?"
725071599,7912,madolson,2020-11-11T01:34:56Z,@yossigo i'll buy the simplicity argument. I do think we should say returning 0 should be explicitly documented that it calls the async free function every time. Seem reasonable?
725071729,7912,chenyang8094,2020-11-11T01:35:21Z,"> so in my eyes, only thing that's left is a [call](https://github.com/redis/redis/pull/7912#discussion_r519400486) to `moduleNotifyKeyUnlink` in `dbOverwrite`
> @chenyang8094 can you take care of this?

@oranagra @yossigo Yes, I understand very well, it is very necessary to do things right. @oranagra  @yossigo Thanks for your review, I also thought that `moduleNotifyKeyUnlink` also needs to be called during `flushall/flushdb`, right? Since I have been busy with the Double 11 these past two days, I will update PR again after this. Thank you for your suggestions and patience."
725499564,7912,oranagra,2020-11-11T15:48:57Z,"@chenyang8094 good point about FLUSHDB, and more importantly FLUSHDB ASYNC.
but i don't think we can afford to run on the entire keyspace when detaching the dict from the database, just to call the module unlink callback.

@MeirShpilraien WDYT? maybe it should be documented that when the module gets the flushdb event hook it should expect the unexpected?"
725504829,7912,MeirShpilraien,2020-11-11T15:58:03Z,@oranagra so on flushbd async we will not get the unlink callback and module should use the flush event to handle such case? LGTM but we need to make sure to document it.
725524082,7912,oranagra,2020-11-11T16:31:35Z,"@chenyang8094 trying to sum up what's left (64 comments, LOL):
1. call moduleNotifyKeyUnlink in dbOverwrite (better be done before calling dictSetVal)
2. when free_effort returns 0, always do an async free (consider that an infinite return value)
3. document the above.
4. document in the unlink callback, that it won't be called on flushdb (both sync and async), and the module can use the RedisModuleEvent_FlushDB to hook into that. "
727824371,7912,oranagra,2020-11-16T08:34:58Z,"merged.
@chenyang8094 thank you for your patience and dedication."
