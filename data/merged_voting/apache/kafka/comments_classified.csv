id,pr_number,body,distilbert_sentiment_label,codebert_sentiment_label,deberta_sentiment_label,distilbert_confidence,codebert_confidence,deberta_confidence,majority_label,final_decision,decision_reason
297865351,2929,can you review this patch? the patch has been rebased onto trunk. both integration test and the newly-added system test has passed. thanks.,1,1,1,0.9651951789855956,0.880834698677063,0.9610828757286072,1.0,accept,unanimous_agreement
298077449,2929,i have gone through the code again after rebasing the patch and it should be ready for review.,0,0,0,0.9834294319152832,0.969407320022583,0.9924506545066832,0.0,accept,unanimous_agreement
306559832,2929,": thanks for the patch. haven't looked at it in details. just a couple of quick comments. (1) once a disk is marked offline, it might be useful for the admin to be able to fix the bad disk (e.g., remounting) while the broker is online. this reduces the time that a broker has to be down while waiting for a disk to be fixed. if we do support this, it would be useful to test this out a bit. (2) according to [a link] it seems that an i/o exception in mmap buffer crashes the jvm. this will affect the failure detection in jbod. not sure how much we can do to improve it in the short term. at the minimum, it would be useful to do some testing to see how much this impacts us and document the impact.",1,1,1,0.9581487774848938,0.8895887732505798,0.96886146068573,1.0,accept,unanimous_agreement
306989136,2929,"thanks for helping with the review! i wanted to rebase earlier but got delayed due to the busy oncall week. i should be able to rebase/test the patch and review it myself again by this friday. hope we can get this reviewed and committed soon instead of another major rebase :) thanks for the suggestion. it is definitely useful for the admin to be able to fix the bad disk while the broker is running. i think it requires a new kip in addition to kip-112 and kip-113 because it takes new tool script and notification event which is not currently included in those two kips. is it ok for me work on this as a new kip after kip-112 and kip-113 is completed? the handling of i/o exception in mmap seems similar to the sixth future work mentioned in kip-112 wiki, i.e. handling various failure scenario case-by-case. kip-112 currently only handles disk failure that can be caught in the form of ioexception instead of jvm crash. i will think about how to handle this i/o exception and let you know my answer.",1,1,1,0.9931106567382812,0.9952215552330016,0.9964962601661682,1.0,accept,unanimous_agreement
307003408,2929,"the patch is ready for review. i have rebased it onto the latest trunk, addressed comments, passed integration tests and gone over the patch myself. thanks!",1,1,1,0.9814428687095642,0.9892861247062684,0.986565887928009,1.0,accept,unanimous_agreement
309918401,2929,"""t is definitely useful for the admin to be able to fix the bad disk while the broker is running. i think it requires a new kip in addition to kip-112 and kip-113 because it takes new tool script and notification event which is not currently included in those two kips. is it ok for me work on this as a new kip after kip-112 and kip-113 is completed?"" : for the above, i wasn't referring to the case when the admin fixes the disk and then notifies the broker about the fix online. the case that i was referring to is that when a disk is marked offline, if the admin can fix the disk while the broker is still online, then the admin can just restart the broker quickly after the disk is fixed. the downtime window for that broker is just the restart time. if the admin has to first shut down the broker and then fix the disk, the time for fixing the disk is part of the downtime window. so, my question is that is there any issue that prevents the admin from fixing the offline disk while the broker is up (e.g., the mmap-ed files on the bad disk).",0,0,0,0.6597643494606018,0.8144981265068054,0.9180817008018494,0.0,accept,unanimous_agreement
309958368,2929,"thanks for the explanation. i think i understand your point now. my understanding is that you are concerned with the possibility of broker crash with single disk failure due to mapped files issue. because i don't have multiple disk devices on my desktop to test mount/unmount of log directories while broker is running, i will test it in dev test cluster and i am waiting for sre to mount/unmount log directory for me since they have the sudo access. i will let you know the result tomorrow. i have also gone through the blog of mapped files you provided. most of the issue (e.g. limited number of mmap handlers) are the same between raid-10 broker and jbod broker. since we are not having them now, i assume we won't have them with jbod in the near future. the only issue that is more likely to happen with jbod is that jvm may crash when disk gets full. it is more likely to happen with jbod because the single jbod log directory will have smaller size than the raid-10 log directory. we currently use mmap files for reading/writing to index files. the solution suggested by the blog is to write to index files using `filechannel` instead of mmap files. it is mentioned that performance is minimal if large blocks are used. however, it is not clear whether there will be significant performance hit for index file write operation since the block size for index file write is small. suppose we can not use `filechannel` to write to index files due to performance reason, then i think the solution is for kafka administrator to properly monitor the usage of log directories on brokers and rebalance usage across log directories before any log directory gets full. for example rebalance can be triggered if usage exceeds 60%. in the rare case that there is extreme traffic spike that exhausts the space of a log directory, it seems ok to just let this broker fail. it will reduce availability of the broker in any unnecessary manner but it seems ok if this happens rarely. to summarize, i will test how broker handles umount of log directory while it is running. and kip-113 provides a reasonable solution to deal with vm crash due to full disk. does this sound reasonable?",1,1,1,0.8262093663215637,0.9615357518196106,0.9672363996505736,1.0,accept,unanimous_agreement
310195927,2929,i have updated the code (see [a link] and verified that the all files handlers in the offline log directory will be removed and we can unmount the disk while the broker is still running after the corresponding log directory goes offline.,0,0,0,0.9871270656585692,0.9926179051399232,0.9945582151412964,0.0,accept,unanimous_agreement
310785873,2929,"talked to offline. i think there might be a cleaner way to handle the disk exceptions. it looks that what we want to do when disk exception happens is the following: 1. notify controller there is a disk failure and mark the disk as failed. 2. return an error to whoever sent the request if the failure was triggered by a request. i think we can have a util method to do (1) immediately when the disk io occurs, and bubble up a kafkastorageexception which is a retriable exception. when the clients received this exception, it will act accordingly. the benefit of doing this is that we don't need to care about the what exact operation was in progress when the disk io exception occurs. the leader will always be moved to somewhere else and the related operation will be retried if needed.",0,0,0,0.8979358077049255,0.9816833734512328,0.9661317467689514,0.0,accept,unanimous_agreement
310809552,2929,"after further discussion with , we agree not to handle ioexception in asyc read for fetchrequest. i will add `logmanager` to the constructor of `logcleaner` and `logcleanermanager` so that they can call `logmanager.handlelogdirectoryfailure(...)` when there is `ioexception`. in addition, `partition.getorcreatereplica(...)` will check whether the log of an existing local replica is actually offline and throw `kafkastorageexception` if it is offline. we need this extra check because with the change in logcleaner, it is possible for replicas to be offline in `logmanager` while they are still in the cache `replicamanager.allpartitions`. this guarantees that if `logcleaner` encounters `ioexception`, controller will be notified via zookeeper path and `leaderandisrresponse` will tell controller the offline replicas.",0,0,0,0.9825248122215272,0.9938212633132936,0.9889425039291382,0.0,accept,unanimous_agreement
310817415,2929,"regarding your suggestion to go through usages of all methods in `filerecords`. there are currently many io related operations in kafka whose ioexception is either explicitly swallowed or not explicitly handled. i am not sure we should find out all of them and mark the corresponding log directory and replicas as offline in this patch. one reason is that this maybe over engineering and even reduce the availability of kafka in an unnecessary manner. given that we are not explicitly shutting down broker after catching those ioexception, either most log directory failure will already be caught and trigger `exit.halt(1)`, or kafka broker may be working just fine with its current way of handling those ioexception. thus it may not do us much benefit to handle them in a different way. it is also about the efficiency and the priority of work. i agree with you that there exists ioexception that we can handle in a better way to improve the quality of jbod support. but this task seems independent to the features added in this patch. the main goal of this patch is to change the way we handle log directory failure (i.e. those that currently trigger `halt()`) and it has covered most log directory failure (i.e. those that trigger ioexception during producerequest, fetchrequest, checkpoint read/write and logcleaner read/write). on the other hand, what you asked for is what other ioexception should be treated as log directory failure. i am wondering if we can do that separately in a follow-up patch after kip-112 and kip-113 are implemented. this may help speedup the overall implementation efficiency and reduce the need for rebase by splitting a large patch into smaller ones. also, by finishing kip-112 and kip-113 first, we can deploy the jbod feature sooner and the experience from its deployment can help us determine whether we need to handle some exception differently.",0,0,0,0.921944797039032,0.9703881740570068,0.9051088094711304,0.0,accept,unanimous_agreement
311121325,2929,": i will try to make a pass of the patch later this week as well. about the disk failure detection, we can probably start with something simple. currently, we only fail a broker on ioexception during writes. we can probably just treat that as disk failure to start with. it would be useful to make the failure detection part a bit more general so that we can incorporate more sophisticated failure detection in the future (e.g., consistent long i/os, on-disk crc failure, etc).",0,0,0,0.9405105113983154,0.9924875497817992,0.9376205801963806,0.0,accept,unanimous_agreement
311122631,2929,"it would be great to have your review! thanks! yeah i also would like to start with something simple. becket and i have agreed to limit the scope of disk failure in this patch to mainly those that currently cause `exit.halt(1)` as of the current kafka implementation. and yes, we need to expand it later to make the failure detection more general. for example, detection of long i/os is included as the sixth future work in the kip-112.",1,1,1,0.9878860712051392,0.9929147958755492,0.9963911175727844,1.0,accept,unanimous_agreement
311594553,2929,"thanks much for your review! i have updated the patch to address all the comments. i have added one integration test to simulate the log directory failure and verify that producer will receive notleaderforpartitionexception. note that producer request does not necessarily trigger ioexception immediately after log directory failure because log append operation uses mmap and its write operation to disk is delayed. on the other hand, our write operation to checkpoint file can see ioexception immediately because it uses filechannel. and if the ioexception is triggered when broker writes to checkpoint file, produceresponse will show notleaderforpartitionexception instead of kafkastorageexception. i will try to add another test to verify that producer can send message after retry.",1,1,1,0.9865745902061462,0.97945374250412,0.9919021725654602,1.0,accept,unanimous_agreement
311597230,2929,"it seems that has finished reviewing the patch and i think the kip-112 implementation is in good shape. on the other hand, i am in the process of writing kip-113 and it should be ready for review this week. i am not sure if you have time to review both kip-112 and kip-113. kip-113 is like going to be more complicated than kip-112 and needs closer look by senior committers. i am wondering if you can help review kip-113 if you have time for only one kip. thanks!",1,1,1,0.9315871596336364,0.9889490604400636,0.9912461638450624,1.0,accept,unanimous_agreement
311820470,2929,"i have updated the newly-added test and it verifies that the producer will see notleaderforpartitionexception when there is log directory failure and succeed after it retries. i have also rebased the patch onto trunk. all tests have passed except for the one test in kafka connector, which i don't think is caused by this patch.",0,0,0,0.9831757545471193,0.9799780249595642,0.9402201175689696,0.0,accept,unanimous_agreement
311823840,2929,: i am taking a look a the patch now. should be done by tomorrow.,0,0,0,0.9570634961128236,0.9712604880332948,0.9298561215400696,0.0,accept,unanimous_agreement
311824795,2929,thanks much!,1,1,1,0.9719381928443908,0.9401196837425232,0.9186553955078124,1.0,accept,unanimous_agreement
312411970,2929,"thanks so much for detailed review. i have addressed most of the comments in the updated patch. the main remaining issues are 1) whether we should invoke `replicamanager.handlelogdirfailure()` in `logmanager.handlelogdirfailure()` and 2) whether/how we can handle `ioexception` in the methods of `transactionstatemanager`. i have provided explanation under the corresponding comments. btw, there is one comment regarding a line in `partition.scala` that can only be replied in [a link] but not in this page.",1,1,1,0.9811896085739136,0.9550435543060304,0.9784339070320128,1.0,accept,unanimous_agreement
313317734,2929,thanks much for the comment .,1,1,1,0.7687335014343262,0.544926643371582,0.5755707025527954,1.0,accept,unanimous_agreement
313796522,2929,"finally i was able to fix the test failure by closing the newly added logdirfailurehandler thread, removing newly-added when the replicamanager shutdown, and optimizing the `logmanager.livelogdirs` not to instantiate new array when there is no offline log dir. these are only causing test failure in github (not on my machines) probably because the virtual machine used in github is slower with less memory. the patch is fully ready for review again :)",1,1,1,0.8726813793182373,0.9955081939697266,0.9911670088768004,1.0,accept,unanimous_agreement
314612960,2929,"excuse me.. i am wondering if there is any further issue to be addressed with the patch. or if there is no major issue and if you are busy, is it ok for becket to give it a final pass and merge it?",-1,-1,0,0.7676095962524414,0.8809893727302551,0.8365232348442078,-1.0,accept,majority_agreement
314808010,2929,: i started looking at your latest patch. i will finish my review today.,0,0,0,0.8756312727928162,0.6513437628746033,0.9797266721725464,0.0,accept,unanimous_agreement
314838404,2929,: thanks much for your time! i also reviewed the entire patch again and found 3 minor issues. i will address all of them after your review.,1,1,1,0.9907726049423218,0.987619161605835,0.9931339025497437,1.0,accept,unanimous_agreement
315308226,2929,"thanks so much for taking so much time to review this patch! i thought the patch is ready but clearly there was space for improvement. i have updated the patch to address all comments. i have gone over the patch carefully with the hope that i don't miss anything this time. besides minor changes such as the method comment and removal of unused import, here are the bigger changes that were made: - simplified the way we handle kafkastorageexception such that we no longer need to invoke `maybeaddlogfailureevent()` if `kafkastorageexception` is caught. - added a final static variable `replicamanager.offlinepartition` and use it to make sure that kafkastorageexception is consistently returned when client attempts to access an offline replica. this is verified with the updated test code. - replaced all usage of `kafka.common.kafkastorageexception` with `org.apache.kafka.common.errors.kafkastorageexception` so that we can remove `kafka.common.kafkastorageexception` going forward. - added metric `offlinereplicacount` which counts the number of offline local replicas on a broker. - added logdirfailurechannel to log and logsegment and invoke `maybeaddlogfailureevent()` right after the ioexception is caught. - renamed the new exception to unknownerrorcodeexception",1,1,1,0.9886959791183472,0.9748873114585876,0.992957353591919,1.0,accept,unanimous_agreement
315517505,2929,"this afternoon i reviewed the patch myself by going over the entire patch line by line. i improved the comments in a few places, removed unnecessary imports and reverted a few changes that have become unnecessary now. i sincerely think that i should have fixed these before your previous review.. for ease of review, i have squashed the commits before your previous review into one commit ([a link]. the second commit ([a link] includes the change that were made to address your previous comments from the previous review. the summary of this change can be found in the earlier comment i posted yesterday. the 3rd commit ([a link] ) improved comment and reverted unnecessary changes. some major changes where made based on that previous two rounds of review from you, e.g. introduction of the logdirfailurechannel and the consistency of kafkastorageexception in response. i am 95% confident now that the patch won't need similar major change anymore in its current state. i will give this patch another two rounds of end-to-end review, make sure that there is indeed no further improvement i can figure out by myself, before giving it to you for further review. thanks again for your time!",1,0,1,0.7108097076416016,0.5997154116630554,0.721525251865387,1.0,accept,majority_agreement
315590692,2929,"i gave the patch another round of review, made some minor improvements (e.g. comment) and fixed system test failure. the only notable change is that `kafkastorageexception` and `unknowncodeexception` now extends `invalidmetadataexception`. this is needed so that producer can update metadata if the leader replica is now offline. these changes are in [a link] commit. i will give the patch the third review tomorrow.",0,0,0,0.971595048904419,0.9901519417762756,0.7333003282546997,0.0,accept,unanimous_agreement
315707331,2929,i have carefully reviewed the patch three times and made all the improvements i can think of. all system tests and integration tests have passed. would you have time to review the patch sometime this week? thank you! there is only one improvement that i haven't made in this patch. currently the producer will still send producerequest with version 3 if the message magic version is 2. this is because the newly-added produce version 4 is incompatible with 0.11 broker. the downside of this is that server will translate kafkastorageexception to notleaderforpartitionexception. but this should not cause any problem to the functionality of the producer. we can fix this in a followup patch if needed.,0,1,1,0.5153802037239075,0.9813135862350464,0.9904698133468628,1.0,accept,majority_agreement
315982647,2929,thanks so much for the quick and detailed review! i have addressed most of the comments. please see my reply above.,1,1,1,0.9901201725006104,0.9945918917655944,0.9951570630073548,1.0,accept,unanimous_agreement
316247280,2929,i have updated the patch and addressed all the comments. all tests have passed. i have reviewed the latest commit and it looks ok to me. i am reviewing the entire patch with all the commits.,0,1,1,0.8790882229804993,0.5933583974838257,0.6937696933746338,1.0,accept,majority_agreement
316276230,2929,thanks much for all the help! i have rebased the patch onto trunk head and finished reviewing the patch. the following changes were made in the latest commit. - catch ioexception before it is propagated to replicamanager - removed a few other ioexception handling now that the ioexception will be caught and thrown as kafkastorageexception in log's methods. - halt broker on log directory failure if inter.broker.protocol < 0.11.1 - removed unknownerrorcodeexception - updated upgrade.html,1,1,1,0.9885138869285583,0.9903899431228638,0.9936338663101196,1.0,accept,unanimous_agreement
316926503,2929,"thanks much for the quick help! i have rebased the patch onto trunk head and addressed all the comments. in particular, i have further moved ioexception from logsegment to those methods in log and updated the comments as suggested. i have reviewed the entire patch again end-to-end and fixed everything to my best knowledge. it should be ready for your review again. thanks!",1,1,1,0.990331768989563,0.9952422380447388,0.9954198598861694,1.0,accept,unanimous_agreement
317064344,2929,"thank you so much for taking so much time to review the patch. i definitely think the current patch is much more consistent in style and cleaner than it was before. initially i was focusing only on the functionality. i will focus more on the code style and consistency next time, e.g. in kip-113. i have addressed jun's comments and rebased patch onto trunk head. all tests have passed. i will also review the patch again myself. can you take a look at the patch? thanks!",1,1,1,0.9868191480636596,0.9922651052474976,0.9951692223548888,1.0,accept,unanimous_agreement
317103017,2929,i have finished reviewing the patch and made minor changes in the latest commit. i verified that the patch has passed all ducktape system tests.,0,0,0,0.9737339615821838,0.9642255306243896,0.9291926622390748,0.0,accept,unanimous_agreement
317157179,2929,thanks much for your help! i have updated the patch to address all comments except those that can be done in the followup patch. and the patch has been rebased onto trunk head.,1,1,1,0.991232931613922,0.977240264415741,0.99184650182724,1.0,accept,unanimous_agreement
656589433,9001,i just noticed that you haven't updated the code which creates the `apiversionsresponse` in `saslserverauthenticator`. is it intentional or something left to be done?,0,0,0,0.9719942808151244,0.98542582988739,0.9914728403091432,0.0,accept,unanimous_agreement
656902905,9001,"thank you for taking a look! iiuc you are referring to these lines: [a link] my requirement is that under the hood of the newly added api: `org.apache.kafka.clients.admin#describefeatures`, the `apiversionsresponse` returned to the `adminclient` needs to contain the features information. note that this new api issues an explicit `apiversionsrequest` under the hood. in such a case do you think i should populate the features information in the above lines in `saslserverauthenticator` too? i'm trying to understand where would this come into play (sorry i know little to nothing about `saslserverauthenticator`).",1,1,1,0.9843313097953796,0.972069263458252,0.98844575881958,1.0,accept,unanimous_agreement
659752993,9001,retest this,0,0,0,0.9849403500556946,0.956061065196991,0.9793913960456848,0.0,accept,unanimous_agreement
659793027,9001,retest this,0,0,0,0.9849403500556946,0.956061065196991,0.9793913960456848,0.0,accept,unanimous_agreement
700585584,9001,"thanks a lot for the review! i've addressed the comments in the recent commit: 06d8b47131f168db88e4f7d5bda3dd025ba9a2a2. i've provided a response to all of your comments. there are few i couldn't address, and 1-2 comments i'll address in the near future.",1,1,1,0.9908559322357178,0.9929937720298768,0.9936659932136536,1.0,accept,unanimous_agreement
702023940,9001,"thanks for the review! i've addressed the comments from your most recent pass in a7f4860f5f8bb87cfb01452e208ff8f4e45bcd8b. to answer your question, the deployment will fail if the feature was finalized at say `[minversionlevel=1, maxversionlevel=6]` previously, but the new broker only supports version range: `[minversion=4, maxversion=6]`. this is where `firstactiveversion` becomes useful. by bumping it up during a release (instead of the supported feature's `minversion`), we are able to get past this situation. when `firstactiveversion` is advanced in the code, and the cluster is deployed, the controller (and all brokers) know that the advancement acts a request to the controller to act upon the feature deprecation (by writing the advanced value to `featureznode`). so, in this case we would release the broker with the supported feature version range: `[minversion=1, firstactiveversion=4, maxversion=6]`, and the deployment wouldn't fail.",1,1,1,0.9767707586288452,0.9759522080421448,0.9873286485671996,1.0,accept,unanimous_agreement
702026788,9001,thanks for the review! i've addressed the comments in c31d6b5245c635e659ff0f203bd08bc015a15ffb.,1,1,1,0.9844849109649658,0.9860641956329346,0.9889233708381652,1.0,accept,unanimous_agreement
702621705,9001,thanks for the review comments! i have done the change proposed in [a link] in the most recent commit: 4218f95904989028a469930d0c266362bf173ece . please have a look.,1,1,1,0.9852358102798462,0.9801186323165894,0.9928688406944276,1.0,accept,unanimous_agreement
703048608,9001,"thanks for the review! i've addressed the comments, the pr is ready for another pass. i've also fixed the compilation errors.",1,1,1,0.9778745770454408,0.991933286190033,0.992224097251892,1.0,accept,unanimous_agreement
703873486,9001,thanks for the review! i've addressed the latest comments in e55358fd1a00f12ef98fc4d2d649a297ddf146da . the pr is ready for another pass.,1,1,1,0.9830915331840516,0.990879237651825,0.9878206849098206,1.0,accept,unanimous_agreement
704393633,9001,: any more comments from you?,0,0,0,0.9463761448860168,0.9807887673377992,0.9886167645454408,0.0,accept,unanimous_agreement
704596581,9001,"the test failure in `mirrorconnectorsintegrationtest.testreplication` does not seem related. i have rebased the pr now against latest ak trunk, i'd like to see if the failure happens again.",0,0,0,0.9767048954963684,0.9744440913200378,0.989267885684967,0.0,accept,unanimous_agreement
704786154,9001,the test failures in the latest ci runs do not seem related to this pr: * jdk 8: `org.apache.kafka.connect.integration.exampleconnectintegrationtest.testsourceconnector` * jdk 11: `org.apache.kafka.streams.integration.eosbetaupgradeintegrationtest.shouldupgradefromeosalphatoeosbeta` the test that failed previously under jdk 15 has passed in the latest ci run: `mirrorconnectorsintegrationtest.testreplication`.,0,0,0,0.9766725301742554,0.995072901248932,0.9920592308044434,0.0,accept,unanimous_agreement
705068372,9001,: there are 27 system test failures with this pr. [a link] are they existing test failures compared with [a link] ?,0,0,0,0.9710078835487366,0.9934394955635072,0.9910210967063904,0.0,accept,unanimous_agreement
705073059,9001,"thanks for the links! i had a look at the links. i found similar stats in both links, with exactly 27 test failures in both links. i compared the individual test failures and found that they have all failed on the same tests. would that mean we are ok to merge this pr (since it doesn't seem to introduce a new failure)?",1,1,1,0.9742069840431212,0.9915105104446412,0.9917942881584167,1.0,accept,unanimous_agreement
705079102,9001,": thanks for following up. i will merged this pr as it is since the system test failures are not new. also, in scala, we prefer sealed traits over enumeration since the former gives you exhaustiveness checking. with scala enums, you don't get a warning if you add a new value that is not handled in a given pattern match. maybe you can address that in your followup pr.",1,1,1,0.9507796168327332,0.8943139910697937,0.9788358807563782,1.0,accept,unanimous_agreement
705378300,9001,sorry for bringing trivial comments after this is merged. i just noticed those nits in testing new apis in 2.7.0.,-1,-1,-1,0.9894053339958192,0.992623805999756,0.9929087162017822,-1.0,accept,unanimous_agreement
705419934,9001,"no worries, thanks for the suggestions! i have opened a separate pr addressing your comments. would you be able to please review it? [a link]",1,1,1,0.9883341789245604,0.9929437041282654,0.9947340488433838,1.0,accept,unanimous_agreement
276871714,2476,-rosenblatt i have manually tested it successfully using the `./bin/kafka-purge-data.sh` in the patch. i am going to add unit test and probably ducktape integration test as well. but the core code should be ready for review. would you have time to review the patch?,0,0,0,0.9702541828155518,0.9464831352233888,0.9362202286720276,0.0,accept,unanimous_agreement
277101595,2476,", thanks for the pr. i probably won't have time to review before next week. cc as well since he reviewed the kip.",1,1,1,0.9309491515159608,0.9752041697502136,0.978408694267273,1.0,accept,unanimous_agreement
279289445,2476,-rosenblatt i have added tests and the patch is fully ready for review. would you have time to review this patch?,0,0,0,0.8714249730110168,0.9209185242652892,0.975707709789276,0.0,accept,unanimous_agreement
279343352,2476,thanks for the patch. it seems the patch has conflicts. could you rebase?,1,1,1,0.7443819046020508,0.886552095413208,0.9223902225494384,1.0,accept,unanimous_agreement
279471864,2476,i thought it will take 1+ week for the patch to be reviewed and there will be conflict again anyway. thus i was going to rebase it after first round of review. what is our general guideline for rebasing big patches? i can certainly rebase it now if you think it is useful.,0,0,0,0.9396243095397948,0.9686024785041808,0.9850539565086364,0.0,accept,unanimous_agreement
279524819,2476,all conflicts have been resolved and all tests are passed. thanks!,1,1,1,0.9843260645866394,0.9924522042274476,0.9877304434776306,1.0,accept,unanimous_agreement
279567997,2476,"thanks for updating the patch. i'll take a look. usually if there are multiple big patches in parallel, the committers who are reviewing the code would hold back some of the patches to avoid unnecessary rebase.",1,1,1,0.8141388297080994,0.7161868810653687,0.96309494972229,1.0,accept,unanimous_agreement
281270870,2476,thanks so much for taking time to review the patch! can you check if the updated patch has addressed your comments?,1,1,1,0.9875401854515076,0.9289849996566772,0.9882556200027466,1.0,accept,unanimous_agreement
281594223,2476,i have addressed all your comments and all tests have passed. could you take another look? thanks!,1,1,1,0.9712234139442444,0.9947236776351928,0.9734085202217102,1.0,accept,unanimous_agreement
282936279,2476,do you have time to take a look? given that we probably have a few big patches from kip-98 (and potentially kip-82 and kip-112). we probably need to synchronize on them to avoid unnecessary rebase. it might be better to get this patch in first. what do you think? thanks.,1,1,1,0.7808355689048767,0.8732680082321167,0.8646334409713745,1.0,accept,unanimous_agreement
283340571,2476,"also, it would be good to merge trunk into this branch. i know this can be annoying, but the request/response code has changed enough that some parts of the review can't be done properly without that.",-1,0,0,0.8981440663337708,0.5500573515892029,0.9755402207374572,0.0,accept,majority_agreement
283426390,2476,"thanks for reviewing the patch. this kip depends on the kafka-4820 and i will rebase the patch after kafka-4820 is committed. and i will also explain the purgedatacommand in the mailing thread and the kip wiki. initially we considered to use purge, truncate or delete in the method name. i don't have a strong preference among them. i chose purge because joel/becket prefers it and no one else raises concerns with it in the open source discussion. i think one reason is to distinguish it from `deletetopic` method. now that we have `data` in the name, i agree that it may be better to use `deletedatabefore`. let me propose it in the open source mailing list. i think the purpose of including `data` in the method name is to distinguish it from `deletetopic` method name. the reason of not having `data` in the api name purgerequest is because `producerequest` or `fetchrequest` doesn't include `data` in the name. the reason of including `before` in the method name is because we may want to have a method to delete data after a given offset in the future.",1,1,1,0.5860020518302917,0.8587996363639832,0.949891984462738,1.0,accept,unanimous_agreement
283438149,2476,thanks for explaining the reasoning for the name choices. let me think about it and see how it fits with the other proposed adminclient methods.,1,1,1,0.8545352816581726,0.594362735748291,0.949396789073944,1.0,accept,unanimous_agreement
284215633,2476,i have addressed all the comments and rebased the patch onto trunk. do you have time to review the latest patch?,0,0,0,0.9826006293296814,0.9833412766456604,0.9948177933692932,0.0,accept,unanimous_agreement
284963025,2476,thanks much for your review. i have updated the patch to address most of the comments. can you see if my reply to your comments make sense?,1,1,1,0.9707301259040833,0.9160307049751282,0.973227858543396,1.0,accept,unanimous_agreement
285518268,2476,do you have time to give the patch another pass? i have gong through the patch and all the reviews and the patch seems good now.,0,0,0,0.5995041728019714,0.6752936840057373,0.6417263746261597,0.0,accept,unanimous_agreement
285582265,2476,: have you run system tests on this pr?,0,0,0,0.98201584815979,0.9935790300369264,0.9941755533218384,0.0,accept,unanimous_agreement
285972512,2476,sorry for late reply. i didn't run ducktape-based system tests previously. i have been trying to run `tests/kafkatest` since your comment. but it seems that some tests will fail even on kafka trunk without my patch. i am doing more detailed analysis to see if my patch is the cause of any test failure.,-1,-1,-1,0.9894444346427916,0.990358293056488,0.9874558448791504,-1.0,accept,unanimous_agreement
286312386,2476,some tests fail consistently even without my patch if i run ducktape-based system test on my desktop. i am not sure if it is because the setup on my desktop. becket just told me that it may be more reliable to run system test via [a link] but all recent system tests have failed due to error `importerror: no module named setuptools` since last friday. do you know who can look into this problem and fix the jenkins setup?,0,0,0,0.7843039631843567,0.6841356754302979,0.639737606048584,0.0,accept,unanimous_agreement
286471347,2476,: sorry for the inconvenience. we just fixed an issue in the branch builder. could you try it again?,-1,-1,-1,0.9881644248962402,0.9902024269104004,0.993374526500702,-1.0,accept,unanimous_agreement
286545382,2476,thanks much for the quick help! please ignore my previous comment.. i thought my test has succeeded but i was looking at the wrong test result. it should take another 3 hours for my test to finish.,1,1,1,0.988249897956848,0.9935078620910645,0.9934127926826476,1.0,accept,unanimous_agreement
286560968,2476,"i noticed that in a test i started yesterday using `system-test-kafka-branch-builder-2` (instead of `system-test-kafka-branch-builder`), my branch has indeed passed all tests. see [a link] the patch has been rebased onto trunk without any conflict and `nextoffsetfromlog()` has been made private. do we need further review or test? is there anything else i can do? thanks for help!",1,1,1,0.9739872217178344,0.952191948890686,0.9891610741615297,1.0,accept,unanimous_agreement
286928771,2476,"i have renamed `purgerequest` to `deleterecordsrequest`, `purgeresonse` to `deleterecordsresponse`, and `purgedatabefore()` to `deleterecordsbefore()`. for simplicity and consistency of internal code, i am still using `purge` to refer to `deleterecords` when `records` is not explicitly specified as the operation target. for example, i use `logpurgeresult` instead of `logdeleteresult` or `logdeleterecordsresult`. and `delayedpurgepurgatory` instead of `delayeddeletepurgatory` or `delayeddeleterecordspurgatory`. this should address jun's goal of differentiating between removing whole log vs removing portion of the log since we only use `delete` when `records` is specified. can you review this patch again? thanks!",0,0,0,0.9832206964492798,0.9946554899215698,0.9905312657356262,0.0,accept,unanimous_agreement
287506966,2476,thanks for the patch. lgtm except one pending comments. do you have time to take another look? thanks.,1,1,1,0.9438236951828004,0.9915850162506104,0.9823870658874512,1.0,accept,unanimous_agreement
287695596,2476,thanks for the comment! i have addressed most of them. can you check if my reply make sense?,1,1,1,0.988249659538269,0.990971565246582,0.9927390217781068,1.0,accept,unanimous_agreement
288525644,2476,test with jdk 8 and scala 2.11 failed because `kafkastreamstest.testcannotstartonceclosed` took more 2+ hours before it is killed. `./gradlew -pscalaversion=2.11 streams:test --tests org.apache.kafka.streams.kafkastreamstest` works on my machine. thus the failure is probably not caused by this patch. all system tests have passed. see [a link],0,0,0,0.9607561826705932,0.9885073900222778,0.9877618551254272,0.0,accept,unanimous_agreement
288563254,2476,": all comments are addressed. all integration tests and system tests have passed. and has reviewed the latest patch. since we are so close to finish review for this patch, would you have time to review the patch again so that we can merge it before other major change in trunk?",0,0,0,0.9321140646934508,0.9763087630271912,0.5548630952835083,0.0,accept,unanimous_agreement
288931216,2476,"all comments have been addressed as suggested. all tests have passed except `org.apache.kafka.streams.integration.kstreamaggregationdedupintegrationtest.shouldreduce` with scala 2.12, which i don't think is caused by this patch. thanks for all your time to review the patch!",1,1,1,0.9834507703781128,0.9888907670974731,0.989155113697052,1.0,accept,unanimous_agreement
289188244,2476,thanks for the review. yeah it took me quite a while to rebase the patch. i just finished rebase and the integration tests have passed. i will run system tests as well. because it is very time consuming to rebase each single commit. i end up squash the all commits into one commit onto trunk. can you see if my reply to your most recent comment make sense?,1,1,1,0.8021131157875061,0.9745657444000244,0.9532040953636168,1.0,accept,unanimous_agreement
289336913,2476,the patch has been rebased again on ismael's most recent commit. all integration and system tests have passed. can you review the latest patch? thanks!,1,1,1,0.9611859321594238,0.9837254881858826,0.9612260460853576,1.0,accept,unanimous_agreement
289613754,2476,"as of current patch the logstartoffset of compacted topic is always 0. it seems reasonable because if consumer seek to offset 0 of a compacted topic, they will still be able to consume (instead of receiving offsetoutofrangeexception) even if the offset of the first message is larger than 0. and it is not straightforward to retrieve the offset of the first message in the segment. according to java doc, `recordbatch.baseoffset()` will `return the first offset of the original record batch for magic version prior to 2`, which means the baseoffset of the first batch of the first log segment doesn't necessarily tell us the offset of the first message in the log. does this sound reasonable?",0,0,0,0.9786460995674132,0.9882414937019348,0.9908258318901062,0.0,accept,unanimous_agreement
289655962,2476,"it seems that right now, for a compacted topic, the base offset of the first segment is always 0. so, the patch is fine.",0,0,0,0.9537915587425232,0.9919528365135192,0.9736289381980896,0.0,accept,unanimous_agreement
289656024,2476,: thanks for the patch. lgtm. : do you want to make another pass and then merge?,1,1,1,0.9259454011917114,0.9796974062919616,0.9825415015220642,1.0,accept,unanimous_agreement
289827754,2476,all integration tests have passed except `org.apache.kafka.connect.runtime.workertest.testaddremovetask` with scala 2.11. i don't think this is caused by this patch because this test passed all other recent tests. thanks so much for taking time to review this patch!,1,1,1,0.9891571998596193,0.9832102060317992,0.993254005908966,1.0,accept,unanimous_agreement
289838515,2476,thanks for updating the patch. merged to trunk. thanks for the review!,1,1,1,0.9776110649108888,0.974005401134491,0.9931790828704834,1.0,accept,unanimous_agreement
283204521,2614,"cc a couple initial points of discussion: 1. message class hierarchy. the `logentry` interface now represents the record set and `logrecord` represents the record. i have left `records` with its current name which is a bit weird when you need to write things like `records.entries.records`. i have also been thinking of changing `logentry` to `logrecordset`, which would make the `records` name a bit more intuitive. previously i considered changing `records` to simply `log`, though that's a little annoying because of the server class with the same name. 2. the new magic version is implemented in `eoslogentry` and `eoslogrecord`. this name should be changed of course. any suggestions? the old magic version is implemented in `oldlogentry`, so we could call it `newlogentry`?",0,-1,0,0.6652546525001526,0.9647103548049928,0.772894561290741,0.0,accept,majority_agreement
283401129,2614,"fyi: system tests run here: [a link] the one failure looks like a known problem, but i haven't investigated.",0,0,0,0.9495686292648317,0.97633695602417,0.9884845614433287,0.0,accept,unanimous_agreement
283982801,2614,", good questions! 1. i don't like the `set` suffix because order is relevant, so if we wanted to go that way, i think it should be `logrecordslist`. which is kind of just `logrecords`, which is confusing given that it's part of `records`. is record set just a record batch? 2. instead of `newlogentry`, should it be `defaultlogentry`? and `oldlogentry` could be `legacylogentry` or something like that.",1,1,1,0.9870298504829408,0.9729636907577516,0.9930047392845154,1.0,accept,unanimous_agreement
284025203,2614,"thanks for the suggestions. 1. it is a batch. the producer already uses `recordbatch`, but maybe we could commandeer that name? i'm not too fond of `logrecordslist`. it's a bit tough to come up with something reasonable given the name of `records`. i'd rename that personally to something which conveyed the fact that it was a sequence of bytes of the log (but of course i already tried that and failed). 2. `defaultlogentry` sounds good to me. no strong preference between `oldlogentry` and `legacylogentry`. but perhaps ""legacy"" is a bit more suggestive of its use. the other thing i wanted to mention is that i've left around the old `record` class more or less as it currently is. this is why i needed to introduce the `logrecord` interface. this seemed fine given use of the ""log"" prefix in `logentry`, but i've considered several times moving the current `record` into `oldlogentry`, and then using `record` in place of `logrecord`. what do you think?",1,1,1,0.9501662254333496,0.9691131711006165,0.9847307205200196,1.0,accept,unanimous_agreement
284029702,2614,"i'm not fond of `logrecordslist` either, in case it was not clear from my message. :) i was thinking that maybe we could rename the existing `recordbatch` to `producerbatch` or something. yes, i prefer `legacy` a bit for the reason you state. i think it makes sense to rename `logrecord` to `record`. the current record could then either be moved to `legacylogentry` (already using this name ;)) or it could just be `legacyrecord`.",1,1,1,0.9796518683433532,0.993431031703949,0.99314546585083,1.0,accept,unanimous_agreement
284031945,2614,"so if we follow those suggestions, then we have the following hierarchy: [code block] that seems reasonable to me. the `recordbatch` -> `producerbatch` renaming will probably cause some confusion, but people will get over it.",0,0,0,0.9704809188842772,0.973712682723999,0.9852132201194764,0.0,accept,unanimous_agreement
284033990,2614,", what do you think?",0,0,0,0.9668228030204772,0.9634602069854736,0.9417104721069336,0.0,accept,unanimous_agreement
284537436,2614,"a few other high level comments. 1. in the eos message format, it's probably worth considering including the message count in the message format. this has the benefit that the consumer can allocate the right size of the array to store those messages and is also consistent with how we represent an array in other places. 2. about eoslogentry and oldlogentry. perhaps we can name them based on the magic. so, it would be v2logentry, v1logentry and v0logentry.",0,0,0,0.901813507080078,0.9919252991676332,0.9094181060791016,0.0,accept,unanimous_agreement
284557074,2614,", about including the message format version as a prefix or suffix. i considered that, but the issue is that more than one version is supported by each class. unless we want to move away from that, it seems a bit awkward.",0,-1,-1,0.5978465676307678,0.8707265257835388,0.5151100754737854,-1.0,accept,majority_agreement
285983415,2614,": currently, the broker supports a debuggingconsumerid mode for the fetch request. should we extend that so that the consumer can read the control message as well? should we also have some kind of debuggingmessageformatter so that consoleconsumer can show all the newly introduced fields in the new message format (e.g., pid, epoch, etc) for debugging purpose? both can be done in a followup patch if needed.",0,0,0,0.985759139060974,0.9928207993507384,0.9901124835014344,0.0,accept,unanimous_agreement
286815104,2614,": also, it would be great if you could do some basic perf test to make sure there is no noticeable performance degradation.",1,0,0,0.8078712821006775,0.9703906774520874,0.7095149755477905,0.0,accept,majority_agreement
289112773,2614,ran system core and client system tests here: [a link] the failing zookeeper upgrade test seems to be transient. here's a passing run: [a link] i investigated the failure in one case and it seems to be unrelated (apparently caused the overridden min isr setting of 2 and the fact that the test does not actually ensure that 2 brokers are always up).,0,0,0,0.9851834177970886,0.9904488921165466,0.9918631315231324,0.0,accept,unanimous_agreement
289124423,2614,lgtm!,1,1,1,0.7413927316665649,0.9057464599609376,0.8055883049964905,1.0,accept,unanimous_agreement
308997035,3131,please close this pr and use the mailing list for questions.,0,0,0,0.9807594418525696,0.9877907037734984,0.9933521747589112,0.0,accept,unanimous_agreement
330710008,3874,"also, in the request protocol and objects, we have alter_replica_dir_request and describe_log_dirs_request. could we make them consistent?",0,0,0,0.9894142150878906,0.9931562542915344,0.9943600296974182,0.0,accept,unanimous_agreement
330768947,3874,"thanks much for reviewing the patch! i have answered all comments and addressed most of them. i think it is reasonable to rename alter_replica_dir_request to alter_replica_dirs_request. given that it is merely a refactor and it touches many files, should we do this in a follow up patch before or after this patch is merged? i am not sure if you prefer to see this refactor in this pull request because it may make it harder to review this patch.",1,1,1,0.982559323310852,0.9616563320159912,0.9891554713249208,1.0,accept,unanimous_agreement
330771247,3874,i have update the patch to help discussion. i will fix test failures tomorrow.,0,0,0,0.9773569703102112,0.9586617946624756,0.9888213872909546,0.0,accept,unanimous_agreement
330891633,3874,we can probably do the request naming fixes in a follow-up. i also raised some issues regarding the naming of the adminclient methods. maybe we can consider fixing those in the same follow-up.,0,0,0,0.9816930294036864,0.9938172698020936,0.9904946088790894,0.0,accept,unanimous_agreement
330891767,3874,", there is a merge conflict and 6 test failures.",0,0,0,0.938265860080719,0.9222041964530944,0.896422803401947,0.0,accept,unanimous_agreement
331022962,3874,", let me know when is a good time to start a system tests run.",0,0,0,0.9405156970024108,0.969697654247284,0.9708704352378844,0.0,accept,unanimous_agreement
331024201,3874,"sure. i am trying my best to address commands and add test. can you tell me the deadline for getting this patch committed and pass system tests if we were to include it in 1.0 release? btw, i am flying to shanghai and will be on vacation 9/21 - 10/16. my response maybe a bit delayed due to the flight and various errand. admittedly it makes it harder for me to focus on the patch and the test. i will do best effort.",1,-1,0,0.5630216002464294,0.8496652245521545,0.5818274617195129,,review,no_majority_disagreement
331086912,3874,"i have rebased the path onto trunk head and fixed most tests. currently i disabled 5 tests in replicafetcherthreadtest because these tests replied on the previous internal implementation of replicafetcherthread.maybetruncate() which is changed in my patch. i tried to fix the test for 1 hour but couldn't probably due to my lack of experience with easymock library. although all tests can pass, it seems that there is file descriptor leak because my mac explains about `too many open files in system`. i will try to fix it tomorrow. i don't have a good idea where it comes from.. i think you can run the system test with my patch. it is probably a good idea to also run the system test without this patch.",0,0,0,0.7997032999992371,0.5631060600280762,0.7527862191200256,0.0,accept,unanimous_agreement
331087122,3874,"fyi, there is one remaining issue related to how we rename directories to replace current log with the future log. this may also take some time to discuss and implement.",0,0,0,0.9845666289329528,0.9775257706642152,0.9880759716033936,0.0,accept,unanimous_agreement
331212055,3874,i started the system tests a few hours ago. i will check and post the status soon.,0,0,0,0.974709451198578,0.9672320485115052,0.9882254600524902,0.0,accept,unanimous_agreement
331243188,3874,"i have fixed all tests in replicafetcherthreadtest and rebased patch onto latest trunk. ""too many open files in system"" seems to stop showing up when i run tests on mac laptop after rebasing the patch. the only remaining issue is how to promote future replica to be the current replica. i am awaiting jun's reply.",0,0,0,0.9164934754371644,0.9713941216468812,0.8426180481910706,0.0,accept,unanimous_agreement
333150135,3874,"there was a discussion about potentially tweaking the name of the adminclient and request/response for `alterreplicadir` and `describelogdirs`. the adminclient api would have to be changed before the code freeze next wednesday. what is the current thinking? two points from me: 1. as i raised previously, `alterreplicadir` doesn't follow the naming convention of other adminclient methods that are `plural`. i think we should definitely fix this one. 2. the lack of symmetry between `alterreplicadir` and `describelogdirs` is not ideal. could we not simply call the latter `describereplicadirs` as well? this would have to be a separate pr, but i just raised it here since there was an ongoing discussion about it. happy to take it to a separate pr or jira.",0,0,0,0.8154770731925964,0.9865129590034484,0.898744523525238,0.0,accept,unanimous_agreement
333152696,3874,"regarding 1), i think it is a good idea to rename `alterreplicadir` to `alterreplicadirs`, and similarly rename the corresponding request and response. i will submit a patch to do it. regarding 2) , describelogdirs has this name because it returns the information per log directory. there is already an existing adminclient api `describereplicalogdir` which returns the log directory information per replica. so i am not sure we need to have symmetry between alterreplicadir and describelogdirs.",0,0,0,0.8219431042671204,0.9675635695457458,0.8706763386726379,0.0,accept,unanimous_agreement
333157758,3874,"sounds good , i had missed the other method.",1,1,1,0.9226396083831788,0.8730019927024841,0.771960973739624,1.0,accept,unanimous_agreement
333158559,3874,"talking about describereplicalogdir, maybe we should rename this to describereplicalogdirs as well?",0,0,0,0.98139888048172,0.9917517304420472,0.9907926917076112,0.0,accept,unanimous_agreement
333159096,3874,"yes. also, shouldn't `describereplicalogdir` be `describereplicadirs`? i thought that this one was supposed to be the dual of `alterreplicadirs`?",0,0,0,0.9854567646980286,0.9941748976707458,0.988226354122162,0.0,accept,unanimous_agreement
333160232,3874,"it is describereplicalogdir because we return the log directory of the partition in the response, rather than the directory of the partition. the directory of the partition is essentially path_to_log_dir/topi-partition. previously i used alterreplicadirs because alterreplicalogdirs seems a bit tedious. technically speaking we are changing both the log directory and the directory of the partition. so it seems fine to say alterreplicadirs. do you think we should rename it to alterreplicalogdirs?",0,0,0,0.8986905813217163,0.9622710943222046,0.9799531698226928,0.0,accept,unanimous_agreement
333161547,3874,"oh, i see. yes, it does seem to me that it would be clearer if they both talked about `logdirs`. it may make sense to hear if has any thoughts before doing the renaming as it's a bit tedious to change it a second time.",0,0,0,0.8849189877510071,0.8760209083557129,0.965218186378479,0.0,accept,unanimous_agreement
333276886,3874,all issues have been addressed as discussed. i have made a pass through the patch and made minor improvements. i will pass through the patch another 2 times and let you know.,0,0,0,0.9689865708351136,0.9270983338356018,0.7899742126464844,0.0,accept,unanimous_agreement
333287139,3874,"also, thanks so much for taking time to review the patch! your comments are really helpful.",1,1,1,0.989750325679779,0.9944902658462524,0.9958027005195618,1.0,accept,unanimous_agreement
333711087,3874,yes. i think we should commit [a link] first. i am not sure about the details of the 1.0 release plan. my understanding is that we should focus first on the stability for 1.0 release at this time. thus we should commit bug fixes such as [a link] before 1.0 code freeze. this patch can be committed later after we have finished all necessary works needed for 1.0 release. i will work on your comments. thanks!,1,1,1,0.964734673500061,0.9923245906829834,0.991275429725647,1.0,accept,unanimous_agreement
333935484,3874,"thanks for your detailed comments! i have rebased patch onto trunk, renamed methods following kafka-5995, reviewed patch myself end-to-end, and replied to all comments. i think the only remaining issue is whether future replica should be truncated using leader epoch after the current replica is truncated.",1,1,1,0.9835909008979796,0.985939621925354,0.9917930960655212,1.0,accept,unanimous_agreement
335013622,3874,i have rebased patch onto trunk and replied or addressed the latest comments. i will review the patch end-to-end after we have agreed on the solution to the latest comments. thanks!,1,1,1,0.984259068965912,0.9882631897926332,0.9851509928703308,1.0,accept,unanimous_agreement
335988493,3874,let me review the patch one more time before committing this patch. thanks!,1,1,1,0.9749329686164856,0.9813352823257446,0.9487960934638976,1.0,accept,unanimous_agreement
336215355,3874,": there are a few system test failures on your branch. [a link] most of them seem to be related to the old consumer, which is potentially affected by the abstractfetcherthread change in this patch.",0,0,0,0.983346939086914,0.9883561730384828,0.990877330303192,0.0,accept,unanimous_agreement
336464525,3874,"there are 6 system test failure in the link you provided. i run system test locally for 3 of the 6 tests and all of these tests succeeded. then i downloaded the log for these three tests (e.g. `kafkatest.sanity_checks.test_verifiable_producer`). for all these three tests, verifiable_producer.stdout shows the following exception: [code block] i am not sure how this exception could be related to this patch since this patch does not touch the constructor of producerrecord. could you re-run the test, maybe locally, and see if the failure can be reproduced? could you tell me which git hash was used in the system test? btw, i made one more commit to further improve the patch after i reviewed the patch end-to-end.",0,0,0,0.9389808773994446,0.978669047355652,0.9898743629455566,0.0,accept,unanimous_agreement
336473772,3874,you should rebase against trunk to fix the `nosuchmethoderror` issue.,0,0,0,0.9837032556533812,0.9912125468254088,0.9943254590034484,0.0,accept,unanimous_agreement
336475395,3874,sure. i have rebased the patch onto trunk. is this `nosuchmethoderror` a known issue which has been recently fixed in the trunk?,0,0,0,0.987936794757843,0.994031012058258,0.9899978041648864,0.0,accept,unanimous_agreement
336476902,3874,yes [a link],0,0,0,0.9842668175697328,0.9636505842208862,0.9936238527297974,0.0,accept,unanimous_agreement
336480112,3874,thanks for the information. this explains the test failure and it is very likely that system test can succeed now that i rebased the patch onto trunk. could you re-run the system test for this patch? i have finished reviewing the patch myself and made all the improvements i can think of. can you see if this patch is good to go? thanks!!,1,1,1,0.9899606704711914,0.9943183064460754,0.9952021837234496,1.0,accept,unanimous_agreement
337089038,3874,thanks for all the review! i have addressed all comments and rebased the patch onto trunk. i will go over the patch again. do we need to re-run system test for this patch?,1,1,1,0.9844682812690736,0.993777871131897,0.992100179195404,1.0,accept,unanimous_agreement
337093789,3874,": thanks for the latest patch. lgtm. i kicked off another run of system tests. once the tests finish, i can merge the patch.",1,1,1,0.9789577722549438,0.969093918800354,0.9826656579971312,1.0,accept,unanimous_agreement
337138220,3874,thanks much! i have reviewed the patch again and slightly improved the error message.,1,1,1,0.9880401492118835,0.98423033952713,0.9889305233955384,1.0,accept,unanimous_agreement
337272700,3874,: thanks a lot for working on the pr. merged to trunk.,1,1,1,0.9767497181892396,0.9835579991340636,0.9777013063430786,1.0,accept,unanimous_agreement
337406756,3874,this is great. thank you so much for all your time and review!!,1,1,1,0.993586003780365,0.9957410097122192,0.9973377585411072,1.0,accept,unanimous_agreement
208608858,1215,it seems this conflicts with kafka-3510. i'll do the rebase.,0,0,0,0.923208475112915,0.9589375257492064,0.9930700659751892,0.0,accept,unanimous_agreement
208640666,1215,cc . will you be able to help review this patch? thanks!,1,1,1,0.9727978706359864,0.9793052077293396,0.9822123050689696,1.0,accept,unanimous_agreement
214130026,1215,"also, could you patch the dumplogsegment tool to support the time index?",0,0,0,0.9901276230812072,0.9938030242919922,0.9955015778541564,0.0,accept,unanimous_agreement
214479103,1215,", it would be nice to have some numbers for how long it takes to do search by timestamp before and after the change on a cluster with a reasonable amount of data.",0,0,0,0.9309239387512208,0.9780223369598388,0.96073979139328,0.0,accept,unanimous_agreement
214572966,1215,"is there a specific use case where the performance of searching by timestamp matters much? supposedly searching by timestamp should be a relatively rare operation. with logappendtime, the time will be similar to offset search. with createtime, the time cost largely depends on how the data look like. i.e. if we have to scan tons of logs, it is going to be slow, otherwise it is fast. one thing for sure is that it is going to be slower than before which did not do much except for looking at the last modification time of segment files.",0,0,0,0.9544540643692015,0.9449018239974976,0.9799071550369264,0.0,accept,unanimous_agreement
215312071,1215,"as a general rule, we should be very thorough with regards to performance testing whenever we introduce a new disk structure (like an index). i gave one example, but we should really also verify the impact on normal operations as well. even if we believe that the impact should be minimal, we cannot be sure until we have tested it.",0,0,0,0.9634541273117064,0.971895158290863,0.98147451877594,0.0,accept,unanimous_agreement
215997683,1215,"thanks for the review. i see, yes, i agree that we should understand the performance impact of the changes we make in general. i was just curious if you care about the search by timestamp specifically. i updated the patch to let the index point to the offsets of shallow messages instead of the next offsets. would you be able to take another look? i am not sure if we are still able to make it into 0.10.0 given 0.10.0 should have cut off on yesterday.",1,1,1,0.8820239901542664,0.8472165465354919,0.9553864002227784,1.0,accept,unanimous_agreement
223086486,1215,i addressed your previous comments. would you take another look?,0,0,0,0.978314995765686,0.982796549797058,0.9911053776741028,0.0,accept,unanimous_agreement
224696267,1215,": thanks for updating the patch. as the patch is getting closer to be ready, could you run your branch on the system tests? also, could you do some performance testing like you did in kip-31/32 to see if there is any noticeable performance degradation?",1,0,1,0.9379582405090332,0.6968458890914917,0.9690921306610109,1.0,accept,majority_agreement
225242928,1215,thanks a lot for the review. i have updated the patch. the only pending thing is whether we want to allow the empty time index or not. i replied to your comment. please let me know if you think the benefit is good to have or not. i have sent email to the mailing list regarding the time based log rolling behavior change to see if people have any concern. i will run the performance test as we did for kip-31/32 and update the result.,1,1,1,0.9769529104232788,0.9811091423034668,0.9868773221969604,1.0,accept,unanimous_agreement
227025455,1215,thanks a lot for the review. i just updated the patch to address your comments. i will create another kip to discuss the new search for timestamp semantic.,1,1,1,0.9740343689918518,0.9783408045768738,0.9845693707466124,1.0,accept,unanimous_agreement
227599503,1215,": thanks for the latest patch. i made another pass and left some comments. most of them are minor though. once you have addressed those, it would be good if you could run through our system tests and post some performance testing results on the jira. ideally, we probably want to load some sizable amount of data on at least 2k - 4k partitions in a broker and see if there is any performance degradation.",1,1,1,0.954216718673706,0.8854914903640747,0.970784604549408,1.0,accept,unanimous_agreement
228078724,1215,thanks a lot for the patient and careful review. i am on vacation in china from jun 22 to jul 11. the network becomes a little flaky. could you help kick off a system test? i will see if i can go to our newly opened shanghai office and use the vpn there to run the performance test on our test cluster. thanks again.,1,1,1,0.9858850240707396,0.9917171597480774,0.9923263788223268,1.0,accept,unanimous_agreement
231480822,1215,"also, in dumplogsegments, could we dump the timestamp in the .log file too?",0,0,0,0.9895987510681152,0.9952476620674132,0.9950193166732788,0.0,accept,unanimous_agreement
231481580,1215,": i made another pass and left a few more comments. it would be good if you can run some performance tests to see if there is any degradation. for instance, with this pr, we will need to read the last entry of each time index during broker startup. not sure how much impact this will have on starting time. so, it will be good to test this out.",0,0,0,0.6854627728462219,0.8150893449783325,0.8442391753196716,0.0,accept,unanimous_agreement
232181730,1215,"thanks for the review and sorry for the late response. i just returned from china yesterday. i have updated the patch to address your comments. because we are in the middle of data center cutting over, it may take some time to setup the new dev test environment. i will do the performance test this week.",-1,-1,-1,0.9863344430923462,0.913223683834076,0.9554991722106934,-1.0,accept,unanimous_agreement
232449796,1215,"also, i left a comment in the jira about making the output of dumplogsegments clearer. could you take a look?",0,0,0,0.9891668558120728,0.9884981513023376,0.9943304061889648,0.0,accept,unanimous_agreement
232879331,1215,"i just updated the patch to address the latest comments. one thing worth mentioning is that dumping the active time index may be slow because time index requires looking up the offset index. the offset index of the active segment is always the max index size. this will cause the dump log segment tool to create an offset index with a lot of empty index entries in the end so the binary search in this case does not quite work. also because the active segment is still growing, dumping it may have some race condition and result in false alarm. so i suppose it is fine to assume that people won't dump the active log segment.",0,0,0,0.9306665658950806,0.977891445159912,0.9541313052177428,0.0,accept,unanimous_agreement
233226827,1215,: thanks for the latest patch. it looks good to me. do you have any updates on performance tests and the system tests?,1,1,1,0.984384536743164,0.9855878353118896,0.9923071265220642,1.0,accept,unanimous_agreement
233382135,1215,"thanks for the review. i have tested the produce and consume performance with the same script we used for kip-31/32. the producer and consumer performance are the same. our new dev clusters are ready for use now, but i am still waiting for the mirror maker instances, which should be setup by today. i will run the performance test on the real data and test the segment loading and recovery time with more partitions after that.",1,1,1,0.955860197544098,0.9579312801361084,0.9694300293922424,1.0,accept,unanimous_agreement
233383485,1215,"for system test, i was trying to use the confluent jenkins server before. but it seems that it does not support building against an arbitrary branch anymore. i am setting up a hudson job to run the system test internally at this point. but that may take some time as i am not familiar with that.",0,0,0,0.5854587554931641,0.837670624256134,0.9356231093406676,0.0,accept,unanimous_agreement
233386240,1215,", you can use [a link]",0,0,0,0.986526906490326,0.9902244210243224,0.9937390089035034,0.0,accept,unanimous_agreement
233400013,1215,got it. thanks.,1,1,1,0.9481641054153442,0.9407241940498352,0.9930570125579834,1.0,accept,unanimous_agreement
234819292,1215,"sorry for the late response. i have run the performance test. on a broker with ~15500 log segment files. the log segments loading time is about 6 seconds without this patch. on another broker with 17700 log segments files, the log loading time was 35 seconds with this patch. the results varied a little from run to run, but it is 4x - 6x slower when we need to load the timestamp from the log segment. for a large kafka cluster, this might be an issue. one optimization may be load the log maximum timestamp lazily. i'll update the new patch after testing that. i did not see any throughput issue with this patch. so the impact is only at startup time.",-1,-1,-1,0.9893224835395812,0.9919307827949524,0.9850639700889589,-1.0,accept,unanimous_agreement
235023979,1215,: thanks for the results. so the 4x-6x overhead mostly comes from reading the last entry of the time index? you also did producer/consumer perf test on a large number of topic/partitions and saw no degradation in throughput?,1,1,1,0.9572269916534424,0.784633219242096,0.9637874960899352,1.0,accept,unanimous_agreement
235178030,1215,"i made some further test. the previous 4x - 6x slow down may not be very accurate. i noticed a bug in the code that may cause unnecessary rebuild of the indexes if a time index is empty. after fixing the bug, on a broker with ~20000 log segment files, i got the following results: 1. log loading time: ~13 seconds without time index. 27-28 seconds with time index.: 2. i tested a patch that skips sanity check and loads the max timestamp lazily so we don't read the last entry at starting time. the time reduced by 1-2 seconds but not much. so it looks that the slow down was mainly caused by the system calls that create the memory mapped files. because we are creating 2x many memory mapped files, it takes about 2x of the time. i tested the throughput with part of the production data. the brokers handled fine. and the produce request total time and local time looks similar. i will try to push more traffic and see if that makes any difference.",0,0,0,0.9018348455429076,0.9569109678268432,0.952622413635254,0.0,accept,unanimous_agreement
235442848,1215,": thanks for the update. is memory-mapping the index really that expense? could you do some micro-benchmark to test this out? also, have you run the system tests?",1,1,1,0.9292631149291992,0.665910542011261,0.9669370055198668,1.0,accept,unanimous_agreement
235443110,1215,"yes, i have run system test and it passed. sure, i can run the micro benchmark on mmap.",0,0,0,0.9544271230697632,0.9627801179885864,0.957768738269806,0.0,accept,unanimous_agreement
235443205,1215,the link to system test run. [a link],0,0,0,0.9878871440887452,0.970846712589264,0.9947516322135924,0.0,accept,unanimous_agreement
235774517,1215,"i did some investigation on the segment loading and here are the findings: 1. mmap() and resize() takes much time during reloading. 2. mmap() speed dramatically slows down after 20000 files. 3. resize() is also much slower when timeindex files are added. the summary can be found here: [a link] in the graph each bucket contains 250 mmap() or resize() call. the time is the summary of the time taken by all the calls in that bucket. for example, the first 250 mmap() calls during log loading goes to the first bucket, the next 250 mmap() calls goes to the second bucket, and so on. i am not sure what is the right solution here. maybe there are some os level tweaks to avoid the deterioration of the mmap() time and resize() time, but i haven't looked into that yet.",0,0,0,0.9421011209487916,0.987904131412506,0.9395155310630798,0.0,accept,unanimous_agreement
236021027,1215,": thanks for the results. the results in ""log loading time distributing"" shows that loading the log with timestamp index takes 15 times longer than without (399 secs vs 21 secs). is that correct? if the overhead is in mmap(), with timestamp index should be at most twice as slow. does this suggest there is sth in the timestamp index that is much more expensive than the offset index?",1,1,1,0.9617135524749756,0.5549567937850952,0.9715718626976012,1.0,accept,unanimous_agreement
236038932,1215,"sorry i did not explain it clearly. in the google sheet there are also two sheets showing that the time taken by mmap() is related to how many files has already been memory mapped. after we have mapped ~20000 files, the time taken for further mmap() calls increased dramatically. the reason we see a 15 times larger loading time is the following. currently i have about 22000 log segments in a broker. so that results in 22000 offset index files. it took 21 seconds to load. when we do mmap for those files, it is roughly at the tipping point where the mmap() becomes taking much longer. now we added the time index file so the number of mmap() call is doubled. even though the first 22000 mmap() calls took the same time as before, which is 21 seconds, the next 22000 mmap() calls took much longer. it does not matter whether the mmap() calls are for offset index file or time index file. because after adding the time index, presumably the first 22000 mmap() calls are half/half for each type of index files. i am not sure if this is os specific and only an issue for our environment or it is a general issue.",-1,-1,-1,0.9893625378608704,0.9822508692741394,0.9617859721183776,-1.0,accept,unanimous_agreement
236065457,1215,: what happens if you do an experiment with 44k segments and turn off the time index? does that also take 15 times longer? were the mmap results based on a micro benchmark or did you extract that from the segment loading test?,0,0,0,0.9678719639778136,0.9797303676605223,0.9903541803359984,0.0,accept,unanimous_agreement
236270191,1215,"i ran test with 8500 log segments with time index files. they are loaded in less than 3.5 seconds. and it took 1.5 second without time index files. not sure if that is equivalent to testing files with 44k, but apparently the time does not grow linearly with the number of log segments. the mmap results are extracted from the segment loading test. micro benchmark which repeatedly maps a single file does not reveal this scalability issue.",0,0,0,0.9821194410324096,0.952118754386902,0.979872465133667,0.0,accept,unanimous_agreement
236301416,1215,": thanks for the new results. it feels a bit weird that the turning point is at exactly 22k segments. also, in the ""time to resize"" sheet, the results seem to suggest that resizing at 10 batches with time index is 6 times more expensive than resizing at 10 batches w/o time index. this seems to be at the pointe where the number of loaded segments is still well below 22k? is it possible to do an experiment with 44k segments and turn off the time index? this will tell us more about whether the overhead comes from scalability related to mmap() or something else in the time index.",1,1,1,0.8840315341949463,0.8071300983428955,0.6566397547721863,1.0,accept,unanimous_agreement
236312009,1215,"i refined the profiling code a little, i can stably reproduce the mmap() time issue. but the resize time seems to be consistent now. i'll do an experiment on a broker with ~50k segments without time index and see what happens there.",0,0,0,0.9792397022247314,0.9806997179985046,0.9771077632904052,0.0,accept,unanimous_agreement
237590174,1215,: any new updates on the experiment?,0,0,0,0.9825878739356996,0.9919933080673218,0.9873065948486328,0.0,accept,unanimous_agreement
239509491,1215,": are you still working on this? there are other kips such as kip-58 that will be depending on this patch. so, we probably don't want this patch to sit for too long. we are pretty close to committing this patch.",0,0,0,0.634307324886322,0.9602701663970948,0.9409867525100708,0.0,accept,unanimous_agreement
239598105,1215,"sorry, i just saw your ping... yes, i am still working on this but was distracted by something else before. i will work on this this weekend and get it done. i tried with 50k segments, and it took about 11 seconds to load all the segments without time index, and about 30 seconds with the time index. i ran a bunch of tests on different machines. it seems that the machine i originally ran the tests on has some disk issue. a stably reproduceable result was that loading the segments took 3-4x longer with the time index. and what interesting is that for some of the logs with small number of log segments (e,g 100) i saw the loading time is significantly longer than other logs with much more segments (e.g. 3000). this happened both w/ and w/o time index. i am trying to see what caused this. i'll update the findings this weekend.",-1,-1,-1,0.9804837107658386,0.9910148978233336,0.9773195385932922,-1.0,accept,unanimous_agreement
239683336,1215,": thanks for the update. is the loading time affected by # of log segments, but not by the size of log segments? it seems that loading the time index is still more expensive than the offset index? looking at the code, we could make a couple of simple improvements. 1. in the constructor of logsegment, we call timeindex.lastentry twice unnecessarily. /\* the maximum timestamp we see so far */ private var maxtimestampsofar = timeindex.lastentry.timestamp private var offsetofmaxtimestamp = timeindex.lastentry.offset 2. in timeindex.sanitycheck(): we also call lastentry twice unnecessarily. not sure how much difference they will make. does profiling reveal which part of loading time index is expensive? if it's in calculating maxtimestamp and offsetofmaxtimestamp, it may be possible to do the calculation lazily. this probably will make the code more complicated. so, probably only worth doing if there is significant performance gain.",1,1,1,0.9116230010986328,0.5982584953308105,0.974535882472992,1.0,accept,unanimous_agreement
239713049,1215,"i have been running tests this afternoon. actually the previous results i got have included the changes you mentioned above. in addition, in the sanity check one difference for time index is that we also check that the last indexed timestamp is greater than the first indexed timestamp. the requires reading both the first and last entry. i also commented out that check. the broker that used to have > 40000 segments has only 12000 segments now due to the retention. so i was not able to run the tests on the 40000 partitions. i started to populate data to the cluster again. hopefully i will have >40000 segments by tomorrow. one promising experiment i did this afternoon was changing the number of log recovery threads. this seems reduce the mmap time effectively. but since i do not have a 40000-segments broker this afternoon, it is hard to say if that is going to help in a large broker. i'll try this out tomorrow. back to the tests i ran earlier on a 40000 segments broker and this afternoon, the profiling i did was the following: 1. the time used to load each partition 2. the time used for mmap function specifically to see if there is long mmap (>100 ms) 3. the time used in sanity check. the profiling shows time spent on (1) is not completely related to the # of segments in the partition. for example, a partition with 1 segment may takes 35 ms to load while another partition with 12 segments only takes 4 ms to load. even for the two partitions of the same topic that have the exact same number of segments, the time can differ by 10x from 5 ms to 50 ms. **this happens both with and without time index.** when i look at the partitions that takes longer to load (e.g. 2 seconds w/o time index and 6 seconds w/ time index), those segments tend to be loaded later in the segment loading process. as for time spent on (2), i did not see any long mmap when the number of segments is less than 12000, whether with or without time index. but after the # of segments is greater than 20000, some long mmap() are witnessed. again, those long mmap call only occur on the log segments that are loaded later in the segment loading process. the loading time was pretty linear when number of segments is 12000. it took about 2 seconds to load the segments without time index, and took about 4 seconds with time index. regarding the time spent on (3), when we have to read both the first timestamp and last timestamp in the time index, the sanity check time significantly grew, but still short compared with the time spent on loading the files. based on the test i have run, it seems clear that 1. the biggest part of segment loading time is mmap(), no matter w/ or w/o time index. 2. it does not seem that loading time index is more expensive than loading offset index.",0,0,0,0.726679265499115,0.8085483312606812,0.5507188439369202,0.0,accept,unanimous_agreement
239843860,1215,": thanks for the updates. 1. what file system were you using in your testing? 2. earlier you mentioned that loading 50k segments w/o time index took 11 seconds. how does that compare with say loading 25k segments with time index? if loading time index is no more expensive than loading the offset index, you would expect the latter to take no longer than the former.",1,1,1,0.9188061952590942,0.6862626075744629,0.961982011795044,1.0,accept,unanimous_agreement
239872879,1215,"the file system type is xfs. currently i have ~19000 segments on the broker. using 10 log recovery threads, it takes about 3 seconds to load without time index and about 10 seconds with time index. if i reduce the log recovery threads number to 5, the loading time becomes 6 seconds with time index. so it looks that the number of log recovery threads was causing the slow log segments loading. i will test this on a 25k segments cluster with time index as well after the broker reaches there.",0,0,0,0.9764299988746644,0.9634802341461182,0.986409068107605,0.0,accept,unanimous_agreement
239929404,1215,": thanks for the info. so using more log recovery threads slows down the loading with time index? also, about mmap() becoming more expensive with more log segments. mentioned to me that metadata lookups in file systems get more expensive with more open file handles since the lookups are based on an implementation of red-black tree. that's mostly cpu overhead. in your tests, do you notice any i/o activity when mmap() becomes more expensive?",1,1,1,0.9559861421585084,0.6344270706176758,0.9664903879165648,1.0,accept,unanimous_agreement
240002314,1215,"i did not check i/o activities when mmap() becomes more expensive, it is kind of hard to check because the segments loading only takes a few seconds. i tested log loading on 25k segments with time index using 5 loading threads. it takes about 13 seconds, which is comparable to the 11 seconds on a 50k segments cluster without time index. i will try to test it on a 40k cluster tomorrow. if the test results look good, i will update the patch with a few bug fixes and rebase on trunk as well.",0,0,0,0.8799089789390564,0.9571187496185304,0.7823153734207153,0.0,accept,unanimous_agreement
240139087,1215,": thanks for the update. in your new patch, could you include a summary of the performance impact (and perhaps the ideal number of log recovery threads) in the upgrade doc?",1,1,1,0.906250298023224,0.7470726370811462,0.9612851738929749,1.0,accept,unanimous_agreement
240276231,1215,: i am trying to hold off [a link] and [a link] to avoid the rebasing overhead of this patch. do you think you could provide your latest patch in the next day or two?,0,0,0,0.7047080397605896,0.9741278886795044,0.9819313287734984,0.0,accept,unanimous_agreement
240319219,1215,"i have all the code ready now. i am still trying to find the reasonable number of recover threads. it seems there is system cache effect, i.e. if i do a start-shutdown-start sequence, the second start would be much faster than the first one because some of the disk read may come from the system cache. to avoid this effect, i need to wait for a while between each broker startup. i will upload the patch latest by thursday. is that ok?",0,0,0,0.8779152035713196,0.9405896067619324,0.7846601605415344,0.0,accept,unanimous_agreement
240436736,1215,": thursday is fine. not sure if this helps, but you can empty dentries/inodes through a command ([a link]",0,0,0,0.8602033853530884,0.537235677242279,0.9065018892288208,0.0,accept,unanimous_agreement
240630032,1215,"thanks for the dentries/inode cleaning command. i just saw them and haven't tried that yet. currently i have a script reboot a broker every hour and it seems stable enough to give some results. some updates on the number of log recovery thread tests. for a cluster with about 40k segments, with one log recovery thread, it takes 14 - 17 seconds to load the logs, and about 20 seconds with 2 threads, > 30 seconds with 5 threads. so it seems that having a single thread loading segments is the best setting. i have updated the patch after rebasing on trunk. i also squashed some previous commits.",1,1,1,0.9407724142074584,0.9770506620407104,0.9514629244804382,1.0,accept,unanimous_agreement
240802214,1215,": thanks for the update. are your latest numbers with time index? what about the numbers w/o time index? also, how many log dirs are there since the number of recovery thread is per dir?",1,1,1,0.9319519996643066,0.8783259987831116,0.9722766876220704,1.0,accept,unanimous_agreement
240815948,1215,the latest numbers are with time index. i see log loading time to be 21 - 23 seconds on a 46k segments with time index. it takes about 10 seconds to load the segments without time index. so the loading time with time index is about 2x of without time index. this is also comparable to the 11 seconds loading time without time index on a 50k segments broker. i have ~8500 directories (partitions) on the broker. some of the partitions are pretty big (> 2500 segments).,0,0,0,0.8810141682624817,0.9716923236846924,0.9751175045967102,0.0,accept,unanimous_agreement
240838240,1215,": thanks. could you confirm the number of log dirs you have and the number of disks per dir? intuitively, if there are multiple disks per dir, it seems using more than one 1 recovery thread should lead to better performance.",1,1,1,0.9695678353309632,0.8355727195739746,0.958341896533966,1.0,accept,unanimous_agreement
240859249,1215,"we only have one log directory on a raid 10 containing 10 disk. so in our case, multiple thread might not help. in a jbod environment, i agree that multiple threads would probably help.",0,0,0,0.9801067113876344,0.9866558313369752,0.9855234622955322,0.0,accept,unanimous_agreement
240870837,1215,": interesting, in theory, using more than 1 thread should still help since those threads can drive the i/os on different disks in parallel.",1,0,0,0.5667228102684021,0.9551873207092284,0.8892533779144287,0.0,accept,majority_agreement
240883073,1215,"i am not sure, one thing i noticed is that when there is one thread, the log loading time of each partition is pretty linear to the number of log segments. but if multiple threads are used, the linearity goes away. for example, the following logs are from running the code using 10 log recovery threads without time index (`old_mark` indicates no time index, and the same random old_mark means they are from the same test run). for topic1, it only took ~400 ms to load about 2000 log segments, at about the same time (300 ms earlier) topic2 takes 22 ms to load ~70 log segments. however, later on the same run (3 seconds later), for some other partitions of topic2 that has about the same number of segments, it takes much longer (~400 ms) to load. this was the reason that i suspected that the later mmap() calls are more expensive, which seems also proven from profiling by measuring the time on mmap calls in different buckets. i am not sure why this happens, but i didn't see such issue when there is one log loading thread. [code block]",0,0,0,0.9097532033920288,0.9015156030654908,0.8871511220932007,0.0,accept,unanimous_agreement
240895715,1215,": thanks for the latest patch. i made another pass and only had a few minor comments. once those are addressed, i can merge in the patch.",1,1,1,0.9473196268081664,0.9651308059692384,0.9799196720123292,1.0,accept,unanimous_agreement
240931044,1215,thanks a lot for all the patient reviews. i updated the patch to address you latest comments and added a few unit tests. thanks again.,1,1,1,0.9793726801872252,0.9808900356292723,0.9922089576721193,1.0,accept,unanimous_agreement
241075286,1215,thanks for the patch. lgtm,1,1,1,0.8875595331192017,0.812760591506958,0.9300410747528076,1.0,accept,unanimous_agreement
241077444,1215,: thanks a lot for working on this diligently. a couple of followups. 1. i left a couple of minor comments after merging. could you address them in a followup jira? 2. do you plan to work on a followup kip to add a new seektotimestamp() api to the consumer and a new request for getting offsets based on timestamp?,1,1,1,0.9838941693305968,0.9734103679656982,0.9907622933387756,1.0,accept,unanimous_agreement
241471040,1215,"thanks a lot for the patient review. i will submit a follow up pr soon. and yes, i will start working on the kip to add seektotimestamp() api now and post the wiki this week.",1,1,1,0.9806227684020996,0.9813653826713562,0.9847107529640198,1.0,accept,unanimous_agreement
241557337,1215,", it would be good to update the kafka documentation to mention the time index. maybe an easy way is to search it for offset index and see if it makes sense to mention the time index in those cases as well. would you be willing to file a jira and take this on as well?",0,0,0,0.9319908022880554,0.9890873432159424,0.9857960939407348,0.0,accept,unanimous_agreement
418101464,5527,"we should just remove all the `final` keywords, i don't think they add any benefit?",0,0,0,0.9512731432914734,0.9797946810722352,0.983805239200592,0.0,accept,unanimous_agreement
418140255,5527,"i had to add all the final keywords to pass the linting check - iirc, my first run had dozens of linting errors preventing compilation.",0,0,0,0.9654712677001952,0.5682675242424011,0.9756084084510804,0.0,accept,unanimous_agreement
470747449,5527,what is your jira id? would like to assign the ticket to you.,0,0,0,0.9802071452140808,0.9912420511245728,0.994964361190796,0.0,accept,unanimous_agreement
470940088,5527,jira id is abellemare,0,0,0,0.9858894348144532,0.974682867527008,0.9774512648582458,0.0,accept,unanimous_agreement
481338093,5527,"hi, sorry to intrude on a potentially stale pr, but is this functionality still in development? would be extraordinarily useful for joining two changelog-like entities.",-1,-1,-1,0.9859125018119812,0.9889111518859864,0.9823617339134216,-1.0,accept,unanimous_agreement
481350147,5527,"i sure hope so, my team is looking forward to it as well! given that the kip was accepted a few weeks ago, i think it's safe to say it will make it in fairly soon. i would definitely pick up development if can't continue.",1,1,1,0.9877382516860962,0.971340000629425,0.9963841438293456,1.0,accept,unanimous_agreement
481367621,5527,hey folks - i'm still trying to get the code put together and finalize some of the changes that were outlined in the kip. stay tuned!,1,1,1,0.9176039695739746,0.914961040019989,0.9377217888832092,1.0,accept,unanimous_agreement
482606869,5527,"hi all - i'm at a point where i need some feedback on a couple of things: 1) the organization of the code 2) the organization of the graph nodes in ktableimpl 3) insights into why i am getting nullpointerexception in ktablektableforeignkeyinnerjoinmultiintegrationtest (though not consistently). i believe this is a misunderstanding on my part as to how partitions are co-partitioned, but there may be more to it that i am missing. basically, it seems that depending on how the tasks and partitions are assigned, we either get a `java.lang.nullpointerexception: task was unexpectedly missing for partition table1-1` or it we don't. **this must be resolved if we wish to have flexible partition counts for joining, ie: fk join a topic with 3 partitions and a topic with 7 partitions** 4) anything else. feedback is very much appreciated, as this is the first pr i've put up against kafka and i'm sure i've violated a number of things.",-1,0,1,0.7040014266967773,0.8472191095352173,0.6899272799491882,,review,no_majority_disagreement
483831611,5527,"hi , thanks for your pr! i'll review this as soon as i get the chance, and pay particular attention to the points you called out. -john",1,1,1,0.9826648831367492,0.9940156936645508,0.9954627156257628,1.0,accept,unanimous_agreement
491508007,5527,"hi john - thanks for the feedback so far! i haven't had time to attend to this due to some recent personal matters, but i should be able to take a crack at it this upcoming week. i think that (in my mind) the discussion about the topic names has been resolved, so i don't think there are any impediments other than me getting this cleaned up and then rebased to trunk.",1,1,1,0.9815306067466736,0.9864369034767152,0.993263304233551,1.0,accept,unanimous_agreement
492267940,5527,"- completed all your feedback so far john. thanks so much. i also updated the wiki page to more clearly show the underlying implementation in the diagram. i renamed some of the processors and hopefully added more clarity with the comments and other work. currently, i do not know enough about the underlying mechanisms to get variable-partition counts working (ie: join a ktable with 7 partitions with that of 11 partitions). this is explained above in the april 12th post on ktablektableforeignkeyinnerjoinmultiintegrationtest. multi-partition support could be added in a later revision if we wish to get this in for 2.3. i will rebase this to trunk and commit that too shortly.",1,1,1,0.9708399772644044,0.9869416356086732,0.9929961562156676,1.0,accept,unanimous_agreement
492710161,5527,"rebasing to trunk has been considerably longer than i planned. dealing with the new timestamped data stores has been a bit of a nightmare. additionally, data which used to be present in the ktableimpl class is no longer available. in the ktableimpl constructor,storebuilder and isqueryable have been replaced by materialized.queryablestorename(), which means that i do not have the ability to attach my resolver to the original, ""this"" materialized instance in the case where a queryable name is not set. i will look at ways to resolve this, but i do not anticipate being done before 2.3. i have spent considerable time on it in the past day and it's looking like much more is required.",-1,-1,-1,0.8587004542350769,0.7982534170150757,0.8827419877052307,-1.0,accept,unanimous_agreement
492729592,5527,"hey , thanks for the update, and for the rebase work. yes, the new timestamped stores changed a *lot* of implementation classes. it's a bummer that it happened to get merged after you forked. i agree, it's unlikely that this be able to get merged by friday (the feature freeze for 2.3). but no worries, it just means that we'll have more time to review it, write lots of tests, system tests, work on docs and blogs, etc., before it does get released, which decreases the overall risk of such a big feature. once you finish up the rebase, i'll take another pass. i didn't make it all the way through last time, and wound up just commenting on the little things i noticed along the way. -john",1,1,1,0.868764340877533,0.9445655345916748,0.9463163614273072,1.0,accept,unanimous_agreement
493129698,5527,"there's an issue with the changing of the ktableimpl api 2.3-trunk: [code block] 2.0-trunk (what i had rebased it to last) [code block] `isqueryable` and `queryablestorename` in 2.0 have been replaced by just `queryablestorename` in 2.3, with a null value intending to mean that it is not queryable. the problem is that the onetomany joiner needs to query the underlying statestore previously represented by `this.queryablestorename` during resolution of the hash code values. previously, queryablestorename was populated even if the underlying state store was anonymous (ie: not passed in as a materialized parameter). now, however, if the user does not materialize a state store, the anonymous one is not passed in as `this.queryablestorename`. i think that it was only my code which would be affected by this unfortunately. i think i will need to change it back to how it was in 2.0 unless someone has any other ideas about how to get the underlying queryablestorename when it is anonymous.",0,0,0,0.9663858413696288,0.9951830506324768,0.98473858833313,0.0,accept,unanimous_agreement
495390035,5527,"with kip-258 being merged, i think this pr should switch to use `timestampedkeyvaluestores`, too. i try to do a full review soon, now that feature freeze deadline is over i have a little more head room :)",1,1,1,0.9280219078063964,0.9890549182891846,0.9850693345069884,1.0,accept,unanimous_agreement
496669680,5527,"hi, i was using this pr before successfully, but i get an exception now: [code block] this happens on tableprocessornode(""ktable-source-output0000000046""), which is a child of ktablektableforeignkeyjoinresulutionnode(""ktable-source-resolver0000000045"") i think what broke it is that this time the join is done after filter (to emulate a left join): [code block] - i see ""is this right?"" comment in the code (org.apache.kafka.streams.kstream.internals.ktableimpl#dojoinonforeignkey) - this.queryablestorename is null, but it is asserted not to be null later. [code block]",0,0,0,0.9630720615386964,0.9750961661338806,0.9872575402259828,0.0,accept,unanimous_agreement
496750446,5527,"-traderev this is a consequence of the changes made in [a link] by (of which i need his guidance on for how best to remedy this issue). the workaround is to manually materialize the ktables you are going to use this join function on. for instance, instead of: [code block] use: [code block] and then this should work (because you provided a queryablestorename): [code block] prior to this change, the queryablestorename was either the user-provided name materialized.as(""storename"") or the internal name, if no materialized name was provided. from the change notes itself: `ktableimpl's queryablestorename and isqueryable are merged into queryablestorename only, and if it is null it means not queryable. as long as it is not null, it should be queryable (i.e. internally generated names will not be used any more).` the consequence of this is that there is now no way (as far as i can tell) for the ktableimpl to access internally generated state stores, which effectively kills this ticket. previously i was able to access both user-materialized ktable state stores as well as internal ones. now i can only access user-materialized ones, which is a non-starter for getting this committed. the only way forward that i can see at the moment (due to my own limited understanding of the code) is to revert the ktableimpl changes that implemented. to do so would be non-trivial, and i would like his guidance to do so. until we get past this detail, this jira is blocked.",0,0,0,0.9795739650726318,0.9898127913475036,0.9724637269973756,0.0,accept,unanimous_agreement
497091665,5527,", thank you, with your workaround i was able to progress further. i have another problem now - fk join calls serializers for both tablea and tableb records with the same topic name, causing compatibility error from the schema registry. the first call is for tableb value, triggered from [a link] the second call is for tablea value, triggered from [a link] the topic name is ""applicationid-steam-ktable-joinother-ktable-repartition-0000000041-value"" is it right to pass topic name in the wrapper serde here? [a link]",1,1,1,0.9671197533607484,0.9409221410751344,0.9442560076713562,1.0,accept,unanimous_agreement
497104592,5527,"-traderev this is where it gets tricky. there was a discussion previously in this review (and spilling over into the confluent schema registry git repo) over whether considerations for the schema registry need to be taken into account or not. my intention was to have this resolved so that it would be favourable for the schema registry serdes (since i too use them) but i have not gotten far enough with testing to hit the error you encountered. i believe a quick and dirty workaround, if you don't care about the registering dummy schemas, is to change the following: [a link] i believe if you create a randomly generated integer during the creation of this class, you can add it to the end of the topic name in l89. it should remedy the issue of registration. if it does not... i suggest just mucking around with the context.topic() names in the various processors and serdes until it works. it's not great, but since i don't have this running in front of me it's the best i can offer right now (and is still definitely a hack). the problem with the confluent schema registry is that it registers even null-topics, so unless you pass in dummy topics or are absolutely sure your topology isn't registering two schemas on the same internal topic (which it shouldn't even be doing in the first place...), you're kinda screwed. the next step in ktables should be separating the consumed.with() serdes from the materialized.with() serdes, so that we can use the confluent ones only for consuming the topic but not at all within the internalized topology.",-1,0,0,0.9619188904762268,0.7096767425537109,0.9218870401382446,0.0,accept,majority_agreement
497406670,5527,"i left a comment under your question above, but since i've not done a full pass over it i'm actually not sure if my proposal would work for you (briefly looked into `subscriptionresolverjoinprocessorsupplier` but still not clear the exact line where it would npe)",0,0,0,0.9751141667366028,0.9410431385040284,0.982258915901184,0.0,accept,unanimous_agreement
497731250,5527,"-traderev i threw in a fix for the schema registry. the main issue is that the valueserializer here does not have access to the correct topic name. [a link] i have made a dummy topic name, since i do not think we can actually access the true ""tablea"" topic name without a bunch of hacky work. if anyone strongly objects to this they can come up with a better suggestion.",0,-1,0,0.5552830100059509,0.609445333480835,0.9238206148147584,0.0,accept,majority_agreement
497766138,5527,"thanks, i had to solve it myself by creating a wrapper serde that always passes the correct topic name",1,1,0,0.7655589580535889,0.8210293054580688,0.8163527846336365,1.0,accept,majority_agreement
497778195,5527,"-traderev can you elaborate? my impression here is that the serializers i have created in this ticket are always receiving the correct topic, since they're simply using the existing streams framework when they get forwarded. if i am doing something wrong in this pr, please point it out.",0,0,0,0.9716562628746032,0.97786146402359,0.964717984199524,0.0,accept,unanimous_agreement
497794730,5527,", sorry, i meant that i did this before your change as a workaround - have not had time to test your change yet. so, to clarify, with the latest changes i can: - remove materialized from tables if i don't need them (except for fk join method) - use avro serdes without a need for a fixed topic name wrapper i will do this changes and will let you know thank you",-1,-1,-1,0.984581172466278,0.8758653998374939,0.9689699411392212,-1.0,accept,unanimous_agreement
497802451,5527,"okay sounds good. i appreciate all the feedback you have given, so if you think i'm doing something incorrectly i definitely want to hear about it :)",1,1,1,0.9931719303131104,0.9956423044204712,0.9966455101966858,1.0,accept,unanimous_agreement
498871200,5527,"hi all, to those still paying attention to this pr. i have done some major work on this over the past couple of days. i suspect i am about 95% complete on it now, but again, i still need feedback. still need to incorporate kip-307, but currently blocked on waiting for [a link] to be approved. - added a large number of tests in line with ""statestore.range(...)"" tests (and found a few bugs while doing so...) - unit tests for the various serdes - prefixscan appears to be working with all necessary stores, including timestamped. - works with timetamped tables things to consider: 1) the murmur3 code copied from hive has a number of `fallthrough` warnings. we either need to suppress the warnings or alter the code. currently i have the warnings suppressed. 2) currently needs the same partition count for both `this` and `other` table, as i have not been able to dig into the task/partition assignment logic to get it working with variable partitions. this could be done at a later date. 3) folder structure - all in foreignkeyjoin - i suspect i should not do that.",0,1,1,0.6858152747154236,0.642449140548706,0.934637725353241,1.0,accept,majority_agreement
499291988,5527,"thanks for the great summary about the progress, i think the rest of kip-307 should be done soon and pinging and to continue reviewing on this pr.",1,1,1,0.9705294370651244,0.9929165840148926,0.9941416382789612,1.0,accept,unanimous_agreement
499586049,5527,"thanks -traderev and i have been working on a specific bug (introduced by this pr, not existing) that he discovered around some incorrect casting between timestamped and nontimestamped stores. i hope to resolve that today or tomorrow, but it shouldn't affect 99% of the pr.",1,1,1,0.8053176999092102,0.5519003868103027,0.9584203958511353,1.0,accept,unanimous_agreement
499632932,5527,"okay i think i isolated the issue and submitted a fix. the tests that were failing should now be passing. in the meantime i'm running tests locally to validate. -traderev, let me know if this works for you, your feedback has been very helpful.",1,1,1,0.6616647243499756,0.907059907913208,0.9897748231887816,1.0,accept,unanimous_agreement
499928589,5527,anyone know how to handle licensing for tests? [code block],0,0,0,0.9867841005325316,0.99372535943985,0.9951536655426024,0.0,accept,unanimous_agreement
499935136,5527,"for any added files (both src and test) you'd need to add apache license as the javadoc at the top, you can take a look at any existing files. basically it looks like: [code block]",0,0,0,0.9780991673469543,0.987985908985138,0.99459707736969,0.0,accept,unanimous_agreement
499996967,5527,- of course! obvious in hindsight. thank you.,1,1,1,0.9844865202903748,0.9955800175666808,0.9941642880439758,1.0,accept,unanimous_agreement
500947033,5527,"alright, well there's not much more for me to do here without more feedback from reviewers and the completion of [a link]",0,0,0,0.8553310632705688,0.9544387459754944,0.9858654737472534,0.0,accept,unanimous_agreement
505013615,5527,thanks john! i'll address them all over the next few days.,1,1,1,0.9892430305480956,0.9708189964294434,0.9933184385299684,1.0,accept,unanimous_agreement
505137962,5527,"thanks for your hard work on this feature. it's going to be super valuable to our team!! in my integration of your branch i noticed something that seems like a bug, or at least warrants some further discussion: i have a topology with two ""input"" `ktables` (`a` and `b`) and one output (`c`). simply put: * this topology expects stream `a` to be messages like (`key`, `x1`), (`key`, `x2`), etc... * it expects stream `b` to be messages like (`x1`, `v1`), (`x2`, `v2`), etc. * on `c` it emits the key from `a` and the value from `b` (where the key on `b` is the value referenced in the message value from `a`). some behavior that works as expected: * when a message in stream `a` references a known (or later known) message key in stream `b`, the resulting stream emits the key in stream `a` and the value from stream `b`. success! * when a message (on either side) has its value updated, the join's new result is emitted. success! what seems worth discussing / incorrect: * if a message value from stream `a` is updated to reference a non-existent key on stream `b`, nothing is emitted. this means that ""old"" results are still considered correct in downstream consumers. for example if i send a message on topic `a`: (`k`, `1`) and a message on topic `b`: (`1`, `one`), it correctly emits on `c`: (`k`, `one`). if i then send an update on `b`: (`1`, `one`) then a message on `c` is emitted: (`k`, `one`). if i then send an update on `a`: (`k`, `2`), but no message with key `2` has ever been seen on `b`, nothing is emitted. on one hand this seems correct: the results of an inner join should never have a `null` value for either side of the join. on the other hand this seems incorrect: the resulting stream is not updated to reflect the latest value `null` value/tombstone for key `k`. i suggest that the output stream should contain tombstones to indicate that the key extracted from `keyextractor` no longer matches a value on the ""right"" side of the join. what do you think? /cc for what it's worth, the topology i am using for testing: [code block]",1,1,1,0.9907426238059998,0.991236448287964,0.9964402318000792,1.0,accept,unanimous_agreement
505147263,5527,"thanks for the excellent discovery! i believe that this is indeed a bug, and i think it's because of the decisions made around propagating the `null` events on a foreign key change. i will take a look at this more closely, because what you have written about expecting a tombstone output makes sense to me. let me consider this.",1,1,1,0.9889707565307616,0.9931727051734924,0.9945816397666932,1.0,accept,unanimous_agreement
505517987,5527,"hi john i pushed a fix to the prefixscan, along with some tests illustrating how they work. things seem to be working correctly now, but any feedback on this specific set of changes is well welcomed. i have a solution in mind for the bug you discovered, i'll try to get to that in the next day or two.",1,1,1,0.5692880153656006,0.9513585567474364,0.9869565963745116,1.0,accept,unanimous_agreement
505859127,5527,"hi i have updated the kip to reflect what i think can be done, and it is working in my dev branch. the main issue is that we do not maintain sufficient state to know that a delete was propagated out to downstream consumers on a previous change. a chain of changes with no available fk on the rhs would emit a (k, null) for each change. this is much more akin to a left join, but the big difference is that we wouldn't execute the join logic if inner is selected. anyways, check out the table i made in [a link] i'm going to continue working on the changes i have in mind, and more importantly, build out a number of test scenarios so i can get better coverage.",0,0,0,0.9532405138015748,0.9366450309753418,0.5903060436248779,0.0,accept,unanimous_agreement
505936671,5527,"hey and , i started a new follow-up discussion thread in the mailing list to evaluate the proposed design modification. thanks, -john",1,1,1,0.971630036830902,0.97381192445755,0.9792653322219848,1.0,accept,unanimous_agreement
506104616,5527,"- i just pushed a commit that should fix your issue. it also adds left joins since we were 99% of the way there anyways. please let me know if it's meeting your expectations, because feedback like yours has been extremely valuable in working out the kinks, and i thank you for that.",1,1,1,0.9655020236968994,0.936806857585907,0.9889848232269288,1.0,accept,unanimous_agreement
506493558,5527,"i'm pleased to report that this is working perfectly in my project's test cases. i have replaced a significant portion of my internals to prove this works in our test cases. so that's great! i did notice some `closing n open iterators` upon shutdown after processing ~1/2 million records, and can't confirm whether it's expected or even troubling. i thought i would pass it on in case we are ""leaking"" iterators in an unexpected way. thanks again for your hard work on this feature! [code block]",1,1,1,0.9931051135063172,0.9953151941299438,0.9964801669120787,1.0,accept,unanimous_agreement
506747200,5527,- a classic programming blunder - i definitely forgot to close the iterator after the prefix scan. i am adding that now.,0,0,-1,0.6632212996482849,0.5704048275947571,0.8309895992279053,0.0,accept,majority_agreement
506795729,5527,did a rebase to trunk and force pushed. looking to add named and incorporate more of john's feedback.,0,0,0,0.9759311079978944,0.9879956841468812,0.9926698207855223,0.0,accept,unanimous_agreement
506810499,5527,"one issue with using named... there are 5 nodes to name, 2 topics and 1 internal materialized store. i don't think it's appropriate to use 8 named elements, but i could see about using it as a common prefix for each of those.",0,0,0,0.9545257687568665,0.9476869106292723,0.9383765459060668,0.0,accept,unanimous_agreement
506820978,5527,"sorry... correction - 5 processing nodes, 2 sink nodes, 2 source nodes, 2 topics and 1 materialized internal state store.",-1,-1,-1,0.988500714302063,0.9904442429542542,0.9922450184822084,-1.0,accept,unanimous_agreement
507096788,5527,"- added a bunch of updates. and - perhaps you, along with john, can recommend where ""prefixscan"" should live in the web of keyvaluestore interfaces. all i really need it is: 1) support in rocksdb, caching and logging 2) must be able to access both `prefixscan` and `get` i tried previously to find a clean way to include it, but could not find a way without effectively adding it to the base keyvaluestore interface. i ended up settling on trying to get it to parity with `range` since they're very similar. thanks.",1,1,1,0.9502072334289552,0.9840415716171264,0.986346423625946,1.0,accept,unanimous_agreement
507786579,5527,"hi folks i'm going to try to keep this moving along, as i've just hit the 1-year birthday of working on this. we have it working internally and it would be good to get it in to 2.4, but i need more eyes on it to get it over the barrier, otherwise i fear it will languish in pr-purgatory forever. outstanding issues (as raised by john): 1) where in the interface definitions should prefixscankeyvaluestore.prefixscan() live? - not every type will have a prefix / won't make sense. - many state stores need the data definition (caching, logging, rocksdb, timestampedrocksdb) but many do not `need` it per se, for the purpose of this pr. => i don't have a good take on this. i want to avoid completely rehauling the keyvaluestore interfaces, but i also want to be able to use the caching and logging stores, as well as rocksdb to do the scan. everything else is optional to me. 2) the implicit dependencies between combinedkey, its serialization format, and prefixscan operation. prefixscan operates on the byte serialization of combinedkey, and the byte serialization is important for the cases where we just have a prefix (foreign/rhs update). => i don't see this one as much of a problem, as it seems obvious that byte-level prefix scanning should rely on the serialized format. while it could possibly be simplified, i don't have any immediate ideas. thanks again for any help... adam",-1,0,-1,0.3689786791801452,0.4285371005535126,0.713373064994812,-1.0,accept,majority_agreement
513779794,5527,", , - do any of you have any thoughts on point #1 and #2 above? currently everything works in the pr, but i suspect the api for #1 needs some more thought, as does the inter-dependencies of the serialization for #2. pinging you because it's been 20 days with inactivity.",0,0,0,0.92846018075943,0.848552942276001,0.8771730661392212,0.0,accept,unanimous_agreement
514062726,5527,i also wanted to say that along with and we are eagerly looking forward to this functionality as it solves many common uses cases we have.,1,0,1,0.9269384741783142,0.9276841282844543,0.9774187207221984,1.0,accept,majority_agreement
514693865,5527,"hey , sorry (again) for the delay (again). i'm going to check out your branch and see what i can come up with in response to the issues. -john",-1,-1,-1,0.9904882907867432,0.992499053478241,0.9920712113380432,-1.0,accept,unanimous_agreement
516523887,5527,"hey , like i mentioned before, i needed to check out the code and hack on it to address your questions... it was simpler to send my thoughts in pr form, so i created [a link] . it looks like we made some concurrent modifications, but the point was just to express the ideas anyway. wdyt? by the way, after getting my hands dirty, i have to give you mad props on this pr. you had to work though a bunch of the most complex inner workings of streams to pull this off. nice work! -john",1,1,1,0.9817241430282592,0.992625892162323,0.9829795360565186,1.0,accept,unanimous_agreement
517427553,5527,"i put your changes into a single commit and merged them in. currently the build + tests seem to pass fine! thanks so much for your help, it is extremely appreciated. i think the major stumbling blocks i had were succinctly resolved by john's changes, so i believe the next step is to ensure that proper test coverage exists. i will take a look at that in short order, but just wanted to get his work in so that others can look at it. the bulk of it is summarized in his own descriptions here ( [a link] ), but boils down to: 1) removed the need for prefixscan 2) use a basic byte store with range scan to access elements (this is part of where we need to ensure it's fully working as expected) 3) centralize the specificity of prefixes and byte ordering into a single combinedkeyschema class. thanks again john!",1,1,1,0.989457666873932,0.9948635697364808,0.9955427050590516,1.0,accept,unanimous_agreement
519149386,5527,"so there is (theoretically) a bit of a problem. currently our combinedkeyschema is represented as follows: `{integer.bytes foreignkeylength}{foreignkeyserialized}{optional-primarykeyserialized}` `store.range(start,end)` will not work when the prefix is the maximum value of a particular byte array. for example: [code block] namedcache currently still has a fragment of code to remove from the previous prefixscan implementation, but it illustrates the issue perfectly: ([a link] **the range iterator:** [code block] **the prefix iterator.** note that we use tailmap when the prefix increment would cause a wrap-around: [code block] options to fix this: 1) ignore it - we likely never hit integer.bytes for foreignkey length (that is, after all, 2,147,483,647 bytes for a signed integer... a bit bigger than we would expect to see in a kafka event). it is also worth pointing out that this would require the event coming through a repartition topic, and i don't think 2gb+ keys are reasonably likely to be used by anyone. 2) custom implementation of bytes specifying a custom comparator. i am leery about this one because technically the current byte comparison is correct, and we would be making it incorrect simply to work around the range wrap-around corner case. 3) add an extra byte to the start of combinedkeyschema just for the wrap-around corner-case: //{empty byte preventing wrap-around issues}{integer.bytes foreignkeylength}{foreignkeyserialized}{optional-primarykeyserialized} in this way, our previous example will now return the correct results, because end > start according to the default bytes comparator. [code block] my inclination is to somehow use option 1 while acknowledging that it could possibly be an issue. after all, the current key size limit is arbitrary as is (why not use long?). i suspect that adding the byte would be technically correct but simply a waste of space. i do not think anyone would ever use a 2gb key, but i think we still need to guard against it.",0,0,0,0.9265087246894836,0.9903212189674376,0.9821565747261048,0.0,accept,unanimous_agreement
519198629,5527,"nice catch, , i also think option 1 is ok. actually, i _think_ this is not a problem. the key length is actually not arbitrary, since all keys must fit into byte arrays, and arrays in java cannot be larger than ""max int"", aka `0x7fff ffff` thus, we already have room to wrap around, with the first byte becoming `0x8f`. does that add up, or is it just wishful thinking on my part? -john",1,1,1,0.8799720406532288,0.9820038676261902,0.9917311072349548,1.0,accept,unanimous_agreement
519198864,5527,"retest this, please.",0,0,0,0.9817549586296082,0.9711947441101074,0.788542628288269,0.0,accept,unanimous_agreement
519223034,5527,the builds are failing because of some style violation. you can debug it with `./gradlew :streams:test-utils:checkstylemain`.,0,0,0,0.9239835143089294,0.972916066646576,0.9769777059555054,0.0,accept,unanimous_agreement
519223220,5527,"oh, found it: [code block]",0,0,0,0.9797866344451904,0.957287847995758,0.9414973855018616,0.0,accept,unanimous_agreement
519512086,5527,"ah, correct! 7fff ffff should indeed be the maximum. i have just been testing at the byte level and had erroneously thought that ffff ffff was the max. since that's the case, i don't think that there's any issue with the usage of range as we have it now.",0,0,0,0.9522180557250975,0.6075124144554138,0.6183099746704102,0.0,accept,unanimous_agreement
520025675,5527,"thanks for double-checking, . maybe we can get some other reviews at this point? also, the test failures were unrelated. retest this, please.",1,1,1,0.693565845489502,0.7975477576255798,0.9509329199790956,1.0,accept,unanimous_agreement
520162968,5527,"by retest, do you mean rerun the scala 2.13 tests? i can't figure out how to do that without submitting a dummy commit...",0,0,0,0.8991263508796692,0.8293600082397461,0.9832478761672974,0.0,accept,unanimous_agreement
520532972,5527,"oh, sorry, . if you say that sentence in a comment, it triggers the build to re-run. in other words, that last sentence was directed at jenkins, no you ;) after that re-test, two of the builds are passing, so i think we don't need to run them again until after the next cycle of reviews. (just guessing there'll be more commits later). i just wanted to see some green checkmarks.",-1,1,1,0.95768541097641,0.5798219442367554,0.6464702486991882,1.0,accept,majority_agreement
520847928,5527,"thanks! i was a bit confused... i think it's the politeness of which the restesting was asked for that made me think it was directed at me... anyways, til!",1,1,1,0.9082493185997008,0.9423202872276306,0.9896017909049988,1.0,accept,unanimous_agreement
524442847,5527,"hey , i was just looking over the interface in ktable. can you correct the javadoc so that the `return` field is filled out? also, one of the `named` params is missing a description, and two of the overloads have no javadoc at all. thanks! -john",1,1,1,0.9849742650985718,0.9932175278663636,0.9937698245048524,1.0,accept,unanimous_agreement
524500288,5527,"hey again, , i was talking to about reviewing this pr, and he had a good suggestion... can you send a separate pr just adding the murmur3 hash class and test (and guava dependency)? this would let us discuss the new depenency in isolation, as well as consider adopting the new hash algorithm elsewhere sooner. and as soon as it gets merged, this pr will automatically get almost 1,000 lines of code shorter. wdyt? -john",1,1,1,0.4956446886062622,0.9769080877304076,0.6367982625961304,1.0,accept,unanimous_agreement
525309148,5527,"okay, will do.",0,0,0,0.9462106823921204,0.9564443230628968,0.9375463724136353,0.0,accept,unanimous_agreement
525315925,5527,i assume you mean i should make a new pr and then rebase this one off of the new one? i think i may have to butcher this one up a bit since i don't think i committed the murmur3 hash code neatly in one commit.,0,0,0,0.9637060165405272,0.9584726095199584,0.9774430990219116,0.0,accept,unanimous_agreement
526226069,5527,- the murmur3 pr - [a link],0,0,0,0.9837728142738342,0.987640917301178,0.9914286732673644,0.0,accept,unanimous_agreement
530440677,5527,"one additional thought came to mind. we should include a test case where we have the `streamsconfig` set to `optimize` (and use overloaded build method `streambuilder.build(props)` ) to make sure everything still works as designed. i think it should, but it will good to confirm.",0,0,0,0.8526901602745056,0.5680432915687561,0.9001902937889099,0.0,accept,unanimous_agreement
530527737,5527,- would it be sufficient to put it in one of the integration tests? it seems to all work when i include the following: [code block],0,0,0,0.982077419757843,0.9825960397720336,0.9936291575431824,0.0,accept,unanimous_agreement
530592010,5527,"yeah, that sounds fine. maybe the `ktablektableforeignkeyjoinintegrationtest` since it has 8 tests in it?",0,0,0,0.916789174079895,0.984822392463684,0.9513373970985411,0.0,accept,unanimous_agreement
532466041,5527,"the job output was lost to the sands of time. retest this, please.",0,0,-1,0.9854357838630676,0.9127567410469056,0.8184787034988403,0.0,accept,majority_agreement
533170339,5527,"okay, i added code to the streamsresetter, but i can't find a good place to test for the correct deletion of the topics during a full app reset. streamsresettertest doesn't seem to provide for that. any suggestions?",0,0,0,0.9398552775382996,0.6215603947639465,0.9644367098808287,0.0,accept,unanimous_agreement
533212559,5527,"off the top of my head maybe add the `streamsresetter` to one of the integration tests? i'm thinking something like 1. process some records, confirm the output 2. reset the app confirm topics deleted 3. re-run the same records and confirm all processed again wdyt? \cc",0,0,0,0.9493560791015624,0.8671727776527405,0.98729008436203,0.0,accept,unanimous_agreement
533217402,5527,"i think it's worth adding the test right now, though, given our recent experience with repeated the multiple operational regressions after implementing suppress. see the `org.apache.kafka.streams.integration.abstractresetintegrationtest` for testing resets.",0,0,0,0.9608626961708068,0.9907111525535583,0.9836380481719972,0.0,accept,unanimous_agreement
533355567,5527,", now that #7271 has been merged, you're going to have to rebase (since the location of murmur3 is different now). we should see all the murmur3 code and tests disappear from this diff.",0,0,0,0.986395001411438,0.9899633526802064,0.987981617450714,0.0,accept,unanimous_agreement
533356431,5527,java 11 builds timed out. java 8 failed with all tests from `org.apache.kafka.streams.integration.ktablektableforeignkeyinnerjoinmultiintegrationtest` and `org.apache.kafka.streams.integration.ktablektableforeignkeyjoinintegrationtest` but running locally all test pass for me. can you rebase this pr? nm just saw the comment from,0,0,0,0.9740337133407592,0.9858113527297974,0.9899391531944276,0.0,accept,unanimous_agreement
536039458,5527,"hi, what is the expected output with a leftjoin when an event is published to the lhs with null foreign key (assume no other events have been published). i am observing no output, which was unexpected for a leftjoin. my apologies if this is described in the kip, i couldn't find this particular situation called out.",-1,-1,-1,0.7676814794540405,0.9700623154640198,0.9397686719894408,-1.0,accept,unanimous_agreement
536145694,5527,"hey , you mean the key itself is `null`? normally, this would be considered invalid data (in any join), and streams would just drop the record with no output (which you've observed). although, there should be a warning log, if memory serves. i don't think this was discussed in the kip discussion, so we might need to amend the kip to handle this case. as i understand it, in many relational dbs, foreign key references are allowed to be null because the reference itself is understood to be an implication. the presence of a foreign key reference implies that the key is a primary key on the foreign table, but a null reference is the _absence_ of a foreign key, so it doesn't imply anything. in this situation, the db implementation is free to define the result however it likes. so, i guess that we can go for consistency and say that null foreign key references work the same way as null equi-join (primary) keys and just drop the record with a warning. on the other hand, the situation is slightly different, and if it's useful to emit a `(lhs, null)` result, we could try to make it work. however, the implementation would be quite difficult, maybe impossible. the reason for this is that to do the join, we need to send a ""subscription"" to the rhs primary key (ie the foreign key). when the foreign key is `null`, there is no where we can send the subscription, and we can't even pick a sentinel value, since we don't control the type of the foreign key. some quick details: the (caller-provided) serdes would have to handle null keys, which they normally do not, probably resulting in npes, and they (maybe caller-provided) topic partitioner would have to handle null keys also, which it probably does not. in light of this reflection, i guess i'm leaning toward just documenting the behavior you've observed. although it might be a bit of dissonance if you're used to working with rdbs that allow it, at the end of the day, it would be less headache for you overall not to have to cope with `null` in all these unexpected places. hopefully, you could accomplish the same objective by mapping the `null` reference to some non-null, but out-of-domain sentinel value that's guaranteed not to be the primary key of an actual rhs record. how does this all sound to you? thanks for bringing it up, -john",0,0,0,0.974542796611786,0.9828805327415466,0.9811366200447084,0.0,accept,unanimous_agreement
536186044,5527,"thanks, john. specifically, i meant the result of foreignkeyextractor is `null`. in a normal case that could be because the foreign key reference is missing or the value is null, or maybe you're doing something weird in the extractor which is giving a null. i think a sentinel value will work fine. my goal is something like ""enhance the left side with data from the right side"", but the left side is the important part. so i don't want to drop records that have invalid or missing references. thanks for your help! i do think it would be good to document this, because it surprised me in comparison to a left equi join.",1,1,1,0.9878917932510376,0.9869790077209472,0.9922620058059692,1.0,accept,unanimous_agreement
537581324,5527,"yep, it looks like the issue is as you described. the thing is that it's hard to do, as outlined by john. i think you would have to use your own sentinel value to get an adequate output, and i think i will have to document this into the code.",0,0,0,0.9653964638710022,0.5332152247428894,0.9746538400650024,0.0,accept,unanimous_agreement
537582199,5527,"i'm going to merge all of the current commits into a single patch and apply it before rebasing to trunk. there are so many commits right now that rebasing takes a significantly long time and is error prone - so i will force push a new, single commit, rebase to trunk, fix up the murmur3 hash and add the comments regarding left-hand joins with null foreign keys. if anyone objects or has a better idea as to how to handle this, let me know, as this is my first big multi-month(/year) commit where rebasing is painful.",-1,-1,-1,0.7498007416725159,0.7097418904304504,0.9757999777793884,-1.0,accept,unanimous_agreement
537599188,5527,"hey , i started working on this just now, since we didn't hear back from you for a while. i wound up just merging trunk into your branch again, it wasn't too bad. i'd recommend not rebasing or squashing. since there's a merge commit in the history of this branch, it's just going to make you sad. once alternative (which i'd also not recommend) is checking out a clean branch from trunk and using diff/patch to just apply the diff between this branch and trunk. this is risky, as you could easily lose some important recent change in trunk, and it would be hard to notice or track down later. on the other hand, if you just merge trunk into this branch, you can resolve the merge conflicts (there are only two), and then the github (squash+merge) button should take care of the rest.",-1,-1,-1,0.9708393216133118,0.9845178723335266,0.8913500905036926,-1.0,accept,unanimous_agreement
537601076,5527,"- ah thanks! sorry, i was away on vacation for ~9 days and had some other things come up - did you want to carry it across the line or did you want me to keep at it?",-1,-1,-1,0.9384425282478333,0.991759717464447,0.9897291660308838,-1.0,accept,unanimous_agreement
537605591,5527,"ah i updated it all anyways. let me know if that comment is sufficient, or if there needs to be more clarity around it.",0,0,0,0.9666337370872498,0.9630491733551024,0.9806560277938844,0.0,accept,unanimous_agreement
537606172,5527,sounds good! can you also add this to the `build.gradle` (in the `:streams` project): [code block] i noticed it when i was running the tests locally,1,1,1,0.9872303605079652,0.986763060092926,0.987558901309967,1.0,accept,unanimous_agreement
537608972,5527,how's that?,0,0,0,0.9513322710990906,0.9413928985595704,0.9798794388771056,0.0,accept,unanimous_agreement
537610839,5527,"since jenkins has been a bit twitchy recently, i'll just make a note that `./gradlew clean :streams:test` has just passed for me.",0,0,0,0.9547947645187378,0.9118902683258056,0.971504271030426,0.0,accept,unanimous_agreement
537694233,5527,"hmm. i just checked the jenkins queues, and i don't see this pr in there. retest this, please.",0,0,0,0.9823331236839294,0.9580777883529664,0.9014365673065186,0.0,accept,unanimous_agreement
537732545,5527,checkstyle error: [code block],0,0,0,0.9762839078903198,0.9926325678825378,0.9889177083969116,0.0,accept,unanimous_agreement
537754256,5527,"huh. the streamspartitionassignor isn't part of this pr, and that import isn't there in this pr's branch. maybe someone merged a broken commit to trunk, and then the jenkins pr builder sucked it in here? retest this, please.",0,0,0,0.97849440574646,0.7183426022529602,0.9052226543426514,0.0,accept,unanimous_agreement
537754397,5527,"oh, yep, it was trunk: [code block]",0,0,0,0.9762317538261414,0.9473796486854552,0.9852561950683594,0.0,accept,unanimous_agreement
537974203,5527,"retest this, please.",0,0,0,0.9817549586296082,0.9711947441101074,0.788542628288269,0.0,accept,unanimous_agreement
537983099,5527,"the prior builds had all failed on one test or another. one had failed on 5 integration tests, the other on 1 other integration test, and i forget about the third. it looks like it is flakiness and unrelated to the code changes, but best to run it again.",0,-1,0,0.634019672870636,0.5251256227493286,0.9673274159431458,0.0,accept,majority_agreement
537995486,5527,"thanks, , yeah, it would be nice to see jenkins give us at least 1 green build.",1,1,1,0.9029421806335448,0.9361569881439208,0.9323140978813172,1.0,accept,unanimous_agreement
538084227,5527,"the java 8 build had just one failure, which i think is ok: [code block] likewise with the java 11+scala 2.13 build: [code block] and the java 11+scala 2.12 build: [code block]",0,0,0,0.9794334173202516,0.9898327589035034,0.8492560982704163,0.0,accept,unanimous_agreement
538085084,5527,"(jdk 11 & scala 2.13) and (jdk 8, scala 2.11) both failed on the same test, though it appears totally unrelated: [code block]",0,0,0,0.9792020916938782,0.987044394016266,0.9897199273109436,0.0,accept,unanimous_agreement
538086489,5527,"hmm, it's almost like we're all just sitting around watching these builds... i agree, i don't think that failure is related, and i also don't think it's worthwhile to keep running the tests to see if that broken test passes next time.",0,0,-1,0.5085008144378662,0.6231916546821594,0.9411638975143432,0.0,accept,majority_agreement
538108501,5527,"note, the failing test is known to be broken: [a link]",0,0,0,0.9824165105819702,0.9418793320655824,0.9940688610076904,0.0,accept,unanimous_agreement
538160544,5527,"test failures unrelated and local build passes, so merging this.",0,0,0,0.959659993648529,0.9628230333328248,0.9788020849227904,0.0,accept,unanimous_agreement
538161279,5527,merged #5527 into trunk.,0,0,0,0.9865666031837464,0.9948112964630128,0.9928476810455322,0.0,accept,unanimous_agreement
538161381,5527,thanks for the hard work and perseverance to get this done!,1,1,1,0.9703364372253418,0.9688798189163208,0.9875538349151612,1.0,accept,unanimous_agreement
538182923,5527,"yeah, congratulations, for this awesome contribution!",1,1,1,0.9914501309394836,0.995890736579895,0.9964874982833862,1.0,accept,unanimous_agreement
538184423,5527,kudos! i’ve been following this feature for a year and am extremely excited to see it make it in.,1,1,1,0.98928701877594,0.995486319065094,0.9967214465141296,1.0,accept,unanimous_agreement
538189810,5527,"congratulations , it's been a long kip and journey :)",1,1,1,0.99052631855011,0.995870053768158,0.9956284761428832,1.0,accept,unanimous_agreement
538226162,5527,congratulations ! and thanks for all the hard work! also thanks to for his initial proposal and the many hours you invested in this kip! check out [a link],1,1,1,0.9900283217430116,0.995879888534546,0.9962596893310548,1.0,accept,unanimous_agreement
538396002,5527,"thanks for all the help everyone, especially from . i couldn't have done it without you all. and thanks to jan too for getting it started so long ago.",1,1,1,0.9762505888938904,0.989960551261902,0.9927465319633484,1.0,accept,unanimous_agreement
538471438,5527,"do we need to update the docs for the new feature? we should at least mention in the upgrade guide. would you like to update the wiki, too: [a link]",0,0,0,0.9750465154647828,0.9885168075561525,0.9953097701072692,0.0,accept,unanimous_agreement
538481205,5527,"besides the upgrade guide, i think we can also update the developer-guide on dsl section: [a link]",0,0,0,0.9771761894226074,0.9824597835540771,0.9938622713088988,0.0,accept,unanimous_agreement
539648633,5527,i'll take a look at updating them. what/where is the upgrade guide?,0,0,0,0.97898530960083,0.9698367714881896,0.992923617362976,0.0,accept,unanimous_agreement
539843057,5527,"i would add maybe one bullet point to [a link] (as ""notable changes"") and also list it in [a link] not sure if both files have already a section for `2.4` release -- if not, just add one :)",0,1,1,0.7808036804199219,0.6966174840927124,0.8002471923828125,1.0,accept,majority_agreement
541166957,5527,"do you know of any cases that would lead to duplicate updates being emitted by these joins? we first noticed this because we saw ""detected out-of-order ktable update"" messages for topics emitted from some of these left joins. looking at the updates, they were identical except for the timestamps. but it happens intermittently -- and not consistently even with the same input data. we switched to inner joins, and still saw them. sometimes they updates had the same timestamp, but when they had different timestamps, one would have the timestamp from the corresponding left-table update, and the other would have the timestamp from the right-table update. so far i don't think this is causing bad data or any real problems. just wanted to check if you had seen it before i keep digging. thanks!",0,0,0,0.9743738770484924,0.9557843804359436,0.9796828627586364,0.0,accept,unanimous_agreement
541769834,5527,i think this is fine: as we've discussed in kip duplicates may happen for the same join-results but they will happen intermittently as you've observed. since it is for a ktable changelog streams overwriting multiple times would not cause correctness issues. and could chime in with more insights.,0,0,0,0.911500871181488,0.9837443232536316,0.5364359617233276,0.0,accept,unanimous_agreement
541770417,5527,"that also makes me thinking, should we make [code block] an info level entry than warn to not cause unnecessary panic since it would not cause correctness issues anyways?",0,0,0,0.943239688873291,0.9817423224449158,0.9684370160102844,0.0,accept,unanimous_agreement
541961886,5527,"interesting though -- mid to long term, i think we need to allow better handling of out-of-order data for `ktables` anyway. the main purpose to the log message was for the `builder.table()` operator -- it required data be ordered because it applied updates in offset-order, not timestamp order. hence, upstream the single-writer principle should be applied -- the warning makes sense for that case imho as it indicated a potential upstream problem. maybe we should make the warning more flexible and only turn it on for `builder.table()` operator but disable it if we use the same processor elsewhere?",0,0,0,0.8815215229988098,0.98851215839386,0.7941574454307556,0.0,accept,unanimous_agreement
541973602,5527,"hi, , just to be clear i am getting the warning from a `builder.table()` operator, which is reading from a topic which is written to by one of these joins. the join operator itself does not issue this warning.",0,0,0,0.9843958616256714,0.9540899395942688,0.9707072973251344,0.0,accept,unanimous_agreement
542099166,5527,"thanks for the clarification. maybe we introduce more out-of-order records due to the round-trip via two repartition topics than we anticipated... but i am not 100% sure why -- each update to the left-hand side would send one message to the right hand side and should receive zero or one respond messages. an update to the right hand side could send multiple message to the left hand side, however maximum one per key. -- if we compute the join result timestamp as maximum of both input timestamps, i don't see atm why we would introduce much out-of-order data. \cc thoughts?",1,1,1,0.7986449003219604,0.958810567855835,0.904107928276062,1.0,accept,unanimous_agreement
542302479,5527,"do you have any other information on how common it is, or steps to reproduce? i don't have any ideas off the top of my head, but i will take a look at the code again with this in mind...",0,0,0,0.9352028965950012,0.8469542860984802,0.9815250635147096,0.0,accept,unanimous_agreement
542481324,5527,"i'll tell you everything i can think of, most of which won't be relevant. i didn't mean to take up more of your time. i'll be offline until next week, and i mean to dig into it more then. i don't have any way yet to reproduce this in development. i've only seen it running in ec2 with 3 instances against large topics, with 16 partitions. i assume there is some timing component to the issue. - we're not using the absolute latest version of this code. i think we built your branch when it was ""feature complete"" sometime around mid-august. - we're joining two topics with hundreds of millions of messages each. they're fairly old and compacted, so i don't think there are any records with duplicate keys. - the join itself is simple -- we just keep the right-side value and discard the left-side. - it's not very common -- i would say duplicate records occur a few thousand times in a few million input records. of those, far fewer have out-of-order timestamps.",0,-1,-1,0.90077406167984,0.59748375415802,0.5646754503250122,-1.0,accept,majority_agreement
629872893,8680,"hi and , this pr is ready for code review. please have a look and do let me know your thoughts. cc and thank you!",1,1,1,0.9885699152946472,0.9929499626159668,0.9917691946029664,1.0,accept,unanimous_agreement
641549841,8680,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
641561083,8680,it seems there are some checkstyle failures. [code block],0,0,0,0.925961971282959,0.9914451837539672,0.9852927327156068,0.0,accept,unanimous_agreement
641765964,8680,thanks for the review! i have fixed the checkstyle issue now in 74ff66f.,1,1,1,0.985615849494934,0.9914931654930116,0.9908533692359924,1.0,accept,unanimous_agreement
641766088,8680,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
642155141,8680,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
642180658,8680,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
642751664,8680,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
642855293,8680,the test failures seem unrelated. merging this to trunk.,0,0,0,0.9566928148269652,0.657883882522583,0.9857468605041504,0.0,accept,unanimous_agreement
1728441122,14406,"can you add the `ctr` label, please :pleading_face:",-1,0,0,0.7278282642364502,0.7419019937515259,0.9824740290641784,0.0,accept,majority_agreement
1773483995,14406,fyi: i rebased against `trunk` to remove the conflicts.,0,0,0,0.9866914749145508,0.9860695600509644,0.9943593144416808,0.0,accept,unanimous_agreement
1776108652,14406,"no, i do not believe they are. there are 8 ""new"" failures, of which two are the same test with different parameters. here are the tests and any jiras for flakiness: * `kafka.server.describeclusterrequesttest.testdescribeclusterrequestexcludingclusterauthorizedoperations`: kafka-15419 * `kafka.server.dynamicbrokerreconfigurationtest.testlogcleanerconfig`: kafka-7966, though the jira is resolved :man_shrugging: * `o.a.k.common.network.sslversionstransportlayertest.testtlsdefaults`: kafka-9714, marked as ""critical"" but open for 3 1/2 years * `o.a.k.connect.integration.connectorrestartapiintegrationtest.testmultiworkerrestartonlyconnector`: kafka-15675, just filed (by me) with some pointers to a recent flakiness rate of 9% * `o.a.k.streams.integration.standbytaskeosmultirebalanceintegrationtest.shouldhonoreoswhenusingcachingandstandbyreplicas`: no jira. only around 2% flaky * `o.a.k.tiered.storage.integration.transactionswithtieredstoretest.testsendoffsetswithgroupid`: kafka-8003, though the jira is filed against a different test, but test failures in `testsendoffsetswithgroupid` are mentioned as being related * `o.a.k.trogdor.coordinator.coordinatortest.testtaskrequestwitholdstartmsgetsupdated`: kafka-8115, which is another old, 4.5 year old ""critical"" issue there are 22 ""existing"" failures, which are all related to the `verifynounexpectedthreads` check. `kafka.server.dynamicbrokerreconfigurationtest` is reporting that there are threads when it attempts to tear down the test harness. the remaining 21 failures all report the same unexpected threads in their check during test harness setup. there are other recent, unrelated pull requests that have experienced similar issues. in the other cases, i haven't seen that it's the `dynamicbrokerreconfigurationtest` that is the cause, though.",-1,0,0,0.5676670670509338,0.9896607398986816,0.8970609307289124,0.0,accept,majority_agreement
1776110492,14406,closing and reopening to trigger another jenkins test run.,0,0,0,0.987860381603241,0.9828290343284608,0.9910257458686828,0.0,accept,unanimous_agreement
1776377391,14406,"ugh. one of the builds failed with: [code block] no integration test failures due to threads, though :smirking_face:",-1,-1,-1,0.9732171297073364,0.7993395328521729,0.5952684283256531,-1.0,accept,unanimous_agreement
1776378053,14406,closing and reopening to restart test.,0,0,0,0.9853540658950806,0.985584318637848,0.9908498525619508,0.0,accept,unanimous_agreement
1777454027,14406,closing and reopening to restart test. jenkins doesn't seem happy lately.,-1,-1,-1,0.9165482521057128,0.9464709758758544,0.7206256985664368,-1.0,accept,unanimous_agreement
1777855712,14406,: the build for jdk 21 and scala 2.13 failed this time. did it succeed before?,0,0,0,0.9362768530845642,0.9879339933395386,0.9887804985046388,0.0,accept,unanimous_agreement
1777892441,14406,"—yes. here's a brief history for jdk 21, starting with the most recent build (build 74): * [a link]: couldn't clone repo * [a link]: tests ran, but jenkins timed out * [a link]: tests ran * [a link]: tests ran * [a link]: tests ran, but jenkins shut down in the middle :thinking_face: these same intra-jenkins communication, `git` cloning, unexpected threads, and flaky tests affect all of the jdks at random times :crying_face:",-1,0,-1,0.9616992473602296,0.5009175539016724,0.9592645168304444,-1.0,accept,majority_agreement
956456320,11390,it'd be helpful if you could please update the pr description explaining the scope of the draft pr (in its current form) and what's remaining to be done.,0,0,0,0.9475003480911256,0.9755091667175292,0.976818323135376,0.0,accept,unanimous_agreement
971736892,11390,this pr is ready for review. please take a look.,0,0,0,0.9073725938796996,0.8582626581192017,0.82227623462677,0.0,accept,unanimous_agreement
992617350,11390,"thanks for the review. please find inline replies, addressed most of them with latest commits.",1,1,1,0.8997784852981567,0.8234594464302063,0.9278669953346252,1.0,accept,unanimous_agreement
998818025,11390,": thanks for the review. please find inline replies, updated the pr addressing them with the latest commit.",1,1,1,0.9368203282356262,0.8426945805549622,0.9521385431289672,1.0,accept,unanimous_agreement
1015392509,11390,"thanks for the review. please find inline replies, updated the pr addressing them with the latest commits.",1,1,1,0.8929269909858704,0.6969464421272278,0.91409033536911,1.0,accept,unanimous_agreement
1019966726,11390,: thanks for the review. addressed your comments with the latest commits. rebased with the trunk to resolve the conflicts.,1,1,1,0.9498854279518129,0.930351972579956,0.9471571445465088,1.0,accept,unanimous_agreement
1330143560,11390,"thanks for your latest comments. addressed them inline, let me know if you have any comments.",1,1,1,0.8081779479980469,0.8820225596427917,0.9504775404930116,1.0,accept,unanimous_agreement
1330147549,11390,"thanks for your comments, addressed them with inline comments.",1,1,1,0.5462212562561035,0.5692004561424255,0.832221508026123,1.0,accept,unanimous_agreement
1330147705,11390,"thanks for your comments. addressed most of them, working on couple of the remaining comments to update the javadoc.",1,1,1,0.8860439658164978,0.906986653804779,0.9546918272972108,1.0,accept,unanimous_agreement
1340522323,11390,", do you want to have another review? since branch 3.4 has created, and this pr blocks some following tiered storage feature development (ex: copying segments to tiered storage, retention checks to clean local and remote log segments), we might need to consider to merge it first and have follow-up prs for complicated changes. wdyt? thanks.",1,1,1,0.9365419149398804,0.980219066143036,0.7732605934143066,1.0,accept,unanimous_agreement
1341457933,11390,: i plan to take another look at the pr in the next few days.,0,0,0,0.9095698595046996,0.9158912897109984,0.98281991481781,0.0,accept,unanimous_agreement
1348935454,11390,"thanks for your review, addressed them with inline replies. i updated the pr with latest commits addressing the review comments.",1,1,1,0.8899063467979431,0.8401487469673157,0.9577106833457948,1.0,accept,unanimous_agreement
1348940728,11390,: filed [a link] as you suggested.,0,0,0,0.9846559762954712,0.98713219165802,0.9924654960632324,0.0,accept,unanimous_agreement
1354541220,11390,thanks for your updated review. addressed them with inline comments and updated with the latest commits.,1,1,1,0.8554612994194031,0.8447975516319275,0.9414440393447876,1.0,accept,unanimous_agreement
1356256797,11390,"thanks for the review, addressed it with the latest commit. there are a few tests that are failed but they do not seem to be related to this pr.",1,1,1,0.8264633417129517,0.9209979772567748,0.923693835735321,1.0,accept,unanimous_agreement
1356390240,11390,"hey , i wanted to let you know about kafka-14470 as i think it affects some of the future kip-405 prs. can we align these efforts so that we can get to the desired end state faster? for example, once the prs that have been submitted are merged, we can move `remoteindexcache` to the `storage` module too.",0,0,0,0.9823570251464844,0.9672855734825134,0.9672260880470276,0.0,accept,unanimous_agreement
471557966,6363,"the pr is ready for review. the following items are expected to be addressed along with your comments: * the code on task assignment is currently written to be readable and evaluate correctness. i'd like to add micro-benchmarks and i expect it'll be optimized. * unit tests for `distributedherder` and `workercoordinatorincrementaltest` will be expanded to guard against changes in the new protocol more extensively. * more logging will be added appropriately and some integration with metrics will be considered. * a few more integration tests will be added. some files contain changes that are introduced by other outstanding prs. if still present here, please skip or review the changes in their respective prs. really looking forward to your comments! thanks!",0,0,0,0.901117980480194,0.925736963748932,0.6922513842582703,0.0,accept,unanimous_agreement
484717843,6363,", thanks for responding to my feedback! i just had one remaining comment. looking great!",1,1,1,0.9920592308044434,0.9954885840415956,0.9967017769813538,1.0,accept,unanimous_agreement
493287766,6363,"thanks and for all the insightful and useful comments! i believe i've addressed everything, except a few cleanup/refactoring suggestions that deemed high risk at this point and will be addressed in a follow up pr after this feature is merged. soak testing has been also performed and has confirmed correct execution for several days. more extensive testing and performance benchmarking will follow up in the next few days. i'll be glad if we can get this in. thanks!",1,1,1,0.990708589553833,0.995332419872284,0.9961559176445008,1.0,accept,unanimous_agreement
554337624,6363,"hi can you please suggest in which kafka version is this issue fixed as i am still seeing this problem every time i add a new connector, all the connector gets restarted?",0,0,0,0.9795498251914978,0.9785258769989014,0.960752546787262,0.0,accept,unanimous_agreement
554407084,6363,", the jira issue ([a link] shows that this was merged and completed in ak 2.3.0. if you're using ak 2.3.0 or later and still having problems, please create a new jira issue and provide the connect worker configs and a lot more detail about a procedure to replicate the problem. thanks!",1,1,1,0.974219799041748,0.9868243336677552,0.9736402630805968,1.0,accept,unanimous_agreement
417397352,5582,"beginning to add tests, and i need to rebase to get `kafka-7324: npe due to lack of saslextensions in sasl/oauthbearer (#5552)`",0,0,0,0.9842860102653505,0.9921993613243104,0.9931446313858032,0.0,accept,unanimous_agreement
417676910,5582,rebasing again to resolve conflicts with `kafka-6950: delay response to failed client authentication to prevent potential dos issues (kip-306)`,0,0,0,0.9874197840690612,0.994050681591034,0.9924699664115906,0.0,accept,unanimous_agreement
423861956,5582,i rebased and pushed a squashed commit that implements your approach except it triggers a re-auth upon a write as described by the kip rather than upon a read (which is what your prototype used). i think triggering upon a write is correct. i'll add metrics and tests soon. hopefully we will get the 3rd binding up-vote in time for the 2.1.0 deadline today. thanks again for that prototype -- i could not have gotten there myself in a reasonable amount of time because the low-level network code is pretty tough to pick up.,1,1,1,0.8624454140663147,0.9280322790145874,0.9785102605819702,1.0,accept,unanimous_agreement
424499373,5582,"this pr now has everything except: - implementation and tests for client-side latency metrics - implementation and tests for server-side connection kill metrics - a test to see if server-side kill works i added re-authentication to the `nioechoserver` and i test for it with bearer tokens and delegation tokens within `saslauthenticatortest`. i added re-authentication every 50 ms for all end-to-end scala authorization integration tests as well. i will work on the remaining 3 items on wednesday. can you review what is here already, or would you prefer to wait? ron",0,0,0,0.9045959711074828,0.9502716064453124,0.8425325751304626,0.0,accept,unanimous_agreement
424644619,5582,i will review the pr tonight or tomorrow. there is a failing test iin the pr builds (`org.apache.kafka.common.security.authenticator.saslauthenticatortest.testtokenreauthenticationoversaslscram[reauthenticate=true]`) - can you check if that is a test issue or a code issue? i will start a system test run once you get a chance to look at that.,0,0,0,0.9833734035491944,0.9922526478767396,0.989467203617096,0.0,accept,unanimous_agreement
424707930,5582,it is a test issue. i cannot reproduce the error locally -- it passes on my laptop. i pushed a commit to sleep a bit longer in the hope that it will fix the issue during the build.,0,0,0,0.6485366225242615,0.6414073705673218,0.9746735692024232,0.0,accept,unanimous_agreement
424718026,5582,"thanks, i have started a system test run on your branch: [a link] will review the pr later tonight.",1,1,1,0.9160335063934326,0.9202512502670288,0.8625325560569763,1.0,accept,unanimous_agreement
424802468,5582,test still fails here but cannot reproduce the issue locally. pushed another commit to try to fix it...,-1,0,0,0.6206949353218079,0.7592334151268005,0.9716787934303284,0.0,accept,majority_agreement
424950015,5582,"pushed a commit that i think fixes/resolves most of the issues you raised. it's getting late here so i couldn't get to all of them, but i think we are in a better place at this point.",0,0,1,0.6001424789428711,0.9511675238609314,0.571796715259552,0.0,accept,majority_agreement
425078873,5582,"the test failures are due to the tests being flaky rather than a coding problem in the feature implementation. i may have been able to solve the delegation token-related test that was failing (it didn't fail in these runs) by increasing the session lifetime in that test (from 50 ms to 500 ms). i will try to fix these flaky tests by increasing the associated session lifetimes as well. i think what may be going on is the tests are taking a long time to run and by the time they send data the session expiration time has arrived and the connection is being killed. it has also occurred to me that setting the session lifetime to something less than 1000 ms is probably not a good idea in general -- it may invite this type of flakiness due to unexpected delays, for example -- and maybe we should change the units of the configuration value to seconds instead of milliseconds to make sure it is never less than 1000 ms. thoughts? commit pushed with larger lifetimes, it was clean on my laptop.",0,0,0,0.7654852867126465,0.9456668496131896,0.9649165272712708,0.0,accept,unanimous_agreement
425097942,5582,i just pushed a commit to add an expiredsessionskilledcount metric. the kip called for expired-connections-killed-total as the metric name but expiredsessionskilledcount is consistent with the existing metric requesthandleravgidlepercent. thoughts?,0,0,0,0.986974000930786,0.9914263486862184,0.971773326396942,0.0,accept,unanimous_agreement
425170620,5582,"sorry, i haven't had any time today to look into this pr. given the complexity of the pr and how close it is to feature freeze, it would be very helpful if we could get another reviewer. i will try and take another look tomorrow, but not sure if we will have enough time to get this merged without help from one more reviewer. if it didn't meet feature freeze deadline, would it work for you if it was committed only to trunk and not the 2.1.0 branch?",-1,-1,-1,0.9819985628128052,0.9695447683334352,0.9769267439842224,-1.0,accept,unanimous_agreement
425176688,5582,"we would really like this in 2.1.0 if at all possible. is there anything i can do to help? maybe find another reviewer, perhaps someone else who has voted for the kip?",1,0,1,0.6854456067085266,0.9737338423728944,0.8558603525161743,1.0,accept,majority_agreement
425179089,5582,"yes, getting another reviewer would definitely speed up the process.",0,0,0,0.9615402817726136,0.9718461632728576,0.973580837249756,0.0,accept,unanimous_agreement
425180086,5582,"there are a couple of changes that are not necessary as part of this pr -- more like cleanup of things i discovered while working on the alternate implementations that we considered and rejected. i could remove those changes and submit them as minor tickets post-2.1.0 if it helps to minimize the surface area requiring review. `expiringcredentialrefreshinglogin` is one, for example, and `oauthbearersaslclienttest` is another.",0,0,0,0.9846174716949464,0.991788685321808,0.9908899664878844,0.0,accept,unanimous_agreement
425261409,5582,i discovered that many of the failing tests were due to the fact that gssapi was not re-authenticating because it uses the client_complete state on the client and i wasn't handling that possibility correctly. this meant its connections were continually being killed by the server. this is now fixed. i also moved the expiration check from kafkarequesthandler to socketserver so that the check is done more upstream and therefore with as little delay as possible; i think that delay sometimes has an impact when we run with really tight expiration times on the order of milliseconds. i'll be curious to see how the tests look now.,0,0,1,0.8925604224205017,0.925128698348999,0.6503489017486572,0.0,accept,majority_agreement
425285024,5582,"both builds passed. i'm going to work on adding the last metrics, and if i get that done tonight then i believe this will be complete.",0,0,0,0.6437253952026367,0.921186625957489,0.8692435622215271,0.0,accept,unanimous_agreement
425312612,5582,"i was able to add the additional metrics. this pr now has all functionality, but i could not figure out how to test server-side expired connection kill or its metric since i have no way of running an older client. i know it works because it was the cause of the test failures in `saslgssapisslendtoendauthorizationtest` -- i just don't have a way to test it in an automated fashion because there is no way to disable re-authentication at the client side. i probably have to add metric documentation as well. but i think that's it. i asked someone to review, but this section of the product is pretty difficult to understand without experience, so i'm not optimistic about it. any chance you can review this on friday? al tests passed last run, and i hope the same will be true of this one.",0,0,0,0.92314475774765,0.8727054595947266,0.8663526177406311,0.0,accept,unanimous_agreement
425365864,5582,"in terms of testing, i think system test may be the way to go for testing with older clients since we already have other system tests for testing with older clients. if this goes in by monday, you have until code freeze two weeks later to add the system test to 2.1.0.",0,0,0,0.9856520891189576,0.9891701340675354,0.9758005738258362,0.0,accept,unanimous_agreement
425408404,5582,ok. last set of builds was clean again. i'm confident this is working as expected and is able to be merged assuming a review cycle can be completed.,0,0,0,0.6578578948974609,0.7552536725997925,0.6457082033157349,0.0,accept,unanimous_agreement
425438808,5582,"metric documentation is added, this is now done except for any additional pr review required. all conversations from above are resolved/fixed. i've contacted 5 people (4 of whom up-voted the kip) in the hope that at least one will be able to review.",0,0,0,0.9380462169647216,0.9796074628829956,0.9653214812278748,0.0,accept,unanimous_agreement
425588102,5582,i reverted 9 files and eliminated 500 changed lines from this pr. i will submit minor tickets for these changes separately. these changes perform cleanup of code that was discovered while implementing rejected alternatives. these changes are minor and can be submitted separately at a later date. they have no impact on functionality or performance and are simply reducing technical debt. keeping these separate reduces the surface area of this pr.,0,0,0,0.9623684883117676,0.9734928011894226,0.9344301223754884,0.0,accept,unanimous_agreement
426148561,5582,"i added functionality to prevent changing the principal or the sasl mechanism during re-authentication and added tests for both cases. i defined a 1-second minimum before you can re-authenticate a second time (see comment above) to prevent the rogue/buggy client from re-authenticating over and over again (the connection will be closed if the 1-second timeframe is violated, and then there will be the new ddos delay as well, so i think this covers it). i will address client interoperability with system tests (i assume it will be easier as you stated, though i am unfamiliar with the system test suite at the moment). assuming everything looks good, any chance of this being merged and included in 2.1.0 even though it is the morning after?",0,0,0,0.9283722043037416,0.9274160265922546,0.9720940589904784,0.0,accept,unanimous_agreement
426184322,5582,i can do another review today to make sure it is ready to merge. can you check with the 2.1.0 release manager on whether it can still go into 2.1.0? the alternative is to merge to trunk only after the branch is cut.,0,0,0,0.9853885769844056,0.9929608702659608,0.9944557547569276,0.0,accept,unanimous_agreement
426244843,5582,see comment from above -- any chance this can get into the 2.1.0 release if it is merged today?,0,0,0,0.9886800050735474,0.9944937825202942,0.9946426153182985,0.0,accept,unanimous_agreement
426246111,5582,"do you think we can merge this pr by end of wednesday? since this pr is almost ready to be merged, it is reasonable to wait for a day or two to include it in 2.1.0 release.",0,0,0,0.985765278339386,0.9833241105079652,0.9923128485679626,0.0,accept,unanimous_agreement
426253681,5582,"the test failures seem to be timing-based. i can't reproduce any of the issues locally, and the jdk 8 and 10 results are widely divergent as well. i'm going to revert the part of the checks for re-authentications in the endtoend tests -- i'll only check to make sure nothing has been killed and no re-authentications have failed and allow zero re-authentications instead of demanding that at least one occurs. i'll also set the max session re-auth to something greater than 1 second since that is the cutoff for repeated re-authentication rejection and i do see a re-authentication failure in the jdk 8 build (probably in the jdk 10 build as well since there are so many more failures there, but i haven't looked hard enough to find one). let's see if the next builds are clean again.",0,0,0,0.9083757996559144,0.8601540923118591,0.9833557605743408,0.0,accept,unanimous_agreement
426260661,5582,"i will do another review today and see where we are. we also need a good system test run on this branch. unfortunately the last one i started didn't work, so started another one: [a link] since we don't have any interop tests yet, it will be good if you can run some manual tests with different versions of broker/clients.",0,0,0,0.5581116676330566,0.5515901446342468,0.9157339930534364,0.0,accept,unanimous_agreement
426294584,5582,i am unable to see the system test at the link [a link] do i need some kind of permission in the jenkins instance that i do not have? i get http 404 not found. i will work on manually running some interop tests.,0,0,0,0.9011795520782472,0.8795300722122192,0.9834716320037842,0.0,accept,unanimous_agreement
426304117,5582,"sorry, not sure about access since it is a confluent jenkins. i will update with the system test results when the test run completes.",-1,-1,-1,0.9863685965538024,0.993354558944702,0.9119098782539368,-1.0,accept,unanimous_agreement
426329033,5582,we need to get to the bottom of these test failures today if we want to merge tomorrow.,0,0,0,0.929238736629486,0.97970312833786,0.979885756969452,0.0,accept,unanimous_agreement
426358785,5582,"agreed. the tests failures in the jdk 8 build appeared when i ""fixed"" the expiredconnectionskilledcount metric. i'm going to comment out that metric to see if the failures go away. unfortunately this is a long turnaround cycle -- a couple of hours -- but it's the best i can do given that i can't reproduce the issue locally under jdk 1.8.",0,0,0,0.8964889049530029,0.8957987427711487,0.9400529861450196,0.0,accept,unanimous_agreement
426396730,5582,"still seeing this error on jdk 8 build: this is the same type of problem that occurred before. it isn't happening as frequently without the metric there, but it is still happening. i pushed a commit to add more debug information to the error message. will have to wait for these builds to finish and then wait another 1.5 hours for a failure to occur in the next build.",0,0,0,0.5594919919967651,0.8499006628990173,0.7391797304153442,0.0,accept,unanimous_agreement
426462405,5582,"i agree it would be unwise to include this in the 2.1.0 release. thank you both for your willingness to entertain the possibility over the past days. the point that convinces me is this one: i am actively working on the authenticationexception issue, and while it is difficult to run experiments where the turnaround time is on the order of hours, i am hopeful that i can figure out what is going on there. let's shoot for merging this to trunk when the above two issues are resolved. i don't expect to be able to write exhaustive tests to guarantee that authentication is not impacted by re-authentication, but i will try to re-organize the code so that the diff makes it much easier to confirm by inspection that there is no impact. i did not keep this in mind as i worked, so i think i can accomplish the same things with more clearly fenced-in changes. i think that -- at a minimum -- will be helpful. thanks again for your willingness to give the pr a chance for 2.1.0, and rajini thank you especially for the technical advice on the implementation and the reviews. hopefully i can get this into a trunk-mergeable state soon.",1,1,1,0.9115281701087952,0.9608296751976012,0.8073725700378418,1.0,accept,unanimous_agreement
426643419,5582,the system test run from this branch from yesterday passed all tests except the nine variations of quotatest which are also failing from trunk.,0,0,0,0.9834729433059692,0.994001567363739,0.9859477281570436,0.0,accept,unanimous_agreement
426653511,5582,"i am now unable to reproduce the errors. i've run 3 separate builds -- some were clean, and while others had errors they seem to be unrelated. i can't fix something that doesn't show up!",-1,-1,-1,0.8117485046386719,0.9876806735992432,0.9809107184410096,-1.0,accept,unanimous_agreement
426656500,5582,"i will respond to review comments incrementally so that i kick off a bunch of builds. one of them is bound to exhibit the problem at some point, and then maybe i'll get useful information from the additional detail i put in the error message.",0,0,0,0.968698799610138,0.9728614687919616,0.978326976299286,0.0,accept,unanimous_agreement
426986850,5582,"still unable to reproduce the problem, both jdk 8 and 10 builds are totally clean at this point. today i'm going to try to re-work the `saslclientauthenticator` and `saslserverauthenticator` code so that we can more easily say via inspection that the existing authentication flow is unchanged. hopefully that will be useful and then the builds will either remain clean or the `authenticationexception` problem will arise (and i will get additional information via the exception message, which i augmented). then we can decide whether to commit this to trunk or not.",0,0,0,0.9557701349258424,0.8151571154594421,0.914858877658844,0.0,accept,unanimous_agreement
426999888,5582,"thanks for making the updates. fyi if you want to rerun tests without a commit, you can just add a comment `retest this please`.",1,1,1,0.9492290019989014,0.782893180847168,0.9746401906013488,1.0,accept,unanimous_agreement
427260765,5582,"i reorganized `saslclientauthenticator` and `saslserverauthenticator` to make it much easier to tell by inspection that authentication is unaffected by the re-authentication code. i am unable to reproduce the `authenticationexception` problem we were seeing the other day -- the issue has not reappeared across several builds. the reorganized code and a successful system test should give us high confidence that authentication is unaffected and the issue is likely something that happens rarely -- maybe due to some kind of race condition that is possible only during specific re-authentication states. that's conjecture on my part, but i don't know what else i can do if the issue doesn't reappear. i spent the entire day staring at the code and reorganizing it, and nothing jumped out at me. please review the latest code and let me know your thoughts on how you wish to proceed with respect to potentially merging this to trunk.",0,0,0,0.7104009985923767,0.973459005355835,0.9510104060173036,0.0,accept,unanimous_agreement
427291312,5582,"thanks for the updates. i will do another round of review early next week. since there have been no more failures, i think we can commit to trunk. since we are early in the release cycle for the next release, we will have plenty of builds before that and if we do run into that exception again, hopefully the additional debug you added will pinpoint the cause.",1,1,1,0.8937363028526306,0.7650981545448303,0.9774093627929688,1.0,accept,unanimous_agreement
427367528,5582,"thanks. i'll look forward to addressing any review comments. fyi, i just updated the kip to refer to 2.2.0 instead of 2.1.0 and the current metric names (some of which changed over the course of this pr). i also just pushed a commit updating the `ops.html` document to refer to 2.2.0 instead of 2.1.0.",1,1,1,0.9848355054855348,0.9160059094429016,0.9841519594192504,1.0,accept,unanimous_agreement
429698198,5582,"everything is finished except re-doing `saslauthenticatortest` to make it less wasteful (i.e. don't just blindly re-run all tests in ""re-authenticate"" mode) and performing a rebase. i found rebase without squash to be difficult given that i had a bunch of oauth technical debt cleanup changes in here along the way that i backed out. i have a squash rebase ready to push that runs the test suite cleanly; i'll push that sometime monday edt, then assuming that runs well on the build farm all that will remain will be `saslauthenticatortest`.",0,0,0,0.8625635504722595,0.9473837018013,0.9194850325584412,0.0,accept,unanimous_agreement
430094679,5582,"i believe this is all set, all review comments are addressed. note that there were lots of errors on the previous jdk 11 build ([a link] but i looked at some of the other pr builds and the same thing happens elsewhere at times (e.g. [a link] the jdk 8 build was clean. so i suspect this is fine. **i have yet to see a recurrence of the `authenticationexception` problem**.",0,0,0,0.9056928753852844,0.9397702217102052,0.722006618976593,0.0,accept,unanimous_agreement
430175809,5582,thanks for the updates. agree that the test failures are unrelated to this pr. i will do another review tomorrow and hopefully we can merge this week.,1,1,1,0.9254968166351318,0.9480398297309875,0.9755502343177797,1.0,accept,unanimous_agreement
430468029,5582,rebasing to resolve conflict with `clients/src/test/java/org/apache/kafka/common/network/networktestutils.java`,0,0,0,0.9886593818664552,0.9948392510414124,0.992679476737976,0.0,accept,unanimous_agreement
430472303,5582,looks like `trunk` build is broken due to [a link] when i rebased onto trunk for this pr i am getting this compile error locally and i assume it will also happen here: [code block] i think `org.apache.kafka.streams.kstream.windowstest` has a similar compile error.,0,0,0,0.9626722931861876,0.9593712687492372,0.9754037857055664,0.0,accept,unanimous_agreement
430474995,5582,"ok, i see [a link] is where this will be fixed.",0,0,0,0.9857940077781676,0.9853535294532776,0.9924232959747314,0.0,accept,unanimous_agreement
431457065,5582,any progress on another review? would like to merge asap so it gets as many builds and test runs as possible prior to the next release.,0,0,0,0.9846166372299194,0.9892016649246216,0.986047387123108,0.0,accept,unanimous_agreement
431789440,5582,"sorry, haven't had time to review yet, will try and do it today or tomorrow.",-1,-1,-1,0.98478764295578,0.9905211925506592,0.989299774169922,-1.0,accept,unanimous_agreement
432309421,5582,latest comments are addressed/pushed. the one issue that might need more attention is the `reauth_bad_mechanism` state in `saslserverauthenticator`. see my comment above.,0,0,0,0.975100040435791,0.993362545967102,0.9893777966499328,0.0,accept,unanimous_agreement
433043426,5582,my recent merge #5684 created some conflicts with this pr. please rebase the pr.,0,0,0,0.9748889207839966,0.8856775164604187,0.974477469921112,0.0,accept,unanimous_agreement
433181272,5582,"made two suggested changes, both builds were clean. rebasing onto latest trunk now to resolve conflicts.",0,0,0,0.9800910353660583,0.9847011566162108,0.9943471550941468,0.0,accept,unanimous_agreement
433383564,5582,"after rebase jdk8 build was clean, jdk 11 build has 2 failures but they seem unrelated/transient issues.",0,0,0,0.969170093536377,0.9058299660682678,0.9806134700775146,0.0,accept,unanimous_agreement
433390920,5582,"thanks for the updates. i think we are good to go. i will take a look later today and merge to trunk (if there are any other changes required, we can do them in follow-on prs).",1,1,1,0.971972644329071,0.9676498770713806,0.9819298386573792,1.0,accept,unanimous_agreement
627487350,8657,please take a look :),1,1,0,0.9530982971191406,0.9254632592201232,0.9434741735458374,1.0,accept,majority_agreement
637668386,8657,could you take a look?,0,0,0,0.98567134141922,0.9828481674194336,0.9896900653839112,0.0,accept,unanimous_agreement
643890644,8657,so... could we keep it simpler?,0,0,0,0.962125301361084,0.9890135526657104,0.9809629917144777,0.0,accept,unanimous_agreement
643933638,8657,[code block] the flaky is traced by #8853,0,0,0,0.9872459173202516,0.9938703775405884,0.99415922164917,0.0,accept,unanimous_agreement
644521032,8657,my bad :( i'll keep that in mind,-1,-1,-1,0.9903530478477478,0.9919707775115968,0.9961332082748412,-1.0,accept,unanimous_agreement
646487562,8657,thanks for all your reviews. how to see the result from this url?,1,1,1,0.7646340727806091,0.6872099041938782,0.942221224308014,1.0,accept,unanimous_agreement
646685855,8657,": the system test results can be found in [a link] if you click on report.html, it shows there were 76 failures. could you take a look and see if it's related to this pr? i will trigger another run just to see if any of those failures were transient.",0,0,0,0.9837142825126648,0.9912009239196776,0.9906392097473145,0.0,accept,unanimous_agreement
646718970,8657,i compare the failed tests to [a link] (the link is attached to [a link] there are two failed tests happens only on this pr. 1. consumebenchtest [code block] 2. streamsoptimizedtest [code block] wait for the new run.,0,0,0,0.9870488047599792,0.9915777444839478,0.9937731623649596,0.0,accept,unanimous_agreement
647020292,8657,this is a second run of the system tests. [a link],0,0,0,0.9877477884292604,0.9689054489135742,0.991803765296936,0.0,accept,unanimous_agreement
647233460,8657,[code block] pass on second run. [code block] still fail and the error message is [code block] need to dig in it :( could you trigger a system test for trunk branch ? the related code have been updated recently (0f68dc7a640b26a8edea154ea4ea2b6d93b5104b) i run system test [code block] on my local for trunk branch and it always fails :(,-1,-1,-1,0.9859678745269777,0.9871598482131958,0.9959922432899476,-1.0,accept,unanimous_agreement
647352301,8657,i have filed [a link] to trace [code block],0,0,0,0.9865581393241882,0.9865601062774658,0.9955959916114808,0.0,accept,unanimous_agreement
647688705,8657,it seems we just fixed a bunch of client compatibility related failures in [a link] another 18 test failures were due to tlsv1.3 and are tracked in [a link] i started another run of system tests.,0,0,0,0.9828116297721864,0.9864012598991394,0.9892340898513794,0.0,accept,unanimous_agreement
648249808,8657,there were still lots of client compatibility related failures [a link] . not sure why since those tests were passing in [a link],0,0,0,0.7720794677734375,0.8259912133216858,0.9434743523597716,0.0,accept,unanimous_agreement
648254407,8657,: i think the client compatibility test failures are probably because you haven't rebased the pr. #8841 was fixed 6 days ago. could you rebase your pr?,0,0,0,0.969895839691162,0.9757707118988036,0.985202133655548,0.0,accept,unanimous_agreement
648254987,8657,done,0,0,0,0.976450741291046,0.8974218964576721,0.8682363629341125,0.0,accept,unanimous_agreement
648259565,8657,thanks. triggering another round of system tests.,1,1,1,0.8974058628082275,0.8498037457466125,0.8500022292137146,1.0,accept,unanimous_agreement
649615869,8657,latest test results with 31 failures. [a link] i will trigger a system tests for trunk.,0,0,0,0.9872697591781616,0.9874382615089417,0.993065595626831,0.0,accept,unanimous_agreement
649635872,8657,big thanks !,1,1,1,0.982089638710022,0.9850693345069884,0.9871113896369934,1.0,accept,unanimous_agreement
650240109,8657,"for comparison, 29 test failures in trunk [a link]",0,0,0,0.971255660057068,0.9919599890708924,0.9928989410400392,0.0,accept,unanimous_agreement
650503360,8657,it seems to me there are many flaky in system tests and i need more time to dig in them (tlsv1.3 is traced by [a link] **following failed tests happens with this pr** [code block] [code block] [code block] [code block] [code block] [code block] [code block] [code block] --- **following failed tests happens without this pr** [code block] [code block] [code block] [code block] [code block] [code block] [code block],-1,0,0,0.848048210144043,0.986729085445404,0.9693294167518616,0.0,accept,majority_agreement
650583070,8657,i just merged the pr for kafka-10180. perhaps you can rebase again.,0,0,0,0.9802901148796082,0.992566704750061,0.994870901107788,0.0,accept,unanimous_agreement
650586275,8657,done,0,0,0,0.976450741291046,0.8974218964576721,0.8682363629341125,0.0,accept,unanimous_agreement
650786185,8657,latest system test results. down to 15 failures. [a link] will do another trunk run for comparison.,0,0,0,0.9840360283851624,0.9768983721733092,0.988918960094452,0.0,accept,unanimous_agreement
650923843,8657,1. connect_rest_test.py is traced by #8944 2. zookeeper_tls_test.py is traced by #8949,0,0,0,0.9852865934371948,0.9944720268249512,0.9917069673538208,0.0,accept,unanimous_agreement
651902726,8657,14 system test failures in trunk. [a link],0,0,0,0.9660362601280212,0.9904172420501708,0.9781166315078736,0.0,accept,unanimous_agreement
651941352,8657,there are three failed tests which take place with this pr. 1. core/transactions_test 1. core/downgrade_test 1. core/upgrade_test will dig in them :),1,1,1,0.5742272734642029,0.9858457446098328,0.6992456316947937,1.0,accept,unanimous_agreement
653224351,8657,core/downgrade_test core/upgrade_test those are flaky on my local as well. should i fix all flaky before merging this pr?,0,0,0,0.577236533164978,0.8435799479484558,0.9786493182182312,0.0,accept,unanimous_agreement
653227128,8657,": thanks for the investigation. you don't need to fix all those flaky tests. it would be helpful if you could file separate jiras to track them, if not already. overall, do you think there is no new system test failure introduced by this pr?",1,1,1,0.9594716429710388,0.9770079851150512,0.9630935192108154,1.0,accept,unanimous_agreement
653232232,8657,i did not observe obvious relation between flaky and this pr. i have filed 4 prs to fix flaky system tests. maybe we can run system tests again after those prs are merged. hope there is good luck to me :),1,1,1,0.9923366904258728,0.9949146509170532,0.9939467906951904,1.0,accept,unanimous_agreement
653953935,8657,: thanks. are the unit test failures also due to flaky tests?,1,1,1,0.8934978246688843,0.8025602102279663,0.9424300789833068,1.0,accept,unanimous_agreement
653956055,8657,"i think so. however, i’m trying to resolve all of them so as to make this pr more safe :) #8981 #8974 #8913 are pending for reviewing. the remaining failed tests on this pr are downgrade_test and upgrade_test. i will check them carefully tomorrow.",1,1,1,0.90493643283844,0.9751004576683044,0.992801547050476,1.0,accept,unanimous_agreement
656365594,8657,: all 3 prs you fixed above have been merged. do you want to rebase again so that i can run system tests one more time?,0,0,0,0.981978952884674,0.9923328161239624,0.9927747249603271,0.0,accept,unanimous_agreement
656382338,8657,sure!,1,0,0,0.7571635842323303,0.8522761464118958,0.972205400466919,0.0,accept,majority_agreement
657739902,8657,could you trigger system tests again?,0,0,0,0.9876635670661926,0.9936508536338806,0.9933815002441406,0.0,accept,unanimous_agreement
658027392,8657,[code block] it is traced by [a link],0,0,0,0.9882601499557496,0.9906024932861328,0.9958465695381165,0.0,accept,unanimous_agreement
658281873,8657,: here is the latest system test result. [a link] the number of failures went up to 17.,0,0,0,0.9869663715362548,0.9857160449028016,0.9878489971160888,0.0,accept,unanimous_agreement
658294532,8657,ok. let me dig in them again :) there are 3 failed tests related to [code block] and i have filed #9021 to fix them. [code block] is traced by #9026 and the approach of #9026 works for [code block] i think. [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link],1,1,1,0.9874370098114014,0.9907181859016418,0.9925474524497986,1.0,accept,unanimous_agreement
661862318,8657,"could you run system tests on both trunk and this pr? the tests which fail frequently on my local are shown below. 1. transactions_test.py #9026 1. group_mode_transactions_test.py #9026 1. security_rolling_upgrade_test.py #9021 1. streams_standby_replica_test.py (i'm digging it) also, i have filed tickets for other failed system tests.",0,0,1,0.967853307723999,0.9712358713150024,0.5139414668083191,0.0,accept,majority_agreement
662642708,8657,: i just merged #9026. could you rebase again? i will run the system tests after that. thanks.,1,1,1,0.9725329279899596,0.9705829620361328,0.9718896746635436,1.0,accept,unanimous_agreement
662810186,8657,done. the known flaky [code block] is traced by #9059,0,0,0,0.9874643087387084,0.9873654842376708,0.9913592338562012,0.0,accept,unanimous_agreement
663109443,8657,: only 6 test failures in the latest run with your pr. [a link] i will do another run on trunk for comparison.,0,0,0,0.9821633696556092,0.9767051339149476,0.983199179172516,0.0,accept,unanimous_agreement
663597341,8657,3 system test failures with trunk. [a link],0,0,0,0.971755027770996,0.9895798563957214,0.9866788983345032,0.0,accept,unanimous_agreement
663618045,8657,"thanks for the reports. unfortunately, the failed tests are totally different in both results :( this pr has been rebased so the fix for [code block] is included. [code block] will get fixed by #9066. i will test the others on my local later.",-1,-1,-1,0.95876806974411,0.8584510087966919,0.9925179481506348,-1.0,accept,unanimous_agreement
663818439,8657,"i have rebased this pr to include fix of [code block]. could you run system tests again? except for [code block], [code block] and transaction tests, other tests work well on my local.",0,0,0,0.9615612030029296,0.9895087480545044,0.9933072924613952,0.0,accept,unanimous_agreement
664652669,8657,": thanks for the diligence. running the system tests again. : there are two proposed ways of solving this issue. 1. async completion of delayed operations in a separate thread pool ([a link] 2. this pr where we let the caller of replicamanager.appendrecords() to complete returned delayed operations without holding a lock. approach 1 makes the api a bit cleaner. however, it introduces yet another thread pool used in the common path.this thread pool needs to be configured and monitored properly, which adds complexity. since the deadlock issue is isolated in groupcoordinator, i am not sure if it's worthwhile to add another thread pool just to avoid the deadlock. so, even though approach 2 adds a bit complexity in the api, it's probably still the better solution at this stage. please let me know your thoughts.",1,1,1,0.9706624150276184,0.8352610468864441,0.9684828519821168,1.0,accept,unanimous_agreement
665146517,8657,2 system test failures in the latest pr. [a link],0,0,0,0.9625806212425232,0.986193835735321,0.9888650178909302,0.0,accept,unanimous_agreement
665149860,8657,[code block] -> [a link] i will take a look at [code block] ([a link],0,0,0,0.9849795699119568,0.9820032715797424,0.9943631887435912,0.0,accept,unanimous_agreement
666398632,8657,those 2 failed tests are flaky on my local and there are issue/pr related to them. please take a look.,0,-1,-1,0.5596147775650024,0.6676591634750366,0.5209233164787292,-1.0,accept,majority_agreement
666403581,8657,"thanks for the summary. with regards to the thread pool option, would this be used for completion of delayed operations for the group coordinator only? when it comes to tuning and monitoring, you're thinking we'd have to introduce a config for the number of threads and an `idle` metric?",1,1,1,0.8757750988006592,0.6132675409317017,0.9546399116516112,1.0,accept,unanimous_agreement
666753554,8657,": i think the proposal is to complete all delayed operations in a separate thread pool. my concerns for that approach are the following: (1) configuration: how many threads should be used? should that be dynamically configurable? should the queue to this thread pool be unbounded or fixed size? if it's the latter, how do we configure the queue size? (2) monitoring: how do we monitor the utilization of the new thread pool and the potential back pressure it creates (if the queue is fixed size)? (3) quota: do we need to account for the resource utilization of this new thread pool per client/user for quota purpose? (4) debugging: if there is a latency issue, how do we know whether this is due to delays in the new thread pool? so, overall, i feel that we should be careful with adding another thread pool used in the common code path since it adds other complexity. given that this issue is isolated in groupcoordinator, i am not sure adding a thread pool is an overall simpler solution. : i saw that you consolidated the commit history. were there significant changes since the last review?",0,0,0,0.8334861397743225,0.9922852516174316,0.9293897747993468,0.0,accept,unanimous_agreement
666882466,8657,i rebased code base to include [a link] which fixes flaky [code block]. retest this please,0,0,0,0.9877939224243164,0.9351083040237428,0.9904909133911132,0.0,accept,unanimous_agreement
667233872,8657,": so, it's just a rebase and there is no change in your pr?",0,0,0,0.908228874206543,0.991950511932373,0.9883735775947572,0.0,accept,unanimous_agreement
667442912,8657,the last change to this pr is to rename a class (according to ’s comment),0,0,0,0.9849393367767334,0.9932940006256104,0.9944671392440796,0.0,accept,unanimous_agreement
669330984,8657,": the following is my thought after thinking about this a bit more. the changes that we made in delayedjoin is complicated and it still doesn't completely solve the deadlock issue. adding more complexity there to solve the issue is probably not ideal. as a compromise, i was thinking another approach. we could pass down a flag to replicamanager.appendrecords() to complete the delayed requests in a separate thread there. only groupcoordinator will set this flag. so, the background completeness check is limited to the offset_commit topic. since this topic is low volume, a single thread is likely enough to handle the load. so, we don't have to make the number of thread configurable and don't have to worry about this thread being overwhelmed. the benefit of this approach is that the code is probably easier to understand since we could keep most existing logic in groupcoordinator unchanged. what do you think?",0,0,0,0.8535999059677124,0.9346455335617064,0.7758811116218567,0.0,accept,unanimous_agreement
669647781,8657,"i also don't want to include more mechanisms to complicate this story. it seems to me we can do a litter factor for [code block] to resolve this issue. [code block] is thread-safe already so it does not need lock. it means [code block] can be decoupled from [code block]. with this change, [code block] is involved by [code block] and it is called after releasing lock. for example: [code block] also, [code block] can be renamed to [code block]. the method name is consistent to its actual behavior. the benefit from this change are shown below. 1. simplify behavior of [code block]. the subclasses don't need to call [code block] always 1. lower possibility of conflicting lock",0,0,0,0.8692579865455627,0.976630687713623,0.9662795662879944,0.0,accept,unanimous_agreement
670113688,8657,": if we could solve the issue by simplifying delayedoperation, it would be ideal. i am not sure how your proposal avoids the above potential deadlock. could you provide a bit more detail? thanks.",1,1,1,0.903903305530548,0.8959634900093079,0.8692460060119629,1.0,accept,unanimous_agreement
670293057,8657,"the new deadlock you mentioned is caused by pr. this pr introduces extra lock ([code block]) to [code block] (by contrast, previous [code block] has a group lock only). the thread has to get two locks (group lock and reentrantlock) to complete [code block] and so deadlock happens when two locks are held by different thread. hence, my approach is to remove the extra lock introduced by this pr and so deadlock will be gone. the changes to [code block] in this pr is to resolve deadlock happening on [code block]. [code block] has to get other group lock to resolve delayed request but it is executed within a group lock. in order to resolve deadlock, this pr tries to move [code block] out of group lock. however, after thinking about this a bit more, why [code block] is executed within a lock? it is thread-safe already and it is not always executed within a lock (if it is executed on timeout). hence, the implementation of my approach is to refactor [code block] to move [code block] out of unnecessary lock. the refactor includes following changes. 1. [code block] should be executed by [code block] other than subclasss 1. [code block] is executed after releasing lock [code block] 3. [code block] is renamed to [code block]",0,0,0,0.969793736934662,0.98874831199646,0.9863210320472716,0.0,accept,unanimous_agreement
671520491,8657,": i agree mostly with your assessment. for most delayed operations, the checking for the completeness of the operation and the calling of oncomplete() don't have to be protected under the same lock. the only one that i am not quite sure is delayedjoin. currently, delayedjoin.trycompletejoin() checks if all members have joined and delayedjoin.oncomplete() modifies the state of the group. both operations are done under the same group lock. if we relax the lock, it seems that the condition ""all members have joined"" may no longer be true when we get to delayedjoin.oncomplete() even though that condition was true during the delayedjoin.trycompletejoin() check. it's not clear what we should do in that case.",0,0,0,0.9159423112869264,0.9816399216651917,0.7727771997451782,0.0,accept,unanimous_agreement
671749755,8657,"your feedback always makes sense. :thumbs_up: it seems to me the approach has to address following issue. 1. avoid conflicting (multiples) locks 1. small change (don't introduce complicated mechanism) 1. keep behavior compatibility ([code block] and [code block] should be included in same lock) i'd like to add an new method (default implementation is empty body) to [code block]. the new method is almost same to [code block] except for that it is executed without locking. currently, only [code block] has to use it. [code block]",1,1,1,0.6764707565307617,0.9388850927352904,0.9955980181694032,1.0,accept,unanimous_agreement
679256967,8657,": sorry for the late response. i just realized there seems to be another issue in addition to the above one that i mentioned. the second issue is that we hold a group lock while calling `joinpurgatory.trycompleteelsewatch`. in this call, it's possible that delayedjoin.oncomplete() will be called. in that case, since the caller holds the group lock, we won't be completing partitionstocomplete in completedelayedrequests().",-1,-1,-1,0.9896873831748962,0.9839306473731996,0.9828102588653564,-1.0,accept,unanimous_agreement
679855792,8657,"i go through group lock again and it is almost used anywhere :( i'm worry about deadlock caused by current approach so i'd like to address refactor and your approach (separate thread). the following changes are included. 1. [code block] does not complete delayed operations. instead, it adds those delayed operations to a queue. the callers ought to call the new method [code block] to complete those delayed operations in proper place (to avoid conflicting locking). 1. apple above new method to all callers who need to complete delay operations. 1. [code block] is called by [code block] only but it always held a group lock. to resolve it, we complete delayed requests in a separate thread. 1. keep using [code block]. the known conflicting locks should be resolved by above changes. using [code block] can protect us from deadlock which we have not noticed :) wdyt?",-1,-1,-1,0.9841023087501526,0.987419068813324,0.9952425956726074,-1.0,accept,unanimous_agreement
681109748,8657,": thanks for the reply. i like your overall idea and i think it can be used to solve the problem completely in a simpler way. 1. instead of at `partition`, we collect all pending delayed check operations in a queue in replicamanager. all callers to replicamanager.appendrecords() are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without holding any conflicting locks. 2. most callers to replicamanager.appendrecords() are from kafkaapis. we can just add the logic to check the replicamanager queue at the end of kafkaapis.handle(), at which point, no conflicting locks will be held. 3. another potentially caller is the expiration thread in a purgatory. systemtimer always runs the expiration logic in a separate thread and delayedoperation.onexpiration() is always called without holding any conflicting lock. so, for those delayed operations using replicamanager.appendrecords(), we can pass down a flag to delayedoperation so that at the end of onexpiration, we check the replicamanager queue if the flag is set. 4. we keep `delayedjoin` as it is, still passing in the group lock to delayedoperation to avoid deadlocks due to two levels of locking. 5. we can still get rid of the `trylock` logic in delayedoperation for simplification since there is no opportunity for deadlock. what do you think?",1,1,1,0.9839816689491272,0.9728671312332152,0.989725649356842,1.0,accept,unanimous_agreement
681318790,8657,"just double check. we use a separate thread to handle [code block] and [code block] in [code block], right? [code block] is called by [code block] only and it is possible to hold a group lock already.",0,0,0,0.9881824851036072,0.9942954182624816,0.9930041432380676,0.0,accept,unanimous_agreement
681361030,8657,"the following is my understand. the current pr introduces a new deadlock through the following path. path 1 hold group lock -> joinpurgatory.trycompleteelsewatch(delayedjoin) -> watchforoperation (now delayedjoin visible through other threads) -> operation.maybetrycomplete() -> hold delayedjoin.lock path 2 delayedjoin.maybetrycomplete -> hold hold delayedjoin.lock -> trycomplete() -> hold group lock the existing code doesn't have this deadlock since (1) delayedjoin.lock is the same as the group lock held in the caller and (2) a delayed join operation is registered under the group key (so each time we check completeness for a group key, only one delayed join operation will be affected). by switching back to this code, we avoid the new deadlock. the existing code has a different deadlock issue that groupmanager.storegroup() in groupcoordinator.oncompletejoin may need to complete to other delayed operations and potentially hold a different group lock while already holding a group lock. this issue will be resolved if we complete those delayed operations due to groupmanager.storegroup() elsewhere without holding any locks.",0,0,0,0.9728323817253112,0.9866459369659424,0.9932700991630554,0.0,accept,unanimous_agreement
681375395,8657,so [code block] does not complete any delayed requests in this path anymore and we expect that someone who don't hold lock should complete them?,0,0,0,0.9809442162513732,0.9943637847900392,0.9919548630714417,0.0,accept,unanimous_agreement
681708424,8657,"i have submitted a draft patch. as this new approach is totally different from previous code, i delete previous commits in order to clean git history.",0,0,0,0.9694002270698548,0.9612667560577391,0.9798231720924376,0.0,accept,unanimous_agreement
682879578,8657,thanks for all suggestions! is jenkins on vacation? could you trigger a system test?,1,1,1,0.9796121716499328,0.9840594530105592,0.986700475215912,1.0,accept,unanimous_agreement
682991870,8657,see kafka-10444 with regards to jenkins.,0,0,0,0.9833869338035583,0.9942830204963684,0.9950594902038574,0.0,accept,unanimous_agreement
683052000,8657,: do you want to take another look at the latest solution from chia-ping? it (1) solves the known issues completely; (2) doesn't require new threads; (3) adds minimal changes to existing code; (4) simplifies existing code by removing the trylock logic.,0,0,0,0.9849042892456056,0.9930751323699952,0.978694498538971,0.0,accept,unanimous_agreement
683075891,8657,"this looks promising. one question, do we want every request to drain this replicamanager queue or only the callers of `appendrecords`? i think this answer affects the design a bit. if it's meant to be called by every request, then maybe we should have the delayed actions in a separate class instead of replicamanager. other classes could, in theory, add their own delayed actions to this queue too. if it's meant to be called after calling `appendrecords`, then it may be cleaner to add the call within the method that calls `appendrecords` (with maybe a helper method in `kafkaapis` to make it less error prone).",1,1,0,0.826187252998352,0.8625439405441284,0.5108675956726074,1.0,accept,majority_agreement
683284825,8657,i preferred this idea as it prevent us from missing any action.,0,0,1,0.8549340963363647,0.9566171169281006,0.5676546692848206,0.0,accept,majority_agreement
683400677,8657,are there any suggested official benchmark tools?,0,0,0,0.985300362110138,0.9916552305221558,0.9931226372718812,0.0,accept,unanimous_agreement
683434648,8657,the result of [code block] is attached below. it seems the patch gets better throughput :) i will run more performance tests tomorrow. **before** [code block] **after** [code block],1,1,1,0.9893571734428406,0.9918780326843262,0.995964527130127,1.0,accept,unanimous_agreement
683602732,8657,"the result of [code block] is attached (see description) the main regression ({""records_per_sec"": 3653635.3672, ""mb_per_sec"": 348.4378} -> {""records_per_sec"": 2992220.2274, ""mb_per_sec"": 285.3604}) happens in case [code block]. i re-run the case 5 times and it seems the throughput of that case is not stable. **before** 1. {""records_per_sec"": 3653635.3672, ""mb_per_sec"": 348.4378} 1. {""records_per_sec"": 3812428.517, ""mb_per_sec"": 363.5815} 1. {""records_per_sec"": 3012048.1928, ""mb_per_sec"": 287.2513} 1. {""records_per_sec"": 3182686.1871, ""mb_per_sec"": 303.5246} 1. {""records_per_sec"": 2997601.9185, ""mb_per_sec"": 285.8736} **after** 1. {""records_per_sec"": 2992220.2274, ""mb_per_sec"": 285.3604} 1. {""records_per_sec"": 3698224.8521, ""mb_per_sec"": 352.6902} 1. {""records_per_sec"": 2977076.5109, ""mb_per_sec"": 283.9161} 1. {""records_per_sec"": 3676470.5882, ""mb_per_sec"": 350.6156} 1. {""records_per_sec"": 3681885.1252, ""mb_per_sec"": 351.1319}",0,0,0,0.9803891777992249,0.993206262588501,0.9891403913497924,0.0,accept,unanimous_agreement
683859366,8657,": thanks for the performance results. it seems that the average across multiple runs doesn't change much? also, 1 failure in the latest system test run. [a link]",1,1,1,0.958642601966858,0.9455966353416444,0.977794110774994,1.0,accept,unanimous_agreement
683863356,8657,yep. i didn't observe obvious regression caused by this patch. [code block] was flaky (see [a link],-1,-1,0,0.670883297920227,0.5415236949920654,0.9404476881027222,-1.0,accept,majority_agreement
685831526,8657,it seems there are some compilation errors in jenkins? [a link] ` 00:31:51 [error] /home/jenkins/jenkins-agent/workspace/kafka_kafka-pr_pr-8657/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/streamtotablejoinscalaintegrationtestimplicitserdes.scala:136: value stringserde is not a member of object org.apache.kafka.streams.scala.serdes `,0,0,0,0.9860047698020936,0.994207501411438,0.9935306906700134,0.0,accept,unanimous_agreement
685833800,8657,this error is caused by #8955 and #9245 is going to fix it,0,0,0,0.964788258075714,0.8902119398117065,0.9822537302970886,0.0,accept,unanimous_agreement
685837334,8657,[code block] pass on my local. retest this please,0,0,0,0.9876112341880798,0.9858471751213074,0.9951626062393188,0.0,accept,unanimous_agreement
688481474,8657,"thanks for all reviews again :thumbs_up: my bad. i forgot this request :( expect for [code block], the other unused methods (in production scope) are removed by this pr. it seems we can remove [code block] from[code block]. that is similar to this pr and [code block] should not complete delayed request anymore. i can take over this in separate pr :)",-1,-1,1,0.776296854019165,0.715516209602356,0.99522203207016,-1.0,accept,majority_agreement
689241869,8657,: i think this pr is ready to be merged. any further comments from you?,0,0,0,0.8666406273841858,0.8538037538528442,0.945490062236786,0.0,accept,unanimous_agreement
689256048,8657,"[code block] [code block] on my local, they are flaky on trunk branch.",-1,0,-1,0.7708389163017273,0.6807913184165955,0.9371920228004456,-1.0,accept,majority_agreement
689838971,8657,: thanks a lot for staying on this tricky issue and finding a simpler solution!,1,1,1,0.9866334199905396,0.9742582440376282,0.9881393909454346,1.0,accept,unanimous_agreement
689921735,8657,thanks for all suggestions. i benefit a lot from it.,1,1,1,0.954576015472412,0.9681453704833984,0.9895251989364624,1.0,accept,unanimous_agreement
267731925,2264,"previously, there were many places in the code that were creating abstractrequest objects directly and passing them to the networkclient. unfortunately, abstractrequest objects are version-specific. for example, a listoffsetrequest object of version 0 will have different fields inside its struct than a listoffsetrequest object of version 1. as soon as the message constructor is invoked, the contained structs are formatted in the version-specific format. this change adds the abstractrequest#builder types. client code in the fetcher and producer, and other places, can create these objects and pass them to the networkclient. the networkclient can then decide what version of each request to use based on the apiversionsrequest data. when the version that needs to be used is too old to support a necessary feature, an error is returned to the upper layer, which can then decide what to do.",0,0,0,0.9745432138442992,0.9932821989059448,0.977389633655548,0.0,accept,unanimous_agreement
270150362,2264,", can you please rebase this branch? thanks!",1,1,1,0.9632184505462646,0.987430453300476,0.9786832332611084,1.0,accept,unanimous_agreement
270250551,2264,rebased on trunk and fixed some unit tests,0,0,0,0.9877474308013916,0.9931817650794984,0.9860310554504396,0.0,accept,unanimous_agreement
270380394,2264,"sorry, can you please rebase again?",-1,-1,-1,0.9864730834960938,0.9907634854316713,0.9932543635368348,-1.0,accept,unanimous_agreement
270446437,2264,rebased,0,0,0,0.9831162095069884,0.9435001611709596,0.9767982959747314,0.0,accept,unanimous_agreement
270447481,2264,"also removed a few places where this change duplicated kafka-4548, and reverted a change to the test log4j.properties file",0,0,0,0.9884068369865416,0.9944773316383362,0.994160294532776,0.0,accept,unanimous_agreement
270510560,2264,"fix authorizerintegrationtest, fetchrequesttest, saslapiversionsrequesttest, plaintextconsumertest. fixed a bug where fetcher#beginningoffsets and fetcher#endoffsets incorrectly required a broker with the new listoffsetsrequest rpc. will add this to compatibilitytest.",0,0,0,0.9886460304260254,0.9939783811569214,0.9584379196166992,0.0,accept,unanimous_agreement
271399925,2264,"good point about kip-74 compatibility handling. it sounds like we will have to reproduce the pre-kip-74 behavior of ""getting stuck"" when messages are too big rather than the new behavior of returning at least one message. i'll add back in that code shortly and work on a compatibility test for it. the other comments should be addressed now in the latest patch.",1,1,1,0.8485838174819946,0.797542154788971,0.8813934922218323,1.0,accept,unanimous_agreement
271674875,2264,i posted an update with a lot of fixes. i still need to address: * brokers should not send apiversionsrequest * implement pre-kip-74 message-too-big behavior * more centralized handling of versionmismatch exception * some other stuff,0,0,0,0.8905021548271179,0.647578239440918,0.52281254529953,0.0,accept,unanimous_agreement
271680561,2264,"i think your builder suggestion seems worth investigating.. if we continue with the model of delayed version determination, i was considering whether we could decouple the `struct` representation from the request objects. haven't really thought this through, but i was thinking something like this: [code block] that way the request objects remain immutable. i think you also had a suggestion which allowed us to construct the right version up-front.",0,0,0,0.7838124632835388,0.9620166420936584,0.5613734126091003,0.0,accept,unanimous_agreement
271720119,2264,"* brokers no longer send apiversionsrequest, to allow inter-broker communication with versions before 0.10 * addressed some style comments",0,0,0,0.9829401969909668,0.9911509156227112,0.9802339673042296,0.0,accept,unanimous_agreement
271724878,2264,", : unfortunately github doesn't let me post comments on the thread you have going about refactoring abstractrequest. so i'll comment here. there are really two questions here: 1. should the abstractrequest objects represent serialized data or unserialized data? 2. can we know the version of the request that we need before we send it to networkclient for #1, currently, abstractrequest objects represent serialized data, so they inherently have a version and a binary format. a lot of the methods in abstractrequest focus on serialization.. for example, abstractrequest#getrequest deserializes, abstractrequestresponse#sizeof gets the size of the serialized data, etc. you have to change the whole class hierarchy to modify this assumption. probably if you do that you would want to use a different name, such as message, to represent the fact that the new class hierarchy you are creating has nothing to do with serialized data. this might be worth doing. it would be a very big change. for #2, there are methods in networkclient such as leastloadednode that send the request to a node of the networkclient's own choosing. this doesn't really work unless the networkclient is in charge of picking a version. there also seems to be a bit of a layering violation here if the users of nc need to start concerning themselves with questions like has the peer they're talking to restarted or dropped its tcp session since we last learned about its version? #1 might make sense as part of a general refactor of the rpc class hierarchies. getting rid of the request class altogether might be a good idea since it invites confusion with clientrequest. i'm not sure i see a big win from #2 or a way to do it (but perhaps i am missing something).",-1,-1,-1,0.9599094986915588,0.8078056573867798,0.9308547377586364,-1.0,accept,unanimous_agreement
271765728,2264,i submitted a pr with some clean-ups: [a link],0,0,0,0.9773085713386536,0.9749882817268372,0.9938919544219972,0.0,accept,unanimous_agreement
271775275,2264,: thanks for the cleanup pull request. looks good... i merged it into this branch.,1,1,1,0.9831493496894836,0.9909935593605042,0.990856409072876,1.0,accept,unanimous_agreement
271867738,2264,"there are a number of failures in the system tests run. i started another one to double-check that it's not an environmental issue. in the meantime, i submitted another pr with clean-ups: [a link] i think we can merge after that (assuming the system tests pass) and do additional work in follow-up prs.",0,0,0,0.9688020348548888,0.984205424785614,0.9897095561027528,0.0,accept,unanimous_agreement
1728555687,14182,created new pr [a link],0,0,0,0.9856529235839844,0.985213339328766,0.9958928823471068,0.0,accept,unanimous_agreement
173692160,764,would you add the remaining kip-31 and kip-32 work to this patch (client side work and timestamp in produce response)? or that would be a different patch?,0,0,0,0.9889070987701416,0.9932754039764404,0.9946382641792296,0.0,accept,unanimous_agreement
173710219,764,this patch contains all the features in kip-31 and kip-32. the rest of the work is probably adding integration test. i have already added some unit tests but we can also add more if needed.,0,0,0,0.9833549857139589,0.9892317652702332,0.9820079207420348,0.0,accept,unanimous_agreement
173749672,764,"my question about remaining kip-31 and kip-32 work was based on outdated info -- i did not refresh my window and did not see that you added client-side implementation + returning timestamp in produce response. i see now that you also updated the pr description, thanks!",1,1,1,0.9576513171195984,0.9826722145080566,0.9759012460708618,1.0,accept,unanimous_agreement
174748282,764,"here are some high-level thoughts about the protocol: - basically we want the consumer to return the timestamp of the type specified by that topic even for compressed message set, but without the additional information the consumer would not know if logappendtime or logcreationtime is used. and as mentioned by just setting the wrapper message as the max value of all inner message timestamps and letting consumer check if wrapper timestamp is the max value does not perfectly work since 1) it requires the consumers to always decompress the whole message before returning any to the user, hence restricting buffer memory management we wanted to add in the future, 2) there is a corner case that if logappendtime is used and broker overrides the wrapper timestamp, it happens to be the same as the max of inner timestamps. i think we can add this information into the attribute field of the message, which currently only used 2 bits for four different compression types; instead we can make it a mask manner where the first 3 (or if we want to be safer, use 4) bits are preserved for indicating compression codec, leaves us a total of 8 (or 16) supported compression types, and use the forth (fifth) bit for indicating if the wrapper timestamp (for logappendtime, hence it is overridden) or the inner timestamp (for logcreationtime) should be used to set the consumer record's timestamp. and with this neither producer nor consumer needs to learn about this per-topic config from metadata responses, which makes the client change simpler, and other languages' adoption easier. - i am curious if the ducktape integration tests will be added in another pr?",0,0,0,0.8803070783615112,0.9864155650138856,0.975982666015625,0.0,accept,unanimous_agreement
174759962,764,"using attribute field is a good approach. it also lets consumers know the timestamp type. to make sure i understand your suggestion correctly: 1. the producer simply send message assuming the broker is using createtime. i.e. both attributes and timestamp will be using createtime. 2. if log append time is used, the broker only overrides the outer message's attribute field and timestmap field to use logappendtime 3. when consumer sees the message, it checks both magic as well as attribute field to see which timestamp is used (if magic > 0), and then decide whether it will override the inner message's timestamp or not. another thing is that we still need to decompress the entire compressed message, because of the reason i mentioned in one of the comments. given the stream compression used by producer, we will not have a actual ""relative offset compared with last message"" until we close the batch. instead, we only have the ""relative offset compared with the first message"" when we write a message into a batch. because the outer message only has the absolute offset of the last message, in order to have the absolute offset of an inner message, we have to decompress the entire compressed message to find out the ""relative offset compared with the last message"", then compute the absolute offset. i feel this is fine for new consumer because we are delivering messages in batch to use anyways.",1,0,1,0.6585328578948975,0.9518064260482788,0.6228975057601929,1.0,accept,majority_agreement
174762898,764,"btw, currently the compressioncodecmask is set to 0x7, so it is 3 bits. changing that to 4 bits is backward compatible so that should be fine.",0,0,0,0.9857670068740844,0.9912880063056946,0.9871783256530762,0.0,accept,unanimous_agreement
174773567,764,"yes your understanding is correct. i was initially thinking about possibilities of not decompressing the whole message when we add the memory management feature in the future so that we can choose to buffer less decompressed messages. but it seems not possible now, which maybe still fine for us so let's forget about it.",0,0,0,0.9362871050834656,0.9592393040657043,0.9373942613601683,0.0,accept,unanimous_agreement
175792263,764,i updated the patch with guozhang's proposal. i will add integration test in a separate pr. the intended tests are: 1. change timestamp type on the fly. 2. test message format version upgrade i actually want to do end to end test using different version of producers and consumers. but not sure if it is possible the current integration test because that requires different clients jars.,0,0,0,0.9634760022163392,0.9879327416419984,0.976334810256958,0.0,accept,unanimous_agreement
175972676,764,you can do end-to-end compatibility testing with system tests. take a look at compatibility_test.py. it currently tests 0.9.x java producer against 0.8.x brokers and 0.8.x consumer against an 0.9.x brokers. they both succeed on expected failure. you can add couple of more system tests to that to test newer brokers with older producers and/or consumers. note that you would need to update vagrant/base.sh to get kafka release 0.9.0.0.,0,0,0,0.972971498966217,0.9919087290763856,0.9699802994728088,0.0,accept,unanimous_agreement
175982253,764,"maybe i missed it, but i don't see where producer assigns timestamps if the user does not specify the timestamp in producerrecord. the code was there before, but maybe it got accidentally removed with recent changes?",0,0,0,0.9072781801223756,0.9701462388038636,0.960070788860321,0.0,accept,unanimous_agreement
175992639,764,"thanks for the direction on compatibility test. extracting timestamp type out makes sense, given we already did that for compressiontype. i will change server side as well.",1,1,1,0.905959129333496,0.922261655330658,0.9443004727363586,1.0,accept,unanimous_agreement
176356903,764,i reviewed the kip-32 part of the patch (did not go in detail about kip-31 related changes). using timestamp type in attributes made producer/consumer code much cleaner! i made minor comments. otherwise looks good to me.,1,1,1,0.9579424262046814,0.9903905391693116,0.9887391328811646,1.0,accept,unanimous_agreement
176481781,764,thanks for the review. will you help take a look at the patch? thanks.,1,1,1,0.8987830281257629,0.8425929546356201,0.9628108739852904,1.0,accept,unanimous_agreement
176513163,764,the test failure is intermittent and is not related to this change.,0,0,0,0.9591606259346008,0.9476086497306824,0.9852110147476196,0.0,accept,unanimous_agreement
176885686,764,. thanks for the review. i addressed your previous comments. would you take another look? thanks.,1,1,1,0.9434329867362976,0.9894898533821106,0.9908349514007568,1.0,accept,unanimous_agreement
178268615,764,"correct me if i am wrong, but it looks like we don't have integration test for the case when the broker rejects messages because the timestamp is outside of configured time difference. i think it would be easier to track the additional tests we need to do (beyond this pr) if we created separate kafka jiras for them. could you please create jiras?",0,0,0,0.9765241146087646,0.9263582229614258,0.9200668931007384,0.0,accept,unanimous_agreement
178321814,764,good catch. i am going to add the rejected message unit integration test. this actually exposed a separate bug on the server side that causing it not returning the correct error code. i create kafka-3189 for that bug. will update unit integration test after that bug is fixed.,1,1,1,0.9710066318511964,0.9311030507087708,0.9835026860237122,1.0,accept,unanimous_agreement
178911759,764,thanks a lot for the review .i updated the patch to address your comments. some explanation on the changes: 1. the broker now return invalidmessageexception when producer does not set the timestamp type to createtime. previously it just overrides it. 2. it seems difficult to completely avoid unnecessary message magic value check when send fetch response back. i chose the way which i think has the least impact. but it might still break if people roll back message.format.version. 3. i haven't add the configuration sanity check for message.format.version yet. i will do that after we decide on whether it should be a topic level configuration and whether we want to use magic value or apiversions.,1,1,1,0.9754888415336608,0.96125727891922,0.985576331615448,1.0,accept,unanimous_agreement
178974585,764,: thanks for the patch. do you plan add some integration tests to cover backward compatibility? we can do that in a separate jira.,1,1,1,0.9503635168075562,0.6580177545547485,0.9704362154006958,1.0,accept,unanimous_agreement
179291438,764,"also, could you update the doc on upgrade?",0,0,0,0.987767457962036,0.9888333082199096,0.9954195022583008,0.0,accept,unanimous_agreement
180561754,764,thanks for the review. your comments are addressed. i have created separate tickets for integration test. i am working on the upgrade doc now and will update it later.,1,1,1,0.9207152128219604,0.966253697872162,0.9680095314979552,1.0,accept,unanimous_agreement
181683922,764,i will start a poll on the mailing list as jun suggested. i updated the patch with upgrade doc. would you take a look? thanks. the test failure is irrelevant. i will open a ticket for that as it fails from time to time.,0,1,1,0.858666181564331,0.7471269965171814,0.9148574471473694,1.0,accept,majority_agreement
183538336,764,"hi, jiangjie, thanks for addressing the review comments. do you expect to submit a new patch soon? thanks,",1,1,1,0.9507823586463928,0.8376008868217468,0.9807784557342528,1.0,accept,unanimous_agreement
183561045,764,hi thanks a lot for the review. i just submitted a new patch. thanks!,1,1,1,0.9833162426948548,0.9944958090782166,0.9949592351913452,1.0,accept,unanimous_agreement
185176868,764,", thanks for the pr and for the quick response in addressing review comments. i filed a pr with your branch as the target with some additional minor improvements: [a link] please integrate into your pr (merge, cherry-pick or any other way you prefer) if you agree with the proposed changes. i expect to finish reviewing tomorrow. if jun merges in the meantime, we can use follow-up prs (if needed).",1,1,1,0.9581316113471984,0.8979222774505615,0.9709619283676147,1.0,accept,unanimous_agreement
185363481,764,thanks for the review. i just merged your pr.,1,1,1,0.8743969798088074,0.9338158369064332,0.9349986910820008,1.0,accept,unanimous_agreement
185510642,764,"thanks for the patch. looks good overall. just left a few minor comments. also, in topiccommand, when listing the available config options, could we add a description that messageformat will be ignored if it's not consistent with the inter broker protocol setting?",1,1,1,0.9862127304077148,0.9941707253456116,0.9931025505065918,1.0,accept,unanimous_agreement
185992097,764,thanks for the patient review. i think i have addressed previous comments. could you take another look?,1,1,1,0.8871671557426453,0.9219608306884766,0.961575448513031,1.0,accept,unanimous_agreement
186009747,764,": thanks for the latest patch. it looks good to me. once you address the last few minor comments, i can merge this in.",1,1,1,0.9810266494750975,0.9900804162025452,0.988359808921814,1.0,accept,unanimous_agreement
186274027,764,: thanks a lot for working on the patch! lgtm,1,1,1,0.9904416799545288,0.9916644096374512,0.9955869913101196,1.0,accept,unanimous_agreement
186277555,764,nice work . and the reviewers too. :),1,1,1,0.9911764860153198,0.995715081691742,0.9968433380126952,1.0,accept,unanimous_agreement
186463160,764,thank and so much for the great help on review!,1,1,1,0.9901143312454224,0.995402216911316,0.9943318367004396,1.0,accept,unanimous_agreement
189037289,764,"some of the streams tests were incorrect when adding the timestamp. for example in processorstatemanagertest: `new consumerrecord<>(persistentstoretopicname, 2, 0l, offset, timestamptype.create_time, key, 0)` should be `new consumerrecord<>(persistentstoretopicname, 2, offset, 0l, timestamptype.create_time, key, 0)` actually i'm thinking if it harms to keep the old constructor for consumerrecord and make default values of 0l and timestamptype.create_time, and revert all the changes in stream tests? that way we can be free of incorporating the metadata timestamp until it is supported.",0,0,0,0.9716639518737792,0.9750686287879944,0.9802842736244202,0.0,accept,unanimous_agreement
189040692,764,"well-spotted. i actually wanted to suggest moving `timestamptype` before the timestamp to make this kind of error harder, but i only noticed this potential problem late in the process and then wasn't sure if it was worth the effort. having real bugs instead of theoretical ones adds motivation. i would prefer if we don't add the old `consumerrecord` constructor personally as `consumerrecord` is used outside of streams too. maybe we could add a utility method in streams in the meantime?",0,0,0,0.8518962860107422,0.8997411727905273,0.9337953925132751,0.0,accept,unanimous_agreement
189047528,764,"makes sense. the only place streams use `consumerrecord` directly is in `timestampextractor`, what kind of utility method do you have in mind?",0,0,0,0.9871886372566224,0.992133617401123,0.9911581873893738,0.0,accept,unanimous_agreement
189048926,764,i just mean a method like `newconsumerrecord` that behaves exactly like the old constructor. then you could revert the changes in the streams tests and then do a search and replace in the streams folder.,0,0,0,0.988575279712677,0.9922956228256226,0.993520200252533,0.0,accept,unanimous_agreement
189049558,764,sounds good.,1,1,1,0.920201539993286,0.9417163729667664,0.857205867767334,1.0,accept,unanimous_agreement
1625488949,13870,there are issues with the build as well. could you look into this?,0,0,0,0.9809727668762208,0.95804625749588,0.9693862199783324,0.0,accept,unanimous_agreement
1625762579,13870,"thanks for the review, i have addressed your comments.",1,1,1,0.7525657415390015,0.8175089955329895,0.8530596494674683,1.0,accept,unanimous_agreement
1631769383,13870,thanks for the review. i have addressed your comments.,1,1,1,0.7795889377593994,0.8453478217124939,0.903476357460022,1.0,accept,unanimous_agreement
1632143234,13870,"now that [a link] is merged, could you update your pr?",0,0,0,0.9860150814056396,0.9857520461082458,0.9961806535720824,0.0,accept,unanimous_agreement
1633127990,13870,i have updated with latest and unified the mockcoordinatortimer.,0,0,0,0.9855959415435792,0.9846618175506592,0.995233952999115,0.0,accept,unanimous_agreement
1641304041,13870,the test failures are unrelated,0,0,0,0.948349118232727,0.7692809700965881,0.98296457529068,0.0,accept,unanimous_agreement
767804738,9944,"added fetch session components. will add some versioning tests and final cleanups, then open for review",0,0,0,0.982992708683014,0.9901962280273438,0.9920756816864014,0.0,accept,unanimous_agreement
767987726,9944,i'm aware that the latest changes to ensure the correct fetch version is sent seem to be causing more test timeouts. will need to investigate further and hopefully decrease the time needed for fetch requests. i may need to check some other flaky tests related to my changes.,0,0,0,0.8337321877479553,0.8643290400505066,0.5053834915161133,0.0,accept,unanimous_agreement
776097476,9944,"just to clarify this, the top is the fetch branch, so i think it is better than trunk i do want to take another look at the fetcherthread and fetchsession benchmarks which are slightly worse.",0,0,0,0.9151913523674012,0.9577434062957764,0.9709986448287964,0.0,accept,unanimous_agreement
790133100,9944,waiting on [a link] before proceeding since this pr touches a lot of the same files.,0,0,0,0.960018277168274,0.9749205112457277,0.989951193332672,0.0,accept,unanimous_agreement
794755556,9944,"currently working on merge conflicts. should have a first pass out in the next day or so. there are a few changes that don't work with the previous refactor. , can you take a look when i have the commit ready?",0,0,0,0.9787020683288574,0.9755440950393676,0.9150750041007996,0.0,accept,unanimous_agreement
794897185,9944,sure :),1,1,0,0.96258282661438,0.984525740146637,0.9888246655464172,1.0,accept,majority_agreement
796317046,9944,here's the commit. as mentioned in the commit message: i also acknowledge that this code creates a ton of data structures in fetchrequest and fetchresponse. i hope to clean those up soon.,0,0,0,0.9504802823066713,0.8872289061546326,0.7284329533576965,0.0,accept,unanimous_agreement
796886007,9944,there was a lot of back and forth about whether we should simply include the topic id and the name in the protocol (to uniquely identify the topic--which is something we may want to do to make sure we are consuming the right topic) versus use the topic id to replace the topic name. the main arguments were: 1. topic ids are in most cases shorter than topic names so we can shorten an already somewhat large protocol 2. we will eventually want to move over to using topic ids for everything so we might as well make this change now. i do agree that the topicpartitions used make this task much harder and i tried to find a clean way to accomplish this. the main idea is that we keep track of topics that did not have topic ids server-side (receiving-side) and send an error response back. the idea is that the sender will always have the topic id -> name conversion for topic ids it sends.,0,0,0,0.8407952785491943,0.9861389994621276,0.9336165189743042,0.0,accept,unanimous_agreement
796902838,9944,do you mean the client (consumer) must have a mapping (name -> id) in local cache before fetching data?,0,0,0,0.9865022897720336,0.9941678047180176,0.9930074214935304,0.0,accept,unanimous_agreement
796906853,9944,"yes. the consumer already gets periodic metadata updates, so i use that to get the mapping.",0,0,0,0.9878612756729126,0.9852594137191772,0.9941562414169312,0.0,accept,unanimous_agreement
798608479,9944,i've run jmh benchmarks before but not since the merge. i plan to do so again once i figure out how to improve the efficiency regarding unconvertedfetchresponse and fix up some the one test that is failing. i also plan to rerun system tests to make sure those are passing as well.,0,0,0,0.9580078721046448,0.98071026802063,0.960055410861969,0.0,accept,unanimous_agreement
804157913,9944,"new benchmark results: these are pretty similar to the previous results, but thought i should include to compare to updated code (both this branch and trunk). note: i have modified `fetchresponsebenchmark.testconstructfetchresponse` on trunk to match the benchmark in this branch (so it doesn't count the time to build the map) and added `fetchresponsebenchmark.testpartitionmapfromdata` to the trunk code to capture that benchmark. [code block] [code block]",0,0,0,0.945732355117798,0.9894116520881652,0.9013230204582214,0.0,accept,unanimous_agreement
814408153,9944,~currently blocked on [a link] (need to add topic ids to the metadata topic for fetching)~ no longer blocked,0,0,0,0.975378692150116,0.9886584877967834,0.9734651446342468,0.0,accept,unanimous_agreement
828663698,9944,"thanks for taking another look i agree. this has been in the back of my mind for a while, specifically whether all the changes in fetchsession are necessary for such cases. so thanks for bringing this up. i think the only thing i was really concerned about was during a roll to upgrade/the new topic case you mentioned. but even using the current approach, i wasn't sure if the fetch session stuff was really helping. i think my biggest confusion comes from when the client will refresh metadata. will returning a top level error guarantee a refresh? (vs given an unknown topic id response?) i think that the top level approach will likely be better.",1,1,1,0.9171521067619324,0.9217699766159058,0.9826093912124634,1.0,accept,unanimous_agreement
828795518,9944,"i think the upgrade case is similar---it's rare and transient. so, we could choose to have a simpler and less optimized way for handling it. i am not sure if we trigger metadata refresh for top level error right now. if not, we could probably just add the logic to refresh metadata for all topics on topicid error at the top level.",0,0,0,0.9379271268844604,0.9755395650863647,0.9805263876914978,0.0,accept,unanimous_agreement
830445376,9944,"ok, updated the code. one thing i assumed here is that we don't switch from not using topic ids in the session (requests versions 12 or below) to using 13+. i ensure this in the fetcher + abstractfetcherthread code, but maybe we can't assume this for all clients. if we can update from not using ids in the session to using them, i'll update the code.",0,0,0,0.9850415587425232,0.9906976819038392,0.9559665322303772,0.0,accept,unanimous_agreement
1540289311,13443,-22 the build failed due to compilation errors.,0,0,0,0.8325884342193604,0.9419857263565063,0.7407544255256653,0.0,accept,unanimous_agreement
484241405,6592,usage: `new listserde<>(serdes.string())` to create a serde for `list ` values,0,0,0,0.9871674180030824,0.9939842820167542,0.9944778680801392,0.0,accept,unanimous_agreement
485460971,6592,"thanks a lot for the pr ! i like the idea and think it's a good addition. however, adding those classes is a public api change and not a `minor` pr. thus, it should be backed by a jira ticket and it also requires a kip: [a link] let me know if you have any question about the kip process (should actually be well document in the wiki), or if you need any other assistance. btw: you should also add unit tests :)",1,1,1,0.9937496185302734,0.9956874251365662,0.9970953464508056,1.0,accept,unanimous_agreement
489700008,6592,"great to see some progress on this pr, but we still need a kip... i would recommend to work on the kip first (or in parallel),",1,1,1,0.9642525911331176,0.9577953219413756,0.988657295703888,1.0,accept,unanimous_agreement
489709013,6592,"kip, jira, and discuss thread are started: [a link]",0,0,0,0.984929859638214,0.983244001865387,0.9842737913131714,0.0,accept,unanimous_agreement
492021215,6592,"hmmm, strange i added a test case, but it still says that test results were not found",-1,-1,-1,0.7239518761634827,0.8694313764572144,0.5445331335067749,-1.0,accept,unanimous_agreement
516673508,6592,any updates on this?,0,0,0,0.9801620244979858,0.99077308177948,0.9843987226486206,0.0,accept,unanimous_agreement
516952229,6592,"sorry, i thought to finish all the discussions first, that's why i did not push. new commit is in place! :grinning_face_with_smiling_eyes: upd: merged with apache:trunk",-1,-1,-1,0.9862479567527772,0.991480827331543,0.7565692663192749,-1.0,accept,unanimous_agreement
518247929,6592,"hey, i'm sorry i thought you wanted me to merge with `trunk` that's why so many files popped up. i think i know how to resolve this. i'll force push **before** the merge, and then keep pushing my commits only related to `introduce_list_serde` work. and at the end, once all reviews are completed, i'll do the final merge and we can close this pr. upd: ok it should be much cleaner now :)",1,1,1,0.9596523642539978,0.9688192009925842,0.9787847399711608,1.0,accept,unanimous_agreement
519362418,6592,"seems there is a merge conflict again. and yes, i usually rebase to `trunk` (not merge `trunk`) and then push force to update a pr.",0,0,0,0.9852370619773864,0.9897750616073608,0.9926851987838744,0.0,accept,unanimous_agreement
519700266,6592,"hmmm, i think i did it! i haven't worked with git on a project of this scale, so bear with me :) i think i performed a rebase with `apache/kafka:trunk`, resolved couple conflicts along the way, and now everything should be fine.",1,1,1,0.9904723167419434,0.9957154393196106,0.9938141107559204,1.0,accept,unanimous_agreement
520019651,6592,"build failed with spotbug issue. can you address this? to test locally before pushing, i recommend to run `./gradlew streams:clean streams:test` -- this will run checkstyle and spotbug, too.",0,0,0,0.981543779373169,0.9842185378074646,0.9749099612236024,0.0,accept,unanimous_agreement
520549718,6592,"i fixed all checkstyle warnings, except for: [code block] under `listserializer.java` and `listdeserializer.java` the build is failing bc of that. --- also, lots of tests are failing because for some reason they are expecting `default_list_key/value_serde_inner_class`, `default_list_key/value_serde_type_class` and other properties to have default values: [code block] i understand that i can set those default values during `.define(...` call. i guess i should set them to `null`?",0,0,0,0.7527721524238586,0.9710859060287476,0.8955865502357483,0.0,accept,unanimous_agreement
520550127,6592,"as per unchecked warnings, i suppressed them at the variable or method levels.",0,0,0,0.9845155477523804,0.9851357340812684,0.9932277798652648,0.0,accept,unanimous_agreement
526295380,6592,would you have time to take a look? :),1,1,1,0.977876842021942,0.9860259890556335,0.9394195675849916,1.0,accept,unanimous_agreement
530406657,6592,rebased the branch with `trunk`. still seeing unrelated `spotbugs` errors in failed jenkins builds.,0,0,0,0.8781911134719849,0.9845408201217652,0.9632943272590636,0.0,accept,unanimous_agreement
531210633,6592,"retest this, please",0,0,0,0.9849490523338318,0.958921492099762,0.9489298462867736,0.0,accept,unanimous_agreement
531377737,6592,"those were actually _related_ spotbug errors, my fault :grinning_face_with_sweat:",-1,-1,1,0.9876525402069092,0.8645954132080078,0.9678316116333008,-1.0,accept,majority_agreement
531892218,6592,i think it is ready for the next round of review :) thank you in advance!,1,1,1,0.9927876591682434,0.9962384700775146,0.9913188815116882,1.0,accept,unanimous_agreement
535305283,6592,bump :),1,1,0,0.8885239362716675,0.9939910769462584,0.9189615249633788,1.0,accept,majority_agreement
537524269,6592,ready for another round :),1,1,0,0.9749433994293212,0.9891013503074646,0.9094529151916504,1.0,accept,majority_agreement
539604139,6592,bump :1st_place_medal:,0,0,1,0.6585193276405334,0.9686258435249328,0.7508653402328491,0.0,accept,majority_agreement
541655992,6592,bump :input_numbers: also updated kip and jira ticket,0,0,0,0.9870449900627136,0.9742149114608764,0.993388056755066,0.0,accept,unanimous_agreement
545630718,6592,bump :1st_place_medal:,0,0,1,0.6585193276405334,0.9686258435249328,0.7508653402328491,0.0,accept,majority_agreement
547412636,6592,would you be so kind to take another look? :),1,1,1,0.9833611845970154,0.9937020540237428,0.9825631380081176,1.0,accept,unanimous_agreement
548114057,6592,rrrrready for another round :) thank you :1st_place_medal:,1,1,1,0.9909399747848512,0.9960086345672609,0.9965241551399232,1.0,accept,unanimous_agreement
549672088,6592,"thanks for looking into that, , i also tried it out just now, and i think you're right. the only thing i think we could do to be able to spit out a ""fully typed"" result is to introduce a ""dummy interface"" like jackson's `typereference`. this kind of interface can be used to capture the full generic argument list at run time and use it to construct the collection. short of that, i believe we'd be doomed to be producing `rawtypes` warnings in user code. which is probably not a good choice for the api. note, this is all in retrospect. it's not obvious at all that this would crop up until you try it. in light of that, i think we probably need to change up the interfaces a little. something like: `class listdeserializer extends deserializer >` with a constructor like `public listdeserializer(class listclass, deserializer innerdeserializer)`. when you call `deserialize`, you'd get a `list ` (e.g., `list `) out, but this is probably fine, since best practice is to declare the variable type as the interface, not the implementation anyway. and the advantage is that the calling code (aka ""user code"") wouldn't have any ""rawtypes"" or ""unsafe"" warnings at all. again, thanks a million for catching this!",1,1,1,0.7621539831161499,0.9140489101409912,0.9512585401535034,1.0,accept,unanimous_agreement
550966966,6592,"thanks for your input! i agree that that it would be better to preserve the ""inner type"" instead of the ""list type"", ie, `deserialize()` should return `list ` instead of raw `arraylist`. -- do you agree this this assessment? if yes, please update the pr accordingly.",1,1,1,0.9708186388015748,0.9621958136558532,0.9840412139892578,1.0,accept,unanimous_agreement
553067625,6592,just pushed a commit with changes according to your description. not entirely sure if i got everything right.,-1,0,0,0.5051457285881042,0.8007558584213257,0.7183414697647095,0.0,accept,majority_agreement
555213976,6592,updated kip and jira ticket as well,0,0,0,0.9802170991897584,0.992697536945343,0.9905374646186828,0.0,accept,unanimous_agreement
555233480,6592,thanks -- i am currently fully loaded with `2.4` release issues... will try to cycle back to your pr but might take some time.,1,1,1,0.8799092769622803,0.9159609079360962,0.8734429478645325,1.0,accept,unanimous_agreement
571205357,6592,happy new year! :) i rebased the branch according to `upstream/trunk`. i noticed that branches aren't getting built anymore. let me know if there is anything else i can do here!,1,1,1,0.9931164979934692,0.996016561985016,0.9971743822097778,1.0,accept,unanimous_agreement
585361037,6592,any progress on this pr?,0,0,0,0.9793014526367188,0.988725244998932,0.9906713962554932,0.0,accept,unanimous_agreement
585364165,6592,"sorry, have been busy at work and personal stuff. should start making progress soon!",-1,-1,-1,0.9881118535995485,0.9936531782150269,0.9940091371536256,-1.0,accept,unanimous_agreement
585969307,6592,introduced [code block] they test non-arg-constructors and use `configure` methods. i had to also update `configure` logic in both classes. take a look :) thank you!,1,1,1,0.9924864768981934,0.9956831932067872,0.9946417808532716,1.0,accept,unanimous_agreement
587819699,6592,"hmmm i was thinking about this magic flag, and there are some corner cases that i want to discuss. my initial idea was to create an enum: [code block] then it becomes counterintuitive: case 1: encoding primitives `arr={1, 2, null, 3}` user chooses `serializationstrategy.negative_size`, i guess we'd have to throw an exception saying that this serialization strategy is forbidden? case 2: encoding non-primitives `arr={'a', 'b', null, 'c'}` user chooses `serializationstrategy.null_index_list`, but then what do we store for size of `arr[2]`? zero? negative one? we are still wasting 4 bytes that way. i think our best solution is to simply use ""null index list"" for all primitives, and `-1` strategy for all non-primitives avoiding any magical, serialization strategy flags. what do you think?",0,0,0,0.9569748640060424,0.8424953818321228,0.9300262331962584,0.0,accept,unanimous_agreement
606241093,6592,"sorry for the late reply... adding an enum sounds like a good idea. i don't think we would need to throw, however, we would need to choose the non-optimized encoding and encode the length for each list item (even if it is the same length) over and over again. we would store zero bytes, because we can skip the entire entry. during deserialization we can just do a `list.add(null)` when we hit the corresponding index as encoded in the null-index-list header, ie, during deserialization we maintain a counter for the index of the element we deserialize. does this make sense? that would also be possible as an initial non-configurable default implementation, but i would still encode a magic header byte in case we want to change it later (just to be future prove). my suggestion to only implement one strategy was aiming to limit the scope of the pr. but if you want to implement both strategies, i have no objection either. however, if we actually do implement both strategies, than we could make it configurable from the very beginning on?",-1,-1,-1,0.9889014959335328,0.9817711114883424,0.9849586486816406,-1.0,accept,unanimous_agreement
606252503,6592,i will implement both strategies. my problem with making it configurable is user might choose inefficient strategy and shoot him/herself in the foot. that's why i was thinking just but i do agree on leaving future prove 1 byte magic flag in the header.,0,0,0,0.9100258946418762,0.9091740250587464,0.963625192642212,0.0,accept,unanimous_agreement
606277356,6592,"i guess it's called freedom of choice :) if we feel strong about it, we could of course disallow the ""negative size"" strategy for primitive types. however, it would have the disadvantage that we have a config that, depending on the data type you are using, would either be ignored or even throw an error if set incorrectly. from a usability point of view, this would be a disadvantage. it's always a mental burden to users if they have to think about if-then-else cases. i guess, in the end it's subjective which approach is better. but we could discuss pros/cons on the mailing list. feel free to propose whatever you prefer personally (and list all pros/cons for different approaches) and people can just chime in. personally, i have a slight preference to allow both strategies for all types as i think easy of use is more important, but i am also fine otherwise.",1,1,1,0.9032282829284668,0.9929514527320862,0.9776477813720704,1.0,accept,unanimous_agreement
657756747,6592,"aaaaaan i am back! :grinning_face_with_smiling_eyes: sorry for such a long break, was dealing with some personal stuff and work. pushed a few commits with `null-index-list` and `negative-size` serialization strategy functionality. rrrready for another round! the code syntactically is kinda raw, made it so to facilitate the reviewing process. the following set of commits is recommended for reviewing: [a link]",1,-1,1,0.868731677532196,0.9926355481147766,0.9507694840431212,1.0,accept,majority_agreement
665078937,6592,any updates?,0,0,0,0.9814685583114624,0.9840015172958374,0.9840564727783204,0.0,accept,unanimous_agreement
665304810,6592,"-- your pr is in my review queue. not sure how quickly i will find time to have a look though atm -- maybe next week, but i can't promise.",0,0,0,0.5210050344467163,0.6306552886962891,0.8477714657783508,0.0,accept,unanimous_agreement
698414958,6592,any updates :)?,1,1,1,0.830883264541626,0.989795744419098,0.9895402789115906,1.0,accept,unanimous_agreement
701300767,6592,"this would be an awesome addition, as i wouldn't have to maintain my own impl :grinning_face_with_smiling_eyes:.",1,1,1,0.9809397459030152,0.9769649505615234,0.9945775270462036,1.0,accept,unanimous_agreement
702774321,6592,let me know when i can jump back to it :),1,1,0,0.9785721302032472,0.9115281105041504,0.9320210814476012,1.0,accept,majority_agreement
730815895,6592,"hi, any updates on this?",0,0,0,0.9852307438850404,0.9898149967193604,0.9787452220916748,0.0,accept,unanimous_agreement
731626798,6592,"-- sorry for the delay. happy to hear that you are still on top of your kip! i did not find time yet to review the pr after your latest updates. (not sure if it makes you feel any better, i did also not review any other kip related pr for a while, and yours is actually at the top of my list...) -- i hope to find time soon and hope we can get this merged before the end of the year... cannot guarantee it though. at least, i would like to get it into 2.8 release.",-1,-1,-1,0.969847559928894,0.9269834756851196,0.94046813249588,-1.0,accept,unanimous_agreement
763952940,6592,"hey, just rebased my branch with the trunk, updated my tests to use junit 5. let me know when you guys will have time to review it.",0,0,0,0.976267635822296,0.9045760035514832,0.9145374894142152,0.0,accept,unanimous_agreement
776422927,6592,any updates on this? :),1,1,1,0.9467843770980836,0.9912095069885254,0.7287705540657043,1.0,accept,unanimous_agreement
776934773,6592,-- sorry that i did not get to review your updates yet... i need to finish #9744 first that is very close to get merged. your pr is next in the list... (just trying to work through my kip pr backlog one-by-one...) btw: seems there is a merge conflict. could you rebase the pr in the mean time?,-1,-1,-1,0.9912713766098022,0.9885112047195436,0.9917210340499878,-1.0,accept,unanimous_agreement
778674834,6592,"ok, resolved all conflicts. thank you for dedicating your time into this :)",1,1,1,0.9892471432685852,0.9949830770492554,0.99528968334198,1.0,accept,unanimous_agreement
808566124,6592,bump,0,0,0,0.9710121750831604,0.9707394242286682,0.9793093204498292,0.0,accept,unanimous_agreement
819183273,6592,"hey , are you still working on this/looking for reviews? i know everyone is often busy but if you can just respond to let us know you're still active, we can try to pitch in to get this across the finish line. cc for help reviewing",1,0,1,0.7219955921173096,0.6689140200614929,0.9222532510757446,1.0,accept,majority_agreement
819226355,6592,"i am! :) routinely checking this pr. i totally understand, we all get busy sometimes :thumbs_up:",1,1,1,0.9934514760971068,0.9961297512054444,0.9973691701889038,1.0,accept,unanimous_agreement
821515617,6592,"replied under each review comment, waiting for response before pushing the review changes.",0,0,0,0.9785029292106628,0.992133617401123,0.9928795695304872,0.0,accept,unanimous_agreement
823384191,6592,did you have time to look at it? let me know if you need more context on my comments.,0,0,0,0.9793779253959656,0.9636915326118468,0.9805068969726562,0.0,accept,unanimous_agreement
829779733,6592,"hey one other thing, can you give the kip document a quick pass and make sure everything in there is up to date with what we've discussed and anything else that's evolved during the pr review? for example we might want to point out the `null` handling, though it's technically an implementation detail it would be good to clarify what is and isn't possible with this new serde",0,0,0,0.9414947628974916,0.9796718955039978,0.9814899563789368,0.0,accept,unanimous_agreement
840015831,6592,"ok, updated the kip with serializing nulls for different strategies.",0,0,0,0.9875267148017884,0.991447925567627,0.9935286641120912,0.0,accept,unanimous_agreement
840856470,6592,seems like all checks passed :),1,1,0,0.9398362636566162,0.9925077557563782,0.952567458152771,1.0,accept,majority_agreement
840878936,6592,build has only some unrelated test failures in known flaky `ktablektableforeignkeyinnerjoinmultiintegrationtest#shouldinnerjoinmultipartitionqueryable` and `raftclustertest`,0,0,0,0.9873099327087402,0.98884516954422,0.9900352358818054,0.0,accept,unanimous_agreement
840880983,6592,"merged to trunk! thanks for all the work and patience it took to get this pr in, and to all the reviewers who helped get it here along the way. , can you update the kip to note that it's completed, and then move this kip to the ""adopted"" section on both the kip main page and the streams kips subpage? :folded_hands:",1,1,1,0.9769277572631836,0.9891033172607422,0.9951220154762268,1.0,accept,unanimous_agreement
840925233,6592,thanks for pushing it through and thanks for helping out reviewing! really nice addition!,1,1,1,0.9918749928474426,0.9956293106079102,0.9957321286201476,1.0,accept,unanimous_agreement
1603267651,6592,hey getting serializationexception in this serde serializationexception: invalid serialization strategy flag value flag value is derived from bytes size (which is coming >2) which is amount of enum variables hence its breaking seems to be a bug please check,0,0,0,0.9087446331977844,0.6450614333152771,0.9732995629310608,0.0,accept,unanimous_agreement
1603284146,6592,this is while using list where e is class which is inner serde used for inner is jsonserde of type e,0,0,0,0.9878943562507628,0.9940547347068788,0.9945007562637328,0.0,accept,unanimous_agreement
1603491428,6592,could you please provide a code snippet?,0,0,0,0.9878537654876708,0.9922120571136476,0.9942482709884644,0.0,accept,unanimous_agreement
416341868,5567,"note: both builds passed, but jenkins got rate-limited trying to report it to github. and i'm just now wondering... if jenkins failed to report the job status to github... why do we see the status here on github? o_o",0,-1,1,0.8974494338035583,0.6537647247314453,0.9686179161071776,,review,no_majority_disagreement
416457836,5567,could you take a look?,0,0,0,0.98567134141922,0.9828481674194336,0.9896900653839112,0.0,accept,unanimous_agreement
417199290,5567,"i have not reviewed the latest changes on this pr yet, but here are two meta-level thought i'd like to share: 1. serdes: here's my reasoning on whether we need to enforce serdes. we have the following scenarios: a) the ktable-to-be-suppressed (i will just call it ktable from now on) has user-specified serdes. in this case we do not need to require serdes again for suppression. b) the ktable is materialized and users do not specify serdes during materialization. in this case we will try to use the default ones from config (or we can use the inherited ones in the future, but that is not guaranteed to be always correct anyways), so if the default serde to use is incorrect, we will get the exception even before suppression at all. so we do not need to require serdes either. c) the ktable is not materialized and users do not specify serdes. today this case is not possible but in the future it may be the case due to optimizations, e.g. `ktable#filter / mapvalues` generated ktable. in this case if we do not require users to specify serdes and default ones are not correct, it will indeed have unexpected exceptions. but i think this case can still be walk-around by users to provide the `materialized` object in those operators; plus in the future we can have further optimization to ""push the suppression ahead"" which i will talk about later in this comment. so in sum, i think it is not necessary to always enforce users to provide serdes in the buffer config. 2. changelogs: about whether or not we should add new changelog topics for the suppression buffer itself, i think it depends on how we will implement the commit behavior. my suggestion is the following: a) for size / interval based intermediate suppression, we will still honor the commit operation to always ""flush"" the suppression buffer, i.e. the intermediate suppression is still best-effort which yields to commits. in this case, we do not need to bookkeep ""what records have been buffered and not emitted"" in the changelog either but can simply assume none have been emitted until commit. b) for final result suppression of window stores, we cannot yield to commit because that will violate the intended guarantees, but since we will not emit any records before the grace deadline anyways, the ""state"" of the buffer does not need to be book-kept anyways: if it is beyond the grace deadline, then every records should be flushed, otherwise, none should be flushed. note that the above approach works for both eos and non-eos: for non-eos, if there is a crash in between commits, we may emit a single record multiple times but that is fine for non-eos; for eos, if there is a crash in between commits we need to restore the whole state from the beginning anyways as of now (until we have consistent checkpoints someday), so this is also fine. one caveat though is that 2.b) above relies on the current stream time to determine upon re-start whether or not the window store data have been emitted to downstream; but stream time today is not deterministically defined even with kip-353 merged in so if we re-process, it may generate different behavior. i think this is acceptable as of now and can be fixed in the future: for example, we can include the ""current stream time"" in the commit message when adding consistent checkpoints to make the stream time deterministic on those checkpoints. so in sum, i think we can accept to not have changelogs for the buffer itself as long as we still respect commits except for final result suppression, which we can implement as a special case. in addition, the ktable-before-suppression's changelog can be suppressed along with the buffer as well: we can only write to the change logger when the buffer emits. more details in the next section. 3. future roadmap: we have discussed about how to re-design kip-63 after this is done, and one thing is to consider how to maintain the suppression effect on the state store's changelog topics as well. together it has some implications on our memory management story as well. here's my thinking following the above proposal on changelogs: a) say if we remove the buffer on top of the state stores, the saved memory can be given to 1) the added buffer ""behind"" the state stores, and 2) to enlarge the state store's only write buffer (e.g. rocksdb). b) we can optimize the topology to ""implicitly"" add suppression when necessary in addition to user-requested suppressions to reduce the traffic to the original ktable's store changlog topics. more concretely, think about the following examples: [code block] * with logical materialization, only table1 will be materialized. * with logical materialization plus implicit suppression, the above will become: [code block] in which case table1's store changelog will be suppressed as well: we only write to the change logger when we emit. now if users explicitly calls suppression: [code block] it will also be re-written to [code block] as well, in which case table1's changelog will be suppressed still based on the `suppress()` config, and then the suppressed changelog stream will be filtered to table2, which can be logically materialized still. c) finally about memory management: we can deprecate the current `max.cache.bytes` and replace with the `total.memory.bytes` which controls the total amount of memory that streams will use, similarly to what we described in [a link] note this total bytes will cover both user-requested and implicitly added suppression buffers. in other words, each buffer's own buffer config's semantics will be a soft-limit which is only best-effort full-filled, since it is yield to commit interval, and the total bytes usable. this is just a sketchy thought and may need more detailed implementation discussions. let me know wdyt.",0,0,0,0.945225179195404,0.982608675956726,0.8413693308830261,0.0,accept,unanimous_agreement
417352990,5567,"hi , thanks for the thoughtful feedback. 1. i think your argument about the serdes is sound. * for any non-k/v-changing operation that produces a ktable, we will be able to forward serdes from upstream, through the operator, and to suppress * the rest of the operators may change keys or values, but in all cases, it's possible to provide serdes at the key/value-changing operator, and then forward to suppress. so in all cases, we don't need to ask for serdes in suppress, which i vastly prefer. thanks! 2. for changelogs, i think it would be much better if we offered tight semantics in all cases. forcing people to reason about how the commit interval interplays with the suppression is needlessly complicated. but i do think that we can still optimize it to avoid the extra changelog. the good news is that at this stage, realizing that we can get serdes without asking for them in the suppression config means that we don't have to worry about the changelog or commit behavior. so this pr is not blocked on that conversation. i'll take it as a design goal to avoid an extra changelog and spend some time to see what i can come up with. at the least, you've offered a way to do it by relaxing the suppression semantics. 3. yes, i think that's a good long-term vision. and it would be all the more important to avoid an extra changelog if we wind up tacking a suppression on to every ktable. 3c. thanks for that reference. it would be nice to have a simple control bounding the memory usage. however, i'm not sure i agree that that config should be allowed to alter the program we've been asked to execute. if we were to add a `streams.memory.bytes`, we will also have to consider what to do if it's overconstrained. clearly, we cannot execute a streams program within 7 bytes, so we would have some validation on startup that says ""hey, you asked for no more than 7 bytes, but we need at least 800mb for this program"". rather than relaxing the suppression semantics, i'd advocate for explicit user-specified buffer sizes to be included in this arithmetic. but that is again a problem for the future. so in conclusion: i'll drop the serdes from the api, and forward from the source ktable instead. we should then be able to resume the review of this pr, right?",1,1,1,0.9915640354156494,0.9953368306159972,0.9942818880081176,1.0,accept,unanimous_agreement
417429229,5567,"yup, thanks! yeah i'm not married to my approach, so i'm all ears if you do already have some concrete ideas :)",1,1,1,0.992581844329834,0.9952844977378844,0.9958305954933168,1.0,accept,unanimous_agreement
417804756,5567,i also agree with the approach for serdes management approach.,0,0,0,0.9686670303344728,0.973666489124298,0.957353413105011,0.0,accept,unanimous_agreement
418515105,5567,", i've added the missing tests. i think this is the remainder of the comments. i'll update the kip and send out a notice now. thanks, -john",1,1,1,0.9572235941886902,0.983043611049652,0.9759418964385986,1.0,accept,unanimous_agreement
418892024,5567,"i think i've addressed all your concerns. about the system tests / performance concerns: yes, i plan to follow up with that once the features themselves are merged: 1. this pr (api only) 2. independent names for suppression nodes 3. in-memory buffering (not resilient to node shutdown/crash; spill-to-disk not implemented) 4. resilient buffering (wrap the buffer in a changelog) 5. spill-to-disk buffering 6. performance testing & optimization this was the set of smallest diffs i could think of that made sense to review. please let me know if some other subdivision makes sense. /cc",0,0,1,0.8127073049545288,0.8010486364364624,0.7241166830062866,0.0,accept,majority_agreement
419145301,5567,thanks for the review ! i know this one was a lot of work.,1,1,1,0.9901420474052428,0.9905905723571776,0.9901604652404784,1.0,accept,unanimous_agreement
419982389,5567,i do not have further comments. lgtm.,0,0,0,0.7969146370887756,0.4738160371780395,0.9830526113510132,0.0,accept,unanimous_agreement
422444318,5567,", i believe this pr is ready for a final pass. in response to your comments, i have revised the `suppressed` interface: * renaming the static/instance methods to follow our conventions * flattened the intermediatesuppression config into the top-level suppressed interface * renamed the suppressed methods to be clearer (like `untilwindowcloses` instead of `finalresults`) and also match each other (`untilwindowcloses` and `untiltimeelapses`). * added javadocs",0,0,0,0.5969420671463013,0.9646825194358826,0.917597234249115,0.0,accept,unanimous_agreement
422910901,5567,"thanks so much for the detailed review. i realized that your concern about ambiguity between the configured suppression time and the grace period was actually due to a regression i introduced earlier in refactoring. apologies for not realizing this sooner. i have made a couple of changes to resolve this: * `suppressed` now only declares static factory methods (as it did initially). in particular, this ensures that we can rely on the config for `untilwindowclose` to be just as we create it in that method. it's not possible to mutate it afterwards. * i replaced the `isfinalresults` boolean in `suppressedimpl` with a `finalresultssuppressionbuilder`. previously, the code just assumed that some fields, like the suppress duration, would be null for final results `suppressed` configs. now, there is no ambiguity about that. i'm going to make a final pass over this diff myself, but otherwise, i think it's ready for (another) final review. thanks!",1,1,1,0.9679885506629944,0.9854076504707336,0.9030577540397644,1.0,accept,unanimous_agreement
422992786,5567,"hey , the tests have (finally) passed. did you have any other feedback, or are we good to go?",0,1,0,0.9688053131103516,0.5236316919326782,0.5136205554008484,0.0,accept,majority_agreement
423587312,5567,"ok , i've addressed your concerns. i'll second the request for a final review from and .",0,0,0,0.9763813614845276,0.9142296314239502,0.9818511009216307,0.0,accept,unanimous_agreement
423589986,5567,i've made a pass over the updated kip wiki and the api lgtm. one clarification question: what's the semantics of [code block] compared with [code block] the javadoc of these two functions are exactly the same?,0,0,0,0.9868326187133788,0.9669491648674012,0.992197036743164,0.0,accept,unanimous_agreement
423594358,5567,"thanks for that catch. i failed to add the extra parameter to the javadoc for the second method. the semantics are basically the same. the first one defaults to an unbounded buffer. in retrospect, this seems like an unnecessary shortcut. i think i'll just ditch the first method and document the buffer config on the second.",1,1,1,0.8769620060920715,0.8976868391036987,0.9487636089324952,1.0,accept,unanimous_agreement
423614660,5567,"hmm. that java 10 build is failing for something in core, but i can't figure out what (see also [a link]",0,-1,0,0.5364749431610107,0.8160786628723145,0.8559154272079468,0.0,accept,majority_agreement
423625474,5567,update: i've submitted [a link] to fix trunk.,0,0,0,0.9882045984268188,0.9884875416755676,0.9821078181266784,0.0,accept,unanimous_agreement
423994933,5567,"this pr was incompatible with the rename of `internalprocessorcontext.initialized`, so i rebased and fixed it.",0,0,0,0.9737667441368104,0.985162079334259,0.9950050711631776,0.0,accept,unanimous_agreement
423995090,5567,"i think this is ready to merge now, once the tests pass.",0,0,0,0.9652734994888306,0.8363088965415955,0.973119020462036,0.0,accept,unanimous_agreement
424120339,5567,"thanks so much for all your time reviewing, and",1,1,1,0.9404380917549132,0.9778255820274352,0.979299247264862,1.0,accept,unanimous_agreement
932750960,11331,"thanks for the pr. a high-level question, what are we trying to optimize for here? 1. requests that don't include topic ids 2. requests that include topic ids 3. both 4. some kind of balance of both where we compromise a bit to keep the code maintainable",1,1,1,0.9508853554725648,0.959709644317627,0.952087938785553,1.0,accept,unanimous_agreement
933821031,11331,"requests that include topic ids both some kind of balance of both where we compromise a bit to keep the code maintainable the goal of this pr is to gracefully handle the new topic case. currently in kafka, when we create a new topic, the leader and isr request is sent first, then the update metadata request. this means that we will often encounter transient ""unknown_topic_id"" errors. in the new world of topic ids, we will see this as ""unknown topic id"" errors. the current logic returns a top level error and delays all partitions. this is a regression from previous behavior, and so this pr's goal is to return to the behavior where we store the unknown partition in the session until it can be resolved. see [a link] for more information.",0,0,0,0.9579674601554872,0.9777973890304564,0.9672999382019044,0.0,accept,unanimous_agreement
949274575,11331,"todos: 1. ~change inconsistent topic id so it is no longer a top level error~ 2. ~maybe refactor some of the receiving side code, we have a map with topicidpartition, partitiondata and both contain topic id~ decided to hold off on this as there are still usages for topicidpartition in the receiving side. 3. maybe change fetchsession to update newly unresolved partitions to no longer include topic name.",0,0,0,0.977807343006134,0.9939018487930298,0.9905299544334412,0.0,accept,unanimous_agreement
961259072,11331,"it seems that there are a few compilation errors, at least for `jdk 8 and scala 2.12`. could you check?",0,0,0,0.988187849521637,0.991884708404541,0.9917089343070984,0.0,accept,unanimous_agreement
965553054,11331,"system test results: [a link] a previous run was all green, so will need to confirm the 3 failed tests are unrelated to this change.",0,0,0,0.9847266674041748,0.98537677526474,0.9939980506896972,0.0,accept,unanimous_agreement
965677394,11331,looks like the topic id partition changes broke the build. i'll probably need to pull the latest version.,0,0,0,0.9644232988357544,0.9389620423316956,0.9776909351348876,0.0,accept,unanimous_agreement
966339841,11331,have you been able to triage these failures?,0,0,0,0.9330291152000428,0.9900785684585572,0.9817626476287842,0.0,accept,unanimous_agreement
968676970,11331,system test failures are not related. merged to trunk and to 3.1.,0,0,0,0.9728286266326904,0.9874256253242492,0.9893192648887634,0.0,accept,unanimous_agreement
375409892,4756,\cc,0,0,0,0.9724836349487304,0.9068992137908936,0.984728217124939,0.0,accept,unanimous_agreement
375974891,4756,added commit with changes for code review feedback. waiting for more feedback and some of the still unresolved questions. should i rebase now or wait till all questions resolved ?,0,0,0,0.9803296327590942,0.984921395778656,0.9459773898124696,0.0,accept,unanimous_agreement
376668126,4756,"meta comment: for javadocs, so we need to set up some pipeline to get javadocs published? or will this happen automatically? additionally, this pr should include updates to the web docs in `docs/streams/...` and in ""notable changes"" in `docs/upgrade.html` ?",0,0,0,0.985176682472229,0.993067741394043,0.9947510361671448,0.0,accept,unanimous_agreement
377176774,4756,"meta comment - besides updating documentation and javadoc, is there any outstanding item in this pr that needs to be addressed ?",0,0,0,0.9869891405105592,0.9902404546737672,0.994281530380249,0.0,accept,unanimous_agreement
377706175,4756,"for user documentation my plan is to update the following: * [a link] - update the scala example to use scala dsl * developer guide * [a link] - add a reference to the streams-scala artifact and add scala examples * [a link] - note that default key/value serdes not required for scala dsl. * [a link] - add a new section for the scala dsl. packages, examples, implicit serdes and other differences from java dsl. the bulk of the content would go here. * [a link] - reference implicit serdes from streams dsl page which includes implicit serdes explanation and usage, or vice versa. i could use some advice on how to actually go about updating the docs efficiently. the [a link] doesn't go into detail about a workflow i can use while developing docs (make a change, preview, etc). i noticed the code samples for the streams docs are formatted as html, but i couldn't find the source for these examples in the project, are they generated somehow? edit: i also added to the [a link] section, to explain how to include the dependency in maven build file, e.g. to use the scala wrapper. edit 2: i also updated the section 2.3 (streams api) to reference the new client library. edit 3: i also updated the streams upgrade guide.",0,0,0,0.9652554988861084,0.99072003364563,0.9854341745376588,0.0,accept,unanimous_agreement
377755547,4756,sounds great. you are right that it is html -- just edit it directly. i have no concrete workflow suggestion. what code examples do you mean? it should all be in the html files. there is nothing in streams docs thats generated.,1,1,1,0.9825698733329772,0.9853874444961548,0.979144275188446,1.0,accept,unanimous_agreement
377786142,4756,i'm referring to the code snippets found in pages like the dsl api. they look like they've been generated with syntax highlightling. [code block] [a link],0,0,0,0.9764074087142944,0.9878820180892944,0.9912830591201782,0.0,accept,unanimous_agreement
377829608,4756,"for the doc changes, as i mentioned before we should also add a new section in the `[a link] section, to explain how to include the dependency in maven build file, e.g. to use the scala wrapper.",0,0,0,0.9891772866249084,0.9927731156349182,0.994665026664734,0.0,accept,unanimous_agreement
377863998,4756,"i see -- i guess, somebody used some special html editor... \cc -hamill might be able to shed some light... most people just use plain text editors. you can just add plain html without any syntax highlighting. cf [a link] (i was just not sure what you mean by ""generated"" because the configs html is generated from the source code directly -- cf. [a link] and [a link]",0,0,0,0.935352385044098,0.8755109906196594,0.966699242591858,0.0,accept,unanimous_agreement
378364212,4756,i've implemented the user docs. however i don't know how to view the documentation in my browser. none of the html files in `docs` subdir render when i open them locally with chrome (even before i made any edits). there are likely some formatting mistakes since they're difficult to spot in the markup alone. can someone familiar with editing these docs please describe a workflow i can use to view rendered documentation locally? in the meantime i encourage people to review the content. /cc -hamill,0,0,0,0.8573163151741028,0.8622862100601196,0.6478700041770935,0.0,accept,unanimous_agreement
378411984,4756,"you can read this wiki page for render the pages locally: [a link] to do that you'd need to first copy-paste the docs to `kafka-site` repo, and start the apache server locally in order to see the difference.",0,0,0,0.9875664710998536,0.9906415343284608,0.9954975843429564,0.0,accept,unanimous_agreement
378682617,4756,"i was able to setup a local apache2 webserver (wow, i haven't done that in 10 years!) i fixed formatting issues, typos, added the blurb to the streams upgrade guide, and other misc. feedback you provided.",0,0,1,0.6342618465423584,0.5020908117294312,0.9527606964111328,0.0,accept,majority_agreement
378730925,4756,thanks! could you also rebase your pr against trunk to resolve the conflicts as well?,1,1,1,0.9701177477836608,0.8909263610839844,0.977831482887268,1.0,accept,unanimous_agreement
378796513,4756,done!,0,0,1,0.514024555683136,0.6890168786048889,0.6799882054328918,0.0,accept,majority_agreement
379273985,4756,"- one of the reasons we wrote the tests was to demonstrate the idiomatic usage of the scala apis. if you think we should write additional tests to verify if the correct topology is built, what kind of tests can do this verification ? is there any example test in the java apis that does this verification of building the correct topology ?",0,0,0,0.97088360786438,0.9927719235420228,0.985039472579956,0.0,accept,unanimous_agreement
379284118,4756,- good enough reason. for additional tests i can think of a few of ways of doing it. 1. use the `topologytestdriver` - so you can build the topology for say `mapvalues` and then run that thought the `topologytestdriver` to verify the output. 2. call `describe` on the built `topology` and verify it has the correct nodes (see `topologytest`) 3. mock/stub the the java interfaces and verify the correct interactions.,1,0,0,0.679961621761322,0.7527719736099243,0.8908059000968933,0.0,accept,majority_agreement
379284887,4756,- thanks! we will take a look and have additional tests.,1,1,1,0.9834886193275452,0.9880487322807312,0.9937654733657836,1.0,accept,unanimous_agreement
379378808,4756,the jenkins failures seems relevant: [code block] we should exclude `logs/kafka-streams-scala.log` from rat check.,0,0,0,0.9874223470687866,0.9932112097740172,0.992946445941925,0.0,accept,unanimous_agreement
379628965,4756,- the kafka streams scala api is a dsl level api rather than a `processor` level one. so are u suggesting that for a specific scala api (say `kstream#mapvalues`) we write the equivalent `processor` level code and verify if the results match with the scala dsl api ? another option could be to use both the scala and the java api for `kstream#mapvalues` and verify if we get the same results. isn't the latter option better in the sense that we are comparing apis at the same level ?,0,0,0,0.9817314743995668,0.994238257408142,0.9920327663421632,0.0,accept,unanimous_agreement
379758448,4756,- yes comparing them would be fine.,0,0,0,0.9205002188682556,0.9823194146156312,0.9697664976119996,0.0,accept,unanimous_agreement
380139596,4756,- in the last commit i wrote the original test cases using the java apis in scala. this verifies if the 2 apis deliver the same result. is this what u meant ?,0,0,0,0.9879125952720642,0.9908658862113952,0.9923771619796752,0.0,accept,unanimous_agreement
380143884,4756,"- i was thinking more along the lines of using the scala api to build simple streams topologies, i.e, `topology topology = streams.mapvalues(..).build()` and then using the `topologytestdriver` to pipe some input through the topology and verify the output is as expected. on tue, 10 apr 2018 at 16:19 debasish ghosh wrote:",0,0,0,0.8713308572769165,0.9829427003860474,0.9903188943862916,0.0,accept,unanimous_agreement
380151904,4756,"- may be i am missing something .. in your example `topology topology = streams.mapvalues(..).build()`, what is `streams` ? `mapvalues` is a method on `kstream` - right ? is there any example in the code base that i can refer to for similar stuff ?",0,0,0,0.968993842601776,0.9938761591911316,0.9823545217514038,0.0,accept,unanimous_agreement
380154612,4756,"- yes that is correct. sorry, my bad. yes you would do something like: [code block] you can have a look at [a link] for how to pipe input and verify the output etc. hth on tue, 10 apr 2018 at 16:53 debasish ghosh wrote:",-1,-1,-1,0.9896425604820251,0.9923107624053956,0.9908532500267028,-1.0,accept,unanimous_agreement
380381008,4756,- i added a suite `topologytest` which verifies that the topology generated by the java and scala apis are identical. please let me know if this works. i did not use `topologytestdriver` as i did not want to execute any topology. the earlier tests verify that the results we get from the java and the scala api are identical. these tests verify that the topologies are identical as well.,0,0,0,0.9749937057495116,0.9838063716888428,0.6533818244934082,0.0,accept,unanimous_agreement
380382707,4756,thanks - that works,1,1,1,0.8826233744621277,0.9826006293296814,0.8631601333618164,1.0,accept,unanimous_agreement
380383532,4756,btw - did you start a voting thread for the kip yet?,0,0,0,0.9874898791313172,0.9922266006469728,0.9924807548522948,0.0,accept,unanimous_agreement
380384424,4756,"thanks .. regarding voting thread, no we haven't done yet. also we are not sure how to do it :-) .. how do we start a voting thread ?",1,1,1,0.9753566980361938,0.9906655550003052,0.9961056113243104,1.0,accept,unanimous_agreement
380392395,4756,"on the dev.apache.org list you start a thread, similar to the [discuss] thread you already started, but use [vote] instead of [discuss]. the vote needs to run for at least 72 hours and needs at least 3 binding votes. on wed, 11 apr 2018 at 10:15 debasish ghosh wrote:",0,0,0,0.9110420346260072,0.9884661436080932,0.9939190149307252,0.0,accept,unanimous_agreement
380400070,4756,done ..,0,0,0,0.9689553380012512,0.9705991744995116,0.992914080619812,0.0,accept,unanimous_agreement
380672649,4756,", , - sent out the [vote] email on `kafka-dev`",0,0,0,0.9752861857414246,0.9935239553451538,0.9917393922805786,0.0,accept,unanimous_agreement
382386413,4756,", for some reason github doesn't let me reply inline. your ""more efficient version"" is still allocating a tuple, right? to avoid it, mapper would have to return a keyvalue.",0,0,0,0.9831460118293762,0.7741710543632507,0.5657455325126648,0.0,accept,unanimous_agreement
382389088,4756,"- the `map` function doesn't allocate the tuple. it takes a function of the form `(k, v) => (kr, vr)` and constructs the `keyvalue` from the tuple that `mapper` returns. wanted to abstract the construction of `keyvalue` to avoid noise. [code block]",0,0,0,0.986791729927063,0.9938568472862244,0.9933839440345764,0.0,accept,unanimous_agreement
382391983,4756,"my point is that there's still a throwaway tuple for each mapped item which is the issue raised. the gc is generally good at dealing with such short lived objects, so not sure if it matters in this case, but it's clearly adding some overhead when compared to the java implementation.",0,0,0,0.9676928520202636,0.9613445401191713,0.932015061378479,0.0,accept,unanimous_agreement
382401057,4756,"yeah .. the point is this tuple is generated within the `map` function through the call of the `mapper`. to avoid this, the way out will be to change the `mapper` to `(k, v) => keyvalue[kr, vr]`. which can be done .. but makes the api contract a bit noisy. otoh we can rely on the jit and the ability of gc to deal with such short lived objects. suggestions welcome :-)",1,1,1,0.9752214550971984,0.9942525029182434,0.9734383821487428,1.0,accept,unanimous_agreement
382462315,4756,"maybe i was a bit paranoid on the gc pressure before, just raised it as a concern. since you mentioned `we did run some tests in bulk to check the diff in performance between the 2 versions. couldn't find much of a difference though.` i'm fine with keeping the api as elegant as of now and assume jit doing the right thing.",0,0,0,0.8935850262641907,0.9509996175765992,0.8731497526168823,0.0,accept,unanimous_agreement
383113457,4756,"out of curiosity, how long does it take to build (compile and run the tests) kafka-streams-scala.",0,0,0,0.9573787450790404,0.9867699146270752,0.976814329624176,0.0,accept,unanimous_agreement
385124698,4756,"i have been unable to reply to this pr since it has continuously failed to load for me for 1 week. so i am resorting to ""reply by email"". :) "" how are multiple versions of kafka core published at part of the release process? is the build script called twice with appropriate scalaversion parameter?"" we currently invoke it twice with -pscalaversion=2.12 for the second invocation because we haven't enabled scala 2.12 builds by default. the reason is that scala 2.12 requires java 8. once we switch our build to require java 8 (should happen soon), we will enable 2.12 by default and then a single command with the *all suffix will do it for all scala versions supported. to give an example, `test` will run the tests with the default scala version while `testall` will run the tests with all supported scala versions. for what it's worth, i strongly agree that we should do the same we do for core, i.e. publish for both scala 2.11 and scala 2.12 and include the scala version in the artifact id, as it's standard practice for scala libraries. ismael on wed, apr 25, 2018 at 1:21 pm, sean glover wrote:",1,1,1,0.9462813138961792,0.9925950169563292,0.9943653345108032,1.0,accept,unanimous_agreement
222823006,1446,", what do you think? i was able to run the examples and see the metrics per node in a jmx console.",0,0,0,0.9838991165161132,0.9716690182685852,0.9675722122192384,0.0,accept,unanimous_agreement
222835496,1446,"thanks , could you take a look first at this ticket? i have assigned you as the reviewer on the ticket, and please feel free to re-assign to me otherwise.",1,1,1,0.752938449382782,0.676296591758728,0.8117787837982178,1.0,accept,unanimous_agreement
223304691,1446,"perhaps the pr name should be ""kafka-3715: add granular metrics per node""? the jira number is usually part of the pr name. minor thing but just for consistency.",0,0,0,0.9864746928215028,0.9919822812080384,0.985332190990448,0.0,accept,unanimous_agreement
224061436,1446,thanks . two higher level questions: does it make sense to add a unit test or two for the new metrics? and do we have any overhead measurements in the sense of how much to the new recordings add to the end to end latency?,1,1,1,0.8774197101593018,0.8905053734779358,0.9133504629135132,1.0,accept,unanimous_agreement
225488213,1446,"ran org.apache.kafka.streams.perf.simplebenchmark with the following configuration (i.e. without state store backed streams and simple print statements indicating which part of the benchmark is being run) [code block] // benchmark.processstreamwithstatestore(); then attached a yourkit profiler and saw the following differences (see attached screenshots) without any changes to the code and using cpu sampling in yourkit saw 61% cpu contention with the per node metrics and using cpu sampling in yourkit, saw 70% cpu contention, without any changes org.apache.kafka.streams.perf.simplebenchmark producer producer performance [mb/sec write]: 8.853212193170378 consumer [yourkit java profiler 2016.02-b38] log file: /users/aartikumargupta/.yjp/log/simplebenchmark-1754.log consumer performance [mb/sec read]: 4.596191726854892 simple stream performance source->process streams performance [mb/sec read]: 14.361679855964493 simple stream performance source->sink streams performance [mb/sec read+write]: 4.535097423059803 with node metrics producer performance [mb/sec write]: 5.035256582778346 consumer [yourkit java profiler 2016.02-b38] log file: /users/aartikumargupta/.yjp/log/simplebenchmark-1549.log consumer performance [mb/sec read]: 2.751484036579496 simple stream performance source->process streams performance [mb/sec read]: 8.014018691588785 simple stream performance source->sink streams performance [mb/sec read+write]: 6.562667414985077 ran this multiple times and the results varied between 63%(no changes) and 72%(with per node metrics) the difference seems to be around the point at which yourkit profiler is attached that said, not sure if this is a valid load simulating scenario mentions in [a link] that is the simplebenchmark a good scenario to be profiling ? if not any suggestions on another scenario, maybe we can add (check in) such a scenario under examples, which can be used for all similar future profiling exercises still working on the unit tests for per node metrics.",0,0,0,0.9811134338378906,0.9936519861221312,0.9922550916671752,0.0,accept,unanimous_agreement
227377187,1446,hey it's kind of hard to tell based on your screenshots where the time is going since i don't see any drilldown into the call stacks of the streamthread run loops. it's probably necessary for you to flip things on in the yourkit profiler so you can get the full call stacks and determine if `sensor.record` is the source of most of the time.,0,0,0,0.9434478282928468,0.6251646280288696,0.953683078289032,0.0,accept,unanimous_agreement
227602467,1446,"thanks , some general comments: 1. for naming consistency as with other metrics objects, for finer grained metrics we tend to name the sensors as ""level-name.level-id.metrics-name"", for example in `sendermetrics` we used `topic.[topic-name].records-per-batch` etc for per topic-level metrics and in `selectormetrics` we used `node-[node-id].bytes-sent` etc for per node-level metrics, and in my latest pr #1530 i was doing similar naming. you may already notice that this is for creating different sensors as we synchronize at the per-sensor basis, and since in producer / consumer we always has single-thread, today we do not have any contentions for the lock yet, and in streams we are trying to add per-thread metrics and consider adding global metrics only after the syncrhonization is removed in kafka-3155 since as we have discussed in other prs with multiple threads contention overhead can be large. 2. different metrics reporter has the freedom of constructing their reporting metrics name from the hierarchy of ""metrics-prefix, group-name, metrics-name, metrics-tags"" where metrics-prefix are ""kafka.producer"" / ""kafka.consumer"" / ""kafka.streams"" depending on which client library you are using. and in this case the sensor names are actually ignored as they are used internally of the metrics object for grouping different metrics only. for example in `jmxreporter` we create the mbeanname / attributename as [code block] so we need to make sure that the hierarchy is sufficient for different reporters to differentiate these metrics in their own space.",1,1,1,0.9516730904579164,0.9577065110206604,0.9570897221565248,1.0,accept,unanimous_agreement
227604083,1446,"btw the `simplebenchmark` numbers are pretty low compared to my laptop (4gb memory, and low-end cpus). what environment did you run the profiler?",0,0,0,0.9355926513671876,0.9624691605567932,0.9744608402252196,0.0,accept,unanimous_agreement
228123517,1446,"mackbook 12 inch 2015 early edition, 1.3ghz dual-core intel core m processor (turbo boost up to 2.9ghz) with 4mb shared l3 cache. 8gb of 1600mhz lpddr3 onboard memory i think that it has to do with attaching yourkit profiler. without the profiler i get the following producer producer performance [mb/sec write]: 22.247686586525987 consumer consumer performance [mb/sec read]: 56.39283169836138 simple stream performance source->process streams performance [mb/sec read]: 40.33237957119899 simple stream performance source->sink streams performance [mb/sec read+write]: 18.71113212350438 process finished with exit code 0",0,0,0,0.8859776258468628,0.9878823757171632,0.9798070192337036,0.0,accept,unanimous_agreement
229234020,1446,is there a way to register user-defined metrics?,0,0,0,0.9865810871124268,0.9934259653091432,0.9939716458320618,0.0,accept,unanimous_agreement
263255635,1446,would you still have time for this pr or should i have a look? thanks.,1,1,1,0.7355340123176575,0.6670251488685608,0.8498115539550781,1.0,accept,unanimous_agreement
264912720,1446,"![a link] added two levels of logging metrics, as described in kafka-3811. showing performance of pr when logging all metrics, vs just some higher level metrics in streams (see last two bars of each benchmark, the rest is details). average of 3 runs for each test shown. all data at: [a link]",0,0,1,0.9390929341316224,0.8986649513244629,0.8987430334091187,0.0,accept,majority_agreement
264913402,1446,"as part of this pr we are introducing different levels of logging for sensors, as initially described in kafka-3811 [a link] could you see if you are satisfied with the way these levels are introduced? in particular, i'd focus on everything outside of streams, while can focus on the streams part too. thanks. cc too if they have time for looking at the sensor changes. thanks.",1,1,1,0.969373881816864,0.9765793085098268,0.9899613857269288,1.0,accept,unanimous_agreement
264927773,1446,(pr failures due to known reset tests),0,0,0,0.964522659778595,0.9813711047172546,0.9617502689361572,0.0,accept,unanimous_agreement
264964781,1446,registering user-defined metrics will be in another jira. thanks.,1,1,1,0.9578168392181396,0.9481846690177916,0.913843870162964,1.0,accept,unanimous_agreement
265074848,1446,"looks good! thanks for cleaning this up and adding the missing metrics. when i took this on earlier, jay mentioned on a related jira ([a link] that the usage of metrics in streams incurs an overhead because we walk through a list of sensors inside a synchronized method. when i ran the code in the pr and the simple benchmark tests, and looked at the hrof counter recordings using yourkit profiler, i observed a 5 percent cpu overhead with my fix as compared to without it. wondering now if that was a measurement error/ test setup error? out of curiosity, can you describe your perf testing experiment. did you use simple benchmark for the perf counters? , can you share the example use case /automated test that you ran? how many nodes? how many tasks, topology , and which profiler tool, flight recorder/yourkit profiler?",1,1,1,0.9902218580245972,0.9884313344955444,0.993923842906952,1.0,accept,unanimous_agreement
265104382,1446,"thanks i did use yourkit, but the final numbers i put on the pr are using simplebenchmark.java that is included with streams. that runs locally on my macbook pro 16gb, ssd, dual core. so on one node we have kafka and the simplebenchmark application. i don't think yours was a measurement error, but some metrics were missing back then.",1,1,1,0.9245178699493408,0.6309389472007751,0.8860141038894653,1.0,accept,unanimous_agreement
265452922,1446,test passed. weird error on jenkins: `hudson.remoting.channel:ubuntu-4: java.io.ioexception: remote call on ubuntu-4 failed`,-1,-1,-1,0.9823952317237854,0.9339765906333924,0.5058077573776245,-1.0,accept,unanimous_agreement
265559198,1446,if you have time.,0,0,0,0.9745649695396424,0.984370768070221,0.9097360372543336,0.0,accept,unanimous_agreement
265830060,1446,"about the synchronization overhead, i think jay was referring to kafka-3769 and i have fixed in some time ago (you can see the graphs before and after the fix): [a link]",0,0,0,0.9696225523948668,0.9860556125640868,0.99228036403656,0.0,accept,unanimous_agreement
266758773,1446,"thinking about it a bit more, i now understand your arguments before: tags were not used to distinguish sensors in the sensor registry (i.e. `concurrentmap sensors`), so without the processor-node as part of the sensor name they will collapse into the same sensor, that can be potentially accessed by multiple threads. in that case, you are right that we do need to have both the processor node name as well as the task id string as part of the `nodemetrics` sensors, and have the task id string as the `taskmetrics` sensors. your current naming as the format of [code block] where `thread.thread-id` comes from the prefix in `streamsmetricsimpl`. and there is no separators between `taskid` and `processor name`, and we use the `-` separator between `processor name` and the actually sensor name. i am wondering if we could remove this prefix and refactor the rest prefixes as [code block] the reason is that tasks can migrated between threads from time to time, and when that happens some of the existing sensor will not have any data any more while some more sensors need to be created if we keep the thread id as the prefix, and for using the `.` separator it is mainly for consistency with other sensors. ditto for task metrics as well.",0,0,0,0.9537376165390016,0.9796804785728456,0.9735569953918456,0.0,accept,unanimous_agreement
266827021,1446,"about your latest comment, when a task migrates, when we close the topology, i remove all previous sensors. so when the task migrates it will only have new sensors. the old sensors will be gone. let me know what you think. thanks.",1,1,1,0.8914822340011597,0.9860796332359314,0.972105085849762,1.0,accept,unanimous_agreement
267309387,1446,will be adding unit tests shortly.,0,0,0,0.9811633229255676,0.9867148995399476,0.9928575754165648,0.0,accept,unanimous_agreement
267466910,1446,"what i meant is mainly around user experience: by having task / processor-node level metrics to be distinguished by the thread name (e.g. define sensor name as `thread.streams-thread-xxx.task.1-1.processor-xxx-processor-throughput`), users then need to enumerate all possibly created sensors in their monitoring system and / or web ui, while only one of them will be valid / having non-blank graphs at a given point of time; on the other hand, since at any given time only one thread will be owning the task / processor node, it is safe to collapse them into a single sensor that could be accessed and updated by different threads at different times, and users then can simply polling / monitoring this single sensor (e.g. `task.1-1.processor-xxx-processor-throughput`).",0,0,0,0.9469811320304872,0.9909850358963012,0.9887380599975586,0.0,accept,unanimous_agreement
268056393,1446,i agree with you on removing the thread prefix. will do so with next commit. thanks.,1,1,1,0.956275463104248,0.9575417041778564,0.9775130152702332,1.0,accept,unanimous_agreement
268075855,1446,is this relevant? [code block],0,0,0,0.9877927899360656,0.9921112060546876,0.9954581260681152,0.0,accept,unanimous_agreement
268085496,1446,"i don't think so, looks like an environment one.",0,0,0,0.957434356212616,0.8874136209487915,0.9577443599700928,0.0,accept,unanimous_agreement
268106260,1446,"![a link] updated performance graph (avg of 3 runs). a couple of takeaways: - trunk and pr have comparable performance in the ""no metrics recorded"" case. worse case difference is around 2% better for trunk in one case and 5% better for pr in another case. - both trunk and pr see a similar drop in performance when collecting some metrics (info) - notice how the pr's info metrics does better than trunk's metrics. that is because in the pr the state store metrics are now debug, so we're collecting fewer metrics by default. i'll get another graph where the pr temporarily assigns info to the store metrics, for a better apple-to-apple comparison. - worst case scenario drop in perf for debug: 60%. best case drop: 20% - worst case drop in perf for info: 11%. best case drop: 3% (compare with current trunk below). - worst case drop in perf for trunk metrics: 17%. best case drop: 9%. - not relevant for this pr, but looks like all numbers have moved up from the last graph. that's good.",0,0,1,0.8736165761947632,0.9441503882408142,0.8862244486808777,0.0,accept,majority_agreement
268108598,1446,"thanks for the newly updated stats . a few quick question: 0. for the comparison you mentioned above ""worst case .. best case"", they are all compared with trunk with metrics turned on right? 1. regarding `worst case drop in perf for info: 11%. best case drop: 3% (compare with current trunk below).` since you mention this pr's info metrics perf is better than trunk's metrics, i was assuming that the best case perf drop should be a negative ratio? 2. regarding `worst case drop in perf for trunk metrics: 17%. best case drop: 9%.` what does this mean? i thought for `trunk metrics` it means info + state store metrics, but it seems you have not got the results for this yet.",1,1,1,0.9573058485984802,0.9213385581970216,0.9785269498825072,1.0,accept,unanimous_agreement
268219056,1446,"not quite: 0. no, they are not compared with trunk, they are compared with the pr with metrics turned off. 1. again, not compared with trunk, just with no metrics. sure, if you compare with trunk you'd get the negative ratio. 2. this is trunk today, so yes, it is info + state store metrics. i'm just pointing out that their cost is between 9-17% today. compared with no metrics.",0,0,0,0.9089074730873108,0.9720482230186462,0.9506675601005554,0.0,accept,unanimous_agreement
268227910,1446,"![a link] updated with an additional run where we collect both info + state store metrics (just like trunk does now). overall, the main takeaway is that we're not introducing any particular overhead with the pr with an apples-to-apples comparison with trunk. at this point i'm personally happy with the numbers. thanks.",1,1,1,0.9799436926841736,0.9931159615516664,0.9947906732559204,1.0,accept,unanimous_agreement
268304113,1446,thanks i think i addressed your comments.,1,1,1,0.5718134641647339,0.6828854084014893,0.7320516109466553,1.0,accept,unanimous_agreement
270527621,1446,"some of the comments seem not addressed yet, and could you rebase as well?",0,0,0,0.98502779006958,0.990797996520996,0.9894989728927612,0.0,accept,unanimous_agreement
270619470,1446,"i removed the extra sensor registration apis as you suggested, and i also agree with the rest of your comments and have addressed them. thank you.",1,1,1,0.9359691739082336,0.9803481698036194,0.9682478308677672,1.0,accept,unanimous_agreement
270620026,1446,"i removed the extra sensor registration apis as you suggested, and i also agree with the rest of your comments and have addressed them. thank you.",1,1,1,0.9359691739082336,0.9803481698036194,0.9682478308677672,1.0,accept,unanimous_agreement
271103036,1446,"failures unrelated to pr, e.g., kafka.api.sslproducersendtest.testsendcompressedmessagewithcreatetime",0,0,0,0.9334274530410768,0.989241063594818,0.9884053468704224,0.0,accept,unanimous_agreement
271436295,1446,"made another pass over the latest patch. lgtm overall. just one minor comment about testing coverage: would you consider adding some test cases for `streammetricsimpl` inside `streamthreadtest` as well, for testing is naming conventions, etc?",0,0,0,0.9855373501777648,0.8282137513160706,0.9706405997276306,0.0,accept,unanimous_agreement
271662859,1446,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
271671814,1446,what do you mean by `test this please`? which part? thanks.,1,1,0,0.8195200562477112,0.5432549118995667,0.796534538269043,1.0,accept,majority_agreement
271675184,1446,[a link] :),1,1,1,0.6224406957626343,0.9942322373390198,0.801796555519104,1.0,accept,unanimous_agreement
271679307,1446,wow!,1,1,1,0.9728956818580629,0.9884353876113892,0.9457429051399232,1.0,accept,unanimous_agreement
271681953,1446,unrelated error: kafka.integration.uncleanleaderelectiontest.testuncleanleaderelectiondisabled,0,0,0,0.8109449744224548,0.9882194995880128,0.9886488318443298,0.0,accept,unanimous_agreement
271951164,1446,unrelated: kafka.api.sslproducersendtest.testclosewithzerotimeoutfromsenderthread,0,0,0,0.9847360253334044,0.983915627002716,0.9934808015823364,0.0,accept,unanimous_agreement
271969606,1446,the old org.apache.kafka.streams.integration.resetintegrationtest failure is back but shouldn't be related to pr.,0,0,0,0.96545672416687,0.9924174547195436,0.9670741558074952,0.0,accept,unanimous_agreement
271977388,1446,merged to trunk. many thanks to and !!,1,1,1,0.9885013103485109,0.9955409169197084,0.9955037236213684,1.0,accept,unanimous_agreement
395236826,5101,"dong, i have updated the pr to address the comments. could you take a second look? thanks!",1,1,1,0.9666185975074768,0.9907084703445436,0.973901391029358,1.0,accept,unanimous_agreement
406153652,5101,"hey , is this patch ready for review?",0,0,0,0.983810305595398,0.8957124948501587,0.99151211977005,0.0,accept,unanimous_agreement
406507085,5101,yes. can you help take a look?,0,0,0,0.9727714657783508,0.9567012190818788,0.985859215259552,0.0,accept,unanimous_agreement
406659794,5101,sure. can you rebase the patch to resolve the conflict?,0,0,0,0.9793128967285156,0.9928192496299744,0.99334454536438,0.0,accept,unanimous_agreement
406738020,5101,rebased onto trunk. thanks dong!,1,1,1,0.988450050354004,0.995563507080078,0.9956788420677184,1.0,accept,unanimous_agreement
412941942,5101,i have updated the pr to add more logging and address the naming issues. could you take a look again? thanks!,1,1,1,0.9547423720359802,0.9667385816574096,0.9236109852790833,1.0,accept,unanimous_agreement
414773731,5101,thanks for your comments. i have rebased the patch. can you take a look?,1,1,1,0.9187117218971252,0.9116588830947876,0.9369244575500488,1.0,accept,unanimous_agreement
415120454,5101,thanks dong for the review. i have addressed the comments and will add more tests for the controller behavior.,1,1,1,0.9668134450912476,0.9746291637420654,0.973045289516449,1.0,accept,unanimous_agreement
415132003,5101,rebased.,0,0,0,0.982489824295044,0.958116054534912,0.9845768809318542,0.0,accept,unanimous_agreement
415160017,5101,"hey , i have finished reviewing this patch except tests. i will wait for to add tests. would you like to review this patch as well?",0,0,0,0.9618648290634156,0.5420933365821838,0.7306746244430542,0.0,accept,unanimous_agreement
415241065,5101,": another thing is that i am wondering if you have done any perf testing. we probably don't expect any slowdown with the additional controller epoch check. however, it would be useful to verify that common operations such as controlled shutdown, broker startup, leader balancing are not slower after this patch.",0,0,0,0.949942946434021,0.9857900738716124,0.975807011127472,0.0,accept,unanimous_agreement
415954168,5101,thanks for the comments. i have added some controller integration tests for the patch and address the comments. i will do some perf testing and post the results once i get the numbers.,1,1,1,0.9563738107681274,0.731205940246582,0.9682808518409728,1.0,accept,unanimous_agreement
415955473,5101,"also, we didn't have the protection against a stale controller deleting `\controller` znode when `controllermovedexception` is thrown in `oncontrollerfailover()` during controller initialization. this can cause controller switch storm. consider the following events: 1. broker a creates /controller znode 2. broker a reads the controller epoch zkversion 3. broker a lose its zk session -> /controller node gets deleted 4. broker b creates /controller znode 5. broker b reads the controller epoch zkversion 6. broker b increments the controller epoch and bumps the zkversion of /controller_epoch znode 7. broker a tries to update zookeeper (e.g. update `/admin/reassign_partitions`) but fails because /controller_epoch zkversion mismatch. a controllermovedexception is thrown. 8. broker a removes the /controller znode (currently owned by broker b) because we didn't explicitly catch `controllermovedexception` in `elect()` and trigger a controller move in the following code block: [code block] 9. broker c creates /controller znode ... this loop may never end and the controller role will keep switching across brokers without making any progress. i have also updated the pr to include the fixes: 1. catch `controllermovedexception` in `elect()` without triggering the controller move. this makes sense because when the `controllermovedexception` is thrown, the new controller has been elected and there is no need to trigger another round of controller election. 2. guard against the `\controller` deletion by controller epoch zkversion. this ensures stale controller cannot delete `\controller` znode.",0,0,0,0.9792295098304749,0.992603361606598,0.988442838191986,0.0,accept,unanimous_agreement
416999642,5101,looks like tests are getting stuck while shutting down the controller. observed this while running tests using [code block]. not able to reproduce if i run individual tests . looks like some race condition. [code block],-1,0,0,0.7497878074645996,0.5928472876548767,0.889979362487793,0.0,accept,majority_agreement
417161498,5101,"thanks for the comments. there is indeed a race condition between clearing the queue when handling `controllermovedexception` and closing controllereventmanager. because we use a special event to interrupt and shutdown the controller event thread, if `controllermovedexception` happens after we put the shutdown event into the queue and before event thread picks it up, the shutdown event get cleared when handling `controllermovedexception`. to resolve it, i added the `isshuttingdown` flag to ensure that the event queue will not be cleared if the shutdown event has been put into the queue.",1,1,1,0.9592257738113404,0.6466678977012634,0.9556483030319214,1.0,accept,unanimous_agreement
417161614,5101,thanks a lot for the review and i really appreciated the comments. i have updated the pr to address the issues. could you take a look again?,1,1,1,0.985468327999115,0.9935592412948608,0.9932750463485718,1.0,accept,unanimous_agreement
417195949,5101,"it seems that if controller receives controllermovedexception, the controller should not simply remove all events from the controller queue. events such as `reelect` and `controlledshutdown` probably should stay in the queue as these events need to be processed even if the broker is not controller. if we do this, we can solve the race condition without having the additional `isshuttingdown`. and we can also make the change that jun suggested previously without worrying about `reelect` event being removed from the controller event queue. what do you think?",0,0,0,0.9813530445098876,0.9922682642936708,0.975830614566803,0.0,accept,unanimous_agreement
417217971,5101,"thanks for the comment. you brought up a very good point. we need to differentiate between events that need to be processed by the active controller and events that need to be processed by every broker. because `controllermovedexception` only indicates active controller role switched, we shouldn't try to stop processing events in the latter category. from the implementation point of view, we can just mark the controller as inactive and resign without touching the event queue at all instead of doing a conditional clear operation on the queue. the reason why we want to clear the queue before is that if controller has moved, we don't want the event thread to waste cycles on unnecessary events and want it to get to `reelect` as soon as possible by clearing the potential backlog. since we already have logic like `if (!isactive) return` to guard against events related to active controller, we just need to preempt the ""mark inactive and resign"" operation when `controllermovedexception` happens. this will ensure correctness as well as simplicity and resolve the event queue backlog with small overhead (most cases we just dequeue controller event and do a boolean check)",1,1,1,0.964923858642578,0.9723942875862122,0.98752623796463,1.0,accept,unanimous_agreement
417919248,5101,thanks for the review. i have updated the pr to address your comments. appreciated if you can take a look again when available.,1,1,1,0.9610351920127868,0.9762879610061646,0.9813151955604552,1.0,accept,unanimous_agreement
418243421,5101,"perf testing has finished. overall, there is no significant overhead after fencing zookeeper updates for common controller events (controllerfailover, controlledshutdown, brokerstartup, preferredreplicaleaderelection). the environment: - 5 node zookeeper and 5 broker kafka cluster with brokers on different racks - 2,000 topics each with 50 partitions and rf = 1 - 10k single partition topics with rf=1 + 10k 3 partitions topics with rf=2 here are the results: **1. controller fails over** trunk (`bf0675`) - run 1 [code block] trunk (`bf0675`) - run 2 [code block] kafka-6082 - run 1 [code block] kafka-6082 - run 2 [code block] trunk avg = ~11s kafka-6082 avg = ~9s **2. preferred replica leader election** trunk (`bf0675`) - ~2.4k leadership movements [code block] kafka-6082 - ~2.4k leadership movements [code block] trunk = 491ms kafka-6082 = 544ms trunk (`bf0675`) - ~4.8k leadership movements [code block] kafka-6082 - broker 1497 (~4.8k leader) [code block] trunk = 1.104s kafka-6082 = 1.084s trunk (`bf0675`) - ~6k leadership movements [code block] kafka-6082 - broker 1494 (~6k leadership movements) [code block] trunk = 1.339s kafka-6082 = 1.152s **3. controlled shutdown** trunk (`bf0675`) - run 1 [code block] trunk (`bf0675`) - run 2 [code block] kafka-6082 - run 1 [code block] kafka-6082 - run 2 [code block] trunk avg = 3.63s kafka-6082 avg = 3.65s **4. broker start** trunk (`bf0675`) - run 1 [code block] trunk (`bf0675`) - run 2 [code block] kafka-6082 - run 1 [code block] kafka-6082 - run 2 [code block] trunk avg = 4.67s kafka-6082 avg = 2.21s",0,0,0,0.9708885550498962,0.9874609112739564,0.980774462223053,0.0,accept,unanimous_agreement
418570274,5101,: thanks for the perf results. they look good.,1,1,1,0.9753423929214478,0.958546221256256,0.992179274559021,1.0,accept,unanimous_agreement
419242869,5101,thanks so much for all of your comments and suggestions.,1,1,1,0.9205859303474426,0.8703120350837708,0.9661015868186952,1.0,accept,unanimous_agreement
419568907,5101,thanks much for the patch . lgtm. merged to trunk.,1,1,1,0.9662582874298096,0.977447748184204,0.9799053072929382,1.0,accept,unanimous_agreement
419572127,5101,thanks a lot for the reviews.,1,1,1,0.8218941688537598,0.7353101372718811,0.6895106434822083,1.0,accept,unanimous_agreement
419750329,5101,"did we check the performance impact of this change? if so, it would be great to include the details in the pr description.",0,0,1,0.8768279552459717,0.921702206134796,0.8407052159309387,0.0,accept,majority_agreement
420325390,5101,: there were some perf results. see the above comment on sep. 4.,0,0,0,0.9868256449699402,0.9855619072914124,0.9891608357429504,0.0,accept,unanimous_agreement
420347877,5101,thanks . that's great. i suggest adding a summary of the results to the pr description.,1,1,1,0.9845625758171082,0.9936994314193726,0.9931208491325378,1.0,accept,unanimous_agreement
420371333,5101,thanks for the suggestion. i have updated the description to include the perf test.,1,1,1,0.9219471216201782,0.6509097218513489,0.9460740685462952,1.0,accept,unanimous_agreement
472781135,6295,- checked new topic creation in remote cluster - checked acl syncing the code looks good and ideally we get some tests for the above as well. as part of the above check i'm wondering if it makes sense to add an integration test plan to the kip (apologies if i missed) since some of this functionality needs full e2e testing.,1,0,1,0.5917395949363708,0.9529849290847778,0.6832236051559448,1.0,accept,majority_agreement
493280895,6295,thanks for taking a look. i could see min.insync.replicas causing problems if sync'd. i'll add min.insync.replicas to config.properties.blacklist by default to avoid that problem.,1,1,1,0.9016037583351136,0.8294550180435181,0.9730926156044006,1.0,accept,unanimous_agreement
498269358,6295,"any idea when this pr will be merged? we are thinking to use mirrormaker, but depending when this pr is merged, we could wait for it instead of using the current mirrormaker v1.0. thanks",1,1,1,0.9815493226051332,0.6830272674560547,0.9805296659469604,1.0,accept,unanimous_agreement
498439837,6295,thanks for your interest . current plan is with the 2.4 release. the best way to make sure this happens is to contribute reviews!,1,1,1,0.9695479273796082,0.981767475605011,0.9840540289878844,1.0,accept,unanimous_agreement
500217111,6295,does it make sense to mention this in the kip or document it elsewhere? this seems important enough that it ought to jump out at people the first time they deploy mm2,0,0,0,0.9741679430007936,0.9889712929725648,0.9885311126708984,0.0,accept,unanimous_agreement
503503275,6295,did some more testing and happy to see the progress from the last comments. there are some usability issues still but we can probably address them in a separate round. at this point it would be good if committers had a look and reviewed. thanks.,1,1,1,0.9771711826324464,0.9870936274528505,0.9939897060394288,1.0,accept,unanimous_agreement
505123732,6295,"i have been testing with this for a while and messages are being copied. however, i noticed that record headers are not copying correctly. it appears that the record header data received by the mirrored broker data is base64 encoded. by default, connect uses simpleheaderconverter. i noticed that the mirrormakerconfig is not setting a header converter. i think it should default the header converter to the byte array converter just like the key and value converters. clients: kafka 0.11 based clients. brokers: 2.1 mm2.0 - latest branch manually defaulting the convert via configuration resolves the issue.",0,0,0,0.9031833410263062,0.9574108719825744,0.8931597471237183,0.0,accept,unanimous_agreement
506736259,6295,"what would be the proper way to monitor each replication lag with this new architecture, the offsets of upstreams topics being stored in the kafka backing store topic of kafka connect ?",0,0,0,0.985893964767456,0.9948200583457948,0.993004024028778,0.0,accept,unanimous_agreement
506793879,6295,"this is an interesting question. the kip does not emit offset lag metrics -- only ""replication latency"", which is a measure of time. offset lag is measured from the perspective of a particular consumer, and it would be possible to supply an interceptor to mm2's consumers in order to get this measure, if you like. looking at the connect offsets is an interesting idea as well. that said, i think latency is a better metric to monitor wrt replication, as there is no good way to aggregate offset lag over a bunch of unrelated topic-partitions. offset lag is more meaningful when looking at a particular consumer and a particular topic.",0,0,1,0.6674689650535583,0.7517428994178772,0.6588729023933411,0.0,accept,majority_agreement
517308971,6295,"hey, i use kubernettes to spin up 2 kafka clusters locally (3 brokers each). and then i run mm2 locally as well to sync topic messages. when i send a message to source topic, source.topic is created in the sink cluster, but the message is not delivered. an exception is thrown in the console (see below). when i restart mm2, the message arrives in the source.topic. does anyone recognize this error? moreover, when i move one of the kafka cluster to a different machine, everything works again. i tried to increase network/io threads in the local setup, it still doesn't solve the issue. [2019-08-01 10:18:29,033] info workersourcetask{id=mirrorsourceconnector-0} flushing 21 outstanding messages for offset commit (org.apache.kafka.connect.runtime.workersourcetask:418) [2019-08-01 10:18:29,072] info workersourcetask{id=mirrorsourceconnector-0} finished commitoffsets successfully in 39 ms (org.apache.kafka.connect.runtime.workersourcetask:500) [2019-08-01 10:18:29,072] error workersourcetask{id=mirrorsourceconnector-0} task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.workertask:179) java.lang.nullpointerexception at org.apache.kafka.connect.mirror.mirrorsourcetask.poll(mirrorsourcetask.java:140) at org.apache.kafka.connect.runtime.workersourcetask.poll(workersourcetask.java:245) at org.apache.kafka.connect.runtime.workersourcetask.execute(workersourcetask.java:221) at org.apache.kafka.connect.runtime.workertask.dorun(workertask.java:177) at org.apache.kafka.connect.runtime.workertask.run(workertask.java:227) at java.base/java.util.concurrent.executors$runnableadapter.call(executors.java:515) at java.base/java.util.concurrent.futuretask.run(futuretask.java:264) at java.base/java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128) at java.base/java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628) at java.base/java.lang.thread.run(thread.java:834) [2019-08-01 10:18:29,073] error workersourcetask{id=mirrorsourceconnector-0} task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.workertask:180) [2019-08-01 10:18:29,073] info [producer clientid=connector-producer-mirrorsourceconnector-0] closing the kafka producer with timeoutmillis = 30000 ms. (org.apache.kafka.clients.producer.kafkaproducer:1153) [2019-08-01 10:18:29,080] info [producer clientid=producer-7] closing the kafka producer with timeoutmillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.kafkaproducer:1153) [2019-08-01 10:18:44,602] info [worker clientid=connect-2, groupid=sec-mm2] attempt to heartbeat failed since coordinator localhost:31000 (id: 2147483647 rack: null) is either not started or not valid. (org.apache.kafka.clients.consumer.internals.abstractcoordinator:931) [2019-08-01 10:18:44,602] info [worker clientid=connect-2, groupid=sec-mm2] group coordinator localhost:31000 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.abstractcoordinator:780) [2019-08-01 10:18:44,612] info [worker clientid=connect-2, groupid=sec-mm2] discovered group coordinator localhost:31002 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.abstractcoordinator:728)",0,0,0,0.9047016501426696,0.7698917388916016,0.9416871070861816,0.0,accept,unanimous_agreement
519080763,6295,"upon startup, i'm also getting this exception: [code block]",0,0,0,0.9847102761268616,0.9440364837646484,0.9788455963134766,0.0,accept,unanimous_agreement
520463091,6295,i can replicate clusters when running as dedicated mirrormaker cluster but i'm having issues running on top of kafka connect (no errors but topics are not replicated). is that supposed to work at the moment? imho kafka connect support is key. a rest api to manage connectors is much better than having to start processes on remote hosts,0,0,1,0.9420122504234314,0.978407084941864,0.4962358474731445,0.0,accept,majority_agreement
520863798,6295,"there also seem to be an issue with metrics. after starting mm2 in dedicated mirrormaker cluster mode, i only have the following metrics registered: according to [a link], there should also be `mirrorcheckpointconnector`. looking at different values, i can strange behaviours to like `record-count` going up and down even with no traffic!",-1,0,-1,0.7056165337562561,0.7452064156532288,0.8784656524658203,-1.0,accept,majority_agreement
522074048,6295,"the connectors will work with connect just fine, but the configuration properties differ slightly from the mm2.properties file. try something like: [code block] n.b. properties like ""primary->backup.topics"" are understood by the mm2 driver but not the underlying connectors. here's a working configuration as returned from the connect rest api during mirrorconnectorsintegrationtest (new!): [code block] most of those properties are redundant, but `mirrormakerconfig.connectorbaseconfig()` fills them in anyway.",0,0,0,0.9704735279083252,0.9869322180747986,0.9868060350418092,0.0,accept,unanimous_agreement
522078873,6295,"mirrorcheckpointconnector won't emit any metrics unless it is actively producing checkpoints, which won't happen until there is a whitelisted consumer group subscribed to a whitelisted topic. by default kafka-console-consumer groups are blacklisted, so you need to set a specific --group-id to see this happen. record-count is windowed, so it resets after a short while. might make sense to change that. also, even if there is no data to replicate, mirrorsourceconnector always replicates heartbeats, so there is always _some_ replication happening.",0,0,0,0.9777894020080566,0.9854971766471864,0.9905614256858826,0.0,accept,unanimous_agreement
525802843,6295,"with 2.4.0 approching, do you think the pr is ready to be considered for that release? or do you still have work planned? if it's ready, let's grab the attention of committers asap",0,0,0,0.9330685138702391,0.9511061906814576,0.9462067484855652,0.0,accept,unanimous_agreement
526404615,6295,"yes, we should get some committers to take a look.",0,0,0,0.975597620010376,0.9832634329795836,0.9868423342704772,0.0,accept,unanimous_agreement
531890077,6295,"thanks, for the contribution and patience in getting reviewed and thanks to everyone for reviewing the code. i went through the code a few times to get a better understanding, overall lgtm. i don't have any further comments on the code itself. i am running a few tests on our clusters and will report back my findings.",1,1,1,0.963930606842041,0.9842740893363952,0.9885450005531312,1.0,accept,unanimous_agreement
533214612,6295,thanks for your patience and following upon the pr do you want to take a look. we have 4 reviewers approved already. so let us know your thoughts otherwise i would like to merge this in.,1,1,1,0.9168368577957152,0.9570533633232116,0.9767003655433656,1.0,accept,unanimous_agreement
533322760,6295,: i will make another pass of the pr by early next week.,0,0,0,0.9412001371383668,0.9788001775741576,0.985600769519806,0.0,accept,unanimous_agreement
533557824,6295,"since this is a major feature, we need to add few system tests. it is good have system tests before code freeze. for tracking purpose, can you please create jiras for system tests and docs?",0,0,1,0.7675907611846924,0.8921273946762085,0.9002697467803955,0.0,accept,majority_agreement
533628482,6295,"i've created kafka-8929 and kafka-8930 to track tests and docs. i will update the javadocs as part of this pr, but i'll create a separate pr for the system tests. should be done next week.",0,0,0,0.981325387954712,0.9900158047676086,0.945153534412384,0.0,accept,unanimous_agreement
535781371,6295,": also, the kip includes both a mirrorsourceconnector and a mirrorsinkconnector, but the pr only has the former?",0,0,0,0.987318217754364,0.9946227073669434,0.9880004525184632,0.0,accept,unanimous_agreement
538051233,6295,"i've got ducktape tests implemented and will create a second pr for those, which we can merge before 2.4 code freeze. please note pr 6295 (this one) includes mirrorconnectorsintegrationtest, which uses connect's integration test framework. in particular, these cover replication, topic detection, task rebalances, offset translation, and consumer migration. the ducktape tests cover basic replication and bouncing the cluster. basically they are the same ducktape tests from legacy mirrormaker, just with mm2 dropped in. mirrorsinkconnector (the missing fourth connector) will come in a subsequent pr. the sink connector is not necessary for the driver to function so i've deprioritized it for this pr. there will be very little new code for the sink connector, as almost everything will be extracted from mirrorsourceconnector. also missing from the kip is the ""legacy mode"" script, which will eventually replace the existing mirror-maker.sh. both should land in 2.5, with legacy mm being deprecated as soon as 2.6. looks like some churn on master has caused a few failed builds, but once green this is ready to merge.",0,0,0,0.957613468170166,0.9770398139953612,0.9677455425262452,0.0,accept,unanimous_agreement
538107371,6295,looks like the deadline to make the 2.4 release is on friday. can you please take a look at the pr and see if there is anything else missing in merging it in for 2.4. cc,0,0,0,0.9360823035240172,0.9929579496383668,0.9811291098594666,0.0,accept,unanimous_agreement
538144648,6295,: there are still a couple of comments that haven't been addressed. the biggest one is on dealing with compacted topic for offset translation.,0,0,0,0.9806388020515442,0.9818879961967468,0.9816992282867432,0.0,accept,unanimous_agreement
538159605,6295,"sorry, i didn't see a few comments that were folded/hidden for some reason. hopefully i've addressed everything that would otherwise delay the merge.",-1,-1,-1,0.975866973400116,0.9881665110588074,0.9784316420555116,-1.0,accept,unanimous_agreement
538609224,6295,any update on test failures? please merge the pr once the jun's comments are addressed and we have green builds.,0,0,0,0.9868912100791932,0.9907243847846984,0.9949167966842652,0.0,accept,unanimous_agreement
538614075,6295,i traced the build failures to an npe from kip-507 committed yesterday. it is breaking mm2's and other connect integration tests. i'll fix here i guess.,0,0,0,0.9826236367225648,0.8539836406707764,0.975932002067566,0.0,accept,unanimous_agreement
538614836,6295,i fixed the npe from kip-507 -- let's see if we can get a green build now.,0,0,0,0.9782373309135436,0.9885380268096924,0.9857544898986816,0.0,accept,unanimous_agreement
538645845,6295,good to go!,1,1,1,0.9865218997001648,0.9865951538085938,0.9939682483673096,1.0,accept,unanimous_agreement
538650932,6295,", , : as mentioned above in one of the comments on `workersourcetask`, this pr also implements [a link]. i've edited the description of the pr to mention this. according to the [a link], kip-415 has not yet been approved, and per the [a link] the kip deadline was on sept 25. , what's required here?",0,0,0,0.9721413850784302,0.9916625022888184,0.9753021597862244,0.0,accept,unanimous_agreement
538669326,6295,": we can fold the changes in workersourcetask into this kip. ryan, could you update your kip wiki and send an email to the voting thread about the additional changes to make sure that no one objects to this? if kip-416 is completely subsumed by this, we can just cancel it. otherwise, kip-416 can cover the remaining part.",0,0,0,0.9531586170196532,0.9910972118377686,0.9905253648757936,0.0,accept,unanimous_agreement
538671064,6295,"thanks, . kip-416 is straightforward, limited to the one new method change we discussed (and agreed to), and required for kip-382, so pulling those changes into kip-382 makes sense to me.",1,1,1,0.9526948928833008,0.8948350548744202,0.9638603925704956,1.0,accept,unanimous_agreement
538830899,6295,"some of the comments mentioned here can be handled in a follow-up patch and can be part of minor release that will follow-up. given the no.of users that are interested and the number of reviews, we had it in the pr and also maintaining this big of patch as the trunk continues to evolve will be challenging. if you don't have any major concerns lets merge this in for 2.4 release and address any new comments in follow-up patch. cc",0,0,0,0.8536850214004517,0.9864631295204164,0.9614855051040648,0.0,accept,unanimous_agreement
538890813,6295,"thanks for the pr. merging the pr. lets address any issues in follow-up prs. pls raise jiras for any pending work (mirrorsinkconnector, legacy mode etc.) for next releases.",1,1,1,0.9132161140441896,0.9110808968544006,0.9676076769828796,1.0,accept,unanimous_agreement
603995709,6295,try to find jira of mirrorsinkconnector but search return nothing. can you please point me where i can find status of this work? thanks!,1,1,1,0.931891143321991,0.9885335564613342,0.931495726108551,1.0,accept,unanimous_agreement
603998987,6295,[a link] the primary jira for kips are linked from the kip document itself: [a link],0,0,0,0.9879939556121826,0.9885501265525818,0.994972825050354,0.0,accept,unanimous_agreement
604003001,6295,thanks! will comment in there,1,1,1,0.9635144472122192,0.9563689827919006,0.9908864498138428,1.0,accept,unanimous_agreement
620318837,6295,"hi - i just got done setting up mirror maker v2 where i work. it was successful, but the documentation doesn't match the code in various places. a couple of things that ""got me"" - - the jmx metrics in the documentation: it should actually be something like (pardon me, i'm using the jmx-prometheus exporter): `kafka.connect.mirror `. note how it's actually **kafka.connect.mirror** not **kafka.mirror.connect**, also it's mirrorsourceconnector, not mirrorsourceconnect. - in the documentation, there is a part about blacklisting groups: in the code, it is `public static final string groups_blacklist_default = ""console-consumer-.*, connect-.*, __.*"";`; here: [a link] - this got me because we're actually using some kafka connect stuff for other projects, but we're running mirror maker as it's own cluster. maybe you could blacklist only mirror maker's kafka connect connect- consumer group? - this one is kind of a nit-pick, but in remoteclusterutils.translateoffsets, the timeout parameter isn't really a timeout in the conventional sense. stepping further into the call that's made using timeout, `return client.remoteconsumeroffsets(consumergroupid, remoteclusteralias, timeout);` we can see that it's being used as a deadline, not a timeout. in mirrorclient's remoteconsumeroffsets method, it's being used as a timeout, and as a deadline. setting this too low caused me to miss some offsets because i didn't understand how it was being used. setting this absurdly high got me the results that i wanted. all in all a great product and vastly superior to mirror maker v1. thanks!",1,0,1,0.648928701877594,0.6135126948356628,0.6921836137771606,1.0,accept,majority_agreement
1004211004,6295,"i can't seem to find the new metrics mentioned in the kip. like -trelinski said above, i've tried both kafka.connect.mirror / kafka.mirror.connect and mirrorsourceconnector / mirrorsourceconnect and all combinations of them but still i don't see the mirror domain the respective mbeans. i installed jmxterm on my mirror-maker pod and this is what i got: domains: [code block] beans for connect: [code block] as you can see, no mentions of ""mirror"" of any kind anywhere. anyone able to help me understand what's going on?",0,0,0,0.7933135032653809,0.9140716195106506,0.8413029313087463,0.0,accept,unanimous_agreement
736700777,9485,"build successful in 18s 122 actionable tasks: 3 executed, 119 up-to-date docker exec ducker01 bash -c ""cd /opt/kafka-dev && ducktape --cluster-file /opt/kafka-dev/tests/docker/build/cluster.json ./tests/kafkatest/ --subset 0 --subsets 15"" traceback (most recent call last): file ""/usr/local/bin/ducktape"", line 8, in sys.exit(main()) file ""/usr/local/lib/python3.7/dist-packages/ducktape/command_line/main.py"", line 118, in main os.makedirs(consoledefaults.metadata_dir) file ""/usr/lib/python3.7/os.py"", line 211, in makedirs makedirs(head, exist_ok=exist_ok) file ""/usr/lib/python3.7/os.py"", line 221, in makedirs mkdir(name, mode) permissionerror: [errno 13] permission denied: '.ducktape' ducker-ak test failed thanks for the review . i've addressed all the previous comments. can you take another look at this pr? also, any idea how to resolve this ducktape permissionerror?",0,0,0,0.957356333732605,0.982929229736328,0.98056960105896,0.0,accept,unanimous_agreement
736701573,9485,6206tests1failures61ignored6m45.08sduration | 6206tests | 1failures | 61ignored | 6m45.08sduration | 99%successful -- | -- | -- | -- | -- | -- 6206tests | 1failures | 61ignored | 6m45.08sduration failed tests ignored tests packages classes kafkaproducertest. testinittransactiontimeout which is flaky,0,0,0,0.925119698047638,0.8650396466255188,0.7977332472801208,0.0,accept,unanimous_agreement
738942757,9485,"make a new aggregated index called resourceindex, which is a combination of ace, resourcetype, and patterntype. new benchmark looks much better: [a link] when `aclcount=100` and `resourcecount=200000`, in the worst case where every ""allow resource"" of a given ace has a dominant ""deny resource"" and the ""deny resource"" name is allow_string.substring(0, len-1), the time cost is 45.897 ± 4.748 ms/op",0,0,0,0.9710931181907654,0.9913215041160583,0.9902742505073548,0.0,accept,unanimous_agreement
740036778,9485,"[a link] benchmark with aclauthorizer::updatecache with aclcount=50 and resourcecount=200000, 200000 call to updatecache took ~59 seconds the cpu overhead is mainly on re-hashing (everytime when the resourcecache hits 75% of the capacity). but we can overlook it. the memory overhead is m = (aclcount * 8) * (resourcecount * 8) + unique(resourcecount) * (average_resource_name_length) bytes, since strings are cached in the constant string pool. when aclcount=50 and resourcecount=200000, m < 20mb.",0,0,0,0.9798392653465272,0.9930665493011476,0.9904513359069824,0.0,accept,unanimous_agreement
740056125,9485,"hi . thanks for the detailed review! i’ve addressed all comments you left. also, after some optimization on the `resourcecache`, the benchmark looks much better now. please take another look and let me know if there's anything we need to change. thanks.",1,1,1,0.986454963684082,0.9948549270629884,0.9954738020896912,1.0,accept,unanimous_agreement
740199487,9485,benchmark results run against trunk: [a link],0,0,0,0.9882400035858154,0.9915211200714112,0.9955599308013916,0.0,accept,unanimous_agreement
740501882,9485,"![a link] chart description (from left to right) 1. the performance comparison btw the trunk's and my branch's of `aclauthorizer#updatecache`. 2. the performance comparison btw the trunk's and my branch's of `aclauthorizer#authorize`. 3. the performance comparison btw my branch's `aclauthorizer#authorize` and `aclauthorizer#authorizebyresourcetype` 1 and 2 indicate that the newly added `resourcecache` doesn't add too much overhead to the time complexity. the time complexity increasing is constant (my branch doubled the hashmap add/remove operation). in chart 3, the horizontal axis ""percentage of dominant deny"" means the percentage of ""allow resource"" which has a ""deny resource"" added to the authorizer. the benchmark used in chart 3 has the ""dominant deny"" distributed evenly. define the average of resource name length is l, the expected ""allow resource"" in the ""allow resource set"" to iterate is e_a, the time complexity of my implementation of `authorizebyresourcetype` is l * e_a. how to compute e_a? define the ""percentage of dominant deny"" as p. every resource in the ""allow resource"" set has the probability (100-p)/100 to be allowed by the authorizer. so expected_elements_iterated_in_the_allow_resource_set. 1. the time complexity of `aclauthorizer#authorize` is irrelevant to the ""percentage of dominant deny"" 2. if we treat l as a constant, the time complexity of `aclauthorizer#authorizebyresourcetype` is only relevant to e_a, where e_a = 1/[(100-p)/100] = 100/(100-p), which is a hyperbola. chart 3 can confirm this. ![a link] from the benchmark, when p = 99.99, if ""dominant deny"" distributed evenly, authorizebyresourcetype only took 2.5ms. in the worst case, if ""dominant deny"" distributed unevenly or p = 100, the api will take ~40ms, but this is very unlikely.",0,0,0,0.9610838294029236,0.9791991114616394,0.8607653975486755,0.0,accept,unanimous_agreement
743884225,9485,"thanks for another round of detailed review. i've 1. add the corner case checks in aclauthorizer when there's no deny acl binding in aclauthorizer. 2. re-write the aclwrapper logic to use the `baseauthorizer` to give the information about if we should ""allow everyone if no acl found"" in the configure() method. 3. fix the ""candenyall"" logic in aclwrapper. 4. revert the unnecessary changes in the tests and benchmark. there're a few outstanding comments that need further discussion as i replied, mostly the nit changes. please let me know what you think about those. thanks.",1,1,1,0.9507561326026917,0.9625304937362672,0.9768021702766418,1.0,accept,unanimous_agreement
744493191,9485,"shouldupgradefromeosalphatoeosbeta is flaky. other than this, all tests passed.",0,-1,0,0.7545359134674072,0.6146125197410583,0.9707648158073424,0.0,accept,majority_agreement
745712214,9485,"thanks, for the review. according to the comments, i've 1. support super user in authorizer, aclauthorizer, and authorizerwrapper. add tests to the three corresponding testing classes. 2. rename resourceindex to resourcetypekey, and make it an inner class of aclauthorizer. 3. cleaned the tear-off logic in the unit tests since zk starts in setup() indeed. 4. addressed other nit suggestions. please let me know if anything else needs to be changed, especially if the super user part looks good to you. thank you.",1,1,1,0.971875250339508,0.9910224676132202,0.9897631406784058,1.0,accept,unanimous_agreement
747116007,9485,thanks for the nit and test structure suggestions. i've adopted those and re-struct the test classes. please let me know if we are good to merge now.,1,1,1,0.9031328558921814,0.9828456044197084,0.963813304901123,1.0,accept,unanimous_agreement
747881257,9485,thanks for going through the pr once again. i really appreciate your time spent on this work. i've addressed all the new comments. please let me know if we're good to go after the pr built.,1,1,1,0.9777337908744812,0.993778109550476,0.9911417961120604,1.0,accept,unanimous_agreement
748190567,9485,"thanks for your patience with this pr, builds look good, merging to trunk.",1,1,1,0.9690256714820862,0.9867513179779052,0.9889973402023317,1.0,accept,unanimous_agreement
748242438,9485,thank you,1,1,1,0.7400884628295898,0.6702879667282104,0.9420937299728394,1.0,accept,unanimous_agreement
756982595,9485,"i don't see any updates to the release notes. unless i missed it, we should add one since authorizers should implement the new method we introduced.",0,0,0,0.9768930673599244,0.9799925684928894,0.9729089736938475,0.0,accept,unanimous_agreement
756985555,9485,sure. could someone point me to the release note page so i can edit it?,0,0,0,0.9795936346054076,0.991647481918335,0.9853901863098145,0.0,accept,unanimous_agreement
756987814,9485,there you go: [a link],0,0,0,0.978078544139862,0.9828223586082458,0.9638202786445618,0.0,accept,unanimous_agreement
247191884,1776,"i left the following comment on simplerate earlier. having two different rates is going to make it harder for developers to decide which one to use. if this is strictly better than rate, perhaps we should just change rate.windowsize(). if this is just for testing, perhaps we can create simplerate in test?",0,0,0,0.9828109741210938,0.988712191581726,0.9833690524101256,0.0,accept,unanimous_agreement
247520427,1776,thanks for the patch. lgtm. we can address the remaining minor issues in a followup jira.,1,1,1,0.9327239990234376,0.962758481502533,0.9660866856575012,1.0,accept,unanimous_agreement
1789830091,14690,"i am a bear of very little brain, so the chained `future` mechanism is simultaneously interesting, hard to follow, but ultimately useful. my one concern is that we don't allow those `future`s to be `complete`d by the application thread, since we don't want it to run all the chained logic. that would be bad :grinning_squinting_face:",-1,-1,1,0.9779191613197328,0.9755586981773376,0.6222511529922485,-1.0,accept,majority_agreement
1793014796,14690,"hi and - thank you for putting this pr out, much appreciate for the effort. i have concern about the extensive use of callback here because it is kind of against the original design goal, i.e. making background thread operate in a linear fashion. with the main thread involved, i'm also seeing the risk of multithreading access to the background thread. here are my questions: do we need to use the completable future to facilitate the rebalance callback completion cycle? my original thought was to use queues to relay the callback invocation and completion, because i see that in some cases, it is unclear to me which thread is completing which callback. to avoid the confusion all together, i think we should try to use the queue as much as possible. also, it make the program to operate more linearly. if we don't use completable future for the callback, we just need to do two things 1. if the listen ispresent, we just need to enqueue an event to the backgroundeventqueue and wait for the consumer to poll. once the main thread completes the callback, it enqueues an ack event for background thread to consume, to transition to the next state. 2. if the listener is not presented. then we directly invoke the next state transition method. also, it seems like `reconciliationresult.whencomplete((result, error) -> {` this is completed by the application thread no? i think we really want the background thread to perform in a single-threading fashion, i'm afraid using callback might result in a some sort of race condition.",1,1,1,0.943099081516266,0.9933995008468628,0.981431484222412,1.0,accept,unanimous_agreement
1793496177,14690,"hi , thanks for you comment. it does raise an interesting point. my view is that completablefuture provides a really nice abstraction for asynchronous processing. using it in this pr is entirely appropriate. however, i also take your overall point. the threading for completablefuture depends upon the details of how it is used. if a future is completed on the ""wrong"" thread, then processing which is dependent upon completion of that future will also execute on the same ""wrong"" thread. if one of the more complex methods such as `supplyasync` is used, it runs on a thread from a worker pool. i don't think we're using that pattern here. if we actually need to achieve signalling back to a specific thread, we could use the cf to enqueue an event. that compromise seems pretty sensible to me, but i'm not sure whether we need this with the pr as written. i need to re-read the code before i'll be certain.",1,1,1,0.9762791395187378,0.9874157309532166,0.9876530170440674,1.0,accept,unanimous_agreement
1793538245,14690,"hi - thank you for your inputs here, your point is absolutely valid in terms of the abstraction it provides. my main concern is the rebalance callback invocation. maybe i misunderstood the pr, but i think the state transition in the whencomplete can be completed by the main thread if the user supplies a callback, unless the callback is not supplied. i will double-check the syntax. could you be more specific about `using cf to enqueue an event`? do you mean by enqueuing a callback completion signal to some background thread queue, to notify the thread to perform the subsequent action?",1,1,1,0.9639543294906616,0.8562720417976379,0.9730520844459534,1.0,accept,unanimous_agreement
1793877944,14690,i meant that the code which runs in the `completablefuture.whencomplete()` could enqueue an action for the background thread to ensure that the main logic all runs in the background thread.,0,0,0,0.9855377078056335,0.9948104619979858,0.9918556213378906,0.0,accept,unanimous_agreement
1793892733,14690,"- thanks, i think that's what i have in mind as well.",1,1,1,0.8009534478187561,0.8571867942810059,0.9250828623771667,1.0,accept,unanimous_agreement
1794951522,14690,"comment regarding the async reconciliation process and callback execution. i get your concerns , but this pr does not include callback execution for now, the membership manager triggers the callbacks and needs to know when they complete. i expect that will follow in a separate pr based on events as mentioned. without having gotten into the implementation details yet, i expect that the membershipmanager (background thread) will enqueue an app event to execute the callbacks in the app thread, and when that app thread completes the callbacks, it will enqueue a background event to notify the manager about the completion (btw, nice here again that according to the protocol there will be only one assignment being reconciled at a time ). the async nature of the membership manager in this pr it is not only due to the callbacks. the reconciliation process handles 3 main async operations: metadata, commit, callbacks (triggering and notification when completed, no execution). by basing the them on completable futures we ensure that the background thread continues it operations while there is a reconciliation in process. note on the reconciliation not being time bounded on the client. the reasoning is that the time boundaries are set by the broker, that keeps a rebalance timer. if the reconciliation takes longer than allowed (ex. stuck in any of the 3 async operations), the expectation is that the broker will re-assign the partitions and kick the member out of the group. so from the client side we just care about triggering the async reconciliation, making sure we keep sending hb and processing responses (will let us know if kicked-out), and make sure that when reconciliation completes we check if it is still relevant.",0,0,1,0.8537088632583618,0.540814220905304,0.7602457404136658,0.0,accept,majority_agreement
1818883304,14690,i just merged trunk to fix conflicts. we can merge it when the build completes.,0,0,0,0.9883713126182556,0.9930342435836792,0.9939727187156676,0.0,accept,unanimous_agreement
1830640924,14690,follow-up pr [a link],0,0,0,0.9852991104125975,0.970458209514618,0.9959075450897216,0.0,accept,unanimous_agreement
276160087,2466,:thumbs_up:,0,0,1,0.8380307555198669,0.9771975874900818,0.9533231854438782,0.0,accept,majority_agreement
276214966,2466,one more thing. please add some tests!,0,0,0,0.8560066819190979,0.6584023237228394,0.9919323325157166,0.0,accept,unanimous_agreement
276293619,2466,"i don't see a good reason why you ""wanted to get rid of"" this line... just leave it, as i suggested. furthermore, semantically the timestamp extractor does not belong to the context, and imho it would not be a good design/abstraction adding it their.",0,0,0,0.8279556035995483,0.725082278251648,0.8960798978805542,0.0,accept,unanimous_agreement
276335874,2466,sorry it is my bad. i realized this short after writing comment and removed comment immediately.,-1,-1,-1,0.9896332025527954,0.993605613708496,0.9933009147644044,-1.0,accept,unanimous_agreement
276491890,2466,please review,0,0,0,0.978767156600952,0.9374555349349976,0.9914347529411316,0.0,accept,unanimous_agreement
277006630,2466,"- i added extra test for `kstreambuilder.addsource` with `timestampextract` arguments. not sure it is much different from `topologybuilder.addsource` test. - in `sourcenode` class, not all variables can be final as they are initialized in `init()` method. - in `streamtask` 121, the `sourcenode` was getting `null` if the source is defined with `pattern`. i deleted ""pattern [ ]"" string from `sourcenode`name (it was initialized like ""pattern [ "" + regex + ""]"" ) so that i can compare with `topic` names. i could not get the reason to insert new overloaded methods further below. the next method below is the most generic one (with most arguments) which is [code block]",0,0,0,0.9800300002098083,0.9938011765480042,0.9852460026741028,0.0,accept,unanimous_agreement
277353548,2466,would you mind to review again please?,0,0,0,0.9694995284080504,0.9888245463371276,0.99052095413208,0.0,accept,unanimous_agreement
277392859,2466,"would you mind fixing this typo, too: [a link]",0,0,0,0.9717003107070924,0.9816712737083436,0.9913964867591858,0.0,accept,unanimous_agreement
277395085,2466,there is a build error. please fix before we can do a review.,0,0,0,0.9380414485931396,0.9554069638252258,0.946325421333313,0.0,accept,unanimous_agreement
277395661,2466,"yes, some pr changed the (test) classes related with this pr so. i will fix them.",0,0,0,0.9797576069831848,0.9817281365394592,0.9917848110198976,0.0,accept,unanimous_agreement
277395841,2466,"one more general comment. i would be better if you would not squash your commits on every update -- this can simplify reviewing the pr. and when the pr get's merged, all commits get squashed automatically anyway.",0,0,0,0.963866889476776,0.9607067704200744,0.9858353734016418,0.0,accept,unanimous_agreement
277471760,2466,i fixed the errors,0,0,0,0.9817678928375244,0.6560403108596802,0.9886513948440552,0.0,accept,unanimous_agreement
278685807,2466,i had to rebase because of conflicts in different commits,0,0,0,0.8551483154296875,0.8995133638381958,0.676482081413269,0.0,accept,unanimous_agreement
278729643,2466,anyof call for second review,0,0,0,0.9816080927848816,0.9864004850387572,0.9906274080276488,0.0,accept,unanimous_agreement
278970132,2466,i did changes,0,0,0,0.9817859530448914,0.9066819548606871,0.983963429927826,0.0,accept,unanimous_agreement
279079856,2466,"i just realized, that we do have a kip for this. i am very sorry that i missed to mention this from the beginning on. are you familiar with the kip process? would you like to do the kip? see [a link]",-1,-1,-1,0.9844026565551758,0.9902737736701964,0.9923419952392578,-1.0,accept,unanimous_agreement
279081152,2466,"np. in general, i am familiar with kip process so is there some existing kip related with this pr (if yes, can you provide the number) or should i create kip for this pr and discuss in mailing list?",0,0,0,0.9734209775924684,0.9630481600761414,0.9929748773574828,0.0,accept,unanimous_agreement
279091209,2466,"we don't have a kip. please, use the template to create a new kip page, add it to the table ""kips under discussion"" (and to kafka streams page in the wiki) and start a discuss thread on the dev mailing list. thanks a lot!",1,1,1,0.99027681350708,0.9943249821662904,0.9920143485069276,1.0,accept,unanimous_agreement
279102611,2466,i don't have `create` button on my confluence page ([a link] do i have to ask somebody to give me authorization to create kip?,0,0,0,0.9342837929725648,0.9541795253753662,0.9840264320373536,0.0,accept,unanimous_agreement
279119214,2466,can you give write access to to the kafka wiki so he can prepare the kip ?,0,0,0,0.9882519841194152,0.9855157732963562,0.9953492283821106,0.0,accept,unanimous_agreement
279120938,2466,what is your apache id?,0,0,0,0.987240731716156,0.991853415966034,0.9943608641624452,0.0,accept,unanimous_agreement
279188872,2466,jeyhun.karimov see link in jeyhun's comment :),1,1,1,0.8443441390991211,0.8203299641609192,0.945999801158905,1.0,accept,unanimous_agreement
279241592,2466,oh right. done.,0,0,0,0.8657759428024292,0.7992722988128662,0.923977792263031,0.0,accept,unanimous_agreement
281548260,2466,"ignore my last comment (deleted it already)... i did update the jira, but it does not require a change of this pr. you might want to sent another reminder to dev list about the kip. ask for further feedback and if nodoby response, you might want to start the vote thread.",0,0,0,0.9657832384109496,0.979107677936554,0.9811756014823914,0.0,accept,unanimous_agreement
281740050,2466,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
281839301,2466,the test errors are a little weird. the test that fails is not part of you current pr branch but was added to `trunk` later on. can you please rebase to `trunk` -- i hope this fixed the issue. not sure why it occurred in the first place.,-1,-1,-1,0.9820871949195862,0.9713729619979858,0.9696875810623168,-1.0,accept,unanimous_agreement
282328998,2466,"the tests will fail when there are no source nodes defined in the test topology. also, if the defined source nodes are not related with partitions, the tests will also fail due to the nullpointerexception in `streamtask` 125: [code block] maybe we should add extra check specifically for that issue, when validating the topology.",0,0,0,0.9772367477416992,0.9939140677452089,0.9934937357902528,0.0,accept,unanimous_agreement
282346689,2466,"i think that's fine -- for tests it would be a testing error to not add a source. not sure if we have a proper test for real topologies to guard against it (only required for papi) -- for kstreambuilder it cannot happen. maybe `topologybuilder` should contain a test that ensures that the dag is connected and ""well-formed"" ? but i actually think, we would not even generate the `processortopology` in the first place -- we start building it from the sources, and if no source was added, the whole `processortopology` would be empty. this would also be a testing error only. for real topologies, if there are not partitions, we would not create a task for the `sourcenode` in the first place and thus, this code should never be executed. \cc (does this make sense?)",0,0,0,0.8334208726882935,0.9876254200935364,0.9502052068710328,0.0,accept,unanimous_agreement
282424079,2466,"sorry i had to rebase again, because of conflicts.",-1,-1,-1,0.9864707589149476,0.9916935563087464,0.9917635917663574,-1.0,accept,unanimous_agreement
282426549,2466,"no worries for rebasing, you actually don't need to squash previous commit (but i know that rebasing is simple if you have a single commit). however, you could squash and rebase before you do the pr update and put a new commit with the change on top :)",1,1,1,0.8887901902198792,0.9873055219650269,0.9870484471321106,1.0,accept,unanimous_agreement
288674025,2466,"sorry, i had to rebase again as there were a lot of conflicts among my commits and among mine and other commits. i deprecated [code block] for test classes, i replaced the deprecated variables with new ones. for non-test classes, i put an if condition to check both of the variables.",-1,-1,-1,0.9800838232040404,0.9834429621696472,0.9783592820167542,-1.0,accept,unanimous_agreement
289434341,2466,can you please check?,0,0,0,0.9800094962120056,0.9889206886291504,0.99441659450531,0.0,accept,unanimous_agreement
297880899,2466,can you rebase once again -- we merged a few conflicting pr today. sorry for that. call for review,-1,-1,-1,0.9901676177978516,0.9903860092163086,0.994153082370758,-1.0,accept,unanimous_agreement
298576227,2466,could we get this in please? thanks.,1,1,1,0.7108194828033447,0.8448656797409058,0.9366081357002258,1.0,accept,unanimous_agreement
490292130,6694,ping for review,0,0,0,0.9843391180038452,0.9702709913253784,0.987627387046814,0.0,accept,unanimous_agreement
490298156,6694,thanks for the pr! i agree this seems like a straightforward patch but i'm wondering if we shouldn't try and think through the eos case a bit more? or is there really no way to safely cover it as well?,1,1,1,0.9781784415245056,0.9890350103378296,0.9901638627052308,1.0,accept,unanimous_agreement
490311761,6694,"hi thanks for reviewing! i was planning on attacking the eos case. but as you could guess from the code, trying to retrieve the metadata committed is not as simple as in the non eos case. i was hoping for some input on that. so some small amount of advice is greatly appreciated. :)",1,1,1,0.9933644533157348,0.9961389899253844,0.9969078898429872,1.0,accept,unanimous_agreement
490746569,6694,"oh, just found out something. regardless if it is eos case or not, calling [code block] should be sufficient. so there shouldn't be any big problems with this. i will try to add a test case. :)",1,1,1,0.9600103497505188,0.990927755832672,0.9956921935081482,1.0,accept,unanimous_agreement
490750108,6694,"that sounds reasonable. looks like the build failed on checkstyle, can you try running it? +1 on adding a test case(s)",0,0,0,0.6282529234886169,0.6085140109062195,0.5416786670684814,0.0,accept,unanimous_agreement
491130846,6694,"alright, done. added a test case as well. would be good if you could take a look. :)",1,1,1,0.9883140325546264,0.9950773119926452,0.9962604641914368,1.0,accept,unanimous_agreement
493146367,6694,pinging and for review,0,0,0,0.9849753975868224,0.9630190134048462,0.9850538372993468,0.0,accept,unanimous_agreement
493695650,6694,"oh sorry, my bad. underestimated the scope of the pr. sorry for pinging you guys. will dig some more.",-1,-1,-1,0.9908795356750488,0.9934300780296326,0.9949913620948792,-1.0,accept,unanimous_agreement
497908544,6694,cc i have rebased the pr. want to take a look?,0,0,0,0.9809973239898682,0.9865822792053224,0.9939981698989868,0.0,accept,unanimous_agreement
498844777,6694,you mind taking a look?,0,0,0,0.9734762907028198,0.9811892509460448,0.9808496832847596,0.0,accept,unanimous_agreement
509822799,6694,"ok, so i added a test to try and see if we can handle the eos enabled scenario and it doesn't appear to be that straightforward (since the test fails). we either have to mess around with the [code block] config or think of something else. you could see the test failure in jenkins, and the nullpointerexception indicates that we cannot retrieve the timestamp from consumer when eos is enabled (probably since we used producer's commit api instead of consumer's).",0,0,0,0.9795573949813844,0.9903262257575988,0.9811025261878968,0.0,accept,unanimous_agreement
509877987,6694,"there is a question relating to timestampextractor and its usage that has been bugging me a little. i have added a [code block] instance field to recordqueue, and then we use that as the [code block] input argument. is that the right way to use it? after all, from what i could understand of timestampextractor, the previous timestamp should be the previous _record_'s timestamp rather than the max timestamp seen so far.",-1,0,-1,0.8038537502288818,0.7177469730377197,0.8655877113342285,-1.0,accept,majority_agreement
510693180,6694,you want to check this out?,0,0,0,0.9820418953895568,0.9911772012710572,0.9346608519554138,0.0,accept,unanimous_agreement
511595872,6694,might want some of your input.,0,0,0,0.9858373999595642,0.9811688661575316,0.970781147480011,0.0,accept,unanimous_agreement
512005573,6694,with regard to `previoustime` it's actually a miss leading name. there is already a pr to clean this up: [a link] -- it should be called `partitiontime`.,0,0,0,0.9882105588912964,0.993124783039093,0.991663098335266,0.0,accept,unanimous_agreement
512015515,6694,"the test result are not available any longer. however, looking into the test code, i assume that the issue is, that it is a unit test. for this case, the commit of a producer does not make it to the consumer (because there is no broker, and the consumer is mocked). hence, we need to mock that the consumer is returning to correct metadata -- i guess, the test setup code will need to call `mockconsumer#commitsync(map offsets)` to do this. however, we also need to have an integration test to verify that the producer committed offsets make it to the consumer, too. i think we need a completely new test (maybe called `partitiontimeintegrationtest`) that uses an `embeddedkafkacluster`. compare the existing integration tests for this. the integration test should run twice, once with eos enabled and once with eos disabled. does this make sense?",0,0,0,0.934583842754364,0.9924478530883788,0.9875001311302184,0.0,accept,unanimous_agreement
512031374,6694,"oh, so that was the reason why the eos case wasn't working. it had never occurred to me that there was no broker, causing the failure. thanks for the heads up on that one. the integration test i will add. we will definitely need one.",1,1,1,0.6622179746627808,0.6383265852928162,0.9662644267082214,1.0,accept,unanimous_agreement
512606577,6694,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
512892534,6694,"alright, so after some checks, it appears that if we move the logic for restoring the correct partitiontime to newtasks, it will cause several tests relating to resetting streams to break, particularly in resetintegrationtest. i've traced the root cause to be possibly both the committed() call / the setpartitiontime() call. the why of what happened is still unknown, but we need to dig deeper.",0,0,0,0.9642244577407836,0.9657336473464966,0.9873108863830566,0.0,accept,unanimous_agreement
513067124,6694,"what do you think about this? when moving this logic into newtasks(), several tests seem to break due to it. your thoughts?",0,0,0,0.9023520946502686,0.6642805337905884,0.9381290078163148,0.0,accept,unanimous_agreement
513328157,6694,test results are not available any longer. checked out the pr and ran it locally. - `assignedstreamstaskstest` -> just needs some updates to the mocks bunch of integration test fail all with the same error message: [code block] or [code block] this is definitely your new code that does not yet work properly. you will need to dig into those issues and fix your code.,0,0,0,0.5651026964187622,0.9872617125511168,0.9764966368675232,0.0,accept,unanimous_agreement
513479272,6694,"there is some system.out.println statements i added for debugging purposes. later, i will remove them.",0,0,0,0.9869495034217834,0.9774351119995116,0.9922027587890624,0.0,accept,unanimous_agreement
513562958,6694,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
513981439,6694,"alright, added an integration test for both eos and non eos case. you think we need anything else?",0,0,0,0.9765132069587708,0.9861178994178772,0.9934715032577516,0.0,accept,unanimous_agreement
514459621,6694,call for review from .,0,0,0,0.9871138334274292,0.9879310131072998,0.9919052124023438,0.0,accept,unanimous_agreement
516211153,6694,i guess we want to try to wrap this pr up?,0,0,0,0.962978184223175,0.9856648445129396,0.9793537855148317,0.0,accept,unanimous_agreement
517098808,6694,"ok, test modifications are a little more complicated since when i converted the test i added to a more simple [code block] test, it failed. still working to identify the cause. (some debugging statements has been added, they will be removed shortly).",0,0,0,0.8778157234191895,0.9659096002578736,0.9815475940704346,0.0,accept,unanimous_agreement
517497668,6694,"alright, i have proof that initializetasktime is called at the wrong location. it appears that when we switch to a simple builder.stream().to() program, it is _never called_. i've found that if you put it in front of the addrecords() method, it will be called regardless. so i thought it was a pretty safe bet.",0,0,0,0.9066720604896544,0.8874533772468567,0.7825230360031128,0.0,accept,unanimous_agreement
517778823,6694,okay addressed most of your comments.,0,0,0,0.979909121990204,0.8861514329910278,0.9887827038764954,0.0,accept,unanimous_agreement
517871590,6694,"about why initializetasktime() and the reasons it was moved. in a comment above, i actually stated the reason: ""alright, i have proof that initializetasktime is called at the wrong location. it appears that when we switch to a simple builder.stream().to() program, it is never called [where it is right now in initializenewtasks()]. i've found that if you put it in front of the addrecords() method, it will be called regardless. so i thought it was a pretty safe bet."" strange, i know. integration test however seems to indicate this should be the better approach.",-1,0,0,0.501420795917511,0.5515082478523254,0.8816178441047668,0.0,accept,majority_agreement
517874623,6694,"thinking about it again, maybe you added it to the wrong line in `initializenewtasks()` [code block] however, we call `transitiontorunning()` multiple times and need to call `initializetasktime()` each time before we. hence, it would be simpler to call initializetasktime() within `transitiontorunning()`",0,0,0,0.9835314750671388,0.9930556416511536,0.9833127856254578,0.0,accept,unanimous_agreement
517876334,6694,"oh, nice catch! tried it out and it worked. i didn't think that we would have to move it to the else branch of the method.",1,1,1,0.990617573261261,0.991519033908844,0.995039403438568,1.0,accept,unanimous_agreement
517888203,6694,"oh looks like i can get rid of the sleep now. weird, could no longer replicate the behavior any longer. might have been a fluke.",-1,-1,-1,0.9666442275047302,0.9865311980247498,0.9712517261505128,-1.0,accept,unanimous_agreement
518775221,6694,do you think that this pr as it is could be merged? feel like its pretty close to completion.,0,0,0,0.6210816502571106,0.6740143895149231,0.6397898197174072,0.0,accept,unanimous_agreement
519186670,6694,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
519339761,6694,"actually, bouncycastle dependency has been added. there shouldn't be any problems from here on out. so everything should be fine from here on out.",0,0,0,0.9618391394615172,0.971516728401184,0.8336514234542847,0.0,accept,unanimous_agreement
519598495,6694,"all comments has been addressed. a thought did occur to me though while doing this pr. if a streams has been restarted by the user, shouldn't the user expect the timestamp of a streamtask to be -1 before it starts processing (meaning that there is no retrieval of committed timestamps) ? i don't really know if a streams restart is a good simulation of a crash because it might perform contrary to user expectations.",0,0,0,0.9069249629974364,0.9494225978851318,0.9904943704605104,0.0,accept,unanimous_agreement
519618724,6694,"not sure. stopping and restarting an application is the same as fail-over. if there are committed offsets you want to continue where you left off, and the application should be in the exact some state as when you stopped it. hence, why would you expect -1 as start timestamp? of course, if you reset the application and loose the offsets, you trigger auto-offset-reset and you would start with -1 (similar if you use the reset tool to seek to a specific start offset). hence, i don't think this would be a problem.",0,0,0,0.9588652849197388,0.948921799659729,0.9351387619972228,0.0,accept,unanimous_agreement
519659608,6694,think we need anything else?,0,0,0,0.976316511631012,0.9718833565711976,0.9860851764678956,0.0,accept,unanimous_agreement
519725293,6694,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
519753100,6694,"ok, comments should be addressed.",0,0,0,0.9824803471565248,0.9908357858657836,0.9879103899002076,0.0,accept,unanimous_agreement
520000977,6694,java 8: [code block] java 11 / 2.13: [code block] java 11 / 2.12: [code block],0,0,0,0.9870554208755492,0.9913474917411804,0.994533896446228,0.0,accept,unanimous_agreement
520058640,6694,"looks like we still need to manually commit offsetandmetadata. mock producer doesn't do that for us, interestingly enough.",0,0,0,0.946342706680298,0.8966273665428162,0.978661298751831,0.0,accept,unanimous_agreement
520096521,6694,"well. i guess i missed to explain something: the producer adds and commits the offsets as part of the transaction: [a link] the `mockproducer` first adds the `offsetandmetadata` into its ""uncommitted cache"": [a link] on commit, it moves the offsets to ""committed offsets"": [a link] thus, `task.initializetasktime();` will not find them because the mock consumer and mock producer are not connected to each other. hence, your observation is correct. however, committing the offsets using the mock consumer in the test code, dose not verify if the producer did commit the correct offsets and metadata. therefore, instead of calling `task.initializetasktime()`, the test should check if the producer did the correct commit. if we really want to keep `initializetasktime()`, the test code should extract the offsets from the producer and commit the on the consumer to ""connect"" both mocks. (this is still different to the current code, that provide the offset as part of the test what does not help to test what we want to verify). does this make sense?",0,0,0,0.922072172164917,0.9872325658798218,0.9444565176963806,0.0,accept,unanimous_agreement
520113404,6694,"thanks for the tip! got everything done and updated the streamtask test. we hopefully should be pretty close, so one last push. :)",1,1,1,0.993755340576172,0.9961589574813844,0.997038722038269,1.0,accept,unanimous_agreement
520463985,6694,any more comments?,0,0,0,0.9284979701042176,0.9766708016395568,0.9907190203666688,0.0,accept,unanimous_agreement
520910285,6694,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
521360908,6694,pinging for final review.,0,0,0,0.9827128648757936,0.9807137846946716,0.9908936023712158,0.0,accept,unanimous_agreement
521716728,6694,"alright, done.",0,0,0,0.9584578275680542,0.9361395835876464,0.9857764840126038,0.0,accept,unanimous_agreement
522779481,6694,another round of comments would be good or approval of pr if everything looks right. :),1,1,1,0.9902400970458984,0.9960734844207764,0.9957224130630492,1.0,accept,unanimous_agreement
524449143,6694,this issue is the cause of critical bugs we recently faced up in our applications that rely on the `sessionstore` for processing retroactive events. do you think this fix can be included as part of 2.3.1?,0,0,0,0.9286734461784364,0.9617834687232972,0.9851258993148804,0.0,accept,unanimous_agreement
525998259,6694,any last comments?,0,0,0,0.9609412550926208,0.9840871691703796,0.9919871091842652,0.0,accept,unanimous_agreement
528057138,6694,pinging.,0,0,0,0.9729470014572144,0.9366099238395692,0.9158015251159668,0.0,accept,unanimous_agreement
528148998,6694,i don't think that we will include it in 2.3.1 -- it's not really a bug fix but an improvement. i try to review again in the next days.,0,0,0,0.8792445063591003,0.9559422135353088,0.9516434669494628,0.0,accept,unanimous_agreement
529068486,6694,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
530728164,6694,all builds reported spotbug issues.,0,0,0,0.9799179434776306,0.9220226407051086,0.8876434564590454,0.0,accept,unanimous_agreement
531017385,6694,"yeah, got it fixed.",0,0,0,0.9470892548561096,0.9467191696166992,0.9042342305183412,0.0,accept,unanimous_agreement
531208299,6694,the following test failures seem related: [code block],0,0,0,0.919663429260254,0.9915115237236024,0.9865230917930604,0.0,accept,unanimous_agreement
531255894,6694,"oh, just realized that 's comment caused a regression. if you would look earlier in the conversation, you would find a segment where a call to close() resets all partition times to negative one. therefore, we need to store the partition times in a map _before_ they are reset, and then they are passed into the commit() method. the extra parameter is needed after all due to the order of operations in close(). we will need to rollback some changes.",0,0,0,0.973953366279602,0.9338422417640686,0.9673534035682678,0.0,accept,unanimous_agreement
533262618,6694,thanks for the hard work !,1,1,1,0.9674715399742126,0.9288851022720336,0.9708004593849182,1.0,accept,unanimous_agreement
385120644,4931,thanks for the comments. i will certainly be adding more javadocs when ready for the final review. i still didn't do the java serviceprovider api changes because i just wanted to finalize the public changes here.,1,1,1,0.955252170562744,0.8340084552764893,0.9658650159835817,1.0,accept,unanimous_agreement
391870227,4931,any final thoughts here before i request a committer to take a look?,0,0,0,0.9706736207008362,0.9695863723754884,0.9917258024215698,0.0,accept,unanimous_agreement
391898959,4931,there are a couple of unaddressed comments.,0,0,0,0.9706844091415404,0.9160752296447754,0.9836286306381226,0.0,accept,unanimous_agreement
392278971,4931,i have addressed all the comments from your last pass.,0,0,0,0.9821625351905824,0.9770495891571044,0.9936957955360411,0.0,accept,unanimous_agreement
392637083,4931,what are you thoughts about enforcing a version to be non null & nonempty? i think we can enforce this for rest extension only when one tries to use it.,0,0,0,0.9848946928977966,0.9820615649223328,0.9911755323410034,0.0,accept,unanimous_agreement
392879147,4931,i have pushed a change where i check for the version when the plugin gets instantiated. this should be effective for all plugins that use the newplugin() method in future.,0,0,0,0.9883121848106384,0.992009162902832,0.9855371117591858,0.0,accept,unanimous_agreement
393006763,4931,thanks . lots of great detailed comments that helped this pr evolve.,1,1,1,0.9825214743614196,0.9937071204185486,0.9931105375289916,1.0,accept,unanimous_agreement
393030354,4931,"thanks . regarding the connectclusterstate being mutable, i think that's the real use of it. however in the implementation the 2 methods that it exposes are currently not atomic. also, the connectclusterstate doesn't expose the connector configs at all, so i'm not how kip-297 would affect this.",1,1,1,0.624924898147583,0.8608819246292114,0.9622601866722108,1.0,accept,unanimous_agreement
456933267,6177,call for review folks!,1,1,0,0.5386247038841248,0.7604895830154419,0.7743203639984131,1.0,accept,majority_agreement
458241606,6177,thanks stanis for the great suggestions. could i get some further review? ?,1,1,1,0.978395700454712,0.9902867078781128,0.9953510761260986,1.0,accept,unanimous_agreement
459831480,6177,pinging for more reviews thanks a lot!,1,1,1,0.9830979704856871,0.9669494032859802,0.9851675629615784,1.0,accept,unanimous_agreement
460243061,6177,"for future reference, could we not force-push changes after review comments? we will squash everything in the end anyway. it makes subsequent reviews much easier",0,0,0,0.9462719559669496,0.9907459020614624,0.975593090057373,0.0,accept,unanimous_agreement
460350699,6177,"sounds good, i will try to see if i could work around that when i rebase to trunk.",1,0,1,0.7023575901985168,0.7948826551437378,0.6381371021270752,1.0,accept,majority_agreement
468834722,6177,do you have time to take a closer look at the group coordinator logic?,0,0,0,0.9806078672409058,0.9909861087799072,0.9894557595252992,0.0,accept,unanimous_agreement
470385891,6177,"i have addressed most comments except two things: 1. the assertion vs exception 2. use option[string] instead of option i have written my thoughts on both questions. for #2, my question is primarily on whether we should still use option when upstream is confirming to pass in a valid string? let me hear your feedback, thank you!",1,1,1,0.9819183349609376,0.9818339943885804,0.9849758744239808,1.0,accept,unanimous_agreement
470752834,6177,"thanks for the updates. there is one additional point that i wanted to discuss. let's use the following example. suppose we have three consumers in the group with static instance ids: a, b, and c. assume we have a stable group and the respective memberids are 1, 2, and 3. so inside the group coordinator, we have the following state: [code block] in fact, the consumer leader of the group is not aware of the instance ids of the members. so it sees the membership as: [code block] now suppose that a does a rolling restart. after restarting, the coordinator will assign a new memberid to a and let it continue using the previous assignment. so we now have the following state: [code block] the leader on the other hand still sees the members in the group as {1, 2, 3} because it does not know that member a restarted and was given a new memberid. suppose that eventually something causes the group to rebalance (e.g. maybe a new topic was created). when the leader attempts its assignment, it will see the members {2, 3, 4}. my basic question is how can it know that 4 is the same member as 1? i think the proposal essentially relies on some auxiliary information provided by streams in order to determine this, but i cannot think of a good reason why we should not just use the instance id itself. in other words, we can change the joingroup response to include both the instance id and the member id. [code block] this approach provides some benefit even for the simple partition assignors. consider, the default range assignor, for example. basically it works by sorting the members in the group and then assigning partition ranges to achieve balance. suppose we have a partition with 9 partitions. if the membership were {1, 2, 3}, then the assignment would be the following: [code block] now when the membership changes to {2, 3, 4}, then all the assignments change as well: [code block] so basically all of the assignments change even though it's the same static members. however, if we could consider the instanceid as the first sort key, then we can compute the assignment consistently even across restarts: [code block] and after the restart: [code block] to summarize, i think basically what i'm saying is that the full benefit of static assignment can only be realized if the assignor knows the instance ids of the members in the group. it shouldn't be necessary to do anything fancy with additional metadata. if that makes sense, i'd suggest that we alter the joingroup response here since we are bumping the protocol anyway. we can leave any changes to the `partitionassignor` interface for another pr.",1,1,1,0.7025652527809143,0.8890953063964844,0.9540450572967528,1.0,accept,unanimous_agreement
471014920,6177,"i like 's idea, it looks a good thing to have with very small cost on protocol changes.",1,1,1,0.8372892737388611,0.993259608745575,0.9869044423103333,1.0,accept,unanimous_agreement
471020583,6177,"thanks for the great proposal jason! i think the benefit of this change is more valuable for range or round-robin assignors because they rely on member id to do the sharding. for sticky assignors, my understanding is that as long as the application is not rolling restart, we should preserve the same subscription info when rebalance happens. the leader should be able to see the current subscription info for all members iiuc? i would definitely take some time to read through the code and make sure i understand the sticky assignment better.",1,1,1,0.989649534225464,0.9931899905204772,0.9935560822486876,1.0,accept,unanimous_agreement
472058830,6177,"yeah, my point is that it is generally useful for assignors to have access to the static ids of the current group members. the static id allows us to reliably detect the same instance across restarts which frees users from having to either embed the static id in the user data or implement some kind of heuristic to try and guess the same member. anyway, i think it would be a missed opportunity to introduce this fancy new notion of static id and not allow the assignors to leverage it. do you see any downsides?",0,0,0,0.8285917043685913,0.955930471420288,0.947644293308258,0.0,accept,unanimous_agreement
472079669,6177,"nope, after reading the code i think the idea is brilliant! could you first take a look at [a link] since i need to automate join group protocol first, thank you!",1,1,1,0.992189645767212,0.9954018592834472,0.9965092539787292,1.0,accept,unanimous_agreement
472462552,6177,fwiw i also think that idea is great. could we make sure to update the kip if we'll be following this approach?,1,1,1,0.9887999296188354,0.9236812591552734,0.9889047741889954,1.0,accept,unanimous_agreement
472539911,6177,"yep, i will definitely do that!",1,1,1,0.9466332197189332,0.4903000295162201,0.9041525721549988,1.0,accept,unanimous_agreement
478372433,6177,"we should also update kip-345 itself for both the protocol change, as well as the susbscription class (as it's a public class) which is passed via `partitionassignor#assign`.",0,0,0,0.98947936296463,0.99568909406662,0.9950181245803832,0.0,accept,unanimous_agreement
478382417,6177,thanks guozhang for the great summary! i will address the agreed changes and update the kip for subscription class change.,1,1,1,0.99188494682312,0.9933086633682252,0.9960013031959534,1.0,accept,unanimous_agreement
480142674,6177,"ah, i checked the `subscription` class and it doesn't contain any id field [code block] what are we trying to add here?",0,0,0,0.9861982464790344,0.9906522035598756,0.9923436045646667,0.0,accept,unanimous_agreement
480145428,6177,"as suggested, we want to encode the instance.id into the subscription so that assignor can behave on sticky assignment.",0,0,0,0.9884309768676758,0.9929963946342468,0.9939751029014589,0.0,accept,unanimous_agreement
480400795,6177,"i wanted to suggest an alternative to the proposal above for handling the ""dueling consumers"" problem. i would say the underlying issue is that the coordinator cannot distinguish the two cases when a memberid has been replaced and when it has been removed. if it could, then we would be able to use member_id_mismatch consistently and the consumer could treat this error as fatal. here are two options: 1. include instanceid in the heartbeat and offsetcommit apis. then the coordinator can return the proper error code. 2. we can can use a convention to embed the instanceid into the generated memberid. at the moment, the current format is `{clientid}-{random uuid}`. for static members, i think instanceid is more useful than clientid and we could probably use timestamp as a more concise alternative to uuid. so we could have `{instanceid}-{timestamp}` as the memberid for static members. then we would be able to extract this from any request and the coordinator could use the proper error code. my preference is probably for option 1 because it is explicit, but i know guozhang expressed some unwillingness to change additional apis. a nice benefit of option 2 is that it will be easy to see the instanceid of existing members using tools like `kafka-consumer-groups.sh`. this will make debugging easier.",0,0,0,0.9612727165222168,0.992970049381256,0.96757173538208,0.0,accept,unanimous_agreement
480439917,6177,"for option 1), today we call resetgeneration on five places upon `unknown_producer_id`: leave, join, sync, commit, heartbeat. i think for leave-group we can probably not adding instance.id, but for other three excluding join, we will need to add instance.id to make it work. my personal feeling is it is a bit too intrusive to add on all these request protocols, but honestly i do not have a very objective rationale either. option 2) looks better to me, although it requires to make such string construction rules as part of the public apis since other client implementations would need to rely on that as well.",1,0,0,0.5936492085456848,0.7595492601394653,0.8590254187583923,0.0,accept,majority_agreement
480445141,6177,"i think we should probably use something like the member id generation scheme proposed in 2) in any case, whether or not we rely on it internally. i think it will be useful for debugging to always be able to derive the instance id from the member id. i'm a bit torn on whether or not to include instance id in apis that you mentioned. it seems like a cleaner solution to me because it allows the coordinator to be unambiguous about the error. i think the approach which requires retrying the joingroup in order to disambiguate the error is a little hard to explain. i also feel a little annoyed about modifying more protocols, but i'm not really sure we should be trying to optimize for the number of protocol bumps. by the way, if we cannot rely on the generated memberid, then it seems inevitable that we will also need to modify the describegroup api so that it is easy to find the instance ids associated with current members.",-1,0,0,0.7151861190795898,0.6314437389373779,0.8952459692955017,0.0,accept,majority_agreement
480647194,6177,"thanks for the discussion here! in high level, i think both approaches make sense here. the issue with 1) is that we are altering many protocols all at once just for the purpose of solving a client mis-configured scenario, which seems like an over-kill to me. the attractive side of 2) is extra benefit in terms of debugging, however it still involves a lot of logic changes to double check static member's identity in all consumer protocols and client needs to handle `member_id_mismatch` exception everywhere, not mentioning corresponding unit tests. no matter which approach we eventually take, it seems better to have the change go in with another pr to reduce the review burden for the current one, which is already high. the severity of instance id conflict should be tolerable for now, and i think we still have enough time to get the fix diff ready before 2.3 ddl. so if you both agree, i could revert the generation reset change and leave the id conflict fix for next diff if possible. wdyt?",1,1,1,0.9613714218139648,0.9751555919647216,0.9879563450813292,1.0,accept,unanimous_agreement
480672408,6177,yeah i think we should keep this pr small as it already grows from 600 to about 1200 loc anyways. i'm fine with having a follow-up pr with whatever approach we've agreed upon to handle the instance id conflict issue.,0,0,0,0.9513730406761168,0.9626413583755492,0.9577500224113464,0.0,accept,unanimous_agreement
480693499,6177,"sounds good, will revert the current changes of `resetgeneration()`.",1,0,0,0.7565771341323853,0.7272257208824158,0.6129031181335449,0.0,accept,majority_agreement
481373001,6177,sounds good to me to resolve this problem separately. i'll do another pass on the pr today and hopefully we can merge this week.,1,1,1,0.9749994874000548,0.5277320146560669,0.5970245599746704,1.0,accept,unanimous_agreement
481796564,6177,"`let me try one more time. how about fenced_instance_id?` lol, you got it.",1,1,1,0.9667921662330629,0.9839338660240172,0.9709837436676024,1.0,accept,unanimous_agreement
482215766,6177,"added most comments except ones that we haven't cleared, do you mind taking another look when possible? thank you!",1,1,1,0.9562753438949584,0.9741894602775574,0.9488797187805176,1.0,accept,unanimous_agreement
482400070,6177,"i've made another pass on it. one meta comment is that we've not yet updated the web docs for this -- if you want to do it in another pr that's fine, just pointing it out so we do not forget about it. otherwise, i'm +1 modulo 's comments.",0,0,1,0.9583593010902404,0.9421064257621764,0.5663701295852661,0.0,accept,majority_agreement
482429127,6177,that sounds good guozhang! will do it in another pr.,1,1,1,0.9883727431297302,0.9934855103492736,0.993363618850708,1.0,accept,unanimous_agreement
483362126,6177,could you take another when you got time? thank you!,1,1,1,0.9779565334320068,0.9722204804420472,0.94538676738739,1.0,accept,unanimous_agreement
484348012,6177,"i refactored the usage of `group.instance.id` on client side to use `option ` so that we could avoid string comparison. let me know if this works, thank you!",1,1,1,0.9711071252822876,0.9785141348838806,0.9667983055114746,1.0,accept,unanimous_agreement
484955097,6177,do you think we have luck to get this done today?,0,0,0,0.9604784846305848,0.966136395931244,0.8996146321296692,0.0,accept,unanimous_agreement
485040283,6177,i think the failure is due to a flakey test fyi,0,0,0,0.8698416948318481,0.9395756125450134,0.962563931941986,0.0,accept,unanimous_agreement
485060306,6177,some of the consumer / upgrade tests have failed: [a link],0,0,0,0.9469932317733764,0.9845817685127258,0.9887908697128296,0.0,accept,unanimous_agreement
485294349,6177,"fyi, in `verifiableconsumer.java`, i realized that i couldn't do `consumerprops.put(consumerconfig.group_instance_id_config, null);` because hashtable rejects null value. so i did a workaround to use empty string as default value for `group.instance.id` but still keeps all the optional classes in the consumer/broker code paths.",0,0,0,0.9752702116966248,0.9899839162826538,0.9883657693862916,0.0,accept,unanimous_agreement
485474245,6177,"shouldn't really be necessary. for `properties` objects, you can just leave out the key if the value is null.",0,0,0,0.9752986431121826,0.9908798933029176,0.9908079504966736,0.0,accept,unanimous_agreement
485553178,6177,that's a bit tricky since non-existing key get will through exception: [code block] should we do a try-catch in `kafkaconsumer` for instance id retrieval then?,0,0,0,0.6560462713241577,0.7284528017044067,0.9625741839408876,0.0,accept,unanimous_agreement
485599691,6177,"hmm that's weird, if we have a default value defined already, then upon the config object construction if there's no user-provided value the default value will be used to fill in the `values` map (note only the `originals` map contains user-overridden values).",-1,-1,-1,0.9811997413635254,0.9724685549736024,0.9798561334609984,-1.0,accept,unanimous_agreement
486845915,6177,"i've taken another look at the added commits, and besides this minor [a link] it lgtm.",0,0,0,0.9872530102729796,0.9748268127441406,0.992344319820404,0.0,accept,unanimous_agreement
486848243,6177,[a link] this shows that the system tests are fixed now.,0,0,0,0.987855851650238,0.979364573955536,0.9938485026359558,0.0,accept,unanimous_agreement
486848349,6177,triggered [a link] (`345_system_test` is on top of `345_merge`) and passed.,0,0,0,0.989355206489563,0.9935187697410583,0.9953778982162476,0.0,accept,unanimous_agreement
486867096,6177,[a link] triggered and passed exactly on top of current `345_merge` branch.,0,0,0,0.9894992113113404,0.9927237629890442,0.995797872543335,0.0,accept,unanimous_agreement
487161856,6177,"thanks for the great patience and great work on the first step towards kip-345, .",1,1,1,0.963112771511078,0.9902724623680116,0.9940080642700196,1.0,accept,unanimous_agreement
290643473,2772,"need to rebase, dd71e4a8d830c9de40b5ec3f987f60a1d2f26b39 changed test class's breaking fetchertest on ci build",0,0,0,0.9847811460494996,0.9846451878547668,0.9936109185218812,0.0,accept,unanimous_agreement
290944811,2772,"can someone check trunk or the pr build plan, the build failure is due to: pure virtual method called terminate called without an active exception :streams:unittest failed i note other preceding builds for other pr's also have this issues, e.g. [a link] [a link]",0,0,0,0.9681949019432068,0.9695200324058532,0.990532636642456,0.0,accept,unanimous_agreement
290948772,2772,"-rosenblatt thanks for the time and the review feedback :), i've committed some changes based on it, if you wish to re-review. p.s. it seems the pr builds in jenkins ci plans are not stable atm. i assume someone is looking into as its not just my pr affected.",1,1,1,0.9902843832969666,0.9541974067687988,0.9931572079658508,1.0,accept,unanimous_agreement
291421176,2772,", would you like to review this one?",0,0,0,0.9748575091362,0.9912235736846924,0.9052082896232604,0.0,accept,unanimous_agreement
291990765,2772,thanks for the patch michael. i'll take a look either today or tomorrow.,1,1,1,0.884588897228241,0.9406534433364868,0.9686777591705322,1.0,accept,unanimous_agreement
292451094,2772,"thanks for the review, i have left response on all, i hopefully have marked clearly the ones we don't need any discussion on and will just implement, will aim to do today. if you could read and comment on those with responses.",1,1,1,0.9262666702270508,0.9510869979858398,0.9707677364349364,1.0,accept,unanimous_agreement
296411045,2772,"thanks for the time and the review feedback :), i've committed some changes based on it. also have added the additional integration tests. plus yet another rebase",1,1,1,0.9921007752418518,0.9909548163414,0.9946082830429076,1.0,accept,unanimous_agreement
297570869,2772,"thanks for the patch. lgtm. we need to update `upgrade.html` as well, but that can also be done in the follow up patch that removes the serde hack.",1,1,1,0.9319329261779784,0.9468634724617004,0.9621647000312804,1.0,accept,unanimous_agreement
297874309,2772,thanks for the patch! i'm planning to merge this tomorrow. it would probably be a good idea to send an e-mail to the dev list with the changes to the kip to make sure there are no concerns.,1,1,1,0.9855475425720216,0.9853455424308776,0.9936601519584656,1.0,accept,unanimous_agreement
297883375,2772,battery on laptop just died at 2am here. managed to quickly commit some javadoc changes. so i don't have to go through yet another annoying rebase tomorrow could we merge as is and i commit to raise a new pr with the extra tests you mentioned.,0,0,-1,0.533484697341919,0.8150056004524231,0.9423081278800964,0.0,accept,majority_agreement
297933355,2772,rebased .... again .... [a link],0,0,0,0.9575592875480652,0.93366277217865,0.9941114783287048,0.0,accept,unanimous_agreement
298133490,2772,"merge to trunk is imminent. the record header saga is finally drawing to a close. what battles await tomorrow?! (in all seriousness, thanks for the patch and the persistence with this kip)",1,-1,1,0.8951286673545837,0.8986969590187073,0.9345569014549256,1.0,accept,majority_agreement
298144836,2772,"sorry i'm late to the party and i'm going to be a pain, but if the headers are making it to the producerrecord, they should also make it to the sourcerecord, so that the connect framework directly benefits from it. also, imo this would have been a perfection opportunity of adding such headers as a method or something... right now, many constructors are missing the headers parameter, and the only way to get to add headers is the very dangerous [a link] (because of the partition parameter). just saw this got merged an hour ago... cf [a link] cc",-1,-1,-1,0.9900997281074524,0.9896448850631714,0.9944573640823364,-1.0,accept,unanimous_agreement
298147071,2772,"hi steph, you actually add a header in normal usage by calling record.headers.add(key,value), this constructor was added in the record really only aimed at mirror making solutions. also adding headers is mostly aimed to be added by interceptors (where record is given) thus mutable at that stage. this is why on produce once sent the headers are made read only. yes connect framework isn't done as wasn't covered in this kip, once this kip was done i was actially going to raise another kip immediately to add them (a bit like timestamp was first added to core and then a second kip was raised to add them to connect), but first step it was important to get headers added at the core level. we actually have the code for that ready so hold on :)",0,0,0,0.9115954637527466,0.9626927375793456,0.8855746984481812,0.0,accept,unanimous_agreement
298147275,2772,that helps thanks !,1,1,1,0.9712526202201844,0.9928610920906068,0.9688069820404052,1.0,accept,unanimous_agreement
298152782,2772,"was going to raise all this in a week or so time, but as we had the code ready and obviously you have a need thus raised the point re connect, i have raised it all now. here's kip [a link] here's pr (which we had the code for) [a link] obviously first needs kip discussion and vote. so please feel free to get active in that space supporting that :)",1,1,1,0.9655287265777588,0.979788601398468,0.9947799444198608,1.0,accept,unanimous_agreement
298181606,2772,see [a link] for details about this work. you can just update your own kip accordingly.,0,0,0,0.9812108278274536,0.9807226061820984,0.993977189064026,0.0,accept,unanimous_agreement
271833277,2330,", you know the networking code pretty well, so maybe you could help with this review. also , but his time is very limited this week.",0,0,0,0.8095791935920715,0.9137884378433228,0.9240135550498962,0.0,accept,unanimous_agreement
271888557,2330,one concern that was raised also in kip-81 (reusing the same memory pool logic on the consumer side) is whether the muting might impact group coordination. one suggestion is to only allocate in the memory pool requests that are larger than a small threshold (~256bytes for example) and let smaller requests be allocated on the heap as usual. this should not add too much load/memory pressure on the broker (since we also have the queued.max.requests setting) and would allow heartbeats and other group requests to be processed uninterrupted.,0,0,0,0.9622714519500732,0.994481086730957,0.9717338681221008,0.0,accept,unanimous_agreement
273309380,2330,"- if you want to bypass the memory pool for small allocations (which is perfectly valid) you could implement a compositememorypool that delegates requests under a certain size to the none pool and requests over that size to a ""real"" pool. that would give you the behaviour you want.",0,0,0,0.9862985014915466,0.9936287999153136,0.9916553497314452,0.0,accept,unanimous_agreement
273972279,2330,- i've added a basic test of oom functionality to selectortest. i didnt write an ssl equivalent yet,0,0,0,0.9787287712097168,0.986607551574707,0.9837682843208312,0.0,accept,unanimous_agreement
274049159,2330,"-rosenblatt the implementation looks good to me. for testing ssl, it may be easier to add a memory pool to `nioechoserver` and add a test to `ssltransportlayertest`.",1,1,1,0.9498854279518129,0.9271103143692015,0.7374776601791382,1.0,accept,unanimous_agreement
274122627,2330,"for some reason i cant respond directly to your comment above on networkreceive.java, but some of the ""no-pool"" ctr calls come from sasl, for example. reasons for not using a pool would be: 1. cost-benefit - sasl for example probably doesnt allocate large buffers and allowing small allocations to bypass the central pool is probably a good idea 2. focus of this change set. my immediate concern is broker being overwhelmed by large produce requests, as that is a common (relatively) scenario for me. future work could broaden the scope to include: 2.1 (de)compression - you'd want to use the pool for decompression target buffers. doing this efficiently might require a wire format change (would be nice to know decompressed size up front) 2.2 maybe send buffers? without ssl they get zero-copied from disk out to socket, but under ssl theres a byte buffer allocated. havent fully investigated so dont know how much of a potential oom risk that is. 2.3 usage on the client-side - both producer and consumer, also interacts with (de)compression.",0,0,0,0.7642078995704651,0.9810994267463684,0.9135515093803406,0.0,accept,unanimous_agreement
274196525,2330,"- i've implemented the ssl version of testmuteonoom(). its not pretty, but it directly tests the core functionality. also, thank you very much for your time spent reviewing this.",1,1,1,0.9707521200180054,0.9415208697319032,0.9757768511772156,1.0,accept,unanimous_agreement
274617983,2330,"- i've changed the tests to use the non-strict mode, like actual code. note that the scenario you've described of constantly muting and unmuting is also possible with a non strict pool: 1. server has n incoming connections 2. at every iteration pool only has enough memory to accommodate a single request 3. at every call to poll, n connections are unmuted, 1 is services, n-1 are muted again, progress indication is set to true 4. next iteration either makes no further progress (in which case the next after that will actually wait for a while), or again repeats this. this isn't as tight a loop as with a strict pool (as the 2nd invocation will not have memory and will not make progress), but its still sub-optimal. a more optimal solution would be to somehow remember the smallest unfulfilled request, have some pool.canallocate(long bytes) - which will account for strict vs non-struct - and only unmute is the result is true. since there's one pool and multiple selectors this isn't guaranteed to not unmute for nothing. to me this is a corner case though, as under these conditions currently the broker just explodes. i can implement the canallocate(bytes) api if you think its required, but i think this is optimizing a corner case.",0,0,0,0.974105715751648,0.979341447353363,0.9833396673202516,0.0,accept,unanimous_agreement
282928080,2330,- to the best of my understanding i've addressed/answered everything? anything still pending from you guys?,0,0,0,0.9437226057052612,0.8814630508422852,0.9348159432411194,0.0,accept,unanimous_agreement
290571351,2330,"- rebased again. slight modifications to comply with method complexity checks that were introduced to master since i last rebased. also kicked off an integration test, will report the results (everything passes locally)",0,0,0,0.986221194267273,0.9887669682502748,0.9058315753936768,0.0,accept,unanimous_agreement
290735579,2330,successful integration test run - [a link],0,0,0,0.963870882987976,0.7577805519104004,0.9113845229148864,0.0,accept,unanimous_agreement
299710401,2330,just rebased this again on trunk. the following tests are being flaky (on trunk as well as on my other pr): [code block],0,0,0,0.6317501068115234,0.726865291595459,0.9882704019546508,0.0,accept,unanimous_agreement
314771034,2330,what's the status of this pr ? i'd like to start working on kip-81 but it has a dependency on this.,0,0,0,0.964574933052063,0.9653672575950624,0.9885507225990297,0.0,accept,unanimous_agreement
314807274,2330,: i dropped the ball of finishing the review. i will make another pass in the next few days.,0,0,0,0.9371591210365297,0.9170409440994264,0.6764193773269653,0.0,accept,unanimous_agreement
314819925,2330,i've rebased it just yesterday. passes tests both locally and on the build server.,0,0,0,0.9826059341430664,0.9886175990104676,0.9911147356033324,0.0,accept,unanimous_agreement
317422141,2330,"-rosenblatt i had a look through the code and it is looking good. it is a bit harder to review because the commits were squashed. the last time i had reviewed this, `selector#pollselectionkeys()` was returning a boolean which indicated if progress was made. some of those changes seem to have been reverted - perhaps those changes are still required to update `selector#madeprogresslastpoll`?",1,0,0,0.8590136170387268,0.7498191595077515,0.6402234435081482,0.0,accept,majority_agreement
317587446,2330,i've fixed all the issues youve pointed out above,0,0,0,0.973710000514984,0.9613698720932008,0.9137632250785828,0.0,accept,unanimous_agreement
317880700,2330,- removed setting the progress flag where it wasnt strictly required.,0,0,0,0.9441479444503784,0.973805606365204,0.9909517765045166,0.0,accept,unanimous_agreement
317958785,2330,-rosenblatt : thanks for the latest patch. lgtm.,1,1,1,0.965937316417694,0.9797001481056212,0.9827212691307068,1.0,accept,unanimous_agreement
317960830,2330,thanks for sticking with us -rosenblatt. and thanks for merging this during your holiday!,1,1,1,0.9767077565193176,0.9772692918777466,0.9912473559379578,1.0,accept,unanimous_agreement
518840417,7170,thanks for the pr. there are some checkstyle errors. can you please fix them before we review your pr?,1,1,1,0.9234290719032288,0.9011275172233582,0.9363015294075012,1.0,accept,unanimous_agreement
518955481,7170,done,0,0,0,0.976450741291046,0.8974218964576721,0.8682363629341125,0.0,accept,unanimous_agreement
519222258,7170,"this is the first pr (written in pr description) `kstream#groupby` changes will be part of next pr if this one passes. i didn't want to create big pr without validating underline machinery and logic in the first place. i'm okay doing all the changes here, i was just thinking it will make review harder.",0,0,0,0.6170361042022705,0.8585333228111267,0.769711434841156,0.0,accept,unanimous_agreement
519225997,7170,"agreed, i'll do that. i wanted to tag as seems like he's the main author behind optimization logic. i'll add tests for optimization logic to make sure nothing breaks.",0,0,0,0.9299132823944092,0.9470475316047668,0.9338890314102172,0.0,accept,unanimous_agreement
519303125,7170,"ah. i guess, i skipped the pr description... sorry for that. i discussed the proposal with in person, and thinking about the semantics once more, i am actually wondering if it is wise to change `groupby` at all (this thought also affects my previous comment to include `join()` -- i would like to retract this idea :) ) we have basically two dimensions which 2 cases each to consider for `groupby` (and also `join()`: 1) repartition not required -- no `repartitioned` object provided 2) repartition not required -- `repartitioned` object is provided 3) repartition required -- no `repartitioned` object provided 4) repartition required --`repartitioned` object is provided case (1), (2), and (4) are straight forward. however, case (2) is somewhat awkward because we actually want to treat `repartitioned` as a configuration object but specifying it in `groupby` should not enforce a repartitioning (if one want to enforce a repartitioning, they should use the new `repartition()` operator). hence, for case (2) the `repartitioned` configuration would be ignored. therefore, only case (4) is left in which passing in `repartitioned` would have an effect. for this case, it would be possible to change the number of partitions or to specify a different partitioner. (i ignore the serde and naming because this can be achieved via `grouped` anyway). however, if one wants to change the number of partitions or wants to set a specific partitioner, it seems they want to to apply (ie enforce) this configuration independent of the upstream topic configuration; ie, it is actually a case for which the user wants to _enforce_ a repartitioning. hence, it seems perfectly fine (and actually better, because it's semantically more explicit) that a user should call `repartition()` instead. therefore, i don't see a good use case for which it make sense to pass in `repartitioned` into `groupby`. would be great if you could share your thoughts about it? a second point i discussed with is about the optimization. we both have the impression that `repartition()` should not be subject to the repartition topic optimization. instead, an enforced repartitioning step, should be added to the topology in a hard coded way similar to a call to `through()`. maybe we can be some advanced optimization rules later, but it seems difficult (and potentially unsafe/incorrect) to apply the repartition topic optimization for this case. hence, we would suggest to skip this optimization in this pr.",-1,-1,1,0.986333191394806,0.9298449158668518,0.583954930305481,-1.0,accept,majority_agreement
519307224,7170,"not sure i agree -- maybe you just want to control the parallelism *in case* a repartition is required? you could enforce users to step through their whole topology, figure out when/where repartitioning is needed, and use `repartition` to set the parallelism. or, you could let streams do this for you -- as it currently does, way more conveniently and probably less error-prone -- and supply a configuration to be used if streams figures out you need to repartition.",0,0,0,0.980063796043396,0.9892200827598572,0.8355790376663208,0.0,accept,unanimous_agreement
519309839,7170,"i don't see this as a use case in practice. why would one want to change the parallelism? because, the aggregation operation is over or under provisions and thus one wants to decrease or increase the parallelism. if i am ok with the ""default"" parallelism in case there is no repartitioning, why would i not be ok with it if data is repartitioned? this is less an issue imho, because if i want to scale up for example, it's sufficient to insert `repartition()` once upstream and all downstream auto-created topics would inherit the parallelism implicitly (as they inherit it now from source topics). hence, i don't need to insert `repartition` all over the place.",0,0,0,0.8752354979515076,0.972380757331848,0.9034265279769896,0.0,accept,unanimous_agreement
519315591,7170,"maybe you're processing data from a topic on your company's cluster, which has a huge number of partitions to begin with. maybe your workload needs nowhere near this many partitions (you're filtering out most records, it's overpartitioned to begin, your just testing). you run your streams app which creates some n topics all of which have this huge number of partitions. your brokers struggle and your boss gets mad? why does anyone want to change this ever? (ie with `repartition`) that's a good point though. so the `repartition` operator is really a ""auto-create topic"" + ""set parallelism"" operator -- should be sure to document this well. now, what if someone wants to configure the parallelism of a certain repartition topic(s) but would like to continue using the source topic's parallelism as the default? not saying we should necessarily support that, but we should at least make it very clear to users how using this will affect downstream topics.",-1,-1,1,0.7029004693031311,0.9342556595802308,0.5829803347587585,-1.0,accept,majority_agreement
519319181,7170,"my question is, why do you need `groupbykey(repartitioned.with())`? if you want to scale down, it seems better to explicitly call `repartition(repartitioned.with()).groupbykey()` -- otherwise, you might not scale down if not auto-repartition topic is created and it seems rather error prone that we allow to specify the number of partitions and than ignore the config entirely. similar to your second comment, if you want to ""scale up"" again later, you call `repartition()` again. i agree that we need to document this explicitly if we follow this route. however, it's similar behavior as we have as-of now. if you insert a `through()` all downstream operators inherit the parallelism from it.",0,0,0,0.96790611743927,0.962237536907196,0.9694854021072388,0.0,accept,unanimous_agreement
519328010,7170,"ok, well i am fine with this framing it as a ""set parallelism"" operation...i don't want to stall this kip/pr further, but what if this was split into a new set of `setparallelism` and `repartition` operators, where the `repartition` just auto-creates the topic while upstream `setparallelism` is responsible for setting the number of partitions? just wondering if it's worth making this more explicit, since there's really two new functionalities being introduced here. doesn't hurt to bundle them into one operator, as long as users know what two things it actually does...",0,0,0,0.7196427583694458,0.5580400824546814,0.9145365953445436,0.0,accept,unanimous_agreement
519385569,7170,"hello thanks a lot for valuable insights and interesting discussion. - your arguments make sense, but i'm leaning more towards points. in addition, in my mind, one other important point that we need to take into account is not only parallelism but general configuration of repartition topics. in my mind, this kip can be the foundation of actually giving users control over each individual repartition topic configuration. to be honest, i was tempted to propose deprecating `kstream#groupby(grouped)` operation altogether. let me explain my reasoning a bit. after this kip, i don't see any actual benefit nor need of actually using `kstream#groupby(grouped)`. with `kstream#groupby(repartitioned)` user can do exact same things, plus more. right now, in kstream there're `grouped` and `joined` configuration classes (and maybe some others that i'm missing) that are used for specifying a) topology name (which translates to repartition topic naming) b) producer serdes c) partitioner all those configurations can be encapsulated under `repartitioned`, in addition with all other topic level configurations that user may want to pass to internal topic creation. maybe this was discussed before, and there's a good reason why there're separate configuration classes for each operation (besides giving api the nice look, of course :) ). one argument that comes to mind why we may actually want to have `repartitioned` for `groupby` is simple syntax sugar. for example, there isn't fundamental different between this two topologies: 1) [code block] 2) [code block] while, for me, as a user, 2nd option looks much more appealing, similarly how key selector for `kstream#groupby` merges together two operations (`selectkey().groupby()`). again, your arguments are totally valid, and all can be achieved just by having `repartition(repartitioned)` operation. but on the other hand, i don't see anything bad with adding `repartitioned` option to groupby. it won't break api semantics (at least i think it won't) and will give the user extra flexibility around controlling repartition topics.",1,1,1,0.9588487148284912,0.9893725514411926,0.989418089389801,1.0,accept,unanimous_agreement
519387441,7170,"it's harder to comment on this. first, i would like to see how optimization logic behaves with this changes. based on the first glance, it should be fine, but would need to verify this by adding more tests. if necessary, optimization can be easily removed i guess. would love to hear your thoughts/suggestions as well on this.",0,1,1,0.529538094997406,0.8865290284156799,0.9630510210990906,1.0,accept,majority_agreement
520079726,7170,"hey, all, just to wade in (hopefully not stalling this pr too long): we should exclude this change from optimizations. the optimization logic around repartitions is already quite complex. it pushes repartition nodes around the processor graph and merges them together when possible (and the merge results in picking some repartition topic names over others). adding a new class of repartition nodes (manually specified ones via this new operator) would only complicate the algorithm further. it's always safe to skip an optimization, so we can just skip it for now and consider it in a separate scope of follow-on work. further arguments against optimization: it's not clear that, if i put a `repartition()` node at a specific point in my topology, i would _want_ streams to move it somewhere else. also, if i have two different `repartition()` calls with _different_ parallelisms, and the optimizer wants to merge them, what can it do? what about if it wants to merge a groupby repartition (which implicitly has a parallelism already) and an explicit `repartition()` with a different parallelism? we can reason through all these cases, but it will still: * drag out the kip discussion (since we have to revisit all these cases) * increase code complexity in the optimizer * increase system complexity for the people using streams (who will have a hard time understanding the results of the optimization) we can avoid all this by just excluding the manual repartitions from optimization for now. regarding the other question about overlap with grouped (and joined): we should keep all these config objects separate. we have had a lot of trouble in the past trying to use one operation's configuration objects on other operations. it may seem like a good match, but it inevitably puts us in an awkward bind later on. when we have two different operations, we should have two different config classes as well. `grouped` is for configuring `groupby`, `joined` is for configuring `join`, and `repartitioned` is for configuring `repartition`. this may result in code duplication, but it also results in an api that is very consistent, clear, easy to learn, and easy to use. likewise, nesting one operation's configuration inside another operation's configuration is again sacrificing a clean api in favor of code de-duplication. it's not a good tradeoff. far better to configure `groupby` with `grouped` using setters for the direct properties we want to set, and likewise configure `repartition` with `repartitioned` using setters for the direct properties we want to set. then, we don't have to worry about the implications for `groupby` every time we consider changes for `repartition`. as long as they are separate operations, they should have separate configuration classes. if we want to give people control over the parallelism in `groupby`, we should just add `grouped#numberofpartitions(final int numberofpartitions)`. anyway, that's my 2 cents ;)",0,0,0,0.7556269764900208,0.8037670254707336,0.9016559720039368,0.0,accept,unanimous_agreement
520097324,7170,"this does not resolve my main concern, that one passed in `numberofpartitions` and we just ignore the configuration...",-1,0,0,0.7322512865066528,0.9223774671554564,0.94795560836792,0.0,accept,majority_agreement
520181014,7170,"thanks, , i'm not saying that we should (you have a good point). i was just saying that we can add/deprecate methods in different config objects independently, but if we switch to use the same config object in two operations, it ties our hands. on the optimization front, what we could do is make a small change to the optimization algorithm that, when searching upstream to find out if a repartition is necessary, if it finds a repartition operation, it can decide that one is not necessary. then, if you do `ktable...map(...).repartition(repartitioned.numberofpartitions(1234)).groupby(...)`, we won't get a double-repartition, just the one we specifically requested to scale out parallelism. thanks, -john",1,1,1,0.8966513276100159,0.6785424947738647,0.9609622955322266,1.0,accept,unanimous_agreement
520222537,7170,"thank you for the thorough review, much appreciated! i think i've addressed all of the comments mentioned in this pr. to summarize: - repartitioning is now always performed when calling `kstream#repartition` operations. - i've added separate, `unoptimizablerepartitionnode` which is **not** subject of optimization algorithm. `kstream#repartition` operations create `unoptimizablerepartitionnode` when they're called. we can create followup ticket to investigate optimization possibilities for `kstream#repartition`. thank you for the clarification. motivation behind having separate configuration classes per operation does make a lot of sense after your explanation. actually, it won't do double re-partitioning even now (there's even test for this `kstreamrepartitionintegrationtest#shouldcreateonlyonerepartitiontopicwhenrepartitionisfollowedbygroupbykey`). thing is, `kstream#repartition` creates `kstreamimpl` with `repartitionrequred` as `false`. regarding changes for `groupby` operations - those changes are not part of this pr either way, so i think we can leave that discussion for now. i propose to resurrect discussion in mailing list when this pr is merged and have followup discussion on that, wdyt? . even if we go through with `groupby` changes, after reading arguments on why it's not good idea to have same configuration class for multiple operations, it makes more sense to have `numberofpartitions` in `grouped` class for `groupby` operations. personally, i still feel that control over repartition topic configurations is necessary in the long run (sometimes i really want to configure retention per specific repartition topics), and if we gonna have configurations duplicated in each operation class, it may create a lot of pain as well. anyway, since there isn't other use-case yet besides specifying number of partitions per repartition topic, we can cross that bridge when we get there.",1,1,1,0.9891312122344972,0.9950824975967408,0.9930242300033568,1.0,accept,unanimous_agreement
520973806,7170,would appreciate second review.,0,1,0,0.5914449691772461,0.9063072800636292,0.7746174335479736,0.0,accept,majority_agreement
522163447,7170,"hey, not sure if you looked into this or not but there may be some changes needed in streamspartitionassignor. it tries to validate that repartition topics have been created with the correct number of partitions (defined as the max number of partitions of any source topics) this might not affect this pr so much as the `groupby(repartitioned)` one, but it might be good to verify with an integration test that goes through a rebalance",0,0,0,0.9744575619697572,0.9829586148262024,0.988382875919342,0.0,accept,unanimous_agreement
522426073,7170,"hi , thanks for the info. i've added integration test where rebalancing is triggered.",1,1,1,0.8772960305213928,0.8954984545707703,0.9710260033607484,1.0,accept,unanimous_agreement
522645678,7170,"hey , thanks for the update! i'll take a look as soon as i can. -john",1,1,1,0.9823708534240724,0.9932438731193542,0.9941262006759644,1.0,accept,unanimous_agreement
523701860,7170,sgtm. why would you need that? repartition topics are configured with infinite retention anyway and kafka streams does explicit purge data calls after records are fully processed.,0,0,0,0.9889504313468932,0.9706413745880128,0.9823070168495178,0.0,accept,unanimous_agreement
523808665,7170,"fair, i was thinking more about older version when retention wasn't set as -1. missed the part that it's long-time fixed now :) anyway, internal topic configuration options is subject for whole new discussion (if there will be ever need for that).",1,1,1,0.7101649045944214,0.990814745426178,0.9884139895439148,1.0,accept,unanimous_agreement
524659120,7170,"hi , thank you for thorough review, much appreciated. i've addressed all of your comments, please have a look when you got time. i've run `./gradlew streams:reportcoverage` and can verify that main code that i've changed has over ~97% test coverage. i've also added more tests related to `repartitioned#streampartitioner` (both integration and unit tests)",1,1,1,0.9701802134513856,0.9932445883750916,0.9888550639152528,1.0,accept,unanimous_agreement
526316345,7170,"hello , can you please have a look at this pr one more time :) (pinging you just in case so that it won't get lost) regards, levani",1,1,1,0.9739556312561036,0.9955701231956482,0.996618390083313,1.0,accept,unanimous_agreement
527985897,7170,"thanks, , fwiw, i've left a few replies on threads that are marked as ""outdated"" or ""resolved"". i think the only one that's really important is: [a link]",1,1,1,0.8079103827476501,0.8993659019470215,0.9738851189613342,1.0,accept,unanimous_agreement
527991663,7170,"hi thanks for the review. i've left reply as well. btw, i tried my best to make tests pass on `jdk 11 and scala 2.13` but they seem to fail, right now because of `kafka.api.plaintextconsumertest.testlowmaxfetchsizeforrequestandpartition` is there anything i can do about it?",1,1,1,0.9119980335235596,0.9497694373130798,0.9634015560150146,1.0,accept,unanimous_agreement
528126203,7170,there is [a link] -- feel free to comment on the ticket.,0,0,0,0.9348244667053224,0.9658668637275696,0.9751819968223572,0.0,accept,unanimous_agreement
528230308,7170,"thanks , left the comment. please let me know if you have any more concerns/comments about this pr. thanks, levani",1,1,1,0.9606980681419371,0.9747095704078674,0.9861337542533876,1.0,accept,unanimous_agreement
529540016,7170,"hello just small update - there were conflicts with trunk around `internaltopicconfig` class. i've synced the feature branch with trunk and resolved the conflicts. regards, levani",0,1,1,0.9574154615402222,0.7462866306304932,0.7735110521316528,1.0,accept,majority_agreement
534176196,7170,"hi , bumping this thread so it won't get lost. any updates around this pr? would love to finalize this pr and move on to other kips :) regards, levani",1,1,1,0.9849649667739868,0.9954468607902528,0.997041642665863,1.0,accept,unanimous_agreement
536229726,7170,"hello , thanks for the review and thanks for pointing out challenges around `join`. i've added two integration test to address your concerns. basically, in both cases `copartitionedtopicsenforcer` now chooses max partition number from the repartition topics and updates the number of partitions config accordingly. initially, there was a bug in my implementation, basically co-partitioning wasn't working properly when using `repartition` operation. it's fixed by this commit: [a link] problem was that in the `repartition` implementation, when creating new `kstreamimpl` i wasn't passing new set of source nodes, therefore `abstractstream#ensurejoinablewith` was choosing wrong `sourcenodes` for ensuring co-partitioning. i guess this wasn't problem with other internal topics, since they were inheriting number of partitions from the source topic. but since `repartition` operation may change number of partition, it was necessary to set new `sourcenodes` when creating `kstreamimpl`. new integration tests verify that max number of partitions is chosen when doing join operation. as you pointed out, it may be different what user specified, but not sure if it's a bad thing... in my mind it kinda makes sense if internal implementation of kstream chooses proper number of partitions based on the operations. not sure what's the other way around it... also, tested it when topology optimization is specified and added integration test for it: `shouldchoosemaxpartitionnumberfromsourcetopicsforjoinoperationwhentopologyoptimizationisspecified`. seems like optimization algorithm is removing/updating graph nodes without modifying `internaltopologybuilder#copartitionsourcegroups`. this was breaking `copartitionedtopicsenforcer` because nodes that are added in `copartitionsourcegroups` maybe completely removed by the optimizer. therefore, when calling `internaltopologybuilder#copartitiongroups`, it would try to get topic with old node name which results in null. i added `internaltopologybuilder#maybeupdatecopartitionsourcegroups` and it's called during optimization when node is replaced. this was quite fun debugging :) now i know much more how all co-partitioning works around repartition topics. regards, levani",1,1,1,0.9042125940322876,0.9445473551750184,0.9688828587532043,1.0,accept,unanimous_agreement
538802836,7170,"hey levani, just a heads up in case you're struggling to get a 3/3 green build with all tests passing: given the number of flaky tests it can be difficult to get a clean pass, so it's useful to record which tests failed on a given run before retesting. that way we can tell if it's the same tests failing every time, or any potentially related streams test, vs flaky tests in connect or core (ideally also create a ticket for each flaky test or if a ticket already exists, comment with the failed link, to draw attention to the flakiest and hopefully get them addressed/fixed)",0,0,1,0.5560409426689148,0.6606033444404602,0.630705714225769,0.0,accept,majority_agreement
538903669,7170,"hey , thanks for the suggestion. i've added comments on relevant jira tickets.",1,1,1,0.6998576521873474,0.943504512310028,0.967063009738922,1.0,accept,unanimous_agreement
554428862,7170,latest commit has the code that verifies number of partitions when `repartiton()` operation is used next to `join()` notable changes are: 1) introduced new `immutablerepartitiontopicconfig extends repartitiontopicconfig` which has `setnumberofpartitions` as no-op in order to make sure that number of partitions specified by the user won't be altered. 2) `copartitionedtopicsenforcer` has updated logic which throws exception whenever number of partitions do not pass the validation all is covered with unit/integration tests.,0,0,0,0.9273385405540466,0.9954074025154114,0.9604477286338806,0.0,accept,unanimous_agreement
573815440,7170,"hello , sorry for pinging, but would love to get some estimate when this pr will be reviewed. it's getting harder and harder to keep this branch in sync with the trunk. would like to finalize this while i'm able to actively support this kip. also, seems like branch builds aren't triggered, any ideas how to trigger the build?",-1,-1,-1,0.98667174577713,0.9908573031425476,0.9888665080070496,-1.0,accept,unanimous_agreement
574055141,7170,"retest this, please",0,0,0,0.9849490523338318,0.958921492099762,0.9489298462867736,0.0,accept,unanimous_agreement
576140932,7170,"totally understandable and sorry for pushing, i'm just myself excited to move this kip forward. thanks for the review, i'll address your comments shortly.",-1,1,-1,0.9868599772453308,0.5483585596084595,0.836976170539856,-1.0,accept,majority_agreement
581141756,7170,ran `streams` test suite locally. the only test that failed was `kstreamimpltest#shouldsupporttriggermaterializedwithktablefromkstream` [code block] seems like it's not connected to this pr. haven't created jira ticket / pr to fix it because as per [a link] seems like people are on it.,0,0,0,0.953980267047882,0.9901542067527772,0.992131531238556,0.0,accept,unanimous_agreement
585833956,7170,ran `streams` module tests locally. all the tests passed. can we somehow trigger the jenkins build? cc,0,0,0,0.9860244393348694,0.9575134515762328,0.9936769604682922,0.0,accept,unanimous_agreement
593048414,7170,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
596474986,7170,"hi any chance we could push this changes by 2.6 release? it's been a while :) regards, levani",1,1,1,0.9848441481590272,0.9944243431091307,0.9928820133209229,1.0,accept,unanimous_agreement
598798702,7170,ran `streams` module tests locally. all the tests passed.,0,0,0,0.9875954389572144,0.9887561202049256,0.9907034635543824,0.0,accept,unanimous_agreement
599112529,7170,"hey , yes! i'm reviewing it _now_ :) thanks so much for your patience. if it makes you feel better, i've been walking around with this sense of guilt over this pr hanging around.",1,1,1,0.9931796789169312,0.9943782687187196,0.997064173221588,1.0,accept,unanimous_agreement
599235445,7170,thanks for the update and no worries :),1,1,1,0.988965392112732,0.9926537275314332,0.951590895652771,1.0,accept,unanimous_agreement
605279103,7170,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
608775493,7170,"hi , i've addressed your comments, would appreciate another review.",1,1,1,0.6264162659645081,0.9757492542266846,0.7158279418945312,1.0,accept,unanimous_agreement
610434777,7170,"hi small update: [a link] in this commit i've added topology optimization option as test parameter. this pr touches topology optimization (indirectly). in order to make sure that everything works as expected, i though it would beneficial in the integration tests verifying both, `topology.optimization: all` and `topology.optimization: none` configurations. hope this makes sense. regards, levani",1,1,1,0.954245924949646,0.9641581773757936,0.982903778553009,1.0,accept,unanimous_agreement
610521827,7170,"wow, that's great. thanks, !",1,1,1,0.993024468421936,0.9954036474227904,0.9976577758789062,1.0,accept,unanimous_agreement
611807387,7170,merged to `trunk`. congrats ! and thanks a lot for your hard work and patience!,1,1,1,0.992205798625946,0.9958627223968506,0.9963584542274476,1.0,accept,unanimous_agreement
612049574,7170,"yes, thank you for seeing this through!",1,1,1,0.9737119078636168,0.931995153427124,0.9752649664878844,1.0,accept,unanimous_agreement
572334805,7884,this pr is ready for review. :),1,1,1,0.9868921637535096,0.9959734082221984,0.9931542873382568,1.0,accept,unanimous_agreement
572855268,7884,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
574909313,7884,"hi thanks for the comments you left! overall, i managed to simplify the code somewhat and removed a couple of methods that was probably not necessary. notably, there is not as many calls involving [code block] as there was previously. hope this was what you wanted. :)",1,1,1,0.993634819984436,0.9957828521728516,0.9965274930000304,1.0,accept,unanimous_agreement
579021231,7884,"i've mostly resolved your comments. i'm working on how we could trigger a call for a clean when the latest delete horizon had been passed. other than that, feel free to add anything else. :)",1,1,1,0.985068380832672,0.99470853805542,0.9945275187492372,1.0,accept,unanimous_agreement
581047464,7884,do you want to take another look?,0,0,0,0.9815372228622437,0.9867183566093444,0.9910902976989746,0.0,accept,unanimous_agreement
582220845,7884,pinging.,0,0,0,0.9729470014572144,0.9366099238395692,0.9158015251159668,0.0,accept,unanimous_agreement
582709395,7884,: thanks for the updated pr. will take another look.,1,1,1,0.9080039262771606,0.902018427848816,0.9380503296852112,1.0,accept,unanimous_agreement
585015794,7884,"alright, addressed most of the major comments. i thought that some of the comments might need some discussion before tackling. :)",1,1,1,0.98428612947464,0.9948775172233582,0.993286907672882,1.0,accept,unanimous_agreement
587968002,7884,"alright, cool. all done. i decided to keep retrievedeletehorizon since the [code block] flag is set in there, and afterwards, it is also used by [code block] as well. since both of them use the same flag (which is located in logcleaner), i think thats the best we can do.",1,1,1,0.9670464992523192,0.9670099020004272,0.940751314163208,1.0,accept,unanimous_agreement
588366375,7884,"alright, i think this pr is pretty close. we might need to poll for review if need be.",1,0,0,0.6244539022445679,0.8891619443893433,0.9311253428459167,0.0,accept,majority_agreement
589856382,7884,"cool, got these comments resolved.",1,1,1,0.9546191692352296,0.911602258682251,0.9867953062057496,1.0,accept,unanimous_agreement
591514108,7884,alright thanks for your patience! got everything done.,1,1,1,0.9837063550949096,0.9919753670692444,0.9947326183319092,1.0,accept,unanimous_agreement
592296237,7884,thanks for the comprehensive review of my test code changes! it looks like i was a little to aggressive with my find / replace all usage. (that lead to a lot of long.maxvalues being replaced sometimes unnecessarily by largedeletehorizon),1,1,1,0.9750655889511108,0.9907819628715516,0.9927757978439332,1.0,accept,unanimous_agreement
592297453,7884,got these comments addressed.,0,0,0,0.9754424691200256,0.985808491706848,0.9809868931770324,0.0,accept,unanimous_agreement
592984991,7884,"thanks for these comments! about the comments that i left unaddressed. turns out the tests failed if we only do two passes over the batch when we in fact need three. i added some log statements, and this was the following behavior (this is for testabortmarkerremoval): 1. on this pass, containstombstonesortxnmarker is false. i.e. we cannot remove the transaction marker yet (since oncontrolbatchread() returned false). 2. on the second pass, containstombstonesortxnmarker is now true. we can remove the transaction marker now. in this case, we have set the delete horizon, but have not _removed_ the delete horizon marker. 3. it is finally on the third pass that we can remove the transaction marker. i found this was the resulting behavior (which lead to the need for three passes). i think the situation is similar for the other control batch tests as well.",1,1,1,0.9608615040779114,0.974680244922638,0.9880820512771606,1.0,accept,unanimous_agreement
596021569,7884,"cool, we are good.",1,1,1,0.9859368205070496,0.9788628816604614,0.9939941763877868,1.0,accept,unanimous_agreement
596026016,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
596026481,7884,i also kick off system tests on this pr. [a link],0,0,0,0.983046054840088,0.9468047618865968,0.9955059289932252,0.0,accept,unanimous_agreement
596050270,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
596149722,7884,add to whitelist,0,0,0,0.9861479997634888,0.9898309111595154,0.992867112159729,0.0,accept,unanimous_agreement
596151455,7884,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
596226854,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
596226994,7884,"found the spotbugs violation. turns out when i look through the console output, i can't find the error because its never logged. (instead, its stored in some xml file which i cannot access)",0,0,0,0.9629210233688354,0.9307901859283448,0.9754653573036194,0.0,accept,unanimous_agreement
596231999,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
596667016,7884,both tests seem to be failing at the following test. [code block],0,0,0,0.9614958167076112,0.9858200550079346,0.9557912349700928,0.0,accept,unanimous_agreement
596825345,7884,can you retrigger tests? i was not able to replicate the issue in local.,0,0,0,0.7611075639724731,0.9432516694068908,0.9896768927574158,0.0,accept,unanimous_agreement
596841889,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
596896786,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
597320638,7884,"i did a little bit of research. it appears that my pr is _not_ responsible for this failing test. perhaps, some prior pr broke it. see test result for this pr: [a link]",0,0,0,0.6370382905006409,0.8777850270271301,0.968323290348053,0.0,accept,unanimous_agreement
597348217,7884,it seems that test failure was just fixed by kafka-9682. triggering another test.,0,0,0,0.9685702919960022,0.9876297116279602,0.9867435693740844,0.0,accept,unanimous_agreement
597348253,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
597348559,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
597419512,7884,all tests are green.,0,0,0,0.9462746977806092,0.970816731452942,0.987215518951416,0.0,accept,unanimous_agreement
599785168,7884,any comments on logcleaner?,0,0,0,0.985315203666687,0.9900960922241212,0.9903662204742432,0.0,accept,unanimous_agreement
600281660,7884,"i think i might know why the ""add to whitelist"" command doesn't work. do you have to be a contributor in order to trigger a test? i checked the contributors list, but from what i could tell, my account handle isn't listed (which is kind of strange, since i've submitted a handful of prs in the past).",0,0,0,0.931930661201477,0.7901523113250732,0.9488081336021424,0.0,accept,unanimous_agreement
600348752,7884,: filed [a link] to figure out the whitelist issue.,0,0,0,0.9815605878829956,0.9922077059745787,0.9937870502471924,0.0,accept,unanimous_agreement
600876622,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
601433830,7884,all comments addressed. see if there is anything else that we might need to account for.,0,0,0,0.986549437046051,0.99130779504776,0.9936394095420836,0.0,accept,unanimous_agreement
605705828,7884,pinging .,0,0,0,0.9729470014572144,0.95411479473114,0.813514232635498,0.0,accept,unanimous_agreement
613061861,7884,pinging for review,0,0,0,0.9855619072914124,0.9756925106048584,0.9886258244514464,0.0,accept,unanimous_agreement
616751692,7884,: we now have [a link] you can add yourself to jenkins's whitelist by following [a link] .,0,0,0,0.9838322401046752,0.978089153766632,0.9800145626068116,0.0,accept,unanimous_agreement
616773372,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
616776257,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
616776614,7884,cool. it's just that should i edit the [code block] as part of this pr? or will i need to do it some other way?,1,1,1,0.9817246198654176,0.7574636936187744,0.9660069346427916,1.0,accept,unanimous_agreement
616787194,7884,: you can just submit a separate pr to add yourself in .asf.yml.,0,0,0,0.9843708872795104,0.9936007857322692,0.9944809675216676,0.0,accept,unanimous_agreement
616819297,7884,"alright, got it done.",0,0,0,0.958167314529419,0.9329479336738586,0.936964213848114,0.0,accept,unanimous_agreement
617537213,7884,ok to test,0,0,0,0.9850780963897704,0.9851818680763244,0.992953360080719,0.0,accept,unanimous_agreement
617537261,7884,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
618659683,7884,"i don't think the .asf.yaml worked. tried to trigger a few test rounds, but jenkins didn't respond.",0,0,0,0.8808193206787109,0.8037184476852417,0.9506465792655944,0.0,accept,unanimous_agreement
618669412,7884,"could you try ""retest this please""? if it still doesn't work, you can file an apache infra jira for help.",0,0,0,0.9876563549041748,0.993033766746521,0.9930931329727172,0.0,accept,unanimous_agreement
618766295,7884,did try on another pr. looks like it didn't work. i will fire a jira.,0,0,0,0.9397369623184204,0.8219209909439087,0.9643859267234802,0.0,accept,unanimous_agreement
618768078,7884,reported in jira here: [a link],0,0,0,0.9859350919723512,0.992548644542694,0.9956045150756836,0.0,accept,unanimous_agreement
622490343,7884,do you have time to review? just give me a heads-up if there are some comments left unaddressed.,0,0,0,0.9128735065460204,0.9801933169364928,0.9845871925354004,0.0,accept,unanimous_agreement
737674537,7884,hi . what is the status of this pr? we are also experiencing [a link] . thanks!,1,1,1,0.9664438366889954,0.9911397099494934,0.98988276720047,1.0,accept,unanimous_agreement
738119273,7884,: this pr is mostly ready. it's just waiting for another committer more familiar with the transactional logic to take another look. : would you be able to rebase this pr? thanks.,1,1,1,0.921754777431488,0.98640638589859,0.9752163290977478,1.0,accept,unanimous_agreement
760814414,7884,this pr has been stale since april 2020. when would it be ready to merge? we are hitting this issue and it causes insanely long startup times in our applications as they need to read all the tombstones that are not being removed.,-1,-1,-1,0.976594865322113,0.8065723180770874,0.9475837349891664,-1.0,accept,unanimous_agreement
761223136,7884,migrating to a new pr. you could find it here #9915.,0,0,0,0.9768343567848206,0.9926984906196594,0.9926861524581908,0.0,accept,unanimous_agreement
289147056,2735,cc,0,0,0,0.9684008955955504,0.8343546390533447,0.9699252247810364,0.0,accept,unanimous_agreement
289222942,2735,"the newly introduced integration test seems to be flaky on jenkins. there are produce exceptions when bouncing brokers. i didn't see this locally, and i have run it 100's of times. will look into it.",0,-1,0,0.8862918615341187,0.631080150604248,0.941232681274414,0.0,accept,majority_agreement
289626999,2735,"i found the reason why the new integration test was failing. with a recent refactor of `sender.completebatch`, we only requested a metadata update on non-retriable errors. this was a regression, and as a result, when the leader for a partition changed, the metadata would not get updated and all the retries would get exhausted. i moved the metadata update code to the correct place, and the tests are passing now. i also addressed other pr comments. cc",0,0,0,0.9501988291740416,0.9784154295921326,0.7979668974876404,0.0,accept,unanimous_agreement
289627070,2735,"somehow the new integration test only failed with jdk7 or scala 2.12, neither of which i run locally. so i never caught it until this pr.",0,0,0,0.7966989874839783,0.894305408000946,0.9762826561927797,0.0,accept,unanimous_agreement
289952136,2735,"current todos from discussing with jun offline: 1. filed [a link] as a future improvement. 2. need to remove the `log.updateidmap` which removes entries before the current dirty offset from the pid map. this is a bug which would result in real entries being lost. 3. need to name the snapshot files appropriately. currently they are in the partition directory, but have the topic partition in their name, which is redundant. 4. need to ensure that `produceridmapping.maybetakesnapshot` is called periodically.",0,0,0,0.98057758808136,0.9905485510826112,0.9872446656227112,0.0,accept,unanimous_agreement
290309068,2735,"i addressed most of your comments, except two. especially for `sendandawaitinitpidrequest` comment, i would like to sync face to face tomorrow.",0,0,0,0.967641294002533,0.9327375888824464,0.8514869809150696,0.0,accept,unanimous_agreement
290622112,2735,latest run of the system tests on this branch succeeded: [a link] kicked off a muckrake run against this branch too : [a link],0,0,0,0.9829409718513488,0.981052577495575,0.9636521339416504,0.0,accept,unanimous_agreement
290625051,2735,"thanks for the review , , and .. i think i have addressed all the comments. the system tests are running as well (links above).",1,1,1,0.9492311477661132,0.9769002199172974,0.974050521850586,1.0,accept,unanimous_agreement
290880753,2735,"the jenkins builds are puzzling. mostly different streams tests fail in each run. there hasn't been a failure in core or clients for a while. the perf numbers are being collected here: [a link] so far, so good. actually enabling the idempotent producer costs 20% throughput. but using the new code with idempotent producer turned off seems to have no effect. a muckrake test is finally running properly and seems to be passing: [a link] i also addressed most of the major comments from ismael, ie. those which had to do with adding test cases or improving error messages or improving documentation. the remaining ones are more to with minor code style.",-1,-1,1,0.9250100255012512,0.4180240035057068,0.4713169932365417,-1.0,accept,majority_agreement
290935845,2735,muckrake test passed: [a link] system test against this branch also passed: [a link] investigating the jenkins branch builder failures.,0,0,0,0.9891140460968018,0.5913938879966736,0.9886499643325806,0.0,accept,unanimous_agreement
290937397,2735,latest failure (jdk8 and scala 2.11): [code block] somehow my local runs are super stable. i don't know what would cause this kind of thing.,-1,-1,0,0.8128524422645569,0.7926241159439087,0.8628448843955994,-1.0,accept,majority_agreement
291037102,2735,: thanks for the patch. lgtm,1,1,1,0.9360584020614624,0.8637609481811523,0.9780473113059998,1.0,accept,unanimous_agreement
129892398,130,it's quite awkward to see commits like this,-1,-1,-1,0.9757541418075562,0.9893029928207396,0.9930412769317628,-1.0,accept,unanimous_agreement
129935567,130,"apologies for the commits history, i was fighting with git merge history back then from two branches and hence the commits was not well organized. i will create another pr with squashed commits after addressing the collected comments from this pr.",-1,0,-1,0.9660599827766418,0.8486642241477966,0.7519853711128235,-1.0,accept,majority_agreement
131564609,130,"this is an excellent proposal, and after a quick pass this pr looks good. more detailed comments and questions to follow.",1,1,1,0.9868177175521852,0.991122841835022,0.9945967197418212,1.0,accept,unanimous_agreement
132316453,130,"is there a way to make the sources and processors created by the topology aware of or dependent upon the supplied configuration? for example, let's say my job's configuration has a parameter that, if set one way, will result in a slight variation of the topology (e.g., an extra filtering processor between the source and my primary processor). is that possible?",0,0,0,0.9829529523849488,0.9910277724266052,0.9898374676704408,0.0,accept,unanimous_agreement
132731196,130,"thanks for the comments. just to be clear we are actively working on addressing them and will respond them individually once the next version of this patch is finished, with a squashed commit history.",1,1,1,0.9366551041603088,0.9127273559570312,0.949938416481018,1.0,accept,unanimous_agreement
133943310,130,", i'm willing to help resolve issues, add test cases, make suggestions via patches, and even add javadoc. but i suspect that'd be easier after after you squash commit history. please let me know what you think.",0,1,0,0.6553279757499695,0.770794689655304,0.5706316828727722,0.0,accept,majority_agreement
135907602,130,"for ""is there a way to make the sources and processors created by the topology aware of or dependent upon the supplied configuration? for example, let's say my job's configuration has a parameter that, if set one way, will result in a slight variation of the topology (e.g., an extra filtering processor between the source and my primary processor). is that possible?"" for this case, could it be written by adding a filter with a config that depending on its value, do no-op or the real filtering logic? since all these operations will be in-memory, the only overhead will be checking the configured flag for each record, which should be negligible.",0,0,0,0.9826486110687256,0.9930872321128844,0.988286316394806,0.0,accept,unanimous_agreement
135908565,130,"thanks for being interested to contribute! we are currently shooting for a second patch next monday / tuesday, it changes the processor apis and threading models quite a bit from the original patch and hence i will also update the kip wiki accordingly. it will be great if you can then take that patch and help adding more unit tests / java docs / reviews / anything.",1,1,1,0.990461528301239,0.9919517040252686,0.9941537976264954,1.0,accept,unanimous_agreement
138945194,130,", in case you missed it, here's a pr for the npe in meteredkeyvaluestore mentioned [a link]: [a link]",0,0,0,0.9860395193099976,0.9936692118644714,0.9610273838043212,0.0,accept,unanimous_agreement
142681892,130,", i'm not sure what happened, but `build.gradle` is still missing this code: [code block] and without it the build does not upload/publish the test jar to maven (at least locally). my original pr (#34) to add these and a few other lines was merged into this branch, but it looks like auto-merge in 74455f29f2 put them in the 'tools' area rather than 'streams'. again, without it clients cannot reuse any of the test classes (like `kstreamtestdriver`) in their own tests.",0,0,0,0.9350461959838868,0.9535174369812012,0.9417872428894044,0.0,accept,unanimous_agreement
142687808,130,"you are right, the tools package was added at roughly the same time and hence probably messed up with the auto-merge. we can add that again.",0,0,0,0.977084219455719,0.9894243478775024,0.9907338619232178,0.0,accept,unanimous_agreement
142719926,130,", i also have a `keyvaluestore` implementation that maintains an in-memory, limited-size lru cache (e.g., `inmemorylrucachestore`) of recently used entries. sometimes a processor wants to track up to _n_ recently-used items, and this is a convenient way to do this. i'm willing to contribute it via a new pull request if you are interested. it does need a few enhancements to `meteredkeyvaluestore` to better support entries being ""automatically"" removed by the 'inner' store. but a larger problem is that `meteredkeyvaluestore` currently assumes it can get the key and value serializers and deserializers from the context. iiuc this is not always valid since those are merely the _default_ key and value serializers and deserializers. for example, i might have a topology that uses multiple processors with different key and value types, in which case the context's default (de)serializers will be valid for only one of the processors. or, i might have a single processor that uses a key value store with different key and value types than the processor. so this seems like an invalid assumption that needs to be fixed. if you agree that the latter is a problem, i'd be happy to create a pr to address it. if you're interested in the `inmemorylrucachestore` class, that could be a separate pr.",0,0,0,0.9240363240242004,0.9612469673156738,0.9519295692443848,0.0,accept,unanimous_agreement
142749083,130,"i agree about ser-de, maybe we can allow users to pass in their (de)serializers upon construction of the key-value store. about `inmemorylrucachestore`, feel free to go ahead with another pr.",0,0,0,0.905586302280426,0.8756037354469299,0.9827390909194946,0.0,accept,unanimous_agreement
142762261,130,", see my [a link] for the serdes changes; should be up-to-date with the `streaming` branch. feedback appreciated.",1,1,1,0.6426153779029846,0.958825945854187,0.900597095489502,1.0,accept,unanimous_agreement
143052732,130,", i created a pr for the [a link] and a mini test framework for key-value stores, with new unit tests for all current `keyvaluestore` implementations. it does depends on my proposed [a link] changes in key-value stores.",0,0,0,0.9826716780662536,0.9920517206192015,0.9880563020706176,0.0,accept,unanimous_agreement
143380723,130,"update the pr body before merging, and the original message is here: --- some open questions collected so far on the first patch. thanks . - topology api: requiring users to instantiate their own topology class with the overridden build() function is a little awkward. instead it would be great to let users explicitly build the topology in main and pass it in as a class: [code block] so the implementation of kstream.filter look instead like this: [code block] the advantage is that the user code can now get rid of the whole topology class with the builder. i think the order of execution for that api is quite unintuitive. - we can probably move the forward() function from processor to processorcontext, and split processorcontext into two classes, one with all the function calls as commit / send / schedule / forward, and another with the metadata function calls as topic / partition / offset / timestamp. - can we hide the chooser interface from users? in other words, if users can specify the ""time"" on each fetched messages from kafka, would a hard-coded mintimestampmessagechooser be sufficient so that we can move timestamptracker / recordqueue / chooser / recordcollector / etc all to the internal folders? - shall we split the o.a.k.clients into two folders, with o.a.k.clients.processor in stream? or should we just remove o.a.k.clients.processor and make everything under o.a.k.stream? in addition, currently there is a cyclic dependency between that two, would better to break it in the end state. - consider moving the external dependencies such as rocksdb into a separate jar? for example we can just include a kafka-stream-rocksdb.jar which includes the rocksdbkeyvaluestore only, and later on when we deprecate / remove such implementations we can simply remove the jar itself. - the way the keyvaluestore is created seems a bit weird. since this is part of the internal state managed by kafkaprocessorcontext, it seems there should be an api to create the keyvaluestore from kafkaprocessorcontext, instead of passing context to the constructor of keyvaluestore. - merge processorconfigs with processorproperties. - we can potentially remove the processor argument in processorcontext.schedule().",1,1,1,0.909960687160492,0.9604232907295228,0.973282754421234,1.0,accept,unanimous_agreement
320162288,3621,"can you help review this patch if you have time? thank you! this patch has implemented all features in kip-113 except the ability to move replica between log directories on the same broker. the implementation of that feature is more complicated because it requires the creation of a temporary replica on the broker. thus i choose to separate the implementation of kip-113 into two patches. note that this patch allows user to specify the destination log directory of a replica if the replica has not been created on the broker yet. more specifically, user can use adminclient.changereplicadir() before creating the reassignment znode so that the replica will be created in the destination log directory when broker receives leaderandisrrequest later. this allows user to balance load across log directories using the `kafka-reassign-partitions.sh` as long as the replica is always moved to a given log directory on another broker. the caveat is that the replica may be created in the wrong log directory if the broker restarts after it received changereplicadirrequest but before it receives leaderandisrrequest.",1,1,1,0.9485745429992676,0.9409767389297484,0.9898087978363036,1.0,accept,unanimous_agreement
321054272,3621,tests have been added to cover every new request/response/api and i have reviewed the patch end-to-end. it is fully ready for review now. thanks!,1,1,1,0.9857199788093568,0.9887617230415344,0.9894271492958068,1.0,accept,unanimous_agreement
322349033,3621,thanks much for your review. i have addressed all comments and reviewed the patch end-to-end. can you take another look? thanks!,1,1,1,0.9809439778327942,0.9928439259529114,0.9918391108512878,1.0,accept,unanimous_agreement
323557883,3621,"thanks much for taking time to review the patch! i have addressed most of the comments and rebased the patch onto trunk head. the only one that i am not sure is whether we should rename `alterreplicadirrequest` to `alterreplicalogdirrequest` (and similarly for other related classes). after this one is addressed, i will review the patch myself end-to-end and let you know.",1,1,1,0.9869759678840636,0.9355494379997252,0.9891352653503418,1.0,accept,unanimous_agreement
323606600,3621,"could you please provide some comment as to whether we should rename `alterreplicadirrequest` to `alterreplicalogdirrequest`? prefer `alterreplicalogdirrequest` so that it is more explicit that we are referring to log directory instead of arbitrary directory. on the other hand, i prefer `alterreplicadirrequest` because i think `alterreplicalogdirrequest` is a bit verbose. i also think it should be clear enough to user/developer that the dir means log dir as long as the field name in `alterreplicalogdirrequest` is `log_dir`.",0,0,0,0.9690855741500854,0.9852801561355592,0.9878121018409728,0.0,accept,unanimous_agreement
323897171,3621,i can take a look at the patch in the next day or two.,0,0,0,0.981817901134491,0.981544017791748,0.9774935245513916,0.0,accept,unanimous_agreement
324254692,3621,thanks much for taking time to review the patch! i have addressed all the comment. can you take another look when you get time?,1,1,1,0.9891568422317504,0.9872738122940063,0.9920433163642884,1.0,accept,unanimous_agreement
324383296,3621,"thanks for the patch. it looks good overall. i think we should split the changes to the options class, to create a base class, into a separate jira. it's not related to kip-113. i'm also not completely convinced we should do it, but perhaps we can discuss that in another jira or on the mailing list.",1,1,1,0.9807642698287964,0.988760769367218,0.9902515411376952,1.0,accept,unanimous_agreement
324399622,3621,thanks much for the comment! i personally don't mind whether we refactor the options class in this patch or in a separate patch. i just talked to and he prefers to do it in this patch if there is no concern. the argument is that this patch adds three more option classes to apache kafka and it is reasonable to try to reduce the extra code in this patch. can you explain a bit more what is your concern with the change to the option class? we can move it to another jira if there is anything worth further discussion about this change.,1,1,1,0.9853582382202148,0.9756927490234376,0.9908223152160645,1.0,accept,unanimous_agreement
325093325,3621,"thanks much for your review! i have addressed all comments except for the last one which we are discussing. and i have gone over the patch end-to-end, removed unused imports i can find and improved comments. the patch has been rebased onto trunk head.",1,1,1,0.988018035888672,0.9897867441177368,0.9891772866249084,1.0,accept,unanimous_agreement
326144106,3621,thanks for taking time to review the patch! i have rebased patch onto trunk head and addressed all comments from jun. do you have time to take a look and commit the patch? thanks!,1,1,1,0.9834633469581604,0.9921940565109252,0.9946194887161256,1.0,accept,unanimous_agreement
326774010,3621,"thanks much for taking time to review the patch again in detail! i have updated the patch to add comments and throw exception if an expected error is found. regarding the error code in describelogdirresponse, it seems that there is currently no realistic issue with assuming cluster_authorization_failed when the response is empty -- it doesn't affect correctness or performance. then the reason for adding the response level error is purely ideological, which applies to other requests as well (e.g. describe_groups_response or describe_configs_response). we have discussed this offline previously. it probably makes sense to have a response level error for every response instead of only describelogdirresponse. can i open a separate ticket and make a separate pull request for this? i would like to avoid making non-trivial change to this patch at this moment so that it can be finished soon. i feel that the use of this extra field in describelogdirresponse alone is kind of unnecessary and less useful. it probably makes more sense to add the response level error in `abstractresponse` to solve a bigger problem. or we can add this response level error to `describelogdirresponse` in the future when it is needed. doe this make sense?",1,1,1,0.9791656136512756,0.8356395959854126,0.9889410138130188,1.0,accept,unanimous_agreement
326786682,3621,"yeah, it is probably worth adding a response level error for all the requests. we can do it in separate kip.",0,0,0,0.9842915534973145,0.9898660778999328,0.9871929883956908,0.0,accept,unanimous_agreement
326787087,3621,"thanks for the patch. merged to trunk. thanks a lot for the review, and",1,1,1,0.9750394821166992,0.990961730480194,0.9934816360473632,1.0,accept,unanimous_agreement
341874911,3621,"i think this pr broke binary compatibility for the adminclient, code compiled against 0.11.0 fails like: [code block]",-1,-1,0,0.5278162360191345,0.5005799531936646,0.9540842175483704,-1.0,accept,majority_agreement
341942416,3621,"this patch added new methods in the interface adminclient. according to [a link] this should not cause binary incompatibility. not sure why you see that error. i also tried the following steps to hopefully reproduce the error: - git clone -b upgrade-to-11.0.0 [a link] - compile with ./gradlew jar - replace ./build/dependant-libs/kafka-clients-0.11.0.0.jar with kafka-clients-1.0.0-snapshot.jar, where kafka-clients-1.0.0-snapshot.jar is generated by apache kafka adefc8ea076354e. - run ./bin/kafka-monitor-start.sh config/kafka-monitor.properties it appears that the project can still be compiled and run after i replaced the jar. can you tell me how i can reproduce this error? also, do you know which change in this patch can break binary incompatibility, e.g. addition of new methods?",0,0,0,0.9508890509605408,0.9834765195846558,0.9787068367004396,0.0,accept,unanimous_agreement
341957639,3621,"sorry, i thought it was obvious from the error message, but i noticed now that the person that ran into it only pasted the method affected and not the actual error. the binary compatibility issue is due to the removal of the `timeoutms` method from various classes. the method now exists in the abstract class, so code that is recompiled works (source compatible), but code that is not recompiled breaks with a `nosuchmethoderror`. we should reintroduce the removed methods in 1.0.1 and trunk to fix the issue. would you mind filing a jira, please?",-1,-1,-1,0.9573527574539183,0.96034437417984,0.9768127202987672,-1.0,accept,unanimous_agreement
342006765,3621,sure. i created [a link] we can continue discussion there.,0,0,0,0.9477649927139282,0.9829513430595398,0.9813448190689088,0.0,accept,unanimous_agreement
571192193,7898,note: - [a link] - [a link],0,0,0,0.9840960502624512,0.9915319085121156,0.9897171258926392,0.0,accept,unanimous_agreement
571371789,7898,this requires a kip since the log4j2 config is not compatible with log4j.,0,0,0,0.97925466299057,0.9624673128128052,0.9916455149650574,0.0,accept,unanimous_agreement
571505061,7898,no problem. thank you for your guidance. :smile: stay tuned!,1,1,1,0.9907185435295104,0.9958890080451964,0.9973828196525574,1.0,accept,unanimous_agreement
574137873,7898,log4j 2.13.0 contains experimental support for some log4j 1 configuration flies. see [a link],0,0,0,0.9836040139198304,0.9914404153823853,0.9917035102844238,0.0,accept,unanimous_agreement
574187380,7898,"that's awesome, thanks for sharing.",1,1,1,0.9869013428688048,0.99495267868042,0.9937467575073242,1.0,accept,unanimous_agreement
589484883,7898,"i just had to deal with configuring filtered logs in kafka and was shocked to find it uses log4j 1.2.17. log4j v1 has been dead since 2015. 5 years ago it was known that it should not be used for any new designs and applications should migrate to v2, yet still apache kafka stuck with that? wanted to raise a ticket, but there is this pr, which does not seem to be getting merged anywhere... dear lord...",-1,-1,-1,0.9876533150672911,0.9914593696594238,0.9947354197502136,-1.0,accept,unanimous_agreement
589655512,7898,"i am sorry to hear that. i am almost done the kip and will start the discussion next week. if you are interested in this issue, please join in. have a nice weekend.",-1,-1,-1,0.9895959496498108,0.9929510354995728,0.9738194346427916,-1.0,accept,unanimous_agreement
603000463,7898,"man, i really wish more apache projects started this ""upgrade""",1,-1,-1,0.5137198567390442,0.6715071797370911,0.971099019050598,-1.0,accept,majority_agreement
603269642,7898,"sorry for the delay. while working on this issue, i found that this upgrade is much more complicated than i first expected; it is related to lots of module dependencies, api changes, test code modification, and providing backward-compatibility for the logging configuration. anyway, it is almost done. i successfully upgraded the whole project and now working with some race conditions on test suites. i hope i can complete it in a couple of days. :winking_face:",-1,-1,-1,0.9911853671073914,0.9922915101051332,0.9734202027320862,-1.0,accept,unanimous_agreement
603402312,7898,"no worries. i didn't realize the backwards compatible issues either. i was under the impression that slf4j bridges handled that. personally, i've been using logback successfully for years",1,0,1,0.8063579797744751,0.5549811124801636,0.929606556892395,1.0,accept,majority_agreement
604911816,7898,"so, i'm actually working on a project that i just started... can verify (part of) this log4j-slf4j + slf4j-log4j12 bridging definitely works without issue [a link] notice too [code block]",0,0,0,0.969405710697174,0.9831418991088868,0.9715738892555236,0.0,accept,unanimous_agreement
605995589,7898,"here is the wip update. i almost completed the migration into log4j2 `2.13.1` but here are some issues: 1. i migrated all test methods into logj42 api, using the way log4j2 itself does; however, i failed to migrate `statemanagerutiltest`. it seems like this class is tightly coupled with kafka streams' slf4j logging mechanism, but i don't understand it yet. 2. the updated test suites working correctly when running individually, but if run at once (e.g., `./gradlew :streams:test`), some test suites go so flaky. i am also tracking down the reason. if you can give me some advice, it will be a great help! :smiley:",0,0,0,0.9227578639984132,0.9874868392944336,0.947777271270752,0.0,accept,unanimous_agreement
607126286,7898,"i hadn't realised you were working on this via kafka-9366. i was looking at it via kafka-1368. you've made more progress than me, so happy for you to take it forward. but there are a couple of issues i noticed in the course of my effort: 1. the `log4jcontrollermbean.getloggers` returns a scala wrapper implementation of `java.util.list`, which means that jmx tools (e.g. `jvisualvm`) can't deserialize the list unless they have scala library on their classpath. that's easily fixed by returning a `java.util.arraylist` copy of the list. 2. i suspect you already know this, but with log4j there were loggers for things like `kafka.controller` because they appeared in the config file, even though a logger with that name was never created in the code. because log4j2 separates loggers and logger configurations the call `logcontext.getloggers` only returns the loggers created in code. so afaics you won't be able to change the log level for `kafka.controller`, and would have to change the level of all descendant loggers individually. i guess this is a regression, since `log4jcontroller` is used for `alterconfigs` rpc.",1,1,1,0.9181920289993286,0.9175460934638976,0.9915661811828612,1.0,accept,unanimous_agreement
607328633,7898,"here is the update, with rebasing onto the latest trunk. now all tests run properly. however, it still has a problem: when i ran the test suites individually (e.g., `./gradlew :streams:test --tests org.apache.kafka.streams.kafkastreamstest`) all tests passes clearly. however, if i tried to run them in package-wide (e.g., `./gradlew :streams:test --tests org.apache.kafka.streams.internal.*`) or all-in-once (e.g., `./gradlew :streams:test`) the tests go so flaky and some tests fail, with one of the following error messages: [code block] [code block] to understand these errors, here is a background: to validate the logging messages, kafka has been used a test appender (i.e., `listappender` here) attached to the root logger. although this pr updates log4j 1.x to 2.x, the overall approach has not changed. this approach works well when running individually, but when running in batch, the test suite randomly fails to initialize the `listappender` (type 1 error) or forward the log message to `listappender`(type 2 error); i ran the tests more than a hundred of times but what i found was it is totally random and be affected by gradle's `maxparallelforks` parameter - it seems like there is a problem deep inside of log4j here. i have been working on this issue for the last week could not find any perfect solution. if any of you have some ideas on this, it would be a great help. thanks in advance. :smiley:",0,0,0,0.9747548699378968,0.9911019802093506,0.8649480938911438,0.0,accept,unanimous_agreement
607331030,7898,thanks for your valuable comments - absoultely it will be a great help! i am now applying the comments from so as soon as it is completed i will review your comments again and leave a feedback. :smile:,1,1,1,0.9938311576843262,0.9958969354629515,0.9974173307418824,1.0,accept,unanimous_agreement
607512708,7898,"i've written test appenders in other projects just to verify contents of log messages, but not really sure the implications of race conditions on that.",0,0,0,0.9174147844314576,0.943647801876068,0.9004678130149841,0.0,accept,unanimous_agreement
612980927,7898,"it seems like we need to consult to log4j mailing list. okay, i will have a try.",0,0,0,0.8325097560882568,0.8198493123054504,0.982988178730011,0.0,accept,unanimous_agreement
613082686,7898,e.g. [a link],0,0,0,0.979103147983551,0.9896820187568665,0.9937499165534972,0.0,accept,unanimous_agreement
628732206,7898,what is timeline for merging this pr?,0,0,0,0.9839332103729248,0.990494966506958,0.9949210286140442,0.0,accept,unanimous_agreement
636188471,7898,"can please submit a kip for this? we should have a better good idea of the compatibility implications by now, right?",0,0,0,0.9656590819358826,0.9686713814735411,0.9923316240310668,0.0,accept,unanimous_agreement
636191680,7898,"oh, i had missed the comment about the errors we are seeing when running the tests. it may be worth upgrading to the latest release in case it has been fixed.",-1,0,0,0.5816662311553955,0.8038458228111267,0.9867563247680664,0.0,accept,majority_agreement
637684403,7898,"all // sorry for being late, i just got out from my last project; i will have a look at this pr this weekend.",-1,-1,-1,0.989085614681244,0.9935500621795654,0.9863424897193908,-1.0,accept,unanimous_agreement
648795743,7898,"here is the fix. i completed to implement all the features, migrating tests to follow log4j2 api, and rebasing onto the latest trunk, but there is a problem in logging message validation. ![a link] when i run `:stream:test` task in my dev environment, the following 10 tests fail: - `streamsconfigtest`: `shouldlogwarningwhenpartitiongrouperisused` - `kstreamktablejointest`: `shouldlogandmeterwhenskippingnullleft[key,value]withbuiltinmetricsversion[latest,0100to24]` - `inmemorysessionstoretest`: `shouldlogandmeasureexpiredrecordswithbuiltinmetricsversion[latest,0100to24]`, `shouldnotthrowinvalidrangeexceptionwithnegativefromkey` - `rocksdbtimestampedstoretest`: `shouldmigratedatafromdefaulttotimestampcolumnfamily`, `shouldopennewstoreinregularmode` however, if i run the test suites `streamsconfigtest`, `kstreamktablejointest`, and `inmemorysessionstoretest` individually, they work fine. and if i run `rocksdbtimestampedstoretest` test suite, it fails; the expected log message does not forwarded to the appender. ![a link] in contrast, if i run the test methods individually, they also work fine: ![a link] it seems like there is a problem with log4j in forwarding the log message to the appender. (or is the appender closed before the log message arrives?) but i can't certain; i followed the way log4j2 test suites do, but could not find similar cases in their codebase. i tried to fix this problem for several days but not succeeded. if you have some spare time, could you check out this pr and run the tests on your machine? i am working with ubuntu 20.04 + openjdk 8. i am curious the same tests also fail in the other environments. cc/",0,0,0,0.8758523464202881,0.964922308921814,0.6385635137557983,0.0,accept,unanimous_agreement
649282794,7898,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
668034540,7898,"finally, it is finished! :congratulations: all features and tests are now successfully migrated into log4j2 api and passes clearly! i changed the pr title and preparing kip. stay tuned! :smiley: cc/",1,1,1,0.9858044981956482,0.9959114789962769,0.9961482286453248,1.0,accept,unanimous_agreement
669229785,7898,here is the kip - [a link] could you have a look? i have thought you must be the perfect reviewer [a link]. :smile:,1,1,1,0.98047935962677,0.991881787776947,0.9960343241691588,1.0,accept,unanimous_agreement
673145549,7898,what is timeline for merging this pr and making this upgrade available?,0,0,0,0.9841766357421876,0.9903131723403932,0.9950928688049316,0.0,accept,unanimous_agreement
673305906,7898,it is what exactly i hope to ask the pmc members and committers. it seems like they are too busy right now - let us wait for a while. +1. rebased onto the lastest trunk.,0,0,0,0.723605751991272,0.496352881193161,0.9763384461402892,0.0,accept,unanimous_agreement
691668375,7898,"rebased onto the lastest trunk, with including sliding window support. could you have a look?",0,0,0,0.984546959400177,0.9867115020751952,0.9915838241577148,0.0,accept,unanimous_agreement
693414587,7898,could you have a look? :pray:,-1,1,1,0.9599862098693848,0.9741914868354796,0.982730269432068,1.0,accept,majority_agreement
693453914,7898,has the kip been approved?,0,0,0,0.9857859015464784,0.9920247793197632,0.9951505661010742,0.0,accept,unanimous_agreement
693552944,7898,of course not. but i hoped to reboot the discussion. (thanks for the kind reponse! :+1:),1,1,1,0.9866970181465148,0.9955262541770936,0.9965319633483888,1.0,accept,unanimous_agreement
701008405,7898,"hi all, here is the update. all compatibility breaks caused by the root logger name change between log4j and log4j2 (`""root""` → `""""`) is now resolved. plus, i also migrated raft module into log4j2. cc/",0,0,1,0.973707377910614,0.6376491189002991,0.8396101593971252,0.0,accept,majority_agreement
733591265,7898,"here is the update. i rebased the pr onto the latest trunk, with: 1. upgrade log4j to log4j2: this is the main commit. 2. trivial improvements (typos, etc.) 3. wip: enable ignored tests in `plaintextadminintegrationtest` ([a link], cc/ ) it now includes 's [a link]. cc/",0,0,0,0.9857109189033508,0.9797842502593994,0.9745101928710938,0.0,accept,unanimous_agreement
754531924,7898,the newest kafka version is 3.7.0.could i know the reseason that change the log4j 1.x to log4j 2.x isn't merged.,0,0,0,0.9882977604866028,0.9832430481910706,0.9946868419647216,0.0,accept,unanimous_agreement
776772272,7898,"rebased onto the latest trunk, with including [a link] and [a link] by could you have a look? :pray: as you already know, [a link] is approved. sorry for being late. i am planning to provide a customized preview version for this feature.",-1,-1,-1,0.9902042150497437,0.9927697777748108,0.9603076577186584,-1.0,accept,unanimous_agreement
777453160,7898,"regarding the cve-2019-17571 from [a link] is there another way to mitigite the risk? we're looking for a temporary solution until this pr finally gets approved, but are not sure if and how the vulnerability could even get exploited. any thoughts?",0,0,0,0.8318098187446594,0.9629318714141846,0.968582272529602,0.0,accept,unanimous_agreement
777454845,7898,"which version are you using? i am now working for a customized patch for 2.6.0 and 2.7.0, with docker image.",0,0,0,0.987908661365509,0.9889630675315856,0.9936129450798036,0.0,accept,unanimous_agreement
777461164,7898,"we're using strimzi/kafka / 0.21.0-kafka-2.7.0 our sca scanning tool (jfrog xray) found this cve among many others (speaking of third party lib cves only). we're just wondering if there's a way (e.g. via message sanitizing or logging config adjustments, etc.) to be sure the mentioned cve cannot be exploited.",0,0,0,0.9799916744232178,0.9911422729492188,0.9914571642875672,0.0,accept,unanimous_agreement
777469267,7898,"great. :+1: as soon as i complete the custom release, i will have a look at strimzi docker image. i guess it will take at least one week.",1,1,1,0.9882121682167052,0.9957772493362428,0.9970223307609558,1.0,accept,unanimous_agreement
777482442,7898,alright thx for your fast response! we'll keep an eye on this pr :),1,1,1,0.994149088859558,0.9952521324157716,0.9923762679100036,1.0,accept,unanimous_agreement
783322141,7898,"i have similar question, can this security vulnerability cve-2019-17571 get exploited. i use kafka operator from banzaicloud 0.12.3/ kafka:2.13-2.6.0 when will the custom release be available? thanks",1,1,1,0.7811289429664612,0.7318599224090576,0.9573230147361756,1.0,accept,unanimous_agreement
784202224,7898,"sorry for being late. finally, here it is! i succeeded to backport this feature to the latest 2.7.0 release. (commit 466b798412) - working branch: [a link] - custom build distribution: [a link] - 2.7.0 patch: [a link] now i will consult to strimzi, banzaicloud team for a custom release. i am also preparing a graalvm based docker image distribuion [a link]. stay tuned! :smile: cc/",-1,-1,1,0.988741099834442,0.9817233681678772,0.890070915222168,-1.0,accept,majority_agreement
784901850,7898,"rebased onto the latest trunk, with minor log4j2 configuration corrections. if you already downloaded the patch & tarball above, please re-download it. +1. i built a [a link] and validated it in my minikube cluster. as you can see here, it now works like a charm with `log4j-slf4j-impl-2.14.0`! :smile: ![a link]",1,1,1,0.9811906218528748,0.9924463629722596,0.9942375421524048,1.0,accept,unanimous_agreement
786531132,7898,thanks have you consulted banzaicloud about this patch?,0,0,0,0.7804803848266602,0.5197148323059082,0.9296131730079652,0.0,accept,unanimous_agreement
786645278,7898,"sure. here is a guide from the banzaicloud team. (see: [a link] by replacing the docker image to [a link], you can make use of this feature. (don't forget to specify `kafka_log4j_opts` to `""-dlog4j.configurationfile=file:{kafka.home}/bin/../config/log4j2.properties""`) [code block]",0,0,0,0.9593420624732972,0.9862512350082396,0.9556120038032532,0.0,accept,unanimous_agreement
788957476,7898,"to 's question, is the vulnerability invoked by kafka or does it lie dormant?",0,0,0,0.9750936627388,0.978379487991333,0.9851598739624025,0.0,accept,unanimous_agreement
789662902,7898,"really appreciate your guidance here. thanks for the patch. if i chose to not to move to this patch right away, can you please confirm that this vulnerability in log4j (cve-2019-17571) doesn't affect kafka? thanks",1,1,1,0.985403060913086,0.9848308563232422,0.9956637024879456,1.0,accept,unanimous_agreement
789735966,7898,"i'm sorry that i can't be certain. but as far as i know, any project with log4j 1.2.7 is not safe. (it is why i have been working on this issue.) +1. i also released 2.6.1 backport. - working branch: [a link] - custom build distribution: [a link] - 2.6.1 patch: [a link]",-1,-1,-1,0.9888995885849,0.9888463616371156,0.9850987195968628,-1.0,accept,unanimous_agreement
790427075,7898,when will this fix make it in to one of kafka upstream release? thanks,1,1,1,0.912550449371338,0.7115855813026428,0.762908935546875,1.0,accept,unanimous_agreement
790429327,7898,we now checked kafka's source code for any appearances of the socketserver class or corresponding config files but were not able to find any. furthermore we took a closer look at the listening ports inside the running containers. conclusion: it looks like the affected socketserver class is not used by kafka.,0,0,0,0.9812687039375304,0.9940044283866882,0.9878722429275512,0.0,accept,unanimous_agreement
790473929,7898,"since this kip is already passed, it will be included in 2.8.0 release.",0,0,0,0.9886198043823242,0.9924105405807496,0.9947497248649596,0.0,accept,unanimous_agreement
800184688,7898,"rebased onto the latest trunk, with migrating the tests module into log4j2 and windows support.",0,0,0,0.9877420663833618,0.9918814301490784,0.9928258657455444,0.0,accept,unanimous_agreement
829970019,7898,rebased onto the latest trunk.,0,0,0,0.9861706495285034,0.9845799207687378,0.993498921394348,0.0,accept,unanimous_agreement
868564730,7898,thanks for all the effort on this feature. do you have a target eta on this to be merged?,1,1,1,0.9279800057411194,0.9771455526351928,0.9589995741844176,1.0,accept,unanimous_agreement
869010612,7898,"thanks for the interest in this feature. this feature will be included in the 3.0.0 release. if you need this feature urgently, i am now backporting it to 2.7.1 and 2.8.0. please refer [a link].",1,1,1,0.949764609336853,0.8925004601478577,0.9691062569618224,1.0,accept,unanimous_agreement
869010720,7898,"if you need the log4j2 appender, see [a link].",0,0,0,0.9869383573532104,0.9904791712760924,0.994956910610199,0.0,accept,unanimous_agreement
876149123,7898,it's been a week since feature freeze for ak 3.0 and this pr is open (with several conflicts). please let me know asap if this pr is not ready and kip-653 needs to be removed from the 3.0 plan,0,0,0,0.975361943244934,0.9700526595115662,0.962074875831604,0.0,accept,unanimous_agreement
876265899,7898,rebased onto the latest trunk. could anyone review this pr? :bow: cc/,0,1,1,0.9033657908439636,0.9900938868522644,0.9617976546287536,1.0,accept,majority_agreement
876773034,7898,do you have a patch that will work with 2.8.0?,0,0,0,0.988628625869751,0.9927090406417848,0.9942715167999268,0.0,accept,unanimous_agreement
879046873,7898,thank you so much for being so interested. 2.8.0 backport preview will be released this week.,1,1,1,0.9699243307113647,0.9474841952323914,0.986135721206665,1.0,accept,unanimous_agreement
911500667,7898,hi guys is there any plan to release this fix soon? thanks ashish,1,1,1,0.9716354608535768,0.971365749835968,0.9885714650154114,1.0,accept,unanimous_agreement
915363359,7898,"many thanks for your interest in this feature. i think it will be released with ak 3.1, but i can't be certain yet. please refer [a link] if you need a preview or a custom patch for [2.6.1, 2.8.0]. cc/",1,1,1,0.9343301653862,0.9553973078727722,0.9755988717079164,1.0,accept,unanimous_agreement
950328049,7898,rebased onto the latest trunk. could you have a look when you are free?,0,0,0,0.9836496114730836,0.981653928756714,0.9912431240081788,0.0,accept,unanimous_agreement
974801081,7898,the reason for migration isn't cve-2019-17571. this doesn't impact us (kafka don't use socketserver). upgrading from log4j 1.2.17 to log4j 2 can be done because of log4j 2 features but not for this(cve-2019-17571) can you clarify this..?,0,0,0,0.983661949634552,0.9880225658416748,0.9940127730369568,0.0,accept,unanimous_agreement
976480514,7898,"hi , agree. after reconsidering the issue, i concluded that [a link] is rather a minor issue; it is only problematic only when the user tries to use the `socketserver` appender. i think the primary reason for migration is log4j2 features like syntax. (for example, using log4j 1.x syntax is confusing since it is already obsolete several years ago.)",0,0,0,0.9498513340950012,0.9339755773544312,0.9127137660980223,0.0,accept,unanimous_agreement
991150391,7898,will this pr solve [a link]?,0,0,0,0.9872587323188782,0.9913177490234376,0.9951272010803224,0.0,accept,unanimous_agreement
991209328,7898,[a link] should be upgraded to 2.15.0. log4j <= 2.14.0 all have this issue. initially i thought log4j 1.x is not impacted but as per [a link] it is.,0,0,0,0.9851621985435486,0.9929494857788086,0.9813018441200256,0.0,accept,unanimous_agreement
991705948,7898,thank you for sharing the comment. isn't that comment for log4j v1 in general. kafka by default does not use jms appender. do you think it is impacted under the default configuration. also refer to this post: [a link],1,1,1,0.860988199710846,0.9031342267990112,0.9463356733322144,1.0,accept,unanimous_agreement
991894590,7898,"you must be already aware about the new log4j zero day vulnerability with log4j 2.14.1 versions and below that. hope when you will be done with this merge, you will take it to 2.15.0 and then make it available in the final release. thanks for your efforts.",1,1,1,0.9646920561790466,0.9847577810287476,0.9790661931037904,1.0,accept,unanimous_agreement
992556542,7898,yeah -user . my understanding is same too. this should not impact unless there is use of jms.,0,0,0,0.9773390889167786,0.8892613649368286,0.9917056560516356,0.0,accept,unanimous_agreement
994740102,7898,"hi all, sorry for being late. here is the update! this pr is rebased onto the latest trunk and now uses log4j2 2.16.0 to address [a link] and [a link]. according to [a link], log4j 1.x is [a link] but, only when it uses jms appender. so, unless you are using the jms appender, you are safe from this vulnerability.",-1,-1,-1,0.9906755685806274,0.9914693236351012,0.9581974148750304,-1.0,accept,unanimous_agreement
995250657,7898,it would be great if you can provide this change as part of your preview releases for 2.7.1 and 2.8.0,1,0,1,0.7216307520866394,0.8811469674110413,0.5590577721595764,1.0,accept,majority_agreement
997166170,7898,"all // you can find a preview of apache kafka 3.0.0 w/ log4j2 2.16.0 [a link]. -8 i have no plan for 2.7.0 or 2.8.0 but, currently working on 2.8.1 now.",0,0,0,0.982110321521759,0.9748008847236632,0.9851706624031068,0.0,accept,unanimous_agreement
1005938463,7898,how hard is it to backport this all the way back to kafka v2.3.x?,0,0,0,0.6030865907669067,0.9758589267730712,0.9873917698860168,0.0,accept,unanimous_agreement
1006282789,7898,almost impossible. there are too many conflicts.,-1,-1,-1,0.8238853812217712,0.9027334451675416,0.6337986588478088,-1.0,accept,unanimous_agreement
1006913721,7898,"are there any firm plans at this time for when log4j v2 support in kafka will be released, and in what release(s)?",0,0,0,0.9867968559265136,0.994779109954834,0.9922130703926086,0.0,accept,unanimous_agreement
1006920498,7898,"just for information, the use of an obsolete version of log4j is now seen as a critical vulnerability for some security scanners. we received this ""critical warning"" from tenable nessus on our kafka cluster: [code block]",0,0,0,0.9736201763153076,0.9797942042350768,0.9889744520187378,0.0,accept,unanimous_agreement
1007242660,7898,"i hope ak 3.2.0, but can't certain yet.",0,0,0,0.9560609459877014,0.867408812046051,0.9341417551040648,0.0,accept,unanimous_agreement
1007353055,7898,"when is ak 3.2.0 targeted to be released? as indicated, lack of support for log4j2 is increasingly seen as a critical vulnerability, and some corporations using ak are urgently looking for a resolution.",-1,0,0,0.5175031423568726,0.9660534262657166,0.989855170249939,0.0,accept,majority_agreement
1007366645,7898,"i can't certain since i am not a committer. but if someone urgently needs a log4j2 based version, there is [a link]. a 3.1 based one will be also released as soon as the official ak 3.1 is released.",0,0,0,0.9701898694038392,0.96142840385437,0.9755849242210388,0.0,accept,unanimous_agreement
1009197651,7898,"i might be slightly late for the party, however, have you explored the possibility of resurrecting and releasing 1.x? i do think it is worth resurrecting 1.x and fix the critical issues. here are the relevant threads: * dev (my initial proposal): [a link] * general (after logging pmc suggested re-incubating 1.x): [a link] (<-- there are people who expressed interest in resurrecting 1.x) if you are interested in log4j 1.x releases (e.g. review, test changes), it would be great if you could comment on general thread.",0,0,1,0.9502948522567748,0.9539768099784852,0.9141404628753662,0.0,accept,majority_agreement
1009624788,7898,not yet. kip-653 has been started long before the log4j crisis of 2020.,0,0,0,0.9653391242027284,0.9893575310707092,0.9733843803405762,0.0,accept,unanimous_agreement
1009629904,7898,"if the only motivation for moving 1.x -> 2.x is security, then it might be better for both kafka and kafka users to stick with 1.x and ask/wait/help with releasing a hardened 1.x. then the users won't need to learn new configuration/debugging/maintenance approaches of 2.x.",0,0,0,0.9831620454788208,0.9933390021324158,0.9897270798683168,0.0,accept,unanimous_agreement
1010167705,7898,"the problem is that version 1.* is deprecated since 2015. details: [a link] the vulnerability reported by security scanners (like tenable nessus) is exactly that: the use of an ""unsupported version"". so i think the movement for 2.* is correct.",0,0,0,0.9531901478767396,0.9927242398262024,0.9821089506149292,0.0,accept,unanimous_agreement
1010208088,7898,", let me explain: if there are people willing to support log4j 1.x, then it could be supported and maintained just fine. [a link] is a fork by (the one who created log4j in the first place!) as i highlighted above, there are individuals (including asf committers like myself or even asf members) who are willing to volunteer on supporting log4j 1.x. currently, the question has not yet been decided by the asf, however, i do not see how they can ""forbid"" maintaining 1.x provided the version is wildly used in the industry, there's a high demand on the fixes, and there are individuals to work on that. of course, if the asf allows the volunteers to maintain 1.x, then reload4j will not be required. however, if the asf blocks 1.x (for any reason), then reload4j might be way better for the consumers than migrating to 2.x or something else.",0,0,0,0.8018158078193665,0.9294714331626892,0.9043865203857422,0.0,accept,unanimous_agreement
1011241406,7898,confluent also has a fork of log4j 1 [a link] the github repo seems to have disappeared though.,0,0,0,0.9879948496818542,0.9924750924110411,0.9915221929550172,0.0,accept,unanimous_agreement
1014401948,7898,"you can use other log libraries today as well. the other day i tried out logback runtime (on the broker side) and it works well. all you need is to include logback (or i guess reload4j for that matter) jars runtime and configure it up. i think what we should concentrate on is to be independent from logging frameworks. kafka has two problems: kafkalog4jappender and log4jcontroller/loggingresource. the other stuff is mostly testing related to which i say it doesn't really matter, those aren't exposed to production environment anyway. the appender can be replaced by a similar implementation in log4j2 and the controller could be transformed to a pluggable implementation so users won't lose functionality (right know if you switch to logback or something else you won't be able to manipulate log levels dynamically). as the strongly dependent functionality i think isn't critical and users are mostly free to choose implementation today, i also think it's a good idea to upgrade to log4j2 as a new default. however i think this discussion should be had on the kip discussion thread, please let us concentrate here on the code review itself.",0,0,0,0.9082489013671876,0.9809457659721376,0.9322901368141174,0.0,accept,unanimous_agreement
1021440975,7898,thanks for your work. i hope to start reviewing the log4j2 prs later this week. do you recommend starting with this one or with [a link],1,1,1,0.9664207100868224,0.98818701505661,0.9748117327690125,1.0,accept,unanimous_agreement
1023168840,7898,"many thanks for your effort. sure, here it is; i rebased it against the latest trunk, also updating `logcapturecontext` with 's comments. #10244 is also ready. resolving the conflicts, i also found a little inconsistency problem with `admin` api (see `plaintextadminintegrationtest#testincrementalalterconfigsforlog4jloglevelsdoesnotworkwithinvalidconfigs`) i will file a minor kip addressing it.",1,1,1,0.9527326226234436,0.9786291122436525,0.9724770188331604,1.0,accept,unanimous_agreement
1026731878,7898,"hi , can you please help me to build this latest patch you have released on top of kafka_2.8,1. i tried using this following command to compile kafka source code always, but iam facing an issue to build this latest source code of log4j2 patch . the command i used is : gradle -pscalaversion=2.13 releasetargz -x signarchives 1 . the error iam getting is : build file 'c:\kafka-2.8.1-log4j2\build.gradle' line: 1339 * what went wrong: a problem occurred evaluating root project 'kafka-2.8.1-log4j2(patch by github)'. the following types/formats are supported: - instances of dependency. - string or charsequence values, for example 'org.gradle:gradle-core:1.0'. - maps, for example [group: 'org.gradle', name: 'gradle-core', version: '1.0']. - filecollections, for example files('some.jar', 'someother.jar'). - projects, for example project(':some:project:path'). - classpathnotation, for example gradleapi(). comprehensive documentation on dependency notations is available in dsl reference for dependencyhandler type. * try: run with --stacktrace option to get the stack trace. run with --info or --debug option to get more log output. run with --scan to get full insights. * get more help at [a link] deprecated gradle features were used in this build, making it incompatible with gradle 7.0. use '--warning-mode all' to show the individual deprecation warnings. see [a link] 2 . when i tried commenting a line ""testcompile libs.mockitojunitjupiter // supports mockitoextension "" at line 1339 , iam getting following error. c:\checkouts\kafka_2.8.1>gradle jar building project 'core' with scala version 2.13.5 failure: build failed with an exception. * where: build file 'c:\checkouts\kafka_2.8.1\build.gradle' line: 1362 * what went wrong: a problem occurred evaluating root project 'kafka_2.8.1'.",0,0,0,0.9416148066520692,0.9681362509727478,0.6842502355575562,0.0,accept,unanimous_agreement
1027702595,7898,i'm sorry. there was a mistake rebasing onto 2.8.1. you can see the updated patch with built tarball [a link].,-1,-1,-1,0.9882170557975768,0.9903854131698608,0.9899497032165528,-1.0,accept,unanimous_agreement
1027741356,7898,thank you so much . let me try to apply a patch and build.will update you,1,1,1,0.9747229218482972,0.9754666090011596,0.9873366951942444,1.0,accept,unanimous_agreement
1028142767,7898,"hi ,i could able to build latest patch and also need one input from you. is all dependencies of log4j 1.x is completely removed in this patch............?, i could see,still dependency on log4j_1.2.17 in build.gradle and dependency.gradle.also there are dependency on log4j.properties and tools-log4j.properties instead of log4j2.properties and tools-log4j2.properties in some of the files.is it still require or we can remove those dependencies as well.............?. the things i tried from my end is as follows, 1. i tried updating build.gradle and dependency.gradle by removing the dependency of log4j. 2. also,i tried updating some of the files,where you have added echo statement to update log4j.properties into log4j2.properties in those places where u have mentioned in that patch file by removing log4j.properties and connect-log4j.properties and tools-log4j.properties file. 3. after that,i compiled the code and extracted folder under ""c:\kafka_2.8.1\core\build\distributions\kafka_2.13-2.8.1\kafka_2.13-2.8.1"" and named it as kafka.zip file and using in our component by installing and run it as kafka service. 4.but when i tried running kafka,iam getting following exception. 2022-02-02 05:57:17.158 [inf] [kafka] connecting to localhost:2181 2022-02-02 05:57:27.571 [inf] [kafka] watcher:: 2022-02-02 05:57:27.571 [inf] [kafka] watchedevent state:syncconnected type:none path:null 2022-02-02 05:57:27.574 [inf] [kafka] [] 2022-02-02 05:58:17.227 [err] [kafka] error statuslogger reconfiguration failed: no configuration found for '764c12b6' at 'null' in 'null' 2022-02-02 05:58:17.684 [inf] [kafka] deprecated: using log4j 1.x configuration. to use log4j 2.x configuration, run with: 'set kafka_log4j_opts=-dlog4j.configurationfile=file:c:\kafka/config/tools-log4j2.properties' to brief about my requirement is , currently the kafka package we using,contains some of the patches which we have added on top of kafka_2.8.1 source code.in which one the custom change we have made is,we are using apache-log4j-extras 1.2.17 with timebased triggering policy for rolling log files as it is not available in log4j.1.2.17. since this version has vulnerability ,we wanted to use that log4j2 api for this rolling policy logic which is working in your patch. can you please help me on this...............?",0,0,0,0.9697343111038208,0.8160746693611145,0.5913744568824768,0.0,accept,unanimous_agreement
1028778134,7898,"hi , 1. sure, log4j 1.x is removed entirely. it is still defined in `build.gradle` for `log4j-appender`, deprecated with kip-719, but not included in the classpath. (see below) ![a link] 2. as you can see here, this patch includes both of log4j 1.x and 2.x properties files. but it runs with 1.x properties file by default for backward compatibility. to use log4j2 properties, set `kafka_log4j_opts` like the logging message: [code block] you can use included `config/log4j2.properties` for `timebasedtriggeringpolicy`. please update it following your use case.",0,0,0,0.7272340059280396,0.9652009606361388,0.9575468301773072,0.0,accept,unanimous_agreement
1028865528,7898,"hi , yeah i already have made changes to use log4j2 properties, set kafka_log4j_opts. but after that as well, as i sent in my previous comment, iam getting this following exception. 2022-02-02 05:57:17.158 [inf] [kafka] connecting to localhost:2181 2022-02-02 05:57:27.571 [inf] [kafka] watcher:: 2022-02-02 05:57:27.571 [inf] [kafka] watchedevent state:syncconnected type:none path:null 2022-02-02 05:57:27.574 [inf] [kafka] [] 2022-02-02 05:58:17.227 [err] [kafka] error statuslogger reconfiguration failed: no configuration found for '764c12b6' at 'null' in 'null'. but still iam getting this exception,kafka is not running.can you help me with what is the cause of this exception......................?",0,0,0,0.9388468861579896,0.957051157951355,0.9699323177337646,0.0,accept,unanimous_agreement
1028870380,7898,"hi , please refer [a link].",0,0,0,0.9879246950149536,0.9796937108039856,0.992222011089325,0.0,accept,unanimous_agreement
1028876857,7898,"hi , yeah i refered this article earlier,but didn't get to know the soluton what they are suggesting like dlog4j.configuration in vm arguments.i have not seen where we are using this vm arguments. how i can overcome from this issue................?",-1,0,0,0.7738296985626221,0.518937349319458,0.8583658933639526,0.0,accept,majority_agreement
1028885839,7898,which vm paramater are you using? -dlog4j.configuration or -dlog4j.configuration**file**?,0,0,0,0.9888048768043518,0.99528568983078,0.995871603488922,0.0,accept,unanimous_agreement
1028965316,7898,please refer [a link] and the log messages.,0,0,0,0.9867803454399108,0.9846232533454896,0.9936407208442688,0.0,accept,unanimous_agreement
1028979162,7898,"sure .thank you so much for sharing documentation ,sorry i was not aware that -dlog4j.configuration is vm parameter its pointing to in this ,so was confused with that comment. i got it now.let me through the changes what i have made to use log4j2 properties by setting kafka_log4j_opts in detail,whether i have missed to update it any of the file and will update you.thank you",1,1,1,0.9796913266181946,0.9887508749961852,0.9743178486824036,1.0,accept,unanimous_agreement
1047472293,7898,"hi , can you please provide me some information on the vulenerabiities regarding log4j features , is this following vulnerable features of log4j are using in kafka 2.8.1 for any of the kafka related things...................? cve-2019-17571 is a high severity issue targeting the socketserver. log4j includes a socketserver that accepts serialized log events and deserializes them without verifying whether the objects are allowed or not. this can provide an attack vector that can be expoited. => **is log4j's socketserver used in kafka.......?** cve-2020-9488 is a moderate severity issue with the smtpappender. improper validation of certificate with host mismatch in apache log4j smtp appender. this could allow an smtps connection to be intercepted by a man-in-the-middle attack which could leak any log messages sent through that appender. **=> is log4j's smtpappender is used in kafka..........?** cve-2022-23302 is a high severity deserialization vulnerability in jmssink. jmssink uses jndi in an unprotected manner allowing any application using the jmssink to be vulnerable if it is configured to reference an untrusted site or if the site referenced can be accesseed by the attacker. for example, the attacker can cause remote code execution by manipulating the data in the ldap store. **=> is is log4j's jmssink is used in kafka..............?** cve-2022-23305 is a high serverity sql injection flaw in jdbcappender that allows the data being logged to modify the behavior of the component. by design, the jdbcappender in log4j 1.2.x accepts an sql statement as a configuration parameter where the values to be inserted are converters from patternlayout. the message converter, %m, is likely to always be included. this allows attackers to manipulate the sql by entering crafted strings into input fields or headers of an application that are logged allowing unintended sql queries to be executed. **=> is is log4j's jdbcappender is used in kafka.....................?** can you please help me and provide me the info on this....................?",0,0,0,0.9786291122436525,0.9324086904525756,0.787513792514801,0.0,accept,unanimous_agreement
1047473190,7898,"as long as the user explicitly configure the loggers to use them, they are not used yet.",0,0,0,0.9815968871116638,0.994132936000824,0.9918273687362672,0.0,accept,unanimous_agreement
1047496080,7898,"ohhh okk, in that case none of the 4 vulnerable things ,used in kafka as of now right.....................?",-1,-1,0,0.7492876648902893,0.5364706516265869,0.8969193696975708,-1.0,accept,majority_agreement
1047501444,7898,"currently in kafka we have not used socketserver,smtpappender,jdbcappender and jmssink features from log4j,unless user explicitly use it in their custom changes.................? can you please confirm on this...................?",0,0,0,0.9579739570617676,0.9780725240707396,0.9724149107933044,0.0,accept,unanimous_agreement
1047522217,7898,"please note that the apache logging services project continues to receive security vulnerability reports against log4j 1.x. it is not typical to file cve's against an eol'd project. we recently did, however as we were made aware that a fork of log4j 1 claimed to have fixed all the security issues. we may file more but log4j 1 is not a high priority so i cannot say when more might be forthcoming. in addition to the security issues there are several serious bugs that will never be fixed.",0,0,0,0.9490968585014344,0.9659622311592102,0.957758128643036,0.0,accept,unanimous_agreement
1047536617,7898,"if you receive a cve against log4j 1.x, how difficult is it to forward to the reload4j project? more specifically, are you aware of a single cve against log4j 1.x that was not fixed in reload4j? as for the ""other serious bugs that will never be fixed"" that is a bold claim which will not stand the test of time. it is unbecoming for an oss project leader to engage in this sort of fud. frankly, it is embarrassing to watch. for the sake of your own credibility, please stop.",-1,-1,-1,0.9802675247192384,0.988107979297638,0.985528290271759,-1.0,accept,unanimous_agreement
1062401482,7898,"is there any progress on it? according to this: [a link] , kip should be completed by march 2 and feature freeze is on march 16.",0,0,0,0.986580729484558,0.9823770523071288,0.9927843809127808,0.0,accept,unanimous_agreement
1072452078,7898,"here is the update, rebasing onto the latest trunk. :bow:",1,0,1,0.4924290776252746,0.9591714143753052,0.9888699054718018,1.0,accept,majority_agreement
1073801293,7898,hi would you consider this patch to fix the compilation error [code block],0,0,0,0.9880311489105223,0.9889784455299376,0.9934559464454652,0.0,accept,unanimous_agreement
1076403226,7898,rebased onto the latest trunk. cc/,0,0,0,0.9866312146186828,0.8693140149116516,0.9937849044799804,0.0,accept,unanimous_agreement
1105464108,7898,thanks for working on this pr. is there a timeline on when will this be merged? as per this doc [a link] is it pushed to kafka 3.3?,1,1,1,0.9113628268241882,0.8993942737579346,0.9486026763916016,1.0,accept,unanimous_agreement
1113974839,7898,"rebased onto the latest trunk. -8 sorry for being late. for compatibility reasons, the adoption of this pr is postponed to 4.0. in 3.x, the reloadlog4j will be used instead. you can find out the custom build, patch with log4j2 [a link] - i released the ak 3.1.0 based one this week and working on [a link] now.",-1,-1,-1,0.9885606169700624,0.9886786341667176,0.9673873782157898,-1.0,accept,unanimous_agreement
1321549707,7898,"hi , we ran into the issue to run zookeepr and kafka , when we tried to use reload4j instead of log4j in kafka-2.8.1 package. please find below for more details and could you please help us on how to resolve this issue............? issue : currently we are using kafka-2.8.1 in which it has log4j vulnerabilities reported . fix : so we tried to use reload4j-1.2.22 in kafka-2.8.1 to overcome all the vulnerabilities reported by log4j-1.2.1 and made the changes to point to reload4j instead of log4j as its done kafka 3.2.1 latest version below are the 2 files in kafka which we made changes to replace log4j with reload4j. 1. build.gradle 2. dependencies.gradle after pointing to reload4j, its failing to run kafka and zookeeper with below errors. **zookeeper.log** log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.configurerootcategory(propertyconfigurator.java:630) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:516) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""kafkaappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""authorizerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""controllerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""cleanerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""statechangeappender"". **kafka.log** log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.configurerootcategory(propertyconfigurator.java:630) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:516) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""kafkaappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""authorizerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""controllerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""cleanerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""statechangeappender"". could you please help us if we made the changes to those 2 gradle files , is it enought to make kafka-2.8.1 to work using reload4j............?",0,0,0,0.9488633871078492,0.5869394540786743,0.3742356896400451,0.0,accept,unanimous_agreement
1321685023,7898,the correct fully qualified class name for `rollingfileappender` is `org.apache.log4j.rollingfileappender` and not `org.apache.log4j.rolling.rollingfileappender`.,0,0,0,0.9885640144348145,0.9943625926971436,0.9939171671867372,0.0,accept,unanimous_agreement
2386057956,7898,"we're now accepting changes for 4.0 in trunk. do you think you can rebase this pr? if not, let us know so someone can complete this work. thanks.",1,1,1,0.953597366809845,0.9415534138679504,0.972518026828766,1.0,accept,unanimous_agreement
2441591375,7898,", what is the eta for 4.0? i might be able to look at it mid-november.",0,0,0,0.978163719177246,0.9927309155464172,0.9888766407966614,0.0,accept,unanimous_agreement
2442079763,7898,we're actively working on it in [a link] hopefully that will be complete for 4.0.0.,0,0,0,0.9655213952064514,0.9864002466201782,0.9715600609779358,0.0,accept,unanimous_agreement
2480964449,7898,sorry for being late. i have been very busy nowadays and just got some free time by this end of the year. thanks for taking the issue. i will join the pr and follow up for the review & improvements. let's close this pr and continue on #17373! :smiley:,-1,-1,1,0.988698422908783,0.932501494884491,0.890559732913971,-1.0,accept,majority_agreement
668325203,9039,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
679394512,9039,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
683916587,9039,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
684074223,9039,"since jenkins pr builds are still not functioning, i've merged in trunk and verified this pull request locally before merging it.",0,0,0,0.9887420535087584,0.9926159381866456,0.993463933467865,0.0,accept,unanimous_agreement
2262311490,16456,"hi . thanks a lot for the review. i have made the required changes in the last commit. i have also left some replies to your comments, it will be really helpful if you could leave your suggestions there.",1,1,1,0.9798446893692015,0.9935630559921264,0.9937127232551576,1.0,accept,unanimous_agreement
2265855133,16456,"hi , thanks a lot for the review. i have made the changes suggested by you. also, i have replied to some of your comments and have left those conversations unresolved above. could you pls take a look at those and let me know if anything else is required. thanks a lot ! p.s. - i am looking into the test failure",1,1,1,0.989388644695282,0.9950548410415648,0.9955600500106812,1.0,accept,unanimous_agreement
2269577431,16456,"hi , looks like the sharepartitionmanager wasn't being closed on the broker shutdown. i have pushed in a new commit with the changes, it should work now. is there any other remaining gap in the pr ? if not can i get an approval on it ? thanks a lot for all the suggestions !",1,1,1,0.982849657535553,0.9924426078796388,0.994945466518402,1.0,accept,unanimous_agreement
2270479029,16456,": looks like the tests are passing now, a couple of them fail probably because they are flaky",-1,0,0,0.6524507999420166,0.8647728562355042,0.7048301696777344,0.0,accept,majority_agreement
1726880993,14364,"- made some changes based on the comments, but obviously broke some existing tests.",0,0,0,0.9032773971557616,0.9036999344825744,0.9570354223251344,0.0,accept,unanimous_agreement
1728333915,14364,"hey , thanks for the changes! i completed another pass to all the non-test files. agree with the move of the error handling to the hb manager but left a few other comments. i will wait for the fixes to the failing tests and then i will go over the test files. thanks!",1,1,1,0.986396551132202,0.9956984519958496,0.996203362941742,1.0,accept,unanimous_agreement
1728878443,14364,"- much thanks for spending time reviewing the pr, i tried to address most of the comments in the pr. i'll check back with the unit test results - as i've only run them locally. let me know if you have more comments to follow up.",1,1,1,0.9567667245864868,0.9604535698890686,0.977102518081665,1.0,accept,unanimous_agreement
1731476330,14364,"hey , i see several updates here, thanks! let me know when you want to me give it another pass (there are still some test failures)",1,1,1,0.9428642988204956,0.9613919258117676,0.9898890852928162,1.0,accept,unanimous_agreement
1734529330,14364,hey - i made some updates based on your last comments. let me know your thoughts!,1,1,1,0.7886579632759094,0.9429747462272644,0.8149181008338928,1.0,accept,unanimous_agreement
1735694640,14364,"thanks for the changes , left a few other minor comments and questions but lgtm. as i see it, the main areas requiring follow-up in other prs would be: - fully integrate with the state defined in the membershipmanager (getting rid of all the parallel `groupstate` defined here) - integrate with the assignment processing component, driving the logic to delegate callback execution and send hb on completion as required. - extend hb manager test to cover successful path and timeout scenarios. it would be helpful if you can take another look at it now, as it has evolved quite a bit. thanks!",1,1,1,0.9718269109725952,0.9942201375961304,0.9899701476097108,1.0,accept,unanimous_agreement
1743351780,14364,hello - thanks for the review. i hope i've addressed most of your concerns in the recent reviews. thanks!,1,1,1,0.9751284718513488,0.989831268787384,0.994379699230194,1.0,accept,unanimous_agreement
1745567545,14364,"thanks for the feedback. i addressed more of your comments. i wanted to point out that i filed 3 follow-up tickets to close some of the gaps. these are: - propagate time during failure to avoid time.milliseconds(): [a link] - ensure some of the fields are only sent once: [a link] - ensure coordinator node is removed on disconnection: [a link] i believe all of the open comments are addressed/replied, so let me know if there's anything else.",1,1,1,0.9097167253494264,0.952980935573578,0.9742851853370668,1.0,accept,unanimous_agreement
1746844528,14364,thanks. we also need to handle the consumer close case and send the final heartbeat.,1,1,1,0.861648678779602,0.7601792216300964,0.905838668346405,1.0,accept,unanimous_agreement
1747674288,14364,thanks - i refactored some tests based on your comments. thanks a lot for putting time into it. here i've got a list of jira for the follow-ups: - propagate time during failure to avoid time.milliseconds(): [a link] - ensure some of the fields are only sent once: [a link] - ensure coordinator node is removed on disconnection: [a link] - send heartbeat on closing the consumer as part of handling close(): [a link],1,1,1,0.9769679307937622,0.989837646484375,0.9912988543510436,1.0,accept,unanimous_agreement
1751532067,14364,"jdk11 build failed with `command ""git reset --hard"" returned status code 128:` , for the rest here is the list of failing tests: [code block]",0,0,0,0.9719576835632324,0.9921380877494812,0.989974558353424,0.0,accept,unanimous_agreement
1751592366,14364,there's some issue with the jdk11 build - retriggering the tests don't seem to work. so i opened a [a link] to run the test. it seems like that's the only way to pass the build.,0,0,0,0.9177260398864746,0.9764714241027832,0.950433611869812,0.0,accept,unanimous_agreement
1752310508,14364,"- not entirely sure what is the best way to fix the jdk11 build. the rest of the builds seem to be fine with the following failures: [code block] however, i did open a draft pr from this branch and jdk11 was able to complete.",0,0,0,0.5783315896987915,0.9846551418304444,0.578615128993988,0.0,accept,unanimous_agreement
1753238153,14364,"if we combine the last two builds, i am confident that the changes are good so i will merge it to trunk.",0,0,0,0.6119691729545593,0.9200354218482972,0.8767015337944031,0.0,accept,unanimous_agreement
2394094049,17373,this is the initial version. i'd like to run it on ci first.,0,0,0,0.952399492263794,0.9806194305419922,0.9902395009994508,0.0,accept,unanimous_agreement
2395277491,17373,"hello thanks for your feedback. unfortunately, i barely missed the kip for some reason, but i'll take a look and adjust the pr accordingly. :grinning_cat:",1,1,1,0.9755297899246216,0.9675338864326476,0.9934625625610352,1.0,accept,unanimous_agreement
2430811748,17373,"sounds good to me. but if we decided to remove them later, please open a jira ticket for them. thanks.",1,1,1,0.9784437417984008,0.9868705868721008,0.994046688079834,1.0,accept,unanimous_agreement
2430832465,17373,[a link] i have filed a jira for it. :grinning_face_with_smiling_eyes:,1,0,1,0.8963128328323364,0.9850478172302246,0.9918169379234314,1.0,accept,majority_agreement
2434890030,17373,"please fix the conflicts, thanks!",1,1,0,0.934069573879242,0.9528201818466188,0.6830363273620605,1.0,accept,majority_agreement
2453429889,17373,there are lots of tests fail come out after merging `trunk`. :disappointed_face: i will take a look......,-1,0,-1,0.9869536757469176,0.7079644799232483,0.9954753518104552,-1.0,accept,majority_agreement
2454941038,17373,"hello everyone, i am having some trouble debugging the new failures. :crying_face: the root cause seems to be that these failing tests are unable to capture logs correctly (resulting in empty content), which leads to assertion failures. it appears that these issues are caused by #17615, although these test cases worked fine when using log4j1. any feedback or information would be greatly appreciated. thank you!",1,1,1,0.966708242893219,0.9951209425926208,0.9161723852157592,1.0,accept,unanimous_agreement
2462639102,17373,update: it seems that the root cause its because the log event could not be captured correctly. ![a link] ![a link],0,0,0,0.8903161883354187,0.9861598610877992,0.8317343592643738,0.0,accept,unanimous_agreement
2472887781,17373,"hello ,, since this pr modifies a large number of files, particularly `build.gradle`, it’s highly susceptible to conflicts with other prs, making it rather exhausting to resolve these conflicts frequently. it would be helpful if we could merge this pr into trunk sooner, as there aren’t any outstanding issues or points of contention with it. this would also allow us to begin addressing any follow-up issues. many thanks.",1,1,1,0.936199426651001,0.9816626906394958,0.9732868075370787,1.0,accept,unanimous_agreement
2478290150,17373,", do you have any other comments? i'll merge it tomorrow if no other comments. thanks.",1,1,1,0.926963984966278,0.8749722838401794,0.9582186341285706,1.0,accept,unanimous_agreement
2480966246,17373,fyi: i just moved from #7898 and reviewing the differences between our 3.6.x in-house fork. it was already applied to our implementation and is actively running now :) let me have a look!,1,1,1,0.9878531694412231,0.9955790638923644,0.9946976900100708,1.0,accept,unanimous_agreement
2485719308,17373,"all // to clearly state the reasoning for root logger's name, i just updated the [a link]. please have a look on [a link]. fei chang gan xie :pray: (""extremely thankful for your great help."") to keep a record of your effort taking over this huge issue, i added a new section mentioning [a link]. when you merge this pr, please don't omit mentioning my credit. :bowing_man:",1,1,1,0.7984815239906311,0.9940588474273682,0.9743430018424988,1.0,accept,unanimous_agreement
2485801878,17373,will roger that :),1,1,0,0.950251579284668,0.9865795969963074,0.9742090106010436,1.0,accept,majority_agreement
2486208765,17373,yes you totally deserve being marked as a co-author. thanks!,1,1,1,0.9756354093551636,0.9871697425842284,0.9947124719619752,1.0,accept,unanimous_agreement
2488600707,17373,"i'm having issues with the latest code (f3a68e1b9b). [code block] running `trunk` the `logs` folder is automatically created. if i create the folder manually, then it seems to log at warn level by default and i only get a single log line printed while my broker is running: [code block]",0,0,0,0.8047898411750793,0.9182412028312684,0.5577926635742188,0.0,accept,unanimous_agreement
2503451783,17373,can you rebase to fix the conflicts? thanks,1,1,1,0.9379223585128784,0.6570538282394409,0.8015167713165283,1.0,accept,unanimous_agreement
2504196849,17373,"hi , i have filed a jira for the issue with the connect api: [a link] btw, i’m aware that this pr is expected to break some e2e tests, but so far, i’ve only confirmed that `connect_test.py` is affected. should we fix it in this pr, or should we file a separate jira to track it?",0,0,0,0.9769358038902284,0.9542742371559144,0.9828275442123412,0.0,accept,unanimous_agreement
2504201143,17373,the connect issue should be fixed in this pr. adding new tests to verify the connect rest api /admin/loggers endpoint can be done in another pr but not the bug fix.,0,0,0,0.9878036975860596,0.9940617680549622,0.9936707019805908,0.0,accept,unanimous_agreement
2504207669,17373,"apart if the changes to fix the system tests are very large, i'd rather do them in this pr as well. we typically don't merge prs that we know will break tests.",0,0,0,0.9688599109649658,0.9696682691574096,0.9829141497612,0.0,accept,unanimous_agreement
2504369138,17373,i just tested the connect rest api with [a link] and it worked fine. you mentioned issues with the system tests. have you run the full suite? are there only issues with `connect_test.py`?,0,0,0,0.9117464423179626,0.9509170651435852,0.9863459467887878,0.0,accept,unanimous_agreement
2505169162,17373,"hi , unfortunately, i don't have the resources to perform the full test suite as it would take a very long time :grimacing_face: instead, i selected a test related to this pr. the first one i picked is `connect_test.py`, but it failed. here is the assertion failure for `test_file_source_and_sink.converter`: [code block] as we can see, there are 2 exception logs at the end of the array that breaks the test. i am investigating but currently stuck. the rest of the test failures of `connect_test.py` also seem to be related to this `timeoutexception`.",-1,-1,-1,0.9592770338058472,0.9651601910591124,0.9903004169464112,-1.0,accept,unanimous_agreement
2506901731,17373,", we ran system tests on connect related tests (tests/kafkatest/tests/connect) based on this pr, it failed quite a lot. [a link] is the result. compared with the latest trunk, it only has 1 failure. so please help fix them. thanks.",1,1,1,0.9533944725990297,0.9549118876457214,0.9526357054710388,1.0,accept,unanimous_agreement
2507753807,17373,"since this issue is quite tricky for me, i went through a process of elimination to locate the bug. i found this line might be the root cause: [a link] if we use `-dlog4j2.configurationfile=file:`, it will lead to a `timeoutexception` in the console consumer. however, if we use `-dlog4j.configuration=file:`, all tests in `connect_test.py` will pass. i'm still trying to figure it out.",0,0,0,0.6702665090560913,0.8805361986160278,0.8872857689857483,0.0,accept,unanimous_agreement
2507855608,17373,"thanks for investigating. i ran the system tests in our ci and confirm most the connect tests are currently failing with your branch (i've not tried running other tests yet). i'll try to debug next week, if you can't find the issue. i'll also be able to run the tests in our environment once we have a fix.",1,1,1,0.9483340978622437,0.8224107623100281,0.9722263216972352,1.0,accept,unanimous_agreement
2508123859,17373,"in the end-to-end tests, you must use the correct file extension when passing the log4j2 yaml configuration. if you use an incorrect extension - for example - the output of `log.error` called by the console consumer gets redirected to `console_consumer.stdout`. this is the root cause of the connection error as it see output which should not be existent. as a straightforward solution, we can render the configuration file based on the specific node version. [code block] noted that `log4j_config` can be removed as we don't use it anymore. additionally, you can apply this approach to all py files - such as `verifiable_producer.py`, `kafka.py`, `transactional_message_copier.py`, `consumer_performance.py`, `end_to_end_latency.py`, and `producer_performance.py`.",0,0,0,0.979666531085968,0.9940483570098876,0.990915596485138,0.0,accept,unanimous_agreement
2508957854,17373,"[a link] `metadataschemacheckertooltest#testverifyevolutiongit` keep failing in ci, i think it's not related to this pr.",0,0,0,0.871947169303894,0.9886109232902528,0.9605814814567566,0.0,accept,unanimous_agreement
2508981008,17373,"the root cause is the test assumes the repo has ref ""refs/heads/trunk"" but it is not existent in pr, since our pr does not fetch that ref. i have filed [a link] to fix it",0,0,0,0.9868434071540833,0.9766294360160828,0.9925553202629088,0.0,accept,unanimous_agreement
2510443802,17373,failed ci is handled by #17996,0,0,0,0.9784433841705322,0.9882672429084778,0.9926829934120178,0.0,accept,unanimous_agreement
2510857877,17373,could you please rebase code?,0,0,0,0.9887443780899048,0.9913992881774902,0.9917995929718018,0.0,accept,unanimous_agreement
2511535886,17373,"connect e2e has another issue, and i have filed [a link] to fix it",0,0,0,0.9821658730506896,0.9553794264793396,0.9932059049606324,0.0,accept,unanimous_agreement
2515161122,17373,this is another issue ([a link] used to fix connect e2e,0,0,0,0.9692875742912292,0.9562821984291076,0.9951364398002625,0.0,accept,unanimous_agreement
2518956077,17373,the last (maybe) connect end-to-end issue is [a link]. we can revisit this pr after the existing connect end-to-end issues are resolved.,0,0,0,0.9884821176528932,0.9935078620910645,0.9938164353370668,0.0,accept,unanimous_agreement
2540936116,17373,should we consider merging this and disabling the broken system test for now? wdyt?,0,0,0,0.9831019043922424,0.9598144888877868,0.9888784289360046,0.0,accept,unanimous_agreement
2541679409,17373,"let me perform a final review later, and then i will merge it!",0,0,0,0.8116592764854431,0.8493393659591675,0.9834470748901368,0.0,accept,unanimous_agreement
2541895357,17373,i add you to the co-author. see [a link],0,0,0,0.9766001105308532,0.937914490699768,0.9910793900489808,0.0,accept,unanimous_agreement
2542607479,17373,thank you all for your patience in reviewing. i truly appreciate all your help. :person_bowing_medium-light_skin_tone:,1,1,1,0.9821797609329224,0.9930117726325988,0.9970299005508424,1.0,accept,unanimous_agreement
2543435727,17373,"i am currently testing this patch, and all related issues are tracked under [a link]. please feel free to report any issues if i overlook any broken changes. :( the known bugs caused by this pr are listed below. 1. `logger` does not handle the compatibility correctly (see [a link] 2. incorrect log4j2 config in e2e (see [a link]",-1,-1,-1,0.9820907711982728,0.9883699417114258,0.9962177872657776,-1.0,accept,unanimous_agreement
2556096026,17373,"hey -- not sure if there is something up with my setup, but after this pr, i see many tests failing due to kafka server not starting up (in the logs i see ` port already in use: 9192; nested exception is: java.net.bindexception: address already in use`or because there are no process ids (see [a link] in both cases, it seems like something changed in startup -- perhaps some change in kafka.py here triggered the issue. did we run system tests after this change? i see some tests mentioned in kafka-18161, but not all. i'm seeing about 500 failures on trunk now (compared to 50 or so last week). [a link]",0,0,0,0.8468360304832458,0.6722130179405212,0.9071375727653505,0.0,accept,unanimous_agreement
2556229381,17373,"thanks for your report. the known issue caused by this pr is the ""logger"" of connect. and i run the one of test (`transactions_upgrade_test.py`) on my local: [code block] there is no `bindexception`, and i will check other tests later.",1,1,1,0.8919597864151001,0.9086154699325562,0.9557839632034302,1.0,accept,unanimous_agreement
2557032193,17373,i don't see any upgrade notes - did i miss something or did we forget to add that?,0,0,0,0.8014425039291382,0.9518447518348694,0.9477962255477904,0.0,accept,unanimous_agreement
2557302557,17373,i will open a pr adding upgrade notes.,0,0,0,0.98236346244812,0.9802479147911072,0.9950079917907716,0.0,accept,unanimous_agreement
2557311234,17373,could you please open minor to address following items? 1. move the deprecation warnings to `kafka-run-class` [a link] 2. remove unnecessary reference of `jacksondatabindyaml` [a link] 3. add upgrade notes [a link],0,0,0,0.9871031641960144,0.9937572479248048,0.9947081804275512,0.0,accept,unanimous_agreement
2557355489,17373,interesting. this one fails consistently on our infra (and succeeds without this change). i wonder if there is some different test configuration that causes the issue.,0,0,0,0.790866494178772,0.8413596153259277,0.8495808839797974,0.0,accept,unanimous_agreement
2557479743,17373,there are many tests which are failed quickly. [code block] it can pass the check of `grep 'kafka\s*server.*started'` (kraft controller) but the check of process id fails. i open a minor to add more log ([a link] could you please run the patch on your infra?,0,0,0,0.9577759504318236,0.9880040884017944,0.9913539290428162,0.0,accept,unanimous_agreement
2557576698,17373,sure. i will run it and post the results,0,0,0,0.8705407977104187,0.9646968245506288,0.9700381755828856,0.0,accept,unanimous_agreement
2557623349,17373,here's the result. [a link],0,0,0,0.9862935543060304,0.9666929841041564,0.9908163547515868,0.0,accept,unanimous_agreement
2577813317,17373,"this pr breaks downstream project that depends on kafka_2.13 and reload4j because there is clashing in class loggingevent, which exists in `reload4j` and `log4j-1.2-api` jars. the application will crash with this exception [code block]",0,0,0,0.9810152053833008,0.9831637740135192,0.9705801606178284,0.0,accept,unanimous_agreement
2577875259,17373,"you can not have both `reload4j` and `log4j-1.2-api` on your classpath, since they are both **replacements** of `log4j:log4j`.",0,0,0,0.9889053106307985,0.995123326778412,0.9929277300834656,0.0,accept,unanimous_agreement
2577897980,17373,"the point is that kafka was including `reload4j` before and hence many other projects aligned with that. it's one thing to cause logging to change, it's another to cause projects not to start anymore with a `incompatibleclasschangeerror` - that's a much bigger deal.",0,0,0,0.9433544874191284,0.9847153425216676,0.9765560030937196,0.0,accept,unanimous_agreement
2578032374,17373,"i assume the scenario you describe is to add ""kafka_2.13"" as dependency for the downstream project. the log4j2 are declared as ""implementation"" in core module, so downstream project should not ""include"" log4j2 in the dependencies automatically. not sure why `log4j-1.2-api` is included in the downstream project. do you add `log4j-1.2-api` to your project manually?",0,0,0,0.9809502959251404,0.985855519771576,0.9886844754219056,0.0,accept,unanimous_agreement
2578035232,17373,"thanks for your report. i'd like to reduce the gap of upgrading to kafka 4.0 as much as possible, so please share the details to me",1,1,1,0.9167332649230956,0.9637274742126464,0.965753138065338,1.0,accept,unanimous_agreement
2580893706,17373,we include kafka_2.13 as dependency via maven [a link] and it brings log4j-1.2-api as a runtime dependency (below is from mvn dependency:tree) [code block],0,0,0,0.986506462097168,0.9935488104820251,0.9954832792282104,0.0,accept,unanimous_agreement
2581051461,17373,"thanks for your response. `log4j-1.2-api` is used to convert log4j.properties at runtime, and hence maybe we can remove it from gradle runtime scope and then add it into distribution directly. with that change, `log4j-1.2-api` gets removed from published pom file but it remains in the distribution. this is the pom file with above approach and there is not `log4j-1.2-api` [code block] by contrast, `log4j-1.2-api` is still included by distribution. [code block]",1,1,1,0.9436697363853456,0.7480535507202148,0.931071639060974,1.0,accept,unanimous_agreement
2581610985,17373,hi thanks for report. i have filed kafka-18466 to track this issue. i will filed a pr for this issue soon.,1,1,1,0.9636154770851136,0.8077297806739807,0.9656265377998352,1.0,accept,unanimous_agreement
2586347959,17373,"we have resolved the issue in #18472. if you have some free time, could you please test it on your project? please don't hesitate to provide any feedback if you think something has been overlooked. thank you!",1,1,1,0.9765536785125732,0.9888457655906676,0.9850934743881226,1.0,accept,unanimous_agreement
1508030768,13561,"`kafka-14888: remotelogmanager - deleting expired/size breached log segments to remote storage implementation` was created for this task. i've updated the pr title, fyi",0,0,0,0.9713807106018066,0.952587902545929,0.990672767162323,0.0,accept,unanimous_agreement
1555964010,13561,the proposed approach with the changes introduced in this pr avoids inconsistency issues as mentioned [a link] and also avoids remote segment leaks in unclean leader election scenarios.,0,0,0,0.9805694818496704,0.9925518035888672,0.992983341217041,0.0,accept,unanimous_agreement
1571615955,13561,"will you open a separate pr to delete the active segment once it breaches the retention time? or, will handle it in this patch.",0,0,0,0.9875661134719848,0.9922170639038086,0.9939649701118468,0.0,accept,unanimous_agreement
1571973707,13561,"planned to have it in a followup pr, filed [a link] just to clarify , we need to roll the active segment incase remote storage is enabled and eligible for retention cleanup so that this segment can be copied by the remote storage subsystem and eventually picked up for retention cleanup.",0,0,0,0.9857890009880066,0.9926266074180604,0.9943535327911376,0.0,accept,unanimous_agreement
1573234538,13561,thanks for your review comments. addressed them inline and/or with the latest commits.,1,1,1,0.8266949653625488,0.7992684841156006,0.9006824493408203,1.0,accept,unanimous_agreement
1573968832,13561,hey are you planning to address the open comments such as [a link] before i do another pass of code review?,0,0,0,0.9840480089187622,0.9651743173599244,0.9943002462387084,0.0,accept,unanimous_agreement
1575514622,13561,i will let you know once i address the remaining few comments in the next couple of days.,0,0,0,0.9761906266212464,0.9522913694381714,0.9885154366493224,0.0,accept,unanimous_agreement
1576564925,13561,"pulled the latest trunk, resolved the conflicts, and pushed the changes.",0,0,0,0.9815750122070312,0.9926970601081848,0.9943627119064332,0.0,accept,unanimous_agreement
1596907836,13561,", any update for this pr? if you don't have time on it, just let me know. :)",1,1,1,0.987825572490692,0.9931092262268066,0.99037104845047,1.0,accept,unanimous_agreement
1597034800,13561,"addressing the review comments in progress, needs minor refactoring which is going on. will have those changes pushed in the next couple of days.",0,0,0,0.9809302091598512,0.9865358471870422,0.9917460680007936,0.0,accept,unanimous_agreement
1597103576,13561,rebased with the trunk as this pr had conflicts because of other introduced changes in the trunk.,0,0,0,0.981923282146454,0.9897202849388124,0.9785128831863404,0.0,accept,unanimous_agreement
1659937748,13561,", is this pr ready for another round of review?",0,0,0,0.97206050157547,0.9867210388183594,0.990921914577484,0.0,accept,unanimous_agreement
1665486770,13561,thanks for the review. addressed them with the latest commits.,1,1,1,0.7734909057617188,0.87303227186203,0.8359997868537903,1.0,accept,unanimous_agreement
1665489584,13561,thanks for the review. addressed your comments inline or with the latest commits.,1,1,1,0.8273165225982666,0.8046962022781372,0.8380887508392334,1.0,accept,unanimous_agreement
1670963444,13561,thanks for the review. addressed your comments inline or with the latest commits.,1,1,1,0.8273165225982666,0.8046962022781372,0.8380887508392334,1.0,accept,unanimous_agreement
1672759994,13561,thanks for the review. addressed the review comments inline and/or with the latest commits.,1,1,1,0.8653983473777771,0.8060566186904907,0.9020833373069764,1.0,accept,unanimous_agreement
1680370332,13561,thanks for the review. addressed them with inline comments and/or with the latest commits. will add more uts. we will have integration tests in a followup pr once [a link] is merged.,1,1,1,0.925146460533142,0.9808547496795654,0.946024775505066,1.0,accept,unanimous_agreement
1682651711,13561,those failures(except one) are not related to this pr. updated with a few minor changes and tests. the latest run had a few failures which seem to be unrelated to this change.,0,0,0,0.9662050604820251,0.9808858036994934,0.9878860712051392,0.0,accept,unanimous_agreement
1685049750,13561,: the latest build still has 120 test failures.,0,0,0,0.9486539363861084,0.834417998790741,0.9846424460411072,0.0,accept,unanimous_agreement
1685264422,13561,those tests are not related to the changes in the pr. the next [a link] had one related test failure(plaintextadminintegrationtest.testoffsetsfortimesafterdeleterecords) which is fixed with the latest commit.,0,0,0,0.987441062927246,0.99159437417984,0.9945809245109558,0.0,accept,unanimous_agreement
1692562057,13561,"there are a few failures unrelated to this pr, merging it to trunk.",0,0,0,0.9735236763954164,0.950678050518036,0.982260763645172,0.0,accept,unanimous_agreement
1692563258,13561,merging it to 3.6 branch,0,0,0,0.988291561603546,0.990237295627594,0.9900640845298768,0.0,accept,unanimous_agreement
396640388,5201,"apologies for the massive pr but it seems unavoidable at this point. more updates to come to this pr soon, but it's worth reviewing the graph object types and the overall physical plan generation",-1,0,-1,0.7492564916610718,0.9612348675727844,0.8391216993331909,-1.0,accept,majority_agreement
396673545,5201,failure unrelated. retest this please,0,0,0,0.950676202774048,0.7267016172409058,0.7219146490097046,0.0,accept,unanimous_agreement
397827935,5201,updated this,0,0,0,0.9801121354103088,0.9735050201416016,0.9438372254371644,0.0,accept,unanimous_agreement
397829288,5201,rebased from trunk,0,0,0,0.9854912757873536,0.9903222918510436,0.9920662641525269,0.0,accept,unanimous_agreement
398142054,5201,could you rebase this pr?,0,0,0,0.9882051944732666,0.9925855994224548,0.9941056966781616,0.0,accept,unanimous_agreement
399225979,5201,failure unrelated retest this please,0,0,0,0.957226574420929,0.8864413499832153,0.5913138389587402,0.0,accept,unanimous_agreement
399778525,5201,"fixed issue with optimizing joins, updated integration test to include comparing results between optimized and non-optimized results and included a join in the test case.",0,0,0,0.9846382737159728,0.9939688444137572,0.9857892394065856,0.0,accept,unanimous_agreement
401196303,5201,"in this pr we're still relying on the fix put into 2.0, in 4th pr we'll revert that change and include it along with the repartition optimization.",0,0,0,0.9856389164924622,0.9939619898796082,0.9927462935447692,0.0,accept,unanimous_agreement
401196999,5201,required to keep find-bugs happy,0,0,0,0.6654142141342163,0.9859160780906676,0.9789729714393616,0.0,accept,unanimous_agreement
401198070,5201,updated this,0,0,0,0.9801121354103088,0.9735050201416016,0.9438372254371644,0.0,accept,unanimous_agreement
401200256,5201,"i made another pass over the updated pr, and left two follow-up comments as in [a link] and [a link]",0,0,0,0.9872009754180908,0.9880481958389282,0.9939449429512024,0.0,accept,unanimous_agreement
401493756,5201,"regarding the meta comment about `abstractstream#addgraphnode`, now i understand the duplicate logic is because `kstreamaggregateprocessorbuilder` is not `abstractstream` and hence cannot use this function. my personal preference would then be, moving the `addgraphnode` from `abstractstream` to `internalstreamsbuilder`, with a small change on api to require passing in both the child and the parent node reference. in this case 1) beyond `abstractstream` we can still call it, and 2) moving forward if we need to have more then one streamgraphnode within an abstractstream it would be more natural to do so.",0,0,0,0.9631662964820862,0.9894717335700988,0.9838948845863342,0.0,accept,unanimous_agreement
405652620,5201,please let us when this pr is ready for review again.,0,0,0,0.9655287265777588,0.9796394109725952,0.9899017810821532,0.0,accept,unanimous_agreement
408222437,5201,updated this. moved `addgraphnode` from `abstractstream` to `internalstreamsbuilder`. also removed the restriction of only building topology only once and added a unit test for it. also rebased with trunk. ready for reviews again,0,0,0,0.95715993642807,0.9883787631988524,0.7320307493209839,0.0,accept,unanimous_agreement
408230707,5201,"just remembered, need to update `tostring` for individual graph nodes",0,0,0,0.988734781742096,0.9922772645950316,0.9923306703567504,0.0,accept,unanimous_agreement
408895387,5201,"updated this address last comments, rebased from trunk",0,0,0,0.9868568778038024,0.9900304079055786,0.9945255517959596,0.0,accept,unanimous_agreement
408991717,5201,updated to include clean-up from on minor cleanup prs,0,0,0,0.9842958450317384,0.993319034576416,0.9811943173408508,0.0,accept,unanimous_agreement
409269000,5201,kicked off system tests [a link] also kicked off streams smoke tests [a link],0,0,0,0.9864602088928224,0.9910948276519777,0.993061363697052,0.0,accept,unanimous_agreement
409429334,5201,i like `processorgraphnode` wdyt?,0,0,0,0.9618332982063292,0.8993778824806213,0.9908526539802552,0.0,accept,unanimous_agreement
409433491,5201,updated this and rebased from trunk,0,0,0,0.9854597449302672,0.9926170706748962,0.9939814209938048,0.0,accept,unanimous_agreement
409443610,5201,sounds good to me.,1,1,1,0.9596179723739624,0.8850300908088684,0.9217013120651244,1.0,accept,unanimous_agreement
409580226,5201,failure unrelated. retest this please,0,0,0,0.950676202774048,0.7267016172409058,0.7219146490097046,0.0,accept,unanimous_agreement
409619948,5201,updated to rename `statelessprocessornode` to `processorgraphnode` from comments.,0,0,0,0.987785577774048,0.9952008724212646,0.9940392971038818,0.0,accept,unanimous_agreement
409639309,5201,"one more meta comment: we had a bunch of warnings due to lack of types for processorparameters. this can be resolved by adding the following, e.g. in `kstreamimpl#filter()`: [code block] i.e. we can still maintain the typing information here when constructing the logical node.",0,0,0,0.9851064682006836,0.9938969016075134,0.9908039569854736,0.0,accept,unanimous_agreement
409676911,5201,updated this,0,0,0,0.9801121354103088,0.9735050201416016,0.9438372254371644,0.0,accept,unanimous_agreement
409739132,5201,lgtm. merging to trunk.,0,0,0,0.9882312417030334,0.989843487739563,0.9873740077018738,0.0,accept,unanimous_agreement
1557159854,13639,thanks for your reviews. i think that i have addressed all your comments.,1,1,1,0.8558162450790405,0.8972482681274414,0.9661481976509094,1.0,accept,unanimous_agreement
1562650336,13639,"thanks for your comments. i have addressed them, i think.",1,1,1,0.9107713103294371,0.8718923330307007,0.9577818512916564,1.0,accept,unanimous_agreement
378068897,4812,just starting on this; still need to add the missing measurements and actually run the tests.,0,0,0,0.9799700379371644,0.9852981567382812,0.9696855545043944,0.0,accept,unanimous_agreement
378777667,4812,jenkins failures are relevant to `unused import - ..`.,0,0,0,0.9263280034065248,0.9857731461524964,0.9882325530052184,0.0,accept,unanimous_agreement
378780973,4812,"ah, thanks, by the time i went to look at the last failure, the logs were already gone.",1,0,1,0.5981137752532959,0.5134250521659851,0.9346045851707458,1.0,accept,majority_agreement
378791813,4812,"i took the liberty of resolving all my ide warnings for the test files. let me know if i went too far, and i can revert them.",0,0,0,0.972630262374878,0.7065653800964355,0.9765814542770386,0.0,accept,unanimous_agreement
378792028,4812,"checkstyle complained because i explicitly imported log4j, which is appropriate in this case. i isolated the usage to a ""testutils"" package, so i could allow the usage without allowing it for all of ""stream.processor.internals"".",0,0,0,0.8365423083305359,0.9911980032920836,0.95867657661438,0.0,accept,unanimous_agreement
379272773,4812,"i think you're right. for streams, the thread level is the most global scope we have atm. i think what you're pointing out is that i've conflated the global scope with the thread scope. ideally, these would be two separate scopes. let me refactor a bit more, and see what you think.",0,0,0,0.9341009855270386,0.9448119401931764,0.8604787588119507,0.0,accept,unanimous_agreement
379318433,4812,"actually i'm arguing that `streamsmetrics` should only be some sort of a util class, that 1) wraps the actual `metrics` object as the metrics registry, 2) provides util functions like `addxxsensor`, `addsensor` etc. and then `threadmetrics`, `taskmetrics` etc exposes the api of their defined sensors, and in their corresponding impl class they keep a reference of the `streamsmetrics` to register new metrics. we still need to note that, since we allow users to register their custom metrics via the `streamsmetrics`, we need to expose `streamsmetrics` in user-facing apis, while for other built-in `xxmetrics` they are only used internally, while we can still expose their `xxsensor` functions for other internal classes to use.",0,0,0,0.9598877429962158,0.9868203997612,0.9869567155838012,0.0,accept,unanimous_agreement
379376338,4812,"and about that experimental commit, i've decided to ditch it and implement the kip with minimal changes to the structure of the metrics. i think i'd like to submit a kip to alter the metric registration strategy we're employing later on, but i don't want to pollute kip-274.",0,0,0,0.91849023103714,0.9645493030548096,0.9819522500038148,0.0,accept,unanimous_agreement
379401092,4812,"i don't understand why these tests are failing. the message says: [code block] but line 804 in streamtasktest is: [code block] retest this, please.",0,0,-1,0.7635869979858398,0.7593181729316711,0.8687370419502258,0.0,accept,majority_agreement
379406708,4812,"i see [code block] (line 804) -- this makes sense, it should be `task = createstatelesstask(createconfig(true));`",0,0,0,0.9834820032119752,0.9916861653327942,0.9938209652900696,0.0,accept,unanimous_agreement
379412401,4812,"ah, it was because jenkins (surprisingly) merges with trunk before testing. also, there was an undetected merge conflict, resulting in the broken code. i've rebased and corrected it. once the tests pass for me, i'll push again.",0,0,0,0.9776361584663392,0.9442219138145448,0.9746527075767516,0.0,accept,unanimous_agreement
379905976,4812,"i'm currently working on adding metrics there. i'm also adding warning logs, as it's totally silent right now.",0,0,0,0.8782240152359009,0.7559375762939453,0.9844282269477844,0.0,accept,unanimous_agreement
379907479,4812,and regarding this: ack. i might do that in a follow-up pr (under the same jira/kip) to keep the loc in this pr lower.,0,0,0,0.9684808254241944,0.9909590482711792,0.9910752773284912,0.0,accept,unanimous_agreement
379914936,4812,"i've rebased and pushed the latest changes. i still need to add tests for the processors' metrics, but this change is otherwise pretty much where i want this pr to be. note that i rebased and put the change to `sensor` at the beginning so that i can send that one change as a separate pr, if needed, for review by the client library people. this change is necessary, since the existing `maybeaddmetric` implementation in streams is unsafe. to be done properly, it has to be synchronized, which my change does. my change also allows us to differentiate between registering a metric twice for a particular sensor (ok) and registering the same metric name on two different sensors (not ok). also, reminder that we should agree on a log style. i left `[]` in the pr as a placeholder. the discussion for this is above.",0,0,0,0.9306765794754028,0.9622271656990052,0.7852702736854553,0.0,accept,unanimous_agreement
380241162,4812,"ok, , this pr is ready for another pass. i have completed all code and tests. docs will follow in another pr. please comment on: * my strategy for sharing the skipped metric around the code base * my changes to sensor (and whether i need to send a separate pr for that change) * the aforementioned log enclosing delimiter discussion thanks, all.",1,1,1,0.912431538105011,0.881879985332489,0.9595636129379272,1.0,accept,unanimous_agreement
380859168,4812,"huh, that's a new one. it looks like (aside from the kafkaadmintest continuing to flake out), the tests failed because the jenkins worker ran out of disk! i'll wait until the last job completes before starting them again. i've rebased this pr on trunk now that #4853 is merged. i still have a few nits to clean up. i'll notify again when i'm ready for final reviews.",0,0,0,0.7091251611709595,0.4847586154937744,0.7195051312446594,0.0,accept,unanimous_agreement
380971785,4812,meta comment: please update the pr title with the jira number,0,0,0,0.9830506443977356,0.9856230020523072,0.9958103895187378,0.0,accept,unanimous_agreement
380976453,4812,sorry about that,-1,-1,-1,0.9856815338134766,0.989427089691162,0.9929924011230468,-1.0,accept,unanimous_agreement
381155079,4812,the tests passed. the failure was a rate-limit exception publishing coverage to github: [code block],0,0,0,0.9764679670333862,0.9869135022163392,0.9924875497817992,0.0,accept,unanimous_agreement
381174729,4812,"hey , i hear you on the pollution of this pr, and the unsatisfactory state of metrics refactoring here. maybe i'll take a couple of hours and extract all the code cleanup (making variables final, etc.) into a separate pr and then rebase this one on that. then, this pr will be smaller, and i'll feel more comfortable proceeding with some more refactoring of the metrics code.",0,1,0,0.5920813083648682,0.625302791595459,0.7168054580688477,0.0,accept,majority_agreement
381276277,4812,i have pulled out the trivial changes into pr #4872 and rebased this pr on that one. please focus reviews on #4872 until it is merged.,0,0,0,0.967955470085144,0.98541659116745,0.9908231496810912,0.0,accept,unanimous_agreement
382128932,4812,rebased on trunk now that dependee pr is merged.,0,0,0,0.9880796074867249,0.9928487539291382,0.9942936301231384,0.0,accept,unanimous_agreement
382191401,4812,jenkins failure: [code block],0,0,0,0.904329776763916,0.9833050966262816,0.975396990776062,0.0,accept,unanimous_agreement
382848909,4812,"ok, , i believe this is ready for final review. i've made a pass over it to make sure the diff is clean and to comment on the rationale of some of the choices. the diff is still quite long, but it's mostly because of all the processors that now record skipped metrics and the corresponding tests.",1,0,0,0.5699636936187744,0.7716120481491089,0.8329704403877258,0.0,accept,majority_agreement
383137615,4812,thanks for the review! i added [a link] and [a link] in response to your comments. i also have a next pr queued up for after this one is merged (in response to our concerns about namedcachemetrics). please let me know what you think! -john,1,1,1,0.9866071939468384,0.9916114211082458,0.9931491017341614,1.0,accept,unanimous_agreement
383609305,4812,addressed bill's comments and rebased.,0,0,0,0.977120876312256,0.9865452647209167,0.9904154539108276,0.0,accept,unanimous_agreement
534847170,7378,there is still a lot of old topologytestdriver test using deprecated methods needed to migrate to use new ones. so let me know if someone have possibility to help with those.,0,0,0,0.9846130609512328,0.973787784576416,0.9808654189109802,0.0,accept,unanimous_agreement
535173884,7378,example class and develper guide updated.,0,0,0,0.9875848293304444,0.986505687236786,0.9915653467178344,0.0,accept,unanimous_agreement
536172291,7378,deprecated method migrated in streams and streams-scala packages,0,0,0,0.9891499876976012,0.9939686059951782,0.9937534928321838,0.0,accept,unanimous_agreement
537780874,7378,"seems there are some conflicts -- can you rebase this pr? the feature freeze deadline was pushed to friday, so we still have 2 days to get this into 2.4 :)",1,1,1,0.8935751914978027,0.8481850624084473,0.8934893608093262,1.0,accept,unanimous_agreement
538072445,7378,"hey , i just had another thought while reading over 's comments... for each of the ""deprecated"" javadoc lines, can you also mention the version it was deprecated in? like, ` since 2.4. please use xxxxx instead...` it just helps everyone understand when exactly the method should be removed.",0,0,0,0.9613493084907532,0.9489573240280152,0.8158661127090454,0.0,accept,unanimous_agreement
538112784,7378,", conflicts resolved changes made based on review also added deprecated since 2.4 based on suggestion",0,0,0,0.982611119747162,0.9933227896690368,0.9810535311698914,0.0,accept,unanimous_agreement
538365070,7378,java 11/2.12 failed with [code block] java 8 timed out retest this please,0,0,0,0.9892098307609558,0.9901026487350464,0.991784930229187,0.0,accept,unanimous_agreement
538501838,7378,i tried to cover all your comments,0,0,0,0.9193664789199828,0.9079920649528505,0.9773548245429992,0.0,accept,unanimous_agreement
538528904,7378,lgtm.,0,0,0,0.9832575917243958,0.968936800956726,0.9637624621391296,0.0,accept,unanimous_agreement
538614484,7378,unrelated integration test failures. retest this please,0,0,0,0.9704573154449464,0.8212104439735413,0.9417821168899536,0.0,accept,unanimous_agreement
538702218,7378,java 11/2.12 failed with [code block] java 11/2.13 failed with [code block] java 8 failed with [code block] retest this please,0,0,0,0.9884042739868164,0.9918723702430724,0.993151307106018,0.0,accept,unanimous_agreement
538832801,7378,"jdk 11 / 2.12 [code block] jdk 11 / 2.13 [code block] java8 no test failures, but timed out. retest this please.",0,0,0,0.9841630458831788,0.9921348094940186,0.9916927814483644,0.0,accept,unanimous_agreement
538883648,7378,`org.apache.kafka.connect.integration.exampleconnectintegrationtest.testsourceconnector` failed. unrelated.,-1,0,0,0.5943247079849243,0.9755959510803224,0.9875375628471376,0.0,accept,majority_agreement
425418058,5709,failure unrelated retest this please,0,0,0,0.957226574420929,0.8864413499832153,0.5913138389587402,0.0,accept,unanimous_agreement
425575819,5709,updated per comments,0,0,0,0.9792535305023192,0.9720098376274108,0.9704824686050416,0.0,accept,unanimous_agreement
425780847,5709,as we are re-using the repartition topic name when optimizing checking for the `-repartition` suffix was done during those cases when re-using the topic name which is already formatted with `appid-basename-repartition` i have since cleaned up the code and re-use the topic name in the `internalstreamsbuilder` when performing the optimization.,0,0,0,0.9889860153198242,0.9948634505271912,0.994170606136322,0.0,accept,unanimous_agreement
425788398,5709,"hmm. i still cannot fully understand it since from the source code, the existing `repartitionforjoin` function seems not reused elsewhere in this pr.",0,0,0,0.916170060634613,0.9107531905174256,0.9371737837791444,0.0,accept,unanimous_agreement
425791386,5709,updated this,0,0,0,0.9801121354103088,0.9735050201416016,0.9438372254371644,0.0,accept,unanimous_agreement
425924162,5709,failures unrelated retest this please,0,0,0,0.9664865136146544,0.902599573135376,0.8057448267936707,0.0,accept,unanimous_agreement
426015601,5709,"call for final review - i believe i've addressed all of your comments, and this is ready for merging",1,0,0,0.7178871035575867,0.6901230216026306,0.8639419674873352,0.0,accept,majority_agreement
426085849,5709,restest this please,0,0,0,0.9822644591331482,0.962586224079132,0.9829925894737244,0.0,accept,unanimous_agreement
426094341,5709,this pr seems very close to be merged. do you think we can merge it in a day or two for 2.1.0 release?,0,0,0,0.9420077204704284,0.9117249250411988,0.9774976372718812,0.0,accept,unanimous_agreement
426096811,5709,the goal is to merge it today.,0,0,0,0.974306344985962,0.9855164885520936,0.9934985637664796,0.0,accept,unanimous_agreement
426135610,5709,rebased this,0,0,0,0.9852973818778992,0.9729411602020264,0.99217027425766,0.0,accept,unanimous_agreement
426159558,5709,merging this. please address comments if follow up pr.,0,0,0,0.986735701560974,0.9882214665412904,0.9946006536483764,0.0,accept,unanimous_agreement
200954587,812,do you by any chance have time to review this pr? thank you...,1,1,1,0.9607169032096864,0.8673836588859558,0.9057470560073853,1.0,accept,unanimous_agreement
205629220,812,could you also run the system tests on this patch?,0,0,0,0.9880239367485046,0.9928532242774964,0.9948371648788452,0.0,accept,unanimous_agreement
205806557,812,quick note to mention that the pr does not merge cleanly to trunk at the moment.,0,0,0,0.9846617579460144,0.9804903864860536,0.9855645298957824,0.0,accept,unanimous_agreement
206484021,812,thank you for the review. i have done the rebase and started a system test run.,1,1,1,0.929749608039856,0.923941969871521,0.9358679056167604,1.0,accept,unanimous_agreement
206877257,812,system test results are here: [a link] there was one failure (offsetvalidationtest) which is an existing transient failure already being addressed by kafka-3513.,0,0,0,0.9883275628089904,0.9944251775741576,0.9952241778373718,0.0,accept,unanimous_agreement
209425539,812,"the latest commit replaces the request-response to configure sasl mechanisms with asaslhandshakerequest/saslhandshakeresponse pair that conforms to the standard kafka protocol, as discussed in the kip-35/kip-43 threads in the mailing list. the overall code structure for authentication/handshake has not been changed.",0,0,0,0.9885507225990297,0.995472490787506,0.9927229285240172,0.0,accept,unanimous_agreement
212379418,812,will you be able to review the latest changes so that this can be committed for 0.10.0? i have rebased and started another system test run. thank you.,1,1,1,0.9515477418899536,0.9689463973045348,0.9546960592269896,1.0,accept,unanimous_agreement
212644270,812,thank you for the review. i have made most of the updates and left a couple of responses for you to review. many thanks.,1,1,1,0.9631189107894896,0.9848147630691528,0.9886041879653932,1.0,accept,unanimous_agreement
213326258,812,the latest system test results from this branch are here: [a link],0,0,0,0.9831180572509766,0.9839958548545836,0.9941171407699584,0.0,accept,unanimous_agreement
213821646,812,"thanks for running the system tests. are you going to update some of the system tests to test the mechanism functionality? also, weren't there some unit (as opposed to integration) tests for sasl in a previous iteration of the pr?",1,0,1,0.8588538765907288,0.6349695324897766,0.8919422626495361,1.0,accept,majority_agreement
214000262,812,": thanks for the patch. looks good to me. just had a minor comment. also, once the vote for kip-35 passes, would you be up for adding the support of apiversionrequest in the sasl authenticator layer since you are more familiar with the logic there?",1,1,1,0.984097421169281,0.9898276329040528,0.9915338158607484,1.0,accept,unanimous_agreement
214094390,812,thanks for the pr. i have completed my review now and left some comments.,1,1,1,0.9023298621177672,0.9578876495361328,0.9285855889320374,1.0,accept,unanimous_agreement
214276979,812,"thank you for the reviews. i have made most of the updates suggested and left a few comments. i had moved the sasl unit tests out of this pr to make it easier to keep it updated (since the tests have quite a bit of refactoring that makes it hard to rebase). once this is committed, i will rebase and submit the additional tests in another jira. i will also submit the pr for system tests of the changes - i think it should be sufficient to run only a few tests with plain, plain+gssapi. i will be happy to submit the pr for handling apiversionrequests during sasl handshake as well, once the new request is added for kip-35.",1,1,1,0.9789506793022156,0.9879817366600036,0.986957311630249,1.0,accept,unanimous_agreement
214356654,812,", thanks for updating the pr. i reviewed your updates and left some very minor comments. the main item left on this pr in my opinion is how we name the inter-broker sasl mechanism property. i think it would be better if the config name is consistent with the property for inter-broker security protocol (ie there is an `inter.broker` in the name). let's see what thinks. before 0.10.0.0, we need to: 1. add unit, negative and system tests for this pr 2. add handling of apiversionrequests during sasl handshake would you mind please filing jiras with 0.10.0.0 as the ""fix version""?",1,1,1,0.8925437331199646,0.6979864239692688,0.969459593296051,1.0,accept,unanimous_agreement
214464394,812,thank you for reviewing the updates. have filed new jiras for the remaining work: - kafka-3617 : add unit tests for sasl authenticator - kafka-2693 : ducktape tests - kafka-3618: handle apiversionrequest before sasl handshake,1,1,1,0.9282236695289612,0.931324064731598,0.9353963136672974,1.0,accept,unanimous_agreement
214563135,812,"thanks , lgtm.",1,1,1,0.877602756023407,0.8723828196525574,0.8876838684082031,1.0,accept,unanimous_agreement
214868708,812,", i ran the unit/integration tests with this branch twice and both times i got some failures in sasl tests. do the tests pass reliably for you? is it possible that a recent change is causing issues?",0,0,0,0.9612277746200562,0.9709502458572388,0.9857207536697388,0.0,accept,unanimous_agreement
214900437,812,"the tests have been passing consistently for me. i reran them again a few times after rebasing on the latest level and they still pass. if you let me know which tests have been failing for you, i can rerun those in a loop to see if i can recreate the failure. will also see how the automated pr build does after the rebase. thanks.",1,1,1,0.9528500437736512,0.9209118485450744,0.984345018863678,1.0,accept,unanimous_agreement
214906615,812,", thanks. i am running the tests again in case it was a mistake on my end. if they pass in jenkins and locally, i'll merge the pr (i checked with jun and he's happy with it). if not, i'll provide information about the failing tests.",1,1,1,0.9353410601615906,0.9498273134231568,0.969260573387146,1.0,accept,unanimous_agreement
214924091,812,"the failing tests were related to the flaky wireless network in the kafka summit, i got the same failure with ssl tests (where sasl wasn't involved). after i stabilised the network, the tests passed twice and they passed in jenkins too. merged to trunk.",0,0,0,0.9088970422744752,0.9300068616867064,0.9876675605773926,0.0,accept,unanimous_agreement
137300283,165,"i left a few comments, but mostly they were things that could potentially be clarified. only one question about correctness. after those issues are followed up on, this looks good to me.",1,1,1,0.9369076490402222,0.9666168093681335,0.9074286222457886,1.0,accept,unanimous_agreement
146709577,165,"i've updated this patch based on the latest proposal ([a link] which uses a single client to do the group assignment. i'm still doing a bit of polishing and adding tests, but since most existing tests are passing, it seems ready to start reviewing. there is one failing kstreams test that i have yet to investigate. a few general notes on the patch: - one significant difference from the proposal is that i had to push the assignment strategies out of the consumer's embedded protocol and into the joingroup request as a ""subprotocols"" field. this allows the coordinator to do assignment strategy selection similar to the initial patch. it also lets the coordinator reject incompatible members without forcing a group rebalance. otherwise, we can only detect inconsistencies after the group has rebalanced and the leader sees the metadata from all members. the problem then becomes how the group should handle the inconsistency? we could let the leader kick certain members out of the group, but that complicates the protocol. we could also just fail the entire group, but that seemed extreme. in the end, it seemed simplest to give the coordinator a mechanism to validate compatibility of group members. - this brings up the issue of whether each ""subprotocol"" (or assignment strategy) needs to have its own metadata and embedded as in the initial patch. without it, any assignment-strategy-specific information must itself be embedded in the member metadata included in the joingroup request. this is easy if the consumer is using a metadata format like json, but trickier with packed arrays. the advantage of keeping everything in the same metadata, however, is that it simplifies the protocol and avoids redundancies such as including subscriptions multiple times. - if the leader has an unexpected error during synchronization, the patch does not provide a way to propagate that error to the rest of the group as the proposal called for. instead, the leader will propagate the exception to the user and the leader's session timeout will expire on the coordinator which will allow for a new leader to be elected. the problem i found when trying to implement this feature is that it requires a new state on the coordinator which is basically like stable except contains an error to be propagated when the rest of the group syncs, which adds considerable complexity. it also creates a race condition where some members might see the error and immediately rejoin which forces the group out of that state. this means that some members wouldn't generally see the error. in the end, it felt like the cost of propagating the error through the coordinator was too high to justify capability it was exposing. since unexpected leader failures should be rare, the simpler approach is just to let it timeout. - the versioning in the consumer's embedded protocol (as contained in consumerprotocol.java) is clearly inadequate and will require further consideration. i'm inclined to push that to a separate jira so that we can consider it without the rest of the noise around this patch. - the protocol implemented here provides no special support for regex handling. as before, each member watches metadata and adds a normal topic subscription when it finds a new matching topic. if we want to have special handling, that can also be pushed to another jira.",0,0,0,0.901385486125946,0.9310794472694396,0.5209600925445557,0.0,accept,unanimous_agreement
147134061,165,"not critical for this patch, but do we want to reconsider the packages for some of the classes? for example, `groupcoordinator` is currently in `org.apache.kafka.clients.consumer.internals`. keeping it in _some_ `internals` package makes sense at the moment to indicate it shouldn't be relied on publicly, but since it's generalized functionality, housing it under `consumer` might not be ideal. not sure if we want a new package (e.g. `org.apache.kafka.clients.group`), move it under `common`, or just leave it where it is and preserve some indication of its heritage.",0,0,0,0.9569081664085388,0.9859212040901184,0.9655458331108092,0.0,accept,unanimous_agreement
147185767,165,just a heads-up: this branch doesn't merge cleanly against trunk anymore.,0,0,0,0.9733741879463196,0.9842979311943054,0.9776907563209534,0.0,accept,unanimous_agreement
147262916,165,"yeah, i think it makes sense to move it to a new package. to make that possible, i think we might have to merge consumernetworkclient into networkclient, but that's the only complication i can think of.",0,0,0,0.9008021354675292,0.9811148047447203,0.9875232577323914,0.0,accept,unanimous_agreement
147550028,165,"a general comment: i feel adding this to the join-group to allow coordinator-side pre-checking would be ok, since although it adds an extra field in the join group request, it is not very hard to understand. but now the protocol field seems less useful to me given the group-id and sub-protocols, since we only use it to check all members use the same group-protocol string. but in practice how possible that members joining the same group name happen to be different types (e.g. one normal consumer and one copycat connector)? having this field will not be so useful in excluding human-mistakes but even likely increase false-positives that two members that do belong to the same group got kicked out because they happen to specify different protocol names by mistake. so i would suggest we remove the protocol string and rename sub-protocols to protocols, doing this we may end up having group ids like ""kafka-consumer-xxx"", ""kafka-stream-yyy"" and ""kafka-copycat-zzz"", which is already sufficient to differentiate different types of members.",0,0,0,0.8621745109558105,0.962504744529724,0.921872854232788,0.0,accept,unanimous_agreement
147768297,165,"that suggestion sounds fine to me, especially if it makes the protocol easier to understand. it leads to the question of how we are going to implement protocol-specific metadata. there's basically two options: **option 1:** each protocol has its own metadata: this is probably the simplest to implement for client extensions. this would change the joingroup request to look something like this: [code block] the downside is that this leads to redundant data in the request. for example, to support both the round-robin and range assignors in the consumer (at the same time), you'd have to pass the subscription list twice. perhaps this is a rare enough case that we aren't too worried about the overhead? it also leads to somewhat more complex parsing, though i don't think this is a major concern. **option 2:** the second option, which is currently implemented, is that we have only one metadata field and we assume that any protocol-specific attributes are embedded in it. it would look like this: [code block] this is generally more difficult for clients to implement, but that depends largely on the format of the metadata. for example, if json is chosen, then protocol-specific attributes come naturally. however, it's a little tough to see how this would work for kafka's packed structures, which have no field identifiers. as an advantage, since it allows protocols to share metadata, it avoids the duplication in the first approach. the groupprotocol field in the current patch was basically intended to declare compatibility of the metadata as used in option 2. if we get rid of it, then i would tend to favor the first option. what do you think?",0,0,0,0.9210865497589112,0.9316553473472596,0.8609796166419983,0.0,accept,unanimous_agreement
147772825,165,"it might be worth making multiple assignors work to figure out how best to handle this. right now the code only handles one assignor and i think that makes option 2 look like it can work, but the code is probably going to get messy or confusing if you try to actually implement it. since you only have one metadata field, you'll need to somehow combine all the metadata required by different assignors. this means you'll have to assume some format like json and merge them. i think the duplication in option 1 isn't a big deal. it should definitely be rare. it also keeps things simpler -- the joingroup request is a bit more complex, but the whole implementation is easier and we don't end up with completely different protocols having to coordinate in order to make sure their metadata is compatible with each other.",0,0,0,0.6608567833900452,0.88532954454422,0.7813045382499695,0.0,accept,unanimous_agreement
147814103,165,"my understanding is that multiple protocols will only be used for upgrades, and most of time the protocols size should be 1. if that is the case: 1. 98% we will have 1 protocol. 2. 1.9999% we will have 2 protocols. 3. 0.0001% we will have 3 protocols. so i think option 1) would be find, but could clarify if there are no scenarios that multiple protocols are needed life-time.",0,0,0,0.9528111815452576,0.986625909805298,0.9900482892990112,0.0,accept,unanimous_agreement
147815469,165,i think 3 protocols should never happen aside from misconfiguration. the only use case for 2 is to upgrade/switch assignors.,0,0,0,0.98445063829422,0.988462507724762,0.9692364931106568,0.0,accept,unanimous_agreement
147816246,165,"yeah, for upgrade / switch 2 protocols are temporary since we will usually do two rolling bounce to remove the old one in the second round. anyways, i think option 1) should be fine.",0,0,0,0.9047960042953492,0.9675922989845276,0.9628530740737916,0.0,accept,unanimous_agreement
148478770,165,i've reworked the protocol as discussed to allow protocol-specific metadata with coordinator negotiation similar to the initial patch. basically this resulted in pushing serialization concerns into abstractcoordinator extensions. also have a look at the new partitionassignor interface. it should be much clearer how assignor implementations can leverage custom metadata.,0,0,0,0.9815332293510436,0.9817610383033752,0.9855552911758424,0.0,accept,unanimous_agreement
148802867,165,made the changes discussed. i think this does simplify the assignor internals quite a bit.,0,0,0,0.9588058590888976,0.9755654335021972,0.8944786787033081,0.0,accept,unanimous_agreement
148823145,165,the client-side protocol and implementations lgtm overall. thanks! do you want to refactor the server-side fsm a bit now? also some response error codes like syncgroupresponse need to be updated as well.,1,1,1,0.9403064250946044,0.986168086528778,0.9812397956848145,1.0,accept,unanimous_agreement
149285168,165,anything else to clear up before this can be merged?,0,0,0,0.9787792563438416,0.9928264617919922,0.9928701519966124,0.0,accept,unanimous_agreement
149287269,165,"i'm still reviewing the changes made on the server-side coordinator, i thought you did the fsm refactoring right?",0,0,0,0.9822065830230712,0.986622393131256,0.9889937043190002,0.0,accept,unanimous_agreement
149288054,165,"i updated the fsm documentation that was in groupmetadata.scala. was there anything else you wanted me to fix? by the way, i'm rebasing off of flavio's commit and should be able to update shortly.",0,0,0,0.9825165271759032,0.986409604549408,0.9645292162895204,0.0,accept,unanimous_agreement
149288862,165,the problem is that i cannot get the full diff now as it become too big. right now i can only review it per-commits which is not optimal..,-1,-1,-1,0.726676344871521,0.816489577293396,0.8930931687355042,-1.0,accept,unanimous_agreement
149308251,165,"this is what i think about the state machine (if we agree we can also update the diagram in the wiki), with the assumption that for all request handling we first check coordinator availability and group ownership is correct. all the events except consumer failure is triggered by requests. state down: group is either not created or has no members. -> onjoin: create group and add member if necessary (start hb in purgatory), transit to preparerebalance. -> onsync: return rebalance_in_progress (could be from as -> pr -> down). -> onheartbeat: return illegal_generation. -> onoffsetcommit: blindly accepts and return ok. -> onoffsetfetch: blindly accepts and return ok. -> onconsumerfailure: should not happen, throw exception. state preparerebalance: waiting for all members to join -> onjoin: check if all members joined, if yes transit to awaitingsync -> onsync: return rebalance_in_progress (could be from as -> pr). -> onheartbeat: return rebalance_in_progress. -> onoffsetcommit: return rebalance_in_progress. -> onoffsetfetch: return rebalance_in_progress. -> onconsumerfailure: remove consumer (stop hb in purgatory), check if all rest members joined, if yes transit to awaitingsync. state awaitingsync: waiting for leader to assign -> onjoin: update / create consumer if necessary, transit to preparerebalance -> onsync: if from follower, treat it as hb and park it (unmute further hbs); otherwise treat it as hb for the leader and transit to stable. -> onheartbeat: return rebalance_in_progress. -> onoffsetcommit: return rebalance_in_progress. -> onoffsetfetch: return rebalance_in_progress. -> onconsumerfailure: remove consumer (stop hb in purgatory), if that is the leader transit to preparerebalance state stable: group formed with resource assigned and remembered by coordinator -> onjoin: update / create consumer if necessary, transit to preparerebalance -> onsync: handle normally with remembered assignment -> onheartbeat: handle normally. -> onoffsetcommit: check on member-id / generation-id and return ok or corresponding errors. -> onoffsetfetch: check on member-id / generation-id and return ok or corresponding errors. -> onconsumerfailure: remove consumer (stop hb in purgatory), transit to preparerebalance. --- transit action: from down to preparerebalance: 1) start delayed rebalance in purgatory if there are still members in the group 2) mute delayed hb for all members in the group transit action: from preparerebalance to awaitingsync: 1) clear dalyed rebalance in purgatory, send back join response to followers and leader 2) unmute delayed hb for all members in the group transit action: from awaitingsync to preparerebalance: 1) send back sync response to all parked requests with rebalance_in_progress 2) start delayed rebalance in purgatory if there are still members in the group 3) mute delayed hb for all members in the group transit action: from stable to preparerebalance: 1) start delayed rebalance in purgatory if there are still members in the group 2) mute delayed hb for all members in the group --- i think we are already doing this in this patch, but just want to make the comments in groupmetadata to be more clear. and not need to include in this jira, but moving forward we will possibly add a background scheduler that periodically remove groups with zero members or not belonged to coordinator any more and transit them to dead.",0,0,0,0.9695470333099364,0.9916952252388,0.9861970543861388,0.0,accept,unanimous_agreement
149316362,165,"thanks for writing that up! i think there's a couple slight differences in the current patch from what you wrote: state down: group is either not created or has no members. -> onheartbeat: return unknown_member_id -> onoffsetcommit: return illegal_generation if generation is positive, otherwise accept commit -> onoffsetfetch: return unknown_member_id state preparerebalance: waiting for all members to join -> onoffsetcommit: accept commits from previous generation, otherwise illegal_generation -> onoffsetfetch: return offsets blindly state awaitingsync: waiting for leader to assign -> onoffsetcommit: allow commit from the joined generation -> onoffsetfetch: return offsets blindly for the awaitingsync state, i think your suggestion to return rebalance_in_progress for offset commits is better than the current behavior since there should be no need to commit offsets before receiving the assignment and the member still has the preparingrebalance state to commit offsets before rebalances. for fetching committed offsets, i actually wonder if we should skip all group checks and return the offsets blindly. do we really need to impose the restriction that committed offsets can only be queried by members of the group?",1,1,1,0.9619665741920472,0.9640596508979796,0.9935277104377748,1.0,accept,unanimous_agreement
149355804,165,could we make partitioner also configurable as we discussed offline? besides the above comments lgtm.,0,0,0,0.9893001317977904,0.9942734837532043,0.995412528514862,0.0,accept,unanimous_agreement
149362802,165,have a look at the updated state transitions. i wonder if we should have a `ready` state for the initial state when no group members have joined. currently the group starts up in `stable` even though it has no members. what do you think?,0,0,0,0.9670111536979676,0.9443280696868896,0.9754980206489564,0.0,accept,unanimous_agreement
149372398,165,"i agree to the `ready` state, since if we are going to clean up groups moving forward we will no longer need the `dead` state any more.",0,0,0,0.9791359305381776,0.9757975935935974,0.9786460995674132,0.0,accept,unanimous_agreement
149996276,165,"lgtm, the jenkins failures seem irrelevant and transient. also made a passing ducktape system test build on this branch. please remember to address 's comments in the later follow-up path. merging this large patch for now as it is getting very large.",0,0,0,0.9628145098686218,0.9762925505638124,0.9893798232078552,0.0,accept,unanimous_agreement
2427455305,17539,"hi , please review my pr when you get a chance. thanks!",1,1,1,0.9749274253845216,0.990119695663452,0.9875962734222412,1.0,accept,unanimous_agreement
2433136716,17539,"hi , thanks for being kind enough to reviewing this pr. i have addressed both your comments. please take a look when you can!",1,1,1,0.9746993780136108,0.9939011335372924,0.9909900426864624,1.0,accept,unanimous_agreement
2450647031,17539,"hi , i am still in the middle of the refactor. i will raise a re-review once i am done. i was just resolving the comments for which i pushed the fixes in my recent commits, probably that gave a false idea that i was done addressing your changes, sorry about that.",-1,-1,-1,0.9792375564575196,0.9899129867553712,0.9853336811065674,-1.0,accept,unanimous_agreement
2452271960,17539,"hi , i have addressed all your comments. please re-review my pr when you get a chance. thanks!",1,1,1,0.9781217575073242,0.9934496283531188,0.9883302450180054,1.0,accept,unanimous_agreement
2455446039,17539,"hi , i have addressed your comments. please re-review my pr when you get a chance. thanks!",1,1,1,0.9775394201278688,0.9918707609176636,0.9865790009498596,1.0,accept,unanimous_agreement
2461751459,17539,"hi , i've addressed the comments from the latest review. please re-review my pr when you get a chance. thanks!",1,1,1,0.9681522846221924,0.991020143032074,0.98765367269516,1.0,accept,unanimous_agreement
293814064,2849,"for reviews please. note: this is not the complete transactioncoordinator, i.e., transaction expiration, transactionalid -> pid mapping expiration, and recovery in handling initpidrequest all haven't been done yet",0,0,0,0.9810691475868224,0.9910542368888856,0.9858230352401732,0.0,accept,unanimous_agreement
296940620,2849,thanks for taking the time to review again. regarding: correct they have not been done yet.,1,1,1,0.5374612212181091,0.9320643544197084,0.961566150188446,1.0,accept,unanimous_agreement
297542444,2849,collapsed all commits and merged to trunk.,0,0,0,0.9257720708847046,0.9923059344291688,0.9930075407028198,0.0,accept,unanimous_agreement
297552932,2849,"for large prs like this, we should run the system tests before we merge. can we post a link to a successful run for future record (assuming we've done that)?",0,0,0,0.9870768189430236,0.9926032423973083,0.9941540360450744,0.0,accept,unanimous_agreement
297553624,2849,i've kicked off a build here: [a link] let's cross our fingers since it's already merged!,0,0,1,0.8249790668487549,0.8790725469589233,0.9323981404304504,0.0,accept,majority_agreement
297651197,2849,"thanks , build passed. :)",1,1,1,0.9899396300315856,0.9960675239562988,0.9966729879379272,1.0,accept,unanimous_agreement
298811252,2964,i am still working to fixing all the unit tests but the non-testing code is ready for reviews.,0,0,0,0.953214704990387,0.8980469703674316,0.9651191234588624,0.0,accept,unanimous_agreement
298820761,2964,looks like the build fails with compilation errors..,-1,0,0,0.5557010173797607,0.9059169292449952,0.8754266500473022,0.0,accept,majority_agreement
298828273,2964,those are unit test compilations. i'm still working on those but non-testing code does build.,0,0,0,0.9863808751106262,0.9416914582252502,0.9882748126983644,0.0,accept,unanimous_agreement
298833334,2964,ah.. ok.. thanks!,1,1,1,0.9486621022224426,0.9921677708625792,0.976762592792511,1.0,accept,unanimous_agreement
300638549,2964,i have addressed your comments. two major modifications: 1. `transactionmarkerrequestcompletionhandler`: exhaust all possible error codes. 2. merge `transactionmarkerchannel` into `transactionmarkerchannelmanager`.,0,0,0,0.9871981739997864,0.9830516576766968,0.9870672821998596,0.0,accept,unanimous_agreement
300989188,2964,"addressed your comments, and fixed unit tests. also another observation is that we are unnecessarily sending multiple txn marker requests to a single broker for txnids from different txn topic partitions, i have grouped them into a single request as well in this pr. cc",0,0,0,0.946070432662964,0.9879109859466552,0.9874890446662904,0.0,accept,unanimous_agreement
301195878,2964,merged to trunk.,0,0,0,0.9853448271751404,0.991689682006836,0.9864094257354736,0.0,accept,unanimous_agreement
301222344,2964,"thanks for the comments, i will try to incorporate them in a follow-up pr.",1,1,1,0.7200688719749451,0.6612666249275208,0.9164412021636964,1.0,accept,unanimous_agreement
248246823,1884,"one general comment. `internaltopicmanager` isn't really doing anything anymore. i left this comment on the previous pr: it seems to me that they should be merged into a single class? , thoughts?",0,0,0,0.6872100234031677,0.959325075149536,0.9174911975860596,0.0,accept,unanimous_agreement
248379483,1884,made changes according to your feedback.,0,0,0,0.9790167212486268,0.984565794467926,0.9862051606178284,0.0,accept,unanimous_agreement
248967534,1884,made changes according to your feedback and pushed the changes.,0,0,0,0.9789167642593384,0.9852360486984252,0.9918994903564452,0.0,accept,unanimous_agreement
249255156,1884,applied the feedback and pushed the changes.,0,0,0,0.9826194047927856,0.9835379719734192,0.9936460256576538,0.0,accept,unanimous_agreement
249611052,1884,"oic - the problem with making it public on kafkastreams is that it would then be part of the public api. we probably should move the field from kafkastreams to an internal class and have kafkastreams and this class reference that. problem is i'm not entirely sure where to put it right now. [a link] might have an idea. otherwise, just leave it here and we can fix it later on mon, 26 sep 2016 at 16:42 hjafarpour notifications.com wrote:",-1,0,0,0.5893672704696655,0.9581953287124634,0.9256038665771484,0.0,accept,majority_agreement
249612400,1884,"alright, leaving it as it until we decide where we can move the field.",0,0,0,0.9754379391670228,0.986006498336792,0.9902634620666504,0.0,accept,unanimous_agreement
250317873,1884,pushed new changes.,0,0,0,0.9443108439445496,0.9818312525749208,0.9884201288223268,0.0,accept,unanimous_agreement
250342391,1884,"i ran the unit test locally and the jenkins failures seems to be pretty consistent, were your local unit test run passed? [code block]",0,0,0,0.980781078338623,0.983669638633728,0.9911386370658876,0.0,accept,unanimous_agreement
250561766,1884,do you have a timeline for this fix? we need this fix as part of one of our client apps that we are building that uses spring cloud stream. anything my team or i can do to help?,0,0,0,0.9695459008216858,0.9846936464309692,0.9739964604377748,0.0,accept,unanimous_agreement
255222098,1884,"sorry for the late reply, we are currently merging this pr as a post-0.10.1.0 pr, which means that it will likely to be included in the next minor release or in a bug-fix release (0.10.1.1).",-1,-1,-1,0.9880450367927552,0.9876384735107422,0.9757694602012634,-1.0,accept,unanimous_agreement
256424024,1884,could you file a kip for this api change as well?,0,0,0,0.9894428253173828,0.994325578212738,0.995051920413971,0.0,accept,unanimous_agreement
270991824,1884,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
271060729,1884,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
271096444,1884,jenkins failure seems related: [code block] [code block],0,0,0,0.9132050275802612,0.986396551132202,0.9897010326385498,0.0,accept,unanimous_agreement
271452298,1884,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
271714172,1884,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
271930762,1884,"lgtm. merged to trunk, thanks !",1,1,1,0.985774040222168,0.9919650554656982,0.9885109663009644,1.0,accept,unanimous_agreement
406449812,5379,could you give this another look?,0,0,0,0.9832844138145448,0.97771954536438,0.9840785264968872,0.0,accept,unanimous_agreement
406665448,5379,i think the final commit addresses all of the comments. can you confirm?,0,0,0,0.9879205226898192,0.9873199462890624,0.9913039207458496,0.0,accept,unanimous_agreement
406723141,5379,made `saslextensionscallback` return `saslextensions`. please take a look,0,0,0,0.9835264086723328,0.9918143153190612,0.9940153956413268,0.0,accept,unanimous_agreement
406733607,5379,seems like some related tests are failing. currently investigating them but it's not obvious what caused them to fail at all,-1,-1,0,0.747390627861023,0.5282135009765625,0.7453646063804626,-1.0,accept,majority_agreement
406750227,5379,"i had some issues with failing tests due to inappropriate handling of `scramextensionscallback`. i've now separated it from `saslextensionscallback` entirely - it doesn't return `saslextensions` but a `map<string, string`. hopefully this shouldn't matter too much as we deprecated it. should be all clear now. can you review?",0,0,0,0.7195962071418762,0.9592564105987548,0.7354593276977539,0.0,accept,unanimous_agreement
407474576,5379,"this should address all comments , . thanks for being patient with me",1,1,1,0.9708808064460754,0.9496566653251648,0.9845788478851318,1.0,accept,unanimous_agreement
407884676,5379,lgtm,0,0,0,0.9795994758605956,0.7242414951324463,0.9618706703186036,0.0,accept,unanimous_agreement
410763090,5379,merging to trunk. thanks for the reviews.,1,1,1,0.9154109358787536,0.819878876209259,0.9693832993507384,1.0,accept,unanimous_agreement
137326320,191,"anyone interested running this on vagrant , build kafka with this patch in and checkout [a link] . drop the .tgz under kafka-vagrant. vagrant up.",0,0,0,0.984148383140564,0.9931952953338624,0.994122326374054,0.0,accept,unanimous_agreement
137876636,191,"thanks harsha, good to see this. in order to review this, it would be good to have a summary of the status of the pr, what areas (if any) you are still working on?",1,1,1,0.979957640171051,0.987187385559082,0.990365207195282,1.0,accept,unanimous_agreement
137877918,191,"cleaning up the code and adding sasl configs part, more unit tests.",0,0,0,0.9749725461006165,0.98921799659729,0.9931341409683228,0.0,accept,unanimous_agreement
137878143,191,thanks.,0,1,0,0.5150366425514221,0.5804154276847839,0.5270382761955261,0.0,accept,majority_agreement
146770873,191,please review the updated patch. thanks.,1,1,0,0.875430703163147,0.8707457780838013,0.633817732334137,1.0,accept,majority_agreement
146864969,191,thanks . i submitted a pr with your branch as the target: [a link] it merges trunks (and resolves the conflicts) and it includes some fixes and improvements. please review and integrate if you agree with the changes. i am still going through the code and i will submit another pr later that will include a saslconsumertest.,1,1,1,0.9682344198226928,0.9750061631202698,0.9735930562019348,1.0,accept,unanimous_agreement
146999246,191,updated the pr with consumer test . i'll be updating with docs and other fixes mentioned in the above comments tonight.,0,0,0,0.9784730672836304,0.9801443815231324,0.9805617928504944,0.0,accept,unanimous_agreement
147425105,191,here's another pr [a link] which: - merges and resolves conflicts from trunk - address some of jun's and parth's comments - make fields final - reduce scope of variables where possible - remove unused fields and methods - fix javadoc - fix formatting and naming issues - return non-anonymous `kafkaprincipal` in `saslclientauthenticator.principal` please review and integrate as appropriate . there is still feedback to be addressed as well as more tests (particularly for inter-broker communication and sasl_ssl). i will continue working on these.,0,0,0,0.9142449498176576,0.9822610020637512,0.9552488923072816,0.0,accept,unanimous_agreement
147777026,191,thanks for regularly merging my prs harsha. here's another one: [a link],1,1,1,0.8506794571876526,0.9778027534484864,0.9815628528594972,1.0,accept,unanimous_agreement
147903851,191,"also, a general question, if the tgt ticket can't be renewed, do we get exceptions when reading/writing through the sasl port?",0,0,0,0.98661869764328,0.9912304878234864,0.9918882250785828,0.0,accept,unanimous_agreement
148224075,191,yes. it will throw a kafkaexception,0,0,0,0.9861466288566588,0.979816734790802,0.9854483008384703,0.0,accept,unanimous_agreement
148396199,191,"pr 4 is ready: [a link] it: - fixes issues with the loginmanager singleton (we need two instances, one for mode.server and one for mode.client and closing it is not as simple as how it's done in the existing code) - a number of logging clean-ups (avoid string concat so that we don't pay the cost if that particular level is disabled) - remove setconfiguration call as suggested by next on my list: - [x] removeinterestops issues on server and client authenticator mentioned by jun - [x] refactor tests to reduce duplication, test sasl_ssl, test sasl for inter-broker communication and - fix test failure when all the tests are run via gradle (i know what the issue is) - [x] replace kerberosname system property with config - [x] investigate if there's a way to avoid using proprietary classes in `jaasutils.defaultrealm` i think we're getting closer.",0,0,0,0.921710729598999,0.9825224876403807,0.957286536693573,0.0,accept,unanimous_agreement
148564929,191,i see you changed loginmanager to do client and server. but in sasl case we use kafkaserver section for both kafkaserver and any inter broker calls. the reason for this is we want keep kafkaclient section for only the clients and use the keytab in kafkaserver section. hence the reason we want to initialize the loginmanager eitehr with server or client but not with both. so in broker side we do loginmanager(server) and inside the controller or replica we use inter.broker.security.protocol and use client as mode initiate salsclientauthenticator with loginmanager.subject.,0,0,0,0.9689999222755432,0.9913115501403807,0.9783813953399658,0.0,accept,unanimous_agreement
148567215,191,"regarding `loginmanager`, thanks for the explanation, good to know that we want to use `kafkaserver` section for the broker, whether it's a client or a server. however, if `loginmanager` doesn't support client and server simultaneously, how will it work in situations where you have a client and a server in the same jvm (like in tests, for example)? i think `loginmanager` should support both modes, but we should change how we call it from `saslchannelbuilder` perhaps. what do you think?",1,0,1,0.723091721534729,0.6905930042266846,0.9783263206481934,1.0,accept,majority_agreement
148568777,191,yes makes sense.,0,0,0,0.9715992212295532,0.9843671917915344,0.980026364326477,0.0,accept,unanimous_agreement
148571060,191,"is that right? in controllerchannelmanager, replicafetcherthread, and kafkaserver (for controlled shutdown), we create the channel in client mode (instead of server). this seems correct since they initiate client connections. the socketserver is always created in server mode. so, it seems that the broker will always need to support two modes for loginmanager.",0,0,0,0.985096037387848,0.9937008619308472,0.987826645374298,0.0,accept,unanimous_agreement
148572207,191,"i think the confusion is due to the fact that `mode` is being misused in `loginmanager`. it actually means `logincontexttype` in that context and it is only used to derive the logincontextname. i intend to change it along these lines so that it's clearer. , please correct me if i misunderstood.",0,0,0,0.808661162853241,0.7577835917472839,0.9218617081642152,0.0,accept,unanimous_agreement
148574665,191,"hmm, so all channels in controllerchannelmanager, replicafetcherthread, and kafkaserver should be created in server mode to pick up the kafkaserver section?",0,0,0,0.9876327514648438,0.9924829602241516,0.9847829937934875,0.0,accept,unanimous_agreement
148575986,191,"not exactly, i think it's something like this: [a link] (i just did it quickly to show it in code, still need to double-check it)",0,0,0,0.9083099961280824,0.9797132015228271,0.9860209822654724,0.0,accept,unanimous_agreement
148593273,191,your code looks good to me. if you open a pr i can run some tests. thanks.,1,1,1,0.9758121371269226,0.9919997453689576,0.9953396320343018,1.0,accept,unanimous_agreement
148683145,191,"i am not sure _""servicename has always been used in jaas config""_. i can run zookeeper 3.4.6 with sasl either with the default name `zookeeper` or by specifying the system property `zookeeper.sasl.client.username` to override the name. at the moment, using `servicename` in jaas.conf as the only way to configure the name prevents sasl from being used with ibm jdk. at the very least, we need to set a default. the exceptions with ibm jdk when `servicename` is set as well as the exception when `servicename` is not set are below. if `servicename` is specified, the exception is: [code block] if `servicename` is not specified, the exception is: [code block]",0,0,0,0.9679763317108154,0.9890294075012208,0.9491899013519288,0.0,accept,unanimous_agreement
148689700,191,i agree that we should do something about this. my preference is to move it to the kafka config.,0,0,0,0.9294214844703674,0.9578868746757508,0.9711860418319702,0.0,accept,unanimous_agreement
148702719,191,"pr 5: [a link] - introduces `logintype` and simplifies caching code in `loginmanager` - make `auth_to_local` configurable via kafkaconfig instead of a system property. i went with the same name as the one hadoop uses, but it's not the most intuitive so we may want to revise that (it should be a simple change though).",0,0,0,0.979206085205078,0.9919906258583068,0.9903755784034728,0.0,accept,unanimous_agreement
148716439,191,i started on the inter-broker tests and things are looking good. sasl_plaintext and sasl_ssl work fine (the latter required a small fix).,1,1,1,0.5014671683311462,0.5851429104804993,0.8577885031700134,1.0,accept,unanimous_agreement
148738345,191,ok for servicename i want to pick default via jaas config and if its not set we will look for it in the sasl configs. will that work for you?,0,0,0,0.986820638179779,0.9922834038734436,0.9941728711128236,0.0,accept,unanimous_agreement
148739210,191,"yes, thank you. that will work for us,",1,1,1,0.932026982307434,0.9862158298492432,0.9728626012802124,1.0,accept,unanimous_agreement
148740906,191,i pushed the inter-broker tests for sasl_plaintext and sasl_ssl to [a link] working on sasl_ssl tests for producer and consumer now.,0,0,0,0.9871525168418884,0.9937773942947388,0.99416983127594,0.0,accept,unanimous_agreement
149203336,191,", pr 6 is ready [a link] - merged trunk and fixed conflicts - refactored producer and consumer tests to reduce duplication and to also test sasl_ssl (fixed an important, but small bug with sasl_ssl in the process) - fixed issue where interestops was not being turned off when it should be - document `authenticate` in `saslclientauthenticator` and `saslserverauthenticator` - make it possible to configure servicename via kafkaconfg `./gradlew test` passed on my laptop. we need to move fast in order to get this into 0.9.0.0, so i'd appreciate it if you could merge this into your branch so that jun can review it.",0,0,0,0.9669674634933472,0.9757477641105652,0.5801668167114258,0.0,accept,unanimous_agreement
149315813,191,": pr #6 from looks good to me. do you want to merge that into your branch? once you do that, i think we can probably just commit the patch as it is and address other issues, if any, in followup jiras.",1,1,1,0.9787952303886414,0.9063902497291564,0.7125228047370911,1.0,accept,unanimous_agreement
149746290,191,can you open a pr against this branch so that i can merge it in,0,0,0,0.9888691902160645,0.9914141297340392,0.9943174719810486,0.0,accept,unanimous_agreement
149750466,191,"thanks for merging the pr to your branch harsha. since jun merged the sasl pr via #334 (which is exactly the same as this one now), would you mind closing this one please? thanks for your work on this. we'll probably need a follow-up to tweak some things and some more tests (particularly ducktape ones) but we're most of the way there.",1,1,1,0.9164965748786926,0.9755945801734924,0.9867231249809264,1.0,accept,unanimous_agreement
781745956,10070,"hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred?",0,0,0,0.9730047583580016,0.9675247669219972,0.96719890832901,0.0,accept,unanimous_agreement
782521288,10070,"thanks, ! :)",1,1,1,0.9914987683296204,0.995605170726776,0.9964231848716736,1.0,accept,unanimous_agreement
326210780,3765,note that this pr is wip and only replicastatemachinev2 currently attempts to do retries and error handling. retries and error handling will be added to the rest of the controller in later rounds of review.,0,0,0,0.9882735013961792,0.9923539161682128,0.992423415184021,0.0,accept,unanimous_agreement
326461993,3765,can you take a look?,0,0,0,0.9862033128738404,0.9826734662055968,0.992240309715271,0.0,accept,unanimous_agreement
332270742,3765,"i rebased, resolved merge conflicts, and force pushed the change.",0,0,0,0.9814618229866028,0.9829484820365906,0.9819074273109436,0.0,accept,unanimous_agreement
335763940,3765,"i just ran an experiment that measures controller failover time before and after this pr. the environment: * 5 broker kafka cluster with brokers on different racks * 5 node zookeeper ensemble with nodes on different racks * 100,000 single-partition single-replica topics trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585): [code block] kafka-5642: [code block] trunk average controller failover time: ~60 seconds kafka-5642 average controller failover time: ~24 seconds",0,0,0,0.932375967502594,0.9918076395988464,0.9830641150474548,0.0,accept,unanimous_agreement
336032158,3765,"i compared the breakdowns of where time was getting spent in both the trunk and kafka-5642 runs of the experiment. i don't see any performance regressions in the parts that don't interact with zookeeper. so the zookeeper parts got better with kafka-5642 and the rest stayed the same, which is what we expect.",0,0,0,0.9666824340820312,0.989108681678772,0.9792137742042542,0.0,accept,unanimous_agreement
336194567,3765,"i just realized that controlled shutdowns will not improve at all with the current pr because controlled shutdown still calls the state machines one partition at a time, so there won't be any zk pipelining. we can just do a bunch of filters upfront in controlledshutdown.docontrolledshutdown that categorize partitions into the actions to take and then take the actions on the collections we've gathered. i'll try to update the pr within the next few hours.",0,0,0,0.9658695459365844,0.977477490901947,0.9717742800712584,0.0,accept,unanimous_agreement
336318966,3765,"i just ran an experiment measuring controlled shutdown time before and after this pr. the environment: * 5 broker kafka cluster with brokers on different racks * 5 node zookeeper ensemble with nodes on different racks * 25,000 single-partition topics with rf = 2 the replicas were made with the following python script: [code block] special configs: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controlled shutdown time: ~6.5 minutes kafka-5642 average controlled shutdown time: ~3 seconds",0,0,0,0.9539167881011964,0.991016149520874,0.9831175804138184,0.0,accept,unanimous_agreement
336486274,3765,can we update the pr description to include a summary of the changes done in this pr?,0,0,0,0.9877368211746216,0.9924916625022888,0.995636522769928,0.0,accept,unanimous_agreement
336520938,3765,"today, zookeeperclient and zkclient couples the data/child change handlers with the actual watcher registration, which means handler registrations actually send out a request to the zookeeper ensemble to do the actual watcher registration. in `kafkacontroller.oncontrollerfailover`, we register partition modification handlers (thereby registering watchers) and additionally lookup the partition assignments for every topic in the cluster. in an offline discussion, and i realized that we can shave a bit of time off failover if we merge these two operations. there are two ways we can do this: 1. change zookeeperclient to register data change watchers with getdatarequests instead of existrequests. this means the handler and watcher registration will also return the znode data. 2. decouple handler registration from watcher registration in zookeeperclient. handler registrations and unregistrations would be a purely local operation. watcher registrations would be delegated to the user in a following lookup. option 1 made us realize a bug in the existing pr: we don't check the return codes of watcher registrations and attempt retries on connectionloss. option 2 would just reuse the retry logic already in kafkacontrollerzkutils. i prefer option 2, especially since it makes the zookeeperclient registration/unregistration apis more consistent in that they would both now be purely in-memory operations.",0,0,0,0.9591171145439148,0.9814605116844176,0.9674969911575316,0.0,accept,unanimous_agreement
336531088,3765,": yes, i agree that option 2 would be better. it would be useful to add a comment to describe the semantic of zookeeperclient.registerznodechangehandler(). if we do this, we will want to do this consistently with registerznodechangehandler(). also, could you remove wip from the title of the pr?",0,0,0,0.9421789050102234,0.9780091047286988,0.9692524671554564,0.0,accept,unanimous_agreement
336737463,3765,"something that was not factored into the above two experiments was logging inefficiencies. one major issue with kafka today is its use of log4j synchronous logging instead of log4j2's asynchronous logging. this heavily distorts the experiment results. if we want to more purely measure the performance change from this patch, i think it'll make sense to rerun the experiments with controller logs set to something like error and above or just disable it entirely.",0,0,0,0.6455255150794983,0.912320077419281,0.9702954888343812,0.0,accept,unanimous_agreement
336750646,3765,"in an offline discussion, suggested i look into determining whether the long controlled shutdown time was actually from the logger writing to disk or simply from creating the log message itself, in which case async logging wouldn't have helped. to test this, i ran the same controlled shutdown experiment as above for trunk but with: `log4j.appender.statechangeappender=org.apache.log4j.varia.nullappender` `log4j.appender.controllerappender=org.apache.log4j.varia.nullappender` nullappender simply doesn't output the message to a device. controlled shutdown still took 6+ minutes. i later reverted the two appenders back to `org.apache.log4j.dailyrollingfileappender` but this time set the controller and state change log levels to fatal so that they effectively never log. controlled shutdown finished in 11 seconds. this proves that the vast majority of time was getting spent in log message creation.",0,0,0,0.979066014289856,0.9889238476753236,0.9887682795524596,0.0,accept,unanimous_agreement
336792142,3765,"alright i reran the controller failover and controlled shutdown experiments but with some logging tweaks to more purely compare trunk's controller using zkclient with kafka-5642's controller using zookeeperclient. the logging tweaks are the following: - the state machine loggers set to error - the state change logger set to debug - the rest of the controller logger set to info the full log4j.properties is here: [code block] **controller failover** the environment: - 5 broker kafka cluster with brokers on different racks - 5 node zookeeper ensemble with nodes on different racks - 100,000 single-partition single-replica topics trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controller failover time: ~63 seconds kafka-5642 average controller failover time: ~17 seconds **controlled shutdown** the environment: - 5 broker kafka cluster with brokers on different racks - 5 node zookeeper ensemble with nodes on different racks - 25,000 single-partition topics with rf = 2 the replicas were made with the following python script: [code block] special configs: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controlled shutdown time: ~22 seconds kafka-5642 average controlled shutdown time: ~3 seconds",0,0,0,0.9734974503517152,0.9918659329414368,0.9899773597717284,0.0,accept,unanimous_agreement
336801412,3765,"i went back and looked into why logging resulted in distorted results in the controlled shutdown experiment for trunk: 6.5 minutes with logging vs 22 seconds without logging. it's due to this line in partitionstatemachine.electleaderforpartition: [code block] i reran the experiment with the original log levels (controller and state change loggers at trace, no special state machine loggers) but removed the above debug log statement. this brings the controlled shutdown times back down to below 20 seconds: [code block] the reason why this one log line is so problematic is because for each partition undergoing election, it logs every partition state in the cluster. so you end up getting o(n^2) log behavior where n is the total number of partitions in the cluster. rather than log every partition's state per partition undergoing election, it would be sufficient to just log that single partition's state: [code block]",0,0,0,0.8799333572387695,0.8935049772262573,0.9817349314689636,0.0,accept,unanimous_agreement
337302275,3765,"by the way, i reran the controlled shutdown test with the latest changes that adds state change logs for successful state machine transitions and still got around 3 seconds for controlled shutdown: [code block]",0,0,0,0.9840562343597412,0.975951075553894,0.9924877882003784,0.0,accept,unanimous_agreement
337373454,3765,: thanks for the latest patch. lgtm. running all system tests now.,1,1,1,0.9607297778129578,0.9858184456825256,0.9768933057785034,1.0,accept,unanimous_agreement
337373595,3765,rebased against trunk and squashed all commits.,0,0,0,0.974124312400818,0.9702270030975342,0.9890866279602052,0.0,accept,unanimous_agreement
337513329,3765,"looks like the system tests passed: [a link] i also went ahead and ran a modified version of the controller failover test with the same overall partition counts, but with more partitions-per-topic and fewer topics. the environment: - 5 broker kafka cluster with brokers on different racks - 5 node zookeeper ensemble with nodes on different racks - 2,000 topics each with 50 partitions and rf = 1 trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controller failover time: ~28 seconds kafka-5642 average controller failover time: ~14 seconds",0,0,0,0.9659531712532043,0.989495575428009,0.96519672870636,0.0,accept,unanimous_agreement
337644430,3765,: thanks a lot for working on this patiently! lgtm,1,1,1,0.9912970066070556,0.9917444586753844,0.996483564376831,1.0,accept,unanimous_agreement
341092491,3765,"hi , great work! any chance you could give us some insight into the machines you used for the tests? how were they spec'd and the order of magnitude network latency?",1,1,1,0.9928286671638488,0.9951297044754028,0.99638569355011,1.0,accept,unanimous_agreement
1428304503,13240,"hello david (), still working on this but opening a draft if you wish to start reviewing at your convenience.",0,0,0,0.9579190015792848,0.5390335917472839,0.697869062423706,0.0,accept,unanimous_agreement
1428396257,13240,thanks. i will take a look later this week.,1,1,1,0.8715918660163879,0.8678932189941406,0.913498044013977,1.0,accept,unanimous_agreement
1429942230,13240,"thanks for the review, david. i am working on adding unit tests for `offsetcommitresponse` and the server-side handling of the request/response, and fix the bugs you have identified.",1,1,1,0.9683372974395752,0.9559427499771118,0.960873007774353,1.0,accept,unanimous_agreement
1437297790,13240,"hello david, i updated the pr to take into account your comments and have been adding tests.",0,1,0,0.9354879260063172,0.5168830156326294,0.8937339186668396,0.0,accept,majority_agreement
1442380719,13240,"many thanks david for the review. i updated the pr to cover all the points you raised. i still have more unit tests to add for the broker-side code paths, need to check if any further change/test is required on the admin client side. will update the pr with those. thanks!",1,1,1,0.9875040650367736,0.9927803874015808,0.9935755133628844,1.0,accept,unanimous_agreement
1443590758,13240,- added a test for `invalid_request` in case of definition of topic id and name for a topic in the `offsetcommit` request. - added a test exercising a valid `offsetcommit` request across all schema version. we can see that the topic id and name is propagated to the group coordinator in all cases.,0,0,0,0.9865151643753052,0.9943207502365112,0.9883692264556884,0.0,accept,unanimous_agreement
1446539881,13240,"thanks for the update. i will take a look shortly. in the meantime, could you already add a few integration test for the offsetcommitrequest like we have for the offsetfetchrequest in kafka.server.offsetfetchrequesttest? i believe that topic ids are actually lost when they are passed to the group coordinator. hence, i request with topic ids will very likely return a response with topic names instead of topic ids. the issue is that we don't catch those kind of issues with the existing tests because the logic in the consumer supports both ways.",1,1,1,0.6983554363250732,0.5576281547546387,0.9447174072265624,1.0,accept,unanimous_agreement
1447864999,13240,thanks for the review david. updating the pr to enforce the exclusive use of topic ids from version 9 and adding the integration test you mentioned. thanks for the guidance!,1,1,1,0.9849240779876708,0.977566421031952,0.989953875541687,1.0,accept,unanimous_agreement
1449920535,13240,"hello david, thanks for the fast review. apologies for being slow, i hadn't finished the previous revision. will include your comments. working on it right now. thanks!",1,1,1,0.9808088541030884,0.9952924251556396,0.9950633645057678,1.0,accept,unanimous_agreement
1449935049,13240,no worries. you are not slow. i noticed a few new commits so i had a quick look at them.,1,1,1,0.9033178687095642,0.969931960105896,0.9767133593559264,1.0,accept,unanimous_agreement
1449936916,13240,"sorry, used the wrong button...",-1,-1,-1,0.9885622262954712,0.9916070699691772,0.991915225982666,-1.0,accept,unanimous_agreement
1452495333,13240,"thanks david for the review, have a few more tests to add but this should be eligible to another pass.",1,1,1,0.9319648146629332,0.9691651463508606,0.934965431690216,1.0,accept,unanimous_agreement
1453452130,13240,the integration test `mirrorconnectorsintegrationbasetest` is failing due to unknown topic id detected when altering committed offsets with the admin client. this is because the `offsetcommitrequest` generated by the admin client is using version 9 but should be 8 as discussed. this should have been captured by the unit tests on the admin client. this exhibits a gap in the test coverage of this pr.,0,0,0,0.9666054248809814,0.9928554892539978,0.9791406989097596,0.0,accept,unanimous_agreement
1453619279,13240,"used v8 for alterconsumergroupoffsets in the admin client and added corresponding unit and integration tests. `mirrorconnectorsintegrationbasetest` is now successful. also just realized i need to update the `authorizerintegrationtest`. the reason is the tests are committing offsets from a consumer without having the topic subscribed to, and without the topic id in the client metadata cache.",0,0,0,0.9728028178215028,0.988554060459137,0.9465871453285216,0.0,accept,unanimous_agreement
1454676488,13240,"hello david, found a case where the use of offsetcommit requests version 9 from the consumer resulted in an error in the acl authorization tests. the reason is that when the commit offsets are performed by the tests to exercise acls, the topic ids are not yet present in the consumer metadata cache, in this case because the consumer hasn't subscribed to the topics or be assigned any of their partitions. as a result, the offsetcommitrequest version 9 was sent with zero topic ids. in order to avoid this, we could: 1. enforce a metadata update for any topic in the offsets to commit, using the transient topics list exposed by the `consumermetadata` and used for instance by the `offsetfetcher` to fetch offsets for out-of-band topic-partitions which aren't part of any subscription. this approach however adds complexity to the offset commit implementation in the consumer and only address this specific use case. there could be other cases where metadata hasn't converged yet and the topic id would not be available to the consumer. 2. adds a condition on the support of topic ids when constructing the `offsetcommitrequest`. this is the approach used when constructing the `fetch` request. the advantage of the approach is that it provides the invariant that any offsetcommitrequest version >= 9 have valid (non-zero) topic ids in it. one downside is that if a bug in the consumer makes a topic id unavailable, the consumer will keep using version <= 8 permanently and silently, while we would want to know about it as implementors to address any potential gap. the second approach is currently implemented in the pr. happy to discuss more about it.",0,0,0,0.9790582060813904,0.9879134297370912,0.9679002165794371,0.0,accept,unanimous_agreement
1454803355,13240,i think that we have to do 2) anyway because topic ids may not be enabled on the server side.,0,0,0,0.9777047634124756,0.984793484210968,0.9811576008796692,0.0,accept,unanimous_agreement
1455667030,13240,took a look at the authorizer tests (`authorizerintegrationtests`). it seems that authorization with topic and group `read` permissions and unknown topic name is not currently being tested. we could add this use case and extend it for topic ids in a separate pr? (ref: [a link],0,0,0,0.9866191744804382,0.9947500824928284,0.992077648639679,0.0,accept,unanimous_agreement
1459774076,13240,"many thanks, david, for the review. working on fixing the pr now. i will send the corrective commits by eod. thanks.",1,1,1,0.9757204055786132,0.9919153451919556,0.9851391911506652,1.0,accept,unanimous_agreement
1460209520,13240,"hi david, thanks for the review. i addressed all your comments and updated the pr. ~i am just adding a test to add coverage on addition of topic ids in the response generated by the coordinator.~ _edit: added said test._",1,1,1,0.9378840923309326,0.9888490438461304,0.981605350971222,1.0,accept,unanimous_agreement
1469825143,13240,i just merge [a link] we can update this pr now.,0,0,0,0.987893521785736,0.9517183303833008,0.995676338672638,0.0,accept,unanimous_agreement
1469956438,13240,"many thanks david. i will try to get to this in the next couple of days. apologies for the delay, i wish i could get to this sooner.",1,1,1,0.9512516856193542,0.904201865196228,0.933907985687256,1.0,accept,unanimous_agreement
1485037330,13240,"hello david (), this pr has been updated and is ready for review. thanks!",1,1,1,0.9762955904006958,0.9914939403533936,0.9908202290534972,1.0,accept,unanimous_agreement
1505120343,13240,there are a few conflicts. could you please rebase the pr? i plan to make another pass on it afterwards.,0,0,0,0.9820745587348938,0.921704649925232,0.978766679763794,0.0,accept,unanimous_agreement
1505154651,13240,"sure, done. thanks!",1,1,1,0.9703675508499146,0.9931859970092772,0.9897446036338806,1.0,accept,unanimous_agreement
1511110416,13240,"hi david, thanks for the review. understand about refactoring, i will try to see if i can revert some of them if possible.",1,1,1,0.9077581167221068,0.9767532348632812,0.9813665151596068,1.0,accept,unanimous_agreement
1553270438,13240,"heya ! i hope i have addressed your comments on all files except `consumercoordinatortest` and the `offsetcommitrequesttest`, could you review everything except those two and confirm whether this is the case? as far as i understand your general concerns with `consumercoordinatortest` and `offsetcommitrequesttest` is that those tests are either not parameterised in a simple way or they are not parameterised at all - am i correct? for the ones which are not parameterised simply i cannot think of an easier approach - the setup is just long-winded, but the main idea behind the arguments is [a link] i guess we can split them into separate tests were we vary just one of the arguments rather than all of them if that's what you mean? the ones which are not parameterised at all i believe are not parameterised because the same functions are used for tests which only test behaviour in versions >= 9. i am happy to implement any suggestions you might have to improve on what's already there.",1,0,1,0.927284836769104,0.4085156917572021,0.9504090547561646,1.0,accept,majority_agreement
1553290938,13240,"thanks christo () for your help on the pr, i will take a look at the changes tomorrow. thanks!",1,1,1,0.9868326187133788,0.9895269870758056,0.9936042428016664,1.0,accept,unanimous_agreement
1562458362,13240,"hi david (), thanks for the review and apologies for the delayed reply. thanks to christo's help, i believe most of your comments have been addressed. i have one question regarding the behaviour of the offset commit consumer api that you identified [a link]. thanks!",1,1,1,0.9838863611221312,0.994565486907959,0.993885576725006,1.0,accept,unanimous_agreement
1571989946,13240,"hello david (), i was discussing this with christo today as part of his work on the offsetfetch api. would you like this pr on offsetcommit to be split to make the review easier and reduce risks?",0,0,0,0.8228321075439453,0.5815007090568542,0.9403562545776368,0.0,accept,unanimous_agreement
1741511578,13240,closing this pr for now as the topic id work will be done later. we can re-open it when we resume the work.,0,0,0,0.9836793541908264,0.9938656687736512,0.991280734539032,0.0,accept,unanimous_agreement
260852086,2140,"ping this patch modifies the server implementation to use the client-side `record` objects for all internal processing. as you can see, this was a hefty bit of work, but fortunately most of the transformations are straightforward. the main thing to focus on is the implementation of `logvalidator`, which contains the offset assignment and record validation logic that was previously contained in `bytebuffermessageset`. i've been pretty careful to preserve the optimizations that were present previously (e.g. in-place assignment where possible), but don't take my word for it. one quick note on naming. i've renamed the `records` object and subclasses to `logbuffer`. so `memoryrecords` is now `memorylogbuffer`. the reason for this change was that it felt unintuitive for an instance of `records` to be an `iterable `, with the `logentry` instances being the actual container for the records. a `logbuffer` instead represents a range of the log and provides access to the log entries contained in it. that seemed more intuitive to me, but let me know if you agree or if you have other suggestions.",0,1,0,0.9090301990509032,0.5218955874443054,0.9347435235977172,0.0,accept,majority_agreement
260854848,2140,"slightly related, slightly tangential: is there a specific reason why we put the new broker-specific java classes under clients/ ? i'm talking about stuff like: filerecords leaderandisrrequest / leaderandisrresponse stopreplicarequest / stopreplicaresponse updatemetadatarequest / updatemetadataresponse",0,0,0,0.8938719034194946,0.8227542042732239,0.975955069065094,0.0,accept,unanimous_agreement
260856355,2140,"yeah, i've wondered a bit about that also. i'd be ok moving `filerecords` to the server if people prefer that. i was thinking one potential benefit is that it opens the door to adding persistence to the client, which some users have requested (we have an ancient jira for this, but the use case might not be too compelling). in the end, i decided it wasn't that much code, so having it in clients didn't hurt too much and it kept all record-related stuff close together, which may make it easier to share common pieces.",0,0,0,0.8110840320587158,0.8296586275100708,0.916954517364502,0.0,accept,unanimous_agreement
260979285,2140,", thanks for tackling this. about the naming question, i also found it a bit confusing how we sometimes have an offset and sometimes don't when talking about records. that is, `memoryrecords` includes the offset (and record size in the underlying buffer) for each record while `record` does not. it all becomes clearer when one realises that `memoryrecords` (renamed to `memorylogbuffer` in the pr) actually contains `logentry` instances, each being a pair of `offset` and `record`. one thing to think about is whether this fits with the other `record` classes we have and whether that matters (maybe it doesn't). for example, `consumerrecord` contains the `offset` while `producerrecord` does not. also, it would have been a bit easier to review if the rename had been done in a separate pr, but probably too late for that. :) about having the classes in `clients`, i think that's ok as they are in an internal `common` package.",1,0,1,0.8875225782394409,0.8096935153007507,0.9675544500350952,1.0,accept,majority_agreement
261073297,2140,would be nice to get your feedback also. put on the coffee and lose yourself in code review!,1,1,1,0.9514696002006532,0.9189677834510804,0.9729846119880676,1.0,accept,unanimous_agreement
261693933,2140,"wow, a 5000 line change... i'll take a look this weekend...",1,-1,0,0.6020969748497009,0.6644198298454285,0.6666823625564575,,review,no_majority_disagreement
261860811,2140,"i ran into some other stuff today and didn't finish reading the entire patch. just some thoughts when i was reading the code. i think ""message"" (and ""record"" in the new clients) are a well established concept for kafka. it is indeed a little weird that `records` is an `iterable `, but i felt changing all the `reocrds` to `logbuffer` seems introducing a new concept (btw `filelogbuffer` sounds a little weird given it actually does not have a buffer). i would like to see if we can avoid solving the confusion by adding a new concept. not sure if there was any thinking on changing `logentry` to something like `logrecord` which indicate it's something resides in the log? then `records` would contain a few `logrecord` which contains (`offset` + `record`). we also have a pretty symmetric naming for `producerrecord`, `consumerrecord` and `logrecord` clearly indicating where they are used. we can also consider renaming `records` to `logrecords` to make it clear. i am not sure if `records` counts as a public interface or not, though. i know we have a page stating which packages are public and which are not, but i doubt if people follows that given we have an explicit `internals` package... i'll save my other comments until i go through a full pass of the code in case some of them are not valid at all (i already found some...)",-1,-1,0,0.7779680490493774,0.5966769456863403,0.6085002422332764,-1.0,accept,majority_agreement
261887880,2140,", that's a fair point about the rename and introducing a new concept. i have similar concerns and was wondering how we could make things clearer without that. your suggestion looks promising.",1,1,1,0.8675389289855957,0.9766422510147096,0.9614731669425964,1.0,accept,unanimous_agreement
262029927,2140,does it make sense to separate the renaming from the actual task of this patch?,0,0,0,0.985069990158081,0.9937965869903564,0.9912315011024476,0.0,accept,unanimous_agreement
262033620,2140,"thanks for taking a look. i'm not sure i follow why you consider the renaming a conceptual change. the object works the same as before, but i felt the name fit closer to what the object actually represents, which is a range of bytes from the log. the name `records` to me just suggests a container for `record` objects. the suggestion about `logrecord` makes sense to me. i have actually done something similar in work building off of this patch. at the same time, i would like to preserve a concept of `logentry` as a container for records which sits between `logbuffer` (or `records`) and `logrecord` (or `record`). the basic idea is to treat the shallow messages as log entries, and the deep messages as log records (an uncompressed message is treated as a log entry containing just a single log record). to give a bit more background, we're trying to generalize the concept of a message set so that 1) it uses a separate schema from individual messages, and 2) it's extended to uncompressed data. this allows us to amortize the cost of additional metadata which is invariant for the messages contained in a message set. i'm happy to provide some additional detail if you're interested (there will be a kip on the way some time in the next few weeks). yeah, we can do that if it makes this patch easier to get in. let's see what others think. sigh, any suggestion to reduce lines of code is likely to be popular to all except me.",1,1,1,0.8148824572563171,0.8933326601982117,0.9569987654685974,1.0,accept,unanimous_agreement
262306541,2140,"i ran system tests on the latest patch and everything looks good: [a link] i will probably continue to add some additional test cases, but i'll leave the rest as is pending further review comments.",1,1,1,0.9527892470359802,0.6844563484191895,0.8431157469749451,1.0,accept,unanimous_agreement
262421264,2140,: will also take a look at the patch. just a quick note. could you do some performance test to make sure there is no regression?,0,0,0,0.9518609642982484,0.9420232772827148,0.9709427952766418,0.0,accept,unanimous_agreement
262422183,2140,thanks for taking a look. performance testing is next on my plate after i fill in some of the gaps in test coverage.,1,1,1,0.8596463799476624,0.9322426319122314,0.9679991602897644,1.0,accept,unanimous_agreement
263670626,2140,update: i've begun performance testing. i'm seeing a substantial performance degradation on the consumer side. i'll update this pr when i know more.,0,0,0,0.9434162378311156,0.8974629044532776,0.9465256333351136,0.0,accept,unanimous_agreement
263687962,2140,"i found the cause of the performance regression. when handling a fetch, we must read through the log to find the starting position of a given offset (starting from the position given by the index). to do so, we only need to read the offset and size, but one of my recent commits accidentally changed this behavior to unnecessarily read the full record. i've fixed this in the last commit and now it looks like performance is within 5% of trunk for the producer and consumer. perhaps still on the slower side though, so i'll continue investigating.",0,0,0,0.9036523103713988,0.959902286529541,0.8600564002990723,0.0,accept,unanimous_agreement
264081910,2140,"i really appreciate the thorough review. i've addressed the easier items and left a few replies. i'll get to the rest tomorrow. by the way, in the near future, i'd like to squash commits to make rebasing a bit easier. it hasn't been too much of a problem yet, but it will get harder with more iterations.",1,1,1,0.9738858938217164,0.9933792352676392,0.9898862838745116,1.0,accept,unanimous_agreement
264770009,2140,"about the naming of `records` to `logbuffer`, i share the same concern with and . my proposal would be to rename `logentry` to `recordentry` or simply `recordandoffset` (seems more scala-ish), and to me it is ok to have `records.iterator()` return `iterator `. as for `consumerrecord` and `producerrecord`, it would be best if them both contain a `record` field, and then `consumerrecord` to contain a separate `offset` field, but since it is public apis we have to leave it as is, which is not too bad to me. also, i'd like to suggest we separate the renaming out of this pr for the ease of reviewing, if it is still possible to revert it back.",0,0,0,0.9694795608520508,0.973524808883667,0.9824556112289428,0.0,accept,unanimous_agreement
264923518,2140,"`recordentry` would work for me. keep in mind that if kip-98 is approved, it will do more than just track the offset, so i'd rather not use something specific like `recordandoffset`. since my `logbuffer` suggestion is not too popular, i'll go ahead and revert that change. then the hierarchy will be `records` -> `recordentry` -> `record`. does that sound reasonable?",0,0,0,0.9635754823684692,0.9851660132408142,0.9856778979301452,0.0,accept,unanimous_agreement
264983825,2140,i've gone ahead and squashed commits. you can still find the old commit history here: [a link],0,0,0,0.97975492477417,0.9632831811904908,0.993929386138916,0.0,accept,unanimous_agreement
265818436,2140,fyi: here's a system test run from last night: [a link] there was one failure. i'll investigate and report back what i find.,0,0,0,0.9580448269844056,0.9615209698677064,0.9871565699577332,0.0,accept,unanimous_agreement
266538764,2140,"latest round of comments addressed. please take a look. on the question of `iterator` vs `iterable`, i'm pretty open. i used the former for consistency with the old code, but i agree it would be nice to be able to use the ""foreach"" syntax.",1,1,1,0.7266502976417542,0.7793874740600586,0.8038420677185059,1.0,accept,unanimous_agreement
266684166,2140,"thanks for the reviews. i did some testing this evening. i thought i was seeing some performance difference initially in the producer, but it seems within the variance of the test runs. if i were guessing from the results, i'd say the non-compressed path is a tad slower while the compressed path might be a tad faster, but don't put much weight behind either conclusion. in any case, the results seem close enough that i'd recommend merging now. note that i did add one commit to address a couple minor cleanups and tighten up the iteration code a little.",1,1,1,0.7912644743919373,0.9712774753570556,0.9609140753746032,1.0,accept,unanimous_agreement
384162821,4830,the patch lgtm. would you have time to review this patch? thanks much!,1,1,1,0.9847374558448792,0.9856555461883544,0.9871282577514648,1.0,accept,unanimous_agreement
384163802,4830,"thank you very much for the detailed review, . i talked to both and and they are not available for review at the moment. i've asked if she can review.",1,1,1,0.969735622406006,0.9676765203475952,0.9886009097099304,1.0,accept,unanimous_agreement
386722748,4830,"the lastest commit includes the following fixes: 1. bump up all request and response protocol versions 2. add a method called shouldclientthrottling() to abstractresponse to tell the client whether or not it should throttle based on the received response version. 3. fix networkclient to do throttling based on the response version using the new method. 4. add the throttle_time_ms field to the following cluster actions that may be throttled: leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse, saslhandshakeresponse 5. update kafkaapis to pass throttletimems to above responses , can you take a look? thank you.",0,0,0,0.98299902677536,0.9952725768089294,0.9849646687507628,0.0,accept,unanimous_agreement
386752308,4830,"hi jon, thanks for the updates. i will take a look at the pr early next week. we were deliberately avoiding throttling leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse and saslhandshakeresponse. and i don't think we discussed throttling these as part of the kip. can you explain why the change was required? if we are changing request format, it ought to be added to the discuss thread and the kip. on fri, may 4, 2018 at 9:23 pm, jon lee wrote:",1,1,1,0.9101489186286926,0.8165463805198669,0.9811304211616516,1.0,accept,unanimous_agreement
386759605,4830,"hi rajini, thanks for the comment. the reason why i added throttle_time_ms to leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse and saslhandshakeresponse was that these responses are actually throttled on errors (for request quotas). and when they are throttled, i want throttle time to be passed back so that client can refrain from sending more requests to the broker until the throttle time is over. for non-error cases, they won't be throttled, so there are no behavior changes there (except that the response will have zero throttle time). i'll mention this change to the discussion thread. meanwhile, please go ahead and review the changes. the response format changes are well contained and shouldn't block you from reviewing other changes. if we decide to take it back, it shouldn't be much work. thanks, jon ________________________________ from: rajini sivaram sent: friday, may 4, 2018 3:37:25 pm to: apache/kafka cc: jon lee; mention subject: re: [apache/kafka] kafka-6028: improve the quota throttle communication (kip-219) (#4830) hi jon, thanks for the updates. i will take a look at the pr early next week. we were deliberately avoiding throttling leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse and saslhandshakeresponse. and i don't think we discussed throttling these as part of the kip. can you explain why the change was required? if we are changing request format, it ought to be added to the discuss thread and the kip. on fri, may 4, 2018 at 9:23 pm, jon lee wrote: — you are receiving this because you were mentioned. reply to this email directly, view it on github , or mute the thread .",1,0,1,0.784269392490387,0.6685647964477539,0.9549439549446106,1.0,accept,majority_agreement
389022296,4830,"thanks for the review, . i added some questions to your first comment. please let me know what you think.",1,1,1,0.8956090211868286,0.9565091133117676,0.984508752822876,1.0,accept,unanimous_agreement
389401521,4830,"btw, let me know if you want me to squash multiple commits into one. i didn't do that to keep the entire history, but i think i'll need to do that anyway at some point before submission.",0,0,0,0.9191659092903136,0.9707777500152588,0.9802128672599792,0.0,accept,unanimous_agreement
389444225,4830,"thanks for the updates. it is looking good. you don't need to squash the commits, they get squashed when the pr is merged. so the pr can keep the commits to maintain history. since it is a large pr, it will be good to do one full review before committing it. i was expecting that you would do the final review and commit this pr. let me know if that is not the case. thanks.",1,1,1,0.9814428687095642,0.9924706220626832,0.9925498962402344,1.0,accept,unanimous_agreement
389447800,4830,"thanks for the review ! yes, i will do one full review before committing this.",1,1,1,0.9884575009346008,0.9411775469779968,0.986730933189392,1.0,accept,unanimous_agreement
390454844,4830,is there a public jenkins service that we can use to run system tests for this patch?,0,0,0,0.987667441368103,0.9927916526794434,0.9947332143783568,0.0,accept,unanimous_agreement
390694484,4830,"thanks for running the tests. for some reason, however, i cannot access the link. i got an 404 error, saying ""problem accessing /job/system-test-kafka-branch-builder/1754. reason: not found"". do i need a permission or anything else to access it? also, what are these system tests and where can i find them? i may need to change these tests as well since the throttling behavior has now changed.",1,0,1,0.7223365306854248,0.6592029929161072,0.894942581653595,1.0,accept,majority_agreement
390722724,4830,that is not a public url. here are the failed tests: [code block],0,0,0,0.9702911972999572,0.9869958758354188,0.9825594425201416,0.0,accept,unanimous_agreement
390754366,4830,thanks much for helping running the test!,1,1,1,0.9827086329460144,0.8469580411911011,0.8584735989570618,1.0,accept,unanimous_agreement
390902167,4830,"i found a bug in selector.java, which caused the test failures. for socketserver, selector.unmute() unmutes the channel only if the mute state is muted. however, it removes the channel from explicitlymutedchannels regardless of the outcome of unmute. this allows a new client request to be received by socketserver when throttling for the previous request is still in progress and thus breaks the state machine. i ran the system tests locally after fixing this and all previously failed quota tests succeeded. this was not really caught by the existing quota integration tests because they produce/consume till throttled and stop there. i also noticed that passing invalid_session_id in an empty fetch response (when throttled) will close the incremental fetch session. i added a special session id to keep the session open. please review and also start another round of system tests. thank you.",0,0,0,0.9167899489402772,0.9465510249137878,0.9570713639259338,0.0,accept,unanimous_agreement
393244326,4830,this pr has been merged in the trunk with [a link],0,0,0,0.9879812598228456,0.972029983997345,0.9962463974952698,0.0,accept,unanimous_agreement
670693346,9103,"i think it would be good to split this pr up a little bit. it seems like we could have a split like this: pr 1. `add flag to the requestcontext` and `add initial principal name` pr 2. authorization changes for alterconfigs / incrementalalterconfigs, forwarding if required, ibp check, bump rpc versions of alterconfigs / incrementalalterconfigs pr 3. adminclient changes in behavior based on versions of alterconfigs / incrementalalterconfigs, alterconfigsutil, etc.",0,0,0,0.8349421620368958,0.9808549284934998,0.9055256843566896,0.0,accept,unanimous_agreement
696225206,9103,"hi , thanks for the pr! it looks good. i like the idea behind `forwardrequesthandler`. since this class doesn't have any internal state, i wonder if it would be more scala-like to just have a function which just takes some callbacks as arguments. can we get rid of the need for the `customizedauthorizationerror` callback by having `resourcesplitbyauthorization` return a `map[rk, apierror]` instead of `map[rk, rv]`? when would rv be needed for keys where authorization failed?",1,1,1,0.9898701310157776,0.99456524848938,0.9957972764968872,1.0,accept,unanimous_agreement
696281629,9103,sounds good to me to remove the customized error helper.,1,0,0,0.8126707077026367,0.6392214298248291,0.5531272292137146,0.0,accept,majority_agreement
713244123,9103,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
719681493,9103,"one more thing to verify. when the metadata quorum feature flag is not defined, i think we should not expose the envelope api in the `apiversion` request.",0,0,0,0.9859886169433594,0.989014208316803,0.9925582408905028,0.0,accept,unanimous_agreement
722005317,9103,"mvn failure is not related, merging",0,0,0,0.9550952315330504,0.964388072490692,0.980439841747284,0.0,accept,unanimous_agreement
133280679,132,the failed test in the last pr build passed on my laptop locally. it seems to be flaky as it also failed on an earlier build without my changes.,-1,-1,0,0.7687247395515442,0.7871993184089661,0.9235954880714417,-1.0,accept,majority_agreement
170421487,132,"one overall general comment on the implementation is that the brokers properties themselves could cary this information making it so the topic creator doesn't have to know this. the fact is that still a lot of humans run the topic command but in many cases it is some software system operationally doing it. in either case if the broker had a property rack=0 or whatever it could then just be the way you have the topic distribute that information it should already be able to gather. granted, this implementation saves a lot of having to store it in zookeeper so rationally speaking this is better than putting any code in to that. sorry if i missed the entire discussion thread on this just seeing it for first time. i like it, would love to see this get into trunk and start to be used and also in the next release. nice work so far!",-1,-1,-1,0.9906783103942872,0.9614611268043518,0.9234356880187988,-1.0,accept,unanimous_agreement
170429303,132,"please see kip-36 for the latest proposal. ([a link] the biggest difference is that the rack information is added as a broker meta data in zookeeper. consequently the inter-broker (updatemetadatarequest) and client to broker meta data query protocol (topicmetadataresponse) will be changed to have rack information. once the kip is accepted, i will update this pr to incorporate these new ideas.",0,0,0,0.9692215919494628,0.9779723882675172,0.981685221195221,0.0,accept,unanimous_agreement
173002403,132,"wouldn't it be nice to have metric per topic partition, in how many different racks do isrs live?",0,0,0,0.9670528173446656,0.9885735511779784,0.9797862768173218,0.0,accept,unanimous_agreement
190237447,132,", can you please fix the merge conflicts?",0,0,0,0.9877668619155884,0.9898570775985718,0.9921332597732544,0.0,accept,unanimous_agreement
190387743,132,done with resolving conflicts. would appreciate if this can be reviewed and merged with a higher priority as it touches multiple areas and easy to get conflicts if sitting there for a bit longer.,1,0,0,0.4992486834526062,0.5930300354957581,0.6778489351272583,0.0,accept,majority_agreement
190404703,132,"i was looking for the changes to topicmetadatarequest, but didn't see them. were you planning to do them here?",0,0,0,0.9658634662628174,0.9185644388198853,0.9899333119392396,0.0,accept,unanimous_agreement
190409503,132,i need to make changes to topicmetadata response too for kip-4. we also need to migrate to the new topicmetadata request/responses on the server side [a link]. we should probably migrate before making changes. otherwise we need to make the changes in both the java and scala classes.,0,0,0,0.9859375357627868,0.9929719567298888,0.9908884763717652,0.0,accept,unanimous_agreement
190410867,132,makes sense to me. i can pick up 2073 if you haven't already started.,0,0,0,0.9590222835540771,0.943046808242798,0.9792054295539856,0.0,accept,unanimous_agreement
190413309,132,feel free to pick it up. i have some intermediate changes i can share with you later tonight. the hardest part about migrating was metadatacache. you more or less need to re-implement it because its tied very heavily to the old requests/responses. after that i had some test failures that i haven't had a chance to dig into. i was thinking adding some additional tests may help flush out the issues more clearly too.,0,1,1,0.7577909827232361,0.5193400979042053,0.6723057627677917,1.0,accept,majority_agreement
190415650,132,i am not going to include changes to topicmetadatarequest/response in this pr which was indicated in the kip discussion thread. you are free to do it in later on after this is merged.,0,0,0,0.9820322394371032,0.9834047555923462,0.989615559577942,0.0,accept,unanimous_agreement
190418313,132,have you opened another jira for the topic metadata changes? we should probably make sure we don't lose track of them.,0,0,0,0.9831421375274658,0.9847040176391602,0.9914162755012512,0.0,accept,unanimous_agreement
190422237,132,i can include them in my changes to the response for kip-4. i will open a jira to track that. it would be nice to make all the changes at once anyway. i will ask for your feedback when posting to proposed changes.,0,0,0,0.8467605113983154,0.7423939108848572,0.8704010248184204,0.0,accept,unanimous_agreement
190431869,132,i have created [a link] to track updating the metadata response. i should have a preliminary patch this week.,0,0,0,0.9777326583862304,0.9804095029830932,0.9908300638198853,0.0,accept,unanimous_agreement
190547078,132,flaky test? passed on my laptop and on jenkins before the last commit. test result (2 failures / +1) kafka.api.sslconsumertest.testautocommitonrebalance kafka.network.socketservertest.testsocketscloseonshutdown,0,-1,0,0.9556432962417604,0.7395493388175964,0.8266588449478149,0.0,accept,majority_agreement
190795970,132,thanks for the work on this. i did a quick review. will have to come back for a more in depth look.,1,1,1,0.9350736141204834,0.9821980595588684,0.981205940246582,1.0,accept,unanimous_agreement
191917048,132,"thanks for the patch. now that we are changing the broker registration in zk, could we update the upgrade doc to warn users that they need to upgrade from 0.9.0.1 (instead of 0.9.0.0) to 0.10.0? otherwise, the old scala consumer in 0.9.0.0 will break once the broker is upgraded to 0.10.0.",1,1,1,0.95875483751297,0.8227766156196594,0.9604799151420592,1.0,accept,unanimous_agreement
192326728,132,"thanks for the pr . i did an initial pass and left a few comments/questions and i submitted a pr with some fixes and improvements: [a link] please review and integrate in any way you please (merge, cherry-pick, manually, etc.). i will take a second and final pass by the end of monday.",1,1,1,0.9450081586837769,0.9539529085159302,0.9653459191322328,1.0,accept,unanimous_agreement
193290563,132,", thanks for addressing the review comments and merging my pr. as promised, i completed my review today and i left one comment and created another pr with more code style improvements: [a link] i think we're pretty close (but we'll be sure once jun takes another look). as far as i can see, outside of the pr i filed, the following is potentially outstanding: - [ ] a couple of deprecated constructors in `updatemetadatarequest.java` are only used in tests. i'd like to to confirm that we actually need them. - [x] i think we need to tweak the adminutils.assignreplicastobrokers comment a little to integrate it better with what we had there previously. - [ ] jun asked if some tests are actually needed or if they can be simplified. explained why they are needed, so if jun is happy with the explanation, then this is resolved. - [x] finally, the only non-minor change (although mechanical) is whether we should use a `brokermetadata` class instead of broker ids and a separate `rackinfo` map (suggested by jun). allen wasn't sure if this was better so it's still being discussed.",1,1,1,0.9397677183151244,0.6679511070251465,0.9743825197219848,1.0,accept,unanimous_agreement
193465023,132,a couple more things that we need: - [x] update `upgrade.html` to mention that old clients need to upgrade to 0.9.0.1 before the brokers are upgraded to 0.10.0.x. - [x] system tests must pass.,0,0,0,0.9852412343025208,0.9938241243362428,0.9947932362556458,0.0,accept,unanimous_agreement
193921244,132,can you please help look into test failures? it seems that the failures occur after merging your pull request #2.,0,0,0,0.9892472624778748,0.9931423664093018,0.9932534098625184,0.0,accept,unanimous_agreement
193945557,132,investigating now.,0,0,0,0.9556291103363036,0.9679120182991028,0.9790982007980348,0.0,accept,unanimous_agreement
193978242,132,"sorry about that, the following should fix it: [a link] . looks like i managed to push code to my pr that was slightly different to what i had tested.",-1,-1,-1,0.9883884787559508,0.9930260181427002,0.9869415163993835,-1.0,accept,unanimous_agreement
194106914,132,": could you also run the system tests with your patch? if needed, can help you run your branch in confluent's jenkins.",0,0,0,0.9570478796958924,0.9924812316894532,0.9928566217422484,0.0,accept,unanimous_agreement
194129518,132,"i actually started a system tests build yesterday: [a link] there are 7 failures, but some are unrelated. i am going to analyse it in more detail today.",0,0,0,0.9532352089881896,0.9409931898117064,0.9692363739013672,0.0,accept,unanimous_agreement
194365328,132,"with regards to system failures, they are: [code block] i think these were all (transiently) failing in trunk based on the last time trunk was merged to this branch. , can you please merge from trunk again? the streams tests have since been disabled and `test_upgrade` has been fixed. it's particularly important that we know that it passes even after the changes in this branch. i will re-run the system tests once you do this.",0,0,0,0.9519488215446472,0.988732099533081,0.9842895865440368,0.0,accept,unanimous_agreement
194432464,132,merged with trunk and run systemtest task on my laptop. no test failures.,0,0,0,0.978754460811615,0.985186755657196,0.9875079989433287,0.0,accept,unanimous_agreement
194576557,132,"thanks , i started another system tests run: [a link] are you sure you ran the system tests? setting them up is a bit involved: [a link]",1,1,1,0.8174627423286438,0.8669791221618652,0.9234657883644104,1.0,accept,unanimous_agreement
194595633,132,i got it wrong. all i did is `gradlew systemtest`.,-1,0,-1,0.6086333990097046,0.5201993584632874,0.6547091007232666,-1.0,accept,majority_agreement
194641381,132,"to settle the `brokermetadata` versus `brokerlist` and `rackinfo` question, i made the change in the following commit so people can check how it could look: [a link] let me know what you think. is it better, similar or worse?",0,0,0,0.978749692440033,0.964812994003296,0.9677984714508056,0.0,accept,unanimous_agreement
194766656,132,"there were 4 failures in the last system tests run, none of them seem to be specific to this branch: [code block]",0,0,0,0.9726043939590454,0.992730677127838,0.991266667842865,0.0,accept,unanimous_agreement
194975802,132,your approach looks good to me. it has the advantage that the data structure is clear and allows future addition if more metadata is required to do replica assignment. a further variant would be making `rackawaremode` an argument to `adminutils.assignreplicatobrokers`. but i like the way it is now because it keeps `adminutils.assignreplicatobrokers` simple and free of user input and context.,1,1,1,0.9723872542381288,0.9709975719451904,0.9506558179855348,1.0,accept,unanimous_agreement
195082363,132,"ok, great. i cleaned up the commit a little and created a pr: [a link] i will let you decide whether you want to merge it or if you would prefer to wait for 's opinion.",1,1,0,0.97511088848114,0.9078860878944396,0.5329395532608032,1.0,accept,majority_agreement
195110017,132,i will wait for 's comments on this.,0,0,0,0.974614918231964,0.983120322227478,0.952373206615448,0.0,accept,unanimous_agreement
195245185,132,i ran the system tests on a branch with [a link] and master merged into it and there are no new failures when compared to trunk: [a link],0,0,0,0.9858411550521852,0.987196922302246,0.9908798933029176,0.0,accept,unanimous_agreement
195570860,132,": yes, i agree that using brokermetadata in 's patch is better.",0,0,0,0.932558298110962,0.978281021118164,0.9772676825523376,0.0,accept,unanimous_agreement
195573879,132,"also, some conflicts need to be fixed with trunk. , let me know if you'd like me to merge master into [a link] in order to fix the conflicts.",0,0,0,0.9872204065322876,0.9878018498420716,0.9935837388038636,0.0,accept,unanimous_agreement
195574433,132,"yes, please merge with the trunk in your pull request. thanks.",1,1,1,0.9158368706703186,0.9181023240089417,0.7945923209190369,1.0,accept,unanimous_agreement
195596529,132,merged master into [a link] fixed conflicts and verified that tests pass.,0,0,0,0.9876109957695008,0.9919390082359314,0.9914404153823853,0.0,accept,unanimous_agreement
195605604,132,"allen merged my pr, so this is ready for review.",0,0,0,0.9423428773880004,0.936964750289917,0.9932492971420288,0.0,accept,unanimous_agreement
195644348,132,": for your previous question on the json version for zk registration, my preference is still to do the version change now. this way, our hands are not tied for potential future changes and it's also easier to document this. as for compatibility, most people will probably be on 0.9.0.1 before they upgrade to 0.10.0. so the impact should be limited.",0,0,0,0.9672393798828124,0.98015159368515,0.9124611020088196,0.0,accept,unanimous_agreement
196364260,132,thanks for the confirmation. i understand json version will change.,1,1,1,0.901785969734192,0.6868463158607483,0.9541590809822084,1.0,accept,unanimous_agreement
196844027,132,thanks for the patch. lgtm. could you rebase?,1,1,1,0.901540756225586,0.9694357514381408,0.9293829798698424,1.0,accept,unanimous_agreement
196845140,132,", the following merges master into your branch: [a link]",0,0,0,0.9890268445014954,0.9886493682861328,0.9940483570098876,0.0,accept,unanimous_agreement
424496003,5693,"when you get a chance, please review this code. i have done my best locally to produce a nice, clean implementation, but now that the diff is published, i'll make another pass over it looking for sharp edges.",0,0,0,0.8973486423492432,0.8986018300056458,0.7633945345878601,0.0,accept,unanimous_agreement
424781723,5693,"i've partially addressed the comments so far. notably, i've dropped the punctuator and now handle both time and size constraints during `process`.",0,0,0,0.9844339489936828,0.9860471487045288,0.9900847673416138,0.0,accept,unanimous_agreement
424843948,5693,"hi , , and , i believe i've addressed all the comments thus far, with the exception of whether to store the values serialized. i did notice a low-effort optimization in the current implementation to skip serializing if the buffer isn't size-constrained, which would carry over even when we add the changelog, if the buffer delays serializing until flush, so for high-turnover buffers, many records may never be serialized at all. but that's all a little beside the point... i'm still happy to change the whole implementation to store serialized data if that's the reviewers' preference.",1,1,1,0.88620525598526,0.9517198204994202,0.91971617937088,1.0,accept,unanimous_agreement
425098149,5693,test failure unrelated retest this please,0,0,0,0.9834269285202026,0.929627001285553,0.5868735313415527,0.0,accept,unanimous_agreement
425156616,5693,"hey all, fyi, i've just updated this pr to store serialized data instead. it was a bit more work than i anticipated because i ran into some snags related to preserving the window end information for timewindows (and also discovered i was using the wrong serde for session windows). these issues are both fixed now.",0,0,1,0.6309905648231506,0.934966802597046,0.7133944034576416,0.0,accept,majority_agreement
425168281,5693,"ok, , i have added some unit tests that verify that the processor throws the exception, and also some integration tests that verifies that streams shuts down for the same conditions. thanks for that catch.",1,1,1,0.9488836526870728,0.9021468758583068,0.9744318723678588,1.0,accept,unanimous_agreement
425171800,5693,"also, i just had a better idea for maintaining the mintimestamp value that cleans up the implementation quite a bit.",0,0,0,0.8613438606262207,0.9726523160934448,0.991303563117981,0.0,accept,unanimous_agreement
425724425,5693,"just to document the current state... i think the latest set of commits should resolve all the open comments, so this pr should be ready to merge, pending a rebase once #5521 is merged (or we could merge this first, if #5521 needs more reviews).",0,0,0,0.9738282561302184,0.9699577689170836,0.9885488152503968,0.0,accept,unanimous_agreement
426046990,5693,java 8 failure unrelated: [code block],0,0,0,0.9460940361022948,0.98744535446167,0.958608329296112,0.0,accept,unanimous_agreement
426059684,5693,"/ fyi, the tests have passed.",0,0,0,0.9770110249519348,0.6471642255783081,0.9772896766662598,0.0,accept,unanimous_agreement
426086754,5693,retest this please,0,0,0,0.9874829053878784,0.9671362638473512,0.980658769607544,0.0,accept,unanimous_agreement
426103858,5693,i had an unused import. the tests pass for me locally now.,0,0,0,0.9873842000961304,0.9785013794898988,0.9941115975379944,0.0,accept,unanimous_agreement
435276167,5821,can you help to review this pr when you have time? thanks!,1,1,1,0.9640206694602966,0.9448379874229432,0.967726230621338,1.0,accept,unanimous_agreement
438036817,5821,"lgtm. thanks for the update. could you rebase the patch? hey , would you like to review the patch or it is ok for me to merge it?",1,1,1,0.9470974802970886,0.9638183116912842,0.9780253171920776,1.0,accept,unanimous_agreement
438039462,5821,", : yes, i will make another pass in the next couple of days.",0,0,0,0.9038795828819276,0.7626720070838928,0.8445127606391907,0.0,accept,unanimous_agreement
442712153,5821,i have updated the pr to address all the comments. could you take a second look?,0,0,0,0.980402171611786,0.9803453087806702,0.9947673082351683,0.0,accept,unanimous_agreement
443155235,5821,thanks for the prompt reply. i have updated the pr to address your comments and rebase.,1,1,1,0.9022087454795836,0.8074333071708679,0.9526363611221312,1.0,accept,unanimous_agreement
443357329,5821,thanks for the comments. i have addressed your comments in the updated commit.,1,1,1,0.8722274303436279,0.7606366276741028,0.9355854392051696,1.0,accept,unanimous_agreement
443358641,5821,: do you want to take another look of this pr before merging?,0,0,0,0.9750930070877076,0.9869118928909302,0.9918147921562196,0.0,accept,unanimous_agreement
443404840,5821,lgtm. thanks for the in-depth review ! i am currently not able to access my original github account due to loss of my phone number and can not merge this pr myself.,1,1,1,0.9914427399635316,0.9922094345092772,0.9913835525512696,1.0,accept,unanimous_agreement
443437923,5821,: could you address the last comment on line 415 in kafkazkclient.scala?,0,0,0,0.9870184063911438,0.9954184293746948,0.9940541386604308,0.0,accept,unanimous_agreement
443440384,5821,my bad. updated the pr to fix that.,-1,-1,-1,0.9876112341880798,0.9825196266174316,0.994317352771759,-1.0,accept,unanimous_agreement
443601653,5821,"thanks a lot for the review. there are several follow-ups (kafka-6029 & kafka-7283) we can do after getting the broker epoch. i will address them accordingly and submit prs. if you are aware of anything that can also benefit from broker epoch, do let me know and i am open to more discussion. thanks again!",1,1,1,0.9894824028015136,0.9938015341758728,0.995028793811798,1.0,accept,unanimous_agreement
410306248,5428,"for reviews. note the logic of processing enforcement is open for discussion, and currently i'm thinking to change it to time based than iteration based. i will include a kip for this change for open discussions.",0,0,0,0.943324863910675,0.968645453453064,0.990368127822876,0.0,accept,unanimous_agreement
411805845,5428,system test passed at [a link] need to wait for kip-345 to be discussed and voted.,0,0,0,0.9835212230682372,0.9913634061813354,0.9952173233032228,0.0,accept,unanimous_agreement
412221182,5428,"i've done some simple benchmark comparing * trunk [a link] * this branch (default max.idle = 0ms) [a link] * this branch (default max.idle = 500ms) [a link] * this branch (default max.idle = 5000ms) [a link] my observations: 1. branch-0ms and branch-500ms have similar perf compared to trunk with one single input partition, with two partitions (i.e. joins) branch-0ms has degraded 25% compared to trunk, while branch-500ms outperforms slightly over trunk. still need to investigate why branch-0ms is worse with trunk. 2. branch-5000ms performs worse on single partitions with branch-0ms and branch-500ms, while its performance is comparable on two partitions (joins).",0,0,0,0.9532928466796876,0.9672335982322692,0.9376220107078552,0.0,accept,unanimous_agreement
413359579,5428,"i've found out the actual perf difference is that for joins, we are using leftjoin actually. and we are populating the data before starting the run the tests, hence it is actually a ""catching up"" mode where the first fetch request may just return one partition of the data. with no synchronization (ms = 0), it means that there are less records being joined and output to sink topics. i've changing leftjoin to join and the results are following: trunk: [a link] max.idle = 0ms: [a link] max.idle = 500ms: [a link] max.idle = 5000ms: [a link] some observations: 1. from the stream-stream join results we can see that the old manner (max.waits = 5 iterations) is actually very close to max.idle = 0ms (i guess it was maybe a few millis idle time). 2. from the table-table join results, the new code actually processes more data than the old code.",0,0,0,0.9699137210845948,0.9791380763053894,0.9596478343009948,0.0,accept,unanimous_agreement
413940230,5428,can you rebase?,0,0,0,0.988908588886261,0.98933345079422,0.9934529066085817,0.0,accept,unanimous_agreement
414789168,5428,the benchmark results looks good to me: [a link] [a link],1,1,1,0.9035948514938354,0.7331651449203491,0.9776875376701356,1.0,accept,unanimous_agreement
415487809,5428,"the tests i added in unit suite checks that 1) `idle time is respected, and timer is reset correctly once enforce has happened`, 2) optimization on skipping commit indeed happens. i think i can add more on 3) punctuate / commit happens as expected, 4) break loop and calls poll as expected. anything else comes to your mind?",0,0,0,0.9736858010292052,0.981255829334259,0.975376546382904,0.0,accept,unanimous_agreement
415540813,5428,i'm now working on add more tests ( let me know if the above plan is sufficient). i tried about separating commit for active and standby tasks but after some investigation i realized it is related to [a link] and hence i'd leave it to this jira / pr (also left a comment in the ticket for some clarification questions).,0,0,0,0.9729838967323304,0.9618126749992372,0.9575143456459044,0.0,accept,unanimous_agreement
415849669,5428,the latest perf numbers can be found here: [a link],0,0,0,0.9830394387245178,0.9846670627593994,0.9928492307662964,0.0,accept,unanimous_agreement
416423200,5428,"comments replied / addressed, please take another look.",0,0,0,0.9802712202072144,0.9764097929000854,0.9870442748069764,0.0,accept,unanimous_agreement
417518708,5428,"just to clarify, the reason i did this change is that i think recording the sensor every time we enforce processing actually makes more sense than only recording it the first time we start enforcing, since each enforce-processing period may differ and hence the number of records it processed may differ, recording only at the first time may not give us right amount of ""risk assessment"" when it happens.",0,0,0,0.9189530611038208,0.975491464138031,0.989592671394348,0.0,accept,unanimous_agreement
417686888,5428,"lgtm! thanks,",1,1,1,0.9843631386756896,0.9907625317573548,0.9929248094558716,1.0,accept,unanimous_agreement
417743966,5428,failures unrelated retest this please,0,0,0,0.9664865136146544,0.902599573135376,0.8057448267936707,0.0,accept,unanimous_agreement
417778438,5428,"that's a good question. personally i think we should make strict guarantees that if `processorcontext.commit()` is called, we make sure the state is committed before the next record for this task is going to be processed, because some application logic may rely on that (think: i did some external changes, and hence i do want to make sure the state is committed right aware before processing the next record). and hence the implementation of `maybecommitperuserrequested` in stream thread actually always check for `commitrequest` and `commitneeded` while ignoring the commit interval. could you point out which part of the doc indicates that `processorcontext.commit()` is more of a suggestion? maybe we do need to update it.",0,0,1,0.6559416651725769,0.7966375946998596,0.8393359184265137,0.0,accept,majority_agreement
419097804,5428,"i can't seem to find it, i must be mistaken so ignore my previous comment.",-1,0,0,0.7559671998023987,0.7542774677276611,0.8959269523620605,0.0,accept,majority_agreement
419674466,5428,test failures seem to be related. [code block],0,0,0,0.9225221276283264,0.992369532585144,0.98175847530365,0.0,accept,unanimous_agreement
420112814,5428,could you give it another look?,0,0,0,0.9848490953445436,0.9777211546897888,0.9866482615470886,0.0,accept,unanimous_agreement
420474848,5428,nice! super happy we got this finally in! great work !,1,1,1,0.9934868812561036,0.9951158761978148,0.9976009726524352,1.0,accept,unanimous_agreement
289730122,2743,"once the jenkins build is passing, it would be great to run the system tests via the branch builder.",0,0,0,0.8547757267951965,0.9161841869354248,0.8101420998573303,0.0,accept,unanimous_agreement
289955048,2743,", i merged a pr that disables the test_zk_security_upgrade test until this pr lands. when you rebase, can you please remove the ignore annotation from that test? thanks.",1,0,1,0.8744460940361023,0.623151421546936,0.733881950378418,1.0,accept,majority_agreement
290489990,2743,system tests running here: [a link],0,0,0,0.9878913164138794,0.988585650920868,0.9956315755844116,0.0,accept,unanimous_agreement
291013072,2743,i've addressed your second round of comments. i still need to add some code to protect earlier versions of the message format. i'll dig into that tomorrow.,0,0,0,0.975418746471405,0.956671416759491,0.9671790599822998,0.0,accept,unanimous_agreement
291670976,2743,this pr was re-raised from the confluent repo: [a link],0,0,0,0.9860568046569824,0.9879266619682312,0.99628746509552,0.0,accept,unanimous_agreement
2565744411,18240,"yes, i decided not to list that as an option because i felt it was equal to if not worse than the option of having resigned transition to prospective in epoch x + 1. personally i felt it was nicer to have less edge cases to the invariant that only prospective should transition to candidate",0,0,0,0.4649991989135742,0.8606966733932495,0.9825359582901,0.0,accept,unanimous_agreement
266410201,2244,i've not added any metrics to this at the moment as i would rather wait until the metrics prs from make it in (so i can avoid more merge conflicts),0,0,0,0.9777914881706238,0.9911721348762512,0.983029007911682,0.0,accept,unanimous_agreement
270882471,2244,so far reviewed only top level apis and lgtm. need to review rest.,0,0,0,0.9703622460365297,0.9896109104156494,0.9894722104072572,0.0,accept,unanimous_agreement
270945528,2244,i've reviewed next level down up to tests. overall lgtm. there are opportunities for doing a code refactor later on (post-pr) because the separate thread and state manager are similar to the existing code.,0,0,0,0.970919668674469,0.9898999333381652,0.9920469522476196,0.0,accept,unanimous_agreement
271264571,2244,"had a look at tests, they lgtm. overall i personally feel this optimisation adds a lot of complexity but i suppose that ship has sailed.",0,0,0,0.7709999084472656,0.767045259475708,0.8863776326179504,0.0,accept,unanimous_agreement
271393703,2244,"about the `old value computation` issue, i cannot think of a quick solution on top of my head and you may not like my ""long"" solution, but here it goes: 1. materialize the ktable-ktable join results as well as the joining ktales. as we discussed in another thread of ktable api confusions this is the last place where we may not materialize a ktable. 2. after all ktables are materialized, we then do not need to pass the "" "" pair before the join operations at all, but only before the aggregation operations, since we can always read the old joined value from the materialized store. 3. for global ktables, if we add its self joins with global ktables as well in the future, it will no be `virtual` either but also materialized. 4. one potential issue is, say if we have a multi-way table joins, then it may not be really worthwhile to materialize the intermediate tables as well. this can possibly be resolved when we extend the dsl parsing to global context to optimize some of the topologies.",0,0,0,0.9445850849151612,0.8747574090957642,0.9196482300758362,0.0,accept,unanimous_agreement
271529418,2244,"- your points above - that is the same solution i had in mind. i didn't really want to go down that path due to the overhead of introducing another store, changelog etc, but maybe that is the only way. there is also an issue i discovered on ktable/ktable join which means we might need to do something like this anyway - [a link]",0,0,0,0.9669647812843324,0.9787716269493104,0.9687039256095886,0.0,accept,unanimous_agreement
271560509,2244,", - i've fixed the issue by materializing the join as g mentioned above. this does mean i'll need to update the kip to add 2 overloaded methods as we need the serdes to write to the store. at the moment i think we either go this way or we remove the ktable/globalktable joins for now. my preference is probably to go with it",0,0,0,0.965465009212494,0.8987817764282227,0.9421876072883606,0.0,accept,unanimous_agreement
272077407,2244,lgtm. could you rebase?,0,0,0,0.9878706336021424,0.9739921689033508,0.9903671741485596,0.0,accept,unanimous_agreement
272127373,2244,- done.,0,0,0,0.9597840905189514,0.9647000432014464,0.8567222356796265,0.0,accept,unanimous_agreement
272216338,2244,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
272262967,2244,merged to trunk. thanks !! please feel free to close the corresponding jira when asf is back online.,1,1,1,0.9872980117797852,0.9939175844192504,0.9957996010780334,1.0,accept,unanimous_agreement
257023225,2074,this pr also fixes another issue that was introduced by the fix for kafka-3144. i point to it inline. let me know if you think that issue should be fixed and merged (more quickly) in a separate pr. thanks.,1,1,1,0.9547905921936036,0.8310671448707581,0.9571873545646667,1.0,accept,unanimous_agreement
266855542,2074,i've tried to address both kip-88 and kafka-3853 in the same pr here. i'd appreciate your feedback when you get a chance to review this. thanks.,1,1,1,0.9698482155799866,0.9879580140113832,0.9868582487106324,1.0,accept,unanimous_agreement
271678517,2074,"thanks for your thorough review of the pr and your feedback. i updated the pr based on your comments (except for the part that's related to kip-97, as discussed).",1,1,1,0.9205759167671204,0.8773450255393982,0.9795788526535034,1.0,accept,unanimous_agreement
271742743,2074,took a very quick look at this and it seems like it's not too far. i'm thinking we might want to let the client compatibility kip go in first since it is already quite large and it will be easier to make the changes here to continue supporting the old version of the offset fetch request. does that seem reasonable?,0,0,0,0.9438470005989076,0.9235702157020568,0.9236370921134948,0.0,accept,unanimous_agreement
271751793,2074,"yes, that sounds good to me. thanks for reviewing the patch. i'll work on the rest of your review items in the meantime.",1,1,1,0.9688592553138732,0.9374173879623412,0.9868767261505128,1.0,accept,unanimous_agreement
271970642,2074,looks like kip-97 (client compatibility) was merged.,0,0,0,0.9822056889533995,0.9938626289367676,0.9896457195281982,0.0,accept,unanimous_agreement
272019686,2074,thanks again for your feedback. i've tried to address them in the latest update. feel free to take a look while i work on rebasing the pr to cover client compatibility (kip-97).,1,1,1,0.955331563949585,0.9157618880271912,0.9687585234642028,1.0,accept,unanimous_agreement
272361345,2074,"saw your question about why we are only checking 3 of the 5 error codes. haha, i noticed the same thing when reviewing and opened kafka-4622.",1,1,1,0.497253805398941,0.9666106700897216,0.9871833324432372,1.0,accept,unanimous_agreement
272362673,2074,thanks for clarifying. then i can leave that to be fixed as part of that jira.,1,1,1,0.6932880878448486,0.7517186403274536,0.9743737578392028,1.0,accept,unanimous_agreement
272507729,2074,"i merged kip-103, which looks like it causes some conflicts in `apiversion`. hopefully shouldn't be too much trouble to resolve.",0,0,0,0.9689360857009888,0.9717981219291688,0.9694468379020692,0.0,accept,unanimous_agreement
329325194,3849,"thanks for the pr . looking over the changes, it seems that there are two cases from the kip which don't seem to be covered: 1. setting the `polltimeout` to be the expirytime of the oldest batch being sent in the produce request. i think we need this to make sure that we expire batches in a timely manner. 2. related to the previous point, the current patch doesn't seem to expire inflight requests, which is another feature that the kip seems to promise. have i missed something? or are you planning on adding the functionality above?",1,1,1,0.9217859506607056,0.8475664854049683,0.9510508179664612,1.0,accept,unanimous_agreement
329635100,3849,"heads up , the following pr just got merged, and may have some conflicts with the current patch : [a link] there shouldn't be any impact on the logic, however.",0,0,0,0.9835010766983032,0.9887732863426208,0.9924845099449158,0.0,accept,unanimous_agreement
330281613,3849,friendly reminder that the feature freeze is this wednesday.,0,0,0,0.8624887466430664,0.9707180857658386,0.9896777868270874,0.0,accept,unanimous_agreement
330318612,3849,"just want to check. do you think this feature is a ""minor"" feature?",0,0,0,0.9851526021957396,0.9866286516189576,0.982575297355652,0.0,accept,unanimous_agreement
330426320,3849,", it is possible to classify this as a minor feature, but the fact that it affects a core part of the producer puts it in a bit of a grey area. if the pr is almost ready and we miss the feature freeze, my take is that it would be ok to merge it by the end of this week. later than that and it seems a bit risky. it's a bit worrying that the merge conflicts haven't been resolved since last week.",-1,-1,-1,0.8478580713272095,0.945275902748108,0.6340653896331787,-1.0,accept,unanimous_agreement
330584441,3849,i've an new pr but after a rebase i've to fix one more test. working on that now.,0,0,0,0.920788586139679,0.8245474100112915,0.9607422947883606,0.0,accept,unanimous_agreement
330586776,3849,thanks !,1,1,1,0.9308210611343384,0.9159799218177797,0.8865044713020325,1.0,accept,unanimous_agreement
330600198,3849,it's not clear to me why `testexpiryoffirstbatchshouldcauseresetiffuturebatchesfail` and `testexpiryoffirstbatchshouldnotcauseunresolvedsequencesiffuturebatchessucceed` are failing. it looks like a batch that should be in incompletebatches isn't there. any thoughts?,0,0,0,0.5389224886894226,0.7351192831993103,0.7000368237495422,0.0,accept,unanimous_agreement
330674030,3849,i added the following to testexpiryoffirstbatchshouldcauseresetiffuturebatchesfail before the first sender.run() call [code block] the test still fails.,0,0,0,0.9860397577285768,0.9934285283088684,0.9941613078117372,0.0,accept,unanimous_agreement
330699245,3849,where are those tests failing? the latest pr builder suggests that the clients and core tests all passed.,0,0,0,0.9837895035743712,0.993626832962036,0.9841361045837402,0.0,accept,unanimous_agreement
330700192,3849,the sender and recordaccumulator are passing now. the failing tests are connect tests that are irrelevant.,0,0,0,0.9586714506149292,0.991995930671692,0.9913700819015504,0.0,accept,unanimous_agreement
330701042,3849,"i don't think the test failures are irrelevant since the same 3 tests failed in all the runs. further, the cause of the failure is: [code block] i think their mocks may need to be updated to take account of the new configs and attendant `configexceptions`",0,0,0,0.9732943773269652,0.9781827926635742,0.9838132858276368,0.0,accept,unanimous_agreement
331024421,3849,"i had a look at the failing sender expiry tests. what is happening is that the tests are not modified to account for the fact that the inflight batches can be expired. in the tests, we used to expire a batch sitting in the accumulator but _not_ the inflight batch. when the inflight batch returned, it would be re queued. but now, the test sends the response for the inflight batch, but when it goes to requeue, it discovers that there shouldn't be an inflight request an raises an exception. the tests should be updated to account for the new behavior and make sure that the inflight batch is _not_ expired.",0,0,0,0.9632763266563416,0.983342170715332,0.9333487153053284,0.0,accept,unanimous_agreement
331024720,3849,"actually, the test reveals a bug in the current patch: the response for the inflight batch which expired is not being handled correctly. we should not be trying to requeue it to start with. so we need two tests: one where the inflight batch is not expired, and the current case. the reenqueue logic in the sender needs to be updated to not reenqueue the expired batches.",0,0,0,0.9739474058151244,0.9588050246238708,0.98628169298172,0.0,accept,unanimous_agreement
331324500,3849,added a couple of tests. test a batch is correctly inserted into the in.flight.batches if needed. and not inserted if not needed. `recordaccumulatortest.testsoontoexpirebatchesarepickedup` test the callback of an expired batch is fired in time when it is in-flight/not in-flight `sendertest.testinflightbatchesexpireondeliverytimeout`,0,0,0,0.9795793294906616,0.9950705766677856,0.9781977534294128,0.0,accept,unanimous_agreement
331497207,3849,i don't see any changes since my comments. are you sure you have pushed them?,0,0,0,0.8961247205734253,0.8931837677955627,0.9819961786270142,0.0,accept,unanimous_agreement
331581751,3849,"added more tests test when expire an in-flight batch, we still wait for the request to finish before sending the next batch = `sendertest.testwhenfirstbatchexpirenosendsecondbatchifguranteeorder` test we are not going to retry an already expired batch = `sendertest.testexpiredbatchdoesnotretry` regarding the last test ""test when batch is expired prematurely, the buffer pool is only deallocated after the response is returned. (because we are still holding the batch until the response is returned)"". i'm not sure if it's really needed because `recordaccumulator.abortbatches` deallocates all incomplete batches (whether they have been sent or not).",0,0,0,0.9759269952774048,0.988188862800598,0.9908449053764344,0.0,accept,unanimous_agreement
331584625,3849,i am looking at the update patch. `recordaccumulator.abortbatches()` is only used when the producer is force closed. we do not need to worry about the memory consumption at that point anymore because the producer has stopped accepting new messages.,0,0,0,0.966052770614624,0.9906575679779052,0.9885225296020508,0.0,accept,unanimous_agreement
331585074,3849,it looks there are 3 test failures from connect. can you check?,0,0,0,0.985750675201416,0.979766845703125,0.9904306530952454,0.0,accept,unanimous_agreement
331613171,3849,"btw, i did not see the tests to ensure we are not double deallocating memory. will you add those tests?",0,0,0,0.970515251159668,0.9701567888259888,0.9832709431648254,0.0,accept,unanimous_agreement
331983684,3849,"fyi, the connect tests are still failing with: [code block] it would be good to get some green builds before this can be considered safe for merge.",0,0,0,0.9712843298912048,0.9744420051574708,0.973173439502716,0.0,accept,unanimous_agreement
331987993,3849,fixed connect tests.,0,0,0,0.9848302602767944,0.9898508787155152,0.9935925602912904,0.0,accept,unanimous_agreement
332034321,3849,"i have a high level comment about the inflight batch handling. i was wondering what the advantages/disadvantages of the current approach (where batches are grouped by creation time) would be against adding a `map >` to the recordaccumulator. this would complement the existing `map >`. the priority queue would be ordered by the batch creation time (similar to the current `navigablemap`). we would add to the queue on drain. and we would remove from the queue in `sender.failbatch` and `sender.completebatch`, just like we do for the `transactionmanager` (see: [a link] so we don't have any special logic for the `recordaccumulator.expiredbatches` would now iterate over both maps (`topicpartition -> deque` and `topicpartition -> priorityqueue`), with the priority queue being iterated on first. this doesn't change the time complexity of the algorithm, since we are iterating over all partitions in `expiredbatches` once anyway. the advantage of this approach is that managing inflight batches is much simpler (add to the queue on drain, remove on completion). it doesn't complicate the expiry in any way. the main disadvantage that i see is in computing the `earliestdeliverytimeout`. for this you would need to peek at the head of the queue for each partition which has an inflight batch. but this should be fairly cheap, since we would be iterating over 1000's of objects even in the largest installations. what do you think?",0,0,0,0.9655988216400146,0.9718575477600098,0.9089749455451964,0.0,accept,unanimous_agreement
332039241,3849,"i made this comment in a thread, but that may be buried because github collapses comments. there is a bug in the current implementation which means that we will retain references to `producerbatches` in the producer long after they have been successfully completed. the following sequence of assertions in any `sendertest` shows the presence of this bug: [code block]",0,0,0,0.9679157137870787,0.9495760202407836,0.9909768104553224,0.0,accept,unanimous_agreement
332043954,3849,"initially i was under the same impression. one concern of the priority queue is that whenever we need to remove things out of the queue, we will have to iterate over it. since a producer can poll thousands of times per seconds, and maybe remove tens of thousands of batches. that said, in most cases, the queue would be empty because most of the batches are sent when remaining delivery.timeout.ms is greater than request.timeout.ms. however, it may still slowdown things when retries occur. my take on this is that if the implementation complexity does not increase much, we may want to follow the robust way instead. regarding the memory issue. we could not release the memory of an expired batch right away when the batch is still in-flight. this is because the in-flight batch is still referred by the requestcompletionhandler of the request. we could remove it from the requestcompletionhandler, but this is a little tricky.",0,0,0,0.8997835516929626,0.9602686166763306,0.9490423202514648,0.0,accept,unanimous_agreement
332044968,3849,"i am not sure i follow this: is this a response to my comment above where i shared a snippet from `sendertest` to show that we are retaining batches after they are completed? if so, i didn't mean that we need to release an inflight batch right away. what i was pointing out is that the _only_ way the current patch is deleting batches is by expiring them. this may not be a big deal, since the deallocation is done when the batch is completed, but it still seems like an unnecessary anomaly.",0,0,-1,0.5995014905929565,0.9512423872947692,0.6121717095375061,0.0,accept,majority_agreement
332045109,3849,"also, it looks like recent commits are causing threads to leak, .",0,0,0,0.9420400857925416,0.9139803051948548,0.93826162815094,0.0,accept,unanimous_agreement
332046231,3849,not sure i follow. it would be no worse that what we do today with the `deque` except it would be limited only to partitions which had in flight batches. so it is not making things much worse.,-1,0,0,0.644328773021698,0.5652440786361694,0.5246639847755432,0.0,accept,majority_agreement
332058975,3849,i misunderstood your proposal. i though you were suggesting to use a priority queue to store all the in-flight batches. a `map >` sounds reasonable.,0,0,0,0.952013373374939,0.5732282400131226,0.919684112071991,0.0,accept,unanimous_agreement
332080828,3849,"yes, i meant a separate `map >`, to replace the current `navigablemap`.",0,0,0,0.9877411723136902,0.99046528339386,0.9944570064544678,0.0,accept,unanimous_agreement
332662977,3849,any updates on this? do you need help addressing the test failures?,0,0,0,0.9840317368507384,0.9894213676452636,0.9914734363555908,0.0,accept,unanimous_agreement
332973054,3849,sorry for disappearing on you guys. i got sucked into an oncall issue. so i had to context switch. i plan to resume work on this pr next week. i've not had a chance to respond to the comments so far. thank for you the review so far. it's been very useful.,-1,-1,-1,0.979480504989624,0.9698591828346252,0.9438754320144652,-1.0,accept,unanimous_agreement
344707856,3849,i've updated the pr. please take a look,0,0,0,0.9605138301849364,0.9332720041275024,0.978773295879364,0.0,accept,unanimous_agreement
344734652,3849,"please fix the failing tests, there is a config exception due to the new delivery timeout config.",0,0,0,0.98376727104187,0.9356552958488464,0.9851130247116088,0.0,accept,unanimous_agreement
345031109,3849,all checks passed,0,0,0,0.9850959181785583,0.97699773311615,0.9897468090057372,0.0,accept,unanimous_agreement
353216067,3849,i updated the implementation to use `concurrentmap >`. please take a look. i don't see the above test failures on my machine.,0,0,0,0.9718063473701476,0.981386125087738,0.9943063855171204,0.0,accept,unanimous_agreement
358480286,3849,"so the `org.apache.kafka.clients.producer.internals.sendertest.testmetadatatopicexpiry` test has failed twice in a row with: [code block] given that these changes are on the same code, and given the consistent failure of this test, it is probably a regression. can you reproduce the failure locally?",0,0,0,0.9826987981796264,0.9928444027900696,0.9924452900886536,0.0,accept,unanimous_agreement
358480445,3849,"just looking at the stack trace and the test, it may be that an expired batch is being closed twice in some cases.",0,0,0,0.986671268939972,0.9817543625831604,0.987454891204834,0.0,accept,unanimous_agreement
368266171,3849,it would be nice to unblock this. can someone else pick up the work?,0,0,0,0.8189288973808289,0.9656418561935424,0.84945148229599,0.0,accept,unanimous_agreement
370507706,3849,"yeah, this has been pending for too long. i have spoken to and he said he still wants to finish the patch. he will figure out the eta and see if that works.",0,0,0,0.9533793926239014,0.85397869348526,0.748426616191864,0.0,accept,unanimous_agreement
385707559,3849,is there any update on the status of this pr? it would be great if we could get this in the next release.,1,0,1,0.5777892470359802,0.912857174873352,0.6715326309204102,1.0,accept,majority_agreement
387590454,3849,we also hit this issue when running kafka streams library with some high volume output topics. it would be nice to get this moving and push it to the next release.,0,0,0,0.8269931077957153,0.9769420623779296,0.966638207435608,0.0,accept,unanimous_agreement
387592858,3849,becket cant load this page for some reason (some weird issue with his github profile?). we are ok with you taking over this patch.,0,0,0,0.7273319959640503,0.9399076104164124,0.8979479074478149,0.0,accept,unanimous_agreement
387609246,3849,"i don't have any update since dec last year. sorry, the work has stalled and it has been very hard to find cycles for this effort. i don't mind if confluent wants to take this effort forward. better later than never.",-1,-1,-1,0.9833741188049316,0.9876853227615356,0.9887852668762208,-1.0,accept,unanimous_agreement
387636464,3849,cc,0,0,0,0.9684008955955504,0.8343546390533447,0.9699252247810364,0.0,accept,unanimous_agreement
393617255,3849,i made some change based on your pull request to fix style check and test failure. do yo mind i amend the change to this pull request? cc [a link],0,0,0,0.953655481338501,0.9858011603355408,0.9939084053039552,0.0,accept,unanimous_agreement
393700104,3849,i'd suggest you create your own pr against apache kafka trunk and let other reviewers to continue reviewing that one.,0,0,0,0.981797456741333,0.9854727387428284,0.986215353012085,0.0,accept,unanimous_agreement
393760744,3849,sure. will create a separate pull request,0,0,0,0.9797589778900146,0.9681119918823242,0.9913539290428162,0.0,accept,unanimous_agreement
399271744,3849,created new pr [a link] for kafka-5886,0,0,0,0.9857789278030396,0.9927741289138794,0.9951144456863404,0.0,accept,unanimous_agreement
464845906,3849,"this has been merged via a different pr, closing.",0,0,0,0.9846168756484984,0.993473470211029,0.9953041076660156,0.0,accept,unanimous_agreement
217925465,1336,would appreciate your input on this pull request.,0,1,0,0.7674633264541626,0.7988282442092896,0.5240486860275269,0.0,accept,majority_agreement
226875059,1336,thank you for your feedback. the pr was updated to address your comments.,1,1,1,0.8473960161209106,0.8096649050712585,0.9276379942893982,1.0,accept,unanimous_agreement
227269465,1336,i'm wondering about approaches for testing this patch. one possibility is to parse the output in the test case and build assertions off from that. another option might be to decouple the command logic from the print logic so that you can build test cases using a convenient object representation. what do you think?,0,0,0,0.9629626274108888,0.9437401294708252,0.9457823634147644,0.0,accept,unanimous_agreement
227306696,1336,"thank you very much for your careful review and feedback. i made some changes in the new patch based on the feedback received. the only part that is outstanding in addressing your feedback is [a link]. i responded to your comment and will wait for your clarification as i may be missing something. both approaches you suggested for designing unit tests for this patch should work. parsing the output and writing assertions based on that would be faster but probably not the cleaner option, in my opinion. separating the logic from reporting seems more reasonable to me but it would require more time and thinking. i can take either direction once i hear your (and others') feedback about it.",1,1,1,0.969202995300293,0.9788665175437928,0.9733587503433228,1.0,accept,unanimous_agreement
227503027,1336,"yeah, i agree that separating the logic would make testing easier. it's more painful now, but making the code more testable would probably pay off in the long run. i wonder if we could introduce a simple trait to encapsulate the output writing. for example: [code block] then we could test the command using a mock implementation. anyway, if the level of effort to get some basic coverage using this or another pattern is not too high, i'd suggest we do it here. otherwise, we could create a followup jira.",0,0,0,0.8816430568695068,0.9385363459587096,0.9270712733268738,0.0,accept,unanimous_agreement
227780324,1336,thanks for your suggestion on how to go about separating the logic. i'll give it a try and let you know how it goes.,1,1,1,0.8605066537857056,0.7980664372444153,0.9699283838272096,1.0,accept,unanimous_agreement
228908052,1336,i've spent some time on this and it seems to involve restructuring of the code to some extent (assuming i haven't over-complicated things). so i'm wondering whether we should keep the changes simpler and do this work and unit tests in separate work items.,0,0,0,0.9290789365768432,0.8591620922088623,0.9205561280250548,0.0,accept,unanimous_agreement
228910259,1336,"either way works for me. i don't think there's any particular hurry to get this patch in since it will probably go in 0.10.1, so if restructuring the code seems like a good idea, i'm probably slightly in favor of doing it here so we don't forget about it later. that said, i know it can be painful to fill in some of these testing gaps (they're gaps because testing wasn't easy!), so if is ok with it, then doing it separately seems fine to me.",0,1,0,0.7257289290428162,0.6763557195663452,0.6328725814819336,0.0,accept,majority_agreement
229165709,1336,"sounds good. i'm okay both ways and will wait for 's feedback. regarding the unit tests, i'm trying to wrap my head around how to write them with proper mocking. you can see [a link] how i've made changes to the last patch so the `outputwriter` trait that you suggested is used. the issue now is i don't see clearly how to write the unit tests with mocking (for example, for `--describe` the main functionality is inside the protected `describegroup()` method). this tells me perhaps further restructuring is required to make the command properly mockable, but i wanted to ask your opinion before heading down that road. thanks.",1,1,1,0.9425690174102784,0.9438058733940125,0.95365709066391,1.0,accept,unanimous_agreement
232168291,1336,i restructured `consumergroupcommand.scala` a little bit for testing purposes according to your suggestion and added a number of unit tests. i would appreciate your feedback.,1,1,1,0.9118056893348694,0.8015778660774231,0.9527617692947388,1.0,accept,unanimous_agreement
233806677,1336,"this in another pending pr that conflicts with changes required for [a link], and would be great if we can finalize it soon. thanks.",1,1,1,0.9736151695251464,0.9673992395401,0.9914383888244628,1.0,accept,unanimous_agreement
234130456,1336,thanks for the thorough feedback. i hope i addressed all of them in the recent update.,1,1,1,0.9139454364776612,0.9724374413490297,0.9740380048751832,1.0,accept,unanimous_agreement
235795010,1336,thanks for another round of reviews. the pr is updated to address your comments.,1,1,1,0.7744582295417786,0.733340322971344,0.933872640132904,1.0,accept,unanimous_agreement
236029039,1336,thanks . made those changes.,1,1,1,0.6829760670661926,0.9255496263504028,0.6993794441223145,1.0,accept,unanimous_agreement
245448879,1336,the patch is updated to include coordinator info when `--new-consumer` is used. since the group column and the new coordinator column report redundant information i modified the output of `--new-consumer` option to [code block] the zookeeper based output will not print the coordinator line above. your feedback is appreciated.,1,0,1,0.8096293210983276,0.5811030864715576,0.9429442286491394,1.0,accept,majority_agreement
254966900,1336,thanks for the thorough feedback. i hope i was able to address your comments in the recent patch.,1,1,1,0.9304035902023317,0.9675753116607666,0.971606194972992,1.0,accept,unanimous_agreement
255177309,1336,i submitted another update taking into account your comments. thanks again for the thorough reviews and constructive feedback.,1,1,1,0.9514115452766418,0.9121958017349244,0.9836836457252502,1.0,accept,unanimous_agreement
255270819,1336,"i had one more crack at this. if you spot any more potential improvements please let me know. thanks. btw, the junkins error seems to be unrelated to this.",1,1,1,0.5253384113311768,0.9556246399879456,0.9880913496017456,1.0,accept,unanimous_agreement
255469685,1336,just did some cleanup of variable/method/field names based on the recent `member id` to `consumer id` renaming.,0,0,0,0.9880339503288268,0.9929653406143188,0.9943391680717468,0.0,accept,unanimous_agreement
255487773,1336,"lgtm. really appreciate the effort on this pr. it's a huge contribution when you can not only add functionality, but improve the testability of the code.",1,1,1,0.9866337776184082,0.995593249797821,0.9938043355941772,1.0,accept,unanimous_agreement
255492874,1336,thanks for your feedback along the way. very much appreciated.,1,1,1,0.9615730047225952,0.9883344173431396,0.984862506389618,1.0,accept,unanimous_agreement
290404058,2744,"thank you for the review, i have addressed the comments and left one question. system tests passed here: [a link]",1,1,1,0.9277257919311525,0.9575747847557068,0.9501821398735046,1.0,accept,unanimous_agreement
292580163,2744,"thank you for the review. i haven't moved throttle-time to the response header as explained in the previous comment. one alternative to making the code in `kafkaapis` neater would be to make `throttletimems` a non-final field. that way the response could be created as it is now, and throttle time can just be updated in the response. i didn't want to do that initially because it makes the code inconsistent, but perhaps that is ok for this case?",1,1,1,0.8314771056175232,0.903085231781006,0.9299830794334412,1.0,accept,unanimous_agreement
296140304,2744,"when it comes to performance tests, it would also be good to run them on aws as vms like xen often have an impact of time-related methods. for example: [a link]",0,0,0,0.9693058133125304,0.9870833158493042,0.9918033480644226,0.0,accept,unanimous_agreement
296242715,2744,"thank you for the reviews. i have rebased. i have performance tests on my laptop with 8 threads and see no noticeable difference. - producer (100 byte messages): 161.92 mb/s (before) and 164.4 mb/s(after) - consumer (100 byte messages) : 168.27 mb/se (before) and 173.5 mb/s (after) i don't have an account to run the tests on aws, but i will start a system test run to compare.",1,1,1,0.9772309064865112,0.9823009967803956,0.9804973602294922,1.0,accept,unanimous_agreement
296455215,2744,"by the way, since the additional `system.nanotime` call per selection key only happens if the new config is enabled, i think it's ok to merge this and do additional performance testing later. i think this is one of the cases where issues only show themselves when the number of threads and cpus is larger than one can test on a laptop.",0,0,0,0.9760569334030152,0.988563060760498,0.9793459177017212,0.0,accept,unanimous_agreement
296608816,2744,"yes, agree that it is difficult to assess the impact of the change on a laptop. i have addressed most of the issues. left comments for the others. many thanks for the reviews.",1,1,1,0.6543433666229248,0.9535840153694152,0.9708483219146729,1.0,accept,unanimous_agreement
296814266,2744,"thank you for the review. i have run the performance tests on my laptop with high request quota configured. again, there was no noticeable difference. producer (100 bytes): trunk: 151.33 mb/s requestquota: 155.78 mb/s consumer (100 bytes): trunk: 332.32 mb/s requestquota: 339.64 mb/s also started a system test run with the latest level of code: [a link]",1,1,1,0.9600794911384584,0.972625494003296,0.9613811373710632,1.0,accept,unanimous_agreement
297013271,2744,system tests have passed: [a link],0,0,0,0.9864221811294556,0.9849091172218324,0.9885116815567015,0.0,accept,unanimous_agreement
298118490,2744,thank you for the reviews. have rebased and addressed the comments.,1,1,1,0.8301118612289429,0.8711702227592468,0.93013197183609,1.0,accept,unanimous_agreement
298363968,2744,: thanks for the latest patch. lgtm.,1,1,1,0.9310525059700012,0.9416406154632568,0.9688839912414552,1.0,accept,unanimous_agreement
729396557,9487,system test run (still running but so far it's all pass) -- [a link],0,0,0,0.9834907054901124,0.9398593902587892,0.9864120483398438,0.0,accept,unanimous_agreement
729474257,9487,"system tests passed, all three java builds passed. merging now",0,0,0,0.9846124053001404,0.9722050428390504,0.9345157742500304,0.0,accept,unanimous_agreement
729475252,9487,merged to trunk :partying_face:,0,0,1,0.9842432737350464,0.984776258468628,0.7666565775871277,0.0,accept,majority_agreement
764961250,9487,seems we forgot the update the docs for this kip. can you do a follow up pr? this follow up pr should also cover the changes of [a link] that is part if the same kip.,0,0,0,0.9867138266563416,0.9919058084487916,0.9907991290092468,0.0,accept,unanimous_agreement
857564039,10851,call for review,0,0,0,0.9832175970077516,0.9793323278427124,0.990225613117218,0.0,accept,unanimous_agreement
862166415,10851,gentle nudge,0,-1,0,0.7549498677253723,0.9819871783256532,0.965026080608368,0.0,accept,majority_agreement
879032262,10851,sorry for the long silence! some other tasks got in my way. i plan to review this pr tomorrow.,-1,-1,-1,0.9903603792190552,0.9930768013000488,0.993237316608429,-1.0,accept,unanimous_agreement
880127165,10851,thanks for the feedback. i've replied/addressed all of your comments.,1,1,1,0.5593553781509399,0.8553299307823181,0.9715455770492554,1.0,accept,unanimous_agreement
882283484,10851,thanks for the valuable feedback :person_bowing: i've replied/addressed your comments.,1,1,1,0.7648232579231262,0.9459280371665956,0.9950624108314514,1.0,accept,unanimous_agreement
885645624,10851,fyi: i will be offline the next two weeks. i am sorry that i haven't had time to review this pr this week.,-1,-1,-1,0.9896696209907532,0.991302251815796,0.9911136031150818,-1.0,accept,unanimous_agreement
899245208,10851,hi ! thanks for the feedback. i will address your comments this week.,1,1,1,0.976357638835907,0.9865812063217164,0.9876165986061096,1.0,accept,unanimous_agreement
902900779,10851,hi thank you for valuable feedback. i've addressed your comments. please have a look once you got time.,1,1,1,0.9457976222038268,0.9839676022529602,0.9811106324195862,1.0,accept,unanimous_agreement
905490438,10851,"thanks for the feedback , i've pushed the new changes.",1,1,1,0.669278085231781,0.715527355670929,0.9180072546005248,1.0,accept,unanimous_agreement
905514160,10851,"the general thought on the implementation. as of now, we choose concrete `standbytaskassignor` implementation based on passed `assignmentconfigs` value. instead, an alternative approach would be to have a chained standby task assignment based on multiple implementations. for instance, when required client tag configuration is present, we can try to assign the standby tasks based on `list.of(new clienttagawarestandbytaskassignor(), new leastloadedclientstandbytaskassignor())`. this way, `clienttagawarestandbytaskassignor` can try to distribute the standby tasks based on tags dimensions. if there are no more client tag dimensions available, and we still have `any(taskstoremainingstandbys) > 0`, the next implementation in the chain (in this case `leastloadedclientstandbytaskassignor`) will be called. with this, `leastloadedclientstandbytaskassignor` can default to assigning the remaining standbys to the least loaded clients. right now, `clienttagawarestandbytaskassignor` does both assignments based on available dimensions and fallback to the least loaded if there are no enough tag dimensions. current implementation leads to more complex code. while with the approach above, we can clearly separate the implementations without duplication (there's no code duplication, rather ""logic"" duplication). for now, i've chosen to go with the simplest approach - having two independent implementations and selecting an appropriate one based on passed `assignmentconfigs.` still, i wanted to share this idea here, just in case.",0,0,0,0.9814738631248474,0.9946446418762208,0.9841895699501038,0.0,accept,unanimous_agreement
911752624,10851,"hi , i've addressed/replied to your comments. thanks for the feedback. fyi - i'll be offline from next week for 2 weeks.",1,1,1,0.9386003017425536,0.983331024646759,0.9871395230293274,1.0,accept,unanimous_agreement
925659295,10851,hi is it possible to continue pushing this pr forward? i'm back from my holidays.,0,0,0,0.8909318447113037,0.82521653175354,0.9723306894302368,0.0,accept,unanimous_agreement
927659618,10851,welcome back ! sorry for the silence around this pr. i got side-tracked by other tasks. i will try to review this pr this week.,-1,-1,-1,0.9904167652130128,0.990439772605896,0.9941108822822572,-1.0,accept,unanimous_agreement
934149747,10851,thanks for the feedback i will review and address your comments this week.,1,1,1,0.8839636445045471,0.542321503162384,0.91255521774292,1.0,accept,unanimous_agreement
944314838,10851,"hi , small update on my side - wasn't able to find time to work on this pr this week. will try to prioritise this for the next week.",0,0,0,0.7925840616226196,0.6834626793861389,0.8423479795455933,0.0,accept,unanimous_agreement
959050366,10851,"hi very sorry for disappearing, didn't find time to deal with this pr. i've addressed your comments. please have another look when you got time.",-1,-1,-1,0.9866065979003906,0.9930926561355592,0.9907481074333192,-1.0,accept,unanimous_agreement
983959899,10851,"hi , will you have time to look at this pr again?",0,0,0,0.9669431447982788,0.9603529572486876,0.947087287902832,0.0,accept,unanimous_agreement
984453564,10851,i am really sorry that i haven't found the time to look at your updated pr. i will put it on my list for next week.,-1,-1,-1,0.987159252166748,0.9926856756210328,0.9893847107887268,-1.0,accept,unanimous_agreement
984477740,10851,no worries and thank you.,1,1,1,0.8950825333595276,0.9666762948036194,0.9535226821899414,1.0,accept,unanimous_agreement
993399413,10851,hi thanks for the feedback. i'll address your comments shortly.,1,1,1,0.6554189920425415,0.552548348903656,0.92336106300354,1.0,accept,unanimous_agreement
1017438641,10851,hi small update from my side. i came back from holidays a week ago so will continue working on this pr this week. sorry for the delay.,-1,-1,-1,0.9895910620689392,0.9930095672607422,0.9937844276428224,-1.0,accept,unanimous_agreement
1017815908,10851,"hi , i've address your comments with the latest commit. please have a look when you got time. thanks.",1,1,1,0.9084860682487488,0.9739478826522828,0.9760830402374268,1.0,accept,unanimous_agreement
1018535614,10851,"the latest commit (be3dcff4dc463fd8d23998537e36f852b99ec083) has a few changes. 1. there's explicit fallback to fallback to `defaultstandbytaskassignor` logic if rack aware standby task assignment fails to assign standby tasks. 2. rack aware standby task assignment logic takes into account client capacity when assigning the standby task. if client has reached the capacity, algorithm won't assign standby task to it. when that happens, we will fallback to the least loaded clients. i believe this is better approach, as we will avoid overloading the clients. there will be a warning log, to inform the user. **edit:** instead of just checking if capacity is reached on the client, we also now check if load can be balanced [a link]. looking forward hearing your thoughts,",0,0,0,0.9826783537864684,0.9850799441337584,0.9092721939086914,0.0,accept,unanimous_agreement
1025503003,10851,"hi , sorry for the ping. any chance we could review pr this week? thanks",-1,-1,-1,0.9848559498786926,0.9912388920783995,0.992679238319397,-1.0,accept,unanimous_agreement
1030773435,10851,", thanks for the pr. i'll take a look next week. thanks.",1,1,1,0.956785261631012,0.99092835187912,0.9893261194229126,1.0,accept,unanimous_agreement
1032412988,10851,i will try to review this friday. sorry for the delay but i was sick for the last two weeks.,-1,-1,-1,0.9888096451759338,0.9932791590690612,0.9915675520896912,-1.0,accept,unanimous_agreement
1041380775,10851,"hi , thanks for the valuable feedback. i've addressed your comments and pushed the changes. i also resolved conversations feel free to unresolve them if you think i haven't addressed your comments. looking forward hearing your thoughts.",1,1,1,0.9756429195404052,0.9925163388252258,0.9950057864189148,1.0,accept,unanimous_agreement
1044574529,10851,hi thanks for the feedback. i've addressed your comments.,1,1,1,0.5896090865135193,0.833475649356842,0.9243869185447692,1.0,accept,unanimous_agreement
1046758399,10851,"thanks for the valuable feedback. i've addressed your comments, please have a look when you got time.",1,1,1,0.9351596236228944,0.9704071283340454,0.986671507358551,1.0,accept,unanimous_agreement
1048612254,10851,hi i've addressed your comments. please have a look. thank you!,1,1,1,0.983087956905365,0.9931393265724182,0.9840270280838012,1.0,accept,unanimous_agreement
1050043629,10851,hi mind having another look? hoping to finalise this pr as soon as possible :) thanks,1,1,1,0.9875923991203308,0.99502831697464,0.9860947132110596,1.0,accept,unanimous_agreement
1055442027,10851,"i will try to have a look this week. i would also like to get this pr merged as soon as possible. since has already approved it. if i will not manage to have a look, can merge it.",0,0,0,0.8879837989807129,0.6255766153335571,0.8904854655265808,0.0,accept,unanimous_agreement
1055489018,10851,"thanks , appreciate it. please also note that next pr, the protocol change, is also ready to be reviewed. [a link] whenever it's possible, please have a look at that too.",1,1,1,0.9585850834846495,0.9904826879501344,0.9895750880241394,1.0,accept,unanimous_agreement
1056919176,10851,let me know if you want to address my minor comments in this pr. after that or i can merge this pr. nice work!,1,1,1,0.9919961094856262,0.9950866103172302,0.9902796149253844,1.0,accept,unanimous_agreement
1056940382,10851,"hi , thanks for the review. i agree with what you said and made a note to myself to address your comments in the follow-up prs. so if it's okay, i think we can merge this one. thank you!",1,1,1,0.9855915307998656,0.9934364557266236,0.9957320094108582,1.0,accept,unanimous_agreement
1057630852,10851,failed tests are unrelated and also failed in `trunk` build. [code block],0,0,0,0.9651305675506592,0.9947338104248048,0.990555226802826,0.0,accept,unanimous_agreement
1057900673,10851,:partying_face:,0,0,0,0.9438759684562684,0.9843850135803224,0.9041956067085266,0.0,accept,unanimous_agreement
137872274,195,the only test failure reported by the bot was kafka.producer.producertest > testsendwithdeadbroker failed java.lang.assertionerror: message set should have 1 message at org.junit.assert.fail(assert.java:88) at org.junit.assert.asserttrue(assert.java:41) at kafka.producer.producertest.testsendwithdeadbroker(producertest.scala:260) which passes locally and seems very unrelated to the changes i made. i will try and run the test in a loop to see if i can reproduce.,0,0,0,0.9375221729278564,0.9729955196380616,0.9914699196815492,0.0,accept,unanimous_agreement
139683279,195,added a generic zknodechangelistener and removed the scheduler thread. couldn't think of a better name for the class so if you have a better name let me know. addressed all other comments in this pr.,0,0,0,0.9815757274627686,0.9850875735282898,0.9558595418930054,0.0,accept,unanimous_agreement
141352763,195,all comments addressed. there is no return statement in the code now.,0,0,0,0.9853803515434264,0.9838111996650696,0.9955198764801024,0.0,accept,unanimous_agreement
142060970,195,-brahmbhatt i did a pr to your branch ([a link] with some suggested code readability improvements. if you agree you can merge it to your branch.,0,0,0,0.9661359786987304,0.8140340447425842,0.96531480550766,0.0,accept,unanimous_agreement
142071503,195,changes to zknodechangenotificationlistener can not be accepted as wants the import to be local. other than that have incorporated your proposed changes.,0,0,0,0.983992040157318,0.9900596141815186,0.9953698515892028,0.0,accept,unanimous_agreement
142071956,195,"-brahmbhatt, thanks for incorporating the changes. my understanding is that wanted the import to be local if `javaconversions` was used. with `javaconverters`, this is not needed because you have the `asscala` and `asjava` to show you that a conversion is happening (unlike the `javaconversions` case where everything happens silently).",1,0,1,0.9217063188552856,0.5959308743476868,0.9452248811721802,1.0,accept,majority_agreement
142098566,195,"also, for javaconverters, it's ok to import it globally since we are using asscala explicitly.",0,0,0,0.9877947568893432,0.9922175407409668,0.993063747882843,0.0,accept,unanimous_agreement
142102372,195,all comments addressed.,0,0,0,0.981129825115204,0.987054407596588,0.9876030087471008,0.0,accept,unanimous_agreement
142148112,195,thanks a lot for the patch. lgtm.,1,1,1,0.9614427089691162,0.9741387963294984,0.967147171497345,1.0,accept,unanimous_agreement
142148924,195,thanks parth!,1,1,1,0.9782283902168274,0.9699482321739196,0.9744465351104736,1.0,accept,unanimous_agreement
297201690,2910,"i realized that the relationship between the lso and the high watermark is trickier than i originally thought. i had assumed that it was sufficient to take the minimum of the two as the maximum offset that a client could fetch in read_committed mode. the issue is when the commit or abort marker itself is larger than the high watermark. we need to ensure that none of the records from the associated transaction becomes readable until the marker itself is replicated (even if the records have offsets lower than the high watermark). this requires a little more bookkeeping as we need to keep track of which transactions have been completed, but are not replicated. i'll update the patch as soon as i have a solution ready.",0,0,0,0.8013181686401367,0.972248077392578,0.96599143743515,0.0,accept,unanimous_agreement
297508059,2910,"note to self: once the transactional client patch lands, we should update `fetcher` to use the control sequence number when finding the commit/abort markers.",0,0,0,0.9882611632347108,0.9930843114852904,0.9911346435546876,0.0,accept,unanimous_agreement
298828817,2910,"i've added a commit to do transaction index recovery. the approach i've taken is the following: 1. i've removed the periodic pid snapshot. 2. instead, when the log rolls, i take a new snapshot using the offset of the new segment. 3. when a segment has been flushed to disk, i remove the corresponding snapshot. the snapshot corresponding the start of the active segment is never removed. 4. when recovering a segment, i have to read the log from the starting segment in the worst case in order to rebuild the transaction index. once finished rebuilding an individual segment, i take another snapshot so that we don't have to repeat the same work if we have to recover another segment. to make this efficient, i ensure that logs are loaded/recovered in ascending order. 5. when the log is cleanly closed, i take another snapshot. i was a bit unsure of the benefit of this. if we can always or usually expect to truncate upon startup, then perhaps the benefit is low and we need to revisit periodic pid expiration to reduce the chance that we have to scan the last segment upon initialization.",0,0,0,0.9706624746322632,0.9894362688064576,0.9499669671058656,0.0,accept,unanimous_agreement
299591718,2910,: thanks for the patch. lgtm.,1,1,1,0.9204868078231812,0.9083284735679626,0.9669569730758668,1.0,accept,unanimous_agreement
299591871,2910,i will let you merge this after running the system tests.,0,0,0,0.986444056034088,0.9839152097702026,0.9948325157165528,0.0,accept,unanimous_agreement
299648323,2910,thanks for the review cycles. system tests are looking good: [a link] i am going to look at a couple of the build failures above before merging to see if they could be related.,1,1,1,0.9759712815284728,0.9918910264968872,0.9901898503303528,1.0,accept,unanimous_agreement
299653658,2910,"it looks like the recent transient failures are known issues: [a link] [a link] [a link] in one of the previous builds, i saw a failure in kafka.server.logrecoverytest.testhwcheckpointwithfailuressinglelogsegment. this could be related to this patch and there is no active jira, so i'm trying to reproduce it locally.",0,0,0,0.956259787082672,0.9432564377784728,0.98723304271698,0.0,accept,unanimous_agreement
299658852,2910,i wasn't able to reproduce the failure in `logrecoverytest` locally and it didn't occur in the last few builds. i went ahead and increased the timeout that failed just in case. i'm going to go ahead and merge this to trunk.,0,0,0,0.9638450145721436,0.96706223487854,0.9884838461875916,0.0,accept,unanimous_agreement
843289802,10579,this pr is ready for review. pl let me know your comments.,0,0,0,0.8603757619857788,0.6161714792251587,0.8898937106132507,0.0,accept,unanimous_agreement
851023730,10579,thanks for your comment. addressed most of them with the latest commit.,1,1,1,0.8114087581634521,0.9343293309211732,0.8898277282714844,1.0,accept,unanimous_agreement
859400725,10579,thanks for the review comments. addressed them with the latest commits.,1,1,1,0.8044813871383667,0.8292747735977173,0.8917972445487976,1.0,accept,unanimous_agreement
868083433,10579,thanks for the review. addressed them with the latest commit [a link],1,1,1,0.8596553206443787,0.9309231638908386,0.8696068525314331,1.0,accept,unanimous_agreement
874000725,10579,thanks for your comments. addressed them with the latest commit.,1,1,1,0.774107038974762,0.8235024213790894,0.8340033292770386,1.0,accept,unanimous_agreement
875750234,10579,thanks for the comments. pl take a look at my inline replies and the latest commit.,1,1,1,0.727581799030304,0.7292237877845764,0.907971978187561,1.0,accept,unanimous_agreement
881556895,10579,"thanks for your latest comments, replied to them inline.",1,1,1,0.5302136540412903,0.4985027611255646,0.7202057242393494,1.0,accept,unanimous_agreement
881805037,10579,thanks for the comment. i addressed it with the latest commit.,1,1,1,0.835313081741333,0.896706223487854,0.8819029331207275,1.0,accept,unanimous_agreement
308242425,3325,"ping for initial review this still need more test coverage, but i wanted to submit the pr to move the discussion forward. edit: the batch restoration benchmark will undergo some refactoring as well. edit round 2: batch restoration benchmark will be in a follow in pr",0,0,0,0.9461237192153932,0.9715994596481324,0.9819427132606506,0.0,accept,unanimous_agreement
308246447,3325,fixing checkstyle errors,0,0,0,0.9165207147598268,0.8783847689628601,0.9920754432678224,0.0,accept,unanimous_agreement
308258160,3325,addressing errors,0,0,-1,0.9027161598205566,0.9185022115707396,0.9250874519348145,0.0,accept,majority_agreement
308445355,3325,error seems unrelated to this pr `execution failed for task ':core:compilescala'.` can't reproduce locally either.,0,0,0,0.9228794574737548,0.9689526557922364,0.9750738143920898,0.0,accept,unanimous_agreement
308464132,3325,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
308476305,3325,"i think the restore callbacks etc look good. my main question will be around the bulk loading part, i.e., if we go with batch loading, will we still be able to look into kafka-4868 or are they mutually exclusive? thanks .",1,1,1,0.9175387620925904,0.9595289826393129,0.9930642247200012,1.0,accept,unanimous_agreement
308483354,3325,current plan is to have kafka-4868 be part of the bulk/batch loading in this pr,0,0,0,0.9710483551025392,0.9950862526893616,0.9919405579566956,0.0,accept,unanimous_agreement
308511600,3325,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
309618705,3325,- updated code to handle kafka-4868 edit: still owe tests on this pr,0,0,0,0.9312840104103088,0.9933412671089172,0.9864383935928344,0.0,accept,unanimous_agreement
312245974,3325,still owe tests,0,0,0,0.9810991883277892,0.8467161059379578,0.5402585864067078,0.0,accept,unanimous_agreement
316110967,3325,updated per comments - added javadoc and tests. cc\,0,0,0,0.982362687587738,0.9837979674339294,0.9830831289291382,0.0,accept,unanimous_agreement
316818632,3325,updated per comments \cc,0,0,0,0.9832959771156312,0.7257668375968933,0.9902410507202148,0.0,accept,unanimous_agreement
317077171,3325,good point. let's add the check that users can only call this before calling start() then (and hence calling it whatever times are fine then). similarly for `kafkastreams#setstatelistener` and `#setuncaughtexceptionhandler` let's do the same enforcement.,1,0,1,0.8751762509346008,0.6251941919326782,0.8370659351348877,1.0,accept,majority_agreement
317484949,3325,"agreed, but as this pr is large enough i'll make those changes in a follow-up pr.",0,0,0,0.9801913499832152,0.9808293581008912,0.9823049306869508,0.0,accept,unanimous_agreement
317509633,3325,updates per comments - i believe i've addressed all issues.,0,0,0,0.7140976190567017,0.9685004949569702,0.9287193417549132,0.0,accept,unanimous_agreement
317530363,3325,"that is what i meant :) not needed necessarily for this pr, just to make sure we do not forget.",1,1,1,0.976357400417328,0.9956480860710144,0.9933854937553406,1.0,accept,unanimous_agreement
318182744,3325,rebasing now,0,0,0,0.988139510154724,0.9739130139350892,0.982344925403595,0.0,accept,unanimous_agreement
318187014,3325,rebased and comments addressed,0,0,0,0.986879289150238,0.9839653968811036,0.9792408347129822,0.0,accept,unanimous_agreement
318470546,3325,ping for final review and merge,0,0,0,0.9871456623077391,0.985885500907898,0.9664567708969116,0.0,accept,unanimous_agreement
318514748,3325,updates per comments,0,0,0,0.9843404293060304,0.9614431858062744,0.9529802799224854,0.0,accept,unanimous_agreement
318729754,3325,merged to trunk. thanks !,1,1,1,0.974199652671814,0.9949892163276672,0.9930251836776732,1.0,accept,unanimous_agreement
797234253,10218,"thanks for the review comments, addressed with the latest commits/comments.",1,1,1,0.5911840796470642,0.6206552386283875,0.8744174838066101,1.0,accept,unanimous_agreement
806359943,10218,"i discussed the proposed changes in the call on 23rd, and i mentioned that pr is not updated with those changes. i will let you know once those changes are pushed into this pr.",0,0,0,0.9827353358268738,0.9763563871383668,0.9916529059410096,0.0,accept,unanimous_agreement
812572844,10218,"thanks for your review and comments. in the latest commit, addressed the review comments. i have also refactored the code with better abstractions.",1,1,1,0.9323583245277404,0.9390212297439576,0.9711416363716124,1.0,accept,unanimous_agreement
813897020,10218,thanks for the review. addressed them with the latest [a link].,1,1,1,0.8523057699203491,0.859531581401825,0.8612717986106873,1.0,accept,unanimous_agreement
814275837,10218,i renamed `remote-storage` module to `storage` module as you suggested in the commit [a link].,0,0,0,0.9889333248138428,0.9895776510238647,0.9949954748153688,0.0,accept,unanimous_agreement
815062947,10218,"this pr is on top of [a link] . so, [a link] be merged before merging this pr.",0,0,0,0.9888471364974976,0.992902934551239,0.9946130514144896,0.0,accept,unanimous_agreement
815513359,10218,thanks for your comments. replied on comments and addressed them with the [a link] commit.,1,1,1,0.7668206691741943,0.7643484473228455,0.9298732876777648,1.0,accept,unanimous_agreement
816736436,10218,thanks for the comments. addressed them with the [a link].,1,1,1,0.8128100633621216,0.7352902889251709,0.8492908477783203,1.0,accept,unanimous_agreement
817018916,10218,"thanks for the comment, addressed with the commit [a link].",1,1,1,0.6061121225357056,0.5608179569244385,0.7369822859764099,1.0,accept,unanimous_agreement
817452410,10218,it looks like failures are not related to this pr.,0,0,0,0.6220375299453735,0.9470931887626648,0.9847990870475768,0.0,accept,unanimous_agreement
1732507180,14432,", do you think it is ok to have this small pr focussed only on static member departure? i plan to create such small prs for other cases when a static member joins etc. plz let me know if you prefer this approach or a single pr with all changes. thanks!",1,1,1,0.9674974083900452,0.9914133548736572,0.9891833066940308,1.0,accept,unanimous_agreement
1732788357,14432,test failures seem unrelated.,0,0,0,0.7590067386627197,0.7688288688659668,0.8597891926765442,0.0,accept,unanimous_agreement
1736982966,14432,"thanks for the review david! responses inline ack. thanks for the confirmation. sorry about the confusion. i have corrected it. got it. i have reverted to using member epoch -2 for static member leave request. i have updated the member epoch to -2 if the departing worker is a static member. i am not sure if that will be enough though because i am not returning any corresponding results to be written to the topic. all that's going to happen is that we will complete this current request with a result having member epoch id equal to -2. also, the meaning of `member releasing the instance id` is not clear to me.",-1,-1,1,0.9031038284301758,0.832365870475769,0.9832764863967896,-1.0,accept,majority_agreement
1737732772,14432,thanks for the review ! i have addressed the comments.,1,1,1,0.976020574569702,0.9779056906700134,0.9794411659240724,1.0,accept,unanimous_agreement
1739714081,14432,"i realised that a logic similar to [a link] is missing in this pr. i think that's important. to get this working though, i would also need to add the logic to add a different map for staticmembers and also support adding a static member to the group. in that case, i might need to enhance this pr to support both leave request and join request.",0,0,0,0.8175716400146484,0.987700581550598,0.8638477921485901,0.0,accept,unanimous_agreement
1747026910,14432,i updated this pr to handle this case and to also handle (re)-join and leaving the group using consumergroupheartbeat api.,0,0,0,0.9851574897766112,0.9932655096054076,0.9936886429786682,0.0,accept,unanimous_agreement
1747117455,14432,thanks ! i will get to this pr soon.,1,1,1,0.9862738251686096,0.9458083510398864,0.9917961359024048,1.0,accept,unanimous_agreement
1747157933,14432,thank you. i am still coming to terms with the new group coordinator so there might be rough edges in this pr (upfront apologies for that ),1,1,1,0.8494399785995483,0.9597731232643129,0.8911274671554565,1.0,accept,unanimous_agreement
1757221757,14432,"thanks , i have addressed the comments.",1,1,1,0.5301303863525391,0.8093758821487427,0.7056897878646851,1.0,accept,unanimous_agreement
1767770461,14432,"thanks for the review , most of my changes are on my local, but before i push those, wanted your thoughts on [a link] and [a link] comment. i can proceed with the rest of the changes post that.",1,1,1,0.8560516238212585,0.974065601825714,0.939523220062256,1.0,accept,unanimous_agreement
1767897337,14432,thanks. i just replied to your questions.,1,1,1,0.7547841668128967,0.8085564374923706,0.9658553004264832,1.0,accept,unanimous_agreement
1777654947,14432,"thanks , i have addressed all the comments. have added a couple of more questions. thanks for all the pointers, the changes look cleaner than the previous iteration (to me atleast).",1,1,1,0.8835158348083496,0.982635259628296,0.987245500087738,1.0,accept,unanimous_agreement
1795371595,14432,"thanks , i have addressed the review comments. please let me know how the changes are looking now.",1,1,1,0.6926948428153992,0.886259913444519,0.9180700182914734,1.0,accept,unanimous_agreement
1824047001,14432,"thanks . i have addressed all the comments. the second part of [a link] comment, i am not sure which new data structures being referred here. the test `teststaticmembergetsbackassignmentuponrejoin` asserts all the records returned after replay. is that what you are referring to?",1,1,1,0.914261281490326,0.9862448573112488,0.9410712122917176,1.0,accept,unanimous_agreement
1827266684,14432,", thank you for another round of review. i have handled all review comments.",1,1,1,0.9061903953552246,0.983744502067566,0.9675496220588684,1.0,accept,unanimous_agreement
692859976,9100,"recent changes include: * controller no longer sends leaderandisr, only updatemetadata (more more leader epoch bump) * update partition's isr from alterisr response * periodic scheduled thread for calling ""propagateisrchanges"" in alterisrmanager (we could miss updates otherwise) * top-level error handling in alterisrmanager * additional checks in alterisrmanager to prevent multiple in-flight requests and also prevent multiple isr changes for a given partition * changes to the callback logic to ensure we don't leave any partitions stuck",0,0,0,0.9788060784339904,0.9940179586410522,0.9869606494903564,0.0,accept,unanimous_agreement
697953307,9100,"yea, good catch. this works today using a zk watch on the partition ""/state"" znode which is still getting triggered with this pr. we can modify the new isr update path to explicitly call `onpartitionreassignment` after writing out the isr. how about we save this as a follow-on?",1,1,1,0.97667795419693,0.9037874341011048,0.791850209236145,1.0,accept,unanimous_agreement
698458937,9100,"yeah, i'm ok leaving that for [a link]",0,0,0,0.9712273478507996,0.9738348126411438,0.934104859828949,0.0,accept,unanimous_agreement
698637507,9100,i think the failing tests are known flakes and should be fixed by [a link],0,0,0,0.9843246936798096,0.964088261127472,0.9864540696144104,0.0,accept,unanimous_agreement
699438281,9100,"i got a thought, there is a scenario that a leader can see its followers, but cannot see zookeeper, and then the leader will be fenced when it attempts to shink isr or expand isr because it holds the leaderisrupdatelock, but now the leader can't be fenced because it just sends the message and will process message normally. when the ack=1, it will continue processing the msg, and will be lost. should we reject process the msg when the alterisr find the broker is not in livebrokeridandepochs? [a link]",0,0,0,0.9579620361328124,0.9538267850875854,0.9461112022399902,0.0,accept,unanimous_agreement
729316632,9100,"i missed this comment before. it's a good question. in general, the leader will continue in its current state as long as possible. as you say, as soon as it needs to shrink/expand the isr, it grabs the leaderandisr update and attempts to synchronously update the state. if zookeeper can't be reached, then the thread gets stuck. eventually this causes the broker to effectively deadlock, which has the side effect of preventing any produce requests (and any other requests) from getting through. i think it's a fair point that this affords some protection for acks=1 requests, but i think we tend to view the side effect of deadlocking the broker as worse than any benefit. in kip-500, we have an alternative approach for self-fencing. the analogous case is when the leader cannot reach the controller. we use a heartbeating mechanism to maintain liveness in the cluster. unlike with zookeeper, we do not rely on the session expiration event in order to tell that a broker has been declared dead. instead if we do not get a heartbeat response from the controller before some timeout, then we will stop accepting produce requests. i have been thinking a little bit about your suggestion to self-fence after getting an invalid version error from alterisr. it might help in the interim before kip-500 is complete. i think our expectation here was that if we get an invalid version error, then the leaderandisr with the updated state should soon be on the way. i suppose we could come up with reasons why that assumption might fail, so it might make sense to be a little more defensive. i will file a jira about this and we can see what others think. thanks for the suggestion!",1,1,1,0.6154659986495972,0.6880094408988953,0.8809946179389954,1.0,accept,unanimous_agreement
729349575,9100,"thanks for your reply, i also have read kip-500 and other kip-631, it's good about the fence. but it will be released in a few months, before that, i think we also need to try the best to fence the broker when the controller already think the broker has died. in other words, we should fence 2-way. ` self-fence after getting an invalid version error from alterisr ` yes, i think we need self-fence when the session is lost, we can't rely on receiving the other machine’s response because we can't receive the response when the broker2controllerchannel is broken. please let me know if you create a jira.",1,1,1,0.856338381767273,0.9199118614196776,0.9697707295417786,1.0,accept,unanimous_agreement
276844638,2472,good to see some progress on kip-4! one question - does this allow new topics to be created with the cluster defaults for number of partitions and replication factor? having to have a priori knowledge of those configs on the client is a big pain point for us using adminutils and i thought the elimination of that requirement was a goal.,1,1,1,0.9711857438087464,0.9907227158546448,0.9876130223274232,1.0,accept,unanimous_agreement
277339758,2472,"hi , thanks for taking a look! be sure to also check out the discussion thread here: [a link] unfortunately, as far as i know, there is no way to allow the server to use a default replication factor when making a createtopicsrequest. the rpc must have either a manual partitioning assignment, or a number of replicas and number of partitions specified. i think supporting server-side defaults would be a nice improvement, but it would require a separate kip to implement.",1,1,1,0.9756751656532288,0.9566938281059264,0.9906099438667296,1.0,accept,unanimous_agreement
298144387,2472,3 test failures: [code block],0,0,0,0.8796449899673462,0.9923244714736938,0.9847436547279358,0.0,accept,unanimous_agreement
298358441,2472,"i fixed the test failure and some trivial clean-ups: [a link] system tests passed: [a link] i think we can merge this after the pr above is merged. i will file a jira with additional clean-ups that i noticed, but those can be done later.",0,0,0,0.9797263145446776,0.9818012714385986,0.9789156317710876,0.0,accept,unanimous_agreement
298366874,2472,"thanks, . the cleanups look good to me.",1,1,1,0.9682998061180116,0.9905783534049988,0.9859752058982848,1.0,accept,unanimous_agreement
594313472,8218,could we have a summary for this pr?,0,0,0,0.9870827198028564,0.9909743666648864,0.9945647716522216,0.0,accept,unanimous_agreement
594314018,8218,"the summary is not longer than the pr title. we make `taskmanager` the class that is responsible for committing (to allow to commit all tasks at once, instead of each task individually)",0,0,0,0.9842718243598938,0.9920331835746764,0.99504292011261,0.0,accept,unanimous_agreement
594706982,8218,"the title sounds pretty vague to me. the description could at least include what the committing behavior look like under `taskmanager`, what's the motivation, etc as we are already overloading jira 9441. in general we could try to be more specific about the changes in the pr description as i could see you are also adding upgrade flags inside this pr. major side effects should be better to document at first imho.",0,0,0,0.9075575470924376,0.783443033695221,0.7723603844642639,0.0,accept,unanimous_agreement
598504502,8218,thanks for the rebasing ! will take another look soon.,1,1,1,0.9518861174583436,0.9705640077590942,0.9816675186157228,1.0,accept,unanimous_agreement
599368368,8218,"rebased to resolve merge conflicts. addressed more review comments and added more tests. also added a fix (from [a link] as `trunk` is broken atm, to make jenkins pass. triggered systems tests: [a link]",0,0,0,0.9801479578018188,0.9874790906906128,0.9920929074287416,0.0,accept,unanimous_agreement
599779177,8218,"i linked the system test failures i saw on the meeting notes for this week, but just to clarify: 1) two failing consistently: `streamseostest.test_failure_and_recovery{_complex}` 2) one flaky: `streamsstandbytask.test_standby_tasks_rebalance` anything outside of that is a new failure",0,0,0,0.8792682886123657,0.9871509075164796,0.9891629219055176,0.0,accept,unanimous_agreement
599794804,8218,"thanks for pointing out. in the run i triggered, the following failed: - `streamscooperativerebalanceupgradetest.test_upgrade_to_cooperative_rebalance` - `streamsupgradetest.test_metadata_upgrade` - `streamseostest.test_failure_and_recovery` - `streamseostest.test_failure_and_recovery_complex` - `streamseostest.test_rebalance_complex` - `streamseostest.test_rebalance_simple` not sure if this is any helpful to have a safety net for this pr...",1,1,1,0.7411208152770996,0.7406190633773804,0.9627500772476196,1.0,accept,unanimous_agreement
599816765,8218,address review comments and rebased to pick up fixed from `trunk`.,0,0,0,0.9887012243270874,0.9914885759353638,0.9957412481307985,0.0,accept,unanimous_agreement
600345056,8218,addressed review comments and fix bug exposed by system tests. also added more tests. new system test run: [a link],0,0,0,0.9849581718444824,0.9887371063232422,0.968181610107422,0.0,accept,unanimous_agreement
600402163,8218,lgtm. please feel free to merge after green builds.,0,0,0,0.9072409868240356,0.9451000690460204,0.9878312945365906,0.0,accept,unanimous_agreement
600417507,8218,the failed test seems to be just flaky org.apache.kafka.streams.processor.internals.statedirectorytest.shouldreturnemptyarrayiflistfilesreturnsnull,0,0,0,0.9706948399543762,0.9540615081787108,0.9670639038085938,0.0,accept,unanimous_agreement
600422344,8218,"actually the same test failed in both previous runs -- but it passes locally for me. as this pr has nothing to do with the failing test, i might just want go ahead and merge. the last pr that changed the failing test was merged recently: [a link] there is one concerning comment on that pr: `merged to trunk and 2.5 after tests green locally.` ? seems the nightly jenkins job also fails on this test: - - `trunk` -> [a link] - `2.5` -> [a link] did this pr break the jenkins job? \cc",0,0,0,0.9136087894439696,0.9524385333061218,0.9749294519424438,0.0,accept,unanimous_agreement
600424167,8218,"my understanding is that this pr will fix the problem [a link] you could choose to merge it first if looks good, and rebase the current one",0,0,0,0.7793717384338379,0.939682960510254,0.9904235005378724,0.0,accept,unanimous_agreement
600432582,8218,i just double checked and the failure in trunk is fixed as [a link] while the failure (a different one) in 2.5 is also fixed by 's pr. we should be fine now.,0,0,0,0.92225980758667,0.8726827502250671,0.8313895463943481,0.0,accept,unanimous_agreement
600432616,8218,test this please,0,0,0,0.9859992861747742,0.9801037311553956,0.9870431423187256,0.0,accept,unanimous_agreement
600799765,8218,system test failure `streamseostest` (4 different test as mentioned by sophie above). given that some more fixed got pushed to `trunk` i rebased. this should give us a green jenkins and we should be able to merge.,0,0,0,0.985995352268219,0.8800359964370728,0.9804850816726683,0.0,accept,unanimous_agreement
600815165,8218,only two `streamseostest` system tests were failing when i ran them. `test_rebalance_complex` and `test_rebalance_simple` are either newly failing or due to this pr,0,0,0,0.9510443806648254,0.9935081005096436,0.9904283285140992,0.0,accept,unanimous_agreement
600815624,8218,"also, the two already-failing tests (`test_failure_and_recovery`) failed with a different error than they were trunk: [code block]",0,0,0,0.9835290908813475,0.992699921131134,0.9927856922149658,0.0,accept,unanimous_agreement
601059160,8218,thanks for pointing it out! pushed a fix and updated the unit test accordingly. \cc new system test run: [a link],1,1,1,0.9744610786437988,0.992460310459137,0.985331654548645,1.0,accept,unanimous_agreement
601318837,8218,`streamseostest.test_failure_and_recovery` and `streamseostest.test_failure_and_recovery_complex` failed. i only see: [code block] followed by [code block] seems to be unrelated to this pr. are we good to merge?,0,0,0,0.9836386442184448,0.9926360249519348,0.9876420497894288,0.0,accept,unanimous_agreement
601320091,8218,we should be good. i have another pr to fix those [a link],1,1,0,0.9160879254341124,0.8150072693824768,0.5943370461463928,1.0,accept,majority_agreement
198455269,1095,"it would be nice to get this into 0.10. it should be the only required changes for existing protocol messages for kip-4, which would simplify a backport into 0.10.1 when the rest is agreed upon and ready. it also adds rack data to the metadata response which goes along with kip-36 that was added in this release.",0,0,1,0.953820824623108,0.9737299084663392,0.5197836756706238,0.0,accept,majority_agreement
198461416,1095,", i agree that it would be great to get this in if there is agreement on the protocol changes. i haven't reviewed it in detail yet, but 2 things: - the pr needs to be rebased - i think it would be great to make it possible to ask for an update with `0` topics in cases where we just want to get the updated cluster data. people have run into performance issues when clients have 0 topics, but they ask for _all_ topics because the protocol only supports that.",1,0,1,0.5071551203727722,0.8909308910369873,0.7347226738929749,1.0,accept,majority_agreement
198463871,1095,"i will rebase and look at supporting the idea of `null` (versus empty list) indicating that you don't want to request topic metadata at all. its a bit nuanced, but its the most compatible and has fewer edge cases. worst case scenario if someone passes an empty list is performance. this would help solve [a link].",0,0,0,0.9727091789245604,0.9446998834609984,0.9857416152954102,0.0,accept,unanimous_agreement
198519711,1095,"i rebased and added the ""no topics"" functionality.",0,0,0,0.9835504293441772,0.9888902902603148,0.9928472638130188,0.0,accept,unanimous_agreement
198557490,1095,"sorry, noticed a bit late that this pr includes protocol changes that didn't go through a community discussion. lets make sure there are 3 committers who are ok with the protocol modification?",-1,-1,-1,0.9813829064369202,0.9856135249137878,0.9792112708091736,-1.0,accept,unanimous_agreement
208938334,1095,this patch represents the current vote thread for the kip-4 metadata changes. please feel free to review at your convenience.,0,0,0,0.9056551456451416,0.986030340194702,0.9857418537139891,0.0,accept,unanimous_agreement
212985925,1095,pinging for review. the kip-4 metadata changes vote passed and i rebased on trunk.,0,0,0,0.9864217042922974,0.9855535626411438,0.9913777112960817,0.0,accept,unanimous_agreement
213592590,1095,"thanks for the pr. i left a few minor comments, looks good otherwise. i think one thing worth paying special attention to is the `errorunavailableendpoints` change, but it seems like gwen has already done that.",1,1,1,0.971201479434967,0.990294337272644,0.9868033528327942,1.0,accept,unanimous_agreement
214567207,1095,"i updated the patch based on the reviews. i added more tests, and some static factory methods. most notably i changed from using and alltopics boolean to using null and a static method. it required a little bit of re-work on internal apis but looks reasonable to me. if you think the alltopics boolean is better don't hesitate to weigh in. the specific commit with that change is here: [a link]",0,0,1,0.950788915157318,0.7953876852989197,0.656021773815155,0.0,accept,majority_agreement
214889420,1095,"thanks . aside from a few minor comments, lgtm. i started a system tests build here: [a link]",1,1,1,0.965078055858612,0.9942976832389832,0.9753129482269288,1.0,accept,unanimous_agreement
214915320,1095,i updated the patch based on your feedback,0,0,0,0.9795302748680116,0.9847365021705629,0.9927446842193604,0.0,accept,unanimous_agreement
214920024,1095,"thanks , lgtm. , do you want to take a look as well?",1,1,1,0.8353587985038757,0.9479649066925048,0.7982397079467773,1.0,accept,unanimous_agreement
214924898,1095,looks perfect. i was waiting for you to finish nitpicking tests ;),1,1,1,0.9911192059516908,0.9954710006713868,0.9957881569862366,1.0,accept,unanimous_agreement
548594725,7629,ping and for review,0,0,0,0.982895016670227,0.9658663272857666,0.9892039895057678,0.0,accept,unanimous_agreement
548594856,7629,"i still need to test this out with a local deployment of the docs, but i wanted to get feedback on the content sooner.",0,0,0,0.9645704627037048,0.9780108332633972,0.9796810150146484,0.0,accept,unanimous_agreement
548596297,7629,\cc and,0,1,0,0.9779776334762572,0.9072401523590088,0.9893174767494202,0.0,accept,majority_agreement
548596456,7629,i'll also do a follow-up pr to `kafka-site` for the `2.4` folder if necessary.,0,0,0,0.9856610298156738,0.9934739470481871,0.9945275187492372,0.0,accept,unanimous_agreement
549929236,7629,rebased and updated per comments,0,0,0,0.985524833202362,0.983126699924469,0.9893052577972412,0.0,accept,unanimous_agreement
552029806,7629,updated per comments,0,0,0,0.9792535305023192,0.9720098376274108,0.9704824686050416,0.0,accept,unanimous_agreement
553465499,7629,merged #7629 into trunk.,0,0,0,0.9866529107093812,0.9944460391998292,0.993033766746521,0.0,accept,unanimous_agreement
622095405,8589,retest this please. call for review,0,0,0,0.9795029163360596,0.9820591807365416,0.9901093244552612,0.0,accept,unanimous_agreement
622372958,8589,"oh, sorry i didn't run the checkstyle and spotbugs quality checks locally, i will update shortly with these fixed",-1,-1,-1,0.9887551069259644,0.9863015413284302,0.9911350011825562,-1.0,accept,unanimous_agreement
622417550,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
622614438,8589,only committers can trigger jenkins retesting... retest this please.,0,0,0,0.9875166416168212,0.9829447865486144,0.9503278732299804,0.0,accept,unanimous_agreement
622661261,8589,"i see, thanks! and call for review :)",1,1,1,0.9926109910011292,0.996081531047821,0.9913812279701232,1.0,accept,unanimous_agreement
624403133,8589,"thanks a lot for the review, will update soon.",1,1,1,0.923408269882202,0.9457132816314696,0.9374046921730042,1.0,accept,unanimous_agreement
625008508,8589,"hey, updated based on comments, and also left some comments there, thanks.",1,1,1,0.889775812625885,0.8898155689239502,0.9301014542579652,1.0,accept,unanimous_agreement
626141315,8589,"call for retest and review, thanks!",1,1,1,0.9692516922950744,0.9916468858718872,0.935257852077484,1.0,accept,unanimous_agreement
627772018,8589,sure thing!,1,1,0,0.8838678002357483,0.7710263133049011,0.8992865681648254,1.0,accept,majority_agreement
629055884,8589,sorry for causing too much trouble about style....will make sure style check correctly executed before requesting for review next time.,-1,-1,-1,0.9881474375724792,0.9914608001708984,0.9913649559020996,-1.0,accept,unanimous_agreement
629591570,8589,no worry! it's just for first time :),1,1,1,0.9931455850601196,0.9939423203468324,0.9946228265762328,1.0,accept,unanimous_agreement
630714942,8589,"i'm fixing the style error, and i found that it showed success if i run the below cmd locally : ` ./gradlew checkstylemain checkstyletest spotbugsmain spotbugstest spotbugsscoverage compiletestjava ` i'm didn't find the reason after some investigating, but the style check cmdline can capture style error if i cherry-pick the first commit of this pr to another branch, guessing that it might be the second commit: `merge trunk` somehow made the style check doesn't work... so would it break our convention if i revert to the first commit:`add option to force delete active members in streamsresetter` and fix style error from there ? this may change the commit history of this pr. i'm asking because i noticed :""please address feedback via additional commits instead of amending existing commits."" on [a link] thanks!",0,0,0,0.7842145562171936,0.9296311140060424,0.7735111713409424,0.0,accept,unanimous_agreement
630918452,8589,"thanks for the context. i don't worry too much for comment vanish if you change the commit history, as they would not be gone but just show as `outdated` on github. just do whatever you feel makes sense. also just a reminder that we are one week away from the feature freeze, so let's try to ramp up and get this into 2.6.",1,1,1,0.9520661234855652,0.9743748307228088,0.9639580845832824,1.0,accept,unanimous_agreement
631299671,8589,"thanks, will try my best to get this into 2.6",1,1,1,0.9275893568992616,0.828162670135498,0.9094132781028748,1.0,accept,unanimous_agreement
632027791,8589,"updated, call for review, thanks!",1,1,1,0.9685062170028688,0.984022855758667,0.965642273426056,1.0,accept,unanimous_agreement
632246270,8589,"thanks so much for the timely and detailed comments, i will update soon.",1,1,1,0.9678463339805604,0.9294182062149048,0.9858705401420592,1.0,accept,unanimous_agreement
632722266,8589,"updated and req for review agian.. really appreciated your help to pick out so much style violations, also wondering if we can use some format tool like `scalafmt` to automatic format~",1,1,1,0.9842262864112854,0.995676577091217,0.992735743522644,1.0,accept,unanimous_agreement
632974316,8589,"updated, thanks!",1,1,1,0.9502794742584229,0.9693486094474792,0.9716637134552002,1.0,accept,unanimous_agreement
632987873,8589,"ahh, sorry, checking, will fix soon",-1,-1,-1,0.9858997464179992,0.9910695552825928,0.9942724704742432,-1.0,accept,unanimous_agreement
632991430,8589,"just fixed, thanks!",1,1,1,0.957025706768036,0.9890486001968384,0.9858607053756714,1.0,accept,unanimous_agreement
633084014,8589,"thanks very much for the review and comments! wonder if it is still possible to get this into 2.6~ also call for retest, thanks!",1,1,1,0.9890303015708924,0.9925663471221924,0.9943863153457642,1.0,accept,unanimous_agreement
633667548,8589,test this,0,0,0,0.982772171497345,0.9692401885986328,0.988132655620575,0.0,accept,unanimous_agreement
633733992,8589,only known flaky eos tests are failing: [code block],0,0,0,0.8789674639701843,0.8760913610458374,0.9704392552375792,0.0,accept,unanimous_agreement
634846209,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
634846336,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
634855359,8589,thanks a lot for the review and tests triggering!,1,1,1,0.9825910925865172,0.9619086384773254,0.9831578135490416,1.0,accept,unanimous_agreement
634941215,8589,java 8: [code block] java 11: [code block] java 14: [code block],0,0,0,0.9872056245803832,0.9887429475784302,0.9937356114387512,0.0,accept,unanimous_agreement
634941290,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
634941557,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
634943565,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
634944109,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
634944577,8589,jenkins does not cooperate... will try again later,-1,0,0,0.761583685874939,0.7684393525123596,0.9486453533172609,0.0,accept,majority_agreement
634954721,8589,retest this please.,0,0,0,0.983358919620514,0.9706496000289916,0.9739659428596495,0.0,accept,unanimous_agreement
635025385,8589,"finally, jenkins cooperated :)",1,1,1,0.9210296273231506,0.993739128112793,0.9612001180648804,1.0,accept,unanimous_agreement
635041171,8589,thanks for the kip and pr !,1,1,1,0.9569704532623292,0.9751691818237304,0.9605314135551452,1.0,accept,unanimous_agreement
635041609,8589,"great work, !",1,1,1,0.993474543094635,0.9956075549125672,0.9964998960494996,1.0,accept,unanimous_agreement
635293293,8589,"thanks a lot for your kindly review and help! are you guys using some formatting tool, i asked this because this pr had too many formatting issues, would be good if next time i could effectively avoid them, thanks!",1,1,1,0.9909244179725648,0.9925307631492616,0.9950072765350342,1.0,accept,unanimous_agreement
635335273,8589,"fyi, since we took a slightly different implementation(leveraging the empty members rather than introducing a new field to imply the `removeall` scenario), i updated the kip-571 accordingly to keep them consistent.",0,0,0,0.9847453236579896,0.9918179512023926,0.9925056099891664,0.0,accept,unanimous_agreement
635616335,8589,"i use intellij that does have some auto-formatting. use it basically with default settings. in doubt, disable auto-formatting to avoid unnecessary reformatting. -- some things are also a little bit of ""personal taste""... thanks! can you send a follow up email to the voting thread explaining the change? to make sure nobody has concerns about it.",1,1,1,0.921743392944336,0.9473165273666382,0.9705939888954164,1.0,accept,unanimous_agreement
636274254,8589,"sure, updated in the voting thread of kip-571",0,0,0,0.9834454655647278,0.9940284490585328,0.9918389916419984,0.0,accept,unanimous_agreement
402306389,5322,"another class that have some part of the logic is `defaultpartitiongrouper`, and in that impl we should not expect any [code block] any more. sorry the whole task assignment logic was scattered across multiple classes / functions, we just need to make sure we would clean up all the corner cases and do not introduce any regressions. for that we'd better add some integration test for this case as well.",-1,-1,0,0.9687776565551758,0.8830207586288452,0.8988016843795776,-1.0,accept,majority_agreement
402316622,5322,thanks for the comments. i am trying to accommodate meta comment first. please take a look at current form to see if i removed code which wouldn't be executed with the early check in place.,1,1,1,0.929630160331726,0.9444897770881652,0.951912522315979,1.0,accept,unanimous_agreement
402335251,5322,"currently i am looking at the following code (discovered thru shouldupdateclustermetadataandhostinfoonassignment) in onassignment(): [code block] to make the above check pass, receivedassignmentmetadataversion is downgraded to 3 if receivedassignmentmetadataversion is 4 and there is no error in the assignmentinfo.",0,0,0,0.9869440793991088,0.9945411086082458,0.9940394163131714,0.0,accept,unanimous_agreement
402379446,5322,defaultpartitiongroupertest#shouldnotcreateanytasksbecauseonetopichasunknownpartitions still fails. it tests against partitiongrouper where there is no relevant logic after the pr.,0,0,0,0.9624053239822388,0.9755321741104126,0.9912531971931458,0.0,accept,unanimous_agreement
403171070,5322,"about the version control semantics, please read kip-268 ([a link] for the detailed explanation. as for this scenario, if `i` am using the latest kafka version (2.1.0-snapshot) then `usedsubscriptionmetadataversion = subscriptioninfo.latest_supported_version` should be 4. in your pr you've only bumped up the latest_supported_version for assignmentinfo to 4, but not `subscriptioninfo.latest_supported_version`, which is why it's failing.",0,0,0,0.9705227017402648,0.994199514389038,0.991910755634308,0.0,accept,unanimous_agreement
403172175,5322,"when calling completeshutdown in streamsthread, what should be the value for cleanrun ? [code block] i assume it should be false.",0,0,0,0.984155774116516,0.9953880906105042,0.9934685230255128,0.0,accept,unanimous_agreement
403173358,5322,"we can just call `shutdown()` which will simply change the state to `pending_shutdown`, then in the streamthread's main loop `while (isrunning()) {` will exit and continue to `completeshutdown`.",0,0,0,0.9885671138763428,0.993002712726593,0.9932692646980286,0.0,accept,unanimous_agreement
403279407,5322,when i used the following command line: [code block] i was told: [code block] also tried the following command: [code block] [code block],0,0,0,0.984723687171936,0.9925526976585388,0.993633270263672,0.0,accept,unanimous_agreement
403366866,5322,"in the test output, e.g. [a link] , i don't see detailed log. how can i enable -i for the integration tests ?",0,0,0,0.98698228597641,0.952952802181244,0.9889700412750244,0.0,accept,unanimous_agreement
403634460,5322,i think this test should be modified as we should not expect this case any more.,0,0,0,0.9770303964614868,0.9738627076148988,0.9755034446716307,0.0,accept,unanimous_agreement
403634927,5322,"try replace `streams:test` with `streams:streams-scala:test`, note that the package hierarchy is defined as sub-scope in `build.gradle`.",0,0,0,0.9879756569862366,0.994934380054474,0.9942702054977416,0.0,accept,unanimous_agreement
403635159,5322,"you need to run your branch with log4j.properties updated, but i would not recommend that. i think running locally on a single test case that was failed is more effective.",0,0,0,0.9441593885421752,0.9806802272796632,0.9888513684272766,0.0,accept,unanimous_agreement
403742335,5322,i observed the following pattern for the timed out test testshouldcountclicksperregion : [code block] the above repeats till the timeout. here is how the additional log is added around line 495: [code block],0,0,0,0.9850711822509766,0.9919431209564208,0.993602216243744,0.0,accept,unanimous_agreement
403753271,5322,the timeout was caused by rebalancing (streams/streams-scala/logs/kafka-streams-scala.log): {code} 59150 [stream-table-join-scala-integration-test-0544d201-4cfa-41d9-846c-bd8efb7e94d6-streamthread-1] info org.apache.kafka.streams.processor.internals.streamthread - stream- thread [stream-table-join-scala-integration-test-0544d201-4cfa-41d9-846c-bd8efb7e94d6-streamthread-1] version probing detected. triggering new rebalance. {code} which is not observed in the output of successful run.,0,0,0,0.9856632351875304,0.9948329925537108,0.994971752166748,0.0,accept,unanimous_agreement
403757169,5322,"from streamthread: [code block] i added the following in streamspartitionassignor : [code block] in streams/streams-scala/logs/kafka-streams-scala.log , receivedassignmentmetadataversion was 3 while usedsubscriptionmetadataversion was 4.",0,0,0,0.9893361330032348,0.9945346117019652,0.9937082529067992,0.0,accept,unanimous_agreement
403768210,5322,"i looked at related code in streamspartitionassignor and assignmentinfo - it seems receivedassignmentmetadataversion was 3 because [code block] usedversion was 3, decoded from subscriptioninfo",0,0,0,0.98614501953125,0.9932071566581726,0.9926334023475648,0.0,accept,unanimous_agreement
403807709,5322,reassignpartitionsclustertest.shouldexecutethrottledreassignment was not related to the pr. i ran it locally with my changes and it passed.,0,0,0,0.9811262488365172,0.9897610545158386,0.9937173128128052,0.0,accept,unanimous_agreement
405027255,5322,"if i make the following change in streamtotablejoinscalaintegrationtestimplicitserdes.testshouldcountclicksperregion : [code block] in kafka-streams-scala.log , i observe: [code block] the test fails with: [code block]",0,0,0,0.9881674647331238,0.993432879447937,0.9917863011360168,0.0,accept,unanimous_agreement
405285036,5322,"by registering kafkastreams.statelistener, i was able to see the following transitions: [code block]",0,0,0,0.984029233455658,0.9805727005004884,0.9934566617012024,0.0,accept,unanimous_agreement
405289444,5322,"by registering streamthread.statelistener, i was able to see the following transitions: [code block]",0,0,0,0.9858266711235046,0.9769304990768432,0.9943234920501708,0.0,accept,unanimous_agreement
405577056,5322,i ran the two failed tests locally with my pr - they passed.,0,0,0,0.9663004875183104,0.9818890690803528,0.9913015961647034,0.0,accept,unanimous_agreement
405640617,5322,re-triggered system test [a link] for the newest changes.,0,0,0,0.9876783490180968,0.9892780780792236,0.9953586459159852,0.0,accept,unanimous_agreement
405725260,5322,"system test seems relevant, you can go to for more detailed logs: [a link] [a link] the failed one is this: [code block]",0,0,0,0.983079195022583,0.9788271188735962,0.9942492246627808,0.0,accept,unanimous_agreement
405734184,5322,[a link] gave me 404. there is no clickable link on report.html related to the failed test. the test failure probably is related to the new way of passing version probing parameter. should i restore the existing approach ?,0,0,0,0.979470670223236,0.96248596906662,0.9792527556419371,0.0,accept,unanimous_agreement
405737220,5322,"if you click on `detail` link (the last column) on the `streamsupgradetest` row (it is the only row in orange, meaning failed case), you'll be able to download the logs",0,0,0,0.9864369034767152,0.9910412430763244,0.9928112626075744,0.0,accept,unanimous_agreement
405740804,5322,the following timed out: [code block] trying to see why.,0,0,0,0.9810417890548706,0.9530001878738404,0.9808357954025269,0.0,accept,unanimous_agreement
405743077,5322,"this log, from output of streamsupgradetestjobrunnerservice-0-140702392667856/worker4, is probably the reason: [code block] meaning the actual version numbers are bumped from existing values.",0,0,0,0.9814221858978271,0.9955411553382874,0.9921769499778748,0.0,accept,unanimous_agreement
405743406,5322,i can bump the hardcoded version numbers in streams_upgrade_test.py is there better way of modifying streams_upgrade_test.py ?,0,0,0,0.9831088185310364,0.9941841959953308,0.9942646622657776,0.0,accept,unanimous_agreement
405749066,5322,"could you share some thoughts here? my understanding is that `test_version_probing_upgrade` is testing a ""future"" version which is relying on `streamsupgradetest.java` code, but am not 100% percent sure why bumping up the version to 4 now would break this test.",0,0,0,0.9369680881500244,0.979430615901947,0.9790542125701904,0.0,accept,unanimous_agreement
405750218,5322,"there're many streamsupgradetest.java in the codebase e.g. [code block] from [a link] , it was not immediately obvious which class led to the failure.",0,0,0,0.9779014587402344,0.973870038986206,0.9883949756622314,0.0,accept,unanimous_agreement
405752440,5322,from 46/streamsupgradetestjobrunnerservice-2-140702392665808/worker6 [code block],0,0,0,0.9885363578796388,0.9949207901954652,0.9942574501037598,0.0,accept,unanimous_agreement
405753197,5322,mind triggering another system test run ? thanks,1,1,1,0.9149050712585448,0.6968923807144165,0.5143125057220459,1.0,accept,unanimous_agreement
405758276,5322,"please make also sure, that we extend all existing test with regard to version probing, and supported assignment, subscription numbers (ie, `assignmentinfotest`, `subscriptioninfotest`, `streamspartitionassignortest`)",0,0,0,0.9823843240737916,0.9939051270484924,0.9942323565483092,0.0,accept,unanimous_agreement
405759633,5322,i have gone over the listed test classes above and seen new tests added.,0,0,0,0.984033703804016,0.9828051924705504,0.9934096932411194,0.0,accept,unanimous_agreement
405794452,5322,"from guozhang 11 days ago: we can just call shutdown() which will simply change the state to pending_shutdown, then in the streamthread's main loop [code block] will exit and continue to completeshutdown.",0,0,0,0.9857926964759828,0.9925332069396972,0.9927181005477904,0.0,accept,unanimous_agreement
405999388,5322,can you trigger another system test run ? thanks,1,1,1,0.9505182504653932,0.8241711854934692,0.8160760998725891,1.0,accept,unanimous_agreement
406054511,5322,re-triggered [a link],0,0,0,0.9889065027236938,0.990290343761444,0.9951795339584352,0.0,accept,unanimous_agreement
406090972,5322,it still fails: [a link] logs to download: [a link],0,0,0,0.9329602718353271,0.9678218960762024,0.9881382584571838,0.0,accept,unanimous_agreement
406095151,5322,from 46/streamsupgradetestjobrunnerservice-0-139790769699792/worker4 [code block] i will update streams_upgrade_test.py covering missed version numbers.,0,0,0,0.9878569841384888,0.9956496357917786,0.9952954649925232,0.0,accept,unanimous_agreement
406096393,5322,please trigger one more system test run. thanks,1,1,1,0.9661474227905272,0.8964287638664246,0.6903010010719299,1.0,accept,unanimous_agreement
406098237,5322,done: [a link],0,0,0,0.9865983724594116,0.9105059504508972,0.9949890971183776,0.0,accept,unanimous_agreement
406123254,5322,[a link] has passed.,0,0,0,0.983682096004486,0.9873113632202148,0.9953981041908264,0.0,accept,unanimous_agreement
406155515,5322,from the scala 2.12 test run console log: [code block],0,0,0,0.9892749190330504,0.9889299273490906,0.9951077699661256,0.0,accept,unanimous_agreement
406403161,5322,"w.r.t. integration test, is it okay to leave that to a follow-on pr (the functionality has been verified in the current integration test) ? i will add hashcode() and equals() in the next update.",0,0,0,0.9888267517089844,0.993301272392273,0.9952608942985536,0.0,accept,unanimous_agreement
406433536,5322,"merged to trunk. while working on fixing forward the test migration, i realized i would need more time, and during which new commits may got it to veto this one, so i've decided to work on it in a separate pr.",0,0,0,0.9427391290664672,0.9868638515472412,0.9704682230949402,0.0,accept,unanimous_agreement
406448483,5322,thanks for the pr !,1,1,1,0.9562222361564636,0.9214534163475036,0.9210616946220398,1.0,accept,unanimous_agreement
406449043,5322,thanks for your detailed review.,1,0,1,0.7013413906097412,0.5747381448745728,0.6751462817192078,1.0,accept,majority_agreement
1474405568,13391,todo: ~* config for the feature~ * request handling optimizations + bookeeping ~* confirm compatibility --> we may need ibp~,0,0,0,0.9510137438774108,0.9900627732276917,0.9903676509857178,0.0,accept,unanimous_agreement
1504291314,13391,fyi i found this: [a link],0,0,0,0.968900740146637,0.9455795884132384,0.992289125919342,0.0,accept,unanimous_agreement
1504315306,13391,^ seems like this still may be initially caused by my change so i'm investigating.,0,0,0,0.8935311436653137,0.9403502941131592,0.639552652835846,0.0,accept,unanimous_agreement
1505528988,13391,i realized i didn't push sorry :( just pushed now. i will also look at that failure some more today.,-1,-1,-1,0.9895474910736084,0.9933266639709472,0.9960036873817444,-1.0,accept,unanimous_agreement
1505555154,13391,i found the issue. i will fix. [code block],0,0,0,0.9827608466148376,0.9684360027313232,0.9698599576950072,0.0,accept,unanimous_agreement
1505622263,13391,found the issue -- we didn't correctly handle disconnects which would cause npes and force close the producer. pushed the code to handle disconnects -- locally i did not see the issue after running 40 times (typically would see it in the first two runs before the fix),0,0,0,0.966093361377716,0.983400285243988,0.989267885684967,0.0,accept,unanimous_agreement
1506123475,13391,here are errors on the latest build on trunk i could find: [a link] seems to roughly correlate with the failures i see. i think the only suspicious one is [a link] which was failing before without my fix. this error is slightly different and related to endtxn. i did see this flake one time when i was repeatedly testing on my branch. i can look on trunk as well.,0,0,0,0.8933866620063782,0.8675220608711243,0.9649534821510316,0.0,accept,unanimous_agreement
213117738,1251,mind reviewing this?,0,0,0,0.9701660871505736,0.7558810114860535,0.9670141339302064,0.0,accept,unanimous_agreement
215311447,1251,rebased on trunk.,0,0,0,0.9867143034934998,0.9916990995407104,0.9928845167160034,0.0,accept,unanimous_agreement
215454261,1251,i think we should have some tests for this functionality.,0,0,0,0.9752103090286256,0.9535079002380372,0.9833779335021972,0.0,accept,unanimous_agreement
215487593,1251,: mind explaining why you chose to plug in the validation where you did? both why do you think a new state is needed and why do you think handleconnection() (which until now just did some metrics) is the correct place for the validation?,0,0,0,0.9779086709022522,0.9900208711624146,0.9910596013069152,0.0,accept,unanimous_agreement
215489877,1251,": another design level question: it looks like you are validation every response. but - brokers can't respond in anything other than the request version, which the client provides. i thought we discussed only validating the broker apis when connecting. what made you change your mind?",0,0,0,0.9590376019477844,0.9294977188110352,0.9455106258392334,0.0,accept,unanimous_agreement
215490039,1251,i'm pausing the line-level review until i have more clarity on the design decisions / scope.,0,0,0,0.9629367589950562,0.9471680521965028,0.9723310470581056,0.0,accept,unanimous_agreement
220498557,1251,"thanks for the review. below are the goals. - allow clients to not utilize kip-35. - allow clients interested in kip-35 to be able to check if all api versions it needs is supported by a broker. - api version check has to be done for each new connection, even for re-connects. below is the design overview. - for each new connection established, if client, i.e., `kafkaproducer`/ `kafkaconsumer`, has specified api versions it will be using, send `apiversions` request. - when `apiversions` response is received, client validates that client's requested api versions are supported by broker. if not, throw `kafkaexception`. - whether a connection is ready to send requests is determined as below. - if client does not want to check api versions, i.e, client has not specified required api versions, the connection must be in `connected` state. - if client wants to check api versions, i.e., client has specified required api versions, the connection must be in `ready` state. valid connection states transitions. - client does not want to perform api versions check. note that this is what it is right now. `disconnected` -> `connecting` -> `connected` - client wants to perform api versions check. added with this patch. `disconnected` -> `connecting` -> `connected` -> `checking_api_versions` -> `ready` a new state is needed as a connection can be ready to send requests on either in `connected` or in `ready` state, based on client's need to check api versions. as the api versions check is done for each connection, it is best to do it when we have a connection established. if check passes transition it to `ready` state that indicates successful api versions check. if the check fails, the connection is closed and an `kafkaexception` is thrown. i don't think every response is getting validated, only `apiversion` response will be validated. `apiversion` request is only sent while connecting.",1,1,1,0.9337254166603088,0.8230109810829163,0.9468713402748108,1.0,accept,unanimous_agreement
225725972,1251,does the above explanation answer your question?,0,0,0,0.9705918431282043,0.989723801612854,0.9925386309623718,0.0,accept,unanimous_agreement
227901779,1251,would you mind reviewing this?,0,0,0,0.9806817173957824,0.9841398000717164,0.989207923412323,0.0,accept,unanimous_agreement
227978150,1251,", i left some initial comments, i didn't do a detailed review. personally, i think it's really important to have system tests for this feature to make sure it works as expected. some of the system tests can be written now (for brokers 0.8.2.x and 0.9.0.x), but we probably can only write the ones for 0.10.0.0 once trunk has bumped the version of request types used by the producer and consumer. testing at the `networkclient` level is not really enough as we want to make sure the errors are propagated all the way to the user.",-1,0,0,0.5916692018508911,0.9824545979499816,0.8869979381561279,0.0,accept,majority_agreement
228110393,1251,"i am planning on adding system tests for it. however, i want to get some feedback on overall approach. though i had outlined the same approach as a response to jay's question on kip-35 discuss list, it wasn't discussed in great details. gwen too had some questions on overall approach, to which i have posted answers earlier. i have discussed the approach with though, but review from a committer who can eventually help to commit this will help. meanwhile, i can work on system tests.",0,0,0,0.6673732995986938,0.7021666169166565,0.6567157506942749,0.0,accept,unanimous_agreement
228185438,1251,"yes, it makes sense to get agreement on the overall approach before spending more time on this. i am a bit undecided on this one. it seems a bit wasteful to make an additional request to check the versions to simply fail if they don't match. the broker should really be returning an error when an unsupported request arrives. what would be really cool is making the java client support multiple broker versions. but that would require a discussion in the mailing list and it would be a bigger change.",-1,-1,0,0.8343946933746338,0.5822860598564148,0.8586824536323547,-1.0,accept,majority_agreement
228204101,1251,"this was discussed and agreed upon as part of kip-35. below is an excerpt. i think it has value to know that clients are not compatible with brokers it is trying to talk to, rather than getting connection dropped or similar not so sure response. this was proposed in kip-35 and was shot down. we should definitely give it a try again, but as you said this will be a larger and lengthier discussion. getting everyone on same page when it comes to compatibility definition, has not been so easy :).",1,1,1,0.8159863352775574,0.8987753987312317,0.9857527613639832,1.0,accept,unanimous_agreement
228292359,1251,"yes, i am aware of kip-35 discussions and outcome. all those discussions happened with 0.10.0 in mind, which has now been released and this part of the kip didn't make it into that release. part of the motivation for this part was to ensure that apiversions request worked properly. that's a weaker argument now since we have already released it and we relied on other clients to test it for us. i agree that there is value in knowing that clients are not compatible with brokers (which was the main motivation). however, we are only doing it this way because we don't have a way to return a generic error on any request, which is a bit of a shame. on the topic of supporting multiple broker versions, yes, kip-35 wasn't the right place for that. however, now that kip-35 is in, it could be considered again. it's weird that the java clients don't support this when third-party clients do. if we did this, there would be less of a need to backport client fixes to older versions (which people often can't upgrade because of the brokers). anyway, it would be interesting to think if the approach we are doing now could be extended in that direction. it just occurred to me that we are not handling the sasl case correctly. for that case, we need to send the `apiversionsrequest` before the `saslhandshakerequest` (see `saslclientauthenticator`). right? finally, i don't mean to block this from going in, i was just sharing my thoughts on the subject. since there is value and it was approved as part of kip-35, as long as we can ensure that we don't break existing behaviour and that the new functionality works correctly, it seems like it can go in.",0,0,0,0.7986389994621277,0.9177777767181396,0.9418096542358398,0.0,accept,unanimous_agreement
228835703,1251,"i agree with all you said. however, one of the issues i keep running into is that the longer a kip or a pr remains open the more times its intent changes. could we agree on adhering to what we decided on kip-35 scope and let this go in, sure i will work on fixing any issues with it. this will make sure that we will have at least this basic check in java clients in the next release. i have plans on using kip-35's support to make rolling upgrade easier and along that we can initiate thoughts around adding support for multiple brokers. i am sure that it is going to take some time, however we definitely will have more insight as third party clients would have some working examples for us to point at. if you agree, then i can start looking into sasl issues you pointed out and adding tests. will wait for your response.",0,0,1,0.6243329644203186,0.6978082656860352,0.5962759256362915,0.0,accept,majority_agreement
230643285,1251,", i checked with jun and he's happy for this to go in as long as it's not too complicated. perhaps the best thing is to first look at the sasl case to see how we handle that before spending effort on anything else?",0,1,1,0.5407049059867859,0.8938674330711365,0.8830363750457764,1.0,accept,majority_agreement
232095144,1251,"sounds good . i looked into sasl scenario and it looks like sasl is a bit off than other scenarios where we handle `apiversionrequest` in `saslserverauthenticator` itself, which means if a client wants to get api versions before sasl authentication, it is allowed. i think in kip-43 we wanted to add this support to let clients know supported versions of `saslhandshakerequest`. do we currently have a reason to do so? maybe will have some thought here. the current changes will work even for sasl scenario, however if we really want to handle multiple versions of `saslhandshakerequest`, we need to get api versions before sasl authentication, and we have following options for that. 1. move down whole api version checking from `networkclient` to `kafkachannel`. have `kafkachannel` maintain supported api versions. clients will access api versions info from the channel. 2. as sasl authentication has a special handling for `apiversionrequest`, only have api versions fetched for the sasl scenario in `authenticator` and put a check in `networkclient` to go to `ready` state from `connected`, skipping `checking_api_versions`. thoughts?",1,1,1,0.9178683161735536,0.6058794260025024,0.8867927193641663,1.0,accept,unanimous_agreement
232279950,1251,"at the moment, there is only one version of `saslhandshakerequest`, but it will still be good to consider how this can be validated as well while you are doing the others. perhaps a separate request from `saslclientauthenticator`to fetch only the supported versions of `saslhandshakerequest` would be neater? that way, you don't need to propagate the list of used apis to the authenticator or the versions back to the networkclient. the rest of the code can stay as is. it would mean an additional `apiversionsrequest` for sasl, but perhaps that is ok. if that is the agreed approach, you could add a comment in `saslclientauthenticator` and perhaps defer implementation until a new version of `saslhandshakerequest` is required. what do you think?",0,0,0,0.9556831121444702,0.9899724721908568,0.986850380897522,0.0,accept,unanimous_agreement
232622378,1251,", your suggestion makes sense to me. since the current saslhandshakerequest will continue to be supported, it is fair to only add the check once the client needs to use a hypothetical new version of saslhandshakerequest (hopefully never).",0,0,0,0.9515840411186218,0.9575468301773072,0.7769016027450562,0.0,accept,unanimous_agreement
232727718,1251,"sounds like we do not have a strong reason to add special handling for sasl case as of now, and as such the current design for api version checking does not have to change. does this take care of your concern for sasl scenario, if it does i will start looking at other review comments.",0,0,0,0.7461065053939819,0.9491652846336364,0.9533957839012146,0.0,accept,unanimous_agreement
232730951,1251,"yes, worth adding a comment as rajini suggested.",0,0,0,0.953970730304718,0.982450544834137,0.9888924956321716,0.0,accept,unanimous_agreement
234559193,1251,"this should be good for you to take a look. the two test failures are not reproducible, tried locally multiple times with java 7.",0,1,0,0.9767919182777404,0.5586312413215637,0.9433176517486572,0.0,accept,majority_agreement
236014652,1251,one more ping :). let me know if there is still something blocking this from getting merged.,1,1,1,0.8882009983062744,0.7658082246780396,0.9826722145080566,1.0,accept,unanimous_agreement
266899476,1251,"thanks for the review, updated pr.",1,1,1,0.8135951161384583,0.6164942979812622,0.8649253249168396,1.0,accept,unanimous_agreement
266912696,1251,"so i added `latest_0_10` to `apiversionschecktest` and apparently due to kafka-4093, trunk's client won't be able to talk to `0.10.1.0`. not sure if this is known. if not, let me know and i can file a jira.",0,0,0,0.9858273267745972,0.9824209809303284,0.9846315383911132,0.0,accept,unanimous_agreement
266917584,1251,not sure i understand. kafka-4093 was included in 0.10.1.0. why would that prevent trunk clients from talking to 0.10.1.0 brokers?,0,0,0,0.9352328181266784,0.985630989074707,0.7840507626533508,0.0,accept,unanimous_agreement
266919305,1251,"i meant `0.10.0.1`, that is what `latest_0_10_0` is pointing to.",0,0,0,0.986638069152832,0.9937840700149536,0.9904229640960692,0.0,accept,unanimous_agreement
266921743,1251,"it's true that trunk can't talk to `latest_0_10_0`, supposedly we detect this via `apiversionsresponse`? and for the `latest_0_10_1` case, it should work atm.",0,0,0,0.9895750880241394,0.99347585439682,0.9931564927101136,0.0,accept,unanimous_agreement
266926910,1251,"well, as this was supposed to be a step before client compatibility, in current pr api version checks are based on if there is any version overlap between client and broker for an api. however, as the client always sends the latest api version, to be able to catch that i will have to set expected minimum api version to latest version of the api. with that the version mismatch will be caught. however, soon client compatibility work will have to revert it back to what it is right now. i am changing the min expected version to be the latest api version and adding a comment indicating that it needs to be changed to min version when backwards client compatibility is added. makes sense?",0,0,0,0.9469026923179626,0.9784250259399414,0.9529194235801696,0.0,accept,unanimous_agreement
266927464,1251,"if it's a simple change, that sounds good to me. if not, then we can leave as is.",1,0,0,0.8126472234725952,0.9190009236335754,0.8605582118034363,0.0,accept,majority_agreement
267188425,1251,"thanks. left one nitpick, but the latest changes are looking good.",1,1,1,0.9454541206359864,0.9941097497940063,0.98146790266037,1.0,accept,unanimous_agreement
267339807,1251,started system tests build here: [a link],0,0,0,0.9863846898078918,0.9771700501441956,0.9938919544219972,0.0,accept,unanimous_agreement
267714157,1251,", i created a pr to your branch that addresses the feedback that i think we need to address before this is merged: [a link] the rest can be done as part of the client compatibility kip. if you are happy with the changes, can you please merge them to your branch and then merge trunk to your branch?",0,0,0,0.953406035900116,0.9656768441200256,0.8808004856109619,0.0,accept,unanimous_agreement
267714603,1251,"thanks for the changes, i was working on addressing them myself. but, glad you commented here, merging your changes.",1,1,1,0.9780434370040894,0.9880414009094238,0.9913882613182068,1.0,accept,unanimous_agreement
267714875,1251,", sorry, i was afraid you may have been busy and wanted to take advantage of the momentum to get this merged. :)",-1,-1,1,0.9789981245994568,0.9926718473434448,0.8098663091659546,-1.0,accept,majority_agreement
267715212,1251,"no worries, rather thanks!",1,1,1,0.9764641523361206,0.9890930652618408,0.9868183135986328,1.0,accept,unanimous_agreement
267716319,1251,"looks like i didn't include a local change in my pr to get one of the tests to compile. it should be trivial to fix. also, can you please address the overflow issue? i wasn't sure what the intent was on that one.",-1,0,0,0.5802615284919739,0.9505767822265624,0.9594684839248656,0.0,accept,majority_agreement
267725348,1251,system tests run: [a link],0,0,0,0.9882187843322754,0.990402340888977,0.9946409463882446,0.0,accept,unanimous_agreement
267759785,1251,"the system tests failures are also happening in trunk with the exception of the new system test that seems to be failing. because the newly introduced system test will change as part of the client compatibility work, i will remove it and merge this pr.",0,0,0,0.9814421534538268,0.9926695823669434,0.9879900813102722,0.0,accept,unanimous_agreement
267760134,1251,"lgtm, merged to trunk (without the system test as explained in the previous comment).",0,0,0,0.9885498881340028,0.9938843846321106,0.9919651746749878,0.0,accept,unanimous_agreement
132635211,151,tests passed locally.,0,0,0,0.982586681842804,0.9910573959350586,0.9927990436553956,0.0,accept,unanimous_agreement
133041845,151,", i believe i have addressed everything that was discussed. i also merged trunk into this branch to fix a minor import conflict in a test. tests pass locally. please have a look and if you're happy, this may be good to go.",1,1,1,0.9610783457756042,0.9316394329071044,0.9662330150604248,1.0,accept,unanimous_agreement
133056252,151,closed/reopened to trigger another ci build.,0,0,0,0.9866468906402588,0.9830909371376038,0.9932264685630798,0.0,accept,unanimous_agreement
133058687,151,the output doesn't actually say what (if anything) failed in the last ci build.,0,0,0,0.9637247323989868,0.8873043060302734,0.9466656446456908,0.0,accept,unanimous_agreement
133070221,151,ran the tests locally two more times and they passed both times.,0,0,0,0.9813252687454224,0.9914246797561646,0.9922332763671876,0.0,accept,unanimous_agreement
133213237,151,"i think this needs a rebase, but otherwise lgtm. , do you want to take a look too?",0,0,0,0.9781623482704164,0.8673379421234131,0.9804049134254456,0.0,accept,unanimous_agreement
133217955,151,"actually, not good - maybe rebase related - but cloning the pr doesn't pass unit tests on my machine: uncaught exception during compilation: java.lang.stackoverflowerror (scala 2.10.5, java 8)",0,0,0,0.9683346152305604,0.849307119846344,0.7433866262435913,0.0,accept,unanimous_agreement
133224996,151,"ok, the stack size issue is intermittent and seem unrelated to the patch (happened to me once or twice on trunk too). my lgtm is back. wanted to take a look, so i'll hold off committing to give him a chance to comment.",0,0,0,0.9552016854286194,0.8104740977287292,0.9769655466079712,0.0,accept,unanimous_agreement
133226071,151,i fixed the conflict (just an import),0,0,0,0.985933482646942,0.9850008487701416,0.9909718036651612,0.0,accept,unanimous_agreement
133226348,151,cool. the test failure seems like an unrelated flaky. opened kafka-2455 for it.,1,1,1,0.9687928557395936,0.9401666522026062,0.9640233516693116,1.0,accept,unanimous_agreement
133227314,151,thanks!,1,1,1,0.9308210611343384,0.9051083922386168,0.8631753921508789,1.0,accept,unanimous_agreement
133377428,151,", thanks for the review. learned a couple of new things about the `selector` and metrics. :) i pushed two commits to address two of your comments. the other two i was not sure about so i asked some questions.",1,1,1,0.9912289381027222,0.995329976081848,0.9968026876449584,1.0,accept,unanimous_agreement
133491198,151,closing this for now.,0,0,0,0.9723151326179504,0.9609447717666626,0.9799432158470154,0.0,accept,unanimous_agreement
135916524,151,tests passed locally.,0,0,0,0.982586681842804,0.9910573959350586,0.9927990436553956,0.0,accept,unanimous_agreement
136154088,151,tests passed locally and in jenkins once. the second time failed due to an unrelated failure that has been seen in other prs (testmetricsleak).,0,0,0,0.986853837966919,0.9907355904579164,0.992330014705658,0.0,accept,unanimous_agreement
136378962,151,thanks for the review . i have addressed a number of the issues raised and left comments/questions for the rest.,1,1,1,0.8811666965484619,0.9417985081672668,0.9632253050804138,1.0,accept,unanimous_agreement
136400954,151,"`./gradlew test` passed locally (i ran it 3 times). however, there is one system test failure: test_id: 2015-08-31--001.kafkatest.tests.replication_test.replicationtest.test_hard_bounce status: fail run time: 7 minutes 32.756 seconds [a link] i will investigate this tomorrow (pointers are welcome though).",0,0,0,0.958257794380188,0.7244285941123962,0.98299902677536,0.0,accept,unanimous_agreement
136737500,151,", pushed two additional commits to address the two outstanding issues. i implemented things in a slightly different way than what was discussed. i added some comments to the pr explaining. let me know what you think. `gradlew test` passed locally and system tests passed too: [a link]",0,0,0,0.9514980912208556,0.9517574310302734,0.8174282908439636,0.0,accept,unanimous_agreement
136881820,151,pushed a couple of commits addressing review comments.,0,0,0,0.979354739189148,0.9888380765914916,0.9923385381698608,0.0,accept,unanimous_agreement
137114214,151,", merged trunk and added missed scaladoc/javadoc. `gradlew test` passed locally (as expected since there hasn't been any change that would affect behaviour)",0,0,0,0.9884899258613586,0.9949491024017334,0.9833020567893982,0.0,accept,unanimous_agreement
137174188,151,started a kafka 0.8.2.1 cluster with 3 brokers manually and did a rolling upgrade (with kill -9) to to this branch. used console producer and console consumer as well as describe-topic and things seemed to work as expected.,0,0,0,0.9825117588043212,0.987276017665863,0.950524091720581,0.0,accept,unanimous_agreement
137178231,151,thanks for the latest patch. just a minor comment on the names of alive_brokers. otherwise lgtm.,1,1,1,0.795670211315155,0.9900894165039062,0.9659399390220642,1.0,accept,unanimous_agreement
137185985,151,renamed to live_brokers as agreed. tests passed locally.,0,0,0,0.9870866537094116,0.9939364790916444,0.9863452315330504,0.0,accept,unanimous_agreement
299043716,2967,"sure, i can add the benchmarks, wasn't aware of the module",0,0,0,0.9838729500770568,0.9645033478736876,0.9919677376747132,0.0,accept,unanimous_agreement
299595224,2967,"adressed all the comments i believe, now just waiting on getting the build to pass. you might be interested in reviewing this part [a link]",0,0,1,0.968210756778717,0.8575118780136108,0.626905083656311,0.0,accept,majority_agreement
299654668,2967,"probably not necessary to get a full green build if the failures are known issues. on the other hand, if you're just trying to shake out some more transient failures so that we can add jiras, have at it!",0,0,0,0.7202094793319702,0.9328949451446532,0.9132388830184937,0.0,accept,unanimous_agreement
299658313,2967,i give up :grinning_face_with_big_eyes: i think i've have each java/scala combination succeed at least once. also haven't seen any new failures in the last few runs.,0,0,1,0.5598690509796143,0.5043769478797913,0.8424261212348938,0.0,accept,majority_agreement
302507386,2967,i have addressed all your comments,0,0,0,0.971045196056366,0.9246280789375304,0.9696050882339478,0.0,accept,unanimous_agreement
305035542,2967,client system tests build will appear in the following link when it starts: [a link],0,0,0,0.9882805347442628,0.984567165374756,0.995991051197052,0.0,accept,unanimous_agreement
305054123,2967,clients system tests passed: [a link],0,0,0,0.9881305694580078,0.9871240854263306,0.9900038838386536,0.0,accept,unanimous_agreement
305054234,2967,"lgtm, merging to trunk and 0.11.0.",0,0,0,0.9887370467185974,0.9924599528312684,0.987253725528717,0.0,accept,unanimous_agreement
288180972,2719,"already getting an error from this basic test, that needs to be fixed before this pr goes in. org.apache.kafka.streams.errors.streamsexception: could not create internal topics. at org.apache.kafka.streams.processor.internals.internaltopicmanager.makeready(internaltopicmanager.java:69). looks like this might not be a real error though, since the replication factor of the internal topics is not maintained. adding another broker and re-testing.",0,0,0,0.9650703072547911,0.9886566400527954,0.9799235463142396,0.0,accept,unanimous_agreement
288384076,2719,want to have a look? thanks.,1,1,1,0.5424637198448181,0.7470970749855042,0.6913896203041077,1.0,accept,unanimous_agreement
288461109,2719,"making as wip again since 2 tests are failing and i need to investigate. [2017-03-22 16:25:05,575] error stream-thread [smoketest-b25363c8-5bce-4d72-8c51-8fe0b49715f6-streamthread-1] streams application error during processing: (org.apache.kafka.streams.processor.internals.streamthread) java.lang.illegalstateexception: attempt to send a request to node 1 which is not ready. at org.apache.kafka.clients.networkclient.dosend(networkclient.java:284) at org.apache.kafka.clients.networkclient.send(networkclient.java:265) at org.apache.kafka.streams.processor.internals.streamskafkaclient.sendrequest(streamskafkaclient.java:238) at org.apache.kafka.streams.processor.internals.streamskafkaclient.createtopics(streamskafkaclient.java:170) at org.apache.kafka.streams.processor.internals.internaltopicmanager.makeready(internaltopicmanager.java:63) at org.apache.kafka.streams.processor.internals.streampartitionassignor.preparetopic(streampartitionassignor.java:615) at org.apache.kafka.streams.processor.internals.streampartitionassignor.assign(streampartitionassignor.java:445) at org.apache.kafka.clients.consumer.internals.consumercoordinator.performassignment(consumercoordinator.java:347) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.onjoinleader(abstractcoordinator.java:505) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.access$1100(abstractcoordinator.java:93) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$joingroupresponsehandler.handle(abstractcoordinator.java:455) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$joingroupresponsehandler.handle(abstractcoordinator.java:437) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$coordinatorresponsehandler.onsuccess(abstractcoordinator.java:788) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$coordinatorresponsehandler.onsuccess(abstractcoordinator.java:769) at org.apache.kafka.clients.consumer.internals.requestfuture$1.onsuccess(requestfuture.java:190) at org.apache.kafka.clients.consumer.internals.requestfuture.firesuccess(requestfuture.java:153) at org.apache.kafka.clients.consumer.internals.requestfuture.complete(requestfuture.java:120) at org.apache.kafka.clients.consumer.internals.consumernetworkclient$requestfuturecompletionhandler.firecompletion(consumernetworkclient.java:498) at org.apache.kafka.clients.consumer.internals.consumernetworkclient.firependingcompletedrequests(consumernetworkclient.java:342) at org.apache.kafka.clients.consumer.internals.consumernetworkclient.poll(consumernetworkclient.java:251) at org.apache.kafka.clients.consumer.internals.consumernetworkclient.poll(consumernetworkclient.java:167) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.joingroupifneeded(abstractcoordinator.java:351) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.ensureactivegroup(abstractcoordinator.java:307) at org.apache.kafka.clients.consumer.internals.consumercoordinator.poll(consumercoordinator.java:294) at org.apache.kafka.clients.consumer.kafkaconsumer.pollonce(kafkaconsumer.java:1033) at org.apache.kafka.clients.consumer.kafkaconsumer.poll(kafkaconsumer.java:999) at org.apache.kafka.streams.processor.internals.streamthread.runloop(streamthread.java:542) at org.apache.kafka.streams.processor.internals.streamthread.run(streamthread.java:326)",-1,0,0,0.508528470993042,0.9709152579307556,0.9641441106796264,0.0,accept,majority_agreement
289475648,2719,your review is appreciated. this should eventually got to 0.10.2 bug release as well i believe.,1,1,1,0.8465642929077148,0.9340785145759584,0.9739348292350768,1.0,accept,unanimous_agreement
289514114,2719,system tests pass on jenkins too: [a link],0,0,0,0.9870308041572572,0.9846842885017396,0.9897437691688538,0.0,accept,unanimous_agreement
289601989,2719,we wrap exceptions as streams exceptions since the code that calls these functions and retries only understands streams exceptions. i'd argue that's a good thing since that code cannot possibly know all other exceptions in the lower layers.,0,0,0,0.9729755520820618,0.9774627685546876,0.9884999394416808,0.0,accept,unanimous_agreement
289602348,2719,"about `i still think it's better to fail fast and educate users retry creating their apps after the broker is fully up than trying to wait for, say 5 seconds and hopefully it will succeed.` remember that the broker can fail anytime and the user cannot have any guarantee that once a broker is up it won't fail again. in a subsequent jira we can consider unifying the backoff times to match other standard backoff times that clients use.",0,0,0,0.9010496139526368,0.9733841419219972,0.9777555465698242,0.0,accept,unanimous_agreement
289720444,2719,"note: i'll split this pr into multiple prs, one that requires a kip and another that doesn't and is needed for bug fix. cc",0,0,0,0.985615611076355,0.9827484488487244,0.9901536107063292,0.0,accept,unanimous_agreement
289919770,2719,i've adjusted this pr so it doesn't require a kip. needs re-reviewing though. thanks.,1,1,1,0.943494439125061,0.9759051203727722,0.9534632563591005,1.0,accept,unanimous_agreement
290362875,2719,one last look please? thanks.,1,1,1,0.6498085260391235,0.9254323244094848,0.6824686527252197,1.0,accept,unanimous_agreement
290928787,2719,[a link] is green,0,0,0,0.972905933856964,0.9418683052062988,0.99412339925766,0.0,accept,unanimous_agreement
291110511,2719,can i get a final approve so that or others can check in? also just opened 0.10.2 cherry picked version of this pr. thanks.,1,1,1,0.944688618183136,0.9823896884918212,0.9477980732917786,1.0,accept,unanimous_agreement
291204138,2719,"env failure, unrelated to pr",0,0,0,0.6918784379959106,0.912735104560852,0.937584102153778,0.0,accept,unanimous_agreement
291277682,2719,"this can go in, thanks. also the 0.10.2 version of this: [a link] thanks.",1,1,1,0.8819222450256348,0.968121349811554,0.979500651359558,1.0,accept,unanimous_agreement
857875970,10822,"below are some logs when both connector and tasks are restarted. [2021-06-09 12:02:26,049] info [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] received restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1709) [2021-06-09 12:02:26,051] info [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] executing plan to restart connector and 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1137) [2021-06-09 12:02:26,063] debug [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] restarting 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1169) [2021-06-09 12:02:26,114] debug [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] restarted 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} as requested (org.apache.kafka.connect.runtime.distributed.distributedherder:1171) [2021-06-09 12:02:26,114] info [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] completed plan to restart connector and 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1173)",0,0,0,0.9767980575561525,0.9934053421020508,0.9920762777328492,0.0,accept,unanimous_agreement
859064545,10822,passing system tests results ![a link],0,0,1,0.9675259590148926,0.9605324268341064,0.9580123424530028,0.0,accept,majority_agreement
863639444,10822,i have taken care of most of the review comments and resolved them. i left review comments with follow up questions as unresolved. if you get some time could you please review and guide me?,0,0,1,0.9189581274986268,0.960690140724182,0.5455381274223328,0.0,accept,majority_agreement
864493282,10822,"fired up local kafka server and kafka connect and was able to curl the api locally. {""name"":""local-file-source"",""connector"":{""state"":""running"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""running"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}% ➜ kafka git:(kafka-4793) ✗ curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""restarting"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""restarting"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}% ➜ kafka git:(kafka-4793) ✗ curl -xpost [a link] ➜ kafka git:(kafka-4793) ✗ curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""running"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""running"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}% ➜ kafka git:(kafka-4793) ✗ curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""running"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""running"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}% ➜ kafka git:(kafka-4793) ✗ curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""restarting"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""restarting"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%",0,0,0,0.9703553915023804,0.9746425151824952,0.9907341003417968,0.0,accept,unanimous_agreement
870820269,10822,thanks a lot for taking the time and posting the detailed review comments. i was able to resolve all of them except for a few where i left some additional comments. please let me know how you want to proceed with them.,1,1,1,0.9741011261940002,0.9840850830078124,0.985357105731964,1.0,accept,unanimous_agreement
870979782,10822,"fyi, a test failure seems relevant: `testcorsenabled – org.apache.kafka.connect.runtime.rest.restservertest` i don't remember this test being flaky and it failed in both builders.",0,0,0,0.6727572679519653,0.9850871562957764,0.9793940782546996,0.0,accept,unanimous_agreement
871880686,10822,after rebase i do not see the failures,0,0,0,0.9007821083068848,0.8881111145019531,0.9767913818359376,0.0,accept,unanimous_agreement
2050761892,15640,—please review this pr if you have some spare time. thanks!,1,1,1,0.9745693802833556,0.9911561012268066,0.9708532691001892,1.0,accept,unanimous_agreement
2077870915,15640,"—thanks for your review. i have made the requested changes, so please take another pass. thanks!",1,1,1,0.9799399971961976,0.9900402426719666,0.9940410256385804,1.0,accept,unanimous_agreement
2083370524,15640,i agree. what about the timed wait in `awaitpendingasynccommitsandexecutecommitcallbacks()`?,0,0,0,0.9861106872558594,0.98463237285614,0.9876424074172974,0.0,accept,unanimous_agreement
2083394774,15640,"—in most places i removed use of a `timer` to calculate the deadline. event classes no longer require a `timer`, it is the caller who must call `completableevent.calculatedeadlinems()` when creating the event.",0,0,0,0.9891413450241088,0.9933278560638428,0.9902706146240234,0.0,accept,unanimous_agreement
2084906500,15640,do you have something in mind? like using the `commitfuture` as a timeout for the awaiting of the execution of the callback?,0,0,0,0.98620867729187,0.9927910566329956,0.9945165514945984,0.0,accept,unanimous_agreement
2087355371,15640,"agree we should not wait on the `commitfuture` with a timer because the deadline is contained in the event we submitted, and already enforced by the reaper, and not clear about what the proposed relationship with `awaitpendingasynccommitsandexecutecommitcallbacks` is?? i would expect we only need to call `consumerutils.getresult(commitfuture);`, and that is consistent with how we get results for all other completable events now: - we create an event with a deadline - we call `applicationeventhandler.addandget(event)` for the commit case that flow has a different shape just because we use `applicationeventhandler.add(event)` [a link], to cater for commit sync and async, but we should still apply the same approach and just call get without any time boundary i would say.",0,0,0,0.9721070528030396,0.991776704788208,0.9888021945953368,0.0,accept,unanimous_agreement
2087359722,15640,"hey , thanks a lot for the pr, this is a big piece! i completed a pass of all the non-test files, left some comments.",1,1,1,0.9925776124000548,0.9949036836624146,0.9965783953666688,1.0,accept,unanimous_agreement
2089079055,15640,"here's my reasoning on the need for a `timer`-based `get()` in `awaitpendingasynccommitsandexecutecommitcallbacks()`... the `future` that's referenced in `lastpendingasynccommit` comes from an `asynccommitevent` and has a hard-coded deadline of `long.max_value`. as such, the `competableeventreaper` in the network thread will never prune that event. without a timeout when calling `get()` on the `lastpendingasynccommit`, the caller could hang for up to `request.timeout.ms` while we wait for the network i/o request to complete (or timeout). —does that make sense? cmiiw, please :folded_hands:",0,0,0,0.9756935238838196,0.9925629496574402,0.9896765351295472,0.0,accept,unanimous_agreement
2090517636,15640,"i think, we are talking about two separate things here: 1. change `consumerutils.getresult(commitfuture, requesttimer);` to `consumerutils.getresult(commitfuture);` 2. do we need a timer for `awaitpendingasynccommitsandexecutecommitcallbacks(requesttimer, true);` as far as i understand, we agree on 1 but do not know if there is a better solution for 2. is this correct? currently, i do not see a better way than using a timer on awaiting the execution of the async commit callback. as pointed out, since the async commit does basically not have a timeout, we cannot wait on the deadline of the `asynccommitevent`. we can also not wait for the deadline of the `synccommitevent` since if that event completes before the timeout, we would not wait enough for the completion of the async commit callback.",0,0,0,0.889735996723175,0.9818633198738098,0.9735116958618164,0.0,accept,unanimous_agreement
2090837554,15640,"thinking about it, i guess having a thread-safe timer and using it in the background thread and the application thread would be the cleanest solution. however, i do not think it is worth blocking this pr on that.",0,0,0,0.6865130662918091,0.9293710589408876,0.96234530210495,0.0,accept,unanimous_agreement
2092616162,15640,is there any problem if we leave `awaitpendingasynccommitsandexecutecommitcallbacks` as is? it clearly needs the timer,0,0,0,0.987377941608429,0.9928656816482544,0.9910348057746888,0.0,accept,unanimous_agreement
2092808466,15640,"yeah, i think that we should leave this as it is for now.",0,0,0,0.9662280082702636,0.912964403629303,0.9658971428871156,0.0,accept,unanimous_agreement
2115938603,15640,"high level comment, just to clarify and make sure it's something we are considering and will cover with the follow-up prs for timeouts. here we're introducing a component to ensures that app events are expired only after having one chance, but that's only at the app thread level, and not for all events, but only for unsubscribe, and poll. thing is that events can also be expired indirectly when a request is expired (so playing against this changes). so even if the `processbackgroundevents` introduced here gives an expired event a change to run one, that may actually not happen (because the underlying request expires). i expect that side needed to make this whole intention work in practice will be a follow-up pr, am i right? also note, almost none of our integration test cover the poll(zero) case (helper funcs [a link], used by most of the test, poll with timeout > 0). that's probably why we did not find the expiration issues we have with poll(0) before. i guess that after addressing the timeout/expiration (this pr and follow-up), we should be able to add some.",0,0,0,0.9602257609367372,0.9848594069480896,0.9813696146011353,0.0,accept,unanimous_agreement
2117012544,15640,"could you elaborate on this a bit. i cannot completely follow. i followed the `syncevent`. the event is processed in the `applicationeventhandler` and a request is added to the commit request manager. then the commit request manager is polled, the requests are added to the network client and the the network client is polled. as far as i can see the `syncevent` is given a chance to complete. i did also not understand the connection to `processbackgroundevents`. that is for events that originate from the background thread. how does that influence if an application event was given a chance to complete? what do you mean with the ""underlying requests expires""? do you mean it exceeds the request timeout? does the legacy consumer ensure that a request completed when timeout is set to `0`? sorry for all these questions, but i would like to understand.",0,0,0,0.9591909050941468,0.9690340757369996,0.9684089422225952,0.0,accept,unanimous_agreement
2117763853,15640,"hey , the tricky bit is that, for some events, the request managers do expire requests too, so in this flow you described: when the manager is polled, if the event had timeout 0, it will be expired/cancelled before making it to the network thread. currently we have 2 managers that do this (that i can remember): [a link] and [a link]. so for those events, even with this pr, if they have timeout 0, they won't have a chance to complete. my point is not to bring more changes into this pr, only to have the whole situation in mind so we can address it properly (with multiple prs). this other [a link] attempts to address this situation i described, but only in the `commitrequestmanager` for instance. we still have to align on the approach there, and also handle it in the `topicmetadatamanager` i would say. i would expect that a combination of this pr and those others would allow us to get to a better point (now, even with this pr, we cannot make basic progress with a consumer being continuously polled with timeout 0 because `fetchcommittedoffsets` is always expired by the manager, for instance). i can easily repro it with the following integration test + poll(zero) (that i was surprised we have not covered, because testutils always polls with a non-zero timeout) [code block] makes sense?",0,0,0,0.6391821503639221,0.8305445313453674,0.9347489476203918,0.0,accept,unanimous_agreement
2118578191,15640,"yes, the network layer changes are captured in kafka-16200 and build on top of this pr.",0,0,0,0.9891815781593324,0.993499755859375,0.9926161766052246,0.0,accept,unanimous_agreement
2118579935,15640,—i believe i have addressed all the actionable feedback. are there additional concerns about this pr that prevent it from being merged? thanks.,1,1,1,0.9340983629226683,0.7712768912315369,0.96502685546875,1.0,accept,unanimous_agreement
2122575967,15640,thanks for the explanation!,1,1,1,0.9308693408966064,0.6396021246910095,0.8257373571395874,1.0,accept,unanimous_agreement
2123163240,15640,—the latest batch of feedback has been addressed. thanks!,1,1,1,0.977812886238098,0.9917077422142028,0.9743289947509766,1.0,accept,unanimous_agreement
2125302873,15640,i added kafka-16818 to cover the cases to refactor/migrate/remove tests. thanks & for your reviews!,1,1,1,0.983432114124298,0.979416012763977,0.9830489158630372,1.0,accept,unanimous_agreement
