{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fdc1458",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score,\n",
    "    recall_score, f1_score, classification_report,\n",
    "    confusion_matrix, ConfusionMatrixDisplay\n",
    "    )\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.functional import softmax\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from matplotlib.backends.backend_pdf import PdfPages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73f81a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "project_root = \"/Users/tayebekavousi/Desktop/github_sa\"\n",
    "# Save the original directory to go back to it later if needed\n",
    "original_dir = os.getcwd()\n",
    "# Change to the project root directory\n",
    "os.chdir(project_root)\n",
    "# Ensure the project root is in the Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "sys.path.insert(0, '')  # Add current directory (empty string) to path\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "166a66ab-2f73-43d6-b588-00303fc39d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global settings\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_PATHS = {\n",
    "    \"deberta\": \"saved_models/deberta_3class/saved_full_model\",\n",
    "    \"codebert\": \"saved_models/codebert_3class/saved_full_model\",\n",
    "    \"distilbert\": \"saved_models/distilbert_3class/saved_full_model\"\n",
    "}\n",
    "\n",
    "model_name = \"deberta\"  # Change this to 'codebert' 'deberta' or 'distilbert' as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "84559f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = int(self.labels[idx])\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_attention_mask=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"labels\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "\n",
    "def load_model_and_tokenizer(model_name):\n",
    "    \"\"\"\n",
    "    Load best model checkpoint from .pt file with training metadata included.\n",
    "    \"\"\"\n",
    "    model_dir = MODEL_PATHS[model_name]\n",
    "    \n",
    "    # Load config and tokenizer\n",
    "    config = AutoConfig.from_pretrained(model_dir)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "    \n",
    "    # Create model architecture\n",
    "    model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "    # Load best checkpoint file\n",
    "    best_model_files = [f for f in os.listdir(model_dir) if f.startswith(\"model_best_f1\") and f.endswith(\".pt\")]\n",
    "    if not best_model_files:\n",
    "        raise FileNotFoundError(f\"No best model checkpoint found in {model_dir}\")\n",
    "    \n",
    "    best_model_path = os.path.join(model_dir, best_model_files[0])\n",
    "    print(f\"[INFO] 🔥 Loading best model from: {best_model_path}\")\n",
    "    \n",
    "    checkpoint = torch.load(best_model_path, map_location=DEVICE)\n",
    "    \n",
    "    # Extract and load only model weights\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8ef55b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and clean dataset\n",
    "df = pd.read_csv(\"datasets/preprocessed/combined_DeepSentimentSECrossPlatform.csv\")\n",
    "df = df.dropna(subset=[\"text\"]).drop_duplicates(subset=[\"text\"])\n",
    "\n",
    "# Map sentiment [-1, 0, 1] → [0, 1, 2]\n",
    "df[\"label\"] = df[\"sentiment\"].map({-1: 0, 0: 1, 1: 2})\n",
    "\n",
    "# Extract texts and labels\n",
    "texts = df[\"text\"].astype(str).tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "# First: Train (75%) vs Temp (25%)\n",
    "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
    "    texts, labels, test_size=0.25, stratify=labels, random_state=42\n",
    ")\n",
    "\n",
    "# Then: Validation (15%) and Test (10%) from Temp\n",
    "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
    "    temp_texts, temp_labels,\n",
    "    test_size=0.4,  # 40% of 25% = 10% total\n",
    "    stratify=temp_labels,\n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e309eb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] 🔥 Loading best model from: saved_models/deberta_3class/saved_full_model/model_best_f1_0.8954.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model, tokenizer = load_model_and_tokenizer(model_name)\n",
    "\n",
    "test_dataset = ReviewDataset(test_texts, test_labels, tokenizer)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "all_preds, all_labels, all_probs = [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(labels.cpu().tolist())\n",
    "        all_probs.extend(probs.cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9264993-a497-48c3-807a-b1f773028e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions to evaluation_result/deberta_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Save model predictions to CSV for evaluation\n",
    "test_df = pd.DataFrame({\n",
    "    \"text\": test_texts,\n",
    "    \"true_label\": all_labels,\n",
    "    \"predicted_label\": all_preds,\n",
    "    \"confidence\": [max(p) for p in all_probs],\n",
    "})\n",
    "test_df[\"correct\"] = test_df[\"true_label\"] == test_df[\"predicted_label\"]\n",
    "\n",
    "output_path = f\"evaluation_result/{model_name}_predictions.csv\"\n",
    "test_df.to_csv(output_path, index=False)\n",
    "print(f\"✅ Saved predictions to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def11259-0a41-40b4-b05c-96e467f80be4",
   "metadata": {},
   "source": [
    "# ✅ Cell 1: Run Inference on Validation Set\n",
    "## This cell generates val_preds, val_probs, and val_labels using the same tokenizer and model you're already using for test inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2c060dfb-7eeb-4ec7-a099-94d82870903d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 📦 Inference on Validation Set\n",
    "val_dataset = ReviewDataset(val_texts, val_labels, tokenizer)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "\n",
    "val_preds, val_probs = [], []\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        input_ids = batch[\"input_ids\"].to(DEVICE)\n",
    "        attention_mask = batch[\"attention_mask\"].to(DEVICE)\n",
    "        labels = batch[\"labels\"].to(DEVICE)\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        probs = softmax(outputs.logits, dim=1)\n",
    "        preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "        val_preds.extend(preds.cpu().tolist())\n",
    "        val_probs.extend(probs.cpu().tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ba70e5-0722-4b36-a764-7f8f525cc394",
   "metadata": {},
   "source": [
    "# ✅ Cell 2: Generate Separate PDF Reports\n",
    "This cell creates:\n",
    "\n",
    "test_report_{model_name}.pdf\n",
    "\n",
    "val_report_{model_name}.pdf\n",
    "\n",
    "Each report includes:\n",
    "\n",
    "Confusion matrix\n",
    "\n",
    "Normalized matrix\n",
    "\n",
    "Overall metrics\n",
    "\n",
    "Class-wise metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "68aab2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[✅] Test report saved to: evaluation_result/test_report_deberta.pdf\n",
      "[✅] Validation report saved to: evaluation_result/val_report_deberta.pdf\n"
     ]
    }
   ],
   "source": [
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"evaluation_result\", exist_ok=True)\n",
    "os.makedirs(\"error_analysis\", exist_ok=True)\n",
    "\n",
    "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "def generate_report(y_true, y_pred, title_prefix):\n",
    "    # Distribution plot\n",
    "    fig1, ax1 = plt.subplots()\n",
    "    pd.Series(y_true).value_counts().sort_index().plot(kind=\"bar\", ax=ax1, color=[\"#1f77b4\", \"#aec7e8\", \"#c6dbef\"])\n",
    "    ax1.set_title(f\"Class Distribution in {title_prefix} Set\")\n",
    "    ax1.set_xticklabels(class_names, rotation=0)\n",
    "\n",
    "    # Confusion Matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    fig2, ax2 = plt.subplots()\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "    ax2.set_title(f\"{title_prefix} Confusion Matrix\")\n",
    "    ax2.set_xlabel(\"Predicted\")\n",
    "    ax2.set_ylabel(\"True\")\n",
    "\n",
    "    # Normalized Confusion Matrix\n",
    "    cm_norm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    fig3, ax3 = plt.subplots()\n",
    "    sns.heatmap(cm_norm, annot=True, fmt=\".2f\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names, ax=ax3)\n",
    "    ax3.set_title(f\"{title_prefix} Normalized Confusion Matrix\")\n",
    "    ax3.set_xlabel(\"Predicted\")\n",
    "    ax3.set_ylabel(\"True\")\n",
    "\n",
    "\n",
    "    # Overall Metrics Table\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec_macro = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    rec_macro = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    f1_macro = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "    \n",
    "    fig4, ax4 = plt.subplots()\n",
    "    ax4.axis(\"off\")\n",
    "    overall = [\n",
    "        [\"Accuracy\", f\"{acc:.2f}\"],\n",
    "        [\"Macro Precision\", f\"{prec_macro:.2f}\"],\n",
    "        [\"Macro Recall\", f\"{rec_macro:.2f}\"],\n",
    "        [\"Macro F1-Score\", f\"{f1_macro:.2f}\"],\n",
    "    ]\n",
    "    ax4.table(cellText=overall, colLabels=[\"Metric\", \"Score\"], loc=\"center\", cellLoc=\"center\").set_fontsize(10)\n",
    "    ax4.set_title(f\"{title_prefix} Set Overall Metrics\", fontweight=\"bold\", pad=20)\n",
    "    \n",
    "    # Class-wise Metrics Table\n",
    "    prec = precision_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    f1s = f1_score(y_true, y_pred, average=None, zero_division=0)\n",
    "    \n",
    "    fig5, ax5 = plt.subplots()\n",
    "    ax5.axis(\"off\")\n",
    "    classwise = [\n",
    "        [class_names[i], f\"{prec[i]:.2f}\", f\"{rec[i]:.2f}\", f\"{f1s[i]:.2f}\"]\n",
    "        for i in range(3)\n",
    "    ]\n",
    "    ax5.table(cellText=classwise, colLabels=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\"], loc=\"center\", cellLoc=\"center\").set_fontsize(10)\n",
    "    ax5.set_title(f\"{title_prefix} Set Class-wise Metrics\", fontweight=\"bold\", pad=20)\n",
    "\n",
    "    return [fig1, fig2, fig3, fig4, fig5]\n",
    "\n",
    "# ────────────────────────────────────────\n",
    "# Save Test Report\n",
    "test_figs = generate_report(all_labels, all_preds, \"Test\")\n",
    "test_pdf_path = f\"evaluation_result/test_report_{model_name}.pdf\"\n",
    "with PdfPages(test_pdf_path) as pdf:\n",
    "    for fig in test_figs:\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "print(f\"[✅] Test report saved to: {test_pdf_path}\")\n",
    "\n",
    "# ────────────────────────────────────────\n",
    "# Save Validation Report\n",
    "val_figs = generate_report(val_labels, val_preds, \"Validation\")\n",
    "val_pdf_path = f\"evaluation_result/val_report_{model_name}.pdf\"\n",
    "with PdfPages(val_pdf_path) as pdf:\n",
    "    for fig in val_figs:\n",
    "        pdf.savefig(fig)\n",
    "        plt.close(fig)\n",
    "print(f\"[✅] Validation report saved to: {val_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ca0efb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xc/qkmv0r456dn4gsy6rfgnpns80000gn/T/ipykernel_7530/2498070588.py:122: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  band_stats = test_df.groupby([\"confidence_band\", \"correct\"]).size().unstack(fill_value=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[🛠️] Improved error analysis report saved to evaluation_result/error_analysis_deberta.pdf\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_df = pd.read_csv(f\"evaluation_result/{model_name}_predictions.csv\")\n",
    "all_labels = test_df[\"true_label\"]\n",
    "all_preds = test_df[\"predicted_label\"]\n",
    "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# Define improved error report path\n",
    "error_pdf_path = f\"evaluation_result/error_analysis_{model_name}.pdf\"\n",
    "\n",
    "\n",
    "# Confusion matrix\n",
    "fig_err1, ax_err1 = plt.subplots(figsize=(6, 5))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_names, yticklabels=class_names, ax=ax_err1)\n",
    "ax_err1.set_title(\"Confusion Matrix\")\n",
    "ax_err1.set_xlabel(\"Predicted Label\")\n",
    "ax_err1.set_ylabel(\"True Label\")\n",
    "\n",
    "# ─────────────────────────────\n",
    "# Classification Metrics Table\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "recall = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "f1 = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "\n",
    "# Metrics Table Plot\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Compute metrics\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "precision_macro = precision_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "recall_macro = recall_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "f1_macro = f1_score(all_labels, all_preds, average=\"macro\", zero_division=0)\n",
    "\n",
    "precision_per_class = precision_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "recall_per_class = recall_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "f1_per_class = f1_score(all_labels, all_preds, average=None, zero_division=0)\n",
    "\n",
    "class_names = [\"Negative\", \"Neutral\", \"Positive\"]\n",
    "# ─────────────────────────────\n",
    "# Table 1: Overall Metrics\n",
    "fig_overall, ax_overall = plt.subplots()\n",
    "overall_metrics = [\n",
    "    [\"Accuracy\", f\"{accuracy:.2f}\"],\n",
    "    [\"Macro Precision\", f\"{precision_macro:.2f}\"],\n",
    "    [\"Macro Recall\", f\"{recall_macro:.2f}\"],\n",
    "    [\"Macro F1-Score\", f\"{f1_macro:.2f}\"],\n",
    "]\n",
    "ax_overall.axis(\"off\")\n",
    "table1 = ax_overall.table(\n",
    "    cellText=overall_metrics,\n",
    "    colLabels=[\"Metric\", \"Score\"],\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\"\n",
    ")\n",
    "table1.auto_set_font_size(False)\n",
    "table1.set_fontsize(10)\n",
    "ax_overall.set_title(\"Overall Classification Metrics\", fontweight=\"bold\", pad=20)\n",
    "\n",
    "# ─────────────────────────────\n",
    "# Table 2: Class-wise Metrics\n",
    "fig_classwise, ax_classwise = plt.subplots()\n",
    "class_metrics = [\n",
    "    [class_names[i], f\"{precision_per_class[i]:.2f}\", f\"{recall_per_class[i]:.2f}\", f\"{f1_per_class[i]:.2f}\"]\n",
    "    for i in range(3)\n",
    "]\n",
    "ax_classwise.axis(\"off\")\n",
    "table2 = ax_classwise.table(\n",
    "    cellText=class_metrics,\n",
    "    colLabels=[\"Class\", \"Precision\", \"Recall\", \"F1-Score\"],\n",
    "    loc=\"center\",\n",
    "    cellLoc=\"center\"\n",
    ")\n",
    "table2.auto_set_font_size(False)\n",
    "table2.set_fontsize(10)\n",
    "ax_classwise.set_title(\"Per-Class Metrics\", fontweight=\"bold\", pad=20)\n",
    "\n",
    "\n",
    "\n",
    "# Correct vs Incorrect bar chart\n",
    "fig_err2, ax_err2 = plt.subplots()\n",
    "correct_counts = test_df[\"correct\"].value_counts().sort_index()\n",
    "correct_labels = [\"Incorrect\", \"Correct\"]\n",
    "bars = ax_err2.bar(correct_labels, correct_counts, color=[\"#d62728\", \"#2ca02c\"])\n",
    "ax_err2.set_title(\"Correct vs Incorrect Predictions\")\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax_err2.annotate(f\"{height} ({(height/len(test_df)*100):.1f}%)\",\n",
    "                     xy=(bar.get_x() + bar.get_width() / 2, height),\n",
    "                     xytext=(0, 3),\n",
    "                     textcoords=\"offset points\",\n",
    "                     ha='center', va='bottom')\n",
    "\n",
    "# Sample misclassified entries (with ID)\n",
    "misclassified_df = test_df[test_df[\"correct\"] == False].copy()\n",
    "misclassified_df = misclassified_df.reset_index().rename(columns={\"index\": \"id\"})\n",
    "misclassified_df[\"true_class\"] = misclassified_df[\"true_label\"].map({0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"})\n",
    "misclassified_df[\"pred_class\"] = misclassified_df[\"predicted_label\"].map({0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"})\n",
    "top_errors = misclassified_df.sample(n=min(10, len(misclassified_df)), random_state=42)\n",
    "\n",
    "fig_err3, ax_err3 = plt.subplots(figsize=(10, 6))\n",
    "ax_err3.axis(\"off\")\n",
    "error_table = ax_err3.table(\n",
    "    cellText=top_errors[[\"id\", \"text\", \"true_class\", \"pred_class\", \"confidence\"]].values,\n",
    "    colLabels=[\"ID\", \"Text\", \"True\", \"Predicted\", \"Confidence\"],\n",
    "    cellLoc='left',\n",
    "    loc='center'\n",
    ")\n",
    "error_table.auto_set_font_size(False)\n",
    "error_table.set_fontsize(8)\n",
    "error_table.scale(1, 1.4)\n",
    "ax_err3.set_title(\"Sample Misclassified Examples\")\n",
    "\n",
    "# Add confidence bands\n",
    "bins = np.linspace(0, 1, 6)  # e.g., [0.0, 0.2, 0.4, 0.6, 0.8, 1.0]\n",
    "labels = [\"0–0.2\", \"0.2–0.4\", \"0.4–0.6\", \"0.6–0.8\", \"0.8–1.0\"]\n",
    "test_df[\"confidence_band\"] = pd.cut(test_df[\"confidence\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Group by band and correctness\n",
    "band_stats = test_df.groupby([\"confidence_band\", \"correct\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Plot confidence band breakdown\n",
    "fig_err4, ax_err4 = plt.subplots()\n",
    "band_stats.plot(kind=\"bar\", stacked=True, color=[\"#d62728\", \"#2ca02c\"], ax=ax_err4)\n",
    "ax_err4.set_title(\"Correct vs Incorrect Predictions by Confidence Band\")\n",
    "ax_err4.set_xlabel(\"Confidence Band\")\n",
    "ax_err4.set_ylabel(\"Number of Predictions\")\n",
    "ax_err4.legend([\"Incorrect\", \"Correct\"])\n",
    "\n",
    "# Save all to PDF\n",
    "with PdfPages(error_pdf_path) as pdf:\n",
    "    pdf.savefig(fig_err1); plt.close(fig_err1)\n",
    "    pdf.savefig(fig_overall); plt.close(fig_overall)  \n",
    "    pdf.savefig(fig_classwise); plt.close(fig_classwise)\n",
    "    pdf.savefig(fig_err2); plt.close(fig_err2)\n",
    "    pdf.savefig(fig_err3); plt.close(fig_err3)\n",
    "    pdf.savefig(fig_err4); plt.close(fig_err4)\n",
    "    \n",
    "\n",
    "print(f\"[🛠️] Improved error analysis report saved to {error_pdf_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc50288-afa0-475b-bc66-4ff78a312006",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
