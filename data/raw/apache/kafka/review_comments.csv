id,pr_number,user,created_at,body
113856314,2929,onurkaraman,2017-04-28T05:53:31Z,Can we contain the `JavaConverters._` import to be within the specific method needing the conversion as is being done elsewhere in KafkaController? Importing at the file-level can lead to a lot of surprises when reading the code.
113856373,2929,onurkaraman,2017-04-28T05:54:07Z,remove. this import already exists.
113856447,2929,onurkaraman,2017-04-28T05:55:07Z,"Every other data structure in the controller uses the old TopicAndPartition. For consistency, can we use TopicAndPartition instead of TopicPartition? "
113856721,2929,onurkaraman,2017-04-28T05:57:39Z,"can just be:
```
newBrokers.foreach(controllerContext.replicasOnOfflineDisks.remove)
```"
113856779,2929,onurkaraman,2017-04-28T05:58:18Z,"can just be:
```
deadBrokers.foreach(controllerContext.replicasOnOfflineDisks.remove)
```"
113857148,2929,onurkaraman,2017-04-28T06:02:27Z,"Can we make this naming consistent with TopicDeletionStopReplicaResult?

We can either change the other class name or maybe change your new class to something like LeaderAndIsrResult or LeaderAndIsrResponseResult?"
113973577,2929,lindong28,2017-04-28T16:51:48Z,I think it may be better to rename `TopicDeletionStopReplicaResult` to `TopicDeletionStopReplicaResponseReceived`. But I don't have a strong opinion on this. I can also rename `LeaderAndIsrResponseReceived` to `LeaderAndIsrResponseResult`. Do you have a strong preference between the two?
113973608,2929,lindong28,2017-04-28T16:51:59Z,Sure. Fixed now.
113973627,2929,lindong28,2017-04-28T16:52:07Z,Sure. Fixed now.
113975537,2929,lindong28,2017-04-28T17:02:05Z,I was thinking that we may want to use the Java version for new code so that we can gradually migrate to the Java version. I have change the code to use TopicAndPartition.
113975948,2929,lindong28,2017-04-28T17:04:27Z,Fixed now.
113975960,2929,lindong28,2017-04-28T17:04:32Z,Fixed now.
113984161,2929,onurkaraman,2017-04-28T17:47:26Z,I'd prefer renaming `TopicDeletionStopReplicaResult` to `TopicDeletionStopReplicaResponseReceived`.
114925916,2929,ijuma,2017-05-05T03:03:47Z,This rule doesn't apply to `JavaConverters` since you need explicit `asScala` and `asJava` calls anyway. That rule was for `JavaConversions` which did things automatically and has been deprecated.
114925955,2929,ijuma,2017-05-05T03:04:37Z,"Yeah, we definitely want to migrate all the code to the Java class. It probably makes sense to do the whole class in one go though."
119011953,2929,becketqin,2017-05-30T04:52:54Z,Why should we rename the class?
119012138,2929,becketqin,2017-05-30T04:55:35Z,This was a public API so it seems worth keeping and maybe mark it as deprecated.
119012234,2929,becketqin,2017-05-30T04:56:56Z,Maybe add some Java doc to describe the exception a little?
119012458,2929,becketqin,2017-05-30T05:00:15Z,Can we add Java doc for metadata request V3?
119012849,2929,becketqin,2017-05-30T05:06:29Z,Java doc is missing.
119012994,2929,becketqin,2017-05-30T05:07:39Z,Nit: name is a little different from the way zk_version was named.
119013074,2929,becketqin,2017-05-30T05:08:58Z,Can just fall through.
120798287,2929,lindong28,2017-06-08T04:35:36Z,Thanks for the information @ijuma.
120800388,2929,lindong28,2017-06-08T05:03:46Z,This patch adds the Scala class `LeaderAndIsrPartitionState` and `MetadataPartitionState`. `MetadataPartitionInfo` makes it more explicit that this is a Java class that corresponds to `MetadataPartitionState`.
120800491,2929,lindong28,2017-06-08T05:05:06Z,Then I wouldn't bother to rename this class if it is public API.
120800642,2929,lindong28,2017-06-08T05:06:43Z,Sure. I added this comment: `Miscellaneous disk-related IOException occurred when handling a request`.
120800786,2929,lindong28,2017-06-08T05:09:03Z,"Sure. Added this comment:
```
Close file handlers used by the FileChannel but don't write to disk. This is used when the disk may have failed
```"
120800846,2929,lindong28,2017-06-08T05:10:00Z,Good catch. Thanks. I replaced it with `is_new` both here and in the Protocol.java.
120800894,2929,lindong28,2017-06-08T05:10:51Z,I forgot this is Java not scala. Good point. Fixed now.
121294118,2929,becketqin,2017-06-11T23:12:45Z,"The comment seems a little convoluted. I think it can just be ""Whether the replica should have existed on the broker or not."""
121294281,2929,becketqin,2017-06-11T23:20:33Z,"Do we need to create `LEADER_AND_ISR_REQUEST_LIVE_LEADER_V1`? It seems that we do not have a consistent convention on when to create a new internal field version. My preference is that we always bump up Request/Response version if either request or response version is bumped, because the version would be used for compatibility check. But for internal fields, we do not have to bump up version if there is no change."
121294385,2929,becketqin,2017-06-11T23:26:10Z,"Yes, this is a public API. Renaming it requires a KIP."
121294648,2929,becketqin,2017-06-11T23:40:16Z,"If the default value of isNew is false, for brokers which are running IBP version lower than 0.11.1, replicas may not be created if a broker has a disk failure? In LeaderAndIsrRequest earlier than V5, it actually means `create the replica if it does not exist`. In this case, I think it might make sense to keep the same behavior on the broker when storage failure is detected. i.e. let the entire broker halt if IBP is lower than 0.11.0."
121294669,2929,becketqin,2017-06-11T23:40:52Z,"BTW, can you also bump up `ApiVersions` as well?"
121294748,2929,becketqin,2017-06-11T23:44:31Z,Good cleanup.
121294952,2929,becketqin,2017-06-11T23:47:17Z,Is this renaming necessary?
121295031,2929,becketqin,2017-06-11T23:52:10Z,The hierarchy here is a little strange. It might be clearer to create a new `AbstractPartitionState` class and let the PartitionState in LeaderAndIsrRequest and UpdateMetadataRequest inherit from it.
121295155,2929,becketqin,2017-06-11T23:56:56Z,See comments about the `PartitionState` hierarchy. 
121295230,2929,becketqin,2017-06-12T00:00:23Z,No need to do this if the struct does not contain the offline_replicas field. A big if statement seems simpler.
121295331,2929,becketqin,2017-06-12T00:05:39Z,Is this change intended? Can the parent directories on different disk have the same name?
121295556,2929,becketqin,2017-06-12T00:16:13Z,It is weird to pass in both controller and a member of the controller. The eventManager field is already private to controller package so it is accessible from here.
121295627,2929,becketqin,2017-06-12T00:18:48Z,Could be replaced by ```result.get(topicPartition).exists(_.isNew)```.
121296146,2929,becketqin,2017-06-12T00:39:50Z,No need to have the brackets around `brokerId`
121296370,2929,becketqin,2017-06-12T00:48:03Z,Some of the logics in this class are the same as in `onBrokerFailure()` probably worth trying to abstract them out to a shared method to handle offline replicas.
121296397,2929,becketqin,2017-06-12T00:49:05Z,No need to have eventManager passed in.
121296413,2929,becketqin,2017-06-12T00:49:40Z,Empty java doc.
121296662,2929,becketqin,2017-06-12T00:59:18Z,Ditto above.
121296804,2929,becketqin,2017-06-12T01:04:53Z,Ditto above.
121297063,2929,becketqin,2017-06-12T01:13:10Z,Is a mutable map enough here?
121297514,2929,becketqin,2017-06-12T01:22:21Z,Why ArrayBuffer?
121297615,2929,becketqin,2017-06-12T01:25:06Z,Would a mutable map work here?
121297859,2929,becketqin,2017-06-12T01:31:27Z,"There should not be duplicates in the `liveLogDirs`, right? So we can just remove the dir for `liveLogDirs`."
121298466,2929,becketqin,2017-06-12T01:44:00Z,I did not see the logic to handle IO failures in log segments loading. Am I missing something?
121299795,2929,becketqin,2017-06-12T02:00:53Z,We should probably exit if broker id load or checkpoint fail.
121305112,2929,becketqin,2017-06-12T03:19:54Z,"It seems that sometimes we are catching both IOException and KafkaStorageException, sometimes we only catch  KafkaStorageException. It might be better to only catch IOException at the the direct thrown and convert them to KafkaStorageException. So in the caller methods we only need to catch KafkaStorageException."
121305435,2929,becketqin,2017-06-12T03:25:16Z,This exception should probably extend from RetriableException instead of ApiException. A disk failure should not cause the client to throw exception to the users. The clients should just retry after the leadership moves to another healthy replica.
121313873,2929,lindong28,2017-06-12T05:50:36Z,Sure. I have updated the comment as you suggested.
121313909,2929,lindong28,2017-06-12T05:50:58Z,Sure. I updated the patch to removed `LEADER_AND_ISR_REQUEST_LIVE_LEADER_V1`.
121314572,2929,lindong28,2017-06-12T05:58:35Z,"Regarding the first comment, it seems safer to disable replica creation if any replica offline. I think it is probably more backward compatible to do so -- currently LeaderAndIsrRequest will fail to create replica if any disk is offline.

Regarding the second comment, good point. I should have updated the controller code to specify the version for LeaderAndIsrRequest and UpdateMetadataRequest based on the configured `interBrokerProtocolVersion`. I have updated the patch to do this and added `KAFKA_0_11_1_IV0` in `ApiVersion`. Thanks!"
121314687,2929,lindong28,2017-06-12T06:00:02Z,`MetadataResponsePartitionState` will be more consistent and clearer than `PartitionMetadata` in describing the usage of this class. It is not necessary to change the class. I replaced the TODO with a comment that says `This is used to describe MetadataResponsePartitionState`.
121314881,2929,lindong28,2017-06-12T06:02:16Z,"Sure. I think we can do that refactor. Since this is not an important refactor, can we do this refactor later or even in separate patch to reduce the chance of conflicts with other commits?

For now I replaced the TODO with a comment that says `This is used to describe LeaderAndIsrPartitionInfo`."
121314992,2929,lindong28,2017-06-12T06:03:26Z,I think the current class name UpdateMetadataRequestPartitionState is good enough. I have removed this TODO.
121315115,2929,lindong28,2017-06-12T06:04:43Z,"The current code is simpler than having an extra `if` statement. I am wondering what is the benefit of the extra `if` statement. It seems to have negligible impact on performance, right?"
121315417,2929,lindong28,2017-06-12T06:08:39Z,"Yes, it is intended. I think `dir.getParent` is exactly the same as `dir.getParentFile.getAbsolutePath` according to their Java doc. `dir.getParent` is a bit shorter. I think parent directories on different disk must have the different name since they are specified as a list of strings using `log.dirs` config. They must be different so that Kafka can distinguish between them."
121315565,2929,lindong28,2017-06-12T06:10:44Z,Good point. Previously I think my IDE tells me it can not be accessed as `controller.eventManager`. I have updated it to use `controller.eventManager`.
121315578,2929,lindong28,2017-06-12T06:10:53Z,Good point. Fixed now.
121315647,2929,lindong28,2017-06-12T06:11:47Z,Great point. I have moved the overlapping logic to a new method named `onReplicaBecomeOffline(newOfflineReplicas: Set[PartitionAndReplica])`.
121315661,2929,lindong28,2017-06-12T06:11:56Z,Sure. Fixed now.
121316067,2929,lindong28,2017-06-12T06:15:48Z,This comment seems OK since it is similar to the comment of `BrokerChangeListener` and `IsrChangeNotificationListener` etc. I can improve it. Is there any example listener class's comment you want me to follow?
121316335,2929,lindong28,2017-06-12T06:18:22Z,"I agree. But all existing listener class (e.g. PartitionReassignmentListener) takes this as the second argument. It seems better to keep the same code style for `LogDirEventNotificationListener`, or we can update all of them to remove the second argument `eventManager`. Which solution do you prefer?"
121316376,2929,lindong28,2017-06-12T06:18:37Z,It is removed now. Thanks.
121316420,2929,lindong28,2017-06-12T06:18:44Z,It is removed now. Thanks.
121316852,2929,lindong28,2017-06-12T06:22:20Z,"I think it can be changed to a mutable map since a volatile var map can usually be replaced by a mutable map and vise versa. Does Kafka code has a preference between the two? I have chosen to use volatile var here because the code may be simpler. 

For example, I can simply do `checkpoints = checkpoints.filterKeys(_.getAbsolutePath != dir)` in `LogCleanerManager.handleLogDirFailure(dir: String)`."
121317056,2929,lindong28,2017-06-12T06:24:16Z,"I think it can be changed to a mutable map since a volatile var map can usually be replaced by a mutable map and vise versa. 

Does Kafka code has a preference between the two? I have chosen to use volatile var here because the code may be simpler. For example, I can simply do `recoveryPointCheckpoints = recoveryPointCheckpoints.filterKeys(file => file.getAbsolutePath != dir)` in `LogManager.handleLogDirFailure(dir: String)`.
"
121319054,2929,lindong28,2017-06-12T06:43:47Z,"Previously I think it is OK not to handle failure in `loadLogs()` because the subsequent requests (e.g. FetchRequest, ProducerRequest, LeaderAndIsrRequest) can trigger `handleLogDirFailure(...)`. Handling log directory failure in `loadLogs()` could allow broker to fail faster if all log directories are offline at the cost of slightly more code. Anyway, I have updated the code to handle IO failure in log segments loading."
121319440,2929,lindong28,2017-06-12T06:47:22Z,We need to pass `LogManager.liveLogDirs` to `LogCleanerManager` by reference and be able to propagate the change in `LogManager.liveLogDirs` to `LogCleanerManager`. Therefore we need to change its type to ArrayBuffer so that it is mutable. Does this make sense?
121319883,2929,lindong28,2017-06-12T06:51:16Z,Note that `liveLogDirs` has type `ArrayBuffer[File]` and `dir` has type `String`. It seems that `ArrayBuffer` only allows remove by position in the array. It does not have a method to remove element by value.
121320013,2929,lindong28,2017-06-12T06:52:20Z,"Can you explain why we should let broker exit if broker id load fail? My concern is that this will cause broker to exit if any log directory goes offline, which defeats the purpose of this KIP."
121320570,2929,lindong28,2017-06-12T06:57:39Z,"It is possible to catch IOException at the the direct thrown and convert them to KafkaStorageException. But the code change required for those try/catch/indentation is probably much more than the code needed for the extra exception in the catch. 

Also, it is probably easier to verify that we catch both IOException and KafkaStrageException in the request handling code than making sure we have a try/catch around every code that touches IO. Does this make sense?"
121320850,2929,lindong28,2017-06-12T07:00:26Z,I think it is probably better to keep `KafkaStorageException` as `ApiException` similar to the definition of`ReplicaNotAvailableException`. The client code should not see `KafkaStorageException` if any replica is offline due to disk failure. Instead the client should see e.g. `LeaderNotAvailableException` which is `RetriableException`.  `KafkaStorageException` will only be received by controller. Does this make sense?
121347432,2929,ijuma,2017-06-12T09:32:10Z,"Since it's an inner class of `MetadataResponse`, there is no need to also add `MetadataResponse` as a prefix to the inner class name."
121349573,2929,ijuma,2017-06-12T09:41:19Z,"The intent is for `KafkaController.eventManager` to be visible for testing (there's a comment next to the field). Maybe we need to change the tests to access it reflectively since it's understandable that people may miss the comment.

The reasoning is that by passing the `KafkaController` everywhere, it makes it harder to understand and limit the responsibility of each class. It would be nicer if we passed more granular instances . For example, this class needs `eventManager`, `config`, `topicDeletionManager` and `controllerChannelManager`. If we passed those instead, we get a picture of what the class needs immediately instead of having to read every line of code."
121349800,2929,ijuma,2017-06-12T09:42:24Z,I explained in another similar comment why `eventManager` is passed explicitly.
121350268,2929,ijuma,2017-06-12T09:44:36Z,"The semantics are not the same if you simply replace a volatile map with a mutable one in the face of concurrency (I haven't looked at the code, but the volatile implies that multiple threads are in play here)."
121351052,2929,ijuma,2017-06-12T09:48:27Z,You can remove it by value by using `def -= (x: A): this.type`.
121467556,2929,lindong28,2017-06-12T16:56:01Z,"@ijuma It may be useful to rename this class such that, instead of doing `new MetadataResponse.PartitionMetadata(...)` in the code, we can do `import org.apache.kafka.common.requests.MetadataResponse.PartitionMetadata` and `new PartitionMetadata(...)` e.g. in `MetadataCache.java`. The latter is more concise.

Anyway, I have replaced this TODO with just a comment."
121468083,2929,lindong28,2017-06-12T16:58:09Z,"@ijuma Thanks for the explanation. Do you prefer to include `eventManager` explicitly in the constructor of `ControllerBrokerRequestBatch`, or is it OK to reference `eventManager` via `KafkaController`?"
121468423,2929,lindong28,2017-06-12T16:59:26Z,@ijuma Thanks. I think I will keep the existing approach of including `ControllerEventManager` in the constructor of those listeners since it is needed in the test. We can refactor them in a separate patch if it is necessary.
121469351,2929,lindong28,2017-06-12T17:03:14Z,@ijuma Yeah I understand the meaning of volatile. Can you explain a bit more specifically the case in which one can not be replaced by the other? Thank you.
121483933,2929,lindong28,2017-06-12T18:02:17Z,@ijuma @becketqin You are right. I have updated the patch to use `liveLogDirs -= new File(dir)`. 
121559449,2929,ijuma,2017-06-13T00:33:56Z,"If one uses a volatile var, it's possible to update multiple entries in the map without exposing the intermediate states. With mutable maps, that's typically not possible without some form of external locking. On the other hand, one can lose updates by using a volatile var. Generally, it's clearer to use a ConcurrentMap if that is the intent."
121560258,2929,lindong28,2017-06-13T00:41:47Z,"Thanks for the explanation @ijuma. I have two follow-up questions. Suppose we don't use a thread-safe map and choose to use an external lock to protect a non-thread safe map, then are these two approaches equivalent?

And in this specific case, do you recommend me to replace the `@volatile var map` with a `ConcurrentMap`?"
121563032,2929,ijuma,2017-06-13T01:08:16Z,"If you have an external lock during updates in both cases (like in this example), then the volatile version has the advantage that you don't need a lock on reads. The disadvantage is that there is potentially more copying. In this particular case, you are using `filterKeys` which is just a view on the underlying map. That doesn't seem like a good idea. Aside from that, I have to look at the changes more closely to have a worthwhile opinion on what should be done here."
121759231,2929,becketqin,2017-06-13T18:27:55Z,Doing that later is fine.
121762930,2929,becketqin,2017-06-13T18:42:07Z,"I am not sure about why it is safer to disable replica creation on a broker with failed directory. Does that mean a broker will not get assigned any new replica as long as it has one disk failed? Does that require controller awareness? But the controller does not know about that if it runs old IBP, right? "
121763195,2929,becketqin,2017-06-13T18:43:15Z,We should probably just remove the `@param controller` if there is no java doc for it.
121765991,2929,becketqin,2017-06-13T18:53:54Z,"This code is only called at startup time. It is to protect against data messed up from different brokers. Since the broker hasn't started yet, It is probably OK for the broker to shutdown. Users can remove the log dir from the path and restart the broker in this case."
121779680,2929,lindong28,2017-06-13T19:51:54Z,"Note that this only matters when there is disk failure. Let me try to explain it based in the following different scenarios:

1) All brokers are running old code. 

The choice of the default value doesn't matter in this case because the code in this patch won't be used.

2) Brokers are rolling upgraded to use the new code but the IBP is still old.

Here is the concern with setting default value to true. Suppose the broker is running new code and partition p1 is in an offline log directories of this broker. If controller is running old code and sends LeaderAndIsrRequest asking this broker to be leader of p1, the broker will re-create partition p1 on a good log directory. The follower will truncate the log for p1 and the data will be lost. And KIP-112 currently doesn't handle it the case that the offline disk is repaired and used again with p1 still in it.

On the other hand, with default value set to false, broker will refuses to become leader for p1, controller will not handle re-elect leader for p1 because it is running old code. During this period partition will become unavailable because the producer/consumer will send request to this broker and receive NotLeadderForPartitionException. However, this will only happen for a short period of time and the problem will be solved once the controller is moved to a broker which is running new code.

Thus in this case, it is safer to set default value to false.

3) All brokers are running new code but some brokers are still using old IBP.

Suppose controller sends LeaderAndIsrRequest for a new partition p1 to a broker with offline log directory, the broker will refuse to create log for p1 and return error in LeaderAndIsrResponse. Since the controller is running new code, it will handle the error properly by re-electing leader for this partition. In the rare scenario that all brokers elected to be replicas for this partition have offline disk, the partition will be offline. But the chance of this happening is low and this will be addresses once the IBP is fully upgraded.

4) All brokers are running new code with the new IBP.

In this case the LeaderAndIsrRequest will explicitly specify the isNew field and the default value doesn't matter.

Does this make sense?"
121779812,2929,lindong28,2017-06-13T19:52:31Z,Thanks @ijuma! I will also think about it more.
121780457,2929,lindong28,2017-06-13T19:55:32Z,"I think the purpose of KIP-112 is to make sure that broker be able to serve replicas on the good log directory even if there is bad log directory. We can not achieve this goal and the availability will be reduced if broker can not startup due to disk failure.

Can you explain a bit more why it is a concern if broker id load fails in one log directory? Does this cause data corruption?"
121784846,2929,lindong28,2017-06-13T20:15:35Z,Yeah you are right. I have removed it from the patch.
122559063,2929,lindong28,2017-06-17T02:39:55Z,"Discussed with @becketqin offline. We decide to assume isNew = false if IBP is old.

There are two options here when IBP is old. One is to be behavior of the current patch, i.e. broker will not shutdown if there is offline replica, and if there is new replica is assign to a broker will offline log directory, its creation will fail and the replica will be offline even if it can be created on a good replica.

The other solution is to simply let broker shutdown if there is offline log directory. This allows new partition/topic to be assigned to only live broker and they won't be offline immediately after creation.

We choose the first solution because it can keep the replicas on good log directories online at the cost of having all new replicas on that broker offline. It is likely that the number of new replicas assigned to that broker will be much less than the number of replicas on the good log directories of that broker. Also, if user does prefer the second solution, he/she can manually shutdown that broker so that new replica will be online.

I have updated the notes in `upgrade.html` to explain this.

"
122559075,2929,lindong28,2017-06-17T02:40:54Z,"Discussed offline. I have updated the patch so that if there is IOException when reading brokerId from metadata file, the corresponding log dir will be marked failure but the broker will not shutdown. 

On the other hand, after broker registers itself in the zookeeper, it will only try to checkpoint brokerId in the metadata files in the live log directory (i.e. `logManager.liveLogDirs`). Most likely there won't be IOException when writing to metadata files here. If there is, then broker will shutdown entirely."
123092424,2929,becketqin,2017-06-20T20:53:29Z,"Hmm, if that is the case, should the storage exception be converted to LeaderNotAvailableException when it is thrown for requests such as produce/fetch/listOffsets? Also, in that case, does that mean the broker needs to hold on the response until the controller moves leadership? Otherwise broker is essentially giving up leadership by itself without get the confirmation from the controller.

Another thing is that we need to make sure the error code returned matches what is reflected in the metadata response.

I am not sure what is the best exception to return to the user in this case, but in general I would rather not reuse exceptions for different scenarios."
123122372,2929,lindong28,2017-06-20T23:32:52Z,Sure. I have made KafkaStorageException a retriable exception.
123122415,2929,lindong28,2017-06-20T23:33:18Z,I have fixed the issue by using the `if` statement as suggested.
123651559,2929,becketqin,2017-06-23T00:30:24Z,We should also add a 0.11.1 version pointing to KAFKA_0_11_1_IV0
123659570,2929,lindong28,2017-06-23T02:08:06Z,@becketqin Sure. I have updated the patch to address this issue. And the patch has been rebased onto the latest trunk.
123817916,2929,becketqin,2017-06-23T18:38:02Z,Do we need to create this field version?
123832032,2929,becketqin,2017-06-23T19:54:25Z,"For these two cases, the broker needs to read data from internal topics asynchronously. It seems that in both case the broker will only log an error without notify the controller that they cannot serve as leaders for those two partitions."
123836327,2929,becketqin,2017-06-23T20:20:01Z,Looked a bit more. It seems that we are not handling the disk exceptions thrown from FileRecords.readInto(). That method is also used by LogCleaner. Could you check?
123840858,2929,becketqin,2017-06-23T20:45:37Z,The `truncateTo` method can throw storage exceptions. If that storage exception happens in the `ReplicaFetcherThread` it seems we are not handling that.
123869098,2929,lindong28,2017-06-24T03:09:49Z,Sorry. Removed now.
123869183,2929,lindong28,2017-06-24T03:15:45Z,Thanks for catching this. I have added `try/catch` for `truncateTo`.
123871120,2929,lindong28,2017-06-24T05:32:43Z,Sure. I have updated the code to handle IOException here.
123884003,2929,becketqin,2017-06-24T19:40:12Z,This looks a little verbose. Maybe it could just be `UpdateMetadataRequest.PartitionState`
123884171,2929,becketqin,2017-06-24T19:49:55Z,"Should this class be in UpdateMetadataRequest? This patch introduces/modifies some classes containing similar fields, can we clean them up? We had some discussion on that before and I though a follow up patch is also fine. Could you create another ticket to track this? More specifically, if there are common fields, an abstract partition state class would probably help. If there are additional partition state in different requests, those PartitionsState class should ideally extends from the abstract class and sit together with the corresponding requests."
123884367,2929,becketqin,2017-06-24T20:01:18Z,"We are already checking the `isReplicaLocal(replicaId)` above, can this logic be merged into that if statement?"
124189370,2929,becketqin,2017-06-27T06:45:27Z,"Typo, filel -> file"
124197932,2929,becketqin,2017-06-27T07:34:37Z,@jjkoshy @sutambe I have the same question here. Is it just to make sure we do not *lose* the log that failed to be removed?
124200168,2929,becketqin,2017-06-27T07:47:39Z,Is this the same as`logManager.liveLogDirs`?
124413939,2929,becketqin,2017-06-27T22:35:56Z,"It seems that calling `ReplicaManager.handleLogDirFailure()` may cause deadlock here. Say the broker is handling an LeaderAndIsrRequest, it will first grab the `ReplicaManager.ReplicaStateChangeLock` and then grab the `AbstractFetcherThread.partitionMapLock`. However, when handling disk failure in `ReplicaFetcherThread.processPartitionData()`, the locking order is reversed.

One way to solve this is to add an abstract error handling method in AbstractFetcherThread and invoke that out of the `partitionMapLock`."
124416864,2929,becketqin,2017-06-27T22:55:08Z,LogDirUtils?
124417294,2929,becketqin,2017-06-27T22:58:13Z,"It seems that in the existing code the broker will halt if `LogManager.cleanupLogs()` sees a disk exception here. But with the patch, it will just silently fail."
124418494,2929,becketqin,2017-06-27T23:07:19Z,Is this block needed?
124429369,2929,lindong28,2017-06-28T00:32:18Z,Sure. It is renamed now.
124429977,2929,lindong28,2017-06-28T00:38:11Z,"Sure. I can clean them up. In the interest of reducing conflicts with other patches, I refactor the code and clean it up in a follow up patch. I will create the ticket after this patch is committed."
124430275,2929,lindong28,2017-06-28T00:41:32Z,The `isReplicaLocal(replicaId)` used above is only called if `replicaId` is not in `assignedReplicaMap`. I couldn't find a good way to merge them. Do you have a good way to do it?
124430429,2929,lindong28,2017-06-28T00:43:07Z,Thanks. Fixed now.
124430768,2929,lindong28,2017-06-28T00:46:48Z,Not exactly the same because `config.logDirs` is a list of string whereas `logManager.liveLogDirs` is a list of File. I have updated the code to avoid the `if` statement.
124438217,2929,lindong28,2017-06-28T02:07:35Z,Great catch! It is fixed now.
124439381,2929,lindong28,2017-06-28T02:19:21Z,Good point. It is renamed now.
124444527,2929,lindong28,2017-06-28T03:16:48Z,"Thanks for catching this. This becomes a problem due to the use of `LeaderEpochCache` in KIP-101. I have updated the patch to halt the system if `FileNotFoundException` is observed.

While it is possible catch the IOException thrown from this place and handle it, it will require a couple of try/catch/handle distributed across the code. And non-trivial change is needed to avoid deadlock because `ReplicaFetcherThread.handleOffsetOutOfRange()` may also trigger this code.

I think the simplest approach is to just keep the existing code, i.e. halt the system if `FileNotFoundException` is thrown from here. I don't think it will affect the availability of JBOD deployment because according to Mayursh's comment, this `FileNotFoundException` is thrown if the broker is configured with RAID. We can re-investigate this problem if this is also an issue when broker uses JBOD."
124445278,2929,lindong28,2017-06-28T03:25:41Z,Yeah this is no longer needed in this test after a recent comment on Jun 5. I will go through the core code after rebase but typically skip the test code if they pass. Thanks for catching this! It is removed now.
124479048,2929,becketqin,2017-06-28T08:19:26Z,"You are right, never mind."
124685896,2929,junrao,2017-06-29T00:03:27Z,Perhaps add a comment that summarizes what's changed in v1 of LeaderAndIsr request?
124686504,2929,junrao,2017-06-29T00:08:37Z,Perhaps add a comment that summarizes what's changed in v4?
124695356,2929,junrao,2017-06-29T01:37:04Z,Could we avoid wrapping KafkaStorageException if e is already of KafkaStorageException?
124696536,2929,junrao,2017-06-29T01:51:10Z,"State change log is for logging actions to requests from the controller. Since this method could be called from serving non-controller request, perhaps it's better to just log it in the server.log."
124711510,2929,lindong28,2017-06-29T04:49:26Z,I will replace `deleteRecursively(...)` with `org.apache.kafka.common.utils.Utils.delete(...)` in the next commit.
124931997,2929,junrao,2017-06-29T23:01:39Z,"Could we do filter { case (tp, log) .. } and map ( case (tp, log) ...} so that it's a bit clearer what's being referenced?"
124932600,2929,junrao,2017-06-29T23:06:13Z,Is this worth logging?
124932609,2929,junrao,2017-06-29T23:06:21Z,Is this worth logging?
124934166,2929,junrao,2017-06-29T23:18:20Z,We probably want to limit the places with direct zkUtils access. Perhaps it's better to send ZK notification in ReplicaManger.handleLogDirFailure()?
124935126,2929,junrao,2017-06-29T23:25:38Z,Does this need to be synchronized inside logCreationOrDeletionLock since liveLogDirs could be changed concurrently? offlineLogDirs could be called from a different thread for metric reporting.
124935421,2929,junrao,2017-06-29T23:28:00Z,Will it be worth adding a per logDir metric so that we can find out which individual log dir is online/offline?
124936765,2929,junrao,2017-06-29T23:37:56Z,"Perhaps it's better to only capture IOException here? For other exceptions, it's probably better to just fail the broker as before."
124940850,2929,junrao,2017-06-30T00:13:52Z,"There are a few places like LogManager.truncateTo() where we call LogManager.handleLogDirFailure() directly. It seems that they should all call ReplicaManager.handleLogDirFailure() instead since it does things like taking the partition off AllPartitions and removing the partition from replicaFetcherManager, which need to be done on an offline logDir."
124947219,2929,junrao,2017-06-30T01:21:12Z,"So, here, we are not communicating the truncation error back to the replica fetcher thread. Ideally, if the truncation fails, we shouldn't let the replica fetcher proceed with the subsequent fetching. "
124948445,2929,junrao,2017-06-30T01:30:13Z,Is .toSeq needed?
124948786,2929,junrao,2017-06-30T01:33:58Z,"For consistency, it seems that we need to handle IOException in truncateFullyAndStartAt() and call handleLogDirFailure() too?"
125063695,2929,junrao,2017-06-30T15:15:48Z,"We requeue the logs that failed deletion mostly because of IOException. So, I am not sure if we need to requeue removedLog now."
125064418,2929,junrao,2017-06-30T15:18:51Z,Could we add a comment to explain what isNew means?
125069246,2929,junrao,2017-06-30T15:40:50Z,"In line 709, we convert an IOException to KafkaStorageException in log.append.  I am wondering if this is needed since the caller of append handles both IOException and KafkaStorageException.

Also, in ReplicaManager, sometimes we catch KafkaStorageException and some other times we catch both IOException and KafkaStorageException. It would useful to make that consistent. For example, we can make the convention that IOException will be throw from java library and KafkaStorageException will be throw in the Kafka code (but not wrapping IOExceptions from java). Then the caller will catch both IOException and KafkaStorageException"
125073587,2929,junrao,2017-06-30T16:00:19Z,"Hmm, not sure why we need the additional check here. If replicaId is not in assignedReplicaMap, it probably shouldn't lead to a KafkaStorageException."
125087026,2929,junrao,2017-06-30T17:13:36Z,"We probably need to do the following optimization in delete() here?

CoreUtils.swallow(forceUnmap(mmap))"
125088146,2929,junrao,2017-06-30T17:19:41Z,unused import
125094553,2929,junrao,2017-06-30T17:51:33Z,Could we adjust the comment of the return value accordingly?
125097858,2929,junrao,2017-06-30T18:05:59Z,PartitionState seems unused?
125101226,2929,junrao,2017-06-30T18:21:56Z,"Hmm, doLoadGroupsAndOffsets() just runs in a scheduler. Should we call ReplicaManager.handleLogDirFailure() on IOException too? If so, we probably want to do the same in TxnManager."
125109233,2929,junrao,2017-06-30T19:02:30Z,"In controller failover, would it be worth to clean up all leftover sequence nodes in logDirEventPath? Those nodes won't be useful since the new controller will send a LeaderAndIsrRequest to every broker on failover."
125112249,2929,junrao,2017-06-30T19:19:03Z,"If a disk fail in the code path outside of the log cleaner, we should remove that dir from checkpoints too. Is that logic added already?"
125120383,2929,lindong28,2017-06-30T20:05:00Z,Thanks for catching this. I have updated the patch to say `LEADER_AND_ISR_REQUEST_V1 added a per-partition is_new field. This field specifies Whether the replica should have existed on the broker or not`.
125120952,2929,lindong28,2017-06-30T20:08:44Z,"Sure. I added the following comment for `UPDATE_METADATA_REQUEST_PARTITION_STATE_V4`, `UPDATE_METADATA_REQUEST_V4` and `METADATA_RESPONSE_V5`.

`... added a per-partition offline_replicas field. This field specifies the list of replicas that are offline`"
125121473,2929,lindong28,2017-06-30T20:12:07Z,"Sure. I fixed it with the follow code:

```
e match {
  case storageException: KafkaStorageException => throw storageException
  case _ => throw new KafkaStorageException(s""Error processing fetch operation on partition ${tp}, offset $offset"", e)
}
```"
125121745,2929,lindong28,2017-06-30T20:13:47Z,Previously I thought this is used for making partition state change. They are the same because previously all state change are triggered by controller request. I have changed the code to log it in the server.log.
125123322,2929,lindong28,2017-06-30T20:23:17Z,"When `LogCleaner` encounters IOException when cleaning up the log, we want to mark the corresponding log directory as offline and inform controller of the log directory failure via zookeeper. Note that this IOException is not triggered by an external request and thus will not go through `ReplicaManager`. Since we probably don't want to reference ReplicaManager via LogManager, `LogManager.handleLogDirFailure()` seems to be the only reasonable place to have this log of wrting to log directory notification znode.

I couldn't find a better way to address this problem. Do you have a better solution?"
125130310,2929,lindong28,2017-06-30T21:06:04Z,"Another reason to put the logic of zookeeper notification in LogManger is that, currently LogManager is managing which logs are online or offline and thus is the source of truth of offline log directories. Ideally we would like to put the logic of notification in the same place so that the notification is sent if and only if the offline log directories change."
125136812,2929,lindong28,2017-06-30T21:52:18Z,"Previously I think the chance of exception due to this race condition is so small (because disk failure should be rare) that we don't want to degrade performance by getting lock every time we read `liveLogDirs`. For example, metrics reporting will access `AbstractFetcherManager.fetcherThreadMap` and it seems fine so far.

But strictly speaking, you are right that this can cause race condition and it is better to prevent this completely from any unknown consequence. So I made the following changes to address the problem:

1) Use `_liveLogDirs: ArrayBlockingQueue[File]` to record offline log directories. We don't have to worry about blocking operation because we will only remove log directory from it. 

I think this may be a little better than using `logCreationOrDeletionLock synchronized {}` every time we access `_liveLogDirs` (e.g. `checkpointLogRecoveryOffsets()`, `checkpointLogStartOffsets` and metrics reporting) so that these access won't block `LogManager.createLog()` or `LogManager.asyncDelete()`

2) Add `def liveLogDirs: Array[File] = _liveLogDirs.asScala.toArray` so that we don't have to change test code to work with `ArrayBlockingQueue[File]`.

3) Update various checkpoint() methods (e.g. `checkpointLogStartOffsetsInDir(dir: File)`) so that they can handle the case that the directory in the input parameter gets removed from the checkpoint map right after the method begins."
125137370,2929,junrao,2017-06-30T21:56:44Z,"We could probably pass in a onLogDirFailure callback to the LogCleaner. An IOException could be triggered in different places. However, independent of where it's triggered, we always want to react with the same process, which includes (marking the partition/replica as offline, remove partition from replicaFetcher, notify the controller, etc). ReplicaManager seems to be the best place to consolidate that process. "
125138864,2929,lindong28,2017-06-30T22:07:56Z,"Previously I don't think it is needed because if user wants to know which specific log directories are offline in addition to the offline log directory count, it probably means they want to fix the problem and they will login the machine anyway. In this case they can find the offline log directory name in the log. And user probably wants to fix the problem because the log itself gets deleted from the cluster because they need to see the exception trace to debug the problem.

Yes, it can make debug easier to have this metric. I have updated the patch with the following code:

```
  for (dir <- logDirs) {
    newGauge(
      ""OfflineLogDirectoryCount"",
      new Gauge[Int] {
        def value = if (_liveLogDirs.contains(dir)) 0 else 1
      },
      Map(""logDirectory"" -> dir.getAbsolutePath)
    )
  }
```"
125139125,2929,lindong28,2017-06-30T22:10:15Z,Sure. Good point. I have updated the code to only catch `IOException`.
125142699,2929,lindong28,2017-06-30T22:44:00Z,"@junrao Are you suggesting that `LogManager.handleLogDirFailure()` should invoke `ReplicaManager.handleLogDirFailure()` and `ReplicaManager` should be put in the constructor of the `LogManager`? This can be done. But I am worried that this creates circular dependency between `LogManager` and `ReplicaManager` and can make future development harder.

In addition to the worry with this Java class dependency, I also find hard to organize the methods. For example, say the broker receives a `ProduceRequest`, triggers `ReplicaManager.appendToLocalLog()`, which in turn calls `Partition.appendRecordsToLeader()` and fails with `IOException`. Note that we haven't touched `LogManger` in this path and thus `LogManger.handleLogDirFailure()` won't be called.

Now that the ReplicaManager catches an IOException, ideally it should have its own method `handleLogDirFailure()` to deal with it, e.g. remove the corresponding TopcPartition from `ReplicaManager.allPartitions` and `LogManager.logs`. Then it become pretty straightforward to have `ReplicaManager.handleLogDirFailure()` to call `LogManager.handleLogDirFailure()`. But then it seems weird for `LogManager.handleLogDirFailure()` to invoke `ReplicaManager.handleLogDirFailure()` even though we can use some trick to make it work. Do you have a solution to this circular invocation?

Currently if the IOException is triggered by user's request, both `ReplicaManager.handleLogDirFailure` and `LogManager.handleLogDirFailure()` will be called so this is OK. If IOException is triggered by e.g. `LogCleaner`, only `LogManager.handleLogDirFailure()` will be triggered and some partition will still stay in `ReplicaManager.allpartitions`. As of now I don't find this to be a problem. Because the next time any ProduceRequest or FetchRequest tries to access a partition on that offline log directory, ReplicaManager will handle the log directory failure. Also, I have updated the code so that `ReplicaManager.checkpointHighWatermarks()` will trigger `ReplicaManager.handleLogDirFailure()` if any partition in `allPartitions` is on an offline log directory. Thus the period of inconsistency will be limited. Does this address your concern?



"
125142923,2929,lindong28,2017-06-30T22:46:16Z,Yeah I have concern with having `LogManager` call `ReplicaManager.handleLogDirFailure()`. We can continue the discussion in the other comment.
125144710,2929,lindong28,2017-06-30T23:06:01Z,"Yeah this can be improved. I removed this if statement and replaced this log with `info(s""Stopping serving logs in dir $dir"")`."
125144734,2929,lindong28,2017-06-30T23:06:16Z,No. I have removed this log.
125144776,2929,junrao,2017-06-30T23:06:51Z,"@lindong28 : My suggestion is that nobody should directly call LogManager.handleLogDirFailure() except for ReplicaManager.handleLogDirFailure(). The latter knows how to bring all parts to a consistent state with respect to IOException, no matter where it's introduced. So, if a method hits an IOException through ReplicaManager, we will just the IOException bubble up to ReplicaManager and call ReplicaManager.handleLogDirFailure() there. If the IOException is isolated LogCleaner, we can somehow pass ReplicaManager.handleLogDirFailure() to LogCleaner and let LogCleaner call it. This way, any time a disk error is detected, it will be handled consistently no matter in which component the error is detected, which seems easier to reason about."
125145180,2929,lindong28,2017-06-30T23:12:21Z,"Sure. I have replaced this line with `val offlineTopicPartitions = logs.filter { case (tp, log) => log.dir.getParent == dir}.map { case (tp, log) => tp}`"
125145259,2929,lindong28,2017-06-30T23:13:35Z,Hmm.. I couldn't remember why this is added in the first place.. Removed now.
125148715,2929,lindong28,2017-07-01T00:05:01Z,@junrao I see. Do you think it is OK to have a method `LogManger.setLogFailureCallback()` that will be called in `KafkaServer.startup()` after the `ReplicaManager` is constructed? If yes then I will do it. We can not get not pass this callback to the constructor of `LogManager` because `LogManager` is instantiated before the `ReplicaManager`.
125149426,2929,lindong28,2017-07-01T00:19:47Z,"Sure. I added the following Java doc:

```
  /**
   * If the log already exists, just return a copy of the existing log
   * Otherwise if isNew=true or if there is no offline log directory, create a log for the given topic and the given partition
   * Otherwise throw KafkaStorageException
   *
   * @param isNew Whether the replica should have existed on the broker or not
   * @throws KafkaStorageException if isNew=false, log is not found in the cache and there is offline log directory on the broker
   */
```"
125149731,2929,lindong28,2017-07-01T00:27:04Z,OK. I have updated the code to remove this re-enqueue logic.
125152135,2929,lindong28,2017-07-01T01:57:02Z,"@junrao We need this check to address the issue we have been discussing in the other threads, i.e. the partition may have been removed in `LogManager` but not in `ReplicaManager`. In this case, replicaId will be in the assignedReplicaMap even though the replica is offline. This additional check will make sure that `getOrCreateReplica()` will throw KafkaStorageException in this case.

More specifically, here is the currently workflow if LogCleaner encounters IOException:

- `LogManager.handleLogDirFailure()` will remove this log dir and partitions and inform controller via zookeeper.
- Controller sends LeaderAndIsrRequest for all partitions on this broker
- `Partition.getOrCreateReplica()` for partitions on the offline log directory will throw KafkaStorageException, which in turn triggers `ReplicaManager.handleLogDirFailure()` so that those partitions can be removed from `ReplicaManager` as well."
125152284,2929,lindong28,2017-07-01T02:07:26Z,"Yeah I would like to invoke `ReplicaManager.handleLogDirFailure()` whenever there is IOException. But this is not done yet because `LogManager` is not able to reference `ReplicaManager` since it is instantiated before the `ReplicaManager`. We can address this problem by doing `LogManager.setOnLogDirFailureCallback()` after the ReplicaManage is instantiated. I find this to be a big ugly though. What do you think?

"
125152579,2929,lindong28,2017-07-01T02:25:56Z,"@junrao BTW, here is the current workflow if `LogManger.handleLogDirFailure()` is called by `LogCleaner`.

- `LogManager.handleLogDirFailure()` will remove this log dir and partitions and inform controller via zookeeper.
- Controller sends LeaderAndIsrRequest for all partitions on this broker
- `Partition.getOrCreateReplica()` for partitions on the offline log directory will throw KafkaStorageException, which in turn triggers `ReplicaManager.handleLogDirFailure()` so that those partitions can be removed from `ReplicaManager` and the `ReplicaFetcherManager`."
125152647,2929,lindong28,2017-07-01T02:29:54Z,"Sure. I have updated the patch to catch IOException here.

Previously I had a long discussion with @becketqin and we think it is OK to handle IOException that currently doesn't trigger `halt()` in a followup patch. The reasons are recorded in this pull request. That is why I didn't catch IOException here."
125153306,2929,lindong28,2017-07-01T03:12:25Z,"The line in 709 is there before this patch. That is no longer needed after KIP-112. I kept it there to avoid changing the indention of that code block so that the code diff and the code review can be a bit simpler. I will remove that conversion after most all comments have been addressed.

Besides the motivation of avoid changing code indentation, the current patch only convert IOException to KafkaStorageException if the method needs to handle that IOException before throwing that again. I assume that we don't want to have try/catch around all Java library to convert IOException to KafkaStorageException for code simplicity. As a result our Kafka method may potentially throw both IOException and KafkaStorageException and that is why some code needs to catch both.

Regarding the consistency, are you suggesting that the code that currently catches only the `KafkaStorageException` should be updated to catch both `IOException` and `KafkaStorageException` and treat them as `KafkaStoragException`?"
125153341,2929,lindong28,2017-07-01T03:15:02Z,Good point. I didn't realize we had this method. I have updated the code as suggested.
125153345,2929,lindong28,2017-07-01T03:15:33Z,Thanks. Fixed now.
125153388,2929,lindong28,2017-07-01T03:18:58Z,"Ah I should have done this. Thanks for noticing this. I have updated the patch with the following comment:

```
* The log directories whose meta.properties can not be accessed due to IOException will be returned to the caller
*
* @return A 2-tuple containing the brokerId and a sequence of offline log directories.
```"
125153403,2929,lindong28,2017-07-01T03:20:24Z,My bad.. Removed now.
125153742,2929,lindong28,2017-07-01T03:43:25Z,"Good catch! I missed the fact that this method is run in a scheduler. Sure. I have updated the patch to call RelicaManager.handleLogDirFailure() in `doLoadGroupsAndOffsets()` when there is `IOException`. I also reverted the change that was made to update the `responseMap` when there is `IOException`.

Becket had similar comment and asked me to do the same in `TransactionStateManager`. I didn't do that because I am not sure I know `TransactionStateManager` good enough to handle IOException in the best way. This is an existing problem with `TransactionStateManager` and probably other modules in Kafka as well because they don't explicitly handle the scenario when the disk write operation fails. Can we focus on the IOException that currently causes `halt()` and leave the handling of IOExeption that are currently ignored (e.g. in `TransactionStateManager`) in follow up patch?"
125153969,2929,lindong28,2017-07-01T03:57:57Z,"No, that logic is not added. There are two reason I didn't do that. 

One reason is to avoid circular invocation. Currently `LogCleaner.handleLogDirFailure()` and `LogCleanerManager.handleLogDirFailure()` will invoke `LogManager.handleLogDirFailure()`. It seems a bit ugly if we let `LogManager.handleLogDirFailure()` invoke `LogCleanerManager.handleLogDirFailure()`. Ideally the invoke between methods is a directed acyclic graph.

The other reason is that it is not necessary to explicitly remove dir from `LogCleanerManager.checkpoints` as long as `LogManager.handleLogDirFailure()` has already removed all partitions on the offline dir from `LogManager.logs`. Note that all read or write operation on `LogCleanerManager.checkpoints` is triggered by the partition in that log directory, which in turn comes from `LogManager.logs`. Thus if all partitions on a dir is removed from `LogManager.logs`, it is guaranteed that this `dir` will no longer be used as key to access `LogCleanerManager.checkpoints`. Does this address the problem?"
125154247,2929,lindong28,2017-07-01T04:22:13Z,"Good point. I have added the following methods in `LogDirUtils` and invoked this at the beginning of `onControllerFailover()`.

```
def deleteLogDirEvents(zkUtils: ZkUtils) {
  val sequenceNumbers = zkUtils.getChildrenParentMayNotExist(ZkUtils.LogDirEventNotificationPath).toSet
  sequenceNumbers.map(x => zkUtils.deletePath(ZkUtils.LogDirEventNotificationPath + ""/"" + x))
}
```"
125369462,2929,lindong28,2017-07-04T00:07:26Z,@junrao I realized that there is one way to do this without using `LogManger.setLogFailureCallback ()`. We can provide `KafkaServer` to the constructor of `LogManager`. Then `LogManager.handleLogDirFailure()` can call `kafkaServer.replicaManager.handleLogDirFailure()`. The disadvantage is that this exposes a log of things to `LogManger`. Does this sound reasonable?
125761540,2929,junrao,2017-07-05T21:25:42Z,It may not be efficient to do the exists check in every iteration of topicPartition since the exists check requires a scan of allPartitions. Perhaps we could figure out the topics to be removed by first getting the uniqe topics from newOfflinePartitions and then do the exists check on allPartitions?
125762622,2929,junrao,2017-07-05T21:31:15Z,Perhaps it's better to rename createLog to getOrCreateLog?
125773929,2929,junrao,2017-07-05T22:34:28Z,"Hmm, currently, the java producer from 0.11.0.0 will treat this new exception as an unknown exception and won't retry. Since we may add new error code that may be retriable in the future, perhaps we should make UnknownServerException a retriable error? If so, we probably want to do that in a separate patch and patch the 0.11.0 and probably the 0.10.2 branch as well."
125796722,2929,junrao,2017-07-06T01:52:09Z,"@lindong28 : Good point on the circular dependency on constructing those objects. Another approach that I am thinking is to have a DiskFailureChannel which contains an in-memory queue. DiskFailureChannel can be passed into all components such as LogMananger, LogCleaner, ReplicaManager, etc where IO errors can be generated. If an IO error happens in a component, it just enqueues the disk name into the DiskFailureChannel. We can have a separate thread (maybe in ReplicaManager) that reads from DiskFailureChannel and acts on it (e.g., remove partitions in ReplicaManager, remove logs in LogManager, remove partitions from replica fetcher), etc. This way, all disk errors will be handled consistently. The only thing is that disk errors will be handled asynchronously. However, I am not sure if disk errors need to be processed synchronously. "
125804911,2929,lindong28,2017-07-06T03:30:34Z,"@junrao Thanks for the suggestion!

One concern with having this queue for disk failure events is that, in the event a log directory fails, every attempt to access any replica in that log directory will generate a disk failure event for the same log directory. A lot of events may be instantiated and put into this queue which wastes CPU and memory. I am also concerned that we can not bound the number of events generated for a single disk failure since we don't know when the OS will schedule that thread to read event from this queue.

Maybe we don't need this `DiskFailureChannel`. We can just schedule a thread in `ReplicaManager` to read from `LogManager.offlineLogDirs` and call `ReplicaManager.handleLogDirs(dir)` when there is new offline log dir.

I understand that the goal of this alternative approach is to make sure that the state in `ReplicaManager` (e.g. `ReplicaManager.allPartitions`) is consistent with the state in `LogManager` (e.g. `LogManager.logs`). And I agree that we can achieve this goal by having an extra thread in `ReplicaManager`. But I am not sure that the benefit of this alternative approach is worth the extra thread. Note that the disadvantages of the alternative approach is: 1) It requires an extra thread in `ReplicaManager` which makes Kafka's Java class a bit more complicated; and 2) The log failure event will be processed asynchronously which potentially delays the controller notification and leader election. And we can not bound the delay.

Can you help me understand why it is necessary to keep state in `ReplicaManager` and `LogManager` consistent all the time? The current approach in the patch guarantees eventual consistency, i.e. the `ReplicaManager.handleLogDirFailure()` will be called after controller notification is sent and the broker receives `LeaderAndIsrRequest` from controller. It seems that nothing will go wrong during the period of inconsistency -- if anything goes wrong then we will have the same issue with the alternative approach. And the alternative approach makes state consistent by delaying the execution of `LogManager.handleLogDirFailure()`.

I will implement the alternative approach using an extra thread if the consistency is more important than the extra complexity and the potential delay in log failure handling.






"
125806173,2929,lindong28,2017-07-06T03:47:24Z,"Good point. I replaced this with the following code:

```
newOfflinePartitions.foreach { topicPartition =>
  val partition = allPartitions.remove(topicPartition)
  partition.removePartitionMetrics()
}

newOfflinePartitions.map(_.topic).toSet.foreach { topic =>
  val topicHasPartitions = allPartitions.keys.exists(tp => topic == tp.topic)
  if (!topicHasPartitions)
    brokerTopicStats.removeMetrics(topic)
}
```"
125806333,2929,lindong28,2017-07-06T03:49:38Z,Sure. I have renamed this method as `getOrCreateLog()`
125807634,2929,lindong28,2017-07-06T04:07:01Z,"Now I understand the question.

I am not sure we should make `UnknownServerException` a retriable error because I am a bit concerned with having unnecessary retry if we do that. I think we can keep the current practice by making an error retriable only if we know it should be retriable. Is there any reason that we should make `UnknownServerException` retriable?

I understand that `KafkaStorageException` was previously non-retriable because it is send to client as `UnknownServerException`. But I couldn't find anything wrong with making it retriable now. It also doesn't break any contract between client and server because client is now receiving a new error (i.e. KafkaStorageError) from client's perspective. Is there any concern with this approach?





"
126026144,2929,junrao,2017-07-06T21:54:51Z,"The issue that I am thinking is the following. The 0.11.0 client jar doesn't have the error code for KafkaStorageException. If the 011.1 server sends a KafkaStorageException to the 0.11.0 client, the client will treat it as an UnknownServerException and won't retry. However, ideally, we want the 0.11.0 client to retry in this case. "
126036453,2929,junrao,2017-07-06T22:55:31Z,"@lindong28 : My thoughts are the following. (1) When reacting to a disk failure, it would be useful to handle this in order, e.g., removing the partition from ReplicaManager, followed by removing the log from LogManager. If you only remove the log from LogManager w/o removing the partition from ReplicaManager, then a request may hit an unexpected exception when trying to access the log, which is not ideal. (2) The notification to ZK could take a long time if ZK is not performing. Doing the ZK notification in a background thread reduces the potential latency impact to the client request. (3) We may implement a more sophisticated disk failure detection module (e.g., proactively verifying CRCs in the log) in the future. Having a DiskFailureChannel allows us to integrate such a module easier (such a module only needs to interact directly with DiskFailureChannel, instead of ReplicaManager).

I agree that adding a separate disk failure handling thread adds a bit complexity, but probably not too much. To avoid building up the queue, perhaps DiskFailureChannel can just maintain a disk dir set instead of a queue."
126036708,2929,lindong28,2017-07-06T22:57:26Z,"I see. This makes sense. One concern with making `UnknownServerException` a retriable exception is that client may retry unnecessarily. Maybe we can reduce unnecessary retries by distinguishing between `UnknownServerException` and `UnknownException`. An exception is an `UnknownException` if the error code is not found defined, .e.g. 0.11.0 client receives the error code for `KafkaStorageException`. Thus the client wouldn't have to retry if server sends the error code of the existing server unknown exception. And the client will only retry unnecessarily if its client's version is (temporarily) lower than server's version.

Does this sound OK? I can submit a separate patch for 0.11.0  and 0.10.2 branch.

"
126038803,2929,ijuma,2017-07-06T23:11:22Z,"The downside of any of the suggested approaches is that it won't help already released versions. Another option would be to use an existing RetriableException when dealing with older clients. This would require bumping the relevant protocol versions though.

Going forward, it may make sense to define a `RetriableUnknownException` that the broker can use to force older clients to retry."
126090944,2929,lindong28,2017-07-07T08:08:48Z,"@junrao I agree with your points.  Thanks so much for the detailed explanation. I have updated the patch to include `LogDirFailureChannel` and an extra thread in `ReplicaManager`. The `LogManager.handleLogDirFailure()` will only be called by `ReplicaManager.handleLogDirFailure()`, which in turn will only be called by that thread. And the offline log dir name will be put into `LogDirFailureChannel` if there is new offline dir. The new thread will block waiting for new offline log dir.

I have also updated the patch to revert changes such as the ZkUtils in the LogManager constructor. I have reviewed the changes and the entire patches myself before uploading it. I think all comments have been addressed. Can you take another look at the patch? Thanks much for your time!
"
126091184,2929,lindong28,2017-07-07T08:10:28Z,I have updated the patch so that `LogManager.handleLogDirFailure()` will be called only by `ReplicaManager.handleLogDirFailure()`
126091442,2929,lindong28,2017-07-07T08:12:13Z,I have updated the patch so that the partitions will be removed from the replica fetcher if truncation fails.
126092808,2929,lindong28,2017-07-07T08:20:06Z,"@ijuma Thanks for the suggestion. Yes we can also name that new exception as `RetriableUnknownException`. And `Errors.forCode(code)` will return `RetriableUnknownException` if the code is larger than maximum code of the defined error on the client side. 

My previous idea is to name the new exception as either `UnknownException` which will be retriable. The advantage is that the error will be more explicit about that it is -- a code that is not found in the client's library. The advantage of your solution is that we can re-use the new exception for any retriable exception.

@junrao Does this sound good to you?

"
126236649,2929,junrao,2017-07-07T20:19:46Z,"Hmm, if we bump up the protocol, wouldn't we have the same problem on the old client using the old protocol? i.e, If the broker hits a KafkaStorageException serving a request from the old client, we have to convert KafkaStorageException to sth that the old client can recognize."
126243848,2929,lindong28,2017-07-07T20:57:01Z,"I guess I didn't understand Ismael's suggestion w.r.t. the protocol version..

In my opinion, the long term approach is to have a new retriable exception, named as either `UnknownException` or `RetriableUnknownException` such that an known error code will be translated to this exception on the client side.

One short term solution we can do in this patch is to transform KafkaStorageException to the error code of NotLeaderForPartitionException in ProduceResponse and FetchResopnse. This is hacky. But it should address this problem without causing any additional concern since producer/consumer shouldn't really care whether this is KafkaStorageException as long as it is retriable.

@junrao @ijuma What do you think of this short term solution?"
126252343,2929,ijuma,2017-07-07T21:48:02Z,"@junrao @lindong28 Yes, my suggestion was to bump the protocol so that we reuse an existing retriable exception for the old version and `KafkaStorageException` for the new version. And `RetriableUnknownException` would be a way to avoid this hack in the future, but it doesn't help now."
126256742,2929,lindong28,2017-07-07T22:18:36Z,"@ijuma @junrao Thanks much. This makes sense. I have updated the patch to include `UnknownRetriableException`, bumped up the version of ProduceRequest and FetchRequest, and converted KafkaStorageException to the error code of NotLeaderForPartitionException for existing versions of ProduceResponse and FetchResponse."
126448908,2929,junrao,2017-07-10T15:06:52Z,"Hmm, this thread is interruptible. Could we just make takeNextLogFailureEvent() block infinitely?"
126462482,2929,junrao,2017-07-10T15:51:34Z,"This is going to affect how the ReplicaFetchThread works. Before this patch, if the ReplicaFetchThread hits an IOException during truncate, the affected replica will remain in the truncating state and the truncation will be retried. With the patch, since the IOException is not propagated back to ReplicaFetchThread, the ReplicaFetchThread just assumes that the truncation has succeeded and moves onto the next stage. We want to preserve the original behavior."
127005161,2929,lindong28,2017-07-12T16:30:58Z,This line can be removed. It probably becomes redundant after a rebase.
127013042,2929,lindong28,2017-07-12T17:04:42Z,"Change Java doc to the following

```
Get the next offline log dir from logDirFailureEvent queue.
Block waiting for up to the specified amount of time if there is no new offline log dir.
``"
127014976,2929,lindong28,2017-07-12T17:13:00Z,This can be removed now since we have `LogDirFailureChannel`.
127038150,2929,lindong28,2017-07-12T18:41:58Z,"Never mind. This is still needed. If there is offline log dir, `getOrCreatePartition()` will create the partition object before `getOrCreateReplica()` fails with exception. In this case we need this code to remove the partition which doesn't have a valid local replica."
127068479,2929,junrao,2017-07-12T20:51:59Z,Could we adjust the above comment to reflect initialOfflineDirs?
127068824,2929,junrao,2017-07-12T20:53:23Z,This probably should be named isLogDirectorOffline?
127072655,2929,junrao,2017-07-12T21:09:18Z,_liveLogDirs could change after the size check in line 77. Should we tighten this up?
127074402,2929,junrao,2017-07-12T21:17:20Z,Should we swallow IOException from the destroy() call?
127075492,2929,junrao,2017-07-12T21:21:59Z,while load => while loading? Ditto in line 292.
127077430,2929,junrao,2017-07-12T21:30:38Z,Should we call logDirFailureChannel.maybeAddLogFailureEvent() on initialOfflineDirs? We call logDirFailureChannel.maybeAddLogFailureEvent() on log dirs that fail during loading.
127078294,2929,junrao,2017-07-12T21:34:25Z,Could we log the failed dir too?
127078328,2929,junrao,2017-07-12T21:34:34Z,Could we log the failed dir too?
127084663,2929,junrao,2017-07-12T22:05:32Z,Could we log the failed disk dir too?
127084683,2929,junrao,2017-07-12T22:05:39Z,Could we log the failed disk dir too?
127236631,2929,junrao,2017-07-13T14:42:27Z,"Should we add ""either""?"
127238635,2929,junrao,2017-07-13T14:49:02Z,"Hmm, it doesn't seems that partition.deleteRecordsOnLeader() can throw KafkaStorageException. It can throw IOException though."
127242028,2929,junrao,2017-07-13T15:00:27Z,Perhaps it's simpler to just handle KafkaStorageException here instead of in line 579.
127242867,2929,junrao,2017-07-13T15:03:26Z,Perhaps it's simpler to just handle KafkaStorageException/IOException here instead of in line 765?
127247023,2929,junrao,2017-07-13T15:17:18Z,"Hmm, shouldn't we pass in the isNew flag to the getOrCreateReplica() call in line 182 too?"
127262313,2929,junrao,2017-07-13T16:15:26Z,"Once a partition is removed from allPartitions, future produce/fetch request will get an UnknownTopicPartitionException, ideally, it seems that they should get a KafkaStorageException for consistency?"
127267996,2929,junrao,2017-07-13T16:37:56Z,Could we adjust the message a bit so that UNKNOWN_RETRIABLE can be distinguished from UNKNOWN? It would also be useful to add a comment on how it should be used differently from UNKNOWN.
127268535,2929,junrao,2017-07-13T16:40:18Z,"Hmm, I thought the plan is for uncaught exceptions still be UNKNOWN (i.e., not retriable), but if the server throws a new exception that's retriable, the server will send UNKNOWN_RETRIABLE to old clients."
127270473,2929,junrao,2017-07-13T16:49:09Z,Could we add some comment that describe the flow of how disk failure is handled here?
127276896,2929,lindong28,2017-07-13T17:16:22Z,"I think it is probably not necessary to call this on initialOfflineDirs. The purpose of calling `maybeAddLogFailureEvent()` is to cleanup state (e.g. LogManager.logs) and notifying controller. For initialOfflineDirs, the no state will be created for replicas in these offline log directories. And the broker hasn't registered itself in the zookeeper yet and thus controller will query this broker for state of all replicas afterwards -- thus no need to notify controller either.

On the other hand, if any log directory fails during loading, it is possible that states have been created for some logs that directory but not others. In this case we need to call `maybeAddLogFailureEvent()` to clean up the state."
127280905,2929,lindong28,2017-07-13T17:32:33Z,"I think it is probably OK to keep the current code. It is true that a log failure may happen right after the check in line 77. In this case the the caller may try to access that log directory that just became offline. However, there is no way to prevent this from happening since it is always possible for a log directory to become offline immediately before (or while) the caller tries to access it.

Thus caller code always needs to handle the possibility that log directory was removed from the state (e.g. `LogManager.recoveryPointCheckpoints`). Currently the only regular callers of `liveLogDirs()` are those checkpoint routines. The code has handled with e.g. `this.recoveryPointCheckpoints.get(dir).foreach(...)` to avoid NullPointerException.

Does this make sense?"
127281494,2929,lindong28,2017-07-13T17:35:08Z,"Initially I think it may be more concise to re-use the existing metric name with the additional tag. Sure, I will replace this with `isLogDirectorOffline`."
127282168,2929,lindong28,2017-07-13T17:37:54Z,"Sorry. It is bad that I forgot to update comment. I have updated it to `Create and check validity of the given directories that are not in the given offline directories...`

I will go over the patch and see if there are other comments that I should update."
127282599,2929,lindong28,2017-07-13T17:39:43Z,Good point. I have updated the code as suggested.
127283147,2929,lindong28,2017-07-13T17:41:39Z,Thanks. I have fixed the typo now.
127284560,2929,lindong28,2017-07-13T17:47:02Z,Sure. I have also updated the corresponding comments in all checkpoint*() methods.
127284651,2929,lindong28,2017-07-13T17:47:25Z,Sure. Fixed now.
127284798,2929,lindong28,2017-07-13T17:48:01Z,Sure. Fixed now.
127286069,2929,lindong28,2017-07-13T17:53:00Z,Sure. Fixed now.
127289016,2929,lindong28,2017-07-13T18:04:32Z,"I think this is OK. `isNew` flag is only matters for local replica. And the local replica of the leader broker must be in the insync replicas set and it will be created properly in line 173.

Maybe I can add a comment here. Or do you like me to refactor the code a bit to call this method with the isNew flag?"
127291275,2929,lindong28,2017-07-13T18:14:03Z,"Yes that is the plan and it is actually implemented here. Note that `Errors.forException(Throwable t)` will return `Errors.UNKNOWN` if the given exception is not listed in `Errors.java`. And `Errors.forCode(short code)` will return `Errors.UNKNOWN_RETRIABLE` is the error code is out of the range of existing error codes listed in `Errors.java`. 

Our server code is not expected to call `Errors.forCode(short code)` with a not-listed error code. The client may call `Errors.forCode(short code)` with a not-listed error code if the client library version is smaller than the server library version and the server library has a new error code. In this case we want `Errors.forCode(short code)` to return a retriable exception so that producer can re-send the message after metadata update. Does this make sense?"
127294705,2929,lindong28,2017-07-13T18:27:30Z,Sorry for the typo. It is removed now.
127300635,2929,lindong28,2017-07-13T18:50:23Z,Yes you are right. Previously I made a mistake that made me think that `ArrayBlockingQueue.take()` doesn't unblock after interruption. I have updated this to block infinitely.
127301936,2929,lindong28,2017-07-13T18:55:37Z,Good catch! You are right. I have updated the code to call `maybeAddLogFailureEvent` and return KafkaStorageException if it is IOException. I also updated the code not to catch KafkaStorageException.
127305273,2929,lindong28,2017-07-13T19:08:56Z,Sure. I have updated the patch as suggested.
127307745,2929,lindong28,2017-07-13T19:20:02Z,Sure. I have updated the code as suggested.
127361245,2929,lindong28,2017-07-14T00:11:31Z,Removed now.
127361297,2929,lindong28,2017-07-14T00:12:02Z,Fixed now.
127376539,2929,lindong28,2017-07-14T03:02:30Z,"Sure. I have renamed the variable `UNKNOWN` to `UNKNOWN_SERVER_ERROR` to distinguish it from `UNKNOWN_RETRIABLE`. I think it is reasonable because the corresponding exception is `UnknownServerException`. If we want to further differentiate the two, we also rename `UNKNOWN_RETRIABLE` to be `UNKNOWN_CLIENT_RETRIABLE` and rename `UnknownRetriableException` to `UnknownClientRetriableException`. What do you think?

And I added the following comment for `UNKNOWN_RETRIABLE`. Does that look OK?

```
// UNKNOWN_RETRIABLE can only occur on the client side when client's library version is lower than the server's version.
// This is used to allow client to retry if the error code in the response is new and not recognized.
```"
127377150,2929,lindong28,2017-07-14T03:10:28Z,"Sure I added the following comment. Does this look OK?

```
/*
 * LogDirFailureChannel allows an external thread to block waiting for new offline log dir.
 * 
 * LogDirFailureChannel should be a singleton object which can be accessed by any class that does disk-IO operation. 
 * If IOException is encountered while accessing a log directory, the corresponding class can insert the the log directory name
 * to the LogDirFailureChannel using maybeAddLogFailureEvent(). Then a thread which is blocked waiting for new offline log directories
 * can take the name of the new offline log directory out of the LogDirFailureChannel and handles the log failure properly.
 * 
 */
```"
127380697,2929,lindong28,2017-07-14T03:56:50Z,"Good point. I have updated both the `LogManager.truncateTo()` and `LogManager.truncateFullyAndStartAt()` so that these two method will call `maybeAddLogFailureEvent()` and throw a KafkaStorageException if IOException is caught. This preserves the previous behavior.

Based on your previous comments, I get that it is a bit ugly in when and where we should catch IOException or KafkaStorageException. It is also not nice to have code that needs to decide whether we should throw the original KafkaStoragException or encapsulate the original IOException into a KafkaStorageException. After thinking through this issue, I made the following changes to make the logic cleaner

- `LogDirFailureChannel` is now passed to the constructor of `Log` and `LogSegment` so that they can enqueue offline log dir when there is IOException.
- The patch guarantees if `maybeAddLogFailureEvent()` is called before a new KafkaStorageException object is instantiated (except for very few scenarios where we know it is not needed). Thus we do NOT need to call `maybeAddLogFailureEvent()` when KafkaStorageException is caught. 
- We only need to call `maybeAddLogFailureEvent()` when IOException is caught. When IOException is caught, we can either log the error and swallow the exception, or we can throw a new instance of KafkaStorageException so that the outside code can catch it and generate the proper error in the response.


"
127380827,2929,lindong28,2017-07-14T03:58:34Z,Actually I find it better to name the new error as UnknownErrorCode. I will use it in the updated patch. I also moved the comment to `UnknownErrorCodeException.java`. Do you think this name is better?
127381549,2929,lindong28,2017-07-14T04:06:25Z,I still keep the code to catch `KafkaStorageException` since it may be useful if the underlying code re-throws KafkaStorageException in the future. It is a safe choice. But we no longer needs to call `maybeAddLogFailureEvent` when KafkaStorageException is caught for the reasons explained in the other comment.
127410791,2929,lindong28,2017-07-14T08:36:25Z,"Initially I thought it is OK and simpler to just return `UnknownTopicPartitionException` since the client only cares whether it needs to retry or not. But you are right that it is better and consistent to always return KafkaStorageException if client attempts to access a replica that is on an offline log directory.

I have updated the patch with considerable change to achieve this consistency."
127411380,2929,lindong28,2017-07-14T08:40:01Z,I added this comment to line 182: `We don't need to specify isNew flag since the local replica would have been created already`
127600194,2929,lindong28,2017-07-16T06:53:27Z,"After more thought, I think it simpler to just specify the isNew flag so that future developer doesn't need to think about it. I have updated the patch to do so."
127772691,2929,junrao,2017-07-17T17:37:25Z,"For code like this, perhaps it's useful to add a comment since it's easy to forget about the original reason over time."
127794464,2929,junrao,2017-07-17T18:58:46Z,"Hmm, now I am wondering if adding UNKNOWN_ERROR_CODE is really a good idea. By adding this, the broker has to consider 3 different types of clients when adding a new retriable error: (1) clients that don't understand UNKNOWN_ERROR_CODE; (2) clients that understand UNKNOWN_ERROR_CODE, but don't understand the new specific error code (e.g., KAFKA_STORAGE_ERROR); (3) clients that understand the new specific error code. The broker has to send different error codes for those three different cases (1) send an existing retriable error (2) send UNKNOWN_ERROR_CODE; (3) send the specific error code. We have to do this for each type of request that can receive the new error code.

An alternative is the following. We don't introduce UNKNOWN_ERROR_CODE. If the broker introduces a new retriable error, we bump up the request protocol. The broker (a) sends an existing retriable error for clients that don't understand the new specific error code; (b) sends the new specific error code otherwise. This seems simpler."
127807824,2929,junrao,2017-07-17T19:53:39Z,"If would be useful to double check which requests other than produce/fetch can receive and care about KAFKA_STORAGE_ERROR. It seems that OffsetCommit,  OffsetForLeaderEpoch, ListOffsets and DeleteRecords could receive this. Not sure if they all care about it though. A few new requests for EOS could probably also hit this, which can be addressed in s a separate patch."
127811509,2929,junrao,2017-07-17T20:09:05Z,isLogDirectorOffline => isLogDirectoryOffline
127814413,2929,junrao,2017-07-17T20:21:06Z,isLogDirectorOffline => isLogDirectoryOffline
127827127,2929,junrao,2017-07-17T21:12:09Z,The comment seems outdated now.
127837300,2929,junrao,2017-07-17T21:59:10Z,"Hmm, not sure what the convention is now in handling IOException at the Log/LogManager level. We could (1) turn all IOException in Log to KafkaStorageException and call logDirFailureChannel.maybeAddLogFailureEvent(). Then, in ReplicaManager, we don't need to deal with IOException. (2) let the IOException for Log to bubble to ReplicaManager and call logDirFailureChannel.maybeAddLogFailureEvent() in ReplicaManager.

It seems that the patch does a mix of both (1) and (2). In LogManager.getOrCreateLog(), it seems it's possible for an IOException to be thrown. In Log, we are turning all IOExceptions to KafkaStorageException. It seems that it's better to pick either (1) or (2) and do it consistently in all places?"
127838104,2929,junrao,2017-07-17T22:03:17Z,"Hmm, could we get IOException here? I thought now the convention is to catch all IOException in the Log level and convert it to KafkaStorageException?"
127842977,2929,junrao,2017-07-17T22:32:36Z,Perhaps it's better to use eq (reference equality).
127845994,2929,junrao,2017-07-17T22:49:50Z,This seems to break the convention of not calling maybeAddLogFailureEvent on KafkaStorageException?
127846644,2929,junrao,2017-07-17T22:53:47Z,"Hmm, the callers of getLogEndOffset() don't seem to expect an exception."
127849554,2929,junrao,2017-07-17T23:13:16Z,We don't need to mention ProduceRequest here since it's not an inter broker request.
127850607,2929,junrao,2017-07-17T23:20:51Z,Should we do this on any IOException?
127858289,2929,junrao,2017-07-18T00:21:57Z,"The comment seems inaccurate. We are sending all replicas, not just live replicas in the LeaderAndIsrRequest."
127864124,2929,junrao,2017-07-18T01:19:49Z,"It would be useful to replace LeaderAndIsrPartitionState and MetadataPartitionState with PartitionState and 
UpdateMetadataRequest.PartitionState. This can be done in a followup cleaning patch."
127864321,2929,junrao,2017-07-18T01:22:09Z,unused import TimeUnit
127864547,2929,junrao,2017-07-18T01:24:18Z,"unused imports Seq, Set."
127864941,2929,junrao,2017-07-18T01:28:24Z,It seems that we should check if the notification is of LogDirFailureEvent in LogDirEventNotification.process()?
127865003,2929,junrao,2017-07-18T01:29:10Z,unused import
127865578,2929,junrao,2017-07-18T01:34:27Z,Using a null replicaManager to represent an offline partition seems a bit hacky. Could we just add a new offline flag in the constructor?
127865966,2929,junrao,2017-07-18T01:38:40Z,Could we add a comment that LEADER_AND_ISR_RESPONSE_V1 can receive the new KafkaStorage error code?
127866463,2929,junrao,2017-07-18T01:44:15Z,check whether is offline log directory => check whether there is offline log directory ?
127867135,2929,junrao,2017-07-18T01:52:02Z,"Hmm, a couple of thoughts on this. If the broker is still on an old inter-broker protocol, the controller won't be able to handle the failed disk dir event. So, the broker will be up with offline replicas, but new leaders can't be elected. Perhaps it's better to just failed the broker in the old way if the inter-broker protocol is old? Related to this, I am wondering if it's useful to add a config to turn off this new feature. This way, if there is a bug, the user has the option to switch to the old behavior."
127867830,2929,junrao,2017-07-18T01:59:15Z,"In the following line in line 65, shouldn't we set the desired version to 4 if magic is on V2?
            super(ApiKeys.PRODUCE, (short) (magic == RecordBatch.MAGIC_VALUE_V2 ? 3 : 2));
"
127885491,2929,lindong28,2017-07-18T05:32:18Z,"Thanks much for the quick review!

I think the current approach is probably simpler. The broker only needs to know 2 types of the clients instead of three, i.e. one that knows UnknownErrorCodeException and one that doesn't. If the request of the client suggests that the client knows UnknownErrorCodeException, then the broker will simply send the origin error code. The client library will convert the error to UnknownErrorCodeException if the error code is not recognized. Otherwise, the broker should convert the new error code to an existing error code before sending the response. Note that we need at most one `if/else` in each response to check whether the client knows UnknownErrorCodeException given the request version.

On the other hand, the alternative approach requires Kafka to potentially have multiple `if/else` in each response to check whether the clients know each newly-added error code. The number of check will increase overtime as we add more and more new error code. Thus the current approach seems simpler. What do you think?

"
127885621,2929,lindong28,2017-07-18T05:33:53Z,Ah.. fixed now.
127885629,2929,lindong28,2017-07-18T05:34:00Z,Fixed now.
127886423,2929,lindong28,2017-07-18T05:41:53Z,"Sure. I added this comment:

```
                // If producer sends ProduceRequest V3 or earlier, the client library is not guaranteed to recognize the error code
                // for KafkaStorageException. In this case the client library will translate KafkaStorageException to
                // UnknownServerException which is not retriable. We can ensure that producer will update metadata and retry
                // by converting the KafkaStorageException to NotLeaderForPartitionException in the response if ProduceRequest version <= 3
```"
127889378,2929,lindong28,2017-07-18T06:11:39Z,"Yeah @becketqin had similar comment as well but I find that the patch in its current form is simpler. This is because not all exceptions bubble to the ReplicaManager. For example, both ReplicaFetchThread and LogClean may attempt to truncate the Log which doesn't go through ReplicaManager. And we may have more IOExeption from new methods in the future. Thus (2) alone wouldn't work.

Alternatively we can do (1) only, e.g. find all operations that may cause IOException, catch/call logDirFailureChannel.maybeAddLogFailureEvent() and re-throw a KafkaStorageException. My concern with this approach is that we will need to identify all possible calls that may throw IOException and make sure that IOException does not bubble up to ReplicaManager because otherwise we will return UnknownServerException in the response. This is doable but it requires carefully review of the code which may be error-prone both now and in the future development. On the other hand, if we can catch IOException in the ReplicaManager where we generate the error code for various response, we can ensure that if any request handling incurs IOException, we will trigger `maybeAddLogFailureEvent` and return the proper error code. 

Thus the current mixed approach seems simpler and more reliable than the approach (1). Does this explanation sound reasonable? While I understand that consistency is a good feature, is there specific benefits of doing (1) only?"
127890064,2929,lindong28,2017-07-18T06:17:58Z,Good point. I fixed all of them.
127890791,2929,lindong28,2017-07-18T06:24:33Z,"No.. Currently I didn't catch/try IOException for methods like Log.append(), LogSegment.append() or OffsetIndex.append(). I feel that Kafka' code will be cleaner and more reliable if we can catch IOException in the methods which handle request and generate response. The only reason not all IOException are caught in ReplicaManager or KafkaApi is that we have checkpoint file operation, ReplicaFetcherThread or LogCleaner which does IO operation without requiring external request.


"
127891075,2929,lindong28,2017-07-18T06:27:02Z,I see. I replaced the comment with this: `Mark partition as offline in the cache if the local replica creation has failed due to offline log directory`
127891341,2929,lindong28,2017-07-18T06:29:21Z,This is just for code simplicity of not having to duplicate the code of logging error and populating the response map. Do you want me to separate them?
127892176,2929,lindong28,2017-07-18T06:36:30Z,I think the behavior of throwing the exception is the same as returning None for `GroupMetadataManager`. I am not sure very what is the right behavior for `TransactionStateManager` if the corresponding partition is in offline log directory. Previously I think it is reasonable to throw KafkaStorageException if the partition is offline. But I am not strong on this. I have changed it to return None instead.
127892430,2929,lindong28,2017-07-18T06:38:32Z,"Thanks. I see. I have removed ProduceRequest, FetchRequest and MetadataRequest from this comment."
127894263,2929,lindong28,2017-07-18T06:51:19Z,"I have thought about catching IOException here. After reading https://github.com/apache/kafka/pull/698/files I am still not sure why we previously didn't halt the Kafka on any IOException here. There are also some IOException that are explicitly thrown in this class that do not trigger halt.

I couldn't find a reason not to catch IOException either. I choose to be conservative by preserving the behavior by not doing this on all IOException just in case we have a reason not to do so.

Would you like me to do this on any IOException? Alternatively, maybe we can discuss the handling of these specific IOException in a followup patch?"
127894415,2929,lindong28,2017-07-18T06:52:19Z,Good point. You are right. I removed the `live` here.
127894505,2929,lindong28,2017-07-18T06:52:58Z,Thanks!! I will do it in a followup patch.
127894624,2929,lindong28,2017-07-18T06:53:46Z,Ah it somehow escaped from my reviews.. Removed now.
127894717,2929,lindong28,2017-07-18T06:54:26Z,My bad.. removed now.
127896162,2929,lindong28,2017-07-18T07:05:01Z,"Initially I think it is OK not to check it since there is only one type. It is similar to how we currently have a version field in the reassignment json file provided to `kafka-reassign-partitions.sh` but we currently don't check that version either.

But it is also reasonable to check it. I have updated the code to throw IllegalArgumentException is the event type value is not 1.
"
127896355,2929,lindong28,2017-07-18T07:06:23Z,Not sure why I missed these.. Removed now.
127897210,2929,lindong28,2017-07-18T07:12:18Z,"Initially I wanted to avoid adding complexity to the constructor. Sure, I have added isOffline flag to the constructor."
127897438,2929,lindong28,2017-07-18T07:13:34Z,Sure. I added this: `LeaderAndIsrResponse V1 may receive KAFKA_STORAGE_ERROR in the response`
127897524,2929,lindong28,2017-07-18T07:14:08Z,Thanks for catching this. Fixed now.
127898698,2929,lindong28,2017-07-18T07:21:48Z,"As long as the controller and the broker is running new code, the controller will be able to handle the failed disk dir event even if the inter-broker protocol is still old. More specifically, if there is log directory failure, the broker will write a sequential znode in zookeeper, the controller will send LeaderAndIsrRequest, broker will specify error in the response, and the controller will trigger leader election. None of these steps require new inter-broker protocol. Does this make sense?

I am not sure we should add a config just to take care of the potential bug. Ideally we don't want to add a config just for short-term use. The typical way of handling bug is to either hotfix the code or rollback to the last stable version. Any KIP or big code change may potentially have bug and it seems a big weird to add a config to handle the bug.

Adding this config is easy since all we need is to tell LogDirFailureHandler to halt the broker when there is log directory failure. Do you think it is necessary?
"
127899953,2929,lindong28,2017-07-18T07:28:56Z,"No. This is the only improvement in my mind that I haven't made in this patch. I mentioned this in a comment after my last commit yesterday.

As of this patch, producer will still send ProduceRequest with version 3 if the message magic value is 2. This is because the newly-added produce version 4 is incompatible with 0.11 broker. As of now our producer determines the request version based on the minimum magic value in the data instead of using the ApiVersionsRequest to negotiate the this with the broker. This wasn't a problem in the latest Kafka but this is causing problem for patches like KIP-112 which wants to bump up ProducRequest version without bumping up the magic value. I don't think this is a critical problem for KIP-112 since NotLeaderForPartitionException in the response is a reasonable workaround. Can I fix this issue in a followup patch later?





"
127902927,2929,lindong28,2017-07-18T07:44:43Z,Sure. I will go over all existing request and see if they need to handle KafkaStorageException specifically.
127912013,2929,lindong28,2017-07-18T08:28:42Z,"Another way I look at this is that, regardless of what the underlying code does, the outside code, i.e. those methods that determines the response, should catch all exception and determine the error in the response properly. This is similar to why we currently catch `Throwable`. It is just that, instead of catching IOException (as part of the throwable) and return UnknownServerException, it seems more reasonable to call `maybeAddLogDirFailure()` and return KafkaStorageException on IOException."
128041151,2929,junrao,2017-07-18T17:26:57Z,"@lindong28 : Ok, what you said about the approach in the patch makes sense. I was just wondering if we can achieve the same w/o introducing UnknownErrorCode. The alternative that I suggested also just needs 1 if/else check. Basically, you bump up the client request protocol, if the client is on the old protocol, the broker sends an existing retriable error code. Otherwise, the broker sends the new specific error code. So, the logic is about the same as your patch, but we don't need to introduce UNKNOWN_ERROR_CODE. This seems a bit simpler since we don't have to explain to the client developers the subtle difference btw UNKNOWN_ERROR_CODE and UNKNOWN."
128042260,2929,junrao,2017-07-18T17:30:51Z,"@lindong28 : We don't necessarily have to go purely with (1) or (2). However, it would be useful to have a convention that we can follow in the future. Could you summarize that convention? I seems that you are saying that all IOException for client facing requests (but not inter broker requests) will be turned to KafkaStorageException in the Log level?"
128042285,2929,junrao,2017-07-18T17:30:58Z,"Hmm, we do convert IOException to KafkaStroageException in Log.append(), right?"
128042324,2929,junrao,2017-07-18T17:31:05Z,"FetchRequest is used by inter broker replication. So, we need to include it."
128042447,2929,junrao,2017-07-18T17:31:35Z,"In the future, we may need to add new event type. Then, when upgrading a broker, the controller may see a new event type that it doesn't understand yet. So, it would be useful for the logic in the controller to be resilient to that."
128042652,2929,junrao,2017-07-18T17:32:21Z,"Is that true? In ControllerChannleManager, if the inter broker protocol is before 0.11.1, the controller will send v0 of LeaderAndIsrRequest and the response won't have KafkaStorageError, which means that the controller won't be able to move failed replicas to offline?"
128044355,2929,lindong28,2017-07-18T17:38:37Z,"For OffsetCommitRequest, I have updated the patch so that server will send `Errors.NOT_COORDINATOR` in the response if there is KAFKA_STORAGE_ERROR. This is similar to how broker currently converts NOT_LEADER_FOR_PARTITION to NOT_COORDINATOR when handling OffsetCommitRequest. No change on the client side is needed.

For OffsetForLeaderEpoch, the server will send KAFKA_STORAGE_ERROR and the replica fetcher thread will retry if there is error -- it doesn't care which error it is. Thus no change is needed.

For ListOffsets, the server will send KAFKA_STORAGE_ERROR and the client will convert this error to StaleMetadataException which extends InvalidMetadataException. Thus no change is needed.


"
128045320,2929,lindong28,2017-07-18T17:42:24Z,oops.. I added it back.
128048615,2929,lindong28,2017-07-18T17:54:31Z,"Hmm.. not sure if I fully understand the alternative approach.

Let say we introduced two new retriable errors A and B for ProduceResponse in the future. A is added in version 5 and B is added in version 6. If we have UNKNOWN_ERROR_CODE, then the response handling logic would look like this:
```
// Check if the client recognizes UNKNOWN_ERROR_CODE
if (version > 3)
  return original_error_code
else
  return another_existing_error_code (e.g. NOT_LEADER_FOR_PARTITION)
```

On the other hand, if we don't have UNKNOWN_ERROR_CODE, then the response handling logic would look like this, which increases over time as we have more and more new errors.

```
if (version < 5 and original_error_code == A)
  return another_existing_error_code
else if (version < 6 and original_error_code == B)
  return another existing_error_code
else
  return original_error_code
```

Thus it seems that UNKNOWN_ERROR_CODE can make things a bit easier. Does this make sense?"
128049363,2929,lindong28,2017-07-18T17:57:19Z,"Right, this makes sense."
128050930,2929,lindong28,2017-07-18T18:03:02Z,"I still think it is true.. In this case, controller will send LeaderAndIsrRequest V0 which doesn't explicitly specify the isNew flag. The broker will assume isNew = false when it attempts to create the local replica. And if there is log directory fail, then broker either dies (if it is running the old code) or the broker will specify KafkaStorageException in the response (if it is running the new code). Then the controller can move failed replicas offline. This works because LeaderAndIsrResponse V0 already allows broker to specify per-partition error code. Does this make sense?"
128057016,2929,lindong28,2017-07-18T18:26:44Z,My bad... I missed this. I have removed this IOException handling here.
128064844,2929,junrao,2017-07-18T18:58:50Z,Got it. This is assuming that the new error code is a retriable error. How would the logic compare if the new error code is not retriable?
128067682,2929,junrao,2017-07-18T19:08:02Z,"Got it. I forgot that we don't convert the error code in LeaderAndIsrResponse. Then the issue is mostly when you have a mix of brokers with old and new code. If the controller happens to be on the broker with the old code, it wouldn't react to disk failure events until the controller is upgraded. In a larger cluster, rolling upgrade could take some time and leaving the offline replicas unprocessed for long is probably not ideal. If we only apply the new logic when the inter.broker protocol is 0.11.1, then the above issue can be addressed since at that point, we expect every broker to be on the new code.

A minor related issue is that the OfflineReplicas won't be propagated properly until the controller's inter broker protocol is on 0.11.1. This will affect the metadata response. Not sure if we can avoid this completely. It would be useful to at least document this."
128076192,2929,lindong28,2017-07-18T19:45:28Z,"If the new error is not retriable, then the client will convert the unknown error code to UnknownServerException which is non-retriable. I think it is a reasonable solution before the client library is upgraded."
128076999,2929,lindong28,2017-07-18T19:48:56Z,Both are good points. I will update the LogDirFailureHandler so that it will halt the broker if the inter broker protocol is before 0.11.1.
128078560,2929,lindong28,2017-07-18T19:55:33Z,"My previous convention is that:
1) IOException should maybeAddLogDirFailureEvent.
2) IOException should either be logged or be re-thrown as KafkaStorageException.
3) KafkaStorageException should not trigger maybeAddLogDirFailureEvent.

Previously I feel it is simpler to catch IOException in the ReplicaManager similar to how we handle most other exceptions that happen during request processing. And I intend to avoid adding big try/catch because it typically makes code review harder.

Now I agree with you that it is a good idea to keep it consistent and try to catch IOException before the ReplicaManager. I have updated the patch so that the ReplicaManger no longer needs to catch IOException at all. IOException is now caught in e.g. Log, LogManager and CheckpointFile. These IOException will trigger maybeAddLogDirFailureEvent() and be re-thrown as KafkStorageException. I think this address the issue here."
128079255,2929,lindong28,2017-07-18T19:58:32Z,I have updated the code so that ReplicaManager no longer needs to catch IOException at all including here.
128082294,2929,lindong28,2017-07-18T20:12:18Z,"I don't know why we don't halt the server on IOException when reading or writing the checkpoint file. Anyway,  I have updated the patch to catch IOException when reading/writing the checkpoint file, call maybeAddLogDirFailureEvent(), and re-throw a KafkaStorageException. Thus this issue is also addressed."
128097699,2929,lindong28,2017-07-18T21:14:51Z,"Ah, I probably misunderstood your previous question. If the new error code is non-retriable and client library already knows UnknownErrorCodeException, then the client library will treat the new error as a retraible error. It means that client may retry unnecessarily instead of failing fast. I think this is OK and better than having client failing immediately when it should retry."
128097968,2929,junrao,2017-07-18T21:15:58Z,"Hmm, if a new error is not retriable, the server code will probably look like the following.

```
// Check if the client recognizes NEW_NON_RETRIABLE_ERROR_CODE
if (version < 5 and original_error_code = NEW_NON_RETRIABLE_ERROR_CODE) // 5 is the version that understands NEW_NON_RETRIABLE_ERROR_CODE
  return unknown_server_error_code
else
  return NEW_NON_RETRIABLE_ERROR_CODE
```
So, you don't really save code if the new error is not retriable. Another thing that bothers me a bit is that the logic for handling a retriable error is a bit different from that for handing a non-retriable error. In the alternative approach, at least any new error code is handled in the same way."
128106111,2929,lindong28,2017-07-18T21:53:32Z,"@junrao My use-case for UnknownErrorCodeException is based on the assumption that it is always OK for client to retry. The client should always have a reasonable timeout setting if it retries. Thus if the original error is non-retriable, the client will block unnecessarily up to that timeout which is not that bad. Also, for requests that don't want to retry on unknown error code, we can always update its response handling logic to skip retry if error == UnknownErrorCodeException.

Does this sound reasonable? If not, then I think we will have to add specific if/else for potentially a few requests for every new error in the future. I can remove the UnknownErrorCodeException if you prefer not to make the decision in this patch.

"
128109393,2929,junrao,2017-07-18T22:10:42Z,"The issue with retrying on any unknown error is that an app (e.g. MirrorMaker) could set the retry to infinite and then it won't be aware of the error.

Adding a new if/else statement for every new error code is not necessary bad as long as the process is clear. Adding UnknownErrorCodeException seems more complicated to me at this moment. Perhaps we could remove it and reconsider it later if there is a need."
128110294,2929,junrao,2017-07-18T22:14:52Z,"Thanks. Discussed this a bit with Jason. It seems that we probably need to extend the current desired version logic to support the possibility of having multiple versions on the same magic (perhaps having a desired version range). So, we can do this in a followup jira. If you ping Jason on that jira, he can help you with some suggestions."
128133249,2929,lindong28,2017-07-19T01:13:44Z,"@junrao Sure! After this pull request is closed, I will create tickets for all the TODOs mentioned in the discussion of this pull request. I will ping you and Json for review later."
128133400,2929,lindong28,2017-07-19T01:15:19Z,@junrao Sure. I have removed the UnknownErrorCodeException in the latest patch.
128367026,2929,junrao,2017-07-19T21:05:50Z,"isLogDirectoryOffline => LogDirectoryOffline ? The latter seems more consistent with UnderReplicated in Partition. If we change the name, make sure that we change it consistently in the place where the metric is removed as well."
128371038,2929,junrao,2017-07-19T21:22:57Z,Should we catch IOException in fetchOffsetsByTimestamp() in line 1041 too?
128376413,2929,junrao,2017-07-19T21:47:24Z,Since all accesses to LogSegment are through Log/LogCleaner/LogManager/LogCleanerManager. Perhaps we can just catch IOExceptions in those places instead of here?
128390730,2929,junrao,2017-07-19T23:09:30Z,cleanup => clean up
128396923,2929,junrao,2017-07-19T23:55:28Z,Use Exit.halt() to be consistent with what's in ReplicaManager?
128397663,2929,junrao,2017-07-20T00:01:48Z,Do we still need to call maybeAddLogFailureEvent() on KafkaStorageException?
128397699,2929,junrao,2017-07-20T00:02:13Z,Do we still need to call maybeAddLogFailureEvent() on KafkaStorageException?
128399202,2929,junrao,2017-07-20T00:14:21Z,Could we add some comment at the beginning of the class on the process of adding a new server side exception going forward?
128400026,2929,junrao,2017-07-20T00:21:10Z,IOException is unused now.
128401597,2929,junrao,2017-07-20T00:35:49Z,unused import
128404639,2929,junrao,2017-07-20T01:01:30Z,poll(0) => poll(10) to prevent busy loop?
128404788,2929,junrao,2017-07-20T01:02:40Z,poll(0) => poll(10) to prevent busy loop?
128542365,2929,junrao,2017-07-20T15:06:26Z,The comment seems inaccurate now.
128542947,2929,junrao,2017-07-20T15:08:22Z,Could we just set the file to be unreadable and unwritable? 
128543296,2929,junrao,2017-07-20T15:09:22Z,Should we assert the consumer sees at least 2 messages?
128544125,2929,junrao,2017-07-20T15:12:14Z,"Perhaps it's also useful to assert that LogDirEventNotificationPath is empty eventually. It may also be useful to verify the replica state for that replica  in the controller is in offline state,"
128548154,2929,junrao,2017-07-20T15:25:34Z,Perhaps it's better to use KafkaConfig.LogDirsProp
128550482,2929,junrao,2017-07-20T15:33:47Z,"To make this more robust, perhaps we need to disable the periodic metadata fresh in the producer?"
128551469,2929,junrao,2017-07-20T15:37:33Z,Is sudo really needed?
128552122,2929,junrao,2017-07-20T15:39:46Z,"This is for isr, not leader replica."
128552194,2929,junrao,2017-07-20T15:40:00Z,leader => isr
128559280,2929,junrao,2017-07-20T16:04:02Z,"""Leader node"" doesn't cover the case when broker_type is follower."
128559453,2929,junrao,2017-07-20T16:04:45Z,Is sudo needed?
128560468,2929,junrao,2017-07-20T16:08:41Z,Is the test from line 161 to 174 necessary? It doesn't seem to be directly related to offline disks.
128560625,2929,junrao,2017-07-20T16:09:19Z,topic2 actually has min.isr of 1.
128561993,2929,junrao,2017-07-20T16:14:33Z,Why is time-based log rolling needed here?
128582900,2929,lindong28,2017-07-20T17:38:37Z,Yeah.. I also prefer `LogDirectoryOffline`. I changed it to `isLogDirectoryOffline` because I didn't have a good reason not to use `isLogDirectoryOffline` when asked previously. I have updated the patch to use `LogDirectoryOffline`.
128583248,2929,lindong28,2017-07-20T17:40:11Z,Good catch. It is fixed now.
128632191,2929,lindong28,2017-07-20T21:12:23Z,"You are right. I missed this.. I should have gone over all methods in Log.scala.

I just went over all the methods in Log.java and made sure the following rule is satisfied for any method in this class:

1) If the method is used during log load phase, which should only happen before the broker registers itself in the zookeeper, this method doesn't have to convert IOException to KafkaStorageException.

2) Otherwise, if the method in incurs I/O operation and may be called by other classes after broker has registered itself (excluding shutdown()), this method should catch IOException, add it to log dir failure channel, and re-throw KafkaStorageException

3) Otherwise, the method either doesn't throw IOException, or it is only called by internal methods in Log that belong to one of the two above categories, we don't need to catch IOException in this method.

I still keep an unnecessary conversion from IOException to KafkaStorageException in `Log.roll()`. It is unnecessary because it is currently only called by methods in Log.scala that will catch IOException. I still keep it there since it is a public method and may be used in the future. It is a bit tedious to go over all method and make sure IOException is caught `iff` it is needed. So I choose to lean towards over-catch it in Log.scala. I can remove it if needed.

I also added the comment `We don't convert IOException to KafkaStorageException in this method because this method may be called before all logs are loaded` to some methods in Log.scala so that future develop can be aware of this.
"
128634615,2929,lindong28,2017-07-20T21:24:12Z,"Good point. I have updated the code to the following with an extra comment. I think we don't have to catch IOException for logging purpose here because the IOException should specify the file name and be caught and logged by the caller.

```
  /**
   * Change the suffix for the index and log file for this log segment
   * IOException from this method should be logged and handled by the caller
   */
  def changeFileSuffixes(oldSuffix: String, newSuffix: String) {
    log.renameTo(new File(CoreUtils.replaceSuffix(log.file.getPath, oldSuffix, newSuffix)))
    index.renameTo(new File(CoreUtils.replaceSuffix(index.file.getPath, oldSuffix, newSuffix)))
    timeIndex.renameTo(new File(CoreUtils.replaceSuffix(timeIndex.file.getPath, oldSuffix, newSuffix)))
    txnIndex.renameTo(new File(CoreUtils.replaceSuffix(txnIndex.file.getPath, oldSuffix, newSuffix)))
  }
```"
128634780,2929,lindong28,2017-07-20T21:25:03Z,Thanks for catching this. It is fixed now.
128634986,2929,lindong28,2017-07-20T21:26:03Z,Clearly I didn't review the patch careful enough.. Sorry. It is fixed.
128635061,2929,lindong28,2017-07-20T21:26:24Z,I should have noticed this.. It is fixed now.
128639223,2929,lindong28,2017-07-20T21:48:19Z,"Good point. I added the following comment:

```
 * Note that client library will convert an unknown error code to the non-retriable UnknownServerException if the client library
 * version is old and does not recognize the newly-added error code. Therefore when a new server-side error is added,
 * we may need extra logic to convert the new error code to another existing error code before sending the response back to
 * the client if the request version suggests that the client may not recognize the new error code.
```

I also added the following comment in `KafkaStorageException.java`:

```
 * Here are the guidelines on how to handle KafkaStorageException and IOException:
 *
 * 1) If the server has not finished loading logs, IOException should not be converted to KafkaStorageException
 * 2) After the server has finished loading logs, IOException should be caught and trigger LogDirFailureChannel.maybeAddLogFailureEvent
 *    Then the IOException should either be swallowed and logged, or be converted to KafkaStorageException.
 * 3) It is preferred for IOException to be caught in Log rather than in ReplicaManager or LogSegment.
```

"
128639576,2929,lindong28,2017-07-20T21:50:21Z,Ah.. I just realized that I can not rely on intellij to show the unused import in gray color. It is fixed now.
128639643,2929,lindong28,2017-07-20T21:50:52Z,It is fixed now... I need to be more careful.
128640385,2929,lindong28,2017-07-20T21:54:56Z,I missed this. It is fixed now.
128644708,2929,lindong28,2017-07-20T22:20:45Z,"I just tested it by setting the file to be unwritable. The testProduceAfterLogDirFailure() with pass. However, `KafkaServerTestHarness.tearDown()` will fail with `java.nio.file.AccessDeniedException` because it is not able to delete the log directory. It is possible to swallow the exception so that the test will pass. But that the temporary log directory will not be removed after the test and will accumulate over time on the test server.

According to the information provided below, it seems that the log directory can only be removed with sudo access if it is set to be unwritable. Thus it seems simpler to just use the current approach by replacing the log directory with a file.

```
[dolin@dolin-ld2 kafka]$ rm -rf /tmp/kafka-1650964778799526893/log-start-offset-checkpoint
rm: cannot remove `/tmp/kafka-1650964778799526893/log-start-offset-checkpoint': Permission denied
[dolin@dolin-ld2 kafka]$ ls -ld /tmp/kafka-1650964778799526893/log-start-offset-checkpoint
-rw-r--r-- 1 dolin eng 0 Jul 20 15:08 /tmp/kafka-1650964778799526893/log-start-offset-checkpoint
```



"
128644975,2929,lindong28,2017-07-20T22:22:24Z,I think it is not necessary to use poll(10). `waitUntilTrue()` will sleep for 100 ms before trying to poll() again.
128646300,2929,lindong28,2017-07-20T22:31:01Z,"Are you suggesting to set `retry.backoff.ms` to LONG_MAX when we create the producer? I am not sure we should do that because we need metadata to be refresh after producer sees KafkaStorageExceptoin in the ProduceResponse, so that we can verify that producer can produce message after leadership is moved by controller. Actually I intentionally set the `retry.backoff.ms` to 100 ms so that the producer can refresh the metadata almost immediately after it sees KafkaStorageException in the response.

`metadata.max.age.ms` is 5 minutes by default which should be longer than the time needed for this test. Thus it seems that the test is already robust since producer should not automatically refresh metadata unless it sees exception in the ProduceResposne. Does it sound reasonable?"
128652090,2929,lindong28,2017-07-20T23:11:14Z,I don't think we should.. The first message sent by the first produce has already been consumed by the first call of `poll()`. This is the second call of `poll()` and it is possible to consume only one message.
128652222,2929,lindong28,2017-07-20T23:12:32Z,"It is probably not necessary because `waitUntilTrue()` will sleep for 100 ms before trying to poll() again.
"
128654104,2929,lindong28,2017-07-20T23:26:38Z,Good point. I have updated the test to assert both requirements.
128654285,2929,lindong28,2017-07-20T23:28:06Z,Good point. I have updated the patch to use `KafkaConfig.LogDirsProp`.
128654830,2929,lindong28,2017-07-20T23:32:08Z,My bad. I have fixed this.
128654940,2929,lindong28,2017-07-20T23:33:03Z,Thanks. It is fixed now.
128655255,2929,lindong28,2017-07-20T23:36:10Z,"Is this because after the broker has opened the file handler for a log segment, it can continue read/write to the log segment even after the log directory is removed or marked as unreadable. I set the log rolling to 3 sec so that the broker will need to create new log segment every 3 seconds. Then the server will be able to discover log directory failure when it attempts to create file for a new log segment."
128655516,2929,lindong28,2017-07-20T23:38:25Z,"Thanks. I added a line that says ""and another topic with partitions=3, replication-factor=3, and min.insync.replicas=1"""
128655798,2929,lindong28,2017-07-20T23:40:34Z,My bad.. I have updated it to say `Broker %d should be in isr set`
128656024,2929,lindong28,2017-07-20T23:42:30Z,I think it may be useful. This is needed to verify that the broker can still server replicas on the good disks even if it has bad disks.
128659071,2929,junrao,2017-07-21T00:08:19Z,I was referring to metadata.max.age.ms. We can leave it as it is since metadata.max.age.ms is 5 minutes by default.
128659130,2929,junrao,2017-07-21T00:08:49Z,Could we add a comment for that?
128661501,2929,lindong28,2017-07-21T00:32:22Z,"In particular, I made the following code changes:

1) Catch IOException in Log.fetchOffsetsByTimestamp()
2) Catch IOException in Log.deleteSegments()
3) Catch IOException in Log.flush()
4) Catch IOException in Log.delete()
5) Catch IOException in Log.truncateTo()
6) Catch IOException in Log.truncateFullyAndStartAt()
7) Catch IOException in Log.asyncDeleteSegment()
8) Removed IOException catch code in LogManager.truncateTo()
9) Removed IOException catch code in LogManager.truncateFullyAndStartAt()
10) Removed IOException catch code in LogManager.deleteLogs()
11) Removed IOException catch code in LogSegment.changeFileSuffixes()
12) Removed IOException catch code in LogSegment.delete()
"
128676856,2929,lindong28,2017-07-21T03:21:13Z,"Yes.. I think it is needed. Below is the error if this command is executed without sudo.

```
Traceback (most recent call last):
  File ""/export/apps/python/2.7.13/lib/python2.7/site-packages/ducktape/tests/runner_client.py"", line 123, in run
    data = self.run_test()
  File ""/export/apps/python/2.7.13/lib/python2.7/site-packages/ducktape/tests/runner_client.py"", line 176, in run_test
    return self.test_context.function(self.test)
  File ""/export/apps/python/2.7.13/lib/python2.7/site-packages/ducktape/mark/_mark.py"", line 321, in wrapper
    return functools.partial(f, *args, **kwargs)(*w_args, **w_kwargs)
  File ""/home/dolin/research/kafka/tests/kafkatest/tests/core/log_dir_failure_test.py"", line 133, in test_replication_with_disk_failure
    broker_node.account.ssh(cmd, allow_fail=False)
  File ""/export/apps/python/2.7.13/lib/python2.7/site-packages/ducktape/cluster/remoteaccount.py"", line 253, in ssh
    raise RemoteCommandError(self, cmd, exit_status, stderr.read())
RemoteCommandError: vagrant@worker9: Command 'chmod a-rw /mnt/kafka-data-logs-1 -R' returned non-zero exit status 1. Remote error message: chmod: cannot read directory /mnt/kafka-data-logs-1: Permission denied
```

I also tried to execute the command with and without sudo by logging into the vagrant node. Here is what I found:

```
vagrant@worker1:~$ whoami 
vagrant
vagrant@worker1:~$ ls -ld /mnt/kafka-data-logs-1/
drwxrwxr-x 3 vagrant vagrant 4096 Jul 21 03:25 /mnt/kafka-data-logs-1/
vagrant@worker1:~$ chmod a-rw /mnt/kafka-data-logs-1/ -R
chmod: cannot read directory /mnt/kafka-data-logs-1/: Permission denied
vagrant@worker1:~$ ls -ld /mnt/kafka-data-logs-1/
d--x--x--x 3 vagrant vagrant 4096 Jul 21 03:25 /mnt/kafka-data-logs-1/
vagrant@worker1:~$ sudo chmod a-rw /mnt/kafka-data-logs-1/ -R
vagrant@worker1:~$
```

"
128677507,2929,lindong28,2017-07-21T03:29:19Z,"Ah... I realized that I can work without `sudo`. The `chmod a-rw /mnt/kafka-data-logs-1/ -R` can actually change the permission of this log directory. But it still returned error probably because this command attempts to read information of this log directory after it takes effect.

I am able to avoid sudo by replacing the command with `chmod a-w /mnt/kafka-data-logs-2/ -R`."
128678929,2929,lindong28,2017-07-21T03:48:41Z,"Sure. I added the following comment:

```
Set log.roll.ms to 3 seconds so that broker will detect disk error sooner when it creates log segment
Otherwise broker will still be able to read/write the log file even if the log directory is inaccessible.
```"
128679135,2929,lindong28,2017-07-21T03:51:24Z,Yes. The sudo is needed in order to delete an unwritable log directory.
128679816,2929,lindong28,2017-07-21T04:01:28Z,Never mind.. I originally used `OfflineLogDirectoryCount` instead of the `LogDirectoryOffline`. Thanks for the suggestion!
128680876,2929,lindong28,2017-07-21T04:16:46Z,"I updated the code to use `KafkaConfig.LogDirsProp` only if the dir count > 1. This is because some tests such as OffsetCommitTest.scala assumes that there is only one log directory and the will fetch the log directory based on `log.dir`. It seems simpler to keep these tests and only update the `TestUtils.createBrokerConfig()` to fill in the property based on the directory number.

```
    if (logDirCount > 1) {
      val logDirs = (1 to logDirCount).toList.map(i => TestUtils.tempDir().getAbsolutePath).mkString("","")
      props.put(KafkaConfig.LogDirsProp, logDirs)
    } else {
      props.put(KafkaConfig.LogDirProp, TestUtils.tempDir().getAbsolutePath)
    }
```"
128804228,2929,junrao,2017-07-21T16:26:51Z,"Could we just use the existing def replicasInState(topic: String, state: ReplicaState)?"
128805708,2929,junrao,2017-07-21T16:34:35Z,"This method is actually called in places after log loading. But in all other places, IOExceptions are handled by the callers. Perhaps we can adjust the comment a bit."
128806377,2929,junrao,2017-07-21T16:37:50Z,"Ditto as the above since it can be called after loading. Also, we should change @throws?"
128806610,2929,junrao,2017-07-21T16:39:12Z,Ditto as the above since it can be called after loading.
128807592,2929,junrao,2017-07-21T16:44:48Z,It seems that logDirFailureChannel can be removed now.
128818124,2929,lindong28,2017-07-21T17:36:06Z,I missed this. Thanks for catching this. It is fixed now.
128818202,2929,lindong28,2017-07-21T17:36:29Z,Sure. I have removed this method.
128818227,2929,lindong28,2017-07-21T17:36:36Z,It is fixed now.
128818243,2929,lindong28,2017-07-21T17:36:41Z,Sure. It is fixed now.
128818272,2929,lindong28,2017-07-21T17:36:46Z,It is fixed now.
128828324,2929,becketqin,2017-07-21T18:19:52Z,We also added ProduceRequest V4.
128828858,2929,lindong28,2017-07-21T18:22:05Z,Remove logDirFailureChannel from Cleaner.
128833298,2929,becketqin,2017-07-21T18:40:41Z,"Not sure if this is in KIP-113, but we should probably also consider assigning the partitions based on the actual logDir size instead/in addition to the number of partitions. We can have a follow up patch for this."
128850166,2929,becketqin,2017-07-21T20:04:24Z,nit: onReplicaBecomeOffline -> onReplicasBecomeOffline
128850247,2929,becketqin,2017-07-21T20:04:43Z,effected -> affected
128851186,2929,lindong28,2017-07-21T20:10:01Z,I will add code to catch and handle IOException in `lockLogDirs()`
128887429,2929,becketqin,2017-07-22T02:49:22Z,There are a lot of similar try/catch logic in this class. Maybe we can group them into a lambda like we did for inLock.
128889008,2929,becketqin,2017-07-22T04:30:34Z,"In GroupCoordinator and txnCoordinator, we load the state using FileRecords.readInto() and if IOException occurs, currently we log it and let it go. I am not sure if this has been discussed before, but should we notify controller to reelect leader in this case? It does not have to be in this patch. We can do it separately. Let's see what @junrao say."
128889452,2929,becketqin,2017-07-22T04:50:50Z,Can we add a comment explaining the logic here? I was confused first time see this logic.
128889819,2929,lindong28,2017-07-22T05:17:13Z,I am not sure whether we should add comment regarding the ProduceRequest. @junrao said earlier that we only need to add comments regarding requested used between brokers. I guess we can do this in the other followup patch if needed.
128889832,2929,lindong28,2017-07-22T05:18:33Z,Yeah I think it is useful. It is probably better to do it in KIP-113 so that we can finish this KIP sooner :)
128889837,2929,lindong28,2017-07-22T05:18:53Z,Thanks! Fixed now.
128889858,2929,lindong28,2017-07-22T05:19:33Z,Sure. Fixed now.
128889862,2929,lindong28,2017-07-22T05:19:48Z,Good point. It is fixed now.
128889977,2929,lindong28,2017-07-22T05:27:59Z,"I think both GroupCoordinator and txnCoordinator will access log directory via methods in Log, e.g. `log.read` in `GroupMetadataManager.loadGroupsAndOffsets()`. Thus IOException will be caught and trigger `maybeAddLogFailureEvent()`. I think this handling of IOException is good enough for `GroupCoordinator`. If this handling of IOException is not good enough for `txnCoordinator`, it is probably an existing problem and needs feedback from the developers who are more knowledgeable in transaction related logic.

Yeah let's do it in a followup patch if any fix is need for `txnCoordinator`."
128889980,2929,lindong28,2017-07-22T05:28:20Z,Fixed now.
128890112,2929,lindong28,2017-07-22T05:36:58Z,Sure. I have updated the comment to clarify this better.
128903577,2929,becketqin,2017-07-22T19:37:47Z,Would it be better to put this method into CoreUtils?
128903738,2929,lindong28,2017-07-22T19:45:05Z,"I have thought about this. We can do it if we add another 1-2 parameters to this method. This is because both the object `logDirFailureChannel` and `dir` in the body of `maybeHandleIOException` belong to `Log`. Thus the `maybeHandleIOException()` in its current form can not be static.

It seems to me that the code is simpler by putting it only in Log. We can move it to CoreUtils with the extra parameter if you think that is better.
"
452397898,9001,kowshik,2020-07-09T18:05:44Z,add doc
452397966,9001,kowshik,2020-07-09T18:05:50Z,add doc
452398154,9001,kowshik,2020-07-09T18:06:14Z,add doc
452398464,9001,kowshik,2020-07-09T18:06:49Z,remove word 'should'
452399744,9001,kowshik,2020-07-09T18:09:17Z,add doc to entire class
452400240,9001,kowshik,2020-07-09T18:10:10Z,add doc to entire class
452400376,9001,kowshik,2020-07-09T18:10:27Z,attributes can be final
452401987,9001,kowshik,2020-07-09T18:13:20Z,"1. add test code in `KafkaAdminClientTest`
2. final variable names"
452402127,9001,kowshik,2020-07-09T18:13:36Z,add test code in `KafkaAdminClientTest`
452403561,9001,kowshik,2020-07-09T18:16:13Z,1 line gap before `cal`
452406393,9001,kowshik,2020-07-09T18:21:18Z,Eliminate and use INVALID_REQUEST
452406721,9001,kowshik,2020-07-09T18:21:49Z,Eliminate and use INVALID_REQUEST
452407219,9001,kowshik,2020-07-09T18:22:42Z,"space between "","" and ""key"""
452407255,9001,kowshik,2020-07-09T18:22:46Z,final
452407329,9001,kowshik,2020-07-09T18:22:56Z,final
452408614,9001,kowshik,2020-07-09T18:25:15Z,"make variables final throught class
add doc"
452410015,9001,kowshik,2020-07-09T18:27:45Z,Fix ApiKeys
452410620,9001,kowshik,2020-07-09T18:28:54Z,Eliminate timeout?
452413347,9001,kowshik,2020-07-09T18:34:02Z,add doc and explain various cases
452413777,9001,kowshik,2020-07-09T18:34:54Z,call the variable as `nodeContents` ?
452418487,9001,kowshik,2020-07-09T18:44:00Z,Perhaps add info about newFeatures and incompatibleBrokers.
452418679,9001,kowshik,2020-07-09T18:44:24Z,can improve by splitting into few lines
452426595,9001,kowshik,2020-07-09T18:59:05Z,check braces ()
452428079,9001,kowshik,2020-07-09T19:01:44Z,"s/supported/supportedFeatures
same for other one"
452428293,9001,kowshik,2020-07-09T19:02:07Z,"say ""if there are any feature incompatibilities found."""
452428858,9001,kowshik,2020-07-09T19:03:22Z,"Add unit test
Add doc"
452432658,9001,kowshik,2020-07-09T19:10:56Z,Shouldn't the code be waiting here?
452435772,9001,kowshik,2020-07-09T19:16:57Z,add doc
452435913,9001,kowshik,2020-07-09T19:17:15Z,remove these 2 lines
452436233,9001,kowshik,2020-07-09T19:17:41Z,revert the file eventually
453842949,9001,abbccdda,2020-07-13T18:21:40Z,nit: get a ` {@link UpdateFinalizedFeaturesResult}` as well
456123467,9001,abbccdda,2020-07-16T22:57:10Z,s/`as input a set of FinalizedFeatureUpdate`/`in a set of feature updates`
456124708,9001,abbccdda,2020-07-16T23:00:50Z,"For the entire sentence, I assume you want to say something like
```
It could be done by turning on the allowDowngrade flag and setting the max version level to be less than 1
```"
456125212,9001,abbccdda,2020-07-16T23:02:31Z,"Looking at `UpdateFinalizedFeaturesResult`, we don't have a per feature based error code returned. If this is the case, how could we know which feature is missing?"
456126450,9001,abbccdda,2020-07-16T23:06:05Z,"We should suggest in what circumstances a user may require sending the request directly to the controller, to me if there is a case where user wants stronger consistency."
456150664,9001,abbccdda,2020-07-17T00:25:56Z,We should consider using Optional for `finalizedFeaturesEpoch` to indicate absence.
456151287,9001,abbccdda,2020-07-17T00:27:57Z,"nit: one parameter each line, with the first parameter on the same line as constructor name."
456151476,9001,abbccdda,2020-07-17T00:28:41Z,nit: seems not necessary
456151635,9001,abbccdda,2020-07-17T00:29:16Z,`false otherwise` doesn't provide too much useful info.
456603010,9001,abbccdda,2020-07-17T18:20:41Z,nit: we could just `return new UpdateFinalizedFeaturesRequestData().setFinalizedFeatureUpdates(items)`
456849186,9001,abbccdda,2020-07-19T02:19:21Z,missing header
456855815,9001,abbccdda,2020-07-19T04:00:54Z,nit: put first parameter on this line.
456855941,9001,abbccdda,2020-07-19T04:02:52Z,"The definition seems not aligned with the KIP which states `updateFeatures`, do you think it's necessary to mention `finalized` in all the function signatures?"
456857080,9001,abbccdda,2020-07-19T04:19:26Z,"It looks weird to complete `callViaLeastLoadedNode` in a controller response handler. I'm inclined to increase a bit on the code duplication, based on `if (options.sendRequestToController())` to have two separate request traces like:
```
if (options.sendRequestToController()) {
            ...
            runnable.call(callControllerNode, now);
        } else {
            ...
            runnable.call(callViaLeastLoadedNode, now);
        }
```
and try to complete the same future."
456857730,9001,abbccdda,2020-07-19T04:29:50Z,"Would be good to redundantly copy over the expected error codes from `Admin.java` definition, similar to other response class such as `OffsetCommitResponse`"
456858439,9001,abbccdda,2020-07-19T04:39:21Z,Seems not necessary to have this helper as it doesn't reduce the code length.
456858741,9001,abbccdda,2020-07-19T04:42:42Z,space
456859001,9001,abbccdda,2020-07-19T04:45:45Z,"This is a bit unique, since we should commonly rely on the error code to propagate information instead of a message which has unbounded size. Could you explain why we couldn't simply re-invent a new error code if existing ones are not sufficient?"
456859342,9001,abbccdda,2020-07-19T04:50:52Z,"We don't need to include the same error information twice, as the client side will recognize anyway."
456859761,9001,abbccdda,2020-07-19T04:56:54Z,access should be private
456859864,9001,abbccdda,2020-07-19T04:58:05Z,nit: the comment seems unnecessary on L3011
456859960,9001,abbccdda,2020-07-19T04:58:51Z,"Commonly in scala we try to avoid using return, consider using `if-else` instead."
456860187,9001,abbccdda,2020-07-19T05:01:31Z,`setting the allowDowngrade flag to true in the request`
456862645,9001,abbccdda,2020-07-19T05:31:43Z,`ApiKeys.UPDATE_FINALIZED_FEATURES API`
456862745,9001,abbccdda,2020-07-19T05:33:11Z,We only need to mark testing only comment on the functions 
456863153,9001,abbccdda,2020-07-19T05:38:15Z,nit: mark the parameter as `logIncompatibilities = true)`
456863287,9001,abbccdda,2020-07-19T05:39:58Z,redundant {}
456863554,9001,abbccdda,2020-07-19T05:42:50Z,redundant {}
456863914,9001,abbccdda,2020-07-19T05:48:12Z,Do we need this precision of exact wait time? Could we just track the function start time and compare with current system time for expiration?
456931477,9001,abbccdda,2020-07-19T17:03:30Z,"`For a new Kafka cluster (i.e. it is deployed first time), we would like to start the cluster with all the possible supported features finalized immediately.`
I think this comment is hard to understand if reader has zero context on the feature versioning. It would be good to include a short explanation on what does a `supported feature` mean, and what it means to be `finalized`.
`The new cluster will almost never be started with an old IBP config thats less than KAFKA_2_7_IV0.`
This sentence is positioned awkwardly. I would suggest we just propose `As a new cluster starting with IBP setting equal to or greater than KAFKA_2_7_IV0`"
456931559,9001,abbccdda,2020-07-19T17:04:23Z,"`then here is how we it` could be removed:
`Assuming this is the case, then the controller...`"
456932817,9001,abbccdda,2020-07-19T17:17:40Z,"Maybe a newbie question here: since the `supportedFeatures` could be mutated, why couldn't we just assume its min level marks the `defaultFeatureMinVersionLevels`? Trying to understand the necessity for secondary bookkeeping. Might be good to also put reasonings in the meta comment as well to clear confusion."
456934831,9001,abbccdda,2020-07-19T17:38:27Z,"If this is broker required feature set, I feel we could name it something like `brokerRequiredVersionRange`. `Updated` sounds a bit blur for reader, as it couldn't infer the subject."
456935073,9001,abbccdda,2020-07-19T17:40:37Z,"What about the case where `existingVersionRange.min() > updatedVersionRange.max()` is true? For example:
```
existing: [4, 5]
updated: [1, 2]
```
Are we enabling version 3 as well?"
457072387,9001,abbccdda,2020-07-20T05:46:02Z,"Is it ok for us to always do `updateFeatureZNode`, since this call is idempotent?"
457075484,9001,abbccdda,2020-07-20T05:52:11Z,Could the receiving broker analyze the request and decide to shut down itself? What's the gain we have by avoiding sending update metadata to incompatible brokers?
457076040,9001,abbccdda,2020-07-20T05:53:19Z,Replace with `nonEmpty`
457076787,9001,abbccdda,2020-07-20T05:54:49Z,"Could we avoid blocking controller processing here, by putting the callback into a delayed queue or sth?"
457077246,9001,abbccdda,2020-07-20T05:55:46Z,`incompatibleFeatures`?
457077954,9001,abbccdda,2020-07-20T05:57:18Z,"nit: I have seen that we use both `map{` and `map {`, could we try using only one format consistently within the current file?"
457802220,9001,kowshik,2020-07-21T02:40:49Z,Done. Updated the doc now.
457806176,9001,kowshik,2020-07-21T02:56:06Z,Done.
457806399,9001,kowshik,2020-07-21T02:57:00Z,Done.
457806541,9001,kowshik,2020-07-21T02:57:43Z,"To your point, this information is available in the error message returned in the response.

The feature updates are atomically applied to ZK by the controller i.e it is all or none. We don't have a use case (yet) where we have to programmatically learn which feature updates are incorrect. Instead an error message with details seems sufficient to us. Please let me know how you feel about it, and if you feel that we are better off in returning per-feature-update error code. This was discussed in the [KIP-584 thread](https://lists.apache.org/thread.html/r0b04fa38fb3c4a33bc6b6419284f557266e662c9840a2f7661401f30%40%3Cdev.kafka.apache.org%3E), search for the word ""transaction""."
457808548,9001,kowshik,2020-07-21T03:06:14Z,Done.
457809543,9001,kowshik,2020-07-21T03:10:05Z,Done.
457812340,9001,kowshik,2020-07-21T03:19:53Z,Done. Good point.
457812690,9001,kowshik,2020-07-21T03:21:00Z,Done.
457812867,9001,kowshik,2020-07-21T03:21:44Z,Done. Removed.
457813021,9001,kowshik,2020-07-21T03:22:17Z,Done. Removed.
457813152,9001,kowshik,2020-07-21T03:22:53Z,Done. Good point.
457815251,9001,kowshik,2020-07-21T03:32:07Z,Done. Good point.
457821195,9001,kowshik,2020-07-21T03:56:30Z,It calls into couple other helper functions. Let us keep it.
457821791,9001,kowshik,2020-07-21T03:58:56Z,Done.
457821925,9001,kowshik,2020-07-21T03:59:25Z,Done.
457823139,9001,kowshik,2020-07-21T04:04:34Z,"The purpose of the error message is to sometimes describe with finer details on what is the error (such as which feature update is incorrect). To your point, it seems there are existing response types that do allow for an error message, examples are: `CreateTopicsResponse`, `CreatePartitionsResponse`, `DeleteAclsResponse` etc.
There is ongoing related discussion under another PR review comment and we can continue the discussion there: https://github.com/apache/kafka/pull/9001/files#r456125212 . "
457872699,9001,kowshik,2020-07-21T06:46:14Z,Done. Updated the doc.
457873376,9001,kowshik,2020-07-21T06:47:55Z,Done.
457876122,9001,kowshik,2020-07-21T06:54:17Z,"Done. This case is also handled now.
To your point, the case  where `updated.max < existing.min` can never happen unless brokers get downgraded (after finalizing features at higher levels), and especially if the downgrade was done improperly (without applying feature tooling commands). It's a rare case. But even in that case, the broker will start crashing because of incompatibility in supported feature version max level, so the problem is found before it reaches this point."
457879567,9001,kowshik,2020-07-21T07:02:03Z,Not sure I understood. We will only update the `FeatureZNode` if the status is not disabled currently (see the implementation below). What am I missing?
457880383,9001,kowshik,2020-07-21T07:03:54Z,This handles the race condition described in the KIP-584 [in this section](https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features#KIP584:Versioningschemeforfeatures-Incompatiblebrokerlifetimeracecondition). Please refer to the KIP for details. I have also added doc to this method.
457883779,9001,kowshik,2020-07-21T07:11:03Z,Done.
457884903,9001,kowshik,2020-07-21T07:13:24Z,"I feel that there isn't a pressing reason to optimize this API path currently, and make it async.
The API is not going to be frequently used, and an infrequent write to a ZK node with low write contention feels like a relatively inexpensive case that we could block the controller on.
Please let me know how you feel."
457885415,9001,kowshik,2020-07-21T07:14:28Z,Done.
457885467,9001,kowshik,2020-07-21T07:14:38Z,Done. Made the comment better. Pls take a look.
457890046,9001,kowshik,2020-07-21T07:23:55Z,"It is already explained in the class level doc. This is also explained in the KIP-584 [in this section](https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features#KIP584:Versioningschemeforfeatures-Featureversiondeprecation).

This is needed because `defaultFeatureMinVersionLevels` is mainly for feature version deprecation. When we deprecate feature version levels, we first bump the `defaultFeatureMinVersionLevels` in a broker release (after making an announcement to community). This will automatically mean clients have to stop using the finalized min version levels that have been deprecated (because upon startup the controller will write the `defaultFeatureMinVersionLevels` to ZK from within `KafkaController#setupFeatureVersioning` method). Once the write to ZK happens, clients that are using the finalized features are forced to stop using the deprecated version levels.

Then, finally in the future when we remove the code for the deprecated version levels, that is when we will bump the min version for the supported feature in the broker. Thereby we will completely drop support for a feature version altogether."
457891034,9001,kowshik,2020-07-21T07:25:44Z,Done.
457892024,9001,kowshik,2020-07-21T07:27:45Z,Done. Calling it `incompatibleFeaturesInfo` now.
457892374,9001,kowshik,2020-07-21T07:28:29Z,Done.
457892622,9001,kowshik,2020-07-21T07:28:55Z,Done.
457892858,9001,kowshik,2020-07-21T07:29:25Z,Done.
457897331,9001,kowshik,2020-07-21T07:37:49Z,"Done. Made it the way you suggested, pls take a look.
Overall either way looked fine to me but the one you suggested is a bit simpler."
457903477,9001,kowshik,2020-07-21T07:48:51Z,Done.
457903582,9001,kowshik,2020-07-21T07:49:04Z,Done. 
457904782,9001,kowshik,2020-07-21T07:51:11Z,Done. Removed.
457905010,9001,kowshik,2020-07-21T07:51:34Z,Done.
457939241,9001,kowshik,2020-07-21T08:50:07Z,Done.
457981705,9001,kowshik,2020-07-21T10:01:43Z,Done. I'm calling it `brokerDefaultVersionRange` now.
457985348,9001,kowshik,2020-07-21T10:08:39Z,"Done. Removed the word ""finalized""in the context of this API."
457986837,9001,kowshik,2020-07-21T10:11:25Z,Done.
458503099,9001,abbccdda,2020-07-22T02:52:48Z,nit: do `{@link DescribeFeaturesResult}`
458985306,9001,abbccdda,2020-07-22T18:06:59Z,nit: new line
458985676,9001,abbccdda,2020-07-22T18:07:40Z,Could be simplified as `sendToController`
458986288,9001,abbccdda,2020-07-22T18:08:46Z,"Why do we need this override, which seems to be exactly the same with super class?"
458987227,9001,abbccdda,2020-07-22T18:10:24Z,s/featureName/feature
459002813,9001,abbccdda,2020-07-22T18:37:41Z,"The two `Call` structs only have two differences:

1. Used different node provider
2. One would handle not controller, one not

So I would suggest a bit refactoring to reduce the code redundancy, by providing a helper as:
```
getDescribeFeaturesCall(boolean sendToController) {
  NodeProvider provider = sendToController ? new ControllerNodeProvider()  : new 
  LeastLoadedNodeProvider();
  return new Call(""describeFeatures"", calcDeadlineMs(now, options.timeoutMs()), 
provider, ...
  ...
     void handleResponse(AbstractResponse response) {
      ....
      if ( apiVersionsResponse.data.errorCode() == Errors.NOT_CONTROLLER.code() && sendToController) 
     {
        handleNotControllerError(Errors.NOT_CONTROLLER);
     }
  };
}
```
"
459003818,9001,abbccdda,2020-07-22T18:39:25Z,"Same here, why do we need this extension?"
459004097,9001,abbccdda,2020-07-22T18:39:54Z,"As discussed offline, we need to extend the result as per feature."
459004701,9001,abbccdda,2020-07-22T18:40:56Z,Couldn't we just use `isIncompatibleWith`?
459005027,9001,abbccdda,2020-07-22T18:41:29Z,Why do we jump from code 88 to 91?
459006265,9001,abbccdda,2020-07-22T18:43:34Z,"Make sense, after looking further I realized that we also did some data format conversion."
459008772,9001,abbccdda,2020-07-22T18:47:48Z,Do we also need to check `allowAutoDowngrade` here?
459009582,9001,abbccdda,2020-07-22T18:49:09Z,We could consider either making `data` to be private or remove this unnecessary accessor. I would prefer making it private.
459009753,9001,abbccdda,2020-07-22T18:49:24Z,Same here for consistency.
459010149,9001,abbccdda,2020-07-22T18:50:03Z,"Spaces look weird, let's try to remove all `two space` cases in this file."
459014883,9001,abbccdda,2020-07-22T18:58:04Z,"My pt is that since we know the outcome (feature versioning will be disabled), we don't need to do one more lookup but just try to push the update. Anyway, I think this is a nit."
459183702,9001,abbccdda,2020-07-23T02:26:11Z,Parameters could be on the same line to be consistent with L80
459185058,9001,abbccdda,2020-07-23T02:32:59Z,"`testUpdateFeatures` should be suffice, as we sometimes are not passing in a real error."
459185171,9001,abbccdda,2020-07-23T02:33:32Z,nit: prefer using `error == Errors.NONE`
459186000,9001,abbccdda,2020-07-23T02:37:26Z,`Collections.emptyList()` should be suffice. 
459186249,9001,abbccdda,2020-07-23T02:38:39Z,We should have a matcher checking whether the sent request is pointing at the correct controller id.
459186371,9001,abbccdda,2020-07-23T02:39:08Z,"make it a variable, as `int controllerId = 1`"
459186681,9001,abbccdda,2020-07-23T02:40:44Z,`defaultFeatureMetadata` should be suffice. AK repo normally tries to avoid using `get` as func prefix.
459186760,9001,abbccdda,2020-07-23T02:41:09Z,nit: could use `Utils.mkMap` to simplify here.
459188638,9001,abbccdda,2020-07-23T02:50:16Z,nit: `zkClient.getDataAndVersion(FeatureZNode.path)._2` should be suffice
459188844,9001,abbccdda,2020-07-23T02:51:18Z,s/Znode/ZNode
459188931,9001,abbccdda,2020-07-23T02:51:41Z,s/ is is / is
459189057,9001,abbccdda,2020-07-23T02:52:09Z,remove `in ZK`
459189198,9001,abbccdda,2020-07-23T02:52:55Z,nit: s/it's/its
459189220,9001,abbccdda,2020-07-23T02:53:03Z,nit: s/it's/its
459189352,9001,abbccdda,2020-07-23T02:53:49Z,remove `one and`
459189517,9001,abbccdda,2020-07-23T02:54:36Z,remove `and their version levels` or restructure as `the information about finalized features' version levels`
459189854,9001,abbccdda,2020-07-23T02:55:55Z,"Could we just remove ` the feature versioning system (KIP-584) is enabled, and`? It does not provide any useful information."
459189970,9001,abbccdda,2020-07-23T02:56:23Z,s/This status/The enabled status 
459190247,9001,abbccdda,2020-07-23T02:57:49Z,We don't need to capitalize `Broker` here
459190329,9001,abbccdda,2020-07-23T02:58:15Z,same here
459190412,9001,abbccdda,2020-07-23T02:58:44Z,"s/`The reason to do this is that...`/`This process ensures we do not enable all the possible features immediately after an upgrade, which could be harmful to the application.`"
459190870,9001,abbccdda,2020-07-23T03:01:03Z,remove `then`
459191665,9001,abbccdda,2020-07-23T03:05:05Z,{} could be removed.
459192115,9001,abbccdda,2020-07-23T03:07:16Z,nit: would be easier to read if we always compare `existingVersionRange` towards `brokerDefaultVersionRange` instead of flipping in this statement.
459192901,9001,abbccdda,2020-07-23T03:11:05Z,I think we need to override `equals` here.
459192976,9001,abbccdda,2020-07-23T03:11:26Z,cache
459193453,9001,abbccdda,2020-07-23T03:13:22Z,`No UpdateMetadataRequest will be sent to broker...`
459193711,9001,abbccdda,2020-07-23T03:14:45Z,does `features` guarantee to be non-null?
459193767,9001,abbccdda,2020-07-23T03:15:02Z,format
459194593,9001,abbccdda,2020-07-23T03:19:03Z,"Yea, I'm a bit worried about such a blocking call here as we don't have a precedence for relying on zk connect timeout (18 seconds), besides the result doesn't matter to the controller (since client will do the retry). cc @cmccabe @junrao to see if they have a different opinion on this."
459195272,9001,abbccdda,2020-07-23T03:22:08Z,what `clients` are we referring to here?
459195338,9001,abbccdda,2020-07-23T03:22:35Z,`The class is immutable in production`
459528132,9001,abbccdda,2020-07-23T15:16:34Z,Is it necessary to quote `incompatible`?
459538486,9001,abbccdda,2020-07-23T15:30:57Z,Could you explain a bit why we no longer use singletons for feature cache?
459554866,9001,abbccdda,2020-07-23T15:54:51Z,We could explicitly mention this is `either or` result.
459556307,9001,abbccdda,2020-07-23T15:56:45Z,Could be initialized closer to L3005
459562747,9001,abbccdda,2020-07-23T16:06:37Z,Could we assert the expected version here?
459563255,9001,abbccdda,2020-07-23T16:07:24Z,nit: new line
459564526,9001,abbccdda,2020-07-23T16:09:36Z,...`WithInvalidSmallValue`
459564664,9001,abbccdda,2020-07-23T16:09:48Z,...`WithInvalidLargeValue`
459565850,9001,abbccdda,2020-07-23T16:11:41Z,What's the purpose of this second test?
459566826,9001,abbccdda,2020-07-23T16:13:14Z,nit: replace with `nonControllerServers.head`
459569818,9001,abbccdda,2020-07-23T16:17:56Z,This case seems not to be tested yet.
459570396,9001,abbccdda,2020-07-23T16:18:54Z,Format
459653525,9001,abbccdda,2020-07-23T18:41:59Z,format
459655758,9001,abbccdda,2020-07-23T18:45:44Z,We could refactor out a helper in `UpdateFeaturesRequest` to create `FeatureUpdateKey`
459658554,9001,abbccdda,2020-07-23T18:50:45Z,"Could we add some unit tests in `KafkaApisTest.scala`, once the refactoring is finished?"
460365110,9001,kowshik,2020-07-25T04:51:52Z,Done.
460365185,9001,kowshik,2020-07-25T04:52:57Z,Done.
460365229,9001,kowshik,2020-07-25T04:53:47Z,Done.
460365305,9001,kowshik,2020-07-25T04:54:32Z,Done. Removed now. Didn't realize it was present in super class too.
460365462,9001,kowshik,2020-07-25T04:56:23Z,Done. Actually `feature` is removed from this class now.
460369033,9001,kowshik,2020-07-25T05:44:35Z,Done. Good point.
460369103,9001,kowshik,2020-07-25T05:45:25Z,Done. Removed now.
460369423,9001,kowshik,2020-07-25T05:49:31Z,Done. Removed this method now.
460369509,9001,kowshik,2020-07-25T05:50:19Z,Done. Not intentional. Changed to 89 now.
460369545,9001,kowshik,2020-07-25T05:50:51Z,Done.
460370082,9001,kowshik,2020-07-25T05:57:51Z,Done. Made the attribute private.
460370089,9001,kowshik,2020-07-25T05:58:03Z,Done. Made the attribute private.
460370162,9001,kowshik,2020-07-25T05:59:09Z,Done.
460371001,9001,kowshik,2020-07-25T06:10:14Z,Done.
460371104,9001,kowshik,2020-07-25T06:11:43Z,Done.
460371146,9001,kowshik,2020-07-25T06:12:24Z,Done.
460371283,9001,kowshik,2020-07-25T06:14:31Z,Done.
460371443,9001,kowshik,2020-07-25T06:16:22Z,Done.
460371572,9001,kowshik,2020-07-25T06:18:05Z,Done.
460373122,9001,kowshik,2020-07-25T06:38:13Z,"I have improved the matcher now, but how do I check the correct controller id?"
460373211,9001,kowshik,2020-07-25T06:39:17Z,`newVersion` is more readable than `_2`.
460373254,9001,kowshik,2020-07-25T06:39:52Z,Done.
460373296,9001,kowshik,2020-07-25T06:40:15Z,Done.
460373384,9001,kowshik,2020-07-25T06:41:31Z,Done.
460373389,9001,kowshik,2020-07-25T06:41:37Z,Done.
460373454,9001,kowshik,2020-07-25T06:42:14Z,Done.
460373463,9001,kowshik,2020-07-25T06:42:21Z,Done.
460373549,9001,kowshik,2020-07-25T06:43:36Z,Done.
460373561,9001,kowshik,2020-07-25T06:43:52Z,Done.
460373590,9001,kowshik,2020-07-25T06:44:08Z,Done.
460373616,9001,kowshik,2020-07-25T06:44:20Z,Done.
460373708,9001,kowshik,2020-07-25T06:45:15Z,Done.
460373729,9001,kowshik,2020-07-25T06:45:37Z,Done.
460373746,9001,kowshik,2020-07-25T06:45:51Z,Done.
460373809,9001,kowshik,2020-07-25T06:46:45Z,Done.
460374117,9001,kowshik,2020-07-25T06:50:59Z,"`FeatureZNode` is a `case class`, and therefore the `equals` method is auto generated. Let me know if I'm missing something. Here is the doc: https://docs.scala-lang.org/overviews/scala-book/case-classes.html."
460374136,9001,kowshik,2020-07-25T06:51:22Z,Done.
460374558,9001,kowshik,2020-07-25T06:57:15Z,"We can not just push the update, because, we have to decide if the node needs to be created or existing node should be updated. That is why we read the node first to understand if it exists or not, then we update the existing node only if the status does not match (this avoids a ZK write in the most common cases)."
460374653,9001,kowshik,2020-07-25T06:58:43Z,Done.
460375004,9001,kowshik,2020-07-25T07:03:27Z,"Yes, `Broker.features` is just empty when there are no features set or none decoded from the `BrokerIdZNode`."
460375038,9001,kowshik,2020-07-25T07:04:01Z,Done.
460375251,9001,kowshik,2020-07-25T07:06:15Z,Done. I was referring to external clients of Kafka. Have updated the doc now.
460375270,9001,kowshik,2020-07-25T07:06:32Z,Done.
460375378,9001,kowshik,2020-07-25T07:08:11Z,Done. Removed quotes.
460375750,9001,kowshik,2020-07-25T07:13:11Z,"It became painful to write tests using singletons. Particularly in `kafka.server.UpdateFeaturesTest` we would like to simulate presence of multiple brokers and a controller within the same test process. Then we would like to set incompatible features for some brokers, and compatible features for some others. Using a singleton for feature cache made it impossible to set up such an environment for testing. That is why we no longer use a singleton, instead we instantiate the class once in `KafkaServer` and we use the object wherever needed."
460375901,9001,kowshik,2020-07-25T07:15:07Z,Done.
460375935,9001,kowshik,2020-07-25T07:15:25Z,Done.
460376009,9001,kowshik,2020-07-25T07:16:37Z,Done.
460376035,9001,kowshik,2020-07-25T07:16:42Z,Done.
460376322,9001,kowshik,2020-07-25T07:20:35Z,"It is explained in the test doc above, and, I have also added comments now. The purpose is to check that the ZK watch on the FeatureZNode was re-established by the broker, after the first update triggers a ZK notification that populates the cache. The best way to check it is to update the node again and see if the notification is received by the broker again."
460376477,9001,kowshik,2020-07-25T07:22:27Z,Done.
460376521,9001,kowshik,2020-07-25T07:22:56Z,Done.
460376578,9001,kowshik,2020-07-25T07:23:54Z,Done.
461412713,9001,kowshik,2020-07-28T08:35:14Z,Done.
461413382,9001,kowshik,2020-07-28T08:36:21Z,Done.
461413992,9001,kowshik,2020-07-28T08:37:29Z,Done.
461416641,9001,kowshik,2020-07-28T08:42:00Z,Done.
461417458,9001,kowshik,2020-07-28T08:43:22Z,"Sure, we can hear what others say."
461418384,9001,kowshik,2020-07-28T08:44:51Z,Will take a look.
461418530,9001,kowshik,2020-07-28T08:45:03Z,This method has changed greatly and it has been moved to `KafkaController.scala`.
461418681,9001,kowshik,2020-07-28T08:45:17Z,This method has changed greatly and it has been moved to `KafkaController.scala`.
461421563,9001,kowshik,2020-07-28T08:49:51Z,"Hmm, there seem to be very few call sites and therefore seems ok to inline it. Let me know!"
461424717,9001,kowshik,2020-07-28T08:54:59Z,Added a test now in `UpdateFeaturesTest.scala`. Look for `testSuccessfulFeatureUpgradeAndWithNoExistingFinalizedFeatures`.
461771992,9001,abbccdda,2020-07-28T18:03:35Z,nit: s/name/names
462453975,9001,abbccdda,2020-07-29T17:08:15Z,"Note in the post-KIP-500 world, this feature could still work, but the request must be redirected to the controller inherently on the broker side, instead of sending it directly. So in the comment, we may try to phrase it to convey the principal is that `the request must be handled by the controller` instead of `the admin client must send this request to the controller`. "
462456942,9001,abbccdda,2020-07-29T17:13:03Z,should this a per feature error or a top level error?
462458761,9001,abbccdda,2020-07-29T17:15:56Z,"For top level exception such as cluster authorization exception, we could just define a top level error code instead of check-marking every feature with the redundant error code. I know we have been a bit inconsistent in such a case, but personally feel having layered error codes could make the response handling clear of whether it is per feature issue, or a high level issue."
462458948,9001,abbccdda,2020-07-29T17:16:15Z,Space
462459015,9001,abbccdda,2020-07-29T17:16:21Z,Same here
462459152,9001,abbccdda,2020-07-29T17:16:34Z,Same here
462462109,9001,abbccdda,2020-07-29T17:21:30Z,`can be issued only to the controller.`/ `must be processed by the controller`
462462268,9001,abbccdda,2020-07-29T17:21:47Z,`could be processed by any random broker`
462462801,9001,abbccdda,2020-07-29T17:22:41Z,Same here
462463977,9001,abbccdda,2020-07-29T17:24:33Z,"Try to put first parameter on the same line as the constructor, and align the rest parameters."
462465188,9001,abbccdda,2020-07-29T17:26:28Z,"This won't work well with string format, consider doing `orElse`"
462465387,9001,abbccdda,2020-07-29T17:26:50Z,new line
462471038,9001,abbccdda,2020-07-29T17:36:28Z,"I suggest we build a static method in the `UpdateFeaturesRequest` class to avoid exposing the sub modules of feature data, such like:
```
public static UpdateFeaturesRequestData getFeatureRequest(final Map<String, FeatureUpdate> featureUpdate);
```"
462472940,9001,abbccdda,2020-07-29T17:39:43Z,Does this overlap with `completeUnrealizedFutures` check? We could just keep one to reduce the checking complexity. 
462475689,9001,abbccdda,2020-07-29T17:44:02Z,"You are right, it seems not necessary."
462477303,9001,abbccdda,2020-07-29T17:46:32Z,"Do we need to make this a public error? It seems only be used internally, so could be made private if we don't have intention to let user catch."
462480826,9001,abbccdda,2020-07-29T17:52:16Z,Comment here since no better place: createApiVersionsResponse on L198 could be made private
462483315,9001,abbccdda,2020-07-29T17:56:31Z,nit: could be replaced with lambda
462485886,9001,abbccdda,2020-07-29T18:00:41Z,Should we also mention that this flag would fail the request when we are not actually doing a downgrade? 
462488078,9001,abbccdda,2020-07-29T18:04:41Z,"I'm actually wondering whether this is too strict in the perspective of a user. If they accidentally set a feature version larger than the cache, what they only care about is to be able to change the version to it. So it's a matter of whether we think this is a user error, or this could happen when user gets stale feature information from a broker while the downgrade already succeed eventually. 

If we want to keep this check, it makes sense to update the meta comments around `allowDowngrade` to inform user that the request could fail when the target version is actually higher than the current finalized feature."
462489375,9001,abbccdda,2020-07-29T18:07:00Z,Could be moved to the `UpdateFeaturesResponse`
462492285,9001,abbccdda,2020-07-29T18:11:58Z,Could we make `updates` as a pass-in parameter to avoid calling `makeTestFeatureUpdates` twice?
462492825,9001,abbccdda,2020-07-29T18:12:58Z,nit: could use lambda
462496091,9001,abbccdda,2020-07-29T18:18:17Z,"Do we need to call `featureCache.waitUntilEpochOrThrow(newNode, config.zkConnectionTimeoutMs)` here to ensure the update is successful?"
462498961,9001,abbccdda,2020-07-29T18:23:15Z,"I see, still wondering if we could just check whether `newFeatures` is equal to `existingFeatureZNode.features`"
462500314,9001,abbccdda,2020-07-29T18:25:31Z,"Are we good to proceed in this case? When there is no overlapping between broker default features and remote finalized features, is the current controller still eligible?"
462501301,9001,abbccdda,2020-07-29T18:27:19Z,"I see, what would happen to a currently live broker if it couldn't get any metadata update for a while, will it shut down itself?"
462502780,9001,abbccdda,2020-07-29T18:29:57Z,"I see, still I'm a bit worried future changes could break this assumption. Not a bad idea to check `features != null`?"
462504467,9001,abbccdda,2020-07-29T18:32:45Z,State the error explicitly here.
462627189,9001,abbccdda,2020-07-29T22:30:26Z,"Yea, I mean you could use `val newVersion = zkClient.getDataAndVersion(FeatureZNode.path)._2`, but it's up to you."
462649895,9001,abbccdda,2020-07-29T23:38:21Z,Is this case covered by the case on L1931? Could we merge both?
462650343,9001,abbccdda,2020-07-29T23:39:44Z,We should be consistent and remove `()` from `maxVersionLevel`
462651241,9001,abbccdda,2020-07-29T23:42:50Z,nit: new line
462658154,9001,abbccdda,2020-07-30T00:05:36Z,"Could you clarify the reasoning here? If structs are not the same, are we going to do a partial update?"
462714278,9001,abbccdda,2020-07-30T03:34:34Z,Could we get a static method instead of initiating a new `FinalizedVersionRange` for a comparison every time?
462715368,9001,abbccdda,2020-07-30T03:39:07Z,Why don't we just use `System.currentTimeMillis()` to avoid conversion between nano time?
462715603,9001,abbccdda,2020-07-30T03:39:57Z,Seems not covered yet
462716040,9001,abbccdda,2020-07-30T03:41:34Z,Could be moved to `UpdateFeaturesResponse` as a utility.
462716875,9001,abbccdda,2020-07-30T03:44:56Z,"Some methods in the `BrokerFeatures` are not covered by this suite, such as `defaultMinVersionLevel`, `getDefaultFinalizedFeatures` and `hasIncompatibleFeatures`, you could use code coverage tool to figure out any missing part."
462717083,9001,abbccdda,2020-07-30T03:45:45Z,Indentation is not right.
462718258,9001,abbccdda,2020-07-30T03:50:44Z,The meta comment for `FinalizedFeatureCache` should be updated as it is now being accessed for both read and write
462719027,9001,abbccdda,2020-07-30T03:54:08Z,nit: this could be extracted as a common struct.
462719977,9001,abbccdda,2020-07-30T03:57:50Z,"Could we only pass in `featureCache` to reduce the class coupling here? As we already have `brokerFeatures` as a private parameter, it shouldn't be too hard to set a helper to get supported features."
463880076,9001,kowshik,2020-07-31T23:01:47Z,"Sorry, I do not understand why should describeFeatures (in post KIP-500) be handled only by controller?
"
463912157,9001,kowshik,2020-08-01T02:54:07Z,It does not overlap. This checks for unexpected responses for features that we never intended to update. `completeUnrealizedFutures` is for futures that we never got a response for from the server -- we need to complete such futures exceptionally.
463912498,9001,kowshik,2020-08-01T02:57:46Z,This exception corresponds to `Errors.FEATURE_UPDATE_FAILED`. The caller of `AdminClient#updateFeatures` can receive this exception whenever a feature update can not be written to ZK (due to a ZK issue). So this has to be a public error.
463915406,9001,kowshik,2020-08-01T03:23:44Z,"Updated the doc. Let's keep the check, if it happens then it's a user error. Especially because this can not happen if the user is using the tooling that we are going to provide in AK."
463915409,9001,kowshik,2020-08-01T03:23:50Z,Done.
463915553,9001,kowshik,2020-08-01T03:25:20Z,"No, that is not required. Please refer to the documentation above under `NOTE` for this method where I have explained why."
463916219,9001,kowshik,2020-08-01T03:34:23Z,Isn't that what I'm using currently?
463916470,9001,kowshik,2020-08-01T03:37:35Z,Like how? I don't understand. Isn't that what I'm doing currently?
463916610,9001,kowshik,2020-08-01T03:39:49Z,"If the broker has feature incompatibilities, then it should die as soon as it has received the ZK update (it would die from within `FinalizedFeatureChangeListener`)."
463916688,9001,kowshik,2020-08-01T03:40:25Z,Done now.
463934710,9001,kowshik,2020-08-01T07:31:07Z,"We should keep the existing check as it is. The reason is that if the existing node is `(disabled, {})` then here we would like to change it to `(enabled, features)`. Therefore, we have to check the features as well as the `FeatureZNodeStatus`."
463934948,9001,kowshik,2020-08-01T07:34:46Z,"I do not understand the concern.
Which code path can possibly introduce `null` features attribute in `Broker` object? It is impossible...."
463935718,9001,kowshik,2020-08-01T07:44:14Z,"A value < 1 is indicative of a deletion request (a kind of downgrade request).
It is for convenience of generating a special error message, that we handle the case here explicitly: `...less than 1 for feature...`.
"
463936048,9001,kowshik,2020-08-01T07:48:44Z,Done.
463936098,9001,kowshik,2020-08-01T07:49:28Z,Existing approach is equally readable too. I'd rather leave it this way.
463936412,9001,kowshik,2020-08-01T07:54:05Z,"Since the app depends on monotonically increasing elapsed time values, `System.nanoTime()` is preferred. 
`System.currentTimeMillis()` can change due to daylight saving time, users changing the time settings, leap seconds, and internet time sync etc."
463937032,9001,kowshik,2020-08-01T08:02:25Z,"The `FinalizedFeatureCache.getSupportedFeatures` API is not the right fit for the cache's public interface (it is quite unrelated to the other public APIs of the cache). I'd rather not pollute the public API there, just for the sake of convenience."
463937184,9001,kowshik,2020-08-01T08:04:35Z,Just 2 occurrences (one in this test and other in the next test). I'd leave it the way it is as the test is readable with values inlined in the test body.
465565925,9001,kowshik,2020-08-05T08:36:36Z,"Actually this is an error case now. Have updated the code with the fix, and with good documentation."
465570359,9001,kowshik,2020-08-05T08:44:21Z,"This does not seem to be required, since it is already achieved via `UpdateFeaturesTest`. Infact there we test using admin client, which is even better as it tests e2e client to server functionality.

What do we gain by adding the additional tests in `KafkaApisTest` ?"
465572011,9001,kowshik,2020-08-05T08:47:06Z,"I don't see that we consistently use a top level error code across other Kafka apis, so I will leave it as it is. It feels OK for this api to not use it, as it does not make a significant difference."
465572056,9001,kowshik,2020-08-05T08:47:12Z,Answered below.
467279790,9001,junrao,2020-08-07T21:29:27Z,The KIP wiki has AllowDowngrade at the topic level. Could we update that?
467282147,9001,junrao,2020-08-07T21:32:28Z,The KIP wiki doesn't include this field.
467309169,9001,junrao,2020-08-07T22:15:43Z,"When we roll the cluster to bump up IBP, it seems that it's possible for status to be enabled and then disabled repeatedly? This can be a bit weird."
467315417,9001,junrao,2020-08-07T22:40:05Z,"When we roll the cluster to bump up IBP, it seems that it's possible for the min of finalized version to flip repeatedly? This can be a bit weird.

Also, it seems that we should set min version based on the largest min version across all brokers?"
467319530,9001,junrao,2020-08-07T22:57:09Z,"Hmm, do we need to do this? If there is an incompatible feature, the broker will realize that and can just shut itself down."
467326580,9001,junrao,2020-08-07T23:29:11Z,"If update.maxVersionLevel < defaultMinVersionLevel, we throw an IllegalStateException. Should we catch it and convert it to an error code?"
467330197,9001,junrao,2020-08-07T23:48:02Z,"Since we are doing the compatibility check for every broker, do we need to special case here just for the broker feature on the controller?"
467333895,9001,junrao,2020-08-08T00:08:33Z,"If the broker discovers that it's incompatible, should it just shut itself down?"
467341811,9001,junrao,2020-08-08T01:04:11Z,Could you explain how the default min version is different from the min in supportedFeatures?
468000768,9001,junrao,2020-08-10T15:42:18Z,"The return type is different from the KIP. Which one is correct? Since this is a public interface, in general, we don't want to expose anything other than truly necessary. This PR seems to expose a lot more public methods to the user.

FinalizedVersionRange is in org.apache.kafka.common.feature. Currently, all public interfaces are specified under javadoc in build.gradle. So, we need to either include that package in javadoc or move it to a public package."
468000872,9001,junrao,2020-08-10T15:42:28Z,The return type is different from the KIP. Which one is correct?
468002060,9001,junrao,2020-08-10T15:44:20Z,The KIP also exposes host() and port(). Are they still needed?
468005420,9001,junrao,2020-08-10T15:49:30Z,"The KIP doesn't have DescribeFeaturesOptions. If we are changing the KIP, could we summarize the list of the things that are changed?"
468008294,9001,junrao,2020-08-10T15:53:53Z,"Again, this method has a different signature from the KIP."
468009874,9001,junrao,2020-08-10T15:56:13Z,The KIP doesn't have this method.
468020800,9001,junrao,2020-08-10T16:14:00Z,"handleNotControllerError() already throws an exception.

Should other errors like CLUSTER_AUTHORIZATION_FAILED be treated in the same way?"
468084269,9001,kowshik,2020-08-10T18:04:56Z,I'm missing something. Which lines on the KIP-584 were you referring to? I didn't find any mention of the flag being at the topic level.
468085443,9001,kowshik,2020-08-10T18:07:00Z,"Yes, we changed to have an error code per feature update. I'll update the KIP-584 write up."
468089357,9001,kowshik,2020-08-10T18:14:32Z,"To be sure we are on same page, is this because of a controller failover during an IBP bump?
It seems to me that this can happen mainly when IBP is being bumped from a value less than KAFKA_2_7_IV0 to a value greater than or equal to KAFKA_2_7_IV0 (assuming subsequent IBP bumps will be from KAFKA_2_7_IV0 to a higher value, so the node status will remain enabled).

In general, I'm not sure how to avoid this node status flip until IBP bump has been completed cluster-wide. 
"
468097360,9001,kowshik,2020-08-10T18:29:32Z,"> When we roll the cluster to bump up IBP, it seems that it's possible for the min of finalized version to flip repeatedly? This can be a bit weird.

True, this is possible. Good point. To be sure I understood, are you referring broadly to any future IBP bump? Or specifically are you referring to the IBP bump from a value less than KAFKA_2_7_IV0 to a value greater than or equal to KAFKA_2_7_IV0? (since KAFKA_2_7_IV0 is the IBP where the feature versioning system gets activated)

To answer your question, I'm not sure how to avoid the flip. It is to be noted that min version level changes are used only for feature version deprecation. Due to the flipping values, it merely means some version levels would go a few times from deprecated -> available -> deprecated -> available...., until the IBP bump has been completed cluster-wide. I can't (yet) think of a case where the flip is dangerous, since:
1. We have this check: https://github.com/apache/kafka/blob/89a3ba69e03acbe9635ee1039abb567bf0c6631b/core/src/main/scala/kafka/server/BrokerFeatures.scala#L47-L48  and
2. As best practice, we can recommend to not change a) minVersion of SupportedFeature as well as b) default minVersionLevel within the same release. The reason being that we typically first deprecate a feature version level before we remove the code to drop support for it i.e. (b) usually has to happen before (a)."
468101982,9001,kowshik,2020-08-10T18:38:18Z,"Good question. Yes, the broker will shut itself down. But still there is a possible race condition that needs to be handled to prevent an incompatible broker from causing damage to cluster. The race condition is described in the KIP-584 [in this section](https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features#KIP584:Versioningschemeforfeatures-Incompatiblebrokerlifetimeracecondition). Please let me know your thoughts."
468103224,9001,kowshik,2020-08-10T18:40:39Z,"Yes, excellent point. I'll fix this."
468109791,9001,kowshik,2020-08-10T18:52:51Z,"It's required because `defaultMinVersionLevel` does not exist for a feature that's not in the supported list. However, I'll change the code to make the check more obvious to the reader (currently it's not)."
468111300,9001,kowshik,2020-08-10T18:55:30Z,"Good question. The existing behavior is that it shuts itself down, as triggered by this LOC. The reason to do it is that an incompatible broker can potentially do harmful things to a cluster (because max version level upgrades are used for breaking changes): https://github.com/apache/kafka/blob/89a3ba69e03acbe9635ee1039abb567bf0c6631b/core/src/main/scala/kafka/server/FinalizedFeatureChangeListener.scala#L154-L156. "
468111785,9001,kowshik,2020-08-10T18:56:28Z,"Sure, I'll update the PR documenting it."
471627837,9001,junrao,2020-08-17T17:13:02Z,"OK. There are a couple of places that this PR is inconsistent with the KIP.

1. The KIP has 2 levels of arrays: []FeatureUpdateKey and []FeatureKey. This PR only has one array.
2. The KIP has a timeoutMs field and this PR doesn't."
471806378,9001,junrao,2020-08-17T22:20:32Z,"My understanding of the race condition is that the controller finalizes a feature while there is a pending broker registration in the controller event queue. When the controller starts to process the new broker registration, it will realize that its supported feature is not compatible. Here, it's seems that we will still process this new broker registration and only avoid sending UpdatateMetadataRequest to it. I am not sure if this helps since we already acted on this incompatible broker registration and some damage may already be done. The same UpdatateMetadataRequest will still be sent to other brokers and its metadata will be available to the clients.

An alternative way is to just skip the handling of new broker registration if it's detected as incompatible."
494170995,9001,kowshik,2020-09-24T09:28:25Z,Done. I have added a top-level error code now.
494171030,9001,kowshik,2020-09-24T09:28:29Z,Done.
494171814,9001,kowshik,2020-09-24T09:29:49Z,"Done. Fixed the KIP and the code, so that they align with each other now."
494173396,9001,kowshik,2020-09-24T09:32:21Z,"Done. I've updated the KIP-584 write up, please refer to [this section](https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features#KIP584:Versioningschemeforfeatures-UpdateFeaturesResponseschema) in the KIP."
494174446,9001,kowshik,2020-09-24T09:34:09Z,Done. I've fixed this now to align with the KIP.
494179911,9001,kowshik,2020-09-24T09:42:55Z,Done. I've updated the KIP to use `Optional<Integer>` as well.
494180021,9001,kowshik,2020-09-24T09:43:06Z,Done. I've removed those methods from the KIP.
494180167,9001,kowshik,2020-09-24T09:43:22Z,Done. I've updated the KIP to mention `DescribeFeaturesOptions`.
494180305,9001,kowshik,2020-09-24T09:43:36Z,"Done. I've updated the KIP to align with whats used here, so both are the same now."
494180601,9001,kowshik,2020-09-24T09:44:03Z,Done. The KIP has been updated to have this method now.
494183456,9001,kowshik,2020-09-24T09:48:37Z,"> handleNotControllerError() already throws an exception.

Done. Fixed the code to not throw exception again when handling NOT_CONTROLLER error.

> Should other errors like CLUSTER_AUTHORIZATION_FAILED be treated in the same way?

I'm not sure how could we treat it the same way. In the case of the NOT_CONTROLLER error, the admin client code would retry the request once again when the exception is raised. But when cluster authorization fails, would a retry help?"
494519631,9001,kowshik,2020-09-24T18:20:37Z,Done. I've changed the code such that we skip the broker registration if it's detected as incompatible.
494542156,9001,kowshik,2020-09-24T18:54:14Z,Done. This is fixed now.
494569726,9001,kowshik,2020-09-24T19:46:20Z,Done.
494653251,9001,kowshik,2020-09-24T22:52:14Z,Done.
496093216,9001,junrao,2020-09-28T16:48:55Z,"Space before ""name""."
496099482,9001,junrao,2020-09-28T16:59:02Z,This is not included in the KIP. Should we update the KIP?
496104584,9001,junrao,2020-09-28T17:05:46Z,"Since this is public facing, could we include the description in the KIP?"
496118993,9001,junrao,2020-09-28T17:31:27Z,"Since the user is not expected to instantiate this, should we make the constructor non-public?"
496120406,9001,junrao,2020-09-28T17:34:03Z,"Since the user is not expected to instantiate this, should we make the constructor non-public?"
496122127,9001,junrao,2020-09-28T17:37:09Z,"This seems identical to SupportedVersionRange. Should we just have one, sth like VersionRange?"
496124713,9001,junrao,2020-09-28T17:41:52Z,Are we adding the timeout option based on the KIP discussion?
496131186,9001,junrao,2020-09-28T17:52:57Z,"""at a those "" typo?"
496211011,9001,junrao,2020-09-28T20:24:46Z,"I guess after the first step, deprecated finalized versions are no longer advertised to the client, but they can still be used by existing connections?"
496213232,9001,junrao,2020-09-28T20:29:04Z,Perhaps isFeatureVersioningSupported is a better name?
496227488,9001,junrao,2020-09-28T20:56:44Z,Is it useful to expose firstActiveVersion to the client?
496253083,9001,junrao,2020-09-28T21:47:00Z,"Could you define the default finalized features? Also, default minimum version seems outdated now."
496268711,9001,junrao,2020-09-28T22:22:21Z,Perhaps it's better for the following code to use match instead if/else.
496271143,9001,junrao,2020-09-28T22:29:17Z,setupFeatureVersioning => mayBeSetupFeatureVersioning ?
496305291,9001,junrao,2020-09-29T00:20:56Z,map() is supposed to be used with no side effect. Perhaps we could use match here.
496306005,9001,junrao,2020-09-29T00:23:48Z,"Do we need to return the stacktrace to the caller? Since this is unexpected, perhaps we can log a warn?"
496306586,9001,junrao,2020-09-29T00:26:06Z,indentation
496309833,9001,junrao,2020-09-29T00:38:35Z,featureCache => finalizedFeatureCache ?
496311550,9001,junrao,2020-09-29T00:45:23Z,"I think the convention is that if there is a top level error, the second level will just be empty since there is not need to process them individually."
496312688,9001,junrao,2020-09-29T00:50:09Z,-1 =>  -1L?
496315493,9001,junrao,2020-09-29T01:00:56Z,This package is not part of the javadoc and thus is not part of the public interface.
496317328,9001,junrao,2020-09-29T01:08:23Z,"Since we are including the timeout in the UpdateFeature request, perhaps we could just use that timeout here."
496318368,9001,junrao,2020-09-29T01:12:25Z,featureCache => finalizedFeatureCache?
496319189,9001,junrao,2020-09-29T01:15:24Z,"Should we just verify the range [first_active_version, max]?"
496321312,9001,abbccdda,2020-09-29T01:23:04Z,"Yea, you are right, I think this comment belongs to updateFeatures"
496506413,9001,kowshik,2020-09-29T08:09:57Z,Done.
496509097,9001,kowshik,2020-09-29T08:12:24Z,Done. Updated the KIP. Please refer to [this](https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features#KIP584:Versioningschemeforfeatures-UpdateFeaturesRequestschema) section.
496509282,9001,kowshik,2020-09-29T08:12:34Z,"Done. Updated the KIP. Please refer to [this](https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features#KIP584:Versioningschemeforfeatures-AdminAPIchanges) section.

"
496511604,9001,kowshik,2020-09-29T08:14:42Z,"It is instantiated from `kafka.server.UpdateFeaturesTest`, so have to keep the c'tor public."
496511864,9001,kowshik,2020-09-29T08:14:55Z,"It is instantiated from `kafka.server.UpdateFeaturesTest`, so have to keep the c'tor public."
496523315,9001,kowshik,2020-09-29T08:25:21Z,"I considered this, however if we plan to expose `firstActiveVersion` to the client, then, it is better to have 2 separate classes like we do now. This is because `firstActiveVersion` will become an attribute only in `SupportedVersionRange` class. "
496523894,9001,kowshik,2020-09-29T08:25:52Z,"Yes, it is already added. The base class: `AbstractOptions` contains a `timeoutMs` attribute and the value is set in the `UpdateFeaturesRequest`."
496524072,9001,kowshik,2020-09-29T08:26:03Z,Done.
496524793,9001,kowshik,2020-09-29T08:26:44Z,"Yes, correct. I have updated the doc mentioning the same."
496524910,9001,kowshik,2020-09-29T08:26:50Z,Done.
496525949,9001,kowshik,2020-09-29T08:27:48Z,"This is a really good point. Yes, I feel it is useful to expose it to the client via `ApiVersionsResponse`. I can change the KIP suitably and then update the PR."
496526254,9001,kowshik,2020-09-29T08:28:04Z,"Done. I reworded a bit and I'm now no longer using ""default finalized features"" and ""default minimum version"" in the wordings."
496527539,9001,kowshik,2020-09-29T08:29:14Z,Done.
496528169,9001,kowshik,2020-09-29T08:29:47Z,Done.
496528445,9001,kowshik,2020-09-29T08:30:00Z,Done.
496530810,9001,kowshik,2020-09-29T08:32:23Z,Done. Good point. I'm now logging just a warning and I've removed the stacktrace from the return value.
496531037,9001,kowshik,2020-09-29T08:32:31Z,Done.
496534914,9001,kowshik,2020-09-29T08:36:03Z,Done.
496535534,9001,kowshik,2020-09-29T08:36:37Z,Done. Great point.
496535646,9001,kowshik,2020-09-29T08:36:42Z,Done.
496538616,9001,kowshik,2020-09-29T08:39:26Z,Done. I have now moved it to the package: `org.apache.kafka.clients.admin`.
496543685,9001,kowshik,2020-09-29T08:44:03Z,"I agree. But note that in this method, we do not process an `UpdateFeaturesRequest`. This method is only called during controller election to setup feature versioning. So, I have incorporated your suggestion at the point where we process the request, look for `def processFeatureUpdatesWithActiveController` in this file where now I set the ZK write timeout to be `min(timeoutMs, config.zkConnectionTimeoutMs)`."
496544239,9001,kowshik,2020-09-29T08:44:33Z,Done.
496550557,9001,kowshik,2020-09-29T08:54:04Z,"We need to keep the existing validation. Here is a case where `minVersionLevel < firstActiveVersion` is true, but still there are no incompatibilities:
```
SupportedVersionRange={minVersion=1, firstActiveVersion=4, maxVersion=7}
FinalizedVersionRange={minVersionLevel=2, maxVersionLevel=6}
```

For example, the above can happen during step 1 of feature verison level deprecation. Imagine the following:
 * A supported feature exists with `SupportedVersionRange={minVersion=1, firstActiveVersion=4, maxVersion=7}`
 * The above feature is finalized at `{minVersionLevel=2, maxVersionLevel=6}` in ZK already.

Then imagine a new Kafka release is deployed that raises `firstActiveVersion` for the supported feature from 1 -> 4 (in order to deprecate versions: 1,2,3). In such a case, during Kafka server startup (where we check for feature incompatibilities), we would run into the comparison cited above between the new `SupportedVersionRange` and existing `FinalizedVersionRange`. But it is not considered to be a case of incompatibility."
496901588,9001,abbccdda,2020-09-29T17:06:45Z,"Do we want to have a different name from `org.apache.kafka.common.feature.FinalizedVersionRange`, such as `FinalizedVersionLevels`? Same case for `SupportedVersionRange`, personally I feel the same class name makes the navigation harder."
496916316,9001,abbccdda,2020-09-29T17:30:34Z,"I think we could just make the `firstActiveVersion = minVersion` by default, to avoid the requirement for configuring firstActiveVersion"
496917029,9001,abbccdda,2020-09-29T17:31:41Z,Similar here to make `firstActiveVersion = minVersion` as default.
496917907,9001,abbccdda,2020-09-29T17:33:09Z,"So we are saving the ZK epoch in a long, which was supposed to be an int field?"
497255894,9001,kowshik,2020-09-30T05:46:08Z,"Yes, but can I do it in a follow-up PR? The reason is if I were to refactor it now, this PR will bloat up."
497256162,9001,kowshik,2020-09-30T05:47:01Z,Done. I've provided an overloaded c'tor now in `org.apache.kafka.common.feature.SupportedVersionRange` that only takes `minVersion` and `maxVersion` as parameters.
497256475,9001,kowshik,2020-09-30T05:48:03Z,"As mentioned in above response to a different comment, I've provided an overloaded c'tor now in `org.apache.kafka.common.feature.SupportedVersionRange` that only takes `minVersion` and `maxVersion` as parameters."
497257420,9001,kowshik,2020-09-30T05:51:12Z,"We would like to avoid overflow issues once ZK is gone in the future. This change is being done based on Colin's suggestion in the KIP-584 voting thread:
 - [Here](https://lists.apache.org/thread.html/r0dddbe01e2d6991310006b90aa5c9db011461f516f345f621c1f8171%40%3Cdev.kafka.apache.org%3E) is Colin's comment
 - [Here](https://lists.apache.org/thread.html/rf7fb6a033638c43a338be5cc316e9e69df6e2589fab66b69d8b67f0f%40%3Cdev.kafka.apache.org%3E) is my response"
497399425,9001,kowshik,2020-09-30T10:17:16Z,Done. The `firstActiveVersion` is now part of `ApiVersionsResponse`. I added it in the recent commit: a7f4860f5f8bb87cfb01452e208ff8f4e45bcd8b.
497793784,9001,junrao,2020-09-30T20:53:28Z,"Hmm, why do we need to take the min? If the ZK data is propagated quickly, waitUntilEpochOrThrow() will just return early."
497813798,9001,junrao,2020-09-30T21:34:06Z,"I was looking at existing classes fro the return value. For example, CreateAclsResult deliberately makes the constructor non-public."
497848498,9001,junrao,2020-09-30T23:05:04Z,It's useful to return an error message too.
497850391,9001,junrao,2020-09-30T23:11:02Z,Could we use Collections.emptyMap()?
497856858,9001,junrao,2020-09-30T23:32:11Z,"This can throw an exception due to feature mismatch. Currently, this forces the controller to move but keeps the broker alive. Should we force the broker to exit in this case?"
498072158,9001,kowshik,2020-10-01T08:29:43Z,"Done. Good point, removed the min now."
498092489,9001,kowshik,2020-10-01T09:03:09Z,Done. Good catch. Also I've modified `org.apache.kafka.clients.admin.{Supported|Finalized}VersionRange` classes to make constructors non-public.
498093400,9001,kowshik,2020-10-01T09:04:48Z,Would the default error message suffice?: `Unable to update finalized features due to an unexpected server error.`
498094043,9001,kowshik,2020-10-01T09:05:53Z,Done.
498113308,9001,kowshik,2020-10-01T09:38:02Z,"Done. Good point. It looks appropriate to me that we exit the broker in this case. I've captured the exception and added a call to `Exit.exit(1)`, is there a better way to do it?"
498420758,9001,junrao,2020-10-01T17:55:31Z,"Thinking about this a bit more. It seems that the intention of firstActiveVersion is to avoid deploying a wrong version of the broker that causes the deprecation of a finalized feature version unexpectedly. However, the same mistake can happen with firstActiveVersion since the deprecation of a finalized feature version is based on firstActiveVersion. So, I am not sure if firstActiveVersion addresses a real problem.

In general, we tend to deprecate a version very slowly in AK. So, if the mistake is to deploy a new release that actually deprecates a supported version. Old clients are likely all gone. So, moving finalized min version to supported min version may not cause a big problem. We can just document that people should make sure old versions are no longer used before deploying new releases.

If the mistake is to deploy an old version of the broker whose maxSupportedVersion is < maxFinalizedVersion, we will fail the broker. So, this mistake can be prevented."
498495464,9001,kowshik,2020-10-01T20:27:42Z,"@junrao :

I'd like to discuss an example that cites a problem I'm concerned about.

> In general, we tend to deprecate a version very slowly in AK. So, if the mistake is to deploy a new release that actually deprecates a supported version. Old clients are likely all gone. So, moving finalized min version to supported min version may not cause a big problem. We can just document that people should make sure old versions are no longer used before deploying new releases.

Let's say we have some feature `F` whose:
 * Supported version range is: `[minVersion=1, maxVersion=6]`
 * Existing finalized version range in the cluster is: `[minVersionLevel=1, maxVersionLevel=6]`

Now, let us say a point in time arrives when we need to deprecate the feature version `1`.
Let us say we bump up supported `minVersion` to `2` in a subsequent major Kafka release.
Before this new release is deployed, let us assume the cluster operator knows 100% that old clients that were using the feature at version `1` are gone, so this is not a problem.

**PROBLEM:** Still, if we deploy this new release, the broker will consider the following as a feature version incompatibility.
 * Supported version range is: `[minVersion=2, maxVersion=6]`
 * Existing finalized version range in the cluster is: `[minVersionLevel=1, maxVersionLevel=6]`

Upon startup of a broker thats using the new release binary, the above combination will crash the broker since supported `minVersion=2` is greater than `minVersionLevel=1`. Basically the versioning system thinks that there is now a broker that does not support `minVersionLevel=1`, which does not adhere to the rules of the system. We currently do feature version incompatibility checks during KafkaServer startup sequence, [here is the code](https://github.com/confluentinc/ce-kafka/blob/master/core/src/main/scala/kafka/server/KafkaServer.scala#L398).

Here is my thought: This is where `firstActiveVersion` becomes useful. By bumping it up during a release (instead of the supported feature's `minVersion`), we are able to get past this situation. When `firstActiveVersion`is advanced in the code, and the cluster is deployed, the controller (and all brokers) know that the advancement acts a request to the controller to act upon the feature deprecation (by writing the advanced value to the `FeatureZNode`). So, in this case we would release the broker with the supported feature version range: `[minVersion=1, firstActiveVersion=2, maxVersion=6]`, and the broker release wouldn't fail (because the intent is clearly expressed to the versioning system).

What are your thoughts on the above?
Is there a different way to solve it better that I'm missing, without compromising the versioning checks enforced by the system?"
498512579,9001,junrao,2020-10-01T21:04:51Z,"@kowshik : I was thinking what if we relax the current check by just making sure that maxVersion of finalized is within the supported range. Basically in your example, if supported minVersion goes to 2, it's still allowed since it's less than maxVersion of finalized. However, if supported minVersion goes to 7, this fails the broker since it's more than maxVersion of finalized.

Your concern for the relaxed check seems to be around deploying a wrong version of the broker by mistake. I am not sure if that's a big concern. If the wrong broker affects maxVersion of finalized, the broker won't start. If the wrong broker affects minVersion of finalized, if we deprecated slowly, it won't impact the existing clients."
498574911,9001,kowshik,2020-10-02T00:38:05Z,"@junrao Does the below feel right to you?

The key thing seems to be that you feel it is rare to deprecate feature versions in AK. I agree with the same. So, I propose we just do not have to solve the deprecation problem in this PR, until we find a clear route that the AK community agrees with. In this PR I propose to revert the `firstActiveVersion` change, leaving the rest of the things the way they are. In the future, we can develop a concrete solution for version deprecation i.e. the part on how to advance `minVersion` of supported feature, may be (or may not be) using `firstActiveVersion` or other ways (it is up for discussion, maybe in a separate KIP). I have made this proposed change in the most recent commit: 4218f95904989028a469930d0c266362bf173ece.

Regarding your thought:
> I was thinking what if we relax the current check by just making sure that maxVersion of finalized is within the supported range. Basically in your example, if supported minVersion goes to 2, it's still allowed since it's less than maxVersion of finalized. However, if supported minVersion goes to 7, this fails the broker since it's more than maxVersion of finalized.

There is a consequence to relaxing the current check:
The controller can not effectively finalize `minVersionLevel` for the feature, because, with a relaxed check we do not know whether all brokers in the cluster support a particular `minVersion` when the controller finalizes the `minVersionLevel` at a particular value. It seems useful to keep the concept of `minVersionLevel` like the way it is now (i.e. it is the lowest version guaranteed to be supported by any broker in the cluster for a feature). And as I said above, in the future, we can decide on ways to mutate it safely (maybe through `firstActiveVersion` or other means).

"
498959506,9001,junrao,2020-10-02T17:39:15Z,"This this case, existingFeatureZNode.features is expected to be empty? Could we log a warn if this is not the case and always set finalized to empty?"
498960633,9001,junrao,2020-10-02T17:41:40Z,Should we call updateFeatureZNode() so that we can get the logging?
498980025,9001,junrao,2020-10-02T18:19:31Z,"This test may not be enough. The issue is that when a controller fails over, it's possible that new brokers have joined the cluster during the failover. So, if existingFeatureZNode is enabled, it may not be reflecting the state in those newly joined brokers. So, it seems that we need to do the validation for every broker during controller failover in that case."
499034372,9001,junrao,2020-10-02T20:25:16Z,"""we do not know whether all brokers in the cluster support a particular minVersion when the controller finalizes the minVersionLevel at a particular value."" The controller knows the minSupportedVersion for all brokers, right? What if we do the following? When finalizing a feature, the controllers uses the highest minSupportedVersion across all brokers as finalizedMinVersion, as long as it's <= finalizedMaxVersion. On broker restart, we also advance finalizedMinVersion if the new broker's minSupportedVersion has advanced (assuming still <= finalizedMaxVersion). "
499036138,9001,junrao,2020-10-02T20:29:54Z,Could we make the constructor non-public?
499036605,9001,junrao,2020-10-02T20:31:03Z,Could we make the constructor non-public?
499102118,9001,kowshik,2020-10-03T01:14:01Z,Done.
499102130,9001,kowshik,2020-10-03T01:14:05Z,Done.
499102146,9001,kowshik,2020-10-03T01:14:17Z,Done.
499102163,9001,kowshik,2020-10-03T01:14:24Z,Done.
499102176,9001,kowshik,2020-10-03T01:14:36Z,Done. Excellent catch.
499102266,9001,kowshik,2020-10-03T01:15:32Z,"@junrao Awesome. This is a very good point. The approach you proposed is very elegant, and we should shoot for it, when were giving the benefit of the doubt on deprecation to the broker binary version. Ill update the KIP with details and share with community for feedback. As soon as that is done, I'll follow up in separate PR implementing this logic."
499739889,9001,junrao,2020-10-05T16:54:25Z,It's a bit weird that FeatureZNode.status is defined as FeatureZNodeStatus.Value. It seems that it should be defined as just FeatureZNodeStatus?
499740489,9001,junrao,2020-10-05T16:55:28Z,Should we log the non-empty features too?
499753420,9001,junrao,2020-10-05T17:18:52Z,Should we revert the changes here?
499761963,9001,junrao,2020-10-05T17:34:20Z,"This is probably not enough since it only waits for the controller path to be created in ZK, which happens before the processing of the finalized features."
499775367,9001,junrao,2020-10-05T17:59:22Z,Could we add feature to the javadoc above?
499776675,9001,junrao,2020-10-05T18:01:47Z,Should we use a version > 0?
499776894,9001,junrao,2020-10-05T18:02:14Z,typo thats
499810462,9001,kowshik,2020-10-05T19:05:24Z,"Done. I have improved it now introducing a type definition called `FeatureZNodeStatus` that points to `Value`.
IIUC you were referring to this LOC, correct?  https://github.com/apache/kafka/blob/4f96c5b424956355339dd3216c426c1c0388fe9e/core/src/main/scala/kafka/zk/ZkData.scala#L851
Here the enum: `FeatureZNodeStatus` is defined and used in the same file.  I thought I'd add an `import` to fix it like the below, but it was a little unusual to add an `import` statement right above the class definition:

```
import FeatureZNodeStatus._
case class FeatureZNode(status: FeatureZNodeStatus, features: Features[FinalizedVersionRange]) {
}
```

With my recent change, in the future it should be possible to `import FeatureZNodeStatus._` within other files when referring to the enum value."
499811265,9001,kowshik,2020-10-05T19:06:45Z,Done.
499811752,9001,kowshik,2020-10-05T19:07:42Z,Done. Nice catch!
499812076,9001,kowshik,2020-10-05T19:08:21Z,Done.
499816373,9001,kowshik,2020-10-05T19:16:34Z,Done. Good point.
499816619,9001,kowshik,2020-10-05T19:17:03Z,Done.
499847021,9001,kowshik,2020-10-05T20:16:35Z,Done. Please take a look at the fix. I've added logic to wait for processing on a dummy event just after waiting for controller election. I'm hoping this will make sure the controller failover logic is completed before the test proceeds further to make assertions.
501418496,9001,chia7712,2020-10-08T02:53:08Z,"the error message says it can't be null but there is no null check.
for another, this check can happen early (when creating ```updateFutures```)"
501432770,9001,chia7712,2020-10-08T03:51:37Z,"Should we add an empty-parameter variety for ```describeFeatures```? that is similar to other methods, like ```DescribeUserScramCredentialsResult``` and ```describeDelegationToken```. 

"
501489060,9001,chia7712,2020-10-08T06:58:43Z,the top-level error message is not propagated.
501575432,9001,kowshik,2020-10-08T09:26:06Z,Done. Addressed in #9393.
501575489,9001,kowshik,2020-10-08T09:26:11Z,Done. Addressed in #9393.
501575519,9001,kowshik,2020-10-08T09:26:13Z,Done. Addressed in #9393.
102111062,2476,becketqin,2017-02-21T00:13:51Z,The variable names seem a little misleading. Are all partitions without leader information unauthorized?
102111239,2476,becketqin,2017-02-21T00:16:05Z,Should we use the exception returned by the broker in this case?
102113712,2476,becketqin,2017-02-21T00:48:53Z,"When `client.poll(future, remaining)` returns true, the future may either contains a value (succeeded) or an error (failed). If the future has an error, calling `future.value()` will throw exception. It seems better if we can return the full results to the users even if some of the requests failed so the users will be able to know which partitions has failed to purge."
102114179,2476,becketqin,2017-02-21T00:55:27Z,"It seems a single `consumerNetworkClient.poll(0)` cannot guarantee all the requests are sent out. Also, the interface might be a little weird that after `purgeDataBefore()` is returned the users have to keep calling future.client.poll() otherwise the futures will not be completed. I am wondering how would user use the asynchronous purge in this case? At very least we should document this clearly."
102114578,2476,becketqin,2017-02-21T01:00:08Z,This could just be a string concatenation.
102116849,2476,becketqin,2017-02-21T01:29:45Z,This seems a little over optimizing. Any reason we care about invoking time.milliseconds here more than in line 441 and everywhere else?
102117137,2476,becketqin,2017-02-21T01:33:37Z,Should we log the partition information and which replica is unavailable here?
102117519,2476,becketqin,2017-02-21T01:38:43Z,Should this be in write lock?
102118327,2476,becketqin,2017-02-21T01:47:26Z,Is this comment accurate?
102118888,2476,becketqin,2017-02-21T01:54:33Z,Nit: could be `mapValues`.
102119889,2476,becketqin,2017-02-21T02:06:32Z,"Methods with three tuples as return value may be a little hard to follow, may be we can create a case class. At very least we should document each field of the return value."
102121248,2476,becketqin,2017-02-21T02:22:07Z,Do we want to distinguish between NO_LOG_START_OFFSET v.s. LOG_START_OFFSET = 0? Is it clearer to define the NO_LOG_START_OFFSET as -1?
102146034,2476,lindong28,2017-02-21T07:37:14Z,Agree. I have updated the names.
102146077,2476,lindong28,2017-02-21T07:37:42Z,Good point. I have updated code to use error from MetadataRequest.
102146450,2476,lindong28,2017-02-21T07:40:56Z,"Great point. I have updated the AdminClient to create its own thread to do `client.poll(retryBackoffMs)`. I find it necessary for AdminClient to have its own thread in order to support both syn and async operation.

I have also added `testLogStartOffsetAfterAsyncPurge()` to validate the asyn purge operation."
102147486,2476,lindong28,2017-02-21T07:49:25Z,"This should not be a problem for `purgeDataBefore()`, because those `futures` provided to the `CompositeFuture` has been constructed in such a way that they never raises exception. Those future will call `future.complete(result)` in case of `onFailure`, where result indicates has the error information.

I agree it would be make  `CompositeFuture` more useful if this class handles the logic of converting error to result and return the full results to user as you suggested. But I don't have a good way to do it now because `CompositeFuture` doesn't know the type of the return value -- it currently use template `T`."
102147741,2476,lindong28,2017-02-21T07:51:19Z,Is there any negative impact to use StringBuilder as compared to string concatenation? Using StringBuilder here allows us to have the same code style as `toString()` of other requests such as `ProduceRequest` and `LeaderAndIsrRequest`.
102147786,2476,lindong28,2017-02-21T07:51:46Z,Good point. I have updated the log message as you suggested.
102147860,2476,lindong28,2017-02-21T07:52:34Z,I think it is not bad to remove one unnecessary call to `time.milliseconds`. I removed it since you don't like it.
102148192,2476,lindong28,2017-02-21T07:55:22Z,I validated that we can not use `mapValues()` here. This is because `mapValues()` returns a map view which maps every key of this map to `f(this(key))`. The resulting map wraps the original map without copying any elements. As a result `status.acksPending = true` in the constructor of `DelayedPurge` becomes no-op.
102148242,2476,lindong28,2017-02-21T07:55:47Z,Good point. I have updated the code to make it more readable.
102148307,2476,lindong28,2017-02-21T07:56:18Z,"I think we should use readlock since this method doesn't update leader or isr of the partition, right?"
102148443,2476,lindong28,2017-02-21T07:57:24Z,It was not accurate. I have updated the comment to replace `all replicas of this broker` to `in-sync replicas of this broker`
102148589,2476,lindong28,2017-02-21T07:58:36Z,I don't think it is necessary to distinguish between NO_LOG_START_OFFSET v.s. LOG_START_OFFSET = 0. Is there any use-case for NO_LOG_START_OFFSET?
102260819,2476,becketqin,2017-02-21T17:01:27Z,"Not much difference, just for readability (See below) we can keep them the same as other requests.
http://stackoverflow.com/questions/1532461/stringbuilder-vs-string-concatenation-in-tostring-in-java"
102263429,2476,becketqin,2017-02-21T17:12:12Z,"Yes, you are right. Read lock here is fine."
102265149,2476,becketqin,2017-02-21T17:19:34Z,"In general, we want to identify the state of the system as clear as possible. The follower should not take any action if the LOG_START_OFFSET on the broker is NO_LOG_START_OFFSET. But if the follower sees the leader returning the starting offset = 0 while the actual starting offset on the leader is not, this introduces confusion."
102267118,2476,becketqin,2017-02-21T17:27:24Z,The KIP stated that the consumer will put value -1L as the log_begin_offset.
102267688,2476,becketqin,2017-02-21T17:29:46Z,"Nit: In consumer we use the term ""earliest"" instead of ""smallest"", maybe we can make the term consistent."
102269297,2476,becketqin,2017-02-21T17:37:30Z,Maybe we should always use the latest version here?
102269853,2476,becketqin,2017-02-21T17:39:51Z,Latest version?
102289505,2476,lindong28,2017-02-21T19:04:23Z,Agree. I have updated the code to test all versions of FetchResponse.
102289533,2476,lindong28,2017-02-21T19:04:33Z,Sure. Updated to use latest version here.
102289562,2476,lindong28,2017-02-21T19:04:42Z,Sure. Made the change.
102289627,2476,lindong28,2017-02-21T19:04:58Z,Nice catch. Updated to use -1L here.
103133485,2476,becketqin,2017-02-27T03:54:41Z,"The comment is a little confusing here. How about ""The offset before which the messages will be deleted."""
103134243,2476,becketqin,2017-02-27T04:09:47Z,Sleeping here seems not ideal and may miss subsequent action invoked by the users. We probably want to have a long poll() and just wake up the networkThread when needed.
103134387,2476,becketqin,2017-02-27T04:12:42Z,"Since we have separate thread now, retry could be done without involving users, right?"
103136031,2476,becketqin,2017-02-27T04:45:50Z,Nit: Can we just call it timeBeforeLocalPurge?
103136983,2476,becketqin,2017-02-27T05:02:36Z,"If we want to make this optimization, we should do it in line 330 as well."
103138261,2476,becketqin,2017-02-27T05:26:55Z,The earliest offset allowed...
103138990,2476,becketqin,2017-02-27T05:38:11Z,"The statement here is a little misleading. logStartOffset is not used to ""decide"" the log retention. The log retention is still based on time/size limit. The logStartOffset will be updated by retention. A manual update of logStartOffset may trigger the log deletion."
103139730,2476,becketqin,2017-02-27T05:49:15Z,"Hmm, why would truncation change the startLogOffset? Shouldn't truncation always be above the startLogOffset?"
103140001,2476,becketqin,2017-02-27T05:53:11Z,"The update of logStartOffset should probably be synchronized and ensure no rewind will happen, otherwise a fetch request may see an incorrect logStartOffset."
103140524,2476,becketqin,2017-02-27T06:00:58Z,"It seems we should be careful about when to delete the log segments. In the current code, when the leader deleted an old log segment. The low watermark will not increase immediately because the followers has not updated the logStartOffset yet. However, at this point if user tries to fetch from LW, they will receive an offset out of range exception, which is not expected. 

One solution would be letting the segment deletion to be based on the low watermark instead of logStartOffset. So we need to delay the log deletion until low watermark is updated. Similarly the valid range of fetch request should be based on LW instead of logStartOffset.

Nit: There is an existing typo in line 729, which missed a ""t"" in ""deleteRetentionMsBreachedSegments()""."
103140662,2476,becketqin,2017-02-27T06:02:53Z,"Should it be an error to truncate beyond the logStartOffset? LogStartOffset should not decrease, right?"
103140993,2476,becketqin,2017-02-27T06:07:48Z,"It seems that we should also checkpoint low watermarks, right?"
103272179,2476,lindong28,2017-02-27T18:07:47Z,"When there is no pending requests, `ConsumerNetworkClient.poll(...)` blocks for up to `retryBackoffMs`. It means the thread does poll() in kind of busy looping manner if we don't sleep here. Also, the thread holds the lock on `ConsumerNetworkClient` while blocked on `selector.poll()` which means the user will be blocked waiting for lock in order to enqueue requests."
103273306,2476,lindong28,2017-02-27T18:13:28Z,"Yes we can, if we ask user to specify retries num and retry back off. Current implementation assumes retries = 1. This seems like an optimization which may be added later?"
103273330,2476,lindong28,2017-02-27T18:13:35Z,Sure. Fixed now.
103273660,2476,lindong28,2017-02-27T18:15:11Z,"Do you mean `debug(""Produce to local log in %d ms"".format(time.milliseconds - sTime))`? I thought this `time.milliseconds` will only be evaluated if debug level logging is enabled, no?"
103274743,2476,lindong28,2017-02-27T18:20:09Z,Sure. Fixed now.
103275187,2476,lindong28,2017-02-27T18:22:19Z,Fixed.
103276622,2476,lindong28,2017-02-27T18:28:41Z,"I include this scenario because I couldn't provide that log truncation will always be above the logStartOffset. For example, we have had bug such that log may be truncated below high watermark in case of double failure."
103276835,2476,lindong28,2017-02-27T18:29:37Z,Sure. I changed it to `log deletion`.
103288992,2476,lindong28,2017-02-27T19:23:42Z,Good point. I just added `lock` around this.
103293344,2476,lindong28,2017-02-27T19:42:11Z,"Sure, the typo is fixed.

In the current implementation, if user seeks to earliest offset, he/she will seek to the `logStartOffset` of the leader regardless of what is the low watermark. User will not seek to LW since LW is not even exposed to the user. Is there any concern with this approach?

"
103295145,2476,becketqin,2017-02-27T19:49:35Z,"If poll() itself is not a busy looping internally, we will not have a busy loop (even though it look like so). However, in the current case, if retry back off is set to 0, we do have a busy poll(). Holding a lock and blocking on selector.poll() is strange, maybe we should reconsider that. Even if that is true, we can still have a queue outside of the ConsumerNetworkClient and wake up the sender thread to poll from that queue."
103295641,2476,lindong28,2017-02-27T19:51:32Z,If everything is implemented correctly then log start offset should not decrease. But in reality we have seen unlean leader election during double failure where the logEndOffset is truncated beyond high watermark. It seems safer to preserve the existing behavior of `TruncateTo()` and allow logStartOffset to be reduced if such truncation does happen instead of throwing exception. Note that such truncation will only happen as part of unclean leader election initiated by the brokers themselves.
103295886,2476,lindong28,2017-02-27T19:52:40Z,"No, we intentionally only checkpoint logStartOffset. Is there any reason to checkpoint lowWatermark?"
103307226,2476,becketqin,2017-02-27T20:43:41Z,It will call `time.milliseconds` because it is an argument passed to the `debug()`.
103307455,2476,becketqin,2017-02-27T20:44:49Z,"If it is not an expected behavior, we should throw some exception instead of allowing the LW to decrease, right?"
103310427,2476,ijuma,2017-02-27T20:58:41Z,"I'm a bit confused about the discussion here, so I'll just explain how it works. :) Whatever one passes as a parameter to `debug` is only evaluated if debug logging is enabled. At runtime, a `Function0` is passed and the body of that function is the block of code passed as the parameter to `debug`. Does that make sense?"
103362427,2476,becketqin,2017-02-28T02:16:09Z,"Thanks for the clarification @ijuma. You are right, I realized that after taking a closer look."
103580604,2476,lindong28,2017-02-28T23:49:38Z,"Discussed offline. Given that existing APIs such as `AdminClient.listGroupOffsets(..)` and `KafkaConsumer.retrieveOffsetsByTimes(..)` would not retry and just expose the exception to user, it should be OK for `AdminClient.purgeDataBefore(...)` to do the same thing. If we think it is useful to retry for user, we probably want to discuss the new configs and do it in Java AdminClient as part of KIP-117."
103580824,2476,lindong28,2017-02-28T23:51:17Z,"Discussed offline. Given that `Log.truncateTo(...)` has explicitly consider the case that the targetOffset is smaller than base offset of the first log segment, it would be safer and simpler for us to assume that `Log.truncateTo(...)` can truncate to an offset smaller than logStartOffset."
103580946,2476,lindong28,2017-02-28T23:52:03Z,"Discussed offline. Given that Log.truncateTo(...) has explicitly consider the case that the targetOffset is smaller than base offset of the first log segment, it would be safer and simpler for us to assume that Log.truncateTo(...) can truncate to an offset smaller than logStartOffset.
"
103595573,2476,lindong28,2017-03-01T01:49:32Z,"I have updated the AdminClient to remove the sleep operation. And the patch guarantees that if user thread attempts to send any request, it will get the lock on ConsumerNetworkClient and enqueue the requests once the currehnt `client.poll(..)` exits.

The user thread may still wait up to `maxRetryBackoffMs` in order to send the requests as of current implementation. This is because `ConsumerNetworkClient.poll(...)` does not guarantee that it will unblock and send the request if there is available request to be send by `ConsumerNetworkClient.send(...)`. The problem exists even if we have a queue outside of ConsumerNetworkClient and use only one thread to call `ConsumerNetworkClient.poll(...)` and `ConsumerNetworkClient.send(...)`. Having user thread wake up consumer may help reduce the chance of unnecessary block but doesn't prevent this from happening.

Ideally we want to be able to fix ConsumerNetworkClient so that user's request should get sent immediately if there is no other inflight requests. But it turns out to be a non-trivial fix. The problem is that `ConsumerNetworkClient.poll(...)` will hold the lock of `ConsumerNetworkClient` while it is blocked on `nioSelector.select(ms)`. If user calls `nioSelector.wakeup()`, it may be lost if the sender thread is right before the `nioSelector.select(ms)` while `nioSelector.wakeup()` is called. If user attempt to get the lock of `ConsumerNetworkClient` before calling `nioSelector.wakeup()`, then it will block as well while trying to get the lock. We need to find a way for the `poll(...)` to release lock while it is waiting on `nioSelector.select(ms)`. But I don't have an easy way to fix it.

"
103671647,2476,ijuma,2017-03-01T12:21:50Z,"I didn't see this mentioned in the KIP. This is a user facing CLI tool, so we should mention it there."
103671752,2476,ijuma,2017-03-01T12:22:42Z,"We can only do this once there's agreement that the next release is 0.11.0. Luckily, I started that discussion recently and people seem to be in favour. :)"
103671959,2476,ijuma,2017-03-01T12:23:50Z,"Since 0.10.0, we do the `IV0` thing (e.g. `KAFKA_0_11_0_IV0`). The version string should be changed accordingly."
103672175,2476,ijuma,2017-03-01T12:25:02Z,An empty synchronized block doesn't do anything. Is this still in progress?
103681117,2476,ijuma,2017-03-01T13:21:02Z,"Just so that we are on the same page. Until this method is available in the Java AdminClient, there is no public API for it (i.e. the Scala `AdminClient` is an internal class), right? So, the tool is the only way to do it if one doesn't want to deal with breakage later."
103748555,2476,lindong28,2017-03-01T18:00:15Z,@ijuma I included it as a way for myself and probably reviewers to test the patch. I was not sure if it needs to be included in the patch. Let me propose it in the mailing thread and specify it in the KIP. I am OK to remove it if others don't think it is necessary.
103748701,2476,lindong28,2017-03-01T18:00:59Z,Great. Thanks for confirmation :)
103749257,2476,lindong28,2017-03-01T18:03:45Z,I see. I changed this to `KAFKA_0_11_0_IV0`.
103750145,2476,lindong28,2017-03-01T18:08:14Z,"It is actually needed as a way for user thread to get the lock to enqueue requests once sender thread has finished its current `client.poll(...)`. Even though it is empty, this synchronzed block is still useful because some other thread may have the lock here.

This trick will no longer be needed after KAFKA-4820 is committed."
103751509,2476,lindong28,2017-03-01T18:14:51Z,@ijuma Have we explicitly stated in the code or documentation that this class is internal and should not be used by users? Becket is not aware of this as well. It seems that user can already construct Scala AdminClient directly and that is how I expect this API to be used.
103792810,2476,ijuma,2017-03-01T21:25:42Z,"Since this is going away, I won't comment further. :)"
103793255,2476,ijuma,2017-03-01T21:27:59Z,I see. Sounds good to ask in the mailing list thread if people think it's useful.
103793302,2476,ijuma,2017-03-01T21:28:15Z,"If we keep this class, we need to update the comment."
103794835,2476,ijuma,2017-03-01T21:35:34Z,"By the way, I think the code as it is now is a bit dangerous. Imagine that there are some changes and this field starts being used in info logging. It's not unlikely that a bug will be introduced. Is this optimisation really worth doing?"
103795078,2476,ijuma,2017-03-01T21:36:36Z,What is the reason for this change?
103795243,2476,ijuma,2017-03-01T21:37:29Z,What is the reason for this change?
103796063,2476,ijuma,2017-03-01T21:41:20Z,"The vast majority of Kafka code is not public API. The somewhat unintuitive way that public API is defined at the moment is by what has javadoc published:

https://kafka.apache.org/0102/javadoc/index.html

Apart from that, the old producers and consumers are also public API (until we remove them). The admin package is definitely internal and that's why there was no KIP for the Scala AdminClient when it was added in 0.9.0.0 (or any of the changes since)."
104300596,2476,lindong28,2017-03-05T00:07:59Z,OK. The optimization is removed now.
104300644,2476,lindong28,2017-03-05T00:09:59Z,Discussed offline. The issue is resolved after we change the definition of lowWatermark to be the minimum logStartOffset of all live replicas.
104300648,2476,lindong28,2017-03-05T00:10:22Z,Discussed offline. We don't need to checkpoint lowWatermark.
104300660,2476,lindong28,2017-03-05T00:11:23Z,I have updated the KIP to include this script and email the mailing thread about this addition. There is no objection so far. I assume we can include this script in the patch.
104300670,2476,lindong28,2017-03-05T00:12:10Z,The `synchronized` has been removed in the patch after rebasing the patch onto the trunk. KAFKA-4820 has been committed to the trunk.
104300696,2476,lindong28,2017-03-05T00:14:58Z,@ijuma I see. I think the method is available in two ways. User can use the script without having to deal with breakage later. And user can also uses our internal API and will need to deal with breakage later. LinkedIn Kafka team will use the second solution and is OK to deal with breakage later.
104300714,2476,lindong28,2017-03-05T00:16:25Z,"Thanks. I have updated the comment to ""A command for purging data of given partitions to the specified offset."""
104300777,2476,lindong28,2017-03-05T00:19:27Z,It is a side effect of making change and reverting change in the process of implementing this patch. I have changed it to the original order.
104301004,2476,lindong28,2017-03-05T00:38:27Z,I thought `setup()` and `tearDown()` will be executed once for every test method (e.g. testConsumeAfterPurge) but the initialization of `consumers` in `trait IntegrationTestHarness` will be executed only once per test class (e.g. AdminClientTest). I am wrong. I have reverted this change.
104834321,2476,junrao,2017-03-08T02:56:13Z,We probably want to add a comment that this is only used in the fetch requests from the followers.
104834398,2476,junrao,2017-03-08T02:57:11Z,Could we adjust the comment above accordingly?
104834503,2476,junrao,2017-03-08T02:58:43Z,Do we need to override this method to public?
104834563,2476,junrao,2017-03-08T02:59:22Z,unused import
104834590,2476,junrao,2017-03-08T02:59:42Z,"Hmm, not sure about this. In general NetworkClient is not thread safe."
104834673,2476,junrao,2017-03-08T03:00:18Z,It's kind of weird to use a ConsumerNetworkClient to implement purgeDataBefore() since this api has nothing to do with consumer.
104834719,2476,junrao,2017-03-08T03:01:01Z,unused import
104834774,2476,junrao,2017-03-08T03:01:42Z,%s => %d for oldLowWatermark and newLowWatermark
104834848,2476,junrao,2017-03-08T03:02:37Z,"Hmm, should we take max here?"
104834900,2476,junrao,2017-03-08T03:03:15Z,"To be consistent with the logic in recovery, should we initialize this to baseOffset?"
104834928,2476,junrao,2017-03-08T03:03:39Z,Could we add the new param to javadoc above and adjust the comment accordingly?
104834990,2476,junrao,2017-03-08T03:04:21Z,d% => %d
104835046,2476,junrao,2017-03-08T03:05:03Z,unused import
104835080,2476,junrao,2017-03-08T03:05:32Z,hasEnough => lowWatermarkReached ?
104835108,2476,junrao,2017-03-08T03:06:03Z,Could we just reuse LogFlushOffsetCheckpointIntervalMsProp instead of introducing a new config?
104835196,2476,junrao,2017-03-08T03:07:03Z,Should the comment be changed to all live replicas?
104835279,2476,junrao,2017-03-08T03:08:04Z,"Do we want to poll with 0 since it can burn CPU unnecessarily? Also, instead of duplicating the code for waiting the consumer to be ready, could we factor it out to a private method and reuse?"
104835345,2476,junrao,2017-03-08T03:08:58Z,Is there a need to test async? It seems the sync test is enough?
104835380,2476,junrao,2017-03-08T03:09:23Z,"Instead of the sleeping for a fixed amount of time, it would be better to do the checking in a waitUntilTrue() method."
104840525,2476,lindong28,2017-03-08T04:11:36Z,Good point. Maybe we should specify this in the explanation of the field instead of in the comment? I added following to the field's doc: The field is only used when request is sent by follower. For simplicity I didn't specify that the field will be set to -1 if the request is sent by consumer.
104840843,2476,lindong28,2017-03-08T04:16:38Z,Ah I should have updated the comment here. It is updated now.
104841044,2476,lindong28,2017-03-08T04:19:13Z,My bad. I have changed it back to protected.
104841196,2476,lindong28,2017-03-08T04:21:38Z,Thanks. Fixed now.
104841282,2476,lindong28,2017-03-08T04:22:59Z,Yeah `NetworkClient` is not thread-safe. But `ConsumerNetworkClient` explicitly stated in its Java doc that it is thread-safe.
104841527,2476,lindong28,2017-03-08T04:26:14Z,"I agree this name is a bit weird. Maybe we should either rename this class, or implement another `XXXNetworkClient` class to be used for sending requests by Java AdminClient in the future. As of now I don't have a simple solution for this. I think we can keep it as is and think more thoroughly in the Java AdminClient. What do you think?"
104841631,2476,lindong28,2017-03-08T04:27:50Z,"Sorry, I thought checkstyle will catch all unused import but I am wrong. It seems that checkstyle only does this check for Java files. Will check it manually in the future."
104841846,2476,lindong28,2017-03-08T04:30:57Z,Thanks. Fixed in both messages.
104842139,2476,lindong28,2017-03-08T04:35:40Z,"If we truncate log using `truncateFullyAndStartAt` to an offset that is higher than the `logStartOffset`, I think we should keep the logStartOffset not changed instead of increasing it, right? It seems similar to the existing logic of updating `recoveryPoint`, where we use `math.min`. Another argument is that it is safer to assume a small logStartOffset."
104842737,2476,lindong28,2017-03-08T04:43:59Z,"I think we still need to set this to -1 initially so that we can call `nextOffsetFromLog()` the first time we access `nextOffset()`.

The longer answer:
We can set initialize this to `baseOffset` in recovery because we will read all entries in `recovery()`. In the scenario that we do not call `recover()`, we need to initialize `nextOffset` to the `nextOffset` of the latest entry in the log segment file. To trigger this, we need to initialize `nextOffset` to -1 and call `nextOffsetFromLog()` the first time we access `nextOffset()`."
104847614,2476,lindong28,2017-03-08T05:38:08Z,Good point. It is updated now.
104847763,2476,lindong28,2017-03-08T05:39:01Z,My bad. Thanks for catching this! Fixed now.
104847789,2476,lindong28,2017-03-08T05:39:14Z,Fixed now.
104848112,2476,lindong28,2017-03-08T05:40:57Z,Sure. Replaced `hasEnough` with `lowWatermarkReached`.
104849219,2476,lindong28,2017-03-08T05:49:11Z,"I am not sure. It seems safer to have a separate config. The current default value of `LogFlushOffsetCheckpointIntervalMs` is 60 seconds. But we probably want LogFlushStartOffsetCheckpointIntervalMs to be set to a lower value because if a broker fails after user purge data but before the logStartOffset is checkpointed, the offset may not be preserved. Having separate configs allows user to tradeoff between disk write overhead and logStartOffset persistence.

Also, it is probably cheaper to checkpoint logStartOffset than checkpointing recoverOffset. We only need to checkpoint logStartOffset for a partition if its logStartOffset > baseOffset, which only happens when user has explicitly requested data purge. Thus the actually amount of data written to the log-start-offset-checkpoint file is probably much less than the data written to recovery-point-offset-checkpoint. This means that we can set LogFlushStartOffsetCheckpointIntervalMs to a lower value without worrying too much about its overhead.
"
104849362,2476,lindong28,2017-03-08T05:50:56Z,Thanks! Updated now.
104849612,2476,lindong28,2017-03-08T05:53:58Z,"It seems that we don't have to worry about CPU burn because `TestUtils.waitUntilTrue()` will wait for 100L between calls to `poll(0)`.

Sure. I updated the patch to move this wait logic into a separate method."
104851102,2476,lindong28,2017-03-08T06:09:38Z,It is no longer needed after we add a dedicated thread in AdminClient to do `poll()`. Removed now.
104851560,2476,lindong28,2017-03-08T06:15:06Z,This test with offline brokers is needed when we define LW to min offset of all replicas. But it is not needed anymore after we change it to beh min offset of all live replicas. I simply removed this part.
105036939,2476,lindong28,2017-03-08T22:11:18Z,Thought about this more. I think we should just set `logStartOffset` to `newOffset`. The reason is that `truncateFullyAndStartAt(...)` is supposed to create a new log. Thus `logStartOffset` should be the same as `baseOffset` of the first segment.
105060654,2476,junrao,2017-03-09T00:29:07Z,"Ok, we can leave it as it is."
105060659,2476,junrao,2017-03-09T00:29:10Z,"Yes, that makes sense."
105060668,2476,junrao,2017-03-09T00:29:15Z,"Yes, that works. It's just that if we initialize to baseOffset (which is what nextOffsetFromLog() will return on an empty segment), _nextOffset is always initialized properly and we probably can get rid of the if test in nextOffset()."
105067509,2476,lindong28,2017-03-09T01:20:50Z,"@junrao I am not sure we can get rid of the if test in `nextOffset()`. Suppose we initialize `_nextOffset` to `baseOffset` and the segment is not empty, when should we call `nextOffsetFromLog()` in order to set `_nextOffset` to the correct value? Currently we need the if test in `nextOffset()` to call `nextOffsetFromLog()` on the first invocation of `nextOffset()`. Did I miss something?
"
105333239,2476,junrao,2017-03-10T05:30:29Z,Could this be private?
105333267,2476,junrao,2017-03-10T05:30:54Z,Thanks for the explanation. Then this is fine.
105567622,2476,lindong28,2017-03-12T20:11:43Z,Yeah it should be private. I will change it.
106523663,2476,becketqin,2017-03-16T20:31:18Z,Should this be `LEADER_NOT_AVAILABLE` instead?
106525699,2476,becketqin,2017-03-16T20:40:21Z,"Do we want to wait until the NetworkThread to exit before closing the `ConsumerNetworkClient`? Otherwise there will be some exception thrown and logged, which seems no ideal."
106527195,2476,becketqin,2017-03-16T20:47:31Z,Can we use a macro instead?
106549682,2476,becketqin,2017-03-16T22:47:38Z,The terminology LSO is also used by KIP-98 as Last Stable Offset. Just a note so that we do not introduce confustion.
106550281,2476,becketqin,2017-03-16T22:51:38Z,What is LBO?
106550413,2476,becketqin,2017-03-16T22:52:31Z,HW -> LW
106550867,2476,becketqin,2017-03-16T22:55:53Z,Would it be safer to set the initial value to -1L if it is only kept on the leader?
106551121,2476,becketqin,2017-03-16T22:57:58Z,Nit: We usually do not use the `_*` pattern in Apache Kafka. 
106551960,2476,becketqin,2017-03-16T23:03:44Z,attempt => attempting
106560377,2476,becketqin,2017-03-17T00:09:41Z,Shouldn't this be -1L?
106568532,2476,becketqin,2017-03-17T01:29:44Z,We can probably make those methods protected.
106569363,2476,becketqin,2017-03-17T01:40:25Z,This number seems a little too large for DeleteRecordsRequest.
106571166,2476,becketqin,2017-03-17T02:03:40Z,"This could be
```val partition = getPartition(topicPartition).getOrElse(throw new UnknownTopicOrPartitionException(""Partition %s doesn't exist on %d"".format(topicPartition, localBrokerId))```"
106586044,2476,lindong28,2017-03-17T05:24:34Z,Thanks. Good catch. Fixed now.
106587187,2476,lindong28,2017-03-17T05:40:53Z,"Are you suggesting to wait for NetworkThread to exit before or after calling `client.close()`? If we call `client.close()` first, we can not prevent error from being logged because `Selector.close()` will log error. If we wait for NetworkThread to exit first, then close() may become a blocking call which seems unnecessary. 

I agree with you that we do not want to throw Exception here. But I am not sure why we should not log any error here. For example, `Selector.close()` may log error if there is `IOException`.

I have updated the code to catch exception. Does this address the problem?"
106587383,2476,lindong28,2017-03-17T05:43:42Z,Sure. Fixed now.
106587470,2476,lindong28,2017-03-17T05:45:01Z,Sure. Replaced `LSO` with `logStartOffset`.
106587521,2476,lindong28,2017-03-17T05:45:44Z,Good catch. Thanks. It means logBeginOffset. Replaced it with logStartOffset.
106587539,2476,lindong28,2017-03-17T05:46:07Z,Thanks. Fixed now.
106587752,2476,lindong28,2017-03-17T05:49:36Z,"I used `_*` here because we have methods `lastCaughtUpTimeMs`, `logStartOffset` and `lowWatermark`. What would you suggest to name these variables?"
106587831,2476,lindong28,2017-03-17T05:50:30Z,Sure. Changed it to -1L.
106587893,2476,lindong28,2017-03-17T05:51:13Z,Fixed now.
106587916,2476,lindong28,2017-03-17T05:51:32Z,Good catch. Fixed now.
106588046,2476,lindong28,2017-03-17T05:53:18Z,Good point. I changed it to 1.
106588193,2476,lindong28,2017-03-17T05:55:28Z,Cool. Fixed now.
106767687,2476,becketqin,2017-03-18T00:47:23Z,"It seems that if any exception is thrown from the network thread, the futures that have been returned will not be completed forever. Ideally we will want to have those futures get an exception so that users do not wait on that indefinitely."
106770832,2476,lindong28,2017-03-18T02:29:49Z,@becketqin Thanks for your review! I have fixed it in the last commit.
106821825,2476,junrao,2017-03-19T23:00:48Z,Should we update upgrade.html?
106821828,2476,junrao,2017-03-19T23:00:59Z,Should we rename offset to sth like fetchOffset so that it's clear how it's different from logStartOffset?
106821831,2476,junrao,2017-03-19T23:01:08Z,Should we follow the convention by nesting partitions inside topics?
106821839,2476,junrao,2017-03-19T23:01:25Z,"So, we are not calling maybeIncrementLeaderLW() when the old segments are deleted? This means that LW may not be accurate when there is only 1 replica?"
106821844,2476,junrao,2017-03-19T23:01:34Z,"Hmm, it seems that we should be using the logStartOffset from the follower, not from the leader?"
106821857,2476,junrao,2017-03-19T23:01:55Z,"If we modify the log, we should probably coordinate this with the log cleaner. In LogManager.truncateTo(), we first abort the ongoing cleaning, then truncate and finally resume the cleaning. We probably should do the same here."
106821867,2476,junrao,2017-03-19T23:02:15Z,Should we adjust the comment above accordingly?
106821871,2476,junrao,2017-03-19T23:02:27Z,"Hmm, the change doesn't seem right. The first value should be startOffset and the second value should be logStartOffset?"
106821879,2476,junrao,2017-03-19T23:02:53Z,"Is this comment still correct? I thought we check the delete offset is smaller than HW, which is less than log end offset?"
106821907,2476,junrao,2017-03-19T23:03:49Z,"Is it important to track this metric, especially at partition level, given deleteRecords is a rare operation?"
106821909,2476,junrao,2017-03-19T23:04:03Z,"Hmm, could timestampOffset.offset be < localReplica.logStartOffset? It seems that the result returned from fetchOffsetForTimestamp() guarantees that timestampOffset.offset >= localReplica.logStartOffset? "
106821916,2476,junrao,2017-03-19T23:04:13Z,This is not a produce response.
106821917,2476,junrao,2017-03-19T23:04:19Z,Do we need this to be 10000? Could we make it 60000? 
106821918,2476,junrao,2017-03-19T23:04:23Z,log begin offset => log start offset?
106821926,2476,junrao,2017-03-19T23:04:40Z,"It seems that logStartOffset is only needed in ReplicaFetcherThread. Instead of adding it in PartitionFetchState, perhaps it's simpler to just add it here by calling getLogStartOffset()?"
106821934,2476,junrao,2017-03-19T23:04:54Z,"If we do this, perhaps there is no need to pass in metadataCache through methods like becomeLeaderOrFollower() and maybeUpdateMetadataCache()?"
106837370,2476,lindong28,2017-03-20T05:48:26Z,I am not sure we have that convention. This is the same pattern used by `ReassignPartitionsCommand`. I can change it if you think the other pattern is better.
106837391,2476,lindong28,2017-03-20T05:49:00Z,I didn't change it in order to reduce the amount of code change and conflicts with other patches. I will change it as you suggested.
106838059,2476,lindong28,2017-03-20T06:03:20Z,"It seems OK because when there is no follower, LW is only used in the `PurgeResponse` to tell user whether purge has finished. In this case the `PurgeRequest` would have triggered `maybeIncrementLeaderLW`."
106838930,2476,lindong28,2017-03-20T06:20:34Z,Good point. I have updated code so that `deleteLogStartOffsetBreachedSegments` is called periodically by `LogManager` for non-compact topics and by `LogCleaner` compact topics. `DeleteRecordsRequest` only needs to update `logStartOffset` of the `Replica`.
106839374,2476,lindong28,2017-03-20T06:27:55Z,Sure. I updated the comment to say `return error on attempt to read beyond the log end offset or read below log start offset`. Is this OK?
106840055,2476,lindong28,2017-03-20T06:40:15Z,You are right. It is wrong after we add that check. I have removed this comment.
106840295,2476,lindong28,2017-03-20T06:44:02Z,It is not important. I added it because we have similar per-partition metrics in `DelayedProduce`. I realized that we don't per-partition metrics in `DelayedFetch`. It is removed now.
106840866,2476,lindong28,2017-03-20T06:52:31Z,Good point. It is removed now.
106840929,2476,lindong28,2017-03-20T06:53:38Z,My bad. I have corrected this comment now.
106841258,2476,lindong28,2017-03-20T06:59:03Z,"I like this to be smaller to reduce the chance that a logStartOffset updated by DeleteRecordsRequest is lost. This is useful e.g. if there is only one replica and the broker may crash. Anyway, I will change it to 60 sec."
106841300,2476,lindong28,2017-03-20T06:59:37Z,Sure. Fixed now.
106843753,2476,lindong28,2017-03-20T07:28:19Z,Oops... Good point! Thanks! I fixed the bug by passing `followerLogStartOffset` to `updateLogReadResult` via `LogReadResult`.
106843969,2476,lindong28,2017-03-20T07:30:10Z,I think the original error message is probably wrong. Why the range of log segments depend on an offset provided by the user?
106844614,2476,lindong28,2017-03-20T07:37:08Z,`logStartOffset` is also needed in `ReplicaManager.readFromLocalLog`. It seems reasonable to include it in the `FetchRequest.PartitionData` since `FetchRequest.PartitionData` is supposed to contain the same fields of `Protocol.FETCH_REQUEST_PARTITION_V1`. Does this make sense?
106844878,2476,lindong28,2017-03-20T07:40:41Z,"Sure. I have updated the code to remove `metadataCache` from parameters of methods `becomeLeaderOrFollower`, `maybeUpdateMetadataCache` and `makeFollowers`."
106845566,2476,lindong28,2017-03-20T07:50:07Z,Hmm.. I wasn't aware I need to change upgrade.html. I will do it tomorrow.
106956008,2476,lindong28,2017-03-20T16:58:20Z,"Sorry, I missed the first `%d` in the message. I have corrected this now."
107039812,2476,junrao,2017-03-20T23:13:42Z,"Ok, we can leave this as it is since the /admin/reassign_partitions path in ZK already follows the same pattern."
107039880,2476,junrao,2017-03-20T23:14:13Z,"If LW is only used for DeleteRecordResponse, do we need to store and maintain it in Replica? Could we just compute it on the fly when serving DeleteRecordRequests?"
107039905,2476,junrao,2017-03-20T23:14:24Z,"So, DeleteRecordsRequest can't be applied on a compacted topic? Should we return an error code in the response in that case?"
107039943,2476,junrao,2017-03-20T23:14:38Z,I meant is there a need to include logStartOffset in PartitionFetchState in AbstractFetcherThread.scala? It seems that only ReplicaFetcherThread needs it.
107049639,2476,lindong28,2017-03-21T00:29:02Z,Ah I see your point now. You are right that we don't have to include it in `AbstractFetcherThread`. I have removed it from `PartitionFetchState` in `AbstractFetcherThread` and the code change in `ConsumerFetcherThread` and `AbstractFetcherThread` is much smaller now. Thanks!
107052078,2476,lindong28,2017-03-21T00:51:33Z,"Yes we can compute it on the fly when we need it in e.g. in `DelayedDeleteRecords`. Is there reason to do it on the fly instead of maintaining it in the `Replica`? Is it because you think the patch would be simpler if we compute it on the fly?

I think both solution has the same performance. But maintaining it in Replica in the same way as `HW` may make LW easier to reason about and extend in the future. For example, the code is easier to extend if we want to use `LW` in the future as a way to determine the minimum offset available for consumption (currently the minimum offset available for consumption varies between replicas because different replicas may have different logStartOffset)."
107052828,2476,lindong28,2017-03-21T00:58:38Z,"BTW, the reason `maybeIncrementLeaderLW()` is not called when the old segments are deleted is that the concept of `LW` and partition is at a higher level than `LogManager`, `LogCleaner` and `Log`. Ideally those classes such as `LogManager` do not contain reference to the higher level class such as `Partition`."
107058922,2476,lindong28,2017-03-21T02:04:36Z,I have updated `upgrade.html` to change latest version from 0.10.3.0 to 0.11.0 and specify change in the FetchRequet and FetchResponse.
107195440,2476,junrao,2017-03-21T15:56:32Z,"My concern is that if we store it, but don't make it accurate, then someone may start using it in the future without realizing that it's only accurate at DeleteRecordRequest time. So, it's better to either store it and make it accurate, or not store it and always compute it accurately when needed."
107281864,2476,lindong28,2017-03-21T21:35:35Z,"@junrao I see. I just updated the patch to store `lowWatermark` in `Partition` instead of `Replica` and always call `maybeIncrementLeaderLW()` when we retrieve `lowWatermark`. I still maintain lowWatermark as state so that we only need to call `tryCompleteDelayedRequests` if lowWatermark has increased. Other than that, the `lowWatermark` is essentially computed on the fly developer access it through `partition.updateAndGetLowWatermarkIfLocal()`.

I added a comment that says: `lowWatermark should always be accessed through updateAndGetLowWatermarkIfLocal() because we may not update lowWatermark when log segment is deleted`.

"
107512572,2476,lindong28,2017-03-22T19:42:46Z,I will add back this javascript template in the next commit. Guozhang just told me that we are expected to have a local webserver in order to read this upgrade.html.
107589219,2476,junrao,2017-03-23T05:10:16Z,"It still seems that it's a bit weird to maintain lowWatermarkIfLocal just to see if the value has changed. Another way to do that is in Partition.updateReplicaLogReadResult(), we compute LW before and after calling replica.updateLogReadResult. Then, we will know if LW has changed. We can further optimize it by only computing the LW if the deleteRecordPurgatory is not empty."
107589225,2476,junrao,2017-03-23T05:10:22Z,We are resetting log-start-offset to the base offset of first segment?
107589236,2476,junrao,2017-03-23T05:10:29Z,"This logic seems a bit dangerous. There is no synchronization between nextOffset() and the updating of nextOffset in append(). So they can step on each other. Since the deleting record logic never deletes the active segment, it seems that we could just use the base offset of the next segment as the next offset of the current segment. Then we don't need to maintain nextOffset per segment?"
107589241,2476,junrao,2017-03-23T05:10:34Z,"I had a comment on this earlier. If deleteRecords() can't be applied on a compacted topic, should we send some error code in the response?"
107748044,2476,lindong28,2017-03-23T18:26:29Z,That is a good idea. I have updated the patch to avoid maintaining lowWatermark as state of Partition and compute it only on the fly. And `updateReplicaLogReadResult` would only compute LW if deleteRecordPurgatory is not empty.
107750331,2476,lindong28,2017-03-23T18:35:39Z,"This is kind of similar to what we do with recoveryPoints if its checkpoint file is unreadable. For example, if recoveryPoints checkpoint file is unreadable but `.kafka_cleanshutdown` file exists, we will reset recoveryPoint to `activeSegment.nextOffset` after warning `Resetting the recovery checkpoint to 0`. Here we assume the log-start-offset in the checkpoint file is 0 if checkpoint file is not readable.

On a second thought, I agree it is confusing because we will always set `logStartOffset` to at least base offset of the first second if its checkpoint file is unreadable. I will remove this statement."
107752878,2476,lindong28,2017-03-23T18:45:42Z,"@junrao My apology, I missed your comment here. `DeleteRecordsrRequest` can actually be applied on a compacted topic. `DeleteRecordsrRequest` will increase the logStartOffset. When `LogCleaner` calls `log.deleteOldSegments()`, those log segments which breach the logStartOffset will be deleted."
107753068,2476,lindong28,2017-03-23T18:46:29Z,"@junrao Sorry, I missed your comment there previously. My bad. DeleteRecordsrRequest can actually be applied on a compacted topic. DeleteRecordsrRequest will increase the logStartOffset. When LogCleaner calls log.deleteOldSegments(), those log segments which breach the logStartOffset will be deleted. Thus we don't need to send error code in response."
107779887,2476,junrao,2017-03-23T20:48:59Z,"Hmm, in LogCleaner, we have the following code. Only logs configured as compacted and deleted are deletable and will call deleteOldSegments().

```
      val deletable: Iterable[(TopicPartition, Log)] = cleanerManager.deletableLogs()
      deletable.foreach{
        case (topicPartition, log) =>
          try {
            log.deleteOldSegments()
          } finally {
            cleanerManager.doneDeleting(topicPartition)
          }
      }

```"
107785265,2476,lindong28,2017-03-23T21:12:04Z,"@junrao After KIP-71 is committed, log compaction and deletion can co-exist. Strictly speaking DeleteRecordsRequest can be applied on a compacted topic: if a topic is compacted and deletable, its log segment can be deleted by LogCleaner; if a topic is non-compacted and deletable, its log segment can be deleted by the `LogManager.cleanupLogs()` which runs periodically.

Are you suggesting that DeleteRecordsResponse should provide error immediately if the topic is not deletable? This is certainly a good point. I missed this part previously. I will update the patch to do it. Thanks!"
107806053,2476,lindong28,2017-03-23T23:16:43Z,"I get your concern. I am not sure we could just use the base offset of the next segment, because `nextOffset()` is a method that exists prior to this KIP and is actually used by `Log` to initialized `nextOffsetMetadata` (i.e. `nextOffsetMetadata = new LogOffsetMetadata(activeSegment.nextOffset(), activeSegment.baseOffset, activeSegment.size.toInt)`). Thus we actually need `nextOffset` of the active log segment.

I have updated the patch so that `nextOffset()` is a read-only operation and only returns the cached next offset. And I make sure that for non-empty segment that is read from disk, we will always call `updateNextOffsetFromLog()` before the first call to its `nextOffset()`. I think it should resolve the problem."
107819630,2476,junrao,2017-03-24T01:24:55Z,"Before this patch, we have nextOffset(), which computes the next offset by scanning the last portion of the log. This could be expensive, but is only used during the initial log loading. I was suggesting that we could just keep the logic as it is. in deleteLogStartOffsetBreachedSegments(), we always use the baseOffset of the next segment to check if a segment should be deleted. If a segment doesn't have the next segment, we know it's the active segment and won't be deleted. This way, we never need to call nextOffset() in deleteLogStartOffsetBreachedSegments()."
107820483,2476,lindong28,2017-03-24T01:35:40Z,"Thanks for the quick comment @junrao.

Yes it is possible to do that, i.e. we keep the previous expensive implementation of `nextOffset()` which scans the last portion of the log, and implement `deleteLogStartOffsetBreachedSegments()` in such a way that the deletion of a segment depends on the baseOffset of the next segment. I am just not very sure about advantage of that approach as compared to the approach in the current patch. In comparison, that approach keeps nextOffset() expensive, and requires more complex implementation of `deleteLogStartOffsetBreachedSegments()` because it can no longer re-use `deleteOldSegments(predicate)` to delete partition. The benefit of the approach is that we don't need to maintain `nextOffset` as state in `LogSegment`.

I am OK with the suggested approach. Let me do that then."
107825145,2476,lindong28,2017-03-24T02:34:51Z,"After thinking about this more, I agree that it is safer to keep less state in the class unless `nextOffset()` is called frequently. Thus I agree the suggested approach is simpler. Thanks @junrao."
108008403,2476,junrao,2017-03-24T22:07:24Z,We could use mapValues() here?
108008598,2476,junrao,2017-03-24T22:09:03Z,"Thinking a bit more. It seems that a delayedDeleteRecord could be added right after the if check. In the next updateReplicaLogReadResult() call, LW may not advance any more. Then the response will be delayed. Perhaps, if (replicaManager.delayedDeleteRecordsPurgatory.delayed() > 0) , we could just always call tryCompleteDelayedRequests()?"
108008646,2476,junrao,2017-03-24T22:09:26Z,This is not very accurate. LW will increase with DeleteRecordsRequest or log deletion?
108008660,2476,junrao,2017-03-24T22:09:33Z,Should we initialize this to an UnknownOffset?
108009638,2476,lindong28,2017-03-24T22:18:10Z,"No we can not use `mapValues()`. This is because `mapValues()` returns a view and any write operation on that view will be ignored later, e.g. `status.acksPending = true` in `DelayedDeleteRecords`."
108010888,2476,lindong28,2017-03-24T22:23:08Z,"Yes I think it is correct. For example, if there is only one replica, DeleteRecordsRequest or log retention will immediately increase LW."
108011703,2476,lindong28,2017-03-24T22:28:46Z,There is no existing variable named UnknownOffset. There is `LogOffsetMetadata.UnknownSegBaseOffset` but I am not sure we should use UnknownSegBaseOffset for logStartOffset. So I created `Log.UnknownLogStartOffset = -1L`. Does address the problem?
108012358,2476,lindong28,2017-03-24T22:34:33Z,"If there is only one live replica for this partition, then LW will increase immediately to the specified offset and the response can be sent to the client. Otherwise, say there is at least one live follower for this partition, LW will not increase now. LW will increase in the next updateReplicaLogReadResult() call, and the response can be sent back then. Does this work?"
108092052,2476,junrao,2017-03-27T05:16:52Z,"To follow the convention in FETCH_RESPONSE, should this be named FETCH_REQUEST_PARTITION_V5?"
108092057,2476,junrao,2017-03-27T05:17:01Z,Could you include the changes from KIP-98 too?
108266895,2476,junrao,2017-03-27T20:08:06Z,"We don't read the logStartOffset from the checkpoint in this case, which is a bit inconsistent with how we load the log when the broker starts up. This seems ok for now since in the rare case when a follower loses the whole log (but still has the logStartOffset checkpoint), it can discover the logStartOffset from the leader quickly. Not sure if we want to add a comment on this."
108287834,2476,lindong28,2017-03-27T21:44:53Z,"I am not sure I understand the problem of inconsistency here. I think this `createLog()` will only be called when the replica doesn't exist on the broker, which means the broker should not have logStartOffset for this replica, right? Are you considering the scenario that the replica is deleted (either through StopReplicaRequet or manually deleted via `rm -rf`) but the logStartOffset is still there? In that case I think we should actually ignore the outdated logStartOffset in the checkpoint file and read the value from leader. Because logStartOffset can be considered as logically deleted if log file is deleted. Does this make sense?"
108288220,2476,lindong28,2017-03-27T21:46:54Z,"I think it may be better for owners of KIP-98 to add this in the upgrade note (e.g. in KAFKA-4816) because they know much more about the change in KIP-98 than I do.. Anyway, I can go over the KIP-98 doc and add things that I think necessary."
108291575,2476,ijuma,2017-03-27T22:02:38Z,"@lindong28, feel free to leave out the KIP-98 change for now. I am about to file a JIRA for updating the upgrade notes for KIP-98. This is not the only one."
108301862,2476,lindong28,2017-03-27T23:07:34Z,Thanks @ijuma. Then I won't include this in the patch.
108301889,2476,lindong28,2017-03-27T23:07:45Z,Sure. I will change it to be FETCH_REQUEST_PARTITION_V5.
108305808,2476,lindong28,2017-03-27T23:38:42Z,"As of now, the base offset of the first segment of a compacted topic can be always 0. Thus it seems OK to have its logStartOffset as 0. We can consider logStartOffset as the lower bound of the offset of the first message in the partition, but not necessarily the strict lower bound. I couldn't find any use-case that would be broken due to this definition."
108326834,2476,becketqin,2017-03-28T03:17:00Z,"Can we be more clear on this field. In the `FetchResponse` we have log_start_offset which have almost the same comment. Maybe here we can say ""The smallest available offset across all live replicas."""
108326881,2476,becketqin,2017-03-28T03:17:44Z,for the given topic => for the given partition.
108329771,2476,junrao,2017-03-28T03:54:50Z,"I don't think it breaks anything. So, we can leave this as it is."
108333754,2476,becketqin,2017-03-28T04:49:22Z,"There seems a very rare case that may result in message loss. Assuming there is only one replica, consider the following sequence:
1. User deletes a topic, we are not deleting the log starting offset from the checkpoint file. 
2. If the topic is created again with the same name and the partitions happen to be on the same broker. 
3. user produced some messages and before the log starting offset is checkpointed, the broker went down.
4. Now when the broker restarts, the old checkpointed log starting offset may be applied to the newly created topic, which may cause the messages that have been produced into the log to be unavailable to the users.

This is a very rare corner case, though."
108335119,2476,lindong28,2017-03-28T05:07:41Z,Sure. Updated now.
108335175,2476,lindong28,2017-03-28T05:08:03Z,Good catch. Fixed now.
108335392,2476,lindong28,2017-03-28T05:10:53Z,Good point. I fixed the problem by always do `checkpointLogStartOffsetsInDir(removedLog.dir.getParentFile)` when a partition is deleted. The overhead will probably be smaller than checkpointing the cleaner offset which we already do everytime we delete a partition.
103840661,2614,junrao,2017-03-02T02:42:05Z,"We may add new compression codec in the future. Using the bits from 15 downwards makes adding new compression codec a bit easier in the future. Also, unused should be 5-15."
103840673,2614,junrao,2017-03-02T02:42:15Z,"In the comment above the class and in the KIP wiki, LastOffsetDelta is after Attributes."
103840681,2614,junrao,2017-03-02T02:42:21Z,The comment is no longer valid?
103840693,2614,junrao,2017-03-02T02:42:29Z,"Hmm, should we be passing in delta timestamp?"
103840712,2614,junrao,2017-03-02T02:42:36Z,The comment seems outdated. Attributes are no longer used to indicate the presence of key and value.
103840724,2614,junrao,2017-03-02T02:42:43Z,"Could we spell out how this is calculated? Also, does it include the length field?"
103840746,2614,junrao,2017-03-02T02:42:57Z,"If key is null, should we return -1, which is the current convention?. Ditto for valueSize()."
103840757,2614,junrao,2017-03-02T02:43:04Z,"We compute CRC from timestamp, instead of attributes."
103840767,2614,junrao,2017-03-02T02:43:09Z,timestamp => timestampDelta? Ditto in sizeInBytes() and sizeOfBodyInBytes().
103840780,2614,junrao,2017-03-02T02:43:15Z,Probably non-idempotent/non-transactional to make it clear?
103840821,2614,junrao,2017-03-02T02:43:40Z,"For up-converted message, timestamp will be -1. But shouldn't TimestampType be the TimestampType of the topic?"
103840897,2614,junrao,2017-03-02T02:44:17Z,"The comment above suggests that lastOffset() is the same as nextOffset(), which seems inaccurate."
103840910,2614,junrao,2017-03-02T02:44:25Z,Shouldn't we return -1 if there is no key? Ditto for valueSize().
103840930,2614,junrao,2017-03-02T02:44:37Z,Should that be hasValue() to be consistent with hasKey()?
103840934,2614,junrao,2017-03-02T02:44:42Z,Should this be fixed?
103840949,2614,junrao,2017-03-02T02:44:51Z,Should we return 0 or LogEntry.NO_SEQUENCE?
103840960,2614,junrao,2017-03-02T02:45:00Z,Probably mention the magic in the error message.
103840968,2614,junrao,2017-03-02T02:45:05Z,ByteBufferLogEntry => ByteBufferOldLogEntry ?
103840980,2614,junrao,2017-03-02T02:45:10Z,integer => long? Ditto in a few other places.
103840998,2614,junrao,2017-03-02T02:45:19Z,It's a bit weird to assign lastOffset to firstOffset with old magic. Perhaps adding a comment to explain why?
104274159,2614,ijuma,2017-03-04T01:44:08Z,"I think we should introduce a `ByteUtils` class and move all the relevant methods there. Maybe we could do that in its own PR, which could be merged quickly independently of the message format changes?"
104276385,2614,hachikuji,2017-03-04T02:40:23Z,Works for me.
104345921,2614,junrao,2017-03-06T05:29:26Z,1 => LogEntry.MAGIC_VALUE_V1? Ditto in line 84 below.
104345937,2614,junrao,2017-03-06T05:29:39Z,"So, the old consumer won't work on the EOS message format? If so, should we pin the FetchRequest version in ConsumerFetcherThread to an older version?"
104345956,2614,junrao,2017-03-06T05:30:02Z,"Since maxMessageSize now applies to record set, it would be useful to change the description in KafkaConfig and describe that change in the upgrade section of the documentation."
104345961,2614,junrao,2017-03-06T05:30:07Z,entry probably should now be named logRecord?
104345969,2614,junrao,2017-03-06T05:30:18Z,"In validateMessagesAndAssignOffsets(), should we further validate that there is only 1 log entry in records in EOS format?"
104345976,2614,junrao,2017-03-06T05:30:26Z,Does this need to be fixed? Could we reuse AbstractRecords.estimateSizeInBytes() for estimating the size for all magic?
104495063,2614,junrao,2017-03-06T19:07:12Z,"Hmm, this adds some complexity and I am not sure about the benefit. First, when sending the first few batches of the data in the producer or when the leaders are not on all brokers, we don't have a connection to all brokers. So the minUsedMagic may not be very accurate.

Could we just rely on the down conversion logic on the broker side and make it clearer in the documentation that EOS feature should be turned on only when the whole cluster has been upgraded?"
104495083,2614,junrao,2017-03-06T19:07:17Z,Does this array need to be nullable?
104495161,2614,junrao,2017-03-06T19:07:35Z,"Could you add some comment on what needs to be done if we change the version of the key in the future? For example, do we have to explicitly delete keys on the old version since it won't be compacted out by the new version of the key?"
104495190,2614,junrao,2017-03-06T19:07:44Z,Should we support the write method here?
104495211,2614,junrao,2017-03-06T19:07:49Z,The reference ByteBufferLogInputStream.ByteBufferLogEntry in the comment above is no longer valid.
104495248,2614,junrao,2017-03-06T19:07:55Z,Should we include lastStableOffset?
104495273,2614,junrao,2017-03-06T19:08:01Z,The error message seems to be the opposite.
104495288,2614,junrao,2017-03-06T19:08:05Z,Is the TODO still valid?
104495343,2614,junrao,2017-03-06T19:08:20Z,Should timestampType be hardcoded to createTime or should it use the topic level config? Ditto for line 256 below.
104495356,2614,junrao,2017-03-06T19:08:25Z,Is 1024 guaranteed to be always large enough?
104495396,2614,junrao,2017-03-06T19:08:38Z,"Should we print out additional information related to EOS format (e.g. pid, epoch, sequence) etc? Should we support printing out the control message?"
104495429,2614,junrao,2017-03-06T19:08:44Z,"Now that we have increased the param limit in checkstyle, do we still need this?"
105383915,2614,ijuma,2017-03-10T12:01:24Z,We should add a note that this is an internal class since the package doesn't make that obvious.
105384533,2614,ijuma,2017-03-10T12:06:49Z,"Nit: `maxUsableMagic = Math.min(usableMagic, maxUsableMagic)`?"
105385120,2614,ijuma,2017-03-10T12:11:24Z,"I think this `switch` should be a separate method as it's the interesting logic that may need to be updated when we add more produce versions. Maybe it should live in `ProduceRequest`? And we should have a test that breaks when we add a new produce version to the protocol (if we don't already) so that we don't forget to update it (since the client would still work fine, it could go unnoticed)."
105386294,2614,ijuma,2017-03-10T12:20:42Z,"Hmm, would it be better to have a `NodeApiVersions.usableVersion` that takes a `desiredVersion` as well? We could the collapse these two branches and there would be no need to call `ensureUsable`."
105386464,2614,ijuma,2017-03-10T12:21:54Z,"It's an existing issue, but realised that this comment is out of date."
105387601,2614,ijuma,2017-03-10T12:30:43Z,"Can we just simply call `get` on the map? Also, maybe we should return `null` if there's no element (like `Map.get`)."
105388069,2614,ijuma,2017-03-10T12:34:39Z,Maybe we could move the logic that creates the appropriate `UsableVersion` into a static factory method (or constructor) in `UsableVersion`.
105390691,2614,ijuma,2017-03-10T12:54:19Z,"I found the term `default` a bit confusing here. This is `usableVersion`, right? I guess you were trying to avoid repeating it since the class name is `UsableVersion`. Could we just call it `value`?"
105390911,2614,ijuma,2017-03-10T12:55:59Z,"I think the field name should just be `apiVersion`, no?"
105391297,2614,ijuma,2017-03-10T12:58:39Z,"This should be ""0.11.0-IV0"" and the rest should be updated accordingly."
105392682,2614,ijuma,2017-03-10T13:08:27Z,"`CREATE_TIME` is correct, see https://github.com/apache/kafka/commit/7565dcd8b0547f91a5d9d19771d9cd6693079d01"
105394318,2614,ijuma,2017-03-10T13:19:26Z,"It makes sense to leave space for the compression type, but I'd hope we won't need more than 3 bits for it."
105394549,2614,ijuma,2017-03-10T13:20:53Z,"Would `BasicRecord` or `SimpleRecord` be a better name? I think the key feature is that it just captures the essence of what a record is: timestamp, key, value (and potentially headers in the future?)."
105497417,2614,hachikuji,2017-03-10T21:55:38Z,"My guess is we'll have another message format bump before we run out of room for compression codecs, but I don't mind adding another bit if you think it's useful."
105498031,2614,hachikuji,2017-03-10T21:58:44Z,"Oh, I may have misunderstood. Are you suggesting moving the transactional flag and timestamp type to the end of the second byte?"
105515378,2614,hachikuji,2017-03-11T00:20:03Z,"I do this validation inside `ProduceRequest`.  If there is more than one entry, we will raise `InvalidRecordException` before the request reaches `LogValidator`."
105516191,2614,hachikuji,2017-03-11T00:29:01Z,"Yes, I think we can."
105516431,2614,hachikuji,2017-03-11T00:31:41Z,"If fetching in `READ_UNCOMMITTED`, we do not compute the aborted transaction list. I thought `null` would be a good way to communicate this instead of an empty array, which would be ambiguous (are there no aborted transactions or did we just not compute them?)."
105516776,2614,hachikuji,2017-03-11T00:35:28Z,"Discussed offline, but for posterity, down-conversion on the broker only helps when the broker supports the new produce request version. However, the client also needs to support older versions of the produce request, which must use older versions of the message format. The difficulty is that there is a delay between the time that the producer starts building the batch and the time that we send the request, and we may have chosen the message format based on out-dated metadata. In the worst case, we optimistically chose to use the new message format, but found that the broker didn't support it, so we need to down-convert on the client before sending."
105698631,2614,hachikuji,2017-03-13T16:02:00Z,"Yeah, let's do this for now. We can add support in a follow-up if desired."
105722433,2614,hachikuji,2017-03-13T17:33:41Z,I guess the TODO should really go to the other constructor.
105724542,2614,hachikuji,2017-03-13T17:42:04Z,We'd need to increase a bit more to capture this one too. Increasing to 12 dropped two others.
105732368,2614,hachikuji,2017-03-13T18:11:01Z,I assume you are asking whether we can support this method directly using the `FileChannel`? I have implemented this in the latest commit.
105733027,2614,hachikuji,2017-03-13T18:13:44Z,"Yeah, that's fair. Let me see if we can move it to `ProduceRequest`."
105746262,2614,hachikuji,2017-03-13T19:09:50Z,"Yes, that sounds good."
105767662,2614,junrao,2017-03-13T20:48:30Z,"Since we already have 3 bits for compression, so leaving this as it is will be fine."
105767710,2614,junrao,2017-03-13T20:48:40Z,"I was trying to say it seems that we (should) never do writes through FileLogInputStream? If so, perhaps the implementation can just throw UnsupportedException?"
105770550,2614,hachikuji,2017-03-13T21:01:32Z,I guess the name is a little misleading. We're really reading here into the provided buffer. Maybe the name should be `readInto`?
105985925,2614,junrao,2017-03-14T18:09:16Z,"Since we are including the exact key/value length, perhaps we should exclude the max key/value length in  MAX_RECORD_OVERHEAD?"
105985975,2614,junrao,2017-03-14T18:09:30Z,"Perhaps name this to ProducerEpoch to distinguish it from the PartitionLeaderEpoch? If so, probably rename the variables in the class as well."
105985994,2614,junrao,2017-03-14T18:09:37Z,Should we pass in timestampDelta instead of timestamp?
105987708,2614,hachikuji,2017-03-14T18:16:04Z,`SimpleRecord` works for me. I may have actually had this in the code at one time or another.
106071613,2614,junrao,2017-03-15T02:02:41Z,It's a bit weird to call this maxUsableMagic when we take the min. Perhaps this and the method should be MinUsableMagic?
106071657,2614,junrao,2017-03-15T02:03:05Z,"Hmm, the method doesn't do exactly what the comment says. If desiredVersion is not usable, it doesn't seem to fall back to latest usable version. Is the comment incorrect?"
106071664,2614,junrao,2017-03-15T02:03:10Z,Maybe useful to include the min/max version in the error message?
106071700,2614,junrao,2017-03-15T02:03:30Z,"This can be a bit tricky. It seems that maxUsableMagic could change btw when size is computed in line 190 and in line 205. If the estimated size is not correct, we may not be able to append the record to the newly allocated recordsBuilder, which will cause the assertion in line 209 to fail."
106071703,2614,junrao,2017-03-15T02:03:34Z,annotate this with  @Override ?
106071718,2614,junrao,2017-03-15T02:03:46Z,LegacyRecordBatch => AbstractLegacyRecordBatch ?
106071726,2614,junrao,2017-03-15T02:03:52Z,Perhaps it's clearer to name this legacyRecord()?
106071817,2614,junrao,2017-03-15T02:04:48Z,"The comment in LogInputStream says it only does shallow iteration. However, here we are doing deep iteration. So, perhaps the comment needs to be changed?"
106071826,2614,junrao,2017-03-15T02:04:52Z,log logEntries => logEntries
106071834,2614,junrao,2017-03-15T02:04:56Z,Due => due
106071846,2614,junrao,2017-03-15T02:05:04Z,epoch => producerEpoch here and in the comment?
106071855,2614,junrao,2017-03-15T02:05:09Z,Could we call this producerEpoch to distinguish it from partitionLeaderEpoch?
106071869,2614,junrao,2017-03-15T02:05:18Z,"Hmm, is remaining() correct? It seems that limit() is right."
106071874,2614,junrao,2017-03-15T02:05:25Z,"Hmm, the last message has offset 3. So, shouldn't position be 4?"
106071891,2614,junrao,2017-03-15T02:05:39Z,"Could we print out whether this is a control record? And if so, perhaps print out more details about the control record?"
106071917,2614,junrao,2017-03-15T02:05:54Z,Could we print out leaderEpoch as well. Also epoch => producerEpoch?
106083563,2614,junrao,2017-03-15T04:21:17Z,"With this, it's possible to have no records added to the builder. Does the builder support that at close() time?"
106086048,2614,junrao,2017-03-15T04:58:44Z,It seems that the validation has been done in line 203?
106086067,2614,junrao,2017-03-15T04:58:56Z,"Hmm, why do we need to do this check? The batch could be on magic 2? Ditto on the ensureNotControlRecord() check below."
106239779,2614,hachikuji,2017-03-15T18:00:26Z,"I agree it reads a little weird, but I'm not sure `minUsableMagic` is accurate. The minimum usable magic would always be 0, but we are actually trying to get the maximum magic value that is supported across all brokers, which means taking the minimum of the max versions supported by all."
106243896,2614,hachikuji,2017-03-15T18:15:53Z,I'll clarify the comment. What I was trying to say is that `LogInputStream` only handles one level of iteration (it itself does not descend into compressed payloads).
106247667,2614,hachikuji,2017-03-15T18:30:11Z,"The `toArray` we're delegating to begins from `buffer.position()`, so `remaining()` seems correct. I'll add a test case to figure it out."
106249688,2614,hachikuji,2017-03-15T18:37:57Z,"Yes, we basically set the built records to `MemoryRecords.EMPTY` and reset the position in the underlying buffer."
106251349,2614,hachikuji,2017-03-15T18:44:06Z,This was intended to be temporary until we have the transactional portion of KIP-98. I didn't want to get too far into that validation in this patch if possible. Does that seem reasonable?
106257259,2614,hachikuji,2017-03-15T19:08:53Z,Good catch. It seems we need to use the same value throughout this logic. We always have the code to handle conversion later if we need it.
106258412,2614,hachikuji,2017-03-15T19:13:49Z,"Should have known not to try to slip something by Jun! Fixing this is actually a bit tricky since we need to retain the control record long enough to update the consumer's position. This requires a bit of refactoring in `Fetcher`, which I had intended to do in a follow-up. I'll see if there's an easier workaround for now."
106321514,2614,junrao,2017-03-16T01:23:48Z,"It seems that we should either use (1) 0 and buffer.limit() or (2) buffer.position() and buffer.remaining(), but not mixing the two? For (2), typically the buffer is in the write mode. In the read mode, (1) is probably what we want?"
106322699,2614,ijuma,2017-03-16T01:35:51Z,Seems like `offset` is ignored in the `else` case.
106322962,2614,ijuma,2017-03-16T01:38:41Z,"The reason why `0` is being passed is that `position` is called in the overloaded `toArray` method. So, this is actually doing `2` although it's not too clear."
106324045,2614,junrao,2017-03-16T01:50:08Z,"Could we also print out BaseOffset. BaseTimestamp, PartitionLeaderEpoch, BaseSequence?"
106342774,2614,hachikuji,2017-03-16T05:43:08Z,Good catch
106454133,2614,ijuma,2017-03-16T15:45:48Z,"Shouldn't we document the behaviour in 0.11.0 first? We can add a note about the previous behaviour at the end, but if someone sees these docs, they should be interested in 0.11.0."
106455746,2614,ijuma,2017-03-16T15:51:39Z,Nit: would `struct.hasField(TRANSACTIONAL_ID_KEY_NAME)` be a little better?
106455926,2614,ijuma,2017-03-16T15:52:15Z,Nit: would `struct.hasField(TRANSACTIONAL_ID_KEY_NAME)` be a little better?
106460700,2614,ijuma,2017-03-16T16:09:43Z,"Could we call `usableVersion.ensureUsable(version)` here? Also, maybe we can improve the exception message from that method to be as good as this one (i.e. include the supported versions)."
106461149,2614,ijuma,2017-03-16T16:11:24Z,"Can we mention the requested `apiKey` in the exception? Also, it would be good to document that the method throws an exception instead of just returning `null` if no `apiVersion` can be found."
106478107,2614,junrao,2017-03-16T17:17:01Z,Perhaps save this in a constant and reuse?
106478136,2614,junrao,2017-03-16T17:17:07Z,"Hmm, is headers nullable? If so, should we use -1 to represent null for consistency?"
106478167,2614,junrao,2017-03-16T17:17:14Z,This should now be named writeDefaultRecordHeader()? There are a bunch of places like that.
106478340,2614,junrao,2017-03-16T17:17:53Z,This should be appendLegacyRecord()? There are a few other places referencing OldLogRecord.
106525941,2614,ijuma,2017-03-16T20:41:31Z,Nit: `batch`.
106528086,2614,ijuma,2017-03-16T20:51:27Z,"Have we settled on using `magic` instead of `messageFormatVersion`? We may need to clean-up some Scala code later. Personally, I think `messageFormatVersion` is less `magic` (see what I did there). ;)"
106535449,2614,ijuma,2017-03-16T21:25:42Z,Should some of this GitHub response be included in the code comments?
106536471,2614,ijuma,2017-03-16T21:30:15Z,"To avoid this cast, we can add an overrides for `downConvert` in `FileRecords` and `MemoryRecords` that make the type more specific (where we know it cannot fail)."
106537132,2614,ijuma,2017-03-16T21:33:25Z,I think that makes sense for what it's worth.
106541553,2614,ijuma,2017-03-16T21:56:35Z,"We don't have to do this now, but I wonder if `RecordBatch` could simply be an abstract class and we could remove this one. Or we could wait until we move to Java 8, keep the interface and use default methods."
106542021,2614,ijuma,2017-03-16T21:59:19Z,You don't need to say `Beginning in 0.11.0` since that's the case for everything in this list.
106542403,2614,ijuma,2017-03-16T22:01:33Z,Is it worth saying something about `batch.size`? It seems that is what ensures that the batch will be of an appropriate size.
106543613,2614,ijuma,2017-03-16T22:09:03Z,Can we simplify this in a similar way as we simplified `NetworkClient`?
106546543,2614,ijuma,2017-03-16T22:26:41Z,Typo: `Unusuable`
106546942,2614,ijuma,2017-03-16T22:29:16Z,Is this just an optimisation or it's a correctness issue?
106570149,2614,junrao,2017-03-17T01:50:37Z,"entry => batch ? Also, the comment above still references entry."
106570158,2614,junrao,2017-03-17T01:50:45Z,logEntries => recordBatch? There are a few other places like that in this file.
106570162,2614,junrao,2017-03-17T01:50:51Z,convertLogEntry => convertRecordBatch?
106570181,2614,junrao,2017-03-17T01:51:05Z,We can only do this if headerValue.hasArray() is true? Ditto for the crc computation in computeChecksum() and in readFrom(). We can probably write a util for that.
106570196,2614,junrao,2017-03-17T01:51:22Z,Should we assert headerKey is not null?
106570214,2614,junrao,2017-03-17T01:51:26Z,LOG_ENTRY_OVERHEAD => RECORD_BATCH_OVERHEAD ?
106570219,2614,junrao,2017-03-17T01:51:31Z,"Instead of ""magic v2"", referencing magic()?"
106570227,2614,junrao,2017-03-17T01:51:35Z,logEntry => recordBatch ?
106570233,2614,junrao,2017-03-17T01:51:40Z,1 => named constant?
106570245,2614,junrao,2017-03-17T01:51:46Z,Could you add a bit more explanation on why the crc is useless?
106570254,2614,junrao,2017-03-17T01:51:50Z,buildEosRecord => buildDefaultRecord?
106570260,2614,junrao,2017-03-17T01:51:55Z,OneLogEntry => OneRecordBatch ?
106570281,2614,junrao,2017-03-17T01:52:09Z,"In validateMessagesAndAssignOffsets(), when validating the DefaultRecordBatch, should we also validate the the message count matches the actual number of messages in the array and the header count matches the actual number of headers?"
106570283,2614,junrao,2017-03-17T01:52:15Z,"Instead of 0, referencing the constant?"
106581113,2614,junrao,2017-03-17T04:15:14Z,"Ok, perhaps add a TODO comment for now?"
106581349,2614,junrao,2017-03-17T04:18:30Z,Could we include an error message to indicate that this is unexpected? Or could we throw an IllegalStateException instead?
106892521,2614,ijuma,2017-03-20T12:53:15Z,"The implementation of this method in `LegacyRecord` is:

```java
if (!isValid()) {
            if (sizeInBytes() < CRC_LENGTH)
                throw new InvalidRecordException(""Record is corrupt (crc could not be retrieved as the record is too ""
                        + ""small, size = "" + sizeInBytes() + "")"");
            else
                throw new InvalidRecordException(""Record is corrupt (stored crc = "" + checksum()
                        + "", computed crc = "" + computeChecksum() + "")"");
        }
```

I think you need something similar here."
106895148,2614,ijuma,2017-03-20T13:05:52Z,Nit: shouldn't we represent this as `Records => [Record]` instead of having `RecordsCount` as a separate line?
106896027,2614,ijuma,2017-03-20T13:10:21Z,"In the documentation above, we use `LENGTH` while here we use `SIZE`. It would be good to be consistent."
106896581,2614,ijuma,2017-03-20T13:13:16Z,Is it ever possible that we use `DefaultRecordBatch` with `magic == 0`?
106898316,2614,ijuma,2017-03-20T13:21:36Z,Is this UInt32 or Int32? The code does both. :)
106899598,2614,ijuma,2017-03-20T13:28:02Z,We should have a private method for reading the last offset delta. We are using `readUnsignedInt` in `lastOffset` and `getInt` here.
106900105,2614,ijuma,2017-03-20T13:30:31Z,"This should be UInt32, I believe."
106913341,2614,ijuma,2017-03-20T14:26:05Z,"Hmm, shouldn't we have more tests here? Looking at the tests for the legacy records, some ideas:

1. Tests like `SimpleRecordTest.*isValid*`, but for `DefaultRecordBatch`.
2. `LegacyRecordTest.testChecksum` would be useful to have as well.
3. `SimpleRecord.buildEosRecord` should be renamed and moved here.
4. Some tests involving compression? Or is the idea that `MemoryRecordsTest` and `MemoryRecordsBuilderTest` cover those? It may be worth adding some javadoc to the test classes to give an idea of the what they're intended to test."
106913783,2614,ijuma,2017-03-20T14:27:34Z,Nit: logRecord -> record.
106914828,2614,ijuma,2017-03-20T14:31:11Z,"I think we should rename this class `LegacySimpleRecordTest` (or something like that) and move the tests that are not for `LegacyRecord` elsewhere. Also, if there are tests that would make sense for `DefaultRecord`, we should port them."
106915417,2614,ijuma,2017-03-20T14:33:09Z,Should this be in `MemoryRecordsBuilderTest` (or  `MemoryRecordsTest`)?
106915947,2614,ijuma,2017-03-20T14:35:04Z,We should add a comment here.
106916671,2614,ijuma,2017-03-20T14:37:33Z,Nit: I would say Exactly-once as not everyone is familiar with the EOS terminology.
106917876,2614,ijuma,2017-03-20T14:41:50Z,Nit: would `struct.hasField(TRANSACTIONAL_ID_KEY_NAME)` be a little better?
106919175,2614,ijuma,2017-03-20T14:46:02Z,I think we should call the `validateRecords` method for all versions.
106919335,2614,ijuma,2017-03-20T14:46:32Z,"If we do this check, should we not do it for other versions `MAGIC_VALUE_V1` too?"
106919624,2614,ijuma,2017-03-20T14:47:25Z,"Instead of saying ""Version 3 and above"", should we just say ""Version $version""?"
106919757,2614,ijuma,2017-03-20T14:47:55Z,"I think we should call the `validateRecords` method for all versions. Also, why do we only do it on `toStruct` instead of the constructor?"
106920410,2614,ijuma,2017-03-20T14:50:06Z,Why was this moved to a separate line?
106922600,2614,ijuma,2017-03-20T14:57:51Z,"Maybe replace `unexpected` by `unknown`? Given the versioning, it is expected that new versions will be introduced eventually."
106929150,2614,ijuma,2017-03-20T15:20:29Z,"If you use `StandardCharsets.UTF_8`, you don't need the try/catch."
106929931,2614,ijuma,2017-03-20T15:22:48Z,Is there a reason why we don't just call `Crc32.crc32` here? Was it while doing some experiments with other implementations?
106933605,2614,ijuma,2017-03-20T15:36:00Z,"It's a bit odd that `LegacyRecord` is referenced here. If this is the same for legacy and default, we should move it to a shared class. Otherwise, it seems this logic should not live here any more."
106934435,2614,ijuma,2017-03-20T15:38:55Z,Shouldn't this pass the headers as well?
106934918,2614,ijuma,2017-03-20T15:40:29Z,I wonder if some of these constructors that are only used by tests should be static factory methods to avoid accidental usage from non-test code.
106935553,2614,ijuma,2017-03-20T15:42:47Z,"The formatting of this method doesn't seem to follow our Java convention (yes, I know, IntelliJ) since the `return` is in the same line as the `if`. Using `&&` is a concise alternative."
106967419,2614,hachikuji,2017-03-20T17:40:23Z,"An optimization I guess. We could construct a new `MemoryRecords` with the stored buffer, but I didn't see a good reason to."
106967704,2614,hachikuji,2017-03-20T17:41:30Z,"LOL, looks like my mind got stuck somewhere between typing ""unusual"" and ""unusable""."
107003151,2614,hachikuji,2017-03-20T20:09:52Z,I could go either way... I ended up favoring `magic` mainly because it gives more concise variable and method names.
107003332,2614,hachikuji,2017-03-20T20:10:39Z,I debated on this... I'm inclined to leave it for later since this is internal.
107022774,2614,ijuma,2017-03-20T21:34:16Z,"Note that this is only true before KIP-74. After KIP-74, this should never happen."
107026003,2614,ijuma,2017-03-20T21:50:52Z,Shouldn't we use the buffer position here?
107041677,2614,hachikuji,2017-03-20T23:27:01Z,Good point
107043636,2614,hachikuji,2017-03-20T23:42:06Z,Hmm... we should check with Magnus which case it was that he was accidentally using. Was it the old produce request version with the new format?
107044557,2614,hachikuji,2017-03-20T23:48:56Z,That's fair
107044757,2614,ijuma,2017-03-20T23:50:19Z,Is it right that `0` is used if there is none for this and `producerId`? The constants at the top of the file seem to indicate that it's `-1`?
107044853,2614,ijuma,2017-03-20T23:51:03Z,We should probably replace `message set` with `record batch` in the comments.
107045184,2614,ijuma,2017-03-20T23:53:34Z,`log overhead` -> `batch overhead`?
107045235,2614,ijuma,2017-03-20T23:53:59Z,`log entry` -> `record batch`
107045406,2614,ijuma,2017-03-20T23:55:21Z,"Wouldn't this be better as a top level interface? I think an inner interface would make sense if we called it `Mutable` only. As it is, the name stands well on its own and it makes usage a bit less verbose."
107047035,2614,ijuma,2017-03-21T00:08:17Z,The `shallow` should be removed. Maybe we need to do a search and replace for that string.
107048855,2614,ijuma,2017-03-21T00:22:43Z,That's a good point. Maybe we should just add a comment saying that we don't validate older versions because some clients rely on that.
107057682,2614,hachikuji,2017-03-21T01:50:20Z,I think we can remove these utilities. I didn't know about the methods already in `Crc32` until I started updating that class.
107057717,2614,hachikuji,2017-03-21T01:50:50Z,Agreed
107057751,2614,hachikuji,2017-03-21T01:51:12Z,You are my hero
107059245,2614,hachikuji,2017-03-21T02:08:33Z,"After thinking about this, it might be better to move the attribute logic into the record classes. We'll duplicate a little code, but it seems better to keep the usage of record attributes encapsulated."
107146211,2614,ijuma,2017-03-21T12:57:36Z,Nit: formatting (same as other case).
107146442,2614,ijuma,2017-03-21T12:59:00Z,"Nit, if you are importing everything in `Record`, then there's no reason to specify `RecordBatch` on its own, right?"
107146675,2614,ijuma,2017-03-21T13:00:13Z,Why are we not defaulting to `current` any more? Same for the other method.
107146794,2614,ijuma,2017-03-21T13:00:54Z,Do we need to have a method like this for message format 1?
107147049,2614,ijuma,2017-03-21T13:02:08Z,Nit: `logEntry` -> `record`?
107147536,2614,ijuma,2017-03-21T13:04:22Z,Why don't we use `logEntries` here and in other similar methods?
107147613,2614,ijuma,2017-03-21T13:04:43Z,Shall we call this `records`?
107148607,2614,ijuma,2017-03-21T13:09:43Z,"Unfortunately, there are a few magic values in these tests. How did you arrive at the correct values for the array fill and `SegmentBytesProp`?"
107148969,2614,ijuma,2017-03-21T13:11:14Z,"As you know, we have had a lot of issues with the cleaner in the past. And we don't have any system tests with compacted topics. Do you feel like we have enough coverage for all message format versions?"
107149933,2614,ijuma,2017-03-21T13:15:44Z,"It would be a bit clearer if we did:

```scala
records.map { case (key, value, timestamp) => new SimpleRecord(...) }
```"
107152276,2614,ijuma,2017-03-21T13:26:31Z,Nit: entry -> record.
107153794,2614,ijuma,2017-03-21T13:32:43Z,Nice catch.
107156070,2614,ijuma,2017-03-21T13:41:08Z,"If I understand correctly, you have two `flush` calls to ensure that we produce two batches, right? A few suggestions:
1. Add a comment
2. Rename the tests to make it clear that they're about record batches now, not records. 
3. Maybe we should do a `get` on the future in case it fails (in that case, we can probably remove the `flush()`)."
107157124,2614,ijuma,2017-03-21T13:45:19Z,"One more thing, we should consider changing the default to use the new format's overhead. At the moment it is:

`val MessageMaxBytes = 1000000 + MessageSet.LogOverhead`."
107157639,2614,ijuma,2017-03-21T13:47:20Z,"To avoid similar brittleness, would it be better to say something like `maxMessageSize - RECORD_BATCH_OVERHEAD - 24` (the latter being the key size)?"
107159115,2614,ijuma,2017-03-21T13:52:44Z,Should we be returning the current version in these stub returns?
107160387,2614,ijuma,2017-03-21T13:57:21Z,"Answering your question, this index is for the LZ4 checksum value as the comment above says. Maybe we should mention in the comment that it's not our message CRC."
107161317,2614,ijuma,2017-03-21T14:00:52Z,Why do we need the `flush`? We are waiting until the future completes already?
107161936,2614,ijuma,2017-03-21T14:03:08Z,`logEntries` -> `records`.
107162213,2614,ijuma,2017-03-21T14:04:15Z,Is it intentional that we want to wait for a send to complete before doing the next send? Is it related to how many batches we create?
107162877,2614,ijuma,2017-03-21T14:06:35Z,How we get `12`? Maybe worth adding a comment?
107163216,2614,ijuma,2017-03-21T14:07:51Z,"Hmm, can we not use a constant for the offset as we had before?"
107166963,2614,ijuma,2017-03-21T14:21:01Z,Maybe we should have a shared constant for the magic offset since it's the same for all formats?
107168486,2614,ijuma,2017-03-21T14:26:07Z,Maybe we want to update this comment?
107168520,2614,ijuma,2017-03-21T14:26:17Z,entry -> batch
107168876,2614,ijuma,2017-03-21T14:27:23Z,I was wondering about the fact that `baseOffset` is not the same as `firstOffset` for compacted topics. Do we care in cases like this?
107169714,2614,ijuma,2017-03-21T14:30:03Z,Seems like we need to update the scaladoc of this method. Why are we now using `maxTimestamp` instead of the first timestamp?
107170322,2614,ijuma,2017-03-21T14:32:15Z,logEntry -> recordBatch
107171955,2614,ijuma,2017-03-21T14:37:59Z,Could some of this logic be moved to `FetchRequest` like we did for the `ProduceRequest`? We could then also write a unit test to verify the behaviour (if we haven't already).
107173311,2614,ijuma,2017-03-21T14:42:54Z,`logEntryIteratorMap` -> `recordBatchesIteratorMap`
107173784,2614,ijuma,2017-03-21T14:44:22Z,Do we need to mention `RecordBatch` explicitly here?
107174475,2614,ijuma,2017-03-21T14:46:40Z,"Should we change the message to mention batch size instead of message size? Also, we may consider using `RecordBatchTooLargeException` (which already exists) although maybe later in case it may break things."
107175231,2614,ijuma,2017-03-21T14:49:08Z,"Nit: it may make sense to keep all the logic in one place. `shouldRetainMessage` is only called from this method, so is there any reason not to just add the first check there as well?"
107177315,2614,ijuma,2017-03-21T14:55:27Z,"We stated in `RecordBatch` that `baseOffset` remains the same after compaction. It would be good to specify the behaviour of other header fields like `lastOffset` and `maxTimestamp`. Furthermore, we should make sure to test the specified behaviour for these cases (i.e. compaction removes the first/last item in the batch, compaction removed the item with the max timestamp). Do we have these tests already?"
107180500,2614,ijuma,2017-03-21T15:05:51Z,Nit: `s` is not needed and the line below.
107181957,2614,ijuma,2017-03-21T15:10:49Z,haha
107183186,2614,ijuma,2017-03-21T15:15:06Z,`maybeBatch` or we can just remove the val altogether.
107183305,2614,ijuma,2017-03-21T15:15:30Z,logEntry -> recordBatch
107183565,2614,ijuma,2017-03-21T15:16:32Z,shallowLogEntry -> recordBatch
107183893,2614,ijuma,2017-03-21T15:17:43Z,entries -> batches
107183937,2614,ijuma,2017-03-21T15:17:54Z,We can remove the comment.
107184438,2614,ijuma,2017-03-21T15:19:43Z,"I don't understand this comment. What about compaction? Also, we handle compressed and uncompressed the same now, so maybe there's no point in mentioning `uncompressed`?"
107185244,2614,ijuma,2017-03-21T15:22:29Z,Nit: space after `=`.
107186719,2614,ijuma,2017-03-21T15:27:00Z,"It's a bit odd that we use all lowercase for the existing fields and camel-case for the new ones. Would it make sense to use just one style (camel-case seems better, but not sure about compatibility guarantees)?"
107187022,2614,ijuma,2017-03-21T15:28:05Z,"Seems like we have no JUnit tests for `DumpLogSegments` although the system tests do exercise it. I filed https://issues.apache.org/jira/browse/KAFKA-4928 so that we can address this, but worth keeping in mind that manual verification is needed in the meantime."
107193728,2614,ijuma,2017-03-21T15:50:46Z,"I removed this, the other `convert` method, `convertSize` and the tests that called these methods and the code compiled. Seems like they are unused and can be removed. We must make sure that the conversion cases are still tested before removing the tests that call these methods."
107198136,2614,ijuma,2017-03-21T16:05:51Z,Would it be clearer if we called this `outerRecord` or something?
107200862,2614,ijuma,2017-03-21T16:16:00Z,"We don't need to wrap `iterator`. The following makes it compile:

```text
     @Override
     public Iterator<Record> iterator() {
-        final Iterator<AbstractLegacyRecordBatch> iterator;
+        final Iterator<Record> iterator;
         if (isCompressed())
             iterator = new DeepRecordsIterator(this, false, Integer.MAX_VALUE);
         else
-            iterator = Collections.singletonList(this).iterator();
+            iterator = Collections.<Record>singletonList(this).iterator();

-        return new AbstractIterator<Record>() {
-            @Override
-            protected Record makeNext() {
-                if (iterator.hasNext())
-                    return iterator.next();
-                return allDone();
-            }
-        };
+        return iterator;
     }

     public static void writeHeader(ByteBuffer buffer, long offset, int size) {
@@ -266,7 +259,7 @@ public abstract class AbstractLegacyRecordBatch extends AbstractRecordBatch impl
         }
     }

-    private static class DeepRecordsIterator extends AbstractIterator<AbstractLegacyRecordBatch> {
+    private static class DeepRecordsIterator extends AbstractIterator<Record> {
         private final ArrayDeque<AbstractLegacyRecordBatch> batches;
         private final long absoluteBaseOffset;
         private final byte wrapperMagic;
@@ -335,7 +328,7 @@ public abstract class AbstractLegacyRecordBatch extends AbstractRecordBatch impl
         }

         @Override
-        protected AbstractLegacyRecordBatch makeNext() {
+        protected Record makeNext() {
     public static void writeHeader(ByteBuffer buffer, long offset, int size) {
@@ -266,7 +259,7 @@ public abstract class AbstractLegacyRecordBatch extends AbstractRecordBatch impl
         }
     }

-    private static class DeepRecordsIterator extends AbstractIterator<AbstractLegacyRecordBatch> {
+    private static class DeepRecordsIterator extends AbstractIterator<Record> {
         private final ArrayDeque<AbstractLegacyRecordBatch> batches;
         private final long absoluteBaseOffset;
         private final byte wrapperMagic;
@@ -335,7 +328,7 @@ public abstract class AbstractLegacyRecordBatch extends AbstractRecordBatch impl
         }

         @Override
-        protected AbstractLegacyRecordBatch makeNext() {
+        protected Record makeNext() {
             if (batches.isEmpty())
                 return allDone();
```

What do you think?
"
107226762,2614,hachikuji,2017-03-21T17:47:22Z,Guess that's the danger of using the optimize imports shortcut.
107264435,2614,hachikuji,2017-03-21T20:20:55Z,You are right. It seems unnecessary and the test works without it.
107268478,2614,hachikuji,2017-03-21T20:37:34Z,"Right. A lot of the testing around the max message size is obviously sensitive to how the messages are batched. If possible, I'd like to save refactoring of these test cases for a follow-up."
107268800,2614,hachikuji,2017-03-21T20:38:59Z,I think we can use `RecordBatch.MAX_RECORD_OVERHEAD`
107269975,2614,ijuma,2017-03-21T20:43:55Z,Sounds good.
107270196,2614,hachikuji,2017-03-21T20:44:52Z,I'm not sure we need a dependence on the offset here. Maybe we can just flips some arbitrary bits in the payload?
107272979,2614,ijuma,2017-03-21T20:56:40Z,"Ah, I understand what you did now. Maybe add a comment?"
107279979,2614,hachikuji,2017-03-21T21:27:11Z,"I think the invariant we are trying to maintain is that the offsets added to the index are strictly increasing, so using the base offset in place of the first offset seems sufficient and avoids the decompression needed to find the actual first offset."
107281528,2614,hachikuji,2017-03-21T21:34:02Z,"The behavior is not actually different. Recall that this is ""shallow"" iteration in the old code. The timestamp we are accessing is actually the max timestamp when the record is viewed as a batch. In the uncompressed case, the ""batch"" has just a single record so the timestamp is the same as its max timestamp. In the compressed case, the timestamp is the max timestamp of all records in the batch. We preserve the old behavior and adopt the old compressed behavior for the new message format. I can update the comment to clarify this."
107283016,2614,hachikuji,2017-03-21T21:40:57Z,"It is possible, but I am not sure conversion is something we want to hide too deeply. Maybe we can consider this for a follow-up?"
107286163,2614,hachikuji,2017-03-21T21:56:43Z,"This is a good point. The current usage of `RecordBatchTooLargeException` is to indicate when a batch exceeds the log segment size, but this only really made sense for the old format without compression. It might be worthwhile considering in a follow-up whether we have need to reuse this error for other purposes. For now, I'll update the message."
107290844,2614,hachikuji,2017-03-21T22:22:17Z,"See MemoryRecordsTest.filterTo. The test is kind of a bear, but it covers a large number of cases. I'll update the doc for `lastOffset` and `maxTimestamp`. "
107292917,2614,hachikuji,2017-03-21T22:34:27Z,"Yeah... debated on this. All lower-case is hard to read (e.g. ""partitionleaderepoch""), but I wasn't sure about compatibility. "
107294949,2614,ijuma,2017-03-21T22:46:13Z,"OK, maybe we can consider using camel-case for the existing fields in a follow-up."
107295245,2614,ijuma,2017-03-21T22:48:12Z,"Follow-up is fine. Generally, I think it's a good pattern to keep the logic in `KafkaApis` simple since it makes it harder to test."
107296984,2614,hachikuji,2017-03-21T22:58:41Z,"Yeah, that's a good idea."
107297922,2614,hachikuji,2017-03-21T23:04:56Z,Good call. Seems better.
107298677,2614,hachikuji,2017-03-21T23:09:57Z,"Yes, I agree. There are probably ways we can do it so that we keep so visibility into the fact that the down-conversion is happening behind the scenes (maybe just comments)."
107316444,2614,ijuma,2017-03-22T01:37:53Z,"I was going to suggest removing this, but was unsure. Happy that you did it anyway. :)"
107318058,2614,ijuma,2017-03-22T01:55:28Z,"My bad, you're right. Thanks for updating the comment."
107327673,2614,ijuma,2017-03-22T03:43:37Z,"Copied and pasted from the method comment, so we can delete it, I think."
107328065,2614,ijuma,2017-03-22T03:48:46Z,Was this never required? It seems like we don't do it any more.
107334914,2614,junrao,2017-03-22T05:25:37Z,Move this to previous line?
107334929,2614,junrao,2017-03-22T05:25:48Z,logEntries => recordBatch?
107334933,2614,junrao,2017-03-22T05:25:51Z,the the => the
107398114,2614,ijuma,2017-03-22T12:14:51Z,Do you know why we had 2 builders?
107412161,2614,ijuma,2017-03-22T13:29:38Z,Did we decide on this one?
107415960,2614,ijuma,2017-03-22T13:45:00Z,Nice that we avoid the overhead of the streams here.
107425811,2614,ijuma,2017-03-22T14:22:21Z,"Yes, I agree that it's worth adding a brief comment explaining until we fix the issue."
107441751,2614,ijuma,2017-03-22T15:15:15Z,"Is it OK that we don't check the magic here, but we do for the append that takes a legacy record?"
107494215,2614,hachikuji,2017-03-22T18:23:25Z,Looks like I forgot this one. I'll fix.
107498549,2614,hachikuji,2017-03-22T18:39:56Z,"Yeah, that's a good question. I'm not sure there's a good reason for the check in the `LegacyRecord` case. I think initially we were doing something like copying the bytes directly, so it may have made more sense before."
107499252,2614,hachikuji,2017-03-22T18:42:53Z,"Oh, I guess we still do the memory copy in this case. The question is whether we should?"
107510450,2614,hachikuji,2017-03-22T19:32:49Z,I think we were verifying the auto-incrementing behavior. I will verify that we have both cases covered.
107539054,2614,ijuma,2017-03-22T21:34:50Z,This method is only used by `ByteBufferMessageSet` (i.e. Scala clients) so it seems OK to favour consistency over optimisation in this case.
107768943,2614,ijuma,2017-03-23T19:58:09Z,Why doesn't assertEquals work here?
107769018,2614,ijuma,2017-03-23T19:58:34Z,Would it be worth adding some timestamp assertions as well?
107773329,2614,ijuma,2017-03-23T20:19:57Z,"`non-compressed` should be removed, right?"
107773377,2614,ijuma,2017-03-23T20:20:11Z,`verifyConvertedBatches`
107774031,2614,hachikuji,2017-03-23T20:23:08Z,"We only convert messages when necessary. So if we down-convert a batch to version 1, and we have messages which are magic 0, we won't up-convert them to 1."
107774243,2614,hachikuji,2017-03-23T20:24:03Z,Ack. May as well.
107839904,2614,junrao,2017-03-24T06:02:41Z,"Actually, if CRC only covers from Attributes to the end, do we really need to switch PartitionLeaderEpoch and CRC?"
107886287,2614,ijuma,2017-03-24T11:52:21Z,"It's true that the change is not strictly needed. The reasoning for the switch is that this way the CRC includes everything that follows it (same as for V0 and V1 formats). In V0 and V1, the offset and length are before the CRC and are not included in the computation. In V2, the offset, length, partitionLeaderEpoch and magic are before the CRC and not included in the computation.

Given that, do you think the benefit is worth the switch?"
107915842,2614,junrao,2017-03-24T14:33:18Z,"Earlier, Jason's proposal is to put CRC to the very end after records and covers everything from magic. If we do that, the switch is not ideal, but makes sense. If CRC is still at the front, then it seems leaving CRC in its current location causes less confusion."
107921226,2614,ijuma,2017-03-24T14:55:05Z,"If we leave the CRC in the same position as in V0 and V1, would you still change the computation to be from attributes to the end? A couple of alternatives (each with pros/cons):

1. Include magic in the CRC computation, but not partitionLeaderEpoch (con: we'd have to call `Checksum.update` twice instead of once).
2. Compute the CRC from magic to the end (as it was before), but with PartitionLeaderEpoch always assumed to be -1. This makes the computation simple on the producer side, but it's a bit tricky after the PartitionLeaderEpoch has been set. This is a bit similar to how TCP includes the checksum field in the checksum computation, but assumes it to be always 0 (to avoid the chicken and egg problem)."
107945087,2614,hachikuji,2017-03-24T16:30:20Z,"I think we can swap the CRC and leader epoch fields in the current version, but keep the scope of the CRC unchanged. In other words: 
```
 * RecordBatch =>
 *  BaseOffset => Int64
 *  Length => Int32
 *  CRC => Uint32 (covers everything beginning with Attributes)
 *  Magic => Int8
 *  PartitionLeaderEpoch => Int32
 *  Attributes => Int16
 *  LastOffsetDelta => Int32
 *  BaseTimestamp => Int64
 *  MaxTimestamp => Int64
 *  ProducerId => Int64
 *  ProducerEpoch => Int16
 *  BaseSequence => Int32
 *  Records => [Record]
```
It's a little odd that we change the scope of the CRC in this format, but perhaps it's less odd than changing the location of the field entirely."
107954369,2614,hachikuji,2017-03-24T17:13:11Z,"This is a tough one. It's nice being consistent with the old format to some extent, but clients would still need to look at the magic byte first to figure out how to validate the CRC, so maybe the consistency win is marginal. The main advantage of the format in the current patch is that it's clear at a glance what is covered by the CRC, so it seems easier to explain."
107982462,2614,ijuma,2017-03-24T19:27:43Z,Discussed offline and Jun said he was OK with leaving as is.
108027606,2614,lindong28,2017-03-25T04:13:03Z,Bug: we should use `partitionResponseHeader` instead of `partitionResponse` to get the response. I guess this is not discovered by any test because the current patch always has `last_stable_offset = -1L`?
108040126,2614,ijuma,2017-03-25T16:48:09Z,"Thanks for catching this @lindong28. This field won't be used until the transactional code lands. I submitted a PR with tests:

https://github.com/apache/kafka/pull/2737"
139199834,3874,ijuma,2017-09-15T16:59:14Z,I assume this is not intentional :)
139200069,3874,lindong28,2017-09-15T17:00:26Z,@ijuma I made this to skip findbug and speedup test execution while I am still debugging my test :)
139200184,3874,lindong28,2017-09-15T17:00:59Z,@ijuma The code is almost ready. I will let you know once it is done today.
139200865,3874,tombentley,2017-09-15T17:04:32Z,"`renamedDir` was instantiated in the line above, it can't possibly `== dir`. I think you mean `.equals()`"
139204236,3874,tombentley,2017-09-15T17:20:39Z,But if replication factor == 1 we've lost the log? From the description you give it sounds like we lose track of the state of the renaming because the only information to go on are the names of the files in the directory. Could this be solved by having a transaction log file for the deletion in the same directory?
139204483,3874,ijuma,2017-09-15T17:21:46Z,`==` in Scala is _not_ reference equality.
139206306,3874,tombentley,2017-09-15T17:29:35Z,Why is this necessary? A comment to explain why would be useful.
139206895,3874,tombentley,2017-09-15T17:32:07Z,Doh! Thank you.
139276166,3874,lindong28,2017-09-16T02:26:05Z,@tombentley Thanks for the comment. The patch you saw was fully ready for review. The patch has been updated now with better code.
139276169,3874,lindong28,2017-09-16T02:26:25Z,I will comment on this later.
139527305,3874,junrao,2017-09-18T20:11:53Z,Perhaps we can just get rid of the return value since it always returns Errors.NONE?
139529322,3874,junrao,2017-09-18T20:20:37Z,"I thought if destinationDir is Any, we will also cancel any existing disk movement?"
139531084,3874,junrao,2017-09-18T20:27:42Z,There is one otherwise in 1) and another in 2). It's very not clear which is matched to which if.
139532975,3874,junrao,2017-09-18T20:35:32Z,It seems that 1) and 2) are in reverse order of the code below.
139534077,3874,junrao,2017-09-18T20:40:13Z,Should logManager.updatePreferredLogDir() only remember the preferred log dir if the log doesn't exist?
139534824,3874,junrao,2017-09-18T20:43:15Z,Should we handle the case when destinationDir is Any?
139536481,3874,junrao,2017-09-18T20:49:55Z,We use ( for .filter and { for .map. We probably want to be consistent.
139539863,3874,junrao,2017-09-18T21:03:29Z,"Probably simpler to say ""if the replica is not created or is offline""."
139554816,3874,junrao,2017-09-18T22:16:43Z,It seems that we can just combine the logging here and that in line 1398?
139555164,3874,junrao,2017-09-18T22:18:40Z,"Could we add a comment to explain futureLogs a bit? Also, for consistency, should we rename logs to sth like primaryLogs or currentLogs?"
139555761,3874,junrao,2017-09-18T22:22:13Z,We probably want to do foreach{ topicPartition => ...} to be consistent?
139557211,3874,junrao,2017-09-18T22:31:11Z,Perhaps offlineDirs could be a Set?
139558508,3874,junrao,2017-09-18T22:39:37Z,Should we add isFuture to the param list in the comment above?
139558693,3874,junrao,2017-09-18T22:40:43Z,"Hmm, it seems that every time we truncate a log, we want to truncate the future log to the same offset? Ditto for truncateFullyAndStartAt()."
139559651,3874,junrao,2017-09-18T22:46:53Z,Should we add the missing param topicPartition and isFuture in the comment above?
139560220,3874,junrao,2017-09-18T22:50:27Z,New params missing in the comment above.
139562965,3874,junrao,2017-09-18T23:08:38Z,It seems that this should be an IllegalStateException since this is not expected?
139563311,3874,junrao,2017-09-18T23:10:55Z,"If we check the preferredLogDir when it's added, it seems this should also be an IllegalStateException."
139566414,3874,junrao,2017-09-18T23:33:03Z,typo direcotory
139566677,3874,junrao,2017-09-18T23:35:00Z,"We probably want to use consistent naming in comments and variables(e.g., primary/secondary or current/future or sth else.)"
139567451,3874,junrao,2017-09-18T23:40:32Z,"Could we move in the following sequence to avoid this corner case issue?
1. Rename FutureLog to log.completed.
2. Rename CurrentLog to log.deleted.
3. Rename log.completed to log.

This way, if we fail at any step, we will have either log.completed or log to recover from. We probably also don't need the logic to rename back in the failure case. We just need some extra logic in loadLogs()."
139572154,3874,junrao,2017-09-19T00:16:49Z,Should we include futureLogs in logsByTopicPartition too?
139572695,3874,junrao,2017-09-19T00:22:02Z,Could we use named variables instead of _._1 and _._2?
139576864,3874,junrao,2017-09-19T01:03:50Z,"This doesn't seem to be a complete sentence. Are we missing ""when"" before ""the future""?"
139578305,3874,junrao,2017-09-19T01:19:00Z,"The KIP says using zero-copy to move data btw disks, but this is not. We probably can file a separate followup jira to revisit whether such an optimization is worth doing."
139579858,3874,junrao,2017-09-19T01:35:23Z,"Hmm, this doesn't look right. It seems that we want to use the future replica's latest epoch to build the LeaderEpochRequest."
139582141,3874,junrao,2017-09-19T01:58:07Z,We probably want to use current/future replica instead of leader/follower.
139812138,3874,junrao,2017-09-19T20:42:26Z,ReplicaFetcherManager probably needs to be changed accordingly?
139834520,3874,junrao,2017-09-19T22:28:37Z,"After we do the swap, there is no checkpoint file for log cleaner. This means that we need to clean the compacted topic from the beginning, which is not ideal. One option is to copy the checkpoint to the destination log dir at the beginning of the data movement."
139835264,3874,junrao,2017-09-19T22:33:34Z,"Hmm, do we need this? It seems that any replica being moved across disks should be subject to throttling. The reason that we have this for inter-broker replication throttling is to avoid throttling replicas already in sync."
139835968,3874,junrao,2017-09-19T22:37:48Z,typo betwee
139836442,3874,junrao,2017-09-19T22:40:29Z,"We have another quota for log cleaning, which is also intra broker. To avoid confusion, would it be better to name this sth like ""replication.across.dirs.throttled.rate""?"
139836798,3874,junrao,2017-09-19T22:42:41Z,"Hmm, this wasn't in the KIP. Do we need this at the topic level?"
139843596,3874,junrao,2017-09-19T23:32:07Z,"To make it clear, would it be better to rename assignedReplicas to replicasAssignedToFutureDir?"
139844894,3874,junrao,2017-09-19T23:42:00Z,Does log need to be var?
139846597,3874,junrao,2017-09-19T23:55:45Z,"Hmm, why do we need to remove newOfflinePartitions from replicaAlterDirManager?"
139849668,3874,junrao,2017-09-20T00:19:58Z,"Hmm, the issue with this approach is that if one thread has no partition left and another thread has multiple partitions, we can't let the former to pick up the load from the latter. An alternative is to put all partitions in a queue and let AlterReplicaThreads pick up one partition at a time from the queue. This will improve the thread utilization."
139851404,3874,lindong28,2017-09-20T00:32:57Z,Sure. I will update the patch to get rid of the return value in this method.
139852351,3874,lindong28,2017-09-20T00:41:27Z,"Ah I forgot this. I just checked the KIP-113 wiki and it does say that broker will cancel existing movement if ""any"" is specified as the destination log directory.

I am wondering if we should to make the cancellation operation more explicit by saying, if ""cancel"" is specified as the destination log directory, the existing movement of this replica should be canceled. The issue with the existing design in the wiki is that user can not distinguish between ""don't care"" and ""cancel existing movement"". What do you think?"
139852501,3874,lindong28,2017-09-20T00:42:38Z,Yeah it is more readable to reorder the comment. I will change it as suggested. Thanks.
139854278,3874,junrao,2017-09-20T00:59:03Z,"Yes, explicitly modeling cancellation will be better. Perhaps we can change ALTER_REPLICA_DIR_REQUEST to sth like the following.

AlterReplicaDirRequest => [MovedPartitions] [CancelledPartitions]

MovedPartitions => logDir [topic [partitions]]
CancelledPartitions => [topic [partitions]]

With that, I am not sure we still need to support Any in logDir anymore."
139854818,3874,lindong28,2017-09-20T01:04:35Z,"I have considered the suggested approach. I think the current approach may be simpler.

- If we only remember the preferred log dir when the log doesn't exist, then in order to create the future log in the destination directory, the destination log directory needs to be passed to both `Partition.getOrCreateReplica(...)` and `logManager.getOrCreateLog()`, thus adding one more parameter to each method. 

- In addition, the suggested approach means that there will be two different ways that destination log directory is passed to LogManager, i.e. `LogManager.getOrCreateLog(...)` and `LogManager.updatePreferredLogDir(...)`. This makes the logic a bit more complicated than the current patch.

On the other hand, I think we only need to do `logManager.updatePreferredLogDir()` if neither the current log nor the future log is in the user-specified log directory. I will update the patch to do it. What do you think?"
139855447,3874,lindong28,2017-09-20T01:11:06Z,"My bad.. I have updated the comment as shown below:

```
// If the destinationLDir is different from the current log directory of the replica:
// - If there is no offline log directory, create the future log in the destinationDir (if it does not exist) and
//   start ReplicaAlterDirThread to move data of this partition from the current log to the future log
// - Otherwise, return KafkaStorageException. We do not create the future log while there is offline log directory
//   so that we can avoid creating future log for the same partition in multiple log directories.
```"
139855723,3874,lindong28,2017-09-20T01:14:06Z,"In the current patch, `Any` will be read and filtered by `ReassignPartitionsCommand` such that broker will not see or handle `any` as the destination log directory. This may change if we want to allow user to cancel ongoing movement. I will reply to the other command after thinking through this."
139855851,3874,lindong28,2017-09-20T01:14:55Z,Sure. I have updated the patch to use `}` consistently.
139855941,3874,lindong28,2017-09-20T01:15:47Z,Good point. I have updated the comment as suggested.
139856155,3874,lindong28,2017-09-20T01:17:43Z,I think `replicaAlterDirManager` will only be able to fetch data from source log of a partition if the source log of the partition is on an offline log directory. Thus newOfflinePartitions should be removed from replicaAlterDirManager. Does this make sense?
139856530,3874,junrao,2017-09-20T01:21:52Z,"Yes, that makes sense."
139856926,3874,lindong28,2017-09-20T01:26:19Z,"Sure. I have updated the patch to combine these two statements and the statement in line 1401 into this:

```
info(s""Broker $localBrokerId stopped fetcher for partitions ${newOfflinePartitions.mkString("","")} and stopped moving logs "" +
     s""for partitions ${partitionsWithOfflineFutureReplica.mkString("","")} because they are in the failed log directory $dir."")
```"
139856929,3874,junrao,2017-09-20T01:26:21Z,Got it. Make sense. Thanks.
139858472,3874,lindong28,2017-09-20T01:42:28Z,Sure. I have fixed this as suggested. I will pass over the entire patch later to see if this needs to be fixed in anywhere else.
139858629,3874,lindong28,2017-09-20T01:44:04Z,Sure. I have updated the patch to use Set.
139859230,3874,lindong28,2017-09-20T01:50:05Z,You are right. I have updated the patch to document `isFuture` for all methods in LogManager as  appropriate.
139859392,3874,lindong28,2017-09-20T01:51:53Z,Sure. I have updated the patch to document parameters of this method.
139859579,3874,lindong28,2017-09-20T01:53:57Z,Sure. I have updated the patch to document all parameters of this method.
139859792,3874,lindong28,2017-09-20T01:56:03Z,Sure. I have updated the patch to use IllegalStateException here.
139859805,3874,lindong28,2017-09-20T01:56:11Z,Good point. I have updated the patch to use IllegalStateException here.
139868382,3874,lindong28,2017-09-20T03:30:21Z,"Good point regarding the cleaner checkpoint. I am wondering if it may be better to simply pause log cleaner for all those partitions that are being moved to other log directories. 

This approach may have two advantages over the suggested approach of saving copying the checkpoint file at the beginning of the data movement:

- The broker performance is better than the suggested approach. With the suggested approach, broker needs to clean the data after the checkpoint twice. It is better to only do log cleaning after the partition has been moved to the destination log directory.

- The implementation is simpler than the suggested approach. With the suggested approach, the offset for the same partition will be recorded twice in two log directories. And we will need to have more logic in LogCleanerManager.allCleanerCheckpoints() to differentiate between checkpoint of primary log and the checkpoint of the future log. And we probably need to take more care to keep the checkpoints across log directories consistent if e.g. log renames during swap phase failed.

The only concern may be that the size of the compacted partition in the source log directory may be larger than if it is compacted. It is only a concern if replica movement takes a long time. And if this is the case, we probably already have problem with the size of the future log in the destination log directory anyway. What do you think?
"
139870056,3874,lindong28,2017-09-20T03:51:41Z,"`Any` is only used in the json file that is provided to the `ReassignPartitionsCommand`. This is necessary in the case that user only wants to specify log directory for one out of three replicas for a given partition. In that case the `log_dirs` field in the json file will be something like `[""any"", ""/path/to/logdir"", ""any""]`.

Also note that `ReassignPartitionsCommand` will filter out the `any` before it constructs AlterReplicaDirRequest. Thus `any` will not be specified as log directory in AlterReplicaDirRequest and broker does not need to understand this constant.

After more thinking, I think we probably don't need to provide a constant for user to cancel ongoing reassignment. There are two use-case for cancelling replica movement. One use-case is that user wants the replica to stay in the current log directory and he/she already knows the absolute path of the current log directory. In this case user can simply reassign partition again using the current log directory as the destination log directory. ReplicaManager will stop replica movement as appropriate.

The other use-case is that user wants to save broker IO by stop moving replica. In this case it is probably better to use quota to throttle replica movement. And user can also use either `kafka-log-dirs.sh` or `AdminClient.describeReplicaLogDir` to read the current log directory (and lag) of the partitions that are being moved, and reassign replica again with the current log directory as the destination log directory.

Thus it seems that we don't need to support explicit cancellation. Also, given that we are on a tight schedule to put this patch in 1.0 release, it may be better to try to reduce big changes to the patch unless it is necessary for the JBOD feature.

"
139875419,3874,lindong28,2017-09-20T04:56:06Z,Thanks. I have updated the patch to fix it.
139875907,3874,lindong28,2017-09-20T05:03:18Z,Good point. I have updated the patch to consistently use current/future in the comment everywhere in this patch. I verified this by searching for primary in the diff.
139876330,3874,lindong28,2017-09-20T05:08:49Z,This method is only used in test. We can not naively include futureLogs in logsByTopicPartition because it returns a map with TopicPartition as key. I have updated the patch to remove this method and replaced its use in test with LogManger.getLog().
139876533,3874,lindong28,2017-09-20T05:11:11Z,Sure. It is fixed now.
139878127,3874,lindong28,2017-09-20T05:29:13Z,My bad. It is fixed now.
139878292,3874,lindong28,2017-09-20T05:31:07Z,Yes. We should consider zero-copy. We probably don't have time to investigate it in this patch due to the 1.0 release deadline.
139878466,3874,lindong28,2017-09-20T05:32:42Z,"Ah, my bad.. It is fixed now."
139879142,3874,lindong28,2017-09-20T05:39:57Z,I maybe wrong here because I have not read through the KIP wiki of this epoch. The idea here is that ReplicaAlterDirThread will always copy&past the epoch from the current replica to the future replica of this partition. Thus it is safe to simply read the epoch and epoch offset from the current replica's cache. Did I miss something here?
139879604,3874,lindong28,2017-09-20T05:44:50Z,"Yeah I have thought about this problem. The thing is that it requires change in the AbstractFetcherManager to address this issue, or we need to create a new manager class for ReplicaAlterDirThread. The code will be more complicated with either solution.

Currently the replica is assigned to threads using AbstractFetcherManager.getFetcherId(...). Suppose the hash function is good, it should roughly evenly balance the partition across the available ReplicaAlterDirThread. Maybe we can have a follow up JIRA to optimize this if the load imbalance across threads turn out to be an issue. Does this sound OK?"
139879668,3874,lindong28,2017-09-20T05:45:30Z,My bad. Thanks for catching this. It is fixed now.
139879987,3874,lindong28,2017-09-20T05:48:45Z,Ah.. It is fixed now.
139880505,3874,lindong28,2017-09-20T05:54:47Z,"Is `log.cleaner.io.max.bytes.per.second` the config for throttling log cleaning? I am not sure it is easy to be confused with `intra.broker.replication.throttled.rate`. The former uses `log.cleaner` in the name thus we know it is for log cleaner. The latter uses `replication` in the name and therefore we know it is for replication within the broker. Did I miss something here?
"
139881125,3874,lindong28,2017-09-20T06:00:27Z,"Yeah it wasn't in the KIP.. It seems useful and reasonable to include in this patch. I can document this sensor in the KIP-113 wiki.

I think it may be useful to have it as topic level for debug purpose. But I am not strong on this. I have updated the patch to only record it for `allTopicsStats`."
139881570,3874,lindong28,2017-09-20T06:05:15Z,Sure. I have renamed variables in this method and in reassignPartitions() as appropriate.
139882608,3874,lindong28,2017-09-20T06:14:48Z,It needs to be var because `Partition.maybeDeleteAndSwapFutureReplica()` needs to do `replica.log = futureReplica.log`. Alternative we can replace the current replica with the future replica in Partition.allReplicasMap. But that means we need to copy states such as lastFetchTimeMs from the current Replica to the future Replica. And it takes extra care to maintain this when we add new state in the Replica in the future. Thus I choose the current approach because it is simpler.
139883108,3874,lindong28,2017-09-20T06:18:41Z,"I have replied to the other comment. I think we probably don't need to change AlterReplicaDirRequest or ReassignPartitionsCommand. ReplicaManager will not need to handle the case when destinationDir is Any because AlterReplicaDirRequest is not expected to specify ""any"" as log directory."
139884242,3874,lindong28,2017-09-20T06:28:16Z,"Sure. I added the following comment and renamed `logs` to `currentLogs`.

```
// Future logs are put in the directory with ""-future"" suffix. Future log is created when user wants to move replica
// from one log directory to another log directory on the same broker. The directory of the future log will be renamed
// to replace the current log of the partition after the future log catches up with the current log
```"
139885668,3874,lindong28,2017-09-20T06:40:07Z,"My gut feel is that the logic is harder to be correct if we add more suffix and more rename steps.

For example, the following sequence of events may happen with the suggested approach:

- FutureLog is successfully renamed to log.completed.
- CurrentLog can not be renamed due to log directory failure.
- log.completed is renamed to log. If we do not rename it to log now, then when the broker starts with CurrentLog still offfline, broker will forget about CurrentLog and will need to rename log.completed to log
- Now broker restarts again with CurrentLog online. Broker sees two copies of log for the same partition. It will be hard for broker to decide which one to use if the log has been truncated when CurrentLog is still offline.

I think the current approach has simpler with less suffix. It will cause the replica to be deleted in very rare scenario. And even then the broker should be able to recover from this as long as RF > 1. If RF = 1, Kafka can not ensure no data loss anyway due to disk failure.

"
139891303,3874,lindong28,2017-09-20T07:15:24Z,"Currently the the future replica will only be written by the ReplicaAlterDirThread. And the current log will only be written by ReplicaFetcherThread and KafkaRequestHandler. The ReplicaAlterDirThread is responsible for fetch data from current log to the future log in the same way that ReplicaFetcherThread fetches data from leader to follower. 

The advantage of this design is that 1) the design is simpler because ReplicaAlterDirThread uses essentially the same logic as ReplicaFetcherThread; and 2) most new logic for intra-broker replication is handled by ReplicaAlterDirThread and ReplicaFetcherThread won't need to worry about the future log.

In order to address this problem cleanly while still keep the pattern described above, I have updated the patch so that logManager.truncateTo() will always be called from partition.truncateTo(). And the same for truncateFullyAndStartAt(). In partition.truncateTo() will grab the read lock `partition.leaderIsrUpdateLock` before truncating the log.

With the use of partition.leaderIsrUpdateLock, the current patch ensures that when `partition.maybeDeleteAndSwapFutureReplica()` calls `logManager.deleteAndSwapFutureLog`, it is guaranteed that `replica.logEndOffset == futureReplica.logEndOffset` and no thread can truncate or write the log because all write operation will need to grab `leaderIsrUpdateLock`. Therefore, if the ReplicaFetcherThread truncates the log to an offset smaller than the log end offset of the future log, it is guaranteed that ReplicaAlterDirThread() will receives OffsetOutofRangeException and truncate the future log as well.

Does this make sense?

"
139891823,3874,lindong28,2017-09-20T07:18:43Z,"@junrao Just to double check, are you suggesting that we don't need `IntraBrokerReplicationThrottledReplicasProp` and should assume that `IntraBrokerReplicationThrottledRateProp` is applies to all replicas? I think it makes sense. I will update the patch after confirmation."
140006970,3874,ijuma,2017-09-20T15:34:42Z,"Seems like we made a bit of a mistake with the existing throttling configs in that they don't specify the unit. The log cleaner setting gets that right. Also, it seems like `replication` should be reserved for data transfer between two replicas. Are there two replicas here or is this just a case of copying data from one log dir to another?"
140017132,3874,junrao,2017-09-20T16:08:15Z,"Yes, that's not a bad idea. We could pause log cleaning during disk movement and copy the cleaning point after the movement completes."
140017691,3874,junrao,2017-09-20T16:10:29Z,"Yes, we could make it simple for now that the only way to cancel is to specify the source log dir in AlterReplicaDirRequest."
140021307,3874,junrao,2017-09-20T16:24:44Z,"After a power failure, it's possible for the future replica to have more data than the current replica after recovery. New data could then be appended to the current replica before the future replica resumes replicating. You could have data like the following.

current replica:
offset:  0      1     2 
epoch:  1     1     2
value :  v1    v2   v4

future replica:
offset:  0      1     2 
epoch:  1     1     1
value :  v1    v2   v3

To reconcile the replicas, you want to use future replica's latest epoch (i.e. 1) to find the offset of the last offset in that epoch in the current replica so that the future replica can get rid of v3 and copy v4 over.
"
140023208,3874,junrao,2017-09-20T16:32:18Z,"I was thinking that longer term, it may be better to have a single quota that covers all I/Os within a broker (including log cleaner, moving data across disks, etc). In that case, a config name with ""intra.broker"" may be more appropriate. However, if we use that name for moving data across disks, it may make future naming a bit harder."
140023952,3874,junrao,2017-09-20T16:35:04Z,"If we enable replication quota, the quota metric already tracks the byte rate across all moving partitions. So this seems redundant?"
140029825,3874,junrao,2017-09-20T16:58:12Z,"Hmm, does it guarantee that? After the current replica truncates the data, new data could have been written to the current replica. When the future replica fetches from the current replica again, it may have seen the new data and therefore won't receive OffsetOutofRangeException. This will potentially lead to inconsistent data between the future and the current replica."
140030015,3874,junrao,2017-09-20T16:58:55Z,"Yes, that's my suggestion."
140030170,3874,lindong28,2017-09-20T16:59:40Z,Good point. I didn't consider the scenario that the future log may be ahead of the current log. I will updated the patch to use the latest epo of the future replica here.
140031247,3874,lindong28,2017-09-20T17:04:23Z,Thanks. I have updated the patch as suggested.
140032664,3874,lindong28,2017-09-20T17:10:02Z,You are right. We no longer need this metric now that we will apply quota to all topic partitions. I have removed this metric from the patch.
140034012,3874,junrao,2017-09-20T17:15:18Z,"Yes, we can probably optimize this in the future if needed."
140038010,3874,junrao,2017-09-20T17:30:37Z,"@lindong28 : In general, I agree the simpler the better. However, changing suffix in the future potentially will be even more complicated in order to support upgrade. So, it's worth thinking a bit more to see if we can get things right in the first place.

For the approach that I described above. I was thinking that if any step fails, you stop and skip the rest of the steps. You only clean this up during log recovery on startup. For the scenario that you described, the sequencing will be the following.

a) FutureLog is successfully renamed to log.completed.
b) CurrentLog can not be renamed due to log directory failure.
c) CurrentLog will be marked offline and disk movement is stopped.
d) Broker is restarted and the log dir for CurrentLog is still offline. Disk movement is still stopped and log.completed is untouched.
e) Broker is restarted and the log dir for CurrentLog is online. Continue with step 2) and 3) above during log recovery.

This is a bit more complicated than your approach, but seems safer and matches how we do swapping in log cleaner.
"
140038900,3874,lindong28,2017-09-20T17:33:45Z,"@ijuma It is two replicas here instead of simply copying data from one log dir to another because we are maintaining states such as epoch and high watermark in the future replica.

@junrao I think it is a good idea to have a single config to throttle all I/Os within a broker. Given that this probably needs to deprecate the existing `log.cleaner.io.max.bytes.per.second` config, maybe we should do it in a separate KIP and find a good name for it?

Regarding this patch, I think `intra.broker.replication.throttled.rate` is OK because this name says ""replication"" which means fetching data from one replica to another. I am also good with `across.dirs.replication.throttled.rate` (which is more consistent with `leader.replication.throttled.rate`). Do you prefer me to change the name to `across.dirs.replication.throttled.rate`?


"
140062197,3874,junrao,2017-09-20T18:59:37Z,How about log.dirs.balancing.io.max.bytes.per.second ?
140063395,3874,junrao,2017-09-20T19:04:27Z,typo ofr
140065045,3874,lindong28,2017-09-20T19:11:26Z,Sure. I have updated the patch to rename this config and related variables as suggested.
140086569,3874,junrao,2017-09-20T20:40:06Z,"Should we remove the future replica too? Otherwise, we may pick up the wrong futureReplica.logEndOffset if logDir is changed again."
140088276,3874,junrao,2017-09-20T20:47:14Z,"Hmm, the logic seems to be the reverse of the comment."
140090819,3874,junrao,2017-09-20T20:57:47Z,Should we check if destinationDir is offline too and return an error if so?
140097808,3874,junrao,2017-09-20T21:27:55Z,We should be using futureReplica's HW instead of currentReplica. 
140107358,3874,junrao,2017-09-20T22:17:02Z,"Hmm, if the replica is a future one, it seems that we don't read the HW from the checkpoint. This may impact the initial offset for replication in the future replica."
140109134,3874,lindong28,2017-09-20T22:28:01Z,"@junrao Great point.. I have been thinking about the right solution since your comment. I have addressed the problem with the following changes to the patch:

- Add methods truncateTo() and truncateFullyAndStartAt() in Partition.scala. Both methods need to grab readLock of leaderIsrUpdateLock before truncating the log.

- Add method appendRecordsToFutureReplica() in Partition.scala. This method needs to grab inWriteLock of leaderIsrUpdateLock first. If LEO of the current replica >= LEO of the future replica, this method will append data to the future log. Otherwise, this method will throw OffsetOutOfRangeException. This method ensures that, if the current replica is truncated after ReplicaAlterDirThread fetches from the current replica but before ReplicaAlterDirThread try to append data to the future replica, the data will not be appended.

- AbstractFetcherThread.processFetchRequest() will catch OffsetOutOfRangeException thrown from processPartitionData(). When this exception is caught, the future replica will be truncated with handleOffsetOutOfRange AND marked for truncation. And the future replica will be truncated again based on the epoch when ReplicaAlterDirThread calls maybeTruncate() next time.

Does this solution sound OK? I am not sure very sure whether we need to both do handleOffsetOutOfRange() and mark the log for truncation. Maybe we need only one of them?"
140110357,3874,junrao,2017-09-20T22:35:50Z,Are we really removing the partition from the source checkpoint file as the comment says?
140111701,3874,lindong28,2017-09-20T22:44:47Z,My solution has a flaw. Let me think about how to fix it..
140112065,3874,junrao,2017-09-20T22:47:32Z,Should we remove future replicas for newOfflinePartitions too?
140115757,3874,junrao,2017-09-20T23:13:41Z,"Hmm, I am not sure that we should do line 1080-1090 on every LeaderAndIsr request especially the controller could re-send the same LeaderAndIsr request. It seems that it's enough to just do this on the very first LeaderAndIsr request (i.e., inside ""if (!hwThreadInitialized) {"")."
140122257,3874,lindong28,2017-09-21T00:03:31Z,"@junrao After more thinking, I made the following change to address the issue.

- Add method appendRecordsToFutureReplica() in Partition.scala. This method needs to grab inWriteLock of leaderIsrUpdateLock to prevent race condition with log truncation on the current replica. After appending records to the future replica, this method will do the following.

```
if (currentReplicaLatestEpoch > futureReplicaLatestEpoch &&
    currentReplicaStartOffsetForLatestEpoch != UNDEFINED_EPOCH_OFFSET &&
    currentReplicaStartOffsetForLatestEpoch < futureReplica.logEndOffset.messageOffset)
  logManager.truncateTo(Map(topicPartition -> currentReplicaStartOffsetForLatestEpoch), isFuture = true)
```

Here is why this could address the issue. In order for the issue in this thread to happen, the following events need to happen:

1) ReplicaAlterDirThread fetches some messages from the current replica
2) The current replica is truncated such that some data that was fetched above will be deleted from the current replica
3) New data is appended to the current replica after log truncation
4) ReplicaAlterDirThread now appends the fetched data to the future replica. This causes inconsistency because it appends some data that was just truncated on the current replica.

With the change made above, in step 4) the ReplicaAlterDirThread will notice that the latest epoch of the current replica is larger than the latest epoch of the future replica while the offset of the first message with that epoch in the current replica is smaller than the offset of the last message in the fetched records. Then the ReplicaAlterDirThread can truncate the future replica to solve the problem.

Does this make sense?



"
140122398,3874,lindong28,2017-09-21T00:04:44Z,Thanks. It is fixed now.
140122619,3874,lindong28,2017-09-21T00:06:29Z,Thanks for catching this. It is fixed now.
140122997,3874,junrao,2017-09-21T00:09:50Z,"I was thinking that one way to do this is that if the current replica truncates, we just remove the partition from the ReplicaAlterDirThread and add it back again. This will force the initialization with proper truncation if needed."
140123008,3874,lindong28,2017-09-21T00:09:58Z,Thanks. You are right. I have updated the patch to fix this bug.
140123222,3874,lindong28,2017-09-21T00:12:05Z,"I think alterReplicaDir() already checks whether the destinationDir is offline at the beginning. If it is offline, it will return KafkaStorageException as error."
140128580,3874,lindong28,2017-09-21T00:58:12Z,"My concern with doing it only on the first LeaderAndIsrRequest is that this imposes two restrictions that it actually enforced now but may not be true in the future.

One restriction is that controller needs to send all partitions in the first LeaderAndIsrRequest to a broker. It is true as of now. But from design perspective broker should also be able to setup partition state properly if the partition is specified in the subsequent LeaderAndIsrRequest.

Another restriction is that broker always need to shutdown after receiving StopReplicaRequest with delete = false. Otherwise, when broker receives LeaderAndIsrRequest to be leader/follower for a partition, state of this partition may not be setup properly. This is also true for now. But it is just not very nice because ideally we should be able to send StopReplicaRequest and LeaderAndIsrRequest to stop/start a partition in a broker.

I have updated the patch to address the problem by only doing line 1080 - 1090 for partitions that are online and not already created before the broker receives this LeaderAndIsrRequest."
140128968,3874,lindong28,2017-09-21T01:02:10Z,"newOfflinePartitions will be removed from ReplicaManager.allPartitions. I think we probably don't need to remove the replica from partition.allReplicasMap if the partition object itself is going to be removed and garbage collected, right?"
140129564,3874,lindong28,2017-09-21T01:08:09Z,"Previously I think it is more accurate to just use the HW of the current replica.

I have updated the patch to use HW of the future replica."
140130152,3874,lindong28,2017-09-21T01:13:58Z,"Yes I think so. It is same way as how LogCleaner.updateCheckpoints() is used to remove partition from cleaner offset checkpoint file. LogManager.asyncDelete() will delete the log from LogManager.futureLogs before it calls `cleaner.updateCheckpoints`.

The caller of alterCheckpointDir(), in this case LogManager.deleteAndSwapFutureLog(), needs to update LogManager.currentLogs before it calls LogManager.deleteAndSwapFutureLog(). Then `updateCheckpoints(sourceLogDir, None)` will remove the partition from source log directory and updateCheckpoints(destLogDir, Option(topicPartition, offset)) will add the partition to the destination log directory."
140130593,3874,lindong28,2017-09-21T01:18:53Z,"I think we actually read HW from the checkpoint if the replica is a future one. When the future replica is newly created, it is OK that we don't have HW for the future replica because the log for this future replica is empty. Then `ReplicaManager.checkpointHighWatermarks()` will checkpoint HW for the future replica in the destination log directory. When broker restarts and load the log, Partition.getOrCreateReplica() will read the HW checkpoint in the destination log directory for future replica as well.

Does this make sense?"
140131155,3874,lindong28,2017-09-21T01:24:40Z,"It seems that all other issues have been addressed. I need to think more about this issue more. Let me first fix the test, cleanup the patch and upload it."
140131332,3874,junrao,2017-09-21T01:26:46Z,Thanks for the explanation. Makes sense.
140131769,3874,junrao,2017-09-21T01:31:31Z,"Ok, sounds good."
140131983,3874,junrao,2017-09-21T01:34:06Z,Thanks. Make sense. Missed that isReplicaLocal() covers future replica too.
140163072,3874,lindong28,2017-09-21T07:04:14Z,"@junrao I think the suggested approach will work. I am just wondering if the following approach would be simpler:

In LogManager.deleteAndSwapFutureLog():

1. Rename FutureLog to be the current log
2. Rename CurrentLog to be deleted.

And here is how we handle log directory failure:

1) FutureLog is successfully renamed to be the current log
2) CurrentLog can not be renamed due to log directory failure
3) The log in the source log directory will be marked offline. The log in the destination log directory will serve as the current log and this partition is still online.
4) Broker is restarted and the source log directory is still offline. Nothing needs to be done. The partition is online.
5) Broker is restarted and the source log directory is online. But the destination log directory is online. In this case the log in the source log directory will be truncated based on the leader epoch and will be try to catch up with the leader.
6) Broker is restarted and both source and destination log directory is offline. LogManager.loadLogs() will choose the one with the larger latestEpoch as the current log. If these two replicas has the same leader epoch, then the one with the larger nextOffset will be chosen as the current log. The other log will be marked for deletion.

The advantage of this solution is that we don't need to new suffix, and we don't need to have the logic of renaming a log directory back-and-forth. This solution takes advantage of the leaderEpoch to resolve conflicts. Do you think this would be a good solution?



"
140326737,3874,junrao,2017-09-21T18:40:08Z,"@lindong28 : What you suggested is simpler. My concerns are the following: (1) It can't truly protect the case when the same partition shows up in more than one log dir (say by human mistakes). (2) The approach of using LeaderEpoch only works when the message format has been upgraded. However, users may not upgrade the message format immediately after upgrading the code. Picking the replica based on just the offset is less reliable."
140330274,3874,lindong28,2017-09-21T18:54:37Z,"@junrao Regarding (1), if the same partition appears on multiple log directories due to human either, I think log manager can still choose the one with the highest epoch (or nextOffset if epoch is the same) to address the problem. Is my understanding right?

Regarding (2), I am wondering if it is reasonable to support intra-broker replica movement only if message format has been increased to support leader epoch. I personally think it is a good tradeoff to keep the Kafka implementation simpler in the long term. It seems reasonable for a new feature to rely on a message format that has come before it.

BTW, in my solution to another issue you raised, i.e. inconsistency between the current and the future replica, I also used this trick of leader epoch to address the problem. The solution to that problem will probably be less clean if we can not reply on leader epoch. I will need to think more about how to address these two problems if we want to allow user to move replica within broker with older message format.


"
140330487,3874,lindong28,2017-09-21T18:55:28Z,Also note that KIP-112 is only enabled if inter-broker-protocol is 1.0 or higher. I am wondering if the similar logic can be applied to determine whether KIP-113 is fully supported.
140347295,3874,junrao,2017-09-21T20:13:39Z,"@lindong28: For (1), if there is human error, the 2 replicas could correspond to the same topic created at different times. So, their leader epoch may not be directly comparable. It just feels safer if we have a more direct way to detect errors like this. For (2), note that inter-broker-protocol is separate from the message format. Even when the message format is set, leader epoch only applies to newly produced messages. "
140361789,3874,junrao,2017-09-21T21:18:44Z,"Hmm, I am not sure if we should lock the writes to the current replica while writing to the future replica. What you said earlier makes sense: we want the writes to the current and the future replica to be independent. Another concern is that we are duplicating the logic for epoch checking in ReplicaAlterDirsThread here. 

I was thinking that another way to do this is in Partition.truncateTo(), if the truncation is on a current replica, we simply remove it and add it back to the replicaAlterDirManager if it's there. The newly added partition will go through the initialization phase to do the truncation that's needed. This way, the writes to current and the future replica can still be independent. "
140364258,3874,junrao,2017-09-21T21:30:09Z,Could we replace replicaMgr.getPartition(topicPartition).get with partition?
140365688,3874,junrao,2017-09-21T21:36:34Z,It's not very clear what's being deleted from the name. Would it be better to name this replaceCurrentWithFutureLog?
140366268,3874,junrao,2017-09-21T21:39:25Z,It seems that we need to call replicaAlterDirManager.shutdownIdleFetcherThreads() after a partition is removed?
140369569,3874,junrao,2017-09-21T21:55:51Z,"There are various error loggings like that in line 220 with text ""Error to broker .."". Those lines are now shared between the ReplicaFetcherThread and the ReplicaAlterDirThread. So, we probably want to make the logging clearer."
140370782,3874,junrao,2017-09-21T22:02:02Z,The reference to leader is not accurate here.
140371771,3874,junrao,2017-09-21T22:07:07Z,"Hmm, shouldn't we get the startOffset from the future replica?"
140375054,3874,junrao,2017-09-21T22:29:05Z,Should we make this volatile now that it's updatable?
140377631,3874,junrao,2017-09-21T22:47:03Z,We probably want to have a separate window size and window samples config for the intraBroker quota?
140377671,3874,junrao,2017-09-21T22:47:19Z,intraBroker => logDirsBalancing?
140379049,3874,junrao,2017-09-21T22:57:49Z,"To be consistent with the new throttling name, perhaps name this num.log.dirs.balancing.threads? If so, we want to change variable names, comments, and documentations accordingly."
140379353,3874,junrao,2017-09-21T23:00:10Z,This is now unused.
140380696,3874,junrao,2017-09-21T23:09:43Z,"Hmm, leaderEpochCache is maintained when records are appended to the log. So, not sure if we need to call initializeLeaderEpochCache() when renaming."
140380952,3874,junrao,2017-09-21T23:11:40Z,"If the dir name doesn't change, could we skip the steps below?"
140383875,3874,junrao,2017-09-21T23:34:10Z,Could we add topicPartition too?
140385799,3874,junrao,2017-09-21T23:51:11Z,We probably want to rename intra-broker-throttle to log-dirs-balancing-throttle?
140385990,3874,junrao,2017-09-21T23:53:02Z,Should we make it volatile?
140387569,3874,junrao,2017-09-22T00:07:08Z,unused import
140389918,3874,junrao,2017-09-22T00:26:11Z,Is the comment still valid?
140389959,3874,junrao,2017-09-22T00:26:37Z,Should we do the assert in a waitUntil loop?
140390517,3874,junrao,2017-09-22T00:32:46Z,"The replica on broker 100 is just changing the log dir. waitForReassignmentToComplete() only checks for the completion of cross broker replica movement, but not of cross log dirs movement. Perhaps we should do this in a waitUntil() loop?"
140595813,3874,lindong28,2017-09-22T21:01:15Z,"@junrao Regarding (2), I am thinking that it may be reasonable to say we only support inter-broker replica movement only if message format is supports epoch. This is similar to saying that we only support time-based query if message format supports time index, and we only KafkaHeader if the message format support Kafka Header. This may be worth doing if it can keep the Kafka implementation simpler in the long term.

Also regarding (2), my understanding is that we can use leader epoch to choose the newer log directory in this case after message format has been upgraded to support leader epoch, even if there is still old messages in the log that doesn't have leader epoch. This is because the trick here only look at the epoch of the latest message for each log directory. If this is not true then my approach would not work.

Regarding (1), I realized the following shortcomings of the suggested approach while trying to implement it. Let me compare the suggested approach with my approach in more detail below.

To clarify, my approach is the following:
1) Rename FutureLog to be the current log
2) Rename CurrentLog to be deleted.

The approach you suggested is the following:
1) Rename FutureLog to log.completed.
2) Rename CurrentLog to log.deleted.
3) Rename log.completed to log.

Pros of the suggested approach:
- It allows user to move replica between log directories of the same broker before user has upgraded to the message format that support leader epoch. (a short term benefit)
- It matches how we do swapping in log cleaner (I am not very sure the benefit of following the existing swapping logic in log cleaner in this case though. Can you explain a bit more?)
- It can help us detect human error if the user mistakenly copied a directory with old data to a broker which already has directory for this partition on another log directory.

Cons of the suggested approach:
- If we fail at the step 2 (Rename CurrentLog to log.deleted), we will mark the partition offline even if we can use the partition in the destination log directory. This reduces availability of the partition.
- If we fail at step 3 (Rename log.completed to log), we will still delete the replica in the source log directory because it has been successfully renamed for deletion. This reduces the persistence of the partition.
- Extra logic is needed to keep completable logs in a separate map in LogManager so that we can include its size in the DescribeLogDirsResponse. We also need extra metric tag for Log whose directory has be renamed to log.completed. This adds complexity to Kafka implementation in the long term.
- It doesn't protect against more complicated errors, e.g. if user deletes new log directory before copying the old log directory, or if user deletes specific segment for the partition. 

Overall I think the ability to detect some human error is nice to have. But human error detection is generally hard to do and we don't have a good definition for the scope of human error that can be detected. I am not sure if this is worth the cost in availability and code complexity. Actually, if we do want to detect this specific human error, we can also modify the my approach to have broker refuse to start if the same partition appears in more than one directory. This will generate false positive only if source log directory fails exactly at the time the destination partition has caught up with the source partition, which should be pretty. Since user knows the log directory that was offline, he/she can manually delete the partition in the source log directory correctly and restart the broker.

The reply is a bit long. Thanks for taking time to read and discuss this issue.
"
140636915,3874,junrao,2017-09-23T16:22:58Z,"@lindong28 : I like the modified version of your approach. Basically fail the broker if we detect duplicated logs during restart. This makes the logic simpler. With this, we probably don't need to have the constraint on message format."
140643683,3874,lindong28,2017-09-23T23:16:22Z,"@junrao Ah, I didn't realize you have replied to this thread.

Yeah I have considered this option. My concern with this approach is that truncation usually happens at the Partition level or Log level, but the management of ReplicaAlterDirManager and ReplicaFetcherManager ideally should happen at the ReplicaManager level, which is higher than the level of Partition and Log. Usually the level of caller should be higher than the level of caller to avoid deadlock and to make the code logic simpler to maintain and develop in the long term.

In this specific case, if we add/remove partition from ReplicaAlterDirManager, a ReplicaFetcherThread may need to get the following lock if truncation happens: AbstractFetcherThread.partitionMapLock, Partition.leaderIsrUpdateLock, ReplicaManager.replicaStateChangeLock, ReplicaAlterDirManager.mapLock and ReplicaAlterDirThread.mapLock. This may cause deadlock if another KafkaRequestHandler attempts to get the ReplicaManager.replicaStateChangeLock and then Partition.leaderIsrUpdateLock.

Do you think the approach I suggested previously would work? Do you have concern with having this approach depend on the leader epoch and thus the message format?

"
140645077,3874,lindong28,2017-09-24T01:25:35Z,"Yeah I have considered this option. My concern with this approach is that truncation usually happens at the Partition level or Log level, but the management of ReplicaAlterDirManager and ReplicaFetcherManager ideally should happen at the ReplicaManager level, which is higher than the level of Partition and Log. Usually the level of caller should be higher than the level of caller to avoid deadlock and to make the code logic simpler to maintain and develop in the long term.

In this specific case, if we add/remove partition from ReplicaAlterDirManager, a ReplicaFetcherThread may need to get the following lock if truncation happens: AbstractFetcherThread.partitionMapLock, Partition.leaderIsrUpdateLock, ReplicaManager.replicaStateChangeLock, ReplicaAlterDirManager.mapLock and ReplicaAlterDirThread.mapLock. This may cause deadlock if another KafkaRequestHandler attempts to get the ReplicaManager.replicaStateChangeLock and then Partition.leaderIsrUpdateLock.

Regarding your concern with ""lock the writes to the current replica while writing to the future replica"", I think it probably won't hurt performance much. It will reduce the maximum throughput of writing to the current replica by at most half. But I assume that for most replica, the average throughput should be much less than 50% of the maximum throughput that a broker can write to a partition. Furthermore, even if the throughput of this partition is very close to the maximum throughput, we actually want to reduce the throughput that a broker can write to the current replica before the future replica catches up. Does this make sense?

Do you think the approach I suggested previously using leader epoch would work? Do you have concern with having this approach depend on the leader epoch and thus the message format?
"
140645086,3874,lindong28,2017-09-24T01:26:15Z,"Regarding your concern with ""lock the writes to the current replica while writing to the future replica"", I think it probably won't hurt performance much. It will reduce the maximum throughput of writing to the current replica by at most half. But I assume that for most replica, the average throughput should be much less than 50% of the maximum throughput that a broker can write to a partition. Furthermore, even if the throughput of this partition is very close to the maximum throughput, we actually want to reduce the throughput that a broker can write to the current replica before the future replica catches up. Does this make sense?

We can continue discussion under the new batch of comment you provided. Thanks!

"
140645139,3874,lindong28,2017-09-24T01:30:45Z,"My bad. Previously I was very tight on the time and spent on most of the time on addressing the comments and have not spent enough time on reviewing the patch myself. Now that I have time, I will review the patch myself twice and let you know once it is ready."
140645147,3874,lindong28,2017-09-24T01:31:10Z,The issue is fixed as suggested.
140645440,3874,lindong28,2017-09-24T02:02:10Z,"I understand that ideally we want to shutdown idle threads. My concern with this approach is that it will let ReplicaAlterDirThread depend on ReplicaAlterDirManager which introduce circular dependency and make it easier to have deadlock in the future. 

The only drawback of the current approach is that, there may be idle ReplicaAlterDirThread until the broker receives the next LeaderAndIsrRequest. During this period this ReplicaAlterDirThread will call doWork() once every 1 second by default, which doesn't seem like a problem. Thus I think the little performance overhead is better than complicating the Kafka implementation. What do you think?


"
140645451,3874,lindong28,2017-09-24T02:03:35Z,My bad. It is fixed now.
140645510,3874,lindong28,2017-09-24T02:06:24Z,"Previously I thought it is always more accurate to simply read the LogStartOffset (and similarly HW) from the current replica. I forgot to go over the patch and correct this after your previous comment regarding the use of HW. I will go over the patch twice to make sure all these are corrected.

It is fixed as suggested."
140648350,3874,lindong28,2017-09-24T06:20:07Z,It is fixed now. I should be able to fix missing parameters like this in the next update.
140651820,3874,lindong28,2017-09-24T09:39:56Z,Good point. I have updated the name as suggested. Thanks!
140653814,3874,lindong28,2017-09-24T11:24:16Z,"I am not sure I understand the problem here. It seems that current error logging in AbstractFetcherThread is technically correct. For example, the `Error to broker` in line 220 is correct because `sourceBroker.id` still refers to the broker from which the data is fetched, for both ReplicaFetcherThread and ReplicaAlterDirThread. Furthermore, the log4j error logging already identifies the class (either ReplicaFetcherThread or ReplicaAlterDirThread) for those log statements in the AbstractFetcherThread, thus there is probably no need to identify in the logging message whether it is for ReplicaFetcherThread or ReplicaAlterDirThread.

Does this make sense?"
140653857,3874,lindong28,2017-09-24T11:25:59Z,Good point! I have updated the patch to make it volatile.
140653887,3874,lindong28,2017-09-24T11:27:44Z,It is fixed now. I will review the patch myself twice and try to catch all these things.
140654025,3874,lindong28,2017-09-24T11:34:18Z,"I don't have a good reason for adding a separate window size and window samples. I think the main difference between interBroker and logDirsBalancing quota is the throttle rate which can already be configured differently for these two quotas. Is there any scenario where user may want to use different window size and window samples for interBroker and logDirsBalancing quota? If not, maybe we can do it when we need it in the future?"
140654292,3874,lindong28,2017-09-24T11:48:32Z,"ReplicaAlterDir describes the action of these threads and classes, whereas LogDirsBalancing identifies the purpose of this action of moving replica. I think ReplicaAlterDir* is more useful and explicit for Kafka developer to understand the what these ReplicaAlterDirThread is doing.

On the other hand, LogDirsBalancing is probably more useful for user to understand the purpose of this new quota.

If we were to unify the name, I think it is better to use replace LogDirsBalancing with ReplicaAlterDir. The reason is that it is a bit weird and vague to have method like AdminClient.LogDirsBalancing(Map<TopicPartitionReplica, String>). And both developer and user probably needs to translate logDirsBalancing to ReplicaAlterDir in order to understand what the new thread, thread manager and request is doing. What do you think?"
140654314,3874,lindong28,2017-09-24T11:49:27Z,Sorry.. it is fixed now.
140654337,3874,lindong28,2017-09-24T11:50:30Z,Good point. I have updated the patch as suggested.
140654392,3874,lindong28,2017-09-24T11:53:26Z,"I think we need to call initializeLeaderEpochCache() because the LeaderEpochCheckpointFile.checkpoint.file is determined and fixed in initializeLeaderEpochCache(). Thus if we don't call initializeLeaderEpochCache() after the log directory is renamed, LeaderEpochCheckpointFile.write() will still try to write to a file in the old log directory which no longer exists.

"
140654406,3874,lindong28,2017-09-24T11:54:17Z,"Yes, you are right. I will rename these."
140654424,3874,lindong28,2017-09-24T11:54:59Z,Ah.. fixed now.
140654487,3874,lindong28,2017-09-24T11:57:27Z,"Thanks. I have updated the comment to the following. I will review the patch end-to-end twice to try to make comments and the code consistent after the previous changes.

`// When we execute an assignment that moves an existing replica to another log directory on the same broker`"
140654577,3874,lindong28,2017-09-24T12:02:24Z,You are right. It is fixed now.
140654729,3874,lindong28,2017-09-24T12:10:00Z,You are right. Strictly speaking we should use waitUntil() here. I have updated the patch accordingly.
140683628,3874,junrao,2017-09-25T03:34:28Z,"@lindong28 : Your suggested approach probably works in the common case. My concerns are the following. (1) Log truncation is a relatively rare event. Your approach tries to solve the problem by requiring writes to future log to hold on to the lock to the current log on every write. So, we add overhead in the common path to solve a rare event. Intuitively, to solve a rare event, we also want to add overhead in the rare path. (2) Your approach won't be able to distinguish leader epoch inconsistency due to log truncation or bugs in Kafka. In the latter case, we probably want to error out instead of continuing. (3) The dealing with leader epoch is tricky to reason about and is already done during replica initialization in the ReplicaAlterDirThread. If possible, it would be useful to limit the places that we deal with leader epoch directly. (4) In theory, even if a topic has the new message format, a truncation may bring the log to the point where there is only old message. (5) Intuitively, the disk balancing feature seems independent of message format. So, it feels a bit weird to require that. 

As for the concerns that you mentioned on the suggested approach. I am not sure we need to hold Partition.leaderIsrUpdateLock. That lock is only needed when the replica set changes. Truncating an existing future replica doesn't change replica set though. We do want to make sure that the reinitialization in the future replica is synchronized properly with potential concurrent AlterLogDir requests, as you pointed out. So, the following is a slightly modified approach. After the ReplicaFetchThread truncates the log, it will do the following: (1) call a new method in ReplicaAlterDirThread.addPartitionsWithTruncation that takes a partition and a truncating offset. This method won't directly do truncation in the future replica. It simply sets the partition state to be truncating if the partition is still present in the partition map. When ReplicaAlterDirThread sees this new state, it ignores any pending fetch response and truncates the future log to the truncating offset, and then resumes fetching.  addPartitionsWithTruncation() needs to hold partitionMapLock. However, since only ReplicaFetchThread.truncation can call ReplicaAlterDirThread.addPartitionsWithTruncation, but not vice versa. There is no cycle to form a deadlock. This approach seems to address all the above concerns: (1) no additional overhead in the write path, which is common, (2), (3) no additional logic for dealing with leader epoch, (4) and (5) doesn't require new message format.

Neither approach deals with the case when a truncation only happens on the current replica, but not the future replica, and the broker dies. This can still lead to the case when the future replica's log can be ahead of the current replica. If the message format doesn't have the leader epoch, this may not be dealt with cleanly during future replica initialization. We could potentially just truncate the future replica to the log end offset of the current replica in that case during recovery.
"
140683654,3874,junrao,2017-09-25T03:34:53Z,Could we just add a scheduler thread that calls replicaAlterDirManager.shutdownIdleFetcherThreads() periodically?
140683674,3874,junrao,2017-09-25T03:35:14Z,"My point is that for ReplicaAlterDirThread, the logging ""to broker"" doesn't convey much info since it always moves data within the same broker. When there is an error, it's more useful to know the source log dir from which the future replica is copying."
140683704,3874,junrao,2017-09-25T03:35:49Z,"The window size and window samples impact how much load can be put during initialization. During initialization, we give a full window worth of quota. So, the larger the window, the more bytes can be put in initially. Since interBroker quota may be configured based on network capacity whereas logDirsBalancing quota is mostly based on disk capacity, having separate window sizes between the two quotas allows the admin to control the initial load separately."
140683719,3874,junrao,2017-09-25T03:36:06Z,"Hmm, it seems that both the num.threads config and the disk movement quota will be set by the admin. So, it seems it makes sense to make them consistent. Naming them both after replica alter dir will be fine too."
140683728,3874,junrao,2017-09-25T03:36:14Z,"Ah, makes sense. Could you add a comment about this?"
140683945,3874,junrao,2017-09-25T03:39:55Z,"A related concern here is that we are paying the sort overhead on every fetch request. If there are lots of partitions, this could add non-trivial overhead. The queue approach where the ReplicaAlterDirThread takes one partition at a time from the queue obviates the need for that. It's fine to optimize this in a followup patch though."
140922489,3874,lindong28,2017-09-25T23:09:34Z,"@junrao I think your points 1-5 makes sense. 

My concern with the modified approach is that it requires ReplicaFetcherThread to depend on the ReplicaAlterDirThread in order to call fetching. addPartitionsWithTruncation(). It seems a bit clumsy. I have another way of doing that by passing the truncation signal from ReplicaFetcherThread to ReplicaAlterDirThread via the affected Partition. Here is my modified approach:

1) Partition.truncateTo() and Partition.truncateFullyAndStartAt() will grab write lock of leaderIsrUpdateLock. And Partition.appendRecordsToFutureReplica() will grab read lock of leaderIsrUpdateLock. Thus we still ensure that the truncation of the current replica and the append operation of the future replica will be executed in order.
2) When the current replica of a partition is truncated, it will set Partition.futureReplicaNeedTruncation to true.
3) Partition.appendRecordsToFutureReplica() will first check whether Partition.futureReplicaNeedTruncation is true. If the flag is true, the method will set the flag to false and throw OffsetOutOfRangeException.
4) If AbstractFetcherThread.processFetchRequest() catches an OOOR exception from processPartitionData(), it will update partitionState for this partition to set truncatingLog to true. Then the future replica will be truncated based on the leader epoch later.

This approach also seems to address all the above concerns: (1) no additional overhead in the write path, which is common, (2), (3) no additional logic for dealing with leader epoch, (4) and (5) doesn't require new message format. In comparison to the previous modified approach, the class dependency graph will be simpler and more intuitive.

What do you think?




"
140923612,3874,lindong28,2017-09-25T23:17:24Z,@junrao @ijuma Great! I will implement this version in the updated patch.
140928340,3874,lindong28,2017-09-25T23:53:27Z,@junrao Thanks for catching this. I have updated it patch to fix it.
140928991,3874,lindong28,2017-09-25T23:57:55Z,Good point. I have updated the patch to call replicaAlterDirManager.shutdownIdleFetcherThreads() every 2.5 seconds.
140931539,3874,lindong28,2017-09-26T00:19:55Z,"@junrao I see. Currently AbstractFetcherThread can not get the log directory for partition in general because ConsumerFetcherThread, which also extends AbstractFetcherThread, does not have log directory for partition.

I think the existing implementation probably already provides log directory of the partition in the error log when this information is needed. The idea is that we need to know the log directory of the partition in the error log only if this is a log directory failure. But if this is the case, the original exception should have specified the log directory of the partition.

Does this sound reasonable?


"
140931968,3874,lindong28,2017-09-26T00:23:48Z,"@junrao I see. The default size of the full window is 11 seconds. I think the replica movement typically lasts much longer than 11 seconds and in general it is not a big deal to have a slow start. I am not sure it is worth two additional configs to optimize the initial performance of intra-broker replica movement.

I don't have a strong opinion on this. Do you prefer me to add two configs for this new quota?
"
140932270,3874,lindong28,2017-09-26T00:26:57Z,"Sure. I added this comment:

`re-initialize leader epoch cache so that LeaderEpochCheckpointFile.checkpoint can correctly reference the checkpoint file in renamed log directory`"
140936761,3874,lindong28,2017-09-26T01:07:01Z,Sure. I have updated the patch to use replica alter dir consistently.
140938334,3874,lindong28,2017-09-26T01:22:16Z,"@junrao Good point. I have updated the patch with the following code. The new implementation should require only one pass of the partitionMap with O(n) time complexity, which is same as the time complexity of ReplicaFetcherThread.buildFetchRequest()."
140938438,3874,lindong28,2017-09-26T01:23:13Z,"Here is the code. The idea is that we only need the maximum partition in the filtered partitionMap.

```
    val maxPartitionOpt = partitionMap.filter { case (topicPartition, partitionFetchState) =>
      partitionFetchState.isReadyForFetch && !quota.isQuotaExceeded
    }.reduceLeftOption { (left, right) =>
      if ((left._1.topic > right._1.topic()) || (left._1.topic == right._1.topic() && left._1.partition() >= right._1.partition()))
        left
      else
        right
    }
```"
140941543,3874,junrao,2017-09-26T01:55:06Z,"@lindong28 : The modified approach sounds good overall. A few more questions on this. (a) Do we need the leaderIsrUpdateLock in step 1? If we truncate the current replica first and then insert the truncation point in the future replica, eventually the future replica will be able to truncate to the right offset, right? (b) For step 2 and 3, we can set the truncation state through Partition. But we could also just directly set the state in ReplicaAlterDirThread, which seems simpler since it avoids a level of indirection. (c) Not sure why we need to turn the truncation state to OOOR exception. The handling of OOOR may truncate to a different offset than the truncation offset. It seems it's clearer if we handle the truncation state directly. "
140942158,3874,junrao,2017-09-26T02:01:26Z,"Yes, I'd prefer to add two new configs for the new quota since we already have separate window configs for the client and the replication quota."
140952243,3874,lindong28,2017-09-26T03:54:06Z,Sure. I will update the patch to add these two new configs.
140993990,3874,lindong28,2017-09-26T08:53:21Z,"@junrao Good point regarding (a). Indeed, truncateTo() and truncateFullyAndStartAt() can simply use the readLock instead of the writeLock. readLock is needed to avoid race condition with maybeReplaceCurrentWithFutureReplica().

Regarding (b), can you explain a bit more how ReplicaFetcherThread can set state in ReplicaAlterDirThread after log truncation? Are you suggesting that ReplicaFetcherThread constructor should take `ReplicaAlterDirManger.fetcherThreadMap` as input. Then when log truncation happens for partition, it derives the corresponding ReplicaAlterDirThread from `ReplicaAlterDirManger.fetcherThreadMap` by hashing the topic and partition, before calling `ReplicaAlterThread. addPartitionsWithTruncation ()`? It seems a bit weird to put `ReplicaAlterDirManger.fetcherThreadMap` in the constructor of each ReplicaFetcherThread. What do you think?

Regarding (c), I agree we don't need to throw OOOR if we can directly set the truncation state. I am just not sure how we can directly set the truncation state due the concern with (b) described above. Also, in my suggested approach, the handling of this OOOR will not reset offset using handleOffsetOutOfRange. Instead it will set partitionState.truncatingLog to true so that the partition will be truncated based on the leader epoch later. I am going to update the patch soon so that you can see it.



"
141366056,3874,junrao,2017-09-27T14:44:59Z,"@lindong28 : I was thinking of adding a truncatePartition(Map[TopicPartition, Long]) method in AbstractFetcherThread and AbstractFetcherManager. The latter will call the former. The method takes a truncation offset for each partition in the map. ReplicaFetcherThread will have access to ReplicaAlterDirManager to call truncatePartition() when the current log is truncated. Currently, AbstractFetcherThread maintains PartitionFetchState for each partition, if the state is truncatingLog, it will issue leaderEpochRequest, followed by log truncation. We can probably introduce a separate leaderEpochState. During initialization, a partition will go through leaderEpoch state (where leader epoch request is issued) and then truncatingLog state (where log will be truncated). When truncatePartition() is called, AbstractFetcherThread just sets the partition state to truncatingLog state. When AbstractFetcherThread sees a partition in that state, it will just truncate the log to the specified truncating offset and then transition to the fetch state.

For (a), I am not sure we even need a read lock on leaderIsrUpdateLock. In the case when a partition is already removed when AbstractFetcherThread.truncatePartition() is called, we can simply ignore that partition."
141389315,3874,lindong28,2017-09-27T15:57:01Z,"@junrao Regarding (b), it seems that leaderEpochState is not used in the suggested approach? I think the suggested approach would work. My only concern with this approach is that his approach will have ReplicaFetcherThread depend on ReplicaAlterDirThread, which is a bit unintuitive. On the other hand, my current approach is more complicated because it requires a new flag in Parition, OOOR exception from Partition.truncateTo(...), and handling of this OOOR exception thrown from processPartitionData().

I will implement the suggested approach if you think it is OK to have ReplicaFetcherThread depend on ReplicaAlterDirThread. Can you confirm this?

Regarding (a), I think readLock is needed by truncateTo() and truncateFullyAndStartAt() to avoid race condition with maybeReplaceCurrentWithFutureReplica(). For example, if truncateTo() does not grab the read lock, it is possible that maybeReplaceCurrentWithFutureReplica() finds the LEO of the future replica equals LEO of the current replica, truncateTo() truncates the current replica, and then maybeReplaceCurrentWithFutureReplica() replaces the current replica with the future replica, and the future replica has data that should have been truncated. Does this make sense?


"
141399349,3874,junrao,2017-09-27T16:33:14Z,"@lindong28 : In comparison, the approach that lets ReplicaFetcherThread reference ReplicaAlterDirManager seems a bit better.

Your explanation on the locking requirement makes sense. We are abusing the intent of leaderIsrUpdateLock now. So, we probably want to add some comments on that."
141768963,3874,lindong28,2017-09-29T00:49:23Z,@junrao Thanks for all the comments! I have updated the patch as suggested.
142055101,3874,tedyu,2017-10-02T03:57:10Z,"Should we check whether  getReplica(Request.FutureLocalReplicaId).get.log is None (in case we get read lock immediately after write lock is released) ?
See code at line 197 above."
142070001,3874,lindong28,2017-10-02T07:34:43Z,@tedyu maybeReplaceCurrentWithFutureReplica() will not cause NoSuchElementException here because there the thread that calls maybeReplaceCurrentWithFutureReplica() for a given partition is guaranteed to be the same ReplicaAlterDirThread that appends record to the future replica of this partition.
142263275,3874,junrao,2017-10-02T21:41:00Z,unused import OffsetOutOfRangeException.
142266788,3874,junrao,2017-10-02T21:58:47Z,"Hmm, not sure why we need to check partitionStates.contains() here since we already have the check in line 179."
142267378,3874,junrao,2017-10-02T22:01:30Z,indentation
142278626,3874,junrao,2017-10-02T23:08:27Z,"Hmm, I am not sure that we should always let a partition go through the leader epoch logic when a partition is marked for truncation. There are two cases here. (1) If a partition is not in the fetching mode, we should let the replica go through the leader epoch logic and truncate based on the epoch response. After that, we will truncate again based on the marked truncation offset. (2) If a partition is in the fetching mode, we should just do the truncation based on the marked truncation offset w/o going through the leader epoch logic. We probably want to add some comments to document this.

Also, given this new method, the existing includeLogTruncation flag can be a bit confusing. Perhaps it's clearer to rename it to isRecovering? 

"
142283836,3874,junrao,2017-10-02T23:46:58Z,"The name appendToFollower is now a bit mis-leading since it can be used to append records for future replicas, which is not really a follower. Perhaps it's better to rename the methods here and those in Log as appendWithOffsetAssignment() and appendWithoutOffsetAssignment()."
142294260,3874,ijuma,2017-10-03T01:17:04Z,"We can use this in `logFutureDirName` and `logDeleteDirName`. We can maybe add a `suffix` parameter and have `""""` as the default."
142294466,3874,ijuma,2017-10-03T01:19:17Z,It seems like `if/else` could be about appending the third tag instead of duplicating all the logic.
142294684,3874,lindong28,2017-10-03T01:21:36Z,"Previously I had a method called `appendRecordsToFutureReplica`. I merged it with `appendRecordsToFollower` to reduce the number of methods. Since this is misleading, I think it may be better to add method `appendRecordsToFutureReplica`, which is more intuitive and more consistent with the name `appendRecordsToLeader`. What do you think?"
142294773,3874,ijuma,2017-10-03T01:22:21Z,"We typically use dash-separated names for kafka metrics. Should it be `is-future` then? Also, I wonder if this name will be clear to people. I'll think a bit more, don't have any concrete suggestions, right now."
142294900,3874,lindong28,2017-10-03T01:23:46Z,"The reason is that ReplicaDirAlterThread may have removed topicPartition from the partitionStates in `processPartitionData()` if the future replica has caught up with the current replica. In this case if we call `partitionStates.updateAndMoveToEnd()`, this partition will be added back to the `partitionStates` by mistake."
142294964,3874,lindong28,2017-10-03T01:24:06Z,My bad.. It is fixed now.
142295005,3874,lindong28,2017-10-03T01:24:24Z,Thanks much for the detailed review. It is fixed now.
142295483,3874,ijuma,2017-10-03T01:28:49Z,"Does this not need to be `volatile`? Also, now that it's a `var`, we may want to make it private and only provide a public accessor."
142295625,3874,ijuma,2017-10-03T01:30:23Z,We should probably use `Utils.atomicMoveWithFallback`.
142295728,3874,ijuma,2017-10-03T01:31:36Z,"Seems like we should extract a method that just takes the suffix and generates the `uniqueId`, etc."
142421245,3874,lindong28,2017-10-03T14:37:39Z,"@junrao One reason to let a partition go through leader epoch is that, if the leader replica is truncated to offset 10, append data up to offset 20, and truncated again to offset 15, first truncation offset 10 will be overwritten by the second truncation offset 20. In reality the window for this happening is probably small and we expect the truncation to happen almost immediately on the future replica after the leader is truncated. But ideally we would probably want to stay on the safe side and use leader epoch to make sure this does not cause any problem.

Another reason to apply leader epoch is to mimic what we are currently doing with the follower replica. Currently whenever leader replica is truncated, it is guaranteed that there is leadership transfer, and the replica will be removed from the ReplicaFetcherManager and added back in the follower. Thus the follower will always apply leader epoch if leader is truncated. So it seems to make sense to always truncate the future replica using leader epoch whenever the current replica is truncated.

Does this make sense? Also, is there any correct or performance concern if we always use leader epoch to truncate the future replica?
"
142422846,3874,lindong28,2017-10-03T14:42:59Z,"Good point. You are right, it should be volatile. I have updated the patch to make it private and added a public accessor for it."
142423254,3874,lindong28,2017-10-03T14:44:26Z,Yeah it should be is-future. I have updated the patch as suggested. Thanks!
142424683,3874,lindong28,2017-10-03T14:49:18Z,"Sure. I updated the patch with the following code:

```
private val tags = {
  val maybeFutureTag = if (isFuture) Map(""is-future"" -> ""true"") else Map.empty[String, String]
  Map(""topic"" -> topicPartition.topic, ""partition"" -> topicPartition.partition.toString) ++ maybeFutureTag
}
```"
142427674,3874,lindong28,2017-10-03T14:59:18Z,"I have considered this API. `Utils.atomicMoveWithFallback` calls `Files.move`. And according to the Java doc of `Files.move`, this method will fail if the target file exists, which is the case here when we replace the current replica with the future replica.

"
142429205,3874,lindong28,2017-10-03T15:04:25Z,"Sure. I have updated the patch to add a private method `logDirName(topicPartition: TopicPartition, suffix: String)`. I still keep the method `logFutureDirName` and `logDeleteDirName` because it seems easy to use by the caller. I can remove these two methods if you prefer.

"
142429272,3874,ijuma,2017-10-03T15:04:36Z,"`Files.move` does not fail if the target file already exists given the right `CopyOptions`, which `atomicMoveWithFallback` uses."
142429768,3874,ijuma,2017-10-03T15:06:22Z,"In particular, `ATOMIC_MOVE` should work on Linux. And on Windows, we fallback to `REPLACE_EXISTING`."
142430102,3874,ijuma,2017-10-03T15:07:34Z,"Although, this is a directory, so not sure. Maybe we should write a test to verify. Either way, if `ATOMIC_MOVE` doesn't work, we should use `Files.move` with `REPLACE_EXISTING`, which should work, I believe."
142431419,3874,lindong28,2017-10-03T15:12:09Z,Sure. I have updated the patch to use this method in the newly-added private method.
142432032,3874,lindong28,2017-10-03T15:14:15Z,@ijuma I made a mistake previously. The `renameDir` will rename a directory but the destination direction does not exist. Thanks for the explanation. I will try and test this approach.
142433208,3874,ijuma,2017-10-03T15:18:11Z,Sounds good. The main advantage of the `Files` APIs when compared to the legacy ones is that they provide error messages when things go wrong. That can be quite helpful.
142482694,3874,lindong28,2017-10-03T18:24:12Z,I have updated the patch as suggested.
142558743,3874,junrao,2017-10-04T00:34:06Z,Good point. Could you add a comment for that?
142559035,3874,junrao,2017-10-04T00:37:14Z,@lindong28 : Very good point on double truncation. One way to get around this is to maintain the smallest truncation offset if the partition is marked for truncation multiple times. My concern of depending on leader epoch is that it won't apply if the message format is old.
142704994,3874,lindong28,2017-10-04T15:27:09Z,@junrao Sure. Actually the comment is already in the patch.
142707839,3874,lindong28,2017-10-04T15:37:00Z,"@junrao Good point. I have update the patch to use the smallest truncation offset.

Even if we use the smallest truncation offset, I think it is still safer to use leader epoch to truncate the future replica if leader epoch is available. This is because truncation offset is only recorded in the memory which may be lost if broker restarts. Does this make sense?"
142721548,3874,lindong28,2017-10-04T16:24:54Z,@junrao I have rebased patch onto trunk. I will go through this patch end-to-end after we agree on how to truncate the future replica.
142831425,3874,junrao,2017-10-05T01:26:53Z,"@lindong28 : Thinking a bit more. We can use leader epoch to handle truncation since if the leader epoch doesn't exist, we fall back to HW and the truncation point should always be no lower than that. So, this should be safe."
142933529,3874,lindong28,2017-10-05T13:17:04Z,"@junrao Great. Thanks for taking time to think through this. Since it is safe to use leader epoch and there is not performance concern with doing it, to simplify the implementation, I am going to update the patch to use only leader epoch or high watermark to truncate the future replica without recording the truncation offset. Does this sound OK?"
143042586,3874,junrao,2017-10-05T20:04:43Z,extra empty line
143044343,3874,junrao,2017-10-05T20:12:27Z,"In the common case, partitionAndOffsets will have less entries than partitionStates. So, it will be more efficient to iterate partitionStates and do lookups in partitionAndOffsets."
143052139,3874,junrao,2017-10-05T20:45:12Z,"Since this method is only called only by ReplicaAlterDirManager, perhaps it should be defined there?"
143060422,3874,junrao,2017-10-05T21:21:00Z,"In this case, the current replica won't have an leader epoch info after truncation. To deal with this better, it seems that it's better for a follower replica or a future replica to fall back to the initialized offset in the PartitionState in AbstractFetcherThread, instead of HW, when leader epoch can't be found. We can initialize the offset in PartitionState to the truncation point in this case. In other cases, we can pass in HW to initialize the offset in PartitionState."
143061641,3874,junrao,2017-10-05T21:27:09Z,"It maybe possible that the log end offset in the partition is less than leaderEndOffset. In that case, we really want to start fetching from the log end offset instead of leaderEndOffset. So, it seems it's safer for truncateTo() to return the current log end offset after truncation and use that as the starting offset for fetching. Ditto in ReplicaAlterLogDirsThread."
143065376,3874,junrao,2017-10-05T21:45:46Z,Do we need the lock here? It seems that we just need to make sure the current replica is not being updated when maybeDeleteAndSwapFutureReplica() is called.
143067860,3874,junrao,2017-10-05T21:59:13Z,getReplicaOrException can throw an exception. We don't want to kill the ReplicaAlterLogDirsThread because of this. It seems that we need to return ResultWithPartitions?
143068459,3874,junrao,2017-10-05T22:02:31Z,It seems lastPartitionOpt is better than maxPartitionOpt?
143071316,3874,junrao,2017-10-05T22:19:12Z,alterLogDirs => alterReplicaLogDirs ?
143071753,3874,junrao,2017-10-05T22:21:21Z,"NumAlterLogDirsReplicationQuotaSamples => NumAlterReplicaLogDirsQuotaSamples ? Ditto for AlterLogDirsReplicationQuotaWindowSizeSeconds. In general, it would be useful to make the naming consistent."
143072002,3874,junrao,2017-10-05T22:22:44Z,num.replica.alter.log.dirs.threads => num.alter.replica.log.dirs.threads ?
143072168,3874,junrao,2017-10-05T22:23:44Z,alter.log.dirs.replication.quota.window.num => alter.replica.log.dirs.quota.window.num? Ditto for alter.log.dirs.replication.quota.window.size.seconds.
143073815,3874,junrao,2017-10-05T22:33:36Z,shutdownIdleReplicaAlterLogDirsThread => shutdownIdleAlterReplicaLogDirsThread ?
143073900,3874,junrao,2017-10-05T22:33:58Z,replicaAlterLogDirsManager => alterReplicaLogDirsManager?
143074047,3874,junrao,2017-10-05T22:34:43Z,"shutdown-idle-replica-alter-log-dirs-thread => shutdown-idle-alter-replica-log-dirs-thread? Also, 2500L could probably just be 10000L since the idle threads don't have to be closed that quickly."
143079118,3874,junrao,2017-10-05T23:09:46Z,"It seems that we if the source dir is offline, we want to avoid adding the partition to  replicaAlterLogDirsManager?"
143084972,3874,junrao,2017-10-05T23:57:02Z,"Since we are calling sourceLog.close() later, could we just call it here instead calling sourceLog.closeHandlers()?"
143091621,3874,lindong28,2017-10-06T01:01:45Z,Thanks. It is removed now.
143091979,3874,lindong28,2017-10-06T01:05:13Z,"Sorry, I missed the exception here.

Instead of returning ResultWithPartitions, how about we keep it consistent with ReplicaFetcherThread.fetchEpochsFromLeader(), which returns `tp -> new EpochEndOffset(Errors.forException(t), UNDEFINED_EPOCH_OFFSET)` in the resulting map if the current replica for this topic partition is offline?"
143092370,3874,lindong28,2017-10-06T01:09:14Z,"It is called maxPartitionOpt because this is the toipcPartition with largest topic (in alphabetical order) or the largest partition (if topic string is the same). This topic partition is not selected based on its order in partitionStates. `lastPartitionOpt` seems to suggest it is the last partition that is put into the partitionStates. That is why I used `maxPartitionOpt`.

Do you still prefer `lastPartitionOpt`?
"
143092809,3874,lindong28,2017-10-06T01:13:48Z,"I intentionally used alterLogDirs to reduce the length of the variable name and related config key name in other places. I think ""replica"" can be removed from the name because its type is `ReplicationQuotaManager`, which already includes the word ""replication"".

I will change it to `alteReplicaLogDirs` if you prefer. What do you think?
"
143092968,3874,lindong28,2017-10-06T01:15:23Z,"I just think that ""NumAlterReplicaLogDirsQuotaSamples"" is a bit verbose by having both ""Replica"" and ""Replication"". Thus I removed ""Replica"" from the name. I will change it to ""NumAlterReplicaLogDirsQuotaSamples"" if you prefer."
143093247,3874,lindong28,2017-10-06T01:18:38Z,"Though the request is named ""AlterReplicaLogDirsRequest"", the thread is `ReplicaAlterLogDirsThread`, which is more consistent with `ReplicaFetcherThread`. Because this config is used to determine the thread number, it seems more intuitive to name it `num.replica.alter.log.dirs.threads`. What do you think?"
143093359,3874,lindong28,2017-10-06T01:19:51Z,"I just think that ""alter.log.dirs.replication.quota.window.num"" is more consistent with the existing config ""replication.quota.window.num"". What do you think?"
143093452,3874,lindong28,2017-10-06T01:20:49Z,I think it is because the thread class is named `ReplicaAlterLogDirsThread`. I can change it if you still prefer `shutdownIdleAlterReplicaLogDirsThread`.
143093676,3874,lindong28,2017-10-06T01:23:06Z,Do you prefer to rename the manager class from `ReplicaAlterLogDirsManager` to `AlterReplicaLogDirsManager`? Previously I think `ReplicaAlterLogDirsManager` is more consistent with the existing class `ReplicaFetcherThreadManager`. Same for the `ReplicaAlterLogDirsThread`.
143174644,3874,lindong28,2017-10-06T12:11:34Z,"The reason this is defined in AbstractFetcherManager is that, this method currently uses `fetcherThreadMap`, `mapLock` and `getFetcherId`, which are currently private in AbstractFetcherManager. I thought it is better to keep them private and only uses these variables in `AbstractFetcherManager`.

Another reason is that we already have methods such as `AbstractFetcherThread.fetchEpochsFromLeader()`, which are defined in `AbstractFetcherThread` but only used in `ReplicaFetcherThread`.

I am not strong on this. Do you prefer to change variables above to protected and move `markPartitionsForTruncation` to `ReplicaAlterDirManager`?
"
143176721,3874,lindong28,2017-10-06T12:24:02Z,"I am not sure I fully understand your suggestion. I checked the current implementation of ReplicaFetcherThread.handleOffsetOutOfRange(). If log end offset in the partition is less than leaderEndOffset, the current implementation will start fectching from `Math.max(leaderStartOffset, replica.logEndOffset.messageOffset)`. So it already does what you suggested, right?"
143178096,3874,lindong28,2017-10-06T12:32:43Z,"Good point. It is not needed. In addition to the reason you mentioned, another reason is that appendRecordsToFutureReplica() will be called by the same ReplicaAlterLogDirsThread that calls maybeDeleteAndSwapFutureReplica() for this partition.

I will update the patch to remove this lock. Thanks!"
143178280,3874,lindong28,2017-10-06T12:33:55Z,"It is named `shutdown-idle-replica-alter-log-dirs-thread` because the thread class is ReplicaAlterLogDirsThread. Do you think we should change the name of the thread class and thread manger class?

Sure, I will update the patch to use 10000L as interval."
143178857,3874,lindong28,2017-10-06T12:36:56Z,"Yeah I think the current implementation already avoids adding the partition to replicaAlterLogDirsManager if the source dir is offline. If the source dir is offline, the `val replica = getReplicaOrException(topicPartition)` at line 604 will throw KafkaStorageException and this method will not call `replicaAlterLogDirsManager.addFetcherForPartitions` for this partition."
143179842,3874,lindong28,2017-10-06T12:42:37Z,"I think we need to call sourceLog.closeHandlers() instead of sourceLog.close(). If we call sourceLog.close() and if source log directory is offline, this method will throw KafkaStorageException without closing handler for this sourceLog. Later when LogManager.handleLogDirFailure() is called for this source log directory, the handles of this sourceLog will not be closed because sourceLog has already been removed from `currentLogs`.

On the other hand, if we sourceLog.closeHandlers() and if the source log directory is offline, the handler of sourceLog will be closed before sourceLog.renameDir() throws KafkaStorageException. All handlers in the source log directory will be properly closed in this case.

Does this make sense?
 "
143179997,3874,lindong28,2017-10-06T12:43:33Z,Good point. This makes sense. Let me think more about how to handle this case properly.
143258114,3874,junrao,2017-10-06T17:59:55Z,"The case that I am worried about is the following. Suppose the follower replica has logEndOffset of 5 and truncateTo(10) is called. After that, we will be fetching from offset 10, which could be in the offset range of the leader. This means that we will be missing messages from offset 5 to 10 in the follower. Fetching from offset 5 in this case will be safer."
143258987,3874,junrao,2017-10-06T18:03:26Z,"Yes, that covers the common case. The case that I was concerned about is when the source replica is taken offline after getReplicaOrException() but before getPartition(topicPartition).get.getOrCreateReplica(Request.FutureLocalReplicaId). In this case, are we creating a future replica on an offline partition?"
143320042,3874,lindong28,2017-10-07T02:11:34Z,"ReplicaManager.handleLogDirFailure() is the only method that will take a replica offline. This method is synchronized with ReplicaManager.alterReplicaLogDirs() using replicaStateChangeLock.

Thus if the handeLogDirFailure() is executed after alterReplicaLogDirs(), the future replica will be created first, and then both the current replica and the future replica will be taken offline. If the handeLogDirFailure() is executed before alterReplicaLogDirs(), the future replica will not be created on the offline partition. So the current implementation seems correct, right?

"
143320196,3874,lindong28,2017-10-07T02:18:44Z,"In ReplicaFetcherThread.handleOffsetOutOfRange(), we will first check whether `leaderEndOffset < replica.logEndOffset.messageOffset`. The code block which you mentioned here is only executed if `leaderEndOffset < replica.logEndOffset.messageOffset`. 

In the case ""the follower replica has logEndOffset of 5 and truncateTo(10) is called"", I assume you are saying that `replica.logEndOffset.messageOffset` is 5 and `leaderEndOffset` is 10, then the code block here won't be executed. Instead, the code block in the `else` branch will truncate the follower replica to offset `Math.max(leaderStartOffset, replica.logEndOffset.messageOffset)`, which will be offset 5 if the leaderStartOffset is smaller than the follower's LEO. Later ReplicaFetcherThread will actually fetch starting from offset 5, which seems correct. Did I miss something here?

"
143355895,3874,lindong28,2017-10-08T14:15:27Z,"@junrao Sorry for late reply on this issue. Now I think about this more, I think the current patch handles this case well.

I understand that if truncateFullyAndStartAt() is called for the current replica, the current patch will truncate the future replica to high watermark which does not have any effect. But truncateFullyAndStartAt() will also be called for the future replica later. If truncateFullyAndStartAt() is called for the current replica, then we know that futureReplica's logEndOffset < ""currentReplica's logEndOffset before truncation"" < ""currentReplica's logStartOffset after truncation"". This means that ReplicaAlterDirThread will see OffsetOutOfRangeException for futureReplica and call handleOffsetOutOfRange for future replica. Later truncateFullyAndStartAt() will be called for the future replica as well, which makes the result correct.

Do you think this make sense? If not, can you explain a bit more what will go wrong?
"
143356250,3874,lindong28,2017-10-08T14:29:43Z,This lock is removed.
143356393,3874,lindong28,2017-10-08T14:36:43Z,"I have updated the patch to return `tp -> new EpochEndOffset(Errors.forException(t), UNDEFINED_EPOCH_OFFSET)` if the replica is offline. HW will be used to truncate the future replica before the future replica is removed from ReplicaAlterDirThread later."
143356469,3874,lindong28,2017-10-08T14:40:19Z,"Thanks. I realized that the signature of this method could be simplified to `markPartitionsForTruncation(topicPartition: TopicPartition)`. I have updated the patch to do the following to optimize the performance of this method.

```
Option(partitionStates.stateValue(topicPartition)).foreach { state =>
  val newState = PartitionFetchState(state.fetchOffset, state.delay, truncatingLog = true)
  partitionStates.updateAndMoveToEnd(topicPartition, newState)
}
```"
143529807,3874,junrao,2017-10-09T17:22:58Z,"@lindong28 : Yes, it doesn't seem that this can happen here. So we can leave this as it is."
143530433,3874,junrao,2017-10-09T17:25:52Z,"@lindong28 : Good point that we can rely on truncateFullyAndStartAt() being called on the future replica later since its offset will be out of range too. In that case, it seems that  replicaMgr.replicaAlterLogDirsManager.markPartitionsForTruncation() is a no op. Do we need to call it here?

However, the same issue may happen on truncateTo(). Consider the following case. The current replica needs to call partition.truncateTo(leaderEndOffset) since its logEndOffset is > leaderEndOffset, which can happen when an unclean leader election is triggered. We then call replicaMgr.replicaAlterLogDirsManager.markPartitionsForTruncation(). If there is no leader epoch info (e.g., message format is still old), the future replica will default to truncating to its HW, which could be > the leaderEndOffset of the current replica. By the time the future replica fetches again, more data could have been accumulated in the current replica and the future replica's offset could still be in range. However, some of the data in the future replica now are different from the current replica. In this case, it's better if the future replica defaults to the leaderEndOffset of the current replica during truncation.
"
143530570,3874,junrao,2017-10-09T17:26:26Z,"Hmm, do we need to truncate based on HW for those partitions that are already marked as offline? It seems that it's simpler to just let handlePartitionsWithErrors() deals with them before they are removed from ReplicaAlterDirThread."
143545091,3874,lindong28,2017-10-09T18:31:56Z,"@junrao Yeah you are right, markPartitionsForTruncation() is not needed if currentReplica. truncateFullyAndStartAt() is called. I will remove the markPartitionsForTruncation() here.

Regarding the second issue, I agree that the inconsistency between the current and the future replica can happen if the remaining message's format is old. But it seems to me that the same inconsistency can happen between follower and leader as well if the message format is old. Thus the inconsistency between the future and the current replica does not increase the problem we already have. And in the long term, all messages will have the new format and this won't be an issue. Thus I am not sure the reduced change of inconsistency is worth additional code complexity. But I will update the patch as suggested if you think this is worth doing. What do you think?
"
143546055,3874,lindong28,2017-10-09T18:36:29Z,"@junrao Strictly speaking, we don't have to truncate the future replica is the current replica is offline. But it is probably more intuitive to return `EpochEndOffset(Errors.forException(t), UNDEFINED_EPOCH_OFFSET)` here because the purpose of this method is `fetchEpochsFromLeader()`. The caller of this method, in this case AbstractFetcherThread.maybeTruncate(), should decide which to handlePartitionsWithError or truncate the replica. Does this make sense?"
143592973,3874,junrao,2017-10-09T22:36:40Z,"Ok, this is fine then."
143592989,3874,junrao,2017-10-09T22:36:48Z,"@lindong28 : I agree that the same issue may happen between the leader and the follower, which we can improve separately in the future. We probably don't want to introduce new issues here between the current and the future replica. I am not sure if my suggestion will add more complexity though. We already store the initial offset in AbstractFetcherThread.partitionStates, but ignore it in maybeTruncate(). It seems that it's more natural to fall back to this initial offset instead of HW. Then, we just need to set the initial offset in markPartitionsForTruncation()."
143783274,3874,lindong28,2017-10-10T16:33:23Z,"@junrao The reason I think they are the same problem (not new issue) is that, the future replica is conceptually very similar to the follower replica. ReplicaAlterLogDirsThread works in a very similar way as the ReplicaFetcherThread. They currently share the same problem regarding the inconsistency. And if we have a solution for the follower replica, the solution should be applicable to the future replica as well. I personally think the future replica is just another follower replica on the same broker as leader.

Let's say we think the inconsistency in the future replica is a new problem and we want to fix it. I think re-using the offset in `AbstractFetcherThread.partitionStates` for truncation will make the code a bit more complicated for the following reasons:

- If we allow markPartitionsForTruncation() to change the offset in `partitionStates`, then ReplicaFetcherThread (and ReplicaAlterLogDirsThread) will need to handle this properly in processPartitionData(). Currently only one thread will update the offset in `partitionStates` and `fetchOffset` should always equals `replica.logEndOffset.messageOffset` in `processPartitionData()`.

- As of now, the offset in `partitionStates` is always the LEO of the follower (and future) replica. Thus it is no-op if we truncate the follower replica to this offset in maybeTruncate(). Re-using this offset to store the truncation offset may make the code harder to maintain. It may be better to store the truncationOffsets in a separate map and use it in maybeTruncate(...) as shown in https://github.com/apache/kafka/pull/3874/commits/8c45ede9c81b6509d393941db684e4c7b07585e5#diff-6446cfed30fa23cfc249b9537fffbe35R175.

Thanks for all the discussion. Given my explanation above, can you let me know which solution you prefer? Besides, do you want me to change the name of configs and classes as you raised in the other comments?

"
143793959,3874,junrao,2017-10-10T17:14:38Z,"@lindong28 : A couple of things. First, when markPartitionsForTruncation(), the replica is now transitioning to a maybe truncating state (not ready for fetching). If there is a pending fetch response, we should just ignore it. I have a separate PR (https://github.com/apache/kafka/pull/4039) that tries to tighten this up. Second, once we have that. The implementation that I was thinking is the following. markPartitionsForTruncation() just marks the partition as maybe truncating and puts the initial offset in partitionStates. From that point on, the replica won't be used for fetching data until it transitions to the ready for fetch state. During the handling of maybeTruncate() logic, we will try to decide the truncation point based on leader epoch if possible. Otherwise, we will just fall back to the initial offset. So, implementation wise, I am not sure we really need a separate truncationOffsets map.

So, my thinking is that such a change seems relatively small and safer than the current approach.

As for the naming, I don't want have a strong preference. It's up to you to pick sth consistent."
143807179,3874,lindong28,2017-10-10T18:03:08Z,"This solution makes sense. I have updated the patch as suggested.

It seems that all comments have been addressed. I will rebase the patch onto trunk and review it myself tomorrow. Thanks!"
143874421,3874,junrao,2017-10-10T22:55:11Z,"If we tighten the check in AbstractFetcherThread like in https://github.com/apache/kafka/pull/3929/files, we can still throw an exception here since it shouldn't be possible. That seems a more general fix since it covers the ReplicaFetcherThread too."
143876603,3874,junrao,2017-10-10T23:08:56Z,We probably want to throw an IllegalStateException if includeLogTruncation is false since markPartitionsForTruncation() is not meant to be called in that case.
143882150,3874,junrao,2017-10-10T23:52:27Z,partitionsToAlterLogDirWithLogEndOffset =>futureReplicasAndInitialOffset ?
143898257,3874,lindong28,2017-10-11T02:20:26Z,"Good point. I have updated the patch as suggested, i.e. still throw exception here and check `isReadyForFetch` in `processFetchRequest()`. I can rebase the patch after your patch is committed."
143899061,3874,lindong28,2017-10-11T02:28:59Z,Thanks for catching this. I have updated the patch as suggested.
143900496,3874,lindong28,2017-10-11T02:43:07Z,"Actually, since that state is not possible, I have updated the patch to replace RuntimeException with IllegalStateException."
143900528,3874,lindong28,2017-10-11T02:43:31Z,Sure. I have updated the patch to check for this illegal state.
144160448,3874,junrao,2017-10-11T23:14:38Z,"How about we change the comment to the following?

""It's possible that a partition is removed and re-added or truncated when there is a pending fetch request. In this case, we only want to process the fetch response if the partition state is ready for fetch and the current offset is the same as the offset requested."""
144160911,3874,junrao,2017-10-11T23:17:52Z,"We should throw the exception, right?"
144162641,3874,junrao,2017-10-11T23:30:32Z,"How about changing the comment slightly to the following?

""The read lock is needed to prevent the follower replica from being updated while ReplicaAlterDirThread is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica."""
144162836,3874,junrao,2017-10-11T23:32:12Z,"How about changing the comment slightly to the following?

""The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica."""
144162863,3874,junrao,2017-10-11T23:32:26Z,"How about changing the comment slightly to the following?

""The read lock is needed to prevent the follower replica from being truncated while ReplicaAlterDirThread is executing maybeDeleteAndSwapFutureReplica() to replace follower replica with the future replica."""
144168236,3874,lindong28,2017-10-12T00:15:17Z,Sure. I have updated the comment as suggested.
144168256,3874,lindong28,2017-10-12T00:15:22Z,Sure. I have updated the comment as suggested.
144168262,3874,lindong28,2017-10-12T00:15:26Z,Sure. I have updated the comment as suggested.
144168272,3874,lindong28,2017-10-12T00:15:29Z,Sure. I have updated the comment as suggested.
144168299,3874,lindong28,2017-10-12T00:15:46Z,Ah... my bad. Thanks much for catching this.
144629332,3874,junrao,2017-10-13T18:40:47Z,"Earlier, you had assertEquals(numMessages, consumerRecords.size), which seems useful. Is there a reason to remove this?"
144632221,3874,junrao,2017-10-13T18:54:22Z,"Hmm, do we need the read lock here? Replica/Log can only be changed by LeaderAndIsrRequest, StopReplicaRequest, and alterReplicaLogDirs, all of which are already protected under replicaStateChangeLock."
144632346,3874,junrao,2017-10-13T18:54:57Z,"It seems that this case should never happen. So, should we throw IllegalStateException?"
144632596,3874,junrao,2017-10-13T18:56:03Z,should performed => should be performed
144665825,3874,junrao,2017-10-13T22:01:56Z,"Hmm, it seems that sourceLog.close() can only throw KafkaStorageException because closeHandlers() has already been called due to offline log dir. So, not sure why closeHandlers() needs to be called again."
144666248,3874,junrao,2017-10-13T22:05:13Z,Is this comment correct? It seems that logSegment.close() does a superset of logSegment.closeHandlers().
144680990,3874,lindong28,2017-10-14T00:49:04Z,"Thanks for the review. I think we need readLock here is that the log directory of the current replica is not changed by ReplicaAltherLogDirsThread when the KafkaRequestHandler thread is checking whether it needs to create future replica (when it handles AlterReplicaLogDirsRequest).

If we don't use read lock here, the following scenario can happen:

1) Broker receives AlterReplicaLogDirsRequest
2) KafkaRequestHandler calls maybeCreateFutureReplica(). It finds that the log directory of the current replica is different from the user-specified destination log directory. So it creates future replica and will later try to add this partition ReplicaAlterLogDirsManager
3) ReplicaAltherLogDirsThread calls maybeReplaceCurrentWithFutureReplica(), updated the log directory of the current replica, removed the future replica from allReplicasMap, and removed this partition from ReplicaAlterLogDirsManager.
4) KafkaRequestHandler will now add this partition to ReplicaAlterLogDirsManager.
5) Now we have an inconsistent state where ReplicaAlterLogDirsManager has this partition but this partition does not have future replica. The ReplicaAlterLogDirsThread will see ReplicaNotAvailableException and fail.

Does this make sense?"
144681216,3874,lindong28,2017-10-14T00:54:15Z,"On a double thought, I realized that this could actually happen if the broker receives the same AlterReplicaLogDirsRequest multiple times, which could happen because ReassignPartitionsCommand will re-send AlterReplicaLogDirsRequest. Does this make sense?"
144681244,3874,lindong28,2017-10-14T00:54:59Z,Thanks. I will update the patch to fix it.
144681370,3874,lindong28,2017-10-14T00:57:22Z,I think it is correct. I verified that close() does not set AbstractIndex.mmap to null. Note that close() can not set mmap to null so that AbstractIndex.delete() can be executed by the scheduler thread.
144681629,3874,lindong28,2017-10-14T01:03:50Z,"I think Log.close() also does disk IO operation, e.g. when it calls producerStateManager.takeSnapshot() or FileRecords.flush(). Therefore it could throw KafkaStorageException() if the IOException occurs.

I realized that we need maybeHandleIOException() for Log.close() to catch e.g. IOException from producerStateManager.takeSnapshot(). I have updated the patch to do this.

"
144681763,3874,lindong28,2017-10-14T01:06:46Z,"The reason is that TestUtils.consumeTopicRecords() will throw exception if the ConsumeRecords.size() is not exactly the numMessages(). Do you prefer me to add back assertEquals(numMessages, consumerRecords.size) so that this check is more explicit in the test?"
144913957,3874,junrao,2017-10-16T17:30:53Z,Thanks for the explanation. This is fine then.
144913975,3874,junrao,2017-10-16T17:30:58Z,"Thanks, sound goods."
144913998,3874,junrao,2017-10-16T17:31:04Z,"Ok. Then ""File handlers"" can be a bit confusing since we do close the channel associated with the log segment. How about changing the comment to the following?

""The memory mapped buffer for index files of this log will be left open until the log is deleted"""
144914073,3874,junrao,2017-10-16T17:31:23Z,"Hmm, this is the case that the future replica exists, but not in the specified log dir.  Resending the same AlterReplicaLogDirsRequest won't lead to this case, right?"
144914169,3874,junrao,2017-10-16T17:31:52Z,"I guess the scenario that you are describing is that there is already an ongoing replica movement across disks at step 2. However, in that case, it seems that we will remove the partition from replicaAlterLogDirsManager and the future replica in line 591 and 592. After that point, ReplicaAlterLogDirsThread can't call maybeReplaceCurrentWithFutureReplica() any more because the additional partition state check that we added in AbstractFetcherThread, right?"
144995830,3874,lindong28,2017-10-16T23:30:42Z,"Not sure I explained my thought clearly. I think here the case is that the future replica already exists in the specified log dir. 

More specifically, when the broker receives the same AlterReplicaLogDirsRequest again, it calls ReplicaManager.alterReplicaLogDirs(). alterReplicaLogDirs() should only add this partition to replicaAlterLogDirsManager if this future replica has not already been created. So alterReplicaLogDirs() calls maybeCreateFutureReplica(), and maybeCreateFutureReplica() should return true iff the future replica is newly created. Because it is possible that future replica already exists, we should not throw IllegalStateException. Does this sound OK?


"
144995860,3874,lindong28,2017-10-16T23:30:59Z,Sure. I have updated the comment and related method names as suggested.
144996257,3874,lindong28,2017-10-16T23:34:00Z,"@junrao The scenario is that there is already an ongoing replica movement for this same partition to the same destination log directory, which can happen if broker receives the same AlterReplicaLogDirsRequest again. Because the requested destination log directory of the AlterReplicaLogDirsRequest is same as the destination log directory of the ongoing movement, we will not remove the partition from replicaAlterLogDirsManager. Does this make sense?"
144999197,3874,lindong28,2017-10-16T23:56:36Z,"Actually, in the scenario that there is already an ongoing replica movement to a different destination log directory, the following may happen:

- ReplicaAltherLogDirsThread calls maybeReplaceCurrentWithFutureReplica(), updated the log directory of the current replica and removed the future replica from allReplicasMap()

- KafkaRequestHandler calls alterReplicaLogDirs(). Because future replica has been removed, it will not remove this partition from replicaAlterLogDirsManager. Then it will create the future replica for this partition and add fetcher for this partition.

- Then ReplicaAltherLogDirsThread will remove fetcher for this partition. This leads to an inconsistent state where we have future replica for this partition but this partition is not in replicaAlterLogDirsManager.

The main issue is that, we do not have a lock that makes it an atomic operation to 1) add/remove partition from replicaAlterLogDirsManager and 2) add/remove future replica for the partition. Protect maybeCreateFutureReplica() with `leaderIsrUpdateLock` could reduce the chance of this race condition.

"
145007121,3874,junrao,2017-10-17T01:06:10Z,"@lindong28 : Thanks for the info. Makes sense. Perhaps we should throw an IllegalStateException if future replica exists, but is on a log dir different from the input?"
145007148,3874,junrao,2017-10-17T01:06:34Z,"@lindong28 : In the scenario that there is already an ongoing replica movement to a different destination log directory, I am not sure what you described can happen. maybeReplaceCurrentWithFutureReplica() is called inside processPartitionData() and is protected by the AbstractFetcherThread.partitionMapLock. So, while this is ongoing, no new partitions can be added to the fetcher.

The other scenario that there is already an ongoing replica movement for this same partition to the same destination log directory does make sense. So, we can keep the readLock there."
145007951,3874,lindong28,2017-10-17T01:14:41Z,Sure. I have updated the code to throw IllegalStateException if the current log dir of the future replica is different from the requested log dir. Thanks!
145008087,3874,lindong28,2017-10-17T01:16:16Z,Thanks much for the discussion!
1400867677,3874,Hongten,2023-11-21T16:32:47Z,"I have a question about the previous var. If the log dir name doesn't end with ""-future"", the `currentLogs` will put the topic partition as key and log as value, and return log. The simple logic like the below:
```
val previous = {log}
```
If the `previous` is not null, then there is an `IllegalStateException` to be thrown. right?"
59305175,1215,becketqin,2016-04-12T00:37:28Z,"I have a question regarding the code here. On line 199 of this patch (line 192 of the original code), we check if the index file exists or not. However, right before this, we have already created the log segment which will create the index file if it does not exist. So it seems the test here will never be false?
"
59380790,1215,ijuma,2016-04-12T14:02:16Z,"Was it intentional to remove the volatile annotation from this var?
"
59381324,1215,ijuma,2016-04-12T14:05:03Z,"Also, do we actually need to mutate the vars above in the subclasses? If not, they should remain `private[this]` as they were before. We have accessor methods (without the underscore).
"
59381969,1215,ijuma,2016-04-12T14:08:30Z,"A tuple of 3 elements is not great for readability, I think it's time to introduce a case class for the method result.
"
59382177,1215,ijuma,2016-04-12T14:09:43Z,"Unintentional indent.
"
59425499,1215,becketqin,2016-04-12T18:13:02Z,"I was actually wondering why we need these variables? They are just a derived value based on `mmap.limit` and `mmap.position`. Shouldn't we just define them as methods?
"
59429145,1215,ijuma,2016-04-12T18:33:05Z,"It's a good question. I assumed they were there for performance reasons,  but I haven't checked.
"
59433036,1215,becketqin,2016-04-12T18:54:25Z,"Thought about this again. Having the variables makes sense because we do not lock the mmap on read. That means the read can happen when the writing of an index entry is half-done. I'll keep the variables.
"
60862080,1215,junrao,2016-04-25T04:38:00Z,"typo abastract
"
60862083,1215,junrao,2016-04-25T04:38:04Z,"key/value combination => key or value
"
60862084,1215,junrao,2016-04-25T04:38:10Z,"Should we assert that exactly one of targetKey and targetKey is not empty?
"
60862100,1215,junrao,2016-04-25T04:38:40Z,"When searching for offset, the passed in value is the absolute offset whereas the offsets in the index are relative. So, we use to translate the absolute offset to the relative one in indexSlotFor(). That logic seems to be lost in the new code.
"
60862105,1215,junrao,2016-04-25T04:38:48Z,"The description starting from line 155 seems redundant. The description in line 150 and 151 explains this method pretty well. 
"
60862107,1215,junrao,2016-04-25T04:38:53Z,"It seems it's more natural to return None in this case.
"
60862130,1215,junrao,2016-04-25T04:39:07Z,"Passing in handleAndMaybeStop seems a bit more complicated than necessary. I am wondering if we can just reuse the iterator() code to be able to iterate MessageAndOffset from an arbitrary position. The caller can decide what to do.
"
60862136,1215,junrao,2016-04-25T04:39:22Z,"typo: mappying and timestamp
"
60862140,1215,junrao,2016-04-25T04:39:26Z,"Unused imports.
"
60862143,1215,junrao,2016-04-25T04:39:33Z,"The comment on line 172 is now outdated. Perhaps we can just say all index files.
"
60862149,1215,junrao,2016-04-25T04:39:45Z,"That's a good point. It seems that we should check the existence of the index file before creating LogSegment.
"
60862151,1215,junrao,2016-04-25T04:39:56Z,"The warning log in line 205 always refers to indexFile, which is not necessarily correct now.
"
60862158,1215,junrao,2016-04-25T04:40:12Z,"Hmm, it seems that we should insert the offset in segment i that has the largest timestamp. Not sure why we want to insert the offset in the next segment.
"
60862171,1215,junrao,2016-04-25T04:40:34Z,"Hmm, does the seg.largestTimestamp always correspond to logEndOffset?
"
60862192,1215,junrao,2016-04-25T04:41:35Z,"The current getOffsetBefore() api is a bit awkward to use since it's tightly coupled with how we use the last modified time in each log segment. For example, it doesn't seem to make sense to return a sequence of offsets now that we have message level timestamp. So, it's probably simpler and better to leave the current implementation of getOffsetBefore() as it is. In Log, LogSegment, and TimeIndex, we expose a simple api that takes a timestamp and returns a single offset whose timestamp is >= than the input. In the future, we can design a new request that exposes this capability to the client.
"
60862196,1215,junrao,2016-04-25T04:41:44Z,"Hmm, I think we want to show both the actual size and the max size, not the ratio.
"
60862197,1215,junrao,2016-04-25T04:41:50Z,"Could we just call entry.getValue once?
"
60862201,1215,junrao,2016-04-25T04:41:58Z,"Hmm, is this right? The offset associated with the largest timestamp may not be the last offset in the time index.
"
60862203,1215,junrao,2016-04-25T04:42:02Z,"Is lastOffset ever used?
"
60862208,1215,junrao,2016-04-25T04:42:09Z,"Should we compute maxTimestampSoFar before appending to the timeIndex?
"
60862210,1215,junrao,2016-04-25T04:42:13Z,"typo retrun
"
60862213,1215,junrao,2016-04-25T04:42:21Z,"Not sure why we need max here since we should always be able to find a timestamp >= lastTimeIndexEntry.timestamp.
"
60862216,1215,junrao,2016-04-25T04:42:32Z,"Not sure why we need ""The timestamp is the max timestamp before that offset.""
"
60862218,1215,junrao,2016-04-25T04:42:34Z,"typo timestmaps
"
60862221,1215,junrao,2016-04-25T04:42:38Z,"larger than => larger than or equal to
"
60862225,1215,junrao,2016-04-25T04:42:45Z,"store => stored
"
60862228,1215,junrao,2016-04-25T04:42:46Z,"typo monitonically 
"
60862233,1215,junrao,2016-04-25T04:43:05Z,"Is it worth maintaining _lastTimestamp and _lastOffset since they can be obtained from lastEntry?
"
60862237,1215,junrao,2016-04-25T04:43:17Z,"If there is no return value, the convention is not to use =. So, it will be 

def maybeAppend() {

}
"
60862243,1215,junrao,2016-04-25T04:43:25Z,"Do you mean ""timestamp can't be"" instead ""timestamp can be""?
"
60862249,1215,junrao,2016-04-25T04:43:33Z,"TimestampOffset(timestamp, offset) != lastEntry seems a more expensive check. Could we just check timestamp and _lastTimestamp?
"
60862253,1215,junrao,2016-04-25T04:43:41Z,"This check seems useless since _entries is calculated from mmap.position.
"
60862264,1215,junrao,2016-04-25T04:44:04Z,"Hmm, not sure why this logic here is different from that in OffsetIndex.truncateTo. If we find an index entry that matches offset exactly, it seem that we should delete that slot from the time index too.
"
60862266,1215,junrao,2016-04-25T04:44:11Z,"The error message is only for timestamp, but the failure could be due to offset.
"
60862269,1215,junrao,2016-04-25T04:44:15Z,"Should 35% be 33%?
"
60862271,1215,junrao,2016-04-25T04:44:16Z,"the each => each
"
60923190,1215,junrao,2016-04-25T14:36:11Z,"We should insert the offset corresponding to maxTimestampSoFar, right?
"
60952050,1215,becketqin,2016-04-25T17:22:10Z,"This logic has been moved to `parseIndexEntry()` in OffsetIndex and TimeIndex. This is to make `indexSlotFor()` entry format agnostic.
"
60956589,1215,becketqin,2016-04-25T17:50:02Z,"Good point. I think we can use the iterator for timestamp search. We probably still need to keep a separate scan function for offset search because the iterator does not give the position. But this will be only used by offset search so there is no function argument passed in.
"
60967454,1215,becketqin,2016-04-25T18:52:36Z,"If we check the existence of the index files before creating log segment, would it be a little difficult to distinguish between 1) the upgrade case and 2) time index file is really missing? In (1), we want to just create an empty time index without rebuilding the time index. In (2), we want to rebuild the entire time index.

I am wondering in which case will we miss a index? Is it only when the index is deleted manually?
"
60968317,1215,becketqin,2016-04-25T18:57:47Z,"Hmm, if we return None, what timestamp and offset response should we return to the user? We cannot simply use the log end offset or HW because it is possible the log end offset or HW has grown after the search. So it seems we should tell the caller that up until which offset we have searched?
"
60974231,1215,becketqin,2016-04-25T19:35:42Z,"The way we index the timestamp changed a little since the original proposal. 

The original proposal was that a timestamp entry `(T, OFFSET)` means the message with timestamp `T` is at `OFFSET`. Now it means that before `OFFSET`, the largest timestmap we have seen is `T`. So the offset does not point to the message with the largest timestamp any more.
The benefit of the current way is that it is a little easier to build and maintain.

So in this case, we are treating the entire log segment as a big message set and assuming the last modification time is the largest timestamp in this segment. So that means before the base offset of the next segment, the largest timestamp we see in this segment is the last modification time of the segment. That is why the last time index entry in each inactive segment always points to the base offset of the next segment.
"
60974567,1215,becketqin,2016-04-25T19:37:52Z,"There is no real logic change here. I just changed the printing code format to a shorter pattern.
"
60981807,1215,becketqin,2016-04-25T20:23:08Z,"Please see the previous reply. The offsets is no longer the offset of the message with maxTimestampSoFar now.
"
60982883,1215,becketqin,2016-04-25T20:29:09Z,"If the starting position is the position from the last time index entry, it is not guaranteed that we can find a timestamp >= lastTimeIndexEntry.timestamp.
"
60995200,1215,becketqin,2016-04-25T21:45:14Z,"I was trying to say that when the log segment tries to append a time index entry, it is not guaranteed the timestamp of that entry is strictly larger than the timestamp of previous entries. But the offset has to be strictly larger. We probably should also check the timestamp as well.
"
60996355,1215,becketqin,2016-04-25T21:53:24Z,"This check is to avoid throwing exception in cases when we insert the last entry to the time index when the log segment is closed or rolled. In those cases, we may see attempt to insert a duplicate a time index entry whose offset is the same as the previous one. We can probably split it into two if statement.
"
60996692,1215,becketqin,2016-04-25T21:55:58Z,"Actually the last check won't even be evaluated unless the previous two statement are true. Not sure if performance is a concern here.
"
60996885,1215,becketqin,2016-04-25T21:57:13Z,"_entries is a variable loaded from mmap.position when then index is created, but after that it is maintained separately. The original offset index has the check so I just left it unchanged.
"
61005130,1215,becketqin,2016-04-25T23:04:06Z,"This is also related the to semantic meaning of the time index. Because the offset in the index entry is the ""next"" offset. So if the truncated to index is the same offset in the index entry, we should keep the entry because the max timestamp corresponding to that time index entry are not truncated.
"
61006403,1215,becketqin,2016-04-25T23:16:09Z,"If we compute the maxTimestampSoFar before appending to the timeindex, we will require the lastOffset of the messages. If we do it in this way we only need the firstOffset.
"
61188911,1215,Ishiihara,2016-04-27T00:49:44Z,"The parentheses may not be needed here as this function does not have side effect. Correct me if I am wrong. 
"
61188922,1215,Ishiihara,2016-04-27T00:49:50Z,"Maybe add another log after the file is successfully deleted.
"
61188929,1215,Ishiihara,2016-04-27T00:49:56Z,"Are we always rounding up or rounding down? It would be nice to reflect that in the function name. 
"
61188934,1215,Ishiihara,2016-04-27T00:50:01Z,"assume all entries are valid and set the position to the last entry
"
61188942,1215,Ishiihara,2016-04-27T00:50:05Z,"How about using a different error message? e.g. entry size exceeds max index size.
"
61189019,1215,Ishiihara,2016-04-27T00:51:03Z,"May be remove forcefully?
"
61189107,1215,Ishiihara,2016-04-27T00:52:21Z,"How bout log in the error level? 
"
61326011,1215,Ishiihara,2016-04-27T20:03:02Z,"The comments of the return value are inconsistent. largest_timestamp_checked_before_target_timestamp and largest_timestamp_checked. 
"
61326015,1215,Ishiihara,2016-04-27T20:03:04Z,"This equivalent as -> This is equivalent to
"
61674500,1215,becketqin,2016-04-30T20:20:34Z,"It seems this message is warning about a wrong maxIndexSize argument, but not because there are too many entries. So the error message seems right. But we can probably add the current index size in the log.
"
61674869,1215,becketqin,2016-04-30T20:48:16Z,"This is a pre-existing comments. I am not sure if it is really a ""forceful"" free. So I just leave it as is.
"
61674895,1215,becketqin,2016-04-30T20:50:28Z,"Not sure if it is needed. If something wrong happened, an exception should be thrown and we will see that in the log. Otherwise it is removed successfully. So no exception = success?
"
61676699,1215,Ishiihara,2016-04-30T23:10:43Z,"It seems a bit nicer if we use string interpolation. 
"
61676701,1215,Ishiihara,2016-04-30T23:10:47Z,"Again not a big deal, but we can use pattern matching to be more Scala-like. 

```
try {
  m match {
    case dm: sun.nio.ch.DirectBuffer => dm.cleaner().clean()
  }
} catch {
  case t: Throwable => error(""Error when freeing index buffer"", t)
}
```
"
61676704,1215,Ishiihara,2016-04-30T23:10:54Z,"nit: comment is not aligned with code below.
"
61676705,1215,Ishiihara,2016-04-30T23:10:57Z,"Not a bit deal, but we need to import `IOException` to make the IDE happy. 
"
61684116,1215,ijuma,2016-05-01T09:09:56Z,"It is true that pattern matching is a more Scala-like. The suggested code is missing a case entry if the first case doesn't match though.
"
64971303,1215,junrao,2016-05-27T21:53:50Z,"Unused import IOException
"
64971332,1215,junrao,2016-05-27T21:54:10Z,"This interface is a bit awkward. Since we expect either key or value to be set, another option is to have sth like the following.

protected def indexSlotFor(idx: ByteBuffer, target: K, boolean isTargetForKey):
"
64971340,1215,junrao,2016-05-27T21:54:17Z,"The comment on return value is duplicated as the one in line 170.
"
64971410,1215,junrao,2016-05-27T21:54:58Z,"Could we just return the base offset in this case?
"
64971431,1215,junrao,2016-05-27T21:55:10Z,"typo timestmap
"
64971444,1215,junrao,2016-05-27T21:55:19Z,"Perhaps it's better to do the try/catch for each index file so that we know which one is corrupted?
"
64971523,1215,junrao,2016-05-27T21:56:00Z,"Hmm, for those segments w/o time index because they were created in 0.9, we will update the index with the last modified time. However, it seems that we didn't change maxTimestampSoFar in the log segment after that since  logSegmentsSeq(i).loadLargestTimestamp() is called before updating the index?
"
64971535,1215,junrao,2016-05-27T21:56:11Z,"Hmm, it seems that if assignOffsets is false, we won't set  maxTimestampInMessageSet properly? Perhaps we should initialize maxTimestampInMessageSet in analyzeAndValidateMessageSet()?
"
64971545,1215,junrao,2016-05-27T21:56:15Z,"returnOffsets is unused
"
64971577,1215,junrao,2016-05-27T21:56:30Z,"Does the timestamp in the last message necessarily have the largest timestamp?
"
64971583,1215,junrao,2016-05-27T21:56:33Z,"No need for Unit.
"
64971598,1215,junrao,2016-05-27T21:56:37Z,"No need to have Unit =. There are a few other places like that.
"
64971603,1215,junrao,2016-05-27T21:56:39Z,"typo retrun
"
64971625,1215,junrao,2016-05-27T21:56:50Z,"Hmm, in this case, should we return lastOffset + 1 in the segment and perhaps with a max timestamp?
"
64971644,1215,junrao,2016-05-27T21:56:58Z,"Intuitively, if there is no message, should largestTimestamp be MaxLong?
"
64971662,1215,junrao,2016-05-27T21:57:06Z,"Perhaps we should move this comment to before line 125?
"
64971685,1215,junrao,2016-05-27T21:57:18Z,"Since there is no close() method in TimeIndex, we probably want to clarify that the logic of adding the last timestamp index entry is in LogSegment.onBecomingActiveSegment().
"
64971723,1215,junrao,2016-05-27T21:57:40Z,"Typo validatd
"
64971763,1215,junrao,2016-05-27T21:58:00Z,"It seems that maxTimestamp won't be set properly if messageTimestampType is LOG_APPEND_TIME?
"
64971776,1215,junrao,2016-05-27T21:58:05Z,"Could we update the comment with respect to the return type?
"
64971791,1215,junrao,2016-05-27T21:58:11Z,"It would be useful to add a comment explaining the return type before the method.
"
64971800,1215,junrao,2016-05-27T21:58:16Z,"log timestamp => offset
"
64971805,1215,junrao,2016-05-27T21:58:19Z,"Hmm, not sure what this is for.
"
64972732,1215,Ishiihara,2016-05-27T22:10:04Z,"The IOException is used in ScalaDoc.
"
64998112,1215,becketqin,2016-05-29T06:05:53Z,"Hmm, but the Key and Value type might be different, so only defining the target to be the type of key seems not enough.

Maybe it is less awkward if have sth like:
`protected def indexSlotFor(idx: ByteBuffer, target: IndexEntry[K, V], searchByEntity: IndexSearchEntity):`
The IndexSearchEntity is an enumeration that is either SearchByKey or SearchByValue.
"
64998229,1215,becketqin,2016-05-29T06:20:56Z,"Actually never mind. Since we only search by key, so having key is probably good enough.
"
64998294,1215,becketqin,2016-05-29T06:29:16Z,"We do search by value when truncating the time index... So maybe we still need to pass in the index entry.
"
64998628,1215,becketqin,2016-05-29T07:16:57Z,"If there is no message after the starting point, it means we have reached the log end. Ideally we should return the offset that we started to search with, but we only have position in this method. So we return None here to let the caller know that we have reached the log end and the caller will return the offset corresponding to the physical starting position.
"
64999284,1215,becketqin,2016-05-29T08:12:38Z,"I added that according to the Scala coding style guide. Maybe we can do a refactoring separately if we want to.
http://docs.scala-lang.org/style/declarations.html#methods
"
64999465,1215,ijuma,2016-05-29T08:27:13Z,"Yeah, Kafka currently deviates from that convention. I prefer the Scala coding style guide way, but there isn't a strong reason either way (one way is a little more consistent, but is more verbose). So, it probably makes sense to stick with Kafka's way for now as it would create a lot of diffs to change.
"
65019054,1215,becketqin,2016-05-30T02:53:53Z,"If `log.searchForTimestamp()` returns None, that means there is no message at or after the position we started to search with. It could happen when:
1. The log segment is empty
2. The log segment is truncated after we got the position from the offset index.

For case (1) maybe we should just return (NoTimestamp, baseOffset). For case (2) maybe we should propagate the None to the caller so the caller can retry the search. This does not completely solve the problem that we may return an offset to the user that is later truncated, but would avoid it as much as possible. Does that sound reasonable?
"
65019819,1215,becketqin,2016-05-30T03:10:57Z,"Previously we don't have a concept of largest timestamp for a log segment. Implicitly the largest timestamp is the last modification time. So all the logic around timestamp is built based on last modification time. With KIP-33 supposedly most of the previous logic that are depending on `lastModified()` should now be using largest timestamp.

maxTimestampSoFar is only updated when it sees actual timestamp in a message (except for old inactive log segments where maxTimestampSoFar is always the same as last modification time and does not change).

I am thinking to leave `lastModified()` as it is but replace the external usage of `lastModified()` with `largestTimestamp()`. `largestTimestamp()` prefers to use the maxTimestampSoFar if it exists, but falls back to lastModified if no message has a timestamp, such that the external logic would be the same for all segments no matter whether they contains timestamp or not. I may have missed some usages of `lastModified()` in the current patch. I will fix that in the coming patch. 
"
65023366,1215,becketqin,2016-05-30T04:55:32Z,"This is printing out the mismatches. The first timestamp is the timestamp in the index, and the second timestamp is the mismatch timestamp in the log. So the printed string seems correct?
"
65023563,1215,becketqin,2016-05-30T05:01:36Z,"I was trying to list the offsets of the mismatch timestamp index entry. But the format seems wrong, I am thinking of something like the following:

When timestamp does not match.
`timestamp: 1000 (log timestamp: 2000) offset: 100`

When timestamp and offset both do not match
`timestamp: 1000 (log timestamp: 2000) offset: 100 (log offset: 200)`
"
65999852,1215,junrao,2016-06-07T02:00:58Z,"Would it be better to name this SearchType?
"
65999868,1215,junrao,2016-06-07T02:01:09Z,"Hmm, this may not be reliable. Could we just compare this to (Long) Int.Max?
"
65999871,1215,junrao,2016-06-07T02:01:13Z,"unused import
"
65999886,1215,junrao,2016-06-07T02:01:27Z,"To be consistent with the code, maxTimestamp should be before timestamp.

Also, timestamp is too generic and it's not clear what it really means. How about rename it to logAppendTime?
"
65999931,1215,junrao,2016-06-07T02:02:11Z,"Hmm, this means that when upgrading, we are forced to scan all existing log segments since they won't have the timeindex. Could we just create an empty timeindex file in this case? See my other comment in LogSegment on whether it's useful to always have a non-empty timeindex.
"
65999965,1215,junrao,2016-06-07T02:02:42Z,"Hmm, we should be checking timeIndex.entries instead of maxEntries, right?
"
65999990,1215,junrao,2016-06-07T02:03:12Z,"Hmm, is the while loop necessary? It seems that if foundOffset is None, we can just return baseOffset of next segment or nextOffset if the search is on the active segment. Also, if we have an empty log, do we loop forever?
"
66000164,1215,junrao,2016-06-07T02:05:21Z,"In the proposal of KIP-33, we changed the behavior of log rolling to be that if the largestTimestamp in the segment hasn't changed for the log rolling time, we will roll another log segment. Thinking about this a bit more, I am wondering if it's better to preserve the current log rolling logic based on the log segment create time. The current behavior provides a nice property that a log segment is guaranteed to be rolled within certain amount of time. This is useful since there are use cases where people may have some sensitive data that has to be deleted or cleaned after certain amount of time. The new behavior will make this harder to enforce since the active segment is never deleted or cleaned and there is no time bound on when the new segment can be rolled.

The log deletion policy can still be based on the timestamp in the message.
"
66000207,1215,junrao,2016-06-07T02:06:06Z,"extra space before *
"
66000218,1215,junrao,2016-06-07T02:06:19Z,"extra space before *
"
66000237,1215,junrao,2016-06-07T02:06:33Z,"Should we append nextOffset or baseOffset here?
"
66000245,1215,junrao,2016-06-07T02:06:39Z,"extra space before *
"
66000271,1215,junrao,2016-06-07T02:06:58Z,"Hmm, if none of the message has timestamp, should we include the offset of the first message in the segment?
"
66000275,1215,junrao,2016-06-07T02:07:04Z,"We should document when we return None.
"
66000282,1215,junrao,2016-06-07T02:07:08Z,"foundTimestampOffsetOpt is unused.
"
66000320,1215,junrao,2016-06-07T02:07:44Z,"Currently, we have the logic to force adding an index entry (with last modified timestamp) if none of the message has timestamp. I am wondering why this is necessary. It seems that we can just leave the timeindex empty in that case and largestTimestamp() can still return lastModified? That may make the log recovery logic a bit simpler.
"
66000326,1215,junrao,2016-06-07T02:07:49Z,"unused import
"
66000354,1215,junrao,2016-06-07T02:08:10Z,"The comment here says insert a time index entry with the last modified time and the base offset. However, in LogSegment.onBecomeInactiveSegment(), we insert the next offset, not the base offset. It seems that inserting the base offset makes sense.
"
66000366,1215,junrao,2016-06-07T02:08:23Z,"Is the comment accurate? maybeAppend() may see the same timestamp as long as no new messages have a larger timestamp.
"
66000386,1215,junrao,2016-06-07T02:08:43Z,"The comment is not accurate. We don't return a number of entries. Also, we are trying to find an entry with timestamp >= than targetTimestamp
"
66000434,1215,junrao,2016-06-07T02:09:22Z,"We will need to explain the output better. Perhaps when we print ""Mismatches in :"" , we can say these are cases where the timestamp in the wrapper message doesn't match the inner. Also, it would also be useful to capture errors in which the timestamps are not increasing in the index.
"
66187001,1215,becketqin,2016-06-08T03:13:26Z,"The assumption here is that `this.start + position` is a positive integer, and `size` is also a positive integer. When you say not reliable, do you mean `this.start + position` can also overflow?
"
66188197,1215,becketqin,2016-06-08T03:34:23Z,"The current code actually creates the time index file in line 195 when it instantiate the LogSegment. So the else branch seems never be called. I asked that question earlier and you suggested that we can check the index existence before we instantiate the LogSegment. But the problem of that is we are not able to distinguish between 1) upgrade case and 2) the time index is missing. We want to create an empty index in case 1) and want to rebuild the time index in case 2).

I thought case 2) only occurs when the time index is manually deleted, which is rare. Therefore I left the code as it is, i.e. always create an empty time index when it is missing. This is essentially assuming that when a time index is missing it is the upgrade case.
"
66188736,1215,becketqin,2016-06-08T03:46:00Z,"This is to make sure the time index file is not resized to zero so it can hold at least one time index entry. But putting the check here is a little confusing, I will move it into ensureNonEmptyTimeIndex().
"
66296450,1215,becketqin,2016-06-08T17:06:48Z,"I am not sure if we have the same guarantee even if we still let the log rolling based on the log segment create time. There are a few issues,
1. When the timestamp type is CreateTime, the messages in the segment may have older timestamp than the create time of the log segment. So the log rolling may still be delayed from the application perspective.
2. In some linux system, the create time of a file is not available. So based on segment file create time may not always work.

I am thinking that maybe we can just let the log rolling based on the timestamp of the first time index entry. We can always insert an index entry when the first message is appended to the log segment. This provides similar guarantee of the previous log rolling and does not have the above two issues. What do you think?
"
66299695,1215,becketqin,2016-06-08T17:25:05Z,"The main idea behind this while loop is to make sure we give user a good offset to our best effort.

`targetSeg.findOffsetByTimestamp()` only returns None when log truncation occurred after we find the physical position by looking up the offset index. In that case, the may be no message after the physical position because it could have been truncated. We cannot return the nextOffset in that case because it is possible that some new messages have been appended after `targetSeg.findOffsetByTimestamp()` returns. We haven't checked the timestamp of those messages, so returning nextOffset in this case might result in skipping messages with larger timestamp.

That said, this case should be very rare because truncation usually only occurs on the followers, so they should not server offset request. However, if we have a fast leadership fail over and failback, it could still happen.

When the log segment is empty, baseOffset is returned by `targetSeg.findOffsetByTimestamp()`.
"
66301258,1215,becketqin,2016-06-08T17:32:07Z,"I was thinking the semantic meaning of this is that the timestamp of the last message in the segment is lastModified. But you are right that during a search by timestamp, we would want to start from the base offset. 
"
66302572,1215,becketqin,2016-06-08T17:39:05Z,"I think we do return the offset of the first message if none of the messages has timestmap.
"
66318868,1215,junrao,2016-06-08T19:04:05Z,"Right, could this.start + position overflow to a negative value and then + size bring it to positive again?
"
66319046,1215,junrao,2016-06-08T19:05:03Z,"Interesting. Then, in the code, perhaps we should just get rid of the check of the existence of the offset and the time index files since they are guaranteed to be present after LogSegment is created? I agree the case that people manually delete the index files is rare.
"
66319077,1215,junrao,2016-06-08T19:05:14Z,"Ok, perhaps we can add this in the javadoc.
"
66319260,1215,junrao,2016-06-08T19:06:23Z,"Ok, perhaps we can add some comments to make it clear why we need to do the loop. A related question is what happens when there is no offset whose timestamp is >= than the target timestamp? Should we return the log end offset or sth like -1? If it's the former, the user may not know the fact that there is no offset with a larger or equal timestamp. 
"
66319334,1215,junrao,2016-06-08T19:06:48Z,"Yes, what you suggested sounds like a good idea. I would just modify that slightly to do the time-based rolling based on the timestamp of the first message in the segment. We probably want to update the KIP wiki and bring this up in the mailing list to see if people have any concern about the change.
"
66376458,1215,becketqin,2016-06-09T03:05:27Z,"Theoretically it is possible. But if it is a log segment it seems not possible because the `this.start` would be 0, so it is essentially `position + size`. I don't know if we ever call `read()` on a sliced FileMessageSet.
"
66377571,1215,becketqin,2016-06-09T03:22:58Z,"Thought about this a little more, it seems that it would be better to append the nextOffset - 1. The offset is essentially used in two cases:
1) Search by timestamp
2) Log truncation.

For case 1, The previous behavior was that if the target timestamp is greater than the lastModified, we will simply start from the next segment. This behavior will be changed if we append the baseOffset here. i.e. user needs to start consume from the beginning of this segment instead of the beginning of next segment.

For case 2, if an inactive log is truncated and becomes the active log segment again, we will want to truncate the time index entry as well, if we append the nextOffset -1 here, the time index will also be truncated. But if we append baseOffset here, the time index entry won't be truncated.
"
66479759,1215,becketqin,2016-06-09T16:58:29Z,"@junrao The patch currently returns log end offset if there is no offset whose timestamp is greater than the target timestamp. I am wondering in which case user would want to know there is no timestamp greater or equals to the target timestamp? Currently, user can always start consuming from the returned offset and it is guaranteed that no message with larger timestamp will be missed. If we return -1, what user would do in that case? They can not seek to the end because after we return -1, some messages with larger timestamp could have been appended so consuming from log end offset will miss those messages.
"
66480437,1215,becketqin,2016-06-09T17:02:43Z,"Yes based on the timestamp of the first message sounds good. More precisely the time based log rolling will be based on the first message that has a timestamp if there is at least one such message. If there is no message that has a timestamp, the time based log rolling will be based on the create time, which is the same as previous behavior. 
"
66536970,1215,junrao,2016-06-09T23:00:02Z,"Yes, that sounds reasonable.
"
66540277,1215,junrao,2016-06-09T23:33:48Z,"I looked at the commit history on this class a bit more. It seems that the existing logic is because of a bug introduced in KAFKA-2012. We should check the existence of the offset index file before instantiating a LogSegment, not after. Then, if the offset index is missing, we will rebuild the index. So, perhaps we can fix the logic here as the following. (1) If offset index is missing, rebuild both indexes. (2) If only the timeindex is missing, create an empty timeindex file (i.e., assuming that it's an old segment from pre-v10).
"
66561253,1215,becketqin,2016-06-10T04:45:37Z,"That is a good point. I think it should work. 

There is a slight difference. The current patch essentially takes a snapshot of the timestamp when last message is appended to the log segment. So even if user touched the segments later and changed the modification time, it does not affect the log retention. If we change the behavior to only use lastModified of the file, it could change if user touch the file later. I remember our SRE used to do touch the file for some reasons, but I am not sure if this is an important difference. 
"
66561646,1215,becketqin,2016-06-10T04:54:58Z,"I see. That sounds reasonable to me.
"
66696679,1215,junrao,2016-06-11T00:21:04Z,"Yes, after the producer upgrades to 0.10, all new messages will have a timestamp and the time-based retention will be more accurate. For the old messages, the current behavior is to use the current last modified time for retention. We don't really have to improve it if that makes the new code simpler.
"
67100177,1215,junrao,2016-06-15T05:05:33Z,"This iterator actually copies the key and value from the file channel to the buffer. For searching timestamp, we could do a special iteration that just copies the header part (like how we do in search by offset). We need to think through whether it's worth specializing.
"
67100215,1215,junrao,2016-06-15T05:06:14Z,"Hmm, is that true? The segment could have messages with and w/o timestamp. Stopping early may prevent us from finding the offset with the timestamp that we want. 
"
67100246,1215,junrao,2016-06-15T05:06:25Z,"In this case, should we really maxTimestampChecked since it is not really associated with lastOffsetChecked + 1?
"
67100351,1215,junrao,2016-06-15T05:06:48Z,"Do we need to duplicate the code for index and timeindex? Since both indexes are of AbstractIndex, perhaps we can just iterate two AbstractIndex in a loop and do the sanityCheck and recover in the loop (we may need to add a absolutePath() method in AbstractIndex).
"
67100430,1215,junrao,2016-06-15T05:07:17Z,"It's a bit awkward to have to create LogSegment here and in line 196. We can avoid that by just doing sth like the following
val indexExist = indexFile.exists()
segment = new LogSegment(...)
if (indexExist) 
else
"
67100453,1215,junrao,2016-06-15T05:07:40Z,"For segments where no message has timestamp, should we really use last modified time?
"
67100460,1215,junrao,2016-06-15T05:07:43Z,"typo timestaamp
"
67100462,1215,junrao,2016-06-15T05:07:47Z,"an => a
"
67100482,1215,junrao,2016-06-15T05:08:04Z,"It seems that we can retain the tombstone using the message timestamp, instead of segment time. We can file a separate jira to track that.
"
67100485,1215,junrao,2016-06-15T05:08:09Z,"no need for extra space before startMs
"
67100491,1215,junrao,2016-06-15T05:08:13Z,"maxTimestampSoFarBeforeAppend is never used.
"
67100692,1215,junrao,2016-06-15T05:10:25Z,"Do we have to append the timestamp for the first message to the index? If so, we are not doing that consistently in recover(). For log rolling, it seems that we can just track the timestamp of the first message (if NoTimestamp, just use the create time) during startup or append w/o needing the index entry.
"
67100718,1215,junrao,2016-06-15T05:10:48Z,"If we update offsetOfMaxTimestamp, don't we need to update offsetOfMaxTimestamp accordingly?
"
67100724,1215,junrao,2016-06-15T05:10:55Z,"Could we improve the text to make it clearer how the index is corrupted?
"
67100745,1215,junrao,2016-06-15T05:11:19Z,"There is inconsistency in the caller of maybeAppend(). Some of them does 

```
  if (maxTimestampSoFar >= 0)
    timeIndex.maybeAppend(maxTimestampSoFar, offsetOfMaxTimestamp)
```

and some others don't do the check. It would be better if we can do this consistently. It seems if maybeAppend() just ignores negative timestamp, the caller doesn't have to do the check anymore.
"
67100751,1215,junrao,2016-06-15T05:11:26Z,"This comment is a bit weird in that it tries to explain things that we don't do. Is it useful?
"
67100757,1215,junrao,2016-06-15T05:11:31Z,"Could we improve the text to make it clearer how the index is corrupted?
"
67100764,1215,junrao,2016-06-15T05:11:34Z,"on => one
"
67100776,1215,junrao,2016-06-15T05:11:44Z,"Hmm, not sure why we need to have the while loop here. Aren't we just loop the same wrapperMessageOpt over and over?
"
67100784,1215,junrao,2016-06-15T05:11:51Z,"This should probably go to stderr as those mismatches.
"
67398599,1215,becketqin,2016-06-16T18:26:01Z,"Hmm, I actually did that in my first patch. `searchForOffset()` and `searchForTimestamp` were sharing a `scanAndHandleMessages()` function that does a special iteration by only looking at the headers. You suggested maybe we can just use the iterator in the comments and I thought it was a good suggestion because of the following reasons:
1. search by timestamp is a very infrequent call.
2. unlike searching by offset, searching by timestamp do need to look into the payload if the message is a compressed message. So copy the value into the buffer seems reasonable.
3. Because we have the index, the number of bytes we read from the disk should be relatively small assuming the timestamps are not abnormal.

So it seems not worth duplicating the code of the iterator here?
"
67426111,1215,becketqin,2016-06-16T21:20:15Z,"Yes, stopping early means we may not find the exact offset with a timestamp that we want. But it will not miss that message. 

In reality, a log segment may have messages w/ and w/o timestamp because some of the producers have been upgraded and others have not. What concerns me of not stopping when see old message is that if we don't have any message w/ a timestamp (or the timestamp we wanted locates close to the end of the log segment), we may end up with scanning the entire log segment. That will pollute the memory and impact the performance.

Since we are not appending a time index entry to the time index for the first message with a timestamp in a segment. Can we do the following?
1. In `TimeIndex.lookup()`, when slotFor() returns -1 in the timeindex, we can just return the offset of the first entry instead of baseOffset (the offset will be baseOffset if the first message has a timestamp). This will essentially skip all the earlier messages w/o a timestamp in the log segment. If the time index is empty, we still return baseOffset. 
2. In `searchForTimestamp()` we keep searching when we see message w/o a timestamp until we find the timestamp. The only risk is that if two indexed offset in the time index are far away, we may still be scanning the entire log. For example, if message 0 has a timestamp 100, message 100000 has timestamp 200. When people are searching for timestamp 150, we will scan the message from 0 to 100000. But this should be rare.
"
67427456,1215,becketqin,2016-06-16T21:28:46Z,"Currently this method returns the `maxTimestampChecked` and the `nextOffsetToRead`, so it is not the exact timestamp to offset mapping. Do you think it would be better to return a tuple `(timestamp, offset)` instead of a `TimestampOffset`?
"
67445751,1215,becketqin,2016-06-17T00:17:41Z,"It seems reasonable to use the last modification time as the biggest timestamp of the segment. Do you have any specific concern?
"
67449438,1215,junrao,2016-06-17T01:12:15Z,"Ok, sounds good. We can leave this as it is. Sorry for going back/forth on this. 
"
67449469,1215,junrao,2016-06-17T01:12:49Z,"1. slotFor returns -1 only when the timeIndex is empty. In the case, the largest timestamp of the segment is NoTimestamp. So, this segment shouldn't need to be search since NoTimestamp is smaller than any target timestamp, right
2. Yes, not sure how common the case you described can happen. Given that search by timestamp is infrequent, perhaps scanning more is ok?
"
67449480,1215,junrao,2016-06-17T01:12:56Z,"It doesn't seem timestamp is used by the caller. Could we just return Option[Long]?
"
67449925,1215,becketqin,2016-06-17T01:19:39Z,"If a log segment has messages w/ and w/o timestamp, the first message in a log segment may not always have the timestamp. So it seems we still need to store the timestamp of the first message that w/ a timestamp somewhere, right? Or do you mean we only look at the timestamp of the first message to determine whether to use the message timestamp or segment create time?
"
67454855,1215,becketqin,2016-06-17T02:48:30Z,"I am not sure how to make the text clearer. It seems we have already specifically stated the problem?
"
67455183,1215,becketqin,2016-06-17T02:55:00Z,"I added this because offset index will throw exception if we attempt to insert an offset index entry whose offset equals to the previous offset. I am trying to avoid confusing future contributors here by adding these comments.
"
67455678,1215,becketqin,2016-06-17T03:04:39Z,"The wrapperMessageOpt will be updated in line 206. We need to do this because if user is producing uncompressed messages, a ""batch"" would actually be multiple messages. The first message in the batch may not always have the the largest timestamp. So we need to scan a little. We should be able to find the indexed timestamp before the next indexed offset in the time index or the log end, whichever comes first.
"
67554576,1215,becketqin,2016-06-17T18:25:35Z,"1. slotFor also returns -1 if the timestamp is smaller than the first indexed timestamp. So the case I described is something like the following: Say we have 1 GB log segment, the first message w/ a timestamp is offset 1234567 at position 900MB. And the timestamp is 1000. So the first time index entry will be (1000, 1234567) in this case. Now if user come and search for timestamp 100, `indexSlotFor` will return -1, and currently we will return baseOffset if `indexSlotFor` returns -1. So we will scan the log from base offset and page 900 MB into memory.

If we return the offset in the first time index entry. We don't need to scan the previous 900 MB in that segment. Because we know that there is no timestamp before the offset of the first indexed offset in the time index, there is no need to scan that.

In another scenario, if the time index is empty, that means there is no timestamp in the segment. But in that case `LogSegment.largestTimestamp()` will return the last modification time. So `Log.fetchOffsetsByTimestamp()` will not skip the segment but treat it the same way as the segments that have timestamps. This essentially means at Log level, we people search for we do not distinguish between the largestTimestamp from the message timestamp or the largestTimestamp from last modification time. They provides a unified experience to the users.

Are you suggesting that we always ignore the old messages completely in the new way of searching for timestamp. I am worried about the transition period regarding that approach. For example, say some user is now using the timestamp search at segment granularity. If they want to consume all the data after 8:00 AM, they can start to consume the data after the segment whose last modification time is before 8:00 AM. There may be duplicates but it works. Now the users upgraded to new consumer which search for timestamp based on message timestamp, suddenly we skip all the old segments and they always get log end offsets.

That was also why I am not sure if we should skip the old messages or we should just stop on seeing old messages.
1. I agree the case I described would be rare, so scanning until we find timestamp should be fine.
"
67583237,1215,junrao,2016-06-17T22:21:08Z,"Thanks for the explanation. 
1. Yes, I agree that it's better to use last modified time for segments with no timestamp. So, if all messages have NoTimestamp, the semantic is that the search will return the offset of the first message in the segment whose last modified time is >= the target timestamp.
2. If a segment has messages with and w/o timestamp, to find the message matching the precise timestamp may require us scan most the segment, but is probably ok since it's rare.

It would be useful to document the semantics in the comment above Log.fetchOffsetsByTimestamp() on those corner cases.
"
67583244,1215,junrao,2016-06-17T22:21:13Z,"Ok, this is fine then.
"
67583312,1215,junrao,2016-06-17T22:21:47Z,"Yes, I was thinking of just looking at the timestamp of the first message. If it doesn't have timestamp, we use create time. This is probably simpler than having to force an index entry on first message with timestamp. For those segments with a mix of valid timestamp and NoTimestamp, we can decide whether the rolling will be based on the old or the new behavior. Either is fine, but we probably want to pick one that's easier to implement.
"
67583331,1215,junrao,2016-06-17T22:21:59Z,"Corrupt index found, index file (%s) has non-zero size and the last offset is %d is not larger than the base offset is %d
"
67583348,1215,junrao,2016-06-17T22:22:18Z,"Got it. Then, is it enough to read only maxMessageSize from the log? It seems that we may need to scan to the end of the log to find the message that we are looking for.
"
67591522,1215,becketqin,2016-06-18T00:40:28Z,"Yes, you are right. We do need to read till the end of the log.
"
67594888,1215,becketqin,2016-06-18T03:53:16Z,"@junrao Just want to make sure I understand the behavior we want here. It seems that sometimes we want to skip the messages w/o timestamps, and sometimes we don't skip.

For example, if we have two log segments:
segment 0: baseOffset=0, no message has timestamp, last offset 99, last modified=1000.
segment 1: baseOffset=100, the first message with a timestamp is at offset 105, and the timestamp is 2000. The second message with a timestamp is at offset 150, and the timestamp is 3000.

Now if we search for timestamp 1500, it looks that we should return offset 100, i.e. we should not ignore messages 100 - 104 that do not have timestamps, even though the timestamp we wanted is at offset 105.

On the other hand if we search for timestamp 2500, it seems that we will skip the messages 106 - 149 because they don't have timestamps.

It seems a little weird that we treat the messages w/o timestamps differently depending on where the messages are. Do you think that is the behavior we want or It would be better if we have a consistent behavior regarding messages w/o timestamps? Previously I am taking the approach that we always don't skip the messages w/o timestamps. But that does mean we may not able to return the exact message with the timestamp we wanted. What do you think?
"
67604184,1215,junrao,2016-06-18T17:46:39Z,"Hmm, I was thinking that if you search for timestamp 1500, we will return offset 105 since that's the first message whose timestamp is >= 1500. Similarly, if you search for timestamp 2500, we will return offset 150. In the case where a segment has no timestamp (we know that from largestTimestamp), we can optimize by not scanning the whole segment and just return the offset of the first message in the segment. Otherwise, we will just scan the segment as much as needed.
"
67605816,1215,becketqin,2016-06-18T19:57:05Z,"If we return 105 when search for 1500, would that cause problem for the transitional period? Previously user will consume from 100, which may be the messages they actually wanted. Once they switch to new timestamp search, the user will not see message 100 - 104 any more. In our example there are only 5 messages, but in real world there could be more.

It looks there is a trade-off we need to make between backward compatibility and accuracy of search. I am not sure how critical the backward compatibility here is. Personally I feel that when there are mixture of messages with and w/o timestamps, backward compatibility seems more important. Because the accuracy will increase when more and more messages contains timestamp. And we will have the full accuracy once the users have fully rolled out the new message format. 
"
67625139,1215,junrao,2016-06-19T22:34:37Z,"To me, it seems that we just need to preserve compatibility when things are comparable. For example, if a segment has no message with timestamp, it makes sense to use the old behavior, i.e., treating all messages in the segment as if they have the last modified time. If a segment has messages with and w/o timestamp, it's not directly comparable with the old behavior. It seems to me that simply finding the first message with a timestamp >= than the target is easy to understand. Stopping at the first message with NoTimestamp seems harder to explain since it depends on where those messages are and what the indexing interval is.

In any case, we probably need a separate KIP to expose the timestamp search in an API. We can discuss more and finalize the decision then.
"
67625332,1215,becketqin,2016-06-19T22:51:54Z,"I see. I will create another KIP then. 

The concern I have is that from user's perspective, they don't really care or know which segment the messages go into. They only want to make sure that all the messages produced after a certain timestamp **T** will be consumed. Previously the offsets we return to the user are based on last modification time at segment level. It seems that now when we have old messages without timestamps, we should still keep the previous behavior.
"
67796773,1215,junrao,2016-06-21T01:49:35Z,"The previous indentation seems correct?
"
67796777,1215,junrao,2016-06-21T01:49:41Z,"Do we need loadedSegment at all? It seems that loadedSegment is always segment.
"
67796798,1215,junrao,2016-06-21T01:49:54Z,"Could we get rid of indexOrTimeIndex and change warn to log ""Found corrupted index due to e.getMessage() ...""? The message in the exception tells us the file name, which should be good enough.
"
67796809,1215,junrao,2016-06-21T01:50:01Z,"The comment is not accurate. The rolling is now based on the timestamp of the first message.
"
67796815,1215,junrao,2016-06-21T01:50:05Z,"Do we need the extra new line?
"
67796820,1215,junrao,2016-06-21T01:50:09Z,"Do we need the extra space before startMs?
"
67796828,1215,junrao,2016-06-21T01:50:16Z,"The comment seems in the wrong location.
"
67796839,1215,junrao,2016-06-21T01:50:24Z,"unused val maxTimestampSoFarBeforeAppend
"
67796844,1215,junrao,2016-06-21T01:50:28Z,"This comment seems in the wrong location now.
"
67796889,1215,junrao,2016-06-21T01:50:59Z,"This forces us to read one message from every log segment during clean startup, which may incur additional I/Os. Another way is to calculate rollingBaseTimestamp lazily when timeWaitedForRoll() is first called and then remember it. This will avoid the potential extra I/Os.
"
67796907,1215,junrao,2016-06-21T01:51:14Z,"If we update maxTimestampSoFar, don't we need to update offsetOfMaxTimestamp accordingly? Otherwise, the next time we add an entry to the time index, the offset may not match maxTimestampSoFar.
"
67796913,1215,junrao,2016-06-21T01:51:20Z,"To be more precise, we are not adding the last, but the largest time index entry.
"
67796920,1215,junrao,2016-06-21T01:51:23Z,"The comment seems inaccurate.
"
67796925,1215,junrao,2016-06-21T01:51:29Z,"The comment is no longer accurate now that we only return offset.
"
67796958,1215,junrao,2016-06-21T01:51:46Z,"More precisely, the comment should be ""Get the index entry with a timestamp less than or equal to the target timestamp"".
"
67796971,1215,junrao,2016-06-21T01:51:55Z,"Could we use file.getAbsolutePath instead file.getName here to make it consistent?
"
67796980,1215,junrao,2016-06-21T01:52:01Z,"That means the we => That means we
"
67796995,1215,junrao,2016-06-21T01:52:12Z,"Is the comment still accurate? We don't insert the last modification time of the file to the time index, right?
"
67797002,1215,junrao,2016-06-21T01:52:19Z,"We want to find the time index entry whose timestamp is less than or equal to the given timestamp.
"
67797008,1215,junrao,2016-06-21T01:52:24Z,"Could we use file.getAbsolutePath instead file.getName here to make it consistent?
"
67797017,1215,junrao,2016-06-21T01:52:33Z,"That may require a large heap for the tool. Could we read messages in chunks up to maxMessageSize?
"
67797021,1215,junrao,2016-06-21T01:52:37Z,"> should be >=, right?
"
67797065,1215,junrao,2016-06-21T01:53:06Z,"It seems that we just need to print timestamp and offset in the index. Not sure why we need to print s""(log timestamp: $maxTimestamp)"" and s""(log offset: ${partialFileMessageSet.head.offset})"".
"
67797070,1215,junrao,2016-06-21T01:53:09Z,"typo indexs
"
67797073,1215,junrao,2016-06-21T01:53:11Z,"typo indexs
"
67968372,1215,junrao,2016-06-21T23:03:13Z,"Hmm, is that right? The rolling check happens before the append. So, the first message in the segment is ""set"", which has NoTimestamp. Then, we will be using the creation time of the segment. So, it seems that in the case, we should roll another segment after calling append() in line 94.
"
67968389,1215,junrao,2016-06-21T23:03:19Z,"Hmm, do we need the if test since the baseOffset is 0?
"
67968395,1215,junrao,2016-06-21T23:03:22Z,"The comment is no longer accurate.
"
67968397,1215,junrao,2016-06-21T23:03:25Z,"unused val msgPerSeg
"
67968464,1215,junrao,2016-06-21T23:04:08Z,"Hmm, shouldn't we populate messages of format 0.9.0 here since we want to set message format to 0.9.0 later?
"
67968484,1215,junrao,2016-06-21T23:04:20Z,"To trigger the rebuild of the index, don't we need to set recoveryPoint to 0?
"
67968492,1215,junrao,2016-06-21T23:04:24Z,"unused import
"
67968503,1215,junrao,2016-06-21T23:04:31Z,"Hmm, why will the append hit the exception here? The timestamp seems larger than the ones in the index.
"
67968517,1215,junrao,2016-06-21T23:04:37Z,"Hmm, not sure why we will hit the exception since the offset to be appended is larger than the ones in the index.
"
67968523,1215,junrao,2016-06-21T23:04:41Z,"This is no longer accurate.
"
67968533,1215,junrao,2016-06-21T23:04:48Z,"Is ""might"" accurate? It seems it should be ""will"".
"
67968542,1215,junrao,2016-06-21T23:04:52Z,"offset index => offset index entry
"
67968550,1215,junrao,2016-06-21T23:04:55Z,"This is no longer true.
"
68103620,1215,becketqin,2016-06-22T18:02:49Z,"Not sure I fully understand this. It seems we need to read the first message from the file anyway, whether at the startup time or when the first message is appended after startup. If the first message received after startup and the first message in the segment resides in the same disk block, we only page in once. Otherwise we still need to page in two blocks. So it seems the disk I/Os are the same? Do you mean we can save the disk I/O if there is no message appended to a partition after startup?
"
68108307,1215,becketqin,2016-06-22T18:28:10Z,"Hmm, I think FileMessageSet.read() only set a start position and end position. It does not really read anything into the heap, right? This is just allow the iterator to iterate until the end of the segment. It seems not creating any additional memory pressure.
"
68109192,1215,becketqin,2016-06-22T18:32:30Z,"I was thinking this easier for user to find out what when wrong. When I run this command, I found it is a little ugly that we have to switch between the dumped errors and the printed index entries.
"
68109601,1215,becketqin,2016-06-22T18:34:34Z,"Ah... Sorry for the confusion, I changed the assertion expected value but forgot to change the assertion message.
"
68111083,1215,becketqin,2016-06-22T18:42:13Z,"Logically the first message appended to a log segment does not cause the insertion of index entries, so searching for the timestamp of the first message should give base offset. But in our case the base offset happened to be the same as the offset of our first message.
"
68111692,1215,becketqin,2016-06-22T18:45:29Z,"It does not really matter whether we append message in 0.9.0 or 0.10.0 format because the log segment will do down conversion when the message is appended if the format is 0.10.0
"
68112128,1215,becketqin,2016-06-22T18:47:50Z,"In our case, we will have corrupted log index, so no matter what recovery point it is, the index will be deleted and rebuilt.
"
68112512,1215,becketqin,2016-06-22T18:49:59Z,"This is because the time index entry is already full. We only allow maxEntries - 1 entries to be appended if skip full check is set to false (which is the default value).
"
68113246,1215,becketqin,2016-06-22T18:54:01Z,"Our base offset is 45L, max entries = 30L.

The offset of the last time index entry is (30 - 1) \* 10 + 45 = 335.
We are appending an entry with offset (30 - 2) \* 10 = 280.

So it is smaller.
"
68166110,1215,junrao,2016-06-23T02:06:26Z,"maybeRoll() is only called on the active segment. So, if we get first message on demand when maybeRoll() is called, we can avoid reading the first message on all old segments.
"
68166173,1215,junrao,2016-06-23T02:07:29Z,"Good point. So, this is not an issue then.
"
68166576,1215,junrao,2016-06-23T02:14:25Z,"But the log is not configured with 0.9.0 message format when the append happens, right?
"
68167173,1215,junrao,2016-06-23T02:24:51Z,"Got it. Sorry, I thought the first field is the offset.
"
68244074,1215,becketqin,2016-06-23T14:26:12Z,"Got it. Thanks for the explanation. That makes sense.
"
68244889,1215,becketqin,2016-06-23T14:30:19Z,"Ah, you are right. Sorry I was reading the lines below this, which is recovering the log.
"
68306815,1215,ijuma,2016-06-23T20:16:47Z,"Doing this means that `compareKey` and `compareValue` will cause boxing to take place. Two ways to fix this:
- Use `@specialized` although it's a bit hard to know when the optimisation is not applied
- Use `long` everywhere when doing the comparisons
"
68307091,1215,ijuma,2016-06-23T20:18:40Z,"`java.util.Long.compare`?
"
68307419,1215,ijuma,2016-06-23T20:20:40Z,"How can we say it is fully compatible and then say that there are `Potential breaking changes`?
"
68307613,1215,ijuma,2016-06-23T20:21:50Z,"For the two above, it would probably be useful to say what the previous behaviour was as well.
"
68308726,1215,ijuma,2016-06-23T20:28:32Z,"`asInstanceOf` and `AnyVal` are generally avoided in Scala as there is usually a cleaner and safer way to do things. Maybe `indexSlotFor` should take a function that does the comparison?
"
68313140,1215,ijuma,2016-06-23T20:54:35Z,"This is a bit odd, why not the standard `int mid = (low + high) >>> 1`?
"
68313919,1215,ijuma,2016-06-23T20:58:58Z,"A micro-optimisation is to leave this as the last `else` as we return once it succeeds. Statistically, we are more likely to hit the other branches more times.
"
70016160,1215,junrao,2016-07-08T01:51:20Z,"timebased => time-based
"
70016179,1215,junrao,2016-07-08T01:51:40Z,"On the leader, during append, we actually use the first offset in the MessageSet when adding the time index entry. To be consistent, we probably should do it here as well.
"
70016194,1215,junrao,2016-07-08T01:51:52Z,"We need to set skipFullCheck to true when calling maybeAppend here, right?
"
70016202,1215,junrao,2016-07-08T01:51:57Z,"Would it be better to rename this to loadLargestTimestamp()?
"
70016219,1215,junrao,2016-07-08T01:52:15Z,"It's kind of verbose to have to do the assignment here and in line 256. Perhaps we can move these two lines after line 241. Then we can get rid of the two else clauses.
"
70016230,1215,junrao,2016-07-08T01:52:24Z,"We need to set skipFullCheck to true when calling maybeAppend, right?
"
70016242,1215,junrao,2016-07-08T01:52:35Z,"The error message is a bit hard to understand. Could we improve that?
"
70145743,1215,junrao,2016-07-08T21:45:34Z,"Ran into an issue while testing DumpLogSegments (details can be found in the jira since PR comment doesn't support attachment). It seems that if the producer sends a batch of non-compressed messages, in LogSegment.append(), we always pass in the offset of the first message, but the offset of the largest message may not be the first. This seems to be what's failing the check in the tool. Not sure if this is the behavior that we want. If yes, we will need to document this properly, i.e., the index may point to an offset before the message that has the corresponding timestamp. But then, we have to think through other implications such as how this affects truncation.
"
70380300,1215,becketqin,2016-07-12T05:48:19Z,"I meant it was compatible API wise, however there are potential breaking changes in resource footprint. I agree we should be clearer on this. I'll update the doc.
"
70680191,1215,junrao,2016-07-13T18:19:59Z,"Could we add offsetOfMaxTimestamp?
"
70680203,1215,junrao,2016-07-13T18:20:03Z,"Could we rename this to validateAndOffsetAssignResult?
"
70680256,1215,junrao,2016-07-13T18:20:19Z,"We changed the logic to only use the wrapper message offset in time index in append(). So, perhaps we should do the same during recovery? Also, it would be good to avoid calling entry.firstOffset twice since decompression is needed each time.
"
70680297,1215,junrao,2016-07-13T18:20:33Z,"In line 460, do we still need the test messageFormatVersion > Message.MagicValue_V0? It seems it's already covered by line 458.
"
70680342,1215,junrao,2016-07-13T18:20:43Z,"Hmm, in this case, if the targetCodec is NoCompression, we should return the offset of the first message, right?
"
70680368,1215,junrao,2016-07-13T18:20:52Z,"For the code btw 484 and 491, if messageTimestampType and timestamp != maxTimestamp, don't we need to change the timestamp in the wrapper message?
"
70680433,1215,junrao,2016-07-13T18:21:11Z,"typo firxst

Also, the next line of the comment seems to talk about the same thing.
"
70680461,1215,junrao,2016-07-13T18:21:20Z,"When I ran the following command, I got an exception. 

bin/kafka-run-class.sh kafka.tools.DumpLogSegments --files 00000000000000113931.timeindex 
Dumping /Users/junrao/Downloads/00000000000000113931.timeindex
Exception in thread ""main"" java.lang.IllegalArgumentException: Invalid max index size: -1
    at kafka.log.AbstractIndex.<init>(AbstractIndex.scala:54)
    at kafka.log.OffsetIndex.<init>(OffsetIndex.scala:51)
    at kafka.tools.DumpLogSegments$.kafka$tools$DumpLogSegments$$dumpTimeIndex(DumpLogSegments.scala:185)
    at kafka.tools.DumpLogSegments$$anonfun$main$1.apply(DumpLogSegments.scala:101)
    at kafka.tools.DumpLogSegments$$anonfun$main$1.apply(DumpLogSegments.scala:91)
    at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
    at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
    at kafka.tools.DumpLogSegments$.main(DumpLogSegments.scala:91)
    at kafka.tools.DumpLogSegments.main(DumpLogSegments.scala)
"
70684786,1215,junrao,2016-07-13T18:44:25Z,"Since we changed the logic, so for compressed messages, we need the offset of the wrapper message for the timeindex. Also, could we save the last MessageAndOffset to avoid calling retainedMessages.last (since it requires iteration)?
"
70926518,1215,becketqin,2016-07-15T06:19:26Z,"I was not able to reproduce this issue. It seems that the exception would only happen if the time index file does not exist.
"
72359859,1215,junrao,2016-07-26T23:57:16Z,"Is removing the above code correct? It doesn't seem there is code to compute the largest timestamp lazily.
"
72360736,1215,becketqin,2016-07-27T00:06:07Z,"The maxTimestampSoFar and offsetOfMaxTimestamp are loaded when the log segment is constructed. So it seems that we don't need to load it again.
"
72363721,1215,junrao,2016-07-27T00:39:52Z,"Got it. Then the changes look good.
"
75409338,1215,junrao,2016-08-18T23:50:45Z,"typo timestmp
"
75409364,1215,junrao,2016-08-18T23:51:01Z,"Could we just set appendInfo.maxTimestamp here and get rid of maxTimestampInMessageSet?
"
75409371,1215,junrao,2016-08-18T23:51:04Z,"rollingBaseTimestamp => rollingBasedTimestamp ?
"
75409380,1215,junrao,2016-08-18T23:51:14Z,"Perhaps it would be safer for the callers to use named params since the first three params are of the same type.
"
75409406,1215,junrao,2016-08-18T23:51:27Z,"Perhaps we should add largestTimestamp and offsetOfLargestTimestamp to the trace logging too?
"
75409414,1215,junrao,2016-08-18T23:51:31Z,"typo messsage
"
75409417,1215,junrao,2016-08-18T23:51:34Z,"Shouldn't we used created instead of 0 here?
"
75409433,1215,junrao,2016-08-18T23:51:47Z,"We probably shouldn't eat the exception here. If there is an error when appending to the time index, we should probably just halt the jvm so that we can run recover on restart.
"
75409443,1215,junrao,2016-08-18T23:51:54Z,"That means the we => That means we
"
75409460,1215,junrao,2016-08-18T23:52:06Z,"Is the following comment still accurate? We don't insert the last modification time of the file to the time index, right?
"
75409490,1215,junrao,2016-08-18T23:52:23Z,"The name wrapperMessageTimestamp seems inaccurate since the message set is not always compressed?
"
75409524,1215,junrao,2016-08-18T23:52:45Z,"In both this line and line 472, it seems that we should be using offsetOfMaxTimestamp instead of the offset of the last message since validatedMessages may not be compressed when adding to the log. Also, could we make wrapperMessageTimestamp a non-Option field since all possible values are Some.
"
75409563,1215,junrao,2016-08-18T23:52:59Z,"It seems that the ordering of the 2nd and the 3rd params is reversed? To avoid this, perhaps it will be clearer to use named parameters since timestamp and offset are of the same type.
"
75410546,1215,junrao,2016-08-19T00:04:46Z,"Could we not print this if there is no error?
"
75422544,1215,becketqin,2016-08-19T03:14:55Z,"I am thinking that if there is no message in the segment, the log segment would not have been waiting for rolling, so returning 0 seems reasonable. The return value here does not matter much because we check the segment size in Log.maybeRoll(). But it is probably better to return (now - created) since we have a trace level logging in Log.maybeRoll().
"
75424163,1215,becketqin,2016-08-19T03:45:08Z,"Hmm, if the message is not compressed, the offset should be the exact offset of the message that has the largest timestamp. 

When using CREATE_TIME (line 472), that message would be offsetOfMaxTimestamp. But when using LOG_APPEND_TIME, it seems the offset should be the offset of the first message in the message set because all the messages in the validatedMessages will have the same timestamp. (On line 474, offsetCounter has not been incremented yet. So offsetCounter.value is still the offset of first message).

I thought about making wrapperMessageTimestamp non-Option, but we actually will pass in None in the constructor in line 272.
"
75424189,1215,becketqin,2016-08-19T03:45:45Z,"Good catch! I'll add a unit test for this. Can't believe it is not caught in the unit tests.
"
75425328,1215,junrao,2016-08-19T04:06:50Z,"Thanks for the explanation. That makes sense. The current logic is correct then.
"
75517403,1215,junrao,2016-08-19T17:08:21Z,"The comment is no longer valid. We can probably just remove it.
"
75517610,1215,junrao,2016-08-19T17:09:30Z,"It seems that timeIndex.maybeAppend() won't throw IOException. So, swallowing the exception here is actually fine.
"
210999572,5527,Kaiserchen,2018-08-17T18:41:38Z,Serialized wrapper?
211006524,5527,Kaiserchen,2018-08-17T19:08:05Z,"can skip the second length, its known anyway.

"
211006693,5527,Kaiserchen,2018-08-17T19:08:55Z,treating the whole thing as Buffer?
211008763,5527,Kaiserchen,2018-08-17T19:17:47Z,I think the step here an optimizer _could potentially_ exploit is the repartitioning. So one could try to only factor out the repartitioning
211009275,5527,Kaiserchen,2018-08-17T19:20:07Z,"probably need to wrap into 
DelegatingPeekingKeyValueIterator"
211347575,5527,bellemare,2018-08-20T17:38:36Z,"True, I could just do it as the remainder. Thanks"
211347916,5527,bellemare,2018-08-20T17:39:41Z,"I'll have to look more into the optimizer. TBH I built this originally in 1.0 and just did a functional port, not necessarily a best practices one. Thanks"
211390368,5527,bellemare,2018-08-20T20:05:51Z,"I don't understand your question, can you elaborate?"
211396540,5527,bellemare,2018-08-20T20:27:36Z,Will do.
211399371,5527,bellemare,2018-08-20T20:37:52Z,"I notice that in 2.x that I may be able to rework this to allow for enabled cache using a `prefixScan` function similar to `ThreadCache.range`. I will have to look into this a bit more, though I don't think it will affect performance much since I anticipate RocksDB prefixScan to take the longest overall."
211510890,5527,Kaiserchen,2018-08-21T07:57:50Z,"Might be, its one of the places I got stuck once. From experience I can tell that its working sufficiently well w/o cache. I think rocks does a pretty good job in not seeking around to randomly on the disk"
211511167,5527,Kaiserchen,2018-08-21T07:58:50Z,I would not recommend to spend to much energy. At the moment I really don't expect the optimizer to be able to exploit any of this. Probably also not in the future. Was just a though popping into my head
211511539,5527,Kaiserchen,2018-08-21T08:00:21Z,"I think the whole section could look nicer if you would start with ByteBuffer.allocate(totallength).asIntBuffer(keylength).asbyteBuffer.put(key).put(key)...
something"
214673257,5527,Kaiserchen,2018-09-03T12:23:37Z,would need to forward `null` here?
214674662,5527,Kaiserchen,2018-09-03T12:29:31Z,would remove this class and avoid protected final here. Doesn't seem like a convincing java programming style.
214715526,5527,bellemare,2018-09-03T15:01:58Z,"I'll leave it out for now. If someone else thinks otherwise, they can speak up or it can be done in a subsequent PR."
214744117,5527,bellemare,2018-09-03T18:30:42Z,"Yes, cleaner to do so. The value is not relevant. I have fixed that and added a clarification comment (I can remove all comments if required before final submission)."
214744281,5527,bellemare,2018-09-03T18:32:41Z,"I will change this - but I do appreciate any advice on how I should changee that. I'll post my updated code and we can determine if it's any better, or if there is a more specific approach I should follow."
275888898,5527,adaniline-traderev,2019-04-16T16:34:03Z,"Why is the topic passed as null? It causes issues with GenericRecord AVRO serializer, since it tries to register schemas under ""null-value"" subject, and the schema registry responds with ""version not compatible"" error"
275899695,5527,bellemare,2019-04-16T17:00:50Z,"The issue is actually with the Confluent implementation of the SerDe, as they incorrectly attempt to register when null topics are passed in. Read https://github.com/confluentinc/schema-registry/issues/1061 for more details. That being said, it has been extremely quiet in that git repo, I am not sure how much effort Confluent puts into supporting work on that product."
275904020,5527,adaniline-traderev,2019-04-16T17:11:42Z,"If this does not gets fixed either way, this PR will be unusable for most of the practical use cases. What is the downside of passing the topic name to the serializer? I tried it, and it seemed to work as expected.
Is there a workaround if confluentinc/schema-registry#1061 is not fixed?"
275908240,5527,bellemare,2019-04-16T17:22:45Z,"I think the main issue would be the large amount of internal topic schemas registered to the schema registry. This, combined with any breaking changes to the schema (due to normal business requirement changes) would make it such that you are now needing to manually delete schema registry entries made to internal topics. This is a workflow that I do not believe was ever intended to be done with the Confluent Serde.

As it stands right now, there are allegedly other functionalities that require null serialization (""There are several places in Streams where we need to serialize a value for purposes other than sending it to a topic (KTableSuppressProcessor comes to mind), and using `null` for the topic is the convention we have.""). These too will not work with the confluent Serde.

If they do not fix it, then the next best thing to do would be wrap it in your own implementation and intercept null-topic values to avoid registration.  I do not see why it wouldn't be fixed since the current behaviour of registering ""null-topic"" is fundamentally useless. 

Anyways, with all that being said, for this particular line I can certainly pass in the topic since it's fairly well-defined. If you wish to have your internal topics registered to the schema registry, no big deal. For other parts, such as https://github.com/apache/kafka/blob/18a16b8eda3fb3e26d3fd6e7c209ceb983814a3e/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/KTableRepartitionerProcessorSupplier.java#L67, there is no solution using the current Confluent Serde."
275924688,5527,adaniline-traderev,2019-04-16T18:02:56Z,"Confluent serde needs a schema id, and looks like it is not stored in GenericData.Record instance - it may not be trivial to fix  confluentinc/schema-registry#1061..."
275926845,5527,bellemare,2019-04-16T18:08:30Z,"That is a fair point.

@mjsax @vvcephei @guozhangwang: It may not be as easy to get the hash value as we thought if we rely on the value serde, namely because of the interactions with the schema registry to serialize the data. Though this is technically a confluent issue, it may be sufficient to put a blocker on this PR since, as Adaniline demonstrates, many users of this use the confluent serde. Any thoughts on a way forward are welcome."
275978323,5527,vvcephei,2019-04-16T20:28:13Z,"Hmm. This does seem like an impediment. The Serializer interface doesn't specifically state whether the topic is nullable or non-nullable. What it does say is `@param topic topic associated with data`. Getting fully into armchair lawyer mode, it seems like this statement implies non-nullability, since `null` is not a valid topic name. If it could be null, it should say something like ""the topic associated with the data, or null if there is none.""

Also, backing up to a higher level, the Serializer interface is specifically for serializing data for use with Kafka topics. So, it's probably not appropriate to use it for general serialization. If we want to do that, maybe we should add a new interface that doesn't imply we're sending data to a topic.

Then again, this is in support of an optimization. Perhaps we should just drop the optimization for now and go back to storing only the foreign key reference for the correctness check. For bonus points, we could design a message and storage format that leaves the door open for future optimizations like this, without an awkward upgrade procedure.

WDYT, @guozhangwang ?"
276022142,5527,guozhangwang,2019-04-16T22:48:53Z,"@bellemare @vvcephei On the high-level, I think this should be fixed at Confluent SR, rather than letting an AK feature blocked on it -- i.e. users should still be able to pass in not-null topic names while non-schema required typed data will not be registered.

Reading on the source code of avro-serializer, I think the root cause is that today we treat all primitive typed data as a simple avro-schema as well when serializing, hence still registering it. Details are here:

https://github.com/confluentinc/schema-registry/blob/master/avro-serializer/src/main/java/io/confluent/kafka/serializers/KafkaAvroSerializer.java#L52

and 

https://github.com/confluentinc/schema-registry/blob/8f727d74da65a9be4f7fd82fd8a2498ad48c85ec/avro-serializer/src/main/java/io/confluent/kafka/serializers/AvroSchemaUtils.java#L30

We could, instead, fix SR to let it just use primitive serdes directly for primitive types. In this way users can still provide topic names."
276026299,5527,pgwhalen,2019-04-16T23:08:22Z,"FWIW @guozhangwang , as a user of the SR/serdes, I would consider ""fix SR to let it just use primitive serdes directly for primitive type"" to be a regression. Registering primitive schemas with the schema registry is a feature, because it allows other applications (support tooling, etc.) to understand the data in a topic without any context whatsoever."
276236590,5527,bellemare,2019-04-17T13:20:57Z,"I think to answer the question of if Confluent needs to change their SR and serde, or if we need to take a different approach requires us to address John's first paragraph: ""The Serializer interface doesn't specifically state whether the topic is nullable or non-nullable.""

If the topic must be non-nullable, then the current approach with the hash code wont work, since the output topic is unknown by the KTableRepartitionerProcessorSupplier.java. If the topic must be nullable (and still provide a serialized output), then the confluent serde/SR needs to change to support this. This isn't clear from the contract and so it probably does need to be resolved.

Lastly, I do agree with pgwhalen on the usage of primitive schemas in support tooling. We use this functionality all the time with schema registry lookups, as opposed to having our team guess as to what it may have been serialized in. 

One last thought - for the confluent serde, if the topic is null and the type is a primitive, we can avoid registration and do as @guozhangwang outlined. If the type is not a primitive, it can throw an exception (as it effectively does now, upon trying to register to topic-null). This does seem a bit hacky and perhaps a bit complicated, but it may work.


"
276244207,5527,adaniline-traderev,2019-04-17T13:36:43Z,"Can the solution be to have a separate interface to calculate data hash (HashSupplier, etc), and have Confluent Avro Serializer implement it? Looks like it is a separate concern from kafka data serialization
This way KTableRepartitionerProcessorSupplier could do something like
```java
if (valueSerializer instanceof HashSupplier) {
   currentHash = ((HashSupplier) valueSerializer).hash128(change.newValue);
} else {
   currentHash = Murmur3.hash128(valueSerializer.serialize(null, change.newValue)));
}
```"
276324353,5527,guozhangwang,2019-04-17T16:17:04Z,"> because it allows other applications (support tooling, etc.) to understand the data in a topic without any context whatsoever.

@pgwhalen I see. That's a valid point. After a second thought I feel that instead of all / never (i.e. always register a simple avro schema, v.s. never register for primitive types) we should give users options to choose from the two.

> Can the solution be to have a separate interface to calculate data hash (HashSupplier, etc), and have Confluent Avro Serializer implement it?

@adaniline-traderev That should also work, in fact, we can just let Streams to use a special serde other than whatever registered serde for this repartition topic's value, instead of asking the registered serde to provide a hashSerde function: note that, the current serde precedence is, from high to low:

1) User overridden value at the DSL.
2) Streams library hard-coded value internally (think: LongSerde for `count`).
3) Default registered serde value.

But I'm a bit concerned about adding more special handling of serdes at the Streams side, i.e. add more scenarios of case 2) above: today Streams try to reason about the resulted stream key/value type out of an operator in a best effort and try to use the serde correspondingly event if user does not specify any overrides. The goal is to try to be as adherent to case 1) as possible. Although we do have special hard-coded serdes as case 2), e.g. in `count` operator, we hard-coded the serde as LongSerde unless user overrides it, we still want to keep such cases as small as possible, since having Streams to special-handle, e.g. a `KTableRepartitionerProcessor` opens the door for more complexity, as this handling logic would be scattered across the classes and error-prone: note that besides the sink node, at the source node of the downstream sub-topology, Streams then should also remember to use a special serde instead of the registered one to deserialize it as well.

That being said, if people feels it is worth to add this complexity in Streams for good reasons, I can be convinced as well

----------------

Philosophically, I'd still argue that, the AK code should not yield to a single vendor's dependency when considering its logic (disclaimer: I'm currently employed at Confluent). We should consider what's the best approach for AK Streams here, and then if it affects the eco-system dependency we should fix it on the other side."
276332348,5527,bellemare,2019-04-17T16:36:42Z,"@guozhangwang I don't disagree with your stand on AK first. I do, however, agree with John that the contract is not quite clear on if topic should always be non-null or if topic can be nullable. This is pivotal for determining the way forward. I believe that a precedent has been set by the suppress functionality ( https://github.com/apache/kafka/blob/0846f590dac5d3893d41e262d67ee8a228dcc332/streams/src/main/java/org/apache/kafka/streams/kstream/internals/suppress/KTableSuppressProcessor.java#L95 ), and if we wish to be consistent we should make clear that serializers need to handle null topics (though that handling could simply be throwing an exception.. as is effectively the case with the Confluent Serde / SR ). 

"
276340194,5527,guozhangwang,2019-04-17T16:56:51Z,"For this aspect, my personal take is Streams should pass in the topic name whenever it knows -- for the example you brought up, currently it's because the operator does not know the topic name I believe, for which we could refactor internal code to fix it (would like @vvcephei to chime in here whether that is really doable) -- and not relying on any assumptions of the serdes itself, whether the topic is nullable or not."
276340707,5527,vvcephei,2019-04-17T16:58:10Z,"Hah! Well, now I'm really in the middle of it ;) 

It's worth noting that there's plenty of precedent on both sides of the issue. I don't think the suppress implementation constitutes a contract. It could just as easily be argued that L95 there is a bug. Actually, I still feel the way I did toward the beginning of the thread, that it makes more sense to say it's non-nullable (given the javadoc and the origin/ownership of the interface in `kafka-clients`).

I'm happy to fix that bug in suppress, since we actually do send the serialized result there to a changelog topic, we should just provide the CL topic name to the serializer.

This case is more of a bind because I'm not aware of anywhere else in Streams where we legitimately serialize something we have no intention of sending to any topic."
276375010,5527,mjsax,2019-04-17T18:24:46Z,"I think for suppress() it's a bug -- and it seems to break SR integration: on restore the schema needs to be fetches from the SR if AVRO is used, but with `null` topic on serialization the schema would never have be registered correctly and restore would fail with schema not found error.

For passing in `null` in general, I think it should be allowed, but I also think it's not really necessary. We can pass in the repartition topic name here, too.

With regard to the concern about writing something different into the topic compared to a registered schema: We do this already for windowed changelog topics (note that only the user-data-key is serialized as AVRO for this case, and we add one or two additional longs and maybe a unique counter before writing into the changelog). If we believe this is an issue, we can fix it, but I would not block this PR for it. It's orthogonal and if we think we need to fix it, we need to fix it for all topics. However, it was not an issue in practice so far, thus, my personal take it, that we should move forward with this PR and pass in the repartition topic name."
276391436,5527,bellemare,2019-04-17T19:07:18Z,"@mjsax I can certainly pass in the repartition topic name for the serializer without any issue for both the serializer and deserializer here. https://github.com/apache/kafka/blob/18a16b8eda3fb3e26d3fd6e7c209ceb983814a3e/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResponseWrapperSerializer.java#L23

 At that point, it's up to the Confluent serde users to complain about their SR being populated with internal topic schemas (an issue we have run into at my company), though it's not a deal breaker. 

That being said, I am not aware of how to obtain the destination topic in https://github.com/apache/kafka/blob/18a16b8eda3fb3e26d3fd6e7c209ceb983814a3e/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/KTableRepartitionerProcessorSupplier.java#L67. This processor creates the hash code for the current event from the value serde (ie: confluent's), but has no knowledge of the downstream repartition topic, and thus cannot provide a topic. It is my understanding that this would require information not available to the processor, and so will not work as currently structured. This is the real issue that hasn't been adequately addressed yet, aside from the suggestion from John to drop the hash code optimization. This would skip the whole issue, though it may be sub-optimal in terms of developer experience."
276433024,5527,adaniline-traderev,2019-04-17T21:04:07Z,"Here is another instance where a serializer is called with null topic. I was just wondering if it is possible to pass 
```java
context().topic()
``` 
as topic name - seemed to worked in my testing scenario..."
276456917,5527,vvcephei,2019-04-17T22:24:43Z,"At the risk of being annoying, the processor _could_ just make up a topic name to use. This would also fix your problem.

Right now, I think we have three viable solutions (in order of my preference):
1. generate a topic name for the processor to pass to the serializer
2. pass null to the serializer, forcing Confluent's SR and any other Serializer/SR to handle nulls gracefully.
3. drop the optimization

Option 2 is a little unfriendly to Confluent SR (and other Serializer) implementations, but pragmatically, we _are_ upstream, so we have this leverage we can apply to force them to change."
276716731,5527,vvcephei,2019-04-18T15:30:20Z,"Further in support of making up a topic name to give the serializer, this is what we're currently doing with state stores, when change-logging is disabled. We unconditionally generate the topic name to pass, given only the appId and storeName:

```java
    public static String storeChangelogTopic(final String applicationId,
                                             final String storeName) {
        return applicationId + ""-"" + storeName + STATE_CHANGELOG_TOPIC_SUFFIX;
    }
```

When change-logging is disabled for a store, there is no such topic; i.e., it's a made-up topic name. It's not immediately clear how this is different from something like: `applicationId + ""-"" + processorName + ""-recordhash""`"
276720367,5527,vvcephei,2019-04-18T15:38:47Z,(tangential: I've just submitted https://github.com/apache/kafka/pull/6602 to fix the suppression serde handling. Please feel free to review it!)
277324985,5527,bellemare,2019-04-22T15:24:41Z,"@vvcephei John, I do indeed like that option of `applicationId + ""-"" + processorName + ""-recordhash""` because of the existing precedence with the made-up changelog topic. I believe a decision like this would further align us with the statement ""topic should always be non-null in serialize/deserialize"".

So it comes down to 3 options:

1) Punt any question on the SerDe topic nullability by removing the optimization.
2) Require that SerDe topic parameter be nullable by setting topic to null as I have already done in this PR.
3) Require that SerDe topic parameter be non-nullable by creating a fake topic name.

In order of preference, I believe 3 is most reasonable, followed by 1. I think 2 is actually fairly reasonable, but least likely to provide a productive way forward because of the existing ambiguous contract. Nullability would require that the loosest form of String be used.

If no one else objects, I'll go about using a `applicationId + ""-"" + processorName + ""-recordhash""` and post an update shortly.

"
277394421,5527,bellemare,2019-04-22T18:55:23Z,"Small change. Wasn't able to figure out a clean way to get processorName, but since it's a dummy anyways, I went with context().topic() instead of processorName
"
278785940,5527,guozhangwang,2019-04-26T01:44:10Z,"Back to the SR / Avro-serde issue we discussed above: after thinking about that a bit more, I'm wondering if we should just fix it by hard coding a serde for the hash value.

Since our current serde precedence is:

1) User specified serde at the DSL control object.
2) Streams hard-coded serde on some special operator (e.g. at `count` operator we hard-code LongSerde).
3) Globally registered serde via config.

For the hashvalue, we can treat it as case 2) above and hard-code a serde internally that override the registered serde. And for this foreign key join operation we would not expose control objects for serde overrides case 1), hence this way we should be safe. WDYT?"
278872345,5527,mjsax,2019-04-26T09:25:08Z,"From my understanding, the design was to first serializer the data, and afterwards compute a hashvalue based on the `byte[]` array. However, for the first step, we don't know the type and thus cannot hard code a serializer.

Is my understanding incorrect?"
278953143,5527,bellemare,2019-04-26T13:41:49Z,"This is my understanding as well, which is why I do not see another way forward at the moment. "
279118475,5527,guozhangwang,2019-04-26T22:14:28Z,"Ah okay, I think I was the culprit for the misunderstanding here: I was thinking that we will 1) compute the hash code first, and then 2) serialize that hash code, and hence the issue is that SR also register for a primitive type which I thought should be addressed at Streams.

Now I realize that we are 1) serializing first, and then 2) compute the hash value based on the bytes. This actually makes much more sense since we cannot guarantee all typed objects have a consistent implementation of the `hashCode` interface. In this way though, the bottom line is that we are calling the serializer for getting the bytes, but we are not actually sending the bytes over the wire. And there are serializers like Confluent AvroSerde out there which tries to remember which topics are serialized with which schema, which will then break since Streams are not actually using the serialized bytes actually.

Given that scenario (hopefully I got it right this time :P), I think although it is indeed fixable on Confluent AvroSerde to not register schema given `null` topic name, it is still only one serde and we cannot tell there are no other serdes in the wild that relies on the passed in topic name parameters to bookkeep some mappings, and therefore I think we still need to consider this issue at the Streams layer. On this regard my preference is that, at the moment, we just pass in the actual topic name than using a consistent dummy topic name, and my reasoning is that repartition topics are really internal ones specific to the streams app itself, and not supposed to be exposed outside the app (though for changelog topics there may be some exceptions like other apps / consumers may want to read that topic directly, for repartition topics like this one I do feel that they should be really `transient` and not be leveraged by any external clients), and hence protecting such mis-usages sounds like an overkill to me.

In the long run, we should consider adding extra APIs in the general Serde interface which does not take extra parameters (for this purpose I think neither `topic` name or `isKey` boolean should be included) but just object -> bytes and vice versa, to leave no space for any serdes have semantical business logic around those fields. At that time we can then be on the safer side by choose the new APIs."
279160285,5527,bellemare,2019-04-27T16:22:52Z,"@guozhangwang - Sounds very reasonable to me. My question is, do I now simply use `context().topic()`? I believe that is correct, since it'll be the topic that we're sourcing the data from, but I just want to verify."
279167678,5527,mjsax,2019-04-27T20:37:59Z,"I discussed this with some other people, and somebody mentioned, that for the value we serialize, this value is actually also store in RocksDB (input `KTable`). We also know, that the corresponding `byte[]` are written into the store changelog topic. Hence, instead of using the repartition topic, using the changelog topic should be a better option, as it does not leak anything into SR (or whatever other Serdes might do with the topic name).

Even if there is not changelog topic for the input KTable (we do some optimizations and sometimes don't create one (eg, the store might actual be a logical view and is not materialized). But even for this case, using the changelog topic name seems to be save."
279514920,5527,bellemare,2019-04-29T19:58:05Z,"`context().topic()` gives the repartition topic name in the serializer, which is what I want. In the processor sections, where I use null, `context().topic()` gives me the input-topic name for the KTable... which is also fine, since the serializer will check against the input topic schema, which must be valid by definition of the data being within the topic...  so I suspect this issue can be laid to rest, in line with adaniline-traderev's suggestion. 

This removes any requirement for the upstream serializer to have to do special work for null values."
279749459,5527,bellemare,2019-04-30T13:20:15Z,"After a long discussion, yes, this is what I think is currently the best option. Thanks for the suggestion in the first place!"
280145412,5527,guozhangwang,2019-05-01T17:26:43Z,"@mjsax I'm not sure I can fully follow the suggestion of using changelog topic v.s. the repartition topic here: are you suggesting to do it universally or just for this case? If it is the latter case, I felt it a bit awkward due to inconsistency with other source `KTable` cases where we will just follow the `SourceNode / RecordDeserializer` path to deserialize using the source topic; it if it the first case, that also has some drawbacks since with today's topology generation not all source `KTable`s will need to be materialized to a store and hence not necessary having a changelog topic.

I still feel that using the source topic name (and i.e. in this case, the repartition topic) admittedly exposed to SR but is philosophically the right thing to do, and we should consider fixing it on serde APIs in the future. WDYT"
280147603,5527,vvcephei,2019-05-01T17:33:38Z,Might as well just remove it. I think Murmur3 is fine.
280147762,5527,vvcephei,2019-05-01T17:34:10Z,Wouldn't hurt to have some tests for this. Maybe copy those from Hive as well.
280158927,5527,vvcephei,2019-05-01T18:07:46Z,"Recommend making this final as well, and just moving the null initialization to the fk constructor."
280159330,5527,vvcephei,2019-05-01T18:08:52Z,"```suggestion
        return foreignKey;
```

We try to avoid unnecessary qualifiers. Also applies elsewhere. I won't call them out further at this time."
280187466,5527,mjsax,2019-05-01T19:34:29Z,"@guozhangwang I was just talking about the foreign-key case (not sure why you thought it might be anything else?). My understanding is the following: The contract is that we should pass a topic name into the serializer of which we want to write the data into. This contract breaks if we pass in the repartition topic name, because we write something different into the repartition topic.

You are right that the changelog topic might not exist, however, my personal take is, that registering for a non-existing topic, is a smaller violation of the contract that passing in the ""wrong"" repartition topic name. Note, that the changelog topic name is conceptually the ""right"" topic name. However, this case would not happen very often anyway (compare examples below).

Your comment trigger one more thought: the optimization framework could actually check for different cases, and if there is an upstream topic (either changelog or source topic that has the same schema), we could actually use this name.

Some examples (does not cover all cases):

```
builder.table(""table-topic"").foreignKeyJoin(...)
```
For this case we need to materialize the base table (that is also the join-table), and the schema is registered on `table-topic` already, so we can pass in `table-topic` to avoid leaking anything.

```
builder.table(""table-topic"").filter(...).foreignKeyJoin(...)
```
For this case we materialize the derived table from the filter() and we get a proper `filter-changelog-topic` and we can pass this one.

```
builder.stream(""stream-topic"").groupBy().aggregate().foreignKeyJoin(...)
```
For this case, the agg result KTable is materialized and we can pass the `agg-changelog-topic` as name.

```
builder.stream(""stream-topic"").groupBy().aggregate().filter().foreignKeyJoin(...)
```
For this case, the agg result KTable is materialized and we can pass the `agg-changelog-topic` as name, because the filter() does not change the schema. Thus, even if the join-input KTable is not materialized, we can avoid to leak anything by ""borrowing"" the upstream changelog topic name of the filter input KTable.

```
builder.table(""table-topic"").mapValues(...).foreignKeyJoin(...)
```
For this case, we need to materialize the result of `mapValues()` and get a proper changelog topic for the join-input table.

```
builder.table(""table-topic"", Materialized.as(""foo"")).mapValues(...).foreignKeyJoin(...)
```
This might be a weird case, for which the base table is materialized, while the input join-table would not be materialized, and also the type changes via mapValues(). Hence, the `table-topic` schema is not the same as the join schema and we also don't have a changelog topic for the join-input KTable. We still use the changelog-topic name of the non-existent changelog topic (of the mapValues() result KTable).

As you can see, we can cover a large scope of cases for which we don't leak anything and can always use a topic name that contains data corresponding to the schema. Does this explain my thoughts?"
280187571,5527,vvcephei,2019-05-01T19:34:49Z,"```suggestion
    KF getForeignKey() {
```

In general, we try to retrict visibility to the minimum required. I won't call these out further at this time."
280188086,5527,vvcephei,2019-05-01T19:36:37Z,"Might be a good idea to throw an exception here, to make sure we don't modify the code later and falsify this assumption."
280188308,5527,vvcephei,2019-05-01T19:37:30Z,"Also, I'm not sure about the fact that configure is called outside of this chain, but close is part of this chain... Seems like an ambiguous ownership situation. "
280189461,5527,vvcephei,2019-05-01T19:41:36Z,"```suggestion
        if (data.length == Integer.BYTES + foreignKeyLength) {
```

I personally prefer this any time you're specifically referencing the size of an int or long."
280189627,5527,vvcephei,2019-05-01T19:42:09Z,unused
280190516,5527,vvcephei,2019-05-01T19:44:58Z,"Take this with a grain of salt if you don't like it, but you could save a fair bit of code if you just implement all the `Serde`, `Serializer`, and `Deserializer` interfaces in this one class. (the `serializer()` and `deserializer()` getters can just return `this` in that case).

Personally, I kind of like this because it puts the serialization and deserialization logic right next to each other, which seems easier to maintain their symmetry. "
280191410,5527,vvcephei,2019-05-01T19:47:53Z,not sure what this means...
280192430,5527,vvcephei,2019-05-01T19:51:02Z,"My immediate reaction is that it's a little surprising that `byPrimaryKey := false` means ""use _only_ the foreign key"". Boolean flags like this generally contribute to code complexity/obscurity. What do you think about just making separate partitioners for the primary and foreign keys?"
280193830,5527,vvcephei,2019-05-01T19:55:29Z,it's a little surprising that the store's name is the same as the topic name
280194501,5527,vvcephei,2019-05-01T19:57:36Z,"generally, we forbid null keys in KTable APIs for this reason. Feel free to just drop the record, log a message, and increment the right metric (look around for other null-key checks in the KTable processors)."
280195544,5527,vvcephei,2019-05-01T20:00:35Z,"Just dropping this in before I forget, following KIP-307, we need to provide a mechanism to name these operators and internal states. "
280258689,5527,guozhangwang,2019-05-02T00:07:58Z,"I understand your reasoning now, but still I felt Streams should not fix it trying to piggy-back on another topic that happens to be of the same schema that this serde is used for; or rather, I'd prefer to use a non-exist dummy topic than an existing topic if we do not like repartition topics (again, I agree that repartition topic is not ideal, since we are, in fact, not sending the bytes serialized in that way to the topic)."
281378147,5527,vvcephei,2019-05-06T21:53:40Z,"This reads a little funny to me. Can we just say that it's a many:1 join with the other table, and the foreignKeyExtractor selects the key in the other table to join with?"
281378332,5527,vvcephei,2019-05-06T21:54:17Z,Missing `@Override`
281378589,5527,vvcephei,2019-05-06T21:55:13Z,Feels like this deserves a comment.
283745078,5527,bellemare,2019-05-14T11:12:12Z,Done.
283746677,5527,bellemare,2019-05-14T11:17:24Z,Roger that. Will review code + apply.
283748861,5527,bellemare,2019-05-14T11:24:01Z,Done.
283756936,5527,bellemare,2019-05-14T11:49:18Z,"Done. I copied the tests over. They depend on Guava so I added that as a test dependency for the kafka common package. It's also Apache-2.0 license so I don't think there's a problem with this, but if there is please let me know."
283758235,5527,bellemare,2019-05-14T11:52:54Z,"I'm not sure I follow on this point. Is it that the serdes may be closed multiple times, once by my CombinedKey(Ser/De)erializer and also outside of it?"
283758342,5527,bellemare,2019-05-14T11:53:12Z,"Haha yes, programming 101. Fixed. :)"
283759023,5527,bellemare,2019-05-14T11:55:15Z,Thanks - removed.
283759466,5527,bellemare,2019-05-14T11:56:34Z,Deprecated comment. Should have been removed. Fixed.
283760279,5527,bellemare,2019-05-14T11:59:01Z,"Good point. I think you are correct and that it will be clearer. The irony here is that I never use it with byPrimaryKey=True... I think I abandoned that part of the code and forgot to clean it up. So, no more boolean, and I will clean up the name."
283766444,5527,bellemare,2019-05-14T12:16:09Z,"I think this makes sense. I did spend time tabbing back and forth while writing it, so having them together sounds like a better idea to me. Done."
283769402,5527,bellemare,2019-05-14T12:24:13Z,Done.
283779807,5527,bellemare,2019-05-14T12:49:23Z,Bit of a typo there. It should be the state store name. This state store is materializing the CombinedKey and so should not be associated with any particular topic name. I will clear this up.
283816226,5527,bellemare,2019-05-14T14:06:20Z,Done. Let me know if it's still unclear.
283816527,5527,bellemare,2019-05-14T14:06:58Z,Done.
283830915,5527,bellemare,2019-05-14T14:34:01Z,Done.
283833355,5527,bellemare,2019-05-14T14:38:09Z,The wiki says it's currently under discussion - has it already been accepted? Just curious as to if this work needs to be done for 2.3 if it's not going to be a part of it.
284348335,5527,vvcephei,2019-05-15T16:39:07Z,"I think this is fine, but of course I'm not a lawyer. :)"
284349058,5527,vvcephei,2019-05-15T16:40:44Z,"I haven't reviewed the most recent state, so this might not be relevant anymore, but I just meant that this class is *not* responsible for calling configure, but it *is* responsible for calling close. Ideally, the same class should be responsible for both."
284349463,5527,vvcephei,2019-05-15T16:41:40Z,"Heh, I wouldn't say it's so basic. We use literals all over the code base for this; I've just recently started promoting the use of the constant field instead."
284350221,5527,vvcephei,2019-05-15T16:43:25Z,"Woah, it looks like the contributor forgot to update the KIP. It was accepted. I'll update the wiki."
284853800,5527,bellemare,2019-05-16T19:02:00Z,"I put this in its own interface for now because it's not immediately clear where it should live. It needs to be part of StateStore, given how all of the various wrapper classes, like the metered, caching and logging ones work. However, I do not want it to be in the ReadOnlyKeyValueStore as it shouldn't be exposed to the outside world. "
284854252,5527,bellemare,2019-05-16T19:03:19Z,One of the effects of prefixScan needing to be attached to StateStore
284854473,5527,bellemare,2019-05-16T19:03:57Z,"I'm not really sure how the Timestamped statestores work, nor if I even need to support this (since my intention is to only use the non-timestamped versions)."
284854893,5527,bellemare,2019-05-16T19:05:12Z,"This is a weird one. I end up with a stack overflow error as the above function calls just go back and forth. It seems like it's getting some weird scope problem. This works around it, but I am not sure if I stumbled on an existing bug or if it's something I introduced. I'm just not really sure how what I did could have caused this."
284855494,5527,bellemare,2019-05-16T19:06:52Z,All the above will be cleaned up.
284856271,5527,bellemare,2019-05-16T19:09:16Z,"I'm required to supply a materialized store with a name, otherwise the internals of KTableImpl cannot get the name for the state store, and therefore the SubscriptionResolverJoinProcessorSupplier cannot access it (fails with NPE). This is related to the aforementioned changes to the KTableImpl constructor dropping the `isQueryable` boolean."
284857540,5527,bellemare,2019-05-16T19:13:03Z,"Am I going to need to handle all of these statestore types? I suspect I'll need to handle both `TimestampedKeyValueStore` and regular `KeyValueStore`, but given that `context.getStateStore` can return any of them, I don't know if I should expect to handle the remainder."
287059994,5527,bellemare,2019-05-23T17:44:50Z,"@guozhangwang 
Hi Guozhang - I noticed that in this pull ( https://github.com/apache/kafka/commit/c0353d8ddce88bac6fc04f85dd40cb95b8ca5cf9 ) you made it such that the non-queryable store name is no longer obtainable in KTableImpl. I need access to the materializedInternal.storeName, but cannot get it anymore (that I know of). Do you have any suggestions of what I should do to get it? My only other options appear to be rewriting the KTableImpl constructors back to what they were before, but since you made this change I would like your input.

Basically, I am now being given materializerInternal.queryableStoreName, but I need materializerInternal.storeName for when the user does not specify the materialized store (which is frequently).

Please advise - Thanks"
289081888,5527,guozhangwang,2019-05-30T17:06:36Z,"@bellemare Sorry I've just seen this now. The main motivation of that PR is to avoid materializing state stores unnecessarily and the queryable-name refacotring is sort of a by-product. I was thinking that the parent store names (user-specified or internal) can be accessed from `KTableValueGetterSupplier#storeNames`, would that be a possible work-around for you?"
289094679,5527,bellemare,2019-05-30T17:38:45Z,"EDIT: I may have something. I am leaving this here for now to show where I am stuck, but I will update it if I figure out how to properly use your suggestion...

@guozhangwang - thanks for the reply. Basically `KTableA.doJoinOnForeignKey(....) ` needs to be able to access `KTableA's` own internal state store to compare the hash values for the workflow we established in KIP-213. When `KTableA` is not materialized (internally or by user-specified queryable-name), then I do not think we can query the parents directly since they are by definition different data stores. I do not see how they can slot in as a replacement for the following section (originalSource is what used to be the always-populated statestore, user-specified or internally materialized, represented by queryableStoreName)
https://github.com/apache/kafka/blob/0d6d92e4a137ac06f3f6eb28495a38714a5734e1/streams/src/main/java/org/apache/kafka/streams/kstream/internals/foreignkeyjoin/SubscriptionResolverJoinProcessorSupplier.java#L86-L89

Based on my understanding, it appears I NEED to have TableA materialized, otherwise I am not sure how to proceed with this ticket. This is basically where I am stuck.
 
I admit I don't know enough about this part of Kafka Streams to speak with certainty on this.

"
289108628,5527,guozhangwang,2019-05-30T18:13:10Z,"I see.

I think the protocol should be that when a join operator requires to access its parents (the joining KTables, for example in this case) it will quire from its parent's `valueGetter`, which would chase up until it finds some processor with associated state stores, or it would go all the way up and require the source KTable to be materialized (see `KTableImpl#valueGetterSupplier` --- actually you can find the related materials I got from Kafka Summit this year, https://www.slideshare.net/GuozhangWang/performance-analysis-and-optimizations-for-kafka-streams-applications-145539130 around slide 56 :)

So back to your example, even if the source KTable is materialized without a name, calling a join later should cause its `KTableSource#materialize()` be triggered, where its internal `storeName` which is always not-null since it would be auto-generated as  `nameProvider.newStoreName(generatedStorePrefix)` if not specified, would be assigned to its exposable `queryableName`. And then this name can be accessed by its descent processors in the topology if necessary.

Hope this would help for you to figure it out."
289109055,5527,guozhangwang,2019-05-30T18:14:22Z,"Note the logic `should cause its KTableSource#materialize() be triggered` may need to be added by you, i.e. the `SubscriptionResolverJoinProcessorSupplier` should require to get its parent's valueGetter, which eventually would trigger this function."
289121346,5527,bellemare,2019-05-30T18:45:39Z,"@guozhangwang I think I solved it with your original recommendation, though by getting the valueGetterSupplier as you recommended and not the storeNames directly. Your follow up message is exactly what I did, so I am glad to have that confirmation. It's passing my tests now and appears to work with and without user-specified queryableStoreNames, so I believe this did the trick! Thanks for your help!  

Now I just need to work on the remaining highlighted issues. "
289122840,5527,bellemare,2019-05-30T18:49:23Z,"Overcome by events. No longer relevant  
"
289124387,5527,guozhangwang,2019-05-30T18:53:10Z,Great!
296021873,5527,vvcephei,2019-06-20T21:31:11Z,This should get reverted before merging. Also with the other commented-out stuff below.
296022446,5527,vvcephei,2019-06-20T21:33:08Z,unused
296022680,5527,vvcephei,2019-06-20T21:34:01Z,"Please attribute the source, along with noting the commit hash, as you did for the main code."
296023510,5527,vvcephei,2019-06-20T21:36:54Z,"I've always felt that this was a weird use of the `ValueMapper` class, elsewhere. What do you think about using `java.util.function.Function<V, KO>` instead?(just now having this thought)"
296023880,5527,vvcephei,2019-06-20T21:38:09Z,"Thanks! You can ""resolve"" this comment to clean up the PR."
296025422,5527,vvcephei,2019-06-20T21:43:18Z,"documentation nit. Since this is such a complicated operation, I think we might want some more verbose
comments here. Something like:
> Create a processor that registers the primary key to receive updates from the
> foreign key side. Also de-registers in the case of the PK document being deleted
> or its foreign key relationship changing."
296025600,5527,vvcephei,2019-06-20T21:43:54Z,"as mentioned in the KTableRepartitionerProcessorSupplier, I'm wondering if we can do without this
partitioner by using CombinedKey only in the prefix-scan store, and not over the wire?.
"
296026278,5527,vvcephei,2019-06-20T21:46:07Z,"""thisStateStoreName"" is a little esoteric :) Can we try to make this more self-documenting?
I'm assuming that you're copying this convention over from the other join, where we say ""this"" and ""other"", but there's a lot going on here. Maybe you can establish a different convention, like ""primary key table"" and ""foreign key table"", or ""left side"" and ""right side"", etc.."
296026592,5527,vvcephei,2019-06-20T21:47:06Z,"As I understand it, the value getter interface is mainly to support looking up values in another KTable.
It's fine (and more direct) for you to just wire the same state store in to both processors.
Then, you won't need a ""prefix value getter"" interface anymore."
296026762,5527,vvcephei,2019-06-20T21:47:34Z,"This doesn't seem quite right... I expected to see a *this* joiner handling updates from this table, and
and *other* joiner handling updates from the other table (the SubscriptionResolverJoinProcessorSupplier),
followed by a merge node, but it looks like we're doing something else here?
"
296026938,5527,vvcephei,2019-06-20T21:47:58Z,"Didn't totally follow this todo...
The PK-side KTable needs to be copartitioned with the input to the ""SubcriptionResolverJoinProcessor""
The FK-side KTable needs to be copartitioned with the output of the ""KTableRepartitionerProcessorSupplier""
In both cases, there's one pre-determined topic and one repartition topic we control, so the co-partitioning
should be trivial, unless I'm missing something...
"
296027875,5527,vvcephei,2019-06-20T21:50:58Z,"Can we assume both keys are not null (i.e., can we add a condition to skip any null-key updates in both processors so that we can rely everywhere on the fact that keys can never be null? Other stateful operators already do something similar (see aggregations, for example), and there are metrics and logs for it.

EDIT: I later figured out that this is part of the prefix-search magic."
296028126,5527,vvcephei,2019-06-20T21:51:55Z,"not sure I follow what this case means

EDIT: I later figured out that is a ""magic"" case for creating search prefixes..."
296028311,5527,vvcephei,2019-06-20T21:52:35Z,"Can we name the type parameters PK and FK here and elsewhere, just so we don't have to remember which is which?"
296028562,5527,vvcephei,2019-06-20T21:53:35Z,"regarding `&& foreignValueAndTime.value() != null`, is this because we're only doing an inner join?"
296028691,5527,vvcephei,2019-06-20T21:54:08Z,There's a protocol for handling cases like this. Check out the aggregate processors.
296028773,5527,vvcephei,2019-06-20T21:54:26Z,"Not sure if identity is sufficient here, since these objects come from deserializers (i.e., this condition may _never_ be true)
Also not sure if we can rely on `equals` to be any better than identity.
I'm wondering, rather than checking inside this processor, we should ask the upstream table to suppress such no-ops, similar to how we request `sendOldValues`.
Then, the upstream processor can perform this check on the serialized form of the data and save a bunch of work downstream (aka, right here).
"
296028913,5527,vvcephei,2019-06-20T21:55:02Z," Oooh... For reference, I just spent 30 minutes trying to figure out how the prefix scan is correct,
only to realize that you have created a special ""search key"" that just happens to produce a prefix
of the ""stored key"" when used in conjunction with your CombinedKeySerde. This seems like a _very_
mysterious implicit coupling relationship among 4 different classes.
There's got to be a better way..."
296028980,5527,vvcephei,2019-06-20T21:55:17Z,"we'd better close this iterator.
"
296029149,5527,vvcephei,2019-06-20T21:55:53Z,"maybe a nit, this sentinel is constant and small, so caching it in a static field is probably valuable."
296029321,5527,vvcephei,2019-06-20T21:56:30Z,"Can the key actually be an empty array? If so, it's not a safe sentinel value for null.
The hashed value of an empty array is `[-7122613646888064702, -8341524471658347240]`. It seems
like we could do better by actually using an empty array as the sentinel-hash representing a
null value. Then, we don't have to send any bytes over the wire to represent null, or store
any bytes for that matter."
296029499,5527,vvcephei,2019-06-20T21:57:20Z,"alternatively, does this mean there's no need to notify the other side at all, and we can just proceed to recompute the join result? Might break semantics, though..."
296029644,5527,vvcephei,2019-06-20T21:57:52Z," if we move the primary key into the value and just forward to the foreign key, we can drop the extra partitioner and also extra serde from the wire protocol"
296030000,5527,vvcephei,2019-06-20T21:59:09Z,"This hash _must_ match the one we produce in KTableRepartitionerProcessorSupplier.
If we _ever_ produce a different hash for the same value, we will _lose data_.
In light of this, it seems like both locations really need to reference the same logic somewhere,
either in a static method somewhere, or passed in as a ""hash function"" over the values.
As an illustration of the legitimacy of the concern, you're using a different topic here than
in the other location, which could result in a different serial form of the same data.
"
296030317,5527,vvcephei,2019-06-20T22:00:19Z,"maybe a nit... maybe move this byte to the front and make it a bit field, so that we have a straightforward path to pack more boolean flags into the value in the future."
296031064,5527,vvcephei,2019-06-20T22:03:10Z,"Adding this new interface to KeyValueStore may have some serious implications for implementers. Are we sure we have to?

At least, we need to add a default implementation."
296031167,5527,vvcephei,2019-06-20T22:03:33Z," note this isn't a byte[], but a K. Not actually sure if this API makes sense.
For example, what would this API mean if my key type is UUID or MyCustomRecord?
In other words, only some key types even have prefixes.
"
296033792,5527,vvcephei,2019-06-20T22:13:07Z,"Sorry, feeling a bit dense... Is this right? It seems like it would cut off some results that should be at the end of the range. 

In particular, it seems like it cuts off keys like `[prefix]FF01`, since that key is strictly greater than `[prefix]FF`.

You might want to take a look at the function that RocksDB uses to compute the range-end for range queries."
296034425,5527,vvcephei,2019-06-20T22:15:38Z,"Just a thought, does Rocks return in sorted order? If so, we can probably
do better by comparing backwards from the end of the prefix.
Then again, that might screw up cache locality, not sure...
"
296034482,5527,vvcephei,2019-06-20T22:15:50Z,probably should be IllegalStateException
296034728,5527,vvcephei,2019-06-20T22:16:47Z,"Huh, neat...
As I read it, the serial format for our CombinedKeys is [FK length][FK bytes][PK bytes]
This means that all the records with same-length foreign keys are sorted together, a prefix scan won't
suffer any ambiguity between FK and prefixes of the PK.
This seems to avoid a problem we face in the session window store. Putting it in terms of this key
If we have two records with r1.FK=aa r1.PK=b and r2.FK=a and r2.PK=ab,
they are both serialized as aab, and in particular,
a prefix scan would get pseudomatches that are out of the range and
have to handle it by decomposing the serial form and then double-checking the prefix.
But since you prefix by the size of the FK, there's no ambiguity, even if the FKs are variably sized!

But it is worth noting that this depends on the exact serialization format.
"
296034895,5527,vvcephei,2019-06-20T22:17:23Z,"This is due to the subclassing. If you instead wrap and delegate, this won't be a problem."
296857031,5527,bellemare,2019-06-24T18:29:39Z,"Yep, this is the intended design. It definitely is sensitive to how the serialization is handled, but I think that it's okay as it currently stands."
296858048,5527,bellemare,2019-06-24T18:31:57Z,"Yep. I'm taking a look at this now and I don't know what I was thinking. I have tested some of the subMap logic and it indeed seems that I am cutting off some values. Thank you for catching this, ugh.

Range is a bit different because it specifies the entire key, whereas with prefix we're only concerned with the first N bytes of the key. I will think on this some more. I am wondering if it's best to just wrap the entire thing in an iterator that compares the prefix on each ""next()"" call, and only returns those that meet the full prefix, much like I already have established in the RocksDBDualCFPrefixIterator.  
"
296898877,5527,bellemare,2019-06-24T20:22:38Z,"Okay, so I believe that I have figured out what I was doing wrong... I needed to do:
`map.subMap(fromKey, true, toKey, false);`
and set toKey to be fromKey + 1 as a byte array. I am currently testing it and it seems to be working, but I want to test further in case I am misunderstanding the submap results."
296908737,5527,vvcephei,2019-06-24T20:48:08Z,"Yeah, that last thing sounds like what I had in mind. The RocksDB function I had in mind is one that basically computes `+1` for an array of bytes (i.e., iterate backwards from the end to find the first byte that isn't already `0xFF` and add `1` to it)."
298135494,5527,bellemare,2019-06-27T11:45:43Z, 
298136127,5527,bellemare,2019-06-27T11:47:36Z,Cleaned.
298137597,5527,bellemare,2019-06-27T11:52:05Z,"Done. Hash included, along with link to file in Github."
298138716,5527,bellemare,2019-06-27T11:55:17Z,"To me it felt natural as I wanted to apply a mapping function to a value and obtain a result. Semantically they feel the same though, so I don't really care one way or another. If you think it's more appropriate to go with `java.util.function.Function<V, KO>` I will make the change."
298143733,5527,bellemare,2019-06-27T12:10:41Z,"According to their wiki ( https://github.com/facebook/rocksdb/wiki/Basic-Operations ) it does provide a sorted iterator. The range function operates on this principle too, which confirms this to be the case in the code.

We have to validate the entire prefix for each event either way, forwards or backwards. As soon as we run into an event which does not have the full prefix, we terminate and hasNext() will from then on return false.

I don't think there's anything in terms of efficiency to be done here, but let me know if I misunderstood."
298143789,5527,bellemare,2019-06-27T12:10:51Z,yup. Thanks!
298198372,5527,bellemare,2019-06-27T14:11:53Z,Will be adding this in shortly.
298669556,5527,bellemare,2019-06-28T16:45:02Z,I'll do my best to clarify the entire operation in the next commit.
298670452,5527,bellemare,2019-06-28T16:47:50Z,"I'm fine with using the  ""primary key table"" and ""foreign key table"" lingo, but given that this was stripped out of the function name somewhere previously in the review, I wasn't sure it was ""allowed"". To be frank, _I_ find it confusing to talk about this as if it's _not_ a `foreignKeyInnerJoin` and `foreignKeyLeftJoin`, but instead some sort of special-case normal `join` and `leftJoin`. 

I'm just going to go ahead and call it what I want internally, and then we can all bicker about the bike-shed name afterwards. :)"
298799471,5527,bellemare,2019-06-29T13:20:55Z,"It just means the FK is the same - the hash may have changed. We need to update the hash in the subscription state store on the RHS, otherwise valid updates coming from the RHS Table will join on a stale hash, and be discarded during the resolver phase (oldHash != currentHash).
"
298799689,5527,bellemare,2019-06-29T13:30:26Z,"Yep, you're correct. I believe this is an artifact left over from the initial design, when I used to send (CombinedKey<KO,K>, V) as the event, and it was either put the FK in the Key or in the Value. I'll adjust the code accordingly."
298801769,5527,bellemare,2019-06-29T14:52:45Z,"Okay, while I took it out from over-the-wire, but we still need the Serde for the `prefixScanStoreBuilder`, since CombinedKeys are stored to the changelog topic. The end result is that the only thing that is removed is the custom partitioner, which I like as I we no longer rely on a copied + pasted + slightly-edited partitioner."
298802096,5527,bellemare,2019-06-29T15:03:53Z,"While it could go into a static method, it will always need a topic name unique between all topologies because of the weak interface definition of `serialization`. It is ambiguous as to whether topic can be null in `serialize(topic,data)`. From experience, the Confluent Avro Serde _will_ register once with a null topic, but then will forever give already-registered exceptions for all remaining registrations. This means that we will forever need some unique dummy topic per foreignKeyJoin schema, and it will need to live in some common place. 

I can create a dummy topic name in the `KTableImpl.doForeignKeyJoin()` and pass in the necessary common reference to both `SubscriptionResolverJoinProcessorSupplier` and `KTableRepartitionerProcessorSupplier`. This will ensure that `serialize(topic,data)` is consistent, since the topic will not rely on the topology (outside of uniqueness). "
298802436,5527,bellemare,2019-06-29T15:16:43Z,"changed to instructions, no longer relevant"
298802801,5527,bellemare,2019-06-29T15:31:05Z,"I am not sure we have to add this to all KeyValueStores. I had previously tried to limit it to the RocksDB instance only, but previously-given feedback suggested that it should support both caching and logging for performance related reasons. What we have now is due partially to a snowballing of inclusion of caching, logging and rocksDB store, and partially due to my thinking that ""If Range can work in all tables, why not prefixScan since it's basically the same operation?""

The PrefixScanKeyValueStore needs to support both get() (for LHS->RHS lookup) and prefixScan() (RHS->LHS lookup), and get() is tied to ReadOnlyKeyValueStore. I also didn't want to expose prefixScan to the outside world via ReadOnlyKeyValueStore.

If you (or anyone else) feels strongly about removing it, please direct me on how you would see best to limit it to RocksDB + Caching + Logging. I am not confident in messing with the existing structure too much, so guidance / direction on it would be preferred than leaving it strictly up to me.  
"
298803316,5527,bellemare,2019-06-29T15:49:33Z,"The key can be anything. The issue is that Murmur3 cannot hash a null to a unique sentinel that wouldn't also be available by hashing another value. So any value we choose will not work.

That being said, I could do something like (you mentioned using a bit in another comment):
Hash is null: `{1-bit nullhash = true}{remaining-serialized-data}`.
Hash is notNull: `{1-bit nullhash = false}{2-bytes hash}{remaining-serialized-data}`.

The only thing I am rusty on is how the bit is handled in over-the-wire communication. Would it not simply end up being stored as a full byte anyways? In this case, we're adding a full byte to each message just to indicate if the hash is actually null, and this is in both directions. However, for correctness sake I believe we should do this as I don't think there is any safe sentinel we can use.

If we want to save space, we have two more options. 
1) Fold this in with the Version byte, and only give Version 7-bits.  {1-bit-nullhash}{7-bit-version}
2) Can also switch to Hash64 (50% chance of collision in ~4 Billion events) instead of Hash128 (50% chance of collision in ~1.844674407e19).

For the sake of moving this along, I will just include a single-bit of isNullHash and take it from the Version byte. This should satisfy our needs.
"
298808164,5527,bellemare,2019-06-29T18:48:06Z,"I folded all this in, the 7-bit version and the 1-bit and the 1 bit ""isNull"". I'll resolve this one now and if it's no good for whatever reason then just open another."
298808919,5527,bellemare,2019-06-29T19:20:22Z,overcome by events.
298809600,5527,bellemare,2019-06-29T19:50:15Z,Removed in latest version.
298809661,5527,bellemare,2019-06-29T19:52:20Z,"All joins are being executed in the SubscriptionResolverJoinProcessorSupplier. Everything comes in from the SubscriptionResponseWrapper topic, so there is no this/other."
298809878,5527,bellemare,2019-06-29T20:03:23Z,"Sorry, my comment was not very clear. Currently what you described DOES work for a single `leftTable.joinOnForeignKey(rightTable, ...)`. However, I was concerned how it would work when we start chaining them together. I currently have a (somewhat complexly written) integration test (`KTableKTableForeignKeyInnerJoinMultiIntegrationTest`) where I do:

```
        CLUSTER.createTopic(TABLE_1, TABLE_1_SIZE, 1);
        CLUSTER.createTopic(TABLE_2, TABLE_2_SIZE, 1);
        CLUSTER.createTopic(TABLE_3, TABLE_3_SIZE, 1);
```
and then:
`TABLE_1.joinOnForeignKey(TABLE_2, ... ).joinOnForeignKey(TABLE_3, ...)`
The test does not pass (times-out after 60s of waiting) when `TABLE_1_SIZE != TABLE_3_SIZE` , using the co-partitioning as you suggested above. I'm not familiar enough with what's going on under the hood to explain it, nor have I spent much time digging deeper, but it seems that the partition distribution for chained `joinOnForeignKey` needs some attention.

"
298810060,5527,bellemare,2019-06-29T20:12:27Z,It would make it inconsistent with the types in KTableImpl. I standardized them across all the code in this PR because once learned it's consistent everywhere you look. I am hesitant to change them because the next comment will be someone asking to make them consistent with KTableImpl...
298864664,5527,bellemare,2019-07-01T02:18:54Z,"I will remove this. I am not sure how to go about asking the processor to not send no-ops, but unless it's built in I'd like to limit the scope. This PR is big enough that I don't want to add more than necessary."
298865371,5527,bellemare,2019-07-01T02:26:44Z,"I will add more comments to make it clearer. The coupling is indeed tight because it's based strictly on the byte ordering of the key. I couldn't think of a better way, so if you think this is a deal-breaker please let me know and we'll throw it back on the mailing list."
298866241,5527,bellemare,2019-07-01T02:35:21Z,"Corrected the comments - this is probably also related to your question about where should the prefixScan even go.
The only real requirements are RocksDB, Logging and Caching stores. Since they're so intertwined, it's hard for me to give you a good answer on if this should be here or not. If we have to ask, probably not, but TBH I'm really looking for guidance on this since I'm a bit over my head on where it should go. I have asked for feedback on this previously, but so far you're the only one to take not. "
299600884,5527,bellemare,2019-07-02T17:34:00Z,"Can't handle multiple input partition counts if we chain foreignKeyJoins together.

I did a few hours of investigation into this and it appears it's related to a combination of:
a) multiple topicGroups ( https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/InternalTopologyBuilder.java#L986 )
b) Creating Sinks (for both the Subscription and SubscriptionResponse topics)
c) How sinks are handled with copartitioning and a per-topic-group way:
  => The result is that SINK topics are assigned the maximum number of partitions in the topicGroup (as all SINK topics are treated -  https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java#L472).
  ""if this topic is one of the sink topics of this topology, use the maximum of all its source topic partitions as the number of partitions""
  
The result is that we end up with the wrong (ie: a maximum) number of partitions for SOME but not ALL of the internal repartition topics. For instance, if I have the following:

`left.joinOnForeignKey(rightOneTable, Named.as(""join1""), ...).joinOnForeignKey(rightTwoTable, Named.as(""join2""), ...)`
```
leftTable partition count = 3
rightTableOne partition count = 5
rightTableTwo partition count = 7
```
I would expect to have the following topics with partitions:
```
name = join1-subscription-registration, partitions = 5 (matches the rightTableOne partitions)
name = join1-subscription-response, partitions = 3 (matches the leftTable partitions)
name = join2-subscription-registration, partitions = 7 (matches the rightTableTwo partitions)
name = join2-subscription-response, partitions = 3 (matches the leftTable partitions)
```
Instead, due to the aforementioned dynamics, I get:
```
name = join1-subscription-registration, partitions = 5 (matches the rightTableOne partitions)
name = join1-subscription-response, partitions = 7 (because it's copartitioned with join2-subscription-response, which is a SINK topic)
name = join2-subscription-registration, partitions = 7 (matches the rightTableTwo partitions)
name = join2-subscription-response, partitions = 7 (SINK topic, must have maximum partitions)
```

As a result, I will not be supporting variable partition counts in this PR, but will put it off until after this one is comitted and roll it in its own PR. The scope on this PR is large enough.
"
299612961,5527,bellemare,2019-07-02T18:01:42Z,"True - the unfortunate part is that wiring it in and getting it via `ProcessorContextImpl.getStateStore(..)` ( https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/ProcessorContextImpl.java#L88 ) is that it does not match any of the store types, and so cannot be accessed that way. I can modify THAT further, but this is all due to the same original question - where in the interface definitions should `PrefixScanKeyValueStore.prefixScan()` live? "
307344539,5527,vvcephei,2019-07-25T15:01:25Z,"Hey @bellemare, I think your feeling is reasonable, but to me it actually supports using a Function more than using a ValueMapper. What I mean is, you just want to run a _function_ on the value and obtain a result. The ValueMapper implies you're mapping the value to a new value to produce a new KTable with the result value. In other words, it only makes sense in a `mapValues()` operation.

The ValueMapper interface was introduced for the `mapValues()` operator, and we abused (along with others of our functional interfaces) by using it other places where we just want a unary function (because Java had no stdlib functions at that point).

Now that Java has Function, we can clean this mess up. Right now, though, I'm just proposing not to create more mess that we have to clean up later. If people disagree with using Function (for some reason), then we should make a new semantically correct interface instead of using ValueMapper."
307349571,5527,vvcephei,2019-07-25T15:10:40Z,"Ok, just for the record, I think we have to fix this before merging... I'll see if I can figure it out."
307394163,5527,bellemare,2019-07-25T16:39:15Z,I had researched it more extensively a few weeks ago  but just realized now I forgot to post the findings. They were pending publishing in my own review...
307394639,5527,bellemare,2019-07-25T16:40:15Z,I've just posted the findings. I am concerned about scope creep if we tinker with the copartitioner. As it stands right now I think the size of this PR is intimidating enough.
307397528,5527,bellemare,2019-07-25T16:47:19Z,Function it is. Your perspective on the misuse of ValueMapper makes sense.
311715968,5527,abbccdda,2019-08-07T19:05:58Z,nit: newline
318776174,5527,cpettitt-confluent,2019-08-28T20:26:53Z,"It might be better to throw if there is overflow. Otherwise the return prefixed key does not follow the protocol in CombinedKeySchema. I don't think it would happen in practice, but if we're going to address it one way or the other I would probably lean toward explicitly failing on overflow.

Otherwise we should at least doc that this widens the array of bytes in the case of overflow."
318778418,5527,cpettitt-confluent,2019-08-28T20:32:27Z,"As this is public documentation, it would be nice to explain what named is used for."
318778654,5527,cpettitt-confluent,2019-08-28T20:33:05Z,As this is public documentation it would be nice to have documentation on it. It looks like it would be the same as the previous sans named.
318778736,5527,cpettitt-confluent,2019-08-28T20:33:18Z,Same comment as above.
318802475,5527,cpettitt-confluent,2019-08-28T21:32:47Z,Do we need to be careful here about getting DOS'd by an extreme length either due to malice or accident? Do we have a key limit anywhere else that we could use to assert that `foreignKeyLength` is within acceptable limit?
318803831,5527,cpettitt-confluent,2019-08-28T21:36:44Z,"It seems given this that we can at least assert that foreignKeyLength is less than or equal to 2147483643 bytes. Though a better bound, if there is one, would be nice."
318813557,5527,cpettitt-confluent,2019-08-28T22:07:33Z,Should this be sent only when the old and new key do not match?
318814883,5527,cpettitt-confluent,2019-08-28T22:12:16Z,Looks like dead code that can be removed?
318816694,5527,cpettitt-confluent,2019-08-28T22:19:04Z,`value.getForeignValue() == null && (!leftJoin || currentValueWithTimestamp == null)` ?
318817354,5527,cpettitt-confluent,2019-08-28T22:21:28Z,minor: order of visibility keywords in field declarations are reversed here vs. other places in the code.
318819397,5527,cpettitt-confluent,2019-08-28T22:30:04Z,"Maybe worth an assert here that version fits in 7-bits? Not that we're going to hit it any time soon, but if we set the 8th bit then given the semantics below we are considered to have a null hash."
318820465,5527,cpettitt-confluent,2019-08-28T22:34:09Z,Same comment re. version as above.
318821811,5527,cpettitt-confluent,2019-08-28T22:39:37Z,`cache.isEmpty()` seems sufficient? Curious why you needed to change this.
318822498,5527,cpettitt-confluent,2019-08-28T22:42:21Z,Curious: is there any functional change by moving this code?
318822822,5527,cpettitt-confluent,2019-08-28T22:43:38Z,I think we can revert this for cleanliness.
318822997,5527,cpettitt-confluent,2019-08-28T22:44:19Z,"Given the size of this commit, it seems worth reverting non functional changes like this."
318823102,5527,cpettitt-confluent,2019-08-28T22:44:43Z,Same as last few comments :)
318823246,5527,cpettitt-confluent,2019-08-28T22:45:17Z,Same as above.
318824005,5527,cpettitt-confluent,2019-08-28T22:48:25Z,Minor: you could get away with keeping this mutable if you initialize it here and do `addAll` with `topicsToCopartitionGroup.get` below. Just check `copartitionGroup.isEmpty` to determine if you can break.
318824758,5527,cpettitt-confluent,2019-08-28T22:51:26Z,Is it? :D
319131058,5527,bellemare,2019-08-29T15:23:00Z,"My understanding is that if the foreign key is too large it couldn't have been written as a message to Kafka (exceeds maximum batch/message size). Accordingly, the maximum size of the foreignKey is limited to the maximum allowable batch/message size. Is this incorrect?"
319136512,5527,bellemare,2019-08-29T15:33:33Z,"I believe we chose this because with a regular join we propagate the same output value for each event sent in. If there are N events with the same old and new state, we output N joined events, even if they are the same.

If we do not send when oldKey == newKey, then we swallow + hide the event."
319136649,5527,bellemare,2019-08-29T15:33:50Z,Yep. I'll go through and try to clean it up. Lots of stuff changed and got reverted unsuccessfully.
319137563,5527,bellemare,2019-08-29T15:35:36Z,"Haha, good eye. I didn't notice the repeated logic at the end... sigh... Thanks."
319138304,5527,bellemare,2019-08-29T15:37:01Z," 
"
319139534,5527,bellemare,2019-08-29T15:39:27Z,I can put some validation in the serializer since it's the one that's forcing a byte into 7-bits.
319142830,5527,bellemare,2019-08-29T15:45:25Z,Same solution - will validate in serialization since it's the serializer that is limiting it to 7 bits.
319144549,5527,bellemare,2019-08-29T15:48:52Z,"This is an artifact of a broken trunk that I needed to fix to get this to compile a few weeks back.

Currently in trunk it seems to be fixed ( https://github.com/apache/kafka/blob/3619d2f383f65108dfd33686119f675aaeea54b7/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java#L237 ) , so I'll resolve this when I make a new PR rebasing this off of the Murmur3hash PR that needs to be resolved prior...."
319150616,5527,cpettitt-confluent,2019-08-29T16:02:01Z,"My bad! I thought this was coming from a topic, but this is just for the prefix matching in the store. This can be closed out."
319153538,5527,cpettitt-confluent,2019-08-29T16:08:32Z,"Makes sense.

My understanding is that sending an output even for each input is best effort, e.g. the output event is not emitted if the hash changes more quickly than the updates get through the RHS."
319153674,5527,cpettitt-confluent,2019-08-29T16:08:49Z, 
319172463,5527,bellemare,2019-08-29T16:53:53Z,"There was at one point - I believe I was using the storebuilder to build the store, then I had to connect it afterwards. It would throw an exception if I tried to connect a store that was not built yet. The work that John R did to fix up the topology seems to have remedied it, so I'll revert the order back to the original order."
319174179,5527,bellemare,2019-08-29T16:58:03Z,"Yes, I will do so. This was an artifact of my IDE organizing the imports I believe."
319195586,5527,bellemare,2019-08-29T17:50:08Z,Deftly overcome by events! Will remove the whole thing.
319198127,5527,bellemare,2019-08-29T17:56:13Z,"I'll leave that up to @vvcephei to comment on, as he is the one who made these changes... . Actually, now I'm not sure why this change is in here, I think this was supposed to be in its own PR? John?"
319261222,5527,bellemare,2019-08-29T20:38:03Z,"I see what you're saying, but I am not sure it's proper to put the exception in the Bytes.increment as it's doing what it says it should do (ie: increment the byte array by 1). 

John and I noticed this before, but since CombinedKeySchema always starts with a positive integer, wrap-around won't affect us in either case. As John noticed 22 days ago:

```
Actually, I think this is not a problem. The key length is actually not arbitrary, since all keys must fit into byte arrays, and arrays in java cannot be larger than ""max int"", aka 0x7fff ffff Thus, we already have room to wrap around, with the first byte becoming 0x8f.
```

We're implicitly protected as long as the maximum of positive integers are always 0x7fff ffff. "
319263781,5527,bellemare,2019-08-29T20:45:03Z," 
"
319275297,5527,cpettitt-confluent,2019-08-29T21:16:08Z,Good point. That seems to support an assertion / exception vs. adding behavior we don't expect to run though? If we overflow clearly there is a logic error somewhere in the callee and it would be better to find out directly vs. as a side effect of behavior we don't expect to exercise?
320388724,5527,bellemare,2019-09-03T17:24:51Z,"Yep, fair enough. I'll make it such that it'll throw an error if it would result in a wrap around. The caller will have to allocate additional room if they want it to roll over."
320395570,5527,bellemare,2019-09-03T17:40:50Z,IndexArrayOutOfBounds? Or do you have something else in mind? I can't seem to find a reasonable choice from the list...
320401139,5527,cpettitt-confluent,2019-09-03T17:53:34Z,"I don't have a strong opinion. I suppose `IllegalArgumentException(""overflow"")` would be fine. That seems most appropriate, especially if we document that increment only works for inputs up to max int - 1."
320401556,5527,cpettitt-confluent,2019-09-03T17:54:35Z,"BTW, this is not a blocker to me. I think if we get the one change we discussed above (exception on overflow) that things are looking pretty good from my perspective. I'll do a pass on the tests too."
321287361,5527,bbejeck,2019-09-05T14:09:51Z,nit: `PREFIX_SCAN_PROCESSOR` and `JOIN_ON_FOREIGN_KEY_NAME` names are unused.
321398773,5527,vvcephei,2019-09-05T17:49:17Z,"Thanks, @cpettitt-confluent . I think we're better off favoring immutability unless there's a significant readability or performance advantage to mutable state."
321400745,5527,vvcephei,2019-09-05T17:53:29Z,"using `size()` was actually identified as a performance regression. Trunk is now using `isEmpty()`: https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/state/internals/ThreadCache.java#L237

Regardless, this line should not appear in your diff at all. I'd just smash it with whatever is in trunk."
322342024,5527,bbejeck,2019-09-09T16:40:07Z,"Although this class is in an `internals` package, some brief Javadoc would be helpful. 
 The overall flow is complex, and a short description will help grok the functionality and where the class fits in the flow of things.  Same thing for all other classes in `internals.foreignkeyjoin` so I won't repeat the comment."
322368963,5527,bbejeck,2019-09-09T17:42:07Z,This is unrelated to this PR but I'm wondering the `ProcessorGraphNode` _always_ used as the node name we could simplify this class.  But let's just leave it as is for now.
322376681,5527,bbejeck,2019-09-09T17:59:31Z,"nit: The test doesn't cover `prefixBytes` method.  I'm not a proponent of 100% test coverage always, but IMHO that method is a main factor in the ""other->this"" join so it should have coverage in this case."
322855625,5527,bellemare,2019-09-10T16:55:24Z,"According to the commits I made, this should be removed by now... not sure why it's still showing up in the review..."
322918610,5527,bbejeck,2019-09-10T19:20:26Z,"I think it _may_ be possible to implement the integration tests using the `TopologyTestDriver`, the benefit of which would be that we could get coverage reports and the tests run faster without the embedded broker.  The test cases look complete to me, but without coverage reports, we could be missing something.  "
322919371,5527,bbejeck,2019-09-10T19:22:00Z,leftover debugging?
323413934,5527,bellemare,2019-09-11T19:15:44Z,"I'm not familiar enough with this myself. If it's problematic I would be willing to address it in another PR, as I'm experiencing some fatigue on this one. "
323413988,5527,bellemare,2019-09-11T19:15:53Z,Very much so. Thanks!
323985652,5527,mjsax,2019-09-12T23:12:49Z,"@bellemare @vvcephei @bbejeck 

Seems we need to update `StreamsResetter` to delete those topics, too?"
323985742,5527,mjsax,2019-09-12T23:13:15Z,@bellemare @vvcephei @bbejeck As above.
325438742,5527,vvcephei,2019-09-18T00:27:57Z,"Yes, we do. Good catch!"
326269999,5527,bbejeck,2019-09-19T16:30:05Z,"nit: Enabling optimizations is a two-step process.  In addition to setting the `OPTIMIZE` flag in the configs, we also need pass in the configs to the overloaded `StreamBuilder#build(Properties)` method here."
326272813,5527,bellemare,2019-09-19T16:36:36Z,"Whoops! That's not a nit, that's just a miss on my part. Thanks!"
426783396,8680,abbccdda,2020-05-18T17:25:17Z,nit: we could use {@link VersionRangeType} to reference to the classes.
426783652,8680,abbccdda,2020-05-18T17:25:44Z,Could be simplified as new Features<>
426783720,8680,abbccdda,2020-05-18T17:25:51Z,Same here
426805425,8680,abbccdda,2020-05-18T18:06:33Z,nit: extra line
426806321,8680,abbccdda,2020-05-18T18:08:17Z,Is this function only used in unit test?
426806671,8680,abbccdda,2020-05-18T18:09:02Z,We should ensure `features` is not null
426807646,8680,abbccdda,2020-05-18T18:10:58Z,"nit: just a personal preference, but getting one less internal reference to a public function `all` makes the code usage check easier, like `features.get(feature)`. "
426808456,8680,abbccdda,2020-05-18T18:12:36Z,"Also if we could potentially have a not-found feature, we should either fail with illegal state, or make the return type `Optional<VersionRangeType>`"
426808879,8680,abbccdda,2020-05-18T18:13:21Z,Maybe rephrase as `a map with the underlying features serialized`
426809504,8680,abbccdda,2020-05-18T18:14:35Z,s/Deserializes/Deserialize
426810909,8680,abbccdda,2020-05-18T18:17:09Z,We should check `null` for other.
426829453,8680,abbccdda,2020-05-18T18:53:37Z,"nit: might make sense to build meta comment for parameters:
```
/**
  * 
  * @param id
  * @param endPoints
  * @param rack
  * @param features
  */
```"
426836963,8680,abbccdda,2020-05-18T19:07:57Z,s/ !featuresAndEpoch.isEmpty / featuresAndEpoch.isDefined
426838064,8680,abbccdda,2020-05-18T19:10:06Z,This is because the write path has not been implemented?
426865773,8680,abbccdda,2020-05-18T20:07:15Z,nit: add a line
426873359,8680,abbccdda,2020-05-18T20:23:39Z,I think we need to bump the schema version to 4? Same with `ApiVersionsRequest.json`
426875434,8680,abbccdda,2020-05-18T20:28:01Z,"Looks like we have some gaps for unit testing `ApiVersionsResponse`. Could we add unit tests for this class, since the logic `createApiVersionsResponse` becomes non-trivial now?"
426875879,8680,abbccdda,2020-05-18T20:28:59Z,s/AllAPI/GetAllFeatures
426876134,8680,abbccdda,2020-05-18T20:29:35Z,We need the apache license title
426876793,8680,abbccdda,2020-05-18T20:30:54Z,We could use `org.apache.kafka.common.utils.Utils#mkMap` here
426876852,8680,abbccdda,2020-05-18T20:31:01Z,Same here
426877891,8680,abbccdda,2020-05-18T20:33:12Z,nit: new line
426884892,8680,abbccdda,2020-05-18T20:47:45Z,"In terms of naming, do you think `FinalizedVersionRange` is more explicit? Also when I look closer at the class hierarchy, I feel the sharing point between finalized version range and supported version range should be extracted to avoid weird inheritance. What I'm proposing is to have `VersionRange` as a super class with two subclasses: `SupportedVersionRange` and `FinalizedVersionRange`, and make `minKeyLabel` and `maxKeyLabel` abstract functions, WDYT?"
426890390,8680,abbccdda,2020-05-18T21:00:09Z,"Note this function is public, which suggests there could be external dependency that we need to take care of. The safer approach is to keep this static function and create a separate one with augmented parameters. cc @ijuma for validation."
426931875,8680,abbccdda,2020-05-18T22:44:55Z,I think we could delay the addition for these helpers until we actually need them.
426933479,8680,abbccdda,2020-05-18T22:49:35Z,"I gave it more thought, and wonder whether we could just call this function `features` to be more consistent with our convention for getters."
426933776,8680,abbccdda,2020-05-18T22:50:20Z,Need to check null
426934551,8680,abbccdda,2020-05-18T22:52:20Z,Is there a difference between `Objects.equals` and `this.minKeyLabel.equals(that.minKeyLabel)`?
426935082,8680,abbccdda,2020-05-18T22:53:54Z,"nit: testMinMax, and we could reuse the same `new VersionRange(1, 2)` by only creating it once. "
426936469,8680,abbccdda,2020-05-18T22:57:45Z,Does L17-23 really necessary for testing?
426936751,8680,abbccdda,2020-05-18T22:58:34Z,Could we add a reference to the class?
426937532,8680,abbccdda,2020-05-18T23:00:50Z,It seems that we don't have the handling logic for this FeatureCacheUpdateException. Do we think this is fatal?
426937944,8680,abbccdda,2020-05-18T23:02:01Z,Is this function being used?
426940597,8680,abbccdda,2020-05-18T23:10:12Z,Do you expect these helper functions actually to be used in production logic with subsequent PRs? 
426940830,8680,abbccdda,2020-05-18T23:10:56Z,"I don't think we need a nested if-else:
```
 val version = {
      if (apiVersion >= KAFKA_2_6_IV1) 
        5
      else if (apiVersion >= KAFKA_0_10_0_IV1)
        4
      else
        2
    }
```"
426942377,8680,abbccdda,2020-05-18T23:16:03Z,Could we make feature extraction as a helper function?
426942842,8680,abbccdda,2020-05-18T23:17:26Z,Could we make this parameter configurable?
426976264,8680,abbccdda,2020-05-19T01:19:38Z,What would happen if we are dealing with a V4 json map containing features?
426976396,8680,abbccdda,2020-05-19T01:20:02Z,nit: This test could move closer to testFromJsonV4WithNoRack
426976932,8680,abbccdda,2020-05-19T01:22:05Z,Should we test `isDefined` before calling `get`?
426977978,8680,abbccdda,2020-05-19T01:26:17Z,s/existingStr/oldFeatureAndEpoch
426978090,8680,abbccdda,2020-05-19T01:26:43Z,This val seems redundant.
426978267,8680,abbccdda,2020-05-19T01:27:25Z,nit: this errorMsg val seems redundant.
426979984,8680,abbccdda,2020-05-19T01:34:31Z,"This is only used on L53, maybe we could just use supportedFeatures instead"
426980855,8680,abbccdda,2020-05-19T01:37:40Z,"This comment is a bit vague to me, what are you referring by `incompatibilities`?"
426990007,8680,abbccdda,2020-05-19T02:12:55Z,nit: maybe rename to `incompatibleWith` and flip the boolean
426990716,8680,abbccdda,2020-05-19T02:15:41Z,"Might worth getting a ticket to define the handling strategy for such exception, and in general how `updateOrThrow` will be used. "
426997108,8680,abbccdda,2020-05-19T02:39:40Z,Does this event actually happen? Will we hit illegal state exception in `updateLatestOrThrow`?
427022316,8680,kowshik,2020-05-19T04:25:04Z,Done.
427023280,8680,kowshik,2020-05-19T04:29:23Z,"Do you feel strongly about this?
The reasons why I ask the question is:
1. Caller is unlikely to pass `null`.
2.  I looked over a number of other existing classes in Kafka, and there aren't any null checks for most constructor parameters.

It will help me if you could share couple examples from existing code where the `null` check convention is followed in Kafka."
427023377,8680,kowshik,2020-05-19T04:29:55Z,Done. Good point!
427023394,8680,kowshik,2020-05-19T04:30:00Z,Done.
427023705,8680,kowshik,2020-05-19T04:31:11Z,"Done. Yes, I've changed it to default visibility now."
427023770,8680,kowshik,2020-05-19T04:31:31Z,Done. Removed it.
427024341,8680,kowshik,2020-05-19T04:34:02Z,Done. Good point!
427024379,8680,kowshik,2020-05-19T04:34:12Z,Done. Good point!
427025378,8680,kowshik,2020-05-19T04:38:08Z,"The underlying data structure is a `Map`. It would be simpler if this method just returns `null` if the feature doesn't exist. For example, that is how java's `Map.get` works, here is the javadoc: https://docs.oracle.com/javase/8/docs/api/java/util/Map.html#get-java.lang.Object-.

Also, I've documented this method now (doc was previously absent)."
427025539,8680,kowshik,2020-05-19T04:38:51Z,Done.
427025597,8680,kowshik,2020-05-19T04:39:05Z,Done.
427025866,8680,kowshik,2020-05-19T04:40:04Z,Done. Good point! Added test as well.
427027670,8680,kowshik,2020-05-19T04:47:29Z,"It provides slightly better convenience: `Object.equals` will also take care of the `null` checks for you.
Also it turned out it was overkill to use `Objects.equals` for primitive type checks for `minValue` and `maxValue`. So I've simplified the code to use `==` those attributes.

Good point!"
427027909,8680,kowshik,2020-05-19T04:48:28Z,Done.
427033616,8680,kowshik,2020-05-19T05:10:37Z,Done. Also added a test. Good catch!
427034499,8680,kowshik,2020-05-19T05:13:51Z,Done. Good point!
427034758,8680,kowshik,2020-05-19T05:14:57Z,"Are you sure? All newly added fields are tagged (i.e. optional).
Going by [this documentation](https://cwiki.apache.org/confluence/display/KAFKA/KIP-482%3A+The+Kafka+Protocol+should+Support+Optional+Tagged+Fields#KIP-482:TheKafkaProtocolshouldSupportOptionalTaggedFields-TaggedFields) in KIP-482, it is not required to change the schema version whenever tagged fields are introduced."
427035290,8680,kowshik,2020-05-19T05:16:58Z,Done.
427035380,8680,kowshik,2020-05-19T05:17:19Z,Done.
427042310,8680,kowshik,2020-05-19T05:42:26Z,Done.
427042434,8680,kowshik,2020-05-19T05:42:49Z,Done.
427043245,8680,kowshik,2020-05-19T05:45:33Z,"Done. Some of it is not required. Good point, I have removed the unnecessary testing now.
We still need to check if exception is thrown in these 4 basic tests: min < 1, max < 1, min & max < 1 and max > min."
427043710,8680,kowshik,2020-05-19T05:47:05Z,Done.
427044348,8680,kowshik,2020-05-19T05:49:07Z,"No, this constructor overload was simply created to avoid a churn of test code at number of places adding the additional `SupportedFeatures` parameter. How do you feel about keeping it?"
427045136,8680,kowshik,2020-05-19T05:51:37Z,Done. Good point!
427045338,8680,kowshik,2020-05-19T05:52:15Z,Done. It was unused and I have eliminated it now.
427045754,8680,kowshik,2020-05-19T05:53:30Z,Done.
427045906,8680,kowshik,2020-05-19T05:54:00Z,Done.
427046004,8680,kowshik,2020-05-19T05:54:19Z,Done.
427046331,8680,kowshik,2020-05-19T05:55:17Z,It is used intentionally to split the log message into 2 lines (for ~100-char readability limit per line). Otherwise the string will be huge and all in the same line.
427046353,8680,kowshik,2020-05-19T05:55:21Z,It is used intentionally to split the log message into 2 lines (for ~100-char readability limit per line). Otherwise the string will be huge and all in the same line.
427047251,8680,kowshik,2020-05-19T05:58:04Z,"I have added comments now to the code.

The idea I had was that this event may happen, rarely (ex: operational error).
In such a case, we do not want to kill the brokers, so we just log a warning and treat the case as if the node is absent, and populate the cache with empty features.

So, this case is actually handled inside `FeatureCacheUpdater.updateLatestOrThrow()`.
The call to read ZK node will return `ZkVersion.UnknownVersion` whenever the node does not exist in ZK, and I've explicitly handled this returned version."
427047400,8680,kowshik,2020-05-19T05:58:32Z,Done.
427048695,8680,kowshik,2020-05-19T06:02:19Z,Good point. I have improved the doc now. Let me know how you feel about it.
427049093,8680,kowshik,2020-05-19T06:03:24Z,Done.
427049831,8680,kowshik,2020-05-19T06:05:39Z,Done.
427050030,8680,kowshik,2020-05-19T06:06:13Z,"Yes, this will get used in the future. For example the write path will use it."
427050597,8680,kowshik,2020-05-19T06:07:55Z,Done. Good point!
427057916,8680,kowshik,2020-05-19T06:27:29Z,Done.
427060765,8680,kowshik,2020-05-19T06:34:31Z,"The tests have been already added. Pls check out the tests added in `ApiVersionsResponseTest.java`, particularly: `shouldReturnFeatureKeysWhenMagicIsCurrentValueAndThrottleMsIsDefaultThrottle`.

Let me know if this test does not look sufficient."
427063053,8680,kowshik,2020-05-19T06:39:32Z,Done.
427063724,8680,kowshik,2020-05-19T06:40:58Z,Done.
427064271,8680,kowshik,2020-05-19T06:42:10Z,"In my understanding, this is an impossible case. Because, we always write features into the JSON only in v5 or above. That is why, there is no test for it. Let me know how you feel about it."
427064923,8680,kowshik,2020-05-19T06:43:33Z,Done.
427827018,8680,kowshik,2020-05-20T08:21:37Z,"As we discussed offline today, this exception is already handled in `ChangeNotificationProcessorThread.doWork()` method defined in `FinalizedFeatureChangeListener.scala`. Basically, the ZK change notification processor thread exits the Broker with a fatal error (non-zero exit code) when this exception (or any exception) is caught while trying to update `FinalizedFeatureCache`."
427884165,8680,kowshik,2020-05-20T09:51:49Z,Done.
427885025,8680,kowshik,2020-05-20T09:53:14Z,"Done. Good point!
 - I have now created 3 classes as you proposed. `BaseVersionRange` is the base class, and, `SupportedVersionRange` & `FinalizedVersionRange` are it's child classes.
 - The key labels couldn't be made into abstract functions since these constants are needed within `deserialize()` which is a static method defined in the child classes."
428080079,8680,abbccdda,2020-05-20T14:55:57Z,"Yea, the reasoning is that we have `get` call blindly look up inside `features` which in this case null is not valid. And I don't feel passing `null` makes sense for the caller, correct?"
428367400,8680,abbccdda,2020-05-20T23:39:32Z,`deserialize()`? I think the second sentence is redundant.
428370254,8680,abbccdda,2020-05-20T23:48:46Z,Do we want to get a unit test class for `BaseVersionRange`?
428370905,8680,abbccdda,2020-05-20T23:51:04Z,Should be `SupportedVersionRange` 
428371260,8680,abbccdda,2020-05-20T23:52:20Z,"Just for the sake of argument, I feel we could remove this method and just test:
```
min() < supportedVersionRange.min() || max() > supportedVersionRange.max()
```
for incompatibility."
428392754,8680,abbccdda,2020-05-21T01:13:18Z,nit: supportedVersionRange
428392936,8680,abbccdda,2020-05-21T01:14:10Z,"Why this is a `NOTE`? Could we just comment like:
```
An extended BaseVersionRange representing the min/max versions for supported features.
```"
428402864,8680,abbccdda,2020-05-21T01:54:32Z,"Maybe I'm a bit too obsessive about code duplication, but after I made an attempt I thought we could actually have the internal deserialization logic shared between `deserializeFinalizedFeatures` and `deserializeSupportedFeatures` by making a template
```
 public static Features<FinalizedVersionRange> deserializeFinalizedFeatures(Map<String, Map<String, Long>> serialized) {
        return deserializeFeatures(serialized, FinalizedVersionRange::deserialize);
    }

    public static Features<SupportedVersionRange> deserializeSupportedFeatures(
        Map<String, Map<String, Long>> serialized) {
        return deserializeFeatures(serialized, SupportedVersionRange::deserialize);
    }
        
    
    private interface Deserializer<V> {
        V deserialize(Map<String, Long> serialized);
    }


    private static <V extends BaseVersionRange> Features<V> deserializeFeatures(Map<String, Map<String, Long>> serialized, Deserializer<V> deserializer) {
        return new Features<>(serialized.entrySet().stream().collect(
            Collectors.toMap(
                Map.Entry::getKey,
                entry -> deserializer.deserialize(entry.getValue()))));
    }
```"
428403904,8680,abbccdda,2020-05-21T01:58:35Z,Missing header
428404068,8680,abbccdda,2020-05-21T01:59:14Z,Seems we didn't trigger style check on this new class.
428404491,8680,abbccdda,2020-05-21T02:00:52Z,What's the difference between this test class and its super class test case? Same question for `SupportedVersionRangeTest`
428418517,8680,abbccdda,2020-05-21T02:58:16Z,"Could we move this logic as part of inner else? Like:
```
else {
      val incompatibleFeatures = SupportedFeatures.incompatibleFeatures(latest.features)
      if (incompatibleFeatures.nonEmpty) {
        val errorMsg = (""FinalizedFeatureCache updated failed since feature compatibility"" +
          "" checks failed! Supported %s has incompatibilities with the latest finalized %s."" +
          "" The incompatible features are: %s."").format(
          SupportedFeatures.get, latest, incompatibleFeatures)
        throw new FeatureCacheUpdateException(errorMsg)
      } else {
        val logMsg = ""Updated cache from existing finalized %s to latest finalized %s"".format(
          oldFeatureAndEpoch, latest)
        featuresAndEpoch = Some(latest)
        info(logMsg)
      }
    }
```
It makes the if-else logic more tight."
428419538,8680,abbccdda,2020-05-21T03:02:18Z,"I think we don't need to talk about future work inside the comment, just making it clear that the read path for serving ApiVersionsRequest is the only reader as of now."
428420141,8680,abbccdda,2020-05-21T03:04:59Z,nit: provide
428421713,8680,abbccdda,2020-05-21T03:11:58Z,Do we need the comment to be on info level?
428422280,8680,abbccdda,2020-05-21T03:14:25Z,nit: don't feel strong about having this parameter
428422483,8680,abbccdda,2020-05-21T03:15:18Z,feel neutral about this helper function
428422966,8680,abbccdda,2020-05-21T03:17:24Z,"I don't think this is scala accepted comment style to add `-`, do you see a warning?"
428424395,8680,abbccdda,2020-05-21T03:23:53Z,`Feature cache update gets interrupted`
428426189,8680,abbccdda,2020-05-21T03:31:57Z,"Does the version field existence guarantee there is a valid feature data node or not? In fact, `getDataAndVersion` returns an optional data. I checked the getDataAndVersion caller `ProducerIdManager`, there is a handling for empty data which I feel we should have as well. 
Additionally, I think since we haven't implemented the write path yet, could we get a ticket to write down a short description on how the write path shall look like, by defining the different cases like:
```
empty dataBytes, valid version 
valid dataBytes, valid version 
empty dataBytes, unknown version 
valid dataBytes, unknown version 
```
if that makes sense, so that we could keep track of the design decisions we made in the read path PR when implementing the write path."
428426448,8680,abbccdda,2020-05-21T03:33:03Z,"Could we summary the possible thrown error code in the comment as well? For example, does a JSON deserialization error should be treated as fatal?"
428429013,8680,abbccdda,2020-05-21T03:44:51Z,"Is it possible to have no enqueued updater, and cause this function block the thread indefinitely? "
428429615,8680,abbccdda,2020-05-21T03:47:38Z,"For an educational question, does the zkClient have a separate thread to do the node change monitoring?"
428430093,8680,abbccdda,2020-05-21T03:49:45Z,Does the order matter here? I was wondering if there is any concurrent issue if we unregister before the queue and thread get cleaned up.
428430356,8680,abbccdda,2020-05-21T03:50:57Z,We could just comment `For testing only`
428430553,8680,abbccdda,2020-05-21T03:51:55Z,`wait time for the first feature cache update upon initialization`
428430779,8680,abbccdda,2020-05-21T03:52:57Z,"I think the comment is not necessary, since we have already commented on `KAFKA_2_6_IV1`"
428431584,8680,abbccdda,2020-05-21T03:56:16Z,nit: Returns a reference to the latest features supported by the broker.
428432424,8680,abbccdda,2020-05-21T04:00:05Z,This logging is duplicate
428432727,8680,abbccdda,2020-05-21T04:01:36Z,"I'm slightly inclined to return a set of features instead of just strings, and make the string conversion as a helper. But I leave this up to you to decide, and we could always adapt the function to make it more useful in other scenarios as needed."
428432904,8680,abbccdda,2020-05-21T04:02:25Z,"If that's the case, I feel we could remove the testing only comment."
428433279,8680,abbccdda,2020-05-21T04:04:10Z,"aha, the order is wrong for `KAFKA_0_10_0_IV1` and `KAFKA_2_6_IV1`"
428433517,8680,abbccdda,2020-05-21T04:05:31Z,"I think even if this is an operational error, the cluster is at risk of violating the feature semantics previously enabled, which is different from an unknown feature version from the beginning. I feel we should just exit in fatal error for this case, but would open for discussion."
428434151,8680,abbccdda,2020-05-21T04:08:21Z,s/asJavaMap/featuresAsJavaMap
428434945,8680,abbccdda,2020-05-21T04:11:27Z,Could we log statusInt here as well? Also I feel the exception should be thrown from  `FeatureZNodeStatus.withNameOpt`
428435694,8680,abbccdda,2020-05-21T04:15:15Z,Is there a more dedicated exception code for deserialization error? I feel the KafkaException is a bit too general compared with IllegalArgument
428436046,8680,abbccdda,2020-05-21T04:16:59Z,Could we name it V0 for simplicity?
428436573,8680,abbccdda,2020-05-21T04:19:58Z,"I feel we might worth creating a separate thread discussing whether we could get some benefit of the automated protocol generation framework here, as I think this could be easily represented as JSON if we define it in the common package like other RPC data. The difficulty right now is mostly on the serialization and deserialization for feature itself, but these could have workarounds if we want to do so."
428437318,8680,abbccdda,2020-05-21T04:23:22Z,"I'm a bit surprised, do we want to support feature znode deletion in long term?"
428437571,8680,abbccdda,2020-05-21T04:24:39Z,Could we extract some common initialization logic for the tests to reduce duplication?
428437659,8680,abbccdda,2020-05-21T04:24:58Z,nit: space
428438063,8680,abbccdda,2020-05-21T04:27:00Z,"If we are not validating the features by extracting them, I think we do not need to pass in a non-empty feature list?"
428573407,8680,kowshik,2020-05-21T10:32:12Z,"It is thoroughly tested in it's child class test suite: `SupportedVersionRangeTest`.
Personally I feel it is good enough this way, because, anyway to test this class we need to inherit into a sub-class (since constructor is `protected`). And by testing via `SupportedVersionRangeTest`, we achieve exactly the same.

I have now added top-level documentation in the test suite of `SupportedVersionRangeTest`, explaining the above."
428576410,8680,kowshik,2020-05-21T10:39:55Z,"Done. I'm raising an exception now if it is `null`.
I see your point.
Will be good to learn what is the convention in Kafka for constructor param null checks."
428577156,8680,kowshik,2020-05-21T10:41:42Z,Done.
428578424,8680,kowshik,2020-05-21T10:44:59Z,Done.
428578472,8680,kowshik,2020-05-21T10:45:04Z,Done. Good point!
428578829,8680,kowshik,2020-05-21T10:45:53Z,Done.
428579070,8680,kowshik,2020-05-21T10:46:29Z,Done. Good point!
428579884,8680,kowshik,2020-05-21T10:48:19Z,Done.
428580337,8680,kowshik,2020-05-21T10:49:28Z,Done.
428581013,8680,kowshik,2020-05-21T10:51:04Z,Done. Good point!
428581253,8680,kowshik,2020-05-21T10:51:45Z,Done.
429069672,8680,kowshik,2020-05-22T06:41:44Z,Done.
429071500,8680,kowshik,2020-05-22T06:47:02Z,"Done. I have simplified this test suite eliminating the redundant tests, and only keeping the ones specific to `FinalizedVersionRange`. Also I have added documentation to both test suites explaining their purpose."
429074215,8680,kowshik,2020-05-22T06:55:11Z,Done. Changed to `IllegalArgumentException`. Good point!
429076303,8680,kowshik,2020-05-22T07:01:08Z,"Done.
For the other point, I don't feel strongly for it. I feel it is OK to have an API that doesn't throw and just lets the caller decide (based on the context) if an empty returned value is incorrect."
429076742,8680,kowshik,2020-05-22T07:02:23Z,No. But we want to test the behavior about what happens during a deletion (ex: operational error).
429077064,8680,kowshik,2020-05-22T07:03:13Z,Done.
429077469,8680,kowshik,2020-05-22T07:04:23Z,Done.
429078661,8680,kowshik,2020-05-22T07:07:36Z,"As far as I can see, no ZK node class defined in this file is defined in such a way. Every class in this file encodes/decodes JSON by itself, and manages its own attributes.
Should we break the norm?"
429086184,8680,kowshik,2020-05-22T07:27:10Z,Done.
429087972,8680,kowshik,2020-05-22T07:31:25Z,"See L848 below where it is validated. The call to `zkClient. getAllBrokersInCluster` decodes each `BrokerIdZNode` content from JSON to `BrokerInfo` object. Then, we check whether the call returns exactly the same `BrokerInfo` objects defined here, and, along the way features are checked too."
429089245,8680,kowshik,2020-05-22T07:34:40Z,Done.
429089574,8680,kowshik,2020-05-22T07:35:30Z,Done. Good catch!
429089817,8680,kowshik,2020-05-22T07:36:05Z,Done.
429090026,8680,kowshik,2020-05-22T07:36:42Z,Done.
429093727,8680,kowshik,2020-05-22T07:45:18Z,Done. Good point!
429096612,8680,kowshik,2020-05-22T07:51:42Z,Done. Removed extra logging in the caller of this method (see `FinalizedFeatureCache`).
429101010,8680,kowshik,2020-05-22T08:01:36Z,"Done. Yes, I feel JSON deserialization should be treated as fatal. It should never happen, and, can indicate corruption."
429107026,8680,kowshik,2020-05-22T08:14:57Z,Done. Removed.
429107347,8680,kowshik,2020-05-22T08:15:43Z,"Done. But it's actually ""Change notification queue interrupted""."
429109170,8680,kowshik,2020-05-22T08:19:43Z,"The function blocks indefinitely - yes. But this shouldn't cause a problem or lead to deadlock/limbo situation.
Even if this thread is waiting for an item to become available in the queue, the waiting thread can always get interrupted by the `FinalizedFeatureChangeListener.close()` call which calls `ShutdownableThread.shutdown()`.

Note that the `ShutdownableThread.shutdown()` method interrupts the thread, which should unblock any waiting `queue.take()` operation and makes it raise an `InterruptedException`: 

https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/ShutdownableThread.scala#L32-L59
https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/LinkedBlockingQueue.html#take()"
429110087,8680,kowshik,2020-05-22T08:21:36Z,Done. Removed.
429111103,8680,kowshik,2020-05-22T08:23:51Z,"I didn't understand the question. Are you saying the logging severity should be lower or higher?
This is a rare case anyway as the feature node doesn't get created often, so, `info` logging seems fine to me."
429113491,8680,kowshik,2020-05-22T08:28:40Z,"You bring up a good point.

My main concern is availability. If we exit the Broker here, then, whenever the feature ZK node gets deleted (accidentally), it could crash all brokers in the fleet all at once leading to an availability problem.

With regards to violating feature semantics, good point.

I'm in 2 minds here, and perhaps we can also hear @hachikuji 's thoughts on this topic."
429117278,8680,kowshik,2020-05-22T08:36:30Z,"Yes. Here is the documentation explaining the same: https://zookeeper.apache.org/doc/r3.5.7/zookeeperProgrammers.html#Java+Binding.

> When a ZooKeeper object is created, two threads are created as well: an IO thread and an event thread. All IO happens on the IO thread (using Java NIO). All event callbacks happen on the event thread. Session maintenance such as reconnecting to ZooKeeper servers and maintaining heartbeat is done on the IO thread. Responses for synchronous methods are also processed in the IO thread. All responses to asynchronous methods and watch events are processed on the event thread."
429117918,8680,kowshik,2020-05-22T08:37:53Z,Done. Removed.
429120585,8680,kowshik,2020-05-22T08:43:40Z,"The order probably doesn't matter in this case. But logically I decided to follow the below order since I could reason about it better:
1. Stop the inflow of new events
2. Clear pending events
3. Stop the processing of all events"
429120739,8680,kowshik,2020-05-22T08:44:04Z,Done.
429123988,8680,kowshik,2020-05-22T08:50:49Z,"I have added documentation here in this method describing all the cases.
The empty data case should never happen and can indicate a corruption. The reason is that we always return non-empty data in `FeatureZNode.encode`, so the ZK node content should never empty.

Yes, I can add some more info to KAFKA-10028 or in the write path PR summary."
429126751,8680,kowshik,2020-05-22T08:56:54Z,Done.
429126867,8680,kowshik,2020-05-22T08:57:07Z,Done.
429128812,8680,kowshik,2020-05-22T09:01:12Z,Done.
429474760,8680,abbccdda,2020-05-22T22:08:39Z,We could have multiple @throws here
429476928,8680,abbccdda,2020-05-22T22:19:31Z,"I didn't look thoroughly enough, but the only IllegalArgumentException I found is 
```
 case invalidVersion =>
        throw new IllegalArgumentException(s""Expected controller epoch zkVersion $invalidVersion should be non-negative or equal to ${ZkVersion.MatchAnyVersion}"")
```
which should never happen as we always use `MatchAnyVersion` in `retryRequestsUntilConnected`. Are we trying to catch some other exceptions here?"
429480603,8680,abbccdda,2020-05-22T22:38:14Z,"My feeling is that this could be on debug level, but no strong perference."
429494635,8680,abbccdda,2020-05-23T00:13:07Z,nit: minKeyLabel
429495765,8680,abbccdda,2020-05-23T00:22:36Z,"I see, makes sense."
429496037,8680,abbccdda,2020-05-23T00:24:54Z,nit: we could test `emptySupportedFeatures.features().isEmpty()`
429497749,8680,abbccdda,2020-05-23T00:40:54Z,nit: could you elaborate why this helper function and `FinalizedFeaturesAndEpoch` struct is useful in this context? Just for easier message printing?
429498395,8680,abbccdda,2020-05-23T00:48:10Z,So here we will directly throw NoSuchElementException if `mayBeFeatureZNodeBytes` is empty? Do we want to check this case and throw a customized exception instead?
429498408,8680,abbccdda,2020-05-23T00:48:17Z,nit: space
429499008,8680,abbccdda,2020-05-23T00:54:28Z,"This leads to a more general question: is there a way to cleanup all the ZK feature path? Reading from the KIP, I don't see we have any admin API to do so, which makes me wonder how could this case happen in reality. In terms of severity, I think crushing the entire cluster seems to be an overkill as well, maybe we should have some blocking mechanism in place for any feature extraction call here, until we see `handleCreation` gets triggered again?"
429509179,8680,abbccdda,2020-05-23T03:18:11Z,"nit: one liner: `this.features = Objects.requireNonNull(features, ""Provided features can not be null."");`"
429509941,8680,abbccdda,2020-05-23T03:30:12Z,nit: remove `only`
429510099,8680,abbccdda,2020-05-23T03:32:02Z,"This comment should be frequent and the `featureZkNodePath` is staying constant, could we just make it for debugging level?"
429510224,8680,abbccdda,2020-05-23T03:34:25Z,"I don't think this note is necessary, maybe just merge with the first line as:
```
Waits until exactly one updateLatestOrThrow completes successfully.
```"
429510520,8680,abbccdda,2020-05-23T03:39:24Z,"Do you think we should add this config as part of the KIP since it is public? I think it would just be a minor update, but let's wait and see others thoughts on this."
429510746,8680,abbccdda,2020-05-23T03:43:32Z,remove semi-colon
429511091,8680,abbccdda,2020-05-23T03:50:12Z,s/it's/its
429511146,8680,abbccdda,2020-05-23T03:51:19Z,`a Kafka cluster exists already and the IBP config is less than KAFKA_2_6_IV1` to `an existing Kafka cluster with  IBP config less than KAFKA_2_6_IV1`
429511236,8680,abbccdda,2020-05-23T03:52:57Z,"I think the norm exists because we don't have automated framework by then, and doing hand-written json serialization and deserialization is a bit wasting. cc @hachikuji @cmccabe as this is a major direction discussion."
429589291,8680,kowshik,2020-05-24T00:51:52Z,Done.
429589607,8680,kowshik,2020-05-24T01:00:30Z,Done. Also added doc.
429590012,8680,kowshik,2020-05-24T01:12:17Z,Done.
429590179,8680,kowshik,2020-05-24T01:16:53Z,"Actually I've eliminated the helper method now, and, there is only 1 method: `updateOrThrow(...)`."
429590200,8680,kowshik,2020-05-24T01:17:20Z,Done.
429590218,8680,kowshik,2020-05-24T01:17:45Z,Done.
429590248,8680,kowshik,2020-05-24T01:18:15Z,Done.
429590426,8680,kowshik,2020-05-24T01:23:07Z,"Fixed now.
Good point. Actually the code was incorrect. I meant to wrap `FeatureZNode.decode` call with the `try-catch`, since, it throws `IllegalArgumentException`. I have fixed the code now to do the same."
429590504,8680,kowshik,2020-05-24T01:25:20Z,Done.
429590532,8680,kowshik,2020-05-24T01:25:46Z,Done.
429590593,8680,kowshik,2020-05-24T01:27:36Z,"Sounds good. Yeah, it is minor and feels like an implementation detail to me. But we can wait to see what others say."
429590614,8680,kowshik,2020-05-24T01:27:57Z,Done.
429590626,8680,kowshik,2020-05-24T01:28:30Z,Done.
429590656,8680,kowshik,2020-05-24T01:29:04Z,Done.
429591914,8680,kowshik,2020-05-24T01:59:16Z,Done.
429593151,8680,kowshik,2020-05-24T02:26:04Z,"The deletion of ZNode is a rare case, it should never happen in reality unless it is ZK corruption, or rarely an operational error that deletes some ZK nodes. It's not easy to prevent damage in such a case. From a correctness standpoint, imagine what would happen if the feature ZNode gets deleted, and, afterwards a broker restarts. It will start with empty cache, so the damage is done. Therefore, it seems that even if we add a special logic here, we can not prevent damage if the source of truth is lost.

Two things to note here:
1. The client should anyway ignore older stale epoch responses, if it had seen newer epochs that are greater. In that spirit, the client can be also made to treat the absence of finalized features in an `ApiVersionsResponse` just like a stale epoch case, if, it had seen at least one valid `ApiVersionsResponse` earlier (i.e. at least one response with some valid epoch). 

2. Deletion of individual finalized feature is actually supported in [KIP-584](https://cwiki.apache.org/confluence/display/KAFKA/KIP-584%3A+Versioning+scheme+for+features), but not deletion of the entire ZNode. Search for the word 'deletion' in the KIP write-up. If needed, this deletion functionality could be extended to provide the ability to delete all features too."
430737606,8680,abbccdda,2020-05-26T22:15:42Z,"Thanks, I don't think we need to be super paranoid with this rare scenario, but we should also be indicating this error state to the client suggesting that some manual fix is necessary. My proposed idea above is to add such an error state to the feature cache to refuse any further updates until we have: 1. a node creation event 2. restart of the broker (once the issue gets fixed), so this blocking behavior shall be ephemeral and recoverable from broker perspective. We don't have to implement this logic in the current PR, as we don't have a write path yet, just get a JIRA to track it sounds fine.

Make sense to cc @cmccabe and @hachikuji as well."
432781217,8680,junrao,2020-05-29T23:29:07Z,"This won't make 2.6.0 release. So, perhaps we should use KAFKA_2_7 or whatever the next release is?"
432782068,8680,junrao,2020-05-29T23:33:18Z,"I missed this in the KIP, but it seems that long is overkilling for version. The version in request is short and the version in ZK data is int. So, perhaps this should just be short?"
432782926,8680,junrao,2020-05-29T23:37:35Z,"minValue > 1, maxValue > 1 => minValue >= 1, maxValue >= 1"
432783145,8680,junrao,2020-05-29T23:38:43Z,Should we include the label too?
432783472,8680,junrao,2020-05-29T23:40:25Z,serialize typically means generating binary data. Perhaps this is better called toMap()?
433327328,8680,junrao,2020-06-01T15:53:29Z,missing license header
433337829,8680,junrao,2020-06-01T16:10:17Z,empty => isEmpty ?
433347401,8680,junrao,2020-06-01T16:27:29Z,"Could we use map {case(feature, versionLevel, _) => ...} to avoid unnamed references like _1? "
433359805,8680,junrao,2020-06-01T16:50:36Z,The KIP doesn't seems to include this field. Could we add it to the KIP wiki?
433366871,8680,junrao,2020-06-01T17:03:47Z,"InterruptedException can be thrown if the thread is shut down explicitly. In this case, we probably don't want to throw RuntimeException to the caller."
433373309,8680,junrao,2020-06-01T17:16:17Z,"Hmm, could we just use config.zkConnectionTimeoutMs for this, instead of introducing a new config?"
433375459,8680,junrao,2020-06-01T17:20:23Z,"Hmm, is waitOnceForCacheUpdateMs <=0 supported? In that case, it seems that we still need to read the /features path in ZK?"
433382257,8680,junrao,2020-06-01T17:32:58Z,"If this thread is being closed, the InterruptedException is expected and we don't need to log this."
433384475,8680,junrao,2020-06-01T17:37:12Z,"Hmm, this just kills the thread, but not the broker as the comment says. Also, not sure about killing the broker. We probably should just log an error and continue since this is not necessarily fatal."
433395727,8680,junrao,2020-06-01T17:58:03Z,"Hmm, the epoch returned from ZK is int32. Does FinalizedFeaturesEpoch need to be int64?"
433399575,8680,junrao,2020-06-01T18:05:16Z,"Hmm, why is FinalizedFeaturesEpoch an optional but latestSupportedFeatures is not?"
433400012,8680,junrao,2020-06-01T18:06:06Z,Should we add public methods for accessing those fields?
433402784,8680,junrao,2020-06-01T18:11:26Z,It doesn't seem we store security protocol map in the broker registration.
433403260,8680,junrao,2020-06-01T18:12:25Z,Should we include the new field in toString()?
433409944,8680,junrao,2020-06-01T18:25:22Z,The existing comments seem incorrect since we don't store listener_security_protocol_map in ZK.
434343065,8680,kowshik,2020-06-03T06:47:41Z,Done. Made it KAFKA_2_7_IV0.
434344257,8680,kowshik,2020-06-03T06:50:32Z,Done. I have made it `int16` now. Great point.
434345400,8680,kowshik,2020-06-03T06:53:05Z,Done.
434346281,8680,kowshik,2020-06-03T06:55:08Z,Done.
434417020,8680,kowshik,2020-06-03T09:01:38Z,Done.
434428173,8680,kowshik,2020-06-03T09:20:02Z,"It's because non-existing supported features _can_ be represented by an empty map (i.e. broker does not advertise any features). But on the other hand, non-existing finalized features can not be represented by empty map alone, as we need a suitable epoch value that indicates the absence of finalized features. To address this case, I saw 2 ways:
1) Provide a negative epoch value indicating absence of finalized features, OR
2) Represent using an empty `Optional` for both finalized features and epoch.

I chose the latter approach. Please, let me know if you have concerns."
434431330,8680,kowshik,2020-06-03T09:25:27Z,"I had added such APIs previously. But @abbccdda wanted these removed, as they are not currently unused. Please refer to this comment: https://github.com/apache/kafka/pull/8680#discussion_r426931875.
Please, let me know, and I can add them back if you prefer."
434431880,8680,kowshik,2020-06-03T09:26:20Z,Done. Changed to `int32` now. Great point!
434434381,8680,kowshik,2020-06-03T09:30:30Z,Done.
434434804,8680,kowshik,2020-06-03T09:31:10Z,Done. Nice catch!
434435193,8680,kowshik,2020-06-03T09:31:48Z,Done.
434435824,8680,kowshik,2020-06-03T09:32:53Z,Done.
434438288,8680,kowshik,2020-06-03T09:36:50Z,Done. Removed the catch clause and exception wrapping.
434441200,8680,kowshik,2020-06-03T09:41:16Z,Done.
434443007,8680,kowshik,2020-06-03T09:44:10Z,"It kills the broker because `ShutdownableThread` catches `FatalExitError` and triggers exit sequence: https://github.com/apache/kafka/blob/b8d609c207ed3d1e678c2f1eb6f3cae637f92c30/core/src/main/scala/kafka/utils/ShutdownableThread.scala#L98-L102

I have updated the comment to use the word ""eventually"".
Regarding logging fatal and continuing -- the exception caught here almost always indicates a feature incompatibility, and, that means the broker can cause damage if it sticks around. That is why I felt it is better to kill the broker in such a rare incompatibility case.

Please, let me know your thoughts."
434444716,8680,kowshik,2020-06-03T09:46:55Z,Done. I have changed the code disallowing values <= 0.
434444848,8680,kowshik,2020-06-03T09:47:11Z,Done. Great point!
434445428,8680,kowshik,2020-06-03T09:48:14Z,Done.
434448455,8680,kowshik,2020-06-03T09:53:30Z,Done. Removed. Great catch!
434450195,8680,kowshik,2020-06-03T09:56:30Z,Sure. I will be happy to follow up on this. Trying to understand the process -- should I update the KIP and send an email as FYI to `dev@kafka.apache.org` ?
435543865,8680,junrao,2020-06-04T20:56:15Z,"To handle ZK session expiration, we need to register a StateChangeHandler. That way, we can read the /features path from ZK when the new session is established since the feature could have changed btw the old and the new ZK sessions. See object ZkStateChangeHandler as an example."
435558683,8680,junrao,2020-06-04T21:20:35Z,Could we pass in `Optional<FinalizedFeaturesAndEpoch>` instead of two separate Optional?
435560941,8680,junrao,2020-06-04T21:23:29Z,"If finalizedFeaturesEpoch is not present, we probably want to set the field to sth like -1 instead of leaving it as the default value of 0."
435562341,8680,junrao,2020-06-04T21:26:15Z,The comment can be a bit misleading since features is not Optional.
435567942,8680,junrao,2020-06-04T21:39:10Z,It's probably better to close this before zkClient since the close call unregister from zkClient. 
435595571,8680,junrao,2020-06-04T22:53:02Z,The name of the method probably should include failure?
435595915,8680,junrao,2020-06-04T22:54:00Z,missing license header
435596401,8680,junrao,2020-06-04T22:55:36Z,"Could we just assertEquals(featureZNode, decoded)?"
435597745,8680,junrao,2020-06-04T22:59:39Z,`version > CurrentVersion` means that we can't downgrade the broker. We will need to relax this check.
435598956,8680,junrao,2020-06-04T23:03:22Z,missing license header
435601124,8680,junrao,2020-06-04T23:10:31Z,missing license header
435604964,8680,junrao,2020-06-04T23:23:22Z,"Hmm, if the feature is disabled, it seems that updatedFinalizedFeatures shouldn't be reflected in the cache, right?"
435606223,8680,junrao,2020-06-04T23:27:47Z,Do we want to throw an Exception here?
435607391,8680,junrao,2020-06-04T23:31:54Z,missing license header
436493426,8680,kowshik,2020-06-08T07:04:45Z,Done. 
436493633,8680,kowshik,2020-06-08T07:05:14Z,Done.
436494049,8680,kowshik,2020-06-08T07:06:16Z,Done.
436495007,8680,kowshik,2020-06-08T07:08:46Z,Done. Great point!
436495405,8680,kowshik,2020-06-08T07:09:48Z,Done.
436495661,8680,kowshik,2020-06-08T07:10:28Z,Done.
436496035,8680,kowshik,2020-06-08T07:11:21Z,Done.
436496254,8680,kowshik,2020-06-08T07:11:54Z,Done.
436515190,8680,kowshik,2020-06-08T07:53:52Z,"Done. I have modified the code such that `FeatureCacheUpdater.updateLatestOrThrow` will now clear the cache whenever it sees that the feature ZK node is disabled.

Great point! "
436524414,8680,kowshik,2020-06-08T08:11:36Z,"Done. Changed it to use a latch that gets notified when the exit procedure is called.
Great point!"
436527003,8680,kowshik,2020-06-08T08:16:56Z,Done.
436532143,8680,kowshik,2020-06-08T08:26:36Z,Done.
436543251,8680,kowshik,2020-06-08T08:47:05Z,Done.
436544409,8680,kowshik,2020-06-08T08:49:14Z,"Done. I'm no longer passing 2 optionals, since, we decided (below) that epoch can be set as -1 whenever it is absent."
436607402,8680,kowshik,2020-06-08T10:44:45Z,This config has been eliminated now.
437045081,8680,junrao,2020-06-08T22:54:43Z,"This is not really ""change-notification"". So, the name can just be FeatureZNode.path."
437049072,8680,junrao,2020-06-08T23:06:49Z,2.6.x => 2.7.x
437049227,8680,junrao,2020-06-08T23:07:19Z,2.6.x => 2.7.x
437123822,8680,kowshik,2020-06-09T03:56:53Z,Done.
437123965,8680,kowshik,2020-06-09T03:57:29Z,Done.
437124061,8680,kowshik,2020-06-09T03:57:53Z,Done.
437140900,8680,abbccdda,2020-06-09T05:10:27Z,nit: we could use Utils.mkMap here
437141678,8680,abbccdda,2020-06-09T05:13:44Z,"we don't need to check `other == null` here, the next condition check covers it."
437143331,8680,abbccdda,2020-06-09T05:19:56Z,"nit: should all the parameters be final here, not just minMagic?"
437145250,8680,abbccdda,2020-06-09T05:26:56Z,"I overlooked this case, let's maintain this static constructor without renaming it, since it is public."
437146576,8680,abbccdda,2020-06-09T05:31:25Z,"nit: instead of using comments, better to build this into the test name, for example:
`testInvalidSuppportedFeaturesWithMissingMaxVersion`"
437153379,8680,abbccdda,2020-06-09T05:53:27Z,nit: {} not necessary
437580565,8680,abbccdda,2020-06-09T16:57:16Z,nit: use `Introduced` to align with previous comment?
437582007,8680,abbccdda,2020-06-09T16:59:26Z,it's -> its
437582948,8680,abbccdda,2020-06-09T17:00:59Z,could be simplified as `the latest known FinalizedFeaturesAndEpoch or empty if not defined in the cache`
437584758,8680,abbccdda,2020-06-09T17:04:11Z,"I think we could remove `If the cache update is not successful, then, a suitable exception is raised...` which is pretty obvious."
437586235,8680,abbccdda,2020-06-09T17:06:41Z,"Sorry it's been a while since my last review, but have we discussed the recovery path when we hit a data corruption exception for the cluster? Is there a way to turn off the feature versioning completely to unblock, or we have a mechanism to wipe out ZK data?"
437592050,8680,abbccdda,2020-06-09T17:16:33Z,"Being a bit paranoid here, would it be possible to have out-of-order updates from ZK, such that the version number is not monotonically increasing? I'm thinking even we could throw in FinalizedFeatureCache, do we really want to kill the broker, or we should just log a warning and proceed."
437598721,8680,abbccdda,2020-06-09T17:27:52Z,"I think I'm no longer insisting on this point, as we could make this as a follow-up work. Filed JIRA here: https://issues.apache.org/jira/browse/KAFKA-10130"
437604015,8680,abbccdda,2020-06-09T17:36:59Z,nit: space
437605413,8680,abbccdda,2020-06-09T17:39:17Z,V4 doesn't have feature right? What's the purpose of this test?
437606748,8680,abbccdda,2020-06-09T17:41:34Z,add the space back
437607867,8680,abbccdda,2020-06-09T17:43:22Z,nit: we could add a minor test to verify a negative `waitOnceForCacheUpdateMs` will throw
437851289,8680,kowshik,2020-06-10T04:20:38Z,Done.
437851622,8680,kowshik,2020-06-10T04:21:57Z,Done. Good point.
437855643,8680,kowshik,2020-06-10T04:38:46Z,Done.
437855789,8680,kowshik,2020-06-10T04:39:31Z,Done.
437856190,8680,kowshik,2020-06-10T04:41:11Z,Done.
437856298,8680,kowshik,2020-06-10T04:41:37Z,Done.
437856385,8680,kowshik,2020-06-10T04:41:59Z,Done.
437856510,8680,kowshik,2020-06-10T04:42:29Z,Done.
437857795,8680,kowshik,2020-06-10T04:47:48Z,Done.
437858130,8680,kowshik,2020-06-10T04:49:06Z,It's a 2-line block.
437874395,8680,kowshik,2020-06-10T05:50:01Z,"1. Re: out-of-order updates from ZK:
I don't understand. When a watch fires from ZK, we react by issuing a ZK read operation to obtain the latest value of the ZK node (see L75). It is impossible that we get a stale read from ZK after watch fires on the client side.

2. Re: broker death:
The exception thrown here almost always indicates a feature incompatibility, and, that means the broker can cause damage if it sticks around (because feature bumps are breaking changes and you can not allow an incompatible broker to stick around in the cluster). That is why I felt it is better to kill the broker in such a rare incompatibility case. Note that after the controller has finalized features, there should be no brokers in the cluster with incompatibilites, so death here makes sense.

Note: I have also explained point #2 in this comment: https://github.com/apache/kafka/pull/8680/files#r434443007."
437875708,8680,kowshik,2020-06-10T05:54:19Z,Thanks. Good idea to leave a jira. I have linked it to KAFKA-9755.
437875864,8680,kowshik,2020-06-10T05:54:45Z,Done.
437876419,8680,kowshik,2020-06-10T05:56:21Z,It checks backwards compatibility i.e. it checks whether the deserialization code (V5-based) can correctly deserialize V4 such that features are assigned empty value by default..
437877179,8680,kowshik,2020-06-10T05:58:47Z,Done.
437877390,8680,kowshik,2020-06-10T05:59:31Z,Done.
437883374,8680,kowshik,2020-06-10T06:17:37Z,It's very rare especially when controller is the only entity writing to the ZK node. I have now modified the code to handle this case and clear the cache. Perhaps that's better than crashing the broker in such a case. Remediation will need human intervention in fixing the ZK node. We can provide tooling if required.
1333345949,14406,philipnee,2023-09-21T16:40:02Z,is the extra line intentional?
1333349057,14406,philipnee,2023-09-21T16:42:59Z,KafkaException?
1333353029,14406,philipnee,2023-09-21T16:46:45Z,it is handled in the error event handler - which means the user should get the exception upon invoking poll. I wonder if we could just log info or at a different level. or even no logging.
1333353596,14406,philipnee,2023-09-21T16:47:22Z,extra line  
1333355626,14406,philipnee,2023-09-21T16:49:19Z,ditto : as the error is log here - we might not need the extra logging
1333393941,14406,junrao,2023-09-21T17:28:22Z,"Just to understand this. If the consumer gets a FENCED_LEADER_EPOCH, a metadata update is triggered. Once the new metadata is received, the ValidatePositionsApplicationEvent will trigger the OffsetForLeaderEpoch request to validate the fetch position. Until the new metadata is received, the consumer will just continuously fetch from the old leader and receiving the same FENCED_LEADER_EPOCH error?"
1333594757,14406,junrao,2023-09-21T20:54:51Z,"I guess the purpose of this code is when there is no pending records, we block until some new records are fetched. However, I am wondering if it achieves the purpose. When processing a FetchEvent, ApplicationProcessor just calls requestManagers.fetchRequestManager.drain(). If there is nothing to drain, an empty Queue is returned immediately. This will unblock FetchEvent to return immediately without waiting for the poll time?"
1333599432,14406,lianetm,2023-09-21T21:00:26Z,Is this an old comment I guess?
1333605133,14406,lianetm,2023-09-21T21:07:38Z,I find this name combination closer+assertOpen kind of confusing. I even think it would it be easier to follow if we just had the explicit check `if(isClose()) throw` here 
1333613388,14406,lianetm,2023-09-21T21:17:24Z,Do we expect to have nulls here even if these come from the BlockingQueue that does not accept nulls?
1333616692,14406,philipnee,2023-09-21T21:21:51Z,have you ran into a scenario that the module is null?
1333617066,14406,lianetm,2023-09-21T21:22:23Z,"need ""to"" throw"
1333620613,14406,lianetm,2023-09-21T21:27:18Z,debug level better? (same for all other log lines in this func)
1333698259,14406,junrao,2023-09-21T23:36:53Z,Is this request guaranteed to be sent when the consumer is closed? Do we need this guarantee?
1333708126,14406,junrao,2023-09-21T23:55:37Z,Is the rebalance callback called here?
1333739561,14406,philipnee,2023-09-22T00:36:53Z,"current log is at trace level: `log.trace(""Closing the Kafka consumer"");`"
1333760307,14406,junrao,2023-09-22T01:07:10Z,"This logic is a bit weird. ApplicationEventProcessor handles FetchEvent by draining all completeFetches from fetchBuffer, but here we are just adding them back to fetchBuffer again."
1334577989,14406,junrao,2023-09-22T16:08:24Z,Could we describe what this class does? It's a bit weird the `BackgroundEventProcessor.process` is called from `PrototypeAsyncConsumer` in the foreground.
1334591301,14406,junrao,2023-09-22T16:20:46Z,This causes an exception to be thrown in the application thread. It seems that we should avoid doing that for at least the retriable exceptions? Ditto below.
1334595965,14406,junrao,2023-09-22T16:25:38Z,It seems that timer.currentTimeMs() doesn't change btw poll start and poll end because of the way that Timer is constructed?
1334596869,14406,junrao,2023-09-22T16:26:33Z,Is that TODO still needed since we implemented `poll` now?
1334617225,14406,junrao,2023-09-22T16:47:30Z,Should we wait for the time here like what we did in `refreshCommittedOffsetsIfNeeded`?
1334630287,14406,junrao,2023-09-22T17:01:40Z,"This is an existing issue, but I don't quite understand this comment. In other places, we just use `time` directly assuming it's never null. "
1334636045,14406,junrao,2023-09-22T17:08:20Z,Do we need this since closeTimer is already bounded by requestTimeoutMs?
1334639334,14406,junrao,2023-09-22T17:11:53Z,We are not really passing in closeTimer below.
1334651469,14406,junrao,2023-09-22T17:25:40Z,Could we remove `this` for better consistency?
1334655265,14406,junrao,2023-09-22T17:29:54Z,"We haven't implemented the group subscription logic in PrototypeAsyncConsumer, right? Ditto for the pattern subscribe below."
1334668882,14406,junrao,2023-09-22T17:45:36Z,"`fetchPosition` is updated in the background thread, right? So, it could change anytime during the `poll` call in the consumer. Do we need the info on `fetchPosition`  to be accurately reflected here?"
1334677351,14406,junrao,2023-09-22T17:55:10Z,The comment is a bit confusing. The code doesn't seem to do anything related to offsets and rebalance.
1334678608,14406,junrao,2023-09-22T17:56:44Z,No partitions are provided to this method.
1334719457,14406,junrao,2023-09-22T18:48:26Z,"This is an existing issue, but I am not sure why the comment mentions `fetchRecords`."
1334720342,14406,junrao,2023-09-22T18:49:38Z,Could we update the javadoc?
1334733479,14406,junrao,2023-09-22T19:06:57Z,There is still a mention of `deserializers` above.
1334734342,14406,junrao,2023-09-22T19:08:09Z,extra new line
1334738863,14406,junrao,2023-09-22T19:14:25Z,It seems that none of the request managers implements `close`. Does this need to be `Closeable`?
1334740555,14406,junrao,2023-09-22T19:16:49Z,"`RequestManagers` doesn't take <K, V>."
1334821243,14406,junrao,2023-09-22T21:13:33Z,The NoId part is a bit confusing. assignFromUserSingleTopic?
1334829050,14406,junrao,2023-09-22T21:27:55Z,"`client.updateMetadata `eventually calls `metadata.updateWithCurrentRequestVersion`. So, not sure why we are updating cluster metadata twice with different values."
1334857621,14406,junrao,2023-09-22T22:25:49Z,Could we be consistent with the use of `this`?
1334860526,14406,junrao,2023-09-22T22:31:22Z,consumerClient => networkClientDelegate?
1334869446,14406,kirktrue,2023-09-22T22:52:25Z,"Yes, just to set off the child `subpackage` element more cleanly. But I can remove it if you'd like."
1334870059,14406,kirktrue,2023-09-22T22:54:15Z,"Are you saying that the incoming error (`t`) is already logged before the callback is invoked? If so, then yeah, it makes sense to remove it here."
1334870549,14406,kirktrue,2023-09-22T22:55:37Z,"Sorry, can you elaborate? Where is the error logged other than this?"
1334870694,14406,kirktrue,2023-09-22T22:56:06Z,Yep. I'll remove it.
1334870735,14406,kirktrue,2023-09-22T22:56:12Z,Will remove.
1334870782,14406,kirktrue,2023-09-22T22:56:20Z,OK. I'll change it. Thanks.
1334871262,14406,kirktrue,2023-09-22T22:57:46Z,"Hmm... I can see your point. The naming is definitely confusing 

It used to be named `maybeThrowIllegalStateException()`, so it could be worse."
1334871627,14406,kirktrue,2023-09-22T22:59:03Z,I found the occasional `null` event in the past. The collections didn't prevent it. Maybe it's an extra level of paranoia that has outlived its purpose?
1334872019,14406,kirktrue,2023-09-22T23:00:22Z,Do you mean if `networkClientDelegate` is still `null`? Yes. That can happen if the object is constructed and then used without calling `initializeResources` beforehand. Perhaps a comment or a restructuring of the code could be made.
1334872655,14406,kirktrue,2023-09-22T23:02:24Z,"I need ""to"" proofread my comments more thoroughly  "
1334873271,14406,kirktrue,2023-09-22T23:04:12Z,"I'll change the log line at the start of the method from `info` to `trace` and leave the log line at the end of the method as `debug`. In so doing, both will match the levels in `KafkaConsumer.close()`."
1334876767,14406,kirktrue,2023-09-22T23:16:05Z,"Not yet, no. I'll remove the comment because we haven't implemented the callback mechanism here. It's in a draft PR #14357. We do have the _general_ mechanism for how we'll end up calling them, which is via the `BackgroundEventProcessor`.

I did notice a difference between `KafkaConsumer` and `PrototypeAsyncConsumer`the former is potentially invoking the rebalance callback on each iteration of the loop inside `poll()` whereas the latter implementation is only calling it once at the top of `poll()`. I'll change ours to work in a similar fashion."
1334878210,14406,kirktrue,2023-09-22T23:21:07Z,"Yes, this is ugly and ripe for reworking.

There are two buffers to store `CompletedFetch`es: one for the background thread and one for the application thread. This way the background thread can write to _its_ buffer and the application thread can read from _its_ buffer without them stepping on each other's toes."
1334878765,14406,kirktrue,2023-09-22T23:23:21Z,"Yeah, that is a bit weird. It's the same with `ApplicationEventProcessor.process()` being called from the background thread.

Each thread ""processes"" the events it received from the other thread."
1334878800,14406,kirktrue,2023-09-22T23:23:31Z,"But yes, I'll definitely add some comments here."
1334879279,14406,kirktrue,2023-09-22T23:25:15Z,"My understanding was that the failure is only propagated to this callback once the retries have been exhausted.

@philipnee Am I misunderstanding?"
1334881403,14406,kirktrue,2023-09-22T23:32:36Z,"The `Timer` is created inside `poll()` is passed into the other methods. The only one that updates it directly is `pollForFetches` which updates it only in the `finally` block at the end:

```java
timer.update(pollTimer.currentTimeMs());
```

I double-checked our implementation against `KafkaConsumer` and they seem to match in that regard. `KafkaConsumer` passes the `Timer` into the methods related to the consumer coordinator, but since we don't have that functionality just yet, we don't."
1334881860,14406,kirktrue,2023-09-22T23:34:21Z,Good question. @philipnee do we need to update our `poll` implementation to include the wakeup mechanism?
1334882482,14406,kirktrue,2023-09-22T23:37:13Z,@lianetm do you remember why we opted for a non-blocking event here?
1334882689,14406,kirktrue,2023-09-22T23:38:10Z,`OffsetFetcher.resetPositionsIfNeeded()` is used in the `KafkaConsumer` and it appears to send off the reset positions request asynchronously  
1334884022,14406,kirktrue,2023-09-22T23:43:15Z,"`createTimerForRequest` appears to only be called from `close()`. Some of the constructors' contents are wrapped in a `try-catch` block that attempts to close the consumer on initialization failure. So if the constructor fails before we initialize the `time` instance variable, it would be `null` when we attempt to `close()` up any resources.

I'm assuming based on that comment that only `close()` has that concern about using `time`."
1334885217,14406,kirktrue,2023-09-22T23:48:32Z,True. There doesn't appear to be any blocking calls made from the `FetchBuffer.close()` path. I'll dig around some more.
1334885226,14406,kirktrue,2023-09-22T23:48:34Z,"True. We don't have to account for time it takes for the coordinator to close because we don't have a coordinator .

I'll remove that."
1334885719,14406,kirktrue,2023-09-22T23:50:42Z,"The inclusion of `this` here is, in fact, to be more consistent... with the code in `KafkaConsumer.subscribe()`. At one point we took as much of the code from `KafkaConsumer` as we could, warts and all.

I'll remove this `this`, though."
1334885895,14406,kirktrue,2023-09-22T23:51:38Z,"There are several other places in the `PrototypeAsyncConsumer` that we do things sub-optimally, just to match `KafkaConsumer`."
1334886142,14406,kirktrue,2023-09-22T23:52:47Z,I'll dig in on this a little deeper.
1334886595,14406,kirktrue,2023-09-22T23:54:45Z,This is another case of grabbing the code verbatim from `KafkaConsumer`. @philipnee does the code comment make sense to you?
1334886820,14406,kirktrue,2023-09-22T23:55:41Z,Good catch. Old comments  
1334888478,14406,kirktrue,2023-09-23T00:03:26Z,"I think I wrote the initial comment 

I was trying to explain that `drain()`-ing a `CompletedFetch` is like `close()`-ing it, in that we free its resources. But it's _unlike_ `close()` because we can technically still call `fetchRecords()` on it without throwing some sort of `IllegalStateException` or anything.

Is this better?

> It is OK for the caller to invoke `fetchRecords()` on a `CompletedFetch` that has been `drain()`-ed. An exception won't be thrown; it will simply return no records.

Or should I just scrap the whole comment altogether?"
1334892999,14406,kirktrue,2023-09-23T00:28:08Z,Will do.
1334893011,14406,kirktrue,2023-09-23T00:28:13Z,Will exorcise them.
1334893046,14406,kirktrue,2023-09-23T00:28:28Z,I'll remove it.
1334893631,14406,kirktrue,2023-09-23T00:32:19Z,"The `FetchRequestManager` does by virtue of extending from `AbstractFetch`. The sole reason for that it is so that it can prepare to send requests to the brokers to close their fetch sessions. It's kind of kludgey the way it's done, so I'll take another look to see if it can be done more cleanly."
1334894116,14406,kirktrue,2023-09-23T00:35:10Z,Thanks for the catch!
1334896241,14406,kirktrue,2023-09-23T00:49:26Z,"Removed



the 




extra




line."
1335942472,14406,lianetm,2023-09-25T14:07:00Z,"My understanding is that in that case it won't continuously fetch from the old leader because of the partition subscription state, which will transition to `AWAITING_VALIDATION` (not a valid state for fetching). The moment a new metadata is discovered, [partitions are marked as requiring validation](https://github.com/apache/kafka/blob/59d0daf247a066ad408f713dbfb19e92fc243200/clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetFetcherUtils.java#L195), so the [prepareFetchRequest](https://github.com/apache/kafka/blob/59d0daf247a066ad408f713dbfb19e92fc243200/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractFetch.java#L376C12-L376C12) will not fetch from them. 

To complete the validation picture, once the `OffsetForLeaderEpoch` response is received, the subscription state will transition to `FETCHING` again and fetch requests should resume for that partition. 

My thoughts but please @kirktrue correct me if I'm missing something from the fetching story.  "
1336253362,14406,junrao,2023-09-25T18:31:51Z,@lianetm : Thanks for the reply. That's my understanding too. My question was what happens before the new metadata is received. Will the consumer continuously fetch from the older leader?
1336273549,14406,junrao,2023-09-25T18:52:21Z,Should this comment be moved to above the `client.prepareResponse` below?
1337443244,14406,kirktrue,2023-09-26T15:54:24Z,I reverted the change to the comment. I don't remember changing it  
1337444033,14406,kirktrue,2023-09-26T15:55:02Z,I've updated this so that the `close()` method is a lot cleaner.
1337444456,14406,kirktrue,2023-09-26T15:55:21Z,Removed the unnecessary timer.
1337444877,14406,kirktrue,2023-09-26T15:55:43Z,"`createTimerForRequest()` is no longer used, so marking this as resolved."
1337660657,14406,kirktrue,2023-09-26T19:02:44Z,I've changed the mechanism to handle the responses from fetch such that we should now properly block until results are available.
1337661714,14406,kirktrue,2023-09-26T19:03:54Z,"This has been reworked as part of a recent change. Not only are there are still two `FetchBuffer`s, but I've _added_ a blocking queue between them since the `FetchBuffer` should not be updated on the `Future` callback, since that is updated on another thread and `FetchBuffer` is not thread safe."
1337662384,14406,kirktrue,2023-09-26T19:04:41Z,"It is currently being worked on for the KIP-848 work, so its development is running in parallel."
1337849711,14406,kirktrue,2023-09-26T22:39:41Z,It looks like the other call sites that add error events to the handler also log them  
1337881130,14406,kirktrue,2023-09-26T23:43:17Z,"In the refactored version, the logging is not as verbose."
1337881557,14406,kirktrue,2023-09-26T23:44:08Z,"This code has been refactored, but it still has the check, just to be paranoid."
1337882369,14406,kirktrue,2023-09-26T23:45:52Z,Added a brief comment to explain this case.
1338972414,14406,philipnee,2023-09-27T17:35:33Z,"I am actually not sure if we need this as NetworkClientDelegate is already doing the connection checking.  If the node is unavailable for reconnection, then it would fail the unsentRequest and presumably trigger a retry.

The original code does the connection checking because the request is sent right after its creation; however, there's a time gap between the creation and the actual network IO.

I think it is harmless to leave it here but it would be great if we could clean them up later."
1338973379,14406,philipnee,2023-09-27T17:36:21Z,"Per previous comment - if we actually clean up the code, we should only care if there's a fetchTarget or not."
1339157715,14406,philipnee,2023-09-27T20:13:48Z,we are logging debug for the request manager but info here.  Just want to make sure we are consistent with the logging level.
1339158103,14406,philipnee,2023-09-27T20:14:09Z,Thanks for removing the boolean.
1339158811,14406,philipnee,2023-09-27T20:14:54Z,My only fear is that this could spam the trace log
1340402407,14406,philipnee,2023-09-28T16:26:40Z,should we use ( ) instead of { } ?
1340690791,14406,philipnee,2023-09-28T21:36:39Z,"since we also need to send offsetCommit upon closing, maybe it is best to poll the networkClientDelegate upon closing. "
1342913561,14406,junrao,2023-10-02T16:29:31Z,Could we describe what `poll` does?
1342950743,14406,junrao,2023-10-02T17:11:23Z,The above comment that this class is only used from a single thread seems incorrect. The background thread adds fetched data into `completedFetches` and `PrototypeAsyncConsumer` drains `completedFetches` through `fetchCollector.collectFetch(fetchBuffer)`. There is actually very subtle synchronization between the two threads. Could we document the correct way of using this class and how we achieve synchronization so that future developers don't break it?
1342984775,14406,junrao,2023-10-02T17:50:15Z,"Hmm, with this logic, there is a short window between `completedFetch` in `fetchResults` and `completedFetch` be added to back to `fetchBuffer`. During this window, `BackgroundThread` could make a `poll` call, see no buffered data in `fetchBuffer` and fetch the same offset again? Then the consumer could see duplicated data because of this."
1343046752,14406,junrao,2023-10-02T18:58:21Z,"Earlier, we have ""do not have a valid position and are not awaiting reset"". The reset there and the reset here mean different things. The former refers to resetting the offset based on OffsetForLeaderEpoch and the latter refers to resetting the offset to either the earliest or latest. It would be useful to make this clear."
1343059742,14406,junrao,2023-10-02T19:11:22Z,"It still seems weird that we only use the timer for `refreshCommittedOffsetsIfNeeded`, but not for other cases where we don't have valid fetch positions. For example, if all partitions are in AWAIT_VALIDATION state, it seems that PrototypeAsyncConsumer.poll() will just go in a busy loop, which is not efficient.
"
1343061015,14406,junrao,2023-10-02T19:12:57Z,"If `initializingPartitions` is empty, do we still send the OffsetFetch request? I didn't see the logic for short-circuiting."
1343147889,14406,junrao,2023-10-02T20:46:12Z,Is `this` needed?
1343150605,14406,junrao,2023-10-02T20:49:09Z,"To be consistent with other places, it seems that we want to combine this with the previous line. Ditto below."
1343163505,14406,junrao,2023-10-02T21:04:40Z,Where is the callback handler?
1343232059,14406,junrao,2023-10-02T22:44:17Z,"At this stage, we are just propagating connection level errors like DisconnectException, which is retriable. So, it seems that we shouldn't throw this error back to the application. Ditto in `pollOnClose`."
1343246844,14406,junrao,2023-10-02T23:12:14Z,"If the background thread dies, should we throw an exception to the user thread on the next `poll` call?"
1343251934,14406,junrao,2023-10-02T23:21:55Z,"Instead of using `currentTimeMs`, we need to use `timer.currentTimeMs` to pick up the latest time, right?"
1343259817,14406,junrao,2023-10-02T23:40:32Z,Is this comment addressed?
1343265294,14406,junrao,2023-10-02T23:52:58Z,fetchPosition is updated in the user thread.
1343266219,14406,junrao,2023-10-02T23:54:58Z,Was this comment addressed?
1344756235,14406,kirktrue,2023-10-03T21:21:19Z,"It is now run as part of the `Consumer.close()` process, yes."
1344758105,14406,kirktrue,2023-10-03T21:22:10Z,"No, not yet.

@philipnee Do we want to add something for wakeup here, or should I remove the comment? Thanks."
1344762836,14406,kirktrue,2023-10-03T21:26:43Z,Can you look at the new `DefaultBackgroundThread.runAtClose()` method I added? Do we need to update the `CommitRequestManager` to implement the `pollOnClose()` API I added?
1344780023,14406,kirktrue,2023-10-03T21:47:18Z,Updated to overload the `assignFromUser` method name with a single `TopicPartition`. Then that single partition is used to return the topic name to reduce the number of times `noId` appears in that code.
1344786987,14406,kirktrue,2023-10-03T21:54:37Z,"Good question. As a test, I made this change locally:

```diff
        // A dummy metadata update to ensure valid leader epoch.
-       metadata.update(9, RequestTestUtils.metadataUpdateWithIds(""dummy"", 1,
+       metadata.updateWithCurrentRequestVersion(RequestTestUtils.metadataUpdateWithIds(""dummy"", 1,
```

I ran the tests and they all passed, so I don't know why it was written like that  

This code in `FetchRequestManagerTest` is copied from `FetcherTest`; as much as we could was left verbatim."
1344795530,14406,kirktrue,2023-10-03T22:03:44Z,Done.
1344799794,14406,kirktrue,2023-10-03T22:08:14Z,Done.
1344801112,14406,kirktrue,2023-10-03T22:09:55Z,Fixed in both original `FetcherTest` and the copied `FetchRequestManagerTest`.
1344822614,14406,kirktrue,2023-10-03T22:34:18Z,I added documentation to the `RequestManager` interface with pointers to it from the methods in `FetchRequestManager`.
1344897098,14406,kirktrue,2023-10-04T00:05:17Z,"Yes, the process is a bit convoluted...

To perform the process of moving the fetched records from the background thread to the application thread and then on to the user, `PrototypeAsyncConsumer` has these three instance variables:

1. `fetchResults`
2. `fetchBuffer`
3. `fetchCollector`

All three of those objects are created in the application thread when the `PrototypeAsyncConsumer` is created. `fetchBuffer` and `fetchCollector` are only ever referenced by the application thread; `fetchResults`, however, is used by **both** threads.

`fetchResults` is referenced in the background thread when it is used in the `FetchEvent` callback in the `sendFetches()` method:

```java
private void sendFetches() {
    FetchEvent event = new FetchEvent();
    applicationEventHandler.add(event);

    event.future().whenComplete((completedFetches, error) -> {
        fetchResults.addAll(completedFetches);
    });
}
```

Since the `whenComplete()` method is executed when the background thread ""completes"" the `Future`, `fetchResults` is thus modified on the background thread.

The rest of the process should occur on the application thread.

During calls to `poll()` on the application thread, data from `fetchResults` is moved to `fetchBuffer` in `pollForFetches()`:

```java
while (pollTimer.notExpired()) {
    CompletedFetch completedFetch = fetchResults.poll(pollTimer.remainingMs(), TimeUnit.MILLISECONDS);

    if (completedFetch != null)
        fetchBuffer.add(completedFetch);

    pollTimer.update();
}
```

The data in `fetchBuffer` is later extracted in `fetchCollector` during the `poll()` process, but this again is on the application thread.

This roundabout way of getting the data is specifically done so that we don't write to the `FetchBuffer` inadvertently from the background thread. Hence these JavaDoc comment for `fetchResults`:

```java
/**
 * A thread-safe {@link BlockingQueue queue} for the results that are populated in the background thread
 * when the fetch results are available. Because the {@link #fetchBuffer fetch buffer} is not thread-safe, we
 * need to separate the results collection that we provide to the background thread from the collection that
 * we read from on the application thread.
 */
private final BlockingQueue<CompletedFetch> fetchResults = new LinkedBlockingQueue<>();
```

This is a rough idea of what happens on the background thread:

>`FetchRequestManager` -> `fetchResults`

Then later in the application thread during `poll()`:

> `fetchResults` -> `fetchBuffer` -> `fetchCollector`

Let me know if that makes sense or if there is still a gap that I'm not seeing. I can write the above up (with any changes you'd like) in code comments."
1344899034,14406,kirktrue,2023-10-04T00:09:08Z,I've removed the qualifiers where they're not needed.
1344900260,14406,kirktrue,2023-10-04T00:11:31Z,Fixed.
1344900777,14406,kirktrue,2023-10-04T00:12:12Z,Fixed.
1346179200,14406,junrao,2023-10-04T16:45:53Z,"@kirktrue : Thanks for the explanation. Two followup questions.
1. The background thread has the following path` FetchRequestManager.poll  -> handleFetchResponse -> fetchBuffer.add(completedFetch)`. So, it seems that the background thread also writes the fetched data to `fetchBuffer`.
2. This is related to my other [comment](https://github.com/apache/kafka/pull/14406#discussion_r1342984775). The background thread has the following path `FetchRequestManager.poll  -> AbstractFetch.prepareFetchRequests -> AbstractFetch.fetchablePartitions -> reads fetchBuffer.bufferedPartitions()`. Since fetchBuffer is written by the application thread, how do we coordinate the synchronization btw the two threads?"
1346234546,14406,kirktrue,2023-10-04T17:33:01Z,You're right. Thanks for catching that! Changed.
1346270372,14406,kirktrue,2023-10-04T18:05:36Z,Yes. I added a check that it's not closed at the top of `runOnce()`.
1346278813,14406,kirktrue,2023-10-04T18:13:15Z,This was an outdated comment. Fixed.
1346451911,14406,junrao,2023-10-04T20:54:01Z,background network thread => network thread ?
1346463369,14406,junrao,2023-10-04T21:05:03Z,"This will cause a logging of error during normal shutting down of the consumer, right? It would be useful to avoid that."
1346493970,14406,philipnee,2023-10-04T21:35:27Z,Thanks @kirktrue - I think we will need to do that.  I created KAFKA-15548 to handle the closing task.  
1346504058,14406,kirktrue,2023-10-04T21:47:38Z,I removed the error propagation (and logging).
1346508562,14406,kirktrue,2023-10-04T21:51:54Z,@lianetm Any reason we don't check if `initializingPartitions` is non-empty before creating the `OffsetFetchApplicationEvent`?
1346517075,14406,kirktrue,2023-10-04T21:59:23Z,"Regardless of the return value of `updateFetchPositions()`, `poll()` will still go on to call `pollForFetches()` which will block for data availability or timeout expiration. "
1346545044,14406,kirktrue,2023-10-04T22:32:56Z,"For point #1, the background thread has a _separate_ fetch buffer. It doesn't write to the same object. The application thread `FetchBuffer` is created in `PrototypeAsyncConsumer` and the background thread `FetchBuffer` is created in `AbstractFetch`.

For point #2, I think your [other comment](https://github.com/apache/kafka/pull/14406#discussion_r1342984775) is correct. I'm not yet sure how to maintain those two fetch buffers separately without running into that race condition."
1346546088,14406,kirktrue,2023-10-04T22:34:22Z,"Yes, I can see that now. Given that there are two separate `FetchBuffer`s and the way they're populated, I think the window is actually a bit bigger.

I need to noodle on this for a bit. "
1346562666,14406,junrao,2023-10-04T22:58:01Z,`fetcher` is a `TestableFetchRequestManager`. Could we just do `poll` instead of `fetcher.poll`?
1346567694,14406,junrao,2023-10-04T23:06:49Z,"Where is the callback? Also,` NetworkClient#send` happens in the caller, not here, right?"
1346572630,14406,junrao,2023-10-04T23:15:53Z,It's very confusing that the above two `handleFetchResponse` refer to different methods. Could we name them differently?
1346604856,14406,kirktrue,2023-10-05T00:00:01Z,"Yes, it would, but what do you expect when you're logging at `TRACE`?  "
1346606768,14406,kirktrue,2023-10-05T00:02:48Z,Changed to debug
1346607807,14406,kirktrue,2023-10-05T00:04:39Z,I've removed the logging and error forwarding as it is handled in the `NetworkClientDelegate` layer.
1346612374,14406,kirktrue,2023-10-05T00:11:20Z,"@philipnee at the beginning of the `KafkaConsumer.poll()` loop, it does this:

```java
client.maybeTriggerWakeup();
```

Do we support _disabling_ wake-ups? I don't see the analogue to `maybeTriggerWakeup()`, `disableWakeups()`, etc. in `WakeupTrigger`. Can we emulate what `KafkaConsumer` is doing in the `poll()` loop with our current implementation? If not, I'd prefer to a) remove the comment, and b) file a new Jira.

Thoughts?"
1346613925,14406,kirktrue,2023-10-05T00:12:42Z,"@junrao I'll review with @philipnee.
"
1346615044,14406,kirktrue,2023-10-05T00:14:29Z,"@philipnee `AbstractFetch` is used by both the current `Fetcher` as well as the `FetchRequestManager`. I'd have to refactor the code to rid ourselves of it.

What do you think about filing a new Jira to track a follow-up change for this?"
1346616102,14406,kirktrue,2023-10-05T00:16:29Z,"@junrao I'd like to file a separate Jira for this, only because it's been this way for a while. I need more time to understand it before changing it, and would love to get the integration tests and perhaps even system tests online before changing it.

Thoughts?"
1346616646,14406,kirktrue,2023-10-05T00:17:22Z,@junrao I think I need to circle back with @lianetm to get her input on this as she's much closer to it than I am.
1347629215,14406,junrao,2023-10-05T15:39:53Z,"Yes, it's fine to have a separate jira to clean this up."
1347692761,14406,junrao,2023-10-05T16:28:21Z,"This logic looks correct in the test. However, in the actual code, there could be unconsumed data in `PrototypeAsyncConsumer.fetchResults `and `PrototypeAsyncConsumer.fetchBuffer`. How do we prevent those data from being returned to the user? Same question when partition is paused."
1347727982,14406,junrao,2023-10-05T16:57:13Z,"Hmm, should the code be commented out? It doesn't match the comment above."
1347742323,14406,junrao,2023-10-05T17:10:13Z,"I don't quite understand these 3 lines. Why are we polling the fetcher again since typically this doesn't happen after pollOnClose is called on the fetcher? Also, for the test as it is, it seems that the first request is the one with the final epoch, not the second?"
1347748114,14406,junrao,2023-10-05T17:15:36Z,Does this test subsume `testInflightFetchOnPendingPartitions`?
1347751320,14406,junrao,2023-10-05T17:18:29Z,Should the comment be moved to just above `client.prepareResponse` below?
1347752851,14406,lianetm,2023-10-05T17:19:55Z,"Agree @junrao , I will update the comments to better explain.

The bottom line is that positions may be reset in 2 different ways:
- using the committed offsets (retrieved with `OffsetFetch` sent to the group coordinator). This if committed offsets are in use.
- using the partition offsets (retrieved with `ListOffset` sent to the partition leader + reset strategy)

The `OffsetForLeaderEpoch` request is the one used to validate positions, basically retrieving epoch and end offsets from a leader, to validate the the current position held on the consumer side. 

Makes sense? I will update the comments for each step."
1347759469,14406,junrao,2023-10-05T17:26:00Z,Should the comment be moved to just above client.prepareResponse below? Ditto in a few other places.
1347768459,14406,junrao,2023-10-05T17:34:57Z,Is the upgrade on the server or the client side?
1347777973,14406,lianetm,2023-10-05T17:43:31Z,I updated the comments in this other PR that I had just opened https://github.com/apache/kafka/pull/14503/commits/8928d92f3e7093f9915a9804c46ca4f34d9ce0e1
1347791680,14406,junrao,2023-10-05T17:56:33Z,It would be useful to clarify that `fetchedRecords()` is called deep inside `fetcher.collectFetch()`. Ditto in a few other places mentioning `fetchedRecords()`.
1347799078,14406,kirktrue,2023-10-05T18:03:41Z,"I'm planning to remove the comments and create a new Jira to track this change.

cc @philipnee @junrao "
1347806531,14406,junrao,2023-10-05T18:11:27Z,Be consistent with usage of `this`.
1347811905,14406,junrao,2023-10-05T18:17:09Z,Where is the resizing?
1347832688,14406,junrao,2023-10-05T18:40:09Z,its been => it has been
1347834317,14406,junrao,2023-10-05T18:41:55Z,This seems unnecessary given the `assertEmptyFetch` below?
1347865054,14406,kirktrue,2023-10-05T19:14:43Z,TODO: fix the comment in the code and explain about the ApplicationEventProcessor.
1347874026,14406,kirktrue,2023-10-05T19:25:07Z,TODO: find case where we _might_ be sending RPCs with empty sets.
1347874693,14406,kirktrue,2023-10-05T19:25:55Z,TODO: @philipnee to create a separate ticket to review/fix.
1347883272,14406,junrao,2023-10-05T19:35:43Z,Could we write this as parameterized test to avoid duplicating the code in the next few tests?
1347884952,14406,junrao,2023-10-05T19:37:30Z,Should we assert there is no pending request after this?
1347885711,14406,lianetm,2023-10-05T19:38:19Z,"I know for sure we skip empty lists and don't send requests for all the partition offsets related events (on the `OffsetsRequestManager` on [fetchOffsets](https://github.com/apache/kafka/blob/cdf726fd358f9be3438ceefb01073ab40a31a8b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsRequestManager.java#L151), [resetPositions](https://github.com/apache/kafka/blob/cdf726fd358f9be3438ceefb01073ab40a31a8b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsRequestManager.java#L193) and [validatePositions](https://github.com/apache/kafka/blob/cdf726fd358f9be3438ceefb01073ab40a31a8b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/OffsetsRequestManager.java#L216)). 
That being said, here is about fetching the committed offsets, and I don't see a clear early return but maybe I'm missing how it happens so let's wait for @philipnee to give it a closer look. "
1347894828,14406,junrao,2023-10-05T19:45:34Z,It would be useful to add a comment why offset is not reset yet on `OFFSET_OUT_OF_RANGE`. This is because the error handling happens when new records are polled.
1347906316,14406,junrao,2023-10-05T19:57:07Z,Could this be replaced with `fetchRecordsInto`?
1347908197,14406,junrao,2023-10-05T19:58:49Z,Is this redundant given the test in 1811?
1347910980,14406,junrao,2023-10-05T20:01:53Z,"Hmm, why don't we return records from other partitions since maxRecords is maxInt?"
1347913781,14406,junrao,2023-10-05T20:05:21Z,Why don't we return records from tp1 since maxRecords is 2?
1347914377,14406,junrao,2023-10-05T20:06:06Z,affect => effect
1347923838,14406,junrao,2023-10-05T20:17:28Z,What's the purpose of this code? The receive is delayed and thus there is no throttleDelayMs received in the client.
1347931744,14406,junrao,2023-10-05T20:26:15Z,Why is `recordsFetchLeadMin` different from `partitionLead` given there is only 1 assigned partition?
1347936262,14406,junrao,2023-10-05T20:31:04Z,"`expectedBytes` is calculated as total, instead of avg. Is this correct?"
1347938331,14406,junrao,2023-10-05T20:33:31Z,"The name is a bit mis-leading. We get a full response, but skipped an offset before the fetch position."
1347960253,14406,junrao,2023-10-05T20:54:36Z,The expected and actual are reversed.
1348009426,14406,kirktrue,2023-10-05T21:36:47Z,I filed KAFKA-15555 as a separate follow-up task to handle the wakeup mechanics.
1348009622,14406,kirktrue,2023-10-05T21:37:03Z,I will remove the temporary comment in a bit.
1348011353,14406,kirktrue,2023-10-05T21:39:43Z,Removed.
1348019537,14406,junrao,2023-10-05T21:45:42Z,Capitalize in?
1348029408,14406,kirktrue,2023-10-05T21:58:16Z,"I have filed KAFKA-15556 (_Remove NetworkClientDelegate methods isUnavailable, maybeThrowAuthFailure, and tryConnect_) to address this issue since it's affecting other `RequestManager` implementations."
1348029671,14406,kirktrue,2023-10-05T21:58:45Z,True. We'll handle that in KAFKA-15556.
1348047927,14406,kirktrue,2023-10-05T22:14:19Z,I filed KAFKA-15557 (_Fix duplicate metadata update in fetcher tests_) to address this issue.
1348055761,14406,kirktrue,2023-10-05T22:21:17Z,Created KAFKA-15558 (_Determine if Timer should be used elsewhere in PrototypeAsyncConsumer.updateFetchPositions()_) to address separately as the given implementation is the same as the current implementation. Perhaps fixes in both `Consumer` implementations is warranted?
1348062340,14406,junrao,2023-10-05T22:22:48Z,Node => note?
1348062596,14406,kirktrue,2023-10-05T22:22:55Z,@philipnee filed KAFKA-15551 (_Evaluate conditions for short circuiting consumer API calls_) to implement this consistently.
1348083698,14406,kirktrue,2023-10-05T23:00:07Z,"Yes, it appears that it will continue to attempt to fetch from the old leader.

When a fetch response is received from the broker and we notice that it has an error, we call `FetchCollector.handleInitializeErrors()` to deal with the different error conditions. When it notices that the error is `FENCED_LEADER_EPOCH`, it will execute these two statements (from the `FetchUtils.requestMetadataUpdate()` method):

```java
metadata.requestUpdate(false);
subscriptions.clearPreferredReadReplica(topicPartition);
```

That's all it does. The logic doesn't update the `FetchState` for that partition. It doesn't clear out the leader epoch. Nothing.

The next time the user calls the `Consumer.poll()` method, the `Fetcher`/`FetchRequestManager` will determine for which partitions we should issue `FETCH` RPCs. The first step is to call the `SubscriptionState.fetchablePartitions()` method which checks each partition:

```java
public synchronized List<TopicPartition> fetchablePartitions(Predicate<TopicPartition> isAvailable) {
    // Since this is in the hot-path for fetching, we do this instead of using java.util.stream API
    List<TopicPartition> result = new ArrayList<>();
    assignment.forEach((topicPartition, topicPartitionState) -> {
        // Cheap check is first to avoid evaluating the predicate if possible
        if (topicPartitionState.isFetchable() && isAvailable.test(topicPartition)) {
            result.add(topicPartition);
        }
    });
    return result;
}
```

Because we didn't change anything in the underlying state of the partition in `SubscriptionState` previously, when the `topicPartitionState.isFetchable()` method is invoked, it returns `true`. Thus it is included in the list for which to fetch, and we will likely hit the same error."
1348084921,14406,kirktrue,2023-10-05T23:02:32Z,"The error message in `Errors` for `FENCED_LEADER_EXCEPTION` states that this error is caused when...

> The leader epoch in the request is older than the epoch on the broker."
1348087535,14406,kirktrue,2023-10-05T23:07:55Z,"Naively it seems like we want to clear/reset the partition's `Metadata.LeaderAndEpoch` value in the metadata cache locally. Then in the fetch logic, that would allow us to skip that partition when we check for the presence of a leader:

```java
for (TopicPartition partition : fetchablePartitions()) {
    SubscriptionState.FetchPosition position = subscriptions.position(partition);

    if (position == null)
        throw new IllegalStateException(""Missing position for fetchable partition "" + partition);

    Optional<Node> leaderOpt = position.currentLeader.leader;

    if (!leaderOpt.isPresent()) {
        log.debug(""Requesting metadata update for partition {} since the position {} "" +
                  ""is missing the current leader node"", partition, position);
        metadata.requestUpdate(false);
        continue;
    }

    . . .
}
```"
1348088801,14406,junrao,2023-10-05T23:10:38Z,Should this comment be moved to just above `client.prepareResponse` below?
1348091800,14406,junrao,2023-10-05T23:16:50Z,Not sure what we are testing here. Which handler is this referring to?
1348107483,14406,junrao,2023-10-05T23:44:11Z,"Hmm, the consumerNetworkThread shouldn't die because of a TopicAuthorizationException, right? Ditto below."
1348110728,14406,junrao,2023-10-05T23:51:45Z,Which call is returning Long.MAX_VALUE?
1348111165,14406,junrao,2023-10-05T23:52:48Z,Does this test cover `testFindCoordinator`?
1348903678,14406,junrao,2023-10-06T15:51:45Z,Be consistent with the use of `this`. Ditto below.
1348915254,14406,junrao,2023-10-06T16:02:18Z,Should we fix `this.records` above too?
1348955876,14406,junrao,2023-10-06T16:31:47Z,This will run the network I/O in the application thread and break the model that all network I/Os should be done in the background thread? Will this be safe since now there could be two threads driving a shared networkClient?
1348962486,14406,junrao,2023-10-06T16:38:11Z,called from by => called by
1349023459,14406,junrao,2023-10-06T17:13:46Z,The auto offset commit seems to happen asynchronously? Do we guarantee that the new owner of the unsubscribed partitions could pick up the latest committed offset?
1349173413,14406,junrao,2023-10-06T18:18:01Z,"There is a subtle difference between transitioning to reset from initializing and transitioning to reset from `OffsetOutOfRangeException` during fetch. In the latter, the application thread will call `FetchCollector.handleInitializeErrors()`. If there is no default offset reset policy, an `OffsetOutOfRangeException` will be thrown to the application thread during `poll`, which is what we want.

However, for the former, if there is no default offset reset policy, we simply ignore that partition through `OffsetFetcherUtils.getOffsetResetTimestamp`. It seems in that case, the partition will be forever in the reset state and the application thread won't get the `OffsetOutOfRangeException`."
1349184330,14406,junrao,2023-10-06T18:28:19Z,"Thanks, Kirk. Does the old consumer have the same behavior too? Should we file a followup jira to improve this?"
1349221937,14406,philipnee,2023-10-06T19:11:48Z,"hi jun - I think we mostly use ""background thread"" in lieu of network thread, so maybe just use background thread?"
1349342751,14406,kirktrue,2023-10-06T21:44:50Z,"I will triple check, and if so, I will add a Jira."
1349343412,14406,kirktrue,2023-10-06T21:46:03Z,"I just pushed a proposed fix for this, which is basically to make the `FetchBuffer` thread safe. Now there is only one fetch buffer for the consumer and is accessed by both threads."
1349343753,14406,kirktrue,2023-10-06T21:46:49Z,@philipnee You didn't see that I got fed up and changed the name of `DefaultBackgroundThread` to `ConsumerNetworkThread`  
1349361459,14406,junrao,2023-10-06T22:15:08Z,"1. This is an existing issue. But the way we handle paused partitions in `collectFetch` seems problematic. The application thread first calls `fetchBuffer.setNextInLineFetch(null)` and then calls `fetchBuffer.addAll(pausedCompletedFetches)`. This could leave a brief window where the paused partition is not included in either `nextInLineFetch` or `completedFetches`. If the background thread kicks in in that window, it could have fetched another chunk for that partition and added the response back to FetchBuffer. This would violate the assumption there is no more than one pending `CompletedFetch` per partition in FetchBuffer and could cause records returned not in offset order or duplicates to be returned.
2. The second existing issue is on the `fetchBuffer.setNextInLineFetch` call in `collectFetch`. The issue is that after all records are drained from `nextInLineFetch`. We only call `setNextInLineFetch` when there is a new `completedFetch`. However, until the drained `completedFetch` is removed from `nextInLineFetch`, the background thread can't fetch the next chunk. So, it seems that we will just be stuck here.
"
1350689034,14406,junrao,2023-10-09T18:46:55Z,Should we just remove this method?
1350697908,14406,junrao,2023-10-09T18:54:59Z,The only useful part of this call is to wake up the ConsumerNetworkThread so that it could prefetch the next data chunk. Perhaps we could make that an explicit call like `applicationEventHandler.wakeup`?
1350700366,14406,junrao,2023-10-09T18:57:04Z,This call seems unnecessary. 
1350703155,14406,junrao,2023-10-09T18:59:36Z,The comment seems obsolete since we are not fetching data below.
1350707142,14406,junrao,2023-10-09T19:03:50Z,"`notEmptyCondition.await` returns false if the waiting time detectably elapsed before return from the method. This seems to be case to break out of the while loop. So, it seems that the `if` test should be reversed."
1350773132,14406,junrao,2023-10-09T20:44:37Z,"Currently, `fetchBuffer.setNextInLineFetch` and `fetchBuffer.poll` are separate operations and we expect the caller to call them in the right order to avoid a partition missing in FetchBuffer in the transition phase. It still leaves us with the situation that a partition could be in both completedFetches and nextInLineFetch at a particular time. It's not a problem for now, but it may be in the future.  Could we make them an atomic operation? If not, could we add a comment to document the correct usage of the api and the impact on partition being duplicated in completedFetches and nextInLineFetch?"
1351006437,14406,kirktrue,2023-10-09T23:58:22Z,Removed.
1351006470,14406,kirktrue,2023-10-09T23:58:29Z,Done.
1351011656,14406,kirktrue,2023-10-10T00:04:22Z,I removed the latter phrases to reduce confusion.
1351011802,14406,kirktrue,2023-10-10T00:04:32Z,Agreed. I've updated the names.
1351042141,14406,kirktrue,2023-10-10T00:15:32Z,You're right. I've changed the core logic and updated that test (and another one that is essentially the same thing).
1351054656,14406,kirktrue,2023-10-10T00:19:07Z,"Yeah, I don't see it either. I'm not exactly sure what this testing is exercising. @philipnee, do you remember?"
1351063405,14406,kirktrue,2023-10-10T00:24:28Z,Yes. I've removed `testFindCoordinator()` as it's now superfluous.
1351064502,14406,kirktrue,2023-10-10T00:26:59Z,"These are constructors, so even though it's not _strictly_ needed, I've been following the standard Java convention."
1352874538,14406,kirktrue,2023-10-10T16:28:35Z,Removed check to avoid.
1352874719,14406,kirktrue,2023-10-10T16:28:44Z,Done.
1352876631,14406,kirktrue,2023-10-10T16:30:05Z,I've refactored the code to run it in the background thread again.
1352879009,14406,kirktrue,2023-10-10T16:31:35Z,Fixed.
1357182316,14406,junrao,2023-10-12T17:43:47Z,Not sure if this is a useful test since offsetsRequestManager.resetPositionsIfNeeded() seems to never directly throw an exception?
1357194942,14406,junrao,2023-10-12T17:55:19Z,"This needs to wait for the cleanup to be done until the timeout, right?"
1358651225,14406,kirktrue,2023-10-13T18:16:51Z,I removed the commented out code and explained why we need to use `runAtClose()` instead of closing the fetcher directly.
1358658187,14406,kirktrue,2023-10-13T18:19:15Z,"You are correct, sir. I removed those lines. Thanks for the catch!"
1358665833,14406,kirktrue,2023-10-13T18:22:24Z,"The ordering and way they go about testing it is different. As this is an issue from the original `FetcherTest`, I'll open a ticket to resolve separately. Thanks!"
1358682245,14406,kirktrue,2023-10-13T18:33:16Z,"It looks like both.

The test starts off mimicking a client that doesn't support topic IDs. The test sets up the local subscriptions and metadata with a topic that has no ID. Then the test ""upgrades"" the client to a version that _does_ support topic IDs. This is to validate that the fetch session (on the broker) should remove the topic without the ID in favor of the topic with the ID. And then the test ""downgrades"" the client again to ensure the opposite case.

I haven't looked at the underlying code in any depth, however."
1358693714,14406,kirktrue,2023-10-13T18:43:42Z,"`fetchedRecords()` calls `fetcher.collectFetch()`, so it's the other way around. I updated the call sites and changed `fetchedRecords()` to just `fetchRecords()` so that it's hopefully a little more clear."
1358701620,14406,kirktrue,2023-10-13T18:53:30Z,"I had removed most unnecessary uses of `this` in a previous round of updates, but left these because they were necessary. There are three tests that create a locally-scoped variable named `records` that masks the instance-scoped variable of the same name, hence the use of `this` to distinguish them.

I've updated the tests to change the name of the local variable to `testRecords` so it's a) more clear that they're separate from `records`, and b) removes the use of `this`."
1358712167,14406,kirktrue,2023-10-13T19:07:09Z,"The `testUnauthorizedTopic()` test method was added to `FetcherTest` almost eight years ago. Here's what the test looked like when it was initially committed:

```java
    @Test
    public void testUnauthorizedTopic() {
        subscriptions.assignFromUser(Arrays.asList(tp));
        subscriptions.seek(tp, 0);

        // resize the limit of the buffer to pretend it is only fetch-size large
        fetcher.initFetches(cluster);
        client.prepareResponse(fetchResponse(this.records.buffer(), Errors.TOPIC_AUTHORIZATION_FAILED.code(), 100L, 0));
        consumerClient.poll(0);
        try {
            fetcher.fetchedRecords();
            fail(""fetchedRecords should have thrown"");
        } catch (TopicAuthorizationException e) {
            assertEquals(Collections.singleton(topicName), e.unauthorizedTopics());
        }
    }
```

The comment makes a little more sense when taken in its historical context. But it makes a little more sense still when you look at the test method that used to appear directly above `testUnauthorizedTopic()`:

```java
    @Test(expected = RecordTooLargeException.class)
    public void testFetchRecordTooLarge() {
        subscriptions.assignFromUser(Arrays.asList(tp));
        subscriptions.seek(tp, 0);

        // prepare large record
        MemoryRecords records = MemoryRecords.emptyRecords(ByteBuffer.allocate(1024), CompressionType.NONE);
        byte[] bytes = new byte[this.fetchSize];
        new Random().nextBytes(bytes);
        records.append(1L, null, bytes);
        records.close();

        // resize the limit of the buffer to pretend it is only fetch-size large
        fetcher.initFetches(cluster);
        client.prepareResponse(fetchResponse((ByteBuffer) records.buffer().limit(this.fetchSize), Errors.NONE.code(), 100L, 0));
        consumerClient.poll(0);
        fetcher.fetchedRecords();
    }
```

My take is that the author copied the `testFetchRecordTooLarge()` to use as a starting point for the new `testUnauthorizedTopic()` method.

Separately, although `testFetchRecordTooLarge()` is no longer around, we still test the 'record too large' case in `testFetchRequestWhenRecordTooLarge()`.

I'm inclined to remove the comment because it's pretty clear that the intent of `testUnauthorizedTopic()` is to validate that a `TopicAuthorizationException` is thrown if the fetch response includes the `TOPIC_AUTHORIZATION_FAILED` error.

What do you think?"
1358713529,14406,kirktrue,2023-10-13T19:09:06Z,Changed
1358717315,14406,kirktrue,2023-10-13T19:14:04Z,Correct. I've removed the unnecessary `fetchRecords()` call from `testPartialFetchWithPausedPartitions()` in both fetcher tests.
1358758478,14406,kirktrue,2023-10-13T19:40:51Z,Done.
1358761112,14406,kirktrue,2023-10-13T19:43:39Z,Done.
1358764452,14406,kirktrue,2023-10-13T19:48:25Z,Done.
1358791361,14406,kirktrue,2023-10-13T20:21:36Z,"Done. Here's what I added:

```java
        // The partition is not marked as needing its offset reset because that error handling logic is
        // performed during the fetch collection. When we call seek() before we collect the fetch, the
        // partition's position is updated (to offset 2) which is different from the offset from which
        // we fetched the data (from offset 0).
```

I'll admit, I don't know that I totally understand what I wrote or if it's correct :)"
1358832549,14406,junrao,2023-10-13T20:48:48Z,Should the comment be moved to just above `ConsumerNetworkThread.runAtClose` below?
1358914146,14406,kirktrue,2023-10-13T22:24:45Z,"I may be misunderstanding your comment, but the code to denote partitions as being revoked (via `SubscriptionState.markPendingRevocation()` will be handled in KAFKA-15539."
1358914547,14406,kirktrue,2023-10-13T22:25:46Z,Done.
1358914571,14406,kirktrue,2023-10-13T22:25:51Z,Done.
1358918492,14406,kirktrue,2023-10-13T22:36:04Z,Yes. Done.
1358949767,14406,kirktrue,2023-10-13T23:05:23Z,"I looked into this a bit, and it has to do with the records that are used for the different partitions, their counts, offsets, etc. I think it should be investigated and cleaned up for clarity and reassurance.

I will open a new Jira ticket since this is part of the existing `FetcherTest` suite."
1358952151,14406,kirktrue,2023-10-13T23:14:09Z,Filed KAFKA-15606 to track.
1358952187,14406,kirktrue,2023-10-13T23:14:15Z,Added this to KAFKA-15606.
1358953070,14406,kirktrue,2023-10-13T23:17:34Z,Added a link to this question to KAFKA-15606.
1358953131,14406,kirktrue,2023-10-13T23:17:49Z,Fixed
1358953536,14406,kirktrue,2023-10-13T23:19:06Z,I'll file a bug to investigate this question. Thanks!
1358953612,14406,kirktrue,2023-10-13T23:19:25Z,I'll file a bug to investigate this question. Thanks!
1358953637,14406,kirktrue,2023-10-13T23:19:31Z,I'll file a bug to investigate this question. Thanks!
1358954499,14406,kirktrue,2023-10-13T23:22:49Z,Maybe `testFetchResponseMetricsWithSkippedOffset()`?
1358955393,14406,kirktrue,2023-10-13T23:25:43Z,Renamed and reversed.
1358956176,14406,kirktrue,2023-10-13T23:28:19Z,Changed `testReturnAbortedTransactionsinUncommittedMode()` to `testReturnAbortedTransactionsInUncommittedMode()`
1358956462,14406,kirktrue,2023-10-13T23:29:46Z,Fixed.
1358956804,14406,kirktrue,2023-10-13T23:31:02Z,Fixed
1358957417,14406,kirktrue,2023-10-13T23:32:34Z,I believe it is referring to the `Runnable` that is passed into `setWakeupHook()`.
1358990414,14406,kirktrue,2023-10-14T00:22:58Z,Done.
1358990445,14406,kirktrue,2023-10-14T00:23:03Z,Done.
1358990537,14406,kirktrue,2023-10-14T00:23:29Z,Done. Replaced with direct call to `applicationEventHandler.wakeup()`.
1358990617,14406,kirktrue,2023-10-14T00:23:57Z,"We are not fetching, but we are waiting, so I reworded the comment. LMK if it still needs tweaking. Thanks."
1358990797,14406,kirktrue,2023-10-14T00:24:33Z,Thanks for catching that. I fixed this but it's pretty clear now that I need unit tests to validate correctness.
1360933149,14406,junrao,2023-10-16T16:27:06Z,"I made the comment when the code had a `FetchBuffer` in both `PrototypeAsyncConsumer` and `AbstractFetch` since we need to apply the same check on `subscriptions.isFetchable` to both places. Now, we only have a single FetchBuffer in `AbstractFetch`. This is no longer an issue."
1360942134,14406,junrao,2023-10-16T16:34:07Z,unsentRequest => lastUnsentRequest ?
1360995022,14406,junrao,2023-10-16T17:04:12Z,Got it. This is referring to server side upgrade with topicId support.
1360998263,14406,junrao,2023-10-16T17:07:32Z,fetchedRecords() => fetchRecords() ?
1361004569,14406,junrao,2023-10-16T17:13:29Z,@kirktrue : Thanks for the explanation. It makes sense to me to remove the comment here.
1361103184,14406,junrao,2023-10-16T18:17:15Z,"Thanks for updating the comment, Kirk. The comment seems correct. It would be useful to further add that because the fetch position in the request is different from the one in fetch state, the fetched data, including the `OFFSET_OUT_OF_RANGE` error is ignored in `FetchCollector.handleInitializeErrors` because of the following code. 

`                if (position == null || fetchOffset != position.offset) {
`

It's a bit weird that the above check is only done for the `OFFSET_OUT_OF_RANGE` error, instead of any error. It might be useful to file a jira to revisit this logic."
1361109166,14406,junrao,2023-10-16T18:23:52Z,"Yes, the proposed new name sounds good to me."
1361131214,14406,junrao,2023-10-16T18:38:47Z,"Thanks for the reply. I still don't quite understand the test. Why do we duplicate the following code both inside and outside of setWakeupHook?

```
                networkClientDelegate.disconnectAsync(readReplica);
                networkClientDelegate.poll(time.timer(0));

```

MockClient is only woken up through `networkClientDelegate.disconnectAsync`."
1361156117,14406,junrao,2023-10-16T19:05:33Z,The way the code is written. We will wake up `applicationEventHandler` whether fetch is empty or not. Perhaps it's simpler to just always call `applicationEventHandler.wakeup()`  after each `fetchCollector.collectFetch(fetchBuffer)` call?
1361329379,14406,kirktrue,2023-10-16T22:36:19Z,I have filed KAFKA-15615 to resolve.
1361329900,14406,kirktrue,2023-10-16T22:37:18Z,"@junrao this is one of the more serious and subtle issues, so I want to know you're OK with the new approach before I resolve this conversation in the PR. Thanks!"
1361332784,14406,kirktrue,2023-10-16T22:42:39Z,Filed KAFKA-15617
1361345756,14406,kirktrue,2023-10-16T23:07:23Z,"References were changed to either `fetchRequests` or `collectFetch` as appropriate. Some minor refactoring was also introduced to use `assertThrows` instead, where possible."
1361346244,14406,kirktrue,2023-10-16T23:08:26Z,Removed.
1361346884,14406,kirktrue,2023-10-16T23:09:43Z,Changed.
1361353808,14406,junrao,2023-10-16T23:23:31Z,"@kirktrue : Yes, the fix LGTM."
1361355764,14406,kirktrue,2023-10-16T23:27:28Z,Great  Thanks!
1361357104,14406,junrao,2023-10-16T23:29:55Z,The comment is a bit mis-leading since the fetcher doesn't block. It throws an exception.
1361357218,14406,kirktrue,2023-10-16T23:30:05Z,@philipnee Can you chime in on this question? Thanks!
1361357318,14406,kirktrue,2023-10-16T23:30:16Z,@philipnee Can you chime in on this question? Thanks!
1361357471,14406,kirktrue,2023-10-16T23:30:32Z,@lianetm Can you chime in on this question? Thanks!
1361358097,14406,kirktrue,2023-10-16T23:31:42Z,"I am not fond of this code as written, and have made at least two attempts to change it. It's a bit messy but I will take another pass to clean it up. Ideally mostif not allof the logic would live in `FetchBuffer`."
1361358254,14406,kirktrue,2023-10-16T23:32:03Z,"OK, I'll look at this again."
1364694361,14406,kirktrue,2023-10-18T23:39:42Z,Filed KAFKA-15634.
1364696003,14406,kirktrue,2023-10-18T23:42:49Z,Filed KAFKA-15635.
1364698472,14406,kirktrue,2023-10-18T23:47:24Z,Filed KAFKA-15636.
1364699488,14406,kirktrue,2023-10-18T23:49:29Z,Filed KAFKA-15637.
1364700996,14406,kirktrue,2023-10-18T23:52:47Z,Filed KAFKA-15638.
1364704339,14406,kirktrue,2023-10-18T23:59:55Z,Filed KAFKA-15639.
1364706025,14406,kirktrue,2023-10-19T00:02:24Z,I removed the comment since there's a more explanatory comment below already.
1364707771,14406,kirktrue,2023-10-19T00:04:26Z,Done.
1364712500,14406,kirktrue,2023-10-19T00:08:48Z,I've filed KAFKA-15640 to look into this in more detail.
1364715414,14406,kirktrue,2023-10-19T00:13:45Z,I've added this to KAFKA-15640 for a dedicate effort to clean up the modifications to this data to make them atomic operations.
1364758468,14406,kirktrue,2023-10-19T01:15:42Z,"I tried to update the comment, but ended up confused on how to explain it. Because this looks like an existing issue that needs more investigation, I filed KAFKA-15641 to follow up."
1366182334,14406,kirktrue,2023-10-19T21:59:35Z,Both `KafkaConsumer` and `PrototypeAsyncConsumer` commit offsets asynchronously. I've filed KAFKA-15651 to review.
1366237460,14406,kirktrue,2023-10-19T23:37:40Z,"The issue is with how `OffsetsRequestManager.resetPositionsIfNeeded()` handles the call to `OffsetFetcherUtils.getOffsetResetTImestamp()`. Since `getOffsetResetTImestamp()` may throw an exception, `OffsetsRequestManager` should catch it and forward it to the application thread. But instead, `OffsetsRequestManager` allows the error to bubble up to the `ApplicationEventProcessor`, which will then bubble it up to `ConsumerNetworkThread`, which handles it by logging the error and exiting its `run()` method.

In fact, this appears to be the intended behavior since `OffsetsRequestManagerTest.testResetPositionsThrowsPreviousException()` explicitly tests that `OffsetsRequestManager.resetPositionsIfNeeded()` will throw the error directly at the caller.

What needs to change is for `OffsetsRequestManager.resetPositionsIfNeeded()` to catch the exception and enqueue an `ErrorBackgroundEvent` on the background event queue. In this way, the error will be processed  on the application thread as part of `poll()`."
1366237944,14406,kirktrue,2023-10-19T23:38:52Z,I don't see any existing unit tests which track the specific case that @junrao mentioned. I'm currently looking at the integration tests to see if those cover this case.
1366262621,14406,kirktrue,2023-10-20T00:00:34Z,"Yes. I've made the change that on `close()`, the application thread will wait for the network thread to finish its cleanup, up to the given timeout."
1366266506,14406,kirktrue,2023-10-20T00:06:58Z,"Unfortunately, there are no unit tests or integration tests which catch the offset reset case. I'll file a Jira to have one concocted."
1366267515,14406,kirktrue,2023-10-20T00:09:25Z,"I've made the following changes:

1. Now `OffsetsRequestManager` correctly enqueues errors from `getOffsetResetTimestamp()` onto the background event queue for use by the application thread
2. Renamed `OffsetFetcherTest`'s `testRestOffsetsAuthorizationFailure()` to  `testResetOffsetsAuthorizationFailure()`
3. Renamed `OffsetsRequestManagerTest`'s `testResetPositionsThrowsPreviousException()` to `testResetOffsetsAuthorizationFailure()` and updated its logic to ensure the expected error is present in the queue for the application thread to check in `poll()`"
1367260539,14406,junrao,2023-10-20T16:53:42Z,Could we just redirect the implement to the constructor above?
1367369975,14406,junrao,2023-10-20T18:27:18Z,Would it be clearer to rename refreshCommittedOffsetsIfNeeded to initWithCommittedOffsetsIfNeeded?
1367383773,14406,junrao,2023-10-20T18:44:13Z,"In `offsetFetcherUtils.getOffsetResetTimestamp()`, if there is no offset reset policy, currently we just ignore it. It seems that we should throw an exception in that case?"
1367498133,14406,kirktrue,2023-10-20T21:30:10Z,Filed KAFKA-15652 to ensure that we have tests that catch this case.
1367508455,14406,kirktrue,2023-10-20T21:49:59Z,Fixed.
1367508628,14406,kirktrue,2023-10-20T21:50:19Z,Done.
1367510616,14406,kirktrue,2023-10-20T21:53:31Z,Done.
1367515674,14406,junrao,2023-10-20T22:03:20Z,"This doesn't seem quite right. It's possible for a fetchRequest to return no data. In that case, if we don't wake up the the network thread, it may not be able to send the next fetch request for a long time."
1367516322,14406,junrao,2023-10-20T22:04:01Z,`wakeup` => `wakeupNetworkThread` ?
1367517181,14406,kirktrue,2023-10-20T22:05:48Z,"There is a default offset reset strategy which is configured via `ConsumerConfig`, passed to the `KafkaConsumer`, and then to the `SubscriptionState`, right? Why wouldn't we fall back to that if there's no overridden offset reset strategy for a particular partition?"
1367529409,14406,junrao,2023-10-20T22:33:26Z,"[auto.offset.reset](https://kafka.apache.org/documentation/#consumerconfigs_auto.offset.reset) has 3 options: `latest`, `earliest`, `none`. It's possible for a user to choose `none`, which means ""throw exception to the consumer if no previous offset is found for the consumer's group"". But in that case, `offsetFetcherUtils.getOffsetResetTimestamp` just ignores that partition and won't reset it forever. We need to throw an exception so that the user knows.

It's a bit of weird configuration since it means a consumer can't really consume for the very first time with this option. However, I tested it out with the existing consumer and it does throw an exception.
```
bin/kafka-console-consumer.sh --consumer-property auto.offset.reset=none --bootstrap-server localhost:9092 --topic test
[2023-10-20 15:30:07,088] ERROR Error processing message, terminating consumer process:  (kafka.tools.ConsoleConsumer$)
org.apache.kafka.clients.consumer.NoOffsetForPartitionException: Undefined offset with no reset policy for partitions: [test-0]
	at org.apache.kafka.clients.consumer.internals.SubscriptionState.resetInitializingPositions(SubscriptionState.java:711)
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateFetchPositions(KafkaConsumer.java:2459)
	at org.apache.kafka.clients.consumer.KafkaConsumer.updateAssignmentMetadataIfNeeded(KafkaConsumer.java:1243)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1198)
	at org.apache.kafka.clients.consumer.KafkaConsumer.poll(KafkaConsumer.java:1178)
	at kafka.tools.ConsoleConsumer$ConsumerWrapper.receive(ConsoleConsumer.scala:473)
	at kafka.tools.ConsoleConsumer$.process(ConsoleConsumer.scala:103)
	at kafka.tools.ConsoleConsumer$.run(ConsoleConsumer.scala:77)
	at kafka.tools.ConsoleConsumer$.main(ConsoleConsumer.scala:54)
	at kafka.tools.ConsoleConsumer.main(ConsoleConsumer.scala)
```"
1367535648,14406,kirktrue,2023-10-20T22:51:18Z,"I changed the `return null` to `throw new NoOffsetForPartitionException(partition)`. I re-ran the tests and nothing failed, so there seems to be another gap in the tests."
1367536108,14406,kirktrue,2023-10-20T22:52:38Z,"The enqueuing of other events will also call `wakeupNetworkThread`, so it shouldn't be a problem. But I went ahead and changed it as requested."
1367536128,14406,kirktrue,2023-10-20T22:52:43Z,Done
1367537583,14406,junrao,2023-10-20T22:56:48Z,Could we just return empty here instead of calling collectFetch() again since the caller is in a loop and can call this method to collect fetch again?
1367545811,14406,junrao,2023-10-20T23:12:39Z,Would it be clearer to rename `refreshCommittedOffsetsIfNeeded` to `initWithCommittedOffsetsIfNeeded`?
1367556045,14406,kirktrue,2023-10-20T23:25:15Z,"The `refreshCommittedOffsetsIfNeeded()` name is derived from existing code in `KafkaConsumer` that calls out to `ConsumerCoordinator.refreshCommittedOffsetsIfNeeded()`. If we change the method name in `PrototypeAsyncConsumer`, should we change `ConsumerCoordinator`'s corresponding method name too?"
1367557772,14406,junrao,2023-10-20T23:32:12Z,"Yes, in both cases, the method sets offsets for partitions in initializing state."
1367559124,14406,kirktrue,2023-10-20T23:37:47Z,"Yes, we could update `pollForFetches()` to simply return `Fetch.empty()` at the end.

However, there's code that intentionally blocks (for a little bit) waiting for fetch data to arrive in the buffer:

```java
try {
    fetchBuffer.awaitNotEmpty(pollTimer);
} catch (InterruptException e) {
    log.trace(""Timeout during fetch"", e);
} finally {
    timer.update(pollTimer.currentTimeMs());
}

return collectFetch();
```

Also, each loop through `poll()` executes `updateAssignmentMetadataIfNeeded()` before checking the fetch buffer for any data. That method does a lot of work (network I/O) that we'd ideally skip if we already have the data.

Perhaps we could update `awaitNotEmpty()` with a return flag so that the tail end of `pollForFetches()` can look something like this:

```java
try {
    if (fetchBuffer.awaitNotEmpty(pollTimer))
        return collectFetch();
} catch (InterruptException e) {
    log.trace(""Timeout during fetch"", e);
} finally {
    timer.update(pollTimer.currentTimeMs());
}

return Fetch.empty();
```"
1367562895,14406,junrao,2023-10-20T23:52:41Z,"Thanks for the explanation, Kirk. We can just leave the code as it is then."
1367565477,14406,kirktrue,2023-10-21T00:03:02Z,Updated and a bit of cleanup.
1370389793,14406,lianetm,2023-10-24T15:20:23Z,"I had missed a detail here. We're only implementing this with the default timeout, but not the overloaded one below, that has the timeout provided by the user. Is there a reason or is it just that we missed the latter one? (same for the `listTopics`)"
1370452938,14406,lianetm,2023-10-24T15:58:23Z,(I guess it will be all part of the integration with the metadata calls right?)
1370486899,14406,kirktrue,2023-10-24T16:21:12Z,Good call out. I don't know that the necessary topic plumbing code was written at that time. Would you mind filing a bug to resolve?
751663483,11390,ccding,2021-11-17T21:48:45Z,why do we remove `false` from the third parameter? the default is `true`
751670345,11390,ccding,2021-11-17T21:59:29Z,why do we remove the keyword `override` here?
751673278,11390,ccding,2021-11-17T22:04:21Z,is this change intentional?
754749020,11390,satishd,2021-11-23T01:26:46Z,"Good catch, fixed it. "
754749244,11390,satishd,2021-11-23T01:27:35Z,"Intellij auto formatted, fixed it. "
755410314,11390,junrao,2021-11-23T18:38:36Z,Is this change needed?
755430690,11390,junrao,2021-11-23T19:08:52Z,"Hmm, why do we want to eat the IOException? Ditto below."
755446235,11390,junrao,2021-11-23T19:32:40Z,"Does the parent CL have a super set of classpathes than the child? If so, is urls1 a subset of urls2?"
755523984,11390,junrao,2021-11-23T21:42:25Z,i the local log => in the local log
755548648,11390,junrao,2021-11-23T22:22:25Z,ListOffset returns the leaderEpoch for the localStartOffset. Could we just use that leaderEpoch? It will simplify the code below.
760490750,11390,junrao,2021-12-01T19:04:46Z,Could we add remoteLogManager to the javadoc too?
760491245,11390,junrao,2021-12-01T19:05:34Z,Does this need to be volatile?
760630665,11390,junrao,2021-12-01T22:49:37Z,indentation
760661057,11390,junrao,2021-12-01T23:54:44Z,Could this case just be handled below for EARLIEST_LOCAL_TIMESTAMP?
760671044,11390,junrao,2021-12-02T00:20:02Z,Could this be private?
761506462,11390,junrao,2021-12-02T22:15:31Z,"We need to find the first segment matching the timestamp from the remote segments, then the local segments."
761524592,11390,junrao,2021-12-02T22:50:20Z,fetchRemoteIndex() can take time. Could we call this asynchronously so that we don't block the request handler thread?
762123788,11390,junrao,2021-12-03T17:33:27Z,This may not matter for now. But it's probably better to use config.maxIndexSize as the max index size.
762126153,11390,junrao,2021-12-03T17:37:24Z,Should >= be > ?
762129356,11390,junrao,2021-12-03T17:42:27Z,Could we add a description for this class?
762138449,11390,junrao,2021-12-03T17:57:18Z,Should we close inputStream when done?
762171200,11390,junrao,2021-12-03T18:53:08Z,"Should we use a ShutdownableThread?
"
762176132,11390,junrao,2021-12-03T19:01:36Z,"The abort marker for offsets within the fetch range could be in subsequent log segments. So, we need to collect aborted transactions beyond the segment that the fetch offset resides."
762183906,11390,junrao,2021-12-03T19:15:33Z,Could we just call loadClass() once as in createRemoteStorageManager()?
762188249,11390,junrao,2021-12-03T19:23:09Z,Could we add topicIds to javadoc?
762189948,11390,junrao,2021-12-03T19:26:06Z,Could we reuse the method UnifiedLog.remoteLogEnabled()?
762191856,11390,junrao,2021-12-03T19:29:25Z,Should we just add REMOTE_LOG_METADATA_TOPIC_NAME to Topic.INTERNAL_TOPICS?
762193670,11390,junrao,2021-12-03T19:32:32Z,Could we just get rid of filteredLeaderPartitions as we have for followerTopicPartitions?
762195818,11390,junrao,2021-12-03T19:36:31Z,"Hmm, it seems that we only want to remove topicId when all partitions in the topic are removed?"
762200473,11390,junrao,2021-12-03T19:44:52Z,Does this need to be a concurrent map since there is no synchronization on access?
762202911,11390,junrao,2021-12-03T19:49:17Z,"Similar here, since fetch from remove storage can block, it would be better to do this asynchronously so that it doesn't block a request handler thread."
762206303,11390,junrao,2021-12-03T19:55:19Z,Should we also close all opened files in entries?
762325936,11390,junrao,2021-12-03T22:36:03Z,"To use array(), we have to first check hasArray(). It's probably simpler to do buffer.put(logHeaderBuffer)."
762355278,11390,junrao,2021-12-04T00:22:04Z,The code doesn't seem to check topicId.
762355609,11390,junrao,2021-12-04T00:23:48Z,Which request?
762357821,11390,junrao,2021-12-04T00:34:25Z,"When receiving OFFSET_MOVED_TO_TIERED_STORAGE, we know the follower's offset is below leader's local start offset. It seems that we could directly ask for leader's local start offset instead of calling fetchLatestOffsetFromLeader()."
762359179,11390,junrao,2021-12-04T00:41:01Z,"It's less verbose to do 

```
foreach { rlm =>
 ...
}

```"
762362355,11390,junrao,2021-12-04T00:58:51Z,reloadSegments => reloadSnapshots ?
762362423,11390,junrao,2021-12-04T00:59:17Z,Is this needed since the caller calls this already?
762364294,11390,junrao,2021-12-04T01:11:13Z,Where is the logic to rebuild RemoteLogMetadataSnapshotFile?
762366976,11390,junrao,2021-12-04T01:29:24Z,Loading from remote storage could be expensive. It would be useful to do this asynchronously so that the replication thread can make faster progress on other partitions.
762367379,11390,junrao,2021-12-04T01:32:13Z,Do we want to include the change in fetchRequest too?
762367939,11390,junrao,2021-12-04T01:36:36Z,"It's not really ""by local timestamp"". It's localStartOffset."
767417471,11390,satishd,2021-12-13T05:14:43Z,This is added to exclude any generated files by the tiered storage test runs.
767425395,11390,satishd,2021-12-13T05:41:05Z,"It checks if the given resource exists within this class loader first, it delegates to the parent classloader if it is not found in this class loader. "
767426396,11390,satishd,2021-12-13T05:44:05Z,It is a `val` for now and it is not needed to be declared as volatile and the compiler does not allow too. This will be updated in the upcoming changes to `var` and then it can be declared as volatile.
767427042,11390,satishd,2021-12-13T05:45:48Z,Updated with ShutdownableThread.
767427379,11390,satishd,2021-12-13T05:46:46Z,"This is an unused method. RemoteLogManager handles going through subsequent segments and collect all aborted transactions. These changes will be added in the next PR. 
I will remove this unused method for now. "
767427546,11390,satishd,2021-12-13T05:47:16Z,"Actually, that can still be improved. Updated with the changes to avoid creating `ClassLoaderAwareRemoteStorageManager` unnecessarily and calls going through classloader switching. "
767427717,11390,satishd,2021-12-13T05:47:50Z,I plan to do that later. It requires the metadata topic configs to be built and auto topic creation manager needs to handle this topic creation etc. 
767428241,11390,satishd,2021-12-13T05:49:29Z,"Good point. Changed it to use TopicPartition instead of topic as we need to maintain a different data structure for all the partitions and need to synchronize between them. Having ConcurrentMap of TopicPartion with topicId is simpler to manage here.
"
767428501,11390,satishd,2021-12-13T05:50:15Z,I will address in a followup PR.
767428782,11390,satishd,2021-12-13T05:51:13Z,This cleansup all the earlier loaded snapshots. It loads the newly created snapshot.
767430230,11390,satishd,2021-12-13T05:55:41Z,"`rlsMetadata = rlm.fetchRemoteLogSegmentMetadata(partition, epoch.get, leaderLocalLogStartOffset)` returns empty and this method throws an error back to the caller here. It retries again until the required `rlsMetadata` is available. `RemoteLogMetadataSnapshotFile` is loaded when assigning of partitions are done as part of `TopicBasedRemoteLogMetadataManager.assignPartitions()`."
767430483,11390,satishd,2021-12-13T05:56:24Z,I will address this in a followup PR.
767434452,11390,satishd,2021-12-13T06:08:20Z,Used `Option.foreach` here. Can you be more specific here?
767435783,11390,satishd,2021-12-13T06:11:55Z,I will address this in a followup PR.
767698400,11390,satishd,2021-12-13T12:08:40Z,Updated the javadoc.
768141997,11390,junrao,2021-12-13T21:38:33Z,Should we remove this?
772110652,11390,junrao,2021-12-20T06:38:21Z,Was this comment addressed?
772110800,11390,junrao,2021-12-20T06:38:43Z,The return value is not very intuitive. I'd expect a true return value to indicate that the request is handled successfully.
772110895,11390,junrao,2021-12-20T06:38:56Z,Could we add a comment on what this method does?
772110978,11390,junrao,2021-12-20T06:39:11Z,"In the else case, should we truncate all local log?"
772111057,11390,junrao,2021-12-20T06:39:21Z,"Change from

`        replicaMgr.remoteLogManager.foreach(rlm => {
             }
          ) 
`
to 

`        replicaMgr.remoteLogManager.foreach{rlm =>
          }
`
Ditto in a few other places."
772111223,11390,junrao,2021-12-20T06:39:45Z,"Hmm, the remote data could end at leaderLocalLogStartOffset - 1. In that case, we won't find a corresponding rlsMetadata."
772111916,11390,junrao,2021-12-20T06:41:23Z,"This is kind of awkward and inefficient. Could we add a better API to avoid the trial and error approach? For example, we could have an API that waits for the remote log segment metadata up to offset leaderLocalLogStartOffset - 1 becoming available with leader epoch less than or equal to currentLeaderEpoch."
772111987,11390,junrao,2021-12-20T06:41:35Z,It's awkward to stream into a temp file only to load it back in memory.
773121075,11390,satishd,2021-12-21T13:08:06Z,"Updated the comment, we do not really need to have a check here. Let me know if I am missing anything here. "
773121400,11390,satishd,2021-12-21T13:08:36Z,Addressed it in the latest commit.
773121903,11390,satishd,2021-12-21T13:09:20Z,This is inline with `handleOutOfRangeError` contract. I am fine with the suggested change but it is good to have similar semantics to `handleOutOfRangeError` method too for uniformity.
773122505,11390,satishd,2021-12-21T13:10:13Z,Good point. I updated it to address this scenario too in the latest commit. 
773123927,11390,satishd,2021-12-21T13:12:13Z,Changed it to use the leader epoch of leader's local-log-start-offset and find the respective earlier epoch for (leader's local-log-start-offset -1) from the leader. 
773143045,11390,satishd,2021-12-21T13:39:14Z,Filed https://issues.apache.org/jira/browse/KAFKA-13560 to add the suggested improvement.
773499214,11390,wyuka,2021-12-21T23:16:35Z,Shouldn't this method be called `setRemoteLogManager(...)`?
778990618,11390,ccding,2022-01-05T17:08:02Z,`local-log-start-timestamp` instead of `local-log-start-offset` in the comment?
778999529,11390,ccding,2022-01-05T17:20:28Z,"```suggestion
  // This is the ealiest log start offset in the local log. (KIP-405).
```"
779060541,11390,junrao,2022-01-05T18:52:06Z,"When we hit OffsetOutOfRangeException, it's possible that remote storage is enabled. In that case, we also need to rebuild the remote log metadata."
779089066,11390,ccding,2022-01-05T19:40:31Z,Is it possible to make `Topic.isInternal(topicPartition.topic())` to return true if `TopicBasedRemoteLogMetadataManagerConfig.REMOTE_LOG_METADATA_TOPIC_NAME.eq(topicPartition.topic()))`? then we can get rid of the `TopicBasedRemoteLogMetadataManagerConfig.REMOTE_LOG_METADATA_TOPIC_NAME.eq(topicPartition.topic()))` check
779142706,11390,ccding,2022-01-05T21:17:24Z,do we need to log if there are any exceptions? or throw the exception out?
779158936,11390,ccding,2022-01-05T21:47:49Z,is there any reason for not using the existing suffixes? https://github.com/apache/kafka/blob/daaa9dfb54b86ccb93cc2018b374476dcb0beed1/core/src/main/scala/kafka/log/LocalLog.scala#L550-L559
779162870,11390,ccding,2022-01-05T21:55:20Z,Should the error message be `cleaned up`?
779166585,11390,ccding,2022-01-05T22:02:35Z,why do we use `CoreUtils.tryAll` in one case and use `Array().foreach( try catch )` in the other case?
779169864,11390,ccding,2022-01-05T22:09:02Z,"we can use `val DirName = ""remote-log-index-cache""` here"
779171925,11390,ccding,2022-01-05T22:13:11Z,what is the trade-off between using a daemon thread and using Kafka scheduler?
779175283,11390,ccding,2022-01-05T22:20:08Z,"```suggestion
        Utils.atomicMoveWithFallback(tmpIndexFile.toPath, indexFile.toPath, false)
```
I think we don't need to flush the parent dir. It doesn't matter if the cache file is not renamed during an unclean shutdown."
779177001,11390,junrao,2022-01-05T22:23:44Z,space after if.
779177082,11390,ccding,2022-01-05T22:23:57Z,do we need to log/handle exception caused by fetchAndCreateIndex
779191474,11390,junrao,2022-01-05T22:55:27Z,"TopicBasedRemoteLogMetadataManager independently updates the metadata state from the tier topic. When we make the `rlm.fetchRemoteLogSegmentMetadata` call, how does it make sure that it has caught up enough records from the tier topic including the segment covering the requested offset?"
779191948,11390,ccding,2022-01-05T22:56:38Z,"wanted to double check you mean `log.remoteLogEnabled()` or `!log.remoteLogEnabled()` here, because it is `filterNot`"
779192232,11390,ccding,2022-01-05T22:57:20Z,"`log.remoteLogEnabled()` already checks internal and equals REMOTE_LOG_METADATA_TOPIC_NAME, do we need to check again here?"
779196455,11390,junrao,2022-01-05T23:07:36Z,"endOffset is exclusive. So, we could just use leaderLogStartOffset."
779199654,11390,junrao,2022-01-05T23:15:45Z,Should we use foreach instead of map?
779200299,11390,junrao,2022-01-05T23:17:30Z,Should we flush the leader epoch file at the end?
779213401,11390,junrao,2022-01-05T23:55:08Z,Should we write the producer snapshot to a temp file first and then rename?
779216639,11390,junrao,2022-01-06T00:05:34Z,"It possible that the endOffset in rlsMetadata is larger than leaderLocalLogStartOffset. If we fetch the local log from leaderLocalLogStartOffset, duplicated tnx could be added to producerStateManager. It's better to start fetching from rlsMetadata.endOffset + 1."
779223560,11390,junrao,2022-01-06T00:26:56Z,This is the top level error. UNKNOWN_TOPIC_ID and INCONSISTENT_TOPIC_ID are partition level errors and don't need to be handled here.
779233183,11390,junrao,2022-01-06T00:57:41Z,Why does this method need to be protected instead of private? Ditto for fetchOffsetAndBuildRemoteLogAuxState.
784750507,11390,satishd,2022-01-14T10:57:46Z,We have not yet made `TopicBasedRemoteLogMetadataManagerConfig.REMOTE_LOG_METADATA_TOPIC_NAME` as internal topic. I plan to make this change once it is created as an internal topic. 
784751134,11390,satishd,2022-01-14T10:58:44Z,we do not want to log here. It should be handled by the invoker. 
784753702,11390,satishd,2022-01-14T11:02:40Z,There is no specific reason other than having a shorter suffix. I do not have a strong opinion on that. I am fine with eitherways. 
784764008,11390,satishd,2022-01-14T11:18:34Z,"In the earlier case, we want the exception to be thrown with the exception as mentioned in `CoreUtils.tryAll`. But in this case, we want to catch each invocation and ignore it instead of collecting like CoreUtils.tryAll. "
784768268,11390,satishd,2022-01-14T11:25:20Z,"No, we want to throw it back so that invoker can handle it. "
784770314,11390,satishd,2022-01-14T11:28:57Z,Nice catch!!
784772939,11390,satishd,2022-01-14T11:33:21Z,"Good point, I am making a simpler check here to use `filter`. 
```
partitions.filter(partition => partition.log.exists(log => log.remoteLogEnabled()))
```"
784793287,11390,satishd,2022-01-14T12:08:32Z,We do not need to flush it as`cache.assign` already flushes the entry. 
784793923,11390,satishd,2022-01-14T12:09:39Z,"Sure, it is good to do that. Addressed it in the latest commit."
784801502,11390,satishd,2022-01-14T12:22:42Z,"If it does not catchup then it returns `Optional.EMPTY`, and that we throw a RemoteStorageException. As the caller receives RemoteStorageException, it will be retried again."
785875887,11390,satishd,2022-01-17T11:19:35Z,"In that case, I prefer to use '>' instead of '>=' like below.  This is more clear and easy to read. I have also added a comment to talk about a case in leader epoch gap.

```
if (earlierEpochEndOffset.endOffset > highestOffsetInRemoteFromLeader)
```"
786517360,11390,satishd,2022-01-18T08:36:13Z,"I refactored `assign` whether to flush or not with default as true for backward compatibility. In this case, I assign all the entries without flush, and flush to the file at the end."
786518882,11390,satishd,2022-01-18T08:38:15Z,"We encounter `OffsetOutOfRangeException` only when the offsets are beyond the range of [logStartOffset, logEndOffset]. Why do we need to build remote log metadata in that case? I may be missing something here. "
786736586,11390,satishd,2022-01-18T13:01:15Z,Good point! made the respective changes in the latest commit. 
788215466,11390,junrao,2022-01-19T22:58:55Z,space after if
788253183,11390,junrao,2022-01-20T00:23:47Z,"If we hit OffsetOutOfRangeException, we call fetchEarliestOffsetFromLeader() inside fetchOffsetAndApplyFun(), which fetches the localStartOffset. We then truncate the whole log and start fetching from localStartOffset. If the leader has data in remote storage, the follower won't have the remote log metadata for consumer fetch and won't have the same state (e.g. producer state) as the leader."
788269196,11390,junrao,2022-01-20T01:02:58Z,fetchEarlierEpochEndOffset(3) => fetchEarlierEpochEndOffset(2) ?
788270782,11390,junrao,2022-01-20T01:07:16Z,"It's possible for the local data to overlap a bit with what's in the remote storage. So, leaderLocalLogStartOffset - 1 is not necessarily the highest offset in remote storage. Could we name highestOffsetInRemoteFromLeader more properly?"
788279027,11390,junrao,2022-01-20T01:29:23Z,We need to return nextOffset to the caller so that fetchOffsetAndApplyFun() could set the next fetch offset to this.
789307564,11390,wyuka,2022-01-21T02:46:58Z,"We can use

```
Option(epochs.higherKey(epoch))
```

instead"
789308168,11390,wyuka,2022-01-21T02:48:04Z,"We can use
```
Option(epochs.lowerKey(epoch))
```
instead"
791322611,11390,satishd,2022-01-25T03:10:39Z,"Sure, as we discussed offline, I added the approach to keep OffsetOutOfRange like earlier, which is to fetch the log-start-offset and it may get a response of OffsetMovedToTieredStorage if it tries to fetch from log-start-offset and it is moved to tiered storage. "
791957579,11390,yyang48,2022-01-25T17:28:55Z,"Just curious, why we need to read this value from the buffer and discard it immediately?"
791968984,11390,yyang48,2022-01-25T17:42:00Z,"I'm not sure whether this buffer is big enough or not?

From the line 58: the size of the buffer is the record content size + 12.

However, from the line 34, the logHeaderBuffer is: 17.

In the line 59, if we put the entire logHeaderBuffer to this buffer, in the worse case, we need 5 bytes more space in the capacity. 

Is this correct?"
796465473,11390,satishd,2022-02-01T10:40:54Z,Good point. It is not really needed. Looks like it was added earlier to log offset but the log was removed. 
796479255,11390,satishd,2022-02-01T10:58:08Z,"No, we do not really need more space here. I updated with more comments in the code on how it works. This is aligned with the existing code in other places like `FileChannelRecordBatch`.

payload : LOG_OVERHEAD + 4 + 1 (magic:byte) + reocrd-content
LOG_OVERHEAD = 8(offset:long) + 4(size:int) = 12
`size` = 4 + 1(magic) + record-content-size. 

`logHeaderBuffer` is read until magic. 
That is why the complete buffer-size is 12(LOG_OVERHEAD) + size. We do not count ""4 + 1(magic)"" here as it is already taken into account with size_read_from_header
"
804020896,11390,junrao,2022-02-10T19:07:26Z,Should we complete the javadoc? Ditto for fetchEarliestOffsetFromLeader().
804023336,11390,junrao,2022-02-10T19:10:45Z,Could we add the javadoc for this one?
804025075,11390,junrao,2022-02-10T19:13:07Z,"Is ""related to offsets out of rage"" true? Also, since we explained this error, do we need the error code?"
804026471,11390,junrao,2022-02-10T19:15:03Z,"Is ""Handle the offset out of range error"" true?"
804030400,11390,junrao,2022-02-10T19:20:34Z,"Could we make it clear that the reason that we don't need to backoff and retry is because we move the partition to a failed state? Also, could we put true in a separate line?  Ditto for handleOutOfRangeError().

"
804042686,11390,junrao,2022-02-10T19:37:27Z,"The above long comment doesn't quite fit into remote storage. We could make it more general that covers both cases. If that's too complicated, perhaps have a separate method just for handling remote storage."
804145767,11390,junrao,2022-02-10T21:59:14Z,Could we change earliestOrLatest to sth more generic now that it can have 3 values?
804149120,11390,junrao,2022-02-10T22:00:51Z,There is no leader epoch before 0.10. So the leader epoch can just be -1.
804186063,11390,junrao,2022-02-10T22:32:48Z,"This call writes another snapshot, which is unnecessary. Perhaps we could just do producerStateManager.truncateAndReload()?"
804191359,11390,junrao,2022-02-10T22:38:43Z,"If we get here, it means the leader has started tiering the data but the follower hasn't received the remoteStorageEnable config yet. It seems that we should backoff and retry the same offset instead of just fetching from leaderLocalLogStartOffset?"
804196978,11390,junrao,2022-02-10T22:45:04Z,This is not just an offset index.
804200819,11390,junrao,2022-02-10T22:49:26Z,Why is this called CleanableIndex? It's bit confusing given log cleaner. Maybe sth like BaseIndex?
804201757,11390,junrao,2022-02-10T22:50:32Z,time stamp => timestamp
804203296,11390,junrao,2022-02-10T22:52:28Z,space after if
804223128,11390,junrao,2022-02-10T23:14:55Z,"Hmm, it's kind of weird to set the class loader on each call. Is this needed? Do other remote storage classes just use the classloader for RLM and RLMM?"
804234045,11390,junrao,2022-02-10T23:27:08Z,The original configs in config may contain implementation specific properties. How do we pass those to RLMM and RLM through RemoteLogManager?
804239401,11390,junrao,2022-02-10T23:33:20Z,It seems that we never add to topicPartitionIds?
804245130,11390,junrao,2022-02-10T23:43:38Z,"In the local case, if all the messages in the remote storage have larger timestamps, it seems that we return the timestamp of the first message."
804245833,11390,junrao,2022-02-10T23:45:16Z,Could we call maybeEpoch.get() once in the loop?
804250739,11390,junrao,2022-02-10T23:56:46Z,previousEpoch?
804250861,11390,junrao,2022-02-10T23:57:03Z,nextEpoch?
804250995,11390,junrao,2022-02-10T23:57:19Z,epochEntry?
804251574,11390,junrao,2022-02-10T23:58:41Z,Should previousEpoch be initialize to None?
804252215,11390,junrao,2022-02-11T00:00:12Z,This seems unused?
804252997,11390,junrao,2022-02-11T00:02:09Z,It's not really local timestamp.
804265970,11390,junrao,2022-02-11T00:35:38Z,"Should we load up existing files to entries during init()? Otherwise, it's not clear when they will be cleaned up."
804266325,11390,junrao,2022-02-11T00:36:47Z,Should we close the indexes too?
804266637,11390,junrao,2022-02-11T00:37:38Z,This is never read?
804267665,11390,junrao,2022-02-11T00:40:18Z,Is this safe? The entry could be cleaned immediately after this check.
804268708,11390,junrao,2022-02-11T00:43:07Z,rewind() is typically used after the buffer is written. Should we use clear()?
804270391,11390,junrao,2022-02-11T00:47:41Z,"> To use array(), we have to first check hasArray(). It's probably simpler to do buffer.put(logHeaderBuffer).

Was this comment addressed?"
804274827,11390,junrao,2022-02-11T01:00:19Z,"This is not supported yet, right? Should we throw an exception?"
804275488,11390,junrao,2022-02-11T01:02:03Z,Should we just throw UnsupportedException?
804277844,11390,junrao,2022-02-11T01:09:06Z,No need to pass in reloadFromCleanShutdown since it's always false?
804280439,11390,junrao,2022-02-11T01:16:55Z,This seems an indirect way to check message version. Should we just check that explicitly?
804281396,11390,junrao,2022-02-11T01:19:46Z,"Hmm, localLogStartOffset could change after the call the remoteLogManager."
804843541,11390,junrao,2022-02-11T17:03:42Z,"For other broker side plugins (e.g. Authorizer), we never needed a special class loader. Why do we need this for the remote storage plugin?"
812537683,11390,satishd,2022-02-23T03:44:29Z,"This is to avoid library conflicts directly or indirectly with the existing libraries in the system classpath. This classloader loads the libraries in the given paths. Whenever a class needs to be loaded, it delegates to the system classloader only when it is not found in the configured paths. RSM or RLMM providers can have all the required libraries in given directories and add them as classpath for RSM or RLMM. This provides isolation with the libraries in the system classpath. "
813156759,11390,junrao,2022-02-23T17:48:17Z,"Yes, I understand the intent. However, there are quite a few pluggable components on the broker side right now. Sorting out the classpath dependencies among all of them seems quite complicated. Another way to solve the issue is through shading. A potential conflicting library could be renamed to a different package."
814028646,11390,satishd,2022-02-24T16:00:43Z,Updated the statement and the error code is removed.
814031095,11390,satishd,2022-02-24T16:03:07Z,"Sure, I also updated the method to return true if it was able to handle it."
814036592,11390,satishd,2022-02-24T16:09:03Z,`responsePartition.leaderEpoch` returns -1 with <= KAFKA_0_10_1_IV2 version. No need to explicitly set it as -1. 
814039902,11390,satishd,2022-02-24T16:12:30Z,`RemoteLogManagerConfig#remoteStorageManagerProps()` and `RemoteLogManagerConfig#remoteLogMetadataManagerProps()` return respective properties which are used while configuring RSM and RLMM in `RemoteLogManager#configureRSM()` and `RemoteLogManager#configureRLMM()`. 
814041397,11390,satishd,2022-02-24T16:13:59Z,This code missed while merging other changes. Addressed with the latest commit.
814045120,11390,satishd,2022-02-24T16:17:57Z,Changed it to `Version 8 enables listing offsets by local log start offset.`
814046367,11390,satishd,2022-02-24T16:19:15Z,We do not need to explicitly close them as they are already closed inside `deleteIfExists()` method.
814050149,11390,satishd,2022-02-24T16:23:14Z,"This is used in doWork(), updated with the latest commit. "
814127632,11390,satishd,2022-02-24T17:53:23Z,Good catch!! I will address it. 
814640948,11390,kowshik,2022-02-25T10:10:57Z,"> This class represents a common abstraction for operations like delete and rename of the index files.

This class is slim in functionality, and I don't feel there is any real benefit for introducing this.
Also for the future, it is not clear to me what operations can be included in this class, and which ones can't be.
I feel that the earlier design without this base class was simpler.
Are we planning to add new functionality in the future into this class?
"
814642656,11390,kowshik,2022-02-25T10:13:11Z,"Lets rename `x` to something more readable, ex: `index`."
814658812,11390,kowshik,2022-02-25T10:35:11Z,"s/related to offset moved to tiered storage/OFFSET_MOVED_TO_TIERED_STORAGE

Can we also log the topicPartition?"
814660162,11390,kowshik,2022-02-25T10:37:09Z,"This method `fetchOffsetAndApplyTruncateAndBuild` is currently doing a number of things, which is clear from the method name. It will be hard to cover all test cases in unit test. So, it is better if its simplified.
"
814668927,11390,kowshik,2022-02-25T10:50:08Z,"Here to build the remote log aux state we only need the leader local log start offset, right?
In such a case, I think it gets complicated if we try to repurpose `fetchOffsetAndApplyTruncateAndBuild` here. Can we just introduce a separate method that would just attempt to get the leader's local log start offset, and pass it into `buildRemoteLogAuxState`?
"
814678429,11390,kowshik,2022-02-25T11:04:36Z,"This method is doing a lot of things, and it is worthwhile thinking about how to simplify it. In its current form, it is going to be hard to test it.
"
814981129,11390,junrao,2022-02-25T18:00:33Z,"I was referring to ""the timestamp will be Message.NoTimestamp"". It should be the timestamp of the first message."
818028561,11390,junrao,2022-03-02T19:29:10Z,"The purpose of a special class loader is to address potential class conflicts. If the same class exists btw the plugin and Kafka, by switching the class loader, it seems that we don't know deterministically which version of the class will be loaded, which can be confusing."
898097965,11390,divijvaidya,2022-06-15T15:02:56Z,The comment does not match the functionality for the function here. This implementation only performs delete.
981140295,11390,divijvaidya,2022-09-27T11:51:53Z,"nit

I would suggest to move `TestUtils.resource` to `Utils` and use that to mimic Java's try-with-resource in scala."
981150239,11390,divijvaidya,2022-09-27T12:02:42Z,"This could be refactored into a method in LeaderEpochCache, perhaps with signature `assign(Seq(EpochEntry))`"
981152254,11390,divijvaidya,2022-09-27T12:04:55Z,"Do we want to clear the existing cache first using `cache.clearAndFlush()`? Asking because (and correct me if I am wrong) in the case of uncleanLeaderElection, this follower may have log lineage belonging to previous leader. The new leader may have a different log lineage which doesn't contain epochs present in old lineage. Those epochs absent missing in new lineage need to be removed from the cache."
983849860,11390,divijvaidya,2022-09-29T17:39:05Z,"This offset is the `last-tiered-offset + 1` and not the `local-log-start-offset`. Please correct me if I am wrong but this is inconsistent with what is described in the KIP-405 where we say:
> We prefer to go with the local log start offset as the offset from which follower starts to replicate the local log segments for the reasons mentioned above."
985706396,11390,satishd,2022-10-03T12:06:00Z,"This offset is not `last-tiered-offset + 1` but `previousOffsetToLeaderLocalLogStartOffset` which is `leaderLocalLogStartOffset - 1`.  This is aligned with what we mentioned in KIP-405.

You can take a look at the usage of `maybeRlsm` which is 
```
val maybeRlsm = rlm.fetchRemoteLogSegmentMetadata(partition, targetEpoch, previousOffsetToLeaderLocalLogStartOffset)
```"
1025959204,11390,showuon,2022-11-18T03:46:54Z,nit: add a space before `new RcordHeader`
1026063322,11390,showuon,2022-11-18T06:18:22Z,nit: remoteStorageSystemEnable -> remoteStorageSystemEnable[d]
1026064974,11390,showuon,2022-11-18T06:22:09Z,nit: Remote logging -> Remote log
1026065351,11390,showuon,2022-11-18T06:22:57Z,Should we also check `__cluster_metadata` topic? It isn't checked in `isInternal`
1026079869,11390,showuon,2022-11-18T06:48:22Z,nit: remoteStorageSystemEnable -> remoteStorageSystemEnable[d]
1026086446,11390,showuon,2022-11-18T07:01:21Z,"There will be background thread to delete them later, right? I don't think this is necessary."
1026087680,11390,showuon,2022-11-18T07:03:42Z,Where's the implementation?
1026095589,11390,showuon,2022-11-18T07:18:10Z,"This `computIfAbsent` method might do 3 `fetchAndCreateIndex` for all 3 indexes, which will take many time, inside the lock. Could we fetch them before we lock the `entries`?"
1026097879,11390,showuon,2022-11-18T07:21:53Z,Forgot to add javadoc for 3 parameters
1026112608,11390,showuon,2022-11-18T07:44:38Z,"The implementation of `lookupTimestamp` above is also returning the `the timestamp of the first message.`. So,  only Javadoc needs to be updated"
1026121749,11390,showuon,2022-11-18T07:57:35Z,nit: additional comma at the end
1026133917,11390,showuon,2022-11-18T08:14:42Z,@return  updated partition fetch state
1026146723,11390,showuon,2022-11-18T08:28:16Z,Agree. There must be something wrong here. Error log is also necessary.
1026150319,11390,showuon,2022-11-18T08:32:22Z,nit: the indent is not quite right here
1026173480,11390,showuon,2022-11-18T08:57:36Z,nit: give a change -> chance?
1026175979,11390,showuon,2022-11-18T09:00:20Z,ditto
1026242726,11390,showuon,2022-11-18T10:02:31Z,"I think it should `mock the segment of offset 0-4 moved to remote store.`, right?"
1026243503,11390,showuon,2022-11-18T10:03:18Z,ditto
1032869910,11390,satishd,2022-11-27T05:35:05Z,@junrao We were doing that earlier but you suggested to do normal truncation until local-log-start-offset as mentioned [here](https://github.com/apache/kafka/pull/11390#discussion_r772110978). 
1032869957,11390,satishd,2022-11-27T05:36:00Z,This is addressed with the latest changes. 
1032870167,11390,satishd,2022-11-27T05:38:00Z,We can keep it simple for now as you suggested. Removed BaseIndex for now but we may add it later if needed.
1032870377,11390,satishd,2022-11-27T05:39:57Z,Adding space fails with style-check.
1032870521,11390,satishd,2022-11-27T05:41:05Z,This is the existing convention used in other properties like unclean.leader.election.enable etc
1032870562,11390,satishd,2022-11-27T05:41:34Z,This is the existing convention used in other properties like unclean.leader.election.enable etc
1032870593,11390,satishd,2022-11-27T05:42:27Z,These are not loaded as entries marked for deletion. It is good to delete them any pending deletions when a broker is started. 
1032870604,11390,satishd,2022-11-27T05:42:41Z,It is addressed with the latest set of commits.
1032871126,11390,satishd,2022-11-27T05:49:04Z,Changed the text to make it more clear.
1032871130,11390,satishd,2022-11-27T05:49:09Z,Changed the text to make it more clear.
1032944075,11390,satishd,2022-11-27T14:45:54Z,leaderepochcache is already cleaned up by the earlier `truncateFullyAndStartAt` call.
1032945780,11390,satishd,2022-11-27T14:57:26Z,I will address in the next commit. 
1033092123,11390,showuon,2022-11-28T03:31:34Z,Any reason why we don't want to `extends Closeable` here?
1033093221,11390,showuon,2022-11-28T03:34:54Z,"I saw we already updated the `_localLogStartOffset` in `maybeIncrementLogStartOffset` method. If you know there are some other places we also need to update it, maybe you can make it much clear in the comment. "
1033097357,11390,showuon,2022-11-28T03:45:59Z,Fair enough
1033136878,11390,showuon,2022-11-28T05:22:04Z,should we lock it?
1033141283,11390,showuon,2022-11-28T05:32:56Z,"Thanks for the update, but I think we still lock for the period of loading 3 index files. How about this:
```java
if (!entries.containsKey(remoteLogSegmentMetadata.remoteLogSegmentId().id())) {
        val startOffset = remoteLogSegmentMetadata.startOffset()
        // uuid.toString uses URL encoding which is safe for filenames and URLs.
        val fileName = startOffset.toString + ""_"" + uuid.toString + ""_""

        val offsetIndex: LazyIndex[OffsetIndex] = loadIndexFile(fileName, UnifiedLog.IndexFileSuffix,
          rlsMetadata => remoteStorageManager.fetchIndex(rlsMetadata, IndexType.OFFSET),
          file => {
            val index = LazyIndex.forOffset(file, startOffset, Int.MaxValue, writable = false)
            index.get.sanityCheck()
            index
          })

        val timeIndex: LazyIndex[TimeIndex] = loadIndexFile(fileName, UnifiedLog.TimeIndexFileSuffix,
          rlsMetadata => remoteStorageManager.fetchIndex(rlsMetadata, IndexType.TIMESTAMP),
          file => {
            val index = LazyIndex.forTime(file, startOffset, Int.MaxValue, writable = false)
            index.get.sanityCheck()
            index
          })

        val txnIndex: TransactionIndex = loadIndexFile(fileName, UnifiedLog.TxnIndexFileSuffix,
          rlsMetadata => remoteStorageManager.fetchIndex(rlsMetadata, IndexType.TRANSACTION),
          file => {
            val index = new TransactionIndex(startOffset, file)
            index.sanityCheck()
            index
          })

          // start lock here
          lock synchronized {
                entries.putIfAbsent(remoteLogSegmentMetadata.remoteLogSegmentId().id(), new Entry(offsetIndex, timeIndex, txnIndex));
          }
}
```

WDYT?"
1033492033,11390,satishd,2022-11-28T12:38:45Z,"Added Closeable as part of this PR, and we do not really need it with the latest changes.. "
1033493533,11390,satishd,2022-11-28T12:40:19Z,Locking is not really needed here as `init` method is invoked only when RemoteIndexCache instance is getting initialized. 
1033515905,11390,satishd,2022-11-28T13:02:53Z,"This creates race between writers and readers of these files which may fail readers with IO errors. 

For ex: The race can occur in 3 ways
1. Multiple writers writing those indexes concurrently
2. One of them will update the entry with the existing file 
   But that file could have been over written in step-1
3. When readers start reading, those file might have been overwritten in step-1. "
1033516642,11390,satishd,2022-11-28T13:03:35Z,"Please ignore the stale comment, removed with the latest commit."
1034340498,11390,satishd,2022-11-29T06:20:02Z,"https://issues.apache.org/jira/browse/KAFKA-13560 covers to get indexes or data in async manner for critical paths, we can cover as part of that JIRA. wdyt?"
1034345625,11390,showuon,2022-11-29T06:29:16Z,Sounds good to me! I'll resolve this comment.
1034345639,11390,satishd,2022-11-29T06:29:18Z,Good point. This is clarified in later comments. 
1041219212,11390,satishd,2022-12-06T16:50:10Z,"It does not really create another snapshot as it has the below check in ProducerStateManager#takeSnapshot. Also, it is good to use `log.loadProducerState` as any extra logic that may be added does not need to be added separately here.
```
if (lastMapOffset > lastSnapOffset) {
```"
1041821188,11390,showuon,2022-12-07T06:52:56Z,nit: javadoc doesn't include all params
1041842874,11390,showuon,2022-12-07T07:22:48Z,"nit: I know we don't test them, but could we add some comments here? Thanks."
1042140948,11390,divijvaidya,2022-12-07T12:24:53Z,"Consider the following scenario:
1. Leader is archiving to tiered storage and has a follower.
2. Follower has caught up to offset X (exclusive).
3. While follower is offline, leader moves X to tiered storage and expires data locally till Y, such that, leaderLocalLogStartOffset > X and Y = leaderLocalLogStartOffset. Meanwhile, X has been expired from tiered storage as well. Hence, X < globalLogStartOffset as well. Now, there could be a scenario where globalLogStartOffset > leaderLocalLogStartOffset because segments has been expired from remote but not from local.
3. Follower comes online and tries to fetch X from leader, leader throws moved to tiered storage exception.
4. Follower moves to buildAux state and tries to fetch the metadata. The metadata may not exist since the segment has been deleted in remote storage and we will get an error.

This could be addressed at replica manager where it could detect if the remote segments have been deleted and accordingly throw an out of bound instead of move to tiered storage exception, but we should also add a defensive handling check here. In the above scenario, we should directly move to truncation instead of build aux state.

The defensive check could be `&& leaderLocalLogStartOffset > leaderLogStartOffset` over here.

Also, please add a test for this scenario.
"
1043330520,11390,satishd,2022-12-08T13:04:03Z,This is addressed with the latest commit.
1043330573,11390,satishd,2022-12-08T13:04:07Z,This is addressed with the latest commit.
1043981337,11390,junrao,2022-12-09T01:39:11Z,The comment is a bit confusing since 4 bytes + magic doesn't add up to LOG_OVERHEAD.
1043981867,11390,junrao,2022-12-09T01:40:03Z,Same question here. It seems that it's better to make it clear that AbstractIndex is closable?
1043982291,11390,junrao,2022-12-09T01:41:10Z,Does this need to be volatile?
1043982526,11390,junrao,2022-12-09T01:41:43Z,Should we reset HWM too?
1043982906,11390,junrao,2022-12-09T01:42:42Z,Should this be in the storage module like ClassLoaderAwareRemoteLogMetadataManager?
1043983132,11390,junrao,2022-12-09T01:43:19Z,I guess we will add the logic to delete the remote data later?
1043983263,11390,junrao,2022-12-09T01:43:42Z,target offset => start offset?
1043983592,11390,junrao,2022-12-09T01:44:31Z,"Do we need to make ""all the messages in the remote storage have larger timestamps"" a special case here? It seems the last option covers that case already."
1043983792,11390,junrao,2022-12-09T01:45:04Z,Could we just assign to maybeEpoch directly and get rid of startingOffsetEpoch?
1043984064,11390,junrao,2022-12-09T01:45:35Z,"If no message has timestamp, we will fall to here. This doesn't seem to implement what the comment says."
1043984353,11390,junrao,2022-12-09T01:46:29Z,"It would be useful to make logging a complete sentence, So sth like debug(s""Received error ${Errors.OFFSET_MOVED_TO_TIERED_STORAGE} at "" +
                    s""fetch offset: ${currentFetchState.fetchOffset} for "" + s""topic-partition: $topicPartition"")"
1043984841,11390,junrao,2022-12-09T01:47:46Z,haven't => hasn't
1043985073,11390,junrao,2022-12-09T01:48:20Z,"We already stopped processing requests at this point, right?"
1043985230,11390,junrao,2022-12-09T01:48:41Z,This seems unused?
1043985494,11390,junrao,2022-12-09T01:49:22Z,Could this be private?
1043985686,11390,junrao,2022-12-09T01:49:51Z,Should fetchEarlierEpochEndOffset(2) be fetchEarlierEpochEndOffset(90)?
1043985886,11390,junrao,2022-12-09T01:50:14Z,"Could we use complete sentence? e.g. s""Active producers with size of ${log.producerStateManager.activeProducers.size}, ""

s""logStartOffset is $leaderLogStartOffset and logEndOffset is $nextOffset"")"
1043986046,11390,junrao,2022-12-09T01:50:34Z,"Yes, thinking about this more. I am not sure that truncating all local logs is the right thing to do. If we do that, the replica's log may not give a complete view of the data. We could get into this situation when (1) the topic level remote storage flag propagation is delayed; (2) incorrect configuration by the user (e.g. remoteStorageSystemEnable not enabled on all brokers). In both cases, it seems a better strategy is to error out and keep retrying. In the case (1), the issue will be resolved automatically when the topic level flag is propagated to this broker. In the case (2), this issue will be resolve after the user fixes the configuration."
1043987311,11390,junrao,2022-12-09T01:51:30Z,It seems no non-test caller sets flushToFile to false?
1043989048,11390,junrao,2022-12-09T01:52:25Z,truncateOnFetch is always true?
1043989337,11390,junrao,2022-12-09T01:52:47Z,add newline after
1043989757,11390,junrao,2022-12-09T01:53:22Z,Could this be private?
1043990559,11390,junrao,2022-12-09T01:54:23Z,Could we add a description of the class?
1044740416,11390,junrao,2022-12-09T18:47:37Z,Could we use case statement to avoid using unnamed references?
1044741168,11390,junrao,2022-12-09T18:48:41Z,"Hmm, I am not sure that I understand the test. The snapshot corresponding to offset 5 should still be there, right? Where did we delete it?"
1044757469,11390,junrao,2022-12-09T19:11:36Z,Why are we calling the same thing a second time?
1044758640,11390,junrao,2022-12-09T19:13:28Z,Why are we calling the same thing a second time? Ditto in two other places below.
1044791351,11390,junrao,2022-12-09T19:42:37Z,"Hmm, the log segment only has 1 record with timestamp and startOffset. Why do we return timestamp + 1 here?"
1044796318,11390,junrao,2022-12-09T19:48:50Z,add a new line above
1044806867,11390,junrao,2022-12-09T20:05:41Z,_localLogStartOffset could change after we find out the epoch. Perhaps we could save it as a local val and use it in the return value.
1046991743,11390,satishd,2022-12-13T10:58:07Z,"Comment does not say that magic+4 bytes adds upto LOG_OVERHEAD.
Updated the comment to make it more clear about the size used in the below code.

`int bufferSize = LOG_OVERHEAD + size;`
"
1047001228,11390,satishd,2022-12-13T11:04:58Z,I will address it in a followup PR. Filed https://issues.apache.org/jira/browse/KAFKA-14466
1047001890,11390,satishd,2022-12-13T11:05:34Z,"Right. Added the respective delete part for now which is about removing the partitions. More on that will be added respectively when we add handling partition deletes with respect to remote storage also. 
"
1047002015,11390,satishd,2022-12-13T11:05:45Z,Done
1047002223,11390,satishd,2022-12-13T11:05:58Z,Done
1047003879,11390,satishd,2022-12-13T11:07:40Z,"That case is taken care in `lookupTimestamp`. Let me know if I am missing anything here.

`val timestampOffset = lookupTimestamp(rlsMetadata, timestamp, startingOffset)`
"
1047004756,11390,satishd,2022-12-13T11:08:10Z,Done
1047005713,11390,satishd,2022-12-13T11:08:41Z,Updated it with the right text.
1047007959,11390,satishd,2022-12-13T11:09:51Z,Nice catch! This is moved to `MetadataVersion`.
1047008633,11390,satishd,2022-12-13T11:10:15Z,"No, fetchEarlierEpochEndOffset(2) is right. Argument here is the leader epoch. "
1047008950,11390,satishd,2022-12-13T11:10:25Z,Done
1047009169,11390,satishd,2022-12-13T11:10:31Z,Done
1047014746,11390,satishd,2022-12-13T11:14:32Z,Right.
1047015681,11390,satishd,2022-12-13T11:15:31Z,`truncateOnFetch` is the existing code. It is not added by this PR.
1047016127,11390,satishd,2022-12-13T11:15:58Z,Done
1047017673,11390,satishd,2022-12-13T11:17:33Z,This is overridden by `ListOffsetsRequestWithRemoteStoreTest`
1047018234,11390,satishd,2022-12-13T11:17:46Z,Done
1047018559,11390,satishd,2022-12-13T11:17:55Z,Done
1047019754,11390,satishd,2022-12-13T11:18:35Z,Updated with javadoc to explain the testcases with the latest commits. 
1047020063,11390,satishd,2022-12-13T11:18:44Z,Updated with javadoc to explain the testcases with the latest commits.
1047020508,11390,satishd,2022-12-13T11:18:57Z,Done
1047020836,11390,satishd,2022-12-13T11:19:07Z,Done
1047024290,11390,satishd,2022-12-13T11:21:03Z,+1 on this approach. This is aligned with our initial proposal in this PR also. Latest commit is updated with this change.
1047369687,11390,satishd,2022-12-13T16:07:36Z,Added more documentation about the test for better clarity with the latest commit. stateManager.reloadSnapshots() deletes the existing snapshots. 
1049205016,11390,satishd,2022-12-15T04:33:13Z,"As discussed offline, the current follower fetch retries when it receives an error while building aurxiliary state. It will eventually gets the auxiliary data from remote storage for the available leader-log-start-offset. "
1049392445,11390,showuon,2022-12-15T09:20:23Z,"Maybe we should have an assertion before and between the 1st and 2nd call to `getIndexEntry`? Like this:
```java
assertEquals(0, cache.entries.size())
cache.getIndexEntry(metadataList.head)
assertEquals(1, cache.entries.size())
cache.getIndexEntry(metadataList.head)
assertEquals(1, cache.entries.size())
```"
1049393028,11390,showuon,2022-12-15T09:20:56Z,typo: sis -> is
1049396894,11390,showuon,2022-12-15T09:23:45Z,"I saw there are many places have this assertion. I've created a JIRA to address it: https://issues.apache.org/jira/browse/KAFKA-14495 . You can ignore it for this PR. Thanks.
"
1049993923,11390,junrao,2022-12-15T18:15:54Z,"This is still not very precise since size includes other fields like CRC, attributes, etc. So, we could probably just omit it and say the total size of a batch is LOG_OVERHEAD + the size of the rest of the content."
1050008020,11390,junrao,2022-12-15T18:31:58Z,It's better to use localLog.logEndOffsetMetadata since it has the log Metadata in addition to the offset.
1050013671,11390,junrao,2022-12-15T18:39:10Z,"It seems that the ""no message in the remote storage has the given timestamp"" case is covered in the otherwise clause too?"
1050017122,11390,junrao,2022-12-15T18:43:27Z,It seems that both offsetForLeaderEpochRequestVersion and listOffsetRequestVersion are moved to MetadataVersion and can be removed here?
1050029710,11390,junrao,2022-12-15T18:57:17Z,"Then, could we just remove this param?"
1050033092,11390,junrao,2022-12-15T19:01:07Z,inmemory => in memory
1050035787,11390,junrao,2022-12-15T19:04:29Z,Could we add a similar comment as in line 125?
1050039191,11390,junrao,2022-12-15T19:09:04Z,Change to curlocalLogStartOffset = localLogStartOffset ?
1050045012,11390,junrao,2022-12-15T19:16:40Z,Should we add a test that explicitly tests the producer state after handling OFFSET_MOVED_TO_TIERED_STORAGE error? This can be done in a separate PR.
1050536097,11390,satishd,2022-12-16T09:28:23Z,Updated with what you suggested to make it more clear. 
1050536434,11390,satishd,2022-12-16T09:28:47Z,Done
1050536942,11390,satishd,2022-12-16T09:29:21Z,Done
1050537018,11390,satishd,2022-12-16T09:29:26Z,Done
1050537960,11390,satishd,2022-12-16T09:30:25Z,"As you suggested earlier, I created a [KAFKA-14467](https://issues.apache.org/jira/browse/KAFKA-14467) for that and mentioned it in my earlier [comment](https://github.com/apache/kafka/pull/11390#issuecomment-1348940728)."
1051118349,11390,junrao,2022-12-16T20:06:38Z,logEndOffset still uses offset. We want to use logEndOffsetMetadata.
1051439769,11390,ijuma,2022-12-17T18:26:58Z,+1 @junrao 
263150141,6363,kkonstantine,2019-03-06T21:47:30Z,Please skip this file and review: https://github.com/apache/kafka/pull/6340 instead. Thanks!
263150256,6363,kkonstantine,2019-03-06T21:47:51Z,Please skip this file and review: https://github.com/apache/kafka/pull/6340 instead. Thanks!
263151738,6363,kkonstantine,2019-03-06T21:52:09Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263151859,6363,kkonstantine,2019-03-06T21:52:30Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263151902,6363,kkonstantine,2019-03-06T21:52:37Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263151960,6363,kkonstantine,2019-03-06T21:52:46Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263152009,6363,kkonstantine,2019-03-06T21:52:55Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263152049,6363,kkonstantine,2019-03-06T21:53:03Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263152103,6363,kkonstantine,2019-03-06T21:53:11Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263152397,6363,kkonstantine,2019-03-06T21:54:01Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263152448,6363,kkonstantine,2019-03-06T21:54:08Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263152502,6363,kkonstantine,2019-03-06T21:54:17Z,Please skip this file and review: https://github.com/apache/kafka/pull/6342 instead. Thanks!
263242755,6363,kkonstantine,2019-03-07T05:31:22Z,"This file changes can be omitted, although it'd be nice to consider whether we want to enable a few log messages in Connect integration tests. This is something that manually needs to be enabled when someone debugs a failure at an integration test. "
268807968,6363,rhauch,2019-03-25T19:10:32Z,Missing the `@param` for most of the parameters.
268809353,6363,rhauch,2019-03-25T19:14:31Z,"Nit: might this be null, or will it only be empty?"
268809411,6363,rhauch,2019-03-25T19:14:38Z,"Nit: might this be null, or will it only be empty?"
268845523,6363,rhauch,2019-03-25T20:51:45Z,Nit: should the header be in a `<pre>` section so it's easier to read given it uses constant-width formatting?
268846278,6363,rhauch,2019-03-25T20:53:50Z,"The [KIP says](https://cwiki.apache.org/confluence/display/KAFKA/KIP-415:+Incremental+Cooperative+Rebalancing+in+Kafka+Connect#KIP-415:IncrementalCooperativeRebalancinginKafkaConnect-ConfigurationProperties) that `COMPATIBLE` is the default.

Also, why is the protocol string for eager ""`default`""?"
268847672,6363,rhauch,2019-03-25T20:57:16Z,"Why not define a new field in the enum and assign via a constructor, rather than define an abstract method and override in the definitions? See [PluginType](https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/isolation/PluginType.java#L29) for an example."
268847927,6363,rhauch,2019-03-25T20:57:50Z,The [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-415:+Incremental+Cooperative+Rebalancing+in+Kafka+Connect#KIP-415:IncrementalCooperativeRebalancinginKafkaConnect-ConfigurationProperties) mentions that `compatible` is the default.
268848356,6363,rhauch,2019-03-25T20:58:40Z,Nit: Maybe use `TimeUnit.SECONDS.toMillis(300)` for the value? That seems to be more in line with how we're doing time-related constants now.
268848881,6363,rhauch,2019-03-25T21:00:06Z,Can this be final? Can it also be private? (I don't see it's used anywhere else in the code.)
268849865,6363,rhauch,2019-03-25T21:02:40Z,"Doesn't each process only have a single DistributedWorker instance? If so, then would `CONNECT_CLIENT_ID_SEQUENCE.getAndIncrement()` ever return something other than 1?"
268868053,6363,rhauch,2019-03-25T21:53:33Z,"Did you consider having a single log message that output all of these in one message? Would that make it easier to follow what is being decided, especially if there are lots of interjected messages from currently-running connectors and tasks?"
268869092,6363,rhauch,2019-03-25T21:56:44Z,Add JavaDoc
268882992,6363,rhauch,2019-03-25T22:43:19Z,Is this method used anymore?
268911175,6363,rhauch,2019-03-26T01:06:41Z,"The logic of these methods is not trivial, and while ConnectAssignorTest has unit tests that exercise some of these methods, what do you think about writing unit tests for these. They really don't use state, so they would seem straightforward to test and it would help any future work by preventing regressions."
268911374,6363,rhauch,2019-03-26T01:08:03Z,This combination of JavaDoc and line comments is unusual. Is there a reason this isn't in the JavaDoc?
268912546,6363,rhauch,2019-03-26T01:16:43Z,Nit: seems odd to have statics after member fields.
268913739,6363,rhauch,2019-03-26T01:24:46Z,How about a validator to ensure that worker configs are valid before the DistributedWorker starts instantiating its components and throwing IllegalArgumentExceptions?
268913846,6363,rhauch,2019-03-26T01:25:27Z,"What happens if name is not lowercase? There's no validator for this, so it's possible to get an exception at a strange point."
268914485,6363,rhauch,2019-03-26T01:29:39Z,I don't think this is used anywhere except for tests. Is that intentional?
268914753,6363,rhauch,2019-03-26T01:31:27Z,"I think these fields will never be null based upon how it is currently used, but there are no asserts or checks to ensure there are no NPEs here. Is this intentional?"
268915078,6363,rhauch,2019-03-26T01:33:33Z,"It took me a while to find this class. Would it make more sense if there were named `IncrementalCooperativeAssignorTest` instead, since that's all that's being tested?"
268916477,6363,rhauch,2019-03-26T01:43:03Z,"Do you think it's easier to understand the test to simply do:
```
List<WorkerLoad> existingAssignment = new ArrayList<>();
existingAssignment.add(workerLoad(""worker0"", 0, 1));
existingAssignment.add(workerLoad(""worker1"", 2, 3));
existingAssignment.add(workerLoad(""worker2"", 5, 6));
existingAssignment.add(workerLoad(""worker3"", 9, 10));
existingAssignment.add(emptyWorkerLoad(""worker4""));
```"
268916806,6363,rhauch,2019-03-26T01:45:25Z,"Is this all that we can assert here? It doesn't really seem to validate much of the logic by just checking the sizes, does it?"
271858376,6363,kkonstantine,2019-04-03T17:46:44Z,#6342 was merged. This PR is now rebased on top of it. 
271858438,6363,kkonstantine,2019-04-03T17:46:52Z,#6342 was merged. This PR is now rebased on top of it. 
271858506,6363,kkonstantine,2019-04-03T17:47:02Z,#6342 was merged. This PR is now rebased on top of it. 
271858588,6363,kkonstantine,2019-04-03T17:47:16Z,#6342 was merged. This PR is now rebased on top of it. 
271858640,6363,kkonstantine,2019-04-03T17:47:23Z,#6342 was merged. This PR is now rebased on top of it. 
271858676,6363,kkonstantine,2019-04-03T17:47:29Z,#6342 was merged. This PR is now rebased on top of it. 
271858830,6363,kkonstantine,2019-04-03T17:47:41Z,#6342 was merged. This PR is now rebased on top of it. 
271858905,6363,kkonstantine,2019-04-03T17:47:54Z,#6342 was merged. This PR is now rebased on top of it. 
271908729,6363,rayokota,2019-04-03T20:01:15Z,"Wouldn't it be better to return an empty map?  Then it would be consistent with the code in the rest of the method body that also returns an empty map if both are `null`.  And it would also be consistent with `asMap` that never returns `null` as well as a few other methods in this class that return `Collections.emptyList` instead of `null`,  and also remove a bunch of `null` checks."
271909677,6363,rayokota,2019-04-03T20:03:53Z,"If `taskAssignments` never returns `null`, you can simplify these checks."
271910018,6363,rayokota,2019-04-03T20:04:51Z,"If `taskAssignments` never receives `null`, you can simplify this so that it also never returns `null`."
271910748,6363,rayokota,2019-04-03T20:06:58Z,`asMap()` never returns `null`; that's why it would be nice if `asRevokedMap` also never returns `null`.
271911651,6363,rayokota,2019-04-03T20:09:34Z,yay!
271911745,6363,rayokota,2019-04-03T20:09:53Z,yay!
271912716,6363,rayokota,2019-04-03T20:12:39Z,"Wouldn't it be better to have something like

`public static ConnectProtocolCompatibility DEFAULT = EAGER;`"
271913903,6363,rayokota,2019-04-03T20:16:03Z,"Why can't this just be `name().toLowerCase(Locale.ROOT)`.  Otherwise if `protocol()` is allowed to differ from `name()`, then how do I go from a `protocol()` to the Enum (since there is only a way to go from `name()` to Enum)."
271916281,6363,rayokota,2019-04-03T20:22:40Z,Should we check that `update` is `null`?  It's confusing if `running` is allowed to be `null` but `update` is assumed not to be.
271917878,6363,rayokota,2019-04-03T20:27:21Z,nit:  Can be replaced with `computeIfAbsent`
271918316,6363,rayokota,2019-04-03T20:28:35Z,nit:  Can be replaced with `computeIfAbsent`
271918654,6363,rayokota,2019-04-03T20:29:29Z,nit: Can be replaced with `getOrDefault`
271918689,6363,rayokota,2019-04-03T20:29:35Z,nit: Can be replaced with `getOrDefault`
271919947,6363,rayokota,2019-04-03T20:33:02Z,Do we need an `assert` that the `memberConfigs` is never empty?  Otherwise `return maxOffset` will throw NPE.
271920454,6363,rayokota,2019-04-03T20:34:20Z,"nit:  this can be simplified to `maxOffset = Math.max(maxOffset, memberRootOffset)` if we initialize `maxOffset` with `long maxOffset = Long.MIN_VALUE`."
271923960,6363,rayokota,2019-04-03T20:43:37Z,"There is an `equals` call here, but I don't see `equals` and `hashCode` methods for `ConnectAssignment`.  If you are assuming reference equality, perhaps use `==`?"
271927037,6363,rayokota,2019-04-03T20:51:08Z,"Why not return `ConnectAssignment.empty()`?  I found 2 callers to this method:  `IncrementalCooperativeConnectProtocol.derserializeMetadata` sends the result to the `ExtendedWorkerState` constructor, which replaces null with `ConnectAssignment.empty()`.  The other caller,  `WorkerCoordinator.onJoinComplete()`, tries to call `version()` and would get an NPE."
271928754,6363,rayokota,2019-04-03T20:55:39Z,nit: could add `throws SchemaException` to the method declaration if you want
271930819,6363,rayokota,2019-04-03T21:01:10Z,Do we need an `assert` that `allMemberMetadata` is not empty?  Otherwise an NPE would result here I think
271933574,6363,rayokota,2019-04-03T21:08:51Z,Should this return an empty `ByteBuffer`?  I think one of the callers will get a NPE otherwise.
271933845,6363,rayokota,2019-04-03T21:09:40Z,nit:  `.stream().forEachOrdered()` can be replaced with `.forEach()`
271934088,6363,rayokota,2019-04-03T21:10:24Z,nit:  `.stream().forEachOrdered()` can be replaced with `.forEach()`
271934289,6363,rayokota,2019-04-03T21:10:57Z,nit:  `.stream().forEachOrdered()` can be replaced with `.forEach()`
271979425,6363,kkonstantine,2019-04-04T00:16:31Z,"Unfortunately this value has to stay as is, called `default`, otherwise we won't be able to have live rolling upgrades. 

In all the versions up to now, the `WorkerCoordinator` sets the name of the embedded protocol in the JoinGroupRequest (aka metadata request) as `default`. 

Small price to pay I think, because it's internal info. We can deprecate the term `default` eventually"
271979799,6363,kkonstantine,2019-04-04T00:18:57Z,In the `REVERSE` map we put both values lowercase and uppercase (values are matching the enum name which is upper case). They can be used either way then. I don't think I was the first to introduce this. But I have found useful that we don't strictly accept only uppercase values. I don't see much downside to it. 
271980517,6363,kkonstantine,2019-04-04T00:23:32Z,"I find the two ways equivalent, with minor pros and cons in edge cases. "
271980846,6363,kkonstantine,2019-04-04T00:25:39Z,Correct. I didn't get to the removal of `COOPERATIVE` as well as the switch of default yet. Will update in a bigger commit without other cleanup comments. 
271982564,6363,kkonstantine,2019-04-04T00:36:45Z,"The sets are many. I'd still like to take a final look on what is printed before we merge. But seems not printing as soon as possible, might hide issues, even during integration tests. But I see your point. Let's get back to that. "
272760693,6363,kkonstantine,2019-04-05T22:24:34Z,I believe they are there now. 
272761166,6363,kkonstantine,2019-04-05T22:27:38Z,Let me know if that's what you had in mind. 
274026037,6363,mumrah,2019-04-10T15:36:25Z,"`Objects#requireNonNull` can be used during assignment as well, e.g. 

```java
this.revokedConnectorIds = Objects.requireNonNull(revokedConnectorIds, ""..."")
```"
274068757,6363,mumrah,2019-04-10T17:13:39Z,"Minor: since there are so few values to consider, maybe prefer a brute force approach? E.g., `values().stream().findFirst(mode -> mode.name().equalsIgnoreCase(queryMode)` "
275537294,6363,kkonstantine,2019-04-15T20:44:49Z,"This file contains the old code in `WorkerCoordinator` _as-is_. I would prefer not to introduce changes at all, no matter how small the changes. In a PR that big, it's simply a matter of discipline not to try to change multiple execution paths at once. 

This and the optimizations below are small. Additionally, this code is not expected to run if the new rebalancing policy is chosen. I'm happy to return to this code, that is know encapsulated in a separate and well defined class for a similar clean once the the code has been released. "
275537397,6363,kkonstantine,2019-04-15T20:45:07Z,Same as above. 
275537442,6363,kkonstantine,2019-04-15T20:45:16Z,same as above
275537487,6363,kkonstantine,2019-04-15T20:45:21Z,same as above
275537527,6363,kkonstantine,2019-04-15T20:45:30Z,same as above
275537593,6363,kkonstantine,2019-04-15T20:45:41Z,same as above
275538726,6363,kkonstantine,2019-04-15T20:48:59Z,"It's not a checked exception. We don't add unchecked exception to the signature. Only in javadoc. This method is `private`, so I'm adding the javadoc mention on the `public` methods that check version.  "
275549911,6363,kkonstantine,2019-04-15T21:22:22Z,Fixed. With a note to improve logging and log messages in this class. 
275560677,6363,kkonstantine,2019-04-15T21:57:44Z,"The distinction I want to reflect here is that, task assignments have always been a required field of the Connect protocol (V0 and therefore V1) and this can not change in a backwards compatible way. To the contrary, the revoked assignments correspond to a new field, which is also `nullable` which here means also optional. Deprecating such a field if we need to might be easier in the future (although not completely transparent since we don't selectively unpack fields in the protocol schema). 

That's why `asMap` does not return `null` while `asRevokeMap` might return `null`. I'll comment below too. "
275562433,6363,kkonstantine,2019-04-15T22:04:33Z,See previous comment. 
275563850,6363,kkonstantine,2019-04-15T22:10:28Z,"This is different. This takes you from on-the-wire representation to the in-memory representation. Also, it's meant to be used both for assignments and revocations. Of course, for this protocol it doesn't make sense to keep `null` in the in-memory representation of `ConnectAssignment`. See another take on compatibility between V0 and V1 in `ConnectProtocolCompatibilityTest` "
275563890,6363,kkonstantine,2019-04-15T22:10:39Z,See comment above
275567590,6363,kkonstantine,2019-04-15T22:25:40Z,"Enums in java are more expressive. We don't have to restrict ourselves to what `name()` represents. Here we have a good example, where the protocol has to stay as `default` for backwards compatibility but we don't have to call the enum literal `DEFAULT`. Giving the literal a more accurate name, while keeping compatibility is one of the reasons enums in java are more powerful than elsewhere. "
275567702,6363,kkonstantine,2019-04-15T22:26:14Z,Let's revisit when we reduce the enum values to reflect the voted version of the KIP
275571648,6363,kkonstantine,2019-04-15T22:43:44Z,This is not meant to be a general method. I removed the `null` check because assignment collections should not be `null`. If we see fit later on we could factor out this logic to a more general diff method in `ConnectAssignment` itself. 
275575635,6363,kkonstantine,2019-04-15T23:02:18Z,"With Api generator has become a bit harder to argue about, but both current and new code depend on members this list (`allMemberMetadata`) not being `null`. Any gaps of api generator with `final` member fields and when arrays are allowed to be set to `null` instead of an empty array should not be addressed as part of this PR IMO. 

I think it's safe to depend on the previous assumption, that `allMemberMetadata` can not be `null`. "
275576977,6363,kkonstantine,2019-04-15T23:08:48Z,But if we were to introduce these methods on `ConnectAssignment` this code would still work right? While reference equality would be harder to track in this case. 
275585786,6363,kkonstantine,2019-04-15T23:55:41Z,"Currently this doesn't seem possible because we never send a `ConnectAssignment.empty()` intentionally unless this is called in a metadata request. We might send an assignment that is practically empty on an error, but in this case equality here is not true. However, I'll tighten the checks and will increase coverage. 

We might end up diverging, depending on who's using this call. However, again for metadata we need `null` because this goes to a nullable field. "
275586823,6363,kkonstantine,2019-04-16T00:01:33Z,"see explanation above. This ties back to what we send over the wire. A `ConnectAssignment.empty` assignment is never sent over the wire (you'll always have the leader and the leader url fields for instance, even if you receive an empty assignment). "
275587099,6363,kkonstantine,2019-04-16T00:03:19Z,"see answer in the other comment about `asMap`. The two methods are not symmetric to this respect, because one field is required and the other one is optional. "
275587135,6363,kkonstantine,2019-04-16T00:03:30Z,Fixed
275587191,6363,kkonstantine,2019-04-16T00:03:50Z,"As mentioned above, these fields in `V1` protocol are `nullable`. Therefore, instead of going back and forth from empty list to `null` and vice versa, I prefer to have `taskAssignments` handle `null` and potentially return `null`, because `null` is what needs to be used when these optional fields are empty. "
275588182,6363,kkonstantine,2019-04-16T00:09:40Z,"Changed, although with the need to convert `long` to `int` it's less brief than ideal. Though still safer, I agree. "
275588691,6363,kkonstantine,2019-04-16T00:12:39Z,Fixed. I must have had plans for parameterizing it in unit tests that I didn't follow after all. 
275588918,6363,kkonstantine,2019-04-16T00:13:49Z,"This is older code which I chose not to change here. 

One place where you can see the counter making a difference is in integration tests with multiple workers at the moment. "
275589318,6363,kkonstantine,2019-04-16T00:16:11Z,I'm keeping the old comment from `ConnectProtocol`. Not sure it belongs to the javadoc. WDYT?
275589829,6363,kkonstantine,2019-04-16T00:19:14Z,"In general, I'm not sure our checkstyle is very opinionated w.r.t to this order. Changed to bring EMPTY first. "
275590343,6363,kkonstantine,2019-04-16T00:22:13Z,"This class has some overlap with other classes such as `ConnectorsAndTasks` and even `LeaderState`. I agree and I would also like to consolidate, but maybe when we are near the end of the PR review. "
275590597,6363,kkonstantine,2019-04-16T00:23:38Z,I'll add a requirement in the constructor rather than here. 
275592326,6363,kkonstantine,2019-04-16T00:34:24Z,Fixed
275592693,6363,kkonstantine,2019-04-16T00:36:30Z,"Also, to @rayokota comment, java allows us to not have strict binding to enum names. I'm inclined to use this functionality here. "
275593431,6363,kkonstantine,2019-04-16T00:41:13Z,javadoc fixed with reference to `null`
275593446,6363,kkonstantine,2019-04-16T00:41:19Z,javadoc fixed with reference to `null`
275893903,6363,mumrah,2019-04-16T16:46:38Z,Does Connect use the `Time` utility class? My understanding is that it makes instrumenting time-based logic easier in the test code.
275900247,6363,mumrah,2019-04-16T17:02:14Z,"I was wondering about the thread safety of this assignment, but I see that calls to this method are protected by upstream synchronization in AbstractCoordinator. Maybe we should include a note about thread safety in the class javadoc?"
275903378,6363,mumrah,2019-04-16T17:10:05Z,"Maybe consider exposing only what is needed from WorkerCoordinator rather than depending on the whole class here? Instead, you could pass in a ClusterConfigState supplier and a LeaderState consumer. This would also make unit testing this method more straightforward. Just a thought.
"
275904303,6363,mumrah,2019-04-16T17:12:29Z,"How is error handling done for this method? It mutates its own state (previousAssignment) as well as the WorkerCoordinator. If we fail half way through the assignment procedure, how do we ensure a valid overall state?
"
275905898,6363,mumrah,2019-04-16T17:16:39Z,Why the explicit check for log level here?
275908713,6363,mumrah,2019-04-16T17:23:57Z,"nit: a method reference can be used here e.g., `forEach(workerLoad::assign)`"
275912009,6363,mumrah,2019-04-16T17:32:06Z,Why iterator here instead of stream?
275944942,6363,kkonstantine,2019-04-16T18:54:49Z,"We seem to follow this pattern in other enums elsewhere too, so I'm more inclined to keep it the same here too. I agree with your point. But also the footprint of a hashmap here is not an issue too I'd say. Thus this is more stylistic than anything else I believe. WDYT? "
275945434,6363,kkonstantine,2019-04-16T18:56:08Z,"Yes, I'm a big proponent of mocking time and `Time` util specifically. I missed its use here. Will add!"
275946661,6363,kkonstantine,2019-04-16T18:59:28Z,Good point. In general we don't assume multi-threading for the code that is run by the herder and this has simplified things. I'll add a mention
275953129,6363,kkonstantine,2019-04-16T19:18:02Z,To avoid the `forEach` loop if the log level is not at least debug (the methods called on `wl` don't compute anything indeed)
275955774,6363,kkonstantine,2019-04-16T19:25:52Z,"The `numToRevoke` that is used to exit the `for` loops (potentially early) is not effectively final. I could work around it, but I'm not sure the result would be more readable. "
275956315,6363,kkonstantine,2019-04-16T19:27:18Z,TIL. Thanks!
276076567,6363,kkonstantine,2019-04-17T04:39:04Z,Added guard in the constructor
276076869,6363,kkonstantine,2019-04-17T04:41:28Z,Done
276079316,6363,kkonstantine,2019-04-17T04:58:31Z,Done
276861923,6363,rayokota,2019-04-18T23:04:07Z,Perhaps either 1) add `equals` and `hashcode` to `ConnectAssignment` or 2) add a comment here stating that the `equals` method is relying on reference equality since `equals` is not overridden in `ConnectAssignment`.  WDYT?
277123841,6363,kkonstantine,2019-04-20T04:49:03Z,"Sure! Added a comment for now, and I'll revisit method implementation if we need it for general comparisons between assignments (I don't see a reason yet). "
277124243,6363,kkonstantine,2019-04-20T05:14:41Z,Done
277799701,6363,kkonstantine,2019-04-23T17:54:02Z,"@rhauch the changes to the enum that reflect the latest KIP version have been pushed. 
Again, using ""default"" is required in order to be backwards compatible and support older workers. "
277800822,6363,kkonstantine,2019-04-23T17:56:56Z,"Again, I'd prefer to support both: 
`EAGER` and `eager` as well as `COMPATIBLE` and `compatible`. Are we ok with that? "
277933842,6363,ewencp,2019-04-24T02:14:52Z,"why this particular setup with a new class? the naming seems like it's going to be confusing since there's no obvious differentiation in naming.

its been awhile since i looked at the code, but I think the general pattern expected (from both `ConsumerProtocol` and `ConnectProtocol` when we generalized) is that we'd maintain the different version schemas alongside each other, much as we do for lower level message types (e.g. check what `MetadataRequest` code looks like). This means we have mostly reuse and just a bit of delta for various parsing/serialization since lots of the code overlaps.

At a minimum, I'd suggest renaming this class to something clearly tied to the updated code, whether similar to the `IncrementalCooperativeConnectProtocol` class or some variant that is perhaps more general that would apply even with further extensions."
277934191,6363,ewencp,2019-04-24T02:17:07Z,"Currently with only 1 version we just do validation inline, but given this is common state between this class and the base class, why not make it a field and any necessary accessors on the base class? Seems like ideally this class would only add new fields that are truly unique to it (and lack of stored state in one case vs the other seems weird). "
277938290,6363,ewencp,2019-04-24T02:45:57Z,"at some length of fields, `StringBuilder` probably makes sense. not sure how frequently we're logging this, which i assume is main use case here"
277939161,6363,ewencp,2019-04-24T02:52:28Z,"why do we need distinct here but not on the subsequent task list? why would there be dups here but not there?

i think its fine to include this if we want to be defensive, but seems like it should be used in both cases if that's the goal."
277945505,6363,ewencp,2019-04-24T03:40:16Z,these methods could validate the `key` is in the expected set to be more defensive against coding errors
277945625,6363,ewencp,2019-04-24T03:41:08Z,nit: you're doing a redundant lookup here after already assigning to `connectors` to do the `null` check
277946373,6363,ewencp,2019-04-24T03:47:35Z,"why are we documenting v1 subscription here? it's also identical to v0, but even if it weren't, this code deals with v0, right?"
277946470,6363,ewencp,2019-04-24T03:48:27Z,"I guess this kind of explains my above question re: v1 docs here since this doesn't include everything for v0, but again, why re-document this here?"
277947459,6363,ewencp,2019-04-24T03:56:30Z,"if anything, it seems even better to just generalize to case insensitivity (which i am not a fan of in many cases, but in the context of enum configs should be harmless unless we make some pretty bad decisions). tbh, i'm not sure why elsewhere we're optimizing the string -> enum lookup unless it is in a hot path, which it doesn't seem like it should be here"
277947904,6363,ewencp,2019-04-24T04:00:14Z,"not sure if this was intended to apply to both connect protocol and scheduled rebalance max delay, but validator on the delay would make sense as well -- at a minimum to ensure non-negative"
277948672,6363,ewencp,2019-04-24T04:06:36Z,"hmm, so this is a potentially interesting change. client.id can be used for some things like quotas. since this is for workers, i'm not sure we're actually changing behavior in any interesting way here, but we should consider what the possible fallout in behavior could be by changing the default client.id behavior here."
277950243,6363,ewencp,2019-04-24T04:18:49Z,"we use normal logging elsewhere in this file, i assume this is stray debugging code?"
278276834,6363,ewencp,2019-04-24T19:11:50Z,what's the reason for removing this? if anything i would think we should have more logging around these operations rather than less.
278278502,6363,ewencp,2019-04-24T19:16:38Z,"It seems like most of the other logic around the protocol versions is cleanly factored into the protocol classes. Could we refactor this logic as well, e.g. to get this from `assignment`?"
278371438,6363,ewencp,2019-04-25T01:29:36Z,"why are we clearing these here? i would think the key thing in this class would just be to force the rejoin when tasks are revoked and it doesn't look like we use this anywhere else in this class that this would cause a problem. is there some sharing with code elsewhere that relies on this? (and if so, could we just hand this class a copy instead?)"
278372255,6363,ewencp,2019-04-25T01:36:27Z,"this might be worth a comment as well, or actually even restructuring a bit. it seems confusing that the `runningAssignment` after starting all the connectors and tasks would be `empty()`. I *suspect* this is being done because on  the subsequent round, this forces the `assignmentDifference` to be the full set of connectors/tasks in `assignment`. But it might be more intuitive to reset `runningAssignment` to `empty()` in the `onRevoked` (where they actually stop running, and given that it's just based on the parameters, it might be more implicit based on what was passed in than based on the protocol version). I *think* that also handles my above comment because the check for protocol version is no longer necessary -- it should just work out."
278374311,6363,ewencp,2019-04-25T01:54:11Z,"seems like copy paste? and a bunch of the utilities would be. aside from the diff to make protected, did you consider having this inherit from Eager and override the key functionality but reuse the rest (which perhaps is better structured as utilities elsewhere, but your inclination to not change the existing implementation too much seems reasonable to me at least for awhile for maintenance reasons, although even there that code hasn't changed in a long time)?"
278732876,6363,kkonstantine,2019-04-25T21:09:29Z,"Added a validation and a validator class that can allow anyone to write a Validator in place. I think this is useful and will help with easy addition of validations, that currently are missing from several places. 

I believe this part belongs to a separate PR with a jira ticket that informs for the addition of what I called `LambdaValidator`. But before I create this I'd like to get some feedback here. WDYT @rhauch ?"
278733160,6363,kkonstantine,2019-04-25T21:10:18Z,"Given that I've added several integration tests that bring up several workers, I think it makes sense to leave this here. "
278741717,6363,kkonstantine,2019-04-25T21:36:44Z,"I'm using an assignment via `leaderSet` to the coordinator itself though. I'm inclined to say that encapsulation makes sense for now. I feel it doesn't pose a big trade-off. I'd suggest leaving it for now, since it's an easy future refactoring. "
278751495,6363,kkonstantine,2019-04-25T22:12:31Z,That's a good question. We might need to reset this under certain errors. I'll get back to that. 
278752888,6363,ewencp,2019-04-25T22:18:41Z,This is getting a bit confusing -- this seems the same as `currentWorkerAssignment`. What mutates that causes this to be different?
278755759,6363,kkonstantine,2019-04-25T22:31:50Z,Added a note at the class level for both `EagerAssignor` and `IncrementalCooperativeAssignor`. We could potentially add a more detailed note such as the one in `AbstractCoordinator` on locking. 
278758021,6363,ewencp,2019-04-25T22:42:40Z,normally `Time` would be injected for testing?
278765522,6363,kkonstantine,2019-04-25T23:21:58Z,"I like to keep the programmatic way of defining test cases. 
But you are right in that the workers here won't scale significantlly. 
Changed. 
"
278766008,6363,kkonstantine,2019-04-25T23:24:35Z,"The goal of this test is to validate balanced assignment. Thus asserting the size is deterministic and probably sufficient. 

If we agree on the scheduling algorithms additional unit tests will be added (possibly here) and in `WorkerCoordinatorIncrementalTest`"
278772408,6363,ewencp,2019-04-26T00:01:36Z,It feels like we're getting inconsistent with the naming. `ConnectAssignment` and `ConnectProtocol.Assignment` vs `IncrementalCooperativeConnectProtocol.ExtendedWorkerState` vs `ConnectProtocol.WorkerState`.
278785956,6363,ewencp,2019-04-26T01:44:19Z,"i realize we can get the value via `protocolCompatibility.protocol()` here, but might be clearer to just be explicit anyway. definitely in this case it feels like it makes it harder to parse what's going on here (to me at least)"
278786168,6363,ewencp,2019-04-26T01:46:02Z,what about `COOPERATIVE` here?
278786302,6363,ewencp,2019-04-26T01:47:04Z,"Seems we are missing `COOPERATIVE` case here? seems like if we don't handle it here, that setting won't work and having that mode doesn't serve much purpose (even if we encourage people to just leave it in `COMPATIBLE` given expected low overhead generally)"
278787756,6363,ewencp,2019-04-26T01:59:34Z,"would any of this be better handled by dynamic dispatch rather than switch statements (for further extensibility)? not a huge deal, but given (at least in this case) it seems like we could dispatch to one of the `[X]ConnectProtocol` classes, i'm wondering if it avoids switch case hell and keeps the logic for each closer to related protocols and outside of otherwise more general logic. (this admittedly could have been done initially but not nearly as valuable with just one case...)"
278788073,6363,ewencp,2019-04-26T02:02:07Z,"I'm still working through updated code and all the classes, but this feels a bit messy. Admittedly we just dumped the `ConnectProtocol` here initially when we only had one type. But now it seems confusing when I use the new protocol directly vs having to switch on compatibility, etc. Not sure I have a concrete suggestion yet, but I worry about the more general classes here ending up with both explicit (compat mode) and implicit (this case where we always use `IncrementalCooperativeConnectProtocol`) decisions about how to handle logic of different versions.

I'll think more about how we can refactor..."
278788840,6363,ewencp,2019-04-26T02:08:42Z,"We should move this log *above* any actions taken, even if for some reason we only want it in this conditional instead of immediately after the deserialization. since something in onRevoked code could log, ordering of events would be confusing with this where it currently is."
278789848,6363,ewencp,2019-04-26T02:17:05Z,"This approach seems interesting -- we previously had this class handle this as immutable (the entire snapshot was left unchanged or entirely replaced). Do we actually need to mutate like we do here? A few lines down we do

```
assignmentSnapshot = newAssignment;
```

so main value here seems to be for the log statement a couple lines below.

The other value in mutating things in this block seems to be for `newAssignment`. But it's taking anything *left* in the old assignment and adding it to `newAssignment`. But I thought in the KIP we still reiterate to the member all assigned resources, so adding the old leftover ones seems redundant. Shouldn't the `newAssignment` already contain exactly what we want to update `assignmentSnapshot` with before we attempt any of these mutations?"
278790323,6363,ewencp,2019-04-26T02:21:21Z,this is another location where I wonder if we just maintain one assignor and update *it* when we see protocol version change rather than maintaining `currentConnectProtocol` and switching on it could refactor some of this logic to be more general
278791412,6363,ewencp,2019-04-26T02:30:58Z,didn't notice earlier in review that we're making previously internal state mutable. seems to be used primarily by the assignors for identical logic. can we keep this internal state by pushing that logic into this class? i.e. some sort of `tryUpdateSnapshot(maxOffset)`?
278791963,6363,ewencp,2019-04-26T02:35:24Z,"I haven't worked through all of them, but I think at least `leaderState()` and `currentProtocolVersion()` can be private. with ^^ suggested update, I think some of the `config[X]Snapshot()` methods could be as well.

we should try to be conservative about what internal state we're exposing. `configState` and `leaderState` in particular worry me more since making that mutable by other classes makes this code a lot harder to reason about."
278792696,6363,ewencp,2019-04-26T02:41:45Z,"nit: i found this terminology a bit weird. do we (or something else) use `embed` like this normally? If not using a builder pattern but wanting a static method instead of public constructor to build it (which, tbh, seems like a fine solution to me...), I would expect `build(..., ...)` or similar."
278795951,6363,ewencp,2019-04-26T03:07:44Z,appears to be unused?
278796064,6363,ewencp,2019-04-26T03:08:38Z,sigh... one day this class will either disappear or become more than a mostly useless pass-through that is mostly here just for the constructor and fields...
278796757,6363,ewencp,2019-04-26T03:14:29Z,"might want to extend this comment to explain updated conditions, i.e. that in original mode all connectors and tasks are revoked, but in newer modes it may be a partial list. this is internal, so not critical, but if you're updating some of the other TODO javadocs, this would be another nice improvement"
278798597,6363,ewencp,2019-04-26T03:28:13Z,"nit: just naming, but there's only one `ClusterConfigState` in this test... extra number not necessary"
278805479,6363,ewencp,2019-04-26T04:29:09Z,same question about `COOPERATIVE` enum option as before
278806121,6363,ewencp,2019-04-26T04:34:43Z,"we should enable this for sure, at least at some level. they get stored in truncated logs if successful, and (as of recently) we store full logs for debugging purposes if the test fails. tbh, still not sure how we ended up in a log-free state by default -- it just results in having to turn this on manually, and if test failures are not reproducible locally and only in jenkins, this is a huge pain.

in fact, i'd happily merge this as a separate PR even before this PR :) aside from performance or excessive logging, for tests there isn't much downside in ratcheting up the log level"
278806552,6363,ewencp,2019-04-26T04:38:52Z,"i think nm on this, pretty sure it was moved from other code. just hard to see without loading the full hidden diffs in GitHub review"
278806717,6363,ewencp,2019-04-26T04:40:29Z,"primary question here is what coverage looks like. `DistributedHerder.java` now has paths that consider both protocol versions, but looks like these tests are only using `V0` paths. is that ok? based on herder changes, I think LOC coverage may not have changed too much, but critical differences might not be well tested now?"
278807647,6363,ewencp,2019-04-26T04:49:36Z,are we missing other parameterizations? only have one here... `COOPERATIVE`?
278807823,6363,ewencp,2019-04-26T04:51:38Z,logger or remove?
278809207,6363,ewencp,2019-04-26T05:04:48Z,this is never a good sign in a test like this. what does this accomplish?
278810850,6363,ewencp,2019-04-26T05:18:23Z,"might want to highlight the `balanced` pieces below here, that's ultimately the critical bit beyond just running what we expect (which tbh, makes the most important part of these tests hard to find)"
278812987,6363,ewencp,2019-04-26T05:36:38Z,"just be wary that this entire loop capturing state is a bit dangerous. of course you don't expect it to happen, but it's possible there is a rebalance (unintentionally due to timeouts) and you get some inconsistent set of results wrt connector state/location.

at a bare minimum, tons of repeated tests locally and many repeated tests on jenkins would be warranted here to avoid any potential flakiness, especially given AK jenkins' penchant for unexpected timing issues."
278816564,6363,ewencp,2019-04-26T06:00:17Z,this is the kind of assertion that could become flaky given incremental population of `connectors`.
279022852,6363,kkonstantine,2019-04-26T16:40:18Z,"You are right, this used to be bad, but for a few java versions now, the compiler is able to substitute [constant string concatenation expressions](https://docs.oracle.com/javase/specs/jls/se8/html/jls-15.html#jls-15.18.1) to `StringBuilder` on the bytecode. 

For this class, what I get from: 
`javap -c ConnectAssignment.class | less` is:  

```
public java.lang.String toString();
    Code:
       0: new           #13                 // class java/lang/StringBuilder
       3: dup
       4: invokespecial #14                 // Method java/lang/StringBuilder.""<init>"":()V
       7: ldc           #15                 // String Assignment{error=
       9: invokevirtual #16                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
      12: aload_0
      13: invokevirtual #17                 // Method error:()S
      16: invokevirtual #18                 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder;
      19: ldc           #19                 // String , leader='
      21: invokevirtual #16                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
      24: aload_0
      25: invokevirtual #20                 // Method leader:()Ljava/lang/String;
      28: invokevirtual #16                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
      31: bipush        39
      33: invokevirtual #21                 // Method java/lang/StringBuilder.append:(C)Ljava/lang/StringBuilder;
      36: ldc           #22                 // String , leaderUrl='
      38: invokevirtual #16                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
      41: aload_0
      42: invokevirtual #23                 // Method leaderUrl:()Ljava/lang/String;
      45: invokevirtual #16                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
      48: bipush        39
      50: invokevirtual #21                 // Method java/lang/StringBuilder.append:(C)Ljava/lang/StringBuilder;
      53: ldc           #24                 // String , offset=
      55: invokevirtual #16                 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder;
      58: aload_0
... etc
```

Based on that I avoid `+` in loops but in constant expression as this one I keep using `+` for readability. 
I'm inclined to leave it here, since I'd expect this transformation will be common among java compilers but I can also hardcode `StringBuilder`. WDYT?"
279027503,6363,kkonstantine,2019-04-26T16:54:39Z,"Chose to be on the safe side and map to what we've been doing already in `ConnectProtocol#asMap`:

https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocol.java#L239
and 
https://github.com/apache/kafka/blob/trunk/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/ConnectProtocol.java#L247

If we feel confident to diverge on the new protocol (slightly here), I'm happy to do that. "
279086507,6363,kkonstantine,2019-04-26T20:04:33Z,"Since we use `assert` in a few places already, I'll use `assert` here too. 
Let me know if you had something else in mind. "
279086549,6363,kkonstantine,2019-04-26T20:04:40Z,Done here and below
279087129,6363,kkonstantine,2019-04-26T20:06:37Z,Typo since this was added after V1 javadocs. However the descriptions are correct. Nothing should be V1 in this class. 
279087208,6363,kkonstantine,2019-04-26T20:06:56Z,Same typo as above. Format is correct
279088293,6363,kkonstantine,2019-04-26T20:11:01Z,"@ewencp if I understand correctly, you are suggesting something similar to what @mumrah suggested. 
Keep case insensitivity but remove the map. I'll apply this change. "
279088613,6363,kkonstantine,2019-04-26T20:12:09Z,"Comments are not in chronological order :) I hope you find inline validations useful and we can keep this. 
Should I submit another PR with some unit tests?"
279089415,6363,kkonstantine,2019-04-26T20:14:45Z,"Indeed, this code is not an addition. It's moved from `WorkerGroupMember` _as-is_
To your point, here, every time I reload I search for ""load diff"" and I load the hidden files. Thankfully only 5 or 6 so far :)"
279100216,6363,kkonstantine,2019-04-26T20:52:48Z,Got a second comment on that. Changed to runtime check. 
279135611,6363,kkonstantine,2019-04-27T00:28:05Z,"It was moved inside `onJoinPrepare`. That's because now `onRevoked` might also be called in `onJoinComplete`. But this wouldn't mean that a ""rebalance started""."
279136107,6363,kkonstantine,2019-04-27T00:36:22Z,"I did inject it elsewhere, especially after @mumrah's comment. But this was somehow missed even though I thought I grepped. Fixed"
279136319,6363,kkonstantine,2019-04-27T00:39:55Z,"You are bringing up a good point. I considered `ConnectAssignment` more involed and worthy of factoring out to its own class, but not `WorkerState`. 

We haven't decided what we'll do yet, but I'm more inclined to suggest `ConnectAssignment` and `ConnectWorkerState` as separate classes. WDYT?"
279136441,6363,kkonstantine,2019-04-27T00:41:59Z,`COOPERATIVE` was removed as per the latest comments on the KIP (the point was brought up by @hachikuji). There's was a missed javadoc reference which I've just removed. Only `EAGER` and `COMPATIBLE` atm. 
279136636,6363,kkonstantine,2019-04-27T00:45:28Z,I agree
279136696,6363,kkonstantine,2019-04-27T00:46:30Z,"Maybe you started reviewing when it hadn't been removed. As mentioned in a comment above, `COOPERATIVE` has been removed to reflect the KIP. "
279136963,6363,kkonstantine,2019-04-27T00:51:25Z,I thought I had a use of it :) Removed now. 
279137024,6363,kkonstantine,2019-04-27T00:52:32Z,:) If I understand correctly this is not actionable immediately 
279586499,6363,kkonstantine,2019-04-30T00:33:51Z,Make sense. Updated!
279587083,6363,kkonstantine,2019-04-30T00:38:25Z,I'm also hugely in favor of tests that don't depend on `sleep`. Probably this was left here since earlier stages of manual debugging with these tests. Removed from all tests. If needed the assertions will be improved. 
279587559,6363,kkonstantine,2019-04-30T00:41:53Z,"Indeed. I'm saving this comment of yours for better verification of flakiness. Given that reviews are in progress still, I haven't run integration tests in repeated mode, but I'll definitely do so before merging and will tighten the assertions if needed."
279587997,6363,kkonstantine,2019-04-30T00:45:04Z,Maybe you are referring to the assertion for imbalanced assignment? Because the assignment being unique should be a strong deterministic requirement. But I agree with a holistic review of the assertions here. 
279588163,6363,kkonstantine,2019-04-30T00:46:18Z,Fixed!
279588578,6363,kkonstantine,2019-04-30T00:49:31Z,"Indeed, the unit testing gaps currently are found here here and in `WorkerCoordinatorIncrementalTest`. 
These will be filled asap. But wanted to first get a round of reviews on the protocol, because these tests will pin the checks on the proposed functionality tbh (as opposed to integration tests for example). Definitely a known gap atm. "
279588748,6363,kkonstantine,2019-04-30T00:50:41Z,"So, `COOPERATIVE` was removed, but I left the parameterization just in case we go back and for between `EAGER` and `COMPATIBLE` still here (or the original `WorkerCoordinatorTest`). I might as well remove it if not. "
279588838,6363,kkonstantine,2019-04-30T00:51:20Z,Debugging leftover probably. 
279589061,6363,kkonstantine,2019-04-30T00:53:14Z,"As mentioned above, `COOPERATIVE` was removed. Left, in case some common tests are covered by both (although the assertions might be affected - hence the 1 failure I've left hanging around still so we don't have the impression that we are done with those unit tests). "
279589343,6363,kkonstantine,2019-04-30T00:55:29Z,"Removed, thanks!"
279590175,6363,kkonstantine,2019-04-30T01:02:09Z,"Totally agree too. This is indeed weird given that the two commits that refer to this file, don't show evidence of when this change happened. 

I'll submit another PR that can backported. Thanks!"
280291175,6363,rhauch,2019-05-02T05:20:11Z,"Okay, that makes sense. But it might be nice to denote that requirement with a short comment."
280291571,6363,rhauch,2019-05-02T05:24:51Z,I agree with @ewencp (on a [previous comment](https://github.com/apache/kafka/pull/6363/files/#r277947904)) that a validator on the scheduled rebalance max delay is probably pretty beneficial to ensure non-negative values.
280292191,6363,rhauch,2019-05-02T05:30:53Z,+1
280292897,6363,rhauch,2019-05-02T05:36:54Z,"Wouldn't it be helpful here to have more debug or trace logs in this method? I worry that without them, it's going to be hard to track what this logic is doing. It may makes sense to refactor a bit to have a single return preceded by a log message, rather than having multiple log messages for each of the returns."
280292977,6363,rhauch,2019-05-02T05:37:44Z,Same comment hear about adding more debug/trace logging for most of the branches.
280294446,6363,rhauch,2019-05-02T05:51:26Z,"And given Ewen's [point above](https://github.com/apache/kafka/pull/6363/files/#r278752888), if `completeWorkerAssignment` is not different than `currentWorkerAssignment`, then will `connectorAssignments` be any different than before, and `taskAssignments` be any different than before?"
280294974,6363,rhauch,2019-05-02T05:56:18Z,"It'd be really great to have unit tests for many of these methods. The `performTaskAssignment(...)` method is already pretty lengthy, and there are just a few unit tests whereas there seem to be lots of permutations and branches. Not only would they help with confidence, but they'd help with regression testing if/when we have to get back into this code."
280295034,6363,rhauch,2019-05-02T05:56:45Z,Ping
280295298,6363,rhauch,2019-05-02T05:58:58Z,"Right, but everything in the for loop starting on line 335 (except for the return on line 347) is dealing with debug logging, right? If so, then line 336 could be moved into this debug-only block."
280295967,6363,rhauch,2019-05-02T06:04:40Z,"Isn't it possible that some of these are 0 due to integer division truncating? Seems like all 4 of these lines could be replaced with something like:
```
log.debug(""Assigning {} connectors and {} tasks to {} workers, was previously on {} workers"",
        totalActiveConnectorsNum, totalActiveTasksNum, totalWorkersNum, existingWorkersNum);
```"
280296761,6363,rhauch,2019-05-02T06:10:52Z,"+1. The value of logging in this whole class is not really going to be knowing the final end state, but being able to track the logic in this class to figure out why the end state doesn't match an expected state on some user's Connect cluster, and hopefully being able to learn enough to understand what paths are taken so that we can reproduce the case.

The other approach is to log all of the initial state up front and all of the new state at the end, such that with that information we could reproduce any logged scenario with a unit test that we can then debug to trace the logic."
280296881,6363,rhauch,2019-05-02T06:11:51Z,Ping
280297070,6363,rhauch,2019-05-02T06:13:38Z,I think it'd be useful to have just one style of JavaDoc.
280297285,6363,rhauch,2019-05-02T06:15:29Z,+1 on the separate classes.
280297897,6363,rhauch,2019-05-02T06:20:00Z,"Right, and that would be easier to unit test, too."
283517005,6363,kkonstantine,2019-05-13T20:15:26Z,"As mentioned this was an insightful observation. Returning to this after I added several tests on the assignor code. We have the following two cases: 

1) The assignment is computed by the assignor, it contains only revocations and it fails to be delivered. In the next round of rebalancing this failure to change the state of revoked tasks is detected by the assignor, and instead of skipping revocation according to its policy that mandates that there's no consecutive revocations, it re-applies revocation of tasks. This should happen until the assignment containing the revoked tasks succeeds. 
2) The assignment is computed by the assignor, it contains only assignments and it fails to be delivered. Distinguishing this case from the case that active assignments are detected as lost, would result in a more complicated logic of the assignor's state machine. Instead of doing that, the current code, selects to interpret the failed assignment as referring to lost tasks and therefore it enters the deferred rebalancing period, with the specified delay. I believe this keeps things simpler. In a sense whether a worker goes down, or an assignment fails to be delivered, are both considered failures that takes us to the deferred rebalancing logic. 

The current unit tests currently confirm this behavior. WDYT @mumrah ?"
283518084,6363,kkonstantine,2019-05-13T20:18:05Z,"My rule of thumb is that I include in a if statement only the debug statements that would result in eager (and potential wasteful) computation of print arguments. The rest of the debug logging is kept outside the if branch, which will allow us to remove the if branch altogether if the line that requires it is removed. Otherwise, we risk keeping redundant `if`s around. "
283528795,6363,kkonstantine,2019-05-13T20:44:26Z,"This test class, now called `IncrementalCooperativeAssignorTest` has been significantly extended to include a big set of tests for the new assignor. Resolving this comment and will follow up on other test additions. "
283529644,6363,kkonstantine,2019-05-13T20:46:37Z,Changed to an explicit call to `COMPATIBLE.protocol()`. Resolving this comment. Thanks
283530355,6363,mumrah,2019-05-13T20:48:19Z,I think that sounds reasonable. Thanks for the follow-up @kkonstantine!
283530780,6363,kkonstantine,2019-05-13T20:49:28Z,`sleep` statements have been removed. 
283553403,6363,kkonstantine,2019-05-13T21:54:32Z,Fixed
283555350,6363,kkonstantine,2019-05-13T22:00:53Z,Removed
283557138,6363,kkonstantine,2019-05-13T22:07:18Z,I see your point. When I initially imported this comment _as-is_ I considered it non-javadoc comment and then I added javadoc. Upgraded as javadoc now. 
283557471,6363,kkonstantine,2019-05-13T22:08:34Z,Fixed. `stream` is used now instead of a static map. 
283558392,6363,kkonstantine,2019-05-13T22:11:56Z,"Given the generated bytecode, I'll keep the current form of `toString` if you don't mind. Resolving here, but feel free to comment if you feel otherwise. "
283562360,6363,kkonstantine,2019-05-13T22:27:52Z,"Added `between(0, Integer.MAX_VALUE)`"
284026277,6363,rhauch,2019-05-14T22:34:22Z,"I see what Ewen is suggesting - it definitely could be confusing. Concretely, at a minimum, rename  `ConnectAssignment` -> `IncrementalCooperativeAssignment` to tie it more closely with the `IncrementalCooperativeConnectProtocol`. (Even if we think it *might* be useful in a future protocol, we can refactor if/when that happens.)

The design in this PR doesn't follow the `ConsumerProtocol` and `ConsumerAssignor` model too closely. There, the assignor methods dealt with `Subscription` and `Assignment` types, and so those were defined in the `ConsumerAssigner` class. The `ConsumerProtocol` just serialized those types.

In the current design, the `ConnectAssignor` doesn't deal with the `Assignment` and `WorkerState` types, and so defining those types in the `ConnectAssignor` class doesn't make much sense. Instead, the different `ConnectAssignor` subclasses use their respective protocol in their implementation of `performAssignment(...)`.

Given this difference, I suggest that we simply rename all of the classes that go with the incremental cooperative protocol / assignor to begin with `IncrementalCooperative`."
284026633,6363,rhauch,2019-05-14T22:35:33Z,+1
284035771,6363,rhauch,2019-05-14T23:15:43Z,"When a user has this in their logs, what do they do? Maybe add a bit more detail to this log message that lets them know what action they could/should perform, if any. 

When would the member configs be empty?"
284037825,6363,rhauch,2019-05-14T23:25:06Z,Thoughts on this now that you've been running this a while in tests and in soak?
284066405,6363,rhauch,2019-05-15T02:25:31Z,"Is there _any_ chance we might have `totalWorkersNum` might be 0, resulting in termination of the worker?"
284067290,6363,rhauch,2019-05-15T02:31:49Z,"`ConnectorsAndTasks.embed(new ArrayList<>(), new ArrayList<>())` is called 5 places in non-test code. WDYT about adding a `ConnectorsAndTasks.create()` method to reduce the code a bit?"
284067493,6363,rhauch,2019-05-15T02:33:14Z,"If you believe debug logging is sufficient at this point to be able to track behavior based only on logs, then go ahead and resolve this conversation. If not, maybe consider adding more in key places."
284067892,6363,kkonstantine,2019-05-15T02:35:47Z,"Actually, the current form, that prints each assignment set separately has been working out nicely in the logs even with lots of connectors. What I think is key, is to use `assignments: ` suffix (or similar) in the end. This way you can grep/collect these lines all together focusing on the logger of this class as well (either via log4j or again by grepping). The fact that these assignments (every set) are printed in a separate line makes the lineage of the assignment process easy to follow. I'll review their final message but again, I'm inclined to retain a common substring and have each one in their own line. Rebalancing is happening in certain moments, so there's no significant issue with verbosity here. "
284068513,6363,rhauch,2019-05-15T02:39:42Z,These methods will sort `completeWorkerAssignment` -- is that worth a comment above these two lines?
284068845,6363,rhauch,2019-05-15T02:42:03Z,I guess I'm a bit surprised that this method modifies `workerAssignments`. Maybe mention in the `@param` description here and in `assignConnectors(...)`?
284069392,6363,rhauch,2019-05-15T02:45:59Z,"These assign methods are pretty boilerplate, with I think just 2 lines in each that is distinct (the log line and the worker.assign call). Did you consider pulling into a method and supplying a bi-function that takes the ConnectorTaskId and the Worker?"
284069861,6363,kkonstantine,2019-05-15T02:49:38Z,"It's mutated by: 
```
assignConnectors(completeWorkerAssignment, newSubmissions.connectors());
assignTasks(completeWorkerAssignment, newSubmissions.tasks());
```

I see your point and we might be able to consolidate. But I feel that this is higher risk low value optimization at this point. I'm more inclined to leave a comment for a future refactoring. "
284070224,6363,rhauch,2019-05-15T02:52:11Z,"One thing that would be nice to have: what are the characteristics of the parameters and returned map? For example, is it possible that the returned map contain null `ByteBuffer` reference, and if so what does that mean?"
284070727,6363,rhauch,2019-05-15T02:55:39Z,"But at least on the first line of this method, wouldn't it make sense to use an `ConnectAssignment.isEmpty()` method here, rather than relying upon instance equality? Same behavior, but that would seem less brittle."
284071019,6363,rhauch,2019-05-15T02:57:18Z,"I added a comment regarding this regarding the `ConnectAssignor.performAssignment(...)` method's JavaDoc. IIUC, it's possible that the returned `Map<String, ByteBuffer>` can contain a null byte buffer reference. Is null allowed / handled everywhere this method (and others) are used?"
284071662,6363,rhauch,2019-05-15T03:01:55Z,Though wouldn't it be helpful to have the names of the classes related to the incremental protocol all start with `IncrementalCooperative`? This is related to my earlier comment (previous review).
284072023,6363,rhauch,2019-05-15T03:04:32Z,+1
284072326,6363,kkonstantine,2019-05-15T03:06:58Z,These are now specifically tested in `IncrementalCooperativeAssignorTest` alone and as part of the tests cases for `performTaskAssignment`. Also tested in `WorkerCoordinatorIncrementalTest`
284072920,6363,rhauch,2019-05-15T03:10:59Z,"We're starting 4 connectors, so should we check that all 4 are running?"
284072977,6363,rhauch,2019-05-15T03:11:28Z,"We're starting 4 connectors, so should we check that all 4 are running?"
284073066,6363,rhauch,2019-05-15T03:12:15Z,"@kkonstantine, can you provide an update?
"
284073384,6363,rhauch,2019-05-15T03:14:30Z,@kkonstantine can you provide an update?
284074000,6363,rhauch,2019-05-15T03:18:48Z,What is the status of these tests that are commented out?
284365134,6363,kkonstantine,2019-05-15T17:22:13Z,"Indeed, when a `JoinGroupResponse` is received with no errors (`Errors.NONE` in errors field) the response will have at least the leader itself as a member. Removed this warning because it's a no-op. "
284366320,6363,kkonstantine,2019-05-15T17:25:18Z,"True. I'd like as part of this PR to apply minimal or no changes to V0 (now eager) protocol. 
Also, as opposed to `ConnectProtocol` the overlap between the two assignor implementations is quite small, so I think subclassing is not worth it here. I suggest we consider a refactoring (including any helper methods in utils classes) in a subsequent iteration/cleanup. "
284366792,6363,kkonstantine,2019-05-15T17:26:35Z,Good point. Moved to builder pattern
284367865,6363,kkonstantine,2019-05-15T17:29:02Z,We need the aggregated result of the assignment and creating a new list that we return as a result doesn't worth it I think. I'll update the javadoc to make it more clear. The signature of the method without a return type should also be a hint. 
284370260,6363,kkonstantine,2019-05-15T17:34:35Z,I see your point. But given that currently we provide only reference equality I wouldn't like to hide this fact within an `isEmpty` method that would implement this reference equality check. I wouldn't like this to be confused with an assignment that it just has no assigned or revoked resources at the moment. 
284374327,6363,kkonstantine,2019-05-15T17:45:05Z,Fixed and improved all the logs in this method
284375015,6363,kkonstantine,2019-05-15T17:46:44Z,Fixed by introducing builders for both inner classes here. 
284375778,6363,kkonstantine,2019-05-15T17:48:22Z,"This remains the case. But for reasons of symmetry with `ConnectorsAndTasks` which is similar I'm inclined to retain this for now, if that's ok. "
284381964,6363,kkonstantine,2019-05-15T18:02:28Z,I'll add a temp edit to `jenkins.sh` and will run the Connect tests (unit and integration) in repeat today. Locally I have not noticed flakiness after several runs. 
284388778,6363,kkonstantine,2019-05-15T18:19:34Z,Leftovers of an early import of tests. Removed. 
284477385,6363,kkonstantine,2019-05-15T22:32:41Z,https://github.com/apache/kafka/pull/6745
284539491,6363,kkonstantine,2019-05-16T05:00:16Z,"As discussed briefly offline, the new classes are now both separate public classes and are called `ExtendedAssignment` and `ExtendedWorkerState`. One of the primary intentions is to leave the current protocol version (V0) with minimal changes. A subsequent refactoring in one of the next version could consolidate the two classes (maybe with a better name). "
284539787,6363,kkonstantine,2019-05-16T05:02:43Z,"Dynamic dispatch is not very easy or elegant here, because the protocol classes are effectively static classes (have only static methods) and we can't hold on to a reference on their instance and use inheritance effectively. I've moved the creation of `JoinGroupRequestProtocolCollection` in the protocols though, and now the `switch` block (which I also dislike compared to dynamic dispatch) is small. "
284541423,6363,kkonstantine,2019-05-16T05:14:14Z,The above PR is merged. I removed this temp commit. Thanks for reviewing!
284542971,6363,kkonstantine,2019-05-16T05:25:34Z,"As mentioned in another thread, the classes are now named `ExtendedAssignment` and `ExtendedWorkerState` respectively"
284544064,6363,kkonstantine,2019-05-16T05:32:48Z,I have now added tests related to the new rebalancing protocol in this class too. (also the worker coordinator tests as well). 
284834540,6363,kkonstantine,2019-05-16T18:13:23Z,"Added:
`        // We have at least one worker assignment (the leader itself) so totalWorkersNum can't be 0`"
284834759,6363,kkonstantine,2019-05-16T18:13:54Z,Same reason why we removed a warning about empty `memberConfigs` above
284835203,6363,kkonstantine,2019-05-16T18:15:00Z,"This block now reads:
```java

        // We have at least one worker assignment (the leader itself) so totalWorkersNum can't be 0
        log.debug(""Previous rounded down (floor) average number of connectors per worker {}"", totalActiveConnectorsNum / existingWorkersNum);
        int floorConnectors = totalActiveConnectorsNum / totalWorkersNum;
        log.debug(""New rounded down (floor) average number of connectors per worker {}"", floorConnectors);

        log.debug(""Previous rounded down (floor) average number of tasks per worker {}"", totalActiveTasksNum / existingWorkersNum);
        int floorTasks = totalActiveTasksNum / totalWorkersNum;
        log.debug(""New rounded down (floor) average number of tasks per worker {}"", floorTasks);
```

We shouldn't mind these averages being 0. This means there's not much to revoke. "
284843672,6363,kkonstantine,2019-05-16T18:36:05Z,Added
284845286,6363,kkonstantine,2019-05-16T18:39:52Z,This sorting should not matter in any decision. It does it to apply weighted round-robin internally. But the order in the list of assignments does not matter.
284845721,6363,kkonstantine,2019-05-16T18:40:50Z,"I've added and improved debug logs. If during additional testing we consider that some info logs are warranted, we could return here. "
284846327,6363,kkonstantine,2019-05-16T18:42:15Z,Add clarification. 
284846980,6363,kkonstantine,2019-05-16T18:43:45Z,I'll create a ticket for future refactoring to address this. 
284847996,6363,kkonstantine,2019-05-16T18:46:11Z,Made `leaderState` private. But `currentProtocolVersion` is used by `WorkerGroupMember`
284849064,6363,kkonstantine,2019-05-16T18:48:59Z,`null` value for the `ByteBuffer` value of this map is not allowed. Every member should have an assignment even if it's empty. 
284849278,6363,kkonstantine,2019-05-16T18:49:35Z,At this point I will clear this if needed in a subsequent cleanup/refactoring after more testing is applied. 
284849517,6363,kkonstantine,2019-05-16T18:50:12Z,This is also candidate for subsequent refactoring. Will create a jira ticket. 
284849946,6363,kkonstantine,2019-05-16T18:51:18Z,"I'll leave the old method unchanged, but we can also consider enhancement during the refactoring described in other comments that will follow-up. "
284850119,6363,kkonstantine,2019-05-16T18:51:48Z,"Also, leaving as-is since it's the old code. But will revisit"
284850476,6363,kkonstantine,2019-05-16T18:52:51Z,At this point I will avoid the risk of optimizations and will include this in the ticket that will revisit tuning of the new rebalancing code. Thanks!
284853764,6363,kkonstantine,2019-05-16T19:01:54Z,I'm disabling the assertion at the moment because it results in flaky runs on jenkins. Will return to it in a separate PR. Thanks!
284876254,6363,ryannedolan,2019-05-16T20:06:57Z,"Instead of BiConsumer here, and relying on exceptions to pass validation errors back to the caller, maybe this should be `(k, v) -> error`? "
284879355,6363,ryannedolan,2019-05-16T20:15:45Z,"Would be nice to roll up this re-throw into ensureValid(), i.e. catch the exception and format a general error message there."
284879788,6363,ryannedolan,2019-05-16T20:16:54Z,It's strange to rely on RuntimeExceptions for validation like this. Maybe narrow to ConfigExceptions at least.
284882011,6363,kkonstantine,2019-05-16T20:22:55Z,I think the current pattern works. I'll resolve this comment. 
284882177,6363,ryannedolan,2019-05-16T20:23:22Z, 
284882599,6363,kkonstantine,2019-05-16T20:24:25Z,Good points. Let's revisit in a follow up cleanup/refactoring since this is not critical atm. 
284883108,6363,kkonstantine,2019-05-16T20:25:43Z,"Again here the difficulty is part due to protocol classes being effectively static. Since it's not critical for the initial version of the new rebalancing, let's revisit in a follow-up. "
284883316,6363,kkonstantine,2019-05-16T20:26:08Z,See comment above about protocol classes being effectively static. 
284884565,6363,kkonstantine,2019-05-16T20:29:30Z,Made `protected` and added several tests in `IncrementalCooperativeAssignorTest`
284886129,6363,ryannedolan,2019-05-16T20:33:54Z,I think this is a lot of gymnastics to avoid writing a couple `if`s and `for`s.
284886324,6363,kkonstantine,2019-05-16T20:34:28Z,"Since this doesn't influence the actual schemas of the protocols, I'd also suggest to punt to the next refactoring/cleanup of the protocol classes. "
284919786,6363,kkonstantine,2019-05-16T22:20:49Z,The current form maps exactly to `ensureValid` in the Validator interface. This addition here is minor. I'd suggest discussion any improvements on config defs in a separate PR/jira issue. 
284920523,6363,kkonstantine,2019-05-16T22:24:05Z,"Again, this is added for convenience. It's not implementing the validation as other validators do in this class. What developers chooses to throw here, is their own responsibility, as when they implement the `Validator` interface. "
284920787,6363,kkonstantine,2019-05-16T22:25:25Z,"Not a bad idea. But at the moment, I'd suggest taking improvements on this code in a separate PR"
284921328,6363,kkonstantine,2019-05-16T22:27:32Z,This is a two line lambda still and straightforward. Still not in any critical path of this PR. 
284921479,6363,kkonstantine,2019-05-16T22:28:18Z,I've kept uniqueness requirement but have removed assertion around balancing atm. 
284952509,6363,kkonstantine,2019-05-17T01:34:30Z,"10 successful consecutive runs of all the connect tests (including integration tests). 
https://builds.apache.org/job/kafka-pr-jdk8-scala2.11/21871/console"
284952789,6363,kkonstantine,2019-05-17T01:36:30Z,Here just confirming that they've started to go to the next phase (which is to remove a worker). Not an assertion of the connectors per se. But I agree we could tighten it in a subsequent iteration. 
220729441,5582,rajinisivaram,2018-09-26T21:27:38Z,I remember I had `supportsClientReauth` and `supportsServerReauth` in my commit. It may be worth checking if we can have a single `supportsReauth` method and have the caller track mode.
220729477,5582,rajinisivaram,2018-09-26T21:27:44Z,Confusing to have `supportsClientReauth` and `clientSupportsReauthentication`. Perhaps `supportsReauth` instead of `supportsClientReauth` and `remoteSupportsReauth` instead of `clientSupportsReauthentication` (or something along those lines)?
220730428,5582,rajinisivaram,2018-09-26T21:31:36Z,"Can't all this be done in the authenticator? We can move it to some shared class later if we need it in two places, but for now we just need this in `SaslClientAuthenticator`?"
220732429,5582,rajinisivaram,2018-09-26T21:39:59Z,I thought we weren't supporting re-authentication or connection termination for SSL. Why do we need time?
220732576,5582,rajinisivaram,2018-09-26T21:40:38Z,"As before with SSL, can we avoid tracking time unless it is actually used?"
220735918,5582,rajinisivaram,2018-09-26T21:54:37Z,We have to avoid invoking an extra `time.milliseconds()` for every request. Actual time should get propagated from the caller.
220736421,5582,rajinisivaram,2018-09-26T21:56:25Z,"`KafkaChannel` is a network-layer class, not a security-related class, So this should perhaps say `authenticationSession` or something like that to make it more obvious."
220736573,5582,rajinisivaram,2018-09-26T21:57:12Z,Why do we need this?
220740223,5582,rajinisivaram,2018-09-26T22:14:17Z,"Seeing it here, the metric name doesn't look right. `v0` refers to SaslAuthenticateRequest version. Perhaps `sasl-authentication` with tag `version=0` or something similar to our request metrics which has apikey and version would be better?"
220740427,5582,rajinisivaram,2018-09-26T22:15:20Z,Why are we tracking time here?
220741020,5582,rajinisivaram,2018-09-26T22:18:09Z,Should this be `SESSION_LIFETIME_MS`?
220741102,5582,rajinisivaram,2018-09-26T22:18:29Z,As before `sessionLifetimeMs`?
220742581,5582,rajinisivaram,2018-09-26T22:25:48Z,"Do we need to track `interestedInWritingImmediatelyAfterReauthentication`? Aren't we starting reauthentication during write, so this would always be true?"
220745052,5582,rajinisivaram,2018-09-26T22:38:53Z,The comment is now incomplete. Don't we still throttle in some cases?
220745279,5582,rajinisivaram,2018-09-26T22:40:03Z,This code looks totally out of place here.
220786458,5582,rondagostino,2018-09-27T03:49:07Z," I eliminated these two methods by adding `Long clientSessionReauthenticationTimeMs()` and `Long serverSessionExpirationTimeNanos()`.  These new methods only ever return non-null on the client and server, respectively.  `clientSessionReauthenticationTimeMs()` now contains the 85%-95% calculation that was in `KafkaChannel` and you asked if it could be moved into the authenticator."
220786477,5582,rondagostino,2018-09-27T03:49:21Z, I've renamed this to be `boolean connectedClientSupportsReauthentication()` which is clear and no longer is close to any other method names since I eliminated the other two that were close as described above.
220786498,5582,rondagostino,2018-09-27T03:49:30Z, I moved this into `SaslClientAuthenticator` -- the percentage calculation is now factored into the return value of `clientSessionReauthenticationTimeMs()`
220786511,5582,rondagostino,2018-09-27T03:49:40Z," We don't need time.  I added it in order to support the `long sessionBeginTimeMs()` method I had added to `Authenticator`, but now that the percentage calculation is moved into `SaslClientAuthenticator` we don't need the `sessionBeginTimeMs()` method on `Authenticator` anymore.   This file is now untouched."
220786528,5582,rondagostino,2018-09-27T03:49:47Z," Agreed, same as above: I added it in order to support the `long sessionBeginTimeMs()` method I had added to `Authenticator`, but now that the percentage calculation is moved into `SaslClientAuthenticator` we don't need the `sessionBeginTimeMs()` method on `Authenticator` anymore.   This file is now untouched."
220786549,5582,rondagostino,2018-09-27T03:49:59Z, I moved the server-side-kill check from `KafkaApis` to `KafkaRequestHandler` and now I pass in the nanos time that was already calculated.
220786564,5582,rondagostino,2018-09-27T03:50:08Z, It is now `boolean serverAuthenticationSessionExpired(long nanos)`
220786574,5582,rondagostino,2018-09-27T03:50:12Z," We don't need this.  I added it in order to support the `long sessionBeginTimeMs()` method I had added to `Authenticator`, but now that the percentage calculation is moved into `SaslClientAuthenticator` we don't need the `sessionBeginTimeMs()` method on `Authenticator` anymore.   This file is now untouched."
220786590,5582,rondagostino,2018-09-27T03:50:19Z," We don't need this.  I added it in order to support the `long sessionBeginTimeMs()` method I had added to `Authenticator`, but now that the percentage calculation is moved into `SaslClientAuthenticator` we don't need the `sessionBeginTimeMs()` method on `Authenticator` anymore.   This file is now untouched."
220786604,5582,rondagostino,2018-09-27T03:50:30Z," Yes, it is always true -- I removed it."
220786622,5582,rondagostino,2018-09-27T03:50:40Z," I moved it to `KafkaRequestHandler` which also has the side-effect of making a nanosecond time value available for the comparison, which means we can eliminate the call to `time.milliseconds()` on every request that you had flagged as problematic."
220789008,5582,rondagostino,2018-09-27T04:13:55Z,"@rajinisivaram I realized today that we also have to count the connections from older clients that do not send SASL_AUTHENTICATE.  So it might be good to not refer to ""version"" at all.  Maybe it is best to not refer to ""sasl"" either?  How about `successful-authentication-no-reauth` as the metric name?  Then maybe we don't need a special tag?  If we still do need a tag then maybe `reauthSupport=false` to avoid referring to ""version""?"
220789699,5582,rondagostino,2018-09-27T04:21:00Z,"@rajinisivaram Not sure.  I changed the names of the methods on `Authenticator` so that we now have `serverSessionExpirationTimeNanos()` and `clientSessionReauthenticationTimeMs()`.  This value is communicating the expiration time, and then the `SaslClientAuthenticator` applies the 85%-95% factor to get a re-authentication time.  Based on this, it probably shouldn't have ""REAUTH"" in the name since that implies the factor has been applied -- which it has not at this point.  So I think yes, it should be SESSION_LIFETIME_MS.  Do you agree?  if so I will change -- just confirm or suggest otherwise."
220791745,5582,rondagostino,2018-09-27T04:41:24Z, I adjusted the Javadoc.  Let me know if it is now more accurate.
220917846,5582,rondagostino,2018-09-27T13:15:36Z,"Oops, the file is not untouched -- just the sections related to SSL and Plaintext are untouched."
220917981,5582,rondagostino,2018-09-27T13:15:59Z,"Again, oops, the file is not untouched -- just the sections related to SSL and Plaintext are untouched."
220919003,5582,rondagostino,2018-09-27T13:19:09Z,"Oops, the file is not untouched because it still needs to send in a `Supplier<Authenticator>` rather than `Authenticator`  when it creates a `KafkaChannel` -- but the reference to `Time` is now gone."
220919332,5582,rondagostino,2018-09-27T13:20:07Z,"Again, oops, the file is not untouched because it still needs to send in a `Supplier<Authenticator>` rather than `Authenticator`  when it creates a `KafkaChannel` -- but the reference to `Time` is now gone."
221238033,5582,rondagostino,2018-09-28T12:37:26Z, Changing to SESSION_LIFETIME_MS
221238079,5582,rondagostino,2018-09-28T12:37:35Z, Changing to sessionLifetimeMs
221241246,5582,rondagostino,2018-09-28T12:49:59Z,Changing to `successful-authentication-no-reauth-total` without extra tags.
221574183,5582,rajinisivaram,2018-10-01T11:23:19Z,Using a combination of nanos and millis in the reauthentication logic is very error-prone.
221575447,5582,rajinisivaram,2018-10-01T11:28:04Z,Propagate time since we have tests that use MockTime.
221576245,5582,rajinisivaram,2018-10-01T11:31:42Z,This is either a local client connection (on a client or inter-broker connection) or a remote client connection (on a broker).
221576504,5582,rajinisivaram,2018-10-01T11:32:58Z,Can we move this below the `final` fields?
221580005,5582,rajinisivaram,2018-10-01T11:47:25Z,Should this be under the `channel.successfulAuthentications() == 1`? Presumably a client can use v0 authenticate request and still reauthenticate.
221580843,5582,rajinisivaram,2018-10-01T11:50:51Z,`responsesReceivedDuringReauthentication.forEach`? Also null check looks unnecessary since we guarantee it is never null?
221581167,5582,rajinisivaram,2018-10-01T11:52:23Z,Could just be part of the previous `if` statement?
221581375,5582,rajinisivaram,2018-10-01T11:53:24Z,Add `-ms` to the metric names?
221581926,5582,rajinisivaram,2018-10-01T11:55:42Z,typo: `LIFETIME`
221582659,5582,rajinisivaram,2018-10-01T11:58:42Z,Need to either change the name of this constant or where it is defined. Negotiated configs dont come from JAAS.
221583408,5582,rajinisivaram,2018-10-01T12:02:03Z,Also set state to `SEND_HANDSHAKE_REQUEST`
221583541,5582,rajinisivaram,2018-10-01T12:02:42Z,This check is not needed if we set initial state during reauthentication to `SEND_HANDSHAKE_REQUEST`
221583677,5582,rajinisivaram,2018-10-01T12:03:17Z,"Same as before, we dont need this if initial state is set for reauthentication,"
221584414,5582,rajinisivaram,2018-10-01T12:06:25Z,"This should only be done for re-authentication. Otherwise it will end up with tight poll loop after authentication if there is nothing to be sent. In general, we need to make sure that we don't change any behaviour for the initial authentication."
221585462,5582,rajinisivaram,2018-10-01T12:10:56Z,"On the broker-side, I think we need a minimum re-authentication interval as well to restrict the rate at which clients re-authenticate. This is particularly important since we dont apply any quotas for authentication. Without imposing a re-authentication rate limit, a client that enters a re-authentication loop (due to a bug or intentionally) would effectively stop the broker from doing anything useful."
221587034,5582,rajinisivaram,2018-10-01T12:16:57Z,We have an exception that is going to result in the connection being closed. Seems unnecessary to re-authenticate before closing connection.
221587359,5582,rajinisivaram,2018-10-01T12:18:29Z,Initiaize in the constructor similar to other fields?
221588748,5582,rajinisivaram,2018-10-01T12:23:34Z,"Can't we set buffers to a good state from `reauthenticate()` so that this method doesn't have to do anything special for re-authentication? We have received a handshake request, we just need to continue just as with authentication?"
221589195,5582,rajinisivaram,2018-10-01T12:25:18Z,nit: Can we reduce the length of the method name?
221591025,5582,rajinisivaram,2018-10-01T12:32:01Z,"In each of the tests, we could check metrics (expired-and-killed in some tests and re-authenticated where expected)"
221592475,5582,rajinisivaram,2018-10-01T12:37:22Z,This results in errors logged in the broker with stack trace. I don't think we want that.
221824499,5582,rondagostino,2018-10-02T04:47:12Z," Changed so that both use nanos.  We need to use nanoseconds on the server side to avoid a call to time.milliseconds() on each request as per previous review comments, so I changed the client side to use nanos as well."
221824522,5582,rondagostino,2018-10-02T04:47:24Z,"This class uses the TIME variable in multiple places already, so I simply replicated the same solution.  If we wish to fix all of them then perhaps it can be resolved via a separate ticket as opposed to this KIP?"
221824530,5582,rondagostino,2018-10-02T04:47:31Z, Definitely; fixed.
221824536,5582,rondagostino,2018-10-02T04:47:36Z, moved
221824549,5582,rondagostino,2018-10-02T04:47:46Z,"We do not record latency for authentication, which is the case where `channel.successfulAuthentications() == 1 ` -- the value `channel.reauthenticationLatencyMs()` will be null in that case."
221824558,5582,rondagostino,2018-10-02T04:47:54Z,"Converted to `.forEach`, but we do have to check for null since that is the default (Javadoc says null is possible, which it is)"
221824568,5582,rondagostino,2018-10-02T04:48:00Z, fixed
221824578,5582,rondagostino,2018-10-02T04:48:06Z,"`request-latency-avg` and `request-latency-max` don't have it (neither do `commit-latency`, `poll-latency`, and `process-latency` metrics). Assume this means it should remain as-is."
221824588,5582,rondagostino,2018-10-02T04:48:11Z, fixed
221824625,5582,rondagostino,2018-10-02T04:48:16Z, Created `org.apache.kafka.common.security.authenticator.SaslUtils` to hold this constant.
221824632,5582,rondagostino,2018-10-02T04:48:23Z, I created a `PROCESS_APIVERSIONS_RESPONSE` state where we process the response we got -- either from the server in the auth case or from the previous authenticator in the re-auth case.  This will be the initial state when re-authenticating.  I think this makes it clear what is going on.
221824639,5582,rondagostino,2018-10-02T04:48:29Z, removed
221824664,5582,rondagostino,2018-10-02T04:48:45Z," Agreed, no longer needed -- the initial state for client-side re-authentication is now `PROCESS_APIVERSIONS_RESPONSE`."
221824673,5582,rondagostino,2018-10-02T04:48:51Z," fixed.  I think maybe the original code had kept track of whether we we needed to immediately write something or not, and perhaps it was handled there?  Can't remember, but regardless, it is fixed now -- we remove write interest as before when we are not in a ""re-authentication"" scenario."
221824682,5582,rondagostino,2018-10-02T04:48:59Z," I defined a 1-second requirement that applies to the second and subsequent re-authentications.  So the first re-authentication can happen immediately after authentication if desired, but the second re-authentication must then happen at least 1 second later (and so on), otherwise the SASL handshake is passed through without beginning the re-authentication process (and that would mean the connection is closed, which also results in the client experience the newly-implemented DDoS delay).  There are two reasons I set it for the second re-auth.  One is is because `SaslAuthenticatorTest` would end up running longer if we had to set the interval that much longer.  The other is that we don't have a time value that we can use to set the start time until `KafkaChannel.maybeBeginServerReauthentication()` is invoked.  I think this is reasonable -- is it okay with you?"
221824691,5582,rondagostino,2018-10-02T04:49:06Z,"Actually, my understanding of this is that we received something that had been sent prior to the re-authentication process beginning -- it doesn't match what we were expecting back, so we didn't send it, so it must have come from before -- so we have to save it so it can be processed later.  Does that make sense?  If so, then I think this code is correct."
221824708,5582,rondagostino,2018-10-02T04:49:11Z, fixed
221824720,5582,rondagostino,2018-10-02T04:49:20Z," Good point.  I split out the `processPayload()` method to address a cyclomatic complexity issue, and I did not see the simplification at that time.  Done."
221824731,5582,rondagostino,2018-10-02T04:49:24Z, fixed
221824759,5582,rondagostino,2018-10-02T04:49:41Z, Removed
221824978,5582,rondagostino,2018-10-02T04:52:09Z,"Was unable to get this working, and it is almost 1am here at this point.  Will look again ASAP."
221852361,5582,rondagostino,2018-10-02T07:50:04Z,"Actually, I just got it working."
221980284,5582,mk6i,2018-10-02T14:43:37Z,"I am confused by what this part of this sentence, can you elaborate?

```
 that is then subsequently used for any purpose other than re-authentication.
```
"
221988637,5582,rondagostino,2018-10-02T15:02:55Z,"@mkaminski1988 Here's the quote from the KIP related to this:

> From a behavior perspective on the server (broker) side, when the expired-connection-kill feature is enabled with a positive value the broker will communicate a session time via SASL_AUTHENTICATE and will close a connection when the connection is used past the expiration time and the specific API request is not directly related to re-authentication (SaslHandshakeRequest and SaslAuthenticateRequest).  In other words, if a connection sits idle, it will not be closed  something unrelated to re-authentication must traverse the connection before a disconnect will occur.

Does that clarify it?"
222097603,5582,rajinisivaram,2018-10-02T20:20:08Z,`listener.name.sasl_ssl.oauthbearer.connection.max.expired.ms`?
222103261,5582,rajinisivaram,2018-10-02T20:38:38Z,Do you mean in a different class? Because `ChannelBuilders` didn't have `TIME` before this PR.
222104230,5582,rajinisivaram,2018-10-02T20:41:42Z,"nit: We initialize all other instance variables in the constructor, can we do the same here? I dont think we need the initialzations, especially nulls."
222105405,5582,rajinisivaram,2018-10-02T20:45:20Z,"Don't think we close the connection on processing SaslHandshakeRequest, we simply fail the request. Do we want to close the connection for this case?"
222106188,5582,rajinisivaram,2018-10-02T20:48:04Z,`!ready()` here is an `IllegalStateException` since we never expect to here with with an not-ready channel?
222106589,5582,rajinisivaram,2018-10-02T20:49:10Z,"Since this is used only as the minimum reauthentication interval, can give it a name that indicates its usage?"
222108202,5582,rajinisivaram,2018-10-02T20:54:05Z,We could use the same name `reauthenticationLatencyMs` for the method in authenticator as well? Or use `reauthenticationElapsedTimeMs` in both cases?
222111763,5582,rajinisivaram,2018-10-02T21:05:26Z,Do we really need this method - test could just create one using hard-coded params?
222115747,5582,rajinisivaram,2018-10-02T21:18:41Z,"Now that we dont support Java 7, we can use lambdas for these conditions (and the ones below)."
222116790,5582,rajinisivaram,2018-10-02T21:22:30Z,Is this used?
222117341,5582,rajinisivaram,2018-10-02T21:24:36Z,nit: `else if`?
222118885,5582,rajinisivaram,2018-10-02T21:30:08Z,These two method use names that are not consistent with the config name.
222119990,5582,rajinisivaram,2018-10-02T21:34:35Z,"So many tests with these two calls, couldn't we just have additional parameters to `verifyAuthenticationMetrics` that optionally verify reauthentication and no-reauth metrics?"
222121974,5582,rajinisivaram,2018-10-02T21:42:18Z,"As before, can we move initializations to the constructor and remove unnecessary null iniitializations?"
222122932,5582,rajinisivaram,2018-10-02T21:46:03Z,"Yes, this code is fine."
222123268,5582,rajinisivaram,2018-10-02T21:47:24Z,"Same as before, null initializations unnecessary."
222124202,5582,rajinisivaram,2018-10-02T21:51:04Z,Can we create an inner class to store the re-authentication state? There are just too many of these relevant only for re-authentication.
222125964,5582,rajinisivaram,2018-10-02T21:58:50Z,"Why is this code here? `reauthenticate()` has the SASL request, so it can do this check specific to reauthentication."
222127047,5582,rajinisivaram,2018-10-02T22:03:34Z,We should be able to set up the state and call `authenticate` without separating out the methods.
222144498,5582,harshach,2018-10-02T23:40:10Z,"is it going to be used for sasl only if so can you make it ""sasl.connection.max.reauth.ms"""
222161863,5582,rondagostino,2018-10-03T01:51:29Z,I think `listener.name.sasl_ssl.oauthbearer.connections.max.reauth.ms`?
222164887,5582,rondagostino,2018-10-03T02:18:47Z,"Oops, sorry, you are correct.  Fixed."
222358775,5582,rondagostino,2018-10-03T15:33:28Z, 
222359402,5582,rondagostino,2018-10-03T15:35:11Z,"Ah, you are correct, we fail the request in KafkaApis and the connection is nor closed.  Doc adjusted accordingly here."
222360598,5582,rondagostino,2018-10-03T15:38:22Z, `MIN_REAUTH_INTERVAL_ONE_SECOND_NANOS`
222361705,5582,rondagostino,2018-10-03T15:41:08Z, Renamed `Authenticator` method to match this one: `reauthenticationLatencyMs`
222390337,5582,rondagostino,2018-10-03T17:04:46Z, We now throw `IllegalStateException` if that occurs
222452923,5582,rondagostino,2018-10-03T20:18:01Z, Removed it.
222476244,5582,rondagostino,2018-10-03T21:37:07Z, Added `@FunctionalInterface` annotation to `TestCondition` and implemented lambdas here.  Did not adjust any other uses of `TestCondition` anywhere else unrelated to this PR.
222477179,5582,rondagostino,2018-10-03T21:41:11Z, Removed.  This file is no longer affected by this PR.
222477623,5582,rondagostino,2018-10-03T21:42:51Z, Done
222478232,5582,rondagostino,2018-10-03T21:45:26Z, Fixed
222478999,5582,rondagostino,2018-10-03T21:48:38Z,@rajinisivaram I believe there was a desire to keep our options open in this regard and not mention SASL explicitly -- thoughts?
222479519,5582,rondagostino,2018-10-03T21:50:49Z, Done
222480744,5582,rondagostino,2018-10-03T21:55:41Z,@rajinisivaram Just want to confirm that this is acceptable.
222486198,5582,rondagostino,2018-10-03T22:19:53Z," Done.  Also shortened the max session reauth ms value from 500 ms to 100 ms, which shaved 20 seconds off of the test runtime (85 seconds dropped to 65 seconds locally).  I'm interested to see if this exposes any errors when run on the build farm."
222500340,5582,rondagostino,2018-10-03T23:39:47Z, Fixed
222501837,5582,rondagostino,2018-10-03T23:49:12Z,"@rajinisivaram The `reauthenticate()` method has the `NetworkReceive` instance.  We don't parse that and extract the `SaslHandshakeRequest` that it contains until later in the flow, at this point in the code.  Is it okay to keep the changed mechanism check here, as-is?"
222509254,5582,rondagostino,2018-10-04T00:39:49Z," Created a `private static class ReauthInfo` that has three public final fields:
>`String previousSaslMechanism;`
>`public final KafkaPrincipal previousKafkaPrincipal;`
>`public final long reauthenticationBeginNanos;`

Eliminated the `private boolean reauthenticating;` field with `private ReauthInfo reauthInfo;`
"
222901371,5582,rondagostino,2018-10-05T06:19:47Z, I added a new `REAUTH_PROCESS_HANDSHAKE` state and that is where re-authentication starts.
223997295,5582,rajinisivaram,2018-10-10T09:16:02Z,"Yes, since this config is used by the broker to force reauthentication using connection termination as well, there is no reason why we can't apply it for SSL as well in future. So it makes sense to keep it neutral."
224002046,5582,rajinisivaram,2018-10-10T09:29:31Z,Dont think we need the `muteState` check on the server-side since we are processing a received packet when we invoke this.
224005987,5582,rajinisivaram,2018-10-10T09:39:25Z,Is there a reason why this is passing in `time.milliseconds` while the others don't? There is some scope to use a common time value in all of these records to avoid multiple calls to `time.milliseconds()`.
224007360,5582,rajinisivaram,2018-10-10T09:42:49Z,Could this just use `currentTimeNanos`?
224008571,5582,rajinisivaram,2018-10-10T09:46:24Z,"Move this to the end of the class? We tend to have enum definitions at the start, but typically other inner classes at the end."
224009396,5582,rajinisivaram,2018-10-10T09:48:26Z,just return `pendingAuthenticatedReceives` and remove the check for `null` in Selector?
224009880,5582,rajinisivaram,2018-10-10T09:49:51Z,update comment since reauth starts in the state above?
224010118,5582,rajinisivaram,2018-10-10T09:50:36Z,Include `REAUTH` in the state name?
224010735,5582,rajinisivaram,2018-10-10T09:52:28Z,"Isn't this the same as `INITIAL`? From this state onwards, we could use common states? Perhaps you have them separate to make it easy to fall through rather than loop back to the start of `authenticate()` when handshake response is received. If it is hard to keep a common state, it is fine to leave as-is."
224012583,5582,rajinisivaram,2018-10-10T09:58:35Z,"Couldn't we move all four of these (or some of these) into `reauthInfo`? We could create `reauthInfo` early on and populate reauthentication metadata into it. If we really need to separate out fields related to next reauth from the fields related to current reauth, perhaps we could have a separate class with these fields."
224014312,5582,rajinisivaram,2018-10-10T10:04:18Z,Move `Math.min` to `saslAuthenticateVersion()` to avoid duplication?
224017228,5582,rajinisivaram,2018-10-10T10:14:56Z,"This comment is odd because that is not quite what we would do. If we can't fall through, we would put `authenticate` in a loop to process the next state."
224017388,5582,rajinisivaram,2018-10-10T10:15:26Z,"If we need to keep `INITIAL` and `REAUTH_INITIAL`, we should have. a method to use common code for this state."
224018091,5582,rajinisivaram,2018-10-10T10:17:43Z,"Do we need this check at all? If it isn't, it is a bug in the implementation and we would see a ClassCastException with the class names."
224018434,5582,rajinisivaram,2018-10-10T10:18:55Z,"If we always stored `apiVersionsResponseFromOriginalAuthentication` in `reauthInfo`, we can avoid this check."
224021054,5582,rajinisivaram,2018-10-10T10:28:24Z,"Looks like we are getting `time.milliseconds()` just for logging. Log entries contain date and time anyway, so we could just log the intervals we actually use instead of computing new ones just for logging."
224021468,5582,rajinisivaram,2018-10-10T10:30:00Z,nit: unnecessary `Long.valueOf`
224023926,5582,rajinisivaram,2018-10-10T10:39:22Z,Move this class to the end to be consistent with other classes?
224024097,5582,rajinisivaram,2018-10-10T10:39:54Z,Can we move these three fields into `ReauthInfo`?
224024425,5582,rajinisivaram,2018-10-10T10:41:11Z,"Add checkstyle suppression instead for this file, rather than split the method with a checkstyle comment."
224025199,5582,rajinisivaram,2018-10-10T10:44:19Z,"nit: This is not a `Utils` class, more like `Configs`?"
224026269,5582,rajinisivaram,2018-10-10T10:48:29Z,"nit: too many boolean params, making it hard to know what this is doing."
224027337,5582,rajinisivaram,2018-10-10T10:52:17Z,Why do we need this boolean?
224028035,5582,rajinisivaram,2018-10-10T10:55:07Z,I can't tell from the method name what the function returned is.
224028525,5582,rajinisivaram,2018-10-10T10:56:47Z,why?
224029495,5582,rajinisivaram,2018-10-10T10:59:26Z,why is this in `TestUtils`?
224031032,5582,rajinisivaram,2018-10-10T11:05:24Z,"We are doing at least two `time.nanoseconds` calls per channel, can we get the value at the start and use it in the three usages here."
224031497,5582,rajinisivaram,2018-10-10T11:07:25Z,"Not sure this matches the actual implementation,"
224033627,5582,rajinisivaram,2018-10-10T11:16:04Z,"I think parameterization was useful at the start, but not sure we want to commit that. There are several tests where this parameter is not used at all, causing the same tests to be run twice. It feels like we should add some extra reauthentication tests and perhaps update some existing tests to also verify reauthentication. I think a new test which waits for multiple reauthentications while sending and receiving data continuously is sufficient (run with SASL_PLAINTEXT and SASL_SSL). It could run with a mechanism where reauthentication latency is higher and the test could run in a loop until latency > 0 to avoid timing errors in the test. All existing tests can stay as-is and use the existing `server.verifyAuthenticationMetrics()` that just checks that there are no reauthentications. A new version of that with reauthentication count as arg could check for reauthentication metrics as well. What do you think?"
224035834,5582,rajinisivaram,2018-10-10T11:24:51Z,Do we need a boolean here? Are there tests where it is guaranteed to be > 0?
224041831,5582,rajinisivaram,2018-10-10T11:47:25Z,Same as in SaslClientAuthenticator - unnecessary check since ClassCastException gives all the information required in case there is a bug in the code.
224042409,5582,rajinisivaram,2018-10-10T11:49:16Z,do we need this method?
224042825,5582,rajinisivaram,2018-10-10T11:50:51Z,"Why is this conditional, can't we always set it?"
224043656,5582,rajinisivaram,2018-10-10T11:53:15Z,Need `Ms` in the method name since it is returning millis.
224044350,5582,rajinisivaram,2018-10-10T11:55:17Z,Just `credentialExpirationMs` is sufficient since this is not the server's credential?
224045454,5582,rajinisivaram,2018-10-10T11:59:08Z,Can we move this into `ReauthInfo` (rename that class if required)?
224045546,5582,rajinisivaram,2018-10-10T11:59:27Z,"Same as above, move to `ReauthInfo`?"
224045843,5582,rajinisivaram,2018-10-10T12:00:34Z,This can be in `reauthInfo` as well. And `ReauthInfo` can have a method `authenticating()` or `reauthenticating()` to avoid all the checks for `null`. Same for ClientAuthenticator as well.
224162804,5582,rondagostino,2018-10-10T17:04:57Z, Fixed
224165591,5582,rondagostino,2018-10-10T17:13:46Z, Now define `long readyTimeMs = time.milliseconds()` at the top and use that time value for all metric `record()` calls.
224189017,5582,rondagostino,2018-10-10T18:20:58Z,"I don't think we can use `currentTimeNanos` because it represents the time when `poll()` was invoked, and that could be as far back in time as the timeout value (i.e. a large timeout value could in theory result in `java.nio.channels.Selector.select()` blocking for quite a while).  If we use a time to far in the past it increases the chance of the reauth decision yielding `false` when in fact it should yield `true` -- and in that case we could end up with our connection being killed.

What we can do is reuse the `channelStartTimeNanos` if it is available (i.e. `channelStartTimeNanos != 0 ? channelStartTimeNanos : time.nanoseconds()`) since it is a very recent value.  Furthermore, we can delay actually getting the time as long as possible -- and therefore maybe we don't need to calculate it at all -- by making `KafkaChannel` accept a `Supplier<Long>` rather than a `long`.  I've made both of these changes."
224209301,5582,rondagostino,2018-10-10T19:21:42Z, Moved.
224210770,5582,rondagostino,2018-10-10T19:26:35Z, Return value is now always non-null.
224211024,5582,rondagostino,2018-10-10T19:27:28Z, Fixed
224250048,5582,rondagostino,2018-10-10T21:35:37Z, Now include `REAUTH_` prefix for all re-authentication states.
224250994,5582,rondagostino,2018-10-10T21:39:12Z,"Yeah, it is a fall-through issue.  Will keep code as-is given that the duplicated code is only two lines:
```
sendSaslClientToken(new byte[0], true);
setSaslState(SaslState.INTERMEDIATE);
```
However, in light of the comment below, I will refactor this out into a common method."
224252644,5582,rondagostino,2018-10-10T21:45:34Z, Created `private static class AuthInfoForReauth` to hold all of these in one place.
224270430,5582,rondagostino,2018-10-10T23:11:04Z," Done, we now send in the `ApiVersionsResponse` instance."
224271563,5582,rondagostino,2018-10-10T23:17:28Z, Fixed the comment -- it now states that we won't add the loop to minimize changes.
224271904,5582,rondagostino,2018-10-10T23:19:29Z, Added method `sendInitialTokenAndSetIntermediateState()`
224272371,5582,rondagostino,2018-10-10T23:22:31Z, Removed.
224273014,5582,rondagostino,2018-10-10T23:26:45Z,`reauthInfo` will be null for the `SaslClientAuthenticator` instance associated with the initial authentication; that instance will have an instance of `AuthInfoForReauth` to hold the `ApiVersionsResponse` received from the broker.  This check is therefore necessary under these conditions.
224276026,5582,rondagostino,2018-10-10T23:46:27Z, Done
224276273,5582,rondagostino,2018-10-10T23:47:58Z, Removed
224436894,5582,rondagostino,2018-10-11T12:58:25Z, Moved
224439886,5582,rondagostino,2018-10-11T13:07:00Z,Created `private static class AuthInfoForReauth`
224442030,5582,rondagostino,2018-10-11T13:13:28Z, Done
224444370,5582,rondagostino,2018-10-11T13:19:45Z,"Renamed it `SaslInternalConfigs` (there is already a `org.apache.kafka.common.config.SaslConfigs` class, and it is part of the public API, so it is not an appropriate place to put this internal constant)."
224506417,5582,rondagostino,2018-10-11T15:57:43Z," Created `public enum MetricType` and the method `public void waitForMetrics(String namePrefix, final double expectedValue, Set<MetricType> metricTypes)`.  This method now looks like this:
```
    public void verifyAuthenticationMetrics(int successfulAuthentications, final int failedAuthentications)
            throws InterruptedException {
        waitForMetrics(""successful-authentication"", successfulAuthentications,
                MetricType.setOf(MetricType.TOTAL, MetricType.RATE));
        waitForMetrics(""failed-authentication"", failedAuthentications,
                MetricType.setOf(MetricType.TOTAL, MetricType.RATE));
    }

```"
224508051,5582,rondagostino,2018-10-11T16:02:15Z," This is now removed, and the expected/actual values are now included as part of the assertion error message."
224509330,5582,rondagostino,2018-10-11T16:05:44Z," True that it is not really reusable; `NioEchoServer` needs that functionality, so now it's a `private static boolean` method on that class rather than a `public static boolean` method on `TestUtils`."
224514170,5582,rondagostino,2018-10-11T16:20:30Z,"The above change resulted in a disallowed import error, so I refactored out the following method and put it back into `TestUtils`. This is better than having the whole `maybeBeginServerReauthentication()` method there.
```
    public static ApiKeys apiKeyFrom(NetworkReceive networkReceive) {
        return RequestHeader.parse(networkReceive.payload().duplicate()).apiKey();
    }
```"
224523568,5582,rondagostino,2018-10-11T16:49:37Z,"Good question!  There was a comment above that method that stated:
```
        // Check that we can serialize, deserialize and serialize again
        // We don't check for equality or hashCode because it is likely to fail for any response containing a HashMap
```
 I didn't think about it much; I just read that comment and figured that since I'm making a change to `SaslAuthenticateRequest` and `SaslAuthenticateResponse` and they don't contain a `HashMap` I could -- and should -- test for equality and hashCode.

But now that you ask, and I do spend the time to think about it, it seems that testing equality and hashCode doesn't provide the value I thought it would (and that the comment seemed to imply that it would except for the annoying tendency of a HashMap to screw up the results)!  All we would be testing for is to make sure the result of serializing a request to a `Struct` can be deserialized back to a request and then serialized again to an equivalent `Struct`.  In other words, it doesn't actually test that the serialization code (i.e. `SaslAuthenticateResponse.toStruct()`) is working perfectly -- the equality and hashCode tests will still succeed even if that code serializes a field incorrectly because the same field will be serialized incorrectly both times (for example).

Note that incorrect serialization would presumably be caught indirectly via failure of other unit or integration tests.

What maybe has to change here is the original comment.  Should I adjusted it?"
224526490,5582,rondagostino,2018-10-11T16:59:07Z,"Ok, it's now just a single call to `time.nanoseconds()` under all circumstances except for when `SASL_HANDSHAKE_REQUEST` is sent and either 1) re-authentication is not enabled; or 2) re-authentication is enabled but it occurred less than a second ago.  In these two cases `time.nanoseconds()` will be invoked twice, but it probably doesn't matter since this is a rare occurrence and we are going to send an error back to the client if/when it happens."
224600485,5582,rondagostino,2018-10-11T20:52:28Z,"Fixed.  There is now a processor-level metric `expired-connections-killed-count` that tracks the value on a per-(listener,processor) basis as well as an aggregated sum of these to provide a broker-wide metric.  The aggregated sum metric is called `ExpiredConnectionsKilledCount`.  The `ops.html` doc is also updated to reflect this."
224643320,5582,rondagostino,2018-10-12T00:36:27Z, Removed
224643550,5582,rondagostino,2018-10-12T00:38:09Z,No -- removed.
224644898,5582,rondagostino,2018-10-12T00:49:37Z, Changed.
224645195,5582,rondagostino,2018-10-12T00:52:07Z, Renamed
224645547,5582,rondagostino,2018-10-12T00:54:51Z, Moved
224645875,5582,rondagostino,2018-10-12T00:57:22Z, Moved
224646884,5582,rondagostino,2018-10-12T01:06:23Z, I moved `authenticationOrReauthenticationText()` into `AuthInfoForReauth` and added a `private boolean reauthenticating()` method to `SaslServerAuthenticator`.
224647371,5582,rondagostino,2018-10-12T01:11:20Z,Did the same thing for `SaslClientAuthenticator`.
224650731,5582,rondagostino,2018-10-12T01:42:13Z,"Actually, after seeing some other review comments, I decided to do what you said:

> create reauthInfo early on and populate reauthentication metadata into it.

There is now a `ReauthInfo` class only -- no additional `AuthInfoForReauth` class -- and we add the re-authentication data to it."
224652056,5582,rondagostino,2018-10-12T01:53:43Z,"Now that I decided to get rid of the `AuthInfoForReauth` the code looks like this:
```
ApiVersionsResponse apiVersionsResponseFromOriginalAuthentication = previousSaslClientAuthenticator.reauthInfo
        .apiVersionsResponse();
```"
224652317,5582,rondagostino,2018-10-12T01:56:07Z,I decided to get rid of `AuthInfoForReauth` and keep everything in a single `ReauthInfo` instance as you originally suggested.
224888052,5582,rondagostino,2018-10-12T19:09:14Z,"The client sends multiple `SASL_AUTHENTICATE` requests, and I was thinking the client is known to support the latest version if it sends at least one of them with the required version.  I realize it is unlikely to send different versions each time, but technically if it sends the required version just once then we know it supports that version.  So I was trying to take that (admittedly rare) possibility into account.  Another way to do it would be:
```
reauthInfo.connectedClientSupportsReauthentication = reauthInfo.connectedClientSupportsReauthentication
        || version > 0;
```
I figured the way I did it was better than that since it avoids the write when the value is already true.

I can always set it if you feel that is more appropriate -- just let me know, otherwise I'll leave it as-is."
224890138,5582,rondagostino,2018-10-12T19:17:38Z,"SASL PLAIN authenticates so quickly in the test case that the latency is 0, so if I check to make sure there is a non-zero latency in that case the test fails.  I know you have a comment below about adjusting the unit tests to not be so wasteful. I may be able to get rid of this as I do that."
224907777,5582,rondagostino,2018-10-12T20:34:55Z,"In the meantime, I added a comment describing the reasoning."
225389120,5582,rondagostino,2018-10-16T03:52:30Z,"I eliminated this problem by always recording a latency of at least 1 ms when there is a non-zero latency to record.  So now 100,000 nanoseconds of latency is recorded as 1 ms, for example."
225389599,5582,rondagostino,2018-10-16T03:56:59Z,I just added a new `REAUTH_BAD_MECHANISM` state on the `SaslServerAuthenticator` because a change in the mechanism wasn't being recorded as a failed re-authentication in the metrics.  Now it is being recorded correctly.
225389740,5582,rondagostino,2018-10-16T03:58:28Z,Resolved.  I now record a latency of at least 1 ms when there is any non-zero latency.
225389970,5582,rondagostino,2018-10-16T04:00:32Z,"Latest commit attempts to resolve this.  All mechanisms include at least 1 re-authentication test.  There is a multiple mechanism re-authentication test; there are tests for changing the principal, changing the mechanism, and re-authenticating too fast; and there is a test that continually sends data over the connection until the number of re-authentications is 5."
227157687,5582,rajinisivaram,2018-10-22T22:16:49Z,Do we need these two fields `clientSessionReauthenticationTimeNanos` and `serverSessionExpirationTimeNanos`? Couldn't we just use `authenticator.clientSessionReauthenticationTimeNanos()` and `authenticator.serverSessionExpirationTimeNanos()`?
227158301,5582,rajinisivaram,2018-10-22T22:19:31Z,Reword exception message since channel is not ready on receiving first handshake?
227159398,5582,rajinisivaram,2018-10-22T22:24:42Z,Can we move this into `Authenticator`?
227159491,5582,rajinisivaram,2018-10-22T22:25:04Z,Can we move this into `Authenticator`?
227160254,5582,rajinisivaram,2018-10-22T22:28:35Z,"There are so many methods in KafkaChannel that simply call a method in `authenticator`. I wonder if it would be better to add an `authenticator()` method and let callers directly use `authenticator`, reducing the amount of code in `KafkaChannel`."
227162913,5582,rajinisivaram,2018-10-22T22:40:48Z,Not sure why this needs to be a Suppiler of time rather than the value itself. The code would be more readable with just a value.
227164081,5582,rajinisivaram,2018-10-22T22:46:25Z,"This no longer reflects the sequence of states, so it will be good to add javadoc for `SaslState` showing the two sequences for initial auth and reauth. I would probably move the REAUTH states to the end so that INITIAL flows through to COMPLETE."
227164773,5582,rajinisivaram,2018-10-22T22:49:51Z,Leave `setSaslState(SaslState.INTERMEDIATE)` here similar to other cases?
227165780,5582,rajinisivaram,2018-10-22T22:54:21Z,"As with client `SaslState`, since states don't flow through, we should have comment for `SaslState` showing the two sequences for initial auth and reauth. Again, I would move REAUTH to the end so that `INITIAL_REQUEST` flows through to `COMPLETE`."
227166408,5582,rajinisivaram,2018-10-22T22:57:10Z,Not sure we need this state. We can set state to FAILED and throw the appropriate exception.
227168000,5582,rajinisivaram,2018-10-22T23:04:47Z,"Looking at just this code, it looks like we record this for auth and reauth, even though it actually happens only once as expected. It would be more readable to move this code under the `if (channel.successfulAuthentications() == 1)`."
227168301,5582,rajinisivaram,2018-10-22T23:06:23Z,Why can't we just throw `SaslAuthenticationException` here?
227168794,5582,rajinisivaram,2018-10-22T23:09:04Z,Not used?
227168816,5582,rajinisivaram,2018-10-22T23:09:12Z,Not used?
227169229,5582,rajinisivaram,2018-10-22T23:11:14Z,"Not sure whether these methods add any value since you could just use `EnumSet.of` instead of `MetricType.setOf`? And actually looking at `waitForMetrics`, it would be even better to use varargs."
227170791,5582,rajinisivaram,2018-10-22T23:19:42Z,We don't need a try-finally block when tearDown does the cleanup.
227170823,5582,rajinisivaram,2018-10-22T23:19:53Z,We don't need a try-finally block when tearDown does the cleanup.
227170844,5582,rajinisivaram,2018-10-22T23:20:00Z,We don't need a try-finally block when tearDown does the cleanup.
227170925,5582,rajinisivaram,2018-10-22T23:20:23Z,We don't need a try-finally block when tearDown does the cleanup.
227170945,5582,rajinisivaram,2018-10-22T23:20:30Z,We don't need a try-finally block when tearDown does the cleanup. Lots of these changes in this file are unnecessary (and the test is more readable without the try-finally.
227393856,5582,rondagostino,2018-10-23T13:37:45Z, Done
227397762,5582,rondagostino,2018-10-23T13:46:33Z,"Here's what I changed it to.  Not sure if this is what you were getting at.  Let me know if there is something else it should say.  I also fixed the Javadoc -- it was still referring to ""not muted"" when we got rid of that check based on a review comment.

```
        if (!ready())
            throw new IllegalStateException(
                    ""KafkaChannel should be \""ready\"" when processing SASL Handshake for potential re-authentication"");
```
"
227401789,5582,rondagostino,2018-10-23T13:54:48Z,"I don't think so, no.  For one, if we did, then `kafka.network.Processor` would have to be able to get to the `Authenticator` instance, and currently `KafkaChannel` does not expose that publicly.  The guts of this method also refers to state within `KafkaChannel` as well (specifically, `lastReauthenticationStartNanos`), and while we could pass that in instead, fundamentally the `KafkaChannel` instance also has to swap out its `Authenticator` instances.  So I think this method really has to belong in `KafkaChannel`."
227402058,5582,rondagostino,2018-10-23T13:55:19Z,"Similar to above: I don't think so, no, for the same reasons."
227408464,5582,rondagostino,2018-10-23T14:07:56Z,"Not sure.  My gut says the benefit would be minimal and the downside to exposing the `Authenticator` (adding complexity to code that now simply asks the `KafkaChannel` to do stuff) would outweigh any benefit, but regardless, if you decide it is something you would like to do maybe it can be addressed as a separate ticket?"
227409726,5582,rondagostino,2018-10-23T14:10:29Z,"There was a desire to not call `time.nanoseconds()` unnecessarily.  We had a time value already, but it wasn't guaranteed to be current, so we have to get it again; making it a `Supplier` meands we can delay getting it for as long as possible and avoid it completely if the logic is short-circuited first."
227414037,5582,rondagostino,2018-10-23T14:18:22Z, Javadoc added/declarations re-ordered
227418118,5582,rondagostino,2018-10-23T14:26:33Z,"lol.  I changed this based on a previous review comment: `If we need to keep INITIAL and REAUTH_INITIAL, we should have. a method to use common code for this state.` . It didn't seem worthwhile to make a method with just `sendSaslClientToken(new byte[0], true)`  in it, so I put both that and `setSaslState(SaslState.INTERMEDIATE) ` in there and named it accordingly.  I'm good whichever way you want to go -- just let me know if you decide it should be different than what it currently is (i.e. maybe put it back to the way it was?).  Will leave as-is otherwise."
227421513,5582,rondagostino,2018-10-23T14:34:05Z, Javadoc added/declarations re-ordered
227426184,5582,rondagostino,2018-10-23T14:43:53Z, Moved
227450301,5582,rondagostino,2018-10-23T15:37:53Z,See comment below.
227451106,5582,rondagostino,2018-10-23T15:39:43Z,"I added this state because without it the attempt to change the mechanism isn't recorded as a failed re-authentication in the metrics.  If we simply throw the exception then it is caught either by `NioEchoServer.maybeBeginServerReauthentication()` (in the case of a unit test) or by `kafka.network.Processor.processCompletedReceives()` (in real-world use).  In the unit test case the exception is simply ignored, and eventually the channel is closed while we are waiting in vain for the metric to update.  In real-world use the channel would end up being closed -- but again no metric update occurs because the server-side `Selector` never sees the channel as being ""`!channel.ready()`"" -- it needs to see this in order to call `channel.prepare()`, catch the `AuthenticationException`, and record the failed re-authentication in the metric.  I realize it is a bit odd to have to create this extra state, and we can avoid it if we are willing to not record the attempt as a failed re-authentication in the metrics, but I figured we would want to to record it."
227451806,5582,rondagostino,2018-10-23T15:41:21Z, Removed
227451895,5582,rondagostino,2018-10-23T15:41:36Z, Removed
227453157,5582,rondagostino,2018-10-23T15:44:39Z," Agreed, eliminated all the methods and now invoke `EnumSet.of()` directly."
227458546,5582,rondagostino,2018-10-23T15:57:23Z,"Oops, didn't see that cleanup code.  Removed (everywhere)"
227458722,5582,rondagostino,2018-10-23T15:57:48Z, Removed
227458807,5582,rondagostino,2018-10-23T15:57:59Z, Removed
227458896,5582,rondagostino,2018-10-23T15:58:11Z, Removed
227458997,5582,rondagostino,2018-10-23T15:58:27Z, Removed (everywhere)
227798966,5582,rajinisivaram,2018-10-24T13:56:03Z,"Yes, let's leave it as is for now."
227799411,5582,rajinisivaram,2018-10-24T13:57:04Z,Does it really need to be that uptodate? Couldn't we just use the time that we have?
227800879,5582,rajinisivaram,2018-10-24T14:00:27Z,"Looking at the rest of the code, I think it will be good to set the state here like in the rest of the code. Perhaps:
```
sendIntialToken();
setSaslState(SaslState.INTERMEDIATE);
```
"
227801592,5582,rajinisivaram,2018-10-24T14:02:09Z,"Thanks for the explanation. Yes, we do want to record it, so let's keep the state."
228162494,5582,rondagostino,2018-10-25T13:01:00Z,Done
228247041,5582,rondagostino,2018-10-25T16:32:54Z,"Actually, I just realized that `currentTimeNanos` does not represent the time when `poll()` was invoked -- it actually represents the moment after the `select()` returns.  So we can use that value if necessary."
228248803,5582,rondagostino,2018-10-25T16:37:41Z,"Yeah, we can.  I mistakenly thought `currentTimeNanos` represented the time when `poll()` was called, but it is actually the moment when the `select()` call returns, so it is relatively recent.  So now I use the most recent value we have if there is one, otherwise we use `currentTimeNanos`.  I kept it as a `Supplier<>` though -- for symmetry with `KafkaChannel.maybeBeginServerReauthentication()`.  I can pass the long value in directly if you prefer, otherwise I think we're good here now.
"
423906760,8657,chia7712,2020-05-12T17:25:21Z,"It collects the ""key"" used to complete delayed requests. The completion is execute out of group lock."
423907638,8657,chia7712,2020-05-12T17:26:53Z,this is the main change of this PR (address https://github.com/apache/kafka/pull/6915#issuecomment-626423986).
423908084,8657,chia7712,2020-05-12T17:27:34Z,new check for this PR. Make sure it does not hold group lock 
423908468,8657,chia7712,2020-05-12T17:28:09Z,"This test case is for ""tryLock"" so I just remove it."
423913246,8657,chia7712,2020-05-12T17:35:42Z,"This is another case of deadlock. ```DelayedJoin#tryComplete``` is in a **group lock** and it tries to complete other delayed joins which related to same **__consumer_offsets** partition.

Hence, this PR make it control the group lock manually in order to make sure it does not hold group lock when calling ```GroupCoordinator#onCompleteJoin```"
426943627,8657,hachikuji,2020-05-18T23:19:57Z,"Currently we have a somewhat convoluted model where `ReplicaManager` creates delayed operations, but we depend on lower level components like `Partition` to be aware of them and complete them. This breaks encapsulation. 

Not something we should try to complete in this PR, but as an eventual goal, I think we can consider trying to factor delayed operations out of `Partition` so that they can be managed by `ReplicaManager` exclusively. If you assume that is the end state, then we could drop `completeDelayedRequests` and let `ReplicaManager` _always_ be responsible for checking delayed operations after appending to the log. 

Other than `ReplicaManager`, the only caller of this method is `GroupMetadataManager` which uses it during offset expiration. I think the only reason we do this is because we didn't want to waste purgatory space. I don't think that's a good enough reason to go outside the normal flow. It would be simpler to follow the same path. Potentially we could make the callback an `Option` so that we still have a way to avoid polluting the purgatory."
426944679,8657,hachikuji,2020-05-18T23:23:37Z,"Hmm.. Does the group purgatory suffer from the same deadlock potential? If we call `checkAndComplete` for a group ""foo,"" I don't think we would attempt completion for any other group."
426960637,8657,hachikuji,2020-05-19T00:20:05Z,"For reference, here are links to two alternative approaches that I considered earlier this year:

- Async completion: https://github.com/hachikuji/kafka/commit/3bee4acc442f09e95d60bbb7ce9eb246310dcb63
- Lock-safe offset cache: https://github.com/hachikuji/kafka/commit/3705f3303422915020c317dcee08d6c176977d63

I think Jun was not satisfied with the first approach because it called for another thread pool. Its advantage though was a simpler and more intuitive API than what we have here. An idea which I never implemented was to let the request handlers also handle delayed operation completion so that we did not need another thread pool. Basically rather than calling the callback in `DelayedProduce` directly, we add a new operation to the request queues. Obviously this has its own tradeoffs. 

The second commit tries to use lock-free data structures so that we do not need the lock when completing the callback. This was only a partial solution which handled offset commit appends, but not group metadata appends. I am not sure how to handle join group completion asynchronously, so I gave up on this idea.

Only posting in case it's useful to see how some of these alternatives might have looked. I'm ok with the approach here, but I do wish we could come up with a simpler API. One thought I had is whether we could make the need for external completion more explicit. For example, maybe `appendRecords` could return some kind of object which encapsulates purgatory completion.

```scala
val completion = inLock(lock) {
  replicaManager.appendRecords(...)
}
completion.run()
```
Just a thought."
427095614,8657,chia7712,2020-05-19T07:45:43Z,"@hachikuji  thanks for reviews!

> Async completion: hachikuji@3bee4ac

we are on the same page :) (my previous comment https://github.com/apache/kafka/pull/6915#issuecomment-626292057)

> Only posting in case it's useful to see how some of these alternatives might have looked. I'm ok with the approach here, but I do wish we could come up with a simpler API. One thought I had is whether we could make the need for external completion more explicit. For example, maybe appendRecords could return some kind of object which encapsulates purgatory completion.

this style LGTM :)"
427102729,8657,chia7712,2020-05-19T07:57:45Z,"> Does the group purgatory suffer from the same deadlock potential? If we call checkAndComplete for a group ""foo,"" I don't think we would attempt completion for any other group.

you are right. It requires lock for group ""foo"" only. But the potential deadlock I tried to avoid/describe is that the caller has held a lock of group_a and then it tried to complete delayed request for group_b. It is possible to cause deadlock as it requires the lock of group_b to completing delayed request for group_b.

That way the comment says the caller should NOT hold any group lock.


"
427114083,8657,chia7712,2020-05-19T08:16:25Z,"> Potentially we could make the callback an Option so that we still have a way to avoid polluting the purgatory.

pardon me. I fail to catch your point."
427430032,8657,hachikuji,2020-05-19T16:19:03Z,"The join/heartbeat purgatories are a little different from the produce purgatory. The key is based on the groupId, so when we complete an operation for group ""foo,"" we won't complete for group ""bar"" incidentally. At least that is my understanding. If you look at `DelayedOperationPurgatory.checkAndComplete`, we only complete watchers for the passed key."
427432140,8657,hachikuji,2020-05-19T16:22:06Z,"Sorry, let me be clearer:

1. Mainly I'm suggesting moving the delayed operation checking into `ReplicaManager` instead of `Partition`.
2. We can change the call to `appendRecordsToLeader` in `GroupMetadataManager` to go through `ReplicaManager` as well.
3. We could make the callback optional in `ReplicaManager.appendRecords` so that we do not have to add a callback (which appears to be the only reason we write directly to `Partition` from `GroupMetadataManager`).

Anyway, just a suggestion. I thought it might let us keep the completion logic encapsulated in `ReplicaManager`."
427435780,8657,chia7712,2020-05-19T16:27:30Z,"> so when we complete an operation for group ""foo,"" we won't complete for group ""bar"" incidentally. At least that is my understanding. If you look at DelayedOperationPurgatory.checkAndComplete, we only complete watchers for the passed key.

You are totally right. My above comment is not clear. I added the comment to remind developers following code is dangerous.
```scala
lock group_a
completeDelayedJoinRequests for group_b (this line may produce deadlock if there is another thread which is holding lock of group_b and prepare to complete requests for group_a)
release group_a
```
Of course, the above code is nonexistent currently. I'm just worry that the deadlock is easy to produce in the future if we don't notice the lock issue.
"
427723802,8657,chia7712,2020-05-20T03:40:48Z,"> so when we complete an operation for group ""foo,"" we won't complete for group ""bar""

In fact, there is a example of deadlock in join purgatories. 

```GroupCoordinator#onCompleteJoin``` called by ```DelayedJoin#onComplete``` is possible to call ```storeGroup``` to append records to __consumer_offsets-{**partitionFor(group.groupId)**}. If there are groups related to same partition, ```GroupCoordinator#onCompleteJoin``` which is holding a group lock will require locks for other groups."
428949769,8657,junrao,2020-05-21T22:28:41Z,"Perhaps reword like the following?

Returning a map of successfully appended topic partitions and a flag indicting whether the HWM has been incremented. If the caller passes in completeDelayedRequests as false, the caller is expected to complete delayed requests for those returned partitions."
428950361,8657,junrao,2020-05-21T22:30:37Z,"Could we do `localProduceResults.filter{ case (tp, logAppendResult) => ... }` to avoid unnamed references?"
428956078,8657,junrao,2020-05-21T22:48:37Z,Could we add a comment for the return value?
428958530,8657,junrao,2020-05-21T22:56:39Z,Could we add a comment for the return value?
428972538,8657,junrao,2020-05-21T23:47:16Z,"This is bit tricky to untangle. It seems the original code holds the group lock for both the `group.hasAllMembersJoined` check and the call to forceComplete(). So, we probably want to keep doing that. 

I am thinking that we could do the following.
1. Change `GroupCoordinator.onCompleteJoin()` so that (1) it checks group.hasAllMembersJoined inside the group lock and returns whether hasAllMembersJoined is true.
2. In `DelayedJoin.tryComplete() `, we do 
```
        if (GroupCoordinator.onCompleteJoin()) 
               forceComplete() 
}
```
In onComplete(), we do nothing."
430253921,8657,chia7712,2020-05-26T08:47:57Z,"@junrao Could you take a look? I'd like to address @hachikuji comment but it produces a big change to this PR. Hence, it would be better to have more reviews/suggestions before kicking off."
430748553,8657,junrao,2020-05-26T22:47:10Z,"I was trying to check if it's safe to do this. The intention for this is probably to avoid the deadlock between the group lock and the lock in DelayedOperation. None of the caller of joinPurgatory.checkAndComplete holds a group lock now. The only other caller that can first hold a group lock and then the lock in DelayedOperation is joinPurgatory.tryCompleteElseWatch(). However, that's not an issue since that's when the DelayedJoin operation is first added. So, this changes seems ok."
430750020,8657,junrao,2020-05-26T22:51:20Z,"""as the lock is not free"" : Do you mean ""when the lock is free""?"
430750723,8657,junrao,2020-05-26T22:53:22Z,a flag => a flag in ReplicaManager.appendRecords().
430755656,8657,junrao,2020-05-26T23:08:08Z,Could we add a comment to explain the return value?
430756183,8657,junrao,2020-05-26T23:09:43Z,"A map containing the topic partitions having new records and a flag indicating whether the HWM has been incremented.
"
430757287,8657,junrao,2020-05-26T23:13:06Z,"I agree that it's simpler to let the caller in ReplicaManager to complete the delayed requests. This way, we don't need to pass completeDelayedRequests in here."
430761183,8657,junrao,2020-05-26T23:26:08Z,Could we add a comment to explain the return value?
431001869,8657,chia7712,2020-05-27T10:01:27Z,"> So, we probably want to keep doing that.

It may produce deadlock if we hold the group lock for ```GroupCoordinator.onCompleteJoin```.  

```GroupCoordinator.onCompleteJoin``` is possible to append record to ```__consumer_offsets``` (see https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala#L1168), and hence it will try to complete other delayed joins which have groups belonging to same partition of ```__consumer_offsets```. 

That is why I make ```DelayedJoin``` control the group lock manually.  For another,  ```GroupCoordinator.onCompleteJoin``` is used by ```DelayedJoin``` only so it should be fine to change behavior of group lock in this case."
431002494,8657,chia7712,2020-05-27T10:02:37Z,"> I am thinking that we could do the following.

Copy that."
431003374,8657,chia7712,2020-05-27T10:04:13Z,"> So, we probably want to keep doing that.

hmmm, I misunderstand your point. Please ignore my first comment :)"
431206147,8657,chia7712,2020-05-27T14:58:55Z,make sure ```DelayedJoinTest``` does not cause deadlock 
431208549,8657,chia7712,2020-05-27T15:00:48Z,@junrao both methods are executed with lock
431209368,8657,chia7712,2020-05-27T15:01:31Z,this is a workaround to deal with deadlock caused by taking multiples group locks
431210497,8657,chia7712,2020-05-27T15:02:28Z,this enum type is more readable than ```boolean```
439543737,8657,junrao,2020-06-12T17:12:08Z,It's cleaner to not pass in completeDelayedRequests here and let the caller (`ReplicaManager.appendRecords()`) check and complete purgatory instead.
439549136,8657,junrao,2020-06-12T17:23:34Z,"Could we use `map {case (tp, appendResult) => ...}` here to avoid using unamed references?"
439872747,8657,junrao,2020-06-14T22:15:38Z,"Another way that doesn't require checking lock.isHeldByCurrentThread is the following. But your approach seems simpler.

Override forceComplete() to
```
override def forceComplete() {
    if (completed.compareAndSet(false, true)) {
      // cancel the timeout timer
      cancel()
      partitionsToComplete  = coordinator.onCompleteJoin(group)
      onComplete()
      true
    } else {
      false
    }
}
```
In onComplete(), do nothing.

In tryComplete(), do
```
override def tryComplete() {
  group.inLock {
    if (group.hasAllMembersJoined) 
      isForceComplete = forceComplete()
  }
  completeDelayedRequests(partitionsToComplete)
  isForceComplete
}
```

In onExpiration(),
```
override def onExpiration() {
  completeDelayedRequests(partitionsToComplete)
}
```
"
439872846,8657,junrao,2020-06-14T22:16:58Z,expire ->  onComplete -> completeDelayedRequests
439873175,8657,junrao,2020-06-14T22:21:43Z,"DelyaedOperation.lockOpt defaults to None. So, we don't have to specify it explicitly."
439873441,8657,junrao,2020-06-14T22:25:23Z,"""GroupCoordinator#onCompleteJoin() tries to complete delayed requests"" => since the completion of the delayed request for partitions returned from GroupCoordinator#onCompleteJoin() need to be done outside of the group lock."
439875801,8657,junrao,2020-06-14T22:54:00Z,typo whihc
439875892,8657,junrao,2020-06-14T22:55:07Z,The caller no longer passed in completeDelayedRequests.
439876215,8657,junrao,2020-06-14T22:59:03Z,All callers pass in completeDelayedRequests as false. Could we remove this param?
439876392,8657,junrao,2020-06-14T23:01:18Z,There was => There were
439876493,8657,junrao,2020-06-14T23:02:54Z," ReplicaManager.appendRecords()., => ReplicaManager.appendRecords(),"
439876557,8657,junrao,2020-06-14T23:04:08Z,may requires => may require
439878341,8657,junrao,2020-06-14T23:27:03Z,"Hmm, why do we need this logic now?"
439878736,8657,junrao,2020-06-14T23:31:46Z,"Hmm, why do we need to mock this since replicaManager.getMagic() is only called through replicaManager.handleWriteTxnMarkersRequest()?"
439878969,8657,junrao,2020-06-14T23:34:51Z,"Hmm, this should only be called with LeaderHWChange.LeaderHWIncremented, but the mock later returns LeaderHWChange.None? Ditto below."
439914680,8657,chia7712,2020-06-15T03:35:28Z,"TestReplicaManager#appendRecords (https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala#L207) always complete the delayedProduce immediately so the txn offset is append also. This PR tries to complete the delayedProduce after releasing the group lock so it is possible to cause following execution order.
1. txn prepare
1. txn completion (fail)
1. txn append (this is executed by delayedProduce)"
439915438,8657,chia7712,2020-06-15T03:39:51Z,the caller of ```ReplicaManager#appendRecords``` may hold the group lock so it could produce deadlock if ```ReplicaManager#appendRecords``` tries to complete  purgatory.
439920019,8657,chia7712,2020-06-15T04:03:59Z,"```GroupMetadataManager#storeGroup``` (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L245) also call ```ReplicaManager.getMagic```.

There are delayed ops are completed by ```timer.advanceClock``` so we have to mock the ```replicaManager.getMagic```. the mock is same to https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/coordinator/group/GroupCoordinatorTest.scala#L3823.  "
439923720,8657,chia7712,2020-06-15T04:23:45Z,"read @hachikuji https://github.com/apache/kafka/pull/8657#discussion_r427432140 again. It is a nice idea to refactor ```ReplicaManager``` and ```Partition``` to simplify the behavior of checking delayed operations.

Could I address the refactor in another PR to avoid bigger patch?"
440514740,8657,junrao,2020-06-16T00:14:56Z,"Thanks. I am still not sure that I fully understand this. It seems that by not completing the delayedProduce within the group lock, we are hitting IllegalStateException. That seems a bug. Do you know which code depends on that? It seems that we do hold a group lock when updating the txnOffset.

https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L462"
440515043,8657,junrao,2020-06-16T00:15:55Z,"Yes, we can refactor that in a separate PR. Could you file a followup jira for that?"
440576071,8657,chia7712,2020-06-16T04:15:13Z,"> That seems a bug.

The root cause (changed by this PR) is that the ""txn initialization"" and ""txn append"" are not executed within same lock.

**The test story is shown below.**

```CommitTxnOffsetsOperation``` calls ```GroupMetadata.prepareTxnOffsetCommit``` to add ```CommitRecordMetadataAndOffset(None, offsetAndMetadata)``` to ```pendingTransactionalOffsetCommits``` (this is the link you attached).

```GroupMetadata.completePendingTxnOffsetCommit``` called by ```CompleteTxnOperation``` throws ```IllegalStateException``` if ```CommitRecordMetadataAndOffset.appendedBatchOffset``` is ```None``` (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadata.scala#L664). 

**Why it does not cause error before?**

```CommitRecordMetadataAndOffset.appendedBatchOffset``` is updated by the callback ```putCacheCallback``` (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L407). ```TestReplicManager``` always create ```delayedProduce``` do handle the ```putCacheCallback``` (https://github.com/apache/kafka/blob/trunk/core/src/test/scala/unit/kafka/coordinator/AbstractCoordinatorConcurrencyTest.scala#L188). The condition to complete the ```delayedProduce``` is ```completeAttempts.incrementAndGet() >= 3```. And the condition gets true when call both ```producePurgatory.tryCompleteElseWatch(delayedProduce, producerRequestKeys)``` and ```tryCompleteDelayedRequests()``` since the former calls ```tryComplete``` two times and another calls ```tryComplete``` once. It means ```putCacheCallback``` is always executed by ```TestReplicManager.appendRecords``` and noted that ```TestReplicManager.appendRecords``` is executed within a group lock (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala#L738) . In short, txn initialization (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L464) and txn append (https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L407) are executed with same group lock. Hence, the following execution order is impossible.

1. txn initialization
1. txn completion
1. txn append

However, this PR disable to complete delayed requests within group lock held by caller. The ```putCacheCallback``` which used to append txn needs to require group lock again.
      

"
440578202,8657,chia7712,2020-06-16T04:24:26Z,https://issues.apache.org/jira/browse/KAFKA-10170
440585314,8657,chia7712,2020-06-16T04:53:54Z,nice caching. Most methods don't need this flag. Let me revert them :)
441175806,8657,junrao,2020-06-16T22:23:40Z,typo rebalacne
441177055,8657,junrao,2020-06-16T22:27:14Z, the delayed requests may be completed as much as possible =>  the delayed requests may be completed inside the call with the expectation that no conflicting locks are held by the caller
441177351,8657,junrao,2020-06-16T22:28:07Z,Callers can complete the delayed requests manually => Callers can complete the delayed requests after releasing any conflicting lock.
441178418,8657,junrao,2020-06-16T22:31:00Z,"""If the caller no longer passed in completeDelayedRequests"" The caller still passes this in, just as false."
441180667,8657,junrao,2020-06-16T22:37:19Z,""" If the caller no longer passed in completeDelayedRequests"" => There is no completeDelayedRequests passed in. "
441182450,8657,junrao,2020-06-16T22:42:33Z,to to  => to 
441186463,8657,junrao,2020-06-16T22:54:36Z,"Thanks for the great explanation. I understand the issue now. Essentially, this exposed a limitation of the existing test. The existing test happens to work because the producer callbacks are always completed in the same ReplicaManager.appendRecords() call under the group lock. However, this is not necessarily the general case.

Your fix works, but may hide other real problems. I was thinking that another way to fix this is to change the test a bit. For example, we expect CompleteTxnOperation to happen after CommitTxnOffsetsOperation. So, instead of letting them run in parallel, we can change the test to make sure that CompleteTxnOperation only runs after CommitTxnOffsetsOperation completes successfully. JoinGroupOperation and SyncGroupOperation might need a similar consideration.
"
441289689,8657,chia7712,2020-06-17T05:32:25Z,"> we expect CompleteTxnOperation to happen after CommitTxnOffsetsOperation. So, instead of letting them run in parallel, we can change the test to make sure that CompleteTxnOperation only runs after CommitTxnOffsetsOperation completes successfully. 

will roger that !

> JoinGroupOperation and SyncGroupOperation might need a similar consideration.

I didn't notice something interesting. Could you share it with me?"
441846005,8657,junrao,2020-06-17T21:31:23Z,if there is no completeDelayedRequests passed in => if completeDelayedRequests is false
441846819,8657,junrao,2020-06-17T21:33:16Z,if there is no completeDelayedRequests passed in => if completeDelayedRequests is false
441859110,8657,junrao,2020-06-17T22:02:38Z,I think the intention for the test is probably to use the same producerId since it tests more on transactional conflicts.
441859824,8657,junrao,2020-06-17T22:04:30Z,"Hmm, why don't we need the lock here since CommitTxnOffsetsOperation and CompleteTxnOperation could still run in parallel?"
441862013,8657,junrao,2020-06-17T22:10:14Z,"Perhaps change the comment to sth like the following?

""Setting to true to make CompleteTxnOperation and CommitTxnOffsetsOperation complete atomically since they don't typically overlap. Otherwise CompleteTxnOperation may see a pending offsetAndMetadata without an appendedBatchOffset."""
441938174,8657,chia7712,2020-06-18T02:48:30Z,"Got it. However, the same producerId means the group completed by CompleteTxnOperation is possible to be impacted by any CommitTxnOffsetsOperation (since the partitions are same also). Hence, the side-effect is that we need a single lock to control the happen-before of txn completion and commit so the test will get slower.
"
441938393,8657,chia7712,2020-06-18T02:49:28Z,you are right
442330606,8657,junrao,2020-06-18T15:53:30Z,"Since createGroupMembers() is called in multiple tests, it seems we will be accumulating allGroupMembers across tests. That seems unexpected?"
442337184,8657,chia7712,2020-06-18T16:03:15Z,"Junit, by default, creates a new instance for each test case so ```allGroupMembers``` is always new one for each test case. "
452634516,8657,ijuma,2020-07-10T05:48:24Z,It's weird to have a method that invokes a callback and returns a result. Do we need both? We have a number of other methods that do something similar. It would be good to reconsider that as it's difficult to reason about usage in such cases.
452635111,8657,ijuma,2020-07-10T05:50:38Z,"The usual naming convention is to only capitalize the first letter, eg LeaderHwChange."
452754165,8657,chia7712,2020-07-10T10:12:49Z,"@ijuma thanks for your reviews.

>  It would be good to reconsider that as it's difficult to reason about usage in such cases.

You are totally right and @hachikuji had given a great refactor idea (https://github.com/apache/kafka/pull/8657#discussion_r426943627). Given that refactor will bring a lot of changes to this PR, I had filed a ticket to refactor related code (see https://issues.apache.org/jira/browse/KAFKA-10170) in order to make this PR focus on bug fix."
452755236,8657,chia7712,2020-07-10T10:15:19Z,will roger that!
463043040,8657,ijuma,2020-07-30T14:35:33Z,"I notice that we are including the `$` here and in a few other places, we should not do that."
463044371,8657,ijuma,2020-07-30T14:37:19Z,I raised the point before that it's a bit unusual and unintuitive to have both a callback and a return value. Any thoughts on this?
463066191,8657,chia7712,2020-07-30T15:07:03Z,"The response was https://github.com/apache/kafka/pull/8657#discussion_r452754165

In short, we should have a way of fetching delayed request from partition instead of using return value to carry them."
463066323,8657,chia7712,2020-07-30T15:07:14Z,will copy that!
463318346,8657,ijuma,2020-07-30T23:03:17Z,"Thanks, I had missed that. Will respond in that thread."
464562972,8657,junrao,2020-08-03T17:39:30Z,"Could we change the explanation to sth like the following?

This method may trigger the completeness check for delayed requests in a few purgatories. Occasionally, for serialization in the log, a caller may need to hold a lock while calling this method. To avoid deadlock, if the caller holds a conflicting lock while calling this method, the caller is expected to set completeDelayedRequests to false to avoid checking the delayed operations during this call. The caller will then explicitly complete those delayed operations based on the return value, without holding the conflicting lock. "
464567219,8657,junrao,2020-08-03T17:45:18Z,group lock => conflicting lock
464567267,8657,junrao,2020-08-03T17:45:24Z,group lock => conflicting lock
464574837,8657,junrao,2020-08-03T18:00:13Z,"Perhaps add ""but completes the delayed requests without holding the group lock""."
464580388,8657,junrao,2020-08-03T18:11:18Z,a lot of group lock => multiple group locks
464580891,8657,junrao,2020-08-03T18:12:15Z,"""this method may hold a lot of group lock"" : This is actually not true. Unlike producer/fetch purgatory, which is keyed on partition, joinPurgatory is keyed on the group. So, when we complete a key, only a single group's lock will be held.

The reason that we don't want the caller to hold a group lock is that DelayedJoin itself uses a lock other than the group lock for DelayedOperation.maybeTryComplete() and we want to avoid the deadlock between that lock and the group lock."
464697840,8657,junrao,2020-08-03T22:36:59Z,This could be `completeDelayedJoinRequests(groupsToComplete)` ?
464715376,8657,junrao,2020-08-03T23:33:17Z,Perhaps we could add a comment on what this method is intended to test?
464791414,8657,chia7712,2020-08-04T04:25:52Z,thanks for explanation. I will revise the comment according to your comment. 
465179335,8657,junrao,2020-08-04T16:31:50Z,"Hmm, it seems that we are now introducing a new potential deadlock. The conflicting paths are the following.

path 1
hold group lock -> joinPurgatory.tryCompleteElseWatch(delayedJoin) -> watchForOperation (now delayedJoin visible through other threads) -> operation.maybeTryComplete() -> hold delayedJoin.lock

path 2
delayedJoin.maybeTryComplete -> hold hold delayedJoin.lock -> tryComplete() -> hold group lock


"
465200508,8657,chia7712,2020-08-04T17:07:08Z,How about removing inner lock (```ReentrantLock```) from ```DelayedJoin``` ? It seems to me ```DelayedJoin``` does not need the inner lock.
465206257,8657,chia7712,2020-08-04T17:17:09Z,"another approach is that - we introduce an new method ""afterTryComplete"" to ```DelayedOperation```. the new method is invoked by ```maybeTryComplete``` after lock is released. ```DelayedJoin``` still pass group lock to ```DelayedOperation``` and use ""afterTryComplete"" to complete delayed requests"
465867406,8657,junrao,2020-08-05T16:53:02Z,"@chia7712 : Yes, that's a possibility. It adds some complexity to DelayedOperation. Another possibility is to have a special case to complete the delayed requests from groupManager.storeGroup() in GroupCoordinator.onCompleteJoin() in a separate thread."
478599979,8657,junrao,2020-08-27T18:01:35Z,Unneeded new line.
478600507,8657,junrao,2020-08-27T18:02:36Z,We probably want to add a comment why this is needed.
478603243,8657,junrao,2020-08-27T18:07:40Z,"Hmm, why do we need to override this instead of using the one defined in DelayedJoin?"
478603776,8657,junrao,2020-08-27T18:08:37Z,This probably should be included in the local time as before.
478604292,8657,junrao,2020-08-27T18:09:33Z,Could we add the new param to the javadoc?
479379705,8657,junrao,2020-08-28T15:31:08Z,This can just be private.
479381963,8657,junrao,2020-08-28T15:34:58Z,KafkaApis.handle() => KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)
479382167,8657,junrao,2020-08-28T15:35:19Z,The above comment is outdated now.
479382962,8657,junrao,2020-08-28T15:36:41Z,We probably should rename this to sth like safeTryComplete().
479383807,8657,junrao,2020-08-28T15:38:11Z,The default value is None.
479389617,8657,junrao,2020-08-28T15:49:04Z,at the end of KafkaApis.handle() => at the end of KafkaApis.handle() and the expiration thread for certain delayed operations (e.g. DelayedJoin)
479390056,8657,junrao,2020-08-28T15:49:47Z,in a queue => are stored in a queue
479467548,8657,junrao,2020-08-28T18:24:30Z,The last sentence doesn't complete.
479474904,8657,ijuma,2020-08-28T18:41:05Z,Can we not use a `boolean` here? `false` until it's been incremented and then `true`. Is there value in having the third state?
479475681,8657,ijuma,2020-08-28T18:42:47Z,Should we be guarding against exceptions here?
479661386,8657,junrao,2020-08-29T15:40:22Z,a action => an action
479669354,8657,junrao,2020-08-29T17:05:19Z,"Even if we hit an exception in handleXXX(), it would still be useful to complete the actionQueue."
479670056,8657,junrao,2020-08-29T17:13:30Z,"It seems that we have to distinguish 3 states here: (1) records not appended due to an error; (2) records appended successfully and HWM advanced; (3) records appended successfully and HWM not advanced. In case (1), no purgatory needs to be checked."
479670351,8657,junrao,2020-08-29T17:16:24Z,Is this used?
479670727,8657,junrao,2020-08-29T17:20:46Z,Perhaps we could add a note at the top of DelayedOperation so that people are aware of the need to complete actions for new DelayedOperations in the future.
479677889,8657,chia7712,2020-08-29T18:40:53Z,Completing delayed actions may cause exception. Should exception be swallowed and log if we move the completion to the final block?
479678653,8657,chia7712,2020-08-29T18:50:10Z,you are right. I miss the exception in ```appendToLocalLog```. Let me revert this change
479678761,8657,ijuma,2020-08-29T18:51:18Z,What's the reasoning for taking just 1 item @junrao? Could this cause the queue to grow over time?
479678812,8657,ijuma,2020-08-29T18:52:04Z,Another approach would be for this queue to be per request thread instead of per server. That would simplify concurrency handling.
479679930,8657,chia7712,2020-08-29T19:05:13Z,"It is dangerous to complete delayed actions by ```DelayedOperation``` as most methods of ```DelayedOperation``` are executed with locking. We, now, depend on ```KafkaApi. handler``` to complete the delayed action produced by someone who can't complete delayed action due to locking. For example, ```DelayedJoin.onComplete``` can produce an new action and the new action can't be completed by ```DelayedJoin``` itself due to group locking."
479680637,8657,chia7712,2020-08-29T19:13:19Z,The thread has to pass queue to method of ReplicManager/GroupCoordinator if queue is kept by thread. A lot of methods are included so that is a big patch. I prefer to keep small patch though it gets bigger now :(
479682849,8657,junrao,2020-08-29T19:37:54Z,Good question. It's based on the assumption that each KafkaApis.handle() call only calls ReplicaManager. appendRecords() once. Not sure if this is always true in the future. Perhaps a safer approach is to have Action.tryCompleteAction() get the current size of the queue and complete all those actions.
479683036,8657,chia7712,2020-08-29T19:40:00Z,It should be fine to let handler complete actions as much as possible since the response is created before handling delayed actions.
479683212,8657,junrao,2020-08-29T19:42:24Z,"Yes, if actionQueue.tryCompleteAction() throws an exception, we can just catch it and log a warning in finally since the response has been sent by then."
479683544,8657,junrao,2020-08-29T19:45:56Z,"I was thinking to add a comment so that if someone adds a future delayed operation that calls ReplicaManager.appendRecords() in onComplete() like DelayedJoin, he/she is aware that this operation's onExpiration() needs to call actionQueue.tryCompleteAction()."
479684163,8657,junrao,2020-08-29T19:53:51Z,"Perhaps we can make this a bit clearer. Sth like the following.

leaderHWIncremented has 3 possible values: (1) If records are not appended due to an error, the value will be None; (2) if records are appended successfully and HWM is advanced, the value is Some(true); (3) if records are appended successfully and HWM is not advanced, the value is Some(false)."
479684353,8657,junrao,2020-08-29T19:55:43Z,"Note that the action queue is not only called by requests threads, but also by the expiration thread for certain delayed operations."
479686292,8657,ijuma,2020-08-29T20:17:33Z,Good point @junrao.
479686398,8657,ijuma,2020-08-29T20:19:12Z,@chia7712 I suggest using a sealed trait with 3 case objects to make this clearer. Using `Option[Boolean]` as a tristate value is generally best avoided.
479686517,8657,chia7712,2020-08-29T20:20:30Z,Will copy that
479689381,8657,chia7712,2020-08-29T20:55:45Z,@junrao @ijuma please take a look at this method
479689457,8657,ijuma,2020-08-29T20:56:53Z,"Main thing to decide is what to do in case of exception, do we stop processing or do we continue?"
479689653,8657,ijuma,2020-08-29T20:59:38Z,Why are we using a blocking queue? It doesn't seem like we ever need the blocking functionality. Am I missing something?
479689694,8657,chia7712,2020-08-29T20:59:55Z,I prefer to just catch it and log a warning as the response has been processed. WDYT?
479689838,8657,chia7712,2020-08-29T21:01:42Z,you are right. How about using ```ConcurrentLinkedQueue``` instead?
479691180,8657,ijuma,2020-08-29T21:20:30Z,"Yes, I think that's better. One thing I was wondering about is whether contention is going to be an issue for this `ActionQueue`. Multiple threads are adding items to it and then trying to consume from it. I haven't thought about all the details, but would a thread local work better? In that case, each thread would add and then drain. This works fine for the request threads, but I wasn't sure about the other case that @junrao pointed out."
479692892,8657,chia7712,2020-08-29T21:44:06Z,"Handler (thread) can have local ActionQueue and it is passed to each method to collect delayed actions. ```DelayedJoin``` is specific case since the delayed actions are possible to be access by two thread (timeout thread and handler thread). A simple way to address thread local is that ```DelayedJoin``` owns an ActionQueue and the queue is passed to ```coordinator.onCompleteJoin``` and then the queue is consumed by ```DelayedJoin.onExpiration```. Both ```onComplete``` and ```onExpiration``` are thread-safe so use a thread local queue is safe.

"
479706182,8657,junrao,2020-08-30T00:59:37Z,"If we are unlucky, a single thread could be held up in this loop for a long time. Perhaps we could let each thread only complete the number of actions that it sees when entering tryCompleteActions()."
479706303,8657,junrao,2020-08-30T01:01:16Z,"Perhaps we could do the try/catch of each action here instead of KafkaApis. This way, we are guaranteed that all pending actions are processed in time."
479706407,8657,junrao,2020-08-30T01:02:56Z,"Since we are draining more than 1 item now, this comment is no longer accurate."
479716882,8657,ijuma,2020-08-30T03:41:47Z,Maybe we can go with a single `ActionQueue` in this PR. We can submit a separate PR for reducing the contention by having one per thread.
479737358,8657,chia7712,2020-08-30T08:05:15Z,"Sure. I will file a PR to follow the pattern in #9229

In order to simplify code base, I will revert the action queue in ""per server"" :)"
479802860,8657,junrao,2020-08-30T18:47:31Z,need => needs
479802912,8657,junrao,2020-08-30T18:48:05Z,is failed => failed
479803168,8657,junrao,2020-08-30T18:50:50Z,Probably Increased is clearer than Incremental.
479829275,8657,ijuma,2020-08-30T23:26:35Z,Another nit: `LeaderHwChange` adheres to the coding convention better.
482743072,8657,chia7712,2020-09-03T06:50:50Z,"@junrao This change avoids deadlock in ```TransactionCoordinatorConcurrencyTest```.

If we update ```watchKeys``` before ```tryCompleteElseWatch```, the other threads can take the same key to complete delayed request.  Hence the deadlock happens due to following conditions.

**thread_1**  holds ```stateLock``` of TransactionStateManager to call ```appendRecords``` and it requires lock of delayed request to call ```tryCompleteElseWatch```.

**thread_2** holds lock of delayed request to call ```onComplete``` (updateCacheCallback) and ```updateCacheCallback``` requires ```stateLock``` of TransactionStateManager."
482753529,8657,chia7712,2020-09-03T07:06:44Z,"According to above case, there is a potential deadlock.

```
    var watchCreated = false
    for(key <- watchKeys) {
      // If the operation is already completed, stop adding it to the rest of the watcher list.
      if (operation.isCompleted)
        return false
      watchForOperation(key, operation)

      if (!watchCreated) {
        watchCreated = true
        estimatedTotalOperations.incrementAndGet()
      }
    }

    isCompletedByMe = operation.safeTryComplete()
    if (isCompletedByMe)
      return true

```

```safeTryComplete()``` is executed after updating ```watchKey```. Hence, it is possible that the lock of this request is held by **another thread**. The deadlock happens if this ```tryCompleteElseWatch``` is holding the **lock** required by **another thread**.

It seems to me the simple approach is to remove ```operation.safeTryComplete```. That should be fine since we have called ```tryComplete``` before."
483276075,8657,junrao,2020-09-03T21:58:37Z,"@chia7712 : 

1. I think we still need `operation.safeTryComplete` in `DelayedOperation.tryCompleteElseWatch()`. The reason is that after the `operation.tryComplete()` call, but before we add the key to watch, the operation could have been completed by another thread. Since that thread doesn't see the registered key, it won't complete the request. If we don't call `operation.safeTryComplete` after adding the key for watch, we could have missed the only chance for completing this operation.

2. I am not sure if there is a deadlock caused by TransactionStateManager. I don't see updateCacheCallback hold any lock on stateLock. The following locking sequence is possible through TransactionStateManager.

  thread 1 : hold readLock of stateLock, call ReplicaManager.appendRecords, call tryCompleteElseWatch, hold lock on delayedOperation

  thread 2: hold lock on delayedOperation, call delayedOperation.onComplete, call removeFromCacheCallback(), hold readLock of stateLock.

However, since both threads hold readLock of stateLock, there shouldn't be a conflict.

Do you see the test fail due to a deadlock?
"
483552275,8657,chia7712,2020-09-04T11:15:46Z,"> Do you see the test fail due to a deadlock?

the following read/write lock is from```stateLock``` of ```TransactionStateManager```

1. Thread_1: holding readlock and waiting for lock of delayed op (TransactionStateManager#appendTransactionToLog)
2. Thread_2: waiting for writelock (```TransactionCoordinatorConcurrencyTest#testConcurrentGoodPathWithConcurrentPartitionLoading```)
    ```
    val t = new Thread() {
      override def run(): Unit = {
        while (keepRunning.get()) {
          txnStateManager.addLoadingPartition(numPartitions + 1, coordinatorEpoch)
        }
      }
    }
    private[transaction] def addLoadingPartition(partitionId: Int, coordinatorEpoch: Int): Unit = {
      val partitionAndLeaderEpoch = TransactionPartitionAndLeaderEpoch(partitionId, coordinatorEpoch)
      inWriteLock(stateLock) {
        loadingPartitions.add(partitionAndLeaderEpoch)
      }
    }
    ```
3. Thread_3: holding lock of delayed op and waiting for readlock (another thread is trying to complete delayed op)

**deadlock**
1. Thread_1 is waiting for thread_3
1. Thread_3 is waiting for Thread_2
1. Thread_2 is waiting for thread_1"
483832980,8657,junrao,2020-09-04T20:33:01Z,"@chia7712 : Thanks for the explanation. `stateLock` is created as an unfair ReentrantReadWriteLock. So, in that case, will thread_3's attempt for getting the readLock blocked after thread_2? Did the test actually failed because of this?"
483897245,8657,chia7712,2020-09-05T01:53:31Z,"> will thread_3's attempt for getting the readLock blocked after thread_2?

To the best of my knowledge, writers have preference over readers in order to avoid starvation. That behavior is not public and we can get some evidence from source code. for example:

```java
        final boolean readerShouldBlock() {
            /* As a heuristic to avoid indefinite writer starvation,
             * block if the thread that momentarily appears to be head
             * of queue, if one exists, is a waiting writer.  This is
             * only a probabilistic effect since a new reader will not
             * block if there is a waiting writer behind other enabled
             * readers that have not yet drained from the queue.
             */
            return apparentlyFirstQueuedIsExclusive();
        }
```

At any rate, the non-fair mode does not guarantee above situation does not happen. Hence, it would be better to avoid potential deadlock caused by ```tryCompleteElseWatch```.

> I think we still need operation.safeTryComplete in DelayedOperation.tryCompleteElseWatch()

How about using ```tryLock``` in tryCompleteElseWatch? It avoids conflicting locking and still check completion of delayed operations after adding watches?
"
483966128,8657,junrao,2020-09-05T16:31:58Z,"@chia7712 : Thanks for the explanation. I agree that it's a potential problem.

Does using `tryLock` in `tryCompleteElseWatch()` lead us back to the previous issue that we could miss the opportunity to to complete an operation (fixed with KAFKA-6653)?

Another possibly is that we hold the lock in delayed operation while adding the operation to watch list and do the final `safeTryComplete()` check. This way, when the delayed operation is exposed to another thread, every thread, including the caller, always first acquires the lock in delayed operation. This should avoid all potential deadlocks between `tryCompleteElseWatch()` and `checkAndComplete()`. What do you think?"
484039528,8657,chia7712,2020-09-06T08:22:57Z,"> Another possibly is that we hold the lock in delayed operation while adding the operation to watch list and do the final safeTryComplete() check. This way, when the delayed operation is exposed to another thread, every thread, including the caller, always first acquires the lock in delayed operation. This should avoid all potential deadlocks between tryCompleteElseWatch() and checkAndComplete()

nice idea. I have addressed this approach."
484090245,8657,junrao,2020-09-06T16:38:01Z,requiring => requires
484090563,8657,junrao,2020-09-06T16:41:37Z,It seems that we don't need the `if` here?
484090694,8657,junrao,2020-09-06T16:43:11Z,We should return if `tryComplete()` returns true.
484091466,8657,junrao,2020-09-06T16:51:33Z,"This is an existing issue. I am not sure if calling `tryComplete()` without holding the operation's lock guarantees visibility to another thread. For example, thread 1 changes the state in the operation in `tryComplete()`. It then calls `tryComplete()` holding the operations's lock but doesn't change the state in the operation. thread 2 calls `tryComplete()` holding the operations's lock. Is thread 2 guaranteed to see the changes made by thread 1 since the update was made without crossing the memory boundary by subsequent readers?

If this is an issue, we could extend to lock to do the first `tryComplete()` check."
484092252,8657,junrao,2020-09-06T16:59:20Z,"This approach is fine but leaks operation.lock beyond tests. Another way to package this is to add a new method in DelayedOperation like tryCompleteAndMaybeWatch(). If that's not very clean, we can keep the current approach. "
484092496,8657,junrao,2020-09-06T17:02:00Z,Do we still need this change to avoid deadlocks?
484092945,8657,junrao,2020-09-06T17:06:29Z,"With this change, `DelayedOperations.checkAndCompleteFetch()` is only used in tests. I am wondering if it can be removed. It's fine if we want to do this in a followup PR.

Unrelated to this PR, `DelayedOperations.checkAndCompleteProduce` and `DelayedOperations.checkAndCompleteDeleteRecords` seem unused. We can probably remove them in a separate PR."
484097161,8657,chia7712,2020-09-06T17:52:54Z,"Pardon me, the latest commit does it. If the tryComplete return true, this method return true also.

Or you mean we can do return in the lambda function directly? If so, the reason I dont address it is the impl of return in lambda is to throws and then catch exception.That is anti-pattern in scala and we should avoid it from our hot methods."
484097601,8657,chia7712,2020-09-06T17:57:55Z,There is an new test for this new behavior. In the DelayedOperationTest.
484097948,8657,chia7712,2020-09-06T18:01:29Z,"Yep. if we add it to watch list too early, the concurrent issue happens as tryCompleteElseWatch calls tryComplete without locking. "
484099709,8657,chia7712,2020-09-06T18:21:23Z,"It depends :)

However, it should be fine to extend the lock to do it in this PR. Will address it in next commit"
484101535,8657,junrao,2020-09-06T18:40:54Z,"Yes, you are right. I missed the bracket alignment."
484101769,8657,junrao,2020-09-06T18:43:34Z,"Yes, it's just that if all non-testing usage of DelayedOperation.lock is within DelayedOperation itself, it makes it a bit easier to trace down all usage of lock.
"
484138649,8657,chia7712,2020-09-07T00:59:57Z,"@junrao This approach can not resolve all potential deadlock. For example:

1. thread_a gets lock of op
1. thread_a adds op to watch list
1. thread_a calls op#tryComplete (and it requires lock_b)
1. thread_b holds lock_b
1. thread_b sees op from watch list
1. thread_b needs lock of op

Hence, we are facing the following issues.

1. the last completion check can cause deadlock after the op is exposed to other threads (by watch list).
1. the last completion check can not be removed due to KAFKA-6653.

How about using ActionQueue to resolve it? We add completion check to ActionQueue after adding op to watch list. All handlers are able to complete it even if they don't have same key.
"
484163863,8657,chia7712,2020-09-07T03:20:48Z,"As the potential deadlock caused by holding lock in ```tryCompleteElseWatch```, I DON""T change the ```tryComplete``` to ```safeTryComplete```. Maybe we can deal with this issue when we meet such issue."
484164627,8657,chia7712,2020-09-07T03:24:50Z,"> With this change, DelayedOperations.checkAndCompleteFetch() is only used in tests. I am wondering if it can be removed. It's fine if we want to do this in a followup PR.

I will file a ticket after this PR is merged.

> Unrelated to this PR, DelayedOperations.checkAndCompleteProduce and DelayedOperations.checkAndCompleteDeleteRecords seem unused. We can probably remove them in a separate PR.

this is small change. I will remove them in this PR"
484181930,8657,junrao,2020-09-07T04:52:58Z,"@chia7712 : What you described is possible, but probably not an issue in practice. Since tryComplete() always completes a delayed operation asynchronously, there is no reason for the caller of a delayed operation to hold any lock while calling tryComplete. Therefore, in step 4 above, the first lock that thread_b (assuming it's well designed) can acquire should be the lock in delayed operation. "
484209068,8657,chia7712,2020-09-07T06:32:29Z,"> there is no reason for the caller of a delayed operation to hold any lock while calling tryComplete. 

You are right. However, not sure how to maintain that ""well designed"" code in the future as the deadlock is implicit. It seems to me avoiding multiples exclusive lockings can avoid the deadlock.

I will keep current approach since the story I described may be overkill in practice. "
484519651,8657,junrao,2020-09-07T16:45:35Z,"Since safeTryComplete() is no longer used in tryCompleteElseWatch(), the above comment is not completely relevant. Perhaps we could just explain what this method does ""Thread-safe variant of tryComplete()."""
484521628,8657,junrao,2020-09-07T16:52:44Z,it requires lock_b => it tries to require lock_b
484522816,8657,junrao,2020-09-07T16:57:32Z,"Perhaps we could change this comment to sth like the following.

To avoid the above scenario, we recommend DelayedOperationPurgatory.checkAndComplete() be called without holding any lock. Since DelayedOperationPurgatory.checkAndComplete() completes delayed operations asynchronously, holding a lock to make the call is often unnecessary."
484523257,8657,junrao,2020-09-07T16:59:24Z,There is a potential deadlock => There is a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete() 
484523534,8657,junrao,2020-09-07T17:00:21Z,thread_c holds lock of op => thread_c calls checkAndComplete () and holds lock of op 
484524545,8657,junrao,2020-09-07T17:04:35Z,Is this change still necessary now that we always call tryComplete() with lock in tryCompleteElseWatch?
484543835,8657,chia7712,2020-09-07T18:44:50Z,Will copy that 
484605576,8657,junrao,2020-09-08T01:37:51Z,safeTryCompleteAndElse => safeTryCompleteOrElse ?
484606811,8657,junrao,2020-09-08T01:43:41Z,"The above comment is a bit out of context now. Perhaps we could change ""we do the check in the following way"" to ""we do the check in the following way through safeTryCompleteAndElse()"".

"
484607882,8657,junrao,2020-09-08T01:48:33Z,Do we still need to change the ordering now that we always call tryComplete() with lock in tryCompleteElseWatch?
484609072,8657,junrao,2020-09-08T01:53:44Z,checkAndComplete () => checkAndComplete()
484610866,8657,junrao,2020-09-08T02:01:32Z,"Perhaps change the above to the following?

We hold the operation's lock while adding the operation to watch list and doing the tryComplete() check. This is to avoid a potential deadlock between the callers to tryCompleteElseWatch() and checkAndComplete(). For example, the following deadlock can happen if the lock is only held for the final tryComplete() check."
484622290,8657,chia7712,2020-09-08T02:52:05Z,I will revert this change
484638938,8657,junrao,2020-09-08T04:08:51Z,"I think we still want to keep the rest of the paragraph starting from ""Call tryComplete().""."
484639236,8657,junrao,2020-09-08T04:10:06Z,"We hold => Through safeTryCompleteOrElse(), we hold"
484640168,8657,junrao,2020-09-08T04:14:27Z,thread_b holds lock_b => thread_b holds lock_b and calls checkAndComplete()
484640862,8657,junrao,2020-09-08T04:17:41Z,"Noted that current approach can't prevent all deadlock. => Note that even with the current approach, deadlocks could still be introduced."
484644802,8657,junrao,2020-09-08T04:35:49Z,"Instead of introducing a global var, could we add a new param when constructing CommitTxnOffsetsOperation and CompleteTxnOperation?"
484645849,8657,junrao,2020-09-08T04:40:17Z,thread_a gets lock of op => thread_a calls tryCompleteElseWatch() and gets lock of op
485026583,8657,junrao,2020-09-08T15:52:56Z,to call safeTryComplete => to call the final tryComplete()
485026799,8657,junrao,2020-09-08T15:53:16Z,tryCompleteElseWatch => tryCompleteElseWatch()
485034262,8657,junrao,2020-09-08T16:04:21Z,causes error => causes an error
485034296,8657,junrao,2020-09-08T16:04:23Z,such error => such an error 
94801659,2264,ijuma,2017-01-05T16:36:06Z,It seems like this field is not used anywhere.
95000465,2264,hachikuji,2017-01-06T19:05:19Z,I think we need to enforce a minimum version of 2 since older versions would expect an older version of the message format. This is also an easy way to detect that we're talking with a version of the broker older than 0.10. I'm wondering if it would make sense to raise an exception to the user immediately when we connect to such a broker instead of waiting until we send an incompatible request?
95016586,2264,cmccabe,2017-01-06T20:43:03Z,"Sure, we can enforce a minimum version of 2 here.  Brokers earlier than 0.10 will automatically disconnect our clients, since those brokers don't support ApiVersionsRequest, which is the first thing we send after KAFKA-3600."
95197805,2264,ijuma,2017-01-09T17:01:03Z,"Nit: in Kafka, we don't use `get` prefix for accessors."
95198122,2264,ijuma,2017-01-09T17:02:33Z,"This is not needed, `StringBuilder` appends `null` for you."
95204211,2264,ijuma,2017-01-09T17:35:24Z,Nit: it may make sense to move the requestBuilder to the end as it's likely to be bigger and make it harder to read the other fields.
95204352,2264,ijuma,2017-01-09T17:36:01Z,"We have a method `requestBuilder` that is the same as this, so we can remove this one."
95215232,2264,cmccabe,2017-01-09T18:28:11Z,"ok, I'll remove that here (and elsewhere)"
95215682,2264,cmccabe,2017-01-09T18:30:19Z,k
95239733,2264,cmccabe,2017-01-09T20:32:35Z,"This should be used any time that there is a version mismatch problem between the message we are trying to send and the message versions we can actually send to the desired broker.  I fixed the Fetcher to use this instead of doing its own thing to detect version problems.  I also change the type of this to RuntimeException, so that the exact exception which the builder raised can be preserved here."
95248631,2264,ijuma,2017-01-09T21:20:56Z,Nit: should this be `newRequest` since it's a method in `KafkaClient`? Or is it clearer how it is?
95252239,2264,ijuma,2017-01-09T21:39:37Z,"Nit: do we really need 3 lines for this? Seems like we could do it in two. In Kafka, we generally go for wider lines than the traditional 80 columns (100 to 120 is common)."
95252366,2264,ijuma,2017-01-09T21:40:22Z,Can we expand on when this can happen?
95261313,2264,hachikuji,2017-01-09T22:27:41Z,nit: this alignment is weird. Are you using intellij defaults?
95262756,2264,hachikuji,2017-01-09T22:36:29Z,Does it make sense to special case the ApiVersion request only instead of skipping the check for all internal requests?
95263217,2264,hachikuji,2017-01-09T22:39:17Z,nit: we usually skip braces for one-line branches like this. A few more below.
95264033,2264,hachikuji,2017-01-09T22:44:20Z,"Would it make sense to push this check into `getUsableVersion`? I wouldn't consider a negative version ""usable."""
95264436,2264,hachikuji,2017-01-09T22:46:54Z,"Is this equivalent to this?
```java
responses.addAll(abortedSends);
abortedSends.clear();
```"
95264759,2264,hachikuji,2017-01-09T22:48:56Z,nit: indentation
95265028,2264,hachikuji,2017-01-09T22:50:32Z,nit: use string interpolation
95265544,2264,hachikuji,2017-01-09T22:53:41Z,"There's a bug in `processDisconnection` (not introduced in this patch) that we ought to fix here. The type of `node` here is a String, but `nodeApiVersions` is keyed by Integer. (The conversion between string and int all over this class always pains me.)"
95265740,2264,ijuma,2017-01-09T22:54:41Z,"Unfortunately, 0.9.0.1 has a bug where it won't disconnect when it receives an unknown request. It will keep the connection open until the connection reaper comes along. A subsequent request can cause an immediate disconnection though."
95265836,2264,hachikuji,2017-01-09T22:55:18Z,I think `nodeId` is a more accurate name. 
95265980,2264,ijuma,2017-01-09T22:56:13Z,"Nit: is it right to say it's an `obsolete` broker? Seems a bit too strong, obsolete means `no longer produced or used; out of date` whereas it could be a broker that is just 4 months old given our current time-based release schedule."
95266228,2264,ijuma,2017-01-09T22:57:44Z,Do we really need to do this? I'd prefer if we didn't expose a static mutable array outside this class.
95266232,2264,hachikuji,2017-01-09T22:57:45Z,nit: seems like there's enough room on the previous line for this
95266343,2264,ijuma,2017-01-09T22:58:27Z,Nit: `version`?
95266684,2264,hachikuji,2017-01-09T23:00:39Z,Seems this was our last usage for the `now` parameter.
95266777,2264,hachikuji,2017-01-09T23:01:13Z,nit: can this fit on the line above?
95267609,2264,ijuma,2017-01-09T23:06:14Z,"These 3 lines can simply be `val version = apiVersion.getOrElse(ProtoUtils.latestVersion(apiKey,id))`."
95267715,2264,hachikuji,2017-01-09T23:06:56Z,Seems we can turn this into an assertion when it is not an ApiVersion request.
95267760,2264,ijuma,2017-01-09T23:07:12Z,"Nit: it should be `val version: Short` (space after the colon, not before)."
95268226,2264,hachikuji,2017-01-09T23:10:16Z,`now` is never used.
95268276,2264,ijuma,2017-01-09T23:10:37Z,"Looks like the code for both `Builder(...)` is the same, so maybe we can just do the `liveBrokers` bit in the `if/else`."
95268361,2264,ijuma,2017-01-09T23:11:14Z,"Nit: the map creation is usually done like `Map(SecurityProtocol.PLAINTEXT -> new EndPoint(node.host, node.port))`."
95268422,2264,hachikuji,2017-01-09T23:11:38Z,nit: you can leave out the type on the right-hand side.
95268671,2264,ijuma,2017-01-09T23:13:10Z,Nit: is there a reason why you didn't chain this one too?
95268734,2264,ijuma,2017-01-09T23:13:39Z,Seems like the `()` introduced here are not needed.
95271011,2264,hachikuji,2017-01-09T23:28:58Z,`api` no longer used. Indentation needs fixing also.
95271585,2264,hachikuji,2017-01-09T23:32:24Z,There are a few javadoc references that need fixing as well.
95274471,2264,hachikuji,2017-01-09T23:54:45Z,"I think it might be better to move this check into `ConsumerNetworkClient.RequestFutureCompletionHandler` to ensure that we don't forget any checks. Also `onFailure` seems like a more appropriate callback for that case,"
95275129,2264,hachikuji,2017-01-10T00:00:05Z,"nit: string interpolation is preferred. another one below. Also, not sure about the convention of including the function name. Wouldn't the logger do that for us?"
95276737,2264,hachikuji,2017-01-10T00:13:40Z,"In this case, we're using the presence of the `offsets` field to indirectly determine that the version that was used was 0. I'm not sure we'll always have something so convenient, so I've been wondering if we need a way to determine the version that was used more directly. For example, we could have `ClientResponse` or even `AbstractResponse` include a field for the version."
95276915,2264,cmccabe,2017-01-10T00:15:26Z,"ok, I realigned it with the left paren"
95276973,2264,cmccabe,2017-01-10T00:15:53Z,k
95277223,2264,hachikuji,2017-01-10T00:18:28Z,Why was this changed?
95277345,2264,cmccabe,2017-01-10T00:19:36Z,"Hmm.  I haven't thought about it that hard, but I think special casing will probably get messy here.  It also duplicates work done elsewhere to check if the message is sendable."
95277392,2264,hachikuji,2017-01-10T00:19:57Z,The `Impl` suffix is a little unconventional. Can we just use `getOffsetsByTimes`?
95278127,2264,cmccabe,2017-01-10T00:26:26Z,k
95278471,2264,cmccabe,2017-01-10T00:29:17Z,ok
95278543,2264,cmccabe,2017-01-10T00:29:57Z,removed
95278678,2264,hachikuji,2017-01-10T00:30:54Z,I think we need to pass this exception to the future instead of raising. Would be good to have a test case if we don't already.
95278757,2264,cmccabe,2017-01-10T00:31:41Z,"yeah, let's just do that."
95278815,2264,cmccabe,2017-01-10T00:32:16Z,k
95278891,2264,cmccabe,2017-01-10T00:33:00Z,yeah
95279595,2264,hachikuji,2017-01-10T00:39:47Z,"We didn't have it before, but maybe we should add a null check here for more resilience in the future."
95279607,2264,cmccabe,2017-01-10T00:39:56Z,"yeah, that is pretty nasty.  I'll fix it.  I wish there were some workaround for this, but it looks like Java Map still always supports put(Object) to allow compatibility with the pre-generics days...

We should start running findBugs to catch things like this"
95279753,2264,hachikuji,2017-01-10T00:41:13Z,"I'm a little unclear on the pattern for which fields are included in the builder constructor and which are included through methods. I thought perhaps it would be the required arguments included in the constructor, but we didn't pass the timestamps to query in the `ListOffsetRequest` above, which seems required."
95280248,2264,cmccabe,2017-01-10T00:45:07Z,k
95280508,2264,cmccabe,2017-01-10T00:47:36Z,k
95280577,2264,cmccabe,2017-01-10T00:48:05Z,k
95280644,2264,cmccabe,2017-01-10T00:48:41Z,yeah
95281013,2264,cmccabe,2017-01-10T00:51:28Z,k
95281567,2264,cmccabe,2017-01-10T00:56:07Z,I added a comment
95282327,2264,hachikuji,2017-01-10T01:02:40Z,Maybe `UnsupportedBrokerException`?
95284742,2264,cmccabe,2017-01-10T01:26:27Z,"Hmm, how would the 0.9.0.1 broker even know that it was getting another request?  it doesn't know the size of the first request so it doesn't know where that request ends and a new one begins."
95284792,2264,cmccabe,2017-01-10T01:26:58Z,k
95284826,2264,hachikuji,2017-01-10T01:27:24Z,nit: Would be nice to be consistent on the use of braces or parenthesis. I think we are trying to encourage the latter.
95286297,2264,junrao,2017-01-10T01:43:04Z,"When the transport is SSL, the client can't send requests immediately after the socket is connected. We have to wait for the SSL handshake to complete. So, perhaps the selector needs to further return the transport.ready() state in KafkaChannel to NetworkClient."
95286337,2264,junrao,2017-01-10T01:43:30Z,"Using the current version is ok for now since there is currently only one version of SaslHandshakeRequest. It would be useful to add a comment that when there are new versions of SaslHandshakeRequest, we need to select the version based on the result of ApiRequest accordingly."
95290016,2264,hachikuji,2017-01-10T02:25:10Z,We seem to be missing the version mismatch check in `handleProduceResponse`.
95303775,2264,cmccabe,2017-01-10T05:53:19Z,k
95304065,2264,cmccabe,2017-01-10T05:57:04Z,"setVersion is a method on the base class, AbstractRequest#Builder.  So it returns an instance of AbstractRequest#Builder rather than an instance of the derived class FetchRequest#Builder, which is awkward for Scala's type inference."
95304445,2264,cmccabe,2017-01-10T06:01:54Z,k
95304662,2264,cmccabe,2017-01-10T06:04:57Z,nice
95304719,2264,cmccabe,2017-01-10T06:06:13Z,k
95304766,2264,cmccabe,2017-01-10T06:06:44Z,k
95304877,2264,cmccabe,2017-01-10T06:08:33Z,k
95305418,2264,cmccabe,2017-01-10T06:16:03Z,"ok, we can hide it"
95305559,2264,cmccabe,2017-01-10T06:17:31Z,"obsolete may be a little strong, but it's clear what it means and what the user should do if they want access to the feature.  We could also go with something like ""OutdatedBrokerException""?"
95305659,2264,cmccabe,2017-01-10T06:18:44Z,"It sounds like we would be in the clear then, since we're sending an ApiVersionsRequest followed by a follow-on request?"
95305690,2264,cmccabe,2017-01-10T06:19:14Z,fixed
95305695,2264,cmccabe,2017-01-10T06:19:19Z,k
95305967,2264,cmccabe,2017-01-10T06:22:42Z,k
95305976,2264,cmccabe,2017-01-10T06:22:50Z,sounds good
95305989,2264,cmccabe,2017-01-10T06:22:56Z,removed
95306011,2264,cmccabe,2017-01-10T06:23:05Z,"yes, let's replace it with that"
95306022,2264,cmccabe,2017-01-10T06:23:14Z,fixed
95306030,2264,cmccabe,2017-01-10T06:23:21Z,k
95306063,2264,cmccabe,2017-01-10T06:23:48Z,"ugh.  That's a nasty one.  I fixed it in the patch.

We should run findBugs..."
95306070,2264,cmccabe,2017-01-10T06:23:54Z,k
95306078,2264,cmccabe,2017-01-10T06:23:59Z,k
95306086,2264,cmccabe,2017-01-10T06:24:06Z,yeah
95306094,2264,cmccabe,2017-01-10T06:24:11Z,ok
95306102,2264,cmccabe,2017-01-10T06:24:17Z,yeah
95306242,2264,cmccabe,2017-01-10T06:26:07Z,"Hmm.  I think if it were just newRequest, I would wonder whether it was a new AbstractRequest or a new ClientRequest.  What do you think?"
95306249,2264,cmccabe,2017-01-10T06:26:14Z,ok
95306275,2264,cmccabe,2017-01-10T06:26:38Z,I added a comment
95306664,2264,cmccabe,2017-01-10T06:32:08Z,k
95307070,2264,cmccabe,2017-01-10T06:37:05Z,"Hmm, I didn't see any code path where the return value could be null, or the value of the future itself, or the keys or values of the map.  Maybe an NPE is ok in that case?"
95307372,2264,cmccabe,2017-01-10T06:40:56Z,"I wanted to avoid confusion with the public API of that name.  How about ""retrieveOffsetsByTimes""?"
95307500,2264,cmccabe,2017-01-10T06:42:28Z,"Good catch.  I made some other edits to the for loop, but they're no longer necessary now.  Reverted"
95308523,2264,cmccabe,2017-01-10T06:54:58Z,"I think AbstractResponse should have a version number in it, since the data it contains varies based on the version.  Sometimes the interpretation of the data varies based on version as well.  It's a property of the response and it belongs in the response object.  But hopefully we can hold off on that for now since this patch is kinda big as-is..."
95308701,2264,cmccabe,2017-01-10T06:57:14Z,"Yeah, let's leave the function name out.  Will use string interpolation"
95309085,2264,cmccabe,2017-01-10T07:02:05Z,"You are right that the general pattern is that constructor arguments are required, other things are optional.  Ideally we would have ListOffsetRequest#Builder(Map<TopicPartition, PartitionData>) and ListOffsetRequest#Builder(Map<TopicPartition, Long>), but we can't do that because of a Java limitation... both constructors would have the same type once type erasure has been performed, which is not allowed."
95309234,2264,cmccabe,2017-01-10T07:03:58Z,"Braces seem a little more familiar to me because of JSON.  But if parentheses are the way to go, I can change it.  It's a big change though since all the builders have a toString.  Are we sure we want the parens?"
95324930,2264,ijuma,2017-01-10T09:19:53Z,I was debating that myself before writing the comment. Let's leave it as is then.
95342279,2264,ijuma,2017-01-10T11:04:44Z,"Oh, I see. The compiler is correct in not allowing that then. One way to fix that is to change the definition of `Builder` to be:

```java
public static abstract class Builder<T extends AbstractRequest, B extends Builder<T, B>> {
  ...
  public B setVersion(short version) {
  ...
```

But this makes it more awkward to use the abstract builder directly due to the additional type parameter (in Scala we could use type members instead). An alternative is to override `setBuilder` in each builder and use the more specific type there. It's a bit annoying, but builders are supposedly used more times than they are defined."
95342721,2264,ijuma,2017-01-10T11:07:32Z,"The broker assumes that all requests are consistent with regards to the common request header elements. If we set the minimum versions correctly, we should be in the clear, I think. It would be good to have a test for 0.9.0.1, but we can do that in a follow-up."
95343648,2264,ijuma,2017-01-10T11:13:53Z,"I think I prefer `UnsupportedBrokerVersion`. But it's subjective and `obsolete`/`outdated` are more explicit with regards that you want a newer broker.

Another point: if we think of the protocol as something independent from Kafka (i.e. there could be other implementations), which is something that Jay thinks we should, then `UnsupportedBrokerVersion` also seems better."
95344394,2264,ijuma,2017-01-10T11:19:23Z,"There are indeed a few ways to do this. We tend to follow the `Foo(a=""a"",b=""b"")` model and this is what public classes like `ProducerRecord` and `ConsumerRecord` do.

Also, it's worth saying that, outside of loops, `StringBuilder` is no better than String concatenation with regards to performance. Up to java 8, javac will do the StringBuilder thing itself. From Java 9, invokedynamic will be used to allow the runtime to choose the best strategy (http://openjdk.java.net/jeps/280).

My suggestion is to clean this up in a subsequent PR. It can even be done after the feature freeze."
95358669,2264,ijuma,2017-01-10T13:01:34Z,"@junrao, there is a comment already a couple of lines above (I asked Ashish to add it as part of KAFKA-3600):

```
// When multiple versions of SASL_HANDSHAKE_REQUEST are to be supported,
// API_VERSIONS_REQUEST must be sent prior to sending SASL_HANDSHAKE_REQUEST to
// fetch supported versions.
```

Maybe we can move the comment so that it's closer to the request creation."
95358836,2264,ijuma,2017-01-10T13:02:45Z,"It's a bit odd that we add a hardcoded prefix and suffix in a generic `join` method. Typically, these would be parameters and the default case would be no prefix or suffix (consistent with `join` that takes a `collection`)."
95359080,2264,ijuma,2017-01-10T13:04:29Z,Nit: space missing before `:`.
95359283,2264,ijuma,2017-01-10T13:05:52Z,"@cmccabe, have we tested with SSL enabled?"
95363623,2264,ijuma,2017-01-10T13:33:37Z,Nit: `1: Short` is a little better because it will fail to compile if the conversion is not possible where `toShort` will happily overflow. Doesn't make a difference in these particular cases though.
95368726,2264,ijuma,2017-01-10T14:04:14Z,Nit: `0: java.lang.Long` is a tiny bit shorter.
95369472,2264,ijuma,2017-01-10T14:08:50Z,I think it would be worth adding an overload without the `callback` as it's optional and many callers don't need it.
95370437,2264,ijuma,2017-01-10T14:14:21Z,This is unused.
95373680,2264,ijuma,2017-01-10T14:31:44Z,I think @hachikuji is thinking of the case where `ret.get(partition)` returns `null`. Not sure if we are enforcing that elsewhere though.
95376734,2264,ijuma,2017-01-10T14:46:19Z,"Typo, `r` missing."
95378574,2264,ijuma,2017-01-10T14:54:01Z,Nit: I think this is harder to read by being split into 3 lines like this. Looks like a staircase. ;) There's one other case like that.
95379773,2264,ijuma,2017-01-10T14:59:28Z,Change `versionId` to `short` like you did in other places?
95394973,2264,ijuma,2017-01-10T16:01:54Z,It looks like we only need to support `2` and above: https://github.com/apache/kafka/blob/0.10.0/clients/src/main/java/org/apache/kafka/common/protocol/Protocol.java
95397340,2264,ijuma,2017-01-10T16:11:14Z,Should we be validating that version is always `1`? Version `0` would probably mean that something is wrong.
95398519,2264,ijuma,2017-01-10T16:16:08Z,In other request classes we don't throw an exception in case the version is higher than expected. We should be consistent. I think this approach is good because it forces people to consider how to handle a new version when it's introduced.
95399367,2264,ijuma,2017-01-10T16:19:57Z,"I think we should set it to `-1` in this case to make it clear that it won't be used as we have the following:

```java
if (version >= 1) {
    struct.set(REBALANCE_TIMEOUT_KEY_NAME, rebalanceTimeout);
}
```"
95400451,2264,ijuma,2017-01-10T16:24:42Z,We have 3 `getVersion` calls. One should be enough.
95402987,2264,ijuma,2017-01-10T16:35:38Z,Parenthesis are not needed here or the other branches since a single value is being returned now.
95403350,2264,ijuma,2017-01-10T16:37:05Z,Nit: should be `fetchData()`.
95403601,2264,ijuma,2017-01-10T16:38:06Z,Nit: there should be a space before `:`.
95403993,2264,ijuma,2017-01-10T16:39:51Z,"Personally, I think `Short` and `null` for no usable version is less error-prone (you'll get a NPE if you accidentally try to use it instead of ending with a `-1` somewhere it should not be)."
95404122,2264,ijuma,2017-01-10T16:40:22Z,Seems like this comment should be in the method documentation.
95404878,2264,ijuma,2017-01-10T16:43:20Z,"Why not use `EnumMap<ApiKeys, Short>`? It would be more readable IMO."
95405837,2264,ijuma,2017-01-10T16:47:11Z,Can this ever be less than 0?
95405971,2264,ijuma,2017-01-10T16:47:48Z,`Math.min` and 4 lines become 1?
95406464,2264,ijuma,2017-01-10T16:50:01Z,This `toString` is quite long. :) It would be nice to break it down into smaller methods so that it's easier to understand.
95407688,2264,ijuma,2017-01-10T16:55:15Z,"Any reason we are using `LinkedList` instead of `ArrayList`? We seem to be adding to the end, iterating and then clearing (after Jason's suggestion), so `ArrayList` seems like the better option."
95409034,2264,ijuma,2017-01-10T17:01:28Z,"It would be good to include the most current version in this message as well. Also, as Jun said it may be a good to log at a higher level if we are downgrading a request. Not sure what Jun had in mind, but maybe `debug`?"
95410187,2264,ijuma,2017-01-10T17:06:45Z,"We have the following code for failed metadata requests:

```java
          if (cluster.isBootstrapConfigured()) {
                int nodeId = Integer.parseInt(destination);
                Node node = cluster.nodeById(nodeId);
                if (node != null)
                    log.warn(""Bootstrap broker {}:{} disconnected"", node.host(), node.port());
            }
```

Should we be doing similarly for this case (since we now do `ApiVersionsRequest` before `MetadataRequest`)?"
95410941,2264,ijuma,2017-01-10T17:10:14Z,`now` is no longer used
95411451,2264,ijuma,2017-01-10T17:12:49Z,"We are now doing `ApiVersions` unconditionally. This means that 0.10.2 brokers won't be able to talk to 0.9.0.x brokers, right? We shouldn't do that."
95418264,2264,cmccabe,2017-01-10T17:45:59Z,"Hmm.  Just to clarify, you think that throwing the exception when we see an unexpected version is the right way to go?  I can add that to the other requests if so."
95418858,2264,cmccabe,2017-01-10T17:48:52Z,The other versions are used in unit tests.  The server also can handle them to support older clients.
95419171,2264,cmccabe,2017-01-10T17:50:25Z,"I wanted to do this, but it got messy.  The problem is that there are a bunch of unit tests that are using reflection to call these methods-- and also some scala code.  I think we should have a follow-on JIRA for converting the parse() methods to use 'short' for version."
95419258,2264,cmccabe,2017-01-10T17:50:53Z,k
95419528,2264,cmccabe,2017-01-10T17:52:23Z,That's a good point.  What do you suggest?  One approach is to (re)add a constructor parameter to NetworkClient that will allow the brokers to bypass sending ApiVersions.
95438375,2264,cmccabe,2017-01-10T19:27:44Z,k
95438824,2264,cmccabe,2017-01-10T19:29:50Z,I fixed it to be an exception instead
95440180,2264,hachikuji,2017-01-10T19:36:18Z,I doubt it would add much to the size of this patch if you want to do it here. A follow-up would work as well. I'm anxious to have a good general approach in place so that we have clear patterns to follow going forward.
95442388,2264,cmccabe,2017-01-10T19:47:17Z,"From a user point of view, UnsupportedBrokerVersion is that it doesn't make it clear whether the broker version is too new or old.  It also suggests that the whole broker is unsupported, whereas just this one API is unsupported.

> Another point: if we think of the protocol as something independent from Kafka (i.e. there could be other implementations), which is something that Jay thinks we should, then UnsupportedBrokerVersion also seems better.

Hmm.  It seems like we could still have other implementations of the protocol even if we used OutdatedBrokerException / ObsoleteBrokerException?

I'm open to ideas on this.  I would kind of like something that gives the user a clear hint that they need to upgrade..."
95443442,2264,cmccabe,2017-01-10T19:52:32Z,"Ah, good point.  Let me add a check for that just to be safe.  After all, this data is coming over the wire, so we should not unconditionally trust it."
95445212,2264,hachikuji,2017-01-10T20:02:02Z,Note there was a previous comment about using `ArrayList` instead of `LinkedList`.
95449880,2264,cmccabe,2017-01-10T20:27:07Z,k
95450837,2264,cmccabe,2017-01-10T20:32:37Z,"Yeah, it would be good to have a clear pattern here.  Let's do it in a follow-on."
95452015,2264,cmccabe,2017-01-10T20:39:34Z,"Hmm.  canSendRequest should be making sure that the SSL handshake is complete, right?

Call stack:
```
NetworkClient#doSend
NetworkClient#canSendRequest
Selector#isChannelReady
KafkaChannel#ready
```

KafkaChannel#ready has:
```
    public boolean ready() {
        return transportLayer.ready() && authenticator.complete();
    }
```"
95452301,2264,cmccabe,2017-01-10T20:41:17Z,@ijuma: I'll parameterize our ducktape tests for client compat with SSL
95458316,2264,cmccabe,2017-01-10T21:13:47Z,renamed to Utils#mkString and made this optional with function overloads.
95463538,2264,ijuma,2017-01-10T21:40:30Z,This should be `setVersion(version)` I think.
95465642,2264,hachikuji,2017-01-10T21:50:26Z,Sounds reasonable to me.
95466268,2264,hachikuji,2017-01-10T21:53:37Z,nit: string interpolation
95466619,2264,hachikuji,2017-01-10T21:55:35Z,Probably more helpful to convert the error code to an instance of `Errors` and use `message()`. Even better would be to have `ApiVersionsResponse` use a field of type `Errors` instead of a short.
95468151,2264,hachikuji,2017-01-10T22:03:47Z,"nit: the `toString()` is not needed, right?"
95468429,2264,hachikuji,2017-01-10T22:05:28Z,I was thinking about this a little more. I wonder if we could just change the key type to string to make it consistent with `nodesNeedingApiVersionsFetch`?
95469545,2264,hachikuji,2017-01-10T22:11:46Z,Could we have this accept an `ApiKeys` instance instead of short? Then we wouldn't need the check for a negative api key. Also would be nice to document what exactly this method is returning. Is it the largest version supported by both the client and server?
95471626,2264,hachikuji,2017-01-10T22:23:31Z,"typo: ""reponse"""
95471877,2264,hachikuji,2017-01-10T22:25:04Z,I think I had a comment before about moving this check into `ConsumerNetworkClient.RequestFutureCompletionHandler`.
95472157,2264,hachikuji,2017-01-10T22:26:54Z,"To be clear, if the broker doesn't support this then we'll raise `ObsoleteBrokerException`? I wonder if that handling is consistent with the behavior when the broker is up to date, but has an old message format version. It seems our handling of the `UNSUPPORTED_FOR_MESSAGE_FORMAT` is to add a null entry to the result. Maybe we should do the same?"
95472929,2264,hachikuji,2017-01-10T22:31:10Z,Would we ever return a negative offset to the user?
95473004,2264,hachikuji,2017-01-10T22:31:35Z,nit: move to previous line
95473512,2264,hachikuji,2017-01-10T22:34:31Z,Another suggestion: `IncompatibleProtocolVersion`?
95475950,2264,hachikuji,2017-01-10T22:49:21Z,Seems the only usage is internal to `NetworkClient`. Maybe we could just use the variable directly?
95477354,2264,ijuma,2017-01-10T22:57:56Z,"Also, is `discoverPeerVersions` the right name? Seems like it should be `discoverBrokerVersions` as `NetworkClient` is also used by clients."
95477481,2264,cmccabe,2017-01-10T22:58:42Z,"OK, I added a constructor parameter and a unit test.  Thanks for catching this!"
95477659,2264,ijuma,2017-01-10T22:59:44Z,"Are the comments inside the if/else adding anything over what the code does? To me, they seem to say exactly the same thing as the code."
95479078,2264,cmccabe,2017-01-10T23:09:16Z,k
95479437,2264,cmccabe,2017-01-10T23:11:52Z,"For some reason, there is no Math.min(short, short), only Math.min(int, int).  I could use the int version with a typecast, but I thought the cast was ugly."
95479552,2264,cmccabe,2017-01-10T23:12:44Z,k
95480087,2264,cmccabe,2017-01-10T23:16:37Z,k
95480896,2264,cmccabe,2017-01-10T23:22:12Z,k
95481038,2264,cmccabe,2017-01-10T23:23:07Z,"ok.  will leave it as toShort here since the typecast is kind of annoying visually
(edit: it's an ascription) ;)"
95481685,2264,cmccabe,2017-01-10T23:28:03Z,"not really, can remove"
95482457,2264,hachikuji,2017-01-10T23:33:37Z,Seems this field could be final. Might be worth a pass over the other request types to locate other fields that could be final.
95482720,2264,hachikuji,2017-01-10T23:35:25Z,This seems unneeded.
95483218,2264,hachikuji,2017-01-10T23:39:11Z,"It would be nice to be consistent about whether a field should be included in the constructor or whether it should have a setter. I don't know that we have any cases that call for mutating a field that was initialized in the constructor, so I would prefer to just leave off the setters in that case and make the field final."
95483731,2264,cmccabe,2017-01-10T23:43:13Z,"Oops, sorry, I meant to reply but I closed the browser window.  ArrayList seems like overkill here since we don't expect a lot of aborted sends to happen.  Also, the memory consumed by ArrayList only grows and never shrinks (unless you call trimToSize), which seems like a bad choice here."
95483834,2264,cmccabe,2017-01-10T23:43:59Z,k
95484012,2264,cmccabe,2017-01-10T23:45:13Z,ok
95484274,2264,cmccabe,2017-01-10T23:47:15Z,"Yeah, let's use Errors to get the error enum.  Can refactor further later"
95484396,2264,cmccabe,2017-01-10T23:48:10Z,"true, it should be called implicitly"
95484656,2264,cmccabe,2017-01-10T23:49:56Z,good ideas.  done
95484862,2264,cmccabe,2017-01-10T23:51:24Z,"I don't understand what you mean by ""the broker is up to date, but has an old message format version""."
95484986,2264,cmccabe,2017-01-10T23:52:18Z,"Right, I have been looking at that.  Thinking of doing it in a follow-on JIRA since I will need to take a deeper look at the RPC paths when doing that"
95485081,2264,cmccabe,2017-01-10T23:53:03Z,k
95485096,2264,cmccabe,2017-01-10T23:53:08Z,no
95485142,2264,cmccabe,2017-01-10T23:53:26Z,k
95486629,2264,hachikuji,2017-01-11T00:05:59Z,"I'm thinking of the case where the broker doesn't support v1 of ListOffsets. For this case, I think we currently raise `ObsoleteBrokerException`. I am questioning whether it would be more consistent to return a null entry in this case in the result of `offsetsForTimes`. Currently it is possible for the broker to support the new api version, but not the message format version which is needed to answer the query. In this case, we return a null entry."
95488513,2264,ijuma,2017-01-11T00:20:08Z,"Well, the `ArrayList` array would expand to 10 elements the first time an aborted send is added. And then, it will probably never grow beyond that. LinkedList in Java is just not a very good collection in general. Each node has 3 references and there is a node instance per item. A funny tweet by Joshua Bloch:

""@jerrykuch @shipilev @AmbientLion Does anyone actually use LinkedList?  I wrote it, and I never use it.""
https://twitter.com/joshbloch/status/583813919019573248"
95489336,2264,ijuma,2017-01-11T00:26:51Z,I asked Colin to add this point to the follow-up JIRA.
95490172,2264,ijuma,2017-01-11T00:33:36Z,We should not have it in the javadoc then as this is a class that is exposed to users.
95504733,2264,junrao,2017-01-11T03:03:06Z,"In the patch, it seems that once a channel is connected, but is not necessarily ready, NetworkClient can start issuing ApiRequest. This can fail if transport is SSL and transportLayer.ready() is false. It's also too late to only start issuing ApiRequest after channel is ready since the ApiRequest should be sent before SASL handshake (SASL handshake completes when authenticator.complete() is true). 

What the NetworkClient is supposed to do is only start issuing ApiRequest after transportLayer.ready() is true. It's just that currently, this state is no propagated by Selector."
95506134,2264,ijuma,2017-01-11T03:23:02Z,"@junrao I'm not sure what you mean when you say that the patch starts issuing ApiVersions request before transport layer is ready:

```java
private void handleInitiateApiVersionRequests(long now) {
        Iterator<String> iter = nodesNeedingApiVersionsFetch.iterator();
        while (iter.hasNext()) {
            String node = iter.next();
            if (selector.isChannelReady(node) && inFlightRequests.canSendMore(node)) {
                log.debug(""Initiating API versions fetch from node {}."", node);
                ApiVersionsRequest.Builder apiVersionRequest = new ApiVersionsRequest.Builder();
                ClientRequest clientRequest = newClientRequest(node, apiVersionRequest, now, true, null);
                doSend(clientRequest, true, now);
                iter.remove();
            }
        }
    }
```
Isn't that OK (we call `selector.isChannelReady(node)`)?

The point about SASL. The outcome of the discussion in the PR for KAFKA-3600 was that we don't have to send the api versions request before the SASL request for now since there's only one version of the SaslHandshake request. If we ever introduce an additional version, we can change the client at the same time. It seems like there's no downside to this since brokers will have to keep supporting the old sasl handshake request anyway. Do you feel differently?"
95507893,2264,junrao,2017-01-11T03:50:34Z,"@ijuma : Yes, I missed the selector.isChannelReady(node) check. So, this is fine. About the SASL, that's reasonable. We can keep the changes in the PR as they are."
95579492,2264,ijuma,2017-01-11T13:41:57Z,"I submitted a PR that removes unused setters, makes fields final and tries to make things a bit more regular. Makes it a bit simpler, but more could be done probably."
95579966,2264,ijuma,2017-01-11T13:44:52Z,"@cmccabe, this one seems important to resolve before merging."
95582701,2264,ijuma,2017-01-11T14:01:03Z,I've done this in my PR.
95582725,2264,ijuma,2017-01-11T14:01:11Z,Removed in my PR.
95582772,2264,ijuma,2017-01-11T14:01:25Z,Fixed in my PR.
95582876,2264,ijuma,2017-01-11T14:02:06Z,The server doesn't use the builder so I think that would be OK. But let's leave it for now.
95582919,2264,ijuma,2017-01-11T14:02:21Z,"Yes, but let's consider that in a separate PR."
95583125,2264,ijuma,2017-01-11T14:03:35Z,Changed in my PR.
95583846,2264,ijuma,2017-01-11T14:07:42Z,I did this for a bunch of builders in my PR.
95638931,2264,hachikuji,2017-01-11T18:21:36Z,@cmccabe Can you address this comment please?
95641786,2264,cmccabe,2017-01-11T18:35:12Z,done
95641982,2264,cmccabe,2017-01-11T18:36:01Z,sounds good
1290585581,14182,jeffkbkim,2023-08-10T19:23:36Z,I would include the priorities when considering uniform vs. sticky vs. rack-aware
1290588764,14182,jeffkbkim,2023-08-10T19:27:03Z,"is ""expected"" in the name bring necessary?"
1290589362,14182,jeffkbkim,2023-08-10T19:27:44Z,"is this ""remaining number of partitions""?"
1290591418,14182,jeffkbkim,2023-08-10T19:30:04Z,i don't think we need this comment
1290591834,14182,jeffkbkim,2023-08-10T19:30:34Z,how's `previousOwners`?
1290610215,14182,jeffkbkim,2023-08-10T19:49:17Z,why is all of this under the constructor?
1290610669,14182,jeffkbkim,2023-08-10T19:49:49Z,nit: can we match the argument ordering?
1290611774,14182,jeffkbkim,2023-08-10T19:51:09Z,"`buildAssignment()` makes more sense. also, this should be an abstract method under `UniformAssignor`"
1290636312,14182,jeffkbkim,2023-08-10T20:17:29Z,i'm noticing a pattern where we are using class variables when they can just be method variables. we should aim to use the smallest scope for all variables. 
1290763071,14182,rreddy-22,2023-08-10T23:12:48Z,including balance >rack>stickiness
1290763624,14182,rreddy-22,2023-08-10T23:13:54Z,"yeah because the extra partitions haven't been assigned yet, I'm just taking a count of how many are expected to get the extra partition"
1290764909,14182,rreddy-22,2023-08-10T23:16:54Z,"I think this name makes it clear that its a map, if we just used previousOwners, its not clear whose previous owner right. We would then have to name it previousPartitionOwners, which is kinda the same as partitionToPreviousOwner? "
1290765488,14182,rreddy-22,2023-08-10T23:18:12Z,"Any computations required to initialise the global attributes are done in this constructor, similar to the other existing assignors"
1290766391,14182,rreddy-22,2023-08-10T23:20:13Z,done
1290813322,14182,jeffkbkim,2023-08-11T01:14:50Z,"`numMembersWithExtraPartition` : number of members to get the extra partition, no?

there is no expected vs. actual though, we always distribute that number"
1290813753,14182,jeffkbkim,2023-08-11T01:16:00Z,"it is implied by the type that the previous owners are keyed by TopicIdPartition. like how we are using `potentiallyUnfilledMembers`, `unfilledMembers`, or `newAssignment`"
1290880471,14182,rreddy-22,2023-08-11T04:25:37Z,"its typically called as assignor.build so I thought that makes it kinda clear that its building the assignment, and the return value also specifies the same"
1290880628,14182,rreddy-22,2023-08-11T04:25:58Z,"Changed a few of them, rest we discussed on call"
1291715874,14182,jeffkbkim,2023-08-11T19:39:09Z,nit: `allTopicIdPartitions` and newline each argument
1291716306,14182,jeffkbkim,2023-08-11T19:39:46Z,nit: allSubscriptionsEqual
1291718703,14182,jeffkbkim,2023-08-11T19:43:22Z,"nit:

```
    public GroupAssignment assign(
        AssignmentSpec assignmentSpec, 
        SubscribedTopicDescriber subscribedTopicDescriber
    ) throws PartitionAssignorException {"
1291718960,14182,jeffkbkim,2023-08-11T19:43:43Z,nit: new line each argument
1291724906,14182,jeffkbkim,2023-08-11T19:52:45Z,this and `parttiionRacks` can be initialized as empty collections. then we just handle the case where `!membersByRack.isEmpty()`
1291729367,14182,jeffkbkim,2023-08-11T19:59:06Z,can you explain why we need to create `membersByRack` then transform it to `memberRacks`? can't we just create `memberRacks` from the start?
1291731843,14182,rreddy-22,2023-08-11T20:02:51Z,yeah my point was that numMembersWithExtraPartition sounds like the members already have the extra partition but its actually a number of how many members we expect to have an extra and this number keeps decreasing as we assign the partitions.
1291732541,14182,rreddy-22,2023-08-11T20:03:44Z,okay I'll rename it 
1293887191,14182,jeffkbkim,2023-08-14T19:32:12Z,we can move this to L93 and remove the else block
1293887893,14182,jeffkbkim,2023-08-14T19:33:01Z,"can we use `this.classVariable = ...` for all the class scoped variables in the constructor?

also, don't we get a NoSuchElementException if the iterator is empty? (do we test this case)

lastly, we can just use the first member's subscribed topic ids because we know all members are subscribed to the same topics right?"
1293888288,14182,jeffkbkim,2023-08-14T19:33:30Z,do we need this variable?
1293889620,14182,jeffkbkim,2023-08-14T19:35:13Z,i think a debug log should suffice. logs will get flooded if a topic metadata changes
1293893418,14182,jeffkbkim,2023-08-14T19:38:31Z,should we move this to the top of the method? 
1293898405,14182,jeffkbkim,2023-08-14T19:42:34Z,can we use `minQuota = totalPartitionsCount / numberOfMembers;` and do we need to declare and initialize separately?
1293910818,14182,jeffkbkim,2023-08-14T19:57:07Z,do we need this? we can iterate through a set
1293913755,14182,jeffkbkim,2023-08-14T20:00:36Z,we can remove the parentheses
1294009242,14182,jeffkbkim,2023-08-14T22:00:23Z,"this will iterate through the entire list. let's instead use a set to store subscriptions.

also, if the subscription list does not contain the topic id, should we throw an illegal state exception? every topic Id from the assigned partitions should exist in the subscription list right"
1294021888,14182,jeffkbkim,2023-08-14T22:19:36Z,"how's `retainedCurrentAssignment`?

in kafka, we don't use `get` in method names."
1294034638,14182,jeffkbkim,2023-08-14T22:42:29Z,`k` --> `__`
1294037874,14182,jeffkbkim,2023-08-14T22:47:31Z,ditto
1294047232,14182,jeffkbkim,2023-08-14T23:07:04Z,can we `continue` here instead of using `assigned` variable?
1294051623,14182,jeffkbkim,2023-08-14T23:16:44Z,"let's use streams api when possible

```
            listAllTopics.forEach(topic -> {
                IntStream.range(0, subscribedTopicDescriber.numPartitions((topic)))
                    .forEach(i -> allPartitions.add(new TopicIdPartition(topic, i)));
            });
```"
1294052212,14182,jeffkbkim,2023-08-14T23:18:06Z,"let's use streams api (forEach, IntStream.forEach) "
1294053172,14182,jeffkbkim,2023-08-14T23:20:12Z,"```
        potentiallyUnfilledMembers.forEach((memberId, remaining) -> {

        });
```"
1294054650,14182,jeffkbkim,2023-08-14T23:23:29Z,"this is implying we will be assigning an extra partition for this member right? how come we don't ""assign"" it here?"
1294056808,14182,jeffkbkim,2023-08-14T23:28:09Z,"```
        int remaining = unfilledMembers.values().stream().reduce(0, Integer::sum);
```"
1294062365,14182,jeffkbkim,2023-08-14T23:40:50Z,why can't we just create a new arraylist and add the ones where count > 0?
1294064001,14182,jeffkbkim,2023-08-14T23:44:19Z,you can use getOrDefault
1294068788,14182,jeffkbkim,2023-08-14T23:55:16Z,"assignPartitionToMemberAndUpdateUnfilledMembers

it's hard to assume the side effects when looking at the existing name"
1294068919,14182,jeffkbkim,2023-08-14T23:55:35Z,i think we can use Map#compute here right
1294069200,14182,jeffkbkim,2023-08-14T23:56:10Z,we can break out of the for loop instead of using assigned here
1294076876,14182,jeffkbkim,2023-08-15T00:14:57Z,why do we need to remove here
1294086512,14182,jeffkbkim,2023-08-15T00:38:49Z,"what i don't like about this approach is that the reader needs to know 2 things:
1. unfilledMembers (member -> remaining partition count) only exists when the count is > 0.
2. assignPartitionToMember removes a member from unfilledMembers if the count is 0 after decrementing

or else we don't know whether this for loop will end. also i have not seen a for loop where we add something to the original collection we are iterating through. i think `while (!queue.isEmpty())` should be more readable.

also, if there is a bug somewhere and for some reason the sum of all unfilledMembers` partition counts is greater than sortedPartitions size then we could have an infinite loop since there will be a member that still needs partitions to be assigned. we should add this check somewhere"
1294086912,14182,jeffkbkim,2023-08-15T00:39:55Z,do we need this?
1294086965,14182,jeffkbkim,2023-08-15T00:40:02Z,do we need this?
1294087718,14182,jeffkbkim,2023-08-15T00:42:11Z,ditto on using continue
1294087923,14182,jeffkbkim,2023-08-15T00:42:45Z,how come we iterate on unfilledMembers instead of roundRobinMembers as in `rackAwareRounRobinAssignment`? this looks like a bug; can we add a test that breaks this
1294088129,14182,jeffkbkim,2023-08-15T00:43:19Z,ditto on using a while loop
1295197016,14182,rreddy-22,2023-08-15T22:21:32Z,changed to something else
1295198962,14182,rreddy-22,2023-08-15T22:22:32Z,This shouldn't even happen at all so that's why its a warning. This case is handled by the target assignment builder
1296343128,14182,jeffkbkim,2023-08-16T19:31:45Z,"if a topic is deleted, how does the target assignment builder handle it?"
1300462035,14182,rreddy-22,2023-08-21T17:52:49Z,"Collections.emptyMap is immutable though right, ig a better option would be to initliaze them as new HashMaps"
1300620312,14182,rreddy-22,2023-08-21T20:36:34Z,"we check if rack aware assignment is possible i.e if partition racks and member racks exist and whether there's any intersection between the two sets, only then we assign the memberRacks which is final. "
1300622597,14182,rreddy-22,2023-08-21T20:39:02Z,"The code actually does just use the first members subscribed topic ids
"
1300627015,14182,rreddy-22,2023-08-21T20:44:16Z,"That depends if the members map can be empty I guess which I think is checked before, it definitely can't be null"
1300631692,14182,rreddy-22,2023-08-21T20:50:14Z,"If a topic is deleted then it's basically the case where the topic doesn't exist in the topic metadata, it is just simply left off the subscription list before its sent to the assignor."
1300633100,14182,rreddy-22,2023-08-21T20:52:07Z,we can't cause we might remove topics from the list just in case there's members subscribed to topics that don't exist in the topic metadata
1300633833,14182,rreddy-22,2023-08-21T20:53:05Z,I wanted to keep them in just to segregate the replicaRacks parts
1300637770,14182,rreddy-22,2023-08-21T20:58:04Z,"not necessarily, since there could be partitions belonging to old subscriptions or topics that don't exist anymore and we don't need them anymore aka they're not valid anymore which is fine. Mentioned in the javadoc
"
1300779466,14182,rreddy-22,2023-08-22T00:25:27Z,"its not retained cause we didn't decide if we wanna keep it yet, that's decided in assignStickyPartitions. This is just getting the valid assignment in terms of if the partitions are still part of the subscription topics and also if the racks don't match then we just store the prev owner info for the future"
1300780584,14182,rreddy-22,2023-08-22T00:27:40Z,"I don't understand what's the change?
"
1303273451,14182,rreddy-22,2023-08-23T16:29:35Z,"So there's potentially unfilled members which includes members that have remaining number of partitions to receive as >=0, i.e there could be members that have met the quota but are eligible to receive an extra partition. What I'm doing in this step is increasing the remaining count if there's still a possibility that a member could receive an extra partition. If the remaining count is still > 0 after that we add it to the unfilled members list. "
1303299811,14182,rreddy-22,2023-08-23T16:54:38Z,"isn't it kinda expected that if I assign a partition to a member from the unfilled map and the remaining value is updated aka reduced by 1, and it's removed from the map if it isn't unfilled anymore."
1303309866,14182,rreddy-22,2023-08-23T17:04:30Z,"I still have to update the rr members after assigning the partition, I can't break within this if statement right."
1305945556,14182,rreddy-22,2023-08-25T17:26:18Z,discussed offline that this isn't possible 
1305948679,14182,rreddy-22,2023-08-25T17:29:41Z,"we can't use continue cause there's multiple other steps after that which need to be executed before we exit the loop
"
1305956396,14182,rreddy-22,2023-08-25T17:37:07Z,"we are iterating over the roundRobin members, we're just using the size of all the unfilled members so that we do at least n number of iterations"
1305957286,14182,rreddy-22,2023-08-25T17:37:44Z,discussed offline that this isn't possible
1305998324,14182,rreddy-22,2023-08-25T18:13:40Z,while loop instead of for loop?
1308056437,14182,jeffkbkim,2023-08-28T23:42:28Z,can we keep RangeAssignor changes in a separate PR?
1308059442,14182,jeffkbkim,2023-08-28T23:49:03Z,what if the collections are out of order? can we add a test case for this
1308060754,14182,jeffkbkim,2023-08-28T23:51:42Z,javadocs
1308061244,14182,jeffkbkim,2023-08-28T23:52:48Z,"let's add javadocs for all of the methods below.

also, can there be a case where any of the parameters are null? Collections#disjoint can throw NPE"
1308061768,14182,jeffkbkim,2023-08-28T23:54:00Z,nit: toTopicIdPartitions
1308061977,14182,jeffkbkim,2023-08-28T23:54:26Z,nit: topicIdPartitions
1308062300,14182,jeffkbkim,2023-08-28T23:55:04Z,change k to __
1308062948,14182,jeffkbkim,2023-08-28T23:56:30Z,topics
1308068063,14182,jeffkbkim,2023-08-29T00:08:19Z,"how's
```
                Set<String> partitionRacks;
                Map<TopicIdPartition, Set<String>> racksByPartition;
```
this aligns with the parameters in useRackAwareAssignment"
1308072483,14182,jeffkbkim,2023-08-29T00:18:02Z,"i think we can do the following here:

when initializing membersByRack in L152, also initialize memberRacks. then only set memberRacks to Collections.emptyMap if useRackAwareAssignment is false. then we don't have to do this transformation"
1308073563,14182,jeffkbkim,2023-08-29T00:20:43Z,nit: memberRack
1308074883,14182,jeffkbkim,2023-08-29T00:23:59Z,"can you help me understand which part is rack-aware manner?

also, can we add a test case for this?"
1308075407,14182,jeffkbkim,2023-08-29T00:25:09Z,this looks to be an exact copy of clients/common. can we move that instead?
1308077282,14182,jeffkbkim,2023-08-29T00:29:12Z,do we need this change in this PR?
1308077407,14182,jeffkbkim,2023-08-29T00:29:30Z,what's this change for?
1308077660,14182,jeffkbkim,2023-08-29T00:30:06Z,"nit: we can remove Kafka

also, can we move this to buildAssignment() and direct the reader to the method to see the step by step?"
1308079903,14182,jeffkbkim,2023-08-29T00:35:27Z,"nit: track it ""as the partition's"" prior owner"
1308081089,14182,jeffkbkim,2023-08-29T00:38:29Z,i'm wondering whether to have a useRackAwareStrategy field in RackInfo instead so that we don't have to infer this information here.
1308081761,14182,jeffkbkim,2023-08-29T00:39:56Z,"actually, i think we should throw an illegal state exception. since this is not expected to happen"
1308082801,14182,jeffkbkim,2023-08-29T00:42:39Z,"why can't we do
`final int minQuota = totalPartitionsCount / numberOfMembers;`?"
1308083274,14182,jeffkbkim,2023-08-29T00:43:51Z,`assignmentSpec.members().keySet().forEach(memberId -> ...`
1308085281,14182,jeffkbkim,2023-08-29T00:48:43Z,nit: assignedStickyPartitions
1308086816,14182,jeffkbkim,2023-08-29T00:52:39Z,should we add a debug log indicating that the topic no longer exists in the metadata?
1308086969,14182,jeffkbkim,2023-08-29T00:53:04Z,we can move the parentheses around (partition)
1308087850,14182,jeffkbkim,2023-08-29T00:55:05Z,we don't need this since retainedPartitionsCount will be 0 in L178
1308088960,14182,jeffkbkim,2023-08-29T00:57:28Z,"""previous"" step

might not even need this comment. perhaps ""Assign the extra partition if possible"" or something along those lines"
1308089767,14182,jeffkbkim,2023-08-29T00:59:28Z,nit: assignedStickyPartitions
1308090172,14182,jeffkbkim,2023-08-29T01:00:20Z,"personally, the word ""all"" does not add much value to variable names. how's `sortedTopics`

also, what's the benefit of sorting the topics?"
1308091012,14182,jeffkbkim,2023-08-29T01:02:22Z,nit: can remove parentheses around (topic)
1308091211,14182,jeffkbkim,2023-08-29T01:02:52Z,we can inline partitionCount in L377
1308093254,14182,jeffkbkim,2023-08-29T01:08:18Z,how's unassignedPartitionsSizeEqualsRemainingAssignments
1308095670,14182,jeffkbkim,2023-08-29T01:13:56Z,"nit: ""Sort"" and ""potential members"""
1308097288,14182,jeffkbkim,2023-08-29T01:17:54Z,is this a linked list?
1308098707,14182,jeffkbkim,2023-08-29T01:19:55Z,`while(!roundRobinMembers.isEmpty() && !assigned)` is more readable since most for loops do not change the size
1308102973,14182,jeffkbkim,2023-08-29T01:28:55Z,ah right. that makes sense
1308137334,14182,rreddy-22,2023-08-29T02:48:14Z,"lets decide what to do here cause I've also asked David multiple times, I think he said a warning is enough let me double check"
1308140708,14182,rreddy-22,2023-08-29T02:55:31Z,I feel like a whole PR for minor changes which are kinda related to what I learn from doing the other assignors would be unnecessary especially cause even minor ones take time to go through the review process and stuff. They might just go undone
1308143447,14182,rreddy-22,2023-08-29T03:01:34Z,"That's kinda why I used List/Set before but I changed it to Collections on receiving comments on the interface. But that's a good point, I'll make sure the order doesn't matter"
1308150557,14182,rreddy-22,2023-08-29T03:18:54Z,"no they can't be null, they are all initialized in rackInfo"
1308150974,14182,rreddy-22,2023-08-29T03:19:52Z,"I named the method allTopicIdPartitions to emphasize that its not a subset, so I feel like we can leave this as allTopicIdPartitions as well"
1308151486,14182,rreddy-22,2023-08-29T03:21:17Z,renamed to allTopicIds
1308152462,14182,rreddy-22,2023-08-29T03:23:48Z,Agreed but memberRacks and partitionRacks are more readable and concise I guess in my opinion so I would leave it like this
1308155008,14182,rreddy-22,2023-08-29T03:30:00Z,its a final attribute that why can't assign it twice and I feel like what's the point of initializing it before hand if I'm doing it in an if else block later which guarantees initialization
1308164291,14182,rreddy-22,2023-08-29T03:52:16Z,I don't think I understand the question
1308165291,14182,rreddy-22,2023-08-29T03:54:34Z,"It's not haha I would've used that otherwise, this has partition as an integer whereas that has TopicPartition (another class) as the attribute in addition to topic Id"
1308165924,14182,rreddy-22,2023-08-29T03:55:52Z,yes because we decided how we want to handle the non existent topic case in this PR and hence this is just a side effect of that
1308166084,14182,rreddy-22,2023-08-29T03:56:13Z,"this just keeps happening idky, ignore this file we have to force push it in the end"
1308167128,14182,rreddy-22,2023-08-29T03:58:46Z,"I feel like the code pattern has always been to explain what the assignor does at the top instead of at assign/build, and since this is basically the main function of the assignor, it shouldn't make a difference if its at the top explaining what the Optimized Uniform Assignment Builder does"
1308167658,14182,rreddy-22,2023-08-29T03:59:51Z,"since we mentioned partition's up front in the sentence it makes sense that from then onwards ""it"" always refers to the partition"
1308169146,14182,rreddy-22,2023-08-29T04:03:11Z,makes sense
1308171085,14182,rreddy-22,2023-08-29T04:07:48Z,All is mentioned to emphasize that it's all of the members assigned sticky partitions
1308172074,14182,rreddy-22,2023-08-29T04:10:08Z,it actually means that the topic is no longer subscribed to by the group but yes that also translates into not being in the topic metadata sometimes
1308173102,14182,rreddy-22,2023-08-29T04:12:34Z,why will it be zero?
1308173397,14182,rreddy-22,2023-08-29T04:13:20Z,"I feel like its very hard to understand without this comment, just for the sake of anyone trying to understand why I used that index for the extra partition I would leave it in"
1308174183,14182,rreddy-22,2023-08-29T04:15:04Z,replied before
1308174694,14182,rreddy-22,2023-08-29T04:16:12Z,"all means every single topic right, if it was just sorted topics, which/whose topics?"
1308177084,14182,rreddy-22,2023-08-29T04:21:48Z,It makes sure that all the unassigned partitions are present topic wise and then when we do a rr distribution while assigning these partitions they are distributed more evenly by topic as well
1308179490,14182,rreddy-22,2023-08-29T04:26:47Z,total again emphasizes that I'm talking about all the unassigned partitions and all the remaining assignments right? and also since it returns a boolean value doesn't is make sense still
1308181709,14182,rreddy-22,2023-08-29T04:31:34Z,"I don't want the loop to run until the roundRobinMembers list is empty though, this could cause an infinite loop. I just want it to go through every member at the very least and then if it doesn't find a member that is suitable then we just move on and the partition might get assigned later"
1309478048,14182,jeffkbkim,2023-08-30T00:36:34Z,you can't change what's stored in an empty map but you can change which map the variable points to. though it seems fine in this case since we are using streams api and need a final variable
1309478864,14182,jeffkbkim,2023-08-30T00:38:35Z,space after `this.`
1309479632,14182,jeffkbkim,2023-08-30T00:40:16Z,this can get NPE if the members is empty right?
1309481820,14182,jeffkbkim,2023-08-30T00:45:47Z,that seems like a big assumption; let's at least add something to the javadocs
1309482536,14182,jeffkbkim,2023-08-30T00:47:07Z,can you remind me again why we need to do at least n number of iterations?
1309482922,14182,jeffkbkim,2023-08-30T00:48:08Z,Ok
1309487439,14182,jeffkbkim,2023-08-30T00:57:49Z,"on 
> available for each partition in a rack-aware manner.

i don't see which part is ""in a rack-aware manner"" in the code.

> test case
can we test that the sorting happens as expected"
1309488516,14182,jeffkbkim,2023-08-30T01:00:25Z,"ah i see. i'm still leaning towards reusing that one, but will leave it up for david to decide"
1309488929,14182,jeffkbkim,2023-08-30T01:01:19Z,we should probably fix this..
1309489359,14182,jeffkbkim,2023-08-30T01:02:29Z,we have the step by step details at `assign()` for the range assignor right
1309492110,14182,jeffkbkim,2023-08-30T01:06:29Z,"i meant if currentAssignmentSize is 0, then retainedPartitionsCount will be 0 and hence we won't iterate anyways"
1309493644,14182,jeffkbkim,2023-08-30T01:08:40Z,"not sure why i left this comment, can disregard"
1309498804,14182,jeffkbkim,2023-08-30T01:16:13Z,"> all means every single topic right, if it was just sorted topics, which/whose topics?
that would make sense if we are using collections that form a subset of all topics somewhere in the code. otherwise it seems redundant

i'm not sure i follow. even if the topics weren't sorted, we would still ensure unassigned partitions are present, no? what makes them distribute more evenly?"
1309500746,14182,jeffkbkim,2023-08-30T01:19:20Z,"""is __ equals __"" is not grammatically correct. if we want to, we can do `is __ equal to __` but i felt that was too long.

do we ever have a case where we handle a subset of unassigned partitions? even the variable is called unassignedPartitions"
1309503692,14182,jeffkbkim,2023-08-30T01:23:50Z,"the while loop will still iterate through all members at the very least right? since we poll one every iteration

can you help me understand why the while loop can be infinite whereas the for loop cannot?"
1309504407,14182,jeffkbkim,2023-08-30T01:24:47Z,can we add a test case?
1309505394,14182,jeffkbkim,2023-08-30T01:26:18Z,seems like you've changed this to a set. is the comment above still valid?
1310726962,14182,jeffkbkim,2023-08-30T19:28:56Z,`if (unfilledMembers.containsKey(memberId)`
1310733452,14182,jeffkbkim,2023-08-30T19:35:51Z,"there are 2 cases i'm concerned about:
1. i == unfilledMembers.size() but roundRobinMembers is not empty. This means we still have members to distribute
2. roundRobinMembers.size() < unfilledMembers.size(): we'll get NPE.

These 2 cases may not happen in practice, but the code makes it seem they might. This makes it hard to read and reason about."
1310787109,14182,jeffkbkim,2023-08-30T20:30:35Z,"```
        computedGroupAssignment.members().forEach((memberId, memberAssignment) -> {
            assertEquals(expectedAssignment.get(memberId), memberAssignment.targetPartitions());
        });
```"
1310797271,14182,jeffkbkim,2023-08-30T20:41:10Z,nit: (member -> ... 
1310803310,14182,jeffkbkim,2023-08-30T20:47:16Z,"this looks like overkill and i don't think this works. For instance, if `totalAssignmentSizesOfAllMembers.get(i)` keeps increasing by 1, we can have first member start off with 1 then the last member end with 100 if monotonically increasing. 

can we do a single iteration and track the maximum and minimum size? then we can do `assertTrue((max - min) <= 1)` right

for ""partition assigned at most one member"", we can have a Map<TopicIdPartition, number of members that were assigned the partition> and increment it. then after looping we just check whether the number of topic id partitions equals number of members and that each value is 1

I also wonder if we can add this check to UniformAssignor, and run this at the end of `buildAssignment()`. Then the general assignor can also use this to see if it needs to do another iteration. We will need to add test cases for this"
1310837682,14182,jeffkbkim,2023-08-30T21:19:41Z,"to confirm my understanding:

1. we assign t1p1 and t2p1 to memberB since they have rack info
2. roundrobin the rest (unassignedPartitions, starting with t1p0)

is this correct?"
1310840406,14182,jeffkbkim,2023-08-30T21:22:35Z,"should we also check rack awareness? (if both partition and member rack exists, check they match)"
1310843247,14182,jeffkbkim,2023-08-30T21:25:49Z,we should assert computed assignment here
1310843639,14182,rreddy-22,2023-08-30T21:26:13Z,"cause we want to iterate through/visit every member at least once, n is the number of members."
1310845118,14182,jeffkbkim,2023-08-30T21:27:42Z,"i think ""n Members m Topics"" is understandable and simple. i've noticed other tests follow ""n Members subscribed to m Topics"". Can we unify the format?"
1310846978,14182,rreddy-22,2023-08-30T21:29:45Z,"its just different formatting and every time I push any changes this file gets modified, I'll change it directly in the end"
1310868257,14182,jeffkbkim,2023-08-30T21:51:30Z,"is memberA assigned t1p0 and t2p0 because there is a replica for t1p0 and t2p0 that have ""rack1""? (from mkMapOfPartitionRacks)

i'm just wondering what the assignment would be if we actually did a successful first assignment where A was in rack1 and B was in rack2."
1310869571,14182,jeffkbkim,2023-08-30T21:53:06Z,"t1p0 and t2p0 technically have racks in ""rack1"" right? so they should be both assigned to member B but already met its quota?"
1310880384,14182,jeffkbkim,2023-08-30T22:04:57Z,"no, we could have removed members from unfilledMembers in the if block above. and if we do so, we would also remove them from roundrobinMembers.

Since they start with the same members, they will end up with the same members"
1310892402,14182,rreddy-22,2023-08-30T22:20:42Z,"yeah that's true sorry idr when I changed it, I'll move it
"
1310897847,14182,rreddy-22,2023-08-30T22:28:40Z,But this is for when currentAssignmentSize is non zero
1310903635,14182,rreddy-22,2023-08-30T22:36:50Z,"Distribute evenly topic wise as well, as in if I distributed partitions in random order its possible that all the partitions from a single topic go to one member only, if I rr a list that goes topic wise I'm distributing each topics partitions evenly"
1310906983,14182,rreddy-22,2023-08-30T22:41:32Z,"got it, changed it"
1310919719,14182,rreddy-22,2023-08-30T22:55:28Z,"because we keep adding members back to the round robin queue until the remaining partitions to be received by the member hits 0. Therefore, the roundRobinMembers queue will never be empty unless all the partitions are assigned/ all the quotas are met. "
1310920650,14182,rreddy-22,2023-08-30T22:56:41Z,Here we might assign all the partitions that have matching racks but there's gonna be other partitions that haven't been assigned yet and therefore the remaining number of partitions to be received by members may not have reached zero yet. 
1310923371,14182,rreddy-22,2023-08-30T23:00:26Z,can members be empty?
1310930694,14182,rreddy-22,2023-08-30T23:11:59Z,The efficiency is the same right? This shows that remaining is greater than zero. I also feel like its safer just incase we have any members with negative or 0 value as remaining
1310941152,14182,rreddy-22,2023-08-30T23:28:53Z,its 300 partitions and 50 members xD
1310948792,14182,rreddy-22,2023-08-30T23:40:20Z,cools I'll double check
1310952891,14182,rreddy-22,2023-08-30T23:49:29Z,"yes that's correct, A is rack1 and p0 belongs to rack1
"
1310961579,14182,rreddy-22,2023-08-31T00:03:21Z,"I think I might have to redo this example, thanks for the catch!"
1312351774,14182,jeffkbkim,2023-08-31T22:33:16Z,"if there's no pattern, can we change the name to testValidityAndBalanceForLargeSampleSet?"
1312392393,14182,rreddy-22,2023-08-31T23:49:36Z,"The test is correct if we let go of the fact that the current assignment wasn't done by this assignor, but with whatever information the assignor got, the behavior was as expected"
1312392493,14182,rreddy-22,2023-08-31T23:49:50Z,Changed the current assignment to what this assignor would've returned
1312392605,14182,rreddy-22,2023-08-31T23:50:05Z,explained offline
1312609650,14182,rreddy-22,2023-09-01T06:07:33Z,"Well since we check the exact assignment for each member, when I was writing the exact partitions I made sure that the racks are matching so I feel like that is checked there already. To write a check again we would have to do quite some steps since its not guaranteed that every partition assigned to a member has a matching rack even if both partition and member racks exist"
1312610747,14182,rreddy-22,2023-09-01T06:09:10Z,yes
1312612036,14182,rreddy-22,2023-09-01T06:11:09Z,"1) This part specifically was taken from the client assignor so we know it works.
2) This can't work for the general assignor cause the definition of balance will not remain the same. In general assignor the min and max could have a larger difference and it would still be the most balanced possible"
1312613499,14182,rreddy-22,2023-09-01T06:13:28Z,The total assignment size can't keep increasing by 1 ever xD Total partitions divided by total members is how many they'll each have and at most some of them will get 1 extra partition
1312619017,14182,rreddy-22,2023-09-01T06:22:01Z,"numMembersByPartition is a map that's part of the rack info which tracks how many members are in the same rack as the partition. The sum of those members is used to sort them right, that's the ""rack aware manner"". Sorting happens as expected since I put in print statements to verify them but also without that the assignments wouldn't be as expected and we do a 1:1 check on every assignment in the test cases"
1312623868,14182,rreddy-22,2023-09-01T06:29:08Z,"1) It doesn't matter if round robin members is not empty, we're doing this by partition so it's very possible that after iterating through the list many members are yet to receive partitions. The only time round robin members will be empty/there's no more members that need partitions is for the last partition."
1312624721,14182,rreddy-22,2023-09-01T06:30:23Z,"Unless round robin members is empty how will we get a NPE, we would keep polling and adding it back right even if its just one member"
1312625110,14182,rreddy-22,2023-09-01T06:30:53Z,anyways I changed it to round robin members size just like the rack aware round robin
1312625675,14182,rreddy-22,2023-09-01T06:31:40Z,It is in the javadoc
1312625983,14182,rreddy-22,2023-09-01T06:32:03Z,"     * If the member has met their allocation quota, the member is removed from the
     * tracking map of members with their remaining allocations.
     * Otherwise, the count of remaining partitions that can be assigned to the member is updated."
1313213976,14182,jeffkbkim,2023-09-01T15:59:03Z,can we just use subscriptionSet instead of subscriptionList? it doesn't seem like we need a list to do any of the other operations.
1313281824,14182,jeffkbkim,2023-09-01T16:50:11Z,"the `&& !assigned` part would prevent the infinite loop, no?"
1313283639,14182,jeffkbkim,2023-09-01T16:52:33Z,"no, unless we have a bug. can we add a check in assign() and throw illegal argument exception if members is empty?"
1313292815,14182,jeffkbkim,2023-09-01T17:01:15Z,that makes sense
1313294383,14182,jeffkbkim,2023-09-01T17:02:37Z,"thanks, that makes sense"
1313393423,14182,jeffkbkim,2023-09-01T18:41:47Z,discussed offline. makes sense
1313408646,14182,jeffkbkim,2023-09-01T19:02:23Z,this can be resolved
1313434102,14182,jeffkbkim,2023-09-01T19:33:18Z,"that makes sense -- i forgot we're checking each member. still, i think it's possible to check the balancedness without a nested for loop. not sure if that affects the validity part"
1313436894,14182,jeffkbkim,2023-09-01T19:36:10Z,nit: `forEach(partition -> {`
1313437179,14182,jeffkbkim,2023-09-01T19:36:28Z,nit: forEach(partition ->
1313439928,14182,jeffkbkim,2023-09-01T19:39:17Z,nit: javadocs
1313441197,14182,jeffkbkim,2023-09-01T19:40:29Z,can we put the arguments into new lines?
1313442250,14182,jeffkbkim,2023-09-01T19:41:29Z,"nit: 
```
            Map<Uuid, Set<Integer>> computedAssignmentForMember = computedGroupAssignment
                .members().get(member).targetPartitions();

```"
1313442609,14182,jeffkbkim,2023-09-01T19:41:50Z,"nit: 
```
            Map<Uuid, Set<Integer>> computedAssignmentForMember = computedGroupAssignment
                .members().get(memberId).targetPartitions();
            
```"
1313443677,14182,jeffkbkim,2023-09-01T19:43:00Z,"indentation looks off.

```
                assertTrue(members.get(memberId).subscribedTopicIds().contains(topicId),
                    ""Error: Partitions for topic "" + topicId + "" are assigned to member "" + memberId +
                    "" but it is not part of the members subscription "");
```"
1313444165,14182,jeffkbkim,2023-09-01T19:43:30Z,"nit:
```
                Map<Uuid, Set<Integer>> computedAssignmentForOtherMember = computedGroupAssignment
                    .members().get(otherMemberId).targetPartitions();
                
```"
1313444928,14182,jeffkbkim,2023-09-01T19:44:18Z,"indentation looks off.
```
                    assertTrue(intersection.isEmpty(), ""Error : Member 1 "" + memberId + "" and Member 2 "" + otherMemberId +
                        ""have common partitions assigned to them "" + computedAssignmentForOtherMember.get(topicId));
```"
1313445794,14182,jeffkbkim,2023-09-01T19:45:13Z,"let's split this
```
                assertTrue(Math.abs(size1 - size2) <= 1,
                    ""Size of one assignment is greater than the other assignment by more than one partition "" +
                    size1 + "" "" + size2 + ""abs = "" + Math.abs(size1 - size2));
```"
1313448367,14182,jeffkbkim,2023-09-01T19:47:52Z,"makes sense, let's leave it as is"
1315178473,14182,rreddy-22,2023-09-04T19:46:52Z,"as discussed on call, I added a check in the assign method instead"
1315178560,14182,rreddy-22,2023-09-04T19:47:07Z,done
1315178781,14182,rreddy-22,2023-09-04T19:47:52Z,added check but I don't think we need to throw an illegal state exception
1315178928,14182,rreddy-22,2023-09-04T19:48:32Z,we could but since we're checking for validity anyways we can leave it
1315500509,14182,dajac,2023-09-05T07:41:53Z,nit: `log` -> `LOG` as this is a constant.
1315500739,14182,dajac,2023-09-05T07:42:04Z,nit: We can remove this empty line.
1315501863,14182,dajac,2023-09-05T07:42:59Z,nit: `log` -> `LOG`.
1315502677,14182,dajac,2023-09-05T07:43:41Z,Could we please add javadoc to all the attributes? We have been doing this for all the new code in this module so we should continue.
1315507070,14182,dajac,2023-09-05T07:47:13Z,nit: Do we need to define `useRackAwareStrategy` as an attribute if we can get it from `rackInfo.useRackStrategy`? We could perhaps have a method instead.
1315508408,14182,dajac,2023-09-05T07:48:17Z,nit: There is an space between the `*`.
1315509745,14182,dajac,2023-09-05T07:49:20Z,I suppose that we assume that we cannot get here if members is empty. Is this correct?
1315511582,14182,dajac,2023-09-05T07:50:52Z,I think that we should directly throw the IllegalStateException. It is a bit weird to construct it to wrap it in another one right away.
1315512642,14182,dajac,2023-09-05T07:51:49Z,This comment is incorrect as the implementation fails if the topic does not exist.
1315513303,14182,dajac,2023-09-05T07:52:22Z,Could this be final as well?
1315529530,14182,dajac,2023-09-05T08:04:38Z,nit: `log` -> `LOG`.
1315531958,14182,dajac,2023-09-05T08:06:51Z,Is using an HashSet necessary here? We could perhaps use `firstSubscriptionSet.containsAll(memberSpec.subscribedTopicIds())`. Would it work?
1315558356,14182,dajac,2023-09-05T08:28:24Z,"Is there a reason to have this abstract class here? Personally, I find it a bit weird as the concrete classes are not defined in this class. How about extracting it?"
1315560818,14182,dajac,2023-09-05T08:30:26Z,nit: javadoc.
1315804029,14182,dajac,2023-09-05T12:09:33Z,nit: Could this method be static?
1315804175,14182,dajac,2023-09-05T12:09:42Z,ditto.
1315805497,14182,dajac,2023-09-05T12:10:55Z,nit: Do we really need to create the ArrayList here?
1315808858,14182,dajac,2023-09-05T12:14:13Z,We already have the very same class in the `metadata` module. I wonder if we should move that one to `server-common` module so that we could reuse it here. It could land in the `common` package over there. What do you think?
1315811048,14182,dajac,2023-09-05T12:16:15Z,nit: Could we keep the previous code and use `getOrDefault` instead of `get`? The code would be more concise...
1315814176,14182,dajac,2023-09-05T12:19:10Z,nit: We should use `int` here.
1315815939,14182,dajac,2023-09-05T12:20:48Z,nit: This code is duplicated. Should we have an helper method for it?
1315831564,14182,dajac,2023-09-05T12:34:20Z,nit: We can remove `</p>`.
1315836673,14182,dajac,2023-09-05T12:38:40Z,Shouldn't we keep what we had here?
1315836903,14182,dajac,2023-09-05T12:38:52Z,nit: `log` -> `LOG`.
1316096433,14182,rreddy-22,2023-09-05T15:50:54Z,Yep I'll raise a different PR for range assignor changes
1316390836,14182,rreddy-22,2023-09-05T20:47:41Z,"Its not the exact same, that one has topicId mapped to topicPartition here its just the partition number"
1316396258,14182,rreddy-22,2023-09-05T20:54:00Z,removing this
1316406386,14182,rreddy-22,2023-09-05T21:05:56Z,"ack will remove it but we can ignore this file for now since the implementation isn't in yet, just needed the initial template for now to get the conditional implementation of the specific assignment builder based on the subscriptions."
1316408646,14182,rreddy-22,2023-09-05T21:08:55Z,yeah correct I added a check in the assign method so its not possible anymore
1316429235,14182,rreddy-22,2023-09-05T21:30:10Z,done
1316439651,14182,rreddy-22,2023-09-05T21:40:09Z,"yep makes sense I removed it, I thought in the future if we wanted to have a flag in the assignor to switch it on or off"
1316456003,14182,rreddy-22,2023-09-05T21:55:51Z,I wanted to throw a partition assignor exception since the exception is related to the assignor? can we do that only? Or do we have to do just an illegal state exception?
1316792230,14182,rreddy-22,2023-09-06T06:24:50Z,Which concrete classes? We made it abstract so that any builder that implements this class would need to implement buildAssignment which is the main function called within assign
1316793219,14182,rreddy-22,2023-09-06T06:26:04Z,I believe it's easier to understand since allTopicIdPartitions is a method that returns this list which is used later on no?
1316806195,14182,rreddy-22,2023-09-06T06:41:06Z,will do
1316955014,14182,dajac,2023-09-06T08:52:10Z,GeneralUniformAssignmentBuilder and OptimizedUniformAssignmentBuilder. How about extracting AbstractAssignmentBuilder from UniformAssignor and calling it AbstractUniformAssignmentBuilder?
1316956874,14182,dajac,2023-09-06T08:53:30Z,I was referring to `new ArrayList<>(topicIds)`. Is creating a new ArrayList from the Set necessary? allTopicIdPartitions could take a Collection<Uuid> for instance to avoid it.
1316961213,14182,dajac,2023-09-06T08:56:03Z,I was referring to https://github.com/apache/kafka/blob/trunk/metadata/src/main/java/org/apache/kafka/controller/TopicIdPartition.java. It is really bad to have two classes with the same name but slightly different. I wonder if we can do something about it...
1317663852,14182,rreddy-22,2023-09-06T18:17:32Z,"ohh got it yeah that makes sense
"
1317669213,14182,rreddy-22,2023-09-06T18:23:27Z,"Ohh my bad I got this question before so I instinctively replied, I realized I was thinking of the kafka common topicIdPartition class but yeah this seems usable, would we need a new PR to move the file though?"
1317676113,14182,rreddy-22,2023-09-06T18:30:53Z,"Discussed offline, going to throw only PartitionAssignorException"
1319775413,14182,dajac,2023-09-08T11:53:49Z,nit: We usually add an empty line before the javadoc for attributes.
1319776220,14182,dajac,2023-09-08T11:54:38Z,nit: Should we log at debug level here? 
1319779160,14182,dajac,2023-09-08T11:58:00Z,nit: Empty line can be removed.
1319810543,14182,dajac,2023-09-08T12:31:31Z,nit: int?
1319811185,14182,dajac,2023-09-08T12:32:12Z,Could we elaborate a bit more on why the sorting is important here?
1319812206,14182,dajac,2023-09-08T12:33:18Z,I wonder if we could reuse the linked list that we already used in rackAwareRoundRobinAssignment instead of re-creating a new one from scratch. Is there a reason why we need it?
1319812750,14182,dajac,2023-09-08T12:33:54Z,nit: Should this comment be right before `if (unfilledMembers.containsKey(memberId)) {`?
1319814872,14182,dajac,2023-09-08T12:36:02Z,"Is this really necessary? It is a tad annoying to have to compute the sum of all the unfilled members. If you really want to do this, we could perhaps maintain the total count as we update the unfilled members but I am not sure if it is worth it."
1319816047,14182,dajac,2023-09-08T12:36:57Z,I was wondering whether we could avoid the copy and just update `potentiallyUnfilledMembers`. Have you thought about this?
1319820875,14182,dajac,2023-09-08T12:41:33Z,"Yeah, we can have a separate PR to move that class (and add some javadoc to it). I am actually annoyed by the fact that we have two TopicIdPartition classes but with different content. One with (id, name, partition) and one with (id, partition). This is really misleading..."
1319824362,14182,dajac,2023-09-08T12:45:18Z,nit: Could we just keep everything on one line?
1319824635,14182,dajac,2023-09-08T12:45:35Z,Still there :)
1320217221,14182,rreddy-22,2023-09-08T18:39:21Z,"I added it as a check to ensure we don't have more partitions to assign and less total required assignments count, i.e. jic our calculations are wrong. Its not completely necessary"
1320220040,14182,rreddy-22,2023-09-08T18:42:10Z,"In the comments? Or should I explain it here
"
1320220933,14182,rreddy-22,2023-09-08T18:42:58Z,"We sort so that the partition with the least number of options for a matching rack member can receive the assignment first.
"
1321023603,14182,rreddy-22,2023-09-11T05:54:15Z,added in the javadoc
1321023849,14182,rreddy-22,2023-09-11T05:54:42Z,"rackAware round robin might not be called in every case, it is only called when useRackAware strategy is true. I could make it a global attribute maybe but that seems odd since the queue is only used in these two methods."
1321024671,14182,rreddy-22,2023-09-11T05:56:05Z,I remember getting comments to keep the comments at the beginning of the loop instead of inside
1321033941,14182,rreddy-22,2023-09-11T06:08:17Z,"Yeah you're right, it's an unnecessary extra map, I made it a local map, just wanna still keep it separate cause potentially unfilled members also has ones that have met the quota, but unfilled only has those that still need to be assigned partitions, might be hard to understand what types of members we have at each point. Lmk if what I did makes sense or if we can optimize it more"
1321035644,14182,rreddy-22,2023-09-11T06:10:51Z,"yeah that's true I agree, I wasn't sure what else to call this one, okay I will add a new PR for this."
1325561675,14182,dajac,2023-09-14T08:17:15Z,"small nit: In such case, I usually prefer to put the mutated object first in the list of arguments. Then, I would also put memberId, then topicId and finally partition to follow their hierarchy. I would also rename targetAssignment to assignment because the method is not tight to a target assignment in the end. It could be any assignment map."
1325563579,14182,dajac,2023-09-14T08:18:43Z,"bit: This parenthesis seems misplaced, no?"
1325598891,14182,dajac,2023-09-14T08:44:47Z,"I see... It is a bit annoying to copy the map here but I can live with it if you think that it is better like this. I have another question regarding this code. My understanding is that we basically decide here which members will get the extra partitions. Basically, the first members while iterating over the map will get them. When the rack awareness is enabled, I wonder if we could get in a situation where the members with the extra partitions and the unassigned partitions are completely misaligned to due this.

Let's take an example. We have a group with 10 members (from 1 to 10) subscribed to topic A. Topic A has 20 partitions so each member has 2 assigned partitions. All the partitions are available in a single rack. Members 1 to 5 are in rack R1 and members 6 to 10 in rack R2. Now let's say that we add 5 partitions to A. 3 are in R1 and 2 in R2. If we iterate from 1 to 10 to assign the extra partitions, it means that 1 to 5 will get an extra partitions even though two of the partitions are not in the same rack. Is this a possible scenario? It is perhaps a bit too extreme..."
1325600664,14182,dajac,2023-09-14T08:46:06Z,How about creating the queue just before calling rackAwareRoundRobinAssignment and unassignedPartitionsRoundRobinAssignment and passing it as a parameter?
1325602326,14182,dajac,2023-09-14T08:47:19Z,"Ah ah. You got me :). I actually raised this because in rackAwareRoundRobinAssignment, your put it right before the if. That also works because the comment is really about that if."
1325604357,14182,dajac,2023-09-14T08:48:52Z,nit: Let's log before creating the builder to be consistent with the other branch.
1325633887,14182,dajac,2023-09-14T09:07:37Z,nit: We have the same method in RangeAssignorTest. Could we somehow share it for the two suites? We could perhaps introduce an AssignorTestUtil class in this package and add it there.
1326815225,14182,rreddy-22,2023-09-15T05:42:57Z,yessir done :)
1326817433,14182,rreddy-22,2023-09-15T05:46:40Z,I wasn't sure where it went tbh xD
1326818424,14182,rreddy-22,2023-09-15T05:48:25Z,"Makes sense, I put the order based on the method name like add partition to assignment so first partition then which topic then whose assignment and then which assignment"
1326819932,14182,rreddy-22,2023-09-15T05:50:52Z,"I changed it now so we don't need the queue at all
"
1326826466,14182,rreddy-22,2023-09-15T05:56:06Z,"yeah that's true I didn't think of how we're assigning the extra partitions, used the same logic as before, let me look into it. Thanks for catching this!"
1326838235,14182,rreddy-22,2023-09-15T06:06:32Z,"I think one option would be to iterate through potentially unfilled members instead during the round robin process and if the member is a potential rack match plus can get an extra partition then we can assign it. We basically dynamically decide who gets the extra, instead of deciding before hand"
1326871177,14182,dajac,2023-09-15T06:48:03Z,"Yeah, I was thinking about more or less the same. Is it an small change?"
1326872198,14182,dajac,2023-09-15T06:49:18Z,"In this case, I would put it back on the previous line."
1332072827,14182,rreddy-22,2023-09-20T19:15:32Z,"Wasn't sadly but its done now
"
50180928,764,hachikuji,2016-01-19T21:48:28Z,"Typo in ""messge""?
"
50183536,764,hachikuji,2016-01-19T22:06:29Z,"Unneeded import (Message)?
"
50183902,764,hachikuji,2016-01-19T22:09:14Z,"Unneeded import (ErrorMapping)?
"
50186812,764,hachikuji,2016-01-19T22:32:18Z,"I wasn't clear from the KIP, but is this a broker-wide setting or can it be overridden for each topic?
"
50334745,764,apovzner,2016-01-20T22:57:35Z,"It would be useful to have a comment describing what this method does. Especially because in one ""invalid"" case it throws exception, and in another case (when we need to overwrite timestamp) it returns true/false. 
"
50335106,764,apovzner,2016-01-20T23:00:58Z,"I am wondering if it would be better to pass 'now' as a parameter. We are calling this method for each message in a set, and getting current time every time. Normally getting system time is an expensive call, so maybe better to get it once for a message set, and pass it to this method?
"
50338001,764,apovzner,2016-01-20T23:27:22Z,"I see that in other places you did ""if (magicValue > MagicValue_V0)"" comparison -- I think that one is better, since we would still want timestamp if we have MagicValue_V2 in the future, for example.
"
50367205,764,becketqin,2016-01-21T07:16:24Z,"@hachikuji I was also thinking about that. Currently whatever configurations in LogConfig are per topic configurations. And the message timestamp type is a legitimate log config. So currently it is a per topic configuration. I can see some benefit of doing so from migration point of view. Because most topics are owned by some applications. We can start to use the new format once all the client of that topic has migrated. And in the final state, we can choose to leave the topics whose owner are not able to migrate to use old format and still have zero-copy.
"
50367268,764,becketqin,2016-01-21T07:17:55Z,"@apovzner Good catch :) I remember I had this somewhere but did not find it before submit the PR.
"
50442463,764,hachikuji,2016-01-21T18:36:26Z,"Thanks for the explanation. Makes sense to me. By the way, I've only done a quick pass on this patch so far, but I'm planning to spend a bit more time in the next couple days.
"
50479362,764,apovzner,2016-01-21T23:04:46Z,"Since we are adding timestamp field to ProducerRecord, I think we should add a comment to ProducerRecord class description about meaning of timestamp, what happens if user sets null, etc.
"
50479481,764,apovzner,2016-01-21T23:06:04Z,"Also would be good to add a comment to RecordMetadata class description what timestamp is actually returned (either set by client, producer, or broker).
"
50481259,764,apovzner,2016-01-21T23:23:53Z,"I understand that producer learns about the type of the timestamp after it gets first successful produce response. We are using the type of the timestamp in KafkaProducer.send() to set timestamp in ProducerRecord. I see that we default to handling timestamp as CreateTime timestamp if we don't know the type yet. What is the impact of doing this if the correct type turns out to be LogAppendTIme?  Why don't we just always set timestamp on Producer as if is CreateTime (basically, set local producer time if timestamp in ProducerRecord is null), and broker then overwrites it if the timestamp type is LogAppendTime. Otherwise, looks like lots of added complexity just to decide whether set timestamp on the producer or not.
"
50484068,764,apovzner,2016-01-21T23:52:00Z,"Related to my other comment about learning about timestamp type for topic. So, the first set of produce messages will not have timestamp == INHERITED_TIMESTAMP if timestamp type == LogAppendTime, right? If setting timestamp to INHERITED_TIMESTAMP  is required for compressed messages to work, does it mean we have a bug?
"
50501215,764,becketqin,2016-01-22T04:29:28Z,"The reason we want to set the timestamp to -1 in producer when LogAppendTime is used for the topic is to avoid broker side recompression. If producer send CreateTime to a topic setup for LogAppendTime, recompression will occur on the broker if the received message timestamp is not -1. Avoid recompression on broker is the key motivation of KIP-31 so we don't want people to lose this feature if they are using LogAppendTime.
"
50501355,764,becketqin,2016-01-22T04:34:01Z,"It is not required to be INHERITED_TIMESTAMP, but it is good to be so. 

Like I answered in your other comment, if a topic is using LogAppendTime and a broker receives a message whose timestamp is not INHERITED_TIMESTAMP, it will overwrite it and do the recompression. So the first batch of a new producer might cause recompression on broker side, but after that, no recompression should be needed. I will add some comments so it is more clear.
"
50597218,764,apovzner,2016-01-22T22:03:34Z,"I agree about avoiding broker side recompression. However, I still feel like we can achieve the same behavior with less changes. Let me know if I am missing something, but couldn't we just do the following:
1. Producer always sets timestamps (either producer client or KafkaProducer), as if timestamp type == CreateTime. For compressed message: timestamp of the outer message is set to the largest timestamp of the inner messages.
2. Broker will overwrite timestamps if type == LogAppendTime. In case of compressed message, it will overwrite only outer message timestamp, and let inner messages have ""wrong"" timestamps. If message ever gets uncompressed on broker: set all inner timestamps to outer timestamp if type == LogAppendTime;
3. When messages get uncompressed on consumer (or any case when we don't have timestamp type info): We know that outer timestamp is either max of inner timestamps (if create time) or outer timestamp was overwritten (and so inner timestamps are not valid anymore). We check if outer (compressed message) timestamp == max of inner message timestamps, and if false, set all inner message timestamps to outer timestamp. If there is ever a case that overwriting timestamp results in a timestamp == max of inner message timestamps, this means that producer and broker times are in sync, and inner message timestamps should be close enough to outer timestamp to care overwriting it.
"
50599954,764,becketqin,2016-01-22T22:29:40Z,"In your suggestion, how can we differentiate between the following two scenarios:
1. LogAppendTime is used, the inner message's largest timestamp happened to be the same as the LogAppendTime. 
2. CreateTime is used.

The compressed messages in both case are exactly the same, but one is using LogAppendTime and the other one is using CreateTime. How would the consumer decide which timestamp to use?

Please also notice that we always do decompression on broker side to verify the message. What we want to avoid is re-compression.
"
50621173,764,dajac,2016-01-23T15:25:29Z,"Wouldn't it be better to put this conversion in KafkaApis or directly in MessageSet? I would prefer to keep FetchResponse and FetchResponsePartitionData as simple as possible and focused on the serialization.
"
50621383,764,dajac,2016-01-23T15:40:43Z,"Nitpick: indentation is not correct.
"
50645225,764,becketqin,2016-01-24T23:42:32Z,"I agree it makes sense to put the message set format conversion code block into MessageSet instead of here.

However, I make the IO threads to do the conversion on purpose because I want to share the workload between KafkaApis threads and IO threads. Typically, IO threads are more light-weighted than KafkaApis threads. If we have to make conversion for some requests, I am trying to put the conversion load on IO threads.
"
50743872,764,apovzner,2016-01-25T19:35:46Z,"I should have said ""If message ever gets re-compressed on broker..."" in #2. So we don't over-write timestamp and re-compress just for timestamps.
To answer your question and clarify my suggestions, I am basing my suggestion on the assumption that we don't need to be very exact about timestamp -- meaning +- 1 ms is ok. So my proposal above is assuming that if your scenario happen and the inner message's largest timestamp happened to be the same as the current time on broker and we are using LogAppendTime, this means that producer and broker are in sync, and we don't care whether it is LogAppendTime or CreateTime. One more argument against: what if client sets ""bad"" timestamp in one of the inner messages. I realized that in your scenario my suggestion does not work well. I propose the following: On broker, when messages get decompressed, if timestamp type == LogApendTime, then set outer message timestamp == current time. Check if max timestamp of inner messages also contains this timestamp, and if so, decrement outer message timestamp by 1 ms. In this case, we know that we don't have the scenario above. On consumer, if outer message timestamp != max of inner messages timestamps, we know that this is LogAppendTime and set all inner message timestamps to outer message timestamp.

**Summary**
I think we either need a cleaner way to get topic's timestamp type on producer or simplify timestamp code by allowing timestamps be not super exact, but no more than +- 1 ms error.
The former I think is better solved by getting timestamp type with topic metadata, but that requires another wire protocol change.
The latter is proposed above, but here is updated version based on the scenario in the previous comment:
1. Producer always sets timestamps (either producer client or KafkaProducer), as if timestamp type == CreateTime. For compressed message: timestamp of the outer message is set to the largest timestamp of the inner messages.
2. Broker will overwrite timestamps if type == LogAppendTime. In case of compressed message: it will overwrite only outer message timestamp, and let inner messages have ""wrong"" timestamps. If broker happens to overwrite outer message timestamp with with same timestamp, it means that outer timestamp would be equal to max of inner message timestamps. To allow consumer to differentiate between overwritten and not overwritten timestamp, we want to void the case where outer message timestamp == max of inner message timestamps when type == LogAppendTime. So, when overwriting outer message timestamp with same timestamp, we will decrement (or increment) outer message timestamp by 1 ms.
3. When messages get uncompressed on consumer, we know that outer timestamp is either max of inner timestamps (if create time) or outer timestamp was overwritten (and so inner timestamps are not valid anymore). We check if outer (compressed message) timestamp == max of inner message timestamps, and if false, set all inner message timestamps to outer timestamp.
"
50764320,764,becketqin,2016-01-25T22:09:16Z,"@apovzner Personally I think the timestamp should be accurate. Modifying the timestamp sounds very hacky and creates extra complexity. Please also notice that the timestamp index built by the followers will be purely depending on the timestamp in outer message of compressed messages. The followers will not even decompress the messages. If we play the trick here, the time index on follower will also be affected. 

If we want to make things right, then producer should be able to get the necessary topic configuration info from broker, either from TopicMetadataRequest or some other requests. So the producer can set the timestamp correctly to avoid server side recompression. But like you said this is a bigger change and it is unnecessary to block on that change.

I think the current solution is reasonably clean as of the moment.
Once the producer is able to get the topic configuration from broker, we can simply migrate to use that. Since everything is purely internal, the migration is very simple and transparent to users.
"
51080192,764,apovzner,2016-01-28T04:59:25Z,"If we are exposing timestamp type in ConsumerRecord, should we declare TimestampType outside of Record?
"
51082542,764,becketqin,2016-01-28T05:50:14Z,"It is in KafkaProducer line 437. We just need a one liner now.
"
51165826,764,apovzner,2016-01-28T18:39:07Z,"The comment above the method does not match implementation anymore -- we are now only checking acceptable range for CreateTime timestamps.
"
51221001,764,junrao,2016-01-29T03:00:09Z,"It seems that we only reserved 3 bits for compression codec?
"
51221007,764,junrao,2016-01-29T03:00:14Z,"It seems that checking lastInnerOffset itself is enough.
"
51221024,764,junrao,2016-01-29T03:00:32Z,"Since this is always called on the inner records, we probably don't need shallow in the param?
"
51221029,764,junrao,2016-01-29T03:00:36Z,"Would it be better to name this lastInnerRelativeOffset?
"
51221037,764,junrao,2016-01-29T03:00:44Z,"Fetch response v2 is actually different from v1 since the message format is different.
"
51221069,764,junrao,2016-01-29T03:00:57Z,"Could we add timestamp to @param?
"
51221090,764,junrao,2016-01-29T03:01:13Z,"Is it useful for user to specify a -1 timestamp? Would that be the same as passing in a null timestamp?
"
51221091,764,junrao,2016-01-29T03:01:18Z,"The record is still sent to a topic/partition, not timestamp.
"
51221099,764,junrao,2016-01-29T03:01:23Z,"typo: record.Record
"
51221100,764,junrao,2016-01-29T03:01:25Z,"typo: record.Record
"
51235304,764,becketqin,2016-01-29T08:18:35Z,"Currently the message format change is not reflected in the fetch response protocol. The change is in the Record class when it parses the ByteBuffer. So the FetchResponse fields actually does not change. But I agree that ideally we should define all the wire protocols in Protocol. I was planning to do it in another patch because this patch is already big.

The comment here is actually not accurate. In FetchResponse V2 we may also see message format V0, because the broker will try to avoid losing zero copy by assuming the client sending FetchRequest V2 knows how to parse message format V0.
"
51301463,764,apovzner,2016-01-29T19:08:50Z,"Why do we ever need to recompute CRC for timestamp type == CreateTime? Since CreateTime is default, we should set all right attributes with the timestamp on the producer, and we don't need to update CRC or do any related changes to timestamp/attributes on the broker. I think we should be clear about when CRC can change on the broker and when it will not. 
"
51325920,764,becketqin,2016-01-29T22:37:12Z,"We need to verify both the timestamp attribute bit and the actual timestamp. If one of them is not set properly we need to update it and recompute CRC.
"
51326334,764,apovzner,2016-01-29T22:42:22Z,"I see, so this is only the migration case, right? Upgraded producer will set both attributes and timestamp correctly for the CreateTime topics, right?
"
51329584,764,becketqin,2016-01-29T23:21:45Z,"For migration case and for producers that is not setting the timestamp of outer message correctly somehow. It should be fine since we are not changing the actual timestamps of messages.
"
51502629,764,apovzner,2016-02-02T00:07:14Z,"Thanks, that makes sense.
"
51516461,764,junrao,2016-02-02T02:52:44Z,"Right, perhaps we can make this clearer in the comment. Sth like the following:

Even though fetch response v2 has the same protocol as  v1, the record set in the response is different. In v1, record set only includes messages of v0 (magic byte 0). In v2, record set can include messages of v0 and v1 (magic byte 0 and 1). For details, see ref{ByteBufferMessageSet}.
"
51516607,764,junrao,2016-02-02T02:54:29Z,"Would it be better to rename this to wrapperTimestampType? Also, since the way to interpret the timestamp is a bit subtle, especially with respect to compressed messages, could you document this in a comment?
"
51516616,764,junrao,2016-02-02T02:54:34Z,"The comment says this is the constructor for version 1, but the code uses the latest version.
"
51516635,764,junrao,2016-02-02T02:54:46Z,"We check version here, but uses the new field name in line 127 as the way to check the version. We should probably use a consistent approach.
"
51516646,764,junrao,2016-02-02T02:54:59Z,"Could we add a comment before line 95 that we expect the caller to pass in a struct with the latest schema?
"
51516833,764,junrao,2016-02-02T02:57:34Z,"In addition to have the internal versions, we probably should always have an ""0.10.0"" version in trunk so that it's in a releasable state. I was thinking that we always have ""0.10.0"" map to the last case object. In this case, both ""0.10.0"" and ""0.10.0-DV0"" will be pointing to KAFKA_0_10_0_DV0. When we add KAFKA_0_10_0_DV1, ""0.10.0"" will be pointing to KAFKA_0_10_0_DV1 and we will add ""0.10.0-DV1"", but leave ""0.10.0-DV0"" unchanged. In the code, we can just reference the first internal version in which a format change is introduced. Then, for people deploy from trunk, they can use the internal version. For people who want to try trunk, they can use ""0.10.0"".

Also, would it be better to use IV (internal version) instead of DV?

Finally, it may make sense to make the next major release 0.10 instead of 0.9.1 since we will be including KStream. Do you want to poll the mailing list to see if people are ok with this?
"
51516887,764,junrao,2016-02-02T02:58:28Z,"I agree that it's better to do the message conversion in KafkaApis. The reason is that a single network thread is used to handle many socket connections. If the sending of one response is slow (e.g., recompression when converting the message), it slows down the processing of all connections on this network thread. On the other hand, if the processing in a request handler thread is slow, it only slows down that request. Other requests are unaffected.
"
51516906,764,junrao,2016-02-02T02:58:52Z,"This means that we are paying the overhead of checking hasMagicValue even after the consumer is upgraded to support both message format v0 and v1.
"
51516922,764,junrao,2016-02-02T02:59:10Z,"Should this be a topic level config? The issue is that this has to match the Api version that the broker uses. For example, if the broker is on 0.9.x, setting the format in a topic to v1 is invalid, but is hard to enforce.
"
51516927,764,junrao,2016-02-02T02:59:13Z,"typo writtern
"
51516929,764,junrao,2016-02-02T02:59:16Z,"it simply write => it simply writes
"
51516941,764,junrao,2016-02-02T02:59:28Z,"Can use innerMessageAndOffsets instead of messageAndOffsets.
"
51516961,764,junrao,2016-02-02T02:59:49Z,"If we don't need to re-compress, should we validate the CRC of each of the inner message?
"
51516975,764,junrao,2016-02-02T03:00:03Z,"It seems that we only need to convert message format on FileMessageSet. So, perhaps it's better to move this to FileMessageSet?
"
51516977,764,junrao,2016-02-02T03:00:08Z,"There are a few unused imports.
"
51517004,764,junrao,2016-02-02T03:00:31Z,"This probably shouldn't be info level logging, right?
"
51517033,764,junrao,2016-02-02T03:00:56Z,"Could we document what the valid values are for message format? It seems that we are piggybacking on the ApiVersion number. It's probably clearer if just use v0 and v1 that matches the magic value since not every api version change implies a message format change.
"
51525824,764,becketqin,2016-02-02T05:28:21Z,"Read the code again. That's true. Apparently I had some wrong impression about how we process the requests...
"
51529150,764,becketqin,2016-02-02T06:30:05Z,"Good catch. We only need to do down convert for fetch request lower than V2.
"
51530067,764,becketqin,2016-02-02T06:38:39Z,"That is indeed a caveat. The reason I make message.format.version a topic level config is because it helps to roll out the change. In many cases topics are owned by different applications, so once the application finishes upgrade we can turn on the new message format for them without waiting for others.

One way to enforce this is to add sanity check in both KafkaConfig and TopicConfigHandler. We can check to make sure the message format version config is valid.
"
51531865,764,becketqin,2016-02-02T07:08:00Z,"@gwenshap had some comments on the KIP about this. I feel fine either way. 

The good thing about piggybacking ApiVersion is it is easy to validate the config. And from user's perspective it may be easier to understand. e.g. During upgrade they can simply put their previous Kafka version there and after upgrade they just need to change it to the upgraded version. So users don't need to remember the magic values to put.
"
51615655,764,junrao,2016-02-02T18:54:10Z,"It's probably clearer to rename this to sth like magicValueInAllMessages().
"
51615665,764,junrao,2016-02-02T18:54:14Z,"indentation
"
51615681,764,junrao,2016-02-02T18:54:20Z,"Could you attach Gwen's comment here?
"
51615706,764,junrao,2016-02-02T18:54:26Z,"Could we include the valid property values in the doc?
"
51615770,764,junrao,2016-02-02T18:54:47Z,"We should use the magic configured for this topic and convert each message to the right version if needed, right?
"
51615946,764,junrao,2016-02-02T18:55:53Z,"Could we put those comments in a more prominent place like the beginning of the class? With v1 message format, we are adding a timestamp, a timestamp type attribute, and are using a relative for inner message. It would be useful to document the format in a bit more details for both the outer and the inner message. For example, should the timestamp type attribute be set for inner messages?
"
51634795,764,becketqin,2016-02-02T21:05:21Z,"Gwen gave the following feedback on the voting thread.

""2. ApiVersion has real version numbers. message.format.version has sequence
numbers. This makes us look pretty silly :)""
"
51637287,764,becketqin,2016-02-02T21:23:12Z,"I was thinking only applying the new message format to the messages appended after the change, so the log has a clear cut-off offset where all the new format comes after that. It is probably not necessary. What do you think?
"
51672466,764,junrao,2016-02-03T02:51:10Z,"Could we add a comment to explain what the timestamp is?
"
51672513,764,junrao,2016-02-03T02:51:55Z,"The comment still says using v1 format.
"
51672525,764,junrao,2016-02-03T02:52:14Z,"It's probably useful to use the message format v1 on the topic. This can avoid the recompression overhead when compression is enabled.
"
51672530,764,junrao,2016-02-03T02:52:20Z,"It's probably useful to use the message format v1 on the topic. This can avoid the recompression overhead when compression is enabled.
"
51672563,764,junrao,2016-02-03T02:52:53Z,"The thing is that the way TopicConfigCommand works is that it just writes the config in ZK and then writes the config notification. TopicConfigHandler just follows the notification. If we add the check in TopicConfigHandler, we need to remove the config in ZK, which makes things more complicated.

We can probably just leave this as a topic level config with the caveat that the config may be ignored if the broker property doesn't match.
"
51672667,764,junrao,2016-02-03T02:54:32Z,"The changes in this class is a bit complicated, makes the code a bit hard to read. This and the following are some suggestions on simplification.

Do we need convertNonCompressedMessages()? Since we can't do things in place, could we just use the path that deals with recompression to handle it? This will save some duplicated code.
"
51672737,764,junrao,2016-02-03T02:55:35Z,"requireReCompression is a bit confusing. For example, requireReCompression will be true if the source data is compressed and the broker requires no compression. Perhaps we can rename it to sth like inPlaceConversion and negate the test?
"
51672760,764,junrao,2016-02-03T02:56:06Z,"Passing in messageSetTimestampAssignor makes the code a bit harder to read. I was wondering if we can obviate that and always scan the messages to get the max timestamp. This will add a bit overhead of making another iteration of all messages, but will simplify the code. The overhead should be small.
"
51672770,764,junrao,2016-02-03T02:56:15Z,"So far, we have been using case classes to represent enum in scala (see CompressionCodec). Could we follow the same convention?
"
51672776,764,junrao,2016-02-03T02:56:24Z,"Hmm, producer request v2 is supposed to send message of v1, right? Actually, should we enforce that on the broker?
"
51672781,764,junrao,2016-02-03T02:56:34Z,"Hmm, what is %s for now? Also, do we want to just log the text ""fetchRequest""?
"
51672789,764,junrao,2016-02-03T02:56:43Z,"Would it be better to use the latest version of the message?
"
51672844,764,junrao,2016-02-03T02:57:49Z,"Right, we can use sth like v0, v1 to make it consistent.
"
51682035,764,junrao,2016-02-03T06:06:48Z,"Converting to message format V1 allows us to track timestamp at the message level, which will be useful for things like removing the tombstone.
"
51768874,764,becketqin,2016-02-03T18:57:25Z,"Hi Jun, currently we always do re-compression when compacting the log, even if message format v1 is used. Do you mean we should change that so if a compressed message set does not change after compaction we simply write the original message set back? If we do that (and we probably should), it seems it does not matter whether message format v0 or v1 is used, because it only depends on whether there is message in the message set got compacted out or not.
"
51817003,764,hachikuji,2016-02-04T01:24:23Z,"Took me a while to wrap my head around this line. It seems like the `lastInnerRelativeOffset` and `wrapperRecordOffset` are constants within the life of this instance, so I'm wondering if we could instead use a single constant (e.g. `absoluteBaseOffset`) for this difference? Then this line would just become: 

``` java
long absoluteOffset = absoluteBaseOffset + entry.offset();
```

Which is a lot more readable.
"
51831822,764,junrao,2016-02-04T05:42:22Z,"That's not what I meant. Since this message will be eventually appended to the log through Log.append, if it's compressed and of format v0, we need to recompress it. If it's format v1, the recompression can be avoid. Also, since v1 carries more metadata, it seems that we should always try to use message v1 if possible.
"
51961629,764,becketqin,2016-02-05T00:39:38Z,"Hi Jun, do we also want the change in old producer request and old producer? It seems it will be deprecated pretty soon. So in the current patch I did not even change the scala ProducerRequest format, but I probably should just leave the scala producer version to 1.

Besides that, I realized that we have quite a few tools still using old consumer. I will update them.
"
52074957,764,becketqin,2016-02-05T21:26:46Z,"@hachikuji ack :)
"
52267507,764,junrao,2016-02-09T04:50:22Z,"Reworded this a bit

The default on-disk message format in 0.10.0 in v1. If a consumer client is on a version before 0.10.0, it only understands message format v0. In this case, the broker is able to convert messages of format v1 to v0 before sending a response to the consumer on an older version. However, the broker can't use zero-copy transfer in this case.

To avoid such message conversion before consumers are upgraded to 0.10.0, one can set the message format to v0 after upgrading the broker to 0.10.0. This way, the broker can still use zero-copy transfer to send the data to the old consumers. Once most consumers are upgraded, one can change the message format to v1 on the broker.
"
52267511,764,junrao,2016-02-09T04:50:31Z,"The comment is not accurate since the producer doesn't know the timestamp type.
"
52267516,764,junrao,2016-02-09T04:50:38Z,"Are these comments correct? The timestamp in ProducerRecord is always set by the producer.
"
52267522,764,junrao,2016-02-09T04:50:44Z,"Could we add a comment on the timestamp field?
"
52267534,764,junrao,2016-02-09T04:51:06Z,"typo DV

Also, could we leave some comments so that people know what to do when changing the protocol again before 0.10.0 is released?
"
52267543,764,junrao,2016-02-09T04:51:13Z,"We no longer need this since 0_10_0 will just point to the latest IV.
"
52267549,764,junrao,2016-02-09T04:51:21Z,"The changes in this file seem no longer needed.
"
52267569,764,junrao,2016-02-09T04:51:50Z,"I had a comment on this in the previous round of review. It seems that during compaction, it would be better to write the message in the configured message format : (1) This reduces the message conversion during fetch. (2) Converting to message format V1 allows us to track timestamp at the message level, which will be useful for things like removing the tombstone.
"
52267574,764,junrao,2016-02-09T04:51:56Z,"Should we just represent this as a byte to be consistent with Message.magic?
"
52267583,764,junrao,2016-02-09T04:52:05Z,"Should we assert that wrapperMessageTimestamp is only set if compression is on and timestampType is logAppend?
"
52267592,764,junrao,2016-02-09T04:52:10Z,"adjust => adjusts
"
52267595,764,junrao,2016-02-09T04:52:18Z,"Could we make put the message in the exception clearer? e.g., the payload is null.
"
52267604,764,junrao,2016-02-09T04:52:27Z,"The confusion is that toRelativeOffset() doesn't really return the relative offset as defined in line 150. Would it be better to rename it to toInnerOffset()?
"
52267612,764,junrao,2016-02-09T04:52:41Z,"For compressed messages, we don't really set the timestampType for inner messages. So, we need to make this clear. Also, the inner message offset is not really the relative offset.
"
52267620,764,junrao,2016-02-09T04:52:58Z,"Since the offset of the inner message is not really the relative offset, we can probably just talk about how to derive the AO from the inner offset.
"
52267622,764,junrao,2016-02-09T04:53:03Z,"IO is defined but not used.
"
52267631,764,junrao,2016-02-09T04:53:19Z,"I left this comment in the previous review. Do we need convertNonCompressedMessages()? Since we can't do things in place, could we just use the path that deals with recompression to handle it? This will save some duplicated code.
"
52267646,764,junrao,2016-02-09T04:53:35Z,"Is this check needed since we can only do in-place if magic is > 0?
"
52267649,764,junrao,2016-02-09T04:53:38Z,"5th -> 4th?
"
52267651,764,junrao,2016-02-09T04:53:42Z,"Should we change the description for attribute?
"
52267666,764,junrao,2016-02-09T04:53:56Z,"Could we add a comment to explain when do we expect wrapperMessageTimestamp and wrapperMessageTimestampType to be not none?
"
52267675,764,junrao,2016-02-09T04:54:05Z,"In line 168, should we set timestampType in attribute?
"
52267679,764,junrao,2016-02-09T04:54:08Z," indentation
"
52267681,764,junrao,2016-02-09T04:54:12Z,"There are unused imports.
"
52267687,764,junrao,2016-02-09T04:54:19Z,"We only return the max timestamp of the inner messages if timestampType is createTime.
"
52267690,764,junrao,2016-02-09T04:54:24Z,"Would it be better to change sampleMagicValue to firstMagicValue?
"
52267700,764,junrao,2016-02-09T04:54:32Z,"The message in the exception can be mis-leading since validateMagicValuesAndGetTimestamp may not be called on a set of uncompressed messages.
"
52267706,764,junrao,2016-02-09T04:54:51Z,"The method only iterates shallow messages. So, perhaps changing the method to magicValueInAllWrapperMessages and adjusting the comments?
"
52267737,764,junrao,2016-02-09T04:55:43Z,"We can probably just check if all messages are on v0 since not all existing messages necessarily match the message format config.
"
52267746,764,junrao,2016-02-09T04:55:59Z,"It's probably better to log the # of bytes in the messageSet instead of # of messages. The later will invoke the iterator and is more expensive.
"
52267748,764,junrao,2016-02-09T04:56:04Z,"Incorrect indentation since this is the parameter for responseSize().
"
52267763,764,junrao,2016-02-09T04:56:17Z,"We want to make it clear that the performance impact is only during the upgrade period. Once the clients are upgraded or if people are just starting to use 0.10, there is no performance impact.
"
52356387,764,becketqin,2016-02-09T19:02:42Z,"Hi Jun, this patch is using the message format version configured for this topic when doing compaction. The configuration is passed in in line 373. Do you mean something else?
"
52380036,764,becketqin,2016-02-09T21:52:37Z,"Having a separate `convertNonCompressedMessages()` saves one round of memory copy. 

In `convertNonCompressedMessages()` we read from the old format and write the converted format directly into the new byte buffer. So there is only one memory copy.

If we let the path that deals with re-compression to handle it, we need to first convert messages to required format (first memory copy), then write them together to a new byte buffer (second memory copy). 
"
52387545,764,junrao,2016-02-09T22:48:26Z,"Hmm, I don't see the logic of format conversion though. For example, if there are uncompressed messages of v0 and the format of the topic is configured with v1, we should write messages of v1 to the new log segment. Currently, it seems that we just keep the original message format.
"
52389495,764,becketqin,2016-02-09T23:03:43Z,"We always return the max timestamp of the inner messages as long as they are in v1.
"
52405180,764,becketqin,2016-02-10T01:44:59Z,"Hi Jun, the tests on the configuration here is trying to address the following problem. 

After people just upgrade to 0.10.0.0, most of the fetch request are still in v1. In this case, if we check whether all the messages are v0 or not, we are essentially iterating over the file message set. By checking the configuration, we can avoid that.

After people set message format to v1, we will only do the iterative check for old consumers (hopefully there won't be many at that point). 

The caveat of this approach is mentioned in the comments. After user set the message format to v1, if they decide to change the message format back to v0. The old consumer may see message v1 because of the discrepancy between actual message format and the config as you pointed out.

Do you prefer to simply take the performance cost right after people finish upgrade? This cost will go away after all the clients are upgraded, but that could be an extended period.
"
52503273,764,becketqin,2016-02-10T18:41:47Z,"Ah, you are right. I'll fix that.
"
52550562,764,becketqin,2016-02-11T00:34:48Z,"Hi Jun, I actually hesitated a little here. It seems this constructor should only be used by producer, so the timestamp should always be CreateTime. I added the timestamp type to constructor pretty lately because  ChecksumMessageFormatter also needs to construct message in order to compute checksum. 

Currently ChecksumMessageFormatter only takes key and value and always assume the compression type to be NoCompression. This works because all the messages, including inner messages of compressed messages, should not have compression codec.

However, because timestamp type can be different from message to message, we need to include timestamp type when computing checksum. We also need compression type because for compressed messages, inner message timestamp type is always CreateTime even when the timestamp type of the message is LogAppendTime defined by wrapper message.

I did not find any usage of ChecksumMessageFormatter. I asked Joel about this class, and it looks we used to use it in system test, but now we are not using it anymore. Do you think we can simply remove this class?
"
52609487,764,junrao,2016-02-11T14:44:42Z,"Thanks, got your point. So, setting the message version to 0 in the config not only implies that future messages will be written in version 0, but all existing messages are of version 0 too? We should at least document this, but I am not sure if this is enough to warn people about the impact of switching back and forth of the message format config.

Could you run some experiments about the performance impact of doing the message format check?
"
52780786,764,becketqin,2016-02-12T18:46:05Z,"Hi Jun, I ran the following experiment with the configuration check removed. i.e. always verify messages for fetch request v1.
1. Start a new broker with message.format.version=v0.
2. produce 3000000 messages to topic with 128 partition using console producer.
3. Consume the messages using console consumer in this RB (sending v2 fetch request, so no message format verification needed.)
4. Consume the messages using console consumer in current trunk (sending v1 fetch request, so message verification is needed)

The log below prints out the time cost on the verification code block:

```
[2016-02-12 10:33:33,976] INFO [KafkaApi-0] Avg time taken = 1163 (kafka.server.KafkaApis)
[2016-02-12 10:33:51,126] INFO [KafkaApi-0] Avg time taken = 933 (kafka.server.KafkaApis)
[2016-02-12 10:34:07,205] INFO [KafkaApi-0] Avg time taken = 907 (kafka.server.KafkaApis)
[2016-02-12 10:34:23,627] INFO [KafkaApi-0] Avg time taken = 962 (kafka.server.KafkaApis)
[2016-02-12 10:35:48,784] INFO [KafkaApi-0] Avg time taken = 7914977 (kafka.server.KafkaApis)
[2016-02-12 10:36:05,932] INFO [KafkaApi-0] Avg time taken = 18795324 (kafka.server.KafkaApis)
[2016-02-12 10:36:23,004] INFO [KafkaApi-0] Avg time taken = 20152849 (kafka.server.KafkaApis)
[2016-02-12 10:36:39,973] INFO [KafkaApi-0] Avg time taken = 19948298 (kafka.server.KafkaApis)
[2016-02-12 10:36:57,072] INFO [KafkaApi-0] Avg time taken = 19525645 (kafka.server.KafkaApis)
```

The first 4 lines are from v2 requests. The fifth line is from v1 and v2 mixed. The last 4 lines are from v1 requests. Time unit is nano seconds.

It seems the traversal cost is expensive. 
"
52814031,764,junrao,2016-02-12T23:47:08Z,"Ok, then we can keep the version check there. Could we update the config/upgrade doc to make it clear that by setting the message version on a topic, the user is certifying that all existing data are on that version and if that's not the case, the consumer before 0.10 will break?
"
52815492,764,junrao,2016-02-13T00:08:23Z,"Hi, Jiangjie, 

Independent of ChecksumMessageFormatter, shouldn't we set the attribute based on the timestampType passed in? The caller is responsible for setting the timestampType.
"
52838521,764,junrao,2016-02-14T06:25:35Z,"Could we add the doc for timestampType?
"
52838529,764,junrao,2016-02-14T06:26:33Z,"Since this will be part of the java doc, could we explain what the timestamp will be?
"
52838534,764,junrao,2016-02-14T06:27:44Z,"timestam -> timestamp type?
"
52838535,764,junrao,2016-02-14T06:27:46Z,"Do we need the CreateTime at the end? Ditto for the LogAppendTime at the end of line 30.
"
52838538,764,junrao,2016-02-14T06:28:26Z,"If CreateTime is used, are we returning -1 for timestamp?
"
52838543,764,junrao,2016-02-14T06:29:13Z,"create times => CreateTime
"
52838565,764,junrao,2016-02-14T06:32:40Z,"the message format => that message format
"
52838575,764,junrao,2016-02-14T06:34:35Z,"Need to remove prinitln.
"
52838577,764,junrao,2016-02-14T06:35:01Z,"Should we throw KafkaException instead?
"
52838580,764,junrao,2016-02-14T06:35:43Z,"Since the inner offset is not really the relative offset, we can probably just refer to it as inner offset and point to the description in the later text.
"
52838587,764,junrao,2016-02-14T06:36:03Z,"producer -> the producer
create => creates 
"
52838590,764,junrao,2016-02-14T06:36:36Z,"followings situation => following situations
"
52838592,764,junrao,2016-02-14T06:36:54Z,"Probably better to rename to expectedInnerOffset?
"
52838595,764,junrao,2016-02-14T06:37:24Z,"It doesn't seem that we need to valid the input message here since this is already done in Log,analyzeAndValidateMessageSet().
"
52838598,764,junrao,2016-02-14T06:38:07Z,"Would it be better to return a MagicAndTimestamp so that the caller doesn't have to iterate the message set again to get the magic?
"
52838602,764,junrao,2016-02-14T06:38:33Z,"numFetch and totalTime are unused.
"
52838603,764,junrao,2016-02-14T06:38:51Z,"The value is now in bytes, not messages.
"
52838608,764,junrao,2016-02-14T06:40:00Z,"the whether -> whether
"
52839564,764,becketqin,2016-02-14T08:36:19Z,"It seems that if we do not have the CreatTime at the end, java doc will show the entire qualifier, i.e. org.apache.kafka.common.record.TimestampType#CreateTime. This works but seems a little verbose. Alternatively we can import the TimestampType class so we don't need to have a display name for the `@link`.

Do we have a convention for Java doc in Kafka? It seems we have different styles in the code base.
"
52839682,764,becketqin,2016-02-14T08:48:20Z,"If CreateTime is used, we are returning user provided timestamp if it exists or the time when the record was handed to the producer.
"
52847115,764,junrao,2016-02-14T19:01:21Z,"It seems that we need to reset absoluteBaseOffset when we are done iterating the inner records. Otherwise, the next record could be uncompressed and we will set the offset incorrectly. Could we add a unit test that covers this?
"
52847119,764,junrao,2016-02-14T19:01:28Z,"incomplete sentence
"
52847121,764,junrao,2016-02-14T19:01:36Z,"Reworded this a bit. See if it's clearer.

Since the api protocol may change more than once within the same release, to facilitate people deploying code from trunk, we introduce internal versions since 0.10.0. For example, the first time that we introduce a version change in 0.10.0, we will add a config value ""0.10.0-IV0"" and a corresponding case object KAFKA_0_10_0-IV0. We will also add a config value ""0.10.0"" that will be mapped to the latest internal version object, which is KAFKA_0_10_0-IV0. When we change the protocol a second time while developing 0.10.0, we will add a new config value ""0.10.0-IV1"" and a corresponding case object KAFKA_0_10_0-IV1. We will change the config value ""0.10.0"" to map to the latest internal version object KAFKA_0_10_0-IV1. Config value of ""0.10.0-IV0"" is still mapped to KAFKA_0_10_0-IV0. This way, if people are deploying from trunk, they can use ""0.10.0-IV0"" and ""0.10.0-IV1"" to upgrade one internal version at a time. For most people who just want to use released version, they can use ""0.10.0"" when upgrading to 0.10.0 release.
"
52847126,764,junrao,2016-02-14T19:02:02Z,"We should use the message format specified for the internal topic. In addition, we have to be a bit careful here, during a rolling upgrade, if a broker's inter-broker protocol version is still before 0.10.0, even if the message format is for v1, we will still want to use message format v0. Otherwise, if another broker is still on the pre 0.10.0 code, it can't read the v1 message in the internal topic. Ditto to the magic value setting below.
"
52847133,764,junrao,2016-02-14T19:02:21Z,"There is a similar issue here. During rolling upgrade to 0.10.0, if a broker's inter-broker protocol version is still before 0.10.0, even if the message format is for v1, we will still want to use message format v0. Otherwise, if another broker is still on the pre 0.10.0 code, it can't read the v1 message in the internal topic for things like log cleaning.
"
52847134,764,junrao,2016-02-14T19:02:33Z,"Similar issue as before, we need to further guard the message format based on inter protocol version so that we only use message format ready for every broker.
"
52847137,764,junrao,2016-02-14T19:02:50Z,"It seems that we may need to change the message format when copying out uncompressed messages too.
"
52847141,764,junrao,2016-02-14T19:02:54Z,"The inner message offset is not really relative.
"
52847144,764,junrao,2016-02-14T19:02:57Z,"The inner message offset is not really relative.
"
52847146,764,junrao,2016-02-14T19:03:06Z,"It seems that TimestampType can only be none if magic is 0?
"
52847149,764,junrao,2016-02-14T19:03:19Z,"Could we also add the need for this to be consistent with inter broker protocol (otherwise the setting will be ignored)?
"
52847150,764,junrao,2016-02-14T19:03:23Z,"Are the changes here needed?
"
52847151,764,junrao,2016-02-14T19:03:32Z,"Since the fail and assertions are in the callback and we eat the exceptions, we probably need to propagate the failure to the main test method?
"
52847154,764,junrao,2016-02-14T19:03:40Z,"Can we get the cause and check the exact exception time? Ditto below.
"
52847180,764,junrao,2016-02-14T19:05:18Z,"It seems that we need to call validateTimestamp() in this case as well?
"
52847192,764,junrao,2016-02-14T19:06:14Z,"InvalidMessageException is used for corrupted messages. Could we use a different exception and error code?
"
52847218,764,junrao,2016-02-14T19:08:19Z,"Thinking about this again, since it's possible for us to change the message format more than once within the same release and we need to make sure message format is consistent with inter protocol version, it's probably better to just use the values in ApiVersion to specify the message format. We can probably extend each of the case object in ApiVersion to add a magic field to indicate the message version associated with each protocol version. Sorry for going back and forth on this one.
"
52850892,764,becketqin,2016-02-14T23:16:33Z,"Hmm, absoluteBaseOffset is only non-negative for inner iterators. The outer iterators always have absoluteBaseOffset=-1. Because we create a separate independent inner iterator for each compressed message set, the same absoluteBaseOffset should be used by all the messages in that compressed message set. After we finish iterating one compressed message set and return to the outer iterator, the absoluteBaseOffset of inner iterator will not affect the outer iterator, i.e. the outer iterator absoluteBaseOffset remains -1. So even the next record is an uncompressed record, it should not be affected.
"
52851922,764,becketqin,2016-02-15T00:20:05Z,"Hi Jun, we are doing message format conversion, right? If another broker is on old code and sends FetchRequest v1, we will down convert the message to v0. So it should not break even if leader has message format v1 on disk and follower is fetching using inter-broker protocol version before 0.10.0.

I am wondering if we should simply let the internal topic message format comply with inter-broker protocol version. The reason is this guarantees no message format conversion is needed for internal topic replication. And it avoids manually setting message format version config for the internal topic.
"
52852265,764,becketqin,2016-02-15T00:36:02Z,"Regarding the message format check. Currently we do validate both broker and topic level configuration to make sure message format version is on or below inter-broker protocol version. So  is it sufficient to simply use the message format version in the config?
"
52852401,764,becketqin,2016-02-15T00:43:01Z,"Sorry for the confusion, ""TimestampType"" in the comments should actually be ""wrapperMessageTimestampType"".
"
52852569,764,becketqin,2016-02-15T00:52:19Z,"Good catch. This might cause the test to just hang. It looks that in other multi-threaded test we have timeout in main thread. Maybe we can do the same thing here. 
"
52852973,764,becketqin,2016-02-15T01:14:15Z,"I was also thinking about this when creating KAFKA-3203. Currently we are also throwing InvalidMessageException if we non-keyed message for compacted topic. 

Should we add InvalidMagicByteException, InvalidCodecException, InvalidTimestampException and corresponding error mapping?
"
52861805,764,junrao,2016-02-15T06:01:37Z,"Yes, perhaps we can just add InvalidTimestampException in this patch and add others in KAFKA-3203.
"
52861828,764,junrao,2016-02-15T06:02:18Z,"Well, it may not hang. It just that if the assertion fails, the test may not fail since it only causes an exception in the callback thread. We can probably do sth like maintaining a successCount and check that at the end of flush.
"
52861864,764,junrao,2016-02-15T06:03:24Z,"Yes, you are right. Didn't see the message format check in KafkaConfig. I think it's still better and simpler to just use the message format specified for the internal topic. This way, one can wait until inter protocol is upgraded to 0.10.0 in all brokers and then upgrade the message format w/o any conversion overhead. If we rely upon inter-broker protocol, there will still be some conversion while changing the inter-broker protocol in all brokers.
"
52861870,764,junrao,2016-02-15T06:03:36Z,"Thanks for the explanation. Yes, I think this is fine since absoluteBaseOffset is final.
"
52861920,764,junrao,2016-02-15T06:04:11Z,"Ok, sounds good.
"
52862221,764,junrao,2016-02-15T06:05:32Z,"Could we also add sth like the following.

In particular, after the message format is set to v1, one should not change it back to v0 since it may break the consumer on versions before 0.10.0.
"
52862231,764,junrao,2016-02-15T06:05:39Z,"Could we add the following breaking change in 0.10.0?

MessageFormatter
  def writeTo(key: Array[Byte], value: Array[Byte], timestamp: Long, timestampType: TimestampType, output: PrintStream)
"
52862239,764,junrao,2016-02-15T06:05:55Z,"indentation
"
52862242,764,junrao,2016-02-15T06:06:01Z,"Are the changes in this file needed?
"
52862251,764,junrao,2016-02-15T06:06:09Z,"Should we use magic v1 in this test?
"
52862254,764,junrao,2016-02-15T06:06:13Z,"typo converion
"
52862293,764,junrao,2016-02-15T06:06:47Z,"It's not clear why this is needed. Isn't the default of message version v1?
"
52862309,764,junrao,2016-02-15T06:06:56Z,"Could we put the block from 185 to 191 in a method and reuse it in the subsequent blocks? 
"
52862322,764,junrao,2016-02-15T06:07:02Z,"Should we generate message of v1?
"
52862344,764,junrao,2016-02-15T06:07:12Z,"Could we avoid duplicating the code btw v0 and v1?
"
52862367,764,junrao,2016-02-15T06:07:23Z,"Should we use message v1? Since v1 is the default message format, it seems that v1 should be the format that we use in most tests.
"
52862383,764,junrao,2016-02-15T06:07:31Z,"Hmm, the overhead in message v1 is larger than Message.MinMessageOverhead.
"
52862396,764,junrao,2016-02-15T06:07:36Z,"This same process can be used to upgrade from 0.8.x to 0.10.0, right?
"
52865660,764,becketqin,2016-02-15T07:05:21Z,"Just to clarify, by ""there will still be some conversion while changing the inter-broker protocol"", you meant downgrading inter-broker protocol, right? If user are bumping up inter-broker protocol, there will be no conversion. If so, considering inter-broker protocol downgrading is relatively rare and the impact is only during inter-broker protocol downgrade, does it still worth requiring user to change internal topic configuration as an extra step?
"
52865992,764,becketqin,2016-02-15T07:12:43Z,"Right, we have already have successCount like check. The test hangs because we usually call flush() in main thread to ensure all the messages are sent. Some callbacks won't fire if sender thread dies due to assertion failure, hence flush() blocks forever. I will instead use producer.close(timeout) in main thread so it does not wait forever for the callbacks.
"
52869635,764,becketqin,2016-02-15T08:16:10Z,"Yes, so I subtracted the additional timestamp length.
"
52922474,764,junrao,2016-02-15T17:00:51Z,"If the message format is tied to the inter-broker protocol, the first broker that upgrades its inter-broker protocol to 0.10.0  will start to have v1 messages in its log. Since the rest of the brokers' inter-broker protocol is still before 0.10.0, they can only send v1 fetch request. Therefore, the first broker has to convert v1 messages down to v0 in the fetch response.

If we decouple the message format from inter-broker protocol, we can first keep message format to be v0. After every broker upgrades inter-broker protocol to 0.10.0 and starts using v2 fetch request, we then change message format to v1. At that point, there is no message format conversion needed. 
"
52962132,764,ijuma,2016-02-16T02:14:23Z,"`If user` would read better as `If the user`.
"
52962884,764,ijuma,2016-02-16T02:29:30Z,"`overwritten by broker with broker local time when broker append` would read better as `overwritten by the broker with the broker local time when it appends`
"
52963204,764,ijuma,2016-02-16T02:35:41Z,"`In either of the cases above, the timestamp that has actually been used will be returned to the user in`
"
53015522,764,ijuma,2016-02-16T14:19:28Z,"Shouldn't these be called `NO_TIMESTAMP_TYPE`, `CREATE_TIME` and `LOG_APPEND_TIME` since this is Java code? I personally prefer the Scala convention (which is what we are using here), but it is inconsistent with all the Java enums I looked at.
"
53017909,764,ijuma,2016-02-16T14:36:37Z,"This doesn't seem to be used at the moment.
"
53018211,764,ijuma,2016-02-16T14:38:50Z,"Sorry if this has been mentioned elsewhere, but why don't we reuse the Java `TimestampType` instead of introducing a duplicate instance in Scala code?
"
53018917,764,ijuma,2016-02-16T14:44:21Z,"Why are we using an AtomicLong here? This code doesn't have to be thread-safe, right (we are adding to an ArrayList a few lines below with no locking for example)?
"
53035417,764,ijuma,2016-02-16T16:27:18Z,"Why we are using a LinkedList here? It rarely makes sense to use it, even the person who wrote it says so (https://twitter.com/joshbloch/status/583813919019573248).
"
53036739,764,ijuma,2016-02-16T16:35:11Z,"Maybe `ArrayDeque` would be better?
"
53040684,764,ijuma,2016-02-16T16:57:43Z,"It would probably be good to mention that `innerIter` will always have at least one element (and hence why we can just call `next()` on it without calling `hasNext()` first).
"
53040990,764,ijuma,2016-02-16T16:59:20Z,"Maybe reference `MAGIC_VALUE_V1` instead of hardcoding `1` here?
"
53041472,764,bill-warshaw,2016-02-16T17:02:23Z,"how would you feel about adding a testing constructor for this class that matches the existing signature, and would just use dummy values for the timestamp and `TimestampType`?  Eliminating that constructor will break compilation for any unit tests that rely on building `ConsumerRecord`s
"
53048549,764,ijuma,2016-02-16T17:47:32Z,"case 1 and 2 are the same at the moment. We could fall-through from case 1 to case 2 instead of having the same statement twice.
"
53056249,764,becketqin,2016-02-16T18:39:32Z,"Do you mean the unit tests in projects other than Kafka?
"
53056903,764,bill-warshaw,2016-02-16T18:44:02Z,"yes.  It's a minor change to update anywhere these constructors are used in unit tests, but any test that instantiates a `ConsumerRecord` isn't going to compile as soon as a user upgrades past `0.9.0.0`.  We've used the `@VisibleForTesting` annotation in the past to denote certain constructors that are only there for unit-testing.
"
53058002,764,becketqin,2016-02-16T18:50:05Z,"That makes sense. Originally client side and server side TimestampType have some different methods. And I was thinking to combine the java and scala TimestampType class when we migrate server from Message to Record. We can probably combine the two classes in this patch.
"
53106467,764,becketqin,2016-02-17T00:54:53Z,"I am not sure if we should add the testing constructor for other projects. It is a little bit weird to have a testing constructor which never used by Kafka but for some unknown external project. Technically speaking ConsumerRecord should only be constructed by Kafka, but not other projects. If we do so, arguably we should maintain the constructor backward compatibility for any public class, even though most of them are not supposed to be constructed by any user.
"
53110159,764,bill-warshaw,2016-02-17T01:39:29Z,"if `ConsumerRecord` is only intended to be instantiated by Kafka, then I withdraw my comment.  Internal APIs shouldn't be forced to remain backwards-compatible.
"
53142209,764,ijuma,2016-02-17T09:58:29Z,"`ConsumerFetcherThread` and `SimpleConsumer` still use this class. Given that, are we sure that we don't need to add a mapping here?
"
53143828,764,ijuma,2016-02-17T10:13:32Z,"Would this read better as `MessageTimestampMaxDifferenceMs`?
"
53144348,764,ijuma,2016-02-17T10:18:06Z,"Should we be catching `ApiException` instead of individual cases like this?
"
53202790,764,becketqin,2016-02-17T17:52:22Z,"Only produce will see this error code. So I think we are fine here.
"
53203160,764,becketqin,2016-02-17T17:54:35Z,"I agree that would read better (in fact I used to use that name), but it seems we have `""*MaxMs""` for other configurations, so I just followed the convention.
"
53204217,764,becketqin,2016-02-17T18:00:52Z,"I don't have strong opinion on this. One benefit of having individual cases is that it is clear what kind of API exceptions are expected since not all API exceptions can be thrown from replica manager.
"
53263334,764,junrao,2016-02-18T02:08:56Z,"This comment is a bit hard to understand. Given the comments at the beginning, we probably don't need the comment here.
"
53263338,764,junrao,2016-02-18T02:09:02Z,"relative => inner
"
53263354,764,junrao,2016-02-18T02:09:15Z,"The way timestamp set is following => The way that timestamp is set is the following
"
53263360,764,junrao,2016-02-18T02:09:21Z,"is set => are set
"
53263365,764,junrao,2016-02-18T02:09:26Z,"following => the following
"
53263370,764,junrao,2016-02-18T02:09:35Z,"Note => note; in a stream compressing way => in a streaming way
"
53263375,764,junrao,2016-02-18T02:09:38Z,"avoids => avoid
"
53263382,764,junrao,2016-02-18T02:09:43Z,"Do we still need this comment?
"
53263386,764,junrao,2016-02-18T02:09:50Z,"It seems that MinHeaderSize is the same as MinMessageOverhead. Could we consolidate them?
"
53263400,764,junrao,2016-02-18T02:10:02Z,"Would it be better to rename this to magicAndLargestTimestamp()? Also, the comment is outdated since we now return the magic as well.
"
53263409,764,junrao,2016-02-18T02:10:07Z,"Could we use the constant variable instead of 0?
"
53263420,764,junrao,2016-02-18T02:10:18Z,"Incorrect indentation. The original one is correct.
"
53263431,764,junrao,2016-02-18T02:10:37Z,"unused import here. Could you check other classes too?
"
53263446,764,junrao,2016-02-18T02:10:54Z,"We probably shouldn't mention 0.10.0-IV0 since it's not intended for public usage.
"
53263518,764,junrao,2016-02-18T02:11:50Z,"Reworded the text a bit below. See if it's better.

The maximum difference allowed between the timestamp when a broker receives a message and the timestamp specified in the message, if message.timestamp.type=CreateTime. A message will be rejected if the difference in timestamp exceeds this threshold. This configuration is ignored if message.timestamp.type=LogAppendTime.
"
53263527,764,junrao,2016-02-18T02:11:58Z,"Just to be consistent with what's in writeTo, perhaps we should write the timestamp before key?
"
53263537,764,junrao,2016-02-18T02:12:07Z,"To avoid duplicating code, could we pull line 237 to 245 into a private method and then reuse?
"
53263541,764,junrao,2016-02-18T02:12:11Z,"InValid => Invalid; ditto below.
"
53263556,764,junrao,2016-02-18T02:12:21Z,"message.format.version change is an optimization. So, it's not really required. We can probably just cover that in the section on performance impact. 
"
53263559,764,junrao,2016-02-18T02:12:25Z,"Is this any different from any other topic?
"
53263566,764,junrao,2016-02-18T02:12:33Z,"Instead of using v1/v0 in the message format, it's probably easier to understand if we just use the api version.
"
53263572,764,junrao,2016-02-18T02:12:40Z,"We can just say that ""For clients that are upgraded to 0.10.0.0, there is no performance impact.""
"
53291250,764,ijuma,2016-02-18T09:44:46Z,"Fair enough.
"
53291541,764,ijuma,2016-02-18T09:47:25Z,"My concern is that it's easy to miss new `ApiException` instances that could be thrown by the code above since we don't get help by the compiler. But we can consider handling this in a separate PR as you are maintaining the existing approach.
"
53322688,764,ijuma,2016-02-18T14:48:18Z,"Is there a reason why we don't pass the timestamp as a parameter to `analyzeAndValidateMessageSet`? That would mean that `timestamp` could be a `val` instead of `var`. It's a straightforward change, but it would mean that we read `config.messageTimestampType` outside the synchronized block. Is that a problem?
"
53342228,764,ijuma,2016-02-18T16:43:31Z,"Do we really need to mention the same thing so many times? It seems to me that it would be enough to mention once that message format 0 does not have a timestamp field and message format 1 does.
"
53345876,764,ijuma,2016-02-18T17:05:16Z,"Do we need this overload with timestamp and no TimestampType? There are only 3 usages in tests, I think I'd remove it.
"
53383523,764,becketqin,2016-02-18T21:15:23Z,"@ijuma If we do that, it seems possible to cause inconsistency order of message offset and timestamp. For example, message A comes and is stamped t1 by the broker, but before it is appended to the log, message B comes and is stamped t2 (t2 > t1) and gets appended to the log. After that, message A is appended. In this case, message A will have a smaller timestamp but a larger offset than message B, which is a bit confusing.

We can put everything in the synchronized block, but it seems not worth doing if we only want to change a var to a val.
"
53387576,764,becketqin,2016-02-18T21:44:34Z,"I am OK either way. As far as I understand, the purpose of the pre-existing constructor is to hide the payloadOffset and payloadSize from caller.
"
53390466,764,ijuma,2016-02-18T22:06:34Z,"Makes sense, thanks.
"
53390818,764,ijuma,2016-02-18T22:09:44Z,"OK. Since it was already there, better to handle the excessive use of constructor overloading separately.
"
53413279,764,junrao,2016-02-19T01:51:17Z,"Can this be private?
"
53413289,764,junrao,2016-02-19T01:51:25Z,"the following properties => the following property
"
53413303,764,junrao,2016-02-19T01:51:33Z,"Message format 0.10.0 => The message format in 0.10.0
"
53413305,764,junrao,2016-02-19T01:51:35Z,"of format 0.10.0 to earlier format => of the format in 0.10.0 to an earlier format
"
53413325,764,junrao,2016-02-19T01:51:48Z,"Could we add what the interface is changed from? Also, def is scala specific. We just need to include the java interface.
"
53413342,764,junrao,2016-02-19T01:52:02Z," indicate the client support quota => indicate that the client supports quota 
"
53413403,764,junrao,2016-02-19T01:53:09Z,"Reworded this a bit to the following. See if it's better. Ditto in TopicCommand.

This configuration will be ignored if the value is on a version newer than that specified in inter.broker.protocol.version in the broker.
"
53413412,764,junrao,2016-02-19T01:53:16Z,"Is the above addressed?
"
53415057,764,becketqin,2016-02-19T02:18:05Z,"It seems we cannot add scope modifier to a code block. Compiler gives the following error:

```
/core/src/test/scala/unit/kafka/log/FileMessageSetTest.scala:260: illegal start of statement (no modifiers allowed here)
    private def verifyConvertedMessageSet(convertedMessageSet: MessageSet, magicByte: Byte) {
```

 `verifyConvertedMessageSet` itself seems private by nature and only accessible in `testMessageFormatConversion`.
"
53417265,764,becketqin,2016-02-19T02:50:09Z,"Hi Jun, this comment seems not accurate based on the current code. What we actually check is the message format version, not Kafka version. For example, the current code will allow the message.format.version to be set to 0.9.0 even if the inter.broker.protocol.version is 0.8.2, because the underlying message.format.versions of those two Kafka versions are the same. At some point I thought there was a use case for this, but I cannot think of any now.

It is probably better to enforce the check as you described. I will make this change in the updated patch.
"
1235169801,13870,dajac,2023-06-20T12:07:23Z,nit: Could we prefix all those attributes with `genericGroup`?
1235170208,13870,dajac,2023-06-20T12:07:48Z,We can't use the same config both both the consumer and the generic group because we have two configs for each.
1235171243,13870,dajac,2023-06-20T12:08:49Z,nit: We can remove this one as there is the same phrase in the javadoc.
1235178394,13870,dajac,2023-06-20T12:15:13Z,I am not a fan of returning a `Group` here because it means that the caller have to cast the returned value. Is it possible to avoid it?
1235178804,13870,dajac,2023-06-20T12:15:34Z,Should we move this to the request validation?
1235179626,13870,dajac,2023-06-20T12:16:19Z,I would remove this because it will be outdated extremely quickly.
1235180677,13870,dajac,2023-06-20T12:17:16Z,Should we have a method helper to validate the request?
1235188456,13870,dajac,2023-06-20T12:24:19Z,nit: Would it make sense to update `plainProtocolSet` to take `request.protocols()`?
1235190243,13870,dajac,2023-06-20T12:25:52Z,"When the group is created to the first time, I think that we need to write a record to the log; the group could be reverted in the timeline hash map otherwise."
1235202608,13870,dajac,2023-06-20T12:36:20Z,We need to discuss whether we want to start the heartbeat timer here or not. See [KAFKA-13766](https://issues.apache.org/jira/browse/KAFKA-13766).
1235206095,13870,dajac,2023-06-20T12:39:01Z,"Do we still need `hasSatisfiedHeartbeat` in the new model? If the timeout expires, it seems to me that it means that the member has failed to heartbeat in time; the timer would have been reset otherwise."
1235232078,13870,dajac,2023-06-20T12:59:58Z,I wonder if we should use a different timer for this case. Have you considered it?
1235241285,13870,dajac,2023-06-20T13:07:13Z,"Instead of storing the initial rebalance timeout and the initial rebalance delay in the group, could we imagine passing them as arguments to `tryCompleteInitialRebalanceElseSchedule`? That could simplify the logic as everything would be self contain here."
1235274474,13870,dajac,2023-06-20T13:31:31Z,What happens if the group instance id is an empty string?
1235276378,13870,dajac,2023-06-20T13:32:56Z,I suppose that being here means that group instance id is null.
1235277768,13870,dajac,2023-06-20T13:33:58Z,I have seen this code in a few place. It would be great to avoid it if possible.
1235310632,13870,dajac,2023-06-20T13:56:02Z,Shouldn't we use `CompletableFuture<Void>` and complete the future exceptionally with the exception corresponding to the error?
1235497178,13870,jeffkbkim,2023-06-20T16:11:16Z,i think we should store generic groups separately. the added benefit here is that we wouldn't have to create a new record when a new group was created as you have mentioned in the comment below. wdyt?
1235653740,13870,jeffkbkim,2023-06-20T18:24:23Z,i removed this; we have request validation in GroupCoordinatorService#joinGroup. what do you think of having all request validation there?
1235733198,13870,jeffkbkim,2023-06-20T19:26:09Z,"i don't think this fits well because we may have other records to append in this method. As mentioned above, I will store the generic groups in a separate hash map to prevent this from happening."
1235734010,13870,jeffkbkim,2023-06-20T19:26:49Z,see https://github.com/apache/kafka/pull/13870#discussion_r1235653740
1235752587,13870,jeffkbkim,2023-06-20T19:46:36Z,"so that i understand: during completing rebalance, we schedule both pending sync (rebalance timeout) and heartbeats (session timeout). in practice, session timeout << rebalance timeout so heartbeats would expire and remove members. With cooperative rebalancing members should still be able to fetch records during completing rebalance phase. We want to extend the heartbeat here to rebalance timeout so that members are not removed by session timeout.

is this correct?

If so, i agree removing the heartbeat schedule sounds like the best approach since members are removed when pending sync expires anyways."
1235808438,13870,jeffkbkim,2023-06-20T20:44:35Z,if the member is awaiting on a join/sync response then we can't remove the member on hb expiration right?
1235813521,13870,jeffkbkim,2023-06-20T20:50:21Z,are you suggesting a different key? what would be the benefit?
1235818207,13870,jeffkbkim,2023-06-20T20:54:59Z,initial rebalances don't have check and complete as try complete always returns false so this works. thanks
1235822238,13870,jeffkbkim,2023-06-20T20:58:34Z,the existing protocol allows empty group instance ids for static member
1235823024,13870,jeffkbkim,2023-06-20T20:58:57Z,"you're right, removed. "
1238417804,13870,dajac,2023-06-22T11:51:17Z,"i am not convinced. the downside is that it will be harder to guarantee to uniqueness of the group id. it also means that we would have to check both maps for all other operations (e.g. list, delete, etc.). i think that it would be better to keep them in a single map.

for this particular case, we could just have two methods:  `getOrMaybeCreateConsumerGroup` and `getOrMaybeCreateGenericGroup`."
1238418966,13870,dajac,2023-06-22T11:52:26Z,i think that static validation could be done in the group coordinator service; however we have to keep the validation which depends on internal values in the state machine.
1238484735,13870,dajac,2023-06-22T12:52:04Z,"That's correct. However, it may be better to just cancel the previous one."
1238491877,13870,dajac,2023-06-22T12:58:00Z,I think that the non-error case is actually incorrect based on the implementation in the runtime. The issue is that the future will be completed immediately after the records are written. This means that we would send the response before the record is committed. I think that the future should be completed only when the records are committed.
1238494924,13870,dajac,2023-06-22T13:00:19Z,"What does happen when this exception is thrown? I mean, where is it handled?"
1238559395,13870,dajac,2023-06-22T13:48:16Z,Did you consider using a switch here? It seems that it would fit nicely.
1238565407,13870,dajac,2023-06-22T13:52:35Z,nit: Should we have an method such as `hasAssignment()` in member?
1238585947,13870,dajac,2023-06-22T14:06:54Z,This feels a bit like a hack. I was wondering if we could push that call to `completeGenericGroupJoin(group)` into the `genericGroupJoinNewMember` and `genericGroupJoinExistingMember` paths instead of handling it here for all cases. Is it something that you have considered?
1238591771,13870,dajac,2023-06-22T14:11:10Z,"This is correct. btw, if we remove it, I think that we need to ensure that the session timeout is cancelled when a member rejoins."
1238593470,13870,dajac,2023-06-22T14:12:26Z,"correct. however i think that the fundamental issues is that we do not cancel the session timeout while doing a rebalance. this is why we have this condition here. if we fix this, we may be able to remove it."
1238594022,13870,dajac,2023-06-22T14:12:53Z,right. separation of concerns would be the benefit. 
1238626957,13870,dajac,2023-06-22T14:37:10Z,It seems that the condition was `group.is(Empty)` in Scala. What's the reason for changing it?
1238627417,13870,dajac,2023-06-22T14:37:28Z,nit: extra empty line.
1238630708,13870,dajac,2023-06-22T14:39:46Z,"Actually, it seems that `completeGenericGroupJoin` is already called in a few places on those paths."
1238633046,13870,dajac,2023-06-22T14:41:05Z,nit: Should we have an overload of `supportsProtocols` which takes `request.protocols()`?
1238643069,13870,dajac,2023-06-22T14:48:13Z,nit: Could we move this to the test then?
1238647097,13870,dajac,2023-06-22T14:51:16Z,nit: Let's remove this one as well.
1238724392,13870,dajac,2023-06-22T15:49:12Z,"One issue here is that if `generateRecordsAndAppendFuture` thrown an exception (e.g. due to a bug), the request will never be completed because the future is not available yet. If we reuse `CoordinatorWriteEvent`, we could subscribe to the future returned by it and complete the response when an exception is raised."
1238725344,13870,dajac,2023-06-22T15:49:57Z,We probably need to convert some of the exceptions like I did for the consumer group heartbeat request.
1239316537,13870,jeffkbkim,2023-06-23T04:48:54Z,are you referring to any lingering heartbeats at this point? i think we can just cancel them here right
1239318379,13870,jeffkbkim,2023-06-23T04:53:31Z,"i think we can cancel the existing heartbeat when a member rejoins during a rebalance. this will still expire if the member does not rejoin which is what we want. also, moving the new member join timeout to a different key will help remove this `hasSatisfiedHeartbeat`.

is that what you had in mind?"
1239320095,13870,jeffkbkim,2023-06-23T04:57:15Z,i expected that KafkaApis#handleJoinGroupRequest will handle them. is that not the case?
1240399005,13870,jeffkbkim,2023-06-23T21:09:13Z,"This is now further worsened with the new records on creating a group.

The only ""non-hacky"" approach i can think of is just returning 
`List<CoordinatorResult<CompletableFuture<Void>, Record>`

which the runtime would append & commit then complete in order. but this adds a lot of complexity for something we actually won't use in practice.

The other approach (which i have implemented) is to ignore the result from `completeGenericGroupJoin` when invoked from the join group path.

This works because `completeGenericGroupJoin` only produces records when a member expires.
Also, when a new group is created we don't have any other records to append.

However, this still feels a bit hacky. Not sure how to resolve this."
1240415098,13870,jeffkbkim,2023-06-23T21:23:12Z,"I thought the ""initial"" rebalance only applied to when the group is first created from https://github.com/apache/kafka/pull/2758 but according to https://cwiki.apache.org/confluence/display/KAFKA/KIP-134%3A+Delay+initial+consumer+group+rebalance it looks like we want this for an empty group as well

> Adding a configurable delay to the initial rebalance of a new, or empty, consumer group [...]

So, i will revert this change.

However, it's awkward because here we consider ""initial rebalance"" to be an empty group. But a different part of the code checks generation id == 0. maybe it's because we don't know the previous state

from GroupCoordinator#addMemberAndRebalance:
```
    // update the newMemberAdded flag to indicate that the join group can be further delayed
    if (group.is(PreparingRebalance) && group.generationId == 0)
      group.newMemberAdded = true
```

I think we need to revert the change and include `initialRebalanceDelayMs` to the GenericGroup object and rely on that to check whether the group is undergoing an initial rebalance. "
1240441622,13870,jeffkbkim,2023-06-23T21:58:30Z,"i'm a bit confused, is the coordinator write operation future the result we wait for committing?"
1243806647,13870,dajac,2023-06-27T14:04:01Z,"How about the following? We keep track whether the group was newly created in a boolean. When we get the result from those methods, we check if the group is new, if it is, we check if the result has at least one record. If it does not, we recreate it while adding an empty record for the group."
1246778958,13870,dajac,2023-06-29T15:21:28Z,"Yeah, it seems that the current implementation is inconsistent. "
1246964799,13870,CalvinConfluent,2023-06-29T18:24:56Z,The timer is not used yet. Is it a place holder here?
1246972433,13870,CalvinConfluent,2023-06-29T18:33:21Z,"I guess I missed some of the previous discussions, but why it is always either stable or empty when we load a group? Does it mean the rebalancing process will be reverted if the coordinator fails?"
1246984793,13870,CalvinConfluent,2023-06-29T18:46:27Z,"The genericGroupJoinMember can returns a result, why don't we use the return value?"
1246995181,13870,CalvinConfluent,2023-06-29T18:56:21Z,Do we need to handle the IllegalStateException(when member id is not known) and complete the responseFuture here?
1247810650,13870,dajac,2023-06-30T12:31:40Z,"Sorry, I was not clear. I meant that we need to port this [logic](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/coordinator/group/GroupMetadataManager.scala#L288) here."
1247812716,13870,dajac,2023-06-30T12:34:06Z,"nit: I am not a fan of this validation. I wonder if we should just have two helpers: `isGroupIdNotNull` and `isGroupIdNotEmpty`. In this PR, we would only need `isGroupIdNotEmpty`. What do you think?"
1247814090,13870,dajac,2023-06-30T12:35:49Z,"I wonder if we need to handle the future returned by `scheduleWriteOperation` as well. At minimum, we may want to react to errors. This could for instance happen if something goes wrong before the join group handling is event triggered."
1247815228,13870,dajac,2023-06-30T12:37:09Z,nit: `completionFuture` or smth similar may be a better name here because we could have an operation without any records.
1247815765,13870,dajac,2023-06-30T12:37:47Z,How about adding a boolean `replayRecords` to the CoordinatorResult?
1247816731,13870,dajac,2023-06-30T12:38:40Z,"I wonder if we should complete both future here. As `scheduleWriteOperation` returns a future, it may be missed used otherwise. What do you think?"
1247817889,13870,dajac,2023-06-30T12:39:56Z,nit: Could we revert this?
1247820936,13870,dajac,2023-06-30T12:43:22Z,"`topicPartition` should also be required, I think."
1247822722,13870,dajac,2023-06-30T12:45:23Z,I think that the group should be deleted in this case.
1247825718,13870,dajac,2023-06-30T12:48:37Z,"I wonder if we could avoid passing the version to this method by adding `-1` as the default value of `rebalanceTimeout` in `GroupMetadataValue`. It seems that we could rely on this to decide here.

Another way that I was thinking about would be to pass the `Record` to the replay method as it contains all the available information. Have you considered this?"
1247826747,13870,dajac,2023-06-30T12:49:45Z,"We only write a record when the rebalance completes. This implies that the record is always empty or has members. As you pointed out, a failure happening before the rebalance complete is lost."
1247831310,13870,dajac,2023-06-30T12:54:40Z,Have you considered checking if the group exists in the map instead of adding this field? Using a field like this has the disadvantage that we must ensure that it is set to false. I think that your implementation already misses it.
1247837102,13870,dajac,2023-06-30T13:00:46Z,Don't we need to fail the future if the write fails? Or is it done somewhere else?
1247839533,13870,dajac,2023-06-30T13:03:09Z,"I am not satisfied with this logic here. I think that other folks won't understand this... We need to come up with a better way. I will think about it.

I don't recall if I already asked this but would it be possible to push `completeGenericGroupJoin` into `genericGroupJoinNewMember` and `genericGroupJoinExistingMember` instead of having it here?"
1247843963,13870,dajac,2023-06-30T13:07:41Z,"Hum... Those exceptions will be caught by the `CoordinatorWriteEvent` and used to complete the future there. So, I suppose that they will be propagated to the api layer via this mechanism. Do I get this right?"
1247846953,13870,dajac,2023-06-30T13:10:13Z,Could we remove this? `genericGroupNewMemberJoinTimeoutMs` is very likely passed to this object by the test itself so I am not sure to understand why we need to expose it to the test. Is there a reason?
1253477738,13870,jeffkbkim,2023-07-05T18:25:13Z,"i don't think this is the right place; we need to add the logic inside group metadata manager. i have done so in the sync PR.

the reason is that for generic group apis, the append future is what we want the logic for when handling log append/commit errors.

whereas for the new protocol, the consumer group heartbeat waits to return the results from the append/commit.

"
1253484339,13870,jeffkbkim,2023-07-05T18:32:34Z,i'll remove this for now and add it back if we decide to pass it in later.
1253487385,13870,jeffkbkim,2023-07-05T18:36:08Z,the approach is hacky. will be thinking of a different approach to resolve this
1253522689,13870,jeffkbkim,2023-07-05T19:11:59Z,"i will take your suggestion for this PR. however, it does make more sense to have the logic in one place instead of using isGroupIdNotNull/isGroupIdNotEmpty based on the request."
1253525703,13870,jeffkbkim,2023-07-05T19:15:48Z,"this is only used when records are generated (and need to be appended to the log) so i think append future makes more sense.

`completionFuture` will be confusing alongside coordinator event's `future` field. wdyt?"
1253814655,13870,jeffkbkim,2023-07-06T01:27:38Z,"for 1) doesn't it require a bump in the group metadata value version to add the default value?

2) i don't see much value in this and it feels more different to handle it this way compared to other record types in ReplicatedGroupCoordinator#replay(Record)"
1253817939,13870,jeffkbkim,2023-07-06T01:33:42Z,"will revisit this after refactoring the CoordinatorResult return type / generate multiple records discussion.

the reason that a field was set is because where we add the group to `groups` (and initialize the append future) and where we set it as a return type (L1265) are at different places. once the group is added we can't check via groups.get(groupId)."
1253948284,13870,jeffkbkim,2023-07-06T05:26:40Z,The write event will catch the exception and complete the event's future. I added a handler to GroupCoordinatorService.java for these unexpected exceptions.
1253948390,13870,jeffkbkim,2023-07-06T05:26:50Z,added a previousState to GenericGroup. we will rely on this instead to confirm that a group is undergoing an initial rebalance (previous state == empty)
1253948479,13870,jeffkbkim,2023-07-06T05:27:00Z,"yeah, i will log an error for this"
1254577233,13870,dajac,2023-07-06T15:07:30Z,I think that we can remove this and `metadataImage` as https://github.com/apache/kafka/pull/13901 was merged.
1254579000,13870,dajac,2023-07-06T15:08:56Z,Why do we only handle `IllegalStateException` here? Why if we get an unexpected NPE for instance.
1254579343,13870,dajac,2023-07-06T15:09:12Z,nit: This could be static.
1254580740,13870,dajac,2023-07-06T15:10:16Z,nit: Should we move those two back to where they where?
1254582473,13870,dajac,2023-07-06T15:11:39Z,Could we move this one back to where it was? It should stay together with the other `withConsumerGroup*` methods.
1254582995,13870,dajac,2023-07-06T15:12:04Z,Why did we move this one?
1254584209,13870,dajac,2023-07-06T15:13:07Z,nit: Empty line could be removed.
1254584535,13870,dajac,2023-07-06T15:13:21Z,nit: Should we revert this change?
1254584801,13870,dajac,2023-07-06T15:13:33Z,nit: Should we revert this change?
1254585208,13870,dajac,2023-07-06T15:13:52Z,nit: Should we revert this change?
1254585768,13870,dajac,2023-07-06T15:14:19Z,nit: Should we revert this change?
1254587698,13870,dajac,2023-07-06T15:15:51Z,1) Good question. The schema remains the same so it should be OK. It only adds a default value to the field.
1254612652,13870,dajac,2023-07-06T15:36:23Z,"I am not sure to follow. It seems to me that you could have a local boolean to track this. For instance, before calling `getOrMaybeCreateGenericGroup`, you could initialise a variable `groupExists` by checking the map."
1254999681,13870,jeffkbkim,2023-07-06T22:53:31Z,"i misunderstood, moved to using a local variable"
1255794837,13870,dajac,2023-07-07T13:05:30Z,Would it make sense to move this block into the `if (isNewGroup && result == EMPTY_RESULT)`?
1255797868,13870,dajac,2023-07-07T13:08:02Z,Remember this [comment](https://github.com/apache/kafka/pull/13870#discussion_r1247810650)? Don't we need to convert the exception here as well? This is why I was suggesting to do it in the service to ensure that we do it in all cases.
1255798698,13870,dajac,2023-07-07T13:08:41Z,nit: javadoc?
1255799314,13870,dajac,2023-07-07T13:09:08Z,nit: Empty line could be removed.
1255799771,13870,dajac,2023-07-07T13:09:29Z,nit: Empty line.
1255800519,13870,dajac,2023-07-07T13:10:02Z,nit: Empty line.
1255803548,13870,dajac,2023-07-07T13:12:27Z,nit: Incomplete javadoc.
1255807406,13870,dajac,2023-07-07T13:15:14Z,nit: `containsKey`?
1255809962,13870,dajac,2023-07-07T13:17:08Z,nit: empty line.
1255812336,13870,dajac,2023-07-07T13:18:48Z,nit: Do we need this as all the states are covered?
1255826174,13870,dajac,2023-07-07T13:28:59Z,nit: Should we inline this condition and move the comment within the branch?
1255827611,13870,dajac,2023-07-07T13:30:02Z,nit: Should we name this variable `newMember`?
1255828754,13870,dajac,2023-07-07T13:30:54Z,nit: This could be inlined.
1255828976,13870,dajac,2023-07-07T13:31:03Z,nit: Empty line.
1255830986,13870,dajac,2023-07-07T13:32:33Z,nit: How about using: `.setMembers(isLeader ? group.currentGenericGroupMembers() : Collections.emptyList())`?
1255832177,13870,dajac,2023-07-07T13:33:23Z,nit: I think that the error code is zero by default so we don't have to set it.
1255832458,13870,dajac,2023-07-07T13:33:35Z,nit: empty line.
1255833047,13870,dajac,2023-07-07T13:34:01Z,nit: Empty line.
1255833259,13870,dajac,2023-07-07T13:34:11Z,nit: ditto about error code.
1255833655,13870,dajac,2023-07-07T13:34:27Z,nit: empty line.
1255836858,13870,dajac,2023-07-07T13:36:43Z,nit: empty line.
1255838176,13870,dajac,2023-07-07T13:37:41Z,nit: The code style is a bit inconsistent between this line and L2526. I don't have a preference but it would be great if we could use the same format everywhere.
1255839487,13870,dajac,2023-07-07T13:38:39Z,I think that we can remove those. It is clear that we do nothing for this state if it is not listed before. There are a few other cases.
1255840000,13870,dajac,2023-07-07T13:39:00Z,Is this used anywhere?
1255849529,13870,dajac,2023-07-07T13:45:55Z,nit: empty line.
1255851195,13870,dajac,2023-07-07T13:46:55Z,nit: empty line.
1255863985,13870,dajac,2023-07-07T13:56:07Z,We could inline this as suggested earlier.
1255864318,13870,dajac,2023-07-07T13:56:20Z,We can omit setting the error code.
1255865580,13870,dajac,2023-07-07T13:57:08Z,ditto.
1255868223,13870,dajac,2023-07-07T13:58:55Z,We need to update the javadoc here. I also wonder if we could find a better name now as it may also complete the join phase.
1255878155,13870,dajac,2023-07-07T14:05:34Z,nit: We usually use `maybe`. I understand that this comes from the old purgatories but it may be better to use the correct naming convention.
1255881371,13870,dajac,2023-07-07T14:07:59Z,nit: empty line.
1255883963,13870,dajac,2023-07-07T14:09:59Z,nit: You can replace `{}-{}` with `{}` and directly pass the TopicPartition.
1255885261,13870,dajac,2023-07-07T14:10:54Z,nit: package private for testing.
1255886494,13870,dajac,2023-07-07T14:11:43Z,nit: empty line.
1255889149,13870,dajac,2023-07-07T14:13:25Z,nit: Empty line.
1255892012,13870,dajac,2023-07-07T14:15:01Z,Why do we need a copy here?
1255894010,13870,dajac,2023-07-07T14:16:12Z,Could we expand this comment a little?
1255894583,13870,dajac,2023-07-07T14:16:34Z,We can remove this now.
1256037213,13870,jeffkbkim,2023-07-07T15:48:20Z,which illegal state exception are you referring to?
1256038506,13870,jeffkbkim,2023-07-07T15:49:17Z,"as discussed offline, we will complete both futures. the append future will be completed first and the event future will complete the join response if it's not already completed."
1256056130,13870,jeffkbkim,2023-07-07T16:02:32Z,updated to all errors. 
1256069184,13870,jeffkbkim,2023-07-07T16:08:58Z,reverted the ordering
1256110776,13870,jeffkbkim,2023-07-07T16:37:10Z,"the issue is that the records need to be generated while the group is empty. after performing `genericGroupJoinNewMember()` the group will have added the member metadata. the existing protocol only allows records for empty groups or groups that have a defined protocol.

this only applies to join group requests with `requireKnownMemberId = false` or group instance id."
1256152143,13870,jeffkbkim,2023-07-07T17:10:29Z,the main concern i had was completeGenericGroupJoin() can be invoked by the timer which would miss this conversion but i guess it's not really an issue. 
1256157423,13870,jeffkbkim,2023-07-07T17:14:57Z,i added this in case we add a new state in the future. should i remove it?
1256163265,13870,jeffkbkim,2023-07-07T17:19:38Z,isn't it more readable to keep it?
1256180440,13870,jeffkbkim,2023-07-07T17:33:38Z,in `GroupMetadataManager#expirePendingSync()` we remove members from the set while iterating
1258225411,13870,dajac,2023-07-10T12:59:31Z,I wonder if we should remove this because it will log all errors now.
1258226163,13870,dajac,2023-07-10T13:00:04Z,I am a bit confused here. Don't we need to apply this conversion to `responseFuture` as well?
1258226528,13870,dajac,2023-07-10T13:00:20Z,You can remove this one because it can't happen now.
1258226646,13870,dajac,2023-07-10T13:00:26Z,ditto.
1258228530,13870,dajac,2023-07-10T13:01:52Z,"It may be better to inline this code because the handling could be different depending on the request type. If I remember correctly, it is slightly different for offset commits for instance."
1258228818,13870,dajac,2023-07-10T13:02:07Z,Could we bring this back?
1258229353,13870,dajac,2023-07-10T13:02:32Z,nit: Should we add `throws GroupIdNotFoundException`?
1258230722,13870,dajac,2023-07-10T13:03:38Z,I think that we should rather add this to the `onLoaded` method rather than here. The issue is that it will also log all the non-compacted records and that will be misleading.
1258231101,13870,dajac,2023-07-10T13:03:55Z,nit: `data structure`?
1258231284,13870,dajac,2023-07-10T13:04:03Z,nit: remove empty line.
1258232766,13870,dajac,2023-07-10T13:05:13Z,"For my understanding, we don't fail the future here because the event future will do it. Am I correct?"
1258235016,13870,dajac,2023-07-10T13:06:59Z,I see. I wonder if we could have a `newGroupEmptyMetadataRecord` which generate an empty record for the group in this case to avoid this issue. Would this work? I am asking because I think that centralising would simplify the code.
1258236219,13870,dajac,2023-07-10T13:07:54Z,nit: `maybe...`?
1258269899,13870,dajac,2023-07-10T13:30:30Z,nit: I was considering whether we should have an helper in the JoinRequest class for this. It could be something like `JoinRequest#requireKnownMemberId(short version)`. That advantage is that it would centralize all the version handling in one place. We could do the same for the other similar cases. What do you think?
1258278071,13870,dajac,2023-07-10T13:35:54Z,"We can keep it, I suppose."
1258280008,13870,dajac,2023-07-10T13:37:15Z,"nit: Could we expand this error a little? It would be great if it could capture that it failed to update the metadata for a static member, etc."
1258280313,13870,dajac,2023-07-10T13:37:27Z,I would remove them.
1258281079,13870,dajac,2023-07-10T13:37:58Z,"nit: `"" + ""` The `+` in this case is not needed."
1258287486,13870,dajac,2023-07-10T13:42:02Z,"I just noticed that whenever we use `joinReason`, we also have the `JoinGroupRequestData`. How about adding a helper in `JoinRequest` class to get the reason from `JoinGroupRequestData`? Then, we don't have to pass it anymore to all the methods and we can remove the logic to compute it in `genericGroupJoin`. What do you think?"
1258291357,13870,dajac,2023-07-10T13:44:19Z,nit: `replayRecords`?
1258295996,13870,dajac,2023-07-10T13:47:02Z,"I was wondering if the following is simpler:

```
CompletableFuture appendFuture = result != null ? result.appendFuture() : null;
```

then

```
if (appendFuture != null) appendFuture.complete(...);`

if (appendFuture != null) appendFuture.completeExceptionally(...);`
```

I leave it up to you."
1258313403,13870,dajac,2023-07-10T13:57:05Z,Why is this false?
1258314600,13870,dajac,2023-07-10T13:57:49Z,It may be better to validate the full response here.
1258314933,13870,dajac,2023-07-10T13:58:02Z,Do we need to add tests for the errors convertion?
1258319481,13870,dajac,2023-07-10T14:01:24Z,Should we revert this?
1258320244,13870,dajac,2023-07-10T14:02:02Z,Could we revert this?
1258320551,13870,dajac,2023-07-10T14:02:16Z,Why are we changing this?
1258320790,13870,dajac,2023-07-10T14:02:26Z,nit: Empty line.
1258356121,13870,dajac,2023-07-10T14:29:27Z,This feels weird.... You pass the expected result and you alter it here. I think that it would be better to separate concerns and to return records and future to the caller and to let it do the validation.
1258356713,13870,dajac,2023-07-10T14:29:54Z,"Do we need this? If we do, we should replace `e.printStackTrace();`."
1258356890,13870,dajac,2023-07-10T14:30:03Z,ditto.
1258362905,13870,dajac,2023-07-10T14:34:12Z,Is this needed? I would have thought that the group should have been created by the first request.
1258367695,13870,dajac,2023-07-10T14:37:48Z,nit: Empty line.
1258368278,13870,dajac,2023-07-10T14:38:12Z,Is this needed as well?
1258375164,13870,dajac,2023-07-10T14:43:18Z,"Why do we need to handle the first request differently? Is it because it may generate a record? If so, I would not do this in every tests but only in one test focused on this."
1258375867,13870,dajac,2023-07-10T14:43:48Z,"nit: `IntStream.range(0, groupMaxSize + 1)` that you just used above is pretty nice. I wonder if we could use it here as well."
1258376361,13870,dajac,2023-07-10T14:44:07Z,Do we need this?
1258378853,13870,dajac,2023-07-10T14:45:53Z,ditto.
1258379838,13870,dajac,2023-07-10T14:46:31Z,nit: You should try to use the Stream API more often.
1258387036,13870,dajac,2023-07-10T14:51:10Z,"In Scala, we had `UNKNOWN_MEMBER_ID` here. Is the change expected?"
1258391017,13870,dajac,2023-07-10T14:53:33Z,"In Scala, this test runs with a dynamic member and a static member. We don't do the static part here. Why?"
1258394371,13870,dajac,2023-07-10T14:55:54Z,We were asserting the leader here.
1258396521,13870,dajac,2023-07-10T14:57:25Z,There is a `+ 1` in scala. Don't we need it here?
1258398177,13870,dajac,2023-07-10T14:58:35Z,nit: You can do: `group.allMembers().iterator().next()`.
1258400111,13870,dajac,2023-07-10T15:00:00Z,There is a + 1 in scala.
1258406471,13870,dajac,2023-07-10T15:04:08Z,There is `UNKNOWN_MEMBER_ID` in scala?
1258410034,13870,dajac,2023-07-10T15:06:24Z,Is this a new test?
1258924230,13870,jeffkbkim,2023-07-10T21:17:10Z,"this would log all errors while appending/committing and if `generateRecordsAndResponse` throws an unexpected exception. shouldn't we log them? 

it doesn't seem like we do for `consumerGroupHeartbeat()` -- maybe just filter out the coordinator not available / not coordinator error codes?"
1258933631,13870,jeffkbkim,2023-07-10T21:30:02Z,"the responseFuture if completed inside genericGroupJoin will have an error code corresponding to the join group business logic. basically, we have already completed with the appropriate error if the response future is already completed at this line.

the append future error is from the append/commit process which needs to be converted if we complete the response error here."
1258934915,13870,jeffkbkim,2023-07-10T21:31:46Z,where should i look to confirm/learn this?
1258937169,13870,jeffkbkim,2023-07-10T21:35:02Z,"confirmed that `storeGroup` and `storeOffsets` have different handling.

this will still be shared amongst join/sync/leave group, so i'll rename this to `appendGroupMetadataErrorToResponseError`, wdyt?"
1258999406,13870,jeffkbkim,2023-07-10T23:18:33Z,is your suggestion to iterate through all groups & members and log each member after loading a partition is complete?
1259002211,13870,jeffkbkim,2023-07-10T23:23:59Z,that's correct
1259029305,13870,jeffkbkim,2023-07-11T00:24:18Z,great suggestion. thanks
1259030830,13870,jeffkbkim,2023-07-11T00:28:01Z,A successful join group request will store the response future into the member's `awaitingJoinFuture` (could also complete if the join phase completes)
1259040832,13870,jeffkbkim,2023-07-11T00:43:14Z,we get illegal state exception if it's not initialized and since it doesn't affect the old protocol i thought it best to initialize it here.
1259047855,13870,jeffkbkim,2023-07-11T00:53:11Z,"this is not to create a new group (note the `createIfNotExists=false` argument) but to retrieve the group to do more validations such as group state, generation id, group size, etc.

added a helper method `genericGroup()` to simplify the calls."
1259049772,13870,jeffkbkim,2023-07-11T00:55:49Z,"for all tests, we always generate a new record. some tests hide this as it's called in `GroupMetadataManagerContext#joinGenericGroupAsDynamicMember()`.

maybe we can simplify this and just manually create an empty group for all tests except 1 where we test the new record. wdyt?"
1259054321,13870,jeffkbkim,2023-07-11T01:01:57Z,addressed in above comment.
1259059291,13870,jeffkbkim,2023-07-11T01:08:34Z,"for all places i use the old for each / for loops, there is an error
`Variable used in lambda expression should be final or effectively final`
because i reuse variables (mainly JoinGroupRequestData & responseFutures). i can use new variables instead, would that be better?"
1259062674,13870,jeffkbkim,2023-07-11T01:13:15Z,i was following the new protocol as it made more sense but i have changed to reflect the old behavior.
1259095013,13870,jeffkbkim,2023-07-11T02:11:36Z,thanks for the catch. will add it.
1259096806,13870,jeffkbkim,2023-07-11T02:14:34Z,"there's a +1 on all advance clocks in scala. i haven't actually looked but assumed that it's in place due to how the purgatory works.

the java timer implementation does not require a +1"
1259097546,13870,jeffkbkim,2023-07-11T02:16:06Z,have replied to a thread above
1259098122,13870,jeffkbkim,2023-07-11T02:17:16Z,related to when a group is not found. have reverted and updated the error code
1259098972,13870,jeffkbkim,2023-07-11T02:19:19Z,"yes, that's correct."
1259103526,13870,jeffkbkim,2023-07-11T02:28:15Z,simplified the code a bunch. thanks for the suggestion!
1259115771,13870,jeffkbkim,2023-07-11T02:53:23Z,"i'll think a bit more on this as it will require a large change in this class.

one of the reasons i had it like this is that we mostly care about the responseFuture in the tests and wanted to hide the record/append future validations. The timer could also produce records which require setting things in advance.

i agree it is unclean, i'll address this in the next commit."
1259765086,13870,dajac,2023-07-11T13:46:46Z,Yes. I would actually create a special test to validate this case and simplify all the others.
1259767721,13870,dajac,2023-07-11T13:48:47Z,nit: Empty line.
1259774315,13870,dajac,2023-07-11T13:53:31Z,Should we move `genericGroup` to the context?
1259776117,13870,dajac,2023-07-11T13:54:42Z,nit: empty line.
1259776394,13870,dajac,2023-07-11T13:54:53Z,nit: empty line.
1259779633,13870,dajac,2023-07-11T13:57:09Z,I find the helpers a bit confusing. It is not clear what's the difference between `joinGenericGroup` and `sendGenericGroupJoin` for instance. Is it possible to simplify them?
1259789131,13870,dajac,2023-07-11T14:03:57Z,How do end up in this state here? Is there some code to advance the timer to complete the prepare phase?
1259793958,13870,dajac,2023-07-11T14:07:28Z,This is really surprising. `verify*` suggests that this method only verifies something but it also has side effects. I think that this should rather be done in the context like I did for the new protocol.
1259795334,13870,dajac,2023-07-11T14:08:23Z,We need to align on this one as I also have an implementation [here](https://github.com/apache/kafka/pull/13963). They look pretty close but they are different.
1259836068,13870,dajac,2023-07-11T14:35:58Z,Do we need to add a test for this one?
1260282790,13870,jeffkbkim,2023-07-11T21:06:25Z,"`sendGenericGroup...` methods send the request and return the future.
`joinGenericGroup...` methods invoke `sendGenericGroup` methods then advance the timer to move the group to completing rebalance state."
1260298077,13870,jeffkbkim,2023-07-11T21:24:18Z,"yeah, i noticed. i don't mind using the other implementation. looks like java's priority queue does arbitrary ordering for the same priority so they should have the same behavior"
1260497268,13870,jeffkbkim,2023-07-12T02:39:33Z,"With the latest changes, i renamed `joinGenericGroup...` to ``joinGenericGroupAsDynamicMemberAndCompleteJoin` and `joinGenericGroupAndCompleteJoin` to make it more explicit.

let me know if this is more readable."
1260497352,13870,jeffkbkim,2023-07-12T02:39:42Z,replied to comment below.
1260500922,13870,jeffkbkim,2023-07-12T02:46:27Z,"removed this method, and now individual tests do the validation. 

one exception is for timer operation expirations - as the majority of the cases will not result in any records, i have done the validation inside MockCoordinatorTimer."
1263424766,13870,dajac,2023-07-14T07:56:38Z,"Yeah, I think that it depends on what we mean by unexpected. I would remove it for now given that we also log something when the append future fails. We can always bring it back later if needed."
1263429592,13870,dajac,2023-07-14T08:01:29Z,That makes sense.
1263430125,13870,dajac,2023-07-14T08:02:02Z,That seems reasonable. We can see later if we could also share this logic with the consumer group heartbeat handling.
1263432212,13870,dajac,2023-07-14T08:04:12Z,extremely small nit: Should you move `topicPartition` to the top? This is a common attribute.
1263432742,13870,dajac,2023-07-14T08:04:46Z,nit: Could you also move this one to the top of the attributes?
1263435000,13870,dajac,2023-07-14T08:07:27Z,"nit: I was wondering whether it would make sense to move this to `CoordinatorResult`. We could have a static public constant called `EMPTY`. Then, we could use `CoordinatorResult.EMPTY` in the code. This is a pattern that we already use in a few other places. What do you think?"
1263435600,13870,dajac,2023-07-14T08:08:05Z,nit: Could we revert this change?
1263436098,13870,dajac,2023-07-14T08:08:37Z,nit: Add javadoc for UnknownMemberIdException.
1263437064,13870,dajac,2023-07-14T08:09:38Z,nit: There is a constructor which does not take the response. We could use it and remove `null` here. There are a few other cases. I won't mention them again.
1263437658,13870,dajac,2023-07-14T08:10:17Z,nit: The format of the javadoc in is incorrect here.
1263439020,13870,dajac,2023-07-14T08:11:42Z,Correct. I think that you saw that in my other PR. The issue with logging here is that it will log state metadata as well and we don't want this.
1263441311,13870,dajac,2023-07-14T08:14:13Z,Is this one covered by a unit test?
1263443960,13870,dajac,2023-07-14T08:16:52Z,I was wondering if it would be better to complete the future directly here as well in order to be consistent. I think that you did this in the sync handling if I understood you correctly. What do you think?
1263446381,13870,dajac,2023-07-14T08:19:16Z,"Note: We will have to also verify the number of member after the group is loaded, I think. This is something for another PR but to keep in mind."
1263447316,13870,dajac,2023-07-14T08:20:21Z,nit: Could we prefix this one and the two others with `genericGroup`?
1263450237,13870,dajac,2023-07-14T08:22:52Z,Would it make sense to make the copy on the other side? It is a bit weird to anticipate this here because we don't do this for other accessors. I am usually tempted to return unmodifiable collections in the case to prevent this kind of issue.
1263452425,13870,dajac,2023-07-14T08:25:00Z,nit: `testJoinGroupAppend...`?
1263454213,13870,dajac,2023-07-14T08:26:50Z,nit: Could we move this next to the other final private attributes? Could we also invest private final to be consistent with the others?
1263454828,13870,dajac,2023-07-14T08:27:31Z,nit: Could we move this one back to its original place?
1263458200,13870,dajac,2023-07-14T08:30:58Z,"nit: What the reason for this? If you don't catch it, the test will also fail."
1263459890,13870,dajac,2023-07-14T08:32:34Z,nit: Let's revert this.
1263459980,13870,dajac,2023-07-14T08:32:39Z,nit: Let's revert this.
1263460377,13870,dajac,2023-07-14T08:33:01Z,nit: `null` could be removed.
1263461288,13870,dajac,2023-07-14T08:33:57Z,nit: `null` could be removed.
1263463518,13870,dajac,2023-07-14T08:36:11Z,"This catch is a bit suspicious here. I suppose that it would also catch the error thrown by `assertEquals`, no?"
1263465299,13870,dajac,2023-07-14T08:37:56Z,"nit: While we are here, would it make to normalize the name of all tests starting with `should`? They should ideally start with `test...` like the others."
1263471443,13870,dajac,2023-07-14T08:44:23Z,"It seems based on the usages of this method that only one timeouts is expected all the time. Should we enforce it as well?

More generally, I was wondering if having a `assertEmptyTimeout` helper method and using `assertEmptyTimeout(context.sleep(...))` would have a better separation of concerns. I leave this up to you."
1263905173,13870,jeffkbkim,2023-07-14T16:11:41Z,that forces the CoordinatorResult class to become non-generic which i don't think we want.
1263922655,13870,jeffkbkim,2023-07-14T16:30:21Z,then do you think we can move `appendGroupMetadataErrorToResponseError` back to GroupMetadataManager?
1263925546,13870,jeffkbkim,2023-07-14T16:33:50Z,"to confirm, you're saying we should call `acceptJoiningMember` while loading members?"
1264075041,13870,jeffkbkim,2023-07-14T19:22:59Z,will keep it as assertEmptyResult as the timeout is not empty (can be) but we want to assert that the coordinator result is.
1264953365,13870,dajac,2023-07-17T07:04:25Z,ah.. did not think about that.
1264954924,13870,dajac,2023-07-17T07:06:34Z,"yeah, possibly."
1264955867,13870,dajac,2023-07-17T07:07:49Z,No. I think that the current coordinator triggers a rebalance if the number of members is higher than the max when a group is loaded.
1264961072,13870,dajac,2023-07-17T07:13:31Z,Do we really need to keep the try..catch?
1264962406,13870,dajac,2023-07-17T07:14:49Z,nit: `assertNoOrEmptyResult`?
1264964347,13870,dajac,2023-07-17T07:17:13Z,This request timeout was coming from the delayed produce op in the purgatory. We don't have this anymore.
1264965443,13870,dajac,2023-07-17T07:18:40Z,This was not addressed.
1264967746,13870,dajac,2023-07-17T07:21:18Z,This was not addressed.
1264968675,13870,dajac,2023-07-17T07:22:25Z,"Yeah, we have to stick to the old one here."
1265547819,13870,jeffkbkim,2023-07-17T15:32:01Z,i was thinking about the illegal state exceptions. wouldn't we hide the issue then? maybe we can log only for non api exceptions. wdyt?
1265554449,13870,jeffkbkim,2023-07-17T15:37:23Z,this is required if we want to use the streams api. let me know if we should just use the for each loop
1265574666,13870,jeffkbkim,2023-07-17T15:54:25Z,thought i addressed this. addressed it now
1265577548,13870,jeffkbkim,2023-07-17T15:56:48Z,changed to logging only when the response future is not complete
1265644331,13870,jeffkbkim,2023-07-17T16:51:05Z,updated and added a test case
1265807220,13870,dajac,2023-07-17T19:25:12Z,"This would still log in expected cases, no? For instance, when the coordinator for the group is inactive, loading, etc. If you really want to log something, you could perhaps log only if `exception` is not a KafkaException or only when it is a RuntimeException for instance."
1265807612,13870,dajac,2023-07-17T19:25:41Z,gotcha. it is fine like this.
1265816192,13870,jeffkbkim,2023-07-17T19:35:47Z,ah makes sense. logging only when it is not a kafka exception makes sense.
1266079310,13870,jeffkbkim,2023-07-18T01:36:33Z,@dajac i think our last discussion was to also revert this to the existing behavior right? i.e. not implement https://issues.apache.org/jira/browse/KAFKA-13766
1266264686,13870,dajac,2023-07-18T05:59:19Z,@jeffkbkim That's correct.
1266267667,13870,dajac,2023-07-18T06:00:53Z,Should we replace this by a constant if we can't change it based on config?
1266269140,13870,dajac,2023-07-18T06:02:42Z,"So we actually need to reschedule the timer here, right?"
1266270587,13870,dajac,2023-07-18T06:04:21Z,nit: Could we say `JoinGroup request {} hit....`?
1267070768,13870,jeffkbkim,2023-07-18T17:00:13Z,yes. updated
565612146,9944,jolshan,2021-01-27T20:29:10Z,I think I may want to do this in a simpler way. I want to keep track if we have IDs for all the topics and I'm not sure if there is a better way to figure out when a topic is no longer in a session besides checking all the topic partitions.
566449639,9944,rajinisivaram,2021-01-28T22:26:23Z,We don't use `get` prefix for getters
566449805,9944,rajinisivaram,2021-01-28T22:26:42Z,Comment needs updating?
566453611,9944,rajinisivaram,2021-01-28T22:34:18Z,"We can use Integer::sum as the last arg, but do we even need to maintain `partitionsPerTopic`?"
566458667,9944,rajinisivaram,2021-01-28T22:45:05Z,Could just parameterize `findMissing`?
566464590,9944,rajinisivaram,2021-01-28T22:57:33Z,"There are several places where we use this combination of two maps, should we create a class that maintains a bidirectional map?"
566465893,9944,rajinisivaram,2021-01-28T23:00:16Z,Can we end up with cases with some topics with ids and some without?
566466426,9944,rajinisivaram,2021-01-28T23:01:33Z,The fact that you are running this code implies `ApiKeys.FETCH.latestVersion() >= 13`?
567146583,9944,jolshan,2021-01-29T23:21:36Z,Some of these tests may be flaky so I'm going to keep an eye on them.
567172089,9944,rajinisivaram,2021-01-30T01:12:02Z,The whole FetchRequest class is quite hard to follow without reading the KIP and looking at multiple places. It will be good to add some comments at the class level.
567172368,9944,rajinisivaram,2021-01-30T01:13:50Z,"Since we have session ids and topic ids in the context of a fetch request, we should probably qualify `TopicId`"
567172755,9944,rajinisivaram,2021-01-30T01:15:54Z,`this.partitions.addAll(partitions)`?
567173386,9944,rajinisivaram,2021-01-30T01:19:17Z,Does one non-zero id mean we have all ids?
567173468,9944,rajinisivaram,2021-01-30T01:19:41Z,This suggests we can have a combination of zero and non-zero?
567173993,9944,rajinisivaram,2021-01-30T01:22:18Z,It will be good to see if can separate out new and old forms of FetchRequest/Response. It is not a big deal since it is just wrapping the protocol layer.
567174143,9944,rajinisivaram,2021-01-30T01:23:34Z,Shouldn't we be using versions and expect non-zero ids in new versions?
567174731,9944,rajinisivaram,2021-01-30T01:27:08Z,We need to remember to set this based on which version this is being merge to.
567175350,9944,rajinisivaram,2021-01-30T01:30:45Z,nit: indentation
567175593,9944,rajinisivaram,2021-01-30T01:32:32Z,Does an unresolved partition have all these fields populated? Or do we have it here because the topic may be resolved later?
567175893,9944,rajinisivaram,2021-01-30T01:34:49Z,Is this part intentionally commented out?
567188080,9944,jolshan,2021-01-30T03:13:51Z,No that should be removed :)
567188108,9944,jolshan,2021-01-30T03:14:14Z,We need to keep the data from the fetch request for when we resolve the partition.
567188287,9944,jolshan,2021-01-30T03:16:00Z,The idea is that we should only be able to send this request version if we had an id for each topic. I do need to take a closer look at this
567188652,9944,jolshan,2021-01-30T03:19:22Z,I had trouble getting the version into the response. The constructor is used in some places where we don't have access to the version.
567188957,9944,jolshan,2021-01-30T03:22:37Z,That's true
567189013,9944,jolshan,2021-01-30T03:23:35Z,I think this was a mistake. I need to see why I wrote it this way.
567189128,9944,jolshan,2021-01-30T03:24:54Z,I should also move that comment (and maybe simplify it) to the PartitionIterator where I moved the code for removing partitions with stale ids.
568033865,9944,jolshan,2021-02-01T18:10:10Z,"I remember why I did this. I wanted to not get a set of the zero ID when the version was old. I think if we are able to get better versioning logic, this should be fixed easily."
568034891,9944,jolshan,2021-02-01T18:11:54Z,AbstractResponse does not maintain version like AbstractRequest. So I'm not sure the best way to proceed with this.
568078414,9944,jolshan,2021-02-01T19:20:40Z,"I'm thinking it may be possible if we had a response from a broker that supported topic IDs and then a response from one that did not. Of course, this should eventually get resolved, but I didn't know if it was worth it to try to avoid fetches that are unsupported in a few more cases."
568140154,9944,junrao,2021-02-01T21:07:54Z,space after comma
568196064,9944,junrao,2021-02-01T22:51:09Z,getTopicIds => topicNamesToIds?
568196201,9944,junrao,2021-02-01T22:51:32Z,getTopicNames => topicIdsToNames?
568201144,9944,junrao,2021-02-01T22:58:56Z,"Hmm, why do we need to do collect() at the end? The returned value doesn't seem be be used."
568201276,9944,junrao,2021-02-01T22:59:07Z,"Hmm, why do we need to do collect() at the end? "
568216529,9944,junrao,2021-02-01T23:34:43Z,It's a bit weird to add a comment that breaks the if/else clause. Perhaps we could put the comment inside the `else if`?
568222532,9944,junrao,2021-02-01T23:50:06Z,"It seems that the following code makes changes to unresolvedPartitions, not topic IDs."
568223807,9944,junrao,2021-02-01T23:53:42Z,Should we change `hashCode() `and `equals()` to include topicId?
568230940,9944,junrao,2021-02-02T00:12:40Z,What's the definition of 'interesting'?
568231067,9944,junrao,2021-02-02T00:13:05Z,"Should we add the new params to the javadoc above? In particular, could we explain the relationship between responseData and idErrors? Also, could we name the params clearer? For example, responseData => partitionsWithMatchingTopicId, idErrors => partitionsWithoutMatchingTopicId."
568832800,9944,junrao,2021-02-02T18:26:24Z,"Since we are adding some complexity, it would be useful to make the code a bit easier to understand for other people. For example, perhaps we could add comments to explain (1) what partitions will be included in unresolvedPartitions vs partitionMap? (2) are partitions mutually exclusive between unresolvedPartitions and partitionMap? (3) how are partitions in unresolvedPartitions and partitionMap handled different for fetch response?"
569824849,9944,junrao,2021-02-03T23:29:26Z,topicNames => topicIdToNameMap?
569829415,9944,junrao,2021-02-03T23:41:13Z,Is there a reason to use 0 instead of the default capacity for the hashmap?
569830803,9944,junrao,2021-02-03T23:45:03Z,We could probably just get rid of session since it's part of the session object. Ditto below.
569832071,9944,junrao,2021-02-03T23:48:16Z,"Now that the constructor code is a bit more now, perhaps we could just forward `Builder()` to `Builder(int initialSize, boolean copySessionPartitions) `?"
569832450,9944,junrao,2021-02-03T23:49:14Z,id => topicId ?
569834804,9944,junrao,2021-02-03T23:55:33Z,"Hmm, we should be adding tp.partition() to the hashset and not using it for the initial capacity, right?"
569862273,9944,junrao,2021-02-04T00:49:57Z,It seems that topicNames is unused?
570397481,9944,junrao,2021-02-04T17:11:52Z,"It seems this is about a topic. So, UnresolvedPartitions is better named as UnresolvedTopic?"
570588361,9944,junrao,2021-02-04T22:29:45Z,"Hmm, not sure if we need to distinguish here. It seems that it's easier to just always send a UNKNOWN_TOPIC_ID since the propagation of all topic Ids could be delayed?"
570596351,9944,junrao,2021-02-04T22:46:08Z,"Now that we are changing the semantics for this method to only iterating resolved partitions, it would be useful to have a more appropriate method name to make it clear.

Also, it seems that some of the callers need to iterate all partitions including unresolved ones (e.g., those checking for CLUSTER ACTION permissions) while some others need to iterate resolved ones (e.g, those checking for topic level permissions)."
570606630,9944,junrao,2021-02-04T23:08:08Z,"This method is kind of weird. It's only used in KafkaApis where topic name has already been resolved. The only reason for this method is that FetchContext.updateAndGenerateResponseData() generates FetchResponse, which is used in createResponse(). Instead, could we have FetchContext.updateAndGenerateResponseData() return a different class that includes the resolved partitions?"
570607999,9944,junrao,2021-02-04T23:11:22Z,Perhaps we could log both the resolved partitions' size and unresolved partitions' size.
570648997,9944,junrao,2021-02-05T00:58:06Z,It's kind of weird to pass in a request into FetchSession.
570650572,9944,junrao,2021-02-05T01:02:52Z,"Hmm, do we need to check version here? FetchResponse.fetchDataAndError() already checked the version."
570652174,9944,junrao,2021-02-05T01:07:51Z,"Could we name the methods better to make it easier to understand? For example,

createNewSession => generateResolvedPartitions
createNewSessionIdErrors => generateUnresolvedPartitions"
572203478,9944,junrao,2021-02-08T16:52:18Z,The metadata cache could change between the two calls. Could we have a single call to metadata cache that returns both topicNames and topicIds?
572208853,9944,junrao,2021-02-08T16:58:58Z,"`topicIds.getOrDefault(part.topic(), Uuid.ZERO_UUID)`

If we always expect the topic to be found in topicIds, we should just throw an exception instead of using a default. If this is expected, we probably should convert it to an unresolved partition?"
572218700,9944,junrao,2021-02-08T17:12:09Z,It seems that toForgetIds is intended for unresolved topicIds. Could we name it more clearly together with toForget and add some comment?
572220577,9944,junrao,2021-02-08T17:14:48Z,"I am not sure that I follow the logic here. It seems that we always put the forgot topic into unresolvedIds. It seems that we should check the partitions size? Also, perhaps rename partitions to sth like unresolvedPartitions?"
572228756,9944,junrao,2021-02-08T17:24:43Z,Should we use mustAdd()?
572235515,9944,junrao,2021-02-08T17:33:23Z,Do we need this part of the logic? It seems that the same is already done through FetchReqeust.fetchDataAndError().
572274967,9944,junrao,2021-02-08T18:31:40Z,"This excludes the partition in the response. However, it seems we need to send an error back for this partition?"
572281866,9944,junrao,2021-02-08T18:42:34Z,This seems unused?
572287008,9944,junrao,2021-02-08T18:50:42Z,Could we add the new param to the javadoc?
572308958,9944,junrao,2021-02-08T19:25:33Z,Should we use the latest version or fetchRequestVersion guarded by IBP?
572350818,9944,jolshan,2021-02-08T20:31:46Z,Sorry this was unclear. I meant changes involving topic IDs. I will adjust this comment.
572351801,9944,jolshan,2021-02-08T20:33:35Z,'interesting' was the name of the map of partitionData. I believe they are topic partitions that are authorized and exist.
572351945,9944,jolshan,2021-02-08T20:33:51Z,"Sounds good to me
"
572354491,9944,jolshan,2021-02-08T20:37:24Z,I think I was just matching `sessionPartitions` above
572355958,9944,jolshan,2021-02-08T20:39:51Z,Yeah. Good catch. I'm going to experiment with this code a bit to see if it's faster to maintain this set or just get a set of topics from the map of topic partitions in` FetchRequestData`
572356813,9944,jolshan,2021-02-08T20:41:16Z,"The reason I name it this is we maintain such an object for each partition that was unresolved. If we simply have one object per topic, we would need a way to know all the partitions for the topic that were requested."
572357857,9944,jolshan,2021-02-08T20:43:16Z,"I've gone back and forth on this. One one hand, you are right that this is confusing in the case where we are doing and upgrade and ID propagation is delayed. On the other hand, in the non-upgrade case, returning an UNKNOWN_TOPIC_ID error when topic IDs are not even supported might not be as informative."
572363402,9944,jolshan,2021-02-08T20:52:25Z,"I agree. I think it stems from exactly what you said...that  `FetchContext.updateAndGenerateResponseData()` generates a response only for it to be generated again. Currently ` FetchContext.updateAndGenerateResponseData()` does include all partitions (resolved and unresolved). The issue is that the partitions need to be down-converted. The way this works is that the partitions are pulled from the FetchResponse object itself. However, the issue is that I've changed responseData and since this is the newest version of the response, it will try to reconstruct the map instead of pulling the object `partitionData`. (Which is too slow) I thought about changing the method to always return the map when it is not null, but that caused some issues in some places as well. I can look into this again though."
572367849,9944,jolshan,2021-02-08T20:59:54Z,This is for adding unresolved partitions in the session but not in the request. I can add comments to clarify what is happening.
572370494,9944,jolshan,2021-02-08T21:04:38Z,"Good point, it should probably be along the lines of  `if (fetchRequestVersion >= 13 && !fetchData.canUseTopicIds) 12 else fetchRequestVersion` to match how it is sent below."
572373623,9944,jolshan,2021-02-08T21:09:56Z,"If we run into this scenario, does it make sense to always return with an UNKNOWN_TOPIC_ID error? Sometimes partitions will be skipped over anyway when `mustRespond` is false, so should those also return UNKNOWN_TOPIC_ID?"
572377972,9944,jolshan,2021-02-08T21:13:56Z,"I realize this is a bit confusing. addPartitions method takes a list
What this line is doing is grabbing the idError object and adding partitions to it."
572381787,9944,jolshan,2021-02-08T21:20:43Z,"Ah. This is confusing due to how I named things. Basically, I'm collecting a set of partitions `partitions` for a given topic where the ID was not resolved. Then I'm adding them to unresolvedIds. This is a mapping from the topic ID to all the partitions that should be forgotten. I can rename and add comments to clarify what is happening here."
572413017,9944,jolshan,2021-02-08T22:11:24Z,"I thought about this, but I was worried about some weirdness where we need to support partitions before and after they have an id. (The partitions are techincally equivalent, but equals wouldn't reflect that) This may also cause problems in cases where IDs may change. Consider the code ` val cachedPart = session.partitionMap.find(new CachedPartition(topicPart))`  This is used in the path of deleting partitions with stale IDs. We would need to know the topic ID to find the partition here.  I could see potential issues where we no longer have the ID and would have trouble removing it."
572470829,9944,jolshan,2021-02-09T00:21:46Z,When you mention that some callers need to iterate over all partitions like CLUSTER ACTION -- I'm a little confused. I thought the request context was passed into AuthHelper for that.
572472724,9944,jolshan,2021-02-09T00:25:21Z,I think we do since this is looking at the partitions cached in the session. I'll take another look though.
572475014,9944,jolshan,2021-02-09T00:27:59Z,I think this is used for handling the case of older request versions.
572512463,9944,junrao,2021-02-09T02:00:07Z,"I was referring to the following code. It seems to need to iterate every partition through fetchContext so that the UNKNOWN_TOPIC_OR_PARTITION error code can be added for each partition.

```
      if (authHelper.authorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {
        fetchContext.foreachPartition { (topicPartition, data) =>
          if (!metadataCache.contains(topicPartition))
            erroneous += topicPartition -> errorResponse(Errors.UNKNOWN_TOPIC_OR_PARTITION)
```"
573033865,9944,jolshan,2021-02-09T16:28:11Z,Ok. I understand now. I think in this case though the expected behavior is to return UNKNOWN_TOPIC_ID if the topic ID is unresolved. 
573046669,9944,jolshan,2021-02-09T16:43:18Z,"I just realized that fetchRequest.latestAllowedVersion is the version we use to build the request (latest and earliest are the same.) But this is a bit confusing, so I'll probably just use the logic mentioned above."
573102028,9944,junrao,2021-02-09T17:53:19Z,"Yes, we want to return UNKNOWN_TOPIC_ID for all partitions in this case. The unresolved partitions in this PR could also return the unsupported error code."
574846728,9944,jolshan,2021-02-11T21:42:38Z,"Get rid of the word session in the variable name? I was following sessionPartitions as above. 
Or get rid of this altogether?"
574909648,9944,jolshan,2021-02-11T23:57:11Z,I'm trying to think if we could have a situation where the partition already exists but without topic ID. (Like an older version of the request was sent previously?) Maybe I could check if it was not already added like how it is done below for resolved partitions added to the session.
592335738,9944,chia7712,2021-03-11T12:54:16Z,I don't find any usage of this method. Is it essential to keep this map in `topicNames`?
592360300,9944,chia7712,2021-03-11T13:30:13Z,"It seems this method is used by server only. Is it possible to have empty topic name when `KafkaApis` calls it? If not, we can remove this empty check and replace this method by `toResponseDataMap`"
592520900,9944,jolshan,2021-03-11T16:41:47Z,"I think I added it for completeness, but perhaps we don't need it."
592524660,9944,jolshan,2021-03-11T16:46:21Z,"Yes. We may have topic IDs that could not be resolved. These topics will not have a topic name. I agree that this isn't the cleanest solution, but it was the one that worked while keeping TopicPartitions as keys for the map."
592530926,9944,chia7712,2021-03-11T16:53:36Z,"Maybe we should remove the usage of this method first.

Is it useful if we make all callers use auto-generated data (topic and partition) instead of this map (TopicPartition)?
"
592536126,9944,jolshan,2021-03-11T16:59:57Z,"I can remove it. I was keeping it as a placeholder for now. I was hoping to find a way to not regenerate the map that we just passed in to make the FetchResponse. 

I'm also not quite sure what you mean by ""auto-generated data"""
592542384,9944,chia7712,2021-03-11T17:08:03Z,"> I'm also not quite sure what you mean by ""auto-generated data""

Make all callers use FetchResponseData instead of Map[TopicPartition, ...]. In other words, FetchResponse does not return TopicPartition anymore. "
592546512,9944,jolshan,2021-03-11T17:13:17Z,Ah I see. I'd have to take a look. I'm wondering if we would still need to convert to topic names for certain methods. 
592761061,9944,jolshan,2021-03-11T22:16:27Z,"Yeah, so for example, `TopicPartition`s are given to `ReplicaManager.fetchMessages` where it is used heavily. I think we still want the map for now. 

One thing I was confused about with this code is why we generate `unconvertedFetchResponse` with `updateAndGenerateResponseData`. We pass in the map to create `unconvertedFetchResponse`. With the optimization to lazily compute the map, recompute it a few lines of code later in `createResponse`. I'm hoping to avoid this somehow. (I think the optimization is great everywhere else though and I was heading in that direction originally with this PR)"
592885424,9944,chia7712,2021-03-12T03:27:47Z,"> One thing I was confused about with this code is why we generate unconvertedFetchResponse with updateAndGenerateResponseData. We pass in the map to create unconvertedFetchResponse. With the optimization to lazily compute the map, recompute it a few lines of code later in createResponse. I'm hoping to avoid this somehow. (I think the optimization is great everywhere else though and I was heading in that direction originally with this PR)

you are right. I feel it is the side-effect caused mixing `TopicPartition` and generated code in production. The callers have to create a heavy copy to update some fields of passed data. 

BTW, that is what https://github.com/apache/kafka/pull/10269 want to resolve. The PR makes all code use auto-generated data so they can update fields of passed auto-generated data instead of creating a copy.
"
593283525,9944,jolshan,2021-03-12T16:04:20Z,Got it. I think the issue here then is that some of the information can not be auto-generated data. We need topic names for certain methods but names will not be in the auto-generated data. 
593293236,9944,jolshan,2021-03-12T16:17:49Z,"I suppose we could set them server side, but when we iterate though the topics, we would need to not include those whose topic IDs could not be resolved."
593737385,9944,chia7712,2021-03-13T11:08:59Z,"As we resolve all IDs before check permission, does it produce `TOPIC_AUTHORIZATION_FAILED` with topic name even if it has no read permission to such topic (line#717)?"
593738472,9944,chia7712,2021-03-13T11:20:21Z,remove those code?
593739424,9944,chia7712,2021-03-13T11:29:33Z,not sure whether this idea is valid. Could we resolve all ids to names and then keep using previous code to handle fetch requests?  for example: the code of deleting topic resolve ids to names first and then process the deletion by names. That is a good pattern to me as we don't need to trace id/name everywhere.
593771946,9944,jolshan,2021-03-13T16:35:08Z,"For the most part, this is what we do. But we also have to keep track of unresolved names right?  FetchSession makes this more complicated. We may get a topic ID that we can't resolve, but with a subsequent update we can. I'm not 100% sure of the logistics of fetch sessions, but it doesn't seem like a good practice to leave unresolved IDs out of the session."
593772326,9944,jolshan,2021-03-13T16:37:56Z,"I would think so? The goal was for when the IDs are resolved, the path would be the same. And I think we decided that TOPIC_AUTHORIZATION_FAILED is the correct error when we don't have read permissions?"
593842004,9944,chia7712,2021-03-14T04:48:57Z,Sorry for my unclear comment. My point was that the topic name is included by the response even if there is no read permission to resolve the topic is. That looks like a security issue that we expose the topic name.
593922148,9944,jolshan,2021-03-14T16:01:06Z,Ah right. I will fix that.
595284658,9944,jolshan,2021-03-16T15:31:23Z,I went to fix this but I realized something. Correct me if I'm wrong. The newest version of Fetch will only return topic IDs (not topic names) so we won't expose topic names. The older version of Fetch sends topic names so it will have to send topic names back. I don't think the name is ever incorrectly exposed.
603506960,9944,jolshan,2021-03-29T18:09:58Z,Removed this object
603507748,9944,jolshan,2021-03-29T18:11:16Z,I simplified this path as well.
617932245,9944,junrao,2021-04-21T22:26:04Z,extra newline.
622349780,9944,junrao,2021-04-28T16:32:41Z,id => topicId ?
622349941,9944,junrao,2021-04-28T16:32:54Z,id => topicId ?
625164696,9944,jolshan,2021-05-03T15:23:38Z,"With the top level error change, we are no longer marking the partitions as `partitionsWithError`. I'm wondering if this is something we still want to do somehow to allow the broker's metadata time to update."
625372843,9944,junrao,2021-05-03T21:15:05Z,"When will topicResponse.topic() be """"?"
625422142,9944,junrao,2021-05-03T23:09:32Z,an situation => a situation
625429072,9944,junrao,2021-05-03T23:30:12Z,"It's a bit weird for `forgottenTopics()` to have a side effect that changes the internal data structure since requests are typically immutable. It's all not consistent with toPartitionDataMap(). If we do want to modify the internal data structure, we probably want to name the method more properly.

Also, why do we return a list of ForgottenTopic instead of list of TopicPartition? The latter is easier to understand and it doesn't seem that we need topicId in ForgottenTopic,"
625431122,9944,junrao,2021-05-03T23:36:39Z,no need for extra new line.
625432217,9944,junrao,2021-05-03T23:40:13Z,version seems unused?
625433357,9944,junrao,2021-05-03T23:43:56Z,"Hmm, it seems that we can't pass in an empty topicIds since partition iterator is not empty?"
625440965,9944,junrao,2021-05-04T00:09:25Z,Does topicId need to be a var?
625443921,9944,junrao,2021-05-04T00:19:34Z,"Hmm, if the topicId has changed, it seems that we should send an error (e.g. InvalidTopicId) back to indicate the topicId is no longer valid so that the client could refresh the metadata?"
625446144,9944,junrao,2021-05-04T00:27:26Z,Could we explicitly define the type of topicNames ?
625451257,9944,junrao,2021-05-04T00:46:05Z,The calculation of version is duplicated between here and ReplicaFetcherThread. Could we share them somehow?
625454055,9944,junrao,2021-05-04T00:56:19Z,"Since there is only a single tp, does topics need to be a set?"
626749507,9944,jolshan,2021-05-05T17:09:36Z,I ended up deciding to end the session and throw a top level error when we have an unknown topic ID.
626894717,9944,junrao,2021-05-05T21:01:53Z,"Since toPartitionDataMap() handles all versions, could we just simply call toPartitionDataMap()? Then, I am not sure if we need to call toPartitionDataMap() in the constructor."
626897100,9944,junrao,2021-05-05T21:06:16Z,It seems it will be clearer if we put this in an else clause.
626899596,9944,junrao,2021-05-05T21:11:05Z,It seems that we could share the code to populate this.data and this.metadata. We only use `this` in this method. Should we just remove it?
626906972,9944,junrao,2021-05-05T21:24:45Z,Do we need this method? It seems it's the same as toResponseDataMap().
626907992,9944,junrao,2021-05-05T21:26:38Z,Could we add the new param to javadoc?
626945454,9944,junrao,2021-05-05T21:55:31Z,It seems that this can just be a local val instead of an instance val?
626947091,9944,junrao,2021-05-05T21:57:30Z,Do the new fields need to be included in toString()?
626952683,9944,junrao,2021-05-05T22:09:42Z,Could we build the full toSendTopicIds and toSendTopicNames once and reuse in both full and incremental?
626953162,9944,junrao,2021-05-05T22:10:50Z,The above comment needs to be changed accordingly.
626954746,9944,junrao,2021-05-05T22:14:32Z,Perhaps extra and omitted should be extraPartition and omittedPartitions to make it clear?
626958184,9944,junrao,2021-05-05T22:23:00Z,Could we add the javadoc for the new param?
627026623,9944,jolshan,2021-05-06T01:58:33Z,Realized this was no longer the case and removed in the most recent commit.
627027261,9944,jolshan,2021-05-06T02:00:45Z,"I originally did this when dealing with unresolved partitions. I was wondering if it would be better to not create a second data structure. If creating another structure (as done before) is not a problem, we can go back to that."
627027404,9944,jolshan,2021-05-06T02:01:20Z,ah good catch on this.
627514050,9944,jolshan,2021-05-06T15:15:14Z,"This is a good point. In general, I think I need to go through the session logic for handling different scenarios. (What happens when we have a session with different version requests--should we allow that to happen, etc) Depending on this, we may want topicId to be a var (to update the ID when we change request versions). I'll write up a summary of the logic I'm thinking of when I get it worked out."
627516643,9944,jolshan,2021-05-06T15:18:19Z,The difference is that we already have the topic names here. I added it for the optimization when we build the response data map again in KafkaApis (so we don't have to look up the topic ids again). But it is a little silly. One option is just to put that logic in the one place it is used instead of having such a method. 
627518693,9944,jolshan,2021-05-06T15:20:46Z,"Yeah. That seems cleaner. So the idea is that the constructor, won't set fetchData for either version."
627519381,9944,jolshan,2021-05-06T15:21:33Z,"As mentioned below, we can simply not set fetchData here and use the other two assignments."
627522339,9944,jolshan,2021-05-06T15:25:02Z,Would this work if we added a new topic to the session? I would think that we would need to add the new topic's info and the map is unmodifiable. Please correct me if this is not the case.
627523117,9944,jolshan,2021-05-06T15:26:00Z,"I think the original idea was that I wanted to make sure that we don't go from request version 12 up to version 13 in the same session. But in general, I need to rethink this whole approach, so this will very likely change. As mentioned in another comment, I'll write up the logic I'm thinking about so we are on the same page."
627523900,9944,jolshan,2021-05-06T15:26:58Z,"I wasn't sure on this, but I can add them. Maybe it makes sense just to add topicIds though? (not both maps)"
627723205,9944,jolshan,2021-05-06T19:56:00Z,Ah another artifact from before. Good catch.
627724561,9944,jolshan,2021-05-06T19:58:26Z,Ah wait. The iterator is empty here. We create a new one `(new FetchSession.RESP_MAP).entrySet.iterator` and do not use `updates`. This is because the session is an error one.
627767360,9944,jolshan,2021-05-06T21:11:12Z,rewrote this comment to be more concise.
627776011,9944,jolshan,2021-05-06T21:27:29Z,"Ah I realize that since this is a single topic, we can change this logic."
627840199,9944,jolshan,2021-05-07T00:05:48Z,Decided that we can use either version throughout the course of a session. Removed instance val
628405414,9944,jolshan,2021-05-07T17:55:55Z,"After running through some tests I realized why this didn't work. We can go from version 13 to version 12 within the session, but we can't go from 12 to 13. This is because we have may have topics without IDs in the session. We will try to return them using version 13 and they are all zero UUID. (We also have this issue when we send a full request version 12 and the subsequent request is empty. We could try to send version 13 request since we vacuously have IDs for all topics in the request, but if we do have responses for the topics, then we will try to send them back without topic IDs) If we tried to resolve them, we may end up in a case where there is no valid ID and also no way to communicate this (since we send back IDs). So I think we do need to store the state of the previous request version in the session."
648811500,9944,jolshan,2021-06-10T02:49:34Z,"I noticed that I get topic IDs from metadata here and in the replica fetcher thread, I get from the metadata cache. I don't think it is a big deal since we add to the fetchData using the same source, but it might make sense to use FetchRequestData's topicIds() instead."
649585182,9944,junrao,2021-06-10T23:00:31Z,Space after forEach.
649603470,9944,junrao,2021-06-10T23:57:50Z,from topic name for topic ID => from topic name to topic ID ?
649607001,9944,junrao,2021-06-11T00:09:33Z,Should we include topicId in hashCode() and equals()?
650157796,9944,junrao,2021-06-11T17:31:30Z,Do we need to handle UNKNOWN_TOPIC_ID here too?
650161415,9944,junrao,2021-06-11T17:38:16Z,"Since metadataSnapshot could change anytime, it's more consistent if we save a copy of metadataSnapshot and derive both maps from the same cached value."
650163078,9944,junrao,2021-06-11T17:41:14Z,Could we use case to avoid unnamed reference _._2 to make it easier to read?
650165036,9944,junrao,2021-06-11T17:44:44Z,"Since metadatache could change, it's probably slightly better to get topicIdsToNames and topicNamesToIds once from metadatache so that they are consistent."
650166656,9944,junrao,2021-06-11T17:47:42Z,Is the test ApiKeys.FETCH.latestVersion >= 13 necessary? This code is added when we introduce version 13 as the latest fetch version.
650169478,9944,junrao,2021-06-11T17:52:40Z,Is the test ApiKeys.FETCH.latestVersion >= 13 necessary? This code is added when we introduce version 13 as the latest fetch version.
650612574,9944,jolshan,2021-06-14T01:59:16Z,734fd7f fixes this
651075127,9944,junrao,2021-06-14T15:55:51Z,The KIP talks about bootstrapping the topicId for the metadata topic. Is that part done already? I don't see it included in this PR.
651086239,9944,junrao,2021-06-14T16:10:24Z,"If we get FetchSessionTopicIdException, the existing session is going to be invalid. So, it seems that we should start a new session? The same thing seems to apply to UnknownTopicIdException"
651096649,9944,junrao,2021-06-14T16:24:39Z,Could we do the topicIds part once after the if/else block to avoid duplication?
651099635,9944,junrao,2021-06-14T16:28:37Z,Does this work with topic recreation? Will a client be stuck with the old topicId when topic is recreated?
651124339,9944,junrao,2021-06-14T17:04:08Z,Should we set sessionTopicIds and sessionTopicNames to empty map if canUseTopicIds is false?
651163378,9944,junrao,2021-06-14T18:03:14Z,"> After running through some tests I realized why this didn't work. We can go from version 13 to version 12 within the session, but we can't go from 12 to 13. This is because we have may have topics without IDs in the session. We will try to return them using version 13 and they are all zero UUID. (We also have this issue when we send a full request version 12 and the subsequent request is empty. We could try to send version 13 request since we vacuously have IDs for all topics in the request, but if we do have responses for the topics, then we will try to send them back without topic IDs) If we tried to resolve them, we may end up in a case where there is no valid ID and also no way to communicate this (since we send back IDs). So I think we do need to store the state of the previous request version in the session.

I am wondering if this solves the problem completely. The decision to use version 13 fetch request also depends on the Kafka version on the broker. So, even if the client has all topic ids, the client may still send version 12 fetch requests to a broker. So, canUseTopicIds doesn't accurate capture the state whether a version 13 fetch request has been used.

Another possibility is to handle the switching from version 12 to 13 of the fetch requests on the server side in FetchSession. FetchSession already stores usesTopicIds. So, if usesTopicIds is false and a fetch request passes in topicId, we could send an error to the client to force the client to establish a new session. If we do this, we probably don't need to cache the canUseTopicIds in client fetch session. We can just calculated canUseTopicIds independently for each request. Will this approach be better?"
651170786,9944,junrao,2021-06-14T18:15:00Z,It might be useful to include the topic name and topic id (old and new) for those inconsistent topic IDs.
651293905,9944,junrao,2021-06-14T21:35:11Z,"maxVersion is not necessary the exact version used for Fetch request. The exact version is determined in NetworkClient.doSend() based on the response of ApiVersions. So, here, it seems that we need to pass in the exact version number?"
651294705,9944,junrao,2021-06-14T21:36:47Z,Should we force close the FetchSession in this case too?
651309361,9944,junrao,2021-06-14T22:05:50Z,"Since toPartitionDataMap() is only called here, should we just inline it here?"
651310206,9944,junrao,2021-06-14T22:07:39Z,Should we document FETCH_SESSION_TOPIC_ID_ERROR too?
651310982,9944,junrao,2021-06-14T22:09:16Z,Should UNKNOWN_TOPIC_ID be FETCH_SESSION_TOPIC_ID_ERROR now?
651314171,9944,junrao,2021-06-14T22:16:16Z,Should we just inline toResponseDataMap() here?
651314503,9944,junrao,2021-06-14T22:17:04Z,We choose to cache responseData here but not in FetchRequest. Is there a particular reason for this inconsistency?
651329577,9944,jolshan,2021-06-14T22:53:56Z,"Are you referring to creating a new topic ID for the metadata topic? For now, we are simply using the sentinel ID. "
651331247,9944,jolshan,2021-06-14T22:58:05Z,This happens inside of `fetchSessionHandler.handleResponse`. We set the session to close upon the next request. The code path for Fetcher is slightly different so it made sense for that code to have it there.
651331636,9944,jolshan,2021-06-14T22:59:05Z,"This is no longer a partition level error. We can only get it as a top level error. If it is a top level error, I believe we return an empty map and do not go down this code path. "
651332118,9944,jolshan,2021-06-14T23:00:22Z,"If we try to put in a new topic ID, the session should be closed. "
651332908,9944,jolshan,2021-06-14T23:02:23Z,Ah I see what you are saying here. I think this will still close the session when we send the request. The other option is to set a boolean similar to `missingTopicId` (maybe just change to `inconsistentTopicId` that signals to close the session earlier (upon build)
651333015,9944,jolshan,2021-06-14T23:02:46Z,That makes sense to me.
651334106,9944,jolshan,2021-06-14T23:05:39Z,"I think we already do something like this on the broker. We only get to the point of having a session if the broker had an ID for all the topics in the request. I don't think we can calculate on a request basis since we may respond with topics that did not have IDs associated. I may be misunderstanding what you are saying, but I'm very wary of trying to switch between versions 12 and 13 in the same session."
651335121,9944,jolshan,2021-06-14T23:08:31Z,I see what you mean. It is a little tricky to get the version from the FetchResponse itself. Would `resp.requestHeader().apiVersion()` work?
651335255,9944,jolshan,2021-06-14T23:08:49Z,This closes the session in handler.handlerResponse.
651335666,9944,jolshan,2021-06-14T23:09:59Z,I think I just have the wrong things here completely. There should be INCONSISTENT_TOPIC_ID here as well.
651336157,9944,jolshan,2021-06-14T23:11:15Z,I think this inconsistency existed before I touched the code. 
651373234,9944,junrao,2021-06-15T01:01:41Z,"Got it. Could we clean the existing code up a bit? Since fetchSessionHandler.handleResponse() already handles the closing of the session on error, it seem that we could get rid of fetchSessionHandler.handleError(t). Also, it seems that if fetchResponse.error() != None, we want to throw the error as an exception. Finally, if fetchSessionHandler.handleResponse() returns false, we probably want to throw an exception too?"
651933919,9944,junrao,2021-06-15T15:59:07Z,Got it.  We can keep the code as it is then.
651939380,9944,junrao,2021-06-15T16:06:10Z,"If we are switching from version 12 to version 13 for a session, prevSessionTopicId will be null. Should we also populate inconsistentTopicIds in this case to force a new session in the client?"
651943909,9944,junrao,2021-06-15T16:11:49Z,"> I don't think we can calculate on a request basis since we may respond with topics that did not have IDs associated.

I added another comment in FetchSession. If the session starts with no topicId and a fetch request switches to using topicId, could the server just return an error to force a new session? Will this avoid the need to track canUseTopicIds as a state? Overall, it's probably a bit better to add a bit complexity on the server to simplify the development on the client since we implement the client multiple times in different languages."
651946576,9944,junrao,2021-06-15T16:15:00Z,"Yes, I think that works."
651947708,9944,junrao,2021-06-15T16:16:03Z,Thanks. Sounds good.
651969184,9944,jolshan,2021-06-15T16:42:29Z,I think there are other errors that can occur when trying to send the request which is why we have  fetchSessionHandler.handleError(t). But this all can probably be cleaned up a bit/improved so I will take a look.
651970044,9944,jolshan,2021-06-15T16:43:38Z,"If we switch from 12 to 13, we will not get to this point. We will throw a FETCH_SESSION_ID_ERROR before we get here."
651971564,9944,jolshan,2021-06-15T16:45:41Z,This is something that we are doing in FetchSession. We close the session if the requests switch between 12 and 13 (or vice versa). Is the idea that we will just send the request based on the topic IDs provided to the builder (if we have an ID for each topic) and let the session code on the server handle it?
651976677,9944,junrao,2021-06-15T16:52:21Z,"Yes, if that makes the client code simpler and more consistent. For example, in https://github.com/apache/kafka/pull/9944#discussion_r651933919, the client also chooses to let the server handle the closing of the session."
651977071,9944,junrao,2021-06-15T16:52:49Z,Got it. Make sense.
651985270,9944,jolshan,2021-06-15T17:03:49Z,"I thought about this, and originally we compared the topic IDs in the session by grabbing cached partitions and comparing to the IDs in the request. Since we have a new mechanism (the topic ID map) we may no longer need to do this and I can add the ID to the hashcode and equals methods."
652228425,9944,jolshan,2021-06-15T23:27:39Z,This one is slightly different as we are checking the IBP to get fetchRequestVersion. We could have an IBP where the version is lower than 12.
652250666,9944,jolshan,2021-06-15T23:53:46Z,I was just thinking about this and realized we may send new error types to clients that may not be able to handle them. I need to review this code again.
652835907,9944,jolshan,2021-06-16T16:04:40Z,"Ok. Just went through logic for old clients
1. UNKNOWN_TOPIC_ID should not be returned since we won't ever use topic IDs (version 12 requests and below)""
2. FETCH_SESSION_TOPIC_ID_ERROR should not be returned since we won't send version 13+ in a session and will always have zero uuids
3. INCONSISTENT_TOPIC_ID should not be returned, as we won't have topic IDs in the request/session.

The only thing I can think of is downgrading a client while a session is open. I'm not sure if this can happen. "
652838939,9944,jolshan,2021-06-16T16:08:28Z,"Though in most cases, if we canUseTopicIds we likely have IBP 2.8, or are upgrading to it."
652875100,9944,jolshan,2021-06-16T16:55:24Z,One option is to do what the RaftMetadataCache does and simply create a copy of the maps themselves in topicNamesToIds() and topicIdsToNames()
653921888,9944,jolshan,2021-06-17T20:43:55Z,"I think the main reason we keep the state in the session for using topic IDs is that some requests may not contain any new/updated partitions and we need to know which version to send. We don't want to close the session and simply send the version that was sent last time. I do think this code is quite confusing as is, so I think I can simply a lot of it."
654039106,9944,jolshan,2021-06-17T23:06:25Z,"Ah, I found another use -- we lookup partitions toForget using the hashCode. Right now, toForget is a list of topic partitions and we don't directly use the ID provided in the request. We could look up the topic ID from the topic ID map and use it (we could also remove from the session map if we do remove the topic)"
654673513,9944,jolshan,2021-06-18T20:57:32Z,"This is causing build failures, will update to prevent this."
655561404,9944,junrao,2021-06-21T17:07:25Z,"The INCONSISTENT_TOPIC_ID check in ReplicaManager is not very precise since the topicId could change immediately after the check. I am thinking that another way to do this is to validate the topicId in the session again when we are generating the fetch response. We could pass in the latest topicNameToId mapping from the metadata Cache to updateAndGenerateResponseData(). If the topicId is different from those in the fetch session, we could generate a top level INCONSISTENT_TOPIC_ID error. We could then get rid of the INCONSISTENT_TOPIC_ID check in ReplicaManager."
655565663,9944,junrao,2021-06-21T17:14:12Z,Is there a benefit to have FETCH_SESSION_TOPIC_ID_ERROR in addition to INCONSISTENT_TOPIC_ID? Could we just always use INCONSISTENT_TOPIC_ID?
655574477,9944,junrao,2021-06-21T17:28:11Z,"In the latest PR, it seems that canUseTopicIds is updated on every build() call and can be a local val?"
655574856,9944,junrao,2021-06-21T17:28:40Z,Could we do this once at the beginning of build()?
655590483,9944,junrao,2021-06-21T17:52:19Z,Should we add topicId in toString()?
655593298,9944,junrao,2021-06-21T17:56:38Z,I thought that INCONSISTENT_TOPIC_ID is always a top level error now?
655598054,9944,junrao,2021-06-21T18:04:11Z,Could we first save metadataSnapshot to a local val and then derive both maps so that they can be consistent?
655599428,9944,junrao,2021-06-21T18:06:40Z,Could we first save _currentImage to a local val and derive both maps from it so that they are consistent?
655603017,9944,junrao,2021-06-21T18:12:38Z,"> I think there are other errors that can occur when trying to send the request which is why we have fetchSessionHandler.handleError(t). But this all can probably be cleaned up a bit/improved so I will take a look.

Then could we try/catch just leaderEndpoint.sendRequest and call fetchSessionHandler.handleError(t) on exception? This will make the code easier to understand."
655604345,9944,junrao,2021-06-21T18:14:53Z,Could we just use Errors.forCode() to translate errorCode to exception generically?
655614034,9944,junrao,2021-06-21T18:30:41Z,"Since we cache fetchData before, perhaps we could cache it in the new implementation too? This will make it more consistent with FetchResponse. Ditto for toForget()."
655614908,9944,junrao,2021-06-21T18:32:10Z,extra empty line.
655621465,9944,junrao,2021-06-21T18:42:57Z,"Since this tests non-existing topics, why do we pass in topicNames for fetch requests?"
655622855,9944,junrao,2021-06-21T18:45:21Z,"This is an existing issue, but could we use case to remove unnamed references _._1?"
655627698,9944,junrao,2021-06-21T18:53:31Z,Could we share the common code btw testControllerNewIBP() and testControllerOldIBP()?
655629132,9944,junrao,2021-06-21T18:55:47Z,Should we remove this line? Ditto in a few other places in this file.
655642933,9944,jolshan,2021-06-21T19:19:02Z,Ah apologies I did not clean up as well as I should have.
655643270,9944,jolshan,2021-06-21T19:19:42Z,I can try to pick up all the changes I made that do this.
655647688,9944,jolshan,2021-06-21T19:27:27Z,"@lbradstreet and I discussed this a bit. It seems that the metadata cache may be less accurate than the log itself and that is why we did away with the metadata check. I am also a little unsure (I'd have to check the code) but I'm not sure if the topicId can change. Are we saying that the partition and/or the underlying log can change in this code block? I think we can say we will read from the partition with that ID.

```
val partition = getPartitionOrException(tp)
val fetchTimeMs = time.milliseconds

// Check if topic ID from the fetch request/session matches the ID in the log
if (!hasConsistentTopicId(topicIdFromSession(partition.topic), partition.topicId))
  throw new InconsistentTopicIdException(""Topic ID in the fetch session did not match the topic ID in the log."")
  . 
  .
  .
  val readInfo: LogReadInfo = partition.readRecords(
            lastFetchedEpoch = fetchInfo.lastFetchedEpoch,
            fetchOffset = fetchInfo.fetchOffset,
            currentLeaderEpoch = fetchInfo.currentLeaderEpoch,
            maxBytes = adjustedMaxBytes,
            fetchIsolation = fetchIsolation,
            fetchOnlyFromLeader = fetchOnlyFromLeader,
            minOneMessage = minOneMessage)

```"
655648541,9944,jolshan,2021-06-21T19:28:56Z,"I think the main reason why I made the session ID error was that the inconsistent topic ID error's message was too specific for this use case. I suppose we could just make all the errors here session errors. I do like the inconsistent ID error specifying the log (and being on the partition with the issue), but we can change this."
655649094,9944,jolshan,2021-06-21T19:29:53Z,"It is both a top level and partition error here. I kind of like being able to identify the partition (kind of wish the other errors could do this in some cases), but we can change this."
655649425,9944,jolshan,2021-06-21T19:30:29Z,Ok. I see what you mean here.
655649858,9944,jolshan,2021-06-21T19:31:17Z,Ah good point. I can look into this.
655810138,9944,jolshan,2021-06-22T01:27:11Z,"I think because we still want to build the request. My understanding is that the topic is non-existing on the receiving side, but we still want to receive and handle the response."
660013467,9944,junrao,2021-06-28T18:14:42Z,"It seems that we should never change the topicId in sessionTopicIds? Perhaps we should use putIfAbsent.

Similarly, if the topicId changes, I am not sure if we should update partitionMap below."
660016282,9944,junrao,2021-06-28T18:19:08Z,Do we need to include the new fields in toString()?
660018415,9944,junrao,2021-06-28T18:22:37Z,Should we use useTopicId instead of version?
660032238,9944,junrao,2021-06-28T18:45:23Z,Should we rename error to topLevelError to make it clearer?
660032853,9944,junrao,2021-06-28T18:46:25Z,"Typically, if there is a topic level error, we set the same error in every partition through FetchRequest.getErrorResponse(). Should we do the same thing here? Ditto for IncrementalFetchContext.updateAndGenerateResponseData()."
660034212,9944,junrao,2021-06-28T18:48:30Z,error => topLevelError?
660038680,9944,junrao,2021-06-28T18:55:42Z,"Ok, this is fine.

I was thinking that when topicId changes, a pending fetch request could still reference the outdated Partition object and therefore miss the topicId change. This is unlikely and can be tighten up by clearing the segment list when a partition is deleted.

Regarding the metadata propagation, it's true that right now, we propagate the LeaderAndIsrRequest before the UpdateMetadataRequest. With Raft, the topicId will always flow through metadata update first, followed by the ReplicaManager. When we get there, maybe we could simplify the the logic a bit."
660061129,9944,jolshan,2021-06-28T19:31:39Z,"If a topic ID changes, the FetchSession will become a FetchErrorSession and close. I can change to putIfAbsent if it makes things clearer, but all this state will go away upon an error + session close."
660061358,9944,jolshan,2021-06-28T19:32:04Z,I suppose it won't hurt :)
660061496,9944,jolshan,2021-06-28T19:32:18Z,We can do that to make things clearer.
660062652,9944,jolshan,2021-06-28T19:34:17Z,"I think this goes back to the question of whether it is useful for us to have information on the specific partition that failed. If we do this, should we also return the error values for the other fields as we do in FetchRequest.getErrorResponse?"
660068452,9944,jolshan,2021-06-28T19:44:25Z,"I'm still not sure I follow ""pending fetch request could still reference the outdated Partition object and therefore miss the topicId change"" My understanding is that the log is the source of truth and we will either read from the log if it matches and not read if it doesn't. I see we could get an error erroneously if the partition didn't update in time, but I don't see us being able to read from the log due to a stale partition.

Or are you referring to the getPartitionOrException(tp) call picking up a stale partition and both the request and the partition are stale? In this case, we will read from the log, but will identify it with its correct ID. The client will handle based on this."
660122565,9944,jolshan,2021-06-28T21:16:00Z,"I guess the only issue with using FetchRequest.getErrorResponse is that we may have different topics in the response than in the request. SessionErrorContext deals with this by simply having an empty response besides the top level error. I'm wondering if we should do something like this. (Likewise, with the UNKNOWN_TOPIC_ID error, should we also just send back an empty response?)"
660127887,9944,jolshan,2021-06-28T21:25:48Z,"Taking a second look, seems like we just use partitionMap.size. Not sure if it is useful to have sessionTopicIds size (and if the whole map is too much). I'm thinking maybe just including the usesTopicIds boolean."
660199209,9944,jolshan,2021-06-29T00:33:08Z,"We need to do something like this to easily get the top level error with no partition response for UNKNOWN_TOPIC_ID. I think this works, but we may want a version check as well just to be safe."
660206465,9944,junrao,2021-06-29T00:57:22Z,"This kind of special treatment for UNKNOWN_TOPIC_ID is a bit weird. If you look at the comment above, the reason for setting the same error code in all partitions is for backward compatibility when we don't have a top level error code. So, we probably can just check the request version. If version is >=13, we just always return a top level error code with no partitions. "
660209346,9944,junrao,2021-06-29T01:06:54Z,"This can also cause a bit confusing that we are treating INCONSISTENT_TOPIC_ID differently from other top-level errors. Since the only possible top level error is INCONSISTENT_TOPIC_ID, perhaps we can change topLevelError to hasInconsistentTopicId. Ditto in IncrementalFetchContext."
660211602,9944,junrao,2021-06-29T01:14:20Z,"> I'm still not sure I follow ""pending fetch request could still reference the outdated Partition object and therefore miss the topicId change"" My understanding is that the log is the source of truth and we will either read from the log if it matches and not read if it doesn't. I see we could get an error erroneously if the partition didn't update in time, but I don't see us being able to read from the log due to a stale partition.
> 
> Or are you referring to the getPartitionOrException(tp) call picking up a stale partition and both the request and the partition are stale? In this case, we will read from the log, but will identify it with its correct ID. The client will handle based on this.

A fetch request may pass the topicId check in ReplicaManager and is about to call log.read(), when the topicId changes. I was wondering in that case, if log.read() could return data that corresponds to the old topicId. It seems that's not possible since Log.close() closes all segments."
660228129,9944,jolshan,2021-06-29T02:06:34Z,Yeah. I agree it is a bit weird. We can update as you mentioned.
660228674,9944,jolshan,2021-06-29T02:08:16Z,"The topic ID should not change in the log once it is set. I think what you said in the last sentence is correct. My understanding is that if the log is closed, it can not read from it anymore. "
661716207,9944,junrao,2021-06-30T18:27:18Z,"Could we adjust the above comment on ""The error is indicated in two ways: by setting the same error code in all partitions, and by setting the top-level error code.  The form where we set the same error code in all partitions is needed in order to maintain backwards compatibility with older versions of the protocol in which there was no top-level error code."" ?"
661716835,9944,junrao,2021-06-30T18:28:20Z,"If we can't merge this in 3.0, we will need to change the tag to 3.1."
661788073,9944,jolshan,2021-06-30T20:26:49Z,"We should adjust this to say we will no longer set on all partitions for versions 13+?
"
661822644,9944,junrao,2021-06-30T21:27:16Z,right
671728971,9944,chia7712,2021-07-17T18:30:04Z,"I noticed following warning message from our cluster (building on trunk). 

```
WARN [ReplicaFetcher replicaId=0, leaderId=1, fetcherId=0] Error in response for fetch request (type=FetchRequest, replicaId=0, ....(kafka.server.ReplicaFetcherThread:72)
org.apache.kafka.common.errors.FetchSessionTopicIdException: The fetch session encountered inconsistent topic ID usage
```

According to this code, changing the version in session is disallowed (please correct me if I misunderstood). Should fetch thread keep version in session meta? Or change the log level to `DEBUG`?"
671730072,9944,chia7712,2021-07-17T18:40:57Z,"If there is a removing partition, the topic id is NOT added to this builder. The fetch request with version=13 will carry `topic='xxx'` and `id=AAA...`. However, the topic name get reset to empty string by Kafka protocol. Hence, the following error message is produced.

```
Unexpected error handling request RequestHeader(apiKey=FETCH, apiVersion=13, clientId=broker-2-fetcher-0, correlationId=1) -- FetchRequestData(clusterId=null, replicaId=2, maxWaitMs=500, minBytes=1, maxBytes=10485760, isolationLevel=0, sessionId=620866590, sessionEpoch=1, topics=[], forgottenTopicsData=[ForgottenTopic(topic='', topicId=AAAAAAAAAAAAAAAAAAAAAA, partitions=[3, 0, 6, 9])], rackId='') with context RequestContext(header=RequestHeader(apiKey=FETCH, apiVersion=13, clientId=broker-2-fetcher-0, correlationId=1), connectionId='127.0.0.1:36157-127.0.0.1:45350-0', clientAddress=/127.0.0.1, principal=User:ANONYMOUS, listenerName=ListenerName(PLAINTEXT), securityProtocol=PLAINTEXT, clientInformation=ClientInformation(softwareName=unknown, softwareVersion=unknown), fromPrivilegedListener=true, principalSerde=Optional[org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder@56b8fb84]) (kafka.server.KafkaApis:76)
org.apache.kafka.common.errors.UnknownTopicIdException: Topic Id AAAAAAAAAAAAAAAAAAAAAA in FetchRequest was unknown to the server
```

If this is an expected behavior, should we change the log level from `ERROR` to `DEBUG`? or add more docs to say `This error may be returned transiently when xxx`? 
"
671754420,9944,jolshan,2021-07-17T23:12:32Z,Hi I'm tracking this bug here and will work on the fix: https://issues.apache.org/jira/browse/KAFKA-13079
671754618,9944,jolshan,2021-07-17T23:14:47Z,"In general, we should not send a request without a valid name or ID. 
We remove partitions from a session by not including them in the builder -- so theoretically, we should already have the topic ID. The only case we don't is when we are in a session that doesn't use topic IDs. I will fix this behavior to also check that the topics being removed have IDs in the session, otherwise send a v12 request."
671754732,9944,chia7712,2021-07-17T23:16:05Z,@jolshan thanks for response and tracking! Will watch the issue!
671754759,9944,jolshan,2021-07-17T23:16:31Z,"Changing the version is not allowed. I'm not sure I follow what you mean by keeping version. Current usesTopicIds is set based on the version when the session is first created and maintained throughout the session. 

I think we will see this error transiently, but please let me know if you continue to see this issue after I fix the bug below."
671755992,9944,chia7712,2021-07-17T23:33:04Z,Thanks for explanation. Will test it after you fix the issue!
671773887,9944,jolshan,2021-07-18T03:16:33Z,"This one too: https://issues.apache.org/jira/browse/KAFKA-13102
I have an idea of how to fix this one as well and it should make a big difference based on the testing I've done so far.
"
1146803694,13443,jeffkbkim,2023-03-23T20:35:42Z,this usually suggests that the code can be simplified. do we see this issue?
1146805063,13443,jeffkbkim,2023-03-23T20:37:10Z,is it necessary to use List?
1146877129,13443,philipnee,2023-03-23T21:23:46Z,hey do you want to move the document above the class definition?
1148105873,13443,jeffkbkim,2023-03-24T22:19:19Z,do these methods need to be protected?
1148110690,13443,jeffkbkim,2023-03-24T22:24:26Z,when should we throw PartitionAssignorException? should we throw when topics are not co-partitioned?
1148119606,13443,jeffkbkim,2023-03-24T22:36:05Z,can we just define this as minRequiredQuota?
1148125546,13443,jeffkbkim,2023-03-24T22:48:14Z,"i might be missing something - is it possible to decrease the number of partitions?

there seems to be a related KIP: https://cwiki.apache.org/confluence/display/KAFKA/KIP-694%3A+Support+Reducing+Partitions+for+Topics

but I don't see the new record."
1148129063,13443,jeffkbkim,2023-03-24T22:54:58Z,do we want to sort for every retainedPartitionsCount?
1148132025,13443,jeffkbkim,2023-03-24T23:01:45Z,"can you help me understand why we're incrementing?

remaining is the number of partitions remaining to meet the min required quota so if we're giving one of the extra partitions to this member, my intuition tells me that remaining should decremented here."
1148134237,13443,jeffkbkim,2023-03-24T23:08:04Z,can we create a new arraylist and add it at the end instead of accessing the map each time to add a new partition in L131?
1148141785,13443,jeffkbkim,2023-03-24T23:29:17Z,i think we can make this more efficient. removing all elements from one list with another will take O(n^2) assuming both have same size.
1148145596,13443,philipnee,2023-03-24T23:38:46Z,"i got pointed out several times to make the param final.  I think it's a good practice, WDYT?"
1148146400,13443,philipnee,2023-03-24T23:41:25Z,"I don't think we need this comment, the intent is pretty obvious."
1148149978,13443,philipnee,2023-03-24T23:51:23Z,"is it possible to do?
```
 Set<Integer> assignedStickyPartitionsForTopic = assignedStickyPartitionsPerTopic.getOrDefault(topicId, new HashSet<>());
// assignedStickyPartitionsForTopic are in range [0, numPartitions]
availablePartitionsPerTopic.get(topicId).addAll(assignedStickyPartitionsForTopic);
```"
1148150169,13443,philipnee,2023-03-24T23:52:05Z,"This and Step 2 can be more descriptive.  But we probably don't need to explicitly describe the step, I guess."
1149558270,13443,philipnee,2023-03-27T17:05:32Z,I wonder if we should put this in common - I assume there's a lot of common use case for Pair
1149567823,13443,philipnee,2023-03-27T17:15:08Z,I would refactor this into a separated function as it's kind of long.
1149569835,13443,philipnee,2023-03-27T17:17:12Z,"you could avoid null check using getOrDefault(topicId, new HashSet<>()) "
1149570091,13443,philipnee,2023-03-27T17:17:29Z,"the comment is also not needed, null check here is pretty indicative."
1149572710,13443,philipnee,2023-03-27T17:20:07Z,I'm not entirely clear about the point of conversion here: I think it is because of line 181. There couldn't use use the forEach/for( ... : ...) syntax?
1149630814,13443,rreddy-22,2023-03-27T18:17:17Z,I tried to simplify it as much as possible but since we wanted to many properties the code is a little complex
1149644261,13443,rreddy-22,2023-03-27T18:28:14Z,"No its not necessary but it's easier to work with, is there a reason why we should use collection instead of list?"
1149647453,13443,rreddy-22,2023-03-27T18:31:26Z,"It's specific to the data structures we're using for the assignor so I made it protected so only classes that use the same Maps can use it, to avoid confusion."
1149648624,13443,rreddy-22,2023-03-27T18:32:36Z,"This was added by David in the KIP, I'm not sure when he meant for this exception to be thrown."
1149650468,13443,rreddy-22,2023-03-27T18:34:27Z,"It's easier to understand if it's first named as numPartitionsPerConsumer, I also tried to keep as many original variable names as possible so it's simpler to compare it with the client side/ existing range assignor
"
1149655641,13443,rreddy-22,2023-03-27T18:37:49Z,Oh! I wasn't sure if it was possible but I just designed the algorithm for all sorts of metadata changes. We could remove it and maybe if/when the removal of partitions is possible we can add it back. Removed the code and the test case for now
1149658306,13443,rreddy-22,2023-03-27T18:40:15Z,"my bad, moved it before the for loop"
1149659044,13443,rreddy-22,2023-03-27T18:41:02Z,"remaining = minRequiredQuota - completedQuota
In cases where there are extra partitions after equally dividing the partitions amongst the consumers they will receive more than the minimum required quota, we're essentially increasing the quota by 1 -> requiredQuota = minReq + 1
newRemaining = requiredQuota - completedQuota = minReq + 1 - completedQuota = minReq - completed + 1 = remaining + 1"
1149666320,13443,rreddy-22,2023-03-27T18:48:32Z,"okay yeah makes sense, I'll try to do it with a counter"
1152197788,13443,jeffkbkim,2023-03-29T16:24:47Z,"Collection allows iterating over elements which is the only use i see with subscribedTopics. it makes it more generic

there's no need to use List here"
1152198768,13443,jeffkbkim,2023-03-29T16:25:38Z,can we make these private?
1152199939,13443,jeffkbkim,2023-03-29T16:26:38Z,"you should clarify that, we should be throwing the exception here if that's expected."
1152204557,13443,jeffkbkim,2023-03-29T16:29:48Z,we don't use this variable at all except renaming to minRequiredQuota. the comments in L170-171 already explain well what the variable represents
1152261139,13443,rreddy-22,2023-03-29T17:15:23Z,"yep clarified, we'll add those cases soon!"
1152265456,13443,rreddy-22,2023-03-29T17:19:13Z,cool I'll rename it
1152272541,13443,rreddy-22,2023-03-29T17:26:17Z,yess thanks !
1152275571,13443,rreddy-22,2023-03-29T17:29:21Z,"since its already private, its treated as a final param so prolly not necessary here"
1152276204,13443,rreddy-22,2023-03-29T17:29:59Z,"got it, removed"
1152284259,13443,rreddy-22,2023-03-29T17:36:42Z,"we need the integers that are not in the range [0, numPartitions] on comparison with assignedPartitions i.e we're calculating the difference between the set of assignedPartitions and the total set of partitions for that topic"
1152285443,13443,rreddy-22,2023-03-29T17:37:50Z,I just figured it would be easier to follow/correlate the code with the steps mentioned in the java doc 
1152306205,13443,rreddy-22,2023-03-29T17:58:36Z,ohh got it! Thanks! CHanged it!
1152313744,13443,rreddy-22,2023-03-29T18:05:48Z,"We need the order of the partitions to be guaranteed and sorted when we're retaining the prev assignment, that's why we had to convert it to a list first."
1152314761,13443,rreddy-22,2023-03-29T18:06:51Z,okay I'll refactor it!
1152318162,13443,rreddy-22,2023-03-29T18:10:19Z,changed it!
1152434986,13443,philipnee,2023-03-29T20:14:32Z,i think the point is final makes the params unmodifiable.
1152440653,13443,philipnee,2023-03-29T20:21:08Z,wait why is it specific? this is just a generic tuple no?
1153765068,13443,rreddy-22,2023-03-30T20:40:33Z,"sry my bad! I was thtinking about the putList and putSet
"
1156050423,13443,jeffkbkim,2023-04-03T14:32:42Z,this can be removed
1156090957,13443,jeffkbkim,2023-04-03T15:03:49Z,should this be topic2Name?
1156091273,13443,jeffkbkim,2023-04-03T15:04:06Z,nit: Consumer C
1156108091,13443,jeffkbkim,2023-04-03T15:17:35Z,"for expectedAssignment, can we add a layer for each memberId? this is blindly matching with any member's assignment which doesn't seem right.

then we won't have to remove from the expectedAssignment which is also not ideal"
1156116922,13443,jeffkbkim,2023-04-03T15:24:40Z,you can use Map.values() instead of entrySet() if you don't need the key.
1156117116,13443,jeffkbkim,2023-04-03T15:24:49Z,same here
1156125078,13443,jeffkbkim,2023-04-03T15:31:14Z,"how do we know [0, 1] went to the same member from the initial assignment?"
1156240340,13443,rreddy-22,2023-04-03T17:18:04Z,this is a method though so private is implicitly final right?
1156241130,13443,rreddy-22,2023-04-03T17:18:52Z,No we want all the mappings to be with Uuid
1156251873,13443,jeffkbkim,2023-04-03T17:29:52Z,"no, i'm referring to `AssignmentTopicMetadata(topic1Name, 3)`"
1156255508,13443,rreddy-22,2023-04-03T17:32:34Z,"No, this was done because we don't know which member gets which assignment. The order of members is not guaranteed at the time of assignment since they are stored in a hash map. So we're testing if the sets are correct and not testing the exact 1:1 mapping"
1157238218,13443,Hangleton,2023-04-04T13:20:44Z,Are you referring to adding the `final` modifier to the parameter `assignmentSpec` or the method `consumersPerTopic`?
1160097876,13443,rreddy-22,2023-04-06T17:54:18Z,"ohhh yess thanks!
"
1160098997,13443,rreddy-22,2023-04-06T17:55:36Z,going to add another test for stickiness
1160919328,13443,jeffkbkim,2023-04-07T19:34:00Z,"""Map of assigned partitions by topicId"" is more readable to me. wdyt?"
1160920086,13443,jeffkbkim,2023-04-07T19:35:49Z,in kafka we typically name getters as the field itself. in this case it would be `members()`
1160921964,13443,jeffkbkim,2023-04-07T19:39:58Z,same on getters
1160923243,13443,jeffkbkim,2023-04-07T19:42:41Z,"this doesn't look right in the javadoc. also can we get rid of all of the hyphens after colons? "":-"" to "":""

also, i think
```
This can only be done if the topics are co-partitioned in the first place. Two streams are co-partitioned if the following conditions are met:
```
is more readable"
1160925170,13443,jeffkbkim,2023-04-07T19:46:40Z,you can use <ol> for numbered lists
1160925256,13443,jeffkbkim,2023-04-07T19:46:52Z,"same here for ordered list.

also, ""Generate a map of consumersPerTopic with member subscriptions."" not sure we actually need this point"
1160927503,13443,jeffkbkim,2023-04-07T19:51:37Z,we need another layer of lists
1160929729,13443,jeffkbkim,2023-04-07T19:56:12Z,nit: can we \<code\> all variables \</code\>?
1160933184,13443,jeffkbkim,2023-04-07T20:00:32Z,"i don't think quota = minQuota... is very helpful. how's

> haven't met the required quota where quota = minimum number of partitions per member + 1 (if member is designated to receive one of the excess partitions)"
1160934110,13443,jeffkbkim,2023-04-07T20:02:31Z,"how's
> Generate a list of unassigned partitions by calculating the difference between total partitions and already assigned (sticky) partitions"
1160937277,13443,jeffkbkim,2023-04-07T20:10:03Z,"was this comment actually addressed?

also, i'm wondering if just creating

`Member` private class which contains 
```
String memberId
TopicPartition topicPartition
int remaining
int quota
```
fields.

the current use of the Pair class makes the code harder to read. "
1160938321,13443,jeffkbkim,2023-04-07T20:12:17Z,"the method yes, but the param no. you can still modify assignmentSpec inside the method"
1160939242,13443,jeffkbkim,2023-04-07T20:14:29Z,how's `consumersByTopic`?
1160940413,13443,jeffkbkim,2023-04-07T20:17:16Z,nit: `members`
1160941760,13443,jeffkbkim,2023-04-07T20:20:15Z,"i think
```
        membersData.forEach( (memberId, assignmentMemberSpec) -> {

        })
```
looks simpler. wdyt?"
1160948260,13443,jeffkbkim,2023-04-07T20:34:27Z,nit: per topicId
1160952656,13443,jeffkbkim,2023-04-07T20:44:09Z,do we really need to extract putList and putSet out? it makes the code harder to read. i don't see much benefit from this.
1160953630,13443,jeffkbkim,2023-04-07T20:46:16Z,we can use keySet() here. we're not using the value
1160954169,13443,jeffkbkim,2023-04-07T20:47:26Z,nit: `unassignedPartitionsPerTopic`
1160959103,13443,jeffkbkim,2023-04-07T20:55:25Z,we can remove this comment
1160967563,13443,jeffkbkim,2023-04-07T21:13:02Z,nit: i.e.
1160972155,13443,jeffkbkim,2023-04-07T21:20:08Z,"nit: there's 2 spaces after ""="". also, let's break down the line into 2 lines."
1160977912,13443,jeffkbkim,2023-04-07T21:33:40Z,"nit: ""should get i.e. number of partitions per consumer"""
1160979189,13443,jeffkbkim,2023-04-07T21:37:41Z,"nit: i think we can remove "" = numPartitionsPerConsumer""

also `int excessPartitionCount` is simpler and more readable to me. wdyt?"
1160980503,13443,jeffkbkim,2023-04-07T21:40:55Z,"the numbering is off and ""potentially unfilled consumer""

and L204 ""add to the potentially unfilled consumers"""
1160982741,13443,jeffkbkim,2023-04-07T21:47:34Z,"we are not ""assigning"" here, we're just increasing the quota.

""after possibly increasing the remaining quota with an excess partition"" makes more sense to me."
1160987340,13443,jeffkbkim,2023-04-07T21:55:53Z,are we actually deleting here? we're moving the pointer and grabbing a subset of the unassigned partitions. let's update the comments if we changed the code
1160992340,13443,jeffkbkim,2023-04-07T22:03:34Z,"nit: `List<Integer> partitionsToAssign`, `int unassignedPartitionPointer`"
1164463357,13443,rreddy-22,2023-04-12T17:51:48Z,changed in interface changes
1164463758,13443,rreddy-22,2023-04-12T17:52:14Z,changed in interface changes PR
1164463893,13443,rreddy-22,2023-04-12T17:52:21Z,changed in interface changes PR
1164465796,13443,rreddy-22,2023-04-12T17:54:07Z,done
1164469564,13443,rreddy-22,2023-04-12T17:57:55Z,sounds good thanks!
1164470089,13443,rreddy-22,2023-04-12T17:58:28Z,done!
1164884200,13443,rreddy-22,2023-04-13T02:03:12Z,changed to membersPerTopic
1164884866,13443,rreddy-22,2023-04-13T02:03:51Z,removed the whole comment cause unnecessary
1164893597,13443,rreddy-22,2023-04-13T02:12:37Z,changed it
1164894366,13443,rreddy-22,2023-04-13T02:13:41Z,removed
1164894487,13443,rreddy-22,2023-04-13T02:13:50Z,done
1164899524,13443,rreddy-22,2023-04-13T02:17:50Z,ok
1164901673,13443,rreddy-22,2023-04-13T02:19:25Z,I edited it but I want to keep it in cause its important 
1164901770,13443,rreddy-22,2023-04-13T02:19:32Z,ok
1164902619,13443,rreddy-22,2023-04-13T02:20:32Z,doesn't exist anymore
1164905279,13443,rreddy-22,2023-04-13T02:23:32Z,removed comment
1164906229,13443,rreddy-22,2023-04-13T02:24:25Z,each consumer gets one extra partition and I think the current name clarifies that
1164906996,13443,rreddy-22,2023-04-13T02:25:13Z,done mb with the numbers
1164908245,13443,rreddy-22,2023-04-13T02:26:48Z,cool changed it
1164908509,13443,rreddy-22,2023-04-13T02:27:06Z,yeah sorry updated the comments
1164911085,13443,rreddy-22,2023-04-13T02:30:03Z,done
1169925559,13443,dajac,2023-04-18T12:00:00Z,Should we revert this change now?
1169925666,13443,dajac,2023-04-18T12:00:06Z,Should we revert this change now?
1169925763,13443,dajac,2023-04-18T12:00:11Z,Should we revert this change now?
1169958192,13443,dajac,2023-04-18T12:25:56Z,All the html makes the text quite messy. Should we just remove it?
1169959223,13443,dajac,2023-04-18T12:26:47Z,Should this section go to the javadoc of `assign`? That would be closer to the implementation.
1169966483,13443,dajac,2023-04-18T12:32:52Z,"nit: There is an extra space after `=`. Moreover, it seems that we don't mutate `assignedPartitionsForTopic` so we could actually use `Collections.emptySet()` instead of `new HashSet<>()`."
1169982137,13443,dajac,2023-04-18T12:45:18Z,"nit: A small stylistic comment: It would be better to structure such line as follow: 

```
assignedStickyPartitionsPerTopic
    .computeIfAbsent(topicId, k -> new HashSet<>())
    .add(currentAssignmentListForTopic.get(i));
```

I find this more readable than those long lines."
1170005574,13443,dajac,2023-04-18T13:00:45Z,"Is there a reason why we don't do step 4 and 5 directly in the `membersPerTopic.forEach((topicId, membersForTopic)` loop?"
1170007516,13443,dajac,2023-04-18T13:02:11Z,nit: I have noticed that you use `computeIfAbsent` in a few places where `put` would just work.
1170010343,13443,dajac,2023-04-18T13:04:31Z,"nit: Whenever possible, let's use `Collections.singletonMap`, `Collections.emptyMap`, `Collection.emptyList`, etc."
1170013892,13443,dajac,2023-04-18T13:07:19Z,"nit: Let's format such line as follow:

```
new AssignmentMemberSpec(
     Optional.empty(),
     Optional.empty(),
     Collections.emptyList(),
     Collections.emptyMap()
);

```"
1170014726,13443,dajac,2023-04-18T13:07:58Z,nit: Is `new ArrayList<>` necessary here? There are many other cases.
1170017991,13443,dajac,2023-04-18T13:10:28Z,"I have a few utils [here](https://github.com/apache/kafka/pull/13538/files#diff-fa388cca927c076fbf90f4827b4cfdc0f89e595a3a386a5ec49e7fa56bbfe850). We could reuse them here as well. They allow you to define as assignment as follow:

```
mkAssignment(
     mkTopicAssignment(topicId1, 1, 2, 3),
     mkTopicAssignment(topicId2, 4, 5, 6)
);
```

It makes the code easier to read."
1170021475,13443,dajac,2023-04-18T13:13:12Z,I wonder if it would be better to actually create the expected `GroupAssignment` and to compare the computed one against it. We could then just use `assertEquals` and this would verify the full output.
1170322124,13443,rreddy-22,2023-04-18T16:52:38Z,"we discussed this before when I was writing the tests, that is how I did it earlier but I realized that the order of members isn't guaranteed in the hashmap so we don't know exactly what order the assignor used to predict the expected assignment"
1170357303,13443,dajac,2023-04-18T17:23:07Z,"If two Maps have the same content, they will be equal, no? The order of the members does not matter here."
1170658513,13443,rreddy-22,2023-04-18T23:21:37Z,remove the whole thing or just the html tags
1170658595,13443,rreddy-22,2023-04-18T23:21:47Z,"cool
"
1170659867,13443,rreddy-22,2023-04-18T23:24:20Z,done
1170662644,13443,rreddy-22,2023-04-18T23:30:19Z,the order in which the consumers were assigned partitions is unknown to us since at the time of computing the coordinator accesses the members map (order not guaranteed) so we can't predict which member gets which partitions in the first place.  It could result in flaky tests
1170981380,13443,dajac,2023-04-19T08:13:23Z,gotcha. i understand what you meant now.
1171070403,13443,dajac,2023-04-19T09:23:44Z,just the html tags. the explanation is useful.
1171777998,13443,rreddy-22,2023-04-19T19:37:45Z,"you're right, I've removed them!"
1171778657,13443,rreddy-22,2023-04-19T19:38:30Z,"the formatting is really off without them, that's why we had to add them"
1171781883,13443,rreddy-22,2023-04-19T19:42:18Z,"I removed most of them and converted them to put before, I'll check again"
1171782048,13443,rreddy-22,2023-04-19T19:42:27Z,got it
1171783796,13443,rreddy-22,2023-04-19T19:44:36Z,"like calculate unassigned partitions and assign them at the same time?
"
1171913729,13443,rreddy-22,2023-04-19T22:33:48Z,"Re-checked and computeIfAbsent is the best safe way to do it
"
1172314693,13443,dajac,2023-04-20T09:18:19Z,It does not have to be at the same time. I was wondering if there a reason why we need to do step 4 and 5 afterwards with all the unfilled members vs doing it per topic right after step 3.
1172321570,13443,dajac,2023-04-20T09:23:13Z,let's try at minimum to align/indent/format things correctly. it does not look good as it is.
1172322335,13443,dajac,2023-04-20T09:23:53Z,nit: We usually indent with 4 spaces in this case.
1172324109,13443,dajac,2023-04-20T09:25:26Z,"This is not correctly indented. It should be as follow:
```
members.put(consumerA, new AssignmentMemberSpec(
    Optional.empty(),
    Optional.empty(),
    Collections.emptyList(),
    Collections.emptyMap())
);
```"
1172325053,13443,dajac,2023-04-20T09:26:15Z,"nit: Could we use `Collections.singletonMap(topic1Uuid, new AssignmentTopicMetadata(3))`?"
1172325341,13443,dajac,2023-04-20T09:26:30Z,`singletonList`?
1172824513,13443,rreddy-22,2023-04-20T16:20:00Z,"I had changed it in my IDE and it looked good, idky the formatting changed in the PR :( Will take a look at it thanks!
"
1173050527,13443,rreddy-22,2023-04-20T20:17:51Z,okay
1173172297,13443,rreddy-22,2023-04-20T23:26:12Z,done
1173174594,13443,rreddy-22,2023-04-20T23:31:42Z,we need a set of sets so I added something similar in my test so facilitate the needs of this test case
1173177108,13443,rreddy-22,2023-04-20T23:37:24Z,We could've computed if the partition is still unassigned in step 5 directly and assigned it but since this is a range assignor I need all the available partitions in in a sorted list and the list provided is sorted since we iterate through 0-n where n is the total partitions and only add it to unassigned list iff it doesn't exist in the sticky partitions list
1173177198,13443,rreddy-22,2023-04-20T23:37:37Z,"I hope I understood the question correctly
"
1173179837,13443,rreddy-22,2023-04-20T23:43:38Z,Also the entire unfilled members per topic list needs to be populated since the ranges depend on how many partitions were assigned to the prev member
1175267055,13443,dajac,2023-04-24T13:14:17Z,"Would it be possible to directly use `MemberAssignment` instead of `Map<Uuid, Set<Integer>` here? That would save allocating a HashMap at the end."
1175273145,13443,dajac,2023-04-24T13:19:07Z,"I still wonder if we could combine steps 4 and 5 in this loop. For instance, could we do something like this?

* We start by creating a sorted set with all the partitions of the topic.
* Then for each member, we do what is already done but instead of populating `assignedStickyPartitionsPerTopic`, we remove assigned partitions from the sorted set.
* Then we go through the unfilled members and allocated the remaining partitions in the sorted set.

This could potentially reduce the number of data structures."
1175274363,13443,dajac,2023-04-24T13:20:01Z,Should we break this loop when there are no more partitions left to be assigned?
1175279644,13443,dajac,2023-04-24T13:24:14Z,nit: We need to close `</li>`.
1175280542,13443,dajac,2023-04-24T13:24:54Z,"nit: `<String, Integer>` does not seem to be required."
1175552910,13443,rreddy-22,2023-04-24T16:48:56Z,"We need to able to modify the Map<Uuid, Set<Integer>> throughout the code as we assign partitions and since targetPartitions in MemberAssignment is private final we can't modify it once its initialized. This is why I had to do it this way."
1175567311,13443,rreddy-22,2023-04-24T17:03:25Z,"Theoretically the sum of all the ""remaining"" values in the unfilled members list for the topic will be equal to the total unassigned partitions so we don't need to break the loop cause it happens automatically. I could add a check to ensure this is the case, I've added a check in the uniform assignor anyways just for a correctness check. "
1175689958,13443,rreddy-22,2023-04-24T19:17:01Z,"Steps 3 & 4?
"
1175701652,13443,rreddy-22,2023-04-24T19:30:51Z,"We would need to have a Map<Uuid, SortedSet> called partitions and to remove every assigned partition each removal would cost O(logn) so for n removals worst case O(nlogn). The time complexity of just calculating at the end is O(n). So its really just time vs space. we save space of one map O(n+m) in this method but we spend more time for removal."
1175703194,13443,rreddy-22,2023-04-24T19:32:33Z,2maps and O(n) time VS 1map (potentially takes up more space if its a tree set for sorted order) and O(nlogn) time
1176483468,13443,dajac,2023-04-25T13:01:28Z,"This is not entirely correct. Yes, the assignment is private final but this does not prevent you from mutating the hash map. It only prevents you from re-assigning the attribute."
1176489630,13443,dajac,2023-04-25T13:06:30Z,"I agree that the sorted set may not be the best so let's put this aside for now. Coming back to my other point, would it be possible to compute the unassigned partitions and to assign them directly in this loop? I mean after the current logic in the loop. It does not have to be combined. I understand that we can't assign partitions while we check if we want to keep existing ones or not.

If we do this, we could potentially eliminate step 3 or more precisely combine it with the next step. This would simplify the data structures overall, I think. "
1176631579,13443,jeffkbkim,2023-04-25T14:47:25Z,nit: unassignedPartitionsPerTopic
1176636568,13443,jeffkbkim,2023-04-25T14:50:24Z,"nit: accessing the field looks straightforward enough, do we need this?"
1176654241,13443,jeffkbkim,2023-04-25T15:00:47Z,"nit: `(""Member "" + memberId)`"
1176661712,13443,jeffkbkim,2023-04-25T15:06:20Z,"we can also change this to `int` once <String, Integer> is removed. also, it would be good to describe what this remaining field represents"
1176666308,13443,jeffkbkim,2023-04-25T15:09:50Z,what if the member already has min required quota + 1 assigned to it? i think it's handled in L192
1176688479,13443,jeffkbkim,2023-04-25T15:26:48Z,"how's

""it has min req partitions but it may get an extra partition so it is a potentially unfilled member""?"
1176692608,13443,jeffkbkim,2023-04-25T15:29:52Z,"how's
""If remaining > 0: it has not met the minimum required quota and therefore is unfilled."""
1176694961,13443,jeffkbkim,2023-04-25T15:31:44Z,nit: memberAndRemainingAssignments
1176836313,13443,jeffkbkim,2023-04-25T17:39:36Z,"do we have a test case where a consumer had 4 partitions, reassignment computes 3 + 1 including the extra partition and we ensure all 4 partitions stick?

a case to test whether extra partition is also sticky"
1177109065,13443,rreddy-22,2023-04-25T22:01:44Z,"oh sorry my bad, I've always learnt that final means you can't modify the value after, but I guess for a map you can't modify the reference but can change the values. I'll see what I can change thanks!"
1177128215,13443,rreddy-22,2023-04-25T22:30:12Z,"I think I made a new function cause the assign function was getting super long but yeah we can put just the calculation of unassigned partitions in the same loop, changed it now thanks! sry the step numbers were confusing "
1177128655,13443,rreddy-22,2023-04-25T22:30:59Z,"I think you mean step 4? step 3 is filling in the potentially unfilled members map and that i can't elminate.
"
1177136376,13443,rreddy-22,2023-04-25T22:45:36Z,yeah it is handled 173-179
1177139606,13443,rreddy-22,2023-04-25T22:51:57Z,this whole function is removed now
1177156557,13443,rreddy-22,2023-04-25T23:27:09Z,"removed, since it was derived from a generic pair class, missed removing it, thanks for the catch!"
1177156668,13443,rreddy-22,2023-04-25T23:27:23Z,done
1177168353,13443,rreddy-22,2023-04-25T23:53:27Z,changed
1177168484,13443,rreddy-22,2023-04-25T23:53:41Z,renamed to MemberWithRemainingAssignments
1177177698,13443,rreddy-22,2023-04-26T00:15:03Z,added another test just in case
1177178765,13443,rreddy-22,2023-04-26T00:17:29Z,"not straightforward anymore with the new code, so kept it"
1177432844,13443,dajac,2023-04-26T07:05:14Z,Should those be part of the preceding `<li>`?
1177433390,13443,dajac,2023-04-26T07:05:47Z,nit: Does it have to be public?
1177434215,13443,dajac,2023-04-26T07:06:42Z,"nit: Let's use the javadoc format. Also, I would not mention `potentiallyUnfilledMembers` and `UnfilledMembers` here. Let's describe the purpose only."
1177434539,13443,dajac,2023-04-26T07:07:06Z,"nit: If we put javadoc for attributes, let's do it for all of them."
1177435182,13443,dajac,2023-04-26T07:07:52Z,"nit: As this class is purely internal, I think that we could make the attributes public and remove the getters. They don't bring anything here."
1177435368,13443,dajac,2023-04-26T07:08:04Z,nit: javadoc?
1177436746,13443,dajac,2023-04-26T07:09:29Z,nit: The javadoc is not aligned correctly.
1177437828,13443,dajac,2023-04-26T07:10:37Z,nit: I would remove all the references to variables in the javadoc. They will get out of sync quickly. Let's use plain english instead.
1177438770,13443,dajac,2023-04-26T07:11:38Z,"Now that we have all the logic in the main loop, it seems that those Maps are not necessary anymore. We could just use Lists/Sets defined in the loop."
1177447096,13443,dajac,2023-04-26T07:20:03Z,Now that we have everything in the main loop could we combine step 3 into step 5 and avoid having to recreate MemberWithRemainingAssignments objects here? It seems that we could just adjust the `remaining` when we assign partitions. Is it possible?
1177447497,13443,dajac,2023-04-26T07:20:30Z,We already have `numPartitionsForTopic`. Could we reuse it?
1177450105,13443,dajac,2023-04-26T07:23:08Z,nit: We usually put a space before and after the `:`.
1177452306,13443,dajac,2023-04-26T07:25:12Z,nit: We can remove this empty line.
1177466123,13443,dajac,2023-04-26T07:37:01Z,"I have a general comment about the comments in the code. I think that your comments are very useful to understand the logic. However, they are a bit spread all over the places. I wonder if it would be possible to re-group them a bit. For instance in this case, we could either have one comment for the entire block or one comment per branch.

```
            // Comment for the block here.
            int remaining = minRequiredQuota - currentAssignmentSize;
            if (remaining < 0 && numMembersWithExtraPartition > 0) {
                numMembersWithExtraPartition--;
                assignedStickyPartitionsPerTopic
                    .computeIfAbsent(topicId, k -> new HashSet<>())
                    .add(currentAssignmentListForTopic.get(minRequiredQuota));
                membersWithNewAssignmentPerTopic
                    .computeIfAbsent(memberId, k -> new HashMap<>())
                    .computeIfAbsent(topicId, k -> new HashSet<>())
                    .add(currentAssignmentListForTopic.get(minRequiredQuota));
            } else {
                MemberWithRemainingAssignments newPair = new MemberWithRemainingAssignments(memberId, remaining);
                potentiallyUnfilledMembers.add(newPair);
            }
```

or 

```
            int remaining = minRequiredQuota - currentAssignmentSize;
            if (remaining < 0 && numMembersWithExtraPartition > 0) {
                // Comment for the branch here.
                numMembersWithExtraPartition--;
                assignedStickyPartitionsPerTopic
                    .computeIfAbsent(topicId, k -> new HashSet<>())
                    .add(currentAssignmentListForTopic.get(minRequiredQuota));
                membersWithNewAssignmentPerTopic
                    .computeIfAbsent(memberId, k -> new HashMap<>())
                    .computeIfAbsent(topicId, k -> new HashSet<>())
                    .add(currentAssignmentListForTopic.get(minRequiredQuota));
            } else {
                // Comment for the branch here.
                MemberWithRemainingAssignments newPair = new MemberWithRemainingAssignments(memberId, remaining);
                potentiallyUnfilledMembers.add(newPair);
            }
```"
1177506264,13443,dajac,2023-04-26T08:13:32Z,nit: `testOneConsumerWithNoSubscribedTopics`?
1177508192,13443,dajac,2023-04-26T08:15:02Z,"Indentation of the arguments seems to be off. It should be like this:
```
Map<String, AssignmentMemberSpec> members = Collections.singletonMap(
    consumerA,
    new AssignmentMemberSpec(
        Optional.empty(),
        Optional.empty(),
        Collections.emptyList(),
        Collections.emptyMap()
    )
);
```"
1177509263,13443,dajac,2023-04-26T08:15:56Z,"nit: It is usually better to use assertEquals for collections as it gives more information when it fails. `assertEquals(Collections.emptyMap(), groupAssignment.members())`."
1177510234,13443,dajac,2023-04-26T08:16:35Z,nit: `testOneConsumerSubscribedToNonExistentTopic`?
1177511835,13443,dajac,2023-04-26T08:17:38Z,"nit: The closing parenthesis of `AssignmentMemberSpec` should be on a new line and aligned with `new AssignmentMemberSpec`. The closing parenthesis of `singletonMap` should be aligned with `Map<String, AssignmentMemberSpec>`."
1177530072,13443,dajac,2023-04-26T08:29:30Z,"nit: This comment feels a bit weird here. I also wonder if this comment is necessary. The subscriptions are clear based on the specs. If you want to keep it, I would rather put it before `members` or you could also have one comment before each `members.put`."
1177541233,13443,dajac,2023-04-26T08:38:56Z,"As I told you offline, I am not a fan of this method. The main issue is that it does not really verify the co-partitioning. Moreover, it does not verify the member ids. I am thinking about two alternatives:

Option 1:
```
Set<String> expectedMembers = mkSet(....);
Set<Map<Uuid, Set<Integer>> expectedAssignments = mkSet(
    mkAssignment(
        mkTopicAssignment(topic1Uuid, 0, 1),
        mkTopicAssignment(topic3Uuid, 0)
    ),
    mkAssignment(
        mkTopicAssignment(topic1Uuid, 2),
        mkTopicAssignment(topic3Uuid, 1)
    ),
);

assertMembers(expectedMembers, computedAssignment)
assertAssignment(expectedAssignment, computedAssignment)
```

Option 2:
We could perhaps use a `TreeMap` instead of an `HashMap` for the members that we pass into the `AssignmentSpec`. The `TreeMap` guarantees the order so the algorithm may be deterministic with this. If it is, we could simply compute the expected `GroupAssignment` and use `assertEquals`."
1177543425,13443,dajac,2023-04-26T08:40:49Z,"This is a perfect example to illustrate my previous comment. In this case, `consumerA` cannot get `topic3Uuid` but we don't really verify this."
1177545492,13443,dajac,2023-04-26T08:42:35Z,nit: Here we could use my `mkAssignment` helper method and inline the current assignment. The would reduce the boilerplate.
1177546403,13443,dajac,2023-04-26T08:43:25Z,nit: Indentation is off here.
1177549003,13443,dajac,2023-04-26T08:45:28Z,nit: Indentation is not correct here. There are a few other cases in this file.
1177550694,13443,dajac,2023-04-26T08:46:56Z,nit: This empty line could be removed.
1177553020,13443,dajac,2023-04-26T08:48:55Z,"In this case, the expected assignment seems to be deterministic so we could just use `assertEquals`. This seems to be true for most of the `testReassignment` test cases."
1177565215,13443,dajac,2023-04-26T08:58:36Z,Should we add tests where we remove or add more than one members?
1178059673,13443,rreddy-22,2023-04-26T15:37:31Z,"I got comments before to add <code> tags and put the variable names, thats why I did it"
1179556594,13443,rreddy-22,2023-04-27T18:44:47Z,changing multiple subscriptions has similar effects as adding and removing consumers and that test exists so I didn't add another one.
1179557943,13443,rreddy-22,2023-04-27T18:46:26Z,on it
1179746842,13443,rreddy-22,2023-04-27T22:26:20Z,changed
1179749224,13443,rreddy-22,2023-04-27T22:28:52Z,"it was public in the client assignor so I kept it public, should i change it to private?"
1179787876,13443,rreddy-22,2023-04-27T23:13:05Z,regrouped as much as possible
1179789140,13443,rreddy-22,2023-04-27T23:14:40Z,oh okay got it
1179793263,13443,rreddy-22,2023-04-27T23:23:24Z,Added them since during reassignment its not really clear what the old subscriptions were but I removed them wherever it wasn't required
1179796793,13443,rreddy-22,2023-04-27T23:31:11Z,"this was my concern too which is why I had asked for advice and this was the best idea we had all come up with, but I like the treeMap idea I'm gonna go ahead and do that"
1179797144,13443,rreddy-22,2023-04-27T23:31:54Z,I verified with print statements so there's no issue with the code however jic that was also a concern
1179797402,13443,rreddy-22,2023-04-27T23:32:24Z,"I like the treeMap idea, I wish we thought of this sooner :( "
1179966618,13443,dajac,2023-04-28T06:10:02Z,Ack. We can keep it as public.
1179967228,13443,dajac,2023-04-28T06:10:57Z,I understand that the code is doing the right thing. What I meant is that the assertions would not catch all issues.
1179968856,13443,dajac,2023-04-28T06:13:40Z,Interesting... It is weird to have variable names in the description. Plain english is much better than `membersPerTopic`.
1179969167,13443,dajac,2023-04-28T06:14:13Z,"Yeah, sorry for this. I only thought about it when I raised this comment."
1182715705,13443,rreddy-22,2023-05-02T15:32:13Z,changing it
1188009097,13443,rreddy-22,2023-05-09T00:34:03Z,done
1188009762,13443,rreddy-22,2023-05-09T00:35:53Z,I think for readability its fine to have currentAssignment for B and then just pass it
1188270445,13443,dajac,2023-05-09T07:55:42Z,"I still find the html hard to read mainly because it is hard to visually know what is part of the main list and what is part of the sub-list. I wonder if we could indent things better. For instance, we could format it as follow. This is just a suggestion, there may be other ways.

```
/**
 * This Range Assignor inherits properties of both the range assignor and the sticky assignor.
 * The properties are as follows:
 * <ol>
 *     <li>
 *         Each member must get at least one partition from every topic that it is subscribed to. The only exception is when
 *         the number of subscribed members is greater than the number of partitions for that topic. (Range)
 *     </li>
 *     <li>
 *         Partitions should be assigned to members in a way that facilitates the join operation when required. (Range)
 *         This can only be done if every member is subscribed to the same topics and the topics are co-partitioned.
 *         Two streams are co-partitioned if the following conditions are met:
 *         <ul>
 *              <li>
 *                  The keys must have the same schemas.
 *              </li>
 *              <li>
 *                  The topics involved must have the same number of partitions.
 *              </li>
 *         </ul>
 *     </li>
 *     <li>
 *         Members should retain as much of their previous assignment as possible to reduce the number of partition movements during reassignment. (Sticky)
 *     </li>
 * </ol>
 */
```"
1188270698,13443,dajac,2023-05-09T07:55:58Z,We could use an `int` here.
1188271077,13443,dajac,2023-05-09T07:56:18Z,nit: `topicIds` -> `topic ids`?
1188272308,13443,dajac,2023-05-09T07:57:26Z,I was thinking about this one. This should never happen because the `TargetAssignmentBuilder` handle this. Therefore I wonder if we should throw a `PartitionAssignorException` error with the same error here. What do you think?
1188273956,13443,dajac,2023-05-09T07:58:59Z,"I have the same comment regarding the html here. Moreover, let's remove those variables in the test and replace them with regular text."
1188274698,13443,dajac,2023-05-09T07:59:38Z,nit: `Step 1` alone reads weird. Could we say `Step 1: something...`?
1188278175,13443,dajac,2023-05-09T08:02:57Z,"nit: As `topicData` is never reused, should we just define `numPartitionsForTopic` as `assignmentSpec.topics().get(topicId).numPartitions()`?"
1188281712,13443,dajac,2023-05-09T08:06:21Z,This comment looks out of context here.  Would it make sense to have a comment which covers both `minRequiredQuota` and `numMembersWithExtraPartition` and explains all of this?
1188283167,13443,dajac,2023-05-09T08:07:43Z,nit: Let's add a small explanation here as well.
1188283174,13443,rreddy-22,2023-05-09T08:07:43Z,all tests are checked with 1:1 mapping now so this is taken care of now
1188284004,13443,rreddy-22,2023-05-09T08:08:27Z,plain english for everything?
1188291620,13443,dajac,2023-05-09T08:15:42Z,"nit: I wonder if we should just remove this part of the comment or shorten it. The important part, I think, is that we retain at max the min require quota."
1188300132,13443,dajac,2023-05-09T08:23:09Z,"I feel like there are too many comments here. Could we try to simplify and to re-group them?

For instance, we could structure it as follow:
```
int remaining = minRequiredQuota - currentAssignmentSize;
if (remaining < 0 && numMembersWithExtraPartition > 0) {
  // Comment which explains what we do here...
} else {
  // Comment which explains what we do here...
}
```
"
1188302467,13443,dajac,2023-05-09T08:25:15Z,nit: It would be good to explain why `ascending order` is required here.
1188303243,13443,dajac,2023-05-09T08:26:00Z,nit: Indentation should be 4 spaces in order to be consistent with how you did it previously. The same applies to L226.
1188307585,13443,dajac,2023-05-09T08:29:55Z,"I am curious. Is there a reason why you structured it like this? Everywhere, we usually structure it as follow:

```
expectedAssignment.put(consumerA, mkAssignment(
    mkTopicAssignment(topic1Uuid, 0, 1),
    mkTopicAssignment(topic3Uuid, 0)
));

expectedAssignment.put(consumerB, mkAssignment(
    mkTopicAssignment(topic1Uuid, 2),
    mkTopicAssignment(topic3Uuid, 1)
));
```

This is more readable in my opinion."
1188323433,13443,rreddy-22,2023-05-09T08:43:39Z,changed it thanks!
1188374768,13443,dajac,2023-05-09T09:25:54Z,nit: We should also assert the size.
1188375282,13443,dajac,2023-05-09T09:26:23Z,Is this still useful now that we have `assertAssignment`?
1188378752,13443,dajac,2023-05-09T09:29:18Z,Let's remove step 5 here and include it in step 4.
1188385263,13443,dajac,2023-05-09T09:34:38Z,This is interesting. Should we still create a member in this case but with an empty assignment?
1188387610,13443,dajac,2023-05-09T09:36:26Z,nit: You can use `mkAssignment` to replace those. There are other similar cases.
1188811581,13443,rreddy-22,2023-05-09T15:51:39Z,"It takes away from the fact that its step one if we write everything in the same line, I wanted to draw attention to it"
1188817725,13443,rreddy-22,2023-05-09T15:56:30Z,I had it before and then I was told to remove it 
1188819218,13443,rreddy-22,2023-05-09T15:57:42Z,"this was fixed already, please see the new code, it says outdated on the top"
1188821039,13443,rreddy-22,2023-05-09T15:59:12Z,"I think its fine honestly, its different from step 4. If its too much in one step there isn't really much point in breaking it up right? "
1188821932,13443,rreddy-22,2023-05-09T15:59:57Z,That's what I asked and you had told me that either way is fine. I can change it to anything depending on how the rest of the code works
1188823115,13443,rreddy-22,2023-05-09T16:00:55Z,we don't need it but we wanted separate property tests right?
1188832049,13443,rreddy-22,2023-05-09T16:08:31Z,I just wanted to do it topic wise so its easier to understand but I'll change it 
1188856580,13443,dajac,2023-05-09T16:29:27Z,"```
// Step 1:
// something....
```

is also fine. my point is that `Step 1` alone is weird."
1188857767,13443,dajac,2023-05-09T16:30:29Z,"Interesting... I feel like this part is more important than all the rest, no?"
1188857900,13443,dajac,2023-05-09T16:30:36Z,Ack.
1188858930,13443,dajac,2023-05-09T16:31:36Z,"Yeah, it was because we were not able to use equals. Now that we can use it, I am not sure that this one bring any value. Does it?"
1188873259,13443,dajac,2023-05-09T16:43:17Z,"Yeah, that's right. It does not matter from the TargetAssignmentBuilder perspective. We can keep it as it is."
1188891203,13443,rreddy-22,2023-05-09T16:59:51Z,done.
1188891494,13443,dajac,2023-05-09T17:00:11Z,I think that it is better to have one comment for the entire block of code. It makes reading it easier.
1188893117,13443,dajac,2023-05-09T17:01:47Z,nit: Indentation is still inconsistent here.
1188893187,13443,dajac,2023-05-09T17:01:51Z,nit: Indentation is still inconsistent here.
1188893478,13443,dajac,2023-05-09T17:02:07Z,nit: ` : `.
1188898373,13443,dajac,2023-05-09T17:06:58Z,`currentSize` does not exist any more. This is why I don't like to use variable names in comments :)
1189210485,13443,rreddy-22,2023-05-09T23:13:29Z,"It looks a bit wonky after formatting it like that, I don't think there's a great way to add this html "
1189212267,13443,rreddy-22,2023-05-09T23:17:31Z,I did it in a way that looks best to me in the next commit
1189213049,13443,rreddy-22,2023-05-09T23:19:18Z,"nop we can remove it, ig whoever wants it later can write it again"
1189214614,13443,rreddy-22,2023-05-09T23:23:00Z,okayyy
1189216148,13443,rreddy-22,2023-05-09T23:26:11Z,sorry 
1189216494,13443,rreddy-22,2023-05-09T23:26:54Z,sorry fixed
1189229522,13443,rreddy-22,2023-05-09T23:57:16Z,I got comments saying don't repeat something that's already been mentioned before so I'm pretty sure I had something there and then removed it 
1189229707,13443,rreddy-22,2023-05-09T23:57:46Z,Its already in the java doc step by step so that is merely there to make sure people understand which step we're talking about
1189229928,13443,rreddy-22,2023-05-09T23:58:14Z,sure 
1189231170,13443,rreddy-22,2023-05-10T00:01:09Z,"I removed it, its not that necessary, I just wanted people to have more information on things that I personally got confused about"
1189231412,13443,rreddy-22,2023-05-10T00:01:40Z,"same explanation as before, I was told not to repeat things that have already been mentioned :("
1189231570,13443,rreddy-22,2023-05-10T00:01:58Z,I removed it
1189235728,13443,rreddy-22,2023-05-10T00:11:46Z,okay I wont use variable names again
1189237663,13443,rreddy-22,2023-05-10T00:16:32Z,Thats how it was before and I was told to change it 
1189295560,13443,dajac,2023-05-10T02:31:46Z,Ok. I was not aware of this.
1189296233,13443,dajac,2023-05-10T02:32:48Z,I understand. It was just misplaced in my opinion.
1189296779,13443,dajac,2023-05-10T02:33:30Z,Ok. I was not aware of this. Sorry for this.
1189296913,13443,dajac,2023-05-10T02:33:41Z,Ack.
1189297499,13443,dajac,2023-05-10T02:34:49Z,"Looks good, thanks."
281150684,6592,miguno,2019-05-06T11:43:38Z,Doesn't guard against NPE (`data` might be null).
281151355,6592,miguno,2019-05-06T11:45:36Z,Why isn't there an additional constructor with a default `Comparator`?
281152067,6592,miguno,2019-05-06T11:47:38Z,"Also, why does the Serde need a `Comparator` at all?"
281263059,6592,miguno,2019-05-06T16:41:25Z,We should use try-with-resources here (for `DataInputStream`).
281263464,6592,miguno,2019-05-06T16:42:38Z,We should use try-with-resources here (for `ByteArrayOutputStream` and `DataInputStream`).
281263644,6592,miguno,2019-05-06T16:43:05Z,"This also fixes the problem that, in the current code, the `ByteArrayOutputStream` was not closed."
281264102,6592,miguno,2019-05-06T16:44:24Z,Asking because neither a `List<T>` nor a `Deserializer<T>` need a `Comparator`.
281285669,6592,yeralin,2019-05-06T17:43:15Z,"Put it on a discussion: https://sematext.com/opensee/m/Kafka/uyzND1VU1Ou1y0Lbh?subj=+DISCUSS+KIP+466+Add+support+for+List+lt+T+gt+serialization+and+deserialization

Thank you for your input! I highly appreciate it :)"
287207631,6592,mjsax,2019-05-24T03:52:15Z,I think we should call `deserializer.configure(...)` here
287207698,6592,mjsax,2019-05-24T03:52:47Z,I think we should call `deserializer.close()` here
287208093,6592,mjsax,2019-05-24T03:56:00Z,Should we get the `size` first and pass it into `ArrayList` constructor to make it more efficient?
287208576,6592,mjsax,2019-05-24T04:00:08Z,forward call to `serializer`
287460369,6592,yeralin,2019-05-24T17:55:17Z,Is it sufficient for testing ListSerde?
287470423,6592,mjsax,2019-05-24T18:26:10Z,"We should also test `null` and empty array IMHO.

Please, add new test methods for both cases."
296436321,6592,mjsax,2019-06-22T06:33:31Z,"As mentioned on the KIP discussion, `BytesDeserializer` should not be included."
296436331,6592,mjsax,2019-06-22T06:34:22Z,I using `Stream.of` the best was to populate the Map? Seems to be unnecessarily complex to me?
296436380,6592,mjsax,2019-06-22T06:37:34Z,"I would add test for all primitive types. The test should also check the expected `byte[]` array size after serialization and test a ""round trip"".

We should also have a test for non-primitive type round-trip.

Lastly, I would add a test for deserializing different list-types.

Also `null` corer case should be tested."
303481813,6592,yeralin,2019-07-15T14:59:13Z,"Replaced it with simpler approach:
```
new HashMap<Class, Integer>() {{
            put(LongDeserializer.class, 8);
            ...
    }};
```"
304072719,6592,yeralin,2019-07-16T19:04:30Z,@mjsax Where should I place all of these new test cases? Should I create a new class?
307013332,6592,mjsax,2019-07-24T20:46:22Z,Add new test methods to this test should be sufficient.
307015039,6592,mjsax,2019-07-24T20:50:28Z,Both `_DOC` variables should me moved to `CommonClientConfigs`
307015480,6592,mjsax,2019-07-24T20:51:21Z,Nit: remove whitespace before `Default` (similar for the other 3 `_DOC` strings)
307016006,6592,mjsax,2019-07-24T20:52:38Z,"Why `or DEFAULT_LIST_VALUE_SERDE_INNER_CLASS` ? For the key, we only care about the key part.

(Similar below for value -- we should only care about the value part.)"
307016504,6592,mjsax,2019-07-24T20:53:53Z,We should explain that this config is only used if `key.deserializer` is set to `ListDeserializer`. Similar for the type config below.
307016893,6592,mjsax,2019-07-24T20:54:47Z,The class does not implement `Deserializer` but `List`.
307017972,6592,mjsax,2019-07-24T20:57:33Z,Can we actually include UUID type? It always 16 bytes.
307018186,6592,mjsax,2019-07-24T20:58:04Z,nit: maybe call this `fixedLengthDeserializers` -- it's not about primitive types.
307026623,6592,abbccdda,2019-07-24T21:20:06Z,"Avoid star import, same for the rest"
307036896,6592,mjsax,2019-07-24T21:50:20Z,"Both new configs should be added below:
```
        CONFIG = new ConfigDef()
```

Similar for `ProducerConfig` and `StreamsConfig`"
307037138,6592,mjsax,2019-07-24T21:51:10Z,"I think this could be `String` or `Class` type. Not sure. For any case, we should test for both cases."
307037211,6592,mjsax,2019-07-24T21:51:24Z,Same here
307037543,6592,mjsax,2019-07-24T21:52:22Z,Should we have two try-catch blocks? One for `listClass` and one for `inner` ?
307037605,6592,mjsax,2019-07-24T21:52:35Z,nit: remove `this` (not required)
307037957,6592,mjsax,2019-07-24T21:53:41Z,How do we know that all list types implement a constructor like this? Should we have a fall back to default constructor?
307038250,6592,mjsax,2019-07-24T21:54:41Z,rename similar to ListDeserializer and add UUID type?
307038415,6592,mjsax,2019-07-24T21:55:13Z,rename? why not use `boolean`?
307038590,6592,mjsax,2019-07-24T21:55:50Z,This could also be  `Class` type?
307296391,6592,yeralin,2019-07-25T13:31:34Z,"Fixed, had to change my IntelliJ config."
307302882,6592,miguno,2019-07-25T13:44:40Z,"Why is the `ByteArrayOutputStream` not covered by try-with-resources? It should, no?"
307305083,6592,miguno,2019-07-25T13:48:24Z,"Shouldn't we also add `BytesSerializer` and `ByteArraySerializer` here?  Same question for deserialization.

Edit: I did notice that we do some ""testing for primitives"" by doing `contains()` on `primitiveSerializers`.

+1 to also adding UUID serializer (and deserializer)."
307308603,6592,miguno,2019-07-25T13:55:01Z,"Why is this needed only for non-primitives, and not always?"
307309387,6592,yeralin,2019-07-25T13:56:31Z,"I guess smth like this:
```
try {
    Constructor<?> listConstructor;
    try {
        listConstructor = listClass.getConstructor(Integer.TYPE);
        return (List<T>) listConstructor.newInstance(listSize);
    } catch (NoSuchMethodException e) {
        listConstructor = listClass.getConstructor();
        return (List<T>) listConstructor.newInstance();
    }
} catch (Exception e) {
    throw new RuntimeException(""Could not construct a list instance of \"""" + listClass.getCanonicalName() + ""\"""", e);
}
```"
307309648,6592,miguno,2019-07-25T13:57:02Z,"I think the List serde should return null (after a round trip) if and only if the input was null. If the input was an empty list, then the list serde should instead return an empty list.

That is, I believe the serde needs to distinguish between the absence of a collection (indicated by null) and a collection that happens to be empty."
307311596,6592,yeralin,2019-07-25T14:00:36Z,Changed to boolean primitive. What do you think is the best name for it instead of `isPrimitive`? `isFixedLength` maybe?
307312643,6592,yeralin,2019-07-25T14:02:48Z,I was following impl of `SessionWindowedSerializer`
307316124,6592,yeralin,2019-07-25T14:09:34Z,"@mjsax mentioned during the KIP discussion:
```
For the primitive serializers: `BytesSerializer` is not primitive IMHO,
as is it for `byte[]` with variable length -- it's for arrays, not for
single `byte` (note, that `Bytes` is a Kafka class wrapping `byte[]`).
```

I'll add UUID (de)serializers"
307320943,6592,yeralin,2019-07-25T14:19:01Z,"You mean we can directly cast it to `Class` object?
i.e. `Class listType = (Class) configs.get(listTypePropertyName);`"
307321201,6592,yeralin,2019-07-25T14:19:28Z,I was following impl of `SessionWindowedDeserializer`
307322886,6592,yeralin,2019-07-25T14:22:49Z,"Yes, I think it will make errors more descriptive"
307323343,6592,yeralin,2019-07-25T14:23:38Z,Probably add a warning log? What do you think?
307326098,6592,yeralin,2019-07-25T14:28:36Z,"If I understand your question correctly:
This was an optimization feature. If we have a collection of fixed length elements like `Integer`, `Long`, `UUID`, etc. We don't actually need to encode each element's size. That's why I have this extra if statement.
If that's what you were asking"
307329101,6592,yeralin,2019-07-25T14:33:43Z,"Smth like:
```
public static final String LIST_KEY_DESERIALIZER_INNER_CLASS_DOC = ""Inner deserializer class for key that implements the <code>org.apache.kafka.common.serialization.Deserializer</code> interface. ""
                                                                      + ""This configuration will be read if and only if <code>key.deserializer</code> configuration is set to <code>org.apache.kafka.common.serialization.ListDeserializer</code>"";
```"
307332736,6592,yeralin,2019-07-25T14:40:14Z,What `Importance` should these configs be set to?
307336073,6592,yeralin,2019-07-25T14:46:23Z,"Should I add `DOC`s for `DEFAULT_LIST_KEY/VALUE_SERDE_INNER_CLASS` configs? 

I was looking at `DEFAULT_WINDOWED_KEY/VALUE_SERDE_INNER_CLASS` configs in `StreamsConfig` class, and they don't have docs underneath them."
307429344,6592,mjsax,2019-07-25T18:03:55Z,Yes
307429932,6592,mjsax,2019-07-25T18:05:13Z,I think LOW (or maybe MEDIUM) because it's dependent config
307431168,6592,mjsax,2019-07-25T18:08:12Z,"Yes. The user can use the config two ways:
```
// as string
props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, ""my.fully.qualified.package.MyInnerSerde"");
// or as class
props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, my.fully.qualified.package.MyInnerSerde.class);
// or short (it the class is imported)
props.put(DEFAULT_LIST_KEY_SERDE_TYPE_CLASS, MyInnerSerde.class);
```

Both should be supported and the code need to be able to handle both cases. Hence, we should get is as `Object` and use `instanceof` to check the type."
307432566,6592,mjsax,2019-07-25T18:11:21Z,This may indicate a bug in `SessionWindowedDeserializer`
307433215,6592,mjsax,2019-07-25T18:12:50Z,Don't think we need a warning. Wondering if we should use try-catch or better pro-actively check if an int-constructor exists?
307433392,6592,mjsax,2019-07-25T18:13:16Z,SGTM
307433534,6592,mjsax,2019-07-25T18:13:36Z,Seems like a bug in `SessionWindowedSerializer`
307434421,6592,mjsax,2019-07-25T18:15:42Z,"Sound like something we should fix, ie, add corresponding DOC entries to `StreamsConfig`"
307444815,6592,yeralin,2019-07-25T18:39:33Z,"Yep, I think MEDIUM is more appropriate since it is a interconnected config scheme"
307448021,6592,yeralin,2019-07-25T18:47:13Z,"Something like this I presume:
```
final Object innerSerde = configs.get(innerSerdePropertyName);
try {
    if (innerSerde instanceof String) {
        inner = Utils.newInstance((String) innerSerde, Serde.class).serializer();
    } else if (innerSerde instanceof Class) {
        inner = ((Serde<T>) Utils.newInstance((Class) innerSerde)).serializer();
    } else {
        throw new ClassNotFoundException();
    }
    inner.configure(configs, isKey);
} catch (final ClassNotFoundException e) {
    throw new ConfigException(innerSerdePropertyName, innerSerde, ""Serde class "" + innerSerde + "" could not be found."");
}
```"
307448892,6592,yeralin,2019-07-25T18:49:17Z,"We kind of implicitly check if int-constructor exists using this try-catch block, right?"
307464495,6592,yeralin,2019-07-25T19:28:42Z,"Ok I added the following tests:
`listSerdeShouldRoundtripPrimitiveInput(): Arrays.asList(1, 2, 3)`

`listSerdeShouldRountripNonPrimitiveInput(): Arrays.asList(""A"", ""B"", ""C"")`

`listSerdeShouldReturnEmptyCollection(): Arrays.asList()`

`listSerdeShouldReturnNull(): null`

`listSerdeSerializerShouldReturnByteArrayOfSize(): Arrays.asList(1, 2, 3) => 16`

`listSerdeShouldReturnLinkedList() new LinkedList<>()`

`listSerdeShouldReturnStack() new Stack<>()`

I think I covered it all. Btw you said *all primitive types*, you mean all 6 of them, right?"
309339359,6592,mjsax,2019-07-31T17:22:07Z,Seems this variable is still misssing the corresponding DOC string? (Same for `DEFAULT_LIST_VALUE_SERDE_INNER_CLASS` below)
309341518,6592,mjsax,2019-07-31T17:26:44Z,"We should point out, that this config is only affective iff `key.deserializer` is set to `ListDeserializer`."
309342714,6592,mjsax,2019-07-31T17:29:33Z,"Seems you still did not add the config to the static `CONFIG` variable below. (this must be done for consumerconfig, producerconfig, and streamsconfig)"
309344282,6592,mjsax,2019-07-31T17:33:18Z,"Not: remove space before `"" Default...""` Also, this variable should be moved to `CommonClientConfigs` IMHO.

(Same commend for VALUE below.)"
309344677,6592,mjsax,2019-07-31T17:34:24Z,This comment is not addressed yet
310367315,6592,mjsax,2019-08-04T01:40:26Z,"`Note when list serde class is used` -> seems be a little fuzzy if one does not know the context. It might be better to be very explicit.

What about:
```
""Default class for keys (that are of type <code>java.util.List</code>) that implements the <code>java.util.List</code> interface. ""
+ ""This config is only effective if configuration parameter "" + DEFAULT_KEY_SERDE_CLASS_CONFIG + "" is set to "" + ListSerde.class.getName() + "" and determines the concrete List-type (eg, LinkedList, ArrayList etc.) that is instantiated during deserialization. ""
+ ""Note that you also need to set the inner serde class (for the list elements) that implements the <code>org.apache.kafka.common.serialization.Serde</code> interface via "" + DEFAULT_LIST_KEY_SERDE_INNER_CLASS + "" configuration parameter.""
```

We should be similarly explicit, for the ""inner serde"" configs."
310367323,6592,mjsax,2019-08-04T01:41:02Z,Avoid unnecessary reorderings (this class does not contain any actual code change).
310367328,6592,mjsax,2019-08-04T01:41:34Z,As above (similar for other files below).
310367336,6592,mjsax,2019-08-04T01:42:10Z,nit: fix indention and align to existing code
310367353,6592,mjsax,2019-08-04T01:43:38Z,Should be type `CLASS` (similar below); cf. `KEY_DESERIALIZER_CLASS_CONFIG`
310367365,6592,mjsax,2019-08-04T01:44:10Z,As above: `CLASS` and fix indention
310618942,6592,yeralin,2019-08-05T14:00:08Z,"`DEFAULT_KEY_SERDE_CLASS_CONFIG` is not accessible from `ComminClientConfigs` since it lives in `StreamsConfig`, the same applies for `ListSerde.class.getName()`"
329244889,6592,mjsax,2019-09-27T20:45:58Z,"Should this be `Map<Class<? extends Deserializer>, Integer>` ?

Seem we should declare it as `static` ?"
329245133,6592,mjsax,2019-09-27T20:46:40Z,Should it be `Class<L> listClass` ? (or `Class<List<T>` if we don't introduce `L`)
329249087,6592,mjsax,2019-09-27T20:58:26Z,"I am wondering, if we should get the `List` type as generic (not sure).

`public class ListDeseializer<L extends List<T>, T> implements Deserializer<L>`"
329250304,6592,mjsax,2019-09-27T21:02:21Z,Should this be `Class<List<T>>` (or maybe `Class<L>` if we introduce `L extends List<T>` as class generic?
329250920,6592,mjsax,2019-09-27T21:04:18Z,We should limit this suppression to the method for which we really need it instead of the whole class
329251104,6592,mjsax,2019-09-27T21:04:55Z,`Constructor<List<T>>` (or `Constructor<L>` if we introduce `L`)
329251296,6592,mjsax,2019-09-27T21:05:33Z,Update return type to `L` (if we introduce `L`)
329251591,6592,mjsax,2019-09-27T21:06:32Z,Update return type to `L` (if we introduce `L`)
329251673,6592,mjsax,2019-09-27T21:06:45Z,Update return type to `L` (if we introduce `L`)
329252615,6592,mjsax,2019-09-27T21:10:07Z,Avoid global suppress
329253153,6592,mjsax,2019-09-27T21:12:14Z,"`List<Class<? extends Serializer>>`

Should be `static`"
329253861,6592,mjsax,2019-09-27T21:14:49Z,`Class<L>` (or just `Class<? extends List>`?)
329254244,6592,mjsax,2019-09-27T21:16:20Z,"Should we change to `ListSerde<L extends List<T>, T> extends WrapperSerde<List<T>>` (cf. `ListDeserializer` comments)

Note: We should not use `WrappedSerde<L>` because the wrapper `Serializer` uses `List<T>` but not `L` (not sure if we want to tie the serializer type to a fixed list type, but I guess we should not)."
329254510,6592,mjsax,2019-09-27T21:17:21Z,add missing `<>` to `new ListSerializer<>(...)`
329255698,6592,mjsax,2019-09-27T21:21:49Z,"If we fix the generics, we don't need this suppress"
340187181,6592,vvcephei,2019-10-29T16:29:06Z,"It's better to avoid ""double-brace initialization"", which is actually declaring a new anonymous subclass of HashMap just to add some stuff to it in one statement.

A little while back, I added this method for accomplishing the same thing more safely: `org.apache.kafka.common.utils.Utils#mkMap`, and the accompanying `org.apache.kafka.common.utils.Utils#mkEntry`."
340190515,6592,vvcephei,2019-10-29T16:34:53Z,"This shouldn't be necessary. I believe the config parser will coerce the value to the type you declared the configuration as, `Type.CLASS`. Might be worth to double-check, but we shouldn't add a bunch of branches if they're not necessary."
340191653,6592,vvcephei,2019-10-29T16:36:58Z,"That class is different because it doesn't actually `define` the config, it's just an undeclared ""extra"" config that gets passed around to be interpreted inside the serde.

Actually, this _is_ a bug, and that config _should_ be `define`d there the way you do it here."
340192387,6592,vvcephei,2019-10-29T16:38:20Z,"+1, just add this suppression on the methods that need it."
340237949,6592,vvcephei,2019-10-29T17:58:44Z,"These new tests look good. I agree, though, that we should test round trip + length for each of the primitive types (short, integer, long, float, double, and UUID). Just because it would be easy to mess up just one of them, so we really should have test coverage for them all."
342316240,6592,mjsax,2019-11-04T23:10:31Z,Should we add a similar sentence like `This configuration will be read if....` to `CommonClientConfigs#DEFAULT_LIST_KEY_SERDE_INNER_CLASS_DOC ` (similar for value) ?
342320732,6592,mjsax,2019-11-04T23:27:37Z,nit: merge both lines: `byte[] payload = new byte[primitiveSize == null ? dis.readInt() : primitiveSize];`
342321230,6592,mjsax,2019-11-04T23:29:29Z,nit: indentation should be 4 spaces?
342321277,6592,mjsax,2019-11-04T23:29:38Z,nit: indentation should be 4 spaces?
342326150,6592,mjsax,2019-11-04T23:48:49Z,nit: move this class definition to L127 (after `static public final class UUIDSerde extends WrapperSerde<UUID> {`) to group all defined Serde classes.
342326188,6592,mjsax,2019-11-04T23:48:58Z,can be removed (if we change the class to `extends WrapperSerde<L>`)
342326296,6592,mjsax,2019-11-04T23:49:23Z,The cast to `(Deserializer<List<T>>)` is not necessary if we change the class to `extends WrapperSerde<L>`.
342326601,6592,mjsax,2019-11-04T23:50:40Z,nit: add those into section `// medium` and insert in alphabetical order within the section
342337097,6592,mjsax,2019-11-05T00:35:31Z,nit: this method should be after static method `static public Serde<Void> Void() {` to keep stuff grouped
342345719,6592,mjsax,2019-11-05T01:14:54Z,"I was playing with the code a little bit, and turns out using `Class<L>` instead of `Class<? extends List>` might actually be too strict. Compare my other comments."
342345818,6592,mjsax,2019-11-05T01:15:29Z,Not 100% sure -- but we need tests for this cases. The `configure()` code is untested atm
342346108,6592,mjsax,2019-11-05T01:17:14Z,"We could change the signature to
```
public static <L extends List<T>, T> Serde<L> ListSerde(Class<L> listClass, Serde<T> innerSerde) {
```
to make it type safe... but there are issue (I also mentioned this for `ListDeserializer` above -- also compare my comment below)"
342346253,6592,mjsax,2019-11-05T01:17:59Z,"Also could make types mores strict via `extends WrapperSerde<L>`

Again, as mentioned above, not sure if this might be too strict."
355461955,6592,JakobEdding,2019-12-09T13:56:55Z,"Typo, `DEERIALIZER`"
364943202,6592,zorgz,2020-01-09T20:23:31Z,"inner.serialize() can return null here in case of the list entry is null for example
then NPE will follow at https://github.com/apache/kafka/pull/6592/files#diff-9333dde189fe982b61d2f01aa3b2cba8R83 
"
364993597,6592,yeralin,2020-01-09T22:35:10Z,"Hmmm that's an interesting edge case.
I cannot just return null since a list might contain real values i.e. `List<String> data = {'A', null, 'C'}`
I have to serialize `null` somehow..."
365021029,6592,zorgz,2020-01-10T00:13:25Z,"I get it with AbstractKafkaAvroSerializer and my custom Array serde

AbstractKafkaAvroSerializer code: 
```
    protected byte[] serializeImpl(String subject, Object object) throws SerializationException {
        Schema schema = null;
        if (object == null) {
            return null;
        } else {
            String restClientErrorMsg = """";
```
"
368185579,6592,mjsax,2020-01-18T00:13:59Z,nit: should be `Deserializer<?>` to avoid warnings about using a raw type
368185717,6592,mjsax,2020-01-18T00:14:55Z,nit: should be `<L extends List<Inner>>` to avoid warning about using a raw type
368186003,6592,mjsax,2020-01-18T00:16:30Z,"nit: should be `Class<Deserializer<Inner>>` (2 times) -- (not `Serde` compare comment above) and we want to avoid warning about using a raw type

also `innerSerde -> innerDeserializerClass`"
368187510,6592,mjsax,2020-01-18T00:25:38Z,"This is `ListDeserializer` hence, shouldn't we use `ConsumerConfig.LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG` ? The ""SERDE"" config should be used in Kafka Streams codebase only? (Same for value, and for both inner types in the next line)."
368188170,6592,mjsax,2020-01-18T00:29:56Z,nit: should be `Serializer <?>` to avoid warnings about using a raw type
368188407,6592,mjsax,2020-01-18T00:31:15Z,"As above: use `ProducerConfg.LIST_KEY_SERIALIZER_INNER_CLASS_CONFIG` instead of ""SERDE"" config parameters (2 times)

also `innerSerdePropertyName -> innerSerializerPropertyName`"
368188720,6592,mjsax,2020-01-18T00:33:05Z,nit: `innerSerdePropertyName  -> innerDeserializerPropertyName`
368189063,6592,mjsax,2020-01-18T00:35:29Z,nit: `innerSerde -> innerSerializerClassOrName`
368189160,6592,mjsax,2020-01-18T00:36:10Z,"Should be:
```
inner = Utils.newInstance((String) innerSerializerClassOrName, Serializer.class);
```"
368189561,6592,mjsax,2020-01-18T00:38:33Z,"Should be:
```
inner = Utils.newInstance((Class<Serializer<Inner>>) innerSerializerClassOrName);
```"
368189677,6592,mjsax,2020-01-18T00:39:28Z,"`""Serde class ""` -> `""Serializer class ""`"
368191352,6592,mjsax,2020-01-18T00:51:39Z,"That is a tricky question. There are multiple ways how we could encode this, but this seem to be a design question that required to go back to the KIP discussion?

For example, we could skip the optimization of fixed-length types and encode the length for every entry -- a length of `-1` would indicate a `null`. Or we introduce a ""header"" that tells us if there are `null` in the list (either a bit-array for short list or a ""list of null positions"")

Example for list of null positions would be: `<numberOfNulls><listOfNullPositions><regularEncoding>` 
Ie, with the example for above, we encode `1-1-<bytes-for-A>-<bytes-for-C>`.

As bit array, it would be `<bitArryaSizeInBytes><bitArray><regularEncoding>`
Ie, with the example for above, we encode `1-0100000-<bytes-for-A>-<bytes-for-C>`"
368191525,6592,mjsax,2020-01-18T00:53:02Z,should be `<L extends List<Inner>>` to avoid raw type warning and make build pass
368191582,6592,mjsax,2020-01-18T00:53:30Z,"Should be `<L extends List<Inner>, Inner>` to avoid raw type warning and make build pass"
368191860,6592,mjsax,2020-01-18T00:55:56Z,"I am wondering now, why we actually need `CommonClientConfigs.DEFAULT_LIST_KEY_SERDE_INNER_CLASS` (maybe there was a reason by I forgot) -- can't we add the ""SERDE"" configs only to `StreamsConfig`?"
369420107,6592,mjsax,2020-01-22T08:21:25Z,"Thinking about it once more, it might not work what I suggested, because if you want to call `Serde.ListSerde(ArrayList.class, ...)` the `ArrayList` does not specify any inner type information (but is a raw type) and thus it won't compile. I guess, we need to leave it as-is, and suppress the ""raw type"" warning (might be worth to add a comment why the raw type warning cannot be avoided for this case)."
369420640,6592,mjsax,2020-01-22T08:22:46Z,"Same as below -- I guess my suggestion does not work in practice (it's correct that it would avoid the raw type warning, but it would make the API unusable in practise because we want to be able to pass in raw type list classes)."
370513438,6592,mjsax,2020-01-24T08:15:57Z,"@yeralin I was thinking about this case more. I really think, that the fix-length optimization is valuable as it reduced the serialized byte size by 50% for e.g. Integer lists. For the other two proposals, it's harder to judge which one is better. I see the following (dis)advantages for each:

null-index-list:
 - low overhead for dense lists with few nulls (for zero nulls, its 4 byte overhead, for each null, its additional 4 bytes)
 - the longer the lists, the smaller the overhead

bit-array:
 - low overhead for short lists (4 bytes for byte array length + list-lenght/8 bytes for the byte-array itself)
 - not ideal for long lists with few nulls

Hence, for short list both might be equally ok. For long lists, it depends of they are dense or sparse (for long-dense list, null-index-list seems to be better, for long-sparse-lists, bit-array seems to be better).

It will be hard to tell which one is better, hence, I would suggest that we only implement the null-index-list list for now, because I assume that dense lists are more common and it works better for long dense lists than the bit-array idea.

However, to allow us to support different serialization format in the future, we should add one more magic byte in the very beginning that encodes the choose serialization format. In our case, we will will have one format and the magic byte will always be ""zero"". If we add the byte-array format, we can just set the magic byte to one to indicate the other format. And we could even add more formats of people have a better idea how to do it later on.

Btw: we could actually already go with two formats:
 - 0 => optimized-fixed-length-encoding plus null-index-list
 - 1 => variable length encoding using `-1` in the length field to indicate `null` (no header to mark nulls is required at all for this case).

If we agree on this design, we should update the KIP accordingly.

\cc @vvcephei: would love to hear your feedback, too."
371434275,6592,yeralin,2020-01-27T19:23:57Z,"Thank you for your input @mjsax 

Your outline makes sense, I like the idea of `null-index-list`, and I'm happy to jump back to  the KIP.

However, could we please wrap up this round of review? Make sure that all generics, docs, tests are in place.

Once it is clean and polished, I can start updating the KIP and implementing this new feature. 

What do you think?

P.S. Rebased the branch with latest `trunk` and pushed another commit with review changes"
372120739,6592,mjsax,2020-01-28T23:41:43Z,"can be simplified to `@SuppressWarnings(""unchecked"")`
"
372121895,6592,mjsax,2020-01-28T23:45:47Z,`innerDeserializer` could be null; we should handle to case to avoid a NPE calling `getClass()`
372123553,6592,mjsax,2020-01-28T23:51:31Z,"There a two independent configs for the list-type and inner-type, hence it might be better to handle both independently:
```
if (listClass == null) {
    String listTypePropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_DESERIALIZER_TYPE_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_DESERIALIZER_TYPE_CLASS;
    listClass = (Class<List<Inner>>) configs.get(listTypePropertyName);
    if (listClass == null) {
        throw new ConfigException(""Not able to determine the list class because it was neither passed via the constructor nor set in the config"");
    } 
}
if (inner == null) {
    String innerDeserializerPropertyName = isKey ? CommonClientConfigs.DEFAULT_LIST_KEY_SERIALIZER_INNER_CLASS : CommonClientConfigs.DEFAULT_LIST_VALUE_SERIALIZER_INNER_CLASS;
    Class<Deserializer<Inner>> innerDeserializerClass = (Class<Deserializer<Inner>>) configs.get(innerDeserializerPropertyName);
    inner = Utils.newInstance(innerDeserializerClass);
    inner.configure(configs, isKey);
}
```"
372123738,6592,mjsax,2020-01-28T23:52:12Z,as above; simplify
372125306,6592,mjsax,2020-01-28T23:57:44Z,Use `KafkaException` instead of `RuntimeException`
372125767,6592,mjsax,2020-01-28T23:59:23Z,in `Utils.newInstance()` we catch exceptions more fine grained -- might be worth to do the same here?
372126337,6592,mjsax,2020-01-29T00:01:19Z,Use `KafkaException` instead of `RuntimeException`
372126509,6592,mjsax,2020-01-29T00:01:57Z,as above
372126925,6592,mjsax,2020-01-29T00:03:37Z,"Should we throw `KafkaException` instead? Also, we need to add an error message that clarifies which class was not found."
372128389,6592,mjsax,2020-01-29T00:08:53Z,as above: `serializer` could be `null` and we should handle this case gracefully
372128852,6592,mjsax,2020-01-29T00:10:43Z,"Maybe add a `null` check and throw `ConfigException` with detailed error message similar to the `null`-check for `listClass` in the `ListDeserializer#configure(...)`?

I think `instanceof` would be false for `null` and thus the `null` check within `Utils.newInstance(...)` would not be executed."
372129532,6592,mjsax,2020-01-29T00:13:33Z,Use `KafkaException` instead of `RuntimeException`
372129790,6592,mjsax,2020-01-29T00:14:31Z,We should add a `null` check to allow closing a deserializer that was not properly setup
372129838,6592,mjsax,2020-01-29T00:14:42Z,We should add a `null` check to allow closing a serializer that was not properly setup
372131801,6592,mjsax,2020-01-29T00:22:12Z,`SERIALIZER` -> `SERDE`
372131842,6592,mjsax,2020-01-29T00:22:22Z,`SERIALIZER` -> `SERDE`
372131861,6592,mjsax,2020-01-29T00:22:27Z,`SERIALIZER` -> `SERDE`
372132594,6592,mjsax,2020-01-29T00:25:25Z,This method is hard to read... Can we format it differently? (Maybe a empty line before `return...` is sufficient?)
378457483,6592,yeralin,2020-02-12T19:14:33Z,"`catch (InstantiationException | IllegalAccessException | IllegalArgumentException | InvocationTargetException e)`
Kind of long, but I agree for it to be more fine grained"
453892518,6592,yeralin,2020-07-13T19:50:12Z,Make sure that the serialization flag is known to the application.
453892850,6592,yeralin,2020-07-13T19:50:48Z,It's probably better to wrap it into `if/else` construct instead.
453893460,6592,yeralin,2020-07-13T19:52:01Z,"By default, if we are dealing with a list of primitives, we are using `SerializationStrategy.NULL_INDEX_LIST` vs. a list of non-primitives (`UUID`, `String`, or some custom object) `SerializationStrategy.NEGATIVE_SIZE`."
453894509,6592,yeralin,2020-07-13T19:54:14Z,"This is what I was talking about in https://github.com/apache/kafka/pull/6592#issuecomment-606252503

Even if we are dealing with primitives, and a user chooses `SerializationStrategy.NEGATIVE_SIZE`, we would have to encode each primitive's size in our payload."
453894647,6592,yeralin,2020-07-13T19:54:32Z,Should it be parametrized?
613642681,6592,ableegoldman,2021-04-14T23:09:28Z,"Just wondering, what is the reason for this change?"
613647496,6592,ableegoldman,2021-04-14T23:23:03Z,"Should it be valid for this to be null? I would think that these Serdes should be configured either by instantiating it directly via this constructor, or via the default constructor + setting configs (eg list.key.serializer.inner). It doesn't seem to make sense to use this constructor and not pass in valid arguments.
WDYT about throwing an exception if either parameter is `null` -- not sure if ConfigException or IllegalArgumentException is more appropriate, up to you"
613648141,6592,ableegoldman,2021-04-14T23:24:53Z,nit: use `private static`  ordering (for consistency with the rest of the code base)
613649139,6592,ableegoldman,2021-04-14T23:27:36Z,"If the `listClass` and `inner` have already been set by invoking the non-default constructor, but the user also set the `list.key.deserializer.inner` configs, should we verify that the configs match and throw a ConfigException otherwise?"
613651922,6592,ableegoldman,2021-04-14T23:36:10Z,What about the `LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG`?
613652145,6592,ableegoldman,2021-04-14T23:36:50Z,"```suggestion
            throw new ConfigException(innerSerdePropertyName, innerSerdeClassOrName, ""Deserializer's inner serde class \"""" + innerSerdeClassOrName + ""\"" was not a valid Serde/Deserializer."");
```"
613657189,6592,ableegoldman,2021-04-14T23:52:12Z,"Hey, sorry that I'm jumping in here after there's been a long discussion which I missed, but I'm wondering why the serialization strategy would be configurable? IIUC the serialization strategy correctly, one of them basically means ""constant-size data/primitive type, don't encode the size only length of list"" while the other means ""variable-size data, encode the size of each element only""

I assume this is to allow users to indicate that their data is constant size when its a non-primitize type, to avoid the need to encode this same size data -- that makes sense to me. But I think we can simplify the API a bit so we don't have to let users shoot themselves in the foot, as you said earlier  

How about: if it's a primitive type, and we can detect this (I think we should be able to), then we never encode the size info. If a user opts to do so, just log a warning and ignore it.

By the way, this might also be due to some earlier discussion I missed, but I find the names of the two SerializationStrategy enums super confusing. How about just `VARIABLE_SIZE` and `CONSTANT_SIZE`? Imo it's better to describe what the enum actually _means_ than how its implemented, you can read the code to understand the latter. But you shouldn't need to read the code to understand what a config means. Plus, this way we have flexibility to change the underlying implementation if we ever need to without also having to change the enum names which are now a public API"
613658493,6592,ableegoldman,2021-04-14T23:56:23Z,"What is this? Can you give it a name that describes what it means a little more -- IIUC this is a sentinel that indicates ""this list has variable-sized elements so we encode each element's size"".

That said, coming up with names is hard -- you can probably do a better job than me but just to throw out a suggestion, what about something like `VARIABLE_SIZE_SENTINEL`?"
613658799,6592,ableegoldman,2021-04-14T23:57:19Z,"nit: can we use `Double.SIZE` instead of just `8`, that way it's super clear that this value actually means?"
614950753,6592,yeralin,2021-04-16T15:54:30Z,"Hmmm `Double.SIZE` returns 64:
```
jshell> Double.SIZE
$1 ==> 64
```"
614954404,6592,yeralin,2021-04-16T15:59:41Z,"Otherwise the build will fail with:
```
> Task :clients:checkstyleMain
[ant:checkstyle] [ERROR] .../clients/src/main/java/org/apache/kafka/common/serialization/ListDeserializer.java:19:1: Disallowed import - org.apache.kafka.clients.CommonClientConfigs. [ImportControl]
```

was addressed in https://github.com/apache/kafka/pull/6592/commits/98e51defc612d7af80b0c81ab6de612520d3ad6b"
615005374,6592,yeralin,2021-04-16T17:14:20Z,"It was introduced in https://github.com/apache/kafka/pull/6592/commits/f91d75925aac4dce1f9522b45142ea25b5258748
However, now I am looking at it and seems like we actually don't need any of:
```
LIST_KEY_DESERIALIZER_INNER_CLASS_CONFIG
LIST_KEY_DESERIALIZER_TYPE_CLASS_CONFIG
LIST_KEY_SERIALIZER_INNER_CLASS_CONFIG
LIST_KEY_SERIALIZER_TYPE_CLASS_CONFIG
```

Since we are operating only with:
```
DEFAULT_LIST_KEY_SERDE_INNER_CLASS
DEFAULT_LIST_KEY_SERDE_TYPE_CLASS
DEFAULT_LIST_VALUE_SERDE_INNER_CLASS
DEFAULT_LIST_VALUE_SERDE_TYPE_CLASS
```

Good observation, I'll remove these unused configs."
615008995,6592,yeralin,2021-04-16T17:20:31Z,"That's a good idea. I think `IllegalArgumentException` is the most appropriate.
Something like:
```
throw new IllegalArgumentException(""ListDeserializer requires innerDeserializer to be provided during initialization"");
```
"
615010709,6592,yeralin,2021-04-16T17:23:38Z,"I was following the logic defined in similar (de)serializers like `SessionWindowedSerializer`.
There they are doing similar thing, simply checking whether a (de)serializer is null, then trying to get a value from configs.

They don't perform any verification.

What do you think? Should we divert from that approach?"
615020981,6592,yeralin,2021-04-16T17:41:33Z,"Hey, no worries.

For:
> How about: if it's a primitive type, and we can detect this (I think we should be able to), then we never encode the size info. If a user opts to do so, just log a warning and ignore it.

I think I am already doing that in the constructors:
```
    public ListSerializer(Serializer<Inner> serializer) {
        this.inner = serializer;
        this.isFixedLength = serializer != null && fixedLengthSerializers.contains(serializer.getClass());
        this.serStrategy = this.isFixedLength ? SerializationStrategy.NULL_INDEX_LIST : SerializationStrategy.NEGATIVE_SIZE;
    }

    public ListSerializer(Serializer<Inner> serializer, SerializationStrategy serStrategy) {
        this(serializer);
        this.serStrategy = serStrategy;
    }
```

If a user doesn't pass `serStrategy` flag, we pick the best one for her based on passed serializer.
If a user passes her own `serStrategy` flag, we simply obey to it. 
However, we don't print any warning logs, since I assumed if the user passes the flag, then she probably knows what she is doing.

What do you think? I could add a warning log otherwise."
615021158,6592,yeralin,2021-04-16T17:41:56Z,"As per flag names, totally agree. Changing them to `VARIABLE_SIZE` and `CONSTANT_SIZE`."
615075349,6592,yeralin,2021-04-16T19:23:08Z,"Basically, if we are following `VARIABLE_SIZE` serialization strategy **and** we have a `null` entry in our list, we encode this null entry as `-1`, so that during deserialization when we encounter `-1`, we append `null` entry to our list.

Example, to serialize a list like `{""A"", ""B"", null, ""C""}` of strings, the payload would look smth like:
```
serialization_flag | size_of_list | A_size | A_encoded | B_size | B_encoded | -1 | C_size | C_encoded
```
"
615075871,6592,yeralin,2021-04-16T19:24:11Z,Could be called something like `NULL_ENTRY_VALUE` instead maybe?
618823378,6592,ableegoldman,2021-04-22T23:51:07Z,"Yes, I think we should. And it's not even a diversion from the approach elsewhere because there's a KIP in progress to do so in classes like `SessionWindowedSerializer` as well"
618823669,6592,ableegoldman,2021-04-22T23:52:16Z,"Cool. I think the fewer configs overall, the better. If we can get away with just the Serde configs then let's do so to keep the API surface area smaller for users  "
618824127,6592,ableegoldman,2021-04-22T23:53:50Z,"Ah, my bad. I think the variable I had in mind is actually called `Double.BYTES`. Not 100% sure it's defined for all possible primitive types, but I would hope so"
618824748,6592,ableegoldman,2021-04-22T23:55:45Z,That sounds good to me  
618826368,6592,ableegoldman,2021-04-23T00:00:19Z,"Awesome. Maybe I misunderstood this comment:
>Even if we are dealing with primitives, and a user chooses SerializationStrategy.NEGATIVE_SIZE, we would have to encode each primitive's size in our payload.

or maybe you just wrote that a while ago and it's out of date. Anyways what we're doing now sounds good, no reason to encode extra data even if the user selects this strategy for some reason. But I do think we should at least log a warning telling them they made a bad choice and it will be ignored. Most likely they just didn't understand what the parameter meant, and it's a good opportunity to enlighten them"
618826641,6592,ableegoldman,2021-04-23T00:01:13Z,"Ooooh ok, that makes a lot more sense now. I think your suggestion for the name sounds good"
618910174,6592,yeralin,2021-04-23T03:14:35Z,"Yep, that checks out.

Only for `UUID` I'd have to leave hardcoded `36`."
620449322,6592,yeralin,2021-04-26T16:17:55Z,"Now, I am thinking about it. It seems a bit extra to compare the classes defined between the constructor and configs.

Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other."
620457633,6592,yeralin,2021-04-26T16:28:34Z,"Hmmm, I thought you wanted to simply warn the user that the serialization strategy she chose is not optimal. But seems like you want to ignore the choice completely.

Then it doesn't make sense to expose this flag at all for the user to change. Me and @mjsax were discussing it earlier https://github.com/apache/kafka/pull/6592#issuecomment-606277356"
620716670,6592,ableegoldman,2021-04-26T23:20:45Z,">Maybe, if a user tries to use the constructor when classes are already defined in the configs, we simply throw an exception? Forcing the user to set only one or the other

That works for me. Tbh I actually prefer this, but thought you might consider it too harsh. Someone else had that reaction to a similar scenario in the past. Let's do it  "
620720140,6592,ableegoldman,2021-04-26T23:29:50Z,"Ah, sorry if that wasn't clear. Yes I was proposing to ignore the choice if a user selects the `VARIABLE_SIZE` strategy with primitive type data. And to also log a warning in this case so at least we're not just silently ignoring it.

But I think you made a good point that perhaps we don't need to expose this flag at all. There seems to be no reason for a user to explicitly opt-in to the `VARIABLE_SIZE` strategy. Perhaps a better way of looking at this is to say that this strategy is the default, where the default will be overridden in two cases: data is a primitive/known type, or the data is a custom type that the user knows to be constant size and thus chooses to opt-in to the `CONSTANT_SIZE` strategy.

WDYT? We could simplify the API by making this a boolean parameter instead of having them choose a `SerializationStrategy` directly, something like `isConstantSize`. "
620802248,6592,yeralin,2021-04-27T02:27:58Z,"Hmmm, @mjsax reasoning was that in the future we could introduce **more** serialization strategies https://github.com/apache/kafka/pull/6592#discussion_r370513438
> However, to allow us to support different serialization format in the future, we should add one more magic byte in the very beginning that encodes the choose serialization format

As per ignoring the choice, also from @mjsax https://github.com/apache/kafka/pull/6592#issuecomment-606277356:
> I guess it's called freedom of choice :) If we feel strong about it, we could of course disallow the ""negative size"" strategy for primitive types. However, it would have the disadvantage that we have a config that, depending on the data type you are using, would either be ignored or even throw an error if set incorrectly. From a usability point of view, this would be a disadvantage. It's always a mental burden to users if they have to think about if-then-else cases.
...
Personally, I have a slight preference to allow both strategies for all types as I think easy of use is more important, but I am also fine otherwise.

Here is my thought process, if a user chooses a serialization strategy, then she probably knows what she is doing. Ofc, the user will have a larger payload, and we certainly will notify her that the serialization strategy she chose is not optimal for the current type of data, but I don't think we should strictly forbid the user from ""shooting herself in the foot""."
620804518,6592,ableegoldman,2021-04-27T02:34:31Z,"My feeling is, don't over-optimize for the future. If/when we do want to add new serialization strategies it won't be that hard to pass a KIP that deprecates the current API in favor of whatever new one they decide on. And it won't be much work for users to migrate from the deprecated API. I'm all for future-proofness but imo it's better to start out with the simplest and best API for the current moment and then iterate on that, rather than try to address all possible eventualities with the very first set of changes. The only exception being cases where the overhead of migrating from the initial API to a new and improved one would be really high, either for the devs or for the user or both. But I don't think that applies here.

That's just my personal take. Maybe @mjsax would disagree, or maybe not. I'll try to ping him and see what he thinks now, since it's been a while since that last set of comments. Until then, what's your opinion here?"
621475071,6592,yeralin,2021-04-27T18:13:14Z,"Ok, in this case, I think the best course of action is to completely remove `SerializationStrategy` flag, and replace it with a simple boolean. Do not expose it to the user, and automatically choose the strategy based on the type of data.

If you agree, I'll go ahead and make the change."
621676170,6592,ableegoldman,2021-04-27T23:05:42Z,"Just to clarify you mean don't expose this to the user at all, right? That sounds completely fine to me. If there are enough people trying to serialize lists of custom classes with all constant data size who want this optimization exposed for general use, then someone will request the feature and we can go back and add it in. Then we can debate what the API should look like at that time, and keep things simple for now. Personally I suspect the vast majority of non-primitive data types are not going to be constant size anyways.

Given the above, I think whether to track the strategy as an actual `SerializationStrategy` enum vs a boolean flag becomes a matter of code style and personal preference, since it's no longer exposed to the user. So it's up to you whether you find the enum or the flag to be more readable or clean"
629768563,6592,ableegoldman,2021-05-11T00:34:30Z,"Same here, --> `public static`. Can you also leave it on one line? I know it's super long, but that's just the style we use in Kafka"
629769396,6592,ableegoldman,2021-05-11T00:35:57Z,"super nit: we put the modifier first, ie use `public static` ordering. "
629770618,6592,ableegoldman,2021-05-11T00:40:07Z,"nit: Kafka coding style doesn't use the `get` prefix in getters, ie this should be named `innerDeserializer` (same applies for any other getters in this PR, I won't bug you by commenting on every single one of them)"
629772320,6592,ableegoldman,2021-05-11T00:45:41Z,"Is the unchecked warning coming from something in the test itself, or just from using the serde? It should be possible to just use the Serde without getting a warning. I don't see anything in the test that looks suspicious so I'm guessing we need another suppression somewhere in the Serde implementation? "
629772466,6592,ableegoldman,2021-05-11T00:46:11Z,super nit: extra blank line
629776651,6592,ableegoldman,2021-05-11T01:00:18Z,"I found it a bit difficult to understand what was going on here since I'm reading this first, before the serialize implementation, but I take it we just encode the indices of any null values at the beginning of the serialized list? Can you leave a comment pointing that out, either here on the method itself or else down below where the method is used? "
629777825,6592,ableegoldman,2021-05-11T01:04:18Z,"Since we no longer expose the SerializationStrategy or let users explicitly select it, these two equality checks should have both be true or both be false, right? Might read a bit easier if we only check `serStrategy == SerializationStrategy.VARIABLE_SIZE` here, and then just verify that `primitiveSize` is not null when we parse the serialization strategy flag at the top. WDYT?"
629781672,6592,ableegoldman,2021-05-11T01:17:17Z,"Since we don't know what the underlying list structure is, using `get(index)` like this could be pretty costly -- for example with a LinkedList this will be O(N), which makes it O(N^2) overall. Might be safer to just iterate through the list with a plain `for int i` loop and take note of the nulls that way"
629781907,6592,ableegoldman,2021-05-11T01:18:05Z,nit: put the `out.writeInt` on its own line
629782907,6592,ableegoldman,2021-05-11T01:21:16Z,"Same as my suggestion in ListDeserializer, can you add a quick comment here or above the `serializeNullIndexList` method explaining what this is doing (like you have above with `// write serialization strategy flag`)"
629784136,6592,ableegoldman,2021-05-11T01:25:13Z,"Also similar to a comment in ListDeserializer: it should not be possible for only one of these to be true, so let's just check one or the other here. In fact maybe we can get rid of the `isFixedLength` flag entirely now, since `SerializationStrategy.VARIABLE_SIZE` means exactly the same thing (or rather, the opposite of it)"
630539763,6592,yeralin,2021-05-11T21:04:20Z,"Would something like this work?
```
int i = 0;
List<Integer> nullIndexList = new ArrayList<>();
for (Iterator<Inner> it = data.listIterator(); it.hasNext(); i++) {
    if (it.next() == null) {
        nullIndexList.add(i); 
    }
}
out.writeInt(nullIndexList.size());
for (int nullIndex : nullIndexList) {
    out.writeInt(nullIndex);
}
```"
630542548,6592,yeralin,2021-05-11T21:09:26Z,"The problem is [all of them](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/serialization/Serdes.java) list `static public` first.

![image](https://user-images.githubusercontent.com/8620461/117885230-98d8d200-b27b-11eb-849e-afdc43631c89.png)"
630550102,6592,yeralin,2021-05-11T21:23:30Z,"Unfortunately, it is unavoidable https://github.com/apache/kafka/pull/6592#pullrequestreview-311418205
Had to do it this way and sacrifice type safety for easier usage of this Serde."
630603333,6592,ableegoldman,2021-05-11T23:27:32Z,"Ah, I didn't notice...tbh we should probably just fix all of them, but it's fine with me to leave that out of this PR and just conform to this for now. I'll leave it up to you"
630605834,6592,ableegoldman,2021-05-11T23:34:18Z,"Thanks for the context, it is what it is (and I agree with your decision to prioritize ease of use). I was more wondering whether we might be missing a `SuppressWarnings(""unchecked"")` on one of the methods in the implementation, so that the user isn't forced to do the suppression themselves. But I can't quite tell where the warning is coming from, since it seems like we do already suppress unchecked warnings in `ListDeserializer#createListInstance`  where the casting occurs? Is it possible this was just left over from an earlier version, and we no longer need all the suppressions on these tests?"
630606384,6592,ableegoldman,2021-05-11T23:36:01Z,"Yep, exactly (not sure why I said to use `for int i`, obviously that suffers from the same problem -- the iterator is what I had in mind)"
631151867,6592,yeralin,2021-05-12T15:33:20Z,"This is a common practice leaving empty lines at the end of files:
https://stackoverflow.com/questions/27059239/git-new-blank-line-at-eof"
631158775,6592,yeralin,2021-05-12T15:40:10Z,"No, unfortunately they are still needed.

AFAIK, suppression warnings cannot be propagated upwards.

In `ListDeserializer#createListInstance` we indeed using `SuppressWarnings(""unchecked"")` due to casting.
However, in tests it is used bc `Stack.class` (or `ArrayList.class`, `LinkedList.class`, etc) is a raw-type and does not guarantee the required inner type (`Integer` in this case). 

The only way to deal with these warnings is to create some wrapper classes as @mjsax suggested, like:
`public static class IntegerArrayList extends ArrayList<Integer> {}`
But I do not this it is a scalable and clean solution.


Pretty much every time a user is calling `Serdes.ListSerde(...)` they will have to put SuppressWarnings statement. It is a limitation of Java language.

As @mjsax said:
> Sorry for the complicated review, but generics are especially tricky and we should try hard to get it right"
631327191,6592,ableegoldman,2021-05-12T19:06:47Z,"Well, it's not at the end of the file right? But if you'd prefer to keep it that's fine too, was just a ""super nit"" suggestion  "
631328727,6592,ableegoldman,2021-05-12T19:09:22Z,"Can you move this up to the top of this file, under the `Streams changes in 3.0` section? It's in reverse order, so the newest stuff goes at the top."
631334877,6592,ableegoldman,2021-05-12T19:19:33Z,"Ah, I see, it's from the implicit casting of the parameters. That makes sense, I was just wondering since I didn't see any ""obvious"" casting in the test code itself. Thanks for the explanation"
631339580,6592,yeralin,2021-05-12T19:27:24Z,"Omg I am blind. Sorry, you are right!"
212770210,5567,vvcephei,2018-08-24T22:37:13Z,I need to expose these so that I can query the window spec in KTableImpl
212770275,5567,vvcephei,2018-08-24T22:37:50Z,"Upon second look, I think I'll move these into a utility class to not pollute KTableImpl"
212770518,5567,vvcephei,2018-08-24T22:39:27Z,The basic idea is to traverse back through the topology until we find the window spec and get the grace period from it.
212770687,5567,vvcephei,2018-08-24T22:40:59Z,on-the-side fixup of the generic types for groupBy.
212770823,5567,vvcephei,2018-08-24T22:42:02Z,"This is where the buffering would take place. Right now, we throw an exception unless we detect that we can pass-through the record."
212770994,5567,vvcephei,2018-08-24T22:43:20Z,`Suppress` is split into interface/impl mostly to support these kinds of internal methods.
212771115,5567,vvcephei,2018-08-24T22:44:11Z,Decided to start using fuzzing to avoid magic numbers in the tests. Let me know if you prefer it different.
212771290,5567,vvcephei,2018-08-24T22:45:40Z,"`MockProcessorContext` is pretty nice for unit tests, but we need `InternalProcessorContext` for the suppress processor."
212771356,5567,vvcephei,2018-08-24T22:46:17Z,so you can just print the result of `forwarded()` for debugging.
212771502,5567,vvcephei,2018-08-24T22:47:22Z,Had to fix this to actually get the right timestamp forwarded.
212771525,5567,vvcephei,2018-08-24T22:47:35Z,just to quit spamming the logs.
213025428,5567,vvcephei,2018-08-27T15:56:16Z,"This preserves the existing behavior that if both `to.timestamp` and `this.timestamp` are unset, the forward time would be `-1`."
213039753,5567,vvcephei,2018-08-27T16:46:07Z,"which I think is reasonable, since this context would only be used for unit tests."
213190515,5567,guozhangwang,2018-08-28T05:49:13Z,nit: `bytesToUseForSuppressionStorage` -> `numBytesToStore`?
213190554,5567,guozhangwang,2018-08-28T05:49:30Z,`numberOfKeysToRemember` -> `numKeysToRemember`?
213192247,5567,guozhangwang,2018-08-28T06:01:31Z,`intermediateEvents` -> `emitIntermediateResults`?
213192826,5567,guozhangwang,2018-08-28T06:05:43Z,nit: `IntermediateEmitConfig`?
213193035,5567,guozhangwang,2018-08-28T06:07:02Z,"Hmm.. this makes me thinking if we should consider duplicate the `BufferFullStrategy` to `FinalBufferFullStrategy` and `IntermediateBufferFullStrategy` and also the corresponding caller `BufferConfig` as well, to replace runtime error with complication error?"
213193122,5567,guozhangwang,2018-08-28T06:07:34Z,Why not `TimeWindows`?
213194261,5567,guozhangwang,2018-08-28T06:14:53Z,"Let's use `TopologyException` instead of `IllegalArgumentException` here, ditto below."
213194430,5567,guozhangwang,2018-08-28T06:15:50Z,Actually today we have undefined operator for windowed-table / windowed-table join at all. But the logic itself looks good :P
213196512,5567,guozhangwang,2018-08-28T06:28:15Z,"nit: just define two static `TimeDefinition<K, V>` and `TimeDefinition<Windowed<K>, V>`, one with context.timestamp and one with window end time?"
213321144,5567,bbejeck,2018-08-28T13:50:40Z,"Since [KIP-100](https://cwiki.apache.org/confluence/display/KAFKA/KIP-100+-+Relax+Type+constraints+in+Kafka+Streams+API) should this be `Suppress<? super K, ? super V> suppress` for consistency with the rest of the API?"
213326969,5567,bbejeck,2018-08-28T14:04:48Z,One meta-comment about the static methods on the interfaces in `Suppress`. Would we want to consider making them `default` instead of `static` in case down the line we want to implement any of these interfaces to have different behavior in the various methods? 
213333450,5567,bbejeck,2018-08-28T14:20:32Z,nit: we can get rid of the `else` here 
213336080,5567,bbejeck,2018-08-28T14:26:44Z,is this intentional or left over debugging?
213376919,5567,vvcephei,2018-08-28T16:07:30Z,"This is actually ""suppress intermediate events"". It looks like this in the DSL:

With static import: `table.suppress(intermediateEvents(...))`
Without static import: `table.suppress(Suppress.intermediateEvents(...))`

Does that seem right?"
213377265,5567,vvcephei,2018-08-28T16:08:30Z,"Yeah, I was thinking something similar... I'll sketch something up."
213377504,5567,vvcephei,2018-08-28T16:09:14Z,How do you know it's a `TimeWindows`?
213377758,5567,vvcephei,2018-08-28T16:09:57Z,Ok. Thanks. That sounds much better.
213379204,5567,vvcephei,2018-08-28T16:14:23Z,"I see. Well, it's permitted by the API, so I'd rather keep the sanity check.

Actually, it seems like the only problem with further manipulations of windowed tables is that we won't know to use a windowed store (as by that point, we only know it's a KTable). Might be worth revisiting this at some point..."
213383840,5567,vvcephei,2018-08-28T16:28:32Z,"yes, it should! Thanks for the catch and the reference."
213385791,5567,vvcephei,2018-08-28T16:34:57Z,"In general, this is something that's good to start thinking about. I think we have two kinds of interfaces/non-final classes: ones that are for implementing and ones that are for encapsulation.

For implementing: Serde, Windows, StateStore, etc.
For encapsulation: KTable, Materialized, etc.

To me, `Suppress` is in the latter category: since we cast it to `SuppressImpl` immediately, it's not possible to pass in any other implementations of it. The interface/impl split is purely to provide a clean division of external/internal members. The regular Java access modifiers are insufficient for this purpose, since SuppressImpl's internal members need to be accessed from other internal classes, but outside of its package."
213387916,5567,vvcephei,2018-08-28T16:41:18Z,"It's intentional. We first randomly generate a seed, then we create a (pseudo)Random for test data generation from the seed. If the test fails, we can deterministicaly reproduce the (pseudorandom) test exactly, but only if we know the seed.

That said, I want to play around with it some more, and see if I can get it to only print the seed if there are test failures. Also, there's currently no option to run the tests with the seed, but you could always just drop in the literal when you're debugging locally."
213389544,5567,vvcephei,2018-08-28T16:45:31Z,see prior response
213393351,5567,vvcephei,2018-08-28T16:56:01Z,"Hmm, I don't know if I want to spend a bunch of time of this idea. I think what I'll do is switch to a fixed seed, so that we get deterministic testing while keeping the statement that there's nothing special about the values we're testing with."
213414494,5567,vvcephei,2018-08-28T17:59:15Z,how about `maxBytes`?
213477348,5567,vvcephei,2018-08-28T21:24:05Z,"Ah, it can't be static because of the generic parameters, but I can make it final, which would bring us to one anonymous class per instance

... but you get a new instance every time you call one of the builder methods anyway, so it's the same thing. I think this is what I was originally thinking when I left it like this."
213479566,5567,vvcephei,2018-08-28T21:32:00Z,"Actually, no it shouldn't ;)

`Suppress`'s bounds on `K` and `V` need to be tight, since the operator will actually serialize and deserialize the records (1. when it is size-constrained, 2. when it spills to disk, and 3. because this operator needs a changelog)."
213479948,5567,vvcephei,2018-08-28T21:33:31Z,"I previously missed that last point... it means that Suppress requires Serdes and not just Serializers, and it needs them always, not just when it's size constrained or spilling to disk."
213480277,5567,vvcephei,2018-08-28T21:34:44Z,"Oh, and I've also just now realized that it should be called `Suppressed`, not `Suppress`, in keeping with the rest of the config objects...

This is a lot of changes. I think we'll probably have to recast votes this time."
213753581,5567,vvcephei,2018-08-29T16:45:49Z,"Sorry, I was being silly, but upon second reading, it looks snarky... `Windows<W>` is the tightest bound we can put on the window spec at this point, since this processor takes any window spec."
213760382,5567,vvcephei,2018-08-29T17:07:37Z,Changed the name to `Suppressed` in keeping with the other config objects.
213819571,5567,vvcephei,2018-08-29T20:17:47Z,Added these methods in lieu of the BufferFullStrategy enum
213819739,5567,vvcephei,2018-08-29T20:18:25Z,"Added `Serialized` as a top-level property of the buffer, since the buffer itself will likely need a changelog for resilience."
213820442,5567,vvcephei,2018-08-29T20:20:42Z,Utility class just to encapsulate the graph search for looking back up the topology for the grace period (and verifying it's configured the same on all incoming branches)
213820698,5567,vvcephei,2018-08-29T20:21:33Z,I'll fill this in in the next PR.
213820934,5567,vvcephei,2018-08-29T20:22:26Z,"Added this ""test"" to demonstrate what compiles and what doesn't. "
213822461,5567,vvcephei,2018-08-29T20:27:16Z,This allows us to insist on a `StrictBufferConfig` for the final results use case.
214188984,5567,vvcephei,2018-08-30T21:37:52Z,"I had a potentially kooky idea...

What if instead of bumping up the node index here, we just tack ""SUPPRESS"" on to the parent node name somehow?

This way, it would become fine to slap suppressions into topologies and restart without any other changes (because it wouldn't cause all the other nodes to get re-numbered). This might be especially important if we intend to insert suppressions in the future for optimization reasons.

Thoughts?"
214207725,5567,guozhangwang,2018-08-30T23:14:02Z,"Could you elaborate a bit more on `just tack ""SUPPRESS"" on to the parent node name somehow?` what is the concrete proposal of the naming scheme?"
214481155,5567,bbejeck,2018-08-31T21:38:59Z,"Hmm, I'd have to see the concrete proposal.  If we follow that approach wouldn't we still need to implement some sort of counter in the case of having multiple ""SUPPRESS"" nodes?  Then I think we'd still have the same issue, adding a new suppress operator would change the numbering scheme of the suppress nodes in the topology."
214482095,5567,bbejeck,2018-08-31T21:44:40Z,We should have a unit test for this class covering both base cases as well as the success result
214483409,5567,bbejeck,2018-08-31T21:51:23Z,"This base case and the one below could be refactored into a method 
```java
 checkBaseCondition(Objects::isNull, streamsGraphNode, chain);
 .......
 checkBaseCondition(s -> s.parentNodes().isEmpty(), streamsGraphNode, chain);

.....
private static void checkBaseCondition(final Predicate<StreamsGraphNode> windowClosePredicate,
                                      final StreamsGraphNode graphNode,
                                      final String chain) {
        if (windowClosePredicate.test(graphNode)) {
            throw new TopologyException(
                ""Window close time is only defined for windowed computations. Got ["" + chain + ""].""
            );
        }
    }
```
WDYT?"
214980375,5567,vvcephei,2018-09-04T16:21:43Z,"Yeah, it would be like:

```java
(KTableImpl field) int suppressionIndex = 0;
...
(inside suppress method:)
final String name = this.name + ""-SUPPRESS-"" + (this.suppressionIndex ++);
```

So the suppressions would still be guaranteed a unique name, but any renumbering would only affect suppressions that are peers under the same KTable node."
214982312,5567,vvcephei,2018-09-04T16:28:04Z,"Yes, it seems like this would also work, but TBH it seems a little roundabout to me. Is the objective to avoid duplicates of the error string?

Maybe we could just extract the exception construction into a method:
```java
if (streamsGraphNode == null) {
  throw getNonWindowedGraceSearchException(chain);
}
...
public TopologyException getNonWindowedGraceSearchException(final String chain) {
  return new TopologyException(
    ""Window close time is only defined for windowed computations. Got ["" + chain + ""].""
  );
}
```

Then again, I have doubts about whether adding an extra method to de-duplicate the (to me) simple logic of creating the exception is worth it at all. Especially considering that there's no particular reason that the exception message needs to be exactly the same in these two cases.

What say you?"
214983712,5567,vvcephei,2018-09-04T16:32:32Z,"I should say: I don't want to add it to *this* PR (which is already large). If you think there's merit to this idea, I would tackle it in another PR."
215067976,5567,vvcephei,2018-09-04T21:10:15Z,This is where we insist on strict buffering for final results.
215068069,5567,vvcephei,2018-09-04T21:10:32Z,see: https://github.com/apache/kafka/pull/5567/files#r215067976
215315559,5567,bbejeck,2018-09-05T15:21:49Z,"Makes sense to me to require users to specify how to handle the different scenarios as each user will have different needs concerning when they want a result emitted.  

But I'm wondering if we want to restrict users from having to supply a `StrictBufferConfig` in all cases.  I think there could be a case when faced with the option of either shutting down or emitting an early ""non-final"" result; there is a subset of users that would prefer an initial possible duplicate result vs. a production shut-down. So maybe the type could be `<BC extends BufferConfig>` 

 Unless of course, I'm wrong with my assumptions about the semantics of `StrictBufferConfig.shutDownWhenFull`."
215333841,5567,bbejeck,2018-09-05T16:09:07Z,"What are the semantics around `StrictBufferConfig.unboundedBuffer`?  Wouldn't have the same behavior as the `StrictBufferConfig.spillToDiskWhenFull` option? 

What is the behavior of `shutDownWhenFull`? 

"
215339913,5567,vvcephei,2018-09-05T16:27:17Z,"There are several buffering options available that provide strict buffers:
* unbounded (just keep allocating more until you get an OOME)
* bounded, shut down gracefully when full
* unbounded, using disk instead of memory

I think this provides plenty of options. I'm not sure we want to get into providing an option that says ""emit final results only (unless it turns out you need more memory than you allocated, in which case emit both intermediate and final results)"".

1, that's pretty confusing.
2, The main justification for ""final results mode"" is that the destination is some kind of system that doesn't permit updates. If we *claim* to support final-results-only, but send updates instead, we risk causing harder-to-debug downstream failures.
"
215340550,5567,vvcephei,2018-09-05T16:29:14Z,"an unbounded buffer has no specified bounds. It will just grow until the application runs out of heap.

""spill to disk"" and ""shut down when full"" are both bounded. You specify some limits on the size of the buffer or the number of keys, and it either shuts down gracefully or switches to disk when it runs out of space."
215341057,5567,bbejeck,2018-09-05T16:30:36Z,I'm not opposed to the idea; I guess we'll need to weigh the pros and cons of having a separate approach to naming some nodes. 
215342060,5567,bbejeck,2018-09-05T16:34:06Z,"The overall objective was merely to reduce the duplication in code, as the two blocks seemed the same to me less the condition triggering the exception.  But you have a point so maybe leave as is."
215347029,5567,bbejeck,2018-09-05T16:49:32Z,"Could maybe use `EmbeddedKafkaCluster.deleteAllTopicsAndWait` instead, as we've had problems in the past with test flakiness by not deleting internal topics."
215350959,5567,bbejeck,2018-09-05T17:02:18Z,Why not use `InegrationTestUtils.produce..` method variants?
215352500,5567,bbejeck,2018-09-05T17:07:10Z,"Same here, why not use one of the `IntegrationTestUtils.waitUtill...` methods for consuming records? "
215352630,5567,bbejeck,2018-09-05T17:07:35Z,was this line intentional or leftover debugging?
215354084,5567,bbejeck,2018-09-05T17:11:57Z,"We'll need a timeout here because unless I'm missing something, this method could block forever (or at least make the test run longer than necessary)  if the expected number records aren't received.  

Another plug for using `IntegrationTestUtils.wait..` methods as those methods provide timeout functionality."
215357092,5567,bbejeck,2018-09-05T17:19:52Z,Thanks for adding the integration test.  I'm thinking we'd want to add test methods using the different `StrictBufferConfig` options and the `EagerBufferConfig`.
215360179,5567,bbejeck,2018-09-05T17:29:35Z,"great coverage, thanks for adding"
215371772,5567,vvcephei,2018-09-05T18:05:33Z,"Ah, good catch. Thanks!"
215373053,5567,vvcephei,2018-09-05T18:09:55Z,"To improve these tests' readability, I created a simplified ""record"": `KVT`. This allows us to produce a batch of records, all with potentially different timestamps, at once.

The `IntegrationTestUtils` methods only accept a collection of `KeyValue`, all with the same timestamp, or a collection of `V`, all with the same key and timestamp."
215373138,5567,vvcephei,2018-09-05T18:10:12Z,"Basically, the same explanation as above."
215373244,5567,vvcephei,2018-09-05T18:10:32Z,Doh! This is left over. Sorry.
215373535,5567,vvcephei,2018-09-05T18:11:31Z,Fair enough. I'll add a timeout.
215374308,5567,vvcephei,2018-09-05T18:14:00Z,"Agreed.

Currently, no actual buffering is implemented. Attempts to do anything but immediately emit will throw an exception. This is verified by the processor test.

In the next PR, I implement buffering, and I have the integration tests for the different strategies there."
215374466,5567,vvcephei,2018-09-05T18:14:27Z,Thanks! I actually used the IDEA code coverage tool for this one :)
215375753,5567,vvcephei,2018-09-05T18:18:23Z,"I considered adding to the Utils, but I would also have to add KVT. It seems unnecessary at this point to dump a new method and KeyValue-esque class, which are only used in this test, into Util."
215378941,5567,vvcephei,2018-09-05T18:27:54Z,"Ah, I remember what I was thinking: that if we run integration tests in parallel, we should only delete the topics we need to. But maybe we don't parallelize the methods within a test class, and we don't share embedded clusters between test classes?"
215663637,5567,bbejeck,2018-09-06T15:09:45Z,"Fair enough.  But IMHO I can't envision a case where users will voluntarily let a production system shut down, but again that's just my opinion."
215673339,5567,bbejeck,2018-09-06T15:34:05Z,nit: do you need `cleanStateAfterTest`?   In  `getCleanStartedStreams` you already call `driver.cleanUp` and delete all topics in `cleanStateBeforeTest` 
215677905,5567,vvcephei,2018-09-06T15:46:05Z,"The (maybe imaginary) scenario I had in mind was:
* your business logic depends on exact buffering behavior (otherwise you wouldn't be using ""final"")
* you want to bound how much memory the app uses
* you discover that in practice the app needs more memory than your bound

It seems like in a situation like this, you would like a graceful shutdown with a clear message so that you can reconsider your options.
* maybe you take a look at your metrics and discover that you can decrease the window grace period, thereby relieving memory pressure
* maybe you change the environment so you can allocate more memory to the task

Unless you are using IQ or doing something time-sensitive like high-frequency trading, shutting the app down for short periods of time when it's misbehaving should be acceptable.

Having hard failures like this can actually be more operationally friendly than grey failures like frequent GC or apps stealing memory from each other. But of course, it depends on the situation.

There is a possibility of implementing more complex behaviors, such as pausing some tasks to allow others to flush more work, but I don't think we need to think about that right now.

That was all for context. At the end of the day, if we don't think we need the ""graceful shutdown"" mode, I'd rather not implement it."
215678896,5567,vvcephei,2018-09-06T15:48:47Z,It's a belt-and-suspenders thing. We want it at the end so that the tests don't leave garbage around (for example the last test). I also added it at the beginning just in case a prior run got forcibly killed and never had a chance to clean up.
215804228,5567,mjsax,2018-09-06T23:07:21Z,Nit: `by the supplied {@link Suppressed} specification.` (or `configuration`?)
215804652,5567,mjsax,2018-09-06T23:09:17Z,"nit: `k` -> `key` and `v` -> `value` (we should try to avoid abbreviations)

Should we add `final`?"
215805094,5567,mjsax,2018-09-06T23:12:13Z,"Seem I am missing something, but why do we need `K, V` as generic types here?"
215805317,5567,mjsax,2018-09-06T23:13:35Z,nit: `EagerBCImpl` -> `EagerBufferConfigImpl` (avoid abbreviations -- make the code unnecessarily harder to read for newcomers)
215805813,5567,mjsax,2018-09-06T23:16:33Z,as above
215806026,5567,mjsax,2018-09-06T23:17:59Z,Seem the interface lacks some JavaDocs to explain this?
215806779,5567,mjsax,2018-09-06T23:23:38Z,`boundedByKeys` -> `maxBufferedKeys` (cf. comment below)
215807022,5567,mjsax,2018-09-06T23:25:12Z,"If we rename above, rename this to `withMaxBufferedKeys` ? (other interfaced -- eg, `Produced` -- also use `withXX` for all non-static methods as counterpart to `XX` for static ones. Might be nice to follow this pattern?

Better suggestions are welcome.

"
215807451,5567,mjsax,2018-09-06T23:27:42Z,as above: `maxBufferedBytes` ?
215807482,5567,mjsax,2018-09-06T23:27:54Z,`withMaxBufferedBytes` ?
215807736,5567,mjsax,2018-09-06T23:29:19Z,"nit: to follow other interface, the non-static methods should have the `with` prefix (similar below)"
215808672,5567,mjsax,2018-09-06T23:35:23Z,"@bbejeck For the case you describe, `emitFinalResultsOnly` seem to be the wrong user choice, and they should use  `intermediateEvents` with ""infinite time duration"" and limited buffer size."
215809634,5567,mjsax,2018-09-06T23:42:03Z,"I understand the desire, however, it would not solve the overall re-naming issue -- I would prefer to have a holistic solution for the problem instead of introducing complex code that does not really help."
215809978,5567,mjsax,2018-09-06T23:44:29Z,`findAndVerifyWindowGraceOrThrow` (and remove the comment) ? (self-documenting code FTW :))
215810340,5567,mjsax,2018-09-06T23:46:44Z,nit: comment unnecessary
215810366,5567,mjsax,2018-09-06T23:47:00Z,comment unnecessary
215810406,5567,mjsax,2018-09-06T23:47:25Z,comment unnecessary
215810981,5567,mjsax,2018-09-06T23:51:32Z,"Can we use `<K,V>` directly here? Also let `buildFinalResultsSuppression` return this type instead of windowed?"
215811536,5567,mjsax,2018-09-06T23:55:09Z,"""all parents"" ? `suppress` is only available for `KTable` and thus there should be only one parent node?

Also, do we need to handle chaining of `.suppress()` within `extractGracePeriod`? I also don't understand how there could be multiple parent grace periods (and thus, why do they need to match -- there should only be one anyway?) -- do I miss something?

I also just realize, that windowed-KTables have another issue: we cannot only not join two, but we can also not `.filter(..., Materialize)` because this would also create an ever growing key-value store instead of a windowed store... \cc @guozhangwang @bbejeck Thoughts?"
215813625,5567,mjsax,2018-09-07T00:10:34Z,"Wy not use `new UnsupportedOperationException(""Not implemented yet."")` :)"
215813819,5567,mjsax,2018-09-07T00:11:58Z,"nit: is a negative value valid? If `Duration` does not check this, we should check and throw?"
215814619,5567,mjsax,2018-09-07T00:17:50Z,"Don't understand the second check? Why must the record-time (or window-end-time) be less-or-equal to current stream-time? For record-time, this should be true all the time, but for window-end time, I don't think this is true -- why do we want to fail for this case?"
215815381,5567,mjsax,2018-09-07T00:23:16Z,Could this be `private`?
215816045,5567,mjsax,2018-09-07T00:28:20Z,`KeyValueTimestamp`
215816087,5567,mjsax,2018-09-07T00:28:39Z,`KeyValueTimestamp`
215816526,5567,mjsax,2018-09-07T00:32:14Z,"""shouldAccept ONLY"" -- sound like a negative test but I don't see any exception?

The `doesn't compile because the buffer is eager` part is weird... The condition you express is ensured via the interface -- ie, it does not compile. Thus, I don't think we need a negative test."
215816861,5567,mjsax,2018-09-07T00:35:06Z,"I am fine with this; however, it might be better to take `System.currentTimeMillis()` as seed and LOG the seed -- if it fails, we can reproduced using the logged seed?

Also: it's multiple test methods and I don't think that the execution order is defined. Thus, we should initialize the with known seed, for each test, ie, via `@Before`"
215817497,5567,mjsax,2018-09-07T00:40:48Z,`end` could be `0` and thus `nextInt(end)` would throw. Use `end = 1 + RANDOM.nextInt(Integer.MAX_VALUE - 1);` above
215817777,5567,mjsax,2018-09-07T00:43:16Z,Should `timestamp` not fall into the window boundaries? Not sure if it's important for the test?
215818040,5567,mjsax,2018-09-07T00:45:22Z,Why do we need an extra test for this? Seems to be covered in `finalResultsSuppressionShouldThrow` ?
215818075,5567,mjsax,2018-09-07T00:45:38Z,as above
215818472,5567,mjsax,2018-09-07T00:48:45Z,"There is already an `InternalStreamsConfig` in `StreamsPartitionsAssignor` -- if we want to use it multiple time, should we make it a public internal call that we can reuse here instead of duplicating code? Maybe in a new package `...streams.internals` ?"
215993196,5567,vvcephei,2018-09-07T15:14:19Z,"Good catch on the abbreviations. About `final`, I'll check. It might not be allowed in an interface method header."
215993869,5567,vvcephei,2018-09-07T15:16:21Z,"Hmm. Maybe we don't...
I used to have serdes in the buffer config, but they are gone now.
Also, we need K/V in `Suppressed` so that we can bound the K to be Windowed for final updates.

I'll try and drop them. It would be awesome if we don't need them."
215994325,5567,vvcephei,2018-09-07T15:17:42Z,"It's actually `EagerBritishColumbiaImpl`, but I can see how you'd get confused."
215995058,5567,vvcephei,2018-09-07T15:19:44Z,Thanks for the suggestion. I've really struggled to come up with good names for these static/instance methods that make sense in context. I'll give this a shot.
215997251,5567,vvcephei,2018-09-07T15:26:19Z,"I figured that ""verify"" means ""orThrow"".

The comment is part of a series that explains why the casts in this method are safe.

I think that it would be very difficult to understand later why it's ok to cast `K` to `Windowed`, and then why we would cast the resultant suppression back to `K`."
215998650,5567,vvcephei,2018-09-07T15:30:34Z,"No, the final results suppression is defined only for Windowed tables, by design.

As the comments explained, we happen to know that K is a Windowed, but the generic bound is still just `K extends Object`, so we have to ""forget"" our knowledge that K is a Windowed in order to return the result properly typed.

This is what I was trying to explain in the comments, and your question illustrates why I think the comments are necessary. But I guess I need to try harder to make the comments explain this situation fully."
216000969,5567,vvcephei,2018-09-07T15:37:15Z,"The last point is correct. I thought that we were already aware of this shortcoming... but now I don't remember who I was talking to about it.

About the parents... The immediate parent of suppress is a single KTable, but may not be the one with a defined grace period. It might be a filter, in which case we need to examine the parent of the filter, or it might be a join, in which case we need to examine *both* parents of the join.

The fact that we currently have a design flaw that prevents this situation doesn't imply that we should encode this limitation here. Once we fix that design flaw, we would have to remember that we also coded that flaw into this method and come back to revert it to the state it's in right now!"
216002077,5567,vvcephei,2018-09-07T15:40:42Z,"`\_()_/` Sure, that would work too. I'm removing this exception in the next PR, so I don't think it matters much.

Using the specifically defined exception makes it slightly easier to make sure I remove all usages of it in the next PR, as I can delete the exception class and the code won't compile until I remove all usages."
216002534,5567,vvcephei,2018-09-07T15:42:13Z,I don't see why I need to worry about that here.
216004459,5567,vvcephei,2018-09-07T15:48:09Z,"As you pointed out, a record in an open window will have a time greater than the current stream time. This means we should buffer it (when it's implemented) and *not* immediately emit it. "
216004765,5567,vvcephei,2018-09-07T15:49:08Z,"Hmm, I'll check. I don't think so, but I don't remember why."
216005077,5567,vvcephei,2018-09-07T15:50:07Z,K ;)
216006317,5567,vvcephei,2018-09-07T15:54:01Z,"The purpose of this ""test"" is explicitly to demonstrate what compiles and what doesn't.

It's of course not possible to write code that doesn't compile in order to demonstrate that it doesn't compile, so I wrote the code and commented it out. If you'd like to ""run"" the test, you can uncomment the ""negative test"" lines and verify they are not permitted.

If you think this is silly, I can rename the test."
216007362,5567,vvcephei,2018-09-07T15:57:30Z,"What you are describing is what I had initially. It seemed a little too fancy, though. I think if we like this approach, we should consider bringing in a real fuzzing framework instead of hand-rolling it.

about `@Before`, this is a good point. I'll do it."
216008190,5567,vvcephei,2018-09-07T16:00:14Z,"In practice, it would, but since the processor itself doesn't make any assumption between the record timestamp and the window boundary, it doesn't matter for the test. In fact, the test verifies that the processor makes no such assumption."
216008629,5567,vvcephei,2018-09-07T16:01:31Z,"It's a stub of a test that I have in the next PR. When I extracted this one and replaced buffering with exceptions, I just replaced all the test bodies with verification of the exception."
216008726,5567,vvcephei,2018-09-07T16:01:55Z,explained above.
216009989,5567,vvcephei,2018-09-07T16:06:32Z,"I considered that, but it seemed better to keep the MockProcessorContext in test-utils as decoupled as possible from the internals of Streams.

In general, when we're modifying Streams code, we exercise a lot of freedom in modifying internals, and I don't want to risk accidentally changing the behavior of the mock if we decide to add some more stuff to the one used by StreamsPartitionAssigner.

If it helps, I can give this one a different name..."
216041459,5567,guozhangwang,2018-09-07T18:01:14Z,`EagerBritishColumbiaImpl` yeah that's what I thought too!
216042101,5567,guozhangwang,2018-09-07T18:03:42Z,How about `emitIntermediateEvents` to be better aligned with `emitFinalResultsOnly`?
216043674,5567,guozhangwang,2018-09-07T18:09:34Z,"Could you elaborate why we need `BC extends BufferConfig<K, V, BC>` as its template?"
216044564,5567,guozhangwang,2018-09-07T18:12:29Z,`KStreamWindowAggregate` is used only for time windowed aggregations (the class name was added when we only have time windowed at that time). For session windowed aggregations we have `KStreamSessionWindowAggregate`.
216044943,5567,guozhangwang,2018-09-07T18:13:49Z,"@mjsax With this idea, whether or not we insert a suppression or not would not affect any downstream operators, right? Why that would not solve the re-naming issue?"
216046368,5567,guozhangwang,2018-09-07T18:18:38Z,"nit: why put this class under `suppress`? We usually try to get consistent hierarchy among main v.s. test directories unless there is a strong motivation not to. I.e. it is fine to have `KTableSuppressProcessorTest.java` under `suppress`, but this class may just be `KTableSuppressTest` under `kstream/internals`."
216068984,5567,vvcephei,2018-09-07T19:47:04Z,"Yeah, this is another spot where I really struggled with naming. I liked `suppress(intermediateEvents())` aka ""suppress intermediate events"", but the version for final would be like ""suppress all but final events"", and `suppress(allButFinalEvents()` just seems too confusing, so I compromised.

Note that this method isn't saying to _emit_ the events, but actually the opposite: to _suppress_ them. I think the symmetric name would be `suppressIntermediateEvents()`, which looks a little redundant in practice: `suppress(suppressIntermediateEvents())` or `suppress(Suppressed.suppressIntermediateEvents())` :/

Another idea would be to choose one of the synonyms. Since we are accomplishing this suppression via buffering, we could call it `bufferIntermediateEvents` or just `buffer`...

Thoughts?"
216070493,5567,vvcephei,2018-09-07T19:53:12Z,"Ah! this has a name: ""Curiously Recurring Generics"" (or ""Curiously Recurring Template"" from C++).

This allows us to declare builder methods in the interface that return an instance of whatever subclass was used to invoke the method. Such as `BC bufferKeys(final long maxKeys)`. When we call this on an Eager config, we get back an Eager config, and when we call it on a Strict config, we get back a Strict config.

If you recall the weird comment in Windows that says ""all subclasses should override this method so they can return the correct type"", we were looking for the same property. If we had used this pattern, we wouldn't have needed that comment, as the overridden methods would automatically take on the correct return type."
216071247,5567,vvcephei,2018-09-07T19:56:14Z,"Sure, I can do that. I agree 100% on the unit tests.

For this semi-integration test, I stuck it in this package just because `internals` already has like 1.5M test classes in it.

I'll move it."
216071967,5567,vvcephei,2018-09-07T19:59:15Z,"I think `KStreamWindowAggregate` is used for any subclass of `Windows`, of which `TimeWindows` is one.

It's definitely not `SessionWindows`, since `SessionWindows` is not a subclass of `Windows`, but it could be `UnlimitedWindows` or any user-supplied `Windows` subclass."
216142211,5567,mjsax,2018-09-08T22:26:18Z,Ack -- was just a thought.
216142224,5567,mjsax,2018-09-08T22:27:34Z,"I understood the comments -- was just not sure if we can improve the code. If we cannot change the generic type, it's fine."
216142390,5567,mjsax,2018-09-08T22:36:05Z,"Hmmm... I did originally not consider KTable-KTable join -- however, a grace period is only defined for windowed aggregations, right? Thus, this would only make sense if two windowed-KTables are joined?

Thus, if we consider that we might fix joining two Windowed-KTables in the future (was is broker atm), I am wondering, if we should use the maximum grace period over both base-join-tables as required bound for the suppression, instead of forcing both base-windowed-tables to have the same grace period configured?"
216142405,5567,mjsax,2018-09-08T22:37:35Z,I guess `<=` is ok (it was a nit) -- was just wondering if we should use `==` instead.
216142501,5567,mjsax,2018-09-08T22:43:20Z,"Why should we buffer it? If `suppress.isImmediateEmit()` is true, I though we would not buffer it but emit it immediately to obey the config? Or do I miss understand the semantics of `isImmediateEmit()`?"
216142530,5567,mjsax,2018-09-08T22:45:25Z,"Well, I guess nobody will ever uncomment those lines to test if it does not compile -- seems to be dead code to me."
216142579,5567,mjsax,2018-09-08T22:47:58Z,"Ack. Fine with me to not go too fancy. However, for this case I don't see why we need `Random` at all, and not just hardcode some values for window-start-time etc -- if we seed with `42`, we will get some (unknown) but fixed values anyway -- so what do we gain to use `Random` ? We can just put some fixed values into the code directly."
216149809,5567,mjsax,2018-09-09T07:22:32Z,I don't see a big risk in 'coupling'  for this case -- but not a big deal anyway. Renaming doesn't buy us anything. Just leave it as is.
216149889,5567,mjsax,2018-09-09T07:27:07Z,"Because it does not help us, if somebody inserts a `filter()` for example. The overall renaming issue is, that inserting new operator results in re-indexing. This would be a ""local"" solution for `suppress()` only, but no global solution for all operators. Thus, my concern is, that we end up with different solutions for different operators for the same underlying issue. It's about consistency. Does this make sense?"
216358799,5567,vvcephei,2018-09-10T15:08:27Z,"Ah, sorry, I misunderstood the root of your question.

Since the type system doesn't have evidence that `K` extends `Windowed`, we have to do a cast to assign to `<K,V>`. I separated it into the next line just to avoid doing too much in one line of code."
216361603,5567,vvcephei,2018-09-10T15:15:53Z,"Yeah, we could make it more permissive that way.

This is a discussion for the future when we do fix that operation, but it seems safest to support that join only when both streams have the exact same window configuration (otherwise there's no guarantee that the streams have any keys in common).

In such a situation, we wouldn't have to worry about enforcing it here. But for now, I was thinking to be strict about the common grace periods as a basic precaution against mixing window types. (even though the grace period isn't part of the windowed key)."
216365179,5567,vvcephei,2018-09-10T15:24:25Z,"hmm I guess my method name is misleading. It's not that the suppression is configured like ""emit immediately"" (there is no such config option, but maybe there should be).

Rather, it's an internal utility method to indicate whether the buffer config allows us to just emit events that are on-time or late, rather than buffering them.

Regardless, we still need to buffer future events if there is room to buffer them. This logic is just a little murky right now because there isn't a buffer yet.

So this logic is an attempt to emit any event that we can determine right away is legal to emit, since any buffering operation is actually an exception in this PR.

I guess I could have made it simpler by just unilaterally throwing an exception for any record here, but I thought it would be nice to have some non-exceptional paths to have tests for. 

Even though the implementation details will change when we add the buffer, any tests that currently check for events getting immediately emitted should continue to pass on the buffer-based implementation."
216370098,5567,vvcephei,2018-09-10T15:37:27Z,"Yes, this is the downside I was concerned about. I won't make any change to this in this PR.

I've created a Jira to continue the discussion: https://issues.apache.org/jira/browse/KAFKA-7393"
216394282,5567,guozhangwang,2018-09-10T16:49:09Z,"I see. And with this reasoning I think I also like `intermediateEvents` as well.

How about `suppress(intermediateEvents())` and `suppress(untilWindowEnds())` since the latter should be only called for windowed table result, and hence putting the keyword `window` as part of the func name should be fine?"
216394504,5567,guozhangwang,2018-09-10T16:49:54Z,Ack.
216405880,5567,vvcephei,2018-09-10T17:23:55Z,"Ooh! I like it! (Although I think I'll say `untilWindowCloses`, since it waits for the grace period after the ""end"" of the window (which reminds me that I should make sure the window lifecycle is well documented for the 2.1 release))"
216517843,5567,mjsax,2018-09-11T00:52:49Z,"> otherwise there's no guarantee that the streams have any keys in common

Cannot follow here. Can you elaborate?

Also, thinking about this once more: why do we need to force `suppress` to have a larger grace period that it's parents?"
216518207,5567,mjsax,2018-09-11T00:55:52Z,Ack.
216826714,5567,vvcephei,2018-09-11T21:20:33Z,"What I meant is that if the left stream and the right stream have completely different window specs, then the join will be completely disjoint. This is probably a programming mistake, and I think it's better to fail fast."
216827058,5567,vvcephei,2018-09-11T21:21:45Z,"About:
> Also, thinking about this once more: why do we need to force suppress to have a larger grace period that it's parents?

I didn't understand. We set suppress's suppression time *equal* to the grace period of the parent."
216870298,5567,mjsax,2018-09-12T01:08:21Z,"> have completely different window specs

This makes sense -- however, if no suppress operation is defined, this would not be detected -- and if we put a check somewhere else, suppress does not need to check it either. Does it? Additionally, even if suppress does this check, I would exclude the grace period from the check.

> We set suppress's suppression time equal to the grace period of the parent.

From my understanding, this method should checks that the grace period is larger than the window-size of the upstream KTable -- we don't need to set the grace-period, but just take whatever the users specified for it. There is no comparison of parent grace period and suppress grace period? And there is no need? Or do I miss something?"
217177283,5567,vvcephei,2018-09-12T20:28:55Z,"Yeah, I think we're miscommunicating.
Suppression does not have a ""grace period"", it only has the ""emit after"" config.

The method `findAndVerifyWindowGrace` only extracts the exact specified grace period, as configured upstream.

We pass the value we get back to `org.apache.kafka.streams.kstream.internals.suppress.SuppressedImpl#buildFinalResultsSuppression`, which then creates a `Suppressed` configuration using that time (the extracted grace period) as the ""emit after"" config, along with setting the `timeDefinition` to the window end time.

Together these configs cause suppression to emit immediately right at the end of the grace period (as configured somewhere upstream)."
217179331,5567,vvcephei,2018-09-12T20:34:57Z,"You raised a separate question about whether we should be strict or permissive if the suppression node actually has multiple parents which specify two different grace periods.

I favored strict because I think this makes the situation more debuggable and comprehensible. But you are correct in stating that it's not a correctness issue, since the grace period doesn't affect the key. For this reason, I would be ok with logging a warning and just using the larger grace period.

The consideration we need to weigh is in that situation with two different windowed tables getting merged/joined, assuming they only differ in grace period, how can we ensure that all operators follow the same strategy of taking the larger grace period. It affects windowed store retention as well as suppression. I think taking the larger is a reasonably obvious choice, so maybe it's not a big deal, though."
217476838,5567,mjsax,2018-09-13T17:51:03Z,"Thanks for clarification!

Would be good to get input from @guozhangwang @bbejeck about this. Should we fail? Or should we pick ""max""? I still favor ""max"", but it's not a deal breaker if we fail instead.

Nevertheless, I don't think `suppress` should check the window spec (size/advance etc) -- if a user does a ""weird"" join and we want to disallow it, this check should be done in `join()` instead."
217514478,5567,bbejeck,2018-09-13T19:58:51Z,"I would favor taking the `max` as well, as, IMHO, we can't account for all use cases so better to chose max.  We should document the behavior so users can be made aware of what happens with setting different grace periods

Would it be too much to make this a configurable item?  I'm not sure as we have several config items to consider already."
217805092,5567,guozhangwang,2018-09-14T18:34:31Z,"Regarding the `.filter(..., Materialize)` issue, yes this is a known bug. I think @abbccdda has raised a KIP for fixing this.

Regarding the windowed-KTable / KTable join, there are some very old discussions before on how to tackle it (https://issues.apache.org/jira/browse/KAFKA-7368 is filed recently), and the idea at that time was:

1) we would require the joining table's window-spec to be well aligned, i.e. they must have the same length. Note since window boundaries are the same as they are all starting from the epoch time, it means that same window length guarantees aligned windows, and each the join operation becoming joining each paired windows of the table. We do not yet have session windows at all so this was not discussed, but with session windows it is definitely more complicated..

As for grace period, I think we do not need to make it strict that requires grace to be the same as well. Personally I think either `min` or `max` are fine (I'm slightly leaning towards `min` though :P)."
217850548,5567,mjsax,2018-09-14T21:46:34Z,"Why `min`? `max` seems to be inclusive and guarantees to respect the configs of both upstream operators, while `min` does not?"
217853758,5567,vvcephei,2018-09-14T22:05:17Z,"Ok, 
* fwiw: I 100% agree that it's not this component's job to verify other aspects of the window spec. It should only care about grace period.
* I think the semantics are perfectly well defined with `max`, and I buy these arguments that it's unnecessary to fail. I'd like to log a warning (just during the topology build) in case the mis-match was accidental.

@guozhangwang :
I agree with @mjsax , I don't think that `min` has the right semantics. The purpose of configuring the suppression interval equal to the grace period to begin with is that we already know that the window results will never be updated after the grace period ends. If we set the suppression smaller than the grace period (or in this case _one_ of the grace periods), then there will be an inconsistency between the aggregation results upstream vs. downstream of suppression.

In fact, it's generally ok if we make the suppression interval _larger_ than the grace period. It just means that we'll emit the final result ""after"" the window closes, not ""at"" the window close.

Thus, `max` satisfies everyone's semantics and ensures consistent results throughout the topology. The ""more graceful"" parent will see its final results close after its grace period expires, and the ""less graceful"" one has to wait longer, but they will both never see their materialized state differ from the results downstream of the suppression."
217856097,5567,guozhangwang,2018-09-14T22:22:06Z,"To me the grace period for a window is letting users to trade-off between latency and correctness, and hence users may actually prefer latency over correctness in some cases, so I said in the previous comment that personally I'd prefer `min`, just to express this intention :)

Anyways, I do not have strong preference which option to go, and I'm also fine with `max` if most people feel that way."
217861650,5567,mjsax,2018-09-14T23:04:37Z,"From my understanding, using max will be the default behavior -- users can still manually specify a smaller one, right? It must just be larger than window-size?"
217868845,5567,guozhangwang,2018-09-15T00:24:11Z,"Grace period is the additional time on top of the window-size, and assuming that the window-size of the joining tables are the same, the grace period of 0 can also be used right?"
218138620,5567,vvcephei,2018-09-17T16:29:29Z,"Ok, recall that we are talking about the special case in which the suppress operation has two parents who have different grace periods configured. In this case, ""max"" means that the suppression will be configured with the larger of the two parents' grace periods.

Normally, suppression only has one parent, in which case, it's configured to suppress for N ms, where N equals it's parent's grace period. N might be 0.

If you select ""final results"", there is *no* option to configure the suppression interval. It is _always_ taken from the parent(s)'s configured grace period. To do anything else would threaten consistency.

However, it's always possible just to do regular intermediate suppression, and choose any time you like, larger or smaller than the window size. You're just not guaranteed to get exactly one result per key/window if you pick any time shorter than window size + grace period."
218451116,5567,vvcephei,2018-09-18T14:16:33Z,It is used in the static factory method of the interface.
218604503,5567,bbejeck,2018-09-18T21:40:20Z,nit: can we take this out?
218611217,5567,vvcephei,2018-09-18T22:07:06Z,"sure. I actually used this multiple times during this refactoring, but I think it'll be stable and therefore less useful now."
218622057,5567,mjsax,2018-09-18T23:03:37Z,"This was not included in the KIP, but is also public API. I think, we need to update the KIP accordingly. Similar to `StrictBufferConfig` below?

Or is this for internal use only? For this case, we might want to move them to `internal` package but not nest them within `Suppressed` interface."
218622547,5567,mjsax,2018-09-18T23:06:19Z,"This method is not mentioned in the KIP either. (Or is it renamed `withBufferedKeys` -- if yes, should the return type not be `BufferConfig` as describe in the KIP?)

It seems there is a glitch between the PR and the KIP -- will not comment on it further -- please revisit and update PR and/or KIP to align both (for KIP updates, please follow up on the mailing list; just a FYI email if it's just renaming bunch of methods).

> We fixed the glitched. It will just work itself out naturally."
218623475,5567,mjsax,2018-09-18T23:11:32Z,KIP has different generic types.
218626835,5567,mjsax,2018-09-18T23:30:21Z,nit: remove `this`
218628237,5567,mjsax,2018-09-18T23:39:08Z,nit: `grace` -> `defaultGrace` 
218628588,5567,mjsax,2018-09-18T23:41:03Z,"Should we not honor the grace period as specified in `suppress` and use default one only, if user did not specify one (ie, maybe optimized to `min(userGrace, defaultGrace)` -- not sure if this optimization is desired or not, or if we should ""blindly"" accept `userGrace` even if it's larger than `defaultGrace` what does not by anything and only increases latency...?)

Also, first parameter in `buildFinalResultsSuppression` is called `windowCloseTime`, thus, should this be `windowSize + grace` (or `maxWindowSize + grace` for multiple parents) ?"
218631401,5567,mjsax,2018-09-18T23:57:34Z,Can we define this as `TimeDefinition<K>` ? (and get rid of `getDefaultTimeDefinition()`)
218633801,5567,mjsax,2018-09-19T00:13:26Z,naming: `withUntilTimeElapses` sounds clumsy -- `withElapseTime` ?
218634055,5567,mjsax,2018-09-19T00:15:19Z,`*not*` -> `<emph>not</emph>` (or other html markup)
218634393,5567,mjsax,2018-09-19T00:17:30Z,This is discussed in the KIP. Should it be defined within the interface? Or removed from the KIP if it's not public API but impl detail?
218634430,5567,mjsax,2018-09-19T00:17:49Z,nit: indention
218634537,5567,mjsax,2018-09-19T00:18:21Z,nit: remove empty line
218655637,5567,vvcephei,2018-09-19T03:07:11Z,"Yes, I plan to update the KIP if you all liked this interface."
218656056,5567,vvcephei,2018-09-19T03:10:50Z,"The user is not capable of specifying a suppress time for final results suppressions (see `Suppressed.untilWindowClose`). Only a buffer config. The grace period is the only way to ""set"" the suppression time.

I'll rename the parameter to `gracePeriod` This is correct, since the time definition for final results mode uses the window end as the starting point."
218656226,5567,vvcephei,2018-09-19T03:12:22Z,"We can make it a `<K>`, but only if we drop the `static`. The static may be nice for performance, since we only need one function class & instance, but I'm not sure if it matters that much."
218656366,5567,vvcephei,2018-09-19T03:13:29Z,"agreed. double preposition :( .

On the other hand, I'm not sure what an ""elapse time"" might be. I'll try to think of something."
218656587,5567,vvcephei,2018-09-19T03:15:10Z,"Yeah, if there were no objections on `Suppressed`, I was basically going to smash what I had in the KIP with it and send out an update.

It's still fundamentally the same proposal, but I like the interface we've arrived at via this discussion a _lot_ better than what I originally proposed. So thanks!"
218659126,5567,vvcephei,2018-09-19T03:37:21Z,"But I don't like that this wasn't immediately obvious, so I'll make some clarifying changes."
218666331,5567,mjsax,2018-09-19T04:38:29Z,"I see. Than I need to have a closer look -- I usually compare the PR with the KIP and try to catch gaps... If you don't update the KIP on purpose (what is ok) I need to change my strategy.

> If you could update the KIP, that would be great."
218666810,5567,mjsax,2018-09-19T04:43:23Z,"> The grace period is the only way to ""set"" the suppression time.

Why this limitation? Let's say I have
```
KTable table = ... // windowed with some grace period X
KTable table2 = table.filter(...); // no suppression specified because I want to update filter for each base table update and want to be able to update with max delay X (== grace from above)
table.suppress(...).toString().forEach(); // I also want to react to a single update per key (ie, single final), but I want to have lower latency and want ""single final update"" after X/10 of the time -- I want to specify smaller grace period here
```

Does this make sense?"
218666974,5567,mjsax,2018-09-19T04:45:10Z,"This is not runtime critical -- and having a few more objects (we don't have about hundreds) is not overhead concern IMHO. It would be cleaner with specify type and get rid of the ""cast helper method"" IMHO."
218667036,5567,mjsax,2018-09-19T04:45:41Z,"Maybe ""flushDelay"" ? Or ""suppressPeriod"" ?"
218667438,5567,mjsax,2018-09-19T04:48:30Z,Independent of the KIP update. Why return `EagerBufferConfig` here instead of `BufferConfig`? Just curious. What is the advantage/disadvantage for each case?
218667521,5567,mjsax,2018-09-19T04:49:12Z,"Nit: `maxKeysToStore` -> `max[NumberOf]KeysToStore` ? (also, why `keys`? maybe `Records` is better/more accurate?); do we need `ToStore`? Maybe we can strip this?"
218667577,5567,mjsax,2018-09-19T04:49:42Z,"nit `max[NumberOf]KeysToStore`? (or `Records` -- cf, above)"
218668072,5567,mjsax,2018-09-19T04:54:26Z,`boundedBySize` above vs. `withBytesBound` -- should we align both?
218668328,5567,mjsax,2018-09-19T04:55:56Z,"`passes` -> `passed` ?
`expires` -> `expired` ?"
218668393,5567,mjsax,2018-09-19T04:56:40Z,Why `eventually` ?
218668694,5567,mjsax,2018-09-19T04:59:25Z,"Follow up: for `untilWindowCloses` it makes sense what you say -- my argument is, that offering only `untilWindowCloses` might not be flexible enough."
218669272,5567,mjsax,2018-09-19T05:03:07Z,"I am not 100% convinced about `boundedByKeys` -- no better suggestion atm though. Maybe tomorrow, or anybody else has some more ideas?

Maybe:
`BufferConfig.maxRecords().withMaxRecords()` ?
`BufferConfig.maxBytes().withMaxBytes()`?"
218840039,5567,vvcephei,2018-09-19T15:00:07Z,"Sure, will do right now... including the cover page."
218854676,5567,vvcephei,2018-09-19T15:36:19Z,"Your example use case is legitimate, and it is indeed something that we sacrifice here. 
Allow me to paraphrase:
```
KTable table = ....windowedBy(TimeWindows.of(10).withGrace(100)).count(Materialized.as(""count""));
table.suppress(untilWindowCloses(overrideGrace(10))).to(""output"");
```
This was a point of discussion early on in the KIP. The downside of this API is that querying ""count"" and observing ""output"", we will see divergent results, since ""count"" will permit some records that the suppression drops (say, any record that arrives more than 10ms later than its window).

We felt that if the operator's job is to emit only ""final result"" of the window's aggregation, then that's exactly what it should do. Redefining the window parameters is out of scope.

However, to your second comment, I didn't follow. We don't _just_ offer `untilWindowCloses`. You could alternatively do:

```
KTable table = ....windowedBy(TimeWindows.of(10).withGrace(100)).count(Materialized.as(""count""));
table.suppress(untilTimeElapses(10)).to(""output"");
```

This won't give you ""final results"", since it will still emit updates if they are needed.

Final thought: Like I said, I don't think it's unreasonable what you proposed, but I think it's better to start with something simple and safe. If people are asking for this API later on, we can always add it."
218855860,5567,vvcephei,2018-09-19T15:39:04Z,"Not in this case, since the sentence is in future tense. If I changed the verb to `would`, then it would be subjunctive, and we should say ""passed"" and ""expired""."
218856509,5567,vvcephei,2018-09-19T15:40:45Z,"Because the upstream state is updated immediately, but the suppression buffers the update and only emits it after the grace period passes. _Then_ they will be consistent."
218864237,5567,vvcephei,2018-09-19T15:59:54Z,"I've taken these naming suggestions.
Re: `EagerBufferConfig` -> `BufferConfig`, this works perfectly, it just didn't occur to me!"
218864734,5567,vvcephei,2018-09-19T16:01:10Z,"as discussed elsewhere, this is not a default. It's just the (only) grace period."
218867967,5567,vvcephei,2018-09-19T16:10:53Z,I've imported 12MB of javascript libraries to italicize this word. I hope that's ok.
218918921,5567,vvcephei,2018-09-19T18:38:52Z,"This is the ""partially built"" config you get when you call `untilWindowClose`. It's only partial because we need to get the grace period during the topology build."
218958071,5567,mjsax,2018-09-19T20:47:00Z,Ack. Thanks for clarification. It's a complex discussion... Sorry for repeating part of the KIP discussion here.
218958929,5567,mjsax,2018-09-19T20:49:50Z,"Grammar... At least I can play the ""I am not a native speaker""-card :)"
218958970,5567,vvcephei,2018-09-19T20:50:00Z,No worries. There are indeed a lot of nuances to keep track of.
218959457,5567,vvcephei,2018-09-19T20:51:41Z,"Heh, no worries. Hope I didn't come on too strong."
218959897,5567,mjsax,2018-09-19T20:53:04Z,"That is only regular latency. Also, suppress did not emit anything before -- I wouldn't the ""missing result"" not consider an inconsistency?

I understand what you are trying to say, but ""eventual consistency"" might not be the best term here IMHO. (Might be a nitpick though...)"
218962219,5567,vvcephei,2018-09-19T21:00:43Z,"That is a good point.

I was thinking of ""final results emitted"" in aggregate, as in the downstream state is eventually consistent with the upstream state.

It sounds like your reading is more like ""each result emitted is eventually consistent with its upstream version"". I think this reading is actually the more likely one. And, as you point out, this statement is almost nonsense: if you look at each result when it gets emitted, it is fully consistent with the upstream state.

And of course, ""eventual consistency"" as a term probably opens up a whole bag of worms we don't want to deal with anyway, as people may bring assumptions (and bad associations) from distributed databases.

I'll just say ""will match the upstream"" instead."
219002231,5567,mjsax,2018-09-20T00:14:40Z,Sounds good.
219380904,5567,mjsax,2018-09-21T04:33:21Z,"Do we need this here? Ie, do we care if `StreamsConfig` is logged in the test? If we do care, it might be good to do one PR that fixes it for all test? Or at least make `InternslStreamsConfig` it's own class for sharing?"
219381301,5567,mjsax,2018-09-21T04:37:44Z,Meta comment: why is this called `initialized`? Or should it be `initialize` ? Where does it come from? Could we fix it? Or is it public API?
219381463,5567,mjsax,2018-09-21T04:39:42Z,Nit: simply to `private static long anyLong = 5;`
219381518,5567,mjsax,2018-09-21T04:40:14Z,as above
219381601,5567,mjsax,2018-09-21T04:41:04Z,don't understand the test name. Seem you test for `shouldEmitImmediatelyIfUntilTimeLimitIsZero` ?
219382014,5567,mjsax,2018-09-21T04:43:38Z,as above
219383229,5567,mjsax,2018-09-21T04:51:40Z,"Why do we use a stateless here and a stateful above? If you want to test both cases, might be worth to add corresponding standalone tests?"
219383308,5567,mjsax,2018-09-21T04:52:39Z,Wondering if this test subsumes the test from above?
219384557,5567,mjsax,2018-09-21T05:04:20Z,"If you don't trust the regular cleanup, the question is why? Also, should we apply this patter to other tests, too?"
219384803,5567,mjsax,2018-09-21T05:05:27Z,"This class seems to be duplicated below -- even if it's trivial, might be worth to share the code?"
219385047,5567,mjsax,2018-09-21T05:08:28Z,this can be an `else` as `commitTransaction()` would flush -- not even sure if flushing would be valid without calling `beginTransaction()` before
219385150,5567,mjsax,2018-09-21T05:09:39Z,"Not sure if I understand. If it's a the same argument, could you not use `IntegrationTestUtils.waitUtill..` twice instead of duplicating the code?"
219516422,5567,vvcephei,2018-09-21T14:26:31Z,"This change does fix it for all tests that use the MockProcessorContext.

I'm happy to remove this change from this PR and do a separate one, if you're not happy with the current state of it. I wouldn't want to mess with extracting it and bloating the current PR even more than it currently is. 

I think it's better not to log the config for unit tests. I disabled it because the extra logging made it hard for me to read the unit test output.

Logging the config is really only useful during runtime so we know what configurations people are running when they post logs to the mailing list. When a test fails, you can easily just look at the test's configuration.

Bill brought this up earlier. I can make a non-logging config for sharing between MockProcessorContext and the TopologyTestDriver. I'm hesitant to share the config class between the test utils and production code, as the risk of making an apparently harmless change in one context and screwing up the other is non-trivial.

Let me know if you want me to remove this from this PR now, or I can also just create a Jira to make a class for the test-utils to share."
219516922,5567,vvcephei,2018-09-21T14:28:15Z,It's in InternalProcessorContext. I'll fix it in a separate PR today.
219517355,5567,vvcephei,2018-09-21T14:29:27Z,"uh, yeah, I changed the API and not the test name :/"
219536345,5567,vvcephei,2018-09-21T15:25:42Z,I'll check...
219536496,5567,vvcephei,2018-09-21T15:26:10Z,"Ah, ok."
219540617,5567,vvcephei,2018-09-21T15:38:56Z,"Ah, my apologies to @bbejeck . You are right. This is not the same argument as above, and the `waitUntilMinRecordsReceived` is fine here."
219544268,5567,vvcephei,2018-09-21T15:50:06Z,"Specifically, the ""if"" I described above happens to me when I am debugging integration tests. If you set a breakpoint and then hit the ""stop"" button, it doesn't get a chance to run the normal ""after"" cleanup.

Depending on whether the test uses a randomized state directory or not, this will either give you dirty state for the next run, or it'll just consume more and more disk space in `/tmp` until you run out. I have had both happen to me in the last few months.

I do think we should use the same pattern for all the integration tests."
219544784,5567,vvcephei,2018-09-21T15:52:00Z,"Yes, now it does. They were separate when this method was checking for an exception. Good catch!"
219546526,5567,vvcephei,2018-09-21T15:57:30Z,"The stateless node is the starting point for the search. The two parents are stateful nodes because the test needs the parents to be windowed aggregations, which are stateful.

The child node could be stateful as well, but it's not necessary.

Do you think this test should be duplicated to make sure the search works using both a stateless and a stateful processing code as the starting point?"
219551965,5567,vvcephei,2018-09-21T16:16:47Z,"Ok, here you go: https://github.com/apache/kafka/pull/5671

I'll pull this code from this PR."
219553532,5567,vvcephei,2018-09-21T16:22:11Z,Here you go: https://github.com/apache/kafka/pull/5672
219597029,5567,mjsax,2018-09-21T18:58:08Z,"I understood the setup -- the question is, is there any difference in stateful/stateless parent that is worth testing? If not, we should use the same for all tests (otherwise, it's confusion --- at least to me -> ""why do you need to use the one or the other?"" question arrises)"
710561967,11331,jolshan,2021-09-16T23:19:51Z,nit: remove comment -- we add unresolved names now too.
710590303,11331,jolshan,2021-09-16T23:59:44Z,can remove redundant this.topic == null
710592488,11331,jolshan,2021-09-17T00:03:37Z,Since we moved an inconsistent topic ID check to the FetchResponse.of method we may want to remove this and similar checks in other contexts
710592733,11331,jolshan,2021-09-17T00:04:05Z,nit: fix spacing in imports -- there are a few of these in the PR
712488389,11331,jolshan,2021-09-20T20:24:23Z,Not sure if I should go through and rename some of these to `topicIdPartition`
712491310,11331,jolshan,2021-09-20T20:29:01Z,nit: spacing
712495759,11331,jolshan,2021-09-20T20:35:55Z,"I commented out this test, since I removed the behavior to catch inconsistent IDs at this stage. I can try to simulate catching the inconsistent ID later, but not sure if that is helpful to show in a test."
715684131,11331,dajac,2021-09-24T14:55:31Z,"When a session is used, resolving the topic ids is not really necessary here because we should already have the names in the session or we would resolve them later anyway. I wonder if it would be better to do this entirely in the `fetchManager.newConext` based on the context type. Have you considered something like this?"
715684533,11331,dajac,2021-09-24T14:56:02Z,Do we still need this `sessionTopicIds` mapping if we have the topic id in the `topicIdPartition`?
715685093,11331,dajac,2021-09-24T14:56:44Z,nit: Could we add an overload to `partitionResponse` which takes a `TopicIdPartition`? This would reduce the boiler plate code a bit here. 
715686005,11331,dajac,2021-09-24T14:57:54Z,Could we direclty check if the topic name is null here and put the unresolved ones to `erroneous`? This would avoid the filter on the next line.
715687416,11331,dajac,2021-09-24T14:59:43Z,Should we create `tp` after this check? We could also create a `TopicPartition` as we don't really use `TopicIdPartition` for the metric.
715690689,11331,dajac,2021-09-24T15:03:55Z,Side note here: I think that we should implement `override def elementKeysAreEqual(that: Any): Boolean` from the `ImplicitLinkedHashCollection.Element` interface to make it clear that we do this for comparing elements in the collections.
715690947,11331,dajac,2021-09-24T15:04:17Z,Could we add a scaladoc for this method which explains what we do and why?
715691830,11331,dajac,2021-09-24T15:05:25Z,This might not be necessary if we won't resolve topic ids in the request in all cases (see my previous comment).
715695751,11331,dajac,2021-09-24T15:10:21Z,"Do we still need to return `INCONSISTENT_TOPIC_ID` a top level error? Fetcher prior to this change would need it, for sure. With this PR, we actually don't want the fetcher to treat it as a top level error but rather as a partition error. We need to think/discuss this a little more, I think."
715697860,11331,dajac,2021-09-24T15:13:12Z,Not related to this line. Don't wee need to update the fetcher to handle the topic id errors at the partition level?
715735268,11331,jolshan,2021-09-24T16:04:14Z,We could I suppose? I think the only difference is whether we pass in these values or the fetch request itself (+ topicName map). I don't know if how we handle changes based on context type (besides full/sessionless sessions not having forgotten topics). We could save time translating though if we end up having something like an error session.
715736288,11331,jolshan,2021-09-24T16:05:50Z,Hmmm maybe not. Looks like I just put into this map but never get anything.
715738000,11331,jolshan,2021-09-24T16:08:24Z,yes we will need to do that.
715740312,11331,jolshan,2021-09-24T16:11:26Z,"Yeah. We can change this but the issue was with how we deal with this partition after the error is returned. With the changes to the FetchSessionHandler, we will be able to distinguish the topics, but the implementation I have now still delays partitions on a topic partition level. We don't want to delay the topic partition with the valid ID though! There may be something we can do to handle this case."
715740801,11331,jolshan,2021-09-24T16:12:12Z,we should do that in addition to this method?
715743267,11331,jolshan,2021-09-24T16:16:04Z,"I'm not sure I follow here. We have an unresolved partition in the session and we are updating it. 
Why would we not resolve the partition? I suppose it will get picked up by the forEach partition resolving process, but not sure how the earlier comment applies here."
718910814,11331,jolshan,2021-09-29T21:51:26Z,It seems like right now `elementKeysAreEqual` is just `equals`. Is the idea in implementing this to prevent someone else from doing so and not using `equals`/the logic from equals?
718911636,11331,jolshan,2021-09-29T21:53:20Z,Or is it that the javadoc says things like `key.elementKeysAreEqual(e) and key.hashCode() == e.hashCode()` so we should be using elementKeysAreEqual in FetchSession?
718912221,11331,jolshan,2021-09-29T21:54:28Z,^ this is still something we need to resolve.
719649068,11331,jolshan,2021-09-30T18:15:00Z,Reassigning partitions takes a topic ID partition unfortunately. But I suppose we can change that. Not sure if we want to distinguish between reassigning partitions if we had two with the same name in the session.
719668223,11331,jolshan,2021-09-30T18:42:24Z,"For the replica fetcher, we could choose not delay partitions with this error. Seems like in the fetcher, we just choose whether to update metadata. So maybe this won't be too difficult.

Alternatively, we change the fetching flow to contain topic ID earlier in the process and so we can include in the error response as well. That would be a lot of work. 

Still need to think through the current setup to make sure we aren't losing critical data in this state."
727155822,11331,dajac,2021-10-12T13:48:07Z,"@jolshan I have been looking at the changes in the `FetchSessionHandler` as well at the changes in the related classes. I am a bit worried by two things: 1) the `FetchSessionHandler` is quite complicated now, at least a bit more than before; and 2) the reliance on the request version is spread in many places now.

It seems that we could get away with a simpler solution which, I think, cover all the cases as well. At the moment in the `FetchSessionHandler`, we track the `added`, `removed` and `altered` partitions and the `FetchRequest` is constructed based `next` (`added` + `altered`) and `removed`. Now imagine that we would track another list `replaced` (or `upgraded`...). We would add a partition to this list when we detect that the topic id of the partition in `next` is different from the one in the session. Then, we would pass that new list to the `FetchRequestBuilder` as well. In the builder, we would add it to the forgotten set if version >= 13 or ignore it otherwise.

I have tried to implement this based on `trunk`: https://github.com/apache/kafka/commit/a1de3910ddb9b64d0890dfd61a2e8263f2aa4864. I think that we should be able to do something similar based on your version which uses `TopicIdPartition`.

The pros is that the version handling remains in the `FetchRequest` class. The cons is that it does not allow to restart the session immediately without doing a round-trip to the broker, which is not a big deal as this could only happen during the upgrade.

What do you think? Would this approach cover all the cases?"
727909392,11331,dajac,2021-10-13T10:12:09Z,I have simplified the code and removed a few Maps along the way. Here is the diff: https://github.com/apache/kafka/compare/trunk...dajac:KAFKA-13111.
727996093,11331,dajac,2021-10-13T12:12:52Z,"@jolshan With Ismael's PR (https://github.com/apache/kafka/pull/11374), this trick does not work any more. We need to think about an alternative/better approach. "
728096221,11331,ijuma,2021-10-13T14:00:29Z,"Hmm, how does my PR affect this?"
728115288,11331,dajac,2021-10-13T14:19:30Z,"Actually, you're right. That is not entirely true. I thought that the `requireNonNull` for the `topic` in one of the [constructor](https://github.com/apache/kafka/pull/11374/files#diff-3d6aa1dec2a2548f28148717926536cc937acec2ab4bd03a7bcdc58c84a6cbbaR38) would prevent this to work. However as we use the other `TopicIdPartition` constructor in this case, it is not impacted by the `requireNonNull`."
728124707,11331,dajac,2021-10-13T14:28:47Z,"In this case, it would be nice if we would have a `TopicIdPartition` which contains an optional topic name. For the context, the issue is that we might have partitions in the fetch requests for which the topic name is unknown or not yet known by the broker."
728127262,11331,ijuma,2021-10-13T14:31:14Z,"We should probably remove that non null check, since it's weird to have it only in that one path. I can submit a PR."
728132912,11331,dajac,2021-10-13T14:36:38Z,"Sounds good, thanks!"
728455837,11331,jolshan,2021-10-13T21:20:16Z,"I think we may even be able to get away with fewer maps. I see in the commit you have we add to topicIDs at the start but I'm not sure that works if we have more than one ID for a topic. I was thinking if we stored the ID in the fetch data, we wouldn't need to build a map from ids to names. Do we still use that anywhere?"
735854783,11331,jolshan,2021-10-25T18:25:53Z,TODO: We can also put toForget back after the update step as we handle forgetting using different IDs.
735863741,11331,jolshan,2021-10-25T18:38:22Z,"TODO 2: if we have an update with an unresolved name, should we change the name to be unresolved here? I think we should but want to confirm."
735935021,11331,jolshan,2021-10-25T20:22:50Z,Do we not care to change IDs if the data is equal? We wouldn't usually send a request and I don't know if it is possible to even have the same data in such a case.
741789472,11331,dajac,2021-11-03T10:20:45Z,Could we iterate over `sessionPartitions` and directly populate `sessionTopicNames` by using `putIfAbsent` or even `put`? The grouping seems unnecessary to me here unless I am missing something.
741791658,11331,dajac,2021-11-03T10:22:16Z,"As `toSend` is not used before L288, how about putting this line over there?"
741793046,11331,dajac,2021-11-03T10:23:03Z,Not related to this PR but could we use `Collections.emtpyMap` here? That would avoid allocating a `HashMap` all the times.
741795602,11331,dajac,2021-11-03T10:25:08Z,Same comment as before.
741796143,11331,dajac,2021-11-03T10:25:34Z,nit: Could we align like it was before?
741796825,11331,dajac,2021-11-03T10:26:09Z,nit: This change and the following ones do not seem necessary. I would revert them back.
741797904,11331,dajac,2021-11-03T10:27:04Z,Is this method still used? I can't find any usages of it.
741798898,11331,dajac,2021-11-03T10:27:52Z,It seems that this method is not used anymore. Could we remove it?
741801854,11331,dajac,2021-11-03T10:30:20Z,"This block is identical to the previous one. Should we pull it into a helper method? (yeah, I know, I wrote this...)"
741804871,11331,dajac,2021-11-03T10:32:43Z,Should we add a comment here which explains that the topic name might be null in `TopicIdPartition` if we were unable to resolve it?
741806142,11331,dajac,2021-11-03T10:33:46Z,I would also add a small comment here.
741817990,11331,dajac,2021-11-03T10:46:40Z,"Putting this here but it is not related to this line.

It seems that we have an opportunity in `processFetchRequest` to better handle the `FETCH_SESSION_TOPIC_ID_ERROR` error. At the moment, it delays all the partitions. It seems to me that we could retry directly, no? If you agree, we could file a Jira and address this in a subsequent PR."
741818303,11331,dajac,2021-11-03T10:47:08Z,"Yeah, that would be great. `topicPartition.topicPartition` looks really weird while reading."
741823983,11331,dajac,2021-11-03T10:55:09Z,"nit: Should we format the code as follow?

```
override def hashCode: Int = {
  if (topicId != Uuid.ZERO_UUID)
    (31 * partition) + topicId.hashCode
  else
    (31 * partition) + topic.hashCode
}
```"
741826013,11331,dajac,2021-11-03T10:58:00Z,`that.canEqual(this)` seems weird to me. It seems that we could just remove it.
741938548,11331,dajac,2021-11-03T13:30:57Z,"nit: The if/else inline reads a bit weird. Should we extract the if/else?

```
this.eq(that) || if (this.topicId != Uuid.ZERO_UUID)
  this.partition.equals(that.partition) && this.topicId.equals(that.topicId)
else
  this.partition.equals(that.partition) && this.topic.equals(that.topic)
```"
741939805,11331,dajac,2021-11-03T13:32:18Z,nit: We could add another constructor which takes a `TopicIdPartition`.
741940362,11331,dajac,2021-11-03T13:32:52Z,Is `usesTopicIds` used anywhere in this method?
741943413,11331,dajac,2021-11-03T13:35:54Z,nit: How about naming it `cachedPartitionKey`? We could also benefits from passing `TopicIdPartition` to the constructor directly. 
741945969,11331,dajac,2021-11-03T13:38:24Z,nit: It might be better to encapsulate this in `CachedPartition`. We could add a method called `maybeSetTopicName` or piggy back on `updateRequestParams`. 
741964245,11331,dajac,2021-11-03T13:56:41Z,nit: There is an extra space after `== null`
741967434,11331,dajac,2021-11-03T13:59:40Z,nit: We can remove the parenthesis here.
741968413,11331,dajac,2021-11-03T14:00:35Z,I wonder if we should reply with `UNKNOWN_TOPIC_ID` for the topics whose are not resolved.
741968554,11331,dajac,2021-11-03T14:00:43Z,nit: We can remove the parenthesis here.
741969873,11331,dajac,2021-11-03T14:02:00Z,nit: We can use `tp.partition` here and a few other places.
741974511,11331,dajac,2021-11-03T14:06:51Z,nit: Parenthesis after `partitionIndex` could be omitted.
741976525,11331,dajac,2021-11-03T14:08:55Z,nit: Parenthesis after partitionIndex could be omitted.
741982676,11331,dajac,2021-11-03T14:15:06Z,I already mentioned this before but it seems that we could retry immediately in this case when the session was upgraded/downgraded. That would avoid having to wait for the backoff.
741983857,11331,dajac,2021-11-03T14:16:13Z,nit: `topicIdPartition.topic` should work.
741985410,11331,dajac,2021-11-03T14:17:34Z,nit: We could add another `apply` method to `TopicPartitionOperationKey` which accepts a `TopicIdPartition`. That will be convenient.
741987160,11331,dajac,2021-11-03T14:19:01Z,nit: `tp.topic`
741990473,11331,dajac,2021-11-03T14:22:18Z,Do we still use this constructor?
742113471,11331,jolshan,2021-11-03T16:20:20Z,The idea was to not do a put operation for every partition but instead every topic. Maybe grouping is slower though.
742114292,11331,jolshan,2021-11-03T16:21:11Z,Good catch
742116109,11331,jolshan,2021-11-03T16:23:08Z,"FETCH_SESSION_TOPIC_ID_ERROR occurs when we switch from not using topic IDs in the request to using them (or vice versa). I think maybe we'd want to delay partitions to get the latest metadata, but not sure. "
742119691,11331,jolshan,2021-11-03T16:26:48Z,"Hmmm. So we'd sort out the ones with null names? What benefit are we thinking we'll get from this?
"
742121483,11331,jolshan,2021-11-03T16:28:40Z,I think I wrote all of these before the class was updated. but i will change them. :)
742232425,11331,jolshan,2021-11-03T18:40:50Z,Yeah. It's used in 49 places. Some of the places I intentionally left as zero uuids. I can convert all of them to Uuid.ZERO_UUID if we think this may be bug prone.
742285803,11331,jolshan,2021-11-03T19:58:44Z,"This was here before my change, but I can remove it."
742920899,11331,dajac,2021-11-04T14:58:23Z,nit: Is it worth bringing back this line on the previous one as there is space now? It might be too long though.
742926299,11331,dajac,2021-11-04T15:03:40Z,Would it be more appropriate to move the above assertions to `FetchRequestTest`?
742928420,11331,dajac,2021-11-04T15:05:55Z,"Should we also test when the current topic-partition in the session does not have a topic id? In this case, it should not be added to the `toReplace` set."
742931656,11331,dajac,2021-11-04T15:09:21Z,Why do we use 12 here?
742933244,11331,dajac,2021-11-04T15:11:00Z,It is curious that we don't assert the forgotten partitions here. Is there a reason?
742935279,11331,dajac,2021-11-04T15:13:04Z,Is there any reason for this change?
742936005,11331,dajac,2021-11-04T15:13:51Z,Do we still need this change?
742938518,11331,dajac,2021-11-04T15:16:26Z,Do we still need this change?
742938637,11331,dajac,2021-11-04T15:16:34Z,ditto. There is a few other cases in this file.
742939518,11331,dajac,2021-11-04T15:17:24Z,nit: There are two spaces after `=`.
742944210,11331,dajac,2021-11-04T15:22:16Z,The PR changed how some errors are handled in the `Fetcher`. Do we have any tests for this new behavior?
742950222,11331,dajac,2021-11-04T15:28:22Z,nit: It seems that we could use `TopicIdPartition` directly and remove `topicIds` map entirely. We could also pass the `TopicIdPartition` to `buildFetchMetadata`.
742954020,11331,dajac,2021-11-04T15:32:15Z,`0.equals(0)` was very likely put here by mistake.
742963289,11331,dajac,2021-11-04T15:42:04Z,nit: We could get the topic id from `tp*.topicId`.
742968374,11331,dajac,2021-11-04T15:47:22Z,nit: I would expand this comment a little and stress the fact that topic names are lazily resolved when the partitions are iterated over.
742969700,11331,dajac,2021-11-04T15:48:38Z,Should we assert that the `TopicIdPartition` received here contains the topic name?
742975252,11331,dajac,2021-11-04T15:54:17Z,Should we iterate over the partitions in the context to check the `TopicIdPartition`?
742978184,11331,dajac,2021-11-04T15:57:14Z,It seems to be that it would be simpler to declare `fooId` and `barId` and to use them instead of getting them from the map.
742979446,11331,dajac,2021-11-04T15:58:29Z,I wonder if we should add a third topic which is never resolved. What do you think?
742987345,11331,dajac,2021-11-04T16:06:50Z,Should we add any tests for the new logic in KafkaApis?
742993451,11331,dajac,2021-11-04T16:13:22Z,This is not ideal. Could we validate that the topic id is correct as well?
743011890,11331,dajac,2021-11-04T16:33:33Z,"I wonder if we could add a few more unit tests. For instance, we should test the equals/hash methods of the CachedPartition (and possibly other methods there). We might want to add some for other classes as well. What do you think?"
743041695,11331,dajac,2021-11-04T17:06:58Z,"I think that the grouping is slower because it has to allocate another Map, Sets for each Uuid, etc."
743044369,11331,dajac,2021-11-04T17:10:17Z,"I think that would for instance happen when the controller fails over to an older IBP during an upgrade. This should remove the topic ids which means that v12 will be used for the next fetch request and trigger a FETCH_SESSION_TOPIC_ID_ERROR. In this particular case, re-trying directly would be the optimal way to proceed for a follower. I wonder if they are other cases to consider here.

For the consumer, it is definitely different."
743044970,11331,dajac,2021-11-04T17:10:57Z,Right. It seems to be that the `canEqual(this)` does not make any sense here. Could you double check?
743046003,11331,dajac,2021-11-04T17:12:18Z,I guess that it does not change much in the end. I was considering this in order to be consistent with how we handle this for the consumer.
743046927,11331,dajac,2021-11-04T17:13:29Z,"Yeah, that's a good question. I guess that that constructor is convenient for tests but might be bug prone in the regular code. I am tempted to remove it entirely.... What do you think?"
743239514,11331,jolshan,2021-11-04T22:13:41Z,I moved some back.
743240074,11331,jolshan,2021-11-04T22:14:51Z,ah good catch.
743240526,11331,jolshan,2021-11-04T22:15:49Z,"To clarify -- are you referring to a case where we upgraded? ie, it started with no ID in the first request and added one in the second request?"
743241204,11331,jolshan,2021-11-04T22:17:09Z,I could theoretically check replace in the other test that checks multiple scenarios
743241513,11331,jolshan,2021-11-04T22:17:46Z,I'm not sure I follow. Did you mean the other test file? 
743242346,11331,jolshan,2021-11-04T22:19:26Z,"This was the case I tested when we had the bug of sending v13 for this scenario. The idea was that the session was empty and we had the correct topic ID usage, not whether forgotten partitions were added correctly. I can add a check for forgotten partitions for completeness."
743242849,11331,jolshan,2021-11-04T22:20:26Z,It likely had something to do with how the mock client was handling metadata. But that may have been for the older version where we checked NodeApiVersion. I can try to switch it back.
743243111,11331,jolshan,2021-11-04T22:20:49Z,Nope. Looks like another change I forgot to cleanup.
743243975,11331,jolshan,2021-11-04T22:22:40Z,"> append when the controller fails over to an older IBP during an upgrade.

I think I'm misunderstanding something here. Did you mean to say append? "
743349235,11331,jolshan,2021-11-05T02:23:35Z,"Are you referring to how we changed UNKNOWN_TOPIC_ID and INCONSISTENT_TOPIC_ID?

For these cases we have testFetchInconsistentTopicId and testFetchUnknownTopicId which check that we update the metadata for a partition level error."
743349370,11331,jolshan,2021-11-05T02:23:52Z,These tests changed from returning a top level error to partition level error.
743350813,11331,jolshan,2021-11-05T02:28:10Z,What logic are we thinking? Checking that the unresolved topics are handled correctly?
743351017,11331,jolshan,2021-11-05T02:28:54Z,I can add some for the equals and hash methods in CachedPartition. What classes were you thinking of for others?
743354926,11331,jolshan,2021-11-05T02:42:15Z,Hmm. I'm not quite sure why this would not make sense. I believe it is checking the types are correct.
743355748,11331,jolshan,2021-11-05T02:44:59Z,I think we would want to keep the authorization error. Since it just logs a message. The UNKNOWN_TOPIC_ID error would request a metadata update which doesn't make sense when there is an authorization error.
743360648,11331,jolshan,2021-11-05T03:01:38Z,Or are you just referring to a case where we don't ever have topic IDs?
743361897,11331,jolshan,2021-11-05T03:05:57Z,"Looks like most of these changes were done by this commit: https://github.com/apache/kafka/pull/11331/commits/32c6297adb685f1863b8c7eb85f2f0965853a9f8
so I can remove them pretty easily."
743365481,11331,jolshan,2021-11-05T03:18:35Z,I have no idea why this is here.
743369512,11331,jolshan,2021-11-05T03:33:09Z,"Not quite sure what you meant here but I added this for now:
`context1.foreachPartition((topicIdPartition, _) => assertEquals(topicIds.get(""foo""), topicIdPartition.topicId))`"
743370419,11331,jolshan,2021-11-05T03:36:47Z,"We could do that, but then this check will be a bit more complicated. 
`context2.foreachPartition((topicIdPartition, _) => assertEquals(topicNames.get(topicIdPartition.topicId), topicIdPartition.topic))`"
743370521,11331,jolshan,2021-11-05T03:37:06Z,I can think more on this. 
743370708,11331,jolshan,2021-11-05T03:37:39Z,STILL TODO for Friday
743574922,11331,dajac,2021-11-05T11:16:18Z,"Sorry, I meant below assertions not above. Yes, it seems that they are testing the logic of the `FetchRequest` itself and not really the logic of the FetchSessionHandler."
743575635,11331,dajac,2021-11-05T11:17:38Z,Correct. I was referring to the upgrade case. We might need to handle the downgrade case for https://github.com/apache/kafka/pull/11459.
743576928,11331,dajac,2021-11-05T11:20:00Z,"Yeah, it would be good to assert what we expect in `data2` for completeness."
743577292,11331,dajac,2021-11-05T11:20:37Z,"Yes, I was referring to those. Ack, I missed them during my first read."
743578252,11331,dajac,2021-11-05T11:22:24Z,"Yeah, I meant exactly that. How about using `assertPartitionsOrder` helper? The assertion would be more complete."
743578382,11331,dajac,2021-11-05T11:22:41Z,You could use `assertPartitionsOrder` helper here as well.
743578530,11331,dajac,2021-11-05T11:22:54Z,That is right.
743745314,11331,dajac,2021-11-05T15:09:43Z,Should we add or extend a test in `FetcherTest` to cover this change? I would like to have one which ensure that the request sent is populated correctly (especially the replaced part) by the fetcher based on the session handler. It seems that we don't have such test in the suite at the moment.
743749915,11331,dajac,2021-11-05T15:15:11Z,Should we add a few unit tests to validate the changes that we have done in this class? We could add a few to FetchRequestTest (not use if it already exists though).
743750508,11331,dajac,2021-11-05T15:15:55Z,Do we have a unit test for this one and for `forgottenTopics`?
743751246,11331,dajac,2021-11-05T15:16:48Z,There are a few more cases where we could put the partition data back on the previous line in this file.
743753319,11331,dajac,2021-11-05T15:19:17Z,"Sorry, I wanted to say happen."
743753874,11331,dajac,2021-11-05T15:19:55Z,"Anyway, we don't need to address this in this PR. I just wanted to point out that there is an opportunity for an improvement."
743755936,11331,dajac,2021-11-05T15:22:10Z,Do we have unit tests covering those cases? There are almost no changes in `AbstractFetcherThreadTest` so it seems that we don't. Are they somewhere else perhaps?
743756690,11331,dajac,2021-11-05T15:23:01Z,I guess that we could remove it now.
743759128,11331,dajac,2021-11-05T15:25:46Z,"Should we use the same name for both `maybeSetUnknownName` and `maybeResolveUnknownName`? I guess that you could differ by their argument.

If we add unit tests for other methods of this class, should we cover all the methods that we have changed or added as well?"
743763229,11331,dajac,2021-11-05T15:30:37Z,Do we have tests verifying this change?
743764793,11331,dajac,2021-11-05T15:32:14Z,Should we use `equals` instead of `==`? We use `equals` at L304 btw. 
743830587,11331,jolshan,2021-11-05T16:56:46Z,So you are asking for a test that is checking the fetcher builds the request correctly? Is this a test for the fetcher or the builder? 
743830862,11331,jolshan,2021-11-05T16:57:08Z,I can do that but it will take some time. 
743836225,11331,dajac,2021-11-05T17:04:29Z,We should have a test in the Fetcher which ensure that the builder received the correct information. Then we could have one for the request which ensure that the builder does its job correctly as well.
743839587,11331,jolshan,2021-11-05T17:09:17Z,"The part I don't understand is that this building is in a method that sends the requests. I'm not sure how to pull that out and test specifically that the fetcher is getting the correct info. The fetcher is simply pulling from the FetchSessionHandler's build FetchRequestData, so I feel like that is sufficient unless I'm missing something."
743841128,11331,jolshan,2021-11-05T17:11:26Z,"I thought about the same name, but I thought it was a slightly different approach --> looking up in the map where it is maybe there vs. supplying the name. "
743908454,11331,dajac,2021-11-05T18:56:22Z,"Right. You might have to assert on the request in the fetcher as well. As you said, we can't really get the data out from the builder otherwise."
743909554,11331,dajac,2021-11-05T18:58:13Z,"Yeah, I agree with you. Perhaps, we could just remove the maybeSetTopicName and move its logic into the update request params method."
743949497,11331,jolshan,2021-11-05T20:09:27Z,"Ok, so we'll pass a name and the reqData in that method."
743968305,11331,jolshan,2021-11-05T20:47:38Z,So I can write a separate callback for each one that checks the ID.
743973388,11331,jolshan,2021-11-05T20:58:27Z,"Hmm, so this looks like another case of not having a test file for the java (unit test version) I can create that and add the tests you've been mentioning here. Alternatively I can put the tests in the scala integration test file. Seems like there are unit tests mixed in there too."
743973792,11331,jolshan,2021-11-05T20:59:21Z,So #11459 doesn't touch the FetchSessionHandler code. But I can still add these cases.
743976846,11331,jolshan,2021-11-05T21:06:19Z,"I don't think processFetchRequest is tested anywhere. There tests for the much higher level method doWork, so I can try to write one like that and check if there is that partition with error?"
743977547,11331,jolshan,2021-11-05T21:07:56Z,"I think I have the same confusion here as I do for the fetcher tests. I agree that changes should be tested, but I'm not really sure how to do this here."
743979130,11331,jolshan,2021-11-05T21:11:07Z,"I think for correctness either works, but I will switch to equals for consistency."
743981159,11331,jolshan,2021-11-05T21:15:45Z,Ah I'm already doing this.   Ok. sounds good.
743983833,11331,jolshan,2021-11-05T21:21:23Z,Sorry I'm still a bit confused. The request is sent in this method. We don't get access to the request. We have access to the data that is tested in FetchSessionHandler and that is passed into this method where the request is built and sent. 
743997732,11331,jolshan,2021-11-05T21:56:17Z,Nice. This works well.
744103895,11331,dajac,2021-11-06T09:18:49Z,"We must be able to verify that the request sent out by this method is correct. In the unit tests, we mock the network client for this purpose. If I remember correctly, we can pass a request matcher to it. I need to look into the existing unit tests for this class to see how we have done it for other cases.

We might already have tests verifying that the version of the fetch request sent out is correct based on wether topic ids are used or not. If we do, I suppose that we could proceed similarly."
744104240,11331,dajac,2021-11-06T09:22:33Z,Let's create that file and put new unit tests there. That is the way it should be.
744138150,11331,jolshan,2021-11-06T15:41:50Z,Got it. I guess I was wondering if there would be an issue if we change semantics/expected flow for fetch again.
744138204,11331,jolshan,2021-11-06T15:42:12Z,Confirmed this was a strange quirk from 4 years ago
744177445,11331,jolshan,2021-11-06T22:57:45Z,"Seems like the other issue is that FetchSessionHandler.FetchRequestData constructor is private. So if I want to test in another file I need to either make the constructor public, create a FetchSessionHandler and duplicate the code here, or just put the values into the builder directly (skipping the class). I'm open to just putting the values directly if that makes sense."
744875433,11331,dajac,2021-11-08T16:10:05Z,`buildFetch` seems to be well isolated so it should be quite easy to write a few unit tests for it. `buildFetch` returns a `Builder` so you will have to build the request in order to inspect it.
744877634,11331,dajac,2021-11-08T16:12:31Z,"Yeah, that should work. Otherwise, we could also make the method package private and add a few unit tests for it."
744880575,11331,jolshan,2021-11-08T16:15:40Z,I guess the part I didn't understand is that buildFetch's builder is tested in FetchSessionHandler tests. But I guess there is one more method call we can test.
744883112,11331,jolshan,2021-11-08T16:18:27Z,Is there a reason we do this? If the previous data had a topic ID and this one doesn't we should send a different fetch request version and the session will be closed.
744892585,11331,dajac,2021-11-08T16:28:38Z,"Without this, when a topic id is set back to ""zero"", the former topic id is added to the replaced set which is a bit unintuitive, I think. In the end, it does not matter too much because the version is downgraded in this case so the replaced set is ignored. I was debating if it worth handling this case explicitly here."
744905470,11331,jolshan,2021-11-08T16:43:11Z,"I realize we may still want this as if the partition data is exactly the same, we will actually ignore the downgrade which is not good."
744906035,11331,jolshan,2021-11-08T16:43:48Z,I have a test where this happens. 
744942958,11331,dajac,2021-11-08T17:27:58Z,Right. Here I would like to have tests which ensure that the Builder is fed correctly based on the FetchSessionHandler's data.
745010308,11331,jolshan,2021-11-08T19:00:45Z,Do we need to reassign to empty map here? 
745033891,11331,jolshan,2021-11-08T19:35:27Z,I added the initialization in the other builder since we were missing it.
745050242,11331,dajac,2021-11-08T19:59:14Z,That is a good question. I thought that it is better to empty the map if we don't use topic ids instead of keeping a out-of-date mapping. What do you think?
745050475,11331,dajac,2021-11-08T19:59:35Z,Thanks!
745053425,11331,jolshan,2021-11-08T20:04:05Z,I think the session will already have an empty map or close but it don't think it makes a big difference with or without this change.
745060151,11331,dajac,2021-11-08T20:14:32Z,I am not sure that I follow. We should only test the FetchRequest/Builder in FetchRequestTest.
745091391,11331,jolshan,2021-11-08T21:04:31Z,I ended up making the new test file. I was confused because I thought the data object needed to be tested but it doesn't. I think this can be resolved.
745182731,11331,jolshan,2021-11-08T23:56:28Z,"For my understanding, is this line necessary? We are assigning the same topic partition, right?"
745478693,11331,dajac,2021-11-09T10:24:12Z,nit: It might be worth expanding this comment a little more.
745487251,11331,dajac,2021-11-09T10:34:59Z,nit: Could we use `assertEquals`? The advantage is that it ensure that the map contains only what we want.
745488238,11331,dajac,2021-11-09T10:36:18Z,nit: Could we use `assertEquals` here as well?
745492071,11331,dajac,2021-11-09T10:41:12Z,"nit: I wonder if doing the following would be a bit more complete?

```
if (endsWithTopicIds) {
  assertEquals(singletonMap(topicId2.topicId, topicId2.name), handler.sessionTopicNames());
} else {
  assertEquals(emptyMap(), handler.sessionTopicNames());
}
```"
745495072,11331,dajac,2021-11-09T10:45:06Z,"No, it is not. I kept it for completeness."
745497189,11331,dajac,2021-11-09T10:47:49Z,nit: Could we actually compare the content of both collections instead of only verifying their size? That would be more complete.
745499051,11331,dajac,2021-11-09T10:50:23Z,"nit: In this case, we could actually do the following which seems a bit better:

```
TopicIdPartition tp = new new TopicIdPartition(topicId, ""topic"", 0);
```

Then, we can use `tp.topicPartition` when we need it. What do you think?"
745499747,11331,dajac,2021-11-09T10:51:11Z,nit: Could we put this on the top of the test?
745505533,11331,dajac,2021-11-09T10:58:49Z,"nit: Would it be simpler to do the following?

```
Map<TopicPartition, FetchRequest.PartitionData> expected = new LinkedHashMap<>();
// Build the expected map based on fetchRequestUsesTopicIds.
assertEquals(expected, fetchRequest.fetchData(topicNames));
```

We have to use `new TopicIdPartition(topicId1, new TopicPartition(null, 0))` because https://github.com/apache/kafka/pull/11403 is not merged yet.

The advantage of this way is that it test the whole Map, including the ordering."
745506626,11331,dajac,2021-11-09T11:00:14Z,"I am not sure that we gain much by testing this because testing `fetchData` already verify that all the partitions are included, no?"
745511442,11331,dajac,2021-11-09T11:06:56Z,"This is a bit weird. I would have expected a `null` as the topic name if `topicNames` does not contain the mapping, no?"
745512157,11331,dajac,2021-11-09T11:08:07Z,Can't we use `assertEquals`? It seems that it should work here.
745515647,11331,dajac,2021-11-09T11:13:12Z,"As discussed, we must test this."
745518340,11331,dajac,2021-11-09T11:16:58Z,nit: Should we put `topicId` first to be consistent with `TopicIdPartition`'s constructor?
745518525,11331,dajac,2021-11-09T11:17:17Z,Could we add a unit test for this?
745518658,11331,dajac,2021-11-09T11:17:30Z,Could we add a unit test for this new method?
745518818,11331,dajac,2021-11-09T11:17:43Z,Could we add a unit test for this one as well?
745571481,11331,dajac,2021-11-09T12:31:38Z,"nit: fyi, you could use an anonymous class in this case:
```
val fetcher = new MockFetcherThread(fetchBackOffMs = fetchBackOffMs) {
  override def fetchFromLeader(fetchRequest: FetchRequest.Builder): Map[TopicPartition, FetchData] = {
  
  }
}"
745576080,11331,dajac,2021-11-09T12:38:04Z,nit: A space is missing before `10`.
745576158,11331,dajac,2021-11-09T12:38:10Z,ditto.
745577596,11331,dajac,2021-11-09T12:40:05Z,nit: There is an extra space after `forEach(`.
745580364,11331,dajac,2021-11-09T12:43:45Z,"nit: I am not a fan of this. I would usually prefer something like the following in this case. I guess that it is a matter of taste so I leave it up to you.

```
assertEquals(
  Map(
    ""foo"" -> Errors.NONE.code,
    // others
  ),
  resp2.responseData(topicNames, request2.version).asScala.map { case (tp, resp) =>
    tp -> resp.errorCode
  }
)
```"
745595523,11331,dajac,2021-11-09T13:03:52Z,Should we assert the content of `context2`?
745595783,11331,dajac,2021-11-09T13:04:14Z,"I guess that `respData1` is used by mistake here, isn't it? This is a good example why it is better to use `assertEquals` to verify collections instead of iterating over them. The assertions that you have below have not caught this."
745597673,11331,dajac,2021-11-09T13:06:56Z,`startsWithTopicIds` and `endsWithTopicIds` are a bit misleading here. I suppose that they refer to either the broker knows about the topic id or not (present in its metadata cache). Am I right?
745612156,11331,dajac,2021-11-09T13:25:35Z,nit: A space is missing before `{` and `}` should be on a new line for blocks.
745612631,11331,dajac,2021-11-09T13:26:10Z,ditto about the code format.
745613827,11331,dajac,2021-11-09T13:27:41Z,"The size is implicitly verified by the next assertion. We could remove it, I guess."
745613931,11331,dajac,2021-11-09T13:27:47Z,ditto.
745615375,11331,dajac,2021-11-09T13:29:29Z,Can't we use `assertEquals` here?
745686335,11331,dajac,2021-11-09T14:46:02Z,"Thinking a little more about this one. How about doing the following? We could define an helper method `fetchMessages` which wraps `replicaManager.fetchMessages` (takes the same arguments) and returns `Seq[(TopicIdPartition, FetchPartitionData)]`. This would avoid all these callbacks that we have here."
745693355,11331,dajac,2021-11-09T14:52:41Z,"Should we do another round before this one to ensure that a partition would be removed from the context while still having an `IncrementalFetchContext`? We could perhaps have multiple partitions in the context, resolved and unresolved, and then we could remove them one by one."
745699410,11331,dajac,2021-11-09T14:58:37Z,"Should we pass `topicNamesForRequest1` instead of `topicNames` here? In practice, we already use the same mapping in all cases when the context is created."
745701165,11331,dajac,2021-11-09T15:00:19Z,Should we make it private?
745701266,11331,dajac,2021-11-09T15:00:25Z,Should we make it private?
745751523,11331,dajac,2021-11-09T15:49:55Z,"Should we verify what the context contains? This is very likely the most important point to verify in this test, no?"
745789936,11331,dajac,2021-11-09T16:25:51Z,I find those block of code really hard to read. I wonder if we could simplify them.
745863973,11331,jolshan,2021-11-09T17:47:25Z,Is `assertMapEquals` not already doing this? It seems like we check all entries and make sure there is nothing left over.
745864456,11331,jolshan,2021-11-09T17:47:59Z,Unless you are referring to the sessionTopicNames line. 
745865347,11331,jolshan,2021-11-09T17:49:03Z,assertEquals for the toSend/toReplace lists/map?
745867608,11331,jolshan,2021-11-09T17:51:48Z,Are we thinking this would be in the if block? Or in a separate one outside?
745869793,11331,dajac,2021-11-09T17:54:22Z,"Yes, I was referring to `sessionTopicNames`."
745869947,11331,dajac,2021-11-09T17:54:34Z,Right.
745871819,11331,dajac,2021-11-09T17:56:55Z,"Yeah, that could remain in the if block. We could simply replaces those two lines, I guess."
745924906,11331,jolshan,2021-11-09T18:58:38Z,I caught a bug with our PartitionData.equals method from implementing this. (We should be using .equals and not ==)
745925751,11331,jolshan,2021-11-09T18:59:58Z,"I was mostly testing the serialization here, but maybe that's not important? I can remove if we don't need that."
745930497,11331,jolshan,2021-11-09T19:06:49Z,the expectedName will be null if it is not in the map. map.get returns null if the ID is not in the map.
745941900,11331,jolshan,2021-11-09T19:23:09Z,"Is this different than assertPartitionsOrder(context2, Seq(foo0, foo1, emptyZar0))?"
745942320,11331,jolshan,2021-11-09T19:23:44Z,I can change the ordering of these asserts so they are consistent with the earlier ones
745943714,11331,jolshan,2021-11-09T19:25:42Z,You are correct. I can change to `startsWithTopicIdsInMetadataCache` etc if that is not too verbose.
745947154,11331,jolshan,2021-11-09T19:30:38Z,Would we pass in the topicIdPartition we want to match as well?
745963579,11331,dajac,2021-11-09T19:54:23Z,Gotcha. I missed it. Changing the order to be consistent makes sense.
745965419,11331,dajac,2021-11-09T19:57:02Z,Hum.. I was thinking that the method would return the partitions and we would do the assertion after. That would make the helper generic enough to be reused in other places as well. I guess that either ways would work.
746038595,11331,jolshan,2021-11-09T21:01:47Z,Oh so we wouldn't do the filter as part of the method?
746051435,11331,dajac,2021-11-09T21:14:01Z,Correct. The method would return the full response. Then we can assert it.
746077096,11331,jolshan,2021-11-09T21:56:06Z,We can do that. I believe this is being tested via `testFetchSessionWithUnknownId` already. But an explicit test will be good.
746078020,11331,jolshan,2021-11-09T21:57:39Z,"I can add another one that is more explicitly testing this method, but it is tested via `testUpdatedPartitionResolvesId`"
746096126,11331,jolshan,2021-11-09T22:28:59Z,"I think the issue with that approach is it doesn't quite cover the four cases, right? I could keep as is, but have a second partition that just uses IDs and resolve that one on the second round."
746100239,11331,jolshan,2021-11-09T22:36:15Z,Hmm. Could I also just remove the filter and do that after to use just the single callback?
746217460,11331,jolshan,2021-11-10T03:15:51Z,"Is this replacing `testUpdatedPartitionResolvesId`? This is definitely cleaner, but I'm not sure we are covering the same cases here. For context, the test I mentioned before is testing different update scenarios (I probably named it poorly). Mostly the idea is that the update method works correctly. (ie, we update a partition that once had a topic ID to one that does not, etc). Maybe that is covered in some of the other tests I've added (like `def maybeUpdateRequestParamsOrName`) and we can just remove that test. What do you think? I'll also think about this a bit more."
746220127,11331,jolshan,2021-11-10T03:24:38Z,Alternatively I can just rewrite this.
746220311,11331,jolshan,2021-11-10T03:25:14Z,^ this is what I've done.
746376488,11331,dajac,2021-11-10T09:05:48Z,"Yeah, that works as well."
746404173,11331,dajac,2021-11-10T09:29:25Z,"I wrote that test to illustrate how we could improve the readability. My concern is that they are so many lines/assertions in `testUpdatedPartitionResolvesId` and `testToForgetCases` that we get distracted and we have almost missed the most important assertions - the ones which validate what the session contains (`assertPartitionsOrder`). `assertPartitionsOrder` is actually the piece which ensures that the names are resolved or not, right?

"
746420015,11331,dajac,2021-11-10T09:47:54Z,"Could we simplify all of that by defining two `TopicIdPartition`? For instance, we could have the following:

```
val foo = new TopicIdPartition(Uuid.randomUuid(), new TopicPartition(""foo"", 0));
val unresolvedFoo = new TopicIdPartition(foo.topicId, new TopicPartition(null, foo.partition));
```

Then, we can use them where we need them."
746423986,11331,dajac,2021-11-10T09:52:41Z,We usually prefer to not use `any*` but to rather provide the expected values.
746425514,11331,dajac,2021-11-10T09:54:31Z,We have two paths (fetch from follower and fetch from consumer) in `handleFetchRequest` where we handle unknown topic names. Should we parameterize the test to cover both of them?
746433053,11331,dajac,2021-11-10T10:03:26Z,"I am not sure that I understand the value that we get out of this logic. `updateAndGenerateResponseData` creates a FetchResponse based on its input. Therefore, the response that we assert is not so surprising in the end, right? It will contain `INCONSISTENT_TOPIC_ID` if when the method gets it as an input. This logic would make sense for a test which verifies `updateAndGenerateResponseData` but looks like a distraction in a test which verify the name resolution logic. Am I missing something here?"
746433401,11331,dajac,2021-11-10T10:03:52Z,nit: You could use `assertEquals` as it calls `equals`.
746461644,11331,dajac,2021-11-10T10:38:21Z,I guess that it does not hurt to keep it.
746783340,11331,jolshan,2021-11-10T16:45:20Z,"There are two places it may be resolved -- either in the update method if the partition with the new ID is sent in the request or in the assertPartitionsOrder. I was also trying to ensure the correct error messages are returned in the response specifically via `updateAndGenerateResponseData`, but maybe we don't care about this here?"
746785542,11331,jolshan,2021-11-10T16:47:42Z,I copied this from the test above.   Wasn't sure if we wanted consistency amongst the tests.
746789815,11331,jolshan,2021-11-10T16:52:21Z,"Sure. We can remove it. I think I was concerned about the correct handling of the resolved partitions (ie, we get a response back that we can actually parse), but maybe that's not really necessary."
746803000,11331,dajac,2021-11-10T17:06:46Z,"Do you mean the correct handling of the resolved partitions by `updateAndGenerateResponseData`? I think testing `updateAndGenerateResponseData` is a good thing in general. Perhaps, we should just put this into a separate test specific to that method? We should also test it for all context types with and without topic id, I guess."
746805054,11331,dajac,2021-11-10T17:09:04Z,"Right. The question is how to validate that the first update method works? You have to get the partitions from the session as well, isn't it?"
746807276,11331,jolshan,2021-11-10T17:11:51Z,I suppose so. I wonder if we should even include the update method at all then...
746826740,11331,dajac,2021-11-10T17:35:47Z,What do you mean?
746829235,11331,jolshan,2021-11-10T17:39:01Z,"If we always resolve when iterating through the partitions, then do we need to resolve via the update method?"
746836952,11331,jolshan,2021-11-10T17:49:20Z,Is there a way to make such a test without duplicating the newContext portions? 
746852321,11331,dajac,2021-11-10T18:01:40Z,"Yeah, that is not really necessary as you said. I don't mind if you remove it."
746867390,11331,jolshan,2021-11-10T18:22:07Z,Should this be a check when topic is null?
746870544,11331,dajac,2021-11-10T18:26:06Z,That could be  
746873252,11331,jolshan,2021-11-10T18:29:51Z,I've concluded that your new test will now cover the necessary cases (especially with your new commit) so I think we can just remove this. 
176640732,4756,ijuma,2018-03-23T05:14:32Z,We don't use logback for anything else. I'd suggest keeping it consistent with the project.
176640868,4756,ijuma,2018-03-23T05:15:57Z,`scalaLogging` is already defined in this file.
176640906,4756,ijuma,2018-03-23T05:16:15Z,This is already defined in this file.
176645062,4756,debasishg,2018-03-23T06:02:06Z,done ..
176645077,4756,debasishg,2018-03-23T06:02:13Z,removed.
176645090,4756,debasishg,2018-03-23T06:02:21Z,removed.
176799631,4756,guozhangwang,2018-03-23T16:56:25Z,Do we need to import these two dependencies? Could we use Kafka's own EmbeddedKafkaCluster?
176799962,4756,guozhangwang,2018-03-23T16:57:34Z,Does it worth to include this dependency at test runtime? cc @ewencp  @ijuma .
176800608,4756,guozhangwang,2018-03-23T16:59:42Z,Could we add the default for Short and ByteBuffer as well?
176801533,4756,guozhangwang,2018-03-23T17:02:47Z,I'm wondering if we could provide default serdes for windowed key as well? See `o.a.k.streams.kstream.WindowedSerdes` for Java code.
176802156,4756,guozhangwang,2018-03-23T17:05:08Z,nit: how about rename to `FlatValueMapperFromFunction` for better understanding? Ditto below.
176802758,4756,guozhangwang,2018-03-23T17:06:56Z,"Just for my own education: Is it necessary to add `, _` in the end? What are the possible classes we want to still include?"
176803277,4756,guozhangwang,2018-03-23T17:08:49Z,nit: newline for the second parameter.
176805401,4756,guozhangwang,2018-03-23T17:16:14Z,Why we can ignore the topic here? Ditto below?
176806499,4756,guozhangwang,2018-03-23T17:20:01Z,"nit: `Long2long(_)` to be consistent with others?

Actually, do we need to explicitly call it? I thought it will be implicitly triggered anyways from `Predef`."
176809033,4756,guozhangwang,2018-03-23T17:28:54Z,"Could we add this syntax sugar in the implicit conversion so all classes like `KGroupedStream`, `KTable` and `StreamsBuilder` can use it?"
176809497,4756,guozhangwang,2018-03-23T17:30:34Z,Why do we need the `asValueMapper` here explicitly? Ditto below.
176812165,4756,guozhangwang,2018-03-23T17:39:43Z,"Do we have to convert two parameters to a tuple and then apply the function.tupled? I'm asking this because this is on the critical code path (called per map per record), and if there is non-negligible overhead.."
176812703,4756,guozhangwang,2018-03-23T17:41:40Z,"No `scalastyle:off null` before, is this intentional?"
176812967,4756,guozhangwang,2018-03-23T17:42:37Z,nit: space after comma.
176816332,4756,guozhangwang,2018-03-23T17:53:26Z,"Is it a syntax sugar as `branch`? I'd prefer to keep Java and Scala interfaces consistent, so that if we think it is worthwhile we'd better add it in Java APIs as well, otherwise we should remove it from Scala APIs.

WDYT? @mjsax @bbejeck @vvcephei "
176817831,4756,guozhangwang,2018-03-23T17:57:29Z,"This is a deprecated API in java, we should replace it with Materialized."
176819139,4756,guozhangwang,2018-03-23T18:01:58Z,nit: default to `INFO`?
176819498,4756,guozhangwang,2018-03-23T18:03:20Z,The file `logs/kafka-server.log` seems not appropriate as it is not for kafka broker logs right?
176820184,4756,guozhangwang,2018-03-23T18:05:54Z,"This is a meta comment: I'd suggest we consider adding logback generally for Kafka, instead of sneaking in for Streams Scala wrapper. We can still use log4j for now. See https://issues.apache.org/jira/browse/KAFKA-2717. cc @ijuma "
176897781,4756,debasishg,2018-03-24T03:00:43Z,This library `scalatestEmbeddedKafka` has a nice integration with scalatest. Hence it makes writing tests easier and we don't have to bother starting / managing the embedded kafka instance. The test code becomes very concise.
176897811,4756,debasishg,2018-03-24T03:02:24Z,It's only to demonstrate custom Serdes. We picked up Avro since (AFAIR) @guozhangwang suggested this example in one of the earlier PR discussions. This example goes to show that custom serdes can be handled as seamlessly as primitive ones.
176897854,4756,debasishg,2018-03-24T03:05:35Z,We need this for SAM type conversion which is not fully supported in Scala 2.11. In Scala 2.12 we don't need this. This code base runs both in Scala 2.11 and Scala 2.12.
176898091,4756,debasishg,2018-03-24T03:18:32Z,"The `, _` takes care of the other imports that don't need to be renamed like `Serialized`, `Joined` etc."
176898176,4756,debasishg,2018-03-24T03:23:51Z,Don't find the `topic` being used in de-serializer implementations e.g. https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/serialization/ByteArrayDeserializer.java
176898673,4756,debasishg,2018-03-24T03:40:43Z,"We need to call it explicitly. `Predef` has an implicit conversion between `Long` and `java.lang.Long` but not between `KTable[K, Long]` and `KTable[K, java.lang.Long]` which we are dealing with here."
176899296,4756,ijuma,2018-03-24T04:05:50Z,"Yes, let's stick to log4j in this PR."
176899431,4756,debasishg,2018-03-24T04:10:36Z,This may not be relevant here. We were using scalastyle plugin with sbt. Guess we can ignore it for now.
176899457,4756,debasishg,2018-03-24T04:11:36Z,Maybe we can have a `package object` with such stuff that can be reused across abstractions. Better than repeating for every class. Will do this.
176899731,4756,debasishg,2018-03-24T04:27:28Z,"We can add the following:
```
def count(store: String,
    materialized: Materialized[K, Long, WindowStore[Bytes, Array[Byte]]]): KTable[Windowed[K], Long] = { //..
```
But this one also may be useful when the user just needs to pass in the `keySerde`. She need not construct any `Materialized` which is abstracted within the implementation of the API.

Suggestions ?"
176899763,4756,debasishg,2018-03-24T04:29:11Z,should we remove `logback.xml` ?
176928234,4756,guozhangwang,2018-03-25T04:15:10Z,Generally speaking AK repo tend to avoid dependencies unless it is necessary. I'm wondering if we can improve on Kafka's own EmbeddedKafkaCluster to have the same functionalities as the `net.manub:scalatest-embedded-kafka-streams`.
176928257,4756,guozhangwang,2018-03-25T04:17:34Z,"Are these interfaces only used for built-in primitive types, or are they going to be extended by users for their own serdes, like avro? If it is the latter case we cannot enforce users to always ignore the topic."
176928265,4756,guozhangwang,2018-03-25T04:18:35Z,Ack. Makes sense. Scala `Predef` is not as smart as applying to nested types yet.
176928266,4756,guozhangwang,2018-03-25T04:18:57Z,Ack.
176928367,4756,guozhangwang,2018-03-25T04:28:27Z,"I'd vote for keeping java / scala API consistent, and we are going to remove deprecated APIs in future releases anyway.

In current API we'd only have one additional overload:

```
def count(materialized: Materialized[K, Long, WindowStore[Bytes, Array[Byte]]]): KTable[Windowed[K], Long] = { //..
```

I think for users who do not want to specify the store name at all, they can rely on

```
static Materialized<K, V, S> with(final Serde<K> keySerde, final Serde<V> valueSerde)
```

to still hide the `materialized` parameter with implicit conversion. 

For users who do want to specify the store name, but want to rely on type conversion, we could call `withKeySerde` and `withValueSerde` internally in the implicit conversion so that user only need to give `Materialized.as(storeName)`

Does that work?"
176928378,4756,guozhangwang,2018-03-25T04:28:53Z,"Yup, please remove that file as well."
176929613,4756,debasishg,2018-03-25T05:46:27Z,"We can definitely use Kafka's own `EmbeddedKafkaCluster` to integrate with ScalaTest. In `net.manub:scalatest-embedded-kafka-streams`, the main value add is integration with ScalaTest and hence you don't have to explicitly start / stop server as part of the test. 

Also with Kafka Streams it has very nice constructs like the one we use here .. https://github.com/lightbend/kafka/blob/scala-streams/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/StreamToTableJoinScalaIntegrationTestImplicitSerdes.scala#L77-L99 .. Note you just have to define the transformations and do the publish and consume as part of a closure. No need to start / stop the topology. Hence the test code becomes very concise.

Of course it depends on the opinion of the committee but I think this would be a great addition to the dependency.

Here's a suggestion ..

We use `net.manub:scalatest-embedded-kafka-streams` for now. After all it's a *test* dependency. And work on a separate PR to make the integration between `EmbeddedKafkaCluster` and Scalatest better and in line with the functionalities offered by the library.

WDYAT ? "
176930041,4756,debasishg,2018-03-25T06:12:10Z,+1 .. will remove this overload for `count`.
176931307,4756,debasishg,2018-03-25T07:21:36Z,"The implementation https://github.com/lightbend/kafka/blob/scala-streams/streams/streams-scala/src/main/scala/org/apache/kafka/streams/scala/ScalaSerde.scala#L31-L62 is only for *stateless* serdes implementation where nothing gets stored in topics. For stateful implementation involving topics the user can provide her own implementation of `ScalaSerde`.

We provide this as a reference implementation of stateless serdes that we use in implementing `AvroSerde` in the test.

Of course we can decide if we should bundle this as part of the source or test. But we thought that the implementation may be useful to users implementing stateless custom serdes.

Thoughts?"
176931378,4756,debasishg,2018-03-25T07:25:00Z,+1
176931381,4756,debasishg,2018-03-25T07:25:14Z,+1
176931387,4756,debasishg,2018-03-25T07:25:27Z,+1
176931389,4756,debasishg,2018-03-25T07:25:38Z,+1
176931394,4756,debasishg,2018-03-25T07:25:56Z,+1
176931402,4756,debasishg,2018-03-25T07:26:13Z,+1
176931414,4756,debasishg,2018-03-25T07:26:42Z,Will change the name to `kafka-streams-scala.log`
176931419,4756,debasishg,2018-03-25T07:26:52Z,+1
176985739,4756,mjsax,2018-03-26T05:49:58Z,nit: those are actually sorted alphabetically -- can we clean this up? Thx.
176985752,4756,mjsax,2018-03-26T05:50:06Z,as above.
176986121,4756,mjsax,2018-03-26T05:54:10Z,"Should we add those exclusions? I know that we put exclusions when introducing findbugs because it is not possible to introduce it and rewrite all the code -- but for new code, we should consider changing the code. I am not a Scale person though -- can you elaborate on this?"
176986532,4756,mjsax,2018-03-26T05:58:09Z,Should this be `ByteArrayKeyValueStore`? -- we don't use abbreviations in the Java code base.
176987291,4756,mjsax,2018-03-26T06:05:20Z,Why do we need an `asInstanceOf` here? (same below)
176987359,4756,mjsax,2018-03-26T06:05:52Z,Why do we not import `Bytes` ?
176987713,4756,mjsax,2018-03-26T06:09:03Z,"nit: should we use `[K, V, VR]` as in Java?"
176988085,4756,mjsax,2018-03-26T06:12:20Z,nit: name `V` instead of `VR` ?
176988134,4756,mjsax,2018-03-26T06:12:39Z,nit: name `V` instead of `T` ?
176988730,4756,mjsax,2018-03-26T06:16:53Z,nit: `KVO` -> `KeyValueOther` ?
176988977,4756,mjsax,2018-03-26T06:19:03Z,"For my own education. What is a ""stateless serde"" ?"
176989413,4756,mjsax,2018-03-26T06:22:09Z,"nit: remove spaced -> `{KeyValue, Consumed}`"
176989878,4756,mjsax,2018-03-26T06:26:05Z,This should only take 4 parameters. We simplified the API and this method is deprecated (cf. https://cwiki.apache.org/confluence/display/KAFKA/KIP-233%3A+Simplify+StreamsBuilder%23addGlobalStore)
177137368,4756,debasishg,2018-03-26T15:39:23Z,+1 ..
177184454,4756,mjsax,2018-03-26T18:09:25Z,"Nit: I am not a fan of adding links in JavaDocs, because links might break; better reference to the corresponding class as JavaDoc cross reference?

Also: I am wondering if we should remove this method from the wrapper in the first place? IMHO, it's not a good idea to add deprecated API in new code?"
177185072,4756,mjsax,2018-03-26T18:11:39Z,"A `ValueTransformer` also have `init()`, `punctuate()` and `close()` method. Why is this code much simpler than the wrapper for `transform()` above?"
177185869,4756,mjsax,2018-03-26T18:14:29Z,"As above. What about `init()`, `punctuate()`, and `close()` ?"
177188532,4756,mjsax,2018-03-26T18:23:14Z,"I agree that both APIs should be consistent. Does a `split` add much value compare to `branch`? Btw, this might be related to https://issues.apache.org/jira/browse/KAFKA-5488

I am also open to add a `split()` if we think it's useful."
177189344,4756,mjsax,2018-03-26T18:26:04Z,"Why do we only allow to specify a `keySerde` but not replace the store with a different one?

Scala noob question: would it be possible to have a single `count` / `reduce` etc instead of overloads and use `Option` and implicits to infer optional arguments?"
177201338,4756,seglo,2018-03-26T19:06:51Z,"I'm a fan of `net.manub:scalatest-embedded-kafka-streams`, but I understand the concern about bringing in more deps.  I only mentioned it on the dev-kafka list because I didn't think there was much value in improving the embedded Kafka implementation in `kafka-stream-scala` and making it a public interface because `scalatest-embedded-kafka` already existed.  I wasn't aware of `EmbeddedKafkaCluster`.

If `scalatest-embedded-kafka` were brought into the project then there will be drift between the version of Kafka broker in code and whatever this test lib references.

@debasishg I like your suggestion:

> work on a separate PR to make the integration between EmbeddedKafkaCluster and Scalatest better and in line with the functionalities offered by the library.

Perhaps we can do this now for this PR, but keep it simple.  I could work on it if you're busy."
177206327,4756,vvcephei,2018-03-26T19:25:44Z,"I'd like to confirm that this option is actually safe. Is this a best practice at this point for targeting 2.11? Also, how can we know we're not dragging in other (potentially unwanted) experimental compiler features with this?"
177206719,4756,vvcephei,2018-03-26T19:27:21Z,"Cool. In that case, maybe we should also add 'streams:streams-scala:examples' and put it there?"
177207156,4756,vvcephei,2018-03-26T19:28:57Z,nit: this is a bit cumbersome. Can we do project(':streams:scala-wrapper') and archive: 'kafka-streams-scala-wrapper' or some such instead?
177210106,4756,vvcephei,2018-03-26T19:39:58Z,"I have used `net.manub:scalatest-embedded-kafka-streams` before, and it *is* very nice.

But I also worry about adding dependencies to the core project, even test deps. If our tests become a bit uglier, or if we have some test-util class that duplicates some of the functionality you're using, I would consider that to be a worthy tradeoff for dropping the dependency.

I would absolutely support planning to come back in a follow-up PR to build out support for testing scala code and then terraforming these tests to use the new support. Or even delaying this PR until a test-support one is available."
177211622,4756,vvcephei,2018-03-26T19:46:08Z,"I think it's because we're presenting the `Serde[java.lang.Long]` as a `Serde[scala.lang.Long]`, but casting the Serde won't automatically cast the parameters and returns of its methods. I'm surprised you don't get cast class exceptions trying to use the Java long serde as a Scala long serde. Unless I'm wrong about what this is for..."
177214912,4756,vvcephei,2018-03-26T19:58:11Z,"+1 on not including this method in the wrapper. The code that would use this library is not written yet, so it's better if deprecated methods are simply not available."
177215699,4756,vvcephei,2018-03-26T20:00:27Z,I agree.
177222153,4756,ijuma,2018-03-26T20:23:09Z,"There is no such thing as a `scala.Long` at runtime, Scala changed to use the same classes as Java for boxing around the 2.8 timeframe if I remember correctly. Previously there was a `RichLong`, `RichInt`, etc.

In any case, this seems like a variance issue, but I didn't look into it."
177225591,4756,seglo,2018-03-26T20:35:21Z,"This seems to be a false positive.  FindBugs is reporting that `Serializer` and `Deserializer` should be defined as a different type name than what it's inheriting.  IIRC the consensus earlier is that we want type names the same as the base types they're wrapping (which includes traits and interfaces IMO).  I've updated the FindBugs rule exclusion to be specific to the types generating the violation, rather than the entire `ScalaSerde` file."
177227080,4756,seglo,2018-03-26T20:40:17Z,"I assume it was to disambiguate with `Serdes.Bytes`, but that's not a problem.  I'll update it."
177227812,4756,seglo,2018-03-26T20:42:47Z,I've renamed the type param to `VA` to match the Java DSL.
177228111,4756,seglo,2018-03-26T20:43:44Z,I've renamed the type param to `VA` to match the Java DSL.  Is that OK?
177228492,4756,guozhangwang,2018-03-26T20:45:05Z,"@seglo @vvcephei @debasishg thanks for your thoughts. We do have plans to publish testing-util artifacts inside AK in the future. And in fact we have been doing so for kafka-streams module as a first step and going to do that for kafka-core and kafka-clients soon. In kafka-clients testing utils we are going to include some improved version of embeddedkafkacluster for users to easily write their integration tests that involve interacting with a mock a kafka cluster.

So I'd suggest we stay with the uglier implementation with the existing embedded kafka cluster and not bring in the dependency."
177230451,4756,guozhangwang,2018-03-26T20:51:18Z,"I see. I guess I was a bit misled by the name itself: I was thinking ""stateless"" is for the stateless operators in Kafka Streams DSL, and thinking the inclusion of topic name or not does not necessarily depend on whether the serde is used for stateful or stateless operations.

Your explanation makes sense now. Maybe we can add some comments on top of `StatelessSerde` claiming that this serde class is used for serde where topic names does not affect the serde logic, i.e. topic name will be ignored. If users need some serde mechanism that does differentiate on topic names, please implement the other underlying `Serde` interface."
177231716,4756,guozhangwang,2018-03-26T20:55:24Z,It is discussed in https://github.com/apache/kafka/pull/4756#discussion_r176805401. I think the name `stateless` may be a tart misleading but I cannot come up with a better name yet.
177232798,4756,guozhangwang,2018-03-26T20:59:16Z,"I'm a bit on the fence for introducing Avro as ""the one"" serde in our demonstration examples rather than keeping Kafka and Avro separate, since there are many protobufs / etc fans in the community. 

How about adding avro examples in eco-system repos, e.g. in Lightbend / Confluent / etc's own examples repo They can add their own example? cc @ijuma @gwenshap "
177238405,4756,seglo,2018-03-26T21:19:47Z,"I agree it looks concerning, I'll need to check what other potential features this brings in, unfortunately there's no way to be more specific about just enabling SAM type conversion AFAIK.

We could remove this flag, but we would need to desugar all the places where conversions occur."
177268099,4756,mjsax,2018-03-26T23:44:21Z,"Maybe `SimpleSerde` as name ? ""Stateless"" seems to be confusing."
177268698,4756,mjsax,2018-03-26T23:47:33Z,+1 Sounds reasonable to me.
177289406,4756,debasishg,2018-03-27T02:19:29Z,@mjsax -  Cool .. then we can remove the test `StreamToTableJoinScalaIntegrationTestImplicitSerdesWithAvro` and the dependencies from `build.gradle` .. ok ?
177289806,4756,debasishg,2018-03-27T02:21:20Z,"For the time being, we can remove `split` from the Scala API and rethink if / when it's implemented as a Java API"
177290643,4756,debasishg,2018-03-27T02:28:37Z,In Scala there's an implicit conversion between `scala.Long` and `java.lang.Long` but not between `Serde[scala.Long]` and `Serde[java.lang.Long]` - hence the cast. In fact picked this trick from https://github.com/confluentinc/kafka-streams-examples/blob/4.0.0-post/src/test/scala/io/confluent/examples/streams/StreamToTableJoinScalaIntegrationTest.scala#L106
177296715,4756,debasishg,2018-03-27T03:19:43Z,"@mjsax -

I think I may be missing something here. We are allowing the user to pass in the `store` *and* `keySerde`. And we create the `Materialized` out of the 2 with `Long` as the value serde.

However we were allowing the user to pass in the `keySerde` optionally and in case the user does not supply one we assumed it will be taken from the config. This is actually a remnant from the earlier thoughts where we thought passing serdes through config may be a good idea. However in the current context, we should not make `keySerde` optional. Here's the suggestion for the changed API ..

```scala
def count(store: String, keySerde: Serde[K]): KTable[K, Long] = { 
  val materialized = Materialized.as[K, java.lang.Long, ByteArrayKeyValueStore](store).withKeySerde(keySerde)
  val c: KTable[K, java.lang.Long] = inner.count(materialized)
  c.mapValues[Long](Long2long _)
}
```

An alternative option could have been to allow the user to pass in the `Materialized` instance itself (like we do in the `reduce` function). The problem with that alternative is that the Java API expects `java.lang.Long` as the value serde, while the Scala API needs to take a `scala.Long`. And there is no way we can convert a `Materialized.as[K, scala.Long, ByteArrayKeyValueStore]` to `Materialized.as[K, java.lang.Long, ByteArrayKeyValueStore]` without a cast. Hence the divergence in the API signature between `count` and `reduce`.

Please share if u have any other thoughts."
177297602,4756,debasishg,2018-03-27T03:24:58Z,"> I'm also wondering if we really want to use the same class names as the Java DSL, this seems like it would be confusing to use in an IDE with code-completion. What do you think about using ScalaKStream, etc. instead? This would also get around the desire to rename imports.

Hi @vvcephei - regarding the above, we had a discussion on the 2 approaches on this thread only and the universal suggestion was to use the same name across Scala and Java APIs. In fact in the initial version that we posted, we had different names (`KStream` / `KStreamS`). The reasoning of using the same names is that the renaming of imports in the user code needs to be done only very occasionally when we mix usage of Scala and Java APIs.

"
177305370,4756,debasishg,2018-03-27T04:29:40Z,"Here, the deprecation is on `punctuate`, which is part of the contract of `Transformer`. How do we remove this ? We can *only* remove this when `punctuate` is removed from `Transformer` ? Or am I missing something ?"
177306580,4756,debasishg,2018-03-27T04:41:33Z,"We don't need to provide implementation of `ValueTransformer` here since the passed in `() => ValueTransformer[V, VR]` gets converted to `ValueTransformerSupplier[V, VR]` in the implementation through SAM type conversion. We could not do that for `transform` as the SAM type conversion will not be handled automatically in that case. Please have a look [here](http://www.scala-lang.org/files/archive/spec/2.12/06-expressions.html#sam-conversion) for SAM type conversions in Scala."
177306659,4756,debasishg,2018-03-27T04:42:17Z,Same logic as `ValueTransformer` above.
177320316,4756,debasishg,2018-03-27T06:34:09Z,Thanks for your thoughts @guozhangwang .. we will remove the dependency on `net.manub:scalatest-embedded-kafka-streams` and use `EmbeddedKafkaCluster` instead.
177420275,4756,seglo,2018-03-27T13:20:12Z,"@vvcephei There's no list of what's brought in with the experimental flag, unless you grep the compiler code.  The purpose of the SAM type conversion feature in 2.11 was only to get early feedback and only properly finished in 2.12.  As its name suggests it's not meant to be used in production code.  Since Kafka is still targeting 2.11 it makes sense to not include this flag to build a releasable artifact.  I'll remove the flag and desugar the conversions."
177438559,4756,vvcephei,2018-03-27T14:10:55Z,"@debasishg Thanks for the explanation, I missed that discussion. Sorry to bring it back up!

@seglo Thanks! Am I correct in thinking that only affects our code, and not our users' code? I.e., if they are using 2.12, they can pass in lambdas as arguments, right?"
177440570,4756,seglo,2018-03-27T14:16:01Z,"@vvcephei Yes, this change won't affect end users at all.  If Kafka drops Scala 2.11 support then we can bring back the SAM conversions as they're available without any ominous compiler flags."
177441103,4756,vvcephei,2018-03-27T14:17:26Z,"Huh! Well, that explains it. I have seen the conversions of the raw types, and also suffered from type errors that `Serde[scala.Long]` != `Serde[java.lang.Long]`, so I just assumed that `scala.Long` was a different class than `java.Long`. Thanks for the explanation, @ijuma .
"
177442727,4756,vvcephei,2018-03-27T14:21:33Z,"Oh, right, I thought this was one of your scala replacement classes.

What I have been doing for cases like this is throwing an `UnsupportedOperationException` in the body. It's not as good as not having the method, but it def. ensures it can't be used. And you don't have to maintain the code that's in the body."
177449950,4756,vvcephei,2018-03-27T14:40:09Z,"That makes sense.

My 2 cents:
We're presenting the scala KGroupedStream basically *as* the java one, but not implementing the interface so that we can smooth over a couple of specific gaps. I think this is a good call, but it's also a huge risk for cognitive dissonance and developer confusion, since they will read the java version of the docs and try to use that knowledge in scala. Therefore, it's important to be super disciplined about making sure the methods available are as close to the java interface as possible.

Clearly, moving serdes, etc., to implicit params is the kind of thing we *do* want to do. But I think that presenting `count(String,Serde[K])` instead of `count(Materialized[K, Long, KeyValueStore[Bytes, Array[Byte]]]` is too far off.

I do agree that the method should take a scala type. Apparently, it's perfectly fine to cast `Serde[scala.Long]` to `Serde[java.Long]`. Does that same logic apply here? Alternatively, we can actually convert the `Materialized[java]` to a `Materialized[scala]`."
177457781,4756,debasishg,2018-03-27T15:00:05Z,"@vvcephei .. Sure we can do the following instead ..

```scala
def count(materialized: Materialized[K, Long, ByteArrayKeyValueStore]): KTable[K, Long] = { 
  val c: KTable[K, java.lang.Long] = inner.count(materialized.asInstanceOf[Materialized[K, java.lang.Long, ByteArrayKeyValueStore]])
  c.mapValues[Long](Long2long _)
}
```

WDYT ?"
177531590,4756,vvcephei,2018-03-27T18:44:36Z,"Ok, I'm already a little nervous about the cast in one direction, so this feels super gross, but would this work?
```scala
def count(materialized: Materialized[K, Long, ByteArrayKeyValueStore]): KTable[K, Long] = { 
  inner.count(materialized.asInstanceOf[Materialized[K, java.lang.Long, ByteArrayKeyValueStore]]).asInstanceOf[KTable[K, scala.Long]]
}
```
Please understand I'm wincing as I type this."
177532059,4756,mjsax,2018-03-27T18:46:17Z,"Well, we allowing to pass in a store ""name"" (String) but not a store. Note, that `Materialized` allows to replace default RocksDB with an in-memory story, disable change-capture-logging or even use a custom store implementation."
177532408,4756,guozhangwang,2018-03-27T18:47:27Z,Sounds good.
177532680,4756,guozhangwang,2018-03-27T18:48:20Z,Sounds good.
177533039,4756,guozhangwang,2018-03-27T18:49:18Z,That sounds better.
177534627,4756,debasishg,2018-03-27T18:54:13Z,"How about the API that I suggested above ? It takes Materialized much like the Java API though we need a cast. 

@vvcephei - in my implementation we have 1 cast and the other map for the long conversion in KTable."
177537597,4756,mjsax,2018-03-27T19:04:24Z,I understand that you cannot change the Java `Transformer` interface and must implement the deprecated method when calling `new Transformer` -- what I was wondering is about `scala.Transformer` interface -- should we add one and remove `punctuate` from it?
177540520,4756,vvcephei,2018-03-27T19:13:02Z,"@debasishg Sorry, I should have acked your implementation. I was actually proposing an evolution of it.

It just seems a bit unfortunate to have to add a real function invocation to the topology in order to do the cast back to `scala.Long`. The version I proposed just does a cast back out without adding anything new to the topology. Does that make sense?

At the risk of sounding like an idiot, if it's fine to do the cast on the way in, then it should be fine again on the way out, right?"
177542082,4756,vvcephei,2018-03-27T19:17:17Z,"FWIW, I think adding a new scala interface just to remove a method that we plan to remove from the java interface is not necessary. Better just to implement it and move on.

Also, it would be a bit trickier to swap in a scala replacement for `Transformer` than for the top-level DSL classes, since implementations of the java `Transformer` won't implement the scala `Transfomer`, so you wouldn't be able to plug them in via the scala DSL wrapper. But there's otherwise no reason this shouldn't work."
177554002,4756,mjsax,2018-03-27T20:01:50Z,"Ack. Was just an idea. I don't really speak Scala (yet) -- this is an exercise to learn something about it...

If we need to have it, I vote to throw an exception to forces users to use the new API."
177564717,4756,mjsax,2018-03-27T20:39:26Z,"This and all other classes are public API. Thus, we should improve the JavaDocs for those classes and also add JavaDocs for all methods. I guess we can c&p from existing Java classes."
177636021,4756,debasishg,2018-03-28T04:25:29Z,@vvcephei - cast is a runtime operation and my philosophy is to minimize its use. And `scala.Predef` indeed uses `Long2long` to do such conversions. Hence I would like to prefer using proper functions when available instead of the cast.
177638616,4756,debasishg,2018-03-28T04:56:34Z,"ok, will remove `split` from `KStream` for now."
177660176,4756,debasishg,2018-03-28T07:32:08Z,Renamed to `SimpleScalaSerde` ..
177660317,4756,debasishg,2018-03-28T07:32:45Z,Removed!
177662427,4756,debasishg,2018-03-28T07:42:53Z,@mjsax - Looking for suggestions. Should we copy/paste Javadoc from Java classes or use `@see` annotation ? The problem with copy is maintenance - when one changes someone needs to be careful enough to change the other.
177685679,4756,debasishg,2018-03-28T09:12:55Z,@guozhangwang - Removed all dependencies on `net.manub:scalatest-embedded-kafka` and `net.manub:scalatest-embedded-kafka-streams`. Now using `EmbeddedKafkaCluster` instead for tests. Also removed the test that used avro - hence dependency on `avros` eliminated. 
177793180,4756,deanwampler,2018-03-28T15:34:39Z,"A little more detail; what this import is saying is ""import these items, but give them an alias, then import everything else without an alias""."
177826543,4756,guozhangwang,2018-03-28T17:22:58Z,Thanks @debasishg !
177950361,4756,mjsax,2018-03-29T04:19:27Z,Good question. Not sure. I agree that maintaining JavaDocs twice is a hassle and error prone. But might be annoying for user if it's only linked on the other hand. Would be good to hear what others thing. \cc @guozhangwang @deanwampler @ijuma @vvcephei @bbejeck @seglo 
178029830,4756,seglo,2018-03-29T11:41:02Z,"Adding JavaDocs to all the public API methods in the PR is the same amount of work any way we do it.  From an end user perspective I agree it would be nice to have the same (or slightly tweaked, as necessary) JavaDocs for all public API methods, plus a `@see` or `@link` tag to the corresponding Java API method.  It will be a small burden to maintain it going forward so I defer to the AK committers to make the call on the format."
178136346,4756,guozhangwang,2018-03-29T18:01:06Z,"I'd vote for using `@see` and `@link` to avoid maintaining two copies, because we have some public classes following this pattern in the repo (like https://github.com/apache/kafka/blob/23d01c805bef7504abfa83ecac7e384d121a583a/clients/src/main/java/org/apache/kafka/clients/consumer/Consumer.java), and from the past I find most people would not remember or bother to update two places than one.

> Additionally, this PR should include updates to the web docs in docs/streams/... and in ""notable changes"" in docs/upgrade.html ?

> Meta Comment - Besides updating documentation and Javadoc, is there any outstanding item in this PR that needs to be addressed ?

I think the web docs (in `docs/streams`) needs to be updated as well, especially in the `upgrade-guide.html` page, as well as the `streams-api` page."
178136538,4756,guozhangwang,2018-03-29T18:01:48Z,"For `streams-api` page above, I meant https://github.com/apache/kafka/blob/23d01c805bef7504abfa83ecac7e384d121a583a/docs/api.html"
178139790,4756,ijuma,2018-03-29T18:13:51Z,"Some thoughts:
- I think the Consumer and KafkaConsumer pattern is bad. The documentation should have been on `Consumer` instead. The `AdminClient` follows the latter pattern.
- I think it's a poor user experience to ask users to read the docs in the Java class. My recommendation would be to at least include a short description in the Scala docs along with a link to the relevant Java documentation. The short description is less likely to change and it helps users make progress without having to jump to the Java code all the time. However, for more detailed information (which is more likely to change), they can check the Java code."
178448558,4756,mjsax,2018-04-01T05:20:06Z,"I tend to agree with @ijuma comment about `KafkaConsumer`/`Consumer` pattern -- it's quite annoying to not get the JavaDocs directly. Thus, even if it's a burden it seems to be worth to maintain two copies."
178450057,4756,debasishg,2018-04-01T07:16:38Z,@mjsax - I have started writing the Scaladocs in the commit https://github.com/apache/kafka/pull/4756/commits/631ab9a8357e4667e4b941a0389bc4275f486711 .. Pls review if it's following the correct pattern
178976326,4756,guozhangwang,2018-04-03T22:07:53Z,nit: period at the end of the sentence.
178976444,4756,guozhangwang,2018-04-03T22:08:35Z,Maybe mention again which artifact to include in order to import this package.
178976955,4756,guozhangwang,2018-04-03T22:11:09Z,We do not need indentation in the code block; the following code blocks are formatted correctly.
178977137,4756,guozhangwang,2018-04-03T22:12:07Z,This is not introduced in this PR: duplicated `provides`.
179201680,4756,seglo,2018-04-04T16:20:47Z,"I fixed the typos in this line, but I'm not sure what you mean by it not being introduced in this PR.  This line is to indicate the presence of the Kafka Streams DSL for Scala library."
179202074,4756,seglo,2018-04-04T16:22:13Z,I copied the formatting from the streams main page which indented the WordCount examples: https://github.com/apache/kafka/blob/trunk/docs/streams/index.html#L155
179202109,4756,seglo,2018-04-04T16:22:22Z,:+1: 
179202130,4756,seglo,2018-04-04T16:22:28Z,:+1: 
179202465,4756,seglo,2018-04-04T16:23:36Z,I removed the initial indentation for this example on this page to make it consistent with the others.
179268477,4756,guozhangwang,2018-04-04T20:15:02Z,"I meant the duplicated `provides` exist before this PR, so it is not a regression introduced from this PR."
181648477,4756,mjsax,2018-04-16T07:54:15Z,"""is available here"" is bad phrasing.

`here` -> `in the developer guide`"
181649571,4756,mjsax,2018-04-16T07:58:35Z,`To include it your maven` -- sounds weird
181649928,4756,mjsax,2018-04-16T08:00:05Z,I am wondering if this is correct? Should the Scala version not be included here?
181650115,4756,mjsax,2018-04-16T08:00:56Z,"Don't we need the Scala version, here?"
181650350,4756,mjsax,2018-04-16T08:01:54Z,Don't we need the Scala version here?
181650623,4756,mjsax,2018-04-16T08:03:07Z,typo: is a wrapper around `Stream[s]Builder`
181650718,4756,mjsax,2018-04-16T08:03:31Z,nit: `WordCount` ? 
181650779,4756,mjsax,2018-04-16T08:03:47Z,typo: `Stream[s]Builder`
181652632,4756,mjsax,2018-04-16T08:11:33Z,Scala version missing?
181652730,4756,mjsax,2018-04-16T08:12:00Z,Scala version missing?
181652863,4756,mjsax,2018-04-16T08:12:40Z,"I think, we should not have indention here for better rendering"
181653308,4756,mjsax,2018-04-16T08:14:23Z,nit: should be added such the alphabetical order is maintained.
181653441,4756,mjsax,2018-04-16T08:14:56Z,Can't we merge this with the one from above?
181656561,4756,mjsax,2018-04-16T08:27:10Z,This seems to be rather short compared to `stream` and `table` docs from above.
181656843,4756,mjsax,2018-04-16T08:28:06Z,"Maybe we can add, that a store must still be ""connected"" to a `Processor`, `Transformer`, or `ValueTransformer` before it can be used?"
181657170,4756,mjsax,2018-04-16T08:29:17Z,"Maybe add, that global stores do not be added to `Processor`, `Transformer`, or `ValueTransformer` (in contrast to regular stores)."
181658989,4756,mjsax,2018-04-16T08:36:36Z,"Maybe add a sentence, that stores must be added via `addStateStore` or `addGlobalStore` before they can be connected to the `Transformer` ?"
181659066,4756,mjsax,2018-04-16T08:36:53Z,as above?
181659114,4756,mjsax,2018-04-16T08:37:05Z,as above?
181659257,4756,mjsax,2018-04-16T08:37:38Z,as above?
181660310,4756,mjsax,2018-04-16T08:41:42Z,"Maybe explain, that there is not ordering guarantee for the merged result stream for records of different input streams? Relative order is only preserved for record of the same input stream?"
181662152,4756,mjsax,2018-04-16T08:48:00Z,"Markup seems weird? Why do you have JavaDoc comment markup?

Would a single `#` not be sufficient?"
181904753,4756,guozhangwang,2018-04-16T22:31:18Z,"For different scala version compiled packages, their project name is actually the same. And here people only need to specify the version of the artifact itself, which will be the Kafka version.

Users can, indeed, build kafka-streams-scala with different scala versions other than the default one, but that is to be done before they include it in the dependency. For maven, it will always be whatever is uploaded to maven central."
181909480,4756,guozhangwang,2018-04-16T22:57:47Z,"cc @joel-hamill @ewencp  we are adding a few new sections in web docs regarding the streams scala API, which may be affecting https://github.com/apache/kafka/pull/4536."
181909865,4756,guozhangwang,2018-04-16T23:00:09Z,"I think the Scala version cannot be changed when specifying the `kafka-streams-scala` artifact, as it is encapsulated when that artifact is compiled already. @debasishg please correct me if I'm wrong."
181910307,4756,guozhangwang,2018-04-16T23:02:43Z,I think we do not need avro4sVersion any more? Same as line 86 here.
181910344,4756,guozhangwang,2018-04-16T23:02:56Z,This is not needed.
181910430,4756,guozhangwang,2018-04-16T23:03:34Z,Do we still need `scalatestEmbeddedKafkaVersion`?
181910448,4756,guozhangwang,2018-04-16T23:03:40Z,+1
181910836,4756,guozhangwang,2018-04-16T23:06:10Z,Should we add `Copyright 2018 The Apache Software Foundation.` as well? @ijuma 
181911680,4756,guozhangwang,2018-04-16T23:11:08Z,"@debasishg Ping on this comment again, could you elaborate if my concern is valid or not?"
181911985,4756,guozhangwang,2018-04-16T23:12:59Z,Could we remove this line then?
181916063,4756,guozhangwang,2018-04-16T23:39:02Z,"nit: move `import org.junit.Assert._` after line 22, ditto below elsewhere."
181916302,4756,guozhangwang,2018-04-16T23:40:40Z,"nit: replace the `_1/2/3` suffix with some more meaningful name? E.g. `simple`, `aggregate`, `join`?"
181972834,4756,debasishg,2018-04-17T07:18:17Z,done ..
181973173,4756,debasishg,2018-04-17T07:19:47Z,done ..
181974944,4756,debasishg,2018-04-17T07:28:02Z,enriched ..
181975406,4756,debasishg,2018-04-17T07:30:03Z,done ..
181975843,4756,debasishg,2018-04-17T07:31:59Z,done ..
181978439,4756,debasishg,2018-04-17T07:43:03Z,done ..
181978475,4756,debasishg,2018-04-17T07:43:11Z,done ..
181978790,4756,debasishg,2018-04-17T07:44:41Z,done ..
181979232,4756,debasishg,2018-04-17T07:46:16Z,done ..
181979824,4756,debasishg,2018-04-17T07:48:50Z,done ..
181980358,4756,debasishg,2018-04-17T07:50:55Z,done ..
181980630,4756,debasishg,2018-04-17T07:52:07Z,removed ..
181981140,4756,debasishg,2018-04-17T07:54:09Z,removed ..
181982186,4756,debasishg,2018-04-17T07:57:46Z,done ..
181984159,4756,debasishg,2018-04-17T08:02:37Z,done ..
181985862,4756,debasishg,2018-04-17T08:09:17Z,"@guozhangwang I am not sure I understand your concern. The only purpose of this implicit is to allow an implicit conversion from `Tuple2` to `KeyValue(key, value)`. Just a helper which we found useful in many cases for developing applications or tests."
181986497,4756,debasishg,2018-04-17T08:11:46Z,removed the duplicate entry ..
182133565,4756,guozhangwang,2018-04-17T15:59:15Z,"Here is my concern: in `map` and `flatMap`, we call

```
mapper.tupled andThen tuple2ToKeyValue
```

Does that mean that for each pair of K, V pair parameters, we would first construct a `Tuple2` object of this case class, and then  apply the mapper, and then create a new `KeyValue` from the result `Tuple2` object? If that is true, then we are creating a short-lived object for each record processed in `map`. I'm not sure if it will have a pressure on the GC."
182333791,4756,debasishg,2018-04-18T07:37:56Z,"@guozhangwang - You are correct that with the current implementation there will be `Tuple2`s created. But it's difficult to say if there will be GC pressure. For that we need to analyze runtime behaviors and see what the JIT does. There's of course a way we can fall back to the implementation which does less allocation ..

```
def map[KR, VR](mapper: (K, V) => (KR, VR)): KStream[KR, VR] = {
  val mapperJ: KeyValueMapper[K, V, KeyValue[KR, VR]] = ((k: K, v: V) => {
    val res = mapper(k, v)
    new KeyValue[KR, VR](res._1, res._2)
  }).asKeyValueMapper
  inner.map[KR, VR](mapperJ)
}
```

We did run some tests in bulk to check the diff in performance between the 2 versions. Couldn't find much of a difference though. "
182418338,4756,seglo,2018-04-18T13:05:49Z,:+1: 
182418770,4756,seglo,2018-04-18T13:07:07Z,:+1: 
182424831,4756,ijuma,2018-04-18T13:25:51Z,Not sure what you mean @guozhangwang. The Scala version is usually part of the artifact name.
182448107,4756,seglo,2018-04-18T14:30:39Z,"I'm not very familiar with gradle, but it appears to not support cross building jars in the same manner as SBT.  The build needs to be run for each Scala version you want a jar for, but the output won't encode the version into the filename.  I think what we need to do is add a task to the gradle file, or some other build related packaging script, to pluck the generated `kafka-streams-scala` file, rename it to include the Scala version, and then publish it to maven central.

Ex) The built outputs this when specifying a 2.12 `SCALA_VERSION` (`./gradlew -PscalaVersion=2.12 jar`)

```
-rw-rw-r--  1 seglo seglo 105423 Apr 18 14:17 kafka-streams-scala-1.2.0-SNAPSHOT.jar
```

When a release artifact is published we'll publish a file: `kafka-streams-scala_2.12-1.2.0.jar` with the Scala major version encoded into the artifact name.  A maven user would reference the artifact with:

```
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka-streams-scala_2.12</artifactId>
    <version>1.1.0</version>
</dependency>
```
Please let me know if I'm missing something here about the Kafka build system.

On a related note I found a gradle build plugin that handles cross building projects and referencing Scala dependencies in a SBT style here: https://github.com/ADTRAN/gradle-scala-multiversion-plugin


"
182454071,4756,ijuma,2018-04-18T14:45:42Z,"We already so the right thing for core jars. We just need to follow the same approach. And yes, the Scala version needs to be encoded in the artifact id. Not sure what @guozhangwang was trying to say, but doesn't seem correct to me."
182461208,4756,seglo,2018-04-18T15:04:21Z,"@ijuma Ok, is the approach you refer to in the core project of the `build.gradle`?  I'll take a closer look.  

WRT the docs @mjsax is correct that we should update the maven dependency examples to include the Scala version."
182503294,4756,guozhangwang,2018-04-18T17:13:46Z,"What I was saying is that when we build the artifact we already chose which scala version to use compiling the jar and made the scala version as part of the artifact name, so users do not need to specify the scala version in declaring the dependency, but just:

```
<dependency>
    <groupId>org.apache.kafka</groupId>
    <artifactId>kafka_2.12</artifactId>
    <version>1.1.0</version>
</dependency>
```"
182704470,4756,seglo,2018-04-19T10:40:56Z,I'll correct this to include the Scala version (2.11) across all the maven `pom.xml` references.
182704820,4756,seglo,2018-04-19T10:42:14Z,We don't need to specify the Scala version here.  The `%%` operator in SBT will automatically determine the right artifact based on the running Scala version.
182705719,4756,seglo,2018-04-19T10:46:15Z,:+1: 
182705959,4756,seglo,2018-04-19T10:47:21Z,:+1: 
182706259,4756,seglo,2018-04-19T10:48:44Z,:+1:
182706791,4756,seglo,2018-04-19T10:51:22Z,:+1:
182707211,4756,seglo,2018-04-19T10:53:13Z,"I followed the convention of preceding Java examples which also have indentation, but I'll remove the indentation for the Scala example."
182711723,4756,seglo,2018-04-19T11:13:11Z,:+1:
182742751,4756,mjsax,2018-04-19T13:21:46Z,@guozhangwang @seglo That's what I meant by my comment -- sorry for expressing myself unclear.
184075333,4756,miguno,2018-04-25T14:16:17Z,"This doesn't look right to me.  The latest `trunk` build of Kafka only generates the following artifact(s):

```
$ git log | head -n 1
commit c853ef75a11663cbee3160e64e45f46f5a8ac78c

# Build Kafka trunk
$ gradle && ./gradlew clean installAll

# Find KIP-270 artifacts
$ find . -name ""kafka-streams-scala*.jar""
./streams/streams-scala/build/libs/kafka-streams-scala-2.0.0-SNAPSHOT.jar
./streams/streams-scala/build/libs/kafka-streams-scala-2.0.0-SNAPSHOT-javadoc.jar
./streams/streams-scala/build/libs/kafka-streams-scala-2.0.0-SNAPSHOT-test.jar
./streams/streams-scala/build/libs/kafka-streams-scala-2.0.0-SNAPSHOT-sources.jar
./streams/streams-scala/build/libs/kafka-streams-scala-2.0.0-SNAPSHOT-scaladoc.jar
./streams/streams-scala/build/libs/kafka-streams-scala-2.0.0-SNAPSHOT-test-sources.jar
```

The maven coordinates for the artifacts above have an `artifactId` of `kafka-streams-scala`, not `kafka-streams-scala_2.11`. "
184082552,4756,miguno,2018-04-25T14:34:24Z,"This example code doesn't compile, because e.g. the import for `StreamsBuilder` is missing (from package `org.apache.kafka.streams.scala`).

I would probably double-check the other examples that are shown in the documentation, too."
184119748,4756,guozhangwang,2018-04-25T16:11:54Z,"Looking at the `build.gradle` again, today we only build `kafka-streams-scala` with the default scala versions defined in `dependencies.gradle`, 2.11.12.

If we want to publish multiple artifacts with different scala versions we should follow the `core` project pattern, i.e. sth. like:

```
for ( sv in availableScalaVersions ) {
  String taskSuffix = sv.replaceAll(""\\."", ""_"")

  tasks.create(name: ""jar_core_${taskSuffix}"", type: GradleBuild) {
    startParameter = project.getGradle().getStartParameter().newInstance()
    startParameter.projectProperties += [scalaVersion: ""${sv}""]
    tasks = ['core:jar']
  }

  tasks.create(name: ""test_core_${taskSuffix}"", type: GradleBuild) {
    startParameter = project.getGradle().getStartParameter().newInstance()
    startParameter.projectProperties += [scalaVersion: ""${sv}""]
    tasks = ['core:test']
  }

  tasks.create(name: ""srcJar_${taskSuffix}"", type: GradleBuild) {
    startParameter = project.getGradle().getStartParameter().newInstance()
    startParameter.projectProperties += [scalaVersion: ""${sv}""]
    tasks = ['core:srcJar']
  }

  tasks.create(name: ""docsJar_${taskSuffix}"", type: GradleBuild) {
    startParameter = project.getGradle().getStartParameter().newInstance()
    startParameter.projectProperties += [scalaVersion: ""${sv}""]
    tasks = ['core:docsJar']
  }

  tasks.create(name: ""install_${taskSuffix}"", type: GradleBuild) {
    startParameter = project.getGradle().getStartParameter().newInstance()
    startParameter.projectProperties += [scalaVersion: ""${sv}""]
    tasks = ['install']
  }

  tasks.create(name: ""releaseTarGz_${taskSuffix}"", type: GradleBuild) {
    startParameter = project.getGradle().getStartParameter().newInstance()
    startParameter.projectProperties += [scalaVersion: ""${sv}""]
    tasks = ['releaseTarGz']
  }

  tasks.create(name: ""uploadCoreArchives_${taskSuffix}"", type: GradleBuild) {
    startParameter = project.getGradle().getStartParameter().newInstance()
    startParameter.projectProperties += [scalaVersion: ""${sv}""]
    tasks = ['core:uploadArchives']
  }
}
```

We could also consider just building one artifact with the default Scala version, in this case we would remove the suffix here and add the explanation which scala version users should be expected to use."
184124327,4756,seglo,2018-04-25T16:25:25Z,"I recommend releasing versions of `kafka-streams-scala` for both major versions of Scala currently supported by Kafka.  We should copy build and release conventions used by Kafka core so that both artifacts are produced.  I'm not very familiar with gradle or the Kafka release process, so I wasn't sure how far to go with this, but now that that gradle snippet is right in front of me it's clear that the same should be done for this library.  I believe @ijuma recommended this earlier, but I didn't make the appropriate update before the merge.  

~~@ijuma how are multiple versions of Kafka core published at part of the release process?  Is the build script called twice with appropriate `scalaVersion` parameter?~~ Nevermind, I see how it's done now.

Only making the library available to Scala 2.11 will leave behind a lot of users that are already on 2.12, which has been out for several years now.  Cross building (building an artifact per version of Scala) will also make it a trivial matter to support future of versions of Scala in the release process."
184148190,4756,miguno,2018-04-25T17:42:03Z,"I agree that we should generate artifacts for both 2.11 and 2.12, like we do for Kafka Core."
184153041,4756,guozhangwang,2018-04-25T17:56:24Z,Sounds good. @seglo could you submit a follow-up PR to modify `build.gradle` for publishing multiple artifacts for different scala versions of `kafka-streams-scala` then?
184153302,4756,guozhangwang,2018-04-25T17:57:11Z,@seglo could you take a look?
184193473,4756,seglo,2018-04-25T20:20:20Z,"Yes.  I'm travelling ATM, but I'll make a new PR in the next few days."
184193680,4756,seglo,2018-04-25T20:21:11Z,Yes. I'll test the snippets in the build PR.
186375027,4756,miguno,2018-05-07T09:37:32Z,"The single parameter for `transform()` is a `Transformer`, not a `TransformerSupplier`.  The variable needs renaming and the javadocs updating."
186387729,4756,miguno,2018-05-07T10:44:02Z,I raised https://issues.apache.org/jira/browse/KAFKA-6871 for this.
65546724,1446,enothereska,2016-06-02T14:13:40Z,"These name changes are not strictly part of this fix, I'm wondering if we can open a MINOR pr for these while having this PR focus on streams only (to avoid confusion).
"
65564787,1446,aartigupta,2016-06-02T15:42:11Z,"Agreed, theses were not intended for this fix, they managed to sneak their way in. My bad, fixed it now
"
65923002,1446,jklukas,2016-06-06T16:32:36Z,"The line break here seems unnecessary.
"
65923529,1446,jklukas,2016-06-06T16:36:02Z,"Since there are now two implementations of `StreamsMetrics`, is it confusing to have them both named `StreamsMetricsImpl`? This could be `ThreadStreamsMetrics` and the other could be `ProcessorNodeStreamsMetrics`.
"
65947627,1446,enothereska,2016-06-06T18:56:29Z,"Do we still need the subsequent variable?
"
65948134,1446,enothereska,2016-06-06T18:59:27Z,"I wonder if there is a way to use the other sensor calls for the latency sensor too. Or will this one always remain special?
"
65948578,1446,enothereska,2016-06-06T19:02:06Z,"Would it be better for `metrics` to be passed in the `ProcessorNode` constructor instead? For example, like it's done in the `StreamThread` constructor.
"
65948746,1446,enothereska,2016-06-06T19:03:11Z,"Same comment as above about passing in constructor instead.
"
65948820,1446,enothereska,2016-06-06T19:03:36Z,"Same comment as above about passing in constructor instead.
"
65951530,1446,enothereska,2016-06-06T19:20:32Z,"Is it correct to do the commit sensor recording for task.node() or even here? Why not do it in `StreamTask's` `commit`?
"
65951650,1446,enothereska,2016-06-06T19:21:19Z,"Is it correct to do the punctuation sensor recording for task.node() or even here? Why not do it in `StreamTask's` `punctuate`?
"
66099715,1446,aartigupta,2016-06-07T15:58:00Z,"There seem to be two ways to make this happen

a. for the ProcessorNode to accept Metrics in the constructor, the ProcessorNodeFactory would have to have a pointer to metrics, which would imply that addProcessor in TopologyBuilder would need to have that, and since metrics is internal  to KStreams, users of TopologyBuilder do not have a pointer to metrics.
Alternatively 
b. the build method of ProcessorNodeFactory can take metrics in addition to applicationId and then a ProcessorNode can be constructed by passing Metrics in the ProcessorNode constructor as opposed to piggy backing on the StreamsMetrics which the current review request shows (which is not uniform with StreamThread...)

b,  would look like this 

```
@Override
    public ProcessorNode build(String applicationId, Metrics metrics) {
        return new ProcessorNode(name, supplier.get(), stateStoreNames, metrics);
    }
```
"
66102111,1446,aartigupta,2016-06-07T16:11:35Z,"You are correct, previously this made sense since we did not expose adding arbitrary sensors onto the base metrics registry, but now that we do, having this in the interface stands out as odd

That said there seems to be a lot of boiler plate code around tags, parsing and logging. 

 // first add the global operation metrics if not yet, with the global tags only
            Sensor parent = metrics.sensor(scopeName + ""-"" + operationName);
            addLatencyMetrics(metricGroupName, parent, ""all"", operationName, this.metricTags);

```
        // add the store operation metrics with additional tags
        Sensor sensor = metrics.sensor(scopeName + ""-"" + entityName + ""-"" + operationName, parent);
        addLatencyMetrics(metricGroupName, sensor, entityName, operationName, tagMap);
```

What do these tags buy us ?  (it is not clear :) )

If we can get rid of the tags, then does it make sense to have an a base implementation of the StreamsMetrics interface, because with the exception of the actual sensors contained in them, the two implementations start to look very similar 

 protected class ProcessorNodeStreamsMetrics implements StreamsMetrics ....

  protected class ThreadStreamsMetrics implements StreamsMetrics ....
"
66291188,1446,aartigupta,2016-06-08T16:35:15Z,"You are right, this was not the right place. StreamTask's commit is the right place. Fixed locally and working on a unit test and integration test to ""count down"" commit and punctuate metrics to ensure correctness.
"
91086491,1446,ijuma,2016-12-06T14:23:19Z,Should this be `RecordLevel`?
91086702,1446,ijuma,2016-12-06T14:24:24Z,"We don't need the `SENSOR` prefix in the name. Also, are these the only two levels we care about?"
91231975,1446,aartiguptaa,2016-12-07T06:08:57Z," INFO would map to the level we use for normal production runs, and DEBUG could be used to optimize the job in the development or instrumentation/debugging phase. Can't think of any more use cases, maybe TRACE could be a finer level, but personally have never found that useful."
91587971,1446,guozhangwang,2016-12-08T19:25:50Z,"nit: new line between functions, ditto below."
91588145,1446,guozhangwang,2016-12-08T19:26:45Z,"Also we need to add the javadoc for those newly added functions, especially explain the `recordLevel`."
91588780,1446,guozhangwang,2016-12-08T19:29:50Z,"We can reuse `RecordLevel.SENSOR_INFO_STR` etc here, to avoid split places of those global constant strings."
91589025,1446,guozhangwang,2016-12-08T19:31:07Z,Could this result in NPE? May be we can directly throw an IllegalArgumentException here.
91589076,1446,guozhangwang,2016-12-08T19:31:25Z,"How about returning ""UNKNOWN"" or ""ILLEGAL_LEVEL"" than null?"
91590844,1446,guozhangwang,2016-12-08T19:39:29Z,"This is a meta comment: in additional to skip `record`, we can probably go further to even avoid registering the sensor at all in the reporter. Its benefits are:

1. the reporter will not show these metrics at all (i.e. they will not display in the monitoring UI, for example), whereas today the reporter will still show these metrics but the value is always the initialized value (most likely 0).

2. we can further reduce the overhead of function calls as well as the registry space; I'm not sure by how much though.

Admittedly this will require a larger change on `o.a.k.common.metrics`. What do you think @ijuma @ewencp @junrao ?"
91591063,1446,guozhangwang,2016-12-08T19:40:45Z,Ditto above.
91591235,1446,guozhangwang,2016-12-08T19:41:40Z,Is this really part of this PR?
91592681,1446,guozhangwang,2016-12-08T19:48:09Z,"Won't time be always null here, since it is never initialized?"
91594176,1446,guozhangwang,2016-12-08T19:55:40Z,"`streams-processor-node-metrics` To be consistent with other group names, and only include the node name in the tags as `put(""processor-node-id"", name);` (not sure why you added  a ""-"" before the name?). "
91594356,1446,guozhangwang,2016-12-08T19:56:39Z,"nit: ""processor-node"" to be more clear with other scope, for example ""producer-metrics"" also have a per-node sensor level, where `node` there means the destination brokers."
91595483,1446,guozhangwang,2016-12-08T20:02:31Z,"This is not introduced in this patch, but we could fix it together:

in line 1133 below

```
Sensor sensor = metrics.sensor(sensorNamePrefix + ""-"" + entityName + ""-"" + operationName);
```

We'd better change it to

```
Sensor sensor = metrics.sensor(sensorNamePrefix + ""."" + entityName + ""-"" + operationName);
```"
91596211,1446,guozhangwang,2016-12-08T20:06:29Z,"Again, to be consistent the sensor name better be the form of `(sensorNamePrefix + "".node-forward-time""): to add a `.` after the prefix, and we do not need the `name` as the suffix since it is already used in tags."
91596966,1446,guozhangwang,2016-12-08T20:10:50Z,"The explanation does not sound right to me. I think it is ""The average per-second number of records processed by this processor"". Also maybe we can rename the variable name to `nodeThroughputSensor` to be more meaningful?"
91596999,1446,guozhangwang,2016-12-08T20:11:03Z,Ditto for other sensor names.
91597224,1446,guozhangwang,2016-12-08T20:12:24Z,Seems like the explanation text are not updated after copying :P
91597427,1446,guozhangwang,2016-12-08T20:13:33Z,"Is the creation and deconstruction rate really useful? I feel the creation and deconstruction latency is more useful, since for rate it is mostly 0 unless there is a rebalance."
91597796,1446,guozhangwang,2016-12-08T20:15:57Z,"Also we do not need the prefix in metric name, but only in sensor name."
91598240,1446,guozhangwang,2016-12-08T20:18:23Z,"And following that comment, the sensor name can be changed to `(sensorNamePrefix + "".process-throughput"")`, and the metrics name can be changed to `""record-process-rate""`.

"
91598693,1446,guozhangwang,2016-12-08T20:21:13Z,"I think this is not a per-node sensor, but rather a per-task sensor right? Shall we create a `TaskMetrics` class accordingly for this layer?"
91598975,1446,guozhangwang,2016-12-08T20:22:59Z,"By layer I mean three layers: thread, task, node, and for task I think just one sensor ""commit"" is good enough, for punctuating the node-level metrics should be sufficient to cover."
91599246,1446,guozhangwang,2016-12-08T20:24:37Z,Why we want to pass in these as parameters instead of computing them internally? It seems `StreamsMetricsImpl` is still only used once.
91599430,1446,guozhangwang,2016-12-08T20:25:45Z,Why we do not want to use `computeLatency` any more?
91668435,1446,aartiguptaa,2016-12-09T07:13:36Z,"Removed it for now, don't remember why and when this was added."
91676685,1446,aartiguptaa,2016-12-09T08:42:53Z,"Done, that makes it consistent. Feeling better about it after that refactor."
91679111,1446,aartiguptaa,2016-12-09T09:02:50Z,"Fixed the variable name to streamsMetrics (was sensors)
 StreamsMetricsImpl is used 12 times. 

Also fixed the computation"
91680753,1446,dguy,2016-12-09T09:16:06Z,why protected?
91685809,1446,dguy,2016-12-09T09:49:32Z,why not record this inside `init`? It seems wrong to expose a field of `ProcessorNode`
91700552,1446,enothereska,2016-12-09T11:25:12Z,"@guozhangwang @aartigupta Unfortunately 2 different tasks on the same thread can register the same two metrics and we'll get an exception in the thread library (current test fails because of this). So the prefix is not sufficient just for the sensor, but is also needed for the metric."
91821831,1446,guozhangwang,2016-12-10T00:43:02Z,"@aartigupta @enothereska That's right, but shouldn't the `sensorNamePrefix` contains that? Currently it only contains the taskId but I think it should include both since each task can have its own copy of the topology. As for the group name, it is used for grouping metrics that may come from different classes even (see producer's `Sender` and `Selector` classes), so it should not include the node id, but rather just ""streams-processor-node-metrics""."
91821954,1446,guozhangwang,2016-12-10T00:45:08Z,"This is at the very critical path of streams, called millions of times per sec, so calling `time.nanoseconds()` on each call is really expensive if `DEBUG` level is used."
91822004,1446,guozhangwang,2016-12-10T00:45:45Z,+1.
91822140,1446,guozhangwang,2016-12-10T00:47:38Z,Do we still need to keep this variable as it is now in the TaskMetrics already?
91922123,1446,enothereska,2016-12-12T10:40:57Z,Done.
91923060,1446,enothereska,2016-12-12T10:46:48Z,Done.
91923083,1446,enothereska,2016-12-12T10:46:56Z,Done.
91924605,1446,enothereska,2016-12-12T10:56:27Z,@guozhangwang I'm not sure I understand your comment. `sensorNamePrefix` contains the task id. We use that for both sensor and metric name. With your last comment you seem to be saying the same we're saying. Unless I misunderstood it. So as action item I'm just re-adding `sensorNamePrefix` to the metric name? (I can probably rename `sensorNamePrefix` to just `prefix` since it's used for sensor AND metric). Anything else? Thanks.
91926667,1446,enothereska,2016-12-12T11:10:52Z,Fixed.
91927862,1446,enothereska,2016-12-12T11:19:14Z,Actually I still need protected because Source and Sink node have to use this field.
92004035,1446,dguy,2016-12-12T18:07:12Z,Can this be private now?
92006773,1446,dguy,2016-12-12T18:21:24Z,"In this method and `punctuate` the blocks of code are largely the same. Is this pattern going to be common? if it is just in this class, then i'd probably create a method in here that accepts a `Runnable` and have `process` and `punctuate` delegate to it, i.e., 

```
private void measureLatency(final Runnable action, final Sensor sensor) {
   long startNs = -1;
   if (nodeMetrics.nodeProcessTimeSensor.maybeRecord()) {
       startNs = time.nanoseconds();
   }
  action.run();
  if (startNs != -1) {
      nodeMetrics.metrics.recordLatency(sensor, startNs, time.nanoseconds());
  }
}
```"
92021599,1446,enothereska,2016-12-12T19:31:49Z,Annoyingly SourceNode uses it.
92027755,1446,guozhangwang,2016-12-12T20:01:12Z,"@enothereska My suggestion is to remove the suffix of `name`, which is the processor name in sensor, since it is already included in the tags. 

Take the `JmxReporter` as an example, it generates the mbean for streams as:

```
mbean: ""kafka-streams"": type=""streams-processor-node-metrics"", ""node-id""=""PROCESSOR-XXX..""
    attribute1: ""task.1-1.processor-throughput-PROCESSOR-XXX""
    attribute2: ""task.1-1.processor-create-time-PROCESSOR-XXX""
    attribute3: ""task.1-1.processor-destruction-time-PROCESSOR-XXX""
    attribute4: ""task.1-2.processor-throughput-PROCESSOR-XXX""
    attribute5: ""task.1-2.processor-create-time-PROCESSOR-XXX""
    attribute6: ""task.1-2.processor-destruction-time-PROCESSOR-XXX""
    ...
```

where `kafka-streams` is the prefix passed in `KafkaStreams`, and `streams-processor-node-metrics` is the pre-defined `metricGrpName `. You can see here that the processor name suffix in attribute names are redundant, and more generally for any reporters implementations, metrics will be grouped by ""group name"" and ""tags""."
92028572,1446,guozhangwang,2016-12-12T20:05:35Z,"Tags are not used at all, actually do they need to be added? See my previous comment."
92028877,1446,guozhangwang,2016-12-12T20:07:04Z,Tags are not used here.
92029084,1446,guozhangwang,2016-12-12T20:08:02Z,"This additional tag pair `""processor-node-id"" -> name` is redundant since it is already added in line 168."
92029769,1446,guozhangwang,2016-12-12T20:11:32Z,"I left a follow-up comment about not needing to include `name` in the prefix anymore, let me know what do you think."
92033622,1446,guozhangwang,2016-12-12T20:33:02Z,"One more meta comment about `scopeName`, `entityName` and `operationName`: in latency sensors they are managed as a two-layer metrics with the top-level metrics as:

1. `prefix.scopeName-operationName`; which is the parent of all:

2. `prefix.scopeName-entityName-operationName`; which means that whenever any of the children get updated, the parent also gets update.

So for state store operations. `scopeNames` are `in-memory-state`, `rocksdb-state` etc, and `entityNames` are specific store names, and `operationName` are `put`, `get` etc. In addition, the `scopeName` is also used in the group name, so you can think of groupNames as

`streams-in-memory-state-metrics`
`streams-rocksdb-state-metrics`
`streams-processor-node-metrics`
`streams-task-metrics`
`streams-metrics`  /* this is actually per-thread metrics */

etc where scope is `processor-node`, `task`, etc.

In your implementation, the `scopeName` is removed from the sensor name, which I think is a good fix as it would redundant since it is already in the group name. However, the parental hierarchy is not encoded in the scopeName / entityName / operationName in this function, and I'm wondering if it still makes sense to still capture this hierarchy in these names while still letting users specify ""additional"" parents in the last parameters?"
92224454,1446,enothereska,2016-12-13T17:42:54Z,"Ok, it's latency now."
92228908,1446,enothereska,2016-12-13T18:05:21Z,One problem here is the `prefix` which so far has been hardcoded in `StreamThread.java`
92230530,1446,guozhangwang,2016-12-13T18:12:51Z,"I'm not sure I understand your comment above?

BTW following my comment below I think we still cannot remove the `scopeName` from the sensor name, since otherwise they will be collapsed into the same sensor."
92235489,1446,enothereska,2016-12-13T18:36:31Z,I only have 2 cases of this so far. Worried about one more layer of indirection.
92237153,1446,enothereska,2016-12-13T18:44:00Z,"I see, sure. Thanks."
92248305,1446,enothereska,2016-12-13T19:35:26Z,"With my latest commit I think I have got things consistent, could you have a look if you can. WIth `jconsole` I can verify that I can see the sensors and metrics."
92681462,1446,mjsax,2016-12-15T19:25:52Z,"of -> or
""to ensure name well-formedness and conformity "" -> to ensure that metric names are well-formed and conform"
92709588,1446,mjsax,2016-12-15T22:00:00Z,"In `ProcessorNode` we also do

> this.nodeMetrics.metrics.recordLatency(nodeMetrics.nodeCreationSensor, startNs, time.nanoseconds());

should we do the same here? What about a more general way to measure this latency? It seems we do not do this correctly right -- we could abstract `nodeMetrics` such that it can be private in and avoid code duplication.

I am also wondering why `this.processor.init(...)` is only called `ProcessorNode` and not in `SourceNode` or `SinkNode`."
92719109,1446,guozhangwang,2016-12-15T22:57:38Z,"Honestly I am not sure if we should ever be ""completely generic"" or not. But if we feel that we should do it, then we can simply expose them as

```
    public Sensor sensor(String name);

    public Sensor sensor(String name, Sensor... parents);
```

In stead of still enforcing the naming convention in terms of `scope, entity, operation`? Or simply just expose the internal `Metrics` to let users call its `sensor` functions directly:

```
public Metrics metricsRegistry();
```

If we do not want to allow ""completely generic sensor registry"", then I'd suggest renaming the functions to `addGenericSensor` and make it very clear how the `scope / entity / operation` names will be used to construct the group-name, sensor name, or even tags, etc in the Javadoc."
92724770,1446,guozhangwang,2016-12-15T23:40:34Z,"This is not introduced in this PR, but I'm wondering if there is any value we add this specific `addCacheSensor` instead of just using the ""generic adding sensor"" functions since we have already introduced them in this PR. Afterwards there is only one place using it for recording `cache hits` with `min, max, avg` which to me is quite generic usage."
92725173,1446,guozhangwang,2016-12-15T23:43:58Z,We could save this `nanoseconds` with `maybeRecord` pattern as well with DEBUG level.
92725338,1446,guozhangwang,2016-12-15T23:44:59Z,We could apply the `maybeRecord` pattern in the `MeteredStore` as well for all these DEBUG level sensors.
92725404,1446,guozhangwang,2016-12-15T23:45:36Z,Ditto above.
92725740,1446,guozhangwang,2016-12-15T23:48:22Z,There are a couple of other places where we can potentially save `nanoSeconds` calls (left comments on those places where `SENSOR_DEBUG` sensors may require getting the startNs and endNs). So I feel this could be a common pattern to add.
92870610,1446,enothereska,2016-12-16T19:24:35Z,"This PR is not about allowing users to register their own sensor yet, for the scope of this PR these are helper functions for our own usage internally. I'd like to separate the two if possible. "
92999354,1446,enothereska,2016-12-19T09:57:26Z,"Good catch, thanks!"
92999461,1446,enothereska,2016-12-19T09:58:00Z,Thanks!
93007886,1446,enothereska,2016-12-19T10:48:03Z,Ok.
93012648,1446,enothereska,2016-12-19T11:19:33Z,The answer is that we'll provide helper functions but also expose the metrics registry as well.
93102705,1446,enothereska,2016-12-19T19:32:23Z,"Done, thanks."
93230666,1446,ijuma,2016-12-20T12:50:12Z,Maybe `The higher recording level for metrics` or something like that?
93230738,1446,ijuma,2016-12-20T12:50:39Z,Adding a config typically requires a KIP. Are we planning to do that?
93231030,1446,ijuma,2016-12-20T12:52:30Z,Why is this not simply `INFO` and `DEBUG`?
93231096,1446,ijuma,2016-12-20T12:52:55Z,Why do we expose this instead of getting it via the enum?
93231227,1446,ijuma,2016-12-20T12:53:51Z,"Typically, this would be done by iterating over the enums (i.e. `RecordLevel.values()`) and then it doesn't have to be changed if we add more levels. Any reason why we can't do that?"
93231324,1446,ijuma,2016-12-20T12:54:33Z,"Instead of doing this, we can simply override `toString` in each enum. However, if we go with my suggestion of renaming the enum values, the `toString` will be the right one by default, I think."
93231397,1446,ijuma,2016-12-20T12:55:07Z,This should disappear with my suggestion.
93232020,1446,ijuma,2016-12-20T12:59:15Z,"We shouldn't really be using the `ordinal` in this way IMO. We should expose a method in the `RecordLevel` class to return a boolean in this case. For example, the invocation could look like `recordLevel.shouldRecord(config.recordLevel)`.

Also, using the `ordinal` internally can work, but it's a bit opaque (i.e. if people change the order of definition, they break the code). One often adds a new parameter to the enum instance to make it clearer."
93232147,1446,ijuma,2016-12-20T13:00:06Z,`shouldRecord`? `maybeRecord` sounds like it would record it for you.
93275266,1446,enothereska,2016-12-20T16:39:03Z,ok
93275350,1446,enothereska,2016-12-20T16:39:25Z,@guozhangwang what do you think?
93275549,1446,enothereska,2016-12-20T16:40:24Z,Ok
93275798,1446,enothereska,2016-12-20T16:41:36Z,Good point.
93276750,1446,enothereska,2016-12-20T16:46:33Z,Yup.
93276771,1446,enothereska,2016-12-20T16:46:38Z,Yup.
93276794,1446,enothereska,2016-12-20T16:46:44Z,"Yup, thanks."
93277891,1446,enothereska,2016-12-20T16:51:44Z,"Makes sense, thanks."
93283534,1446,enothereska,2016-12-20T17:18:07Z,ok
93296992,1446,guozhangwang,2016-12-20T18:33:22Z,"yup, sounds good to me."
93298089,1446,guozhangwang,2016-12-20T18:39:23Z,Do we really want to expose this function as a public API ?
93298423,1446,guozhangwang,2016-12-20T18:41:15Z,"So what is the final decision for these two functions? I saw that you have already exposed the underlying `Metrics` registry directly; in this case do we still need these two functions?

Personally I feel it is not necessary any more, but if you have a strong opinion maybe we should at least rename it to `addGenericSensor` to be consistent with other two."
93298928,1446,guozhangwang,2016-12-20T18:43:54Z,Shall we add a `recordThroughput` function as well?
93304064,1446,ijuma,2016-12-20T19:10:19Z,"Thanks for the updates. Looking better. :)

One thing I wasn't too clear about. For the `shouldRecord` case, we can pass a number to make the comparison more efficient. It's pretty similar to using `ordinal`, but the number is explicit instead of being based on the order of definition. Classes like `ApiKeys` and `SecurityProtocol` do that. We could also just use the ordinal if it's just used internally.

Another thing is that enums get a `name` method that returns the declaration name. In this case `INFO` and `DEBUG`. So, again, if it's an internal thing, we could potentially reuse that. Defining it explicitly is fine too (we tend to do that for public enums.

Finally, we don't use getter notation in Kafka so `getValue()` should be `value` (if we decide to keep it)."
93305783,1446,enothereska,2016-12-20T19:19:21Z,Sure. It's useful internally as well. This PR is primarily for internal needs so far. 
93310227,1446,guozhangwang,2016-12-20T19:43:16Z,"The issue is that, StreamsMetrics is a public class so any public functions of this interface will be accessible to users as well. If we really want to add it to let users be able to use it, we need to think about how to clearly differentiate with `recordLatency`, when to use which, etc. Personally I'd rather not exposing it but only for internal usage, and let users to make their own optimizations if they want."
93505557,1446,junrao,2016-12-21T19:39:40Z,Do we need to expose a similar config on the broker side since the broker also uses the client side metrics in certain cases?
94743881,1446,enothereska,2017-01-05T10:22:16Z,Done.
94746977,1446,enothereska,2017-01-05T10:44:00Z,Ok.
94748307,1446,enothereska,2017-01-05T10:53:17Z,"Ok, removing."
94751245,1446,enothereska,2017-01-05T11:16:35Z,Yeah probably. Will add. Thanks.
94757229,1446,enothereska,2017-01-05T12:09:16Z,"Will do this, ideally before feature freeze, but definitely before code freeze. Stay tuned."
95413470,1446,guozhangwang,2017-01-10T17:22:32Z,I am not sure if it is needed: `Metrics.sensor` will recursively link parent sensor to its children; `Metrics.removeSensor` will recursively remove its children sensor as well.
95413953,1446,enothereska,2017-01-10T17:25:00Z,"@guozhangwang `addLatencyMetrics` is supposed to return 1 sensors, but it creates 2 internally. One of them, the parent, is never exposed to the user, so the user has no way of deleting it. With this solution, the user deletes the child sensor and we internally delete the parent sensor."
95415769,1446,guozhangwang,2017-01-10T17:33:39Z,"Got it, thanks."
95559941,1446,enothereska,2017-01-11T11:24:12Z,"Done now, thanks @ijuma "
95629070,1446,guozhangwang,2017-01-11T17:30:33Z,Is this intentional? Ditto below.
191968009,5101,lindong28,2018-05-31T01:29:34Z,Why do we need to change it to 0? Do we expect to initialize `ControllerContext.epochZkVersion` to -1?
192005699,5101,lindong28,2018-05-31T07:05:43Z,nits: we probably don't need `{` here so that the code style is consistent with the existing code.
192008864,5101,lindong28,2018-05-31T07:22:46Z,"Here `SetDataRequest/SetDataResponse` is replaced with `MultiOpRequest/MultiOpResponse`. This may cause problem for existing code (e.g. `ZookeeperClient.send()`) whose logic relies on the type of the request.

Instead of adding a new subclass of AsyncRequest, would it be better to modify the existing request (maybe the AsyncRequest) to include the expected controller epoch version so that, when the expected controller epoch exists, zookeeperClient.send() will take care of the version check?"
192020116,5101,hzxa21,2018-05-31T08:13:18Z,"I think the `InitialControllerEpochZkVersion` represents the initial value when the controller epoch znode first gets created so it should be set to 0 for consistency just like `InitialControllerEpoch`. 

Also, during cluster initialization (i.e controller epoch znode does not exists), we explicitly set `ControllerContext.epochZkVersion` to `InitialControllerEpochZkVersion`, which was 1 before.
(https://github.com/apache/kafka/pull/5101/files/dcf94b0e78208ec1fc11ebe538f743063880a798#diff-ed90e8ecc5439a5ede5e362255d11be1L634)
This will cause problems for the first controller to update zk states after this patch because the actual znode version is 0. This is not a problem before because we didn't use `ControllerContext.epochZkVersion` to fence zk state updates and after controller failover, we will update it by reading controller epoch znode.

This change is only for readbility. Actually we can just remove the line to set `ControllerContext.epochZkVersion` to `InitialControllerEpochZkVersion` and keep `InitialControllerEpochZkVersion` to be 1."
192020231,5101,hzxa21,2018-05-31T08:13:50Z,Thanks for pointing out. Will fix that.
192023554,5101,hzxa21,2018-05-31T08:27:18Z,"That is a good point. The reason why I add the MultiOp subclass is that we can extend `ZookeeperClient` to handle arbitrary multi() operations, not specifically for checking controller epoch version and updating zk states. Also, I think `ZookeeperClient` should only be aware of zookeeper related context not the kafka related context (e.g. controller epoch version) for cleanness. Instead of modifying AsyncRequest, do you think it is better to add some helper functions in `KafkaZkClient` to wrap around the check and set/update/create logic?

Also, I am a little bit confused on what are the problems caused by `ZookeeperClient.send()` if we use `zookeeper.multi()` for `MultiOpRequest`. Can you give me more contexts on that?"
192188682,5101,lindong28,2018-05-31T18:09:13Z,Cool. This makes sense.
192206415,5101,lindong28,2018-05-31T19:10:04Z,"I think it is reasonable not to have kafka specific thing in ZookeeperClient. On the other hand we don't have to -- we can provide anther zk path and expected version as parameters to these APIs, the API should return proper error without executing the original request if the version of the path is different from the expected version. This solution is probably not Kafka specific.

Not sure that we don't need arbitrary multi() operations in the near future. Currently we only need to check the zk version of another path when controller changes zookeeper state.

After checking the code, it seems that there is no current problem caused by using MultiOpRequest. But some information (e.g. `CreateResponse.name` and `SetDataResponse.stat`) is discarded in the response which may potentially be problem in the future.  In general it seems more flexible to be able to use different case class for different requests so that we can have different parameters (as is the case now) and apply different processing logic to different case class. Just my opinion. If you like the current solution, maybe you can keep it and other committers can comment on this.
"
192302796,5101,hzxa21,2018-06-01T05:46:08Z,I see what you mean. That makes sense to me. I will update the PR to differentiate Set/Update/Create with check parameters provided in `ZookeeperClient` to expose different information. Thanks for the explanation.
194241899,5101,lindong28,2018-06-10T00:17:10Z,Should the comment be `expected controller epoch zkVersion`?
194241972,5101,lindong28,2018-06-10T00:24:23Z,Do we need the `path` field here? Can we remove this class and replace it with e.g. `ZkVersionCheckResultCode: resultCode`?
194242568,5101,lindong28,2018-06-10T01:08:00Z,"In general we want to the exception returned to the caller to uniquely identify the problem. But here we can output the BadVersionException if either the controller epoch zkversion is bad or the data znode zkversion is bad. It maybe confusing.

It is probably simpler to create a new exception, e.g. `ControllerEpochZkVersionMismatchException` and do the following before checking `setDataResponse.resultCode`:

```
if (setDataResponse.zkVersionCheckResultCode != Code.OK)
  failed.put(partition, new ControllerEpochZkVersionMismatchException())
```"
194242678,5101,lindong28,2018-06-10T01:17:21Z,"Can we name it `case class ZkVersionCheck(path: string, zkVersion: Int )`"
194242922,5101,lindong28,2018-06-10T01:34:55Z,Typo
194243046,5101,lindong28,2018-06-10T01:44:43Z,We can name it to be `controllerEpochZkVersion` to be consistent with `ControllerContext.epochZkVersion`? Same for other uses of `controllerEpochVersion`.
194243176,5101,lindong28,2018-06-10T01:55:46Z,"The current patch checks `resultCode`, and based on its value, adds additional logic to check `checkResult`.

Similar to the comment for `updateLeaderAndIsr`, could we simplify the logic here by checking `deleteResponse.checkResult` before the existing logic of checking the resultCode?"
194243224,5101,lindong28,2018-06-10T02:00:42Z,It seems that we will return `BadVersionException` for two different scenarios. Can we output a unique exception if the controller znode has different version from what is expected?
194244953,5101,lindong28,2018-06-10T04:37:09Z,"`generateAsyncResponseWithCheckResult()` is called for `CreateRequest`, `SetDataRequest` and `DeleteRequest`. However, most of the code (or logic) in `generateAsyncResponseWithCheckResult()` is different for these three requests anyway. Would it be more intuitive and simpler to remove the method `generateAsyncResponseWithCheckResult()` and moves its request-specific logic in `send()`? We can put the logic that is common to all requests, e.g. the first part of generateAsyncResponseWithCheckResult(), in a method if needed.

"
194577297,5101,hzxa21,2018-06-11T23:28:09Z,Yes. Will fix it.
194577987,5101,hzxa21,2018-06-11T23:32:42Z,"The path is needed to generate keeper exception with path information without assuming it to be controller epoch path. If we are going to generate the `ControllerEpochZkVersionMismatch` exception in `KafkaZkClient` instead of `KeeperException` in `ZookeeperClient`, I think we can remove the path and just keep the resultCode."
194578905,5101,hzxa21,2018-06-11T23:38:42Z,"Thanks for the suggestion. It is simpler and cleaner this way.

Btw, I think we can just use `ControllerMoveException()` in this case. 

Also, related to your comment on my last commit, instead of putting the exception in some data structures, can we just simply throw the exception when the check fails? In this case, we can skip processing unnecessary controller event until we hit `ControllerChange` event. To optimize it further, we can also catch 'ContollerMoveException' explicitly in the `ControllerEventThread` and let the controller resigns immediately."
194578945,5101,hzxa21,2018-06-11T23:38:58Z,Sure. Will do.
194578974,5101,hzxa21,2018-06-11T23:39:07Z,Thanks. Will fix.
194578989,5101,hzxa21,2018-06-11T23:39:14Z,Sure.
194579028,5101,hzxa21,2018-06-11T23:39:29Z,Will fix it. Thanks for pointing out.
204258438,5101,lindong28,2018-07-22T23:09:20Z,Maybe rename `controllerMoveListener` to `controllerMovedListener` so that it is more consistent with the existing name `eventProcessedListener`.
204258805,5101,lindong28,2018-07-22T23:21:35Z,type: should be `emptyEventQueueAndReelect`. Also it seems the name `clearEventQueueAndReelect` is more consistent with the existing method names.
204258904,5101,lindong28,2018-07-22T23:24:44Z,"nits: we typically just use `e: ControllerMovedException` here for simplicity. `cme` does not provide much information since most users would still need to read the actual type to understand what it is. If it makes sense, can you rename it here and in other places of the patch?"
204259174,5101,lindong28,2018-07-22T23:32:47Z,In general the code may be more consistent and readable if we name the variable after this type. And `zkVersionCheck` seems more informative than the `checkInfo`. Can you rename the `checkInfo` here and in other places of the patch (including local variable)?
204259221,5101,lindong28,2018-07-22T23:34:24Z,"Would it be better to rename `controllerEpochZkVersion` to `expectedControllerEpochZkVersion`? The current method signature seems to suggest that the `controllerEpochZkVersion` will be written to the znode. If it makes sense, can you rename the variable here and in other places of the patch?"
204259339,5101,lindong28,2018-07-22T23:37:53Z,I am wondering whether it will be useful to print the expected/current controllerEpoch and zkversion. Not sure if this information is already printed when controller processes `Reelect` etc.
204259617,5101,lindong28,2018-07-22T23:45:57Z,"Just in case this patch causes any issue, it may be useful if we still print the message such as `Error completing reassignment of partition ...`. If it makes sense, can you add the additional log based on the existing log (if exists) here and in other places where the `ControllerMovedException` is caught and thrown?"
204259978,5101,lindong28,2018-07-22T23:56:12Z,"Can we specify `zkVersion` in the name, e.g. `controllerZkVersionCheck`? Also, can you add the return type to the method signature?"
204260316,5101,lindong28,2018-07-23T00:05:52Z,"It seems that checkOpResult should be either `CheckResult` or `ErrorResult`. Maybe we should throw IllegalStateException otherwise? By doing so we could simplify the signature of `getMultiOpResults` to `(Code, OpResult)`. And we can also simplify the signature of e.g. `CreateResponse` such that zkVersionCheckResultCode is of type `Code`."
204260401,5101,lindong28,2018-07-23T00:08:58Z,Would it be simpler to name it `zkVersionCheck`?
204260465,5101,lindong28,2018-07-23T00:10:59Z,Can you add the return type to the signature of `checkOp()`?
210026725,5101,hzxa21,2018-08-14T16:53:58Z,Done.
211706198,5101,hzxa21,2018-08-21T18:12:07Z,Done.
211706290,5101,hzxa21,2018-08-21T18:12:24Z,Done.
211706505,5101,hzxa21,2018-08-21T18:13:01Z,Done.
211706631,5101,hzxa21,2018-08-21T18:13:27Z,Done.
211707860,5101,hzxa21,2018-08-21T18:16:52Z,That is a good point. I have added a log in the Reelect controller event to print out this information.
211708052,5101,hzxa21,2018-08-21T18:17:25Z,Done.
211708354,5101,hzxa21,2018-08-21T18:18:20Z,Done.
211708407,5101,hzxa21,2018-08-21T18:18:30Z,Done.
211708530,5101,hzxa21,2018-08-21T18:18:49Z,Yes. Done.
211708610,5101,hzxa21,2018-08-21T18:19:02Z,Done.
211709050,5101,hzxa21,2018-08-21T18:20:15Z,Done.
211709057,5101,hzxa21,2018-08-21T18:20:16Z,Done.
211719034,5101,lindong28,2018-08-21T18:50:34Z,"Not sure if this line invokes the callback. Maybe we should change it to controllerMovedListener.apply(). If the existing version does not actually execute this callback, then it means all existing test does not catch this issue. Then it may be worthwhile adding a test."
211720966,5101,lindong28,2018-08-21T18:56:54Z,"nits: It seems a bit confusing that we print more information (i.e. newReplicas) for `ControllerMovedException` than all other exception. It may be better to make the log information consistent and still print `error(s""Error completing reassignment of partition $tp"", e)`."
211721256,5101,lindong28,2018-08-21T18:57:52Z,"Can we still print `error(s""Error completing preferred replica leader election for partitions ${partitions.mkString("","")}"", e)` for consistency?"
211721459,5101,lindong28,2018-08-21T18:58:29Z,nits: Can we replace `epoch version is now` with `epoch zk version is now`
211731340,5101,lindong28,2018-08-21T19:32:32Z,"Since `KafkaController.incrementControllerEpoch()` will always print controllerContext.epochZkVersion, the only extra information we are seeking here is the current zkversion of the controller epoch znode. It seems that we only need this information when the broker observes ControllerMovedException when it thinks it is controller. Since `Reelect` is triggered in every broker every time there is controller movement, it may not be very intuitive or necessary to print the extra log here in `Relect.process()`.

Another thing to note that that we would like to know the zkversion of the controller epoch znode that causes the ControllerMovedException, but this zk version may have changed after the controller observes ControllerMovedException but before the controller processes Reelect event. So it is better to read the zkversion earlier (e.g. in `ControllerEventThread.doWork()`) than later. The best solution is probably to include the expected zkversion in the message of `ControllerMovedException` thrown by `maybeThrowControllerMoveException()`."
211734782,5101,lindong28,2018-08-21T19:44:24Z,"Given that there may be other zookeeper operation other than `controllerZkVersionCheck` which can also check the zkversion of the corresponding znode, some response may also show `zkVersionCheckResultCode != Code.OK` and cause `maybeThrowControllerMoveException` to throw `ControllerMovedException` even if it is not for the `controllerZkVersionCheck`. Will this be a problem?

Also, any chance we can also include the expected zkversion in the message of `ControllerMovedException`?"
211735664,5101,lindong28,2018-08-21T19:47:24Z,"For code style consistency, can you change the code to use one of the following styles:
```
if (zkVersionCheck.isEmpty) {
  ...
} else {
  ...
}
```

or 

```
if (zkVersionCheck.isEmpty)
  // one line
else
  // oneline
```"
211738667,5101,lindong28,2018-08-21T19:57:09Z,"nits: I am not sure what is the expected code style here. But if there is no clear standard and it is not very obvious, it is probably simpler to keep the existing style so that we avoid back-and-force change in the open source community."
212047310,5101,hzxa21,2018-08-22T17:51:22Z,Done.
212050784,5101,hzxa21,2018-08-22T18:01:05Z,"Yes, I think it is better to include the expected zkVersion in the zookeeper response and extract the information from the response when throwing `ControllerMovedException`. I have added `zkVersionCheckResult: Option[ZkVersionCheckResult]` to achieve this."
212059088,5101,hzxa21,2018-08-22T18:24:40Z,Make sense. I have removed the extra logs and included the expected zkVersion in the message of `ControllerMovedException`.
212059196,5101,hzxa21,2018-08-22T18:24:57Z,Done.
212059233,5101,hzxa21,2018-08-22T18:25:04Z,Done.
212059283,5101,hzxa21,2018-08-22T18:25:13Z,Done.
212059557,5101,hzxa21,2018-08-22T18:26:05Z,I have changed it to `controllerMovedListener.apply()`. Will add a test in future commits.
212083665,5101,lindong28,2018-08-22T19:43:26Z,nits: can we also replace `-1` with `ZkVersion.MatchAnyVersion`?
212085220,5101,lindong28,2018-08-22T19:49:09Z,It seems that we need extra indentation for the body of the `if` statement.
212086207,5101,lindong28,2018-08-22T19:52:42Z,`ZooDefs` seems to be unused.
212086264,5101,lindong28,2018-08-22T19:52:52Z,`CheckResult` seems to be unused.
212086460,5101,lindong28,2018-08-22T19:53:36Z,`CheckResult` and `ErrorResult` seems to be unused.
212128308,5101,hzxa21,2018-08-22T22:18:47Z,Fixed.
212128341,5101,hzxa21,2018-08-22T22:18:56Z,Removed.
212128387,5101,hzxa21,2018-08-22T22:19:03Z,Removed.
212128438,5101,hzxa21,2018-08-22T22:19:10Z,Removed.
212144205,5101,junrao,2018-08-22T23:42:44Z,"Hmm, technically, only when the error code is BADVERSION, it's an indication that the controller has moved. For other errors, we probably just want to propagate as they are to the caller."
212144563,5101,junrao,2018-08-22T23:44:48Z,Could this just be controllerMovedListener()?
212147981,5101,junrao,2018-08-23T00:05:44Z,"We probably need to be a bit careful about bumping up the controller epoch at the beginning of onControllerFailover(). Currently, the reading and the incrementing of the controller epoch is done independently after the controller path has been created successfully. This can create the following problem. Broker A creates the controller path and is about to call onControllerFailover(). Admin deletes the controller path and broker B creates the controller path, reads the controller epoch and updates it to 1. Broker A reads the controller epoch and updates it to 2. Now broker B is the controller, but its controller epoch is outdated.

One way to address this issue is to use multi() when creating the controller path. To elect a new controller, a broker first reads the current controller epoch from ZK and then do a multi() to (1) write the controller path (2) do a conditional update to the controller epoch. Not sure if this is the best way though."
212153922,5101,junrao,2018-08-23T00:48:34Z,We log e here but not in line 260. It would be useful to be consistent.
212155652,5101,junrao,2018-08-23T01:01:49Z,Could this be private?
212155764,5101,junrao,2018-08-23T01:02:40Z,Could this be private?
212793210,5101,hzxa21,2018-08-25T08:48:27Z,Done.
212793216,5101,hzxa21,2018-08-25T08:48:36Z,Yes. Done.
212793218,5101,hzxa21,2018-08-25T08:48:43Z,Yes. Done.
212793233,5101,hzxa21,2018-08-25T08:49:02Z,That is a good point. Fixed.
212793237,5101,hzxa21,2018-08-25T08:49:13Z,Yes. Fixed.
212793993,5101,hzxa21,2018-08-25T09:25:41Z,"Thanks for pointing this out. This is indeed a very dangerous race condition. If it happens, the current controller (Broker B) cannot update any zookeeper state due to controller epoch zkVersion mismatch and no other broker can become the controller because the current controller (Broker B) does not release the ""lock"" for `\controller` znode.

Wrapping `\controller` creation and `\controller_epoch` update in a zookeeper transaction can prevent this race condition and I think it is a safe option. I will make the change and see whether there will be performance overhead in the perf testing."
212843697,5101,lindong28,2018-08-26T23:56:09Z,"Hey @hzxa21, it seems that what you and Jun suggested to do is to have a single multiops that 1) updates controller path, 2) read controller epoch and 3) updates controller epoch. Another alternative approach is to have a single multiops that 1) updates controller path and 2) reads controller epoch with its zkversoin. Then the controller can updates controller epoch with the addition zkversion check.

Do you think the alternative approach would avoid the race condition and ensure correctness? If so, I am wondering if the alternative would be easier to reason about. I find it a bit easier because the it follows the idea that all zookeeper write operation by controller will be based on the controller epoch zkversion check, except for the controller znode write operation which by design can not rely on the controller epoch zkversion check. And a multiop that does one write and one read seems simpler than a multiop that does write-read-write.

"
212868803,5101,hzxa21,2018-08-27T05:26:48Z,"@lindong28 From a design and code readability perspective, I agree with what you have proposed (First atomic read `\controller_epoch` and create `\controller`, then update `\controller_epoch`). From the implementation perspective, zookeeper does not have a `read` Op meaning that we cannot perform `read` operation with the `multi` (see http://people.apache.org/~larsgeorge/zookeeper-1215258/build/docs/dev-api/org/apache/zookeeper/Op.html).

Basically, we use the time when a broker succeeds in incrementing the controller epoch as the ""commit"" point of the controller election and use the time when a broker succeeds in creating `\controller` znode as the ""prepare"" point. So for the correctness of the controller election ""commit"", we need to ensure `\controller_epoch` doesn't change from ""prepare"" to ""commit"". To achieve, we can implement the logic using zk `multi` following the steps:
1. Read `\controller_epoch` to get the current controller epoch **e1** with zkVersion **v1**
2. Create `\controller` if `\controller_epoch` zkVersion matches **v1** (use zk `multi`)
3. Update `\controller_epoch` to be **e1+1** if its zkVersion matches **v1** (zk conditional set) "
212913449,5101,hzxa21,2018-08-27T09:12:14Z,PR updated to address this issue.
213035777,5101,lindong28,2018-08-27T16:31:37Z,"If `onControllerFailover()` throws `ControllerMovedException` after controller has registered itself as controller, it seems possible that some events may have already been inserted into the controller event queue. Should we propagate the `ControllerMovedException` to `ControllerEventThread` in order to clear the controller event queue?

Also, I think in most places we will just name the exception varaible as `e` and throwable variable as `t`. Naming them e1, e2, and specifically naming a throwalble as `e2`, seems unusual. I know this style is used in the existing controller code. I am wondering if we can change this."
213040676,5101,lindong28,2018-08-27T16:49:32Z,"I am wondering if the code will be more readable by removing this method and putting these three lines in `elect()` directly. The method is used only once and it is very short. And the two additional lines used to update the in-memory controllerContext seems more inline with the update of `activeControllerId` in `elect` than with the name of `tryRegisterController()`.

"
213041250,5101,lindong28,2018-08-27T16:51:44Z,"If there is no existing controller, `getControllerEpoch` returns `None`. Should we still try to register controller in this case?"
213041939,5101,lindong28,2018-08-27T16:54:32Z,"Can we rename this method to `maybeCreateControllerZNode` so that it is more consistent with the existing names such as `KafkaController.maybeTriggerPartitionReassignment()`?
"
213043734,5101,lindong28,2018-08-27T17:01:26Z,Currently the variable `timestamp` is passed all the way from `elect()` to `KafkaZkClient.tryCreateControllerZNode()`. Would it be simpler to replace this variable with `time.milliseconds` in `KafkaZkClient.tryCreateControllerZNode()`?
213087554,5101,lindong28,2018-08-27T19:29:41Z,"Since it is not very intuitive from the method name to understand the meaning of the returned value `(Int, Int)`, can we add Java doc for the returned value?

Since the new implementation of this method will try to create/update controller znode and increments controller epoch in a safe manner, would it be better to rename the method `registerControllerAndIncrementControllerEpoch`?"
213089263,5101,lindong28,2018-08-27T19:35:44Z,"Info level logging is needed if user always want to see the message and it is usually used when something major is completed, e.g. server is started, rather than when something is attempted. It seems that ""Try to create.."" and ""Try to increment controller..."" may be more appropriate to be debug level logging if they are needed. The controller epoch and zkversion have been logged at INFO level in `elect()`."
213097271,5101,lindong28,2018-08-27T20:03:39Z,"Prior to this patch, if setControllerEpochRaw fails and the error is not `NONODE`, ControllerMovedException will be thrown which will be caught and `triggerControllerMove()` will be executed.

After this patch, if setControllerEpochRaw returns a non-OK error code, we will always throw ControllerMovedException(), which will be caught in the upper layer without executing `triggerControllerMove()`. I am wondering if we should throw IllegalStateException if the error code suggests something other than controller move, so that we can still execute `triggerControllerMove()` in this scenario."
213100747,5101,junrao,2018-08-27T20:16:30Z,"For errors other than BADVERSION, we should just propagate the original error as an exception."
213112066,5101,junrao,2018-08-27T20:54:22Z,"Not sure if we need to explicitly do the creation here. When the controller path is removed, every broker's controller listener will fire, which will trigger the controller election logic again."
213118760,5101,hzxa21,2018-08-27T21:18:09Z,"You are right. We should propagate the exception here.
Done."
213118780,5101,hzxa21,2018-08-27T21:18:13Z,Done.
213122229,5101,hzxa21,2018-08-27T21:30:23Z,"No. 
Previously we create `/controller_epoch` on-demand if it does not exist when we try to increment controller epoch. IMO, this makes the code hard to read and reason about, especially after this patch because in that way we need to either create `/controller_epoch` if not exists and retry the atomic operation or we have two different atomic operations (one for check+create, the other one for create+create). 

I think `/controller_epoch` should pre-exists before we actually use it, like other persistent zk paths (e.g. /brokers, /admin/delete_topics) . So I have included `/controller_epoch` in the ""PersistentZkPaths"" so that it will be created if not exists on broker startup. In this case, `getControllerEpoch` should not return None unless admin deletes `/controller_epoch` explicitly, which will ruin the cluster anyway.

One drawback of pre-creating `\controller_epoch` is that admin now cannot re-initialize controller epoch by simply deleting `\controller_epoch`. Instead, admin should delete `/controller` and `/controller_epoch`, then re-create `\controller_epoch` to achieve this. But I don't know whether that is a valid use case. May I have your opinion?"
213122263,5101,hzxa21,2018-08-27T21:30:31Z,Done.
213122308,5101,hzxa21,2018-08-27T21:30:40Z,I agree. Fixed.
213122347,5101,hzxa21,2018-08-27T21:30:45Z,Done.
213122376,5101,hzxa21,2018-08-27T21:30:49Z,Done.
213124615,5101,junrao,2018-08-27T21:39:33Z,"This is called on ControllerMovedException. In this case, we know that another broker has become the controller. We just need to clear the event queue, mark the controller as inactive and call onControllerResignation() . There is no need to do the controller election. If the new controller is gone afterward, every broker's controller path watcher will be triggered and a controller election will be tried. We probably should rename this method accordingly.

Also, we probably want to consolidate this method and triggerControllerMove() somehow. To me, the latter will just do what this method does and one more thing, removing the controller path."
213126066,5101,hzxa21,2018-08-27T21:45:16Z,"Per https://github.com/apache/kafka/pull/5101#discussion_r213122229
`/controller_epoch` should exists and `setControllerEpochRaw` should not return `NONODE`. If that does happen, we will rely on admin to recover and `triggerControllerMove()` will not help.
I agree that throwing `ControllerMovedException` is not a good idea. Maybe we should just throw `IllegalStateException` and indicate that is a FATAL error. What do you think?"
213136758,5101,junrao,2018-08-27T22:37:02Z,Perhaps we should just fold the logic in here to clearEventQueueAndReelect() and always let EventManager handle ControllerMovedException.
213137736,5101,junrao,2018-08-27T22:42:18Z,Could we just do the conditional controller epoch update and the creation of the controller path together in tryCreateControllerZNode()? This avoids an extra ZK step.
213140323,5101,junrao,2018-08-27T22:56:14Z, it process =>  it processes
213140367,5101,junrao,2018-08-27T22:56:27Z,What's PLE?
213141257,5101,junrao,2018-08-27T23:00:57Z,"suspend()/resume() are deprecated. We can probably simulate this by adding a new controller event type. Within the event, we can let it wait on a CountdownLatch. Once the controller is moved, we can unblock the CountdownLatch."
213142512,5101,junrao,2018-08-27T23:08:35Z,Should we assert the return value?
213142665,5101,junrao,2018-08-27T23:09:21Z,It seems that the last one is enough?
213142925,5101,lindong28,2018-08-27T23:10:34Z,"Prior to this patch, we only include znode in ZkData.PersistentZkPaths() if there is no need for the data in the znode. This patch changes this behavior such that we create znode will null data and we assume epoch is -1 if the data is null.

The previous approach says that either the znode does not exist, or the znode exists with valid data. The new approach says that either the znode exists with null, or the znode exists with valid data. I personally prefer the previous approach and I would prefer not to define a znode with null data and add additional code to handle that case. And in general it is probably better to keep the existing code if there is no difference in correctness/performance and the difference in code style is not very obvious w.r.t which one is better.

It looks like the main concern with the previous approach is about code complexity. How about we have create and call method `maybeCreateControllerEpochZnode` at the beginning of `registerController()`?"
213143236,5101,junrao,2018-08-27T23:12:20Z,This is an existing issue. Could you add a space after the comma in the next line?
213144214,5101,lindong28,2018-08-27T23:18:33Z,"Now that `ControllerMovedException` may be handled differently from other exceptions, the logic would be cleaner if we use this exception only when we know the another broker is the controller.

Thinking about it more, IllegalStateException means something impossible has happened inside the controller state. In this case the exception can happen if controller fails to write to controller epoch znode, which is possible from controller's point of view since zookeeper service is out of controller of the controller. How about `ZooKeeperClientException(...)` and include error code in the message of the exception?
"
213144670,5101,junrao,2018-08-27T23:21:30Z,"Since createAndRegister has side effect, we should do createAndRegister()."
213145071,5101,junrao,2018-08-27T23:23:46Z,Should we clear events?
213151722,5101,hzxa21,2018-08-28T00:07:57Z,Thanks for the suggestion. I have added `maybeCreateControllerEpochZnode` and avoid crearting the znode on broker start up.
213158074,5101,hzxa21,2018-08-28T01:00:08Z,Thanks for the comment. I have updated the PR to throw `ControllerMovedException` only when we see BadVersion in `setControllerEpochRaw`. I also applied the same idea to `maybeCreateControllerZNode`.
213179550,5101,hzxa21,2018-08-28T04:11:44Z,"If we don't try to trigger `elect` after we clear the queue and the new controller goes away before we clear the queue, the watch may have put `Reelect` in the queue before the clear happens. In this case, that broker will miss controller election. From the correctness point of view, this may not be a problem because at least one other broker will conduct the controller election and become the controller. It is safe to clear and resign if we don't care about fairness in controller election."
213436446,5101,junrao,2018-08-28T19:05:53Z,"Great point. We could just do Reelect here as you suggested. I was thinking that we could also potentially inline the logic (clear the event queue, mark the controller as inactive and call onControllerResignation()) here instead of enqueuing the logic to the event queue.  However, it seems that the former may be simpler."
213610117,5101,omkreddy,2018-08-29T09:38:39Z,"nit: we can remove ""unit"" "
213726027,5101,omkreddy,2018-08-29T15:29:31Z,Do we need ```deleteTopicEnable: Boolean = false``` flag?  looks like none of the tests depends on deleteTopicEnable=false.  all tests are passing without this line.
213727391,5101,omkreddy,2018-08-29T15:32:47Z,Do we need these changes? tests are passing without these changes.
213880720,5101,hzxa21,2018-08-30T01:26:49Z,Done.
213880874,5101,hzxa21,2018-08-30T01:28:21Z,You are right. Changed to throw `ControllerMovedException` instead.
213880937,5101,hzxa21,2018-08-30T01:28:56Z,Agree. Code refactored.
213880947,5101,hzxa21,2018-08-30T01:29:03Z,Done.
213881008,5101,hzxa21,2018-08-30T01:29:37Z,"Yes, you are right. Fixed."
213881027,5101,hzxa21,2018-08-30T01:29:43Z,Fixed.
213881117,5101,hzxa21,2018-08-30T01:30:27Z,I use that to stand for PreferredLeaderElection. Fixed the comments to make it more clear.
213881217,5101,hzxa21,2018-08-30T01:31:14Z,Remove the deprecated methods and added the additional event for the test.
213881231,5101,hzxa21,2018-08-30T01:31:20Z,Yes. Fixed.
213881242,5101,hzxa21,2018-08-30T01:31:27Z,Yes. Fixed.
213881251,5101,hzxa21,2018-08-30T01:31:33Z,Done.
213881264,5101,hzxa21,2018-08-30T01:31:38Z,Done.
213881314,5101,hzxa21,2018-08-30T01:31:59Z,Done.
213881326,5101,hzxa21,2018-08-30T01:32:04Z,Done.
213881555,5101,hzxa21,2018-08-30T01:34:03Z,"Yes. I added that for `testControllerMoveOnTopicDeletion` but it turns out we don't actually need to enable topic deletion to test throwing and handling `ControllerMovedException` happening in `TopicDeletion` event. 
Fixed."
213881574,5101,hzxa21,2018-08-30T01:34:10Z,Removed.
214443770,5101,lindong28,2018-08-31T18:44:42Z,Would logic be more intuitive to just treat the ControllerMovedException as `ControllerChange` event and do `maybeResign()`? The code would be simpler since this approach doesn't need `markInactiveAndResign`.
214444603,5101,lindong28,2018-08-31T18:48:10Z,"nits: ""controller move listener"" -> ""controllerMovedListener"".

Also, it seems simpler to just remove `Trigger controller move listener immediately` as we typically do not log which method is executed next other than logging the event itself. Developer is expected look into the code and understand what happens next in the code after this event."
214444932,5101,lindong28,2018-08-31T18:49:39Z,Can you remove `AwaitOnLatch` if it is not used?
214447851,5101,lindong28,2018-08-31T19:00:50Z,"This line throws `NoSuchElementException` if controller epoch does not exist. It seems better to do `getControllerEpoch.getOrElse(throw new IllegalStateException(""...""))`."
214524631,5101,lindong28,2018-09-01T22:51:44Z,Would it be more consistent with the other code in this method to do `case Code.OK =>`?
214524720,5101,lindong28,2018-09-01T22:59:50Z,"When SRE deletes controller znode, multiple brokers may be doing `elect()` concurrently and all but one broker will find that the controller znode alread exists. 

Prior to this patch, these brokers will log `debug(s""Broker $activeControllerId was elected as controller instead of broker ${config.brokerId}"")` if controller znode exists and the controller id is not this broker.

After this patch, these brokers will log `error(s""Error while creating ephemeral at ${ControllerZNode.path}, node already exists and owner ${getDataResponse.stat.getEphemeralOwner} does not match current session ${zooKeeperClient.sessionId}"")` and `error(s""Error while electing or becoming controller on broker ${config.brokerId} because controller moved to another broker"", e)`
if controller znode exists and the controller id is not this broker.

Since we expect most brokers to find znode to be created by another broker during `elect()`, we probably want to keep the old behavior instead of having error level logs."
214524844,5101,lindong28,2018-09-01T23:09:45Z,"It seems that we can enter this state only if broker executes `registerControllerAndIncrementControllerEpoch()` and finds that the controller znode has already been created by itself. The question is, is this possible?

Previously if broker tries to create controller znode and node already exists, the broker will simply read the controller id from the controller znode and move on. This patches added quite a few new logic in `controllerNodeExistsHandler()`, e.g. uses zk session id to detect whether the controller znode is created by this broker, handles the scenario that the controller znode is created by this broker. So the new code is more complicated than the previous version. Can you explain a bit why we need these new logic?
"
214525056,5101,lindong28,2018-09-01T23:28:05Z,Is it possible for error code to be `Code.OK` while `zkVersionCheckResult.opResult` is of type  `ErrorResult`?
214525058,5101,lindong28,2018-09-01T23:28:33Z,Is it possible for `ControllerEpochZNode.path` to be different from `zkVersionCheck.checkPath`?
214534027,5101,hzxa21,2018-09-02T09:15:22Z,"Yes. For example, if we wrap `check` + `create` in zookeeper `multi`, and `multi` fails  due to `create` fails, the result of `check` will be of type `ErrorResult` with `Code.OK` as error code."
214534041,5101,hzxa21,2018-09-02T09:15:47Z,`AwaitOnLatch` is used in `ControllerIntegrationTest`
214534378,5101,hzxa21,2018-09-02T09:28:42Z,"In short, the purpose of `controllerNodeExistsHandler` is to mimic `checkedEphemeralCreate `, which previously is used to create `/controller` ephemeral node. In `CheckedEphemeral`, we need to double check the owner of the node if we saw `Code.NODEEXISTS`.

I think the purpose of the check and the additional logic is to handle transient network connection loss while creating the ephemeral.  Let's say our client sent a `create` request to zookeeper to create ephemeral znode and zookeeper receives this request and successfully creates the znode but fail to send back the response to our client because of transient network issue. In `retryUntilConnected`, our client tries to resend the request and gets the `Code.NODEEXISTS`. In this case, our client actually successfully creates and owns the znode."
214534566,5101,hzxa21,2018-09-02T09:36:15Z,Currently no. The check here is for general purpose and safety because the zkVersionCheck can apply on any znode if needed.
214535433,5101,hzxa21,2018-09-02T10:15:22Z,Agree. Fixed.
214535436,5101,hzxa21,2018-09-02T10:15:28Z,Done.
214535441,5101,hzxa21,2018-09-02T10:15:39Z,Done.
214535445,5101,hzxa21,2018-09-02T10:15:54Z,Yes. Done.
214535450,5101,hzxa21,2018-09-02T10:16:05Z,Agree. Fixed.
214542290,5101,lindong28,2018-09-02T14:12:48Z,"The creation of the ephemeral znode `/controller` is probably a bit different from the creation of other ephemeral znode. The broker which creates the ephemeral znode `/controller` is explicitly specified in the znode data. Thus the old approach, which reads the broker id from the controller znode after seeing `Code.NODEEXISTS`, seems OK. And that old approach seems to handle the network connection loss scenario described here. I am wondering if we can use the old approach since its logic looks simpler. What do you think?

"
214542333,5101,lindong28,2018-09-02T14:14:01Z,Cool. I see.
214542476,5101,lindong28,2018-09-02T14:17:24Z,Got it. Could you add a comment above the case class `` that says `Used only by test`? This is similar to e.g. `ReplicaManager.markPartitionOffline(...)`.
214542542,5101,lindong28,2018-09-02T14:19:30Z,Got it.
214542750,5101,lindong28,2018-09-02T14:25:53Z,"BTW, for the case `getDataResponse.stat.getEphemeralOwner != zooKeeperClient.sessionId`, we know this case may happen and the code will automatically recover from this. In this case it is probably better to log at warning level instead of error level. Here is a good explanation for how to choose log level. https://stackoverflow.com/questions/2031163/when-to-use-the-different-log-levels."
214760829,5101,hzxa21,2018-09-03T22:52:10Z,"I am a little bit confused about what do you mean by the old approach. Are you referring to `checkedEphemeralCreate`? The logic in `controllerNodeExistsHandler ` is essentially the same as `checkedEphmeralCreate` when the node already exists, except that `controllerNodeExistsHandler` will read `/controller_epoch` to get back the epoch zkVersion when the owner of `/controller` is the current broker."
214780256,5101,lindong28,2018-09-04T03:43:44Z,"My bad. I missed the fact that the controller znode was created using `KafkaZkClient.checkedEphemeralCreate()` which has the logic similar to what you are doing here.

It seems that the most important logic in the `KafkaZkClient.checkedEphemeralCreate` is to translate `Code.NODEEXISTS` to `Code.OK` for the znode creation operation if `getDataResponse.stat.getEphemeralOwner == zooKeeperClient.sessionId`. This logic was added in https://github.com/apache/kafka/pull/3765 by Onur. My understanding is that, in case of connection issue between broker and zookeeper, it is possible for controller znode to be successfully created and yet the return code is `Code.NODEEXISTS`. `KafkaZkClient.checkedEphemeralCreate` will handle this scenario properly. It will be good for @onurkaraman to clarify whether this understanding is correct so that we can decide whether we should keep this logic.

Here is another question. With the current patch, if the controller znode creation has failed due to znode exists exception and then broker find that `getDataResponse.stat.getEphemeralOwner == zooKeeperClient.sessionId`, it seems `registerControllerAndIncrementControllerEpoch()` can return `(newControllerEpoch, stat.getVersion)` if `epoch == newControllerEpoch`. But is controller epoch incremented in this case? If not, then it seems something is wrong?


"
214995831,5101,hzxa21,2018-09-04T17:11:11Z,"I re-think about your previous suggestion for checking the payload of `/controller` only and I think it will work. I will check with Onur offline to understand more about  `checkedEphemeralCreate` and confirm.

In terms of your second concern, if we already see `getDataResponse.stat.getEphemeralOwner == zooKeeperClient.sessionId`, that means `/controller` has been created successfully. Since the only code path to create `/controller` is within a zookeeper transaction along with the `/controller_epoch` update, we can infer that the controller epoch must get incremented in this case."
215069911,5101,hzxa21,2018-09-04T21:17:26Z,"Discussed with Onur offline, the purpose of `getAfterNodeExists` in `CheckedEphemeral` is indeed used to handle the case when zk connection loss happens. After digging around both zookeeper and kafka codes, we think it is safe to remove the extra complexity for `controllerNodeExistsHandler` in this PR when we make `/controller` creation and `/controller_epoch` update atomic.

So the logic will be:
1). Try to create `/controller_epoch` if not exists
2). Read `/controller_epoch` from zk
3). Atomically create `/controller` and update `/controller_epoch`
4). If 3) throws NodeExistsException, read `/controller` and if controller id in zk equals the current broker id and if controller epoch in zk equals the expected epoch, successfully finish controller election; Otherwise, throw ControllerMovedException."
215105046,5101,junrao,2018-09-05T00:26:22Z,"Since we are doing a conditional setData in line 117, we don't need the check operation here."
215107229,5101,junrao,2018-09-05T00:41:49Z,"If the client loses a connection to a ZK server in the middle of an operation, the client will get a ConnectionLossException. Normally, we handle this by retrying through retryRequestsUntilConnected(). So, we probably need to create a similar routine to retry on ConnectionLossException when doing transaction.commit() too.

The controller path could have been created successfully when ConnectionLossException was incurred. A retry could result in either NodeExistsException or BadVersionException. You handled the former properly in the code below. We will need to do the same thing for the latter."
215109171,5101,junrao,2018-09-05T00:56:27Z,"For any other types of exceptions, we want to just propagate the KeeperException to the caller."
215109447,5101,junrao,2018-09-05T00:58:36Z,"In the common case, the controller epoch path already exists. So, perhaps it would be better to always do getControllerEpoch first and then try maybeCreateControllerEpochZNode if we hit a NoNodeException."
215109576,5101,junrao,2018-09-05T00:59:41Z,The info level will be too verbose if we call maybeCreateControllerEpochZNode() on every controller election.
215112397,5101,junrao,2018-09-05T01:22:17Z,"""before the pre-defined logic is triggered and before it processes controller change."" It seems that we just need one of the two before?"
215135718,5101,junrao,2018-09-05T04:55:18Z,"Hmm, I am not sure this is safe. The controller path could have been deleted and grabbed by another broker in the window between line 123 and here. Then, we would have grabbed the wrong controller epoch."
215176421,5101,hzxa21,2018-09-05T08:22:50Z,"You are right. But in line 133 we will check the epoch value. If it is different from what we expected, we will throw `ControllerMovedException`. In the case of `/controller` gets deleted between line 123 and 127, and another broker becomes the controller, the controller epoch will increment accordingly, causing line 133 to fail."
215359852,5101,junrao,2018-09-05T17:28:33Z,Ah. Ok. That's fine then.
215360130,5101,junrao,2018-09-05T17:29:25Z,"Should we explicitly call return here? Otherwise, it seems that we are always throwing ControllerMovedException."
215418284,5101,hzxa21,2018-09-05T20:40:41Z,That's right. Fixed.
215418324,5101,hzxa21,2018-09-05T20:40:47Z,Done.
215418651,5101,hzxa21,2018-09-05T20:41:51Z,Here we don't catch other types of exceptions so the KeeperException is already propogated to the caller.
215420284,5101,hzxa21,2018-09-05T20:47:19Z,Done.
215420905,5101,hzxa21,2018-09-05T20:49:25Z,"This log will only get print when `/controller_epoch` is absent, which is typically when the cluster gets initialized, not on every controller election."
215420928,5101,hzxa21,2018-09-05T20:49:31Z,Fixed.
215420986,5101,hzxa21,2018-09-05T20:49:43Z,Ah... My bad. Fixed.
215444417,5101,junrao,2018-09-05T22:25:17Z,"On ConnectionLossException, it's not efficient to blindly retry immediately. Instead, it's better to wait until the ZK connection is ready before retry. You can check how this is done in retryRequestsUntilConnected()."
215444912,5101,junrao,2018-09-05T22:27:36Z,"It is possible that the controller_epoch path is created by another broker between getControllerEpoch() and createControllerEpochZNode(). In this case, we want to get the controller epoch again, instead of throwing ControllerMovedException."
215447046,5101,junrao,2018-09-05T22:37:52Z,It seems that every usage of KafkaController.InitialControllerEpoch and KafkaController.InitialControllerEpochZkVersion as 0 requires subtraction by 1. Could we just define them as 0?
215448079,5101,junrao,2018-09-05T22:43:14Z,"According to ZK doc, multi propagates the error from one of the operations. The BadVersionException could be the result of a retry after ConnectionLossException. So, it seems that we need to handle it in the same way as NodeExistsException."
215462227,5101,hzxa21,2018-09-06T00:10:45Z,Got it. Thanks for pointing this out.
215462635,5101,hzxa21,2018-09-06T00:13:29Z,"If the controller_epoch path is created by another broker between getControllerEpoch() and createControllerEpochZNode(), I was thinking whether we can infer that other broker wins in this round of controller election even if it hasn't created the controller znode. 

After a second thought, I think we should follow what you suggested for extra safety because if the broker fails to talk to zk for some reason, the cluster will get into a no-controller state."
215466660,5101,hzxa21,2018-09-06T00:45:31Z,"Correct me if I am wrong, I think there are two cases when we see BadVersionException here:
1. Another round of controller election kicks in and the controller does switch. It is safe to throw `ControllerMovedException` in this case.
2. The current broker loss zk connection after zk successfully finishes the transaction, **but the controller znode is gone before the next retry**. In this case, another round of controller election will be triggered by zk watcher `handleDeleted`. So I think it is also safe to throw `ControllerMovedException` here."
215467856,5101,hzxa21,2018-09-06T00:55:55Z,Yes. Done.
215467866,5101,hzxa21,2018-09-06T00:56:00Z,Fixed.
215467880,5101,hzxa21,2018-09-06T00:56:06Z,Done.
215561349,5101,omkreddy,2018-09-06T09:42:13Z,nit: missing expectedControllerEpochZkVersion params in method comments
215570923,5101,omkreddy,2018-09-06T10:12:30Z,Can we use InitialControllerEpochZkVersion constant in place of zero?
215576221,5101,omkreddy,2018-09-06T10:31:49Z,looks like this line is not required.
215711690,5101,hzxa21,2018-09-06T17:28:21Z,Added.
215711716,5101,hzxa21,2018-09-06T17:28:29Z,Sure. Done.
215711801,5101,hzxa21,2018-09-06T17:28:43Z,Thanks for pointing out. Removed.
215738649,5101,junrao,2018-09-06T18:52:06Z,Perhaps it's better to name this maybeCreateControllerEpochZNode?
215740626,5101,junrao,2018-09-06T18:58:19Z,"I was thinking about case 2, but with the controller path still there. The ZK multi api doesn't say that it will run the operations in a multi request in any particular order. So, during the retry on a ConnectionLossException, it may be possible that the conditional update of the controller epoch path is executed first and a BadVersionException is thrown?"
215769317,5101,hzxa21,2018-09-06T20:38:10Z,"Thanks for the prompt reply!
I actually checked both client and server side codes of zookeeper, the implementation honors the order and the thrown exception will correspond to the first error it sees. But since the muli api doesn't explicitly say that it is the case or it will maintain this guarantee in the future, I agree to also handle BadVersion the same way as NodeExists for safety given that the performance overhead is little."
215773516,5101,hzxa21,2018-09-06T20:49:54Z,Done.
215773534,5101,hzxa21,2018-09-06T20:49:58Z,Done.
215775823,5101,junrao,2018-09-06T20:57:12Z,Could this be private?
215779888,5101,hzxa21,2018-09-06T21:11:24Z,Done.
215847375,5101,lindong28,2018-09-07T05:09:12Z,"I have two questions here:

1) When would we enter a scenario that `controllerId == curControllerId` and `epoch != newControllerEpoch)`?

2) Currently when this happens, `checkControllerAndEpoch` will throw `ControllerMovedException()`, which is caught in `KafkaController.elect()` and trigger `maybeResign()`. However `maybeResign()` will do nothing because `controllerId == curControllerId` and this broker is considered to be the active controller. In this case no other broker will be controller. And the current broker will not function properly as controller because it has not executed `onControllerFailover()`.

So maybe we should throw `IllegalStateException` here if `controllerId == curControllerId` and `epoch != newControllerEpoch)`?"
215847674,5101,lindong28,2018-09-07T05:12:27Z,"In `maybeCreateControllerEpochZNode()`, we throw `IllegalStateException(...)` if we first find controller epoch znode exists and then find it disappeared. Following the same logic, it is probably consistent and reasonable to throw `IllegalStateException(...)` if `checkControllerAndEpoch(...)` can not read controller epoch, right?"
215849517,5101,lindong28,2018-09-07T05:28:04Z,"If `registerControllerAndIncrementControllerEpoch()` has successfully written the broker id to controller znode but then an IllegalStateException is thrown (e.g. in the case `controllerId == curControllerId` and `epoch != newControllerEpoch` described in the other comment), an IllegalStateException will be thrown which is caught in `elect()` and `triggerControllerMove()` will be executed. However, since the `activeControllerId` has not been updated, `isActive()` is evaluated to false and `triggerControllerMove()` will do nothing.

Maybe we should first do `activeControllerId = zkClient.getControllerId.getOrElse(-1)`  in `triggerControllerMove()`.

Also, to be consistent with most other usage of `isActive()`, can we do something like the code below instead of using if/else?

```
    if (!isActive) {
      warn(""Controller has already moved when trying to trigger controller movement"")
      return
    }

```
"
215849923,5101,lindong28,2018-09-07T05:31:43Z,nits: can we use `controllerContext.epochZkVersion` instead of using `expectedControllerEpochZkVersion` to be consistent with other usage of `controllerContext.epochZkVersion` in this patch?
215851146,5101,lindong28,2018-09-07T05:41:07Z,nits: `Successfully create` => `Successfully created`
216039463,5101,hzxa21,2018-09-07T17:54:48Z,"1. Because we first get /controller and then get /controller_epoch (rather than do it atomically), it is possible that after we see `controllerId == curControllerId` and before we get /controller_epoch, another round of controller election is triggered. In this case, we will see `epoch != newControllerEpoch`.
2. When we see `epoch != newControllerEpoch`, `controllerId == curControllerId` does not hold because another broker must become the controller. In this case, `maybeResign` will work fine."
216039681,5101,hzxa21,2018-09-07T17:55:27Z,Make sense to me. Will fix it.
216044300,5101,hzxa21,2018-09-07T18:11:45Z,"In `onControllerResignation`, `controllerContext` will be reset. So here we need to keep the value before `onControllerResignation` and reuse it in deletion."
216048155,5101,hzxa21,2018-09-07T18:25:39Z,Yes. Fixed.
216048195,5101,hzxa21,2018-09-07T18:25:45Z,Done.
216048226,5101,hzxa21,2018-09-07T18:25:51Z,Fixed.
216049295,5101,lindong28,2018-09-07T18:30:20Z,Thanks for the explanation. This makes sense.
216049364,5101,lindong28,2018-09-07T18:30:36Z,This makes sense.
216108832,5101,junrao,2018-09-07T23:23:46Z,"@hzxa21 : Actually, I am wondering if it's simpler to replace lines 116-124 with a check that the ephemeral owner of the controller path equals to the ZK session id (like we did before in checkedEphemeralCreate()). The controller path or the controller epoch path could change after that check. But that's fine and will be handled by the next ZK event on the controller path change."
216150289,5101,hzxa21,2018-09-09T07:49:41Z,It is safe to replace the /controller payload check with session id check but we still need to read /controller_epoch to get back the corresponding zk version if we loss connection when doing the zk transaction.
216521044,5101,junrao,2018-09-11T01:18:07Z,"Hmm, to me, if the ephemeral owner of the controller path equals to the ZK session id, it means that at that particular time, the controller path and the controller epoch path are created by the current ZK session, and therefore the controller epoch used for creation should be valid. This seems to be equivalent as checking the value of the controller path and the controller epoch value. In both cases, after the check, the controller could change again. However, that will be handled by the ZK watcher event."
216524610,5101,lindong28,2018-09-11T01:45:34Z,"My understanding is that the approach using ephemeral owner has the same performance and correctness guarantee as the current approach which uses epoch from the znode data. 

The approach using ephemeral owner is probably more intuitive/readable because it exactly  handles the root cause of `NodeExistsException | _: BadVersionException`, i.e. `checkControllerAndEpoch()` should effectively translate NodeExistsException/BadVersionException to `Code.OK` if and only if `ephemeral owner of the controller path equals to the ZK session id`. And it is also more consistent with the existing logic in `checkedEphemeralCreate()`.

@hzxa21 If it sounds reasonable, maybe we can have a minor followup patch (without requiring a JIRA ticket) to improve it."
216723540,5101,junrao,2018-09-11T15:57:24Z,"I was thinking that the ephemeral owner approach will be cheaper since we only need to read the controller path, not both controller path and the controller epoch path."
216757532,5101,hzxa21,2018-09-11T17:41:08Z,"@junrao From the correctness and performance point of view, checking `/controller` payload and checking `/controller` ephemeral owner is the same. The question is whether we need to read `/controller_epoch`. The reason why I do it is because if `NodeExistsException| BadVersionException` happens, we no longer have the `Stat` (new zkVersion) of `/controller_epoch` even though the `/controller_epoch` update succeeds in zookeeper server. We can avoid the extra read on `/controller_epoch` if we can assume that zkVersion is always incremented by one. Since `/controller_epoch` zkVersion is critical for us after this patch and zookeeper doc does not explicitly say that this assumption holds, I think it is safer to do one extra read during controller election."
217395000,5101,junrao,2018-09-13T14:01:19Z,@hzxa21 : That's a great point. Thanks. Then we can just keep the code as it is.
263337259,6295,enothereska,2019-03-07T11:13:10Z,Not clear from the KIP why you need to keep track of both downstream and upstream offset.
263338227,6295,enothereska,2019-03-07T11:16:00Z,Looks like it's missing the methods like shouldCheckPointTopic. I'm assuming that is because this is still WiP.
263340233,6295,enothereska,2019-03-07T11:22:15Z,The methods here are slightly different from what was described in the KIP but you're using the helper class to create AdminClient so I'm on with it (as long as we update the KIP at some point).
263461244,6295,ryannedolan,2019-03-07T16:27:17Z,"The discussion so far has been about emitting upstream offsets and then translating them within RemoteClusterUtils. However, I've found it's just as easy for the checkpoints to be translated already, which drastically simplifies RemoteClusterUtils.

I'm not certain both upstream and downstream offsets are really necessary here, but it's nice as a sanity check when tailing the checkpoint stream, at least."
263462174,6295,ryannedolan,2019-03-07T16:29:15Z,I'll update the KIP and call out the changes to the discuss thread prior to marking this ready-for-review.
263469394,6295,ryannedolan,2019-03-07T16:44:58Z,"I plan to remove those methods from the KIP, for a few reasons:

1) the same logic is encoded in the config, for the most part. e.g. shouldCheckpointTopic is just combining the topic and group whitelists. If we already have those properties, it's best not to provide a second mechanism to redefine the behavior here. I think regexes are sufficiently powerful.

2) I want to enable using the same replication policy for both the connectors and the clients. The clients don't care about most of the methods in the KIP, so it's best not to make a client define them.

3) I want replication policies to be pretty much static across an entire organization, not per-cluster. The organization decides what remote topics look like, and then all connectors and clients know how to interpret them. So anything related to specific topics or groups doesn't fit that goal.

It's possible that ReplicationPolicy is no longer a good name for this, but I think it works."
266605739,6295,williamhammond,2019-03-18T19:29:17Z,Should non-replicated topics  be filtered before being passed in potentially should this be an exception? In `MirrorClient#upstreamClusters` do we need to filter nulls similar to how `MirrorClient#replicationHops` needs to filter -1? 
266610210,6295,ryannedolan,2019-03-18T19:41:47Z,"Thanks for the suggestion. I think it's reasonable to throw an exception here and in replicationHops(), but I'll need to add an additional method like hasSource() or something. Seems like a good trade to reduce magic numbers and nulls."
269820527,6295,williamhammond,2019-03-28T00:27:21Z,Shouldn't this be false? 
269831729,6295,ryannedolan,2019-03-28T01:45:38Z,thanks :)
271842542,6295,ryannedolan,2019-04-03T17:05:46Z,"Update: I've broken the missing methods out into TopicFilter, GroupFilter, ConfigPropertyFilter, instead of having them all in ReplicationPolicy. lmk what you think."
272189918,6295,viktorsomogyi,2019-04-04T13:53:44Z,nit: I think it'd be better to keep this and the zkclient on info level as they might be useful in a troubleshooting scenario.
272195234,6295,viktorsomogyi,2019-04-04T14:05:26Z,"Have you considered using the protocol generator framework that is available for clients or would it make this more complicated than necessary?
As I see you're simply using these messages as payload and they're not really protocol messages but we might gain something as they generate hashcode and equals methods.
Also, why are these (Checkpoint, heartbeat) not protocol messages?"
272263389,6295,ryannedolan,2019-04-04T16:28:32Z,"will do, thanks"
272269172,6295,ryannedolan,2019-04-04T16:44:13Z,"As you say, these aren't really protocol messages, as they are not requests or responses, so I don't think the generator stuff would be a good fit here. Maybe there are parts of it I could use.

I suppose we could use JSON here as well. These are simple, unstructured records, so there isn't much to be gained from encoding in JSON, but it would be nice to get rid of some of this code."
272558159,6295,kujon,2019-04-05T12:09:56Z,I'm wondering: what is the advantage of using `-1` over say `null` to represent no value?
273276967,6295,ryannedolan,2019-04-08T23:31:34Z,That would work too. Whatever is more conventional in Kafka is fine with me.
274046687,6295,ryannedolan,2019-04-10T16:18:38Z,Let's change to latency.
274671893,6295,halorgium,2019-04-11T21:25:14Z,"this needs the `SECURITY_PROTOCOL` added. 

```
        // security support
        .define(CommonClientConfigs.SECURITY_PROTOCOL_CONFIG,
                Type.STRING,
                CommonClientConfigs.DEFAULT_SECURITY_PROTOCOL,
                Importance.MEDIUM,
                CommonClientConfigs.SECURITY_PROTOCOL_DOC)
```"
281541554,6295,viktorsomogyi,2019-05-07T09:14:22Z,As I understand this is a standard property of the connector config but I think it could be omitted in this case as we're always using MirrorSourceConnector. Perhaps we can dynamically add this config in MirrorMaker on creation time. Or is it there because of the MirrorSinkConnector you'll create according to the KIP?
281572293,6295,viktorsomogyi,2019-05-07T10:47:32Z,"I think it would be safer to use the `await(long, TimeUnit)` method with a reasonable value (maybe taking it from a config) with both this one and with `startLatch` so we won't wait if the other thread died and never gonna count down."
281855279,6295,ryannedolan,2019-05-07T22:35:30Z,@halorgium thanks for catching! Fixed.
282122493,6295,ryannedolan,2019-05-08T15:33:19Z,"Yes, MirrorMaker fills this in for you normally. This config file is provided just for running MirrorSourceConnector in ""standalone mode"" as follows:

   ./bin/connect-standalone.sh config/connect-standalone.properties config/connect-mirror-source.properties

i.e. without the top-level MM2 driver doing the work for you. Organizations that already have a Connect-as-a-Service cluster will find this useful as well, as they may wish to leverage their existing cluster and just configure it to run the MM2 connectors."
282141748,6295,ryannedolan,2019-05-08T16:16:57Z,"This latch is being used as a signal that stop() has been called, so we can't timeout here. But i've added a timeout to the shutdown hook and added a `finally` to ensure that stop() is called."
282144470,6295,ryannedolan,2019-05-08T16:23:54Z,"I was able to drop this file entirely, and just use the existing connect-log4j.properties file. It's a little verbose that way, but no more so than connect-distributed.sh."
284428968,6295,viktorsomogyi,2019-05-15T20:06:42Z,nit: Exception is not thrown by anyone
284431092,6295,viktorsomogyi,2019-05-15T20:12:24Z,"There is no clusters config, I think you should add `clusters=upstream`. I get an NPE without it:
```
[2019-05-15 22:03:20,645] ERROR Stopping due to error (org.apache.kafka.connect.mirror.MirrorMaker:238)
java.lang.NullPointerException
	at org.apache.kafka.connect.mirror.MirrorMakerConfig.clusterPairs(MirrorMakerConfig.java:110)
	at org.apache.kafka.connect.mirror.MirrorMaker.<init>(MirrorMaker.java:78)
	at org.apache.kafka.connect.mirror.MirrorMaker.<init>(MirrorMaker.java:83)
	at org.apache.kafka.connect.mirror.MirrorMaker.main(MirrorMaker.java:225)
```"
284432272,6295,viktorsomogyi,2019-05-15T20:15:29Z,"The adminClient requires a bootstrap.server property. Would it make sense to pass either the source or the target's bootstrap.server property?
```
[2019-05-15 22:12:37,476] INFO Creating Kafka admin client (org.apache.kafka.connect.util.ConnectUtils:43)
[2019-05-15 22:12:37,479] ERROR Stopping due to error (org.apache.kafka.connect.mirror.MirrorMaker:238)
org.apache.kafka.common.config.ConfigException: Missing required configuration ""bootstrap.servers"" which has no default value.
	at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:474)
	at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:464)
	at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:62)
	at org.apache.kafka.clients.admin.AdminClientConfig.<init>(AdminClientConfig.java:196)
	at org.apache.kafka.clients.admin.AdminClient.create(AdminClient.java:65)
	at org.apache.kafka.connect.util.ConnectUtils.lookupKafkaClusterId(ConnectUtils.java:44)
	at org.apache.kafka.connect.mirror.MirrorMaker.addHerder(MirrorMaker.java:179)
	at org.apache.kafka.connect.mirror.MirrorMaker.lambda$new$0(MirrorMaker.java:78)
	at java.base/java.util.ArrayList.forEach(ArrayList.java:1540)
	at org.apache.kafka.connect.mirror.MirrorMaker.<init>(MirrorMaker.java:78)
	at org.apache.kafka.connect.mirror.MirrorMaker.<init>(MirrorMaker.java:83)
	at org.apache.kafka.connect.mirror.MirrorMaker.main(MirrorMaker.java:225)
```"
284433973,6295,viktorsomogyi,2019-05-15T20:20:06Z,Also on a second note I think it'd be nice to give the users some meaningful error here.
284434170,6295,viktorsomogyi,2019-05-15T20:20:36Z,what should be the real version?
284443402,6295,viktorsomogyi,2019-05-15T20:45:39Z,"I would consider using `Optional<String>` here and probably in other places in this interface too. These are interface methods, used in a bunch of places and I think it's better to enforce null checks in a safe way."
284444200,6295,viktorsomogyi,2019-05-15T20:47:42Z,`@InterfaceStability.Evolving` (and generally to all interfaces)
284446688,6295,viktorsomogyi,2019-05-15T20:54:18Z,Distinct is not needed because of it'll be collected into a set.
284446809,6295,viktorsomogyi,2019-05-15T20:54:38Z,Not needed either.
284447065,6295,viktorsomogyi,2019-05-15T20:55:22Z,This method doesn't seem to be used. What's the purpose?
284447829,6295,viktorsomogyi,2019-05-15T20:57:31Z,nit: these are not thrown anywhere
284448042,6295,viktorsomogyi,2019-05-15T20:58:06Z,"nit: could be made private, or is there a reason for this to be protected?"
284448576,6295,viktorsomogyi,2019-05-15T20:59:36Z,This method doesn't seem to be used.
284450836,6295,viktorsomogyi,2019-05-15T21:05:45Z,nit: This doesn't seem to be used nor contains the required information.
284453686,6295,viktorsomogyi,2019-05-15T21:13:32Z,"Usually the kafka convention of internal topic notation is double underscore, such as `__topic-name`. I wonder if we'd should to apply this here too. Also it seems to me that there internal topics are `mm2-offsets...`, `mm2-status...`, `mm2-offset-syncs` and `mm2-configs...` for the source and target clusters so we might be able just dynamically populate the topic blacklist for these and that way we'd probably leave the `.internal` notation.
Just putting this out for conversation."
284455448,6295,viktorsomogyi,2019-05-15T21:18:55Z,"Why do we need to throw `ExecutionException, TimeoutException` in these methods?"
284458764,6295,viktorsomogyi,2019-05-15T21:28:42Z,Would it make sense to be the implementation of `org.apache.kafka.common.utils.Scheduler`?
284459118,6295,viktorsomogyi,2019-05-15T21:29:39Z,nit: `InterruptedException` is not thrown.
284459418,6295,viktorsomogyi,2019-05-15T21:30:27Z,This doesn't seem to be used.
284460793,6295,viktorsomogyi,2019-05-15T21:34:18Z,nit: This isn't actually used.
284461711,6295,viktorsomogyi,2019-05-15T21:37:08Z,what should be the real version?
284462552,6295,viktorsomogyi,2019-05-15T21:39:45Z,nit: these can be private
284462956,6295,viktorsomogyi,2019-05-15T21:41:01Z,How much effort would be to expose this property? As far as I can tell all we need is to provide a config and then we'd be good to go.
284463213,6295,viktorsomogyi,2019-05-15T21:41:53Z,nit: what is the real version? :)
284464247,6295,viktorsomogyi,2019-05-15T21:45:17Z,As I see this is assigned only in `start()` but I think just for the sake of clean code we should use a `final Object` to lock.
284464701,6295,viktorsomogyi,2019-05-15T21:46:34Z,nit: this is not used
284464776,6295,viktorsomogyi,2019-05-15T21:46:47Z,nit: this is not used
284523993,6295,kamalcph,2019-05-16T03:04:56Z,nit: unused variable.
284524085,6295,kamalcph,2019-05-16T03:05:38Z,TOPIC_FILTER_CLASS_DOC -> GROUP_FILTER_CLASS_DOC
284524187,6295,kamalcph,2019-05-16T03:06:13Z,unused variable.
284524413,6295,kamalcph,2019-05-16T03:07:52Z,Whether to include `__transaction_state` internal topic here?
284524591,6295,kamalcph,2019-05-16T03:09:16Z,Could you change the method name in symmetry to the topic name? (offsetSyncsTopic())
284524745,6295,kamalcph,2019-05-16T03:10:37Z,Give a name to this scheduler to track it in ThreadDump.
284524882,6295,kamalcph,2019-05-16T03:11:36Z,Give a name to this thread.
284525104,6295,kamalcph,2019-05-16T03:13:08Z,"Once a lock is taken, no other operation should be done outside the try-catch block. Call the `Consumer.close()` either inside the try-catch or before taking the lock."
284525159,6295,kamalcph,2019-05-16T03:13:30Z,unlock the taken lock.
284525244,6295,kamalcph,2019-05-16T03:14:10Z,IE is not thrown. Could you please remove the `throws IE`?
284526932,6295,kamalcph,2019-05-16T03:26:01Z,Consider taking the `lock` before the try-catch block as it's best practice.
284611735,6295,kamalcph,2019-05-16T09:13:17Z,"You may have to update the description of the max, min and avg for record_age, replication_latency and checkpoint_latency metric name templates.

(eg) The **maximum** age of incoming ..."
284612564,6295,kamalcph,2019-05-16T09:15:13Z,"`pause()` and `resume()` methods are unused. And, you can merge both these methods by taking action as a parameter."
284613640,6295,kamalcph,2019-05-16T09:17:47Z,nit: Pending TODO.
284614159,6295,kamalcph,2019-05-16T09:18:56Z,unused variable.
284614303,6295,kamalcph,2019-05-16T09:19:17Z,unused variable.
284614370,6295,kamalcph,2019-05-16T09:19:26Z,unused variable.
284615520,6295,kamalcph,2019-05-16T09:22:05Z,It's better to unlock the taken lock.
284648386,6295,arunmathew88,2019-05-16T10:48:23Z,"From my experience with large kafka clusters, one node being down for maintenance or so is very common, so for topics with more than one replica in source topic, we should have at least 2 replicas in destination, for the data to be realistically available in failover scenarios.  Just my thought."
284898791,6295,ryannedolan,2019-05-16T21:07:51Z,"This config is for the MirrorSourceConnector alone, not a top-level mm2.properties file. The properties required in either case are distinct. I can see this is a source of confusion, so I'll add a comment here. I think maybe we need an example mm2.properties file as well."
284899900,6295,ryannedolan,2019-05-16T21:11:03Z,"You're using the wrong config file (see above). You need something like:

clusters = upstream, downstream
upstream.bootstrap.servers = ...
downstream.bootstrap.servers = ...

Then, the MirrorMaker driver sets up a bunch of connectors with the required properties, which will look like the connect-mirror-source.properties here."
284900808,6295,ryannedolan,2019-05-16T21:13:47Z,"I think Optional is supposed to be used only within the context of the Java 8 streams API. It's not unusual to return nulls in Java, nor in this code base. (Though coming from Scala this hurts a bit )"
284945642,6295,ryannedolan,2019-05-17T00:41:48Z,"Will fix, thanks."
284947928,6295,ryannedolan,2019-05-17T00:59:12Z,"Let's add min.insync.replicas here too, as this is likely to cause problems. h/t @arunmathew88 "
284948127,6295,ryannedolan,2019-05-17T01:00:47Z,"yeah let's blacklist anything with `__` prefix, thanks."
284948535,6295,ryannedolan,2019-05-17T01:04:18Z,"Yeah, Herder requires an advertisedUrl, tho we don't use it. I'll change this to `NOT USED` to avoid confusion."
284948676,6295,ryannedolan,2019-05-17T01:05:40Z,"We can drop these, thanks."
284950066,6295,ryannedolan,2019-05-17T01:16:28Z,"I've left this as-is because it's convenient to test whether a topic has a source with `topicSource(topic) != null`, rather than to define and use a separate method like `hasSource()` or something."
284950514,6295,ryannedolan,2019-05-17T01:19:51Z,"`toSet()` will throw an exception if it finds duplicates. This seems unlikely unless someone defines a really strange `ReplicationPolicy`, but I think it's worthwhile to avoid the exception just in case."
284952693,6295,ryannedolan,2019-05-17T01:35:53Z,"This is just a convenience method, but I believe it is worthwhile. Otherwise it is a bit cumbersome to recreate externally:
```
ReplicationPolicy policy = client.replicationPolicy();
Set<String> topicsFromSource = client.remoteTopics().stream()
    .filter(x -> source.equals(replicationPolicy.topicSource(x))
    .distinct()
    .collect(Collectors.toSet());
``` "
284954715,6295,ryannedolan,2019-05-17T01:51:24Z,This is useful for external tooling.
284957873,6295,ryannedolan,2019-05-17T02:16:00Z,"I think the `__` should be limited to topics internal to Kafka proper. The topics `mm2-offsets...`, `mm2-status...`, and `mm2-config...` are based on Connect's defaults, `connect-offsets`, `connect-config` etc, which are not internal per se. I've added .internal to these so that you don't need to configure MM2 to blacklist itself, thought it's an interesting idea to just blacklist them automatically."
285062422,6295,enothereska,2019-05-17T10:08:44Z,The readme could be re-worded a bit to start with a quickstart simplest case and then build from there to increasingly more complex cases. This could be done at a later pass too.
285063053,6295,enothereska,2019-05-17T10:10:54Z,Could we add a sentence on what the implications of this are? It's not immediately clear what should happen if MM runs with same source and target...perhaps nothing at all?
285063661,6295,enothereska,2019-05-17T10:12:45Z,Should we check in such a sample file in the config folder?
285064156,6295,enothereska,2019-05-17T10:14:12Z,Nit: mm2.properties or mm2.config?
285480888,6295,viktorsomogyi,2019-05-20T08:40:35Z,"Yea that would be helpful, people could use it as a template."
285488395,6295,viktorsomogyi,2019-05-20T08:57:43Z,"Well, Oracle's own example uses Optional in the context of return values so I think it'd be idiomatic to do so: https://www.oracle.com/technetwork/articles/java/java8-optional-2175753.html
Generally I agree that in this codebase we often return nulls but if you look at for instance the TransactionManager, it uses Optionals this way, so I think it's definitely encouraged here too :)
https://github.com/apache/kafka/blob/fc616cb521c3f7e377b8b0ac65a3a83101156951/clients/src/main/java/org/apache/kafka/clients/producer/internals/TransactionManager.java#L106"
285507212,6295,viktorsomogyi,2019-05-20T09:42:38Z,On this note we probably want to document that it doesn't work for transactional topics (also as this was one of the first questions on the summit :) )
285510919,6295,viktorsomogyi,2019-05-20T09:51:48Z,I think it would make sense to pass `true` (or use the single param super constructor). It helps debugging the config.
285513326,6295,viktorsomogyi,2019-05-20T09:57:38Z,`Type.LIST` would be better maybe?
285518598,6295,viktorsomogyi,2019-05-20T10:10:50Z,"I'd like to make a general point but I didn't know any specific places so I'd put it here.
It'd be helpful from the usability perspective to validate configs. I've tried to specify this config:
```
clusters=upstream;downstream
upstream.bootstrap.servers=whatever.com:9092
downstream.bootstrap.servers=somethingelse.net:9092
upstream->downstream.enabled=true
upstream->downstream.topics=.*
```
You can notice that I put a `;` in the cluster separator config but nothing thrown an exception saying that I haven't specified the correct clusters (MM2 just started up and shut down)."
285562725,6295,viktorsomogyi,2019-05-20T12:21:22Z,We might pass `true` if that makes sense.
286460327,6295,viktorsomogyi,2019-05-22T12:22:07Z,nit: maybeSendOffsetSync?
286465012,6295,viktorsomogyi,2019-05-22T12:34:12Z,"I was wondering if 
- would it make sense to make this configurable?
- wouldn't this be mostly the same as configuring the `max.in.flight.requests.per.connection` property of the `offsetProducer` and have you thought about using just that config?"
286471163,6295,viktorsomogyi,2019-05-22T12:48:53Z,I think it would be better to use some timeout here as it spams the log with the below warn message.
286475429,6295,viktorsomogyi,2019-05-22T12:58:11Z,As far as I understand offsets would be synced eventually so I think we might want to reconsider if this is a warn level log message (maybe even lower it to debug?).
286585537,6295,jeremy-l-ford,2019-05-22T16:41:55Z,Should the returned classloader be restored at the end of the method?
286638392,6295,harshach,2019-05-22T18:58:03Z,"There seems to be null some places and other places -1. If possible can we standardize or leave a comment. 
"
288238131,6295,jeremy-l-ford,2019-05-28T18:17:28Z,"I have been testing this branch and used the connector.class option noted above in my configuration.  I noticed that records were being copied 3X instead of the expected 1.  Debugging through the source, apparently that configuration will override the connector name that is setup during MirrorMakerConfig.connectorBaseConfig.  Since MIrrorMaker attempts to setup the Source, Health, and Checkpoint connectors, I actually ended up with 3 source connectors."
288280080,6295,ryannedolan,2019-05-28T20:07:25Z,"@jeremy-l-ford Funny problem! I think this, along with others' experiences here, indicates I need to remove this sample configuration altogether, and just provide a top-level configuration file like in the ""quick start"" above. The connector configuration is just confusing. Moreover, it doesn't really demonstrate anything beyond the existing generic connect-standalone.properties file.

I'll remove this file, thanks."
289196474,6295,OneCricketeer,2019-05-30T22:26:56Z,"Related to 

> doesn't work for transactional topics

I assume with the addition of record header copying, it will also only work for when `log.message.format.version` >= `0.11.0` ? 

At least, we've noticed that when clients try to use headers (or transactional producers) after broker upgrades, but before log format changes, then they are usually throwing `UnknownServerException`. 

---

One workaround I did (for an SMT) was to conditionally copy the headers to the transformed record. e.g. https://github.com/cricket007/schema-registry-transfer-smt/blob/master/src/main/java/cricket/jmoore/kafka/connect/transforms/SchemaRegistryTransfer.java#L157-L168"
289495296,6295,jeremy-l-ford,2019-05-31T18:03:55Z,try/finally close the consumer
290088869,6295,ryannedolan,2019-06-04T00:36:16Z,Let's just remove the log message altogether.
290090082,6295,ryannedolan,2019-06-04T00:43:44Z,"I'm not certain, but the other drivers (connect-standalone, connect-distributed) do it this way. I'll just cargo-cult here."
290369229,6295,ryannedolan,2019-06-04T15:50:36Z,"@cricket007 My intention is to support 0.11.0 onwards, at least for now. We can revisit making headers optional to support older versions, but I think this probably isn't the only thing that would break before 0.11.0."
290371819,6295,ryannedolan,2019-06-04T15:55:40Z,I dropped this file to avoid confusion.
290372405,6295,ryannedolan,2019-06-04T15:56:58Z,"This method doesn't contain a conditional branch (the existing maybeSendOffsetSync does), so I'll leave this as is."
290372816,6295,ryannedolan,2019-06-04T15:57:53Z,"I dropped the logging entirely. There is no harm if tryAcquire fails, so better to not spam the log as you say."
290407619,6295,ryannedolan,2019-06-04T17:22:51Z,"Fixed. This was b/c MM was finding only a single cluster (""upstream;downstream"") and had no source->target pairs to replicated. I've added an exception ""No source->target replication flows"" in this case, which should at least point you in the right direction."
291329247,6295,vpernin,2019-06-06T19:06:16Z,"Is this intended to not set the interrupted flag again ?
Same question elsewhere like in MirrorSourceTask.cleanup()."
291333700,6295,vpernin,2019-06-06T19:19:28Z,Is there a risk the semaphore not to be released if the async send fails internal before invoking this callback ?
291354482,6295,vpernin,2019-06-06T20:19:21Z,"The underlying client method seems to use a default timeout of Long.MAX_VALUE, TimeUnit.MILLISECONDS.
I'm worried that it can prevent the JVM to stop ?"
291590085,6295,vpernin,2019-06-07T13:24:28Z,"The KIP seems to refresh.topics is true by default. The property REFRESH_TOPICS_ENABLED is named refresh.topics.enabled and the default does not seem to be REFRESH_TOPICS_ENABLED_DEFAULT used.
So the topics are not refreshed by default."
291660048,6295,ryannedolan,2019-06-07T16:11:37Z,"The default is set here:

https://github.com/apache/kafka/pull/6295/files#diff-e3868ea5d15e72c75d65799355c2cc97R72

And used here:

https://github.com/apache/kafka/pull/6295/files#diff-e3868ea5d15e72c75d65799355c2cc97R259

I've verified this works as expected. Happy to take suggestions if this is not clear."
291839414,6295,williamhammond,2019-06-09T14:58:07Z,Don't we need a catch for  `org.apache.kafka.common.KafkaException` as well when calling `consumer.close()`? 
291840811,6295,williamhammond,2019-06-09T15:44:07Z,I feel like I must be missing something obvious but since the source admin client will be an instance of `KafkaAdminClient` since we're calling `https://github.com/ryannedolan/kafka/blob/KIP-382/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorCheckpointTask.java#L83` to create the client and the `KafkaAdminClient` overrides close as such https://github.com/ryannedolan/kafka/blob/KIP-382/clients/src/main/java/org/apache/kafka/clients/admin/KafkaAdminClient.java#L468 are we not actually releasing resources but calling close without a duration? I'm probably misremembering something about inheritance but either way I think we should be passing in a duration here since the default close just is kind of odd vpernin pointed out. 
296221720,6295,jeremy-l-ford,2019-06-21T12:51:34Z,"Based on https://issues.apache.org/jira/browse/KAFKA-8568, seems like the consumer may be null when attempting to close it.  Also, should the call to close the producer be in a separate try/catch to at least attempt to close the producer in the case where closing the consumer causes an exception?"
297175786,6295,vpernin,2019-06-25T13:03:58Z,"You're right.

I might miss something obvious, but the propagation of topic creation on downstream cluster does not seem to work.
The MirrorSourceConnector.refreshTopicPartitions detects a new topic properly and and it requests a task reconfiguration.
But the code that really proceed to the topic creation seems to be in MirrorSourceConnector.createTopicPartitions and this task is only executed once at startup and not at the reconfigure phase."
298207543,6295,vpernin,2019-06-27T14:30:17Z,"Shouldn't we have a check on the enabled status of the herder like B->A.enabled = false ?
With a simple setup enabled on the A->B direction and disabled on the B->A, I see that the herder B->A is created and heartbeats are emitted to cluster[A].topic[heartbeats], to its replicated on cluster[B].topic[A.heartbeats] and also on cluster[B].topic[heartbeats]."
298289975,6295,arunmathew88,2019-06-27T17:34:21Z,"As per the KIP the mirrored topic should be Writable by Mirror Maker only, how is this ensured? I couldn't find a filter rejecting any WRITE acls to the topic in this call?"
298543696,6295,vpernin,2019-06-28T10:37:46Z,"Shouldn't we have a public way to deserialize Heartbeat object, Heartbeat.deserializeRecord not being public ?"
299083439,6295,ryannedolan,2019-07-01T14:55:06Z,"@vpernin This behavior is subtle but correct. We want heartbeats going everywhere, even to clusters that aren't a target of any source->target replication. This is because we need heartbeats to exist upstream in order to replicate them downstream. If we are replicating A->B, we don't want to emit heartbeats only to B -- that wouldn't really tell us much, except that MM can send to B. What we want to know is how long it takes records to travel from A to B. So we emit heartbeats to A and _replicate them_ to B. This lets us monitor latency between A and B even when no other records are being replicated.

Specifically, we create herders between every pair of clusters (a fully-connected mesh) and emit heartbeats everywhere. Then, for the subset of ""enabled"" replications, we start at least one MirrorSourceConnector task. These tasks _always_ replicate heartbeats, so the result is ""A.heartbeat"" in cluster B whenever A->B is enabled.

We could _only_ emit records upstream (i.e. only to sources and not targets) and achieve this same result, but heartbeats are useful for more than measuring cross-cluster latency. Emitting heartbeats everywhere makes various other tooling possible. For example, you can query any single cluster and find out about every other cluster just by consuming the heartbeat topic, since heartbeats will have come from everywhere."
299085902,6295,ryannedolan,2019-07-01T14:59:43Z,I'm fine with that.
299213804,6295,ryannedolan,2019-07-01T20:55:04Z,@vpernin Connector.reconfigure() by default just calls stop() and start(config). There is no other logic related to reconfiguration at present.
299324123,6295,vpernin,2019-07-02T06:45:34Z,"Ok, I understand this conception.
Maybe, this clear explanation has its place in the KIP and the documentation."
299339279,6295,vpernin,2019-07-02T07:32:33Z,"I'm just noticing, that in my case, the creation of a new topic (matching replication pattern) upstream is not propagated downstream.
The subject of the reconfigure process is just a attempt to explain the problem.

KMM2 need to be restarted, so MirrorSourceConnector.createTopicPartitions is called.
Connector.reconfigure() does indeed stop and start, which would work, but this is not invoked. We call context.requestTaskReconfiguration() and it does not do that."
308616567,6295,o-kasian,2019-07-30T09:16:29Z,"This should probably be `this.herderPairs = config.enabledClusterPairs().stream()`, otherwise `A->B.enabled` makes no sense"
308690793,6295,o-kasian,2019-07-30T12:27:26Z,"`kafka/connect/mirror/src/main/java/org/apache/kafka/connect/mirror/MirrorSourceConnector.java:106` offset sync topic is created in `target`, however offsetProducer uses `config.sourceProducerConfig()`

It results in messages like
`Error while fetching metadata with correlation id 416 : {mm2-offset-syncs.backup.internal=UNKNOWN_TOPIC_OR_PARTITION}` in logs, and is not able to write offset mappings"
308953520,6295,ryannedolan,2019-07-30T21:35:28Z,"> offset sync topic is created in target

Good catch, thanks!"
308954111,6295,ryannedolan,2019-07-30T21:37:07Z,"See this earlier comment: https://github.com/apache/kafka/pull/6295#discussion_r299083439

I'll add a comment in the code somewhere to explain."
310134951,6295,mimaison,2019-08-02T13:37:55Z,`record.value()` can be null here. This leads to https://github.com/apache/kafka/pull/6295#issuecomment-517308971
310137707,6295,mimaison,2019-08-02T13:44:46Z,"The order of the arguments is inverted, `(short) 1` should be the 2nd argument and the replication factor the 3rd one.
The prototype is `createTopic(String topicName, short partition, short replicationFactor, Map<String, Object> adminProps)` "
310138665,6295,mimaison,2019-08-02T13:46:59Z,"The order of the arguments is inverted, `(short) 1` should be the 2nd argument and the replication factor the 3rd one.
The prototype is `createTopic(String topicName, short partition, short replicationFactor, Map<String, Object> adminProps)` "
311630254,6295,mimaison,2019-08-07T15:50:11Z,"Should this use `config.sourceAdminConfig()` instead of `config.targetAdminConfig()` ?
Otherwise, I'm getting:
```
WARN [Producer clientId=producer-7] Error while fetching metadata with correlation id 4 : {mm2-offset-syncs.spp2.internal=UNKNOWN_TOPIC_OR_PARTITION}
``` 
And the topic is only created in the target cluster. "
311789488,6295,ryannedolan,2019-08-07T22:29:05Z,"fixed, thanks"
311789612,6295,ryannedolan,2019-08-07T22:29:31Z,"fixed, thx"
311790132,6295,ryannedolan,2019-08-07T22:31:41Z,"fixed, thx"
312966989,6295,mimaison,2019-08-12T14:53:36Z,`x.getKey()` is the topic name. We need to iterate over `topicConfigs.values().entries()` instead to filter out topic properties.
312967159,6295,mimaison,2019-08-12T14:53:56Z,Should we also filter configs with `STATIC_BROKER_CONFIG` as the source?
314489069,6295,ryannedolan,2019-08-15T20:41:47Z,"I dropped the enabledClusterPairs() method, so marking this resolved."
314559556,6295,ryannedolan,2019-08-16T01:52:52Z,great catch! I've fixed and added a unit test.
314801873,6295,ryannedolan,2019-08-16T16:46:05Z,I _think_ isDefault() will catch that case? Not sure.
314809527,6295,mimaison,2019-08-16T17:08:11Z,`isDefault()` only matches `ConfigSource.DEFAULT_CONFIG`. See https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/ConfigEntry.java#L113-L118
314812870,6295,ryannedolan,2019-08-16T17:17:16Z,"Ah, thanks @mimaison. Fixed."
317319292,6295,ryannedolan,2019-08-23T22:20:08Z,We never block on the semaphore -- only tryWait() -- so there is no chance of deadlocking at least. I don't think there is any other consequence if we don't get around to releasing a semaphore.
317335237,6295,ryannedolan,2019-08-24T00:15:52Z,"Do you mean to implement the ..utils.Scheduler interface here, or to instead use the internal kafka.utils.KafkaScheduler? The latter would work just fine, but I'm reluctant to depend on something in kafka.utils."
317342841,6295,ryannedolan,2019-08-24T03:22:37Z,"fixed, thx"
317343051,6295,ryannedolan,2019-08-24T03:30:54Z,"It's hard to imagine a case where you'd need to tweak this property. I guess if you had offsets.lag.max set to zero, which means every replicated record would cause an offset sync, then you'd run up against the max outstanding offset syncs limit pretty quick. But I don't know why you'd do that. Normally offset syncs are very sparse, and it never matters if you drop a few occasionally. So this is mostly an arbitrary number, and anything greater than 0 will work fine."
317346004,6295,ryannedolan,2019-08-24T05:39:36Z,"fixed, thx"
317346028,6295,ryannedolan,2019-08-24T05:41:12Z,"Thanks guys, this does seem like a problem. Fixed by adding a configurable timeout."
323912773,6295,qihongchen,2019-09-12T19:31:32Z,"There's no argument for `""metatadata=%s""`, it should be removed, or add an argument for it (correct the typo as well)."
326590142,6295,dataGeeek,2019-09-20T11:43:38Z,"knownTargetTopics state is currently only refreshed at two occasions:

* startup of the connector, before topics in downstream cluster are
created and therefore stays empty
* new partitions/ dead partitions in upstream cluster are found.

In the case of no creation of new partitions in upstream cluster, the
state remains empty, which blocks config syncing, since only configs of knownTargetTopics will be synced.
I propose refreshing the state of knownTargetTopics at startup after topics in downstream cluster are created  and have provided a PR: https://github.com/ryannedolan/kafka/pull/7
"
327205321,6295,ryannedolan,2019-09-23T16:15:16Z,Great find! Merged.
327276472,6295,dataGeeek,2019-09-23T18:56:10Z,"Your're welcome! Besides that, our tests of the  MM2 were very promising. Great work and LGTM"
328867657,6295,junrao,2019-09-26T23:41:46Z,"Hmm, why do we need to call commitRecord twice?"
328867839,6295,junrao,2019-09-26T23:42:42Z,Should we version the value schema for potential future extension? Ditto is other newly added schemas.
328867958,6295,junrao,2019-09-26T23:43:24Z,Could we add the javadoc for each of the public method in this and other user facing classes?
328867995,6295,junrao,2019-09-26T23:43:34Z,Is it useful to have a metric that just captures the lag of the current record?
328868071,6295,junrao,2019-09-26T23:43:57Z,Should we add max in the description? Ditto in a few other places.
328868144,6295,junrao,2019-09-26T23:44:25Z,"Hmm, the downstream offset doesn't alway advance faster than the upstream. For example, when mirroring a compacted topic, the downstream offset could advance slower than upstream when the upstream offsets have holes."
328868189,6295,junrao,2019-09-26T23:44:40Z,"Hmm, how do we make sure that the latest offset in OffsetSyncStore match what's needed for the consumer offset?"
328868228,6295,junrao,2019-09-26T23:44:51Z,Do we need DEBUG by default? It could slow down the tests on Jenkins.
328868442,6295,junrao,2019-09-26T23:46:06Z,Calling System.currentTimeMillis() on every record could be expensive.
328868644,6295,junrao,2019-09-26T23:47:12Z,Should we update lastSyncUpstreamOffset and lastSyncDownstreamOffset only after the offsets have been successfully written to the offset sync topic?
328868859,6295,junrao,2019-09-26T23:48:22Z,Should offsetSync topic always be single partition?
328869797,6295,junrao,2019-09-26T23:53:43Z,Should we replicate prefix ACLs too?
329268348,6295,ryannedolan,2019-09-27T22:13:48Z,"Offset syncs (and checkpoints and heartbeats) are sparse and rare in practice, so this topic is very small. It's only written to when the upstream and downstream offsets for a partition don't match what is expected, which is approximately as rare as duplicate records being sent to a topic. MM2 can run for days without sending a message here. So there isn't a need to have multiple partitions, and doing so would complicate the logic for scanning the topic for a given offset, to some extent."
329268969,6295,ryannedolan,2019-09-27T22:15:59Z,"Good point, will fix."
329274932,6295,ryannedolan,2019-09-27T22:46:42Z,"Yes that would be useful, however, not nearly as useful as latency of each record, since latency is more easily aggregated over entire topics (and, externally, over entire clusters), and is much more relevant to the operator. Consider that an MM2 operator would not know how to compare a lag of 100 offsets in one partition to a lag of 1 offset in another partition, as they could both represent <1s behind real-time, depending on the size of messages etc. So while offset lag may be very useful to an operator of a specific app consuming from specific topics, it is unlikely useful to an MM2 operator replicating entire clusters."
329277034,6295,ryannedolan,2019-09-27T22:59:00Z,"A SourceTask implementation is unlikely to implement both methods. They are nops by default.

If it does, it's unlikely an implementation would do the same thing in both methods.

If it does, the interface does not guarantee that commitRecord() is called exactly once per record anyway (or even at least once). And indeed other methods, e.g. stop() and commit(), make no such guarantees either.

I'd be in favor of deprecating commitRecord(record) to avoid confusion here, but that is out of scope for this KIP and PR.
"
329835717,6295,ryannedolan,2019-10-01T00:07:02Z,done
329835749,6295,ryannedolan,2019-10-01T00:07:10Z,done
329835788,6295,ryannedolan,2019-10-01T00:07:20Z,done
329835885,6295,ryannedolan,2019-10-01T00:07:49Z,done
329863044,6295,ryannedolan,2019-10-01T03:00:19Z,"Good idea, this would improve accuracy of the offset syncs and checkpoints. I've made this change locally and don't see any problems. However, I'd rather not make a functional change this close to a code freeze. Let's make this improvement in a short follow-up PR."
330097541,6295,junrao,2019-10-01T14:42:48Z,"Since this is a public API change, I think it's important to think through the impact to  connector developers. I am thinking that we can (1) provide a default implementation of commitRecord(SourceRecord record, RecordMetadata metadata) in SourceTask that calls commitRecord(SourceRecord record) by ignoring RecordMetadata to provide backward compatibility; (2) only use commitRecord(SourceRecord record, RecordMetadata metadata) in WorkerSourceTask; (3) mark commitRecord(SourceRecord record) as deprecated to encourage people to use the new API and document the behavior if both methods are implemented (the implementation for the old api will be ignored). @ewencp @rhauch Any thought on this?"
330130857,6295,rhauch,2019-10-01T15:41:22Z,"So far, the Connect API exposes only a few Kafka client types in a few areas (right now just `SinkTask`). We should be careful and explicit about making the `SourceTask` interface use `RecordMetadata`.

But assuming that's okay, I think having Connect call two distinct `commitRecord(...)` methods with different signatures is at best confusing for developers (which do I implement) and at worst a potential source of error. We have resolved similar evolutions in the past by following the approach that @junrao mentioned, and I think that makes the most sense here. (We've not always deprecated the older method when it still applicable, but in this case I think we'd want to deprecate the older method.)

However, before we introduce a new variant of an existing method, we should also consider whether we might need to again modify the signature in the future. If so, we should consider whether it makes sense to create a new interface type to pass to the method that would make it easier in the future. I'm not sure that we do, since the `WorkerSourceTask` calls this from within the producer callback and only when the exception is not null, meaning there currently is no other data available to the invocation."
330134311,6295,rhauch,2019-10-01T15:47:50Z,This PR probably needs to mention it is also implementing KIP-416.
330137341,6295,ryannedolan,2019-10-01T15:53:53Z,Happy to deprecate the older method if we have concensus here.
330149902,6295,ewencp,2019-10-01T16:20:22Z,"given we can use default implementations now, just providing a default implementation and not deprecating seems better. afaik, `RecordMetadata` has never been requested before, but a number of connectors use the existing signature for `commitRecord`. Calling twice and having to understand that does seem confusing, but having overloads and simplified versions doesn't -- you just get to choose to implement something simpler. We've been pretty conservative with deprecation in the connect and that has served us well wrt broad compatibility -- I think only `onPartitionsAssigned/Revoked` for sink tasks and a couple of configs have gone through this, and still haven't been removed (with little maintenance cost afaik), which means we've remained cleanly compatible in most ways all the way back to 0.9. I'm generally for deprecation and removal (e.g. there's lots of core configs that I think are unnecessary and could be cleaned up), but only if the cost is worth it. In this case, there *is* a transition cost since the existing API is being used, and I'm not sure the reduction in confusion is worth it (vs some simple API docs explaining default impl)."
330154422,6295,rhauch,2019-10-01T16:30:30Z,"Good point about not deprecating, @ewencp. I could definitely go either way, and definitely see value in not deprecating and leaving it up to the implementer to choose which they'd implement. But I do think overriding one method is better than having to potentially override both methods for different purposes.

Because this is changing the Connect API, it does seem like we need to come to consensus and approve KIP-416 before this PR can be merged, though."
330162479,6295,ryannedolan,2019-10-01T16:49:07Z,"That would be neat, but there are landmines here. We don't want a prefix pattern to accidentally apply to topics on another cluster, so we can't just copy prefixes across. We could maybe expand a prefix ""foo"" on cluster ""primary"" to an equivalent ""primary.foo"" prefix, but that would only work if ReplicationPolicy used something like the default topic renaming convention -- a custom naming convention like ""topic-DC1"" would not work with prefixes this way.

We could maybe find all prefix patterns and replicate them as explicit literal patterns. So a prefix of ""foo"" turns into literals ""primary.foo-1"", ""primary.foo-bar"", etc. But even that is a little dangerous, as it is not easily reversible. For example, what happens when an upstream prefix ACL is deleted?

If there is a safe way to do this, I haven't thought of one yet. We should stick with literals only for now."
330168213,6295,ryannedolan,2019-10-01T17:01:31Z,"My vote is for keeping both methods (as implemented here), since this doesn't break or deprecate existing code, nor is it especially confusing to see the same method with overloaded parameters in an interface, especially when neither are required to implement. I think the confusing part is just in the WorkerSourceTask implementation, not the SourceTask interface -- it's a little surprising to see the same method called twice here. I suggest we add a comment here and move on.

@AndrewJSchofield thoughts?"
330203738,6295,ryannedolan,2019-10-01T18:20:07Z,"done, thanks."
330206785,6295,AndrewJSchofield,2019-10-01T18:26:49Z,"I'd prefer to see KIP-416 introduce `public void commitRecord(SourceRecord sourceRecord, RecordMetadata recordMetadata)` which is called in exactly the same situations as the existing `public void commitRecord(SourceRecord sourceRecord)`. The new method might be called with null record metadata. The implementor has a choice of which to implement."
330209568,6295,rhauch,2019-10-01T18:32:48Z,"@ryannedolan, I disagree that the confusing part is just in the `WorkerSourceTask` implementation. I believe it is confusing that both methods might be called with the same record, and that there is no benefit from doing that. So the issue I have is that I believe the proposed changes to the Connect API make it less clear how to properly implement it.

It is much clearer for the API to offer two methods and to say the framework always calls the new one, the new one by default calls the old one, and that task implementations can override one or none of them. Then it is entirely up to the task implementation to decide whether to even treat these conditions as distinct."
330213298,6295,ryannedolan,2019-10-01T18:41:02Z,"@rhauch just clarifying, are you suggesting we take @junrao 's suggestion, except we don't deprecate the existing method? i.e. have the new method call the old?

@AndrewJSchofield does that satisfy your concern as well?"
330304627,6295,rhauch,2019-10-01T22:28:58Z,"@ryannedolan, yes, I am suggesting that we follow @junrao's suggestion minus deprecating the old method. IOW, we'd do the following:
1) add the new method that by default calls the old method; and
2) change all uses in the Connect runtime (namely in WorkerSinkTask and its test class) to use the new method

and update KIP-416 accordingly."
330323913,6295,ryannedolan,2019-10-01T23:50:39Z,Sounds like we have concensus. Will update shortly.
330852160,6295,ryannedolan,2019-10-03T03:33:55Z,I made the change. Happy with either approach.
331279220,6295,junrao,2019-10-03T22:24:15Z,Could we add some comments to make it clear that one only needs to implement one of the commitRecord()?
331279346,6295,ryannedolan,2019-10-03T22:24:53Z,"Agree, there is room for improvement here. An obvious improvement would be to track latency only for the last record in each batch. I can address this in a later PR."
331283010,6295,ryannedolan,2019-10-03T22:40:15Z,"We don't use the OffsetSyncStore's offsets until we create a checkpoint. If the offsets are invalid or too far behind (-1 here), the checkpoint is not emitted for that topic-partition. So a downstream consumer would see an older checkpoint record for that topic-partition, which in turn was computed from a previous (valid) offset-sync.

When replication first starts, there will be no checkpoints. Once OffsetSyncStore is primed with offset-offset pairs, checkpoints will be emitted. If, say, MM2 fails to send to the offset-syncs topic for whatever reason, there will be no checkpoints until this process succeeds.

Both offset-syncs and checkpoints are ""opportunistic"" in the sense that it is never _essential_ that they are sent at any given point in time in order for downstream consumers to be correct. So we only send them when we are in a good state and have enough information to send a valid checkpoint."
331283663,6295,ryannedolan,2019-10-03T22:43:07Z,"Yes, this is actually handled here. Any holes upstream will trigger an offset-sync immediately. If such an offset-sync _fails_, for whatever reason, it just means that a checkpoint will not be emitted until the next sync... generally a few seconds later.

I'll call out a unit test that shows this in action."
331284770,6295,ryannedolan,2019-10-03T22:47:52Z,"@junrao holes in a compacted topic are handled, as shown here."
331301537,6295,junrao,2019-10-04T00:10:16Z,"unused exceptions InterruptedException, TimeoutException. Also, does this test cover anything more than the previous test?"
331494794,6295,omkreddy,2019-10-04T13:16:35Z,"@ryannedolan 
We need to include javadocs section for newly added public interfaces/classes.
Example: https://github.com/apache/kafka/blob/trunk/build.gradle#L1539

I assume, we will be adding kafka website documentation as part of KAFKA-8930.
http://kafka.apache.org/documentation/#basic_ops_mirror_maker

Also looks like test failures are related. "
331577566,6295,ryannedolan,2019-10-04T16:08:07Z,"Ah thanks @omkreddy I'll fix the javadocs this morning. For the website documentation, we'll need to keep the existing mirror-maker section for now, but I'll add a section re MM2, probably in a separate PR.

The failing tests seem to be related to flakiness in the Connect integration test framework. I'll see what I can do."
331732524,6295,ryannedolan,2019-10-05T04:44:20Z,"fixed. In the second test, additional topics are added, detected, and replicated. Detecting new topics will trigger a task rebalance in the middle of the test, which doesn't happen in the first.

Granted, it might make sense to collapse these into a single test rather than bring up and tear down the clusters twice."
331732663,6295,ryannedolan,2019-10-05T04:49:34Z,Added a couple lines locally. Will hold on to the commit for now -- I don't want to trigger another build at the moment.
331746988,6295,rhauch,2019-10-05T13:37:45Z,"@ryannedolan, we need to clearly specify that `metadata` parameter can be null, and then in the JavaDoc above specify what this means for the source task, namely that a transform dropped/skipped the record and it was not written to Kafka. 

IMO this is necessary so that developers of connector implementations know what the behavior is so they can properly implement their task. (The JavaDoc was not in [KIP-416](https://cwiki.apache.org/confluence/display/KAFKA/KIP-416%3A+Notify+SourceTask+of+ACK%27d+offsets%2C+metadata).)

I also think that it's also worth mentioning here that `SourceTask` implementations need only implement this method *or* the older `commitRecord(SourceRecord)` *or* neither method, but that generally they do not need to implement both since Connect will only call this method. Again, this will help developers that are implementing their own `SourceTask` what they need to do."
331750101,6295,ryannedolan,2019-10-05T15:15:01Z,@rhauch I improved the javadocs further. Should be clear now.
331750119,6295,ryannedolan,2019-10-05T15:15:46Z,I collapsed the two integration tests into one -- seems to save 30 seconds or so.
331795997,6295,ijuma,2019-10-06T15:42:24Z,Is there a reason why this is not using the `Time` interface? We generally never use `System.currentTimeMillis()` in Kafka.
331800260,6295,ryannedolan,2019-10-06T17:14:55Z,"Good idea, we should replace these in a subsequent PR."
381865765,6295,amanullah92,2020-02-20T09:06:17Z,"This directive (A->B.enabled) is missing in KIP in the ""Running MirrorMaker in production section""-- I got the cluster up but no replication was happening. Until I saw this and fixed this. I am new to MIrrorMaker/Connect Framework- forgive me if this is a well known thing."
526479473,9485,rajinisivaram,2020-11-18T22:58:26Z,We should document what this default implementation does and why a custom implementation may want to override this default.
526479478,9485,rajinisivaram,2020-11-18T22:58:26Z,We should document what this default implementation does and why a custom implementation may want to override this default.
526481573,9485,rajinisivaram,2020-11-18T23:03:40Z,nit: indentation
526482899,9485,rajinisivaram,2020-11-18T23:07:07Z,"This looks identical to the code block above for prefix, we could just run the same code in a loop that checks both allow literals and prefixes."
526483476,9485,rajinisivaram,2020-11-18T23:08:45Z,We should have exactly one call to `logAuditMessage` that says whether access was allowed or denied.
526486028,9485,rajinisivaram,2020-11-18T23:15:24Z,"request.principal can be a custom extension of KafkaPrincipal, we cannot use toString for comparison"
526487176,9485,rajinisivaram,2020-11-18T23:18:26Z,Not sure it is worth making a whole copy of this structure for a method that is not used frequently. It will be good to add microbenchmarks to `AclAuthorizerBenchmark` to understand how the new method performs.
526487761,9485,rajinisivaram,2020-11-18T23:19:56Z,"Same as in the Authorizer default method, we cannot use request.principal().toString()"
526489152,9485,rajinisivaram,2020-11-18T23:23:54Z,"We should optimize for the case where there are no DENY acls. There is no point in finding all matching ALLOW entries in that case, we would just need to check for one ALLOW."
526494014,9485,rajinisivaram,2020-11-18T23:37:05Z,make this all the methods below `private`
526494298,9485,rajinisivaram,2020-11-18T23:37:54Z,We coul just inline all the methods below instead of separate methods for host etc.?
526495928,9485,rajinisivaram,2020-11-18T23:42:06Z,"Hmm, Produce s authorized for topic anyway. Why would we use a very expensive authorizeByResourceType here?"
526496224,9485,rajinisivaram,2020-11-18T23:42:56Z,"First authorize should use `logIfAllowed=true`, `logIfDenied=false`"
526496998,9485,rajinisivaram,2020-11-18T23:44:48Z,`Durability`?
526499159,9485,rajinisivaram,2020-11-18T23:50:40Z,It may be better to put the mock tests into another test class. That wouldn't request ZooKeeper for example.
526499504,9485,rajinisivaram,2020-11-18T23:51:38Z,"As before, references to Durability in authorizer tests are confusing."
526500688,9485,rajinisivaram,2020-11-18T23:54:50Z,Are we going to add tests here?
526502000,9485,rajinisivaram,2020-11-18T23:58:19Z,We should run the microbenchmarks in AclAuthorizerBenchmark to make sure we don't add too much overhead here.
526503975,9485,rajinisivaram,2020-11-19T00:03:44Z,Don't we reuse this in multiple tests? How do we guarantee that no state is preserved between tests?
526504244,9485,rajinisivaram,2020-11-19T00:04:32Z,We could mock this fully instead of using AclAuthorizer?
526504251,9485,rajinisivaram,2020-11-19T00:04:33Z,We could mock this fully instead of using AclAuthorizer?
529008307,9485,ctan888,2020-11-23T21:29:27Z,"Yeah, that's right. Construct a KafkaPrinciple instance with params referred from principal.getType() and getName()

commit 89df4d7600cad4e3785d0d95624d0918efce1f44"
529008818,9485,ctan888,2020-11-23T21:30:18Z,"Yeah, that's right. Construct a KafkaPrinciple instance with params referred from principal.getType() and getName()

commit 89df4d7600cad4e3785d0d95624d0918efce1f44"
529031043,9485,ctan888,2020-11-23T22:15:54Z,"Yes. Using an ArrayList to group allow-literals & allow-prefixes in order to deduplicate the logic using a loop
commit 188536ad8df13fc327008e59c9787ad2230a7186"
529089666,9485,ctan888,2020-11-24T00:45:42Z,"Good point. Deferred the collection generation until we need it. 

commit 3906f978e62255ff266f081bf646a4b3c6b896ad "
529089823,9485,ctan888,2020-11-24T00:46:09Z,"Good catch. 

commit 3906f978e62255ff266f081bf646a4b3c6b896ad "
529090156,9485,ctan888,2020-11-24T00:47:03Z,"Yeah. But I'd guess that the compiler will optimize for us. 

commit 3906f978e62255ff266f081bf646a4b3c6b896ad "
532327560,9485,ctan888,2020-11-30T03:20:04Z,"commit 230ee36b9147a11d7ce299aa9fcbb590324faf68
Added the authorizeByResourceType() API to the benchmark and simulate the worst case: every allow ACL on the same resource has a dominant deny ACL.
Adjust the `resourceCount` parameter to ""10000"", ""40000"", ""80000"" since each cluster is unlikely to have more than 10k resources. Also, since we are testing against the worst case mentioned above, I think the ""10000"" cases are adequate for us.
Performance result here: https://paste.ubuntu.com/p/k8kV3w6fvF/"
532327775,9485,ctan888,2020-11-30T03:21:02Z,"commit 230ee36b9147a11d7ce299aa9fcbb590324faf68
Added the authorizeByResourceType() API to the benchmark and simulate the worst case: every allow ACL on the same resource has a dominant deny ACL.
Adjust the `resourceCount` parameter to ""10000"", ""40000"", ""80000"" since each cluster is unlikely to have more than 10k resources. Also, since we are testing against the worst case mentioned above, I think the ""10000"" cases are adequate for us.
Performance result here: https://paste.ubuntu.com/p/k8kV3w6fvF/"
532336627,9485,ctan888,2020-11-30T04:03:00Z,Yes. commit 254af37df5e2d6ec462e7b70497ceb655edea596
532337358,9485,ctan888,2020-11-30T04:06:39Z,Right. Deleted the else branch. commit 254af37
532337445,9485,ctan888,2020-11-30T04:06:59Z,commit 254af37
532337761,9485,ctan888,2020-11-30T04:08:19Z,"I was trying to prove that the new API can work properly with multiple add / remove operations. Changed to `testAuthorizerAnyMultipleAddAndRemove` for now. Any naming suggestion? 
commit b0aa305d8c043075ef0bb7b41d2c37e0072284c5"
532338240,9485,ctan888,2020-11-30T04:10:31Z,The MockAuthorizer is an AclAuthorizer using the interface default to do `authorizeByResourceType`. I was trying to prevent the duplicated code and ease the test implementation. Do you think we can keep it here?
532341814,9485,ctan888,2020-11-30T04:27:40Z,Right. Didn't realize that staitc variables in the permgen area will stay there during the whole unit test process. Turn the class variable into the instance variable.
532342253,9485,ctan888,2020-11-30T04:29:39Z,"If we are mocking this fully, we'd probably need tests on the `authorize` API which the interface default `authorizeByResourceType` is based on. Also, we'll have much more duplicated code in order to implement all the interfaces. "
532343635,9485,ctan888,2020-11-30T04:36:10Z,"I was trying to prove that the new API can work properly with multiple add / remove operations. Changed to `testAuthorizerAnyMultipleAddAndRemove` for now. Any naming suggestion? 
commit b0aa305d8c043075ef0bb7b41d2c37e0072284c5"
532343754,9485,ctan888,2020-11-30T04:36:40Z,"No. I was going to but that would add tons of duplicated codes. So I added the interface default test logic into AclAuthorizerTest. File deleted.

"
532346213,9485,ctan888,2020-11-30T04:48:19Z,Yes. Let's document this after we finally settle down all the implementations.
533501963,9485,rajinisivaram,2020-12-01T15:29:01Z,"This looks odd, do we really need these to index into arrays?"
533502417,9485,rajinisivaram,2020-12-01T15:29:33Z,Why do we create ArrayList(Arrays.asList)?
533503119,9485,rajinisivaram,2020-12-01T15:30:24Z,We could get host address and store in a variable outside the loop.
533503166,9485,rajinisivaram,2020-12-01T15:30:27Z,Why is this inside the for loop? We could just create one principal and use it inside the loop.
533505561,9485,rajinisivaram,2020-12-01T15:33:34Z,An EnumMap may be neater.
533510467,9485,rajinisivaram,2020-12-01T15:39:45Z,We should probably move this common code to SecurityUtils and use it both here and in the default implementation.
533510477,9485,rajinisivaram,2020-12-01T15:39:46Z,We should probably move this common code to SecurityUtils and use it both here and in the default implementation.
533513021,9485,rajinisivaram,2020-12-01T15:42:54Z,"We have lost the resource type for auditing, we should include a resource pattern with empty name or something."
533564908,9485,rajinisivaram,2020-12-01T16:49:59Z,"ok, I seem to have forgotten this. Why is this code different from the one in the default implementation?"
533567312,9485,rajinisivaram,2020-12-01T16:53:18Z,We should try to preserve the format for this for compatibility with scripts that parse these logs.
533622127,9485,rajinisivaram,2020-12-01T18:15:06Z,Do we have a benchmark for updates (not authorize)?
533623715,9485,rajinisivaram,2020-12-01T18:17:41Z,"if `denyAllResource` is true, we can just return DENIED?"
533626460,9485,rajinisivaram,2020-12-01T18:21:58Z,Looks like a lot of duplicate code here. We should see how to share code for all this. Can we move the default implementation into SecurityUtils and share some of the matching implementation across the classes?
533630212,9485,rajinisivaram,2020-12-01T18:27:59Z,Should this be `&&` since we we only need one?
533644589,9485,rajinisivaram,2020-12-01T18:51:24Z,"I wasn't sure what the result shows (not that familiar with the output format, sorry) The useful comparisons would be:
1) For authorizeByResourceType, what is the performance advantage we get by using this duplicate cache versus just using `aclCache`.
2) What is the impact on updates which hold a lock for maintaining two caches (without the PR vs with this PR)
3) Does this PR impact regular authorize() calls? I think the answer is no.

In any case, it seems unnecessary to maintain a second cache with all ACLs. We never use authorizeByResourceType for anything other than topics, so it seems a waste to store ACLs for other resource types here. We could just use `super.authorizeByResourceType` for other types."
533720425,9485,rajinisivaram,2020-12-01T21:04:39Z,This should perhaps be called DelegatingAuthorizer rather than MockAuthorizer since it is not a mock and requires ZK.
533720971,9485,rajinisivaram,2020-12-01T21:05:42Z,"I am not sure why we would make this change. If we need the change because we have become slower, we need to understand why."
533722349,9485,rajinisivaram,2020-12-01T21:08:29Z,spelling: principal
533723836,9485,rajinisivaram,2020-12-01T21:11:24Z,"We probably want to retain the old benchmark as-is and add a different one for `authorizeByResourceType`. We were testing a common pattern before, but now we seem to be testing a very unlikely scenario. While this may be useful for testing `authorizeByResourceType`, it is not what we want for regression testing the authorizer."
533724379,9485,rajinisivaram,2020-12-01T21:12:21Z,spelling: principal (multiple places)
533727081,9485,rajinisivaram,2020-12-01T21:17:34Z,Can we move testing of `interfaceDefaultAuthorizer.authorizer` into another class? This is `AclAuthorizerTest` and testing of `interfaceDefaultAuthorizer` seems unrelated to this test. 
533737220,9485,ctan888,2020-12-01T21:37:01Z,Right. DelegatingAuthorizer is more reasonable as a design pattern naming here.
533765909,9485,ctan888,2020-12-01T22:32:01Z,"The underlying algorithm of AuthorizeByResourceType() implementation in AclAuthorizer has several characteristics:
1. If any ""allow resource"" of the given ACE does not have a dominant ""deny resource"", the API will return immediately
2. The complexity is O(n*m) where `n` is the number of ""allow resources"" of the given ACE, 'm' is the number of ""deny resources"" of the given ACE, but not related to the number of ""ACE"" in the cluster.

$1 means that, given an ACE,  suppose `p%` of its ""allow resource"" does not have a dominant ""deny resource"", if `resourceCount` is `r`, on average, after checking `r * p * 0.01` ""allow resources"", the API will return. 
a) if we are let the ""dominant deny resource"" distribute evenly, like use the (loop index % something) to determine which ""allow resource"" should have a dominant ""deny resource"", we end up iterating the same amount of the ""allow resource"" and returning from the API call every time, which is `r*p*0.01`
b) if we are determine which ""allow resource"" should have a dominant ""deny resource"", the result will be too noisy. We may iterate only 1 resource or iterate all resources based on the randomize algorithm and seed.

$2 means that, the API time cost is not related to the number of ""ACE"" but is hyperbolically increasing when `resourceCount` is increasing. Under the assumption in (1), the actual complexity would be (r * r * p * 0.01)

As a result, we should get an insight into how long does the worst case takes, as `t`.  Then we can estimate some reasonable values of `p` and then estimate the API cost by `t * p`. 

So I was directly testing the worst case, where p = 1, which means 100% of the ""allow resource"" will have a dominant ""deny resource. The complexity hence would be (r^2). It's rare that a cluster can have 200k ""allow resources"" and 200k corresponding ""dominant deny resources"" for each user, and it's not fair to have a relatively smaller `aclCount` and huger `resourceCount`, as the API is optimizing the performance by indexing on `ACE`.
"
533915036,9485,ctan888,2020-12-02T05:57:25Z,EnumMap make sense. commit 1a139ce744a279e4424188008ee5158186b0fcbe
533915142,9485,ctan888,2020-12-02T05:57:49Z,Yeah. Took out. commit 1a139ce744a279e4424188008ee5158186b0fcbe
533915261,9485,ctan888,2020-12-02T05:58:12Z,EnumMap make sense. commit 1a139ce744a279e4424188008ee5158186b0fcbe
533915275,9485,ctan888,2020-12-02T05:58:15Z,EnumMap make sense. commit 1a139ce744a279e4424188008ee5158186b0fcbe
533916889,9485,ctan888,2020-12-02T06:03:33Z,Right. Just as what we've done to Principle. commit 29ac8628089ddf1210072bbf52e01a41e123a718
533919425,9485,ctan888,2020-12-02T06:10:11Z,commit f6d2a39706998160ebe77a854b8bf64268eec68a
533922526,9485,ctan888,2020-12-02T06:19:38Z,commit 6ab95d3668b3de27a7f6f58fc171a1e2e8925f69
533922580,9485,ctan888,2020-12-02T06:19:47Z,commit 6ab95d3668b3de27a7f6f58fc171a1e2e8925f69
534447773,9485,ctan888,2020-12-02T20:05:42Z,"Use ""NONE"" for the pattern name and ""UNKNOWN` for the pattern type
commit cebbbd47a8e7d318e327e3a279072c718b535abd"
534447894,9485,ctan888,2020-12-02T20:05:57Z,"leave the message as it is now.
commit cebbbd47a8e7d318e327e3a279072c718b535abd"
534477631,9485,ctan888,2020-12-02T20:59:42Z," I just realized that, in order to check the dominant denies, my AclAuthorizer implementation is calling `String::startWith` which also has an O(d) complexity where `d` is the length of the ""deny pattern"" string of the given ACL. So the complexity would be O(n * m * d). 

So given all ""allow pattern"" and ""deny pattern"" of a given ACE, we have 2 algorithms now
1.  Iterate through all the prefixes of the `allow pattern` string and check if any prefix is contained in the set of `deny pattern`, which has a complexity of O(n * a), where `a` is the length of the ""allow pattern"" string. My interface default is using this approach.
2. Iterate through all the ""deny patterns"", which has a complexity of O(n * m * d), where d is the length of the `deny pattern` string. My AclAuthorizer is using this approach.

Comparasion: Since the average of the `allow pattern` string length should be close to that of the `deny pattern`, we can say `a = d`. So O(n * a) = O(n * d) > O(n * m * d), which means approach 1 is much better.

Conclusion: I'll change AclAuthorizer to use approach 1."
534589923,9485,ctan888,2020-12-03T01:07:31Z,right. Just as what AclAuthorizer does. commit 18c5c04ad4d8c98dc3cdaa6d15bf70b9991a6b88
534751595,9485,ctan888,2020-12-03T06:51:40Z,"Yeah, moved to SecurityUtils. commit 30899c45ac50b70625baa2e5f12f58cfe9d79404"
534755163,9485,ctan888,2020-12-03T06:53:59Z,"Now the only difference is that the AclAuthorizer is indexing on ACE, so the number of ACE won't impact the query efficiency. "
534763817,9485,ctan888,2020-12-03T06:59:40Z,"I think adding some dominant denies won't change the performance pattern of AclAuthorizer::acls and AclAuthorizer::authorize. 
1. AclAuthorizer::acls just return all the matching acls by the filter rule.  The portion btw ""allow"" and ""deny"" resources doesn't matter.
2. AclAuthorizer::authorize will iterate the and filter out the allow and deny ACEs respectively. Since it's using ResourcePattern as its indexing method, the portion btw ""allow"" and ""deny"" resources doesn't matter as well."
534983615,9485,ctan888,2020-12-03T09:12:16Z,"Move the interface default test to a new class. Also, created a util class for code sharing.
commit 6c550fd04a0c1912e669bf18d60dee27dd03e53c"
536305219,9485,ctan888,2020-12-04T18:46:54Z,commit 7af4a7ff7ed2dddc06cf11ab7ff2d4b9fee5fb56
537657552,9485,ctan888,2020-12-07T16:46:03Z,Good catch commit 031c2f41e6611df3d18ef9b709c7d98c91b93326 
537667755,9485,ctan888,2020-12-07T16:58:58Z,Please see below
538522173,9485,lbradstreet,2020-12-08T15:49:08Z,"nit, unnecessary whitespace in `i++`."
538524117,9485,lbradstreet,2020-12-08T15:50:45Z,I think it's useful to understand how the cache performs at smaller sizes as well as larger sizes. Is there a reason we went with a fixed size and fixed number of resources now?
538528015,9485,lbradstreet,2020-12-08T15:54:15Z,It might be better for the purpose of this microbenchmark to setup the cache with the desired size ahead of the time and then measure the time to update the cache with one entry. Otherwise you risk measuring a lot of the setup costs rather than the cost of the typical usage.
538553851,9485,lbradstreet,2020-12-08T16:16:33Z,"If you take an async profile of this benchmark method you end up spending most of the time in building the entries and immutable set, and barely any time on `AclAuthorizer#updateCache`."
538613769,9485,ctan888,2020-12-08T17:09:18Z,"Oh, I'm just demonstrating the chart 3 I uploaded. I'll change them back."
538618149,9485,ctan888,2020-12-08T17:13:15Z,"@rajinisivaram Do you think we'll keep this `testUpdateCache` and merge it into trunk? If so, let's setup the cache ahead of time. But I think this benchmark is mainly for comparing the trunk with my branch, which means that we probably won't merge this `testUpdateCache` into master, which also means the same procedure constructing some memory records are acceptable since we are taking the time cost difference."
538618817,9485,ctan888,2020-12-08T17:13:55Z,"Yeah. Agree. Let's see what @rajinisivaram think about the above discussion

"
538768554,9485,ctan888,2020-12-08T20:04:09Z,Thanks. Fixed.
539594382,9485,rajinisivaram,2020-12-09T19:42:17Z,"This package is part of the public API, but the class looks like it should be internal?"
539595584,9485,rajinisivaram,2020-12-09T19:44:19Z,Perhaps ResourceAclEntry or something along those lines would be better than `ResourceIndex` since this class has no notion of index.
539599430,9485,rajinisivaram,2020-12-09T19:50:25Z,Can we remove the TODO comments?
539602809,9485,rajinisivaram,2020-12-09T19:55:39Z,"In the typical case, we have a large number of `allowLiterals` and `allowPrefixes`, no `denyLiterals` or `denPrefixes`. I think it would make sense to special case `denyLiterals.isEmpty && denyPrefixes.isEmpty`. In this case, we don't need to find all matching resources, we just need to check that there is at least one matching resource. "
539604360,9485,rajinisivaram,2020-12-09T19:58:05Z,Why can't this be a `Set` instead of `List of Sets`?
539604845,9485,rajinisivaram,2020-12-09T19:58:51Z,`AclEntry.WildcardPrincipalString`
539607223,9485,rajinisivaram,2020-12-09T20:02:33Z,This method can be in SecurityUtils and shared with the default authorizer?
539607563,9485,rajinisivaram,2020-12-09T20:03:07Z,private def?
539612536,9485,rajinisivaram,2020-12-09T20:11:00Z,"We cannot do this here. `AuthorizerWrapper` is used to wrap any custom authorizer using the old Authorizer API. `AllowEveryoneIfNoAclIsFoundProp` is a custom config of `SimpleAclAuthorizer` and `AclAuthorizer`, we cannot use that with any custom authorizer. We should find a way to support the config for SimpleAclAuthorizer that doesn't impact other custom authorizers."
539614719,9485,rajinisivaram,2020-12-09T20:14:37Z,Does this work with an ACL with wildcard host?
539614739,9485,rajinisivaram,2020-12-09T20:14:38Z,Does this work with an ACL with wildcard host?
539615189,9485,rajinisivaram,2020-12-09T20:15:25Z,"The main logic of this could potentially be moved to SecurityUtils since the default Authorizer implementation, AclAuthorizer and the wrapper all do this."
539630084,9485,rajinisivaram,2020-12-09T20:37:40Z,Why do we need this in tearDown?
539631425,9485,rajinisivaram,2020-12-09T20:40:04Z,a lot of these changes look unnecessary
539632426,9485,rajinisivaram,2020-12-09T20:41:50Z,looks like this hasn't been reverted?
539632653,9485,rajinisivaram,2020-12-09T20:42:14Z,revert?
539634789,9485,rajinisivaram,2020-12-09T20:45:38Z,why? This no longer reflects the comment above. Can we revert?
539635426,9485,rajinisivaram,2020-12-09T20:46:42Z,We should revert changes to existing benchmark because it hard to tell why these changes were made and what impact it has on the original benchmark. 
539638872,9485,rajinisivaram,2020-12-09T20:52:14Z,It makes sense to merge the benchmarks to trunk. Let's make sure it measures just updateCache.
539730884,9485,ctan888,2020-12-09T23:41:08Z,Shall we make the class constructor package-private or make this class an inner class of AclAuthorizer?
539734315,9485,ctan888,2020-12-09T23:47:14Z,I used index as it's used as the index of the hashmap. What about something like `ResourceNameFilter`?
539734983,9485,ctan888,2020-12-09T23:48:51Z,Yeah. Removed.
539736828,9485,ctan888,2020-12-09T23:53:14Z,Yes. commit 2fd4babe2c27ee0723fa1cd720ca35d2bbefe57b
539737110,9485,ctan888,2020-12-09T23:53:55Z,commit 2fd4babe2c27ee0723fa1cd720ca35d2bbefe57b
539737287,9485,ctan888,2020-12-09T23:54:20Z,Sure
539751258,9485,ctan888,2020-12-10T00:28:50Z,"Yes. commit 1dc143fc78a3b9927189751255346ef0b6cafd90
if (noDeny) {
..if (hasAllow) {
....return Authorize.ALLOWED
..} else {
....return Authorize.DENIED // since no allow exists
..}
}"
539782093,9485,ctan888,2020-12-10T01:50:16Z,Because we don't wanna reconstruct a new large set containing all the matching resources. We are constructing a List of  ~ 3 * 3 * 3 elements which refer to existing HashSets maintained by `updateCache`.
539787542,9485,ctan888,2020-12-10T02:03:20Z,I was trying to share it but it seems like the different collection type btw java and scala is a headache. We'll then need some java converters or instantiate a java collection in the scala code. Do you think it deserves this?
540418386,9485,ctan888,2020-12-10T18:56:44Z,"Given that we probably don't want to change the deprecated Authorizer interface, I can only think of one way to achieve this:

Besides checking if the `AllowEveryoneIfNoAclIsFoundProp` exists and if it equals to `true`, I added another check to authorize on a hardcoded session, operation, and resource.

Since configure() will be called immediately after the authorizer instantiation, it's guaranteed that no ACLs would exist when we do this check. 

override def configure(configs: util.Map[String, _]): Unit = {
..baseAuthorizer.configure(configs)
....shouldAllowEveryoneIfNoAclIsFound = (configs.asScala.get(
......AclAuthorizer.AllowEveryoneIfNoAclIsFoundProp).exists(_.toString.toBoolean)
........&& baseAuthorizer.authorize(
..........new Session(KafkaPrincipal.ANONYMOUS, InetAddress.getByName(""1.2.3.4"")),
............Read, new Resource(Topic, ""hi"", PatternType.LITERAL)))
}

commit 2ed79a0a7788f8841475badfd1c26adf0eb3435c"
540655651,9485,ctan888,2020-12-11T03:03:41Z,"Good catch. commit 8263bd319f63d39808f90129db55427b98385dd4
Since it's a bit hard to test `AllowAnyoneIfNoAclFound` and many other logics in the integration test, I added a new test class `AuthorizerWrapperTest`."
540658692,9485,ctan888,2020-12-11T03:13:10Z,"commit 8263bd3 changed the AuthorizerWrapper logic and optimized the performance a bit.

Now AuthorizerWrapper#denyAllResource will 
1. only use Authorizer#acls() to filter out the `WildcardResource` with the pattern type `LITERAL`. 
2. check if any of the filtered out bindings match the `request principle` and `request host`. 

So it's behavior diverges more from the interface default now."
540705023,9485,ctan888,2020-12-11T05:43:38Z,"Otherwise ""deny all"" will remain in ZK during the whole test process since ZK won't be restarted or re-instantiated."
540706117,9485,ctan888,2020-12-11T05:46:49Z,"Since `AuthorizerInterfaceDefaultTest`, `AclAuthorizerTest`, and `AuthorizerWrapperTest` are sharing some test utils, we need to make this method signature abstract a bit, in order to make it usable by AuthorizerTestFactory."
541202572,9485,ctan888,2020-12-11T19:35:27Z,Yeah. I was doing resourceName = resourceName + 95 to re-use this variable. We can revert it.
541206048,9485,ctan888,2020-12-11T19:38:55Z,Yes.
541206648,9485,ctan888,2020-12-11T19:39:31Z,Yes
541212190,9485,ctan888,2020-12-11T19:45:19Z,"The existing benchmark does not have any DENY resource in it. Adding some DENY bindings whose percentage is controlled by parameters will be an improvement to the existing benchmark and help us understand the performance better. 

I've reverted all changes other than adding some DENY bindings. Also, I moved those memory intense operations into the @setup phase so now the benchmark just measures updateCache(). Does the benchmark look good to you now?

commit 6536cea788210860a764f3f0a6901244e8d974fe


@rajinisivaram"
541779975,9485,ctan888,2020-12-12T21:02:27Z,"Reverted other test changes
commit 4f9b79a810c4da3030fe262d4bfdc97df4945e8c"
542039322,9485,ctan888,2020-12-14T00:14:52Z,"Benchmark result: https://paste.ubuntu.com/p/zvjZC4QkMM/
Performance pattern doesn't change, except `testUpdateCache` runs much faster now."
542790510,9485,rajinisivaram,2020-12-14T21:10:16Z,"We have to move the class outside of the public package, so putting it alongside AclAuthorizer makes sense."
542830098,9485,rajinisivaram,2020-12-14T21:47:04Z,"Since this is the javadoc of a public API, we should move the details on how the default implementation works outside of the javadoc. We can move this list of comments inside the method. "
542851551,9485,rajinisivaram,2020-12-14T22:07:06Z,"We don't currently have anything in the default implementation to support super.users right? Unlike `allow.everyone.if.no.acl.found` which is not particularly suitable for production use, `super.users` is a commonly used config that is likely to be in use in a lot of deployments. The simplest fix may be to `authorize()` with a hard-coded name and return ALLOWED if `authorize()` returns ALLOWED before any of the logic below is executed."
542854629,9485,rajinisivaram,2020-12-14T22:09:57Z,This needs to be an immutable map or a ConcurrentHashMap since we read this without lock.
542855316,9485,rajinisivaram,2020-12-14T22:10:36Z,We need to check if the principal is a super.user and return ALLOWED for super users before executing any of the logic below.
542860969,9485,rajinisivaram,2020-12-14T22:16:10Z,"ok, makes sense"
542861839,9485,rajinisivaram,2020-12-14T22:16:57Z,"ok, let's leave as is."
542871579,9485,rajinisivaram,2020-12-14T22:26:24Z,"This is too hacky. And it breaks if ANONYMOUS has all access (e.g. because inter-broker listener alone uses PLAINTEXT). We could check `baseAuthorizer.isInstanceOf[SimpleAclAuthorizer]` perhaps. It is not perfect since it would break if there was a custom authorizer that extended SimpleAclAuthorizer, but doesn't support AllowEveryoneIfNoAclIsFoundProp and the prop was set to true. But that seems like an unlikely scenario."
542879143,9485,rajinisivaram,2020-12-14T22:33:41Z,"This sequence doesn't work with super.users. We probably should do something like:
```
if (super.authorizeByResourceType(requestContext, op, resourceType) == AuthorizationResult.ALLOWED)
  AuthorizationResult.ALLOWED
else if (denyAllResource(requestContext, op, resourceType) || !shouldAllowEveryoneIfNoAclIsFound)
  AuthorizationResult.DENIED
else
  AuthorizationResult.ALLOWED
```"
542881770,9485,rajinisivaram,2020-12-14T22:36:27Z,We could have done principal.toString() once in the caller rather than convert everytime.
542884517,9485,rajinisivaram,2020-12-14T22:39:08Z,ZK is reinstantiated for every test.
542887409,9485,rajinisivaram,2020-12-14T22:41:58Z,"As in the other class, we don't need this in tearDown"
542888614,9485,rajinisivaram,2020-12-14T22:43:13Z,We should add tests for super users.
542890465,9485,rajinisivaram,2020-12-14T22:45:10Z,why are we storing these?
542892942,9485,rajinisivaram,2020-12-14T22:47:34Z,Why?
542893393,9485,rajinisivaram,2020-12-14T22:48:00Z,why was this changed from AclAuthorizer to Authorizer?
542893665,9485,rajinisivaram,2020-12-14T22:48:15Z,why are we storing this?
542900213,9485,rajinisivaram,2020-12-14T22:54:40Z,It would be better to move this inside AuthorizerInterfaceDefaultTest since it is specific to that test.
542906008,9485,rajinisivaram,2020-12-14T23:00:35Z,"For Map, you would say `key` rather than `index`. But this is not a `resource` or `resourceName` - it has no resource name, it is not a filter, but it includes AccessControlEntry. Maybe just ResourceTypeKey is sufficient, but you could also include something to indicate it includes the AccessControlEntry if you want. Either way, putting it along with AclAuthorizer would make naming less critical."
543080612,9485,ctan888,2020-12-15T06:30:16Z,Sure. commit 25e0bfcc97f956ceb4254ab8c457fe5d8d250e82
543093393,9485,ctan888,2020-12-15T07:00:05Z,"So we have three approaches here:
1. use .getClass
2. use .isInstanceOf
3. only configure the property with the key ""AclAuthorizer.AllowEveryoneIfNoAclIsFoundProp"" in the AuthorizerWrapper instance construction so no other property will get in.

Neither of them is perfect but approach 2 also seems better to me. 

commit 1217394c0c3767ac11df958c02a681c8cbc8382b"
543099934,9485,ctan888,2020-12-15T07:14:00Z,"Yeah. I was trying to restrict the type in order to remind people to construct a KafkaPrinciple first. But toString() is an expensive operation.

commit 16576f85a858648cfc4ff882b554ddc65922021c"
543114091,9485,ctan888,2020-12-15T07:41:41Z,"I replied here. Maybe I shouldn't have resolved it. https://github.com/apache/kafka/pull/9485#discussion_r540706117

Since AuthorizerInterfaceDefaultTest, AclAuthorizerTest, and AuthorizerWrapperTest are sharing some test cases, we need to make this method signature abstract a bit, in order to pass the method reference to AuthorizerTestFactory.
"
543116084,9485,ctan888,2020-12-15T07:45:14Z,"For this method, I changed the signature back to AclAuthorzier as the AuthorizerTestFactory is not depending on it."
543117875,9485,ctan888,2020-12-15T07:48:38Z,"Removed as we are not removing ACLs in teadDown() anymore.

commit 825a8ba77ad1766f998a71a9a15f21e73daad84a"
543117908,9485,ctan888,2020-12-15T07:48:42Z,commit 825a8ba77ad1766f998a71a9a15f21e73daad84a
543117954,9485,ctan888,2020-12-15T07:48:48Z,commit 825a8ba77ad1766f998a71a9a15f21e73daad84a
543118616,9485,ctan888,2020-12-15T07:49:54Z,"Removed as we are not removing ACLs in teadDown() anymore.

commit 825a8ba"
543120808,9485,ctan888,2020-12-15T07:53:47Z,Make sense. commit e31f157eaac1213445dd284fd2209a29f4fa18fd 
543134773,9485,ctan888,2020-12-15T08:18:45Z,"Would scala ""foreach"" throw any exception when READ operation races with WRITE in HashMap / HashSet? If not, I think we can tolerate some READ inconsistency as ZK is also broadcasting the ACL changes asynchronously to brokers."
543174139,9485,ctan888,2020-12-15T09:18:00Z,"I tested a bit, using 1 bg thread adding and removing elements to a mutable.HashSet while the main thread constantly iterating the HashSet using ""foreach"". The ""foreach"" call doesn't throw any exception. But I'm a bit unsure what would happen if the iteration hits a bucket where some elements are being added to or deleted from. 

Let me test what's the overhead using the immutable map. I'd prefer this approach as we're expecting much more READ than WRITE to the hashset."
543744087,9485,ctan888,2020-12-15T22:49:36Z,"Good catch. This is super important. 

commit dae1a788b70ebc03eab265b1027a4b43ad8e773b"
543744182,9485,ctan888,2020-12-15T22:49:47Z,"Good catch. This is super important. 

commit dae1a788b70ebc03eab265b1027a4b43ad8e773b"
543745352,9485,ctan888,2020-12-15T22:52:00Z,"Test added for AuthorizerInterfaceDefaultTest, AclAuthorizerTest, AuthorizerWrapperTest.

commit dae1a788b70ebc03eab265b1027a4b43ad8e773b"
543805858,9485,ctan888,2020-12-16T01:18:49Z,"Use Immutable collections:

Benchmark                                           (aclCount)  (denyPercentage)  (resourceCount)  Mode  Cnt     Score      Error  Units
AclAuthorizerBenchmark.testAclsIterator                     50               100           200000  avgt    5  4132.824  2967.122  ms/op
AclAuthorizerBenchmark.testAuthorizeByResourceType          50               100           200000  avgt    5    46.733     5.397  ms/op
AclAuthorizerBenchmark.testAuthorizer                       50               100           200000  avgt    5     6.844     0.915  ms/op
AclAuthorizerBenchmark.testUpdateCache                      50               100           200000  avgt    5  7219.696  4018.189  ms/op
JMH benchmarks done



Use Mutable collections:

AclAuthorizerBenchmark.testUpdateCache                      50               100           200000  avgt    5  4927.832  2570.786  ms/op


When aclCount = 50, denyPercentage = 100, resourceCount = 200000, the time cost is 2.3 seconds more with immutable collections. But since adding 50 * 20000 ACL bindings only takes ~7 seconds, I think the performance should be acceptable."
543813486,9485,ctan888,2020-12-16T01:39:14Z,Removed.
543814295,9485,ctan888,2020-12-16T01:41:28Z,"ResourceTypeKey sounds good:

commit 7fe92c6436432760adf9465c3f0bcf3c91104b10"
543814563,9485,ctan888,2020-12-16T01:42:04Z,"Make ResourceTypeKey an inner class of AclAuthorizer 

commit 7fe92c6436432760adf9465c3f0bcf3c91104b10"
543814681,9485,ctan888,2020-12-16T01:42:19Z,"Good catch. This is super important.

commit dae1a78"
543817949,9485,ctan888,2020-12-16T01:51:12Z,commit 62c44ade550a90671ff41bfb847e2bc28adc7baa
544223127,9485,rajinisivaram,2020-12-16T11:29:11Z,Use `op` rather than READ since that fits with why we are allowing access. We also need a test that verifies that permission to READ everything doesn't imply `authorizeByResourceType` for WRITE.
544223630,9485,rajinisivaram,2020-12-16T11:29:58Z,Use `logIfAllowed=true` since we are granting access in that case.
544229673,9485,rajinisivaram,2020-12-16T11:40:14Z,Can we used named arguments for the booleans: `authorized = false` - we should update all usages of `logAuditMessage` below.
544232558,9485,rajinisivaram,2020-12-16T11:44:53Z,"We should make this a `case class`. We can then remove all the methods (equals, hashCode and toString) since we get those for free."
544239742,9485,rajinisivaram,2020-12-16T11:56:47Z,"not a `custom` principal, just a `principal`."
544239749,9485,rajinisivaram,2020-12-16T11:56:48Z,"not a `custom` principal, just a `principal`."
544240156,9485,rajinisivaram,2020-12-16T11:57:27Z,All these tests are using `READ` which happens to work for the default implementation since we used READ there. 
544244813,9485,rajinisivaram,2020-12-16T12:05:14Z,Can we check how much work it would be to convert `authorizerTestFactory` into an  `abstract BaseAuthorizerTest` class that the three xxxAuthorizerTest classes extend? Having to repeat these tests in all three places makes it too easy to miss one in the future.
544246252,9485,rajinisivaram,2020-12-16T12:07:37Z,nit: principle => principal
544545938,9485,ctan888,2020-12-16T18:57:59Z,commit ec80dc4e55758d83835f3ecde381a988d6dd4779
544545948,9485,ctan888,2020-12-16T18:58:00Z,commit ec80dc4e55758d83835f3ecde381a988d6dd4779
544545971,9485,ctan888,2020-12-16T18:58:04Z,commit ec80dc4e55758d83835f3ecde381a988d6dd4779
544546002,9485,ctan888,2020-12-16T18:58:07Z,commit ec80dc4e55758d83835f3ecde381a988d6dd4779
544546101,9485,ctan888,2020-12-16T18:58:15Z,commit ec80dc4e55758d83835f3ecde381a988d6dd4779
544546172,9485,ctan888,2020-12-16T18:58:22Z,commit ec80dc4e55758d83835f3ecde381a988d6dd4779
544546946,9485,ctan888,2020-12-16T18:59:31Z,"Yeah. Since I've changed `READ` to `op`, this has been resolved."
544547081,9485,ctan888,2020-12-16T18:59:40Z,commit ec80dc4e55758d83835f3ecde381a988d6dd4779
544713670,9485,ctan888,2020-12-17T00:09:14Z,"commit 092fec70a9547ec07cba999e77be1c0cf79fa275
commit e5e3d18f57ab22df20133f9841905af384d9b641

These two commits are condensing the class methods and members into the BaseAuthorizerTest. 

In BaseAuthorizerTest, the only abstract method is an authorizer provider. After overriding the provider, those test cases in it are sufficient to run.

Now the test code looks much cleaner. If the changes look too much to you, we can revert 092fec70a9547ec07cba999e77be1c0cf79fa275 and move the head to e5e3d18f57ab22df20133f9841905af384d9b641"
545246428,9485,rajinisivaram,2020-12-17T16:56:46Z,`resourceIndex` => `resourceTypeKey` and omit `new`.
545247397,9485,rajinisivaram,2020-12-17T16:58:03Z,Couldn't we just check:`resourceCache.contains(resourceKey)` ?
545261128,9485,rajinisivaram,2020-12-17T17:17:49Z,"`resourceIndex` => `resourceTypeKey`, Also we can omit new for ResourceTypeKey since it is a case class."
545264795,9485,rajinisivaram,2020-12-17T17:23:02Z,`private def`
545264898,9485,rajinisivaram,2020-12-17T17:23:10Z,`private def`
545267737,9485,rajinisivaram,2020-12-17T17:27:13Z,"We can use `map` instead of `match`:
```
aclCache.get(resource).map(_.acls.map(_.ace)).getOrElse(Set.empty)
```"
545270222,9485,rajinisivaram,2020-12-17T17:30:45Z,"`resourceIndex` => `resourceTypeKey`, Omit `new`"
545270292,9485,rajinisivaram,2020-12-17T17:30:53Z,"`resourceIndex` => `resourceTypeKey`, Omit `new`"
545273561,9485,rajinisivaram,2020-12-17T17:35:05Z,`canDenyAll` => `denyAll` since `can` doesn't fit with `deny`
545280654,9485,rajinisivaram,2020-12-17T17:45:19Z,"Suggestions to improve this (feel free to ignore/update):

```
Custom authorizer implementations should consider overriding this default implementation because:
1) The default implementation iterates all AclBindings multiple times, without any caching for resource types. More efficient implementations may be added in custom authorizers that directly access cached entries.
2) The default implementation cannot integrate with any audit logging included in the authorizer implementation.
3) The default implementation does not support any custom authorizer configs or other access rules apart from ACLs."
545282374,9485,rajinisivaram,2020-12-17T17:47:45Z,Add a comment to say that we check for one hard-coded name to ensure that super users are granted access regardless of DENY acls.
545287419,9485,rajinisivaram,2020-12-17T17:55:22Z,Can we make this comment two lines instead of 4 since each sentence seems short enough to fit into a line?
545325909,9485,rajinisivaram,2020-12-17T18:53:16Z,"`PatternType.UNKNOWN` looks odd in audit logs, `ANY` may be better."
545363933,9485,rajinisivaram,2020-12-17T19:55:50Z,"The nested for loop can be replaced with:
```
for (p <- Set(principal, AclEntry.WildcardPrincipalString); h <- Set(host, AclEntry.WildcardHost); o <- Set(op, AclOperation.ALL))
```"
545365855,9485,rajinisivaram,2020-12-17T19:58:45Z,We can make this a `val` by using an ArrayBuffer instead of List that we keep recreating
545366101,9485,rajinisivaram,2020-12-17T19:59:08Z,"As before, we can use a single for loop instead of nested loop"
545367497,9485,rajinisivaram,2020-12-17T20:01:27Z,Can use `denyLiterals.exists(_.contains(ResourcePattern.WILDCARD_RESOURCE))`
545368045,9485,rajinisivaram,2020-12-17T20:02:13Z,"Can use `allowPrefixes.exists(_.exists`, similarly for `allowLiterals`."
545368198,9485,rajinisivaram,2020-12-17T20:02:31Z,nit: space before {
545369571,9485,rajinisivaram,2020-12-17T20:04:55Z,Can be `!denyLiterals.exists(_.contains(literalName))`?
545399101,9485,rajinisivaram,2020-12-17T20:57:59Z,We should use the same pattern as the usage of `aclCache` where we get a `aclCacheSnapshot` at the start of the method and then use the same snapshot throughout the method rather than use a changing value of resourceCache within the loop. 
545410322,9485,rajinisivaram,2020-12-17T21:19:25Z,Could just close `interfaceDefaultAuthorizer` instead of creating an `authorizers` collection?
545411361,9485,rajinisivaram,2020-12-17T21:21:34Z,The `authorizer` parameter is not used. Can't we just move this into the test method `testAuthorizeByResourceTypeNoAclFoundOverride` above?
545411829,9485,rajinisivaram,2020-12-17T21:22:24Z,can't this be `aclAuthorizer`?
545412639,9485,rajinisivaram,2020-12-17T21:24:09Z,"There is only one authorizer, we could just use it directly instead of creating a Seq"
545413800,9485,rajinisivaram,2020-12-17T21:26:29Z,"Looks like there is opportunity to move some of this stuff into BaseAuthorizerTest, but we can do that in a follow-up later."
545471749,9485,ctan888,2020-12-17T23:26:46Z,"```
    public ResourcePattern(ResourceType resourceType, String name, PatternType patternType) {
        this.resourceType = Objects.requireNonNull(resourceType, ""resourceType"");
        this.name = Objects.requireNonNull(name, ""name"");
        this.patternType = Objects.requireNonNull(patternType, ""patternType"");

        if (resourceType == ResourceType.ANY) {
            throw new IllegalArgumentException(""resourceType must not be ANY"");
        }

        if (patternType == PatternType.MATCH || patternType == PatternType.ANY) {
            throw new IllegalArgumentException(""patternType must not be "" + patternType);
        }
    }
```

I think the ResourcePattern constructor is preventing us passing PatternType.ANY. It's only usable with Filter."
545525163,9485,ctan888,2020-12-18T02:04:45Z,Right. To prevent the phantom problem.
545527187,9485,ctan888,2020-12-18T02:11:19Z,"I think that the ZookeeperClient has a different metric group name. I'm not sure how the name will be used though. And yes, we can do that in a follow-up PR later."
545529131,9485,ctan888,2020-12-18T02:17:51Z,commit b6a766b228034a442e3a6e8b71ecee78eefdbfd3
545529436,9485,ctan888,2020-12-18T02:18:49Z,commit b6a766b
545529534,9485,ctan888,2020-12-18T02:19:03Z,commit b6a766b
545531049,9485,ctan888,2020-12-18T02:24:18Z,commit b6a766b
545531079,9485,ctan888,2020-12-18T02:24:22Z,commit b6a766b
545576162,9485,ctan888,2020-12-18T05:09:08Z,commit b6a766b
545576394,9485,ctan888,2020-12-18T05:10:16Z,commit b6a766b
545576579,9485,ctan888,2020-12-18T05:11:03Z,commit b6a766b
545576700,9485,ctan888,2020-12-18T05:11:32Z,commit b6a766b
545576808,9485,ctan888,2020-12-18T05:11:58Z,"![image](https://user-images.githubusercontent.com/31675100/102577221-7bc51c00-40ac-11eb-89a1-234d0af3f0fd.png)
commit b6a766b"
545577772,9485,ctan888,2020-12-18T05:15:41Z,"        // Check a hard-coded name to ensure that super users are granted
        // access regardless of DENY ACLs."
545577832,9485,ctan888,2020-12-18T05:15:56Z,commit b6a766b
545577865,9485,ctan888,2020-12-18T05:16:03Z,commit b6a766b
545577952,9485,ctan888,2020-12-18T05:16:18Z,commit b6a766b
545578070,9485,ctan888,2020-12-18T05:16:49Z,"Right, though it's only a list of 8.
commit b6a766b"
545578559,9485,ctan888,2020-12-18T05:18:28Z,commit b6a766b
545578734,9485,ctan888,2020-12-18T05:18:58Z,"Yes. Didn't realize the existence of this syntax be4. Thanks.
commit b6a766b"
545579837,9485,ctan888,2020-12-18T05:23:04Z,commit b6a766b
545581171,9485,ctan888,2020-12-18T05:27:40Z,commit b6a766b
545584468,9485,ctan888,2020-12-18T05:39:14Z,"Right. We can bring the ! to the front.
commit 9407b1697d976fc6cff90703573a64f7a3c9f348"
545584783,9485,ctan888,2020-12-18T05:40:19Z,commit b6a766b
545584825,9485,ctan888,2020-12-18T05:40:28Z,Yes. commit b6a766b
545584872,9485,ctan888,2020-12-18T05:40:35Z,Yes. commit b6a766b
545585037,9485,ctan888,2020-12-18T05:41:19Z,"Yes. Renamed to aclAuthorizer. 
commit b6a766b"
545585355,9485,ctan888,2020-12-18T05:42:34Z,"Yes. Remove the Seq construction and make a single class member call.
commit b6a766b"
76054354,1776,ijuma,2016-08-24T13:20:34Z,"The Scala `Enumeration` class has a bunch of problems and we typically use ADTs. An example is `RackAwareMode`.
"
76104237,1776,benstopford,2016-08-24T17:43:14Z,"Changed - thanks bud. 
"
76720190,1776,junrao,2016-08-30T02:00:43Z,"delta is no longer valid. Typo evalutated
"
76720206,1776,junrao,2016-08-30T02:00:53Z,"Is that only used for testing? If so, could we define it under src/test?
"
76720217,1776,junrao,2016-08-30T02:01:03Z,"Do we need this new class? Rate.windowSize() is modified this way to address KAFKA-2443 and KAFKA-2567. It would be better if all sensors are of the same type of Rate.
"
76720224,1776,junrao,2016-08-30T02:01:07Z,"KIP says using comma separated.
"
76720231,1776,junrao,2016-08-30T02:01:11Z,"The KIP says it's comma separated.
"
76720243,1776,junrao,2016-08-30T02:01:17Z,"Perhaps we can avoid calling split(""-"") twice by doing that once first?
"
76720245,1776,junrao,2016-08-30T02:01:25Z,"To be consistent with the existing naming, perhaps the params can be brokerId and brokerConfig?
"
76720249,1776,junrao,2016-08-30T02:01:26Z,"space after if
"
76720258,1776,junrao,2016-08-30T02:01:31Z,"Shouldn't we do this check before line 127?
"
76720265,1776,junrao,2016-08-30T02:01:37Z,"It's clearer if we use Quota.upperBound(limit) in this and the next line.
"
76720272,1776,junrao,2016-08-30T02:01:41Z,"This can be private?
"
76720336,1776,junrao,2016-08-30T02:02:47Z,"Does this guarantee that the minBytes requirement is met? For example, suppose that all replicas are throttled and quota is not exceeded at this point. However, when we call forceComplete(0, we could get fewer bytes than minBytes and return the fetch response prematurely.

Same question in case C on line 94. If the fetch offset is on an old segment from the last segment, we force a return immediately. However, it could be that the quota is violated and we will return an empty response.
"
76720343,1776,junrao,2016-08-30T02:02:51Z,"To be consistent, should broker be brokers?
"
76720349,1776,junrao,2016-08-30T02:02:55Z,"No need to change this line?
"
76720356,1776,junrao,2016-08-30T02:03:04Z,"This can be private?
"
76720360,1776,junrao,2016-08-30T02:03:07Z,"An => A
"
76720364,1776,junrao,2016-08-30T02:03:12Z,"Could we use the defined name for quota.replication.throttled.replicas?
"
76720369,1776,junrao,2016-08-30T02:03:16Z,"Missing license header.
"
76720377,1776,junrao,2016-08-30T02:03:19Z,"extra space after object
"
76720409,1776,junrao,2016-08-30T02:03:46Z,"Perhaps NoQuota is better?
"
76720415,1776,junrao,2016-08-30T02:03:51Z,"For clarity, would it be better to rename leaderReplication and followerReplication to leaderQuotaManager and followerQuotaManager?
"
76720426,1776,junrao,2016-08-30T02:04:01Z,"No need for the package name org.apache.kafka.common.utils. Ditto two lines below.
"
76720437,1776,junrao,2016-08-30T02:04:14Z,"Do we really need fetchResponseProcessingComplete()? Could we just do the logic here in processPartitionData?
"
76720447,1776,junrao,2016-08-30T02:04:22Z,"Is this still needed? If not, it seems that we don't need to expose bound() as a public api in ReplicationQuotaManager.
"
76720504,1776,junrao,2016-08-30T02:05:11Z,"Does this need to be info? Also, don't need to reference logger. There are a few other places like that.
"
76720509,1776,junrao,2016-08-30T02:05:16Z,"typo excedded
"
76720512,1776,junrao,2016-08-30T02:05:21Z,"Hmm, shouldn't we pass in fetch.messageSet.sizeInBytes to quota.isQuotaExceededBy()?
"
76720516,1776,junrao,2016-08-30T02:05:27Z,"Do we have to read from the log again? It seems that we can just set fetch.messageSet to an empty ByteBufferMessageSet.
"
76720525,1776,junrao,2016-08-30T02:05:31Z,"unused import CoreUtils
"
76720529,1776,junrao,2016-08-30T02:05:37Z,"ReadOnlyQuota seems a bit too general. Perhaps sth like ReplicaQuota?
"
76720550,1776,junrao,2016-08-30T02:05:41Z,"bound() => upperBound() to make it clear?
"
76720605,1776,junrao,2016-08-30T02:06:24Z,"This may not be enough since the sensor can be removed after it's inactive for some time (say, throttled replicas are removed). When that happens, sensor will be pointing to an obsolete object. To be safe, we probably need a getOrCreateQuotaSensors() method like in ClientQuotaManager.
"
76720612,1776,junrao,2016-08-30T02:06:26Z,"Could this be a val instead of a method?
"
76720626,1776,junrao,2016-08-30T02:06:32Z,"It seems that this can be private? Also, getQuota() can be just quota().
"
76720637,1776,junrao,2016-08-30T02:06:41Z,"Does this need to be at info? Also, not need to use logger. info() is enough.
"
76720651,1776,junrao,2016-08-30T02:06:52Z,"Hmm, should bound() return int? With things like infiniband, it actually can be legit to set a quota larger than 2GB/sec. 
"
76815241,1776,benstopford,2016-08-30T15:11:18Z,"Our standard mechanism won't permit commas in config, hence I proposed this. I did actually change the KIP. 
"
76815773,1776,benstopford,2016-08-30T15:13:40Z,"Will change to quota.isQuotaExceededBy(fetchMetadata.fetchMinBytes).
"
76816162,1776,benstopford,2016-08-30T15:15:42Z,"I'll fix all logging etc when I have something I'm prepared to merge. This is just a first cut remember. 
"
76817729,1776,benstopford,2016-08-30T15:22:59Z,"yes - that's better. 
"
76817871,1776,benstopford,2016-08-30T15:23:42Z,"That makes sense. Will change. Thanks for the heads up. 
"
76817957,1776,benstopford,2016-08-30T15:24:08Z,"No should be long everywhere. Will change.
"
76819932,1776,benstopford,2016-08-30T15:33:39Z,"Sorry - deleted previous. As I understand it the fetch, here, is just a single partition's data. Thus I'm keeping a running total of throttled bytes in all partitions, checking against the quota to see when the 'proposed' fetch will exceed to. Remember isQuotaExceededBy() doesn't record anything. Let me know if I'm missing something. 
"
76946201,1776,ijuma,2016-08-31T08:38:48Z,"These should be final and probably exposed via an accessor.
"
76947111,1776,ijuma,2016-08-31T08:44:36Z,"I think Ben just meant the `value` parameter to be called `delta`. From an API perspective, it seems like it would make sense to also have a public `checkQuota` method if we expose a `checkQuotaWithDelta`.
"
76948313,1776,ijuma,2016-08-31T08:52:43Z,"Not sure I understand, we support multiple items separated by commas in configs via the `LIST` type.
"
76948517,1776,ijuma,2016-08-31T08:54:18Z,"Not sure if these formatting changes are intended.
"
76948664,1776,ijuma,2016-08-31T08:55:22Z,"Nitpick: we don't usually include the type annotation for simple local variables like this.
"
76948830,1776,ijuma,2016-08-31T08:56:22Z,"It seems to me that the `partitions.map(_.toString)` doesn't do anything since the default behaviour is to call `toString` on each element anyway.
"
76958141,1776,ijuma,2016-08-31T09:55:37Z,"Generally in Scala, you would write this like:

``` scala
value match {
  case s: String => ...
  case _ => throw new ConfigException(...)
```
"
76958968,1776,ijuma,2016-08-31T10:00:47Z,"Nitpick: we don't need the `String` type annotation.
"
76959741,1776,ijuma,2016-08-31T10:05:48Z,"Can we not make a copy of KafkaConfig instead?
"
76959836,1776,ijuma,2016-08-31T10:06:36Z,"`UnboundedQuota` maybe?
"
76960609,1776,ijuma,2016-08-31T10:09:22Z,"Given that the containing class is `QuotaManagers`, it seems like it may be OK to not repeat `QuotaManager` for each field. If we do rename it as per Jun's suggestion, then we probably should rename `client` too.
"
76961281,1776,ijuma,2016-08-31T10:14:22Z,"This class seems a bit inconsistent in that it groups the client replication types in a map, but not the replication ones. It seems like it might be better to either have them all as fields or all in maps. Is there a reason to do it this way instead?
"
76961490,1776,ijuma,2016-08-31T10:16:00Z,"It would be nice if a `Time` instance was passed instead of hardcoding `SystemTime` here.
"
76962421,1776,ijuma,2016-08-31T10:23:20Z,"There should be no `unit.` here.
"
76964270,1776,ijuma,2016-08-31T10:37:15Z,"Nitpick: methods should start with lowercase letter.
"
77510050,1776,benstopford,2016-09-05T11:45:24Z,"Renamed value->delta. Added an (unused) method: public void checkQuotas() 
"
77542394,1776,benstopford,2016-09-05T16:27:23Z,"Dynamic configs don't support commas. I've added a task to look at changing this.
"
77543779,1776,benstopford,2016-09-05T16:44:59Z,"thanks
"
77544027,1776,ijuma,2016-09-05T16:49:06Z,"@benstopford they should do, see `cleanup.policy` for an example. There's a test in `LogCleanerIntegrationTest.testCleansCombinedCompactAndDeleteTopic` that verifies this.
"
77544671,1776,benstopford,2016-09-05T16:56:04Z,"OK
"
77544719,1776,benstopford,2016-09-05T16:56:56Z,"thanks
"
77544895,1776,benstopford,2016-09-05T17:00:00Z,"Yes, good spot. Thank you. 
"
77545048,1776,benstopford,2016-09-05T17:02:02Z,"Agreed. Changed. 
"
77545600,1776,benstopford,2016-09-05T17:08:33Z,"Yeah - it was just used by a test. Have changed to test the ensureValid method instead. 
"
77545753,1776,benstopford,2016-09-05T17:11:11Z,"Done. thanks
"
77545866,1776,benstopford,2016-09-05T17:13:16Z,"sure can
"
77545936,1776,benstopford,2016-09-05T17:14:33Z,"thanks
"
77545961,1776,benstopford,2016-09-05T17:14:59Z,"good idea. thanks. 
"
77546037,1776,benstopford,2016-09-05T17:16:39Z,"added. thanks
"
77546111,1776,benstopford,2016-09-05T17:17:55Z,"thanks
"
77546161,1776,benstopford,2016-09-05T17:18:51Z,"UnboundedQuota sounds good to me. 
"
77549097,1776,benstopford,2016-09-05T18:16:57Z,"Yeah - I should have put a comment on that. It's a historical artefact from the way the original client quota managers worked. I knew I had to rework it. 

It should be a bit better now. 
"
77549138,1776,benstopford,2016-09-05T18:17:51Z,"done
"
77549657,1776,benstopford,2016-09-05T18:30:34Z,"Nope. Hangover from when we had separate throttled replica fetcher threads. Removed.  
"
77554690,1776,ijuma,2016-09-05T20:36:09Z,"You can express the the RHS of `==` as:

``` scala
item.getLogManager.getLog(topicAndPart).map(_.logEndOffset).getOrElse(0)
```
"
77554722,1776,ijuma,2016-09-05T20:36:44Z,"Is this a left-over debugging thing? It seems like you don't need this var at all.
"
77625349,1776,benstopford,2016-09-06T12:36:06Z,"Nope. Removed. 
"
77626384,1776,benstopford,2016-09-06T12:43:41Z,"@ijuma That test verifies you can create a LogConfig object, but doesn't verify that you can update that same log config from the ConfigCommand, which you can't currently.  
"
77627001,1776,ijuma,2016-09-06T12:48:10Z,"Interesting, this is an issue for KIP-71 too then. cc @dguy
"
77632698,1776,dguy,2016-09-06T13:23:27Z,"Hmm, yes indeed. It appears ConfigCommand doesn't like commas. TopicCommand OTOH does.
"
77806141,1776,benstopford,2016-09-07T11:44:55Z,"thanks
"
77806226,1776,benstopford,2016-09-07T11:45:37Z,"OK
"
77806309,1776,benstopford,2016-09-07T11:46:13Z,"good idea. 
"
77855115,1776,benstopford,2016-09-07T16:14:31Z,"Have extracted the logic from the ClientQuotaManager and reused, which, I think, is better!
"
77855598,1776,benstopford,2016-09-07T16:17:02Z,"sure can. 
"
77855719,1776,benstopford,2016-09-07T16:17:42Z,"yes. thanks
"
77855772,1776,benstopford,2016-09-07T16:18:04Z,"will do all these at the end. 
"
77855817,1776,benstopford,2016-09-07T16:18:21Z,"this was changed
"
77856461,1776,benstopford,2016-09-07T16:22:17Z,"I wouldn't typically add accessors here, although I would have some years back. If you're super keen on this kind of stuff I'll change.  
"
77856804,1776,benstopford,2016-09-07T16:24:06Z,"nope. removed
"
77857026,1776,benstopford,2016-09-07T16:25:25Z,"good spot. thank you. 
"
77857237,1776,benstopford,2016-09-07T16:26:34Z,"Cool. thanks. 
"
77862056,1776,benstopford,2016-09-07T16:53:20Z,"Changed
"
77868592,1776,benstopford,2016-09-07T17:31:01Z,"Interesting. So it's not actually used anywhere. The value is just passed into the quota managers, following the same pattern used in the ClientQuotaManager. So maybe we don't need to change KafkaConfig at all? What do you think?
"
77873545,1776,ijuma,2016-09-07T17:59:02Z,"I'm not super-keen on the accessors, it's just the general Kafka convention. I'm keen on the fields being `final` though. Also, do we want `value` and `bound` to be nullable? If not, then they should be lowercase `double`.
"
77880926,1776,benstopford,2016-09-07T18:44:11Z,"OK
"
77881060,1776,benstopford,2016-09-07T18:44:55Z,"thanks
"
77881100,1776,benstopford,2016-09-07T18:45:11Z,"ok
"
77881343,1776,benstopford,2016-09-07T18:46:39Z,"Ah yes. Thanks
"
77881777,1776,benstopford,2016-09-07T18:49:13Z,"It most certainly is. Thank you. I'll do a refactor of these tests in my final pass. 
"
77893695,1776,benstopford,2016-09-07T20:06:14Z,"ok - changed
"
77974298,1776,ijuma,2016-09-08T09:34:33Z,"We are leaking the thread right? If you just want to execute something in the background, you can do something like:

``` scala
import scala.concurrent.{Future, ExecutionContext}
import ExecutionContext.Implicits._
...
Future(codeToRunInTheGlobalThreadPool)
```

If you store the future, you can also await on the result or just completion by using `Await.result` or `Await.ready` (probably not applicable in this case, but worth knowing).
"
78008266,1776,benstopford,2016-09-08T13:45:41Z,"thanks. good to know. 
"
78140580,1776,ijuma,2016-09-09T07:45:12Z,"Nitpick: `value` and `bound` should be lowercase `double` since they can't be null (annoying that Scala and Java are different in this respect so it's a bit confusing when writing code in both languages).
"
78140755,1776,ijuma,2016-09-09T07:46:58Z,"Nitpick: should be `brokers` to be consistent with the other ones? Or should the other ones be made singular?
"
78141062,1776,ijuma,2016-09-09T07:49:48Z,"Nitpick: no return needed and no blocks are needed. Example:

``` scala
try broker.toInt
catch {
  ...
}
```
"
78141122,1776,ijuma,2016-09-09T07:50:16Z,"I'd include the `broker` variable in the message.
"
78141182,1776,ijuma,2016-09-09T07:50:49Z,"I think it would be a bit better if this method returned an `Int` and the caller just wrapped the result in `Seq(...)`.
"
78141363,1776,ijuma,2016-09-09T07:52:32Z,"Seems like this code would be a bit nicer if we had a `val supportedTypes = Set(ConfigType.Topic, ConfigType.Client, ConfigType.Broker)` somewhere.
"
78141952,1776,ijuma,2016-09-09T07:58:20Z,"Maybe something like:

``` scala
configValue.trim match {
  case """" => ...
  case ""*"" => ...
  case _ => ...
}
```

I think you also need to handle errors if the format doesn't match what you expect, right? At the moment, we will get unhelpful `ArrayOutOfBoundsException`s and `NumberFormatException`s.
"
78142068,1776,ijuma,2016-09-09T07:59:32Z,"I see that there's a `ThrottledReplicaValidator`, is that ensuring that things will be in the right format by the time we get here?
"
78142129,1776,ijuma,2016-09-09T08:00:03Z,"Nitpick: space after `:`
"
78142280,1776,ijuma,2016-09-09T08:01:30Z,"Unless I am missing something, I think this should take a `String` since the caller is passing it a `String`. Then you don't need `toString` below.
"
78142342,1776,ijuma,2016-09-09T08:02:13Z,"This message doesn't mention `broker`. Another case where having a definition of supported types would make the code more robust.
"
78142470,1776,ijuma,2016-09-09T08:03:42Z,"Wouldn't it be better to have a `shutdown()` in `quotas` that closes all of them?
"
78142661,1776,ijuma,2016-09-09T08:05:36Z,"It doesn't seem like this is used any more.
"
78142693,1776,ijuma,2016-09-09T08:05:55Z,"Do we still need this?
"
78142931,1776,ijuma,2016-09-09T08:08:06Z,"Maybe this should be `def props(map: Map[String, String])` and then you would use it like:

``` scala
CoreUtils.props(Map(""foo"" -> ""blah""))
```
"
78143052,1776,ijuma,2016-09-09T08:09:27Z,"Thanks for changing the calling code. Can we remove this method then?
"
78143150,1776,ijuma,2016-09-09T08:10:18Z,"Have you seen `TestUtils.produceMessages`?
"
78143190,1776,ijuma,2016-09-09T08:10:43Z,"`unit.` should not be here.
"
78147862,1776,benstopford,2016-09-09T08:52:10Z,"Good point. I've changed to ""Add/Remove entity config for a topic, client or broker"" (command only lets you change one at a time)
"
78147928,1776,benstopford,2016-09-09T08:52:40Z,"thanks
"
78148028,1776,benstopford,2016-09-09T08:53:24Z,"good idea. thanks
"
78148279,1776,benstopford,2016-09-09T08:55:32Z,"ok
"
78149087,1776,benstopford,2016-09-09T09:02:07Z,"definitely!
"
78149712,1776,benstopford,2016-09-09T09:06:56Z,"Yep. that's the idea. 
"
78150344,1776,benstopford,2016-09-09T09:11:59Z,"it just means the tostring has to be called in the ensureValid method, so six of one and half a dozen of the other. unless I'm missing something. 
"
78150565,1776,benstopford,2016-09-09T09:13:27Z,"done (with enumerated list)
"
78150976,1776,benstopford,2016-09-09T09:16:47Z,"ok. 
"
78151082,1776,benstopford,2016-09-09T09:17:29Z,"thanks
"
78151223,1776,benstopford,2016-09-09T09:18:29Z,"yep. it's the definitive list of broker configs you can change. 
"
78151950,1776,benstopford,2016-09-09T09:23:48Z,"have removed & refactored original. was a bit pointless. 
"
78152713,1776,ijuma,2016-09-09T09:30:00Z,"No, you don't need to do that because of the pattern matching clause. You have to pass `s` instead of `value`.
"
78154280,1776,benstopford,2016-09-09T09:42:58Z,"Have changed
"
78154318,1776,benstopford,2016-09-09T09:43:19Z,"thanks
"
78159088,1776,benstopford,2016-09-09T10:28:01Z,"ah, gottcha. thanks
"
78160093,1776,benstopford,2016-09-09T10:37:03Z,"I've consolidated onto a single class, keeping SimpleRate. This encompasses a different approach to fixing KAFKA-2567 whilst being a little simpler to test. 
"
78160165,1776,benstopford,2016-09-09T10:37:41Z,"ok
"
78160327,1776,benstopford,2016-09-09T10:39:04Z,"done. thanks. 
"
78231105,1776,benstopford,2016-09-09T18:54:39Z,"@ijuma @dguy   I've added support for comma separated lists in the ConfigCommand. You specify the list using a square bracket: k1=v1,k2=[v2,v3]
"
78249903,1776,apurvam,2016-09-09T21:12:02Z,"Nitpick: Would it be better to just use `messageSet` from line 118 instead of doing `partitionData.toByteBufferMessageSet` again? This way you save an allocation. 
"
78259626,1776,apurvam,2016-09-09T22:40:41Z,"Would this be better as a typed enum rather than a string? Makes compile time checks stronger. 
"
78265368,1776,apurvam,2016-09-10T00:04:53Z,"This message doesn't match the test. Shouldn't it be ""Throttled Replication of N ms should > M ms""
"
78269010,1776,junrao,2016-09-10T02:01:24Z,"@param timeMs in the comment is no long valid.
"
78269016,1776,junrao,2016-09-10T02:01:46Z,"Having two different rates is going to make it harder for developers to decide which one to use. If this is strictly better than Rate, perhaps we should just change Rate.windowSize(). If this is just for testing, perhaps we can create SimpleRate in test?
"
78269021,1776,junrao,2016-09-10T02:01:54Z,"The comment in line 470 doesn't seem to match the test. Also, space before 0.
"
78269023,1776,junrao,2016-09-10T02:02:00Z,"Is this comment accurate?
"
78269027,1776,junrao,2016-09-10T02:02:03Z,"space before {
"
78269029,1776,junrao,2016-09-10T02:02:07Z,"space after if
"
78269031,1776,junrao,2016-09-10T02:02:13Z,"This doesn't seem to be used?
"
78269033,1776,junrao,2016-09-10T02:02:17Z,"Does --execute block?
"
78269036,1776,junrao,2016-09-10T02:02:20Z,"B/s => bytes/sec ?
"
78269040,1776,junrao,2016-09-10T02:02:33Z,"addThrottle => maybeAddThrottle ?
"
78269041,1776,junrao,2016-09-10T02:02:37Z,"unused import JavaConverters
"
78269052,1776,junrao,2016-09-10T02:03:12Z,"If quota is not exceeded, should we check (accumulatedSize + accumulatedThrottledSize) >= fetchMetadata.fetchMinBytes?
"
78269056,1776,junrao,2016-09-10T02:03:18Z,"extra space before result
"
78269059,1776,junrao,2016-09-10T02:03:22Z,"A few unused imports.
"
78269063,1776,junrao,2016-09-10T02:03:29Z,"the upper bound => the upper bound in bytes/sec ?
"
78269069,1776,junrao,2016-09-10T02:03:42Z,"unused import
"
78269090,1776,junrao,2016-09-10T02:04:27Z,"Would it be better to just check quota.isQuotaExceeded once and make the same decision for each throttled partition on whether it should be included or not?
"
78269091,1776,junrao,2016-09-10T02:04:31Z,"There is already a trace statement in AbstractFetcherThread that logs each fetch request. Do we still need this trace logging?
"
78269148,1776,junrao,2016-09-10T02:07:42Z,"Unused imports MetricName and Rate.
"
78269151,1776,junrao,2016-09-10T02:07:48Z,"AllReplicas instead?
"
78269161,1776,junrao,2016-09-10T02:08:16Z,"The above may not be 100% safe since the metric could be expired and removed between the two statements. It's safer if we save metrics.metrics.get(rateMetricName) to a local val and then check null and update the config.
"
78269165,1776,junrao,2016-09-10T02:08:23Z,"Since sensor() has side effect, it would be clearer if all references to sensor are sensor().
"
78269169,1776,junrao,2016-09-10T02:08:44Z,"The convention is to use trace() which does does the if check already. Also, for string formatting, we are moving towards the s notation instead of format. Ditto in a few other places. 
"
78269173,1776,junrao,2016-09-10T02:08:56Z,"partitions == allReplicas should probably be partitions eq allReplicas?
"
78269178,1776,junrao,2016-09-10T02:09:08Z,"To be consistent, if there is no return value, we just do method() {}.
"
78269179,1776,junrao,2016-09-10T02:09:11Z,"If partitions is empty, should we remove that topic from the map?
"
78269182,1776,junrao,2016-09-10T02:09:18Z,"Is there a reason to remove these? It seems the test is still useful.
"
78269185,1776,junrao,2016-09-10T02:09:21Z,"unused import
"
78269187,1776,junrao,2016-09-10T02:09:25Z,"10 sec seems inaccurate now?
"
78269190,1776,junrao,2016-09-10T02:09:29Z,"Is this really done in a separate thread?
"
78269192,1776,junrao,2016-09-10T02:09:33Z,"1 second seems inaccurate now?
"
78269194,1776,junrao,2016-09-10T02:09:34Z,"20s seems inaccurate now?
"
78269197,1776,junrao,2016-09-10T02:09:37Z,"remove unit
"
78269199,1776,junrao,2016-09-10T02:09:41Z,"Is len 100 or 1?
"
78269200,1776,junrao,2016-09-10T02:09:44Z,"unused imports
"
78269203,1776,junrao,2016-09-10T02:09:49Z,"Could this and next method be private?
"
78269206,1776,junrao,2016-09-10T02:09:56Z,"Do we ""put replicas for all partitions on the not-started brokers""?
"
78269212,1776,junrao,2016-09-10T02:10:06Z,"The text in here and line 172 seem inaccurate.
"
78269223,1776,junrao,2016-09-10T02:10:35Z,"With this, after recording the bytes, the quota could be exceeded? The earlier approach where we use  quota.isQuotaExceed(expectedBytes) seems more conservative and is less likely for quota to be exceeded. Is there a reason not to use quota.isQuotaExceed(expectedBytes)?
"
78298061,1776,junrao,2016-09-11T16:29:39Z,"It seems that we need to handle case C better. If the follower is lagging on old segments and the quota is exceeded, we may return an empty result well before max wait. Returning an empty result early occasionally is fine. However, in this case, it seems that this can happen continuously.
"
78298089,1776,junrao,2016-09-11T16:31:28Z,"There could be a subtle issue with timeout. Say we check the quota and it's exceeded, and we will put the DelayedFetch in the purgatory. If no more bytes are produced, the DelayedFetch has to wait for maxWait. However, quota could become available before maxWait (and we won't get a chance to check). 

One potential way to address this is that if quota is exceeded, we calculate the amount of time that needs to pass before quota is available. We set the timeout in DelayedFetch to the be smaller of that time and maxWait. Then, if DelayedFetch expires, in DelayedFetch.onComplete(), if minBytes is not satisfied and maxWait hasn't been exceeded. We put DelayedFetch to purgatory again.
"
78318335,1776,benstopford,2016-09-12T06:02:51Z,"thanks
"
78320891,1776,benstopford,2016-09-12T06:39:58Z,"Yes. Agree. 
"
78321116,1776,benstopford,2016-09-12T06:43:20Z,"Thanks. Changed.
"
78321156,1776,benstopford,2016-09-12T06:43:49Z,"thanks
"
78321274,1776,benstopford,2016-09-12T06:45:31Z,"good spot. thanks
"
78321313,1776,benstopford,2016-09-12T06:46:10Z,"yes it is. have clarified the test comment a bit.
"
78321694,1776,benstopford,2016-09-12T06:51:30Z,"thanks
"
78321704,1776,benstopford,2016-09-12T06:51:34Z,"thanks
"
78321742,1776,benstopford,2016-09-12T06:52:05Z,"oops - thanks
"
78321998,1776,benstopford,2016-09-12T06:54:54Z,"No. Mistake. Thanks
"
78322063,1776,benstopford,2016-09-12T06:55:42Z,"done
"
78322189,1776,benstopford,2016-09-12T06:57:17Z,"done. thanks
"
78322192,1776,benstopford,2016-09-12T06:57:21Z,"done. thanks
"
78322413,1776,benstopford,2016-09-12T07:00:09Z,"Yes. That's a great spot. Thank you. 
"
78322533,1776,benstopford,2016-09-12T07:01:44Z,"thanks
"
78322678,1776,benstopford,2016-09-12T07:03:28Z,"good spot
"
78322720,1776,benstopford,2016-09-12T07:03:59Z,"thanks
"
78323225,1776,benstopford,2016-09-12T07:09:45Z,"sure thing. 
"
78323280,1776,benstopford,2016-09-12T07:10:27Z,"thanks. removed
"
78323319,1776,benstopford,2016-09-12T07:10:54Z,"thanks
"
78323346,1776,benstopford,2016-09-12T07:11:08Z,"ok. cool
"
78325258,1776,benstopford,2016-09-12T07:31:41Z,"I've changed this both here and also in the ClientQuotaManager (which had the same logic)
"
78325391,1776,benstopford,2016-09-12T07:33:06Z,"Yes - good call. Thanks
"
78326425,1776,benstopford,2016-09-12T07:43:20Z,"Thanks for the heads up. Changed
"
78327387,1776,benstopford,2016-09-12T07:51:33Z,"OK - makes sense. 
"
78328609,1776,benstopford,2016-09-12T08:02:26Z,"OK - thanks for the heads up - have changed tidied up where this was wrong elsewhere too. 
"
78333596,1776,ijuma,2016-09-12T08:40:30Z,"Probably unintended change?
"
78333653,1776,ijuma,2016-09-12T08:40:58Z,"Probably unintended change?
"
78404954,1776,benstopford,2016-09-12T16:22:47Z,"No, I missed that somehow. Have added it now. 
"
78406716,1776,benstopford,2016-09-12T16:32:52Z,"How strange. That was removed when I merged from my previous branch. Good spot. 
"
78407140,1776,benstopford,2016-09-12T16:35:15Z,"thanks
"
78407153,1776,benstopford,2016-09-12T16:35:20Z,"thanks
"
78407241,1776,benstopford,2016-09-12T16:36:03Z,"It certainly isn't! Thanks. 
"
78407355,1776,benstopford,2016-09-12T16:36:44Z,"thanks
"
78407542,1776,benstopford,2016-09-12T16:38:05Z,"thanks
"
78407632,1776,benstopford,2016-09-12T16:38:36Z,"thanks
"
78407763,1776,benstopford,2016-09-12T16:39:17Z,"thanks
"
78407832,1776,benstopford,2016-09-12T16:39:39Z,"thanks
"
78408120,1776,benstopford,2016-09-12T16:41:14Z,"sure can
"
78408614,1776,benstopford,2016-09-12T16:44:06Z,"clarified. thanks
"
78410438,1776,benstopford,2016-09-12T16:54:31Z,"Yes - the idea seemed like a good one, but it led to some complexities. The main problem was that the ratio of the RequestSize:QuotaSize correlated with the amount the throttle would be undercut. Also, if you asked for more than the quota in a single request you could never make progress. The upshot was that the behaviour was a little unintuitive, and the simpler mechanism seems to work well on aggregate. 
So after thinking about it for a while I decided to ditch the isExceededBy(bytes) approach. Keep it simple ... and this way it more closely matches the way the follower works more closely. I hope that makes sense. 
"
78411666,1776,benstopford,2016-09-12T17:02:01Z,"I did consider this issue. You are right that we could make the leader algorithm more responsive by altering the timeout (I didn't think of that - good idea). 
For now I'm inclined to raise a jira for this as a future enhancement. It shouldn't affect throttling significantly. The extra delay will even out over time.  
Also, this problem exists on the follower too so the optimisation is only of value for the leader side of the throttle. 
The main concern I have is actually the lack of smarts on the follower, particularly if throttled partitions enter the ISR, as the follower logic is very basic. 
"
78412609,1776,benstopford,2016-09-12T17:07:49Z,"thanks
"
78412775,1776,benstopford,2016-09-12T17:08:59Z,"thanks
"
78432823,1776,benstopford,2016-09-12T18:56:06Z,"Good spot. I think this should be as simple as:

```
   if (!(quota.isThrottled(topicAndPartition) && quota.isQuotaExceeded()))
         return forceComplete()
```

but i'll need to write a test which will take a little time. 
"
78783831,1776,junrao,2016-09-14T16:25:33Z,"Yes, the follower has a similar issue. There is already logic to add a delay per partition. So, if a partition is throttled in the follower, we can calculate a delay from quota and delay the partition accordingly. We probably also need to change the backoff logic in AbstractFetcherThread a bit. Instead of always backing off for a fixed amount of time, it's probably better to backoff based on the smallest delay among all partitions.

I agree that this is probably not a common issue. It only becomes a big issue if the maxWait or replica backoff time are configured very large (say close to the metric window \* sample size). So, we can address that in a followup jira.
"
78865421,1776,junrao,2016-09-14T23:43:57Z,"processConfigChanges() only gets called if there is overridden config on brokerId in ZK. So, if that doesn't exist, it seems that we won't apply the static default ThrottledReplicationLimit in broker property file?
"
78865431,1776,junrao,2016-09-14T23:44:05Z,"extra space after :
"
78865451,1776,junrao,2016-09-14T23:44:17Z,"throttle should be type long?
"
78865458,1776,junrao,2016-09-14T23:44:22Z,"convention: use trace().
"
78865572,1776,junrao,2016-09-14T23:45:23Z,"unused import
"
78865576,1776,junrao,2016-09-14T23:45:27Z,"typo bakc
"
78865588,1776,junrao,2016-09-14T23:45:34Z,"Perhaps we should just test excluding the property, which is consistent with the test in line 484?
"
78865602,1776,junrao,2016-09-14T23:45:40Z,"Should we make limit long since ThrottledReplicationRateLimitProp is of type long?
"
78865609,1776,junrao,2016-09-14T23:45:44Z,"take => taken
"
78865626,1776,junrao,2016-09-14T23:45:49Z,"Are we using a separate thread?
"
78865633,1776,junrao,2016-09-14T23:45:55Z,"Are we using a separate thread?
"
78865643,1776,junrao,2016-09-14T23:46:00Z,"Should this be private?
"
78893471,1776,junrao,2016-09-15T05:47:04Z,"I think we can just get rid of the static config ThrottledReplicationRateLimitProp and just rely on the dynamic broker level config, which is more flexible.
"
78959050,1776,benstopford,2016-09-15T13:10:37Z,"Yes - I raised a bug for this, but it's not a big deal so lets lose the config it as you say. 
"
78959367,1776,benstopford,2016-09-15T13:12:20Z,"thanks. done
"
78959375,1776,benstopford,2016-09-15T13:12:24Z,"thanks. done
"
78959967,1776,benstopford,2016-09-15T13:15:33Z,"thanks
"
78959976,1776,benstopford,2016-09-15T13:15:37Z,"thanks
"
78961026,1776,benstopford,2016-09-15T13:20:52Z,"We have that already on line 428. Are you concerned about it being there, or objecting to the format of the test?
"
78961219,1776,benstopford,2016-09-15T13:21:56Z,"ok
"
78979265,1776,benstopford,2016-09-15T14:38:40Z,"The nice thing about having it in the config is validation. If we remove the prop, we'd probably need another list of dynamic broker configs somewhere. What do you think?
"
78994469,1776,junrao,2016-09-15T15:38:36Z,"Yes, we are doing similar things in KIP-55. We are deprecating the static client quota configs in the broker in favor of dynamic quotas. So, for new broker configs, if it can be made dynamically, it seems it's less confusing to also add a static config in the broker property.
"
79071084,1776,apurvam,2016-09-15T21:57:41Z,"You should log a message here, with information of the old and new quota topic for the broker/topic/partition being modified. Otherwise these dynamic changes without any auditing will be impossible to debug.
"
79080424,1776,benstopford,2016-09-15T23:03:46Z,"It should be visible from the logging in the DynamicConfigManager (line 106). Do you not see that?
"
79080650,1776,apurvam,2016-09-15T23:05:40Z,"Yes, I see it now. Sorry for the false alarm.
"
1379392210,14690,kirktrue,2023-11-01T22:26:33Z,"Now that I'm noticing, is this a public API violation? It's not in `internals` and we're adding a `public` method "
1379396122,14690,kirktrue,2023-11-01T22:31:10Z,"Nit: more idiomatic:

```suggestion
        autoCommitState.ifPresent(AutoCommitState::resetTimer);
```"
1379398154,14690,kirktrue,2023-11-01T22:34:25Z,"I'm probably confused by taking the naming of the `onHeartbeatRequestSent` method too literally, but the request hasn't been _sent_, only _enqueued_. Do we need to call this when it's really _sent_ or is enqueued ""good enough?"""
1379399402,14690,kirktrue,2023-11-01T22:36:41Z,"This is the case where the consumer is not in a group _presently_, right? A consumer without a configured group ID wouldn't get to this point, would it?"
1379400497,14690,kirktrue,2023-11-01T22:38:28Z,"As I understand, this is saying the consumer will leave the `ACKNOWLEDGING_RECONCILED_ASSIGNMENT` state as soon as the next heartbeat is sent off, rather than the next heartbeat is received, right? What happens if that heartbeat request gets lost?"
1379402695,14690,kirktrue,2023-11-01T22:42:06Z,"Sorry to retread this: how are `LEAVING_GROUP` and `SENDING_LEAVE_REQUEST` different? They both will call the `onPartitionsLost()` callback first, right?"
1379404682,14690,kirktrue,2023-11-01T22:45:53Z,Does the consumer transition from `JOIN` to `RECONCILING` mean that the first heartbeat response after the join request will (may?) contain an assignment?
1379407419,14690,kirktrue,2023-11-01T22:50:32Z,`targetAssignment()` looks to only be used by unit tests at the moment. Does it make sense to remove it from the interface and leave it as a method on the implementation only?
1379408357,14690,kirktrue,2023-11-01T22:52:23Z,Can this be called directly via the `ApplicationEventProcessor` when the consumer sends an event to the network thread to state it is closing? The only other place I see it called at the moment is from a unit test.
1379413852,14690,kirktrue,2023-11-01T23:02:28Z,"The IDE is showing this line with a warning because it's invoking `get()` without a `isPresent()` check. I know that it's being set in `setTargetAssignment` right above, but can we refactor this code to make it more obvious to the compiler (and any humans reading it)?

Here's a quick take:

 ```suggestion
            // Take new target assignment received from the server and set it as targetAssignment
            // to be processed. Following the consumer group protocol, the server won't send a
            // new target member while a previous one hasn't been acknowledged by the member, so
            // this will fail if a target assignment already exists.
            if (targetAssignment.isPresent()) {
                transitionToFailed();
                throw new IllegalStateException(""Cannot set new target assignment because a "" +
                    ""previous one pending to be reconciled already exists."");
            }

            log.info(""Member {} accepted new target assignment {} to reconcile"", memberId, assignment);
            transitionTo(MemberState.RECONCILING);
            targetAssignment = Optional.of(assignment);
            reconcile(assignment);
```

That way all the logic is together and we can remove `setTargetAssignment()`, too. Just a thought."
1379415462,14690,kirktrue,2023-11-01T23:05:54Z,Would you mind making a constant for `-1` just so it's easier to grep through the code and find places where the consumer is in this state?
1379415743,14690,kirktrue,2023-11-01T23:06:34Z,"Same here, regarding the magic numbers."
1379418198,14690,kirktrue,2023-11-01T23:11:23Z,"I guess it's OK to use `ConsumerMetadata` directly like this as we ""own"" updating it on the consumer network thread."
1379419317,14690,kirktrue,2023-11-01T23:13:40Z,"Yes, we'll have to resolve how the callbacks fit into this model that uses `Future`s, because the callbacks need to be invoked on the application thread."
1379420036,14690,kirktrue,2023-11-01T23:15:16Z,I made a comment up above about removing `targetAssignment()` from the `MembershipManager` interface because it was only used for testing. Does the removal of this statement imply that it will be used in non-testing later?
1379420603,14690,kirktrue,2023-11-01T23:16:23Z,"Would it be ""wrong"" to have the method implementation log the message instead of throwing an error?"
1379420811,14690,kirktrue,2023-11-01T23:16:48Z,This makes sense!
1379422902,14690,kirktrue,2023-11-01T23:20:46Z,"Two questions:

1. `Topic` wants to unify the topic ID and topic name information, but it explicitly allows either to be `null`. Technically, since there are no checks, both values could be `null`. Is that intentional?
2. Notwithstanding the above, can we add this class without a KIP? If not, can we move it to `o.a.k.common.internals`?"
1379423191,14690,kirktrue,2023-11-01T23:21:23Z,"This would throw a `NullPointerException`, wouldn't it?"
1380063605,14690,AndrewJSchofield,2023-11-02T13:00:44Z,"I see what @kirktrue means, but this package is not part of the public javadoc. The closest that the public interface has to exposing this kind of information is `org.apache.kafka.common.Cluster`."
1380073097,14690,AndrewJSchofield,2023-11-02T13:07:41Z,"I'm surprised that the previous valid states for FATAL is not all of the other states, such as FENCED."
1380077957,14690,AndrewJSchofield,2023-11-02T13:11:42Z,`TopicIdPartition`? You do know the topic IDs and they're relevant for guarding against topics which have been recreated.
1380079966,14690,AndrewJSchofield,2023-11-02T13:13:15Z,"I suggest ""skip sending the heartbeat to the coordinator"". I do like the method naming convention you're establishing with ""skip"" in the name."
1380080844,14690,AndrewJSchofield,2023-11-02T13:14:02Z,"Just ""LEAVING"" would match the other states better."
1380083241,14690,AndrewJSchofield,2023-11-02T13:16:03Z,`transitionToJoining`?
1380083590,14690,AndrewJSchofield,2023-11-02T13:16:20Z,`transitionToFatal`?
1380086392,14690,AndrewJSchofield,2023-11-02T13:18:34Z,"Personally, I'd capture the value of `state()` in a local variable and then use it twice, rather than calling the method twice. There are a couple of instances of this."
1380088445,14690,AndrewJSchofield,2023-11-02T13:20:16Z,"I think so. Also, the set return by `ConsumerMetadata` is immutable."
1380090093,14690,AndrewJSchofield,2023-11-02T13:21:37Z,I think that theoretically it could and the action you've proposed is correct.
1380092591,14690,AndrewJSchofield,2023-11-02T13:23:36Z,Interesting :) 
1380121804,14690,AndrewJSchofield,2023-11-02T13:31:50Z,I would say that it needs a KIP in this package.
1380144482,14690,AndrewJSchofield,2023-11-02T13:35:57Z,`Objects.hashCode()` is your friend.
1380218718,14690,lianetm,2023-11-02T14:17:40Z,Totally! I missed that
1380372614,14690,lianetm,2023-11-02T15:53:22Z,"That was considering that the member could only got to FATAL from states where it sends heartbeat, when receiving non-retriable errors in the heartbeat response (and states like FENCED or LEAVING do not send heartbeat)"
1380393617,14690,AndrewJSchofield,2023-11-02T16:06:13Z,That's OK. I was just asking an innocent question. Makes sense to me.
1380592053,14690,lianetm,2023-11-02T17:59:27Z,"I expect that Topic class will be exposed at some point, as we spread the usage of topic ID in the client code, that's why I added it there, but totally missed that it could then require a KIP. So I just moved it to the internal package, as it is truly only internal for now. "
1380659584,14690,lianetm,2023-11-02T19:06:20Z,"I moved it to the internals for now, as it's truly for internal use (even though I expect we might want something similar later on as we use topicId more in the client code)."
1380660843,14690,lianetm,2023-11-02T19:07:45Z,"You're right, this is the case where a consumer, with a groupId, is not part of the group (either it hasn't called subscribed, or it called unsubscribe)"
1380683789,14690,lianetm,2023-11-02T19:31:50Z,"@dajac this is the check we discussed earlier about target assignment and subscription. Leaving it for now only so you can see exactly what it is, but we can remove it then if we still think it should be better to let the broker drive this."
1380693973,14690,lianetm,2023-11-02T19:43:42Z,"Good catch, I needed it public at some point but it ended up not being needed in the end. So putting it back to package-private and ""Visible for testing"""
1380694833,14690,lianetm,2023-11-02T19:44:42Z,"Totally, it seemed it was needed here at some point but not anymore. Removing it & cleaning up, thanks!"
1381667636,14690,dajac,2023-11-03T13:17:23Z,nit: I think that we tend to indent with 4 spaces in this case.
1381676706,14690,dajac,2023-11-03T13:25:32Z,"I think that this is not enough because we only need to send it if it has changed and we also need to re-send them on failure.

I was thinking about introducing a stateful Builder object for the request which remembers the last fields sent out and decider whether the fields must be set or not. On errors, we could just reset the builder to re-send all fields.

I think that could possibly always set all the fields in this PR and tackle this separately as we need to solve it more generally. What do you think?"
1381679771,14690,dajac,2023-11-03T13:27:57Z,I am not a big fan of the `NOT_IN_GROUP` name because the consumer could still have a group id configured and commit offsets to a group. This is why I used `Unsubscribed` earlier. I wonder if we could find a better name... What do you think?
1381680532,14690,dajac,2023-11-03T13:28:35Z,Could we extend the description to explain what we do in this state? I would also do it for the others.
1381683004,14690,dajac,2023-11-03T13:30:21Z,"So I I understand it correctly, the member transitions to this state as soon as the reconciliation is done and then transition to Stable as soon as the ack is sent out. Did I get it right?"
1381683686,14690,dajac,2023-11-03T13:30:53Z,Do we call `lost` in this case?
1381686009,14690,dajac,2023-11-03T13:32:50Z,I also wonder if we should call it `Acknowledging` to follow the naming of the other states. Thoughts?
1381688086,14690,dajac,2023-11-03T13:34:36Z,"My understanding is that `LEAVING` do the pre-leaving steps (e.g. pause partitions, commit offsets, etc) while `SENDING_LEAVE_REQUEST` sends out the actually leave request.

Perhaps, using `PREPARE_LEAVING` and `LEAVING` would make it clearer. Thoughts?"
1381690930,14690,dajac,2023-11-03T13:36:52Z,I think that we already define them in the `ConsumerGroupHeartbeatRequest` class. We could reuse them.
1381692974,14690,dajac,2023-11-03T13:38:13Z,`targetAssignment` seems to be accessible directly. Do we really need to pass it here?
1381801641,14690,dajac,2023-11-03T14:44:53Z,"Okay. I think that we could get into this situations in two cases. 

1. An assigned topic was just created and the metadata request got to a broker unaware of it yet. In this case, ignoring it means that the newly created topic will never be consumed by the member. Or, at least, it won't be consumed until another assignment is received. In the current implementation, I think that the fetcher will keep retrying on those topics. Ideally, we would need something similar here.

2. An assigned topic was just deleted before the member got the chance to get the metadata. This is somewhat the opposite case.

In the case of 1., we could argue that we should just keep retrying until it succeeds and it should eventually succeed. In this case of 2., it would never succeed if the topic is deleted so the member will never send an ack and will eventually be kicked out from the group. To make it worst, the member won't receive an new assignment without the deleted topic because the previous assignment is not ack'ed. The issue is that there is no way to differentiate the two cases.

Ideally, we should set the subscription based on the topic ids instead of the topic names. However, this does not resolve the need to have the topic names for the callbacks. There are really annoying...

Another thing that I wanted to point out is that it is not all or nothing. For instance, the member could get 10 partitions assigned to him and only one is unresolvable. 


"
1381802659,14690,dajac,2023-11-03T14:45:40Z,"As discussed offline, I would remove this. In my opinion, the member should just follow what the coordinator provide and should not try to be too smart here."
1381802850,14690,dajac,2023-11-03T14:45:49Z,nit: Extra line.
1381810747,14690,dajac,2023-11-03T14:52:17Z,I wonder if we should also check if the target assignment is still the same one. I am not sure if it is possible but could we have a callback coming really late and the state machine could have already transitioned to fenced and rejoined the group and got a new assignment so be in reconciling state again?
1381814507,14690,dajac,2023-11-03T14:55:03Z,"One concern that I have with using the manager directly is that it does not seem to populate the metadata cache afterwards. So, we would resolve topics once here and then the fetcher would redo it because the metadata cache does not have the topics. This is not ideal."
1381819971,14690,dajac,2023-11-03T14:58:48Z,"From the KIP:
> Consumer#enforceRebalance will be deprecated and will be a no-op if used when the new protocol is enable. A warning will be logged in this case."
1382108731,14690,philipnee,2023-11-03T18:50:20Z,"We should just return early here.  

`return CompletableFuture.completedFuture(null);`"
1382450219,14690,philipnee,2023-11-04T19:50:27Z,I believe the reconciliation result is completed by the main thread.
1382665763,14690,lianetm,2023-11-05T22:51:45Z,`Unsubscribed` describes the state better to me too. Renamed it and added comments explaining better how the member gets there and what it can do while in this state. 
1382666152,14690,lianetm,2023-11-05T22:54:17Z,"You're right, this state is only until the next HB is sent, and then the member moves on. If the HB with the ack is lost, what happens is that, when the rebalance timeout expires, the broker will re-assign the partitions to another member and kick this one out of the group.  "
1382666539,14690,lianetm,2023-11-05T22:56:18Z,"You're right about what each does, and agree with the `PREPARE_LEAVING` and `LEAVING`. Renamed them and updated comments, it looks clearer. "
1382666673,14690,lianetm,2023-11-05T22:57:15Z,"Removed all static membership logic for now, given that is it not supported yet. "
1382666792,14690,lianetm,2023-11-05T22:57:56Z,Done
1382666862,14690,lianetm,2023-11-05T22:58:42Z,Totally. Done. 
1382667315,14690,lianetm,2023-11-05T23:01:42Z,"Good point. I reused the -1, and removed the static membership constant and logic from our side given that it is not supported yet. "
1382667377,14690,lianetm,2023-11-05T23:02:08Z,"You're right, not needed, removed."
1382668541,14690,lianetm,2023-11-05T23:08:59Z,"Good point, this was not the right way so I updated how metadata is used here, all based on the metadata object now (request update when needed, and get notified when it happened). This ensures that the centralized cache is updated, and this is actually how other managers interact with metadata (ex. `OffsetsRequestManager` when it needs metadata to find leaders). "
1382724053,14690,lianetm,2023-11-06T02:20:05Z,"Exactly, that's the case the transition is covering. "
1382728096,14690,lianetm,2023-11-06T02:31:49Z,"Makes sense, I will include the changes for the initial approach sending all, to tune it afterwards and send only what's needed"
1382742662,14690,lianetm,2023-11-06T03:08:57Z,"Done. I updated them all, explaining more of what the member does in each and the relationship with the HB requests content and timing."
1382772899,14690,lianetm,2023-11-06T04:35:39Z,"Yes, we do, based on the epoch (epoch > 0 => onPartitionsRevoked, else onPartitionsLost). 
The prepare leaving will trigger the `onPartitionsRevoked` in most of the cases I expect, but if the member is not in the group anymore it calls `onPartitionsLost`. I was mainly thinking about the edge case where a member gets fenced, and while FENCED (ex. waiting for user callback to complete), there is a call to unsubscribe. At that point the member would attempt to leave the group, but it is not currently an active member, so will call `onPartitionsLost`. Makes sense?
That being said, I realize that even though the implementation supports that case, it was not a valid transition, so I just added it. Will add tests for it shortly. "
1383374287,14690,lianetm,2023-11-06T13:59:51Z,"The callback execution will be completed in the main thread (when implemented), but this is the reconciliation result that completes here in the background thread, that involves not only the callbacks. It involves 3 main async operations:
- metadata (to resolve topic names for assignment)
- commit
- user callbacks (executed in the main thread) "
1383433245,14690,lianetm,2023-11-06T14:32:23Z,"Thanks for confirming @AndrewJSchofield . After the change to integrate this with the centralized metadata object and cache, we do achieve this behaviour (we keep retrying until all assigned topic IDs are found in metadata) "
1383435641,14690,lianetm,2023-11-06T14:34:08Z,"Totally, all changed, thanks!"
1383513687,14690,lianetm,2023-11-06T15:26:08Z,"Done, I updated it back to sending all fields for now. I will follow up in a next PR to send only what's needed. 

I expect that it will be the existing `HeartbeatState` the one to extend, to be able to build a `ConsumerGroupRequestData` based on the last one sent, the member info, and the subscription info (determine difference to send only what changed, and send all on the failed attempts that it already handles for retry/backoff) "
1383532710,14690,lianetm,2023-11-06T15:36:59Z,"@dajac I think we should have both points solved now with the new metadata approach.

  1. We continue to request metadata updates as long as there are assigned topic IDs not resolved ([here](https://github.com/apache/kafka/blob/1c571fb9ca419f2916d663eaae6bd06b972b90bc/clients/src/main/java/org/apache/kafka/clients/consumer/internals/MembershipManagerImpl.java#L878)). This will solve the first case as you described.

  2. We keep a local cache of assigned topicId->topicNames for assigned topics that have been previously resolved. If topic is not in metadata when it comes in a next target assignment it will be resolved from the local cache ([here](https://github.com/apache/kafka/blob/1c571fb9ca419f2916d663eaae6bd06b972b90bc/clients/src/main/java/org/apache/kafka/clients/consumer/internals/MembershipManagerImpl.java#L609)), as it is a known/assigned topic. This will solve case 2.
  
Thoughts? 
"
1383551733,14690,lianetm,2023-11-06T15:48:15Z,"Just for the record, it wasn't integrated initially when you took a look but it was added in this PR. And yes, you're right, it is integrated via the ApplicationEventProcessor and the HBManager"
1383566278,14690,philipnee,2023-11-06T15:57:47Z,If the reconciliationResult is completed by the main thread then I think the whenComplete block is also completed by the main thread.
1383599082,14690,lianetm,2023-11-06T16:19:09Z,"I will share thoughts on this on our next sync with @dajac and you @AndrewJSchofield, as this is interesting. Just for the record, I do agree that spreading topic IDs is the right way forward, and we do have them now in the assignment path, but I realized when exploring this suggestion that it is a much bigger change if we move away from `TopicPartition`, and we're not ready for it at this point (mainly tricky/ugly because not all paths support topicID yet but most of them access/update the shared `subscriptionState` component and `TopicPartition`)

At this point looks to me that we're better of making use of the topic IDs kind of ""on the side"", like we're doing now in the `membershipManager`, that keeps the assigned topicId/names but still uses the same `TopicPartition`. This is the same approach followed by the `Fetch` and `Metadata` paths, that introduced topic Ids in a similar ""on the side"" way.

There's interesting food for thought here anyway.

   "
1383610788,14690,lianetm,2023-11-06T16:27:55Z,"I'm using `send` to align with how the HB manager was naming the existing `onSendAttempt` and such, and I think that as seen from the manager point of view it is enough, the membership manager only needs to know that the request is sent out of the HB manager to transition accordingly. It is not literally sent over the network. I would leave send/sent to keep it consistent between the 2 managers, but let me know if you think it would be clearer differently. "
1383623183,14690,dajac,2023-11-06T16:33:24Z,"@lianetm I think that a client could still get an unresolvable topic id and be stuck in the reconciling state. For instance, it could happen if the member sees a topic id for the first time and the topic id is deleted just before it could resolve it."
1383699023,14690,philipnee,2023-11-06T17:21:25Z,is there any reason we want to combine onPartitionsLost and onPartitionsRevoke into a single function?  Couldn't we directly invoke `invokeOnPartitionsRevoke` or `invokeOnPartitionsLost`? i.e. by the time the consumer is fenced or would know we need to invoke onPartitionsLost
1383701313,14690,philipnee,2023-11-06T17:23:02Z,"As previously mentioned, it is clearer to directly invoke `invokeOnPartitionsLost` here. Is there a case we don't invoke onPartitionsLost?"
1383780173,14690,philipnee,2023-11-06T18:21:36Z,I don't think the exception will actually be thrown here.
1383786483,14690,philipnee,2023-11-06T18:26:42Z,"Looking at the current consumer, I think it can be quite complicated in this implementation.  Maybe what we should do is to invoke the listener on the spot, then send an even to the background thread to leave group."
1383791886,14690,philipnee,2023-11-06T18:32:02Z,I'm guessing the idea is we need to ignore the backoff and heartbeat.
1383959347,14690,lianetm,2023-11-06T20:42:13Z,"Yes, as we discussed for the states that would send heartbeat without waiting for the interval (sending ack for an assignment and leave group requests basically)"
1383962388,14690,lianetm,2023-11-06T20:44:53Z,"You're right, it should happen in the poll to maintain the current contract. I will just remove it since we don't execute callbacks yet, and it should be included in the PR that introduces the callback execution."
1383968073,14690,philipnee,2023-11-06T20:49:52Z,Can we use the standard toString format? Topic(topicId=...)
1383974744,14690,philipnee,2023-11-06T20:54:17Z,can we just fail the consumer?
1383979443,14690,lianetm,2023-11-06T20:58:28Z,"The reason is the leave group logic. On leave group it could be lost or revoked, depending on the epoch. On fence or fatal is always lost. I was just reusing the same func for convenience, but will change the fence and fatal transitions to directly invoke the `onPartitionsLost` just to make the intention clearer. "
1383983917,14690,lianetm,2023-11-06T21:02:21Z,"The KIP states that we should do exactly this, and actually the consumer would stay functional.
`Consumer#enforceRebalance will be deprecated and will be a no-op if used when the new protocol is enable. A warning will be logged in this case.` 
Do you have a concern that I may be missing?"
1383986418,14690,lianetm,2023-11-06T21:04:19Z,"Actually I intentionally followed the standard of the TopicPartition and TopicIdPartition toString implementations, since this new Topic class is kind of a sibling, makes sense?"
1383995398,14690,philipnee,2023-11-06T21:11:42Z,Thanks for the clarification!
1384004581,14690,philipnee,2023-11-06T21:19:24Z,Thanks for the clarification!
1384016549,14690,philipnee,2023-11-06T21:29:38Z,"I think unsubscribe() actually blocks on callback invocation and throw if possible.  Instead of putting the logic in whenComplete, it seems like we should try to wait till the callback completes then throw if needed.  I assume we want to maintain this behavior for the async consumer."
1384076305,14690,philipnee,2023-11-06T22:00:43Z,"I think this KafkaException might not be in the right place because the rebalance listener needs to be invoked on the mainthread, probably before sending out the event.  I wonder if we could just remove this whenComplete and rely on the background thread to log the failures during the leave group event.  If there's a fatal exception being thrown there, it seems the sensible way is to enqueue to the BackgroundEventQueue and handle in the poll.  wdyt?

In the current code path, I think only exceptions can only be thrown in `onLeavePrepare`."
1385059304,14690,lianetm,2023-11-07T14:59:12Z,"Agree that the unsubscribe blocks on the callbacks, but since we don't have the implementation for how callbacks are going to be executed on this PR this unsubscribe is still not using it. It should come when we nail the implementation details on the follow-up PR (will depend on how we end up doing it, maybe blocking not here, but on the subscribe/unsubscribe events)

As for the exception, it will originate in the Application thread, where the callback is executed, and it should only be returned to the user when it calls poll, to maintain the current behaviour, so I removed it from here to stay consistent and leave all logic related to the callback execution out, I see the confusion introduced.

The when complete should stay because it represents a concept we need, un-related to callbacks: we do need to update the `subscriptionState` only when the Unsubscribe event completes (HB to leave group sent to the broker). We need it know to make sure we are able to run unsubscribe, no callbacks, sending leave group, and clearing up the subscription state after sending the request. 

Trying to leave it in a consistent state with no callbacks. Follow-up PR should introduce implementation for executing them, blocking appropriately on the execution, and throwing the exceptions."
1385082158,14690,lianetm,2023-11-07T15:14:06Z,"Agree, done here. Note my answer above though, about the cases where we do need revoke or lost, that one should stay. "
1385085418,14690,lianetm,2023-11-07T15:15:58Z,"Just for the record, as discussed offline, none of these futures are expected to be completed in the main thread. This is a reconciliation future, which is much more than the callbacks (metadata, commit, callbacks). And callbacks, when implemented, will be based on events shared between the App thread and the Background thread (not based on futures from one thread complete on the other, as that would be problematic)"
1385350785,14690,lianetm,2023-11-07T18:18:35Z,"Agree. Just for the record, we're trying to figure out how to properly handle the cases where metadata wouldn't be available for a target assignment (permanently when topic deleted, or even just temporarily). With the current shape where the client attempts to reconcile the full target assignment, what happens now is rather disruptive, as the member would be kicked out of the group after rebalance timeout expires. "
1387282757,14690,philipnee,2023-11-08T22:48:58Z,This is just a transient state for the member to send the leave group heartbeat with epoch -1/-2 (dynamic/static membership) right?
1389220863,14690,dajac,2023-11-10T10:31:26Z,Do we still need the changes in this class? It seems that we don't use them anymore. I have the same question for the new `Topic` class.
1390582429,14690,lianetm,2023-11-13T03:01:07Z,"You're right, not needed anymore (all metadata interaction is now based on the centralized metadata cache). All removed. "
1390583612,14690,lianetm,2023-11-13T03:03:33Z,"Yes, similar to the ACKNOWLEDGING in the sense that they are just a way to indicate that a heartbeat must be sent without waiting for the interval, and as soon as the request is sent the member transitions out of the state. "
1390590490,14690,lianetm,2023-11-13T03:20:45Z,"Totally valid point. I fixed it, but comparing assignments seemed more complicated and harder to reason about with the current approach. Now the target assignment is kind of a moving target, it can be modified anytime not only from the server, but also from metadata updates. 

So back to the problem of making sure that delayed reconciliations are not applied after a member rejoins, I added just a check based on the current member ID, to identify that a reconciliation completed but the member is already re-joining. What do you think? (whenever there are no rejoins in the picture, I expect that the RECONCILING state check alone should be enough given that reconciliations are always applied sequentially)"
1391828060,14690,kirktrue,2023-11-14T00:01:15Z,"The closure that the application thread is passing to `whenComplete()` will be run in the background thread, right? The closure is modifying the `SubscriptionState`, it shouldn't cause any problems, but still...

Since we have access to the `SubscriptionState` in the background thread already, can the background thread just update the `SubscriptionState` directly?"
1391862343,14690,kirktrue,2023-11-14T01:04:51Z,"There's another `subscribeInternal()` for the topic pattern path. We want this there too, right?"
1391875622,14690,kirktrue,2023-11-14T01:30:29Z,"I apologize if it's here somewhere, but I don't see where we ""register"" the membership manager with the cluster resource listeners."
1391875744,14690,kirktrue,2023-11-14T01:30:45Z,Good call!
1391876960,14690,kirktrue,2023-11-14T01:33:12Z,"The intention of the `CompleteableApplicationEvent` was to have a way for the consumer to block on the results of operations performed in the background thread. Since the `Consumer.unsubscribe()` API call is non-blocking, I'm thinking this should be a subclass of `ApplicationEvent`."
1391879026,14690,kirktrue,2023-11-14T01:37:29Z,Are we missing the initialization of `UNSUBSCRIBED`?
1391880921,14690,kirktrue,2023-11-14T01:41:16Z,"```suggestion
        LIST_OFFSETS, RESET_POSITIONS, VALIDATE_POSITIONS, TOPIC_METADATA, SUBSCRIBED,
```

`SUBSCRIPTION_CHANGE` is a bit vague. Does it encompass more than the event of the user calling `Consumer.subscribe()`?"
1391882860,14690,kirktrue,2023-11-14T01:45:20Z,"```suggestion
                revocationResult.thenCompose(__ -> {
```

Suggestion: use the double-underscore to denote to the reader that the variable is intended to remain unused."
1391883884,14690,kirktrue,2023-11-14T01:47:24Z,"Suggestion: consider moving this to an `assignPartitions()` method, similar to the `revokePartitions` method, for consistency and readability."
1391885027,14690,kirktrue,2023-11-14T01:49:38Z,"Suggestion: make `groupInstanceId` and `serverAssignor` `Optional` as constructor parameters to convey to the callers that they are, indeed, _optional_."
1391885243,14690,kirktrue,2023-11-14T01:50:07Z,"```suggestion

```

Haha. I don't really care "
1391888737,14690,kirktrue,2023-11-14T01:56:43Z,"```suggestion
```

nit: remove extra newline."
1391889559,14690,kirktrue,2023-11-14T01:58:16Z,"```suggestion
            log.debug(""Ignoring reconciliation attempt. "" + reason);
```

Nit: it'll be visually easier to parse with the space before the next sentence."
1391890695,14690,kirktrue,2023-11-14T02:00:28Z,"I think this `equals()` call is OK. From looking at `AbstractSet`, it appears that `SortedSet.equals()` is OK to accept any ol' `Set` implementation."
1392717259,14690,lianetm,2023-11-14T14:50:53Z,"The `Consumer.unsubscribe` does block on the callback execution, that's why it is a `CompletableApplicationEvent`. Only after the callback completes the unsubscribe can send the actual leave group heartbeat request. Makes sense?"
1392732597,14690,lianetm,2023-11-14T14:59:46Z,"I see, I was just intentionally leaving out all the pattern based logic because we don't support it at this point. But this makes me realize that that `subscribeInternal` based on pattern that you mentioned is wired to the `subscribe(Pattern pattern)` API call, when it's truly not supported yet. I think we should disable all the subscribe based on patterns until we implement them properly. What do you think?"
1392733198,14690,lianetm,2023-11-14T15:00:07Z,"Good catch, added. "
1392744058,14690,lianetm,2023-11-14T15:05:01Z,"Indeed, only from the state when the leave group HB is sent out. Added and test. Thanks!"
1392748787,14690,lianetm,2023-11-14T15:07:53Z,"It's exactly when the user changes the subscription via a call to subscribe. I used the name `SUBSCRIPTION_CHANGE` because it seemed clear and to be consistent with the existing `ASSIGNMENT_CHANGE`, but let me know if you think another name would be better."
1392790381,14690,lianetm,2023-11-14T15:32:12Z,"Done. Just to make sure we are on the same page, the whole snippet marked is not really assign. Assign is only ln 581 and ln 591 where the subscription state is updated and calling callbacks, and yes, I extracted those into an `assignPartitions()`. The rest of the checks, cache and errors is related to the revocation (or the transition from revocation to assign) so leaving it here where the revocation and assign are linked. "
1392802141,14690,lianetm,2023-11-14T15:38:34Z,"he he, I do avoid this, missed it here, fixed ;)"
1392827512,14690,kirktrue,2023-11-14T15:55:51Z,Where does it block? I didn't see a call to `Future.get()` when I looked.
1392829031,14690,kirktrue,2023-11-14T15:56:51Z,"I never liked `ASSIGNMENT_CHANGE` either, but I guess it's consistent, so  "
1392830639,14690,kirktrue,2023-11-14T15:57:54Z,"Those kinds of things tend to jump out in _other people's_ code, but I frequently miss them in my own  "
1392954694,14690,lianetm,2023-11-14T17:18:17Z,"Agree that the equals here does what we want, but actually this made me notice another detail. I wasn't passing the custom comparator when creating the `assignnedPartitions` sorted set. Also, for the owned ones, I already have a sorted set a few lines below so just moving it up to reuse it and make the comparison clearer."
1393793669,14690,dajac,2023-11-15T07:59:21Z,nit: We could remove this empty line.
1393794196,14690,dajac,2023-11-15T07:59:49Z,Should we add a unit test for the newly added topic names mapping?
1393830838,14690,dajac,2023-11-15T08:30:47Z,"From an architectural point of view, I wonder if this method and the next one are in the right place. Intuitively, I would have put them into the membership manager directly because they don't interact with the heartbeat manager state at all. What's your take on this?"
1393833703,14690,dajac,2023-11-15T08:33:05Z,"Have we reached a conclusion on this one? It seems correct to me to consider it as a no-op if the member is already leaving. However, I was wondering whether we should return a future here that will be completed only when the on-going leave operation completes."
1393834178,14690,dajac,2023-11-15T08:33:32Z,"nit: We could remove a few empty lines here, I suppose."
1393839254,14690,dajac,2023-11-15T08:37:43Z,nit: -1 or -2.
1393842585,14690,dajac,2023-11-15T08:40:19Z,Could we transition to fatal from prepare leaving and leaving?
1393842889,14690,dajac,2023-11-15T08:40:35Z,Could we transition to fatal from prepare leaving?
1393848768,14690,dajac,2023-11-15T08:45:11Z,`acknowledges the target assignment` is confusing here. My understanding is that it will acknowledge the part of the target assignment that was actually reconciled. Am I correct?
1393850232,14690,dajac,2023-11-15T08:46:22Z,nit: I think that we usually put static variables first when declaring attributes.
1393852447,14690,dajac,2023-11-15T08:47:55Z,nit: Should we move this one to `ConsumerGroupHeartbeatRequest` as we already have `LEAVE_GROUP_MEMBER_EPOCH` there?
1393854091,14690,dajac,2023-11-15T08:49:07Z,I am confused by this. Did we say that we should keep the member id forever when we receive one?
1393856821,14690,dajac,2023-11-15T08:51:05Z,"Note that it is possible to receive the exact same assignment multiple times. I suppose that in this case, we transition to RECONCILING and the reconciliation process will be a no-op because the current and the target are the same. Did I get it right?"
1393914776,14690,dajac,2023-11-15T09:32:14Z,nit: I wonder whether we should log this as an error.
1393915412,14690,dajac,2023-11-15T09:32:42Z,Should we also react to the future completion here for e.g. log something?
1393915845,14690,dajac,2023-11-15T09:33:01Z,Don't we need to call `subscriptions.assignFromSubscribed(Collections.emptySet());` here as well?
1393916982,14690,dajac,2023-11-15T09:33:50Z,nit: We could remove this empty line.
1393917097,14690,dajac,2023-11-15T09:33:55Z,ditto.
1393958634,14690,dajac,2023-11-15T10:03:02Z,"Is this really true? We could have the same topic name in both but with different topic ids for instance. In my opinion, we should move towards using TopicIdPartition for both the assigned partitions and the partitions ready to reconcile. We can of course tackle separately from this PR."
1393958878,14690,dajac,2023-11-15T10:03:13Z,nit: We can remove an empty line here.
1393960227,14690,dajac,2023-11-15T10:04:15Z,Should we trigger both in parallel?
1393962052,14690,dajac,2023-11-15T10:05:30Z,nit: We could remove this empty line.
1393962754,14690,dajac,2023-11-15T10:06:01Z,Should we log this as an error?
1393968230,14690,dajac,2023-11-15T10:09:53Z,"As I said before, this does not seem correct to me because we should keep the member id forever."
1393968517,14690,dajac,2023-11-15T10:10:06Z,nit: We could remove this empty line.
1393970046,14690,dajac,2023-11-15T10:11:17Z,nit: We could remove this empty line.
1393972135,14690,dajac,2023-11-15T10:12:59Z,What happen in this case? I suppose that the reconciliation will be retried. Did I get it right?
1393972399,14690,dajac,2023-11-15T10:13:13Z,nit: We can remove the space after the `.`.
1393972835,14690,dajac,2023-11-15T10:13:35Z,nit: We could remove this empty line.
1393973543,14690,dajac,2023-11-15T10:14:13Z,nit: We could remove this empty line.
1393974377,14690,dajac,2023-11-15T10:14:56Z,I agree that the other state transition should take care of updating the state. We should only abort here.
1393976328,14690,dajac,2023-11-15T10:16:29Z,nit: We could remove this empty line.
1393977197,14690,dajac,2023-11-15T10:17:09Z,"nit: Using `ifPresent` would be a bit more idiomatic, I think."
1393987062,14690,dajac,2023-11-15T10:24:43Z,"I am not sure to understand how the metadata cache knows which new topic ids it should resolve. Or does the consumer request metadata for ALL topics in the cluster? Looking at the code, it is may be what it does."
1393993623,14690,dajac,2023-11-15T10:29:34Z,+1
1394012157,14690,dajac,2023-11-15T10:44:58Z,"There is a subtile behaviour changes here.

1) In the legacy implementation, `this.coordinator.onLeavePrepare()` is called here and it triggers the callback before returning from `unsubscribe`.

2) `subscriptions.unsubscribe()` is actually called before `unsubscribe` returns as well."
1394032677,14690,dajac,2023-11-15T10:55:05Z,"btw, it seems that we could have transitioned to another state while waiting on this one as well."
1394042794,14690,dajac,2023-11-15T11:00:07Z,"I also wonder whether if would be possible to parallelize more. For instance, is there a reason not to trigger the revocation and the assignment callbacks at the same time? This would ensure that they are call within one poll; otherwise, it can take multiple calls to poll to complete the assignment. We could consider this as a optimization for the future.

"
1394053638,14690,dajac,2023-11-15T11:09:11Z,`partitionsAssigned` could also be empty here so we should handle this case appropriately. e.g. we should not trigger the callback.
1394322340,14690,lianetm,2023-11-15T14:55:14Z,Filed [KAFKA-15832](https://issues.apache.org/jira/browse/KAFKA-15832) for this and I will take care of it right after this PR as a follow-up. 
1394344222,14690,lianetm,2023-11-15T15:10:46Z,"Yes, done. "
1394493344,14690,lianetm,2023-11-15T16:57:39Z,"Yes, that's what it does, get metadata for all topics [here](https://github.com/apache/kafka/blob/22f7ffe5e1623d279096b45ab475768eeb05eee1/clients/src/main/java/org/apache/kafka/clients/Metadata.java#L719). It seems that there was an intention of a partial update [here](https://github.com/apache/kafka/blob/22f7ffe5e1623d279096b45ab475768eeb05eee1/clients/src/main/java/org/apache/kafka/clients/Metadata.java#L700) but not fully implemented, so it effectively ends up getting them all anyways. "
1394570775,14690,lianetm,2023-11-15T18:00:25Z,"Yes, you're right, I will rephrase this. It acknowledges the reconciled assignment, which is the subset of the target that was resolved from metadata and actually reconciled."
1394581206,14690,lianetm,2023-11-15T18:07:38Z,"Done, re-arranged a couple of them."
1394585362,14690,lianetm,2023-11-15T18:10:24Z,"Totally, done. "
1394619681,14690,lianetm,2023-11-15T18:28:38Z,"Yes, done. It is actually the level used for this in the legacy coordinator."
1394629300,14690,lianetm,2023-11-15T18:37:34Z,"Cool, thanks for confirming. "
1394646561,14690,lianetm,2023-11-15T18:51:40Z,"The legacy coordinator does trigger the `onPartitionsAssigned` with empty partitions (not the `onPartitionsRevoked` though), so I intentionally left the same behaviour, makes sense? I had also added a note on the [invokeOnPartitionsAssignedCallback](https://github.com/apache/kafka/blob/4a416a2ea7bc9a0d4f1ca1f8736cada784127b1a/clients/src/main/java/org/apache/kafka/clients/consumer/internals/MembershipManagerImpl.java#L859) to make sure that we keep that contract when implementing callbacks. "
1394733589,14690,lianetm,2023-11-15T20:03:07Z,"Yes, done. That's how it's done for other callbacks (aligned the messages too to make them consistent for all callbacks)"
1394742401,14690,lianetm,2023-11-15T20:12:53Z,"You're right, we do. Added it, along with a log in case of error. "
1394746271,14690,lianetm,2023-11-15T20:17:20Z,"sure, done. Logging error in the same way that it's done for other callback failures."
1394753399,14690,lianetm,2023-11-15T20:24:11Z,"Yes, I just updated this. We're on the same page regarding that the client will keep the member ID forever and provide it back....but I was wrongly expecting it would change after rejoining. Updated now. The goal is to be able to identify a rejoin, so using the member epoch (expecting that every time a member rejoins will get a bumped epoch)."
1394759427,14690,lianetm,2023-11-15T20:28:46Z,"You got it right, I expect the same thing (I had this [testReconciliationSkippedWhenSameAssignmentReceived](https://github.com/apache/kafka/blob/280652bc2a794bdd942506931bca756673bbf361/clients/src/test/java/org/apache/kafka/clients/consumer/internals/MembershipManagerImplTest.java#L559) for that)"
1394774045,14690,lianetm,2023-11-15T20:42:16Z,"Agree, same member ID forever. Updated this to use the member epoch as a way of identifying that the member has rejoined. "
1394786770,14690,lianetm,2023-11-15T20:53:01Z,"Yes, makes sense to me. It could be bundled up along with the callbacks triggering (both, revocation and assign), so that we trigger them all in the same poll iteration, while still making sure that they are executed in the right order. Filed [KAFKA-15835](https://issues.apache.org/jira/browse/KAFKA-15835) and I will address that as a follow-up right after this."
1394789418,14690,lianetm,2023-11-15T20:55:11Z,"Agree, filed [KAFKA-15835](https://issues.apache.org/jira/browse/KAFKA-15835) and I will address that as a follow-up right after this (considering all the 3 parts: commit, revoke callback, assign callback)"
1395130865,14690,lianetm,2023-11-16T04:18:22Z,"Agree, I merged them into the membership manager, and this actually goes in the same direction we've discussed about the membership manager becoming a first-class manager (supporting poll, for instance).So for now I integrated it with the `ApplicationEventProcessor` already, to be able to move these 2 funcs that I totally agree make sense in the membership manager (when tackling the poll for triggering reconciliations, I will extend on this same direction)"
1395132794,14690,lianetm,2023-11-16T04:22:54Z,"Done (now in the membership manager `leaveGroup`). No-op if already leaving, and returning the future that will complete when the ongoing leave completes. Also handling the case where the member already left (no-op and return right away)"
1395138233,14690,lianetm,2023-11-16T04:33:38Z,"There is no transition from prepare leaving to fatal with the current usage of fatal (only when receiving fatal errors in HB response), because we don't send HB while in prepare leaving. 

As for leaving, I would say we shouldn't either, because even if it is a state where we do send HB, we transition out of it as soon as the HB request is ready to be sent (without waiting for the actual send or any response). That being said, this makes me realize that the same reasoning applies for ACKNOWLEDGING, there shouldn't be a way of transitioning from ACK to FATAL, just because we transition out of it on heartbeat sent."
1395138981,14690,lianetm,2023-11-16T04:35:13Z,Not with the current usage of fatal as I see it (message above)
1395143611,14690,lianetm,2023-11-16T04:43:07Z,"Agree, filed [KAFKA-15839](https://issues.apache.org/jira/browse/KAFKA-15839) to extend topic ID usage in the whole assignment reconciliation flow to make sure we handle topic re-creation properly."
1395665723,14690,dajac,2023-11-16T13:07:00Z,"> There is no transition from prepare leaving to fatal with the current usage of fatal (only when receiving fatal errors in HB response), because we don't send HB while in prepare leaving.

We don't send an explicit HB while in prepare leaving. However, we continue to heartbeat so I assume that we could receive an error or be fenced while in this state. Regarding the ACK case, I think that we could receive a similar response while in ACK so the same applies. Do you agree?"
1395667965,14690,dajac,2023-11-16T13:08:56Z,Interesting... I wonder if this was done on purpose or if this just a bug. I don't really see the value in calling `onPartitionsAssigned` without any partitions. I suppose that we should double check this. We could perhaps file a JIRA and clarify this separately. What do you think?
1395669911,14690,dajac,2023-11-16T13:10:27Z,"Let's file a jira to improve this as well. Ideally, assuming that we don't use client side regex anymore, the client should only request the topics that it needs."
1395677889,14690,dajac,2023-11-16T13:16:48Z,nit: I suppose that this one should go on the previous line.
1395680718,14690,dajac,2023-11-16T13:17:46Z,I would remove the TODOs for which we have Jiras.
1395687362,14690,dajac,2023-11-16T13:22:10Z,nit: I may be worth logging something here as well to be consistent with `transitionToFatal`.
1395689326,14690,dajac,2023-11-16T13:23:36Z,nit: Should we also log the member epoch?
1395690012,14690,dajac,2023-11-16T13:24:13Z,Is this still valid? It looks like we call onPartitionsLost explicitly now.
1395691085,14690,dajac,2023-11-16T13:25:09Z,I wonder why we clear it here whereas in transitionToFenced we clear it when the callback future is completed. Is there a reason for this subtile difference?
1395691853,14690,dajac,2023-11-16T13:25:43Z,Is there a reason why we don't do this as the first thing in this method? This would be more consistent with transitionToFenced.
1395693217,14690,dajac,2023-11-16T13:26:48Z,"I think that the next HB should pick it up. Otherwise, we could perhaps transition to Joining to force an immediate HB. I am not sure that it is worth it."
1395697755,14690,dajac,2023-11-16T13:30:35Z,"I am a bit confuse by where we call `clearPendingAssignmentsAndLocalNamesCache`. Sometime we call it when the callback future is completed, sometime right after scheduling the callback. I wonder if it would be possible to be more consistent or if there are specific reasons that I did not get."
1395717674,14690,dajac,2023-11-16T13:46:36Z,"Note that here we clear the `assignedTopicNamesCache` but we only clear the assigned partitions after the callback is executed. This means that we have a period of time during which we are not able to resolve ids from the names in the subscription. I suppose that it does not matter in this case but this could be a source of subtile bugs.

As we discussed offline, I think that we really need to update the subscriptions to use TopicIdPartitions. If this is not possible, an intermediate approach would be to keep the assigned TopicIdPartitions in this manager and to update the subscriptions with `subscriptions.assignFromSubscribed` when we update it. Or, we could also move the bookkeeping of the cache closer to calls to `subscriptions.assignFromSubscribed`."
1395721501,14690,dajac,2023-11-16T13:49:34Z,nit: We could use `==` here now.
1395722320,14690,dajac,2023-11-16T13:50:12Z,Do we need to check the epoch here as well?
1395747556,14690,dajac,2023-11-16T14:06:11Z,nit: `t` -> `it`?
1395783384,14690,dajac,2023-11-16T14:31:03Z,`The broker will continue to send the assignment to the member.` This is not entirely true. The broker may not send anything.
1395862422,14690,lianetm,2023-11-16T15:05:18Z,"Agree, rephrased it. This test is just for the case where the broker does keep sending (and the test above for when it does not)"
1395877609,14690,lianetm,2023-11-16T15:14:22Z,"Agree, done. All TODOs in this class have jiras already."
1395894946,14690,lianetm,2023-11-16T15:21:59Z,"You're right, not needed anymore. The member will transition to fatal state but can keep its last member ID and epoch. "
1395907406,14690,lianetm,2023-11-16T15:28:39Z,"No reason, moved it to after the callback completes, consistent with how it is done on fencing, leave and reconcile"
1395912856,14690,lianetm,2023-11-16T15:32:16Z,"No reason, updated it to make it consistent with the fencing transition"
1395929369,14690,lianetm,2023-11-16T15:42:56Z,Agree that the next HB will pick it up based on the interval (also not seeing much need/value in the forced HB). Removed the TODO.
1395947490,14690,lianetm,2023-11-16T15:55:00Z,"Yes, I think we need it too. Added."
1396006392,14690,lianetm,2023-11-16T16:31:16Z,"I moved the clear cache close to the `assignedFromSubscribed` in this manager, as a first step to align both and have a consistent usage"
1396013115,14690,lianetm,2023-11-16T16:35:22Z,"Agree it was not consistent. I moved it close to the call to `assignPartitions`, so when callbacks complete we have a single `updateAssignment` that makes the assignment effective and clears cache if needed."
1396088885,14690,lianetm,2023-11-16T17:26:05Z,"Agree on handling that separately, but leaving the current behaviour as in the legacy coordinator. I also don't see the value either but it would introduce a change on when the `onPartitionsAssigned` is called or not, so let's put some more though on it. Filed [KAFKA-15843](https://issues.apache.org/jira/browse/KAFKA-15843) to follow-up on this."
1396462214,14690,lianetm,2023-11-16T22:50:07Z,"I updated it to align with the current behaviour (callbacks, best effort to send leave group request without any response handling or retry, and call to `subscriptions.unsubscribe` when everything completes). This has the gap of the callback execution that would require a poll. Given that we don't support callbacks in this PR, it won't block the flow, but definitely to be solved (I added the details of the challenge to solve in the [callbacks Jira](https://issues.apache.org/jira/browse/KAFKA-15276))"
1396474166,14690,lianetm,2023-11-16T23:01:39Z,"Makes sense, [KAFKA-15847](https://issues.apache.org/jira/browse/KAFKA-15847)"
1396494887,14690,lianetm,2023-11-16T23:26:51Z,"Yes, it will be retried on the next reconciliation loop (known shortcoming is how we trigger the reconciliation loops. It will be improved right after with [KAFKA-15832](https://issues.apache.org/jira/browse/KAFKA-15832)"
1396826235,14690,lianetm,2023-11-17T07:55:29Z,"Pushed one fix as a first step towards integrating TopicIdPartitions, which I agree should be the way forward. For now it is integrated in the MembershipManager, only in the reconciliation path where we do have all the info clearly in hand. Will continue the integration as follow-up with [KAFKA-15839](https://issues.apache.org/jira/browse/KAFKA-15839) as it requires a little bit more thought"
1398998763,14690,dajac,2023-11-20T10:41:19Z,"I think that this should actually outside of the `else` branch, isn't it?"
1399012130,14690,dajac,2023-11-20T10:50:03Z,"nit: If we would use `ConsumerGroupHeartbeatRequestData.TopicPartitions` in the `HashMap` and the `List`, we could skip this step. "
1399012491,14690,dajac,2023-11-20T10:50:20Z,nit: We could probably use `computeIfAbsent` to simplify this code.
1399013422,14690,dajac,2023-11-20T10:50:43Z,We still need to conclude on this one.
1399026661,14690,dajac,2023-11-20T10:59:17Z,nit: Should we move this code into `assignPartitions`?
1399044241,14690,dajac,2023-11-20T11:14:04Z,I am curious here. Is it better to build a `SortedSet` with all elements and then to add it to `assignmentReadyToReconcile` vs adding to `assignmentReadyToReconcile` directly?
1399045098,14690,dajac,2023-11-20T11:14:49Z,nit: I have noticed that most of the comments end with a period but not all of them. It may be good to be consistent.
1408353902,14690,lianetm,2023-11-28T19:58:37Z,"Yeah, no value in it. I simplified it by just adding the TopicPartition items directly to the `assignmentReadyToReconcile` (in [follow-up PR](https://github.com/apache/kafka/pull/14857))"
1408354526,14690,lianetm,2023-11-28T19:59:17Z,"Totally, and actually it made me realize it could be further simplified by retaining the assigned. It is included now in the [follow-up PR](https://github.com/apache/kafka/pull/14857) with the other minor fixes."
1408355119,14690,lianetm,2023-11-28T19:59:53Z,"Definitely, done in the [follow-up PR](https://github.com/apache/kafka/pull/14857)"
1408357542,14690,lianetm,2023-11-28T20:02:32Z,Agree. It disappeared anyways after simplifying it all with the use of TopicPartitions.
1408361923,14690,lianetm,2023-11-28T20:07:07Z,"Agree, we were definitely missing here transitions to FATAL/FENCED that may occur while the member is leaving (any of the 2 phases of leaving). I included the changes to properly handle them in the [follow-up PR](https://github.com/apache/kafka/pull/14857) so we can continue the conversation there. "
98550533,2466,mjsax,2017-01-30T21:54:48Z,"This should be added to the first paragraph:
```
The default {@code ""auto.offset.reset""} strategy, default {@link TimestampExtractor}, and default key and value deserializers as specified in the {@link StreamsConfig config} are used.
```

Please make sure, that the line is not longer than 120 chars.

Please adjust other JavaDocs, too."
98553210,2466,mjsax,2017-01-30T22:06:11Z,"`return stream(null, null, keySerde, valSerde, topics);`

Do the call directly instead of the cast."
98553705,2466,mjsax,2017-01-30T22:08:30Z,"update to `return stream(offsetReset, null, null, null, topics);` to avoid too many indirections. To this for other overloads, too, please."
98554156,2466,mjsax,2017-01-30T22:10:41Z,This should be the only method with actual code. All other overloads should call this one.
98554261,2466,mjsax,2017-01-30T22:11:18Z,should not have an implementation but call overloaded method.
98554337,2466,mjsax,2017-01-30T22:11:40Z,Should not have an implementation but call overloaded method.
98554471,2466,mjsax,2017-01-30T22:12:27Z,remove this line -- not required.
98554659,2466,mjsax,2017-01-30T22:13:24Z,"Nit: adjust indention of other parameters; line should not be longer than 120 chars. Indent second/third/etc line, too.
Text can be shorter:
```
the timestamp extractor used for this source {@link KStream}
if not specified the default extractor defined in the configs will be used
```
(no need to link to `TimestampExtractor` as there will be a link in the JavaDocs anyway."
98555452,2466,mjsax,2017-01-30T22:17:42Z,as above.
98555469,2466,mjsax,2017-01-30T22:17:48Z,as above
98555494,2466,mjsax,2017-01-30T22:17:56Z,remove
98555525,2466,mjsax,2017-01-30T22:18:06Z,as above
98555721,2466,mjsax,2017-01-30T22:19:11Z,remove
98555745,2466,mjsax,2017-01-30T22:19:21Z,as above
98555929,2466,mjsax,2017-01-30T22:20:16Z,"No reformatting, please"
98555957,2466,mjsax,2017-01-30T22:20:24Z,"No reformatting, please
"
98556721,2466,mjsax,2017-01-30T22:24:30Z,Nice catch!
98557646,2466,mjsax,2017-01-30T22:29:44Z,We should not add this to the context -- see comments below.
98558044,2466,mjsax,2017-01-30T22:31:50Z,keep this but rename to `defaultTimestampExtractor`
98558367,2466,mjsax,2017-01-30T22:33:32Z,"add line:
`TimestampExtractor sourceTimestampExtractor = source.getTimestampExtractor();`
and change to
`RecordQueue queue = createRecordQueue(partition, source, sourceTimestampExtractor != null ? sourceTimestampExtractor : defaultTimestampExtractor);`
"
98770837,2466,mjsax,2017-01-31T21:09:30Z,"Nit: ""default {@link TimestampExtractor}[,] and""

The rule is ""A and B"" (for two things no comma), but ""A, B, C, and D"" (for three or more things, use commas)"
98771506,2466,mjsax,2017-01-31T21:13:00Z,"Nit: Please use the same order in all JavaCode -- above TimestampExtractor is second -- I don't care which order, but please be consistent. Maybe follow parameter order of the method overload that provides all parameters ?"
98771974,2466,mjsax,2017-01-31T21:15:16Z,"remove ""(if any)"""
98772167,2466,mjsax,2017-01-31T21:16:10Z,"Nit: no ""."" at the end
Please update everywhere."
98773132,2466,mjsax,2017-01-31T21:20:22Z,"Update JavaDoc.
same below"
98773266,2466,mjsax,2017-01-31T21:20:59Z,as above
98773635,2466,mjsax,2017-01-31T21:22:48Z,Nit: Can you insert this method further down -- we want to order method overloads with regard to number of parameters -- it simplifies to keep track of what overloads are there.
98777135,2466,mjsax,2017-01-31T21:38:44Z,"check `source != null` not necessary.
In doubt add an assertion. "
98777360,2466,mjsax,2017-01-31T21:39:46Z,This can be reverted.
98777858,2466,mjsax,2017-01-31T21:41:49Z,"Does this add anything -- I doubt it? (ie, using a second mock TsExtractor)"
98779356,2466,jeyhunkarimov,2017-01-31T21:48:56Z,Which does not make sense: using two separate tsExtractors or this test case as a whole?
98780572,2466,jeyhunkarimov,2017-01-31T21:54:56Z,Once I removed it failed most of the tests of `StreamThreadStateStoreProviderTest` class with NullPointerException. 
98781354,2466,mjsax,2017-01-31T21:58:42Z,The test is fine -- but what's the value in testing overwrite the default extractor two times.
98782371,2466,jeyhunkarimov,2017-01-31T22:03:37Z,"I see. So I will remove `MockTimestampExtractor2` class and correct the test accordingly.
"
98847610,2466,dguy,2017-02-01T08:44:58Z,guaranteed -> guarantees
98847825,2466,dguy,2017-02-01T08:46:39Z,"same as above. I guess this is largely copy & pasted from other javadoc, so the issue is most likely elsewhere"
98849518,2466,dguy,2017-02-01T08:58:47Z,"topics -> topic.

This may well be elsewhere in the java-doc, too"
98850120,2466,dguy,2017-02-01T09:02:55Z,"I know you've only added the one param here, but seeing as you are changing it can you make all the params `final`?"
98850172,2466,dguy,2017-02-01T09:03:19Z,Make all params `final`
98850208,2466,dguy,2017-02-01T09:03:31Z,As above
98850243,2466,dguy,2017-02-01T09:03:48Z,As above
98850272,2466,dguy,2017-02-01T09:04:01Z,As above
98850377,2466,dguy,2017-02-01T09:04:40Z,"Here also, would be great if you could make the params `final`"
98850934,2466,dguy,2017-02-01T09:08:22Z,And again with `final` if you don't mind
98851089,2466,dguy,2017-02-01T09:09:29Z,"`final` ?
All of the fields should be `final` really"
98851203,2466,dguy,2017-02-01T09:10:13Z,This method can be package-private
98851316,2466,dguy,2017-02-01T09:10:54Z,"We should make this `final`, too"
98853125,2466,dguy,2017-02-01T09:22:44Z,"+1 to what @mjsax said.
The `source` should never be null. So you should change the `StreamThreadStateStoreProviderTest`. It just needs to have the topic name extracted to a field on line 73. And then that same topic name used on line 189 in `new TopicPartition(...)`"
98853314,2466,dguy,2017-02-01T09:23:49Z,I'd also consider extracting: `source.getTimestampExtractor() != null ? ...` into a local as the line is quite long and it will make the code a bit easier to read.
98853584,2466,dguy,2017-02-01T09:25:26Z,maybe `shouldAddTimestampExtractorPerSource` ?
98853717,2466,dguy,2017-02-01T09:26:13Z,Make all locals `final`
99165540,2466,dguy,2017-02-02T16:51:07Z,"i'd probably extract lines 121 -> 130 into a method, i.e, `findSourceNode(...)`

Also, we -> if"
99165983,2466,dguy,2017-02-02T16:52:46Z,There is no need to test this as it is calling the same method as above.
99167116,2466,dguy,2017-02-02T16:57:08Z,I'm not sure what this test has to do with `StreamTask`? To me this test should be in `TopologyBuilderTest`. You don't need a `StreamTask` in this case to check that the `TimestampExtractor` was assigned to the source
99200243,2466,mjsax,2017-02-02T19:30:02Z,"I just realized, that we use different wording for topics as array of Strings and topic pattern:
""there is no ordering guarantee"" vs ""there are no ordering guarantees"" -- I think we should clean this up for consistency. Would you mind to add this fix to this PR? The singular version sounds better, IMHO."
99200386,2466,mjsax,2017-02-02T19:30:57Z,Sorry -- mixed it up with `table`.
99201790,2466,jeyhunkarimov,2017-02-02T19:37:55Z,"I found `StreamTaskTest` the best suitable place, as it was suggested to make `SourceNode.getTimestampExtractor()` method available within package. So, it is not accessible inside `TopologyBuilderTest` currently. 
Then I am making `SourceNode.getTimestampExtractor()`  public and moving the tests to `TopologyBuilderTest`."
99202642,2466,mjsax,2017-02-02T19:41:54Z,key -> topic
99203389,2466,mjsax,2017-02-02T19:45:19Z,"I am still confused, about `source` being `null`. In the original code (L121) `source` is handed to `createRecrodQueue` and must not be `null` -- because this was never an issues before, I am still puzzled. why it is now."
99204674,2466,mjsax,2017-02-02T19:51:20Z,"I agree with @dguy If you test `TopologyBuilder#addSource()` it should go to `TopologyBuilderTest`, and if you test `KStreamBuilder#stream` it should go to `KStreamBuilderTest` -- this also implies, you should split this test into two.

Also testing `KStreamBuilder#addSource` is redundant because its inherited from `TopologyBuilder`."
99206508,2466,jeyhunkarimov,2017-02-02T19:59:00Z,"When we add the sources by pattern (`KStreamBuilder.stream(final Pattern topicPattern)`), the source name is given like `""Pattern ["" + regex + ""]""`. For example, for `t.*` pattern it would be `""Pattern[t.*]""`. In `StreamTask`, we search for sources (in 121) by topic name. For example, for topic name`""topic1""`,  it gives `null`,because  the source name is `""Pattern[t.*]""`."
99446130,2466,mjsax,2017-02-03T23:55:23Z,"revert this for `.stream(...)`, because it can be multiple here. @dguy original comment only applies to `.table()` has has always a single input topic."
99499027,2466,mjsax,2017-02-05T20:38:46Z,"Can you address this comment, too?"
99499541,2466,mjsax,2017-02-05T20:58:02Z,Can you please add `final` wherever possible.
99499677,2466,mjsax,2017-02-05T21:02:29Z,Why do you not reuse `KStreamBuilderTest#builder` ?
99499696,2466,mjsax,2017-02-05T21:03:00Z,"It's better to split this test into multiple -- here you test if no source specific extractor is set, thus, this should be a test method `sourceExtractorShouldBeNull` (or similar) and the test should end here.

Apply to below tests, too. (split into positive/negative tests and own tests for stream/table -- for stream/table add overload methods same way to test `TopologBuilder.addSource()`"
99500035,2466,mjsax,2017-02-05T21:16:02Z,add `final` wherever possible
99500140,2466,mjsax,2017-02-05T21:19:41Z,why this change?
99500765,2466,mjsax,2017-02-05T21:39:07Z,Why not use `topology.sourceTopicPattern()` ? And than check if `partition.topic()` matches the pattern?
99500810,2466,mjsax,2017-02-05T21:40:36Z,Why this change?
99500845,2466,mjsax,2017-02-05T21:42:09Z,apply `final` wherever possible (also within  method)
100309277,2466,jeyhunkarimov,2017-02-09T13:51:52Z,"I think `sourceTopicPattern()` is a method of `TopologyBuilder`. In `StreamTask` on the other hand, we get `ProcessorTopology` instance."
100309989,2466,jeyhunkarimov,2017-02-09T13:55:31Z,"If we need to find the `topic` of the given source (`StreamTask.findSource()`) by pattern, either we have to remove `""Pattern [ ]""` part from source name and try all matches, or we can remove it (`""Pattern [ ]""` part) when we assign the name for `SourceNode` and directly use its name as `Pattern`. I thought the second case would be more usable."
100310215,2466,jeyhunkarimov,2017-02-09T13:56:37Z,"Because the test classes (`TopologyBuilderTest` for example) cannot access the protected method , I leave it as it is"
100374990,2466,mjsax,2017-02-09T18:23:53Z,Ack. By bad.
100493505,2466,dguy,2017-02-10T08:56:52Z,It would be nice if you made these `final` while you are doing this change.
100493542,2466,dguy,2017-02-10T08:57:06Z,+1
100493741,2466,dguy,2017-02-10T08:58:40Z,`return topology.source(topic);`
100493758,2466,dguy,2017-02-10T08:58:50Z,`final`
100493997,2466,dguy,2017-02-10T09:00:43Z,"using `assertThat` is nicer as it gives better failure messages.
`assertThat(sourceNode.getTimestampExtractor(), instanceOf(MockTimestaampExtractor))`

in other places, too"
100494305,2466,dguy,2017-02-10T09:02:58Z,"typo: kStreamhould... -> kStreamShould
In fact i'd probably rename these methods to begin with should, i.e., 
`shouldAddTimestampExtractorToStreamWithOffsetResetPerSource` etc"
100494435,2466,dguy,2017-02-10T09:03:54Z,"as per previous `assertThat(..., instanceOf(...))` would be better"
107829914,2466,mjsax,2017-03-24T03:39:21Z,please fix this: `use {@link } instead`
107830212,2466,mjsax,2017-03-24T03:42:17Z,Nit: add missing `.` at the end.
107830238,2466,mjsax,2017-03-24T03:42:34Z,Nit: missing `.`
107830487,2466,mjsax,2017-03-24T03:46:55Z,Nit: add `final` twice
107830501,2466,mjsax,2017-03-24T03:47:08Z,nit: add `final`
107830511,2466,mjsax,2017-03-24T03:47:21Z,Nit: add `final`
107830528,2466,mjsax,2017-03-24T03:47:36Z,add `final`
107830534,2466,mjsax,2017-03-24T03:47:45Z,Nit: add `final`
113301565,2466,mjsax,2017-04-25T20:28:20Z,"Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the ""right"" place. Thanks a lot. :)"
113312440,2466,mjsax,2017-04-25T21:09:13Z,"Just some nitpick: we started to to order all configs alphabetically here -- it make it a little simpler to keep an overview and to maintain the code. Would you mind to not move configs that get deprecate and add the new config at the ""right"" place. Thanks a lot. :)"
113312517,2466,mjsax,2017-04-25T21:09:35Z,"For backward compatibility, we need to keep the old default value.
Btw: we don't do any ordering here yet -- just above. so no need to reorder anything here."
113312570,2466,mjsax,2017-04-25T21:09:50Z,As above: need to keep default value.
113312651,2466,mjsax,2017-04-25T21:10:11Z,you can simple call `serde = defaultKeySerde()` here.
113312708,2466,mjsax,2017-04-25T21:10:27Z,"`.configure()` is called within `getConfiguredInstance()` already -- you can remove this line (I know this pattern was there before, but it's wrong -- can you please fit it :))

this can be a single liner within try-catch-block: `return getConfiguredInstance(DEFAULT_KEY_SERDE_CLASS_CONFIG, Serde.class);`"
113312767,2466,mjsax,2017-04-25T21:10:44Z,As above
113312830,2466,mjsax,2017-04-25T21:11:04Z,As above.
113508449,2466,jeyhunkarimov,2017-04-26T17:02:32Z,"If we want to distinguish between `VALUE_SERDE_CLASS_CONFIG` and `DEFAULT_VALUE_SERDE_CLASS_CONFIG` for example, in     `public Serde valueSerde()`  method, we have to differentiate whether 

` Serde<?> serde = getConfiguredInstance(VALUE_SERDE_CLASS_CONFIG, Serde.class);
`
is default or not. If it is default, then we will call `defaultValueSerde()` method. If we don't set the default value to `null`, then its default value will be some Object initialized with `Serdes.ByteArraySerde.class.getName()` class. In this case, it is hard to know whether the old `serde` (`VALUE_SERDE_CLASS_CONFIG`) is overridden (so we return it) or it has  default value (so we call `defaultValueSerde() `method). 

Moreover, because we are controlling the access to `VALUE_SERDE_CLASS_CONFIG` via `valueSerde()` method, we handle `null` cases here as well.

The same applies to `KEY_SERDE_CLASS_CONFIG` and `TIMESTAMP_EXTRACTOR_CLASS_CONFIG` as well."
113564584,2466,mjsax,2017-04-26T21:16:04Z,Ah. Makes sense!
113564805,2466,mjsax,2017-04-26T21:17:08Z,It's actually pretty elegant! :)
113777778,2466,jeyhunkarimov,2017-04-27T19:06:41Z,"I think in `getConfiguredInstance() ` method, `Configurable.configure(Map<String, ?> configs)` method is called, but in `keySerde() `and `valueSerde() `methods we need also to call `Serde.configure(Map<String, ?> configs, boolean isKey)` method.  So I think the two `configure` methods are different."
113808802,2466,mjsax,2017-04-27T21:37:30Z,`Serde` implements `Configurable` -- the object `o` in `((Configurable) o).configure(originals());` is the serde object.
113811258,2466,jeyhunkarimov,2017-04-27T21:50:46Z,"Yes I want to say that `Serde.configure(arg1, arg2)` is different from `Configurable.configure(arg1)`. So, inside `getConfiguredInstance()` method it is not called `configure(arg1, arg2)` but  `configure(arg1)`"
113829911,2466,mjsax,2017-04-28T00:13:13Z,"I did have a closer look into the code. You are right. I also double checked and `Serde` does actually not implement `Configurable` (so there will also not be two calls what would be bad).

Sorry for the confusion -- and thanks a lot for pointing out that it is correct as is!! "
115824780,2466,guozhangwang,2017-05-10T18:59:48Z,@jeyhunkarimov With this change how would the caller differentiate between the case that of a single subscribed topic v.s. a subscribed pattern?
115825542,2466,guozhangwang,2017-05-10T19:02:52Z,Could we add a similar function in `StreamsConfig` as `keySerde` in which we capture the deprecated / new default values there so that we do not need to leak that logic here?
115826174,2466,guozhangwang,2017-05-10T19:05:50Z,"Related to the question I have before: in the `toString` function at line 85, we are printing `topics:string`, and we canno tell if it is a pattern or a single topic right?"
115826873,2466,guozhangwang,2017-05-10T19:09:14Z,"Why do we need to augment this function here? I.e. by the time the stream task is created, we should have populated the `sourceByTopics` map with the pattern matched topics already, so I'm not sure if the additional computational logic is needed? cc @bbejeck ."
115845233,2466,jeyhunkarimov,2017-05-10T20:33:16Z,"@guozhangwang actually `""Pattern[ ]""` string is just cosmetic part of the code snippet 
`""Pattern["" + pattern + ""]""`. We don't use `""Pattern""` string (anywhere in the code) for detecting if it is a single subscribed topic or subscribed pattern. "
115847989,2466,jeyhunkarimov,2017-05-10T20:45:21Z,"@guozhangwang You are right. The main reason that I put this extra function is that I was getting many fails in tests. 

There were some tests with no source nodes defined in the test topology  or the defined source nodes are not related with partitions.

"
115850388,2466,guozhangwang,2017-05-10T20:55:16Z,"Hmm, maybe it's better to fix these tests than modifying the production-code? E.g. we can add some test-only function to fill the topics from the pattern in `@before` rule."
115850952,2466,jeyhunkarimov,2017-05-10T20:57:43Z,"@guozhangwang I fixed the tests as well and added this code snippet too. 
I mentioned this in 
[here](https://github.com/apache/kafka/pull/2466#issuecomment-282328998)"
115852641,2466,jeyhunkarimov,2017-05-10T21:05:15Z,"Yes, once the source is defined and added to topology, we cannot tell if it is defined by name or by pattern. This was the case before this PR as well. "
115853285,2466,jeyhunkarimov,2017-05-10T21:08:34Z,But I got your point
115907018,2466,guozhangwang,2017-05-11T05:24:18Z,"Thanks for the explanation, makes sense."
115907121,2466,guozhangwang,2017-05-11T05:25:37Z,Yeah I think if the logic is not needed in real cases then we should not add that since it may hide some potential bugs. As long as the tests can be covered then we can get rid of the unnecessary logic.
115925128,2466,jeyhunkarimov,2017-05-11T07:53:51Z,"I recognized this issue:

        `builder.stream(Pattern.compile(""t.*""))`;  
// this line adds sourceNode with name ""t*"", (before this PR it was `""Pattern[t*]""`)
       After that, in `StreamTask:120` when one executes:

         `final SourceNode source = topology.source(partition.topic());`  
 // where `partition.topic()` is ""t1""

Here the `source` will be  `null` because  `topology.source()` is just key value lookup. 
So  I moved extra extra lookup for patterns from `StreamTask` to` topology.source()`

   
        
"
116282834,2466,guozhangwang,2017-05-12T17:27:51Z,"@jeyhunkarimov Hmm, in `TopologyBuilder#build` when we are adding the source node we will execute the pattern matching from the current topic metadata so the map should already be filled with the actual topic right?

I think the problem, as I added in `SourceNodeFactory#getTopics`'s comment, is that under debugging / unit testing environment, there is no topic metadata available to pass the topic list to `List<String> getTopics(Collection<String> subscribedTopics)`. What we need to do to fix the unit tests then is to call `SubscriptionUpdates#updateTopics()` before calling `builder.build()` for regex involved tests.

```
                    final List<String> topics = (sourceNodeFactory.pattern != null) ?
                            sourceNodeFactory.getTopics(subscriptionUpdates.getUpdates()) :
                            sourceNodeFactory.topics;

                    for (String topic : topics) {
                        if (internalTopicNames.contains(topic)) {
                            // prefix the internal topic name with the application id
                            topicSourceMap.put(decorateTopic(topic), (SourceNode) node);
                        } else {
                            topicSourceMap.put(topic, (SourceNode) node);
                        }
                    }
```"
116317477,2466,bbejeck,2017-05-12T20:26:15Z,"@jeyhunkarimov Sorry for jumping in a little late.  @guozhangwang  is correct about the test.

An example for unit-testing with regex defined topics using the `TopologyBuilder` is `TopologyBuilderTest#shouldSetCorrectSourceNodesWithRegexUpdatedTopics` (line 675) "
116348919,2466,jeyhunkarimov,2017-05-13T02:50:11Z,"@guozhangwang @bbejeck thanks for your comments.
Done."
281873083,6694,ableegoldman,2019-05-08T00:04:47Z,nit: use RecordQueue.UNKNOWN instead of -1
281885338,6694,ConcurrencyPractitioner,2019-05-08T01:27:58Z,"No problem, could fix that. "
282344028,6694,ableegoldman,2019-05-09T05:38:23Z,"Can we add separate unit tests to confirm this produces the expected behavior? I think the JIRA had some examples highlighting why this is a problem, it would be good to convert those into tests to make sure we're really fixing the problem at hand :)"
282726631,6694,ConcurrencyPractitioner,2019-05-10T02:16:04Z,No problem. Added a new test case to confirm behavior.
289592039,6694,mjsax,2019-06-01T04:34:51Z,"Can we add those method to the end of the class -- we have already a ""section"" for methods that we only need for testing."
289592091,6694,mjsax,2019-06-01T04:37:43Z,"To be future prove, we should encode a version number as prefix in case we ever what to change this metadata. What about `<version>:<partitionTime>` with version number ""1"" ?

Also, line is too long. Move both parameters to their own lines."
289592181,6694,mjsax,2019-06-01T04:41:15Z,"I am not an expert on this API, but I would expect that even if we commit using the producer, the consumer should still be able to read the metadata.

Did you verify that we cannot retrieve the metadata if EOS is enabled? If this is the case, I would claim it's a bug that need to be fix.

"
289592249,6694,mjsax,2019-06-01T04:44:17Z,"We call `addRecords` during regular processing but only need to restore the partition time if a rebalance happened. Hence, I don't think this is the right place to add this code.

From my understanding, we should do this in `StreamThread#RebalanceListener.onPartitionsAssigned` instead? \cc @guozhangwang to confirm/comment"
289592273,6694,mjsax,2019-06-01T04:45:41Z,We should not piggy-back test for new features to existing tests. It's better to add a new test method `shouldAddPartitionTimeToOffsetMetatdataOnCommit`
289592283,6694,mjsax,2019-06-01T04:46:20Z,"nit: `shouldRestorePartitionTimeOnRestart` -- ""update timestamp"" is a little fuzzy"
289609768,6694,ConcurrencyPractitioner,2019-06-01T15:48:41Z,"Alright, I will check on that."
289613688,6694,ConcurrencyPractitioner,2019-06-01T17:51:50Z,Just realized the test I added included the test for commit mechanism anyways. 
289613963,6694,ConcurrencyPractitioner,2019-06-01T18:03:07Z,"Ok, so after some checks, I have discovered the following: the producer and consumer are not assigned to the same TopicPartition (however, that is not necessarily the reason for the failure I'm about to describe.) I'm not 100% sure if this was supposed to happen. When producer committed the transaction, that is supposed to mean the consumer group coordinator was notified to commit the offsets right? However, when the consumer available to StreamTask is called (i.e. ```committed(TopicPartition)```), null is returned (indicating no offsets were committed). 

Producer's sendOffsetsToTransaction requires the user to also enter a consumer group id (which is specified by user config) which we send the offsets to commit to. It is possible that unless the user specifies the right consumer group id, we would not be able to retrieve associated metadata."
289677945,6694,ConcurrencyPractitioner,2019-06-03T03:17:38Z,"Oh, thats a good suggestion. Would need to change test location in that case."
299187366,6694,abbccdda,2019-07-01T19:36:08Z,Should we check for partition existence for 2 calls?
299187593,6694,abbccdda,2019-07-01T19:36:56Z,Better to append suffix for time variables like `timestampMs`
301350910,6694,mjsax,2019-07-09T00:14:07Z,"What do you mean by ""for 2 calls""? The call to `partitionTime()`? If the partition does not exists, it's a bug anyway and current code throws a NPE. What do we gain by checking if the partition exists? There is still a bug and we would need to throw an exception, too, IMHO. What do we gain by manually throwing an exception?"
301351239,6694,mjsax,2019-07-09T00:15:46Z,"It's not a duration, hence, I am not sure if adding `Ms` suffix helps much here? `timestamp` is always UNIX epoch in ms throughout the whole code base and we never add `Ms` so far. Thoughts?"
301351580,6694,mjsax,2019-07-09T00:17:51Z,Might be better to use `RecordQueue.UNKNOWN` instead of `NO_TIMESTAMP` (cf. `clear()` below)
301351875,6694,mjsax,2019-07-09T00:19:26Z,@guozhangwang @bbejeck @vvcephei WDYT about this?
301354251,6694,mjsax,2019-07-09T00:32:55Z,"I sync with Guozhang about this. It's not correct to get the committed offsets within `onPartitionsAssigned()` callback, because the consumer might not have fetched/updated the offsets from the brokers.

However, we should still move the off the main loop. We only need to initialize the partition time if we get a new task assigned. Hence, we should move it into `AssignedTasks#initializeNewTasks()`?"
301355250,6694,mjsax,2019-07-09T00:38:34Z,"> the producer and consumer are not assigned to the same TopicPartition

Not sure what you mean by this? A producer is never _assigned_ any partitions. We use the term _assigned_ for consumers only. Can you clarify?

> It is possible that unless the user specifies the right consumer group id, we would not be able to retrieve associated metadata.

Absolutely. The `group.id` used to commit offsets via the producer must match the `group.id` used by the consumer to read those offsets. In Kafka Streams the `application.id` is used as `group.id` for the consumer and it's also passed into `producer.sendOffsetsToTransaction()`.

Hence, I still think that we don't need to handle EOS differently to get the partition-time.

Does this help?"
301355349,6694,mjsax,2019-07-09T00:39:05Z,nit: revert
301668666,6694,ConcurrencyPractitioner,2019-07-09T16:02:31Z,"Sure, no problem with that.
"
301685923,6694,ConcurrencyPractitioner,2019-07-09T16:41:21Z,"Ah, forgot that producer isn't assigned partitions. 

Let me see what I can do to handle to the EOS case then."
309349352,6694,mjsax,2019-07-31T17:45:04Z,"This code would crash if the cast fails. To avoid the issue, we should add `setAssignmentToStoredTimestamps` to `Task` interface (and remove the cast here) and add an empty implementation of the method to `StandbyTask`.

I would also rename the method to `initializeTaskTime()`"
309349964,6694,mjsax,2019-07-31T17:46:30Z,We don't need to add this any longer. We added `public long streamTime()` in another PR already.
309351486,6694,mjsax,2019-07-31T17:49:54Z,"I think we should throw an exception for this case, because the added partitions of the queue are fixed and should never change (and we should never request the partition time for unknown partitions -- it we do this, it would indicate a bug and would should raise it as an exception).
```
throw new NullPointerException(""Partition "" + partition + "" not found."");
```"
309351989,6694,mjsax,2019-07-31T17:50:59Z,"Nit: Can we rename this to `partitionTime` (to align the name to `streamTime()` method) -- in Kafka, we usually omit the `get` prefix on getter methods, and a record has a ""timestamp"" while for a partition or task it's a ""time"" (of course, both a ""timestamp"" and a ""time"" is just a long and quite similar, whoever, it seems more accurate to refer to their semantic meaning correctly)."
309352168,6694,mjsax,2019-07-31T17:51:22Z,"nit: rename to `setPartitionTime()` (cf. other comment from above)

Also rename parameter `timestamp -> partitionTime`."
309352994,6694,mjsax,2019-07-31T17:53:16Z,"Similar as above, we should throw an exception here."
309354307,6694,mjsax,2019-07-31T17:56:06Z,nit: rename `timestamp -> partitionTime`
309354740,6694,mjsax,2019-07-31T17:57:06Z,"Why do we need to make `partitionTime` `public` ? In any case, please preserve the JavaDocs if you move the method."
309355477,6694,mjsax,2019-07-31T17:58:36Z,"We merge another PR recently that updates `partitionTime` already (cf. below) -- hence, no need to add this any longer."
309359552,6694,mjsax,2019-07-31T18:07:51Z,I don't understand the purpose of this method. Why not just get the `partitionTime` of the task and commit it?
309360061,6694,mjsax,2019-07-31T18:09:05Z,"@guozhangwang @bbejeck @vvcephei Do you think it's worth to add a version number for the binary format of the committed offsets (I tend to think we should add a version number).

I would also not encode the timestamps as `String` but as 8-byte binary long."
309362468,6694,mjsax,2019-07-31T18:14:41Z,"So we need to log this at INFO level? Seems ERROR might be more appropriate because it actually indicates corrupted metadata? We should also update the error message accordingly:
```
log.error(""Could not initialize partition time. Committed metadata is corrupted."", e);
```"
309363884,6694,mjsax,2019-07-31T18:17:51Z,Why is the return type not `void` (similar for `setAssignmentToStoredTimestamps` below)? Seem you added it for testing? I would prefer to keep it `void` and change the tests if possible.
309364369,6694,mjsax,2019-07-31T18:18:53Z,nit: remove `get` prefix (similar below for `getPartitionTime()`
309364980,6694,mjsax,2019-07-31T18:20:05Z,"Instead of using 3 broker for this test, we should reconfigure the brokers to allow using EOS with a single broker. To do this, we need to set `transaction.state.log.replication.factor=1` in the passed-in broker config (maybe something else... not 100% sure)."
309366581,6694,mjsax,2019-07-31T18:23:34Z,nit: simplify to `throws Exception`
309366881,6694,mjsax,2019-07-31T18:24:11Z,This seems to be rather complicate. Just hard code the `appId` ?
309367084,6694,mjsax,2019-07-31T18:24:29Z,Why do we need a store for this test? I think a simple `builder.stream().to()` should be sufficient?
309367958,6694,mjsax,2019-07-31T18:26:20Z,"Not sure what your comment means. Can you elaborate? (I think it makes sense to test with multiple partitions, but I am not sure if I understand the comment -- how id the default key partitioner related?)"
309368689,6694,mjsax,2019-07-31T18:27:42Z,"nit: rename `driver -> kafkaStreams` (we always name it `kafkaStreams` in test, and it would be good to keep the name for consistency)"
309369523,6694,mjsax,2019-07-31T18:29:34Z,"We don't need to sleep (in general, sleeping is bad practice because it makes test flaky), because when `close()` is called below, it is ensured that offsets are committed."
309371664,6694,mjsax,2019-07-31T18:34:30Z,"Why do we need this validation step? The partition time or stream time is not exposed in the `context` and thus, I don't understand what this step verifies? Why do we need to check the topic name?"
309371951,6694,mjsax,2019-07-31T18:35:09Z,nit: rename `maxTimestamp -> partitionTime`
309372240,6694,mjsax,2019-07-31T18:35:52Z,Why do we need to write two records?
309373077,6694,mjsax,2019-07-31T18:37:56Z,This variable is accessed by multiple threads that thus should be declared `volatile`
309373488,6694,mjsax,2019-07-31T18:38:57Z,unnecessary comment
309373698,6694,mjsax,2019-07-31T18:39:30Z,No need to call `cleanUp()` if we remove the state.
309375797,6694,mjsax,2019-07-31T18:44:23Z,"We should not care about `record.timestamp()` IMHO, but instead use a second variable similar to `lastRecordedTimestamp`: something like `Map<Integer,Long> expectedPartitionTimePerPartition`. This allows us to set an expected partition time per partition and compare it to the passed in `partitionTime` (what you now call `maxTimestamp`)

If passed in partition time does not match expected partition time, we can just throw an `RuntimeException` what will kill KafkaStreams and the test will eventually time out."
309375948,6694,mjsax,2019-07-31T18:44:44Z,Seem not to be required?
309457372,6694,ConcurrencyPractitioner,2019-07-31T22:25:10Z,"Ah, this was something I added after I discovered a bug during integration tests. What happened was that offsets are periodically right (that is, it is automated)?  So imagine this, the partitionTime has advanced to 10 milliseconds. We commit that time, and then streams experienced some sort of failure (akin to a restart of streams). That would mean the locally stored partitionTime was reset to -1. Let's say we start processing again, and the partition time is now 9. What happens is that 9 is the timestamp committed, not 10. It overwrites the previous committed timestamp. So what we need to do is retrieve the previously committed timestamp if there was any, and then commit _that_ one instead, since that is the correct one.

I had some second thoughts on this, particularly since it becomes a little difficult to distinguish between restarts, cleanups, or failures which could have the same effect. So I'm not so sure if this is still needed or we still need to modify the behavior for these cases.

The thing is without this method, the test that I have added at any rate fails."
309457844,6694,ConcurrencyPractitioner,2019-07-31T22:27:03Z,"Yeah, forgot to remove it during previous debugging."
309459876,6694,ConcurrencyPractitioner,2019-07-31T22:34:50Z,"Oh, the thing is this logic is neccessary, or otherwise the test will crash.

If you were to look closely at ```RecordQueue``` logic, the class updates the partitionTime _after_ TimestampExtractor was called. This is important, because partitionTime is always passed in first. 

That means that ```RecordQueue.UNKNOWN``` is always passed in first when we first start processing, and if we always return maxTimestamp without checking if record.timestamp() is greater, than that means -1 will be returned no matter how many records are passed through TimestampExtractor.

Thus, what we do here is stimulate an update to partitionTime _before_ it is actually updated in ```RecordQueue```. 
"
309466633,6694,ConcurrencyPractitioner,2019-07-31T23:03:31Z,"Alright, will do."
309469061,6694,ConcurrencyPractitioner,2019-07-31T23:14:47Z,"Actually, on further investigation, this method might not be needed. We will see."
309478195,6694,mjsax,2019-08-01T00:01:01Z,"> That means that RecordQueue.UNKNOWN is always passed in first when we first start processing

Yes.

> and if we always return maxTimestamp without checking if record.timestamp() is greater, than that means -1 will be returned no matter how many records are passed through TimestampExtractor.

Yes, but you code return `return record.timestamp();` anyway. So for the second call of `extract()` method, `partitionTime` (ie, `maxTimestamp`) gets advanced. For a stop-restart of `KafakStreams`, with this fix on restart `partitionTime` should be `UNKNOWN` any longer, as it's should be initialized from the commit-metadata that is preserved.

Hence, we should see `UNKNOWN` only a single time, and the integration test should verify that we only see on the first start of `KafakStreams` but not for the second start.

> Thus, what we do here is stimulate an update to partitionTime before it is actually updated in RecordQueue.

I cannot follow here. `partitionTime` is tracked internally and it will be updated after TimestampExtractor returns, base on the value that is provided in `return`."
309749298,6694,ConcurrencyPractitioner,2019-08-01T15:07:52Z,"Ah, but if we do throw a NullPointer here, the test I added fails. So I don't know if that is what we really should do."
309776403,6694,ConcurrencyPractitioner,2019-08-01T16:01:29Z,"Yeah, accidentally removed it when I was rebasing the PR. Will add it back."
309795005,6694,ConcurrencyPractitioner,2019-08-01T16:45:43Z,"Ah, okay. Then I will change that."
309917436,6694,mjsax,2019-08-01T22:12:31Z,"Not sure -- but maybe you setup the test incorrectly? `PartitionGroup` constructor get a `Map<TopicPartition, RecordQueue> partitionQueues` and you should only call `setPartitionTimestamp()` for `TopicPartitions` that are provided by this map. Could this explain the test issue?"
309918847,6694,ConcurrencyPractitioner,2019-08-01T22:17:53Z,"Oh, that might be the case. I was going through the consumer assignment's topic partitions instead. Will check it out. "
309933316,6694,ConcurrencyPractitioner,2019-08-01T23:23:00Z,"Well, I put the sleep there because otherwise the test (eos enabled case) breaks. 

I have done quite a bit of digging, and it appears what happens is that the committed metadata retrieved is incorrect after the streams restart. I added some debug statements, and the strange thing is though is that committed() doesn't return the right metadata.
I made two calls to committed() -- this is during initializeTaskTime() -- and the first call returns the incorrect metadata (the result suggests that no OffsetAndMetadata was committed), yet on the second call, it returns the correct metadata (perhaps because this time OffsetAndMetadata has been persisted and could now be returned by committed()).

The sleep() method I put there because it seems that OffsetAndMetadata needs enough time to actually persist in Kafka log in eosEnabled=true case, otherwise, Consumer#committed() returns inconsistent results.

 "
309968392,6694,ConcurrencyPractitioner,2019-08-02T03:13:55Z,"Oh, I could remove them. Done that."
310275056,6694,abbccdda,2019-08-02T19:50:28Z,nit: space before `no-op`
310275504,6694,abbccdda,2019-08-02T19:52:04Z,is old metadata missing expected after we start off? Might be useful to add a debug log or trace if this is not normal.
310275800,6694,abbccdda,2019-08-02T19:53:05Z,"would be favorable to order comparison result according to first citizen. Like
`metadataTimestamp >= localPartitionTime ? metadataTimestamp : localPartitionTime;`"
310275999,6694,abbccdda,2019-08-02T19:53:45Z,Maybe refactor out a helper for the above condition?
310276422,6694,abbccdda,2019-08-02T19:55:06Z,Why `-1`Could we define a constant referring to it?
310276582,6694,abbccdda,2019-08-02T19:55:38Z,This comment is not needed.
310276679,6694,abbccdda,2019-08-02T19:56:00Z,s/time stamp/timestamp
310276764,6694,abbccdda,2019-08-02T19:56:17Z,would be good to define `1000` as a variable.
310276812,6694,abbccdda,2019-08-02T19:56:26Z,same here
310319615,6694,mjsax,2019-08-02T22:50:32Z,We can remove this method -- it's declared in the interface and there is no need to have an implementation in `AbstractTask`
310320109,6694,mjsax,2019-08-02T22:53:28Z,Can `setPartitionTime()` be package-private?
310320883,6694,mjsax,2019-08-02T22:58:18Z,Still not sure why we need this method? (or did you forget to remove it?)
310321420,6694,mjsax,2019-08-02T23:02:07Z,Why do we call `initializeTaskTime` in `addRecordsToTasks()` -- in a previous version it was called in `initializeNewTasks()` what seems to be more appropriate -- why did you move it?
310321680,6694,mjsax,2019-08-02T23:03:47Z,"nit: `shouldPreservePartitionTimeOnKafkaStreamRestart` (nothing is reset in this test).

(Also, avoid naming overlap with `KStream` and `KTable`)"
310321935,6694,mjsax,2019-08-02T23:05:34Z,Why do we need to suffix the appId and topic names with the `testId` ?
310322139,6694,ConcurrencyPractitioner,2019-08-02T23:06:58Z,"Well, since before, we never stored any metadata in Kafka log, especially relating to committed timestamps. There is a possibility that an old version of OffsetAndMetadata is committed where it doesn't contain the committed timestamp, so I suppose this is expected behavior."
310322225,6694,ConcurrencyPractitioner,2019-08-02T23:07:37Z,"Well, the order in general doesn't seem to matter that much. But could change it."
310322293,6694,ConcurrencyPractitioner,2019-08-02T23:08:04Z,"Yeah, it looks like it appeared several times in the code, might want to add it as some separate static helper method."
310322589,6694,mjsax,2019-08-02T23:10:17Z,"That is weird. \cc @guozhangwang How could the happen? If we stop the first instance, the transactions should be committed and afterwards, if we start a new instance, the new consumer client should be able to read the correct offset and metadata. Any idea?"
310323805,6694,ConcurrencyPractitioner,2019-08-02T23:19:48Z,"Oh, tried to run a test without this method, but AbstractTaskTest would break unfortunately if this method is not implemented in AbstractTask (apparently, AbstractTask's constructor is called, and compiler complains about it as a result)."
310324030,6694,ConcurrencyPractitioner,2019-08-02T23:21:31Z,"Oh, actually, yeah, we can remove this method. We could just modify the AbstractTaskTest itself."
310324216,6694,ConcurrencyPractitioner,2019-08-02T23:22:56Z,"Sure, there shouldn't be any problems."
310324305,6694,ConcurrencyPractitioner,2019-08-02T23:23:33Z,"Ok, no problem."
310324616,6694,ConcurrencyPractitioner,2019-08-02T23:26:15Z,Just thought it would be a good idea to include more information in the the id names. They can be hardcoded.
310324663,6694,ConcurrencyPractitioner,2019-08-02T23:26:36Z,Done.
310327228,6694,ConcurrencyPractitioner,2019-08-02T23:51:20Z,"Ok, I realized what is happening. During the process to close a StreamTask, the partitionTimes are reset to -1 first before the local partition times are committed. Effectively, what is occurring is that we are committing -1 during close() due to the order of operations we are performing it. I have found a solution to it, so I will push a change shortly."
310327593,6694,mjsax,2019-08-02T23:55:17Z,Good find!
310704615,6694,ConcurrencyPractitioner,2019-08-05T17:10:51Z,"@mjsax How would you do that though? I don't think OffsetAndMetadata could store an 8-byte binary long directly, so we have to use encode and decode the byte array as some string. Is that how we should do it?"
310707714,6694,ConcurrencyPractitioner,2019-08-05T17:19:15Z,I was thinking about using UTF8 conversions.
310722093,6694,mjsax,2019-08-05T17:56:43Z,"Good pointed. I missed that the type is `String` (expected it to be `byte[]`). Hence, for efficient encoding, and to allow us to add a magic/version byte, we should first serialize the timestamp, prefix it with a magic byte and then ""deserialize"" it to `String`.
```
byte[] bytes = ByteBuffer.allocate(9)
    .put(MAGIC_BYTE) // add a corresponding ""final static"" variable 
    .putLong(timestampe)
    .array();
String metadata = StringSerde.deserialize(bytes);
```"
310764557,6694,ConcurrencyPractitioner,2019-08-05T19:50:43Z,"@mjsax Okay. I pushed a version of what I thought was pretty close to the process you were describing. Mind taking a look? At the moment, it doesn't seem to work though. I did some research and what we are using is basically UTF8 encodings. It did look like however that some information was lost. (I did some debug statements and found that the decoded value was 1007 instead of 1000, somewhat bizarre).

"
310803256,6694,ConcurrencyPractitioner,2019-08-05T21:39:26Z,"Alright, I have done thorough investigations and here is what I found. I came across the following on stack overflow:
https://stackoverflow.com/questions/43887307/converting-string-to-utf-8-byte-array-returns-a-negative-value-in-java
If one looks closely, they would quickly realize that UTF8 is not fit for the task at hand. In reality, we would need to do something like using Base64 encodings instead (which is supposedly part of Java since version 1.8). However, gradle does not allow the usage of Base64 since it ""could not be found"" according to the compiler anyways.

In conclusion, I don't think the current approach as it is will work. If things don't progress any further, I'd suggest sticking with the original idea of just sticking the long unencrpyted directly into the string (plus a version number).

Your thoughts @mjsax ? "
311705840,6694,mjsax,2019-08-07T18:42:09Z,`task` is of type `Task` -- no need to cast :)
311707813,6694,mjsax,2019-08-07T18:46:49Z,"Good find! 

> However, gradle does not allow the usage of Base64 since it ""could not be found"" according to the compiler anyways.

How did you try to use it? Everything from the standard library should be available... I would prefer to use Base64 if we can. If not possible, we can still fall back to using String, but I would really like to avoid it if we can."
311713153,6694,mjsax,2019-08-07T18:59:17Z,nit: avoid changes in unrelated files
311714343,6694,mjsax,2019-08-07T19:02:03Z,Why not do this unconditionally? If it's not `clear` we won't commit anyway. It's seems cleaner to avoid to many branches and it's not on the hot code path so the overhead of updating `partitionTime` is not relevant.
311714674,6694,mjsax,2019-08-07T19:02:49Z,I am not sure why we need this variable? Can you elaborate?
311817120,6694,ConcurrencyPractitioner,2019-08-08T00:45:54Z,"Alright, that's fine."
311817419,6694,ConcurrencyPractitioner,2019-08-08T00:47:53Z,"Oh, because we need to differentiate between a commit that is triggered by a regular process or by a close. If we call partitionTime() in a commit triggered by a close() call, then partitionTime() would always return -1. (recall that due to the order of operations in close, the partition times has been reset to -1 first before the commit call was made). 

 "
311819651,6694,ConcurrencyPractitioner,2019-08-08T01:01:37Z,"Ah okay, so this is the error that I've found.

```/Users/richardyu/kafka/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamTask.java:50: error: package org.bouncycastle.util.encoders does not exist```

Notice that the package name does not start with java. It might be some third party library that gradle does not account for.
"
311819717,6694,ConcurrencyPractitioner,2019-08-08T01:02:01Z,@mjsax This might help explain why Base64 might not be used.
311820182,6694,ConcurrencyPractitioner,2019-08-08T01:05:13Z,Yeah. My bad. Didn't realize T extends Task.
311849604,6694,mjsax,2019-08-08T04:14:21Z,Seem you include the wrong class/package: https://docs.oracle.com/javase/8/docs/api/java/util/Base64.Encoder.html
311850528,6694,mjsax,2019-08-08T04:20:46Z,"Is the any advantage of this library compare to https://docs.oracle.com/javase/8/docs/api/java/util/Base64.Encoder.html

\cc @ijuma"
311851742,6694,mjsax,2019-08-08T04:30:36Z,"Thanks. Understood.

It might be better, to actually change `Stream#commit(boolean startNewTransaction)` to accept a second parameter `Map<TopicPartition, Long> partitionTimes` to pass in the information.

In `close()` before we actually ""loose"" the timestamps we preserve them and pass into `commit()` later. In a regular `commit()` we get the timestamps from the `partitionGroup` (ie, some code that is now in `commit(boolean)` would go into `commit()`).

This would avoid the requirement to introduce the flag and make the code more readable, because decision are more local an encapsulated in each method without cross-method dependencies."
311852052,6694,mjsax,2019-08-08T04:32:39Z,We should return `RecordQueue.UNKNOWN` instead.
311852198,6694,mjsax,2019-08-08T04:33:39Z,"Same as above:

Also, we should log a WARN message there, that the the found metadata is corrupted and cannot be decoded."
311859113,6694,ConcurrencyPractitioner,2019-08-08T05:20:50Z,"Actually, just realized that this library existed. :P 
Didn't know until later. Will remove this dependency (the old bouncycastle one). "
311862190,6694,ijuma,2019-08-08T05:38:39Z,Sounds good.
312137582,6694,ConcurrencyPractitioner,2019-08-08T16:48:16Z,"Oh, sure, that would work."
312165368,6694,mjsax,2019-08-08T17:51:53Z,"Passing in `null` is not idea IMHO. At this point, we _know_ that we want to get the timestamps from the `PartitionGroup`. Hence, seems better to build up the correct `Map< TopicPartition, Long>`, by looping over all committed offsets:
```
final Map<TopicPartition, Long> partitionTimes = new HashMap<>(consumedOffsets.size);
for (final Map.Entry<TopicPartition, Long> entry : consumedOffsets.entrySet()) {
    partitionTimes.put(entry.key, partitionGroup.partitionTimestamp(partition));
}
commit(partitionTimes);
```"
312165647,6694,mjsax,2019-08-08T17:52:27Z,I think we don't need this method if we apply my other suggestions
312166158,6694,mjsax,2019-08-08T17:53:38Z,"Input parameter `partitionTimes` should always contain the correct partition time, hence, we can just get it:
```
final long partitionTime = partitionTimes.get(partition);
```"
312166420,6694,mjsax,2019-08-08T17:54:11Z,nit: `partitionTimeMap` -> `partitionTimes`
312167539,6694,mjsax,2019-08-08T17:56:37Z,This block can be moved outside of the `try-catch-block`
312167714,6694,mjsax,2019-08-08T17:56:57Z,I guess we can remove this comment
312169542,6694,mjsax,2019-08-08T18:00:42Z,Might be good to add an `else` and also add a DEBUG log stating that no committed offset was found
312170299,6694,mjsax,2019-08-08T18:02:20Z,This method is not only _receiving_ but also _setting_ the partition time. What about renaming it to `initializePartitionTime()`
312170692,6694,mjsax,2019-08-08T18:03:15Z,return type is `void` -- remove this line
312171157,6694,mjsax,2019-08-08T18:04:15Z,"nit: add comment `// visible for testing` (same for decodeTimestamp() below)

Also add test methods to `StreamTaskTest` to test both methods."
312171865,6694,mjsax,2019-08-08T18:05:40Z,"Nit: (simplify to) `""Unsupported offset metadata version found. Supported version {}. Found version {}.""`"
312235875,6694,mjsax,2019-08-08T20:47:29Z,nit: could we use `getStartedStreams()` again?
312237424,6694,mjsax,2019-08-08T20:51:27Z,"`assertThat(task.decodeTimestamp(consumer.committed(partition1).metadata()), equalTo(DEFAULT_TIMESTAMP));`

Simplify `task.cosumer` -> `consumer`"
312238609,6694,mjsax,2019-08-08T20:54:22Z,nit: remove unnecessary comment
312239083,6694,mjsax,2019-08-08T20:55:32Z,nit: remove unnecessary comment
312239225,6694,mjsax,2019-08-08T20:55:50Z,nit: remove unnecessary comment
312289819,6694,mjsax,2019-08-09T00:06:23Z,"This test setup defeats the purpose fo this test. If EOS is enabled, the producer is used to commit offsets, and thus, we should check if the producer does commit the corresponding metadata correctly.

Therefore, we need to change the test setup a little bit. In `createStatelessTask()`, we create an anonymous `ProducerSupplier` and we need to get hold off the generated mock-producer instance. We can then use `MockProducer#consumerGroupOffsetsHistory` to get the committed offsets and metadata."
312289927,6694,mjsax,2019-08-09T00:07:06Z,as above (similar below)
312309368,6694,ConcurrencyPractitioner,2019-08-09T02:23:56Z,Done that.
312581629,6694,mjsax,2019-08-09T17:38:06Z,Not 100% sure if we nee this `null` check any longer after the refactoring. \cc @guozhangwang @bbejeck @vvcephei @ableegoldman @cadonna @abbccdda WDYT?
312588676,6694,mjsax,2019-08-09T17:57:06Z,"Seems we should test `partitionTimestamp` above already, when we `verifyBuffered(6, 3, 3);` ?

Also, we should check the returned time for both partitions each time?

I would also add a test of `group.streamTime()` for each step in the test (not sure why it's missing -- this would be a good additional improvement)."
312588867,6694,mjsax,2019-08-09T17:57:43Z,We should test this method in it's own test method
312589597,6694,mjsax,2019-08-09T17:59:44Z,"use `assertThrows` instead of the try catch block, and use `assertThat` to verify the exception message.

Also, this should be two tests, one for ""set"" and one for ""get""."
312590312,6694,mjsax,2019-08-09T18:01:52Z,"In addition, we should check if ""stream time"" was updated correctly.

nit: `assertEquals` take ""excepted value"" as first parameter, so you need to flip both (otherwise the error message would be confusing if the test fails)"
312592021,6694,mjsax,2019-08-09T18:07:04Z,Please rewrite using `assertThat` (or at least `assertEquals`) -- similar below
312592448,6694,mjsax,2019-08-09T18:08:18Z,"We should add more test method for the different error cases, too."
312593338,6694,mjsax,2019-08-09T18:10:52Z,"This whole block (L680-687) can be remove -- there is no need for the test to commit anything in addition. Note that the `assert` above tests the previous `commitSync()` what is not useful, as test-code should not test other test-code :)"
312595808,6694,mjsax,2019-08-09T18:17:49Z,"No need to use a nested for loops. This can be simplified to:
``` 
final String metadata = metadataList.get(topic1).get(partition1).metadata();
assertThat(task.decodeTimestamp(metadata), equalTo(DEFAULT_TIMESTAMP));
```"
312634550,6694,ableegoldman,2019-08-09T20:18:36Z,"Yeah, I don't see how a partition could be in `consumedOffsets` but not in `partitionTimes`?"
313846296,6694,cadonna,2019-08-14T12:26:00Z,Why do you need this assertion? `close()` waits for `Long.MAX_VALUE` for state `NOT_RUNNING`. 
313850160,6694,cadonna,2019-08-14T12:36:04Z,"The order of the imports in Kafka Streams is usually as follows:

Kafka imports and 3rd-party imports in one block

a block of `java.*` imports 

`import static`."
314216469,6694,cadonna,2019-08-15T08:26:06Z,"Wouldn't creating a new task be better? AFAIK, that is what happens during a restart. No need to simulate anything. Furthermore, it avoids introducing a new method just for testing."
314220806,6694,cadonna,2019-08-15T08:39:56Z,"Would be good to extract `""stream-task-test""` to a member field of the test and use it in `createConfig()` and here. "
314221852,6694,cadonna,2019-08-15T08:43:09Z,see above
314223844,6694,cadonna,2019-08-15T08:49:05Z,see above
314223931,6694,cadonna,2019-08-15T08:49:18Z,Why do we need this assertion here?
314226105,6694,cadonna,2019-08-15T08:55:49Z,"I see that you tested the different error cases as @mjsax suggested. However, I would put each test in its own test method.  "
314227123,6694,cadonna,2019-08-15T08:58:51Z,See my comment on the usage of this method in `StreamTaskTest`.
314230884,6694,cadonna,2019-08-15T09:09:53Z,I would put methods to write and read record metadata in their own classes. Those classes would be kind of SerDes for metadata. Such SerDes would make the code better testable and separates the concerns of a task and reading and writing metadata which are completely independent. It does not need to be done in this PR. I just wanted to mention it. 
314236760,6694,cadonna,2019-08-15T09:28:24Z,Please remove empty line before this line.
314238406,6694,cadonna,2019-08-15T09:33:50Z,"This tests misses to verify whether `streamTime` is set or not.

Furthermore, I would write two (or three) distinct tests:
- `partitionTimestamp` is set (could be further split for `streamTime` is set or not)
- `NullPointerException` is thrown "
314241100,6694,cadonna,2019-08-15T09:42:10Z,The code block from the beginning of the method until here can be extracted and re-used in this and the previous test methods.
314241548,6694,cadonna,2019-08-15T09:43:59Z,"I think, we use `should...` for newly added test methods."
314375243,6694,ConcurrencyPractitioner,2019-08-15T15:49:28Z,Its just to confirm that there's no problems with the state being removed. Thought it would be good to keep that at the very least.
314398959,6694,ConcurrencyPractitioner,2019-08-15T16:46:54Z,"Yeah, that would probably be a good idea in the future."
314402987,6694,ConcurrencyPractitioner,2019-08-15T16:57:09Z,"Yeah, it can be removed. Its somewhat redundant."
320293937,6694,cadonna,2019-09-03T14:11:38Z,Here it would be better to call `partitionQueues.get(partition)` only once and store its result in a variable. Then check the variable for `null` and call `partitionTime()` on the variable.   
320294207,6694,cadonna,2019-09-03T14:12:03Z,Same as above.
321203916,6694,cadonna,2019-09-05T11:18:20Z,"I am wondering whether we can do better here. Encoding partition time in Base64 seems to me a bit a waste of space. As far as I can see, a 8 byte value is encoded in 11 bytes with Base64. Would be great, if we could store partition time in 8 bytes. 
I am also wondering why `metadata` in `OffsetAndMetadata` is a `String` and not something more bytes friendly.   "
321938111,6694,ConcurrencyPractitioner,2019-09-06T23:14:51Z,"@cadonna Yeah, it is still unclear at this point if the ```metadata``` field in ```OffsetAndMetadata``` could be used in this manner. @guozhangwang or @hachikuji knows this matter better. Anyhow, OffsetAndMetadata right now is the only medium through which we can checkpoint partition time anyways. So we might be stuck with using the ```metadata``` field."
323392057,6694,mjsax,2019-09-11T18:25:53Z,"I know that I recommended to add this parameter, but now, after more refactoring of the code, I am not sure  any longer why we need it? It seems that this method is called twice and both calls pass in the result of `extractPartitionTimes()` as parameter -- hence, it seems we can remove the parameter and do the call to `extractPartitionTimes()` within the method itself?"
323397342,6694,mjsax,2019-09-11T18:37:46Z,"This is a blocking call, and @guozhangwang just proposed KIP-520 to make it more efficient by allowing to pass in multiple partitions at once. Should we wait for KIP-520 to be implemented? If now, we should make sure the update this code after KIP-520 is merged.

I am also wondering how we should handle `TimeoutException` for this call? Maybe not, but might be worth to clarify?

\cc @guozhangwang "
323496050,6694,mjsax,2019-09-11T23:03:23Z,"I don't have the full context on the history, but it would not be easy to change the API... I talked to Jason about it, and it seem we can just move forward with this PR as-is, and could do a KIP later that allows us to store metadata as `byte[]` type if we really need to change it. Atm, the metadata is just a few bytes and the overhead does not really matter IMHO."
323621863,6694,cadonna,2019-09-12T08:40:55Z,Agreed
324510011,6694,mjsax,2019-09-16T04:55:22Z,I remember now -- can we add a comment to explain that we need to get `partitionTimes` before we `closeTopology()` (sorry for my previous comment -- forgot about that)
324917516,6694,guozhangwang,2019-09-16T22:48:38Z,In my PR (https://github.com/apache/kafka/pull/7304) I've refactored this part in StreamTask. I'd suggest we merge that one before this.
324926772,6694,ConcurrencyPractitioner,2019-09-16T23:29:05Z,"Cool, got it done."
325385581,6694,guozhangwang,2019-09-17T21:05:42Z,Just realized I need to do another rebase on my PR. So if this PR is closer to be merged I'd suggest @RichardYuSTUG @mjsax you guys just move forward and I will rebase mine later. 
325432250,6694,ConcurrencyPractitioner,2019-09-17T23:53:00Z,"@mjsax Cool, sounds good. In that case, we could get this one merged since it is about complete."
184775884,4931,rhauch,2018-04-27T18:47:37Z,It'd be nice to have JavaDoc for this interface (and all other public API types) that explains the purpose.
184776184,4931,rhauch,2018-04-27T18:48:44Z,"Nit: this method returns a list of connector _names_, not connector instances. "
184776264,4931,rhauch,2018-04-27T18:49:05Z,How about just `ConnectorState`?
184776561,4931,rhauch,2018-04-27T18:50:16Z,"Missing JavaDoc on the type and method. IMO, the JavaDoc on the interface should fully describe how to provide an implementation (by implementing this class, but what about other interfaces), how to package it (e.g., Java Service Provider file), and how to install it (put it on the plugin path).

It should also go into detail about how Connect uses this interface, when implementations are instantiated and when the register method is called, and when the close method is called.

What exceptions can be thrown by this method? Will the supplied context ever be null? What are the behaviors that are expected/allowed? What happens when a resource is already registered?"
184777667,4931,rhauch,2018-04-27T18:54:30Z,"It should be clear that implementations are provided by the framework, but it should be clear what this does and how it can be used by extension implementations."
184777830,4931,rhauch,2018-04-27T18:55:11Z,"Why not an interface? That would offer us so much more flexibility in the implementation, and all of the implementation details can be hidden from the public API. By having the nested classes here, they are in the public API and need to be managed through KIPs."
184778299,4931,rhauch,2018-04-27T18:57:11Z,Minor: let's not add unnecessary whitespace.
184778412,4931,rhauch,2018-04-27T18:57:41Z,Unnecessary whitespace.
184778471,4931,rhauch,2018-04-27T18:57:58Z,This can be `final`.
184778842,4931,rhauch,2018-04-27T18:59:34Z,How about JavaDoc that explains that this class is for hiding the JAX-RS framework implementation and for handling registration of duplicate resources.
184779307,4931,rhauch,2018-04-27T19:01:24Z,"As I mentioned earlier, we need to use Java Service Provider API to find which plugin has the specified implementation, and we should change Plugins to support creating these instances. This code only works if the implementations are on the classpath."
184829319,4931,mageshn,2018-04-27T23:48:41Z,Yes Agreed. I still haven't integrated with the plugin classloader yet. Hence used this method for the draft so that  we get a better picture of the public interfaces.
184829507,4931,mageshn,2018-04-27T23:51:05Z,ConnectorStateDetails provides both the ConnectorState and Connector TaskState. The original entity class has a name of ConnectorStateInfo but didn't want to use the same name because i thought it might be a little confusing
184829705,4931,mageshn,2018-04-27T23:53:26Z,"The nested classes are also technically part of the public API since user can access it. But IMO, having interfaces for POJO or entities might be a little bit of overhead.  The only thing we can hide by making it an interface is the constructor."
184830969,4931,rhauch,2018-04-28T00:09:38Z,"Using an interface gives us more options on the implementation side, and it helps clarify intentions more explicitly and minimally, without exposing implementation details."
184830991,4931,rhauch,2018-04-28T00:09:56Z,Or maybe just `ConnectorDetail` or `ConnectorDescription`?
184831075,4931,rhauch,2018-04-28T00:11:01Z,"Is it intentional that these tasks are ordered? To the indexes correspond to the task ID? If not, perhaps a Collection is sufficient, or a Map keyed by task ID."
185244935,4931,rhauch,2018-05-01T15:12:34Z,"First, the iterator returned by the `ServiceLoader.load` method will be instances of `T`, not `Class<? extends T>`, so the call to `addPluginDesc` above should result in a class cast exception.

Third, since these are already instantiated by the ServiceLoader and match the specified type, why not forgo the logic in `addPluginDesc` and simply instantiate a `PluginDesc like the only thing we need to do is 

Finally, `ServiceLoader` is `Iterable`, which means the iterator logic can be simpler.

Put all these three together:

```
Collection<PluginDesc<T>> result = new ArrayList<>();
for ( T impl : ServiceLoader.load(klass, loader) ) {
    String version = impl instanceof Versionable ? ((Versionable)impl).version() : ""undefined"";
    result.add(new PluginDesc<>(impl.getClass(), version, loader);
}
```

In order to do this, we would need to add a `Versionable` interface so that Connector and REST Extension can extend it. "
185245899,4931,rhauch,2018-05-01T15:16:10Z,"I know that much of this logic was from the original code, but it might be good to add some debug/trace log messages here, especially in the missing `else` condition (e.g., ""Skipping {} since it is not a concrete type."")."
185246193,4931,rhauch,2018-05-01T15:17:10Z,This won't work for interfaces other than `Connector`.
185256557,4931,mageshn,2018-05-01T15:56:01Z,"Good catch @rhauch on the ServiceLoader returning an implementation. Having said that, do you think we should cache these implementations instead of creating an instance ourselves? I don't personally see a benefit to using the same instance the one returned by the ServiceLoader."
185260491,4931,rhauch,2018-05-01T16:12:16Z,"If we can make this method more generic or general purpose, then this method should probably be called within a conditional block checking whether it implements `Configurable`."
185261384,4931,rhauch,2018-05-01T16:15:52Z,This method does not have much logic that is specific to `ConnectRestExtension`. Have you thought about making this more general-purpose to load any extensions (other than when a more specific method is available)? Doing that might make it easier to add future extensions.
185262140,4931,rhauch,2018-05-01T16:18:36Z,"The ServiceLoader is usually just used directly to load all instances of a particular extension point, but that's not really what we do. And really there's nothing special about _how_ ServiceLoader instantiates the class: it really just does a new instance on the class."
185262431,4931,rhauch,2018-05-01T16:19:51Z,"BTW, we should have a test case that loads a test implementation of `RestExtension` using Plugins."
185384782,4931,mageshn,2018-05-02T03:11:12Z,"@rhauch yes, I will be adding more tests. Once we all agree on the public interfaces, I will update the KIP and refine this PR"
185385261,4931,mageshn,2018-05-02T03:17:30Z,would be good to have state or status in there since its just not a ConnectorDescription
185537702,4931,rhauch,2018-05-02T15:26:54Z,"Use ""component"" rather than ""plugin""?"
185539183,4931,rhauch,2018-05-02T15:30:46Z,"How about `ConnectorHealth`, since this is likely to be used by health check extensions? The `Detail` in `ConnectorStateDetail` just seems superfluous to me."
185540424,4931,rhauch,2018-05-02T15:34:18Z,"Suggest at a minimum:
> Comma-separated names of REST extension classes, loaded and called in the order specified."
185540807,4931,rhauch,2018-05-02T15:35:30Z,This logic is in two places. How about a helper method? If it were named `vesrionFor(Class<T> clazz)` then it could be more easily inlined where it's used.
185892000,4931,wicknicks,2018-05-03T18:13:24Z,"Maybe you guys have already talked about this, but does it make sense to have a per extension prefix for the configs? similar to how it is done for Transformations?"
185893483,4931,wicknicks,2018-05-03T18:18:40Z,should we pass in `connectRestExtensionContext` with the `newConnectRestExtensions(..)` method above? it will be cleaner to create and register the plugin in one place. 
185903792,4931,wicknicks,2018-05-03T18:56:19Z,"yeah, I agree with Randall here. `Detail` is not a good fit here. Maybe you can use `ConnectorsContext`?"
185904747,4931,wicknicks,2018-05-03T18:59:53Z,should we expose connector metrics here? it could be a good fit for the health check resource in the KIP).
186196449,4931,kkonstantine,2018-05-04T19:43:23Z,I think we are stretching the use `-able` here. I'd suggest `Versioned` as one of the less frequent cases where we'd use an adjective that does not and in `-able` but which makes a lot of sense for the functionality that this interface describes. 
186197396,4931,kkonstantine,2018-05-04T19:47:59Z,javadoc?
187750001,4931,kkonstantine,2018-05-11T22:52:32Z,I think we are stretching the use -able here. I'd suggest `Versioned` as one of the less frequent cases where we'd use an adjective that does not and in -able but which makes a lot of sense for the functionality that this interface describes.
187750848,4931,kkonstantine,2018-05-11T23:00:25Z,nit: missing `@return`
187750866,4931,kkonstantine,2018-05-11T23:00:33Z,nit: missing `@return`
187756954,4931,kkonstantine,2018-05-12T00:10:00Z,"If we want to be precise, we shouldn't mention `list` here. This is implementation specific but the interface could return a set or actually anything that is a `Collection`. I'd love if we could rephrase the descriptions here to keep our options open. "
187757323,4931,kkonstantine,2018-05-12T00:15:16Z,"I'd suggest referring to plugin path as only `plugin.path` and basically when it makes more sense referring to the parameter. 

Maybe here you could replace with something like: `For the Connect's class loader's to be able to discover the ..."""
187757374,4931,kkonstantine,2018-05-12T00:16:11Z,"typo? `place` -> `placed`?
Also maybe rephrase into shorter sentences to make easier to follow?"
187757473,4931,kkonstantine,2018-05-12T00:17:59Z,"I'm torn about upper casing here. Feels like lower cased words make more sense: `security (authentication and authorization), logging, request validations, etc`. ..."
187757938,4931,kkonstantine,2018-05-12T00:25:48Z,"nit: Framework means Connect here I assume. So, maybe it's better to call this out as: 
`Connect` or `Connect framework`"
187758006,4931,kkonstantine,2018-05-12T00:27:05Z,nit: again I'd use lower case for anything that's not a name or a code class (mostly): `Connect resources`
187758037,4931,kkonstantine,2018-05-12T00:27:34Z,nit: extra blank line
187758071,4931,kkonstantine,2018-05-12T00:28:08Z,nit: `provides the ability`?
187758104,4931,kkonstantine,2018-05-12T00:28:58Z,`the Connect framework`? which framework?
187758167,4931,kkonstantine,2018-05-12T00:29:55Z,Description is missing. I'm not a fan of javadoc that contains only `@return`. Maybe most text could be in the description and `@return` could be brief. 
187758286,4931,kkonstantine,2018-05-12T00:31:44Z,"typos: return a -> return an (no 1+ spaces), ot -> to"
188085678,4931,kkonstantine,2018-05-14T20:22:57Z,This applies unchecked overriding of the return type. In `ConnectRestExtensionContext` its `Configurable<? extends Configurable>` and the same should be used in the member field as well as the return type here. 
188085791,4931,kkonstantine,2018-05-14T20:23:17Z,`Configurable<? extends Configurable>` same as below
188086026,4931,kkonstantine,2018-05-14T20:24:04Z,"nit: extra blank line, here and elsewhere in this class. "
188086886,4931,kkonstantine,2018-05-14T20:26:56Z,"javadoc would be nice, here and in the rest of the public inner classes (especially since we have `AbstractState` elsewhere too)"
188087495,4931,kkonstantine,2018-05-14T20:28:53Z,"Should this be called `taskId`, equivalently to `workerId` above? It'll make initialization and usage clear (I think)"
188088345,4931,kkonstantine,2018-05-14T20:31:44Z,"An `enum` with the same name and the same `toString` implementation is already defined here: 
`org.apache.kafka.connect.runtime.rest.entities.ConnectorType`

Do we need to add this one?"
188089609,4931,kkonstantine,2018-05-14T20:35:59Z,"it's common to write `username` as `password` (instead of `passWord`). Would you agree changing it wherever we use `userName` in this PR?

(check with `grep -rl userName`)"
188090162,4931,kkonstantine,2018-05-14T20:37:47Z,Not clear what's the meaning of `32` here. Can we declare an intuitive `static final` variable?
188090361,4931,kkonstantine,2018-05-14T20:38:33Z,also a nice candidate for `static final` member variable. 
188091528,4931,kkonstantine,2018-05-14T20:42:51Z,Should we throw `UnsupportedCallbackException` if it doesn't match? Especially since we declare it and that's the intended use of this exception according to the interface (actually we're violating the interfaces contract if we don't). 
188093766,4931,kkonstantine,2018-05-14T20:50:37Z,"Probably makes sense to be `static`, especially because of its size. "
188093995,4931,kkonstantine,2018-05-14T20:51:27Z,nit: should be in the same line as above
188094029,4931,kkonstantine,2018-05-14T20:51:33Z,nit: should be in the same line as above
188094120,4931,kkonstantine,2018-05-14T20:51:51Z,"nit: should be in the same line as above

"
188094141,4931,kkonstantine,2018-05-14T20:51:56Z,"nit: should be in the same line as above

"
188094184,4931,kkonstantine,2018-05-14T20:52:05Z,"nit: should be in the same line as above

"
188094421,4931,kkonstantine,2018-05-14T20:52:55Z,"`class names`. No caps (same as classloader, etc). "
188094764,4931,kkonstantine,2018-05-14T20:53:59Z,"nit: you may fit args in one line, as in `newHeaderConverter`"
188094969,4931,kkonstantine,2018-05-14T20:54:37Z,nit: extra space between: `of  plugins`
188096167,4931,kkonstantine,2018-05-14T20:58:50Z,"no caps pls. `EMPTY` -> `empty list`, `NULL` -> `{@code null}`"
188096202,4931,kkonstantine,2018-05-14T20:58:58Z,"no caps pls. EMPTY -> empty list, NULL -> {@code null}"
188097792,4931,kkonstantine,2018-05-14T21:04:02Z,"Also the generic name of this method contradicts its functionality. In not general, but pertains only to `ConnectRestExtension`. Should probably be named same as with the others above, until we perform some short of consolidation in the functionality. So `newConnectRestExtensionPlugins` here"
188097949,4931,kkonstantine,2018-05-14T21:04:28Z,"For the same reasons as above, this should probably be: `newConnectRestExtensionPlugin`. The log message bellow shows that this method is specific to `ConnectRestExtensions `"
188099111,4931,kkonstantine,2018-05-14T21:08:33Z,nit: extra blank line
188099600,4931,kkonstantine,2018-05-14T21:10:17Z,~java.util.~ Collection
188100389,4931,kkonstantine,2018-05-14T21:13:15Z,should be `final`
188100820,4931,kkonstantine,2018-05-14T21:14:44Z,"If that's a TODO comment (which we should avoid adding if we can), it should be marked as `//TODO: log ... something more`

I'm only guessing what it means here. "
188100846,4931,kkonstantine,2018-05-14T21:14:51Z,"If that's a TODO comment (which we should avoid adding if we can), it should be marked as `//TODO: log ... something more`"
188101268,4931,kkonstantine,2018-05-14T21:16:21Z,both exception are unused here. 
188101632,4931,kkonstantine,2018-05-14T21:17:43Z,"Symmetrically to the above similar method, it's better if `result` is declared close to where it's used. Here just before the `for (T impl : serviceLoader)` loop"
188102042,4931,kkonstantine,2018-05-14T21:19:11Z,"nit: alignment probably better as: 
```java
ServiceLoader<T> serviceLoader = loader instanceof PluginClassLoader 
                                 ? ServiceLoader.load(klass, loader)
                                 : ServiceLoader.load(klass);
```"
188108937,4931,mageshn,2018-05-14T21:47:39Z,There was a debate about moving the required entity class from runtime to api but we decided not to. Hence we see this copy. The ConnectHealth class introduced itself is pretty much same as ConnectorStateInfo. I personally still think that entities can be part of the public API. But i'm fine either ways.
188110010,4931,mageshn,2018-05-14T21:52:15Z,"It started as newConnectRestExtensionPlugins and based on earlier PR discussion, we decided to make it generic. May be I missed out generalizing some of the log statements"
188802909,4931,rhauch,2018-05-16T23:40:05Z,"I'm fine with `Versioned`, since this interface defines something that has a version and is not something that can be versioned."
188803007,4931,rhauch,2018-05-16T23:40:51Z,Nit: REST should be capitalized.
188803095,4931,rhauch,2018-05-16T23:41:21Z,"""plugin class loading mechanism"" rather than ""class loader's""."
188803391,4931,rhauch,2018-05-16T23:43:26Z,"""Implementations should be packaged in a JAR that includes the file {@code META-INF/services/org.apache.kafka.connect.rest.extension.ConnectRestExtension} that contains the fully-qualified name of the implementation class.""

Also, `<p>` tags should always start on new lines, and for readability should probably be preceded by a blank line."
188803420,4931,rhauch,2018-05-16T23:43:39Z,+1 for fixing this.
189633739,4931,rhauch,2018-05-21T16:00:00Z,JavaDoc for this interface. A simple sentence would suffice.
189633925,4931,rhauch,2018-05-21T16:00:34Z,JavaDoc ... a simple sentence would suffice.
189634016,4931,rhauch,2018-05-21T16:00:54Z,JavaDoc ... a simple sentence would suffice.
189634067,4931,rhauch,2018-05-21T16:01:02Z,JavaDoc ... a simple sentence would suffice.
189634263,4931,rhauch,2018-05-21T16:01:43Z,JavaDoc ... a simple sentence would suffice.
189634410,4931,rhauch,2018-05-21T16:02:07Z,"""REST"" is an acronym and should be capitalized."
189634561,4931,rhauch,2018-05-21T16:02:37Z,Paragraphs should begin on a new line and have a blank line before them in JavaDoc.
189634630,4931,rhauch,2018-05-21T16:02:48Z,Paragraphs should begin on a new line and have a blank line before them in JavaDoc.
189634810,4931,rhauch,2018-05-21T16:03:21Z,"""Connect"" is a name and should be capitalized."
189635129,4931,rhauch,2018-05-21T16:04:30Z,"Use quotes or `{@code }` around ""KafkaConnect"" to highlight the importance of that literal."
189635519,4931,rhauch,2018-05-21T16:05:38Z,"How about also mentioned that it must be configured in the worker configuration, and providing an example configuration fragment that shows how to use this sample extension."
189636428,4931,rhauch,2018-05-21T16:08:40Z,"Should this mention that this is a sample implementation? How does it work, and where does it get the credentials? Seems like this needs a lot more context to be useful as an example."
189637087,4931,rhauch,2018-05-21T16:10:39Z,How about `to Connect's REST API` rather than `to Connect REST`?
189637462,4931,rhauch,2018-05-21T16:11:43Z,Nit: this can be defined on one line.
189637562,4931,rhauch,2018-05-21T16:12:03Z,Nit: This can be one line.
189637966,4931,rhauch,2018-05-21T16:13:27Z,"Nit: would using `state` rather than `taskStateFromHerder` make this a bit more readable, such that the instantiation of the `TaskState` could be done on a single line?"
189639210,4931,rhauch,2018-05-21T16:17:52Z,Why do we need this conditional logic? Is it ever called with a ClassLoader that is not a PluginClassLoader?
189640314,4931,rhauch,2018-05-21T16:22:00Z,"This should be usable by plugin types other than just Connect REST extensions, so I think the non-specific name is important. If we want a specific name for the Connect REST extension, then we should add that as the public method and keep this as a protected/private method."
189640748,4931,rhauch,2018-05-21T16:23:33Z,Seems like this boolean check is backwards. Shouldn't this method return true if the component is _not_ already registered?
189640850,4931,rhauch,2018-05-21T16:23:59Z,"Same incorrect boolean logic here, too."
189641853,4931,rhauch,2018-05-21T16:27:28Z,"Is this the only REST extension that we'll have in this project? If so, should we have a better and more descriptive name for the project rather than simple `rest-extension`?"
189966266,4931,rhauch,2018-05-22T16:22:13Z,"Perhaps ""Connect requires some components implement this interface to define a version string."""
189966612,4931,rhauch,2018-05-22T16:23:17Z,"Again, capitalize ""Connect"" as a name rather than ""connect"" as a verb."
189966880,4931,rhauch,2018-05-22T16:24:09Z,Need JavaDoc here. What does the string value represent? Can this method ever return null or an empty string?
189966913,4931,rhauch,2018-05-22T16:24:14Z,Need JavaDoc here. What does the string value represent? Can this method ever return null or an empty string?
189966983,4931,rhauch,2018-05-22T16:24:29Z,Need JavaDoc here. What does the string value represent?
189967160,4931,rhauch,2018-05-22T16:25:04Z,"Are there any requirements about whether these can be null or empty? What is `trace`? Need JavaDoc since this is part of the API, and also verify the arguments match the requirements."
189967589,4931,rhauch,2018-05-22T16:26:22Z,"""Connector"" is not a name and should not be capitalized."
189967877,4931,rhauch,2018-05-22T16:27:17Z,"Grammar: It's more correct to say ""Get the names of the connectors currently running..."", since the names are not running in the cluster. :-)

Also, do the connectors need to be running for them to be included here? What if they died and were not restarted? I suggest the statement refer to connectors **_deployed_** in this cluster."
189968731,4931,rhauch,2018-05-22T16:30:01Z,Need JavaDoc here to define which parameters can be null and/or empty.
189968885,4931,rhauch,2018-05-22T16:30:28Z,"Add JavaDoc, since this is part of the public API."
189968977,4931,rhauch,2018-05-22T16:30:48Z,"Add JavaDoc, since this is part of the public API. Which of the parameters are allowed to be null and/or empty?"
189969183,4931,rhauch,2018-05-22T16:31:28Z,"Nit: remove the unnecessary ""the"" on this line."
189969774,4931,rhauch,2018-05-22T16:33:18Z,"Nit: I'd suggest ""The implementation class must be packaged in a JAR that includes the {@code ...} file containing the fully qualified name of the implementation class."""
189970477,4931,rhauch,2018-05-22T16:35:32Z,"Suggest: ""When Connect's worker configuration uses the REST extension implementation class, upon startup Connect will instantiate the implementation and pass the configuration to the instance via {@link Configurable#configure(Map)}."""
189970682,4931,rhauch,2018-05-22T16:36:13Z,"Use ""The Connect framework ..."" instead."
189970767,4931,rhauch,2018-05-22T16:36:35Z,"Nit: singular ""implementation"""
189971395,4931,rhauch,2018-05-22T16:38:35Z,"Rather than use `<code>` tags, RST files use two sequential back quotes before and after code-like text."
189971743,4931,rhauch,2018-05-22T16:39:36Z,"Grammar: ""... allows you to inject into Connect's REST API user defined resources like filters."""
189971830,4931,rhauch,2018-05-22T16:39:53Z,Replace the `<code>` tags.
190068865,4931,mageshn,2018-05-22T22:09:26Z,Other places like metrics reporter use the same convention. Tried to be consistent with it.
190069307,4931,mageshn,2018-05-22T22:11:13Z,"ATM, this is the only implementation. I named it just like other components for transforms. I'm not too particular about the module name being generic. I could call it connect-basic-auth-extension"
190759541,4931,rhauch,2018-05-24T23:42:08Z,"Yeah, maybe `basic-auth-extension` (e.g., to go with `file`, etc.). I'm just concerned that `rest-extension` is pretty generic and actually sounds like it's the API, not a reference impl."
190793197,4931,mageshn,2018-05-25T05:18:35Z,@rhauch Other places like metrics reporter class use the same convention of using &lt;code&gt; tried to be consistent with the same.
190793246,4931,mageshn,2018-05-25T05:19:06Z,This is fixed
190955448,4931,mageshn,2018-05-25T17:06:13Z,"@wicknicks the method is now generic enough to instantiate any plugin. Irrespective, I think registering resources belongs in RestServer."
190956787,4931,rhauch,2018-05-25T17:12:32Z,ack.
190958825,4931,rhauch,2018-05-25T17:20:37Z,"Don't we want these classes to be in a different package than `org.apache.kafka.connect.rest.extension`, since that's the package that exists in the API? The file source and sink, for example, are in `org.apache.kafka.connect.file`.

I'd recommend something like `org.apache.kafka.connect.extension.auth.jaas`."
190959257,4931,rhauch,2018-05-25T17:22:11Z,"Also, since this is a reference implementation, perhaps we could have some JavaDoc that explains at a high level how this implements the `ConnectRestExtension`, how it works (briefly), and how it is packaged."
190959594,4931,rhauch,2018-05-25T17:23:23Z,"Make sure this package name is changed accordingly. Also, this is a good reason why we want a different package name, since this looks like the extension implementation is built-in to the Connect framework."
190959834,4931,rhauch,2018-05-25T17:24:18Z,"Are we okay with people using this in production? If not, we need to say so. If we're okay with it, we should probably outline a few caveats or important things to keep in mind when evaluating whether to use it. For example, passwords will be stored in cleartext in the property file. This alone suggests that maybe we should call it out as a sample reference implementation that may not be suitable for production use."
190960784,4931,rhauch,2018-05-25T17:28:23Z,"I think we should highlight the characteristics that users should be aware of, such as that passwords are stored in plaintext in the referenced file, and that because of this it is likely not recommended for production but is instead part of a sample implementation of the Connect REST Extension and should not be used in production. Perhaps the same paragraph with bold `NOTE:` in all of the files in this package."
190961652,4931,rhauch,2018-05-25T17:31:58Z,Can this be private?
190961740,4931,rhauch,2018-05-25T17:32:19Z,Nit: blank line at the beginning of the method is unnecessary.
190962339,4931,rhauch,2018-05-25T17:34:48Z,Nit: `into` rather than `in to`.
190962903,4931,mageshn,2018-05-25T17:36:55Z,ack
190996405,4931,rhauch,2018-05-25T19:59:24Z,Ping.
190996589,4931,rhauch,2018-05-25T20:00:18Z,Maybe just `REST_EXTENSION`?
190996744,4931,rhauch,2018-05-25T20:01:10Z,Always need a sentence in JavaDoc.
190996769,4931,rhauch,2018-05-25T20:01:16Z,Nit: add a period.
190997074,4931,rhauch,2018-05-25T20:02:51Z,"Rather than describe the return in the description (2nd sentence), I'd suggest putting it in the param:

    @return state of the connector or task; never null or empty

It's more concise, and it puts the information where people will look for it."
190997151,4931,rhauch,2018-05-25T20:03:12Z,"Nit: ""ID"" or ""identifier"" (or even ""id""), but not ""Id""."
190997266,4931,rhauch,2018-05-25T20:03:48Z,"Again, remove the 2nd sentence in the description and put it in the param:

    @return the worker ID; never null or empty"
190997323,4931,rhauch,2018-05-25T20:04:07Z,"Again, remove the 2nd sentence in the description and put it in the param:

    @return the trace message; may be null or empty"
190997381,4931,rhauch,2018-05-25T20:04:24Z,"How about `traceMessage`, rather than `trace`?"
190997661,4931,rhauch,2018-05-25T20:05:49Z,"For consistency:

    * @param state - the status of connector or task; may not be null or empty
    * @param workerId - the workerId associated with the connector or the task; may not be null or empty
    * @param traceMsg - any error trace message associated with the connector or the task; may be null or empty
 
            
 "
190997897,4931,rhauch,2018-05-25T20:06:53Z,"How about:

    @return the version string; may not be null or empty"
190998205,4931,rhauch,2018-05-25T20:08:09Z,"`isEmpty()` just checks whether the length is 0, so a string with 1+ whitespace will be allowed. Instead, use:

    assert state != null && !state.trim().isEmpty();"
190998580,4931,rhauch,2018-05-25T20:09:18Z,Nit: period.
190998660,4931,rhauch,2018-05-25T20:09:41Z,This should be a sentence.
190998760,4931,rhauch,2018-05-25T20:10:11Z,"    @return the connector name

Can this be null or empty? If not, then should check in the constructor. And if the framework calls the constructor, asserts are fine; if users can call it, then `Objects.requireNotNull` is better."
190999238,4931,rhauch,2018-05-25T20:12:32Z,"Same here. Sentence, and specify whether it can be null"
190999527,4931,rhauch,2018-05-25T20:13:58Z,"No need to specify the type; it can only get lost and it's already in the signature. Again, full sentences are needed in JavaDoc, and

    @return the state for each task ID; never null"
190999572,4931,rhauch,2018-05-25T20:14:09Z,Check the input parameters here.
190999619,4931,rhauch,2018-05-25T20:14:23Z,Same here.
190999676,4931,rhauch,2018-05-25T20:14:45Z,Need a sentence in JavaDoc.
190999795,4931,rhauch,2018-05-25T20:15:23Z,"Nit: it's sufficient to just say ""Describes the status, worker ID, and any errors associated with a connector."" No need to include a link to this class."
190999885,4931,rhauch,2018-05-25T20:15:51Z,"This is public API, so we need JavaDoc for this class and the enumeration literals."
191000029,4931,rhauch,2018-05-25T20:16:37Z,"""Describes the state, IDs, and any errors of a connector task."" No link, no capitalized ""connector"", and consistent use of ""ID""."
191000240,4931,rhauch,2018-05-25T20:17:44Z,"For consistency, use `; may not be null or empty` in parameter descriptions. Also, no `-` after the parameter name since JavaDoc already handles this. Fix these everywhere."
191000313,4931,rhauch,2018-05-25T20:18:12Z,"Full sentence, and `@return the task ID`"
191000490,4931,rhauch,2018-05-25T20:19:03Z,"Nit: multiple `by` in this sentence, so change this one to `using`."
191001129,4931,rhauch,2018-05-25T20:21:54Z,"Perhaps the following helps better explain all of the packaging requirements:

```
<p>
The extension must be packaged as a plugin, with one JAR containing the implementation classes and a
{@code META-INF/services/org.apache.kafka.connect.rest.extension.ConnectRestExtension} file
that contains the fully qualified name of the class(es) that implement the ConnectRestExtension interface.
The plugin should also include the JARs of all dependencies except those already provided by the 
Connect framework.
<p>
To install into a Connect installation, add a directory named for the plugin and containing the plugin's JARs
into a directory that is on Connect's {@code plugin.path}, and (re)start the Connect worker.
```"
191002545,4931,rhauch,2018-05-25T20:29:16Z,"Nit:
```
<p>When the Connect worker process starts up, it will read its configuration and instantiate all of the
REST extension implementation classes that are specified in the `rest.extension.classes` configuration property.
Connect will then pass its configuration to each extension via the {@link Configurable#configure(Map)} method,
and will then call {@link #register} with a provided context.

<p>When the Connect worker shuts down, it will call the extension's {@link #close} method to allow the
implementation to release all of its resources.
```"
191002688,4931,rhauch,2018-05-25T20:30:02Z,    @return the JAX-RS {@link javax.ws.rs.core.Configurable}; never null
191002712,4931,rhauch,2018-05-25T20:30:12Z,missing a period.
191003024,4931,rhauch,2018-05-25T20:31:52Z,"Change to:

    Provides the cluster state and health information about the connectors and tasks.
    @return the cluster state information; never null"
191003130,4931,rhauch,2018-05-25T20:32:27Z,"Nit: Begin without a link to this class: ""A sample REST extension that authenticates incoming ..."""
191013751,4931,mageshn,2018-05-25T21:30:44Z,I don't think we can guarantee or enforce the version
191061506,4931,ewencp,2018-05-27T00:29:20Z,very minor nit: easier to keep track of these if we group common package prefixes together
191061533,4931,ewencp,2018-05-27T00:31:38Z,Isn't this the same subpackage we're defining rules for? self-referential imports shouldn't need to be defined here -- imports from the same package shouldn't need special allowance.
191061585,4931,ewencp,2018-05-27T00:35:26Z,"This doesn't seem like a good thing to enable -- it's the opposite dependency we would normally want to have here. I see the only use is in a javadoc, can we adjust that so the import is not required?  For example, use the fully qualified class name instead of just the class name so we don't rely on the import?"
191062417,4931,ewencp,2018-05-27T01:50:27Z,"We don't enforce this today, though you could reasonable argue this is a ""bug"" that we don't validate them today. I think documenting this in the interface as an expectation would be reasonable. Whether we enforce it or not on various `Versioned` implementations might vary -- we could, for example, enforce proper versioning from day 1 of these new REST extensions, whereas for, e.g., connectors, we might want to think about adding validation that produces warnings if they return `null` or empty values, then eventually actually enforcing it (say, with AK 3.0)."
191062502,4931,ewencp,2018-05-27T01:54:20Z,"Do we want these to be asserts or check conditions and throw, e.g., `IllegalArgumentException` with a more useful message?"
191062585,4931,ewencp,2018-05-27T02:00:04Z,"`connectorState` is the wrong capitalization for a javadoc, I'd also simplify to just ""about the connector and its tasks"", rest of the API docs can give further details."
191062771,4931,ewencp,2018-05-27T02:14:01Z,"Is this actually what we want? Seems like if you wanted to keep a previous `TaskState` and check whether it had changed, this is going to be confusing behavior."
191063008,4931,ewencp,2018-05-27T02:36:32Z,"I'm noticing now that the context object exposes the cluster state, but doesn't explain the semantics for the returned state. Since the Extension only gets a hook into the Context on the register() call, that means this must be returning dynamic state. It seems like you don't necessarily get a consistent snapshot of the state when you call this -- you get a `ConnectClusterState`(`Impl`) object back, but that object can mutate out from under you since it's backed just by the dynamic herder state.

This is actually problematic and kind of difficult for plugin implementations since the calls to get the list of connectors and to get their state are separate and non-atomic. Which means there are chances of hitting `NotFound` exceptions and maybe others, which aren't clear from the interfaces.

Is there any reason not to make collection of the cluster state atomic and take a snapshot instead? That has way more intuitive semantics.

If we don't do that, I think we need to clearly document the semantics in the javadocs."
191063158,4931,ewencp,2018-05-27T02:52:03Z,"nit: typo ""teh"""
191063275,4931,ewencp,2018-05-27T03:03:30Z,"Seems redundant, we could just refactor out the part up to `getPassword` to a statement before this."
191063281,4931,ewencp,2018-05-27T03:04:26Z,is `credentialProperties` guaranteed non-null? It doesn't really seem like it given the logic in `initialize`.
191063473,4931,ewencp,2018-05-27T03:19:11Z,"This is global, static state. I think we should be careful to clean this up after the test."
191063510,4931,ewencp,2018-05-27T03:23:08Z,"Yeah, we just don't have a good way to handle multiple formats today. We'll want to adjust to rst-style if we implement KAFKA-2967"
191063925,4931,ewencp,2018-05-27T03:59:08Z,"nit: typo: ""no re-registering"" -> ""not re-registering"""
191063955,4931,ewencp,2018-05-27T04:01:12Z,"doesn't matter much, but seems weird to use `Boolean.TRUE` and `Boolean.FALSE` instead of just `true`/`false` literals here"
191064077,4931,ewencp,2018-05-27T04:10:34Z,"should these calls be protected by exception handlers for each since they're user pluggable? i.e. so if one fails, we don't just skip closing the rest (and the jetty server)?"
191292103,4931,mageshn,2018-05-29T02:23:49Z,Good point. I will add documentation to mention that these are not atomic and could potentially get an exception.
191292150,4931,mageshn,2018-05-29T02:24:25Z,Good catch. Throwing an exception now if the file can't be loaded.
191293267,4931,mageshn,2018-05-29T02:37:15Z,added some docs on ConnectClusterState. Let me know
191598513,4931,rhauch,2018-05-29T22:52:14Z,"I also think that it's useful to specify the expectations on an interface like this that's intended to be implemented by others. It provides useful guidance for implementers, whether or not we enforce it now."
191599600,4931,rhauch,2018-05-29T22:58:18Z,"With the changes to externalize secrets in [KIP-297](https://cwiki.apache.org/confluence/display/KAFKA/KIP-297%3A+Externalizing+Secrets+for+Connect+Configurations), the existing ConnectClusterState is going to output the transformed configurations, which may include secrets. Do we want to do that? Or, would we prefer to get the pre-transformed configurations?"
191599981,4931,rhauch,2018-05-29T23:00:23Z,Nit: JavaDoc for a class shouldn't really have links to that same class. Readers will think that it's something different.
191600425,4931,rhauch,2018-05-29T23:02:33Z,"""The Connect framework"", not ""Connect Framework"". Check this elsewhere, since it seems like I've requested this change multiple times already and the usage in this commit still is not consistent. :-)"
191600823,4931,mageshn,2018-05-29T23:04:35Z,"IIUC, KIP-297 is for connector configurations and we are only dealing with those here atm."
191601149,4931,mageshn,2018-05-29T23:06:29Z,@rhauch The latest commit already specifies this.
191601292,4931,rhauch,2018-05-29T23:07:18Z,"""... current configuration, which may change over time."""
191601381,4931,rhauch,2018-05-29T23:07:51Z,"The method description in the JavaDoc says that a `NotFoundException` will be thrown if no connector with the supplied name exists when this method is called, but the `@return` description says that the method can return null. We should decide: is it better to throw an exception or return null.

If we throw an exception, we should add `@throws NotFoundException if a connector with the supplied name does not exist` to fully document the behavior. (In general, the Kafka JavaDocs are relatively poor on this front.)
"
191602202,4931,rhauch,2018-05-29T23:12:30Z,Missing a period to terminate the sentence.
191602750,4931,rhauch,2018-05-29T23:15:29Z,Capitalize the first word in the sentence.
191602778,4931,rhauch,2018-05-29T23:15:39Z,"Capitalize the first word in the sentence. (Check for other places, too.)"
191602947,4931,rhauch,2018-05-29T23:16:39Z,"It'd be better to call the parameter `traceMessage` and then use ""any error message..."" in the description."
191603062,4931,rhauch,2018-05-29T23:17:18Z,Nit: missing the period to terminate the sentence.
191603211,4931,rhauch,2018-05-29T23:18:06Z,Nit: should be `class(es)` to match line 33.
191603371,4931,rhauch,2018-05-29T23:19:10Z,"Nit: ""implementations"" (plural)"
191603895,4931,rhauch,2018-05-29T23:22:16Z,"We shouldn't use `<br>`; instead, use a `<pre>` section around the lines."
191604301,4931,rhauch,2018-05-29T23:24:44Z,"These should be `<pre>` rather than `<code>`. The latter is more for phrases, not blocks, and loses all indentation and line breaks within a block of code. Then you can get rid of the `<br>` tags."
191605111,4931,rhauch,2018-05-29T23:29:21Z,"Good point, but it could be clearer. This implementation can be used in production, but the `PropertyFileLoginModule` that also ships with this reference implementation should NOT be used in production."
191605490,4931,rhauch,2018-05-29T23:31:43Z,"Should we log a warning in an `else` block for this `if` block? If somebody does not specify the filename, this login module will always fail authentication, right?"
191605741,4931,rhauch,2018-05-29T23:33:21Z,"For 2.0, should we log this as an error rather than throw an exception? IIUC, this is what you suggested, @ewencp."
191605853,4931,rhauch,2018-05-29T23:34:04Z,Nit: Why `Boolean` rather than `boolean`?
191606424,4931,rhauch,2018-05-29T23:37:49Z,"Sorry, I now realize that this ConnectClusterState is the interface in the API, whereas KIP-297 is changing the implementation in the runtime. The API interface doesn't expose the configuration, so that's a good thing."
191618735,4931,mageshn,2018-05-30T01:14:57Z,My understanding was that we enforce it strictly for RestExtension in 2.0. Older implementations can be enforced in 3.0
191637634,4931,ewencp,2018-05-30T04:09:22Z,"I think there's confusion in the discussion. I think @rhauch's point is that, since we have not enforced correctness in returned values thus far, we should be liberal in what we accept for now. So if they return `null` or an empty value, we should log an error, but not throw an exception that would kill the connector.

On 3.0 or some later version, we'd do as this code currently does and throw an exception since we will have given connectors a reasonable grace period to fix their behavior given that we didn't previously enforce the behavior."
191638435,4931,ewencp,2018-05-30T04:17:50Z,"I actually think the question is still relevant -- `ConnectClusterState` used to be purely immutable and now we'll be exposing an interface that changes based on when you call it. I think it doesn't matter much here, but it is mainly relevant because the docs on `ConnectClusterState` aren't really accurate anymore since the contents can change over time. But it's also a weird mix of mutability -- the set of connectors & tasks won't change in the `ConnectClusterState` object, you would need to re-callthis method to get updated cluster state. However, the actual values returned for a connector/task config *could* change due to KIP-297 replacements."
249744367,6177,stanislavkozlovski,2019-01-22T11:38:07Z,Should we mention that this configuration enabled a static membership and its lack would mean dynamic membership?
249745157,6177,stanislavkozlovski,2019-01-22T11:40:41Z,Is there any reason to not maintain backward compatibility here? Why not have dynamic members continue to rely on `leaveGroupOnClose`? (I lack the context of why this setting exists in the first place)
249749343,6177,stanislavkozlovski,2019-01-22T11:54:37Z,"_This is me thinking out loud. For the record I don't believe we should apply my suggestion in this PR as it would over-complicate things_

I'm wondering whether it will be worth it to think about cleaning up this bloated constructor, it takes almost 20 parameters with no defaults. The book [Growing Object Oriented Software Guided By Tests](http://www.growing-object-oriented-software.com/) makes a good point on how bundling up related parameters into separate classes results in code that is more domain-oriented (reads better) and is easier to mock/construct. ([here are some brief online notes](https://github.com/daryllxd/lifelong-learning/blob/master/programming/ruby/testing/growing-object-oriented-software-guided-by-tests.md#bloated-constructor))

We have done something similar in `GroupCoordinator` with its [GroupConfig class](https://github.com/apache/kafka/blob/fb0db7602abfd54137677d582a7781e4f790d7f9/core/src/main/scala/kafka/coordinator/group/GroupCoordinator.scala#L982)

Regardless, I was just interest in hearing people's thoughts on this matter"
249749782,6177,stanislavkozlovski,2019-01-22T11:55:54Z,Should we maintain the present tense? `member.id does not match the record on coordinator`
249856731,6177,abbccdda,2019-01-22T16:25:04Z,"Yea, good idea! Let me update both the doc and KIP"
249862352,6177,abbccdda,2019-01-22T16:37:27Z,"`leaveGroupOnClose` was set through internal config `internal.leave.group.on.close` which is by default set to true for normal consumer/connect, but set to false for streams. Checking groupInstanceId could perfectly remove this internal config without backward compatibility concern (since the config is not exposed)"
249864434,6177,abbccdda,2019-01-22T16:42:27Z,"Hey Stanis, I'm also in favor of simplifying the constructor logic here. I will get a JIRA to resolve this issue once this diff is landed."
249864597,6177,abbccdda,2019-01-22T16:42:51Z,done!
250660764,6177,stanislavkozlovski,2019-01-24T15:47:08Z,"Could we add a javadoc explaining when we expect to receive this exception and what could cause it?

My understanding is that a consumer that was part of the group used a `group.instance.id-member.id` pair and later that pair got updated with a new `member.id` by another consumer?

That is what I understand as a possibility from the explanation in the KIP:
> For join group requests under static membership (with `group.instance.id` set),

> If the `member.id` uses UNKNOWN_MEMBER_ID, we shall always generate a new member id and replace the one within current map. We also expect that after KIP-394, all the join group requests are requiring `member.id` to physically enter the consumer group, so the behavior of static member is consistent with that proposal."
250662548,6177,stanislavkozlovski,2019-01-24T15:50:48Z,"Could we update the KIP as it currently says:
```
GROUP_INSTANCE_ID_NOT_FOUND(79,""Somegroup.instance.idspecified in the leave group request are not found"", GroupInstanceIdNotFoundException::new)
```
which is not true as we can raise this for JoinGroup requests as well"
250664415,6177,stanislavkozlovski,2019-01-24T15:54:42Z,nit: Should we call this EMPTY_GROUP_INSTANCE_ID? `Unknown` implies that it will be known in some time (like with member.id) but in this case it is intentionally set to None
250672375,6177,stanislavkozlovski,2019-01-24T16:11:05Z,"nit: could we append the comment above with
`If member id required (dynamic membership)`
just to make it even more clearer than static members won't be pending members (I know this is noted in `doJoinGroup`"
250672737,6177,stanislavkozlovski,2019-01-24T16:11:59Z,nit: space between comma and `clientId`. I guess that comma could be on the line above
250672974,6177,stanislavkozlovski,2019-01-24T16:12:33Z,nit: `informing member` - `inform the member to`
250673259,6177,stanislavkozlovski,2019-01-24T16:13:09Z,"nit: `should inform` - `inform duplicate instance...`
This keeps it consistent with the tense in the other comments"
250674281,6177,stanislavkozlovski,2019-01-24T16:15:17Z,nit: I think that it will be clearer if we define this variable inside `doJoinGroup()`
250678345,6177,stanislavkozlovski,2019-01-24T16:24:41Z,nit: `a un-recognized` - `an unrecognized`
250678917,6177,stanislavkozlovski,2019-01-24T16:26:00Z,good call with splitting this logic into a method!  
250685315,6177,stanislavkozlovski,2019-01-24T16:41:01Z,should we also test `member.isStaticMember`?
250686301,6177,stanislavkozlovski,2019-01-24T16:43:27Z,"We don't have `group.getStaticMemberId` in tests anywhere, I think this is a good spot to assert it works as well"
250688550,6177,stanislavkozlovski,2019-01-24T16:48:38Z,"Not sure of the implications here, should we somehow work on removing the static member when the same `memberId` re-joins as a dynamic member? I guess it might not hurt, it will eventually get removed when the member leaves or its heartbeat fails, but we will continue to lock that `group.instance.id` with the `member.id` so new joins from static members with that group.instaince.id won't work. In other words, if a consumer becomes a static member and later re-joins as a dynamic, that `group.instance.id` is still taken. Am I correct?

That might not be unwanted behavior though"
250692730,6177,stanislavkozlovski,2019-01-24T16:58:26Z,Should we add a test which exercises this code path? I think it's critical for the KIP that this works
250693889,6177,stanislavkozlovski,2019-01-24T17:01:14Z,"If it's not too much work, maybe we could add a test to ensure `MEMBER_ID_MISMATCH` is fatal?"
250775259,6177,abbccdda,2019-01-24T21:02:21Z,"Hey Stanis, once I started the implementation, I realized that it's more clear to use the current member.id for static members instead of generating a new one, since we need member.id to track heartbeat & stuffs. I will update the KIP to reflect this change.

As for the `MemberIdMismatchException`, I put the explanation within Errors.java as error message to feedback end user."
250776499,6177,abbccdda,2019-01-24T21:06:21Z,Good catch! Will address this.
250780087,6177,abbccdda,2019-01-24T21:17:15Z,+1
250780539,6177,abbccdda,2019-01-24T21:18:32Z,"Yea of course, will try to see how to make that happen"
250781826,6177,abbccdda,2019-01-24T21:22:22Z,It should be trivial to test.
250782168,6177,abbccdda,2019-01-24T21:23:22Z,lol
251090933,6177,abbccdda,2019-01-25T18:39:32Z,Sounds good!
251091132,6177,abbccdda,2019-01-25T18:40:01Z,Make sense
251092116,6177,abbccdda,2019-01-25T18:41:30Z,+1
251093276,6177,abbccdda,2019-01-25T18:45:00Z,"Hey Stanis, the condition you proposed here is not possible within the current setup. Membership type transformation has to go through service restart, which will inevitably reset the member.id. So there is no way we see a dynamic member joining with its member.id points to a known static member. 
However this is a vaild concern, which I think by enforcing an assertion would be safer!"
251669585,6177,Ishiihara,2019-01-29T02:16:43Z,Do we want to use assert here? It will crash the broker if this happens. 
251683035,6177,Ishiihara,2019-01-29T03:46:22Z,"Can you also add comments to the case when the member doe not have a valid protocol, why do we want to force rebalance? Are we handling the case of rolling upgrades?"
251684242,6177,Ishiihara,2019-01-29T03:55:07Z,"As a disclaimer, I have forgotten the Kafka coding style. Do we use assert in code? "
251685753,6177,Ishiihara,2019-01-29T04:07:32Z,"This should be a fatal exception to the client, right?"
251686132,6177,Ishiihara,2019-01-29T04:10:02Z,This makes sense. Although the name is a bit confusing. 
251686408,6177,Ishiihara,2019-01-29T04:12:09Z,"This handles consumer restarts, correct? In that case, the member id will be unknown. "
251936369,6177,abbccdda,2019-01-29T17:27:40Z,The reason to use `assert` is to prevent future implementation from breaking the existing assumption. Basically known static member should never be `pending`.
251937129,6177,abbccdda,2019-01-29T17:29:40Z,"Yes we do have examples using assert, see `onCompleteJoin()` in GroupCoordinator.scala"
251937266,6177,abbccdda,2019-01-29T17:29:58Z,Yes that's right.
251937638,6177,abbccdda,2019-01-29T17:30:57Z,That is correct. In `doUnknownJoinGroup` we don't have a known member id to process with.
251939458,6177,abbccdda,2019-01-29T17:35:28Z,"Usually a change of protocol indicates that the group needs to use a different strategy to allocate topic partitions. Current logic is to trigger rebalance anyway to find a common agreed strategy for all current members. This diff doesn't change this part of the logic, however this is a good thing to discuss in a separate JIRA! "
260423217,6177,hachikuji,2019-02-26T18:25:01Z,"Rather than adding more exceptions, should we try to refactor the code?"
260426269,6177,hachikuji,2019-02-26T18:32:40Z,"I am wondering if `FENCED_MEMBER_ID` would be a clearer indication of the likely problem. In any case, we should try to give the user a helpful exception message."
260429304,6177,hachikuji,2019-02-26T18:40:32Z,Do we need a new error code for this case? I'm wondering if we could just use UNKNOWN_MEMBER_ID.
260432780,6177,hachikuji,2019-02-26T18:48:54Z,nit: a bit more intuitive to put the static member check first. Maybe we can also have an `isDynamicMember` or an `isStaticMember` method.
260437389,6177,hachikuji,2019-02-26T18:59:43Z,"Is there a good reason to favor """" over null for indicating that no instance id is provided? I think using null would reduce the chance of providing an invalid value by mistake. In fact, we can reject the use of """" and raise an error. So if a user provides any instance id, it must be valid.
"
260439690,6177,hachikuji,2019-02-26T19:05:19Z,"So clearly the intent is to silently fall back to the old join group logic, which means we become a dynamic member. It may be helpful having a log message indicating that this has happened.

One additional note: if the brokers are later upgraded to a version that does support static membership, we don't have any logic to detect it. I think this is probably fine, just worth keeping in mind."
260442064,6177,hachikuji,2019-02-26T19:10:47Z,Hmm.. I think I missed this addition in the KIP. How much effort would it be to pull this change into a separate PR? I think we may need some discussion.
260608268,6177,abbccdda,2019-02-27T06:28:52Z,"Yes, good suggestion! I got a JIRA to track this work https://issues.apache.org/jira/browse/KAFKA-7853, will attempt to fix it once this change is merged."
260609201,6177,abbccdda,2019-02-27T06:33:25Z,Sounds good!
260611209,6177,abbccdda,2019-02-27T06:44:15Z,Maybe unknown instance id is more aligned? It's slightly different comparing with member id unknown.
260611350,6177,abbccdda,2019-02-27T06:44:54Z,Sounds good.
260886942,6177,hachikuji,2019-02-27T18:38:43Z,"Our response to this error is to discard our current memberId and rejoin. That seems true regardless whether static or dynamic membership is used, so I thought we may as well make the error consistent. Does that make sense?"
260902762,6177,abbccdda,2019-02-27T19:17:55Z,"It should be ok since the error message clearly states:
`The group.instance.id is already in the consumer, however the corresponding member.id is not matching the record on coordinator`"
260923497,6177,abbccdda,2019-02-27T20:09:39Z,sure
260935391,6177,abbccdda,2019-02-27T20:43:08Z,Why couldn't we piggy-back the change in this PR? Connect could also benefit from using static membership right.
261272738,6177,kkonstantine,2019-02-28T16:19:30Z,"I agree with @hachikuji this is too significant to be omitted from a KIP and we should probably avoid piggybacking such a change in a subtle way in this already big PR.

The doc of the config below is indicative that we need to give this more thought. The connect worker is not a consumer and it doesn't use the group membership protocol in the same way. This is even more true with the changes being introduced soon with incremental cooperative rebalancing in Connect. The interplay between KIP-415 and static membership has not been sufficiently studied yet and therefore I'd suggest not introducing everything at once with the upcoming release. "
261308442,6177,abbccdda,2019-02-28T17:44:05Z,"@kkonstantine @hachikuji sounds great! My original thought was that the change happens on abstract coordinator layer, so consequently we could cover all the subclass use cases (both consumer and connect). I will revert connect related changes."
261350730,6177,hachikuji,2019-02-28T19:33:17Z,"Possibly so, though I am not sure since it does not have local state like Streams. In any case, I do not want to see this PR blocked by this discussion, so my thought was to split it out.

cc @rhauch Any thoughts about this?"
261353864,6177,abbccdda,2019-02-28T19:41:27Z,"Although we would react the error with same handling logic, I do see the benefit of decoupling error for now, because the error log could better help user triage during consumer incident."
261403394,6177,hachikuji,2019-02-28T22:05:40Z,Apologies for the late comment above. I hadn't refreshed the page and seen the updates.
261448929,6177,abbccdda,2019-03-01T01:09:11Z,It's fine :)
261759212,6177,hachikuji,2019-03-01T21:11:35Z,"Hmm.. The UNKNOWN_MEMBER_ID error is unambiguous in either case. It means that the coordinator isn't aware of the memberId. What debugging benefit is there in having another error code? The reason I'm resisting a little bit is that every error code adds more complexity to the protocol, so we should be sure it's necessary.

Here is the reason I find it confusing. With a provided instance id, there are two join cases: 

1) JoinGroup(instanceId=""foo"", memberId=""""): The consumer has no memberId and needs to be assigned one. 
2) JoinGroup(instanceId=""foo"", memberId=""xyz""): The consumer has a memberId and expects it to be valid.

The GROUP_INSTANCE_ID_NOT_FOUND would only make sense if it was a valid error in both cases. But it only applies to the second case. So my suggestion is that we view the second case as having a missing memberId. Then the behavior is consistent for static and dynamic members."
261792206,6177,hachikuji,2019-03-01T23:41:22Z,I think it would be clearer if we represented this as `Option[String]`.
261792477,6177,hachikuji,2019-03-01T23:42:56Z,We generally frown on assertions. It is usually better to raise an exception with a clear message.
261793705,6177,hachikuji,2019-03-01T23:50:50Z,Hmm.. I thought the proposal called for generation of a new memberId when a static consumer is restarted. The purpose is to fence the old static member. How do we avoid two static members from being active at the same time? Perhaps I'm missing something?
262186803,6177,abbccdda,2019-03-04T18:31:48Z,"I see your point Jason, make sense here. The logic is the since `GROUP_INSTANCE_ID_NOT_FOUND` is not covering the whole cases (like when member id is unknown), we could just bypass this check."
262201770,6177,abbccdda,2019-03-04T19:12:33Z,"Good catch! We have slightly diverged from the original proposal, so that we no longer kick off rebalance when static member rejoins with unknown member id. Thus the generation could not be used to fence against duplicate static members. Will address this problem by replacing with a new member id."
262203003,6177,abbccdda,2019-03-04T19:16:05Z,"Could you share more details? The reason for using assertion is to avoid creating invalid state from the code change stage. For example, we have
```private def onGroupLoaded(group: GroupMetadata) {
    group.inLock {
      info(s""Loading group metadata for ${group.groupId} with generation ${group.generationId}"")
      assert(group.is(Stable) || group.is(Empty))
      if (groupIsOverCapacity(group)) {
        prepareRebalance(group, s""Freshly-loaded group is over capacity ($groupConfig.groupMaxSize). Rebalacing in order to give a chance for consumers to commit offsets"")
      }

      group.allMemberMetadata.foreach(completeAndScheduleNextHeartbeatExpiration(group, _))
    }
```
and 
```
// trigger the awaiting join group response callback for all the members after rebalancing
          for (member <- group.allMemberMetadata) {
            assert(member.awaitingJoinCallback != null)
            val joinResult = JoinGroupResult(
              members = if (group.isLeader(member.memberId)) {
```
It would be great if you shed light on the trade-offs on these cases, thank you!"
262788694,6177,abbccdda,2019-03-06T04:32:21Z,I think we handle the null case when building the join group request struct?
268418696,6177,abbccdda,2019-03-24T05:18:08Z,@hachikuji Could you give me some guidance on this? Thank you!
268419130,6177,stanislavkozlovski,2019-03-24T05:44:40Z,"@abbccdda - judging by `GroupMetadata#replace()`, the current behavior is to generate a new member ID and return it, right?
What happens if a misconfigured consumer joins with an existing, duplicate `consumer.instance.id`? It essentially kicks out the old consumer using it (by invalidating its member.id)? Does the old consumer try to rejoin with its group.instance.id afterwards? We could get into a bad loop if that is the case"
268446363,6177,abbccdda,2019-03-24T18:33:39Z,"Thats a very good question. Previously my thought was to use conflict member.id to shut down duplicate consumer instances. However, this probably wont work because upon receiving UNKNOWN_MEMBER_ID exception in either `SyncGroup, Heartbeat, OffsetCommit` requests will immediately reset the generation info which includes the member.id. One approach I could think of is to restrict the caller of `resetGeneration` on client side to only JoinGroup logic, which means for any other types of requests after receiving UNKNOWN_MEMBER_ID will be rejoining the group with their current generation info (the conflicting member.id). This should be able to help us detect the id collision and shut down duplicate member with MEMBER_ID_MISMATCH exception.

@stanislavkozlovski @hachikuji @guozhangwang Thoughts"
268484117,6177,stanislavkozlovski,2019-03-25T04:13:37Z,"I don't understand, who would receive the UNKNOWN_MEMBER_ID?

If consumer A has `member.id=1, instance.id=one` and consumer B joins with `member.id=2, instance.id=one`, wouldn't A receive MEMBER_ID_MISMATCH and shut down?"
268491234,6177,abbccdda,2019-03-25T05:16:58Z,"@stanislavkozlovski this won't happen automatically. The flow is like: 
1. consumer A with `member.id=1, instance.id=one` is working under stable group. The static member metadata map contains kv entry `one=1`.
2. consumer B starts up, joining with same `instance.id=one` and `member.id=unknown`
3. consumer B enters `doUnknownJoinGroup` block and successfully gets identity `member.id=2`. The static member metadata map now updates to `one=2`
4. consumer A gets fenced by either `SyncGroup, Heartbeat, OffsetCommit` which informs A with `UNKNOWN_MEMBER_ID` error, which will trigger `resetGeneration()` on client side AbstractCoordinator.
5. Now consumer A rejoins with `instance.id=one` and `member.id=unknown`, repeating step 2 like B.

So eventually A, B will bounce forever within the loop 2~5 unless one of them refuses to reset their assigned member.id. Otherwise `MEMBER_ID_MISMATCH` shall never trigger.
"
268790882,6177,stanislavkozlovski,2019-03-25T18:27:48Z,"Aha, yeah. We can only raise `MEMBER_ID_MISMATCH ` in the `JoinGroup` request because that's the only request that has the group instance id field, right?

As you proposed, I think making the consumer issue a new JoinGroup with the same member.id would be the better approach.  Otherwise, we'd probably need to add the new field to all the requests. The old functionality of resetting the generation should continue to work just fine, we'd just be adding an extra hop."
270133661,6177,guozhangwang,2019-03-28T18:06:11Z,"Not clear if this is right to me: from my understanding (https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances?focusedCommentId=109454351#comment-109454351) in case 6, we will still require a member.id and hence would reply the error with `MEMBER_ID_REQUIRED` if it is not specified, right?"
270134521,6177,guozhangwang,2019-03-28T18:08:12Z,"nit: can we just do this check inside `doJoinGroup`, seems unnecessary to create a boolean at the caller and pass in to `doJoinGroup`."
270136870,6177,guozhangwang,2019-03-28T18:13:59Z,nit for doc: .. for static members only.
270137792,6177,guozhangwang,2019-03-28T18:16:15Z,`New member id will be the same`: what does this mean?
270139366,6177,guozhangwang,2019-03-28T18:20:10Z,"Following the comment of `KafkaApis`: current logic is that if instance.id is not empty, then `requireKnownMemberId` would never be required. Is that intentional? I think even with non-empty instance.id, if there's no existing entry in static members, we would still return the created member.id with MEMBER_ID_REQUIRED to let the client re-join?"
270140569,6177,guozhangwang,2019-03-28T18:23:13Z,"Yeah to be honest we do use assertions somewhere like @abbccdda mentioned in onGroupLoaded; they are used to indicate ""this should never happen, and if it happens, it's a bug"". As a hind-sight we can actually just replace with if-throw-illegal-state-exception across the board so that when it happens indeed, it will crash hard but leave us a meaning stack trace."
270140799,6177,guozhangwang,2019-03-28T18:23:53Z,Hmm... this is not what I was thinking. Maybe we can elaborate a bit more on the KIP wiki?
270142218,6177,guozhangwang,2019-03-28T18:26:51Z,What if two consumers joining with the same instance id and member id?
270236415,6177,guozhangwang,2019-03-28T23:28:56Z,"I think restricting `resetGeneration` to only JoinGroup request is not the best approach since we do rely on, e.g. heartbeat response to notify consumers as early as possible. On the other hand, this issue would only raise if users mis configure their `instance.id` to have two running instances to have the same id, such issue is similar to producer client that two instances mistakenly configured with the same `transactional.id` and today it is handled by letting one of them to receive a fatal error (`fenced`) and either handle it themselves or die hard -- the bottom line is, brokers would not need to be responsible for abstracting such human errors from clients.

So I'd like to present an alternative proposal: 

1) when receiving a join group of null member.id, but existing instance.id, create a new member.id just instead of returning the associated member.id to the client (your PR already did this anyways)

2) when receiving a join group of non-empty member.id, and existing instance.id, BUT is inconsistent with the static members map, return error `MEMBER_ID_REQUIRED`.

Now the only issue is what if two instances come with the same instance.id and the same member.id. I think it would not be possible for new members due to 1) since we always generate a new member.id.

-----------------------

EDIT: after thinking about this and discussing with @abbccdda a bit more, I am now inclined towards the original proposal now, i.e. for all responses other than join-group request, we let it client to not reset generation / member-id immediately, but try to re-join the group again. This logic is simpler because:

1. for static members, not reseting the member-id and re-join, will then result in an fatal `member-id-mismatch`, and hence we can avoid the ping-pong scenario of two mis-configured clients keep kicking each other out by reseting the member id and re-join.

2. for dynamic members, not resetting the member-id and then rejoin will likely to get the same `unknown-member-id` again, and then it can reset generation. The cons is that this requires one more round-trip. But to me, simpler logic that does not require much complexity worth the cost, compared to my proposal above that special handles static and dynamic members on client side much more.

3. Moreover, as we move on to KIP-429 which will assume the assignors to be ""sticky"" somehow anyways, so even if somehow the member-id is still recognized by the group-coordinator when re-joining and the member happen to be the leader, this unnecessary rebalance triggered will be cheap.


cc @stanislavkozlovski @hachikuji 
"
270628741,6177,abbccdda,2019-03-30T14:51:43Z,I believe the `MEMBER_ID_REQUIRED` exception is assumed to be used only for dynamic members now.
270645251,6177,abbccdda,2019-03-30T23:46:18Z,"As we have discussed, we shall generate a new member id each time the static member rejoins. So the former consumer will not have the same member.id as the previous one"
270679831,6177,guozhangwang,2019-03-31T19:00:54Z,"Ack, I will update the comment on the KIP regarding the updated logic. Could you update the KIP wiki with that logic and also update the voting thread as well?"
270680233,6177,guozhangwang,2019-03-31T19:12:21Z,"Updated the comment in the wiki page: https://cwiki.apache.org/confluence/display/KAFKA/KIP-345%3A+Introduce+static+membership+protocol+to+reduce+consumer+rebalances?focusedCommentId=109455333#comment-109455333

please double check and also update the wiki page for better illustration if that makes sense."
271098896,6177,abbccdda,2019-04-02T00:37:32Z,"@guozhangwang I see, so we should choose to throw exception for most times?"
272441872,6177,guozhangwang,2019-04-05T04:34:35Z,Yeah I'd suggest so.
273648617,6177,hachikuji,2019-04-09T18:46:33Z,"The thing about an empty id might be misleading since we use null to indicate absence. A few more details may also be helpful. Perhaps we can say something like this:  

> A unique identifier of the consumer instance provided by end user. Only non-empty strings are permitted. If set, the consumer is treated as a static member, which means that only one instance with this ID is allowed in the consumer group at any time. This can be used in combination with a larger session timeout to avoid group rebalances caused by transient unavailability (e.g. process restarts). If not set, the consumer will join the group as a dynamic member, which is the traditional behavior. "
273671882,6177,hachikuji,2019-04-09T19:49:31Z,"nit: perhaps quote the exact config? For example:
```
JoinGroupRequest.EMPTY_GROUP_INSTANCE_ID + "" must be non-empty""
```"
273673756,6177,hachikuji,2019-04-09T19:54:40Z,I think one of the things we have regretted is not limiting the group.id to a reduced character set. This has made ACLs more difficult for example. Do you think it is worth being stricter about the instance ID? Potentially we could limit the character set to the same characters we allow for topics.
273674941,6177,hachikuji,2019-04-09T19:57:54Z,Let me try one more time. How about `FENCED_INSTANCE_ID`?
273727881,6177,hachikuji,2019-04-09T22:22:41Z,Hmm.. This method is also called when a call to `unsubscribe()` is made. Would we not want a static member to leave in this case?
273728352,6177,hachikuji,2019-04-09T22:24:20Z,This is unused since we use the generated classes now.
273733855,6177,hachikuji,2019-04-09T22:47:16Z,"I think we can be a little clearer in this message. How about simply ""The coordinator reports a more recent member.id associated with the consumer's group.instance.id."""
273734276,6177,hachikuji,2019-04-09T22:49:14Z,"I may have asked this before, but do we want to use empty to indicate no group instance id? Alternatively, we can let the `GroupInstanceId` type be nullable in the schema and we can use null. This would be consistent with the config."
273737121,6177,hachikuji,2019-04-09T23:03:22Z,"I'm just saying that it would be clearer to represent the difference between static and dynamic members by using an optional field. Otherwise you have to dig into the code to make sure the uses are all safe. For example, we have a bunch of cases below where we are using the empty instance id in calls to `hasStaticMember`. This opens the door to bugs if we are not really careful with our checking. The nice thing about options is that they force us to check for absence."
273738245,6177,hachikuji,2019-04-09T23:08:38Z,nit: you can drop the `s` since there are no substitutions.
273738796,6177,hachikuji,2019-04-09T23:11:29Z,Can you elaborate on this comment? I'm not sure I understand the problem.
273739061,6177,hachikuji,2019-04-09T23:12:49Z,I think we should not try to overload `requireKnownMemberId`. It makes this pretty confusing.
273739420,6177,hachikuji,2019-04-09T23:14:32Z,"Yes, exactly. We like stack traces and nice error messages! A lot of these impossible states have a way of becoming more possible over time."
273742089,6177,hachikuji,2019-04-09T23:27:52Z,"Maybe slightly nicer:
```
    val oldMember = members.remove(oldMemberId)
        .getOrElse(throw new IllegalArgumentException(s""Cannot replace non-existing member id $oldMemberId""))
```"
273743469,6177,hachikuji,2019-04-09T23:35:19Z,We did get some flack in 2.1 for a change to this format. The problem is that we cannot downgrade once the new format is in use. We will probably have to mention this in the upgrade notes at a minimum. Unfortunately I don't see any great options at the moment to avoid this. Perhaps we should just switch to json.
273744396,6177,hachikuji,2019-04-09T23:40:22Z,I'm trying to think through the implications of this. We are silently discarding the instance id which means that replicas won't know about it. The member will be considered a static member until there is a coordinator change. Then it will suddenly become dynamic again and I think that would trigger this assertion: https://github.com/apache/kafka/pull/6177/files#diff-a210b6f573f800e8a562fda9d53cd148R269. I think we probably need to avoid using the static membership logic entirely until the IBP supports it.
274084303,6177,abbccdda,2019-04-10T17:53:39Z,Sure!
274157146,6177,abbccdda,2019-04-10T20:56:44Z,Thanks!
274157459,6177,abbccdda,2019-04-10T20:57:38Z,Sounds good
274162476,6177,abbccdda,2019-04-10T21:11:41Z,fixed. I was about to say the new heartbeat shall be scheduled with new member id.
274163062,6177,abbccdda,2019-04-10T21:13:40Z,"Yes, for static member we shall never require a rejoin, because its identity is declared by the instance id."
274163632,6177,abbccdda,2019-04-10T21:15:25Z,Thanks!
274261445,6177,abbccdda,2019-04-11T05:23:58Z,"I see, we could discuss offline some time for a holistic solution."
274261996,6177,abbccdda,2019-04-11T05:27:51Z,"That sounds reasonable. I'm not sure I'm fully following here because unless broker upgrades to latest, the group instance id should not include the join group because of automatic request downgrade."
274263375,6177,abbccdda,2019-04-11T05:35:38Z,"Yea, updated explicitly in the KIP"
274263890,6177,abbccdda,2019-04-11T05:38:46Z,"Ugh, `EMPTY_GROUP_INSTANCE_ID` will just be empty string right?"
274265007,6177,abbccdda,2019-04-11T05:45:00Z,"Let me check the code real quick, do you have good example for character set check? Right now what I found on admin client is sth like:
```
   private static boolean topicNameIsUnrepresentable(String topicName) {
        return topicName == null || topicName.isEmpty();
    }
```
which is not very useful."
274265965,6177,abbccdda,2019-04-11T05:50:52Z,"I quickly checked `unsubscribe()` use cases, and there are mainly two:
1. illegal topic/partition data, i.e empty topic partitions to subscribe
2. consumer self managed membership (subscription)

I think it makes sense to make static member behavior consistent in these two cases, because the effect of leaving is minimal."
274266402,6177,abbccdda,2019-04-11T05:53:18Z,This is just for the sake of reducing code duplication and keep if-else blocks intact.
274267268,6177,abbccdda,2019-04-11T05:57:41Z,I don't think that comment is needed here. It's just an edge I caught during my experiment.
274667775,6177,abbccdda,2019-04-11T21:16:28Z,"We still need an empty string field to make sure we could correctly serialize the member metadata. Also checking null for String is not very intuitive in Java compared with Scala Option, so my suggestion is to keep using empty string on client side for now."
274731954,6177,guozhangwang,2019-04-12T01:19:13Z,@abbccdda the topic validation logic can be found at `org.apache.kafka.common.internals.Topic`
274732699,6177,guozhangwang,2019-04-12T01:25:05Z,What's `IBP`?
274750177,6177,abbccdda,2019-04-12T03:39:32Z,@guozhangwang inter broker protocol
275141269,6177,abbccdda,2019-04-14T05:38:54Z,Sounds like a good idea!
275591198,6177,hachikuji,2019-04-16T00:27:22Z,"I'm not sure I follow this. We _can_ serialize null. My point is we should try to be consistent. Null is a good way to represent something which is missing. Java also has an `Optional` type which we could use, but we'd still have to decide what gets transmitted in the protocol."
275592368,6177,hachikuji,2019-04-16T00:34:39Z,nit: this could probably be implemented more concisely with a regex.
275593388,6177,hachikuji,2019-04-16T00:40:57Z,This method doesn't really make sense if `groupInstanceId` is `None`. Wouldn't it clearer to force the caller to ensure that that is the case? Same for the other methods below.
275595976,6177,hachikuji,2019-04-16T00:58:51Z,"We expose the new JoinGroup protocol as soon as the binary is updated. The client will begin using it. That itself is fine, but it is not safe for the broker to use the static member logic until we are sure that all brokers support it, as indicated through the IBP. Otherwise, the case I mentioned is possible. We seem to have removed the assertion I mentioned above, but I am still not sure the logic is correct. The simplest option would be to set `groupInstanceId` to `None` if the IBP is below `KAFKA_2_3_IV0`."
275596088,6177,hachikuji,2019-04-16T00:59:38Z,Shouldn't this be `KAFKA_2_3_IV0`? Do we have any tests?
275596574,6177,hachikuji,2019-04-16T01:02:33Z,Why not let `groupInstanceId` be represented as an `Option` inside `MemberMetadata` as well?
275597196,6177,hachikuji,2019-04-16T01:06:07Z,nit: parenthesis are unneeded
275607536,6177,abbccdda,2019-04-16T02:13:53Z,I tried one time and it failed due to serialization issue. Let me try one more time.
275630797,6177,abbccdda,2019-04-16T05:07:57Z,"To make the full e2e consistency, we should consider supporting Optional[String] type for part of auto-mated protocol in `JoinGroupRequest.json` and other protocol classes. Otherwise, we still need to have `EMPTY_INSTANCE_ID` as a special type to handle null case in the serde of request. @hachikuji "
276082373,6177,abbccdda,2019-04-17T05:19:07Z,good catch!
276083793,6177,abbccdda,2019-04-17T05:28:24Z,"I think adding an assertion here would be helpful. Would be messy if we do null check in caller every time when we call `replace`, `addStaticMember` and `getStaticMemberId`, what do you think?"
276084326,6177,abbccdda,2019-04-17T05:31:37Z,"Sounds good, I think it's probably better to do the refactoring in one diff for both topic and group instance id."
276085073,6177,abbccdda,2019-04-17T05:35:56Z,Addressed in GroupCoordinator.scala
276745404,6177,guozhangwang,2019-04-18T16:45:46Z,How about rename it to `maybeReplaceGroupInstance` and do the check in the callee and make it no-op if `groupInstanceId` is empty then?
276749603,6177,guozhangwang,2019-04-18T16:57:47Z,"The automated protocol supports `nullable string` (it will be serialized and stored as `0xffff` over the wire), and hence could we encode null for this instance id, and then:

1) on client side we can have this parameter nullable, and upon constructing join-group request it will be auto-serialized.
2) on broker side we can have this parameter as `Option[String]`, upon deserializing if the returned value is null construct the field as `None`."
276750288,6177,guozhangwang,2019-04-18T16:59:51Z,"I think on the broker side we should be using `Option` in scala to be consistent with other fields (see my other comment).

On the client side though, it is true that since we only recently dropped J7, `Optional` is not commonly used elsewhere, and I think having it just as a nullable field is fine (we can, do a universal refactoring on client side using `Optional` in another PR but this does not need to be done in this scope)."
276827600,6177,abbccdda,2019-04-18T20:46:28Z,@guozhangwang Thanks for the info!
276828558,6177,hachikuji,2019-04-18T20:49:21Z,"Sorry, if this wasn't clear, but here is what you need to add to the schema definition. See the `nullableVersions` field. 
```
    { ""name"": ""GroupInstanceId"", ""type"": ""string"", ""versions"": ""5+"", ""nullableVersions"": ""5+"",
      ""about"": ""The unique identifier of the consumer instance provided by end user."" },
```

In the common tongue, 0xffff is -1 :wink:. This is how we represent null arrays and strings."
276836547,6177,abbccdda,2019-04-18T21:14:10Z,"Yea, I just figured it out, thank you!! @hachikuji "
276864867,6177,abbccdda,2019-04-18T23:21:00Z,I agree with renaming but feel slightly against second proposal. I think the goal is to avoid people from passing in null group.instance.id in the code level. @guozhangwang 
277374681,6177,guozhangwang,2019-04-22T17:59:41Z,"I've thought about this a bit more, and also searched in github: https://github.com/search?l=Java&q=%22consumer.unsubscribe%22&type=Code

I think a third common case is to use a temporary consumer for its APIs, like get offset by timestamp, get log end offset etc; generally speaking for temporary consumer case, they should not use static members (and by default it would not be the case). 

So I think it really boils down to: for static members, do we consider the admin request kicking it out of the group be the only appropriate way for it to leave in time or not? I.e. even if the consumer shuts down itself, it should not be considered as ""I want to leave"" but another request has to be made to effectively kick him out."
277477265,6177,guozhangwang,2019-04-22T23:55:45Z,@abbccdda Should we remove this const string then?
277478955,6177,guozhangwang,2019-04-23T00:05:31Z,Should we still need these calls if we can get rid of const `EMPTY_GROUP_INSTANCE_ID` with optional?
277480160,6177,guozhangwang,2019-04-23T00:12:40Z,nit: I think it worth being an `info` since this should not happen frequently and hence each time it happens we should pay attention.
277480223,6177,guozhangwang,2019-04-23T00:13:09Z,Also: better include the groupInstanceId as well?
277480459,6177,guozhangwang,2019-04-23T00:14:29Z,"As we discussed before, better change `assert` to `throw IllegalStateException` with a meaningful error message; ditto below."
277481055,6177,guozhangwang,2019-04-23T00:18:13Z,For all the three callers of it: two already checks `member.isStaticMember` and one has the assert already. So I'd suggest we pass in `groupInstanceId: String` as parameter directly from caller.
277481439,6177,guozhangwang,2019-04-23T00:20:36Z,"Hmm.. does this function only have one caller who's already checked `group.hasStaticMember(groupInstanceId)`? In this case I think we can just name it `replaceGroupInstance` as it should always replace unless we have a bug.

Originally I was thinking there are multiple callers of it, and some may really turn into a no-op since it is not for static members, but now the call trace seems to indicate only one caller."
277481592,6177,guozhangwang,2019-04-23T00:21:38Z,Just to confirm: is `map.remove(null)` a no-op with no side-effect?
277481815,6177,guozhangwang,2019-04-23T00:22:58Z,nit: we can just do `else if` and `else`.
277482417,6177,guozhangwang,2019-04-23T00:26:46Z,If this is nullable we can get rid of `EMPTY_GROUP_INSTANCE_ID` right?
277482886,6177,guozhangwang,2019-04-23T00:29:58Z,"Actually, for all such `setGroupInstanceId` calls we can by default remove it since without any setters is equal to using `setGroupInstanceId(null)` right?"
277483066,6177,guozhangwang,2019-04-23T00:31:15Z,Do we have unit test coverage on compatibility? I.e. old formatted data can be loaded with new versioned byte code with new fields set to default (null) values?
277483221,6177,guozhangwang,2019-04-23T00:32:14Z,I think we should remove this part from Streams first. There are some open questions that I've in mind and needs to potentially create a new Streams KIP for it. cc @mjsax 
277484517,6177,guozhangwang,2019-04-23T00:41:13Z,"I think `null` default value should still work, e.g. the group-id used `null` as default values above."
277503483,6177,abbccdda,2019-04-23T02:57:03Z,The tricky thing is that we couldn't set the key to `null` in `VerifiableConsumer` because it will throw exception.
277503751,6177,abbccdda,2019-04-23T02:59:19Z,Sounds good
277503848,6177,abbccdda,2019-04-23T03:00:04Z,"My bad, didn't see the upper comment."
277513354,6177,abbccdda,2019-04-23T04:20:25Z,"We could remove the assertion here, but I guess we still need to throw exception since new caller may forget to check it."
277515294,6177,abbccdda,2019-04-23T04:36:42Z,Yep!
277515412,6177,abbccdda,2019-04-23T04:37:41Z,We don't
277813801,6177,abbccdda,2019-04-23T18:28:46Z,@guozhangwang mind giving me an example for compatibility test? I look around and haven't found one good example.
277912948,6177,abbccdda,2019-04-23T23:51:03Z,"After offline discussion with @guozhangwang, we sort out following key points:
1) Will the static membership affect unit test independence?
Short answer: no. The reason is because without explicitly setting the `client.id` config for stream instances, the static member id will be changed throughout the restarts since we add a random hash to `client.id`. It will essentially behave the same as current dynamic membership. Also one another confusion was that we are changing *max session timeout cap* to 30 min, instead of *default session timeout* which will remain as 10 s for either static or dynamic member. So the out-dated members will be kicked out in 10 seconds as expected.

2) The concern about thread id change throughout restarts. This is a valid concern in case where we configure two stream jobs within one JVM, so the threads will sometime shuffle from job A to job B, which unfortunately breaks the expectation of persistent thread-id numbers. This, however, shall not block us from enabling static membership for streams because the worst case is just doing repetitive rebalances as current dynamic membership. We could choose to address this application layer problem in another diff.

The conclusion is that, it does no harm to enable static membership on streams, we are just realizing there are more subtle cases we need to handle.

Let me know if this addresses your concern, thanks!"
277927199,6177,guozhangwang,2019-04-24T01:28:38Z,That sounds good. We can check that the passed in `String` (not `Option[String]` for simplicity since all current callers actually can pass in the string parameter) is not null and throw otherwise.
277927290,6177,guozhangwang,2019-04-24T01:29:16Z,@abbccdda is that the case?
277927492,6177,guozhangwang,2019-04-24T01:30:40Z,"You can for example take a look at this PR: https://github.com/apache/kafka/pull/6528

When we update the consumer protocol, we added unit test to make sure old versioned code can still deser it, and similarly in this case, we need to change that new versioned code can still deser old versioned data."
277927851,6177,guozhangwang,2019-04-24T01:33:11Z,"@abbccdda I'd still suggest we add streams logic leveraging static members in this PR for further discussion than rush it in this PR.

For people who wants to use the feature in Streams asap, they can still do it by manually set the group.instance.id via consumer config prefix in StreamsConfig. But we need to think through all the cases before making it turned on by default in Streams."
277950657,6177,abbccdda,2019-04-24T04:21:56Z,"@guozhangwang I see. However it's currently not possible, since they need to have access to stream internal to set `group.instance.id` config."
277953016,6177,abbccdda,2019-04-24T04:40:56Z,Thanks!
277953572,6177,abbccdda,2019-04-24T04:45:48Z,I feel we could keep Option[String] in the function parameters. The reason is for consistent handling of this piece of information in GroupCoordinator until we actually extract the String for internal data structure update. The other approach would be using the case switch here which is more Option friendly. WDYT?
278737190,6177,guozhangwang,2019-04-25T21:22:33Z,"Got it, that makes sense. I think we would consider fixing the following the static stream-thread suffix number first, and then requiring users who wants to turn on static membership to specify the client-id then (otherwise internally created client-id would never be the same across lives of a streams instance). I saw you've created JIRA tickets for these tasks."
278737600,6177,guozhangwang,2019-04-25T21:23:36Z,"After a second thought I think I agree with you, it's not worth optimizing the parameter while giving up consistency in call traces."
278739994,6177,guozhangwang,2019-04-25T21:31:03Z,We should check that `staticMembers` is also empty by default when deserializing from old versions.
109102899,2772,michaelandrepearce,2017-03-31T07:29:53Z,"accidental formatting, no need, need to revert."
109102944,2772,michaelandrepearce,2017-03-31T07:30:14Z,remove extra space
109102987,2772,michaelandrepearce,2017-03-31T07:30:33Z,remove extra un-needed whitespace
109103041,2772,michaelandrepearce,2017-03-31T07:30:54Z,is this needed?
109103189,2772,michaelandrepearce,2017-03-31T07:31:53Z,"remove accidental, whitespace formatting change."
109141852,2772,jeroenvandisseldorp,2017-03-31T11:37:45Z,"You use pretty much everywhere K, V, H as parameter order, so would be more consistent to do so here too."
109150487,2772,michaelandrepearce,2017-03-31T12:39:53Z,"Good spot, and Its a very good point, will adjust to make it more consistent. Thanks :)"
109287859,2772,radai-rosenblatt,2017-04-01T16:01:42Z,"nit pick - returns all headers _in the order they were added in_, also clarify if returns null or some empty collection if nothing found"
109287981,2772,radai-rosenblatt,2017-04-01T16:09:02Z,Header doesnt allow for a null key. should lastHeader(null) throw or just return nothing?
109288040,2772,radai-rosenblatt,2017-04-01T16:12:35Z,can this inner class be made static? if so would save an OuterClass.this call above
109292288,2772,michaelandrepearce,2017-04-01T19:32:43Z,"Yes this one could have been, it was using java 8 feature previous, was simply quickly removing our usage of java 8, as kip 118 isn't in master and thus would cause a build failure as still needing java 7 support atm. 

I will change to static inner class for now. didn't actually even need the RecordHeaders.this call it the method call filter was in scope, but will make it a static inner class.

N.B the other closeAware iterator we cannot make static inner, as it needs reference to isClosed, unless we made that an Atomic which would be more of an overkill imo."
109292305,2772,michaelandrepearce,2017-04-01T19:33:15Z,"good point, behaviour should be as per creating a Header with a null, and throw."
109292344,2772,michaelandrepearce,2017-04-01T19:34:00Z,"will update java doc with additional detail, was just copying java doc that was as per kip page for the interfaces."
110308619,2772,becketqin,2017-04-07T02:56:08Z,Can we use standard java doc in the public interface?
110308937,2772,becketqin,2017-04-07T03:00:14Z,This java doc seems a little misleading. Even for Kafka 0.11 we can still use this constructor to construct a consumer record although the headers is empty. And the message format will still be in 0.11.
110310796,2772,becketqin,2017-04-07T03:27:30Z,"It is a little unfortunate that we have to do this hack just to maintain the backwards compatibility of the Serializer and Deserializer interface. There were some discussion about this on the side channel that we hope can start to use Java 8 so a default implementation can be added to the existing SerDe interface.

Personally I think it is fine to just start to set sourceCompatibility to 1.8 in this patch and drop support for Java 1.7 given that KIP-118 has already passed. This way we can avoid this hack. @ijuma What do you think?"
110314115,2772,becketqin,2017-04-07T04:22:00Z,Can we rename this to `closeHeaders`?
110314866,2772,becketqin,2017-04-07T04:34:55Z,Is this change intentional? This change will cause an additional memory copy in `defaultRecord.readFrom()`.
110315042,2772,becketqin,2017-04-07T04:37:46Z,It seems we already has a `Record.EMPTY_HEADERS`. Can we reuse that?
110315300,2772,becketqin,2017-04-07T04:41:46Z,"""RecordHeaders has been closed."""
110315649,2772,becketqin,2017-04-07T04:47:10Z,It is a little tricky here because we would require the order of the header to be the same as well. I am wondering if this would be a little too demanding.
110316068,2772,becketqin,2017-04-07T04:53:50Z,See previous comment about memory copy.
110316561,2772,becketqin,2017-04-07T05:02:32Z,Is there any special consideration of creating a mutable new header here?
110316924,2772,becketqin,2017-04-07T05:08:16Z,Do we want to close the headers here?
110319166,2772,michaelandrepearce,2017-04-07T05:39:29Z,"The discussion in the kip seemed to come to conclusion we only want to close them on produce. As in consume if you consume you may wish via the interceptors to manage the headers again, e.g. remove it. On the front of mutability if you change the headers but consumed again the headers would be per the original messages as it would be created again. This is different to the producer record issue, which is why we closed that during send."
110319402,2772,michaelandrepearce,2017-04-07T05:42:32Z,"So here the only thing we do is create the header object which is string, byte[].  As per kip. Should note the string is memory copied already.

As such we can make this a buffer but a line or two down when we hand over to the header object it would be a byte[]. 

Also same comment as above, in KIP we agreed on interface to be byte[] we can change this to a ByteBuffer, and personally not opposed to this, just we should note, it would be a change to the KIP"
110319457,2772,michaelandrepearce,2017-04-07T05:43:11Z,"Yes we could, alas the import check would fail, I can amend the import check and reuse it. I will do this"
110319516,2772,michaelandrepearce,2017-04-07T05:44:06Z,"Sure, good point, was just copying off the KIP document. I will do this."
110319547,2772,michaelandrepearce,2017-04-07T05:44:24Z,Sure :). I will do this
110319762,2772,michaelandrepearce,2017-04-07T05:47:12Z,"So this is to the equality of arraylist equals. I believe therefor we get this for free.

As per Java doc 

Compares the specified object with this list for equality. Returns true if and only if the specified object is also a list, both lists have the same size, and all corresponding pairs of elements in the two lists are equal. (Two elements e1 and e2 are equal if (e1==null ? e2==null : e1.equals(e2)).) In other words, two lists are defined to be equal if they contain the same elements in the same order.

We should also note ordering is important, as noted in kip discussion as we use the ordering for add/lastHeaders, as such if i had two headers but for the same key the order was different, lastHeaders would return differently, therefor i would argue the headers are there for not equal."
110319886,2772,michaelandrepearce,2017-04-07T05:49:07Z,"This is inline with the comment left when the constructor for timestamp was added. It also is the same, it created a valid / correct message format still, and you can still use the constructor.

see constructor directly above, its pretty much a copy and paste job, with just slight modification.

Constructor comment added when 0.10 changes done.
>
     * Creates a record to be received from a specified topic and partition (provided for
     * compatibility with Kafka 0.9 before the message format supported timestamps and before
     * serialized metadata were exposed).
     *

Our new constructor  comment with 0.11 changes, following same lines.
>
     * Creates a record to be received from a specified topic and partition (provided for
     * compatibility with Kafka 0.10 before the message format supported headers).
"
110320089,2772,michaelandrepearce,2017-04-07T05:51:45Z,"Yes there was.

So that it is set so if someone in their consumer interceptors consumes and needs to modify headers.

Also RecordHeaders uses array list, which does an empty array memory saving, so really the over head for cleaness of code and keeping in line with ProducerRecord, for empty headers the overhead is just the RecordHeaders object, no real sizeable data/memory.

If this really is of a concern, we could do null and then have a if null create lazily on the headers() method.

Though we wouldn't be able to make the same saving on ProducerRecord as we call the headers() method on send to get them, if we wanted to do the same we would need to introduce a hasHeaders() or headersSize() method on the Producer/Consumer Record, so you can avoid calling headers() and initialising the object. 

Also trying to make this saving would cause complexity where we want to provide headers to the ser/des for the linkedin use case's https://cwiki.apache.org/confluence/display/KAFKA/A+Case+for+Kafka+Headers, where they may need or want access to headers, we need to pass headers object, if not to force it to handle null headers also which would uk.
"
110321627,2772,michaelandrepearce,2017-04-07T06:08:18Z,"Yes it was, as the KIP interface and constructor for a header is byte[]. When we create the headers in producer it will be a byte[] as such this would not be any memory copy, like wise on consume when the header is read a byte[] would need to be returned as such any saving would be negated.

If we want ByteBuffer, then it would be best to change that you construct Headers with a ByteBuffer value, and like wise byte[] value(),  changes to ByteBuffer value(). 

Im not opposed to this, just isn't as agreed in KIP, we would update the KIP if we changed this."
110321649,2772,michaelandrepearce,2017-04-07T06:08:36Z,Sure. I will do this.
110321692,2772,michaelandrepearce,2017-04-07T06:09:16Z,"Yes totally agree, and as per commit comment, this was for lack of java 8 optimisation."
110335355,2772,michaelandrepearce,2017-04-07T07:55:24Z,"On seeing what we can do to try alleviate your concern as much as possible we can make it so it uses ByteBuffer, again we should note though, that on Header.value() will incure a memory copy still, as simply we move when this copy occurs. As such its only incurred if the header is read."
110335364,2772,michaelandrepearce,2017-04-07T07:55:29Z,"On seeing what we can do to try alleviate your concern as much as possible we can make it so it uses ByteBuffer, again we should note though, that on Header.value() will incure a memory copy still, as simply we move when this copy occurs. As such its only incurred if the header is read."
110414387,2772,ijuma,2017-04-07T15:17:32Z,"@becketqin As I explained here https://github.com/apache/kafka/pull/2814#issuecomment-291880230, we need to update our system tests infrastructure to run with Java 8 before we can make the switch. It may take a bit of time to get that done, so my suggestion was to do what can be done with Java 7 in the initial PR and file a JIRA for the follow-up work once the Java 8 switch happens. That way, we can make progress instead of being blocked."
110428445,2772,michaelandrepearce,2017-04-07T16:22:17Z,"Ok, so i have pushed a commit to revert back to java 7 (again) so this PR could be merged to make progress. I have locally stashed the java 8 changes for later."
112817815,2772,hachikuji,2017-04-22T20:16:43Z,nit: is this needed?
112817938,2772,hachikuji,2017-04-22T20:22:21Z,nit: could we use `Record.EMPTY_HEADERS` instead of `null` for all of these?
112817953,2772,hachikuji,2017-04-22T20:23:03Z,We should probably update the producer and consumer config documentation to mention these new interfaces. It should probably also be added to the KIP. 
112818039,2772,hachikuji,2017-04-22T20:26:11Z,"Does this need to be public? Not much harm, but maybe unnecessary."
112818193,2772,hachikuji,2017-04-22T20:32:52Z,"Could replace this constructor with `this(key, Utils.wrapNullable(value))`?"
112818215,2772,hachikuji,2017-04-22T20:34:10Z,Maybe we should cache this value and potentially set the buffer to null?
112818309,2772,hachikuji,2017-04-22T20:39:09Z,"If we added another method `add(String key, byte[] value)`, would there be any need to expose a concrete implementation of `Header`?"
112818423,2772,hachikuji,2017-04-22T20:45:15Z,"If we want this package to be included in the javadocs (i.e. if we want it to be exposed to users), then we need to update `build.gradle`."
112819053,2772,michaelandrepearce,2017-04-22T21:17:13Z,"no its not, was just left in by accident, good spot, will remove."
112819065,2772,michaelandrepearce,2017-04-22T21:17:59Z,"makes sense, will update."
112819106,2772,michaelandrepearce,2017-04-22T21:19:55Z,"agreed, was hoping that we get to have source in java 8 and thus then don't need these class's. As discussed previously we will do the java 8 changes in separate PR, once kip 118 is implemented."
112819124,2772,michaelandrepearce,2017-04-22T21:21:09Z,"agreed. though based on below comment on note on cache and set buffer to null, then for the byte[] constructor, we should then not wrap but simply set the byte array value."
112819135,2772,michaelandrepearce,2017-04-22T21:21:43Z,"a nice optimisation, this actually saves us on the produce side, as we don't then need to wrap the byte array and then unwrap it again."
112819183,2772,michaelandrepearce,2017-04-22T21:24:22Z,"agreed, i recall during the kip discussion we originally had add(String key, byte[] value) but someone (will need to trawl the history) requested it to be add(Header header). Im happy having both, as such will add it, and once merged to master will update the kip document and send out a notification."
112819919,2772,michaelandrepearce,2017-04-22T22:14:22Z,"no it doesn't. again anyhow once kip 118  (java 8) is done, will raise separate PR, which this would be removed anyhow.  This is so we can merge/commit to master with current java 7."
112821175,2772,michaelandrepearce,2017-04-22T23:27:53Z,"thanks, will update"
113302283,2772,hachikuji,2017-04-25T20:31:46Z,"Since we now only have the `Header` and `Headers` classes public, maybe we could locate them under `common`? "
113338574,2772,michaelandrepearce,2017-04-25T23:51:50Z,"I was purposely wanting to avoid that and use packaging structure to keep things tidy/grouped together as is nicely done with other parts, else the common package level (one level up) would become a dumping ground over a period, if that approach was constantly taken. 

This is simply allowing package
org.apache.kafka.common.record 

import
org.apache.kafka.common.header
"
113436597,2772,ijuma,2017-04-26T12:22:16Z,This constructor doesn't take a timestamp. Is that intentional?
113438319,2772,michaelandrepearce,2017-04-26T12:31:15Z,"Yes it was, there is a constructor line 66, that does take timestamp, which this one delegates to.

```
    public ProducerRecord(String topic, Integer partition, Long timestamp, K key, V value, Iterable<Header> headers) {
```"
113439712,2772,ijuma,2017-04-26T12:38:23Z,"Yes, but one takes `Headers` and the other takes `Iterable<Header>` and it's unclear why that is so."
113455023,2772,michaelandrepearce,2017-04-26T13:46:11Z,"Ah no that is a mistake it should be `Iterable<Header>`, gotcha now, good spot, will correct it."
113456401,2772,michaelandrepearce,2017-04-26T13:51:12Z,fix committed.
113772475,2772,hachikuji,2017-04-27T18:42:20Z,Fair enough.
113832198,2772,ijuma,2017-04-28T00:38:49Z,"Nit: we typically don't use all caps in our javadocs. It's OK to just say ""all headers"", I think. There are a few cases like this."
113832491,2772,ijuma,2017-04-28T00:41:59Z,"I don't think this comment is accurate.

```java
public ArrayList(Collection<? extends E> c) {
        elementData = c.toArray();
        if ((size = elementData.length) != 0) {
            // c.toArray might (incorrectly) not return Object[] (see 6260652)
            if (elementData.getClass() != Object[].class)
                elementData = Arrays.copyOf(elementData, size, Object[].class);
        } else {
            // replace with empty array.
            this.elementData = EMPTY_ELEMENTDATA;
        }
    }
```

And `Arrays.asList.toArray`:

```java
@Override
        public Object[] toArray() {
            return a.clone();
        }
```

I'd just remove the comment. The code is implemented as one would expect, we don't need to worry about Java's implementation details."
113832530,2772,ijuma,2017-04-28T00:42:27Z,"No need for this comment, it just repeats what the code is doing."
113833156,2772,ijuma,2017-04-28T00:49:14Z,"As you can see in the code I pasted in the other comment, the first thing that the constructor does is call `c.toArray` so it actually depends on the collection. `ArrayList` does `Arrays.copyOf` today, but could be something else later. I think a single comment at the top saying ""Use efficient copy constructor if possible, fallback to iteration otherwise"" is clear and not dependent on implementation details."
113833598,2772,ijuma,2017-04-28T00:53:59Z,`headers` is missing from here and from the other `append` that was added.
113833664,2772,ijuma,2017-04-28T00:54:42Z,`front` seems to be redundant
113833820,2772,ijuma,2017-04-28T00:56:07Z,Nit: `StandardCharsets.UTF_8` is nicer than `Charset.forName`
113834118,2772,ijuma,2017-04-28T00:58:41Z,"It would be good to exercise a few more methods after `remove` is called. Also, it would be good to interleave `add` and `remove` calls in one test."
113834436,2772,ijuma,2017-04-28T01:01:31Z,"I was thinking about this and maybe `close` is not the right name. Because we can still use the class, we simply cannot mutate it any more. The name that came to mind is `seal`, but maybe that's not clear either. We could do the boring `closeForUpdates` or something like that. Thoughts?"
113834747,2772,ijuma,2017-04-28T01:04:50Z,We should check the keys too in every case in this test.
113835397,2772,michaelandrepearce,2017-04-28T01:12:53Z,"close, is what was the end method, in the kip discussion, and is naturally for java what you tend to implement closable interface for. "
113835523,2772,michaelandrepearce,2017-04-28T01:14:22Z,agreed. 
113835615,2772,ijuma,2017-04-28T01:15:30Z,"This is an internal method, so it's really part of the KIP. In Java, you typically can't use a class after you call `close()`, so I don't really agree. For example, using try with resources doesn't make sense for this class."
113836292,2772,ijuma,2017-04-28T01:22:33Z,"One more thing: since this is internal, we can change it later so if we think this is the best name we can find for it at the moment, we can leave as is."
113837107,2772,michaelandrepearce,2017-04-28T01:33:15Z,"It is true, that as you say, it is meant to be no longer useable.

How about setReadOnly()

inline with File.setReadOnly() from java api's"
113837206,2772,michaelandrepearce,2017-04-28T01:34:34Z,Will remove
113837251,2772,michaelandrepearce,2017-04-28T01:35:05Z,Will add
113837325,2772,michaelandrepearce,2017-04-28T01:36:00Z,Laptop died as I was committing this will do tomorrow now. Could we merge? And I open another pr tomorrow?
113837349,2772,michaelandrepearce,2017-04-28T01:36:20Z,Will enhance
113837395,2772,michaelandrepearce,2017-04-28T01:36:57Z,"Sure, more test ideas always welcome"
113869779,2772,michaelandrepearce,2017-04-28T07:34:41Z,added
113869831,2772,michaelandrepearce,2017-04-28T07:34:58Z,added
95493943,2330,ewencp,2017-01-11T01:08:45Z,nit typo: mey
95495767,2330,ewencp,2017-01-11T01:25:32Z,It seems like the ChannelBuilder implementations respect the possibility that these are null but we always seem to pass `NONE` if we don't want an implementation? Did the intended usage just diverge during development of the PR? Should we stick to only one or the other?
95502827,2330,ewencp,2017-01-11T02:37:23Z,"nit throughout -- all lowercase is fine for stuff like comments, but for user-facing messages, it'd be nice to emphasize proper capitalization, grammar, etc."
95503095,2330,ewencp,2017-01-11T02:41:00Z,"Not critical since these aren't public APIs, but there are a bunch of references to methods in these javadocs that could be `@link`ified."
95503684,2330,ewencp,2017-01-11T02:48:51Z,It seems this class isn't even used anywhere. Maybe we should just remove it entirely?
95505219,2330,ewencp,2017-01-11T03:09:58Z,"Using the `MemoryPool` for `NetworkReceive` only works if everyone actually uses the `MemoryPool` for all relevant allocations. There are still uses of the other constructores -- this is only used by `KafkaChannel`. I just want to verify we know the implications of leaving the other ones.

Obviously the constructor with `ByteBuffer` doesn't need the `MemoryPool`. A few are used in `SaslClientAuthenticator`/`SaslServerAuthenticator`. Those seem fairly reasonable (one is unbounded, which doesn't seem ideal, although I'm not sure a bound can easily be placed on it).  The last case is in `BlockingChannel`. It seems this is only used in controlled shutdown. I assume the KIP was mainly targeted at client requests and the controlled shutdown message is constrained enough in its request size that we just don't need to worry about that case? (I'm not sure how completely we want to make the enforcement for this KIP, i.e. want to catch everything except stated exceptions to protect against even malicious users or if we are just trying to address ""accidental"" issues caused by clients.)"
95505706,2330,ewencp,2017-01-11T03:17:19Z,"re: comment, do you have a stacktrace or something from where this happens? Would be good to know if there's a valid case or if the condition checked a few lines up should be `receiveSize <= 0`. Intuitively, a zero length receive seems like it would be invalid (but possible for clients to transmit, and so perhaps handled gracefully even if it is invalid)."
95505922,2330,ewencp,2017-01-11T03:20:24Z,These don't need `public` on them since it's an interface.
95506702,2330,ewencp,2017-01-11T03:31:41Z,Would this be worth raising to `warn`? Seems like it might be relevant for users to know via the logs that they are effectively throttling reads. Or are we assuming the new sensor is sufficient?
95508527,2330,ewencp,2017-01-11T04:00:44Z,"This is fine. You can also put those all in an array and just index with `min(ordinal, units.length-1)`."
95508782,2330,ewencp,2017-01-11T04:04:41Z,"Is there any concern that we might lose track of updating this properly? It's not just an issue with adding new request types; it's also a problem if a request type that didn't have `BYTES` or `NULLABLE_BYTES` fields is updated to a version of the schema that does have them.

I'm skeptical that folks will even know about, let alone remember to update, this list if they introduce such a field. Would some sort of static determination based on the full list of schemas be more reliable but equally fast?"
95511403,2330,ewencp,2017-01-11T04:44:34Z,"Is there any concern about efficiency here? In particular, this allocates a new list and runs an additional linear time algorithm. A simpler alternative that has weaker randomization guarantees would be to select a random starting offset and use a couple of iterators to implement a sort of rotated view of the original list. Or perhaps overall it's not a concern since we have a linear cost to process all the keys anyway?"
95548748,2330,ijuma,2017-01-11T10:15:32Z,"I think it would be nice to avoid unnecessary naming inconsistencies between this and `BufferPool`. It may make sense to deviate in some cases, but could you take a pass and see if some names here or there should be renamed for consistency?"
95550443,2330,ijuma,2017-01-11T10:25:52Z,"`ControlledShutdown` only uses `BlockingChannel` if the `inter.broker.protocol.version < 0.9.0.0`, so it's safe to ignore. It's only there to allow rolling upgrades from 0.8.x.

I  haven't checked the other cases, it would indeed be good to know if there's a good reason why they are not using the memory pool."
95582422,2330,rajinisivaram,2017-01-11T13:59:16Z,Are there scenarios where you would expect this to be different from `TransportLayer#ready()`?
95583150,2330,rajinisivaram,2017-01-11T14:03:43Z,Perhaps you want to return `this.ready()`? It looks like both SSL and SASL handshakes are done without using the memory pool. So the check should be for any handshake.
95584021,2330,rajinisivaram,2017-01-11T14:08:42Z,I am not sure of the value of this loop. It is muting a subset of channels (ones that are not in handshake and have not allocated memory and have started read). Channels not muted here and new channels are muted when and only when allocation for read fails. Wouldn't it be better to do the same for the subset handled here as well and remove this loop altogether? It seems to me that this loop simply prevents channels from reading the 4-byte size for which space has already been allocated. 
95589701,2330,rajinisivaram,2017-01-11T14:38:14Z,"Not sure about this. `SslTransportLayer#hasBytesBuffered` returns true if there is any data in `netReadBuffer`. If more data is needed to unwrap and no data arrives from the client, I think the handling of `keysWithBytesBuffered` results in a tight polling loop with timeout=0."
95611883,2330,rajinisivaram,2017-01-11T16:13:52Z,I think you can have empty message body in SASL exchanges.
95612479,2330,rajinisivaram,2017-01-11T16:16:22Z,Perhaps we don't want to release `EMPTY_BUFFER`?
95613398,2330,rajinisivaram,2017-01-11T16:19:58Z,"Similar to the mute in `poll()` - the mute could be delayed until a buffer needs to be allocated? It is possible that the channel already has a buffer allocated, in which case, we want it to complete read."
95638139,2330,ewencp,2017-01-11T18:17:51Z,"It's just more proactive, right? If you're out of memory, you're not going to be able to do anything (beyond the handshake) on any channel anyway. I think the value is that instead of going through a more polling unnecessarily only to end up muting all the channels, you can just do so immediately."
95754342,2330,rajinisivaram,2017-01-12T09:41:39Z,"@ewencp The code looks like it is proactively closing most channels. But actually it closes a small subset of channels. Channels can be in one of these states:
1. Handshake
2. Authentication
3. Waiting to receive a message (receive == null)
4. Received partial message size (receive != null, buffer == null)
5. Received size and partial message body (receive != null, buffer != null)
6. Muted after receiving size due to OOM
7. Explicitly muted
8. Disconnect

The loop actually handles only 4). It mutes 2) at the moment, but that is pointless since authentication doesn't use the pool, so that needs fixing anyway. 4) already has the size buffer, so there is not much point in muting before size is read, after which it will move to 6) if still OOM. Muting proactively is not particularly helpful since disconnect processing gets delayed as well, hence 3) is not muted. If we decide to allocate small buffers outside the pool to handle consumers as Mickael has suggested, it will be useful to mute only in one place - i.e. when a buffer needs to get allocated and its size is known. I think `isInMutableState`
is unnecessary if muting is done on allocation failure and that makes the code simpler.
"
95868086,2330,radai-rosenblatt,2017-01-12T19:37:53Z,"mostly because I was going after a specific OOM scenario - DOS by large producer requests. anything can be ""opted-in"" to using memory pools later on, I was trying to solve just one problem."
95868466,2330,radai-rosenblatt,2017-01-12T19:39:30Z,I originally had a <= 0 check which triggered while testing my code. it surprised me (as evident by the comment) but I decided to live with it instead of tracking it down. also looks like its an expected scenario. i'll update the comment to reflect this
95868992,2330,radai-rosenblatt,2017-01-12T19:41:59Z,"thats a good idea, i'll see if i can improve this."
96104452,2330,radai-rosenblatt,2017-01-14T01:43:22Z,"I account for null as a safety net even though using NONE is clearer. so its both by design. having said that, i'll gladly go for one or the other if there's a style guideline."
96104700,2330,radai-rosenblatt,2017-01-14T01:48:29Z,personally I dont think this is a warning - its normal operations. users who care can get at this information in a much better way via the sensors exposed in this KIP.
96105233,2330,radai-rosenblatt,2017-01-14T02:00:23Z,this whole clause is (in my opinion) premature optimization of an edge case - trying to guarantee fairness when operating under memory pressure and assuming that selectionKeys iteration order is not pseudo random. I'll improve on it if you insist but i would prefer to wait for real world complaints
96512094,2330,radai-rosenblatt,2017-01-17T21:16:12Z,@ewencp - i've implemented a simple schema visitor and used that to find the relevant API keys for this dynamically. please see the revised code.
96512145,2330,radai-rosenblatt,2017-01-17T21:16:26Z,done
96512217,2330,radai-rosenblatt,2017-01-17T21:16:50Z,done
96512883,2330,radai-rosenblatt,2017-01-17T21:19:43Z,"right now no, this exists as a separate API becuase its a different ""aspect"". ideally under java8 i could have made this a method with a default impl"
96513168,2330,radai-rosenblatt,2017-01-17T21:21:02Z,again - its an issue of mutability vs ready being 2 logically different things (even if they are tied for the 2 current implementations of transport). you could think of a future QoS implementation where inter-broker transports arent mutable (as opposed to client-broker transports)
96514142,2330,radai-rosenblatt,2017-01-17T21:25:51Z,"SslTransportLayer#hasBytesBuffered returns true if either the net OR APP buffers have data. its possible that net is done/empty, nothing will ever again be coming out of the socket, but there is data unread in app buffer (so already decrypted, just not read out)"
96515272,2330,radai-rosenblatt,2017-01-17T21:30:48Z,it looks like this code is only ever called from tests?
96522660,2330,radai-rosenblatt,2017-01-17T22:08:37Z,done
96522883,2330,radai-rosenblatt,2017-01-17T22:09:40Z,"i have removed the ""mute everything in advance"" loop in favor of letting channels mute themselves."
96606921,2330,rajinisivaram,2017-01-18T10:42:38Z,"If this code was only called from tests, then channels would remain in `explicitlyMutedChannels` forever :-) It is actually called by the broker - mute/unmute to control reading from the channel and hence the need to track explicitly muted channels."
96609022,2330,rajinisivaram,2017-01-18T10:54:43Z,"@radai-rosenblatt I agree you do need the logic to read buffered data from `SslTransportLayer`. But I think the implementation needs to ensure that it doesn't end up in a tight polling loop when attempting to drain the buffered data. When there is data in the APP buffer, it is reasonable to set timeout=0 and read the data. When there is some data in the NET buffer, it is likely that more data is required to unwrap the data to move it from NET to APP buffer. If there is a network issue that stops any more data arriving, then I think `keysWithBytesBuffered` will set timeout=0 and continue in a polling loop until idle timeout causes the connection to be closed (i.e. 10 minutes of tight polling)."
96611992,2330,rajinisivaram,2017-01-18T11:11:49Z,@radai-rosenblatt It is also about which layers need to know about these different aspects. Does `SslTransportLayer` really need to know about mutability of buffers? And the reason I suggested the change was because `KafkaChannel.isInMutableState()` should return false if either of the conditions in `this.ready()` is false (i.e. transport layer handshake or authenticating). I don't think it makes sense for transport later or authenticator to have to worry about mutability of buffers.
96612136,2330,rajinisivaram,2017-01-18T11:12:44Z,See comment below.
96726102,2330,radai-rosenblatt,2017-01-18T20:25:46Z,"@rajinisivaram:
1. is it guaranteed that if there's anything in net buffer after a read there MUST ALWAYS be more incoming? because if so i can just react solely to data in app buffer.

2. i think i now understand the scenario you describe. my ""best"" idea of how to solve it would be have a boolean return value from pollSelectionKeys() to indicate if any ""progress"" has been made. if no progress has been made in the previous call to poll() the next call would not set timeout to 0. my issue with this solution is that getting progress indications out of channel.read() / channel.write() is a non-trivial refactor (they are currently designed to return null or a complete object, would need to be extended)"
96774619,2330,radai-rosenblatt,2017-01-19T01:25:16Z,@rajinisivaram - i've introduced a simple (relatively...) notion of progress made to try and prevent the tight loop you pointed out.
96777022,2330,radai-rosenblatt,2017-01-19T01:47:23Z,@rajinisivaram - i've dropped transport.isMutable() in favor of just calling ready()
96855844,2330,rajinisivaram,2017-01-19T13:10:48Z,It may be better to call `this.ready()` rather than `transportLayer.ready()` authenticators don't use the memory pool and channels don't need to be muted during authentication.
96856882,2330,rajinisivaram,2017-01-19T13:17:12Z,"Why does this check `dataInBuffers`? With SSL, poll will go through this conditional block most of the time and it (the trace in particular) can be confusing. Wouldn't the first poll after OOM is reset handle the unmute?"
96858147,2330,rajinisivaram,2017-01-19T13:25:02Z,You want the loop to read even when there is no data from the network. So the condition needs to be something along the lines of `if (channel.ready() && (key.isReadable() || channel.hasBytesBuffered()) && !explicitlyMutedChannels.contains(channel) && !hasStagedReceive(channel))`
96859079,2330,rajinisivaram,2017-01-19T13:30:26Z,"Since`keysWithBytesBuffered` was cleared earlier, it needs to be populated regardless of the status of staged receives. I think `""if(..) { keysWithBytesBuffered.add(..); }""` should be done outside the outer if that checks staged receives."
96860499,2330,rajinisivaram,2017-01-19T13:39:14Z,"The current implementation of `addToCompletedReceives` moves receives from staged to completed state if the channel is not muted. I think it will better to replace `!channel.isMute()` with  `!explicitlyMutedChannels.contains(channel)`. Buffers have already been allocated for the staged receives, so we should allow them to make progress and release the buffers."
96914943,2330,radai-rosenblatt,2017-01-19T17:41:09Z,"this isnt about handling the unmute, this is about not waiting (up to 300ms currently) on other sockets if we know we have socket(s) with data in buffers that we can read immediately."
96919823,2330,radai-rosenblatt,2017-01-19T18:05:03Z,"will do. also, to save on the cost of the explicitlyMutedChannels map, do you think its better to replace it with an extra boolean flag on channel? have boolean muted and boolean explicitelyMuted? (or rather bool mutedForOOM and bool mutedForOrdering)"
96948979,2330,rajinisivaram,2017-01-19T20:21:01Z,"I think it would be slightly neater to store the muted state in channel rather than Selector (not necessarily to save on cost, it just feels like channel state)."
96949265,2330,rajinisivaram,2017-01-19T20:22:30Z,There are two if statements - one just above this one sets timeout to zero and that needs to check `dataInBuffers`. This one is just unmuting and resetting `outOfMemory` flag. Not sure why this needs to check `dataInBuffers`.
97010154,2330,radai-rosenblatt,2017-01-20T04:16:52Z,done
97012152,2330,radai-rosenblatt,2017-01-20T04:47:19Z,"youre probably right. if dataInBuffers = true it means either:

1. there is data in app buffer. only way (i think?) to get to this situation is that it could not be read out of app buffer because no memory, hence outOfMemory will be true, which will be enough to trigger an unmute when memory becomes available

2. there's data only in net buffer. this means must data must come from socket and we have successfully read out everything that may have been in app buffer, so we didnt run OOM, so channel is not muted and will show up in a future poll as a read key"
97012165,2330,radai-rosenblatt,2017-01-20T04:47:27Z,done.
97282273,2330,rajinisivaram,2017-01-23T09:49:48Z,"@radai-rosenblatt I tried running this test and the test passes for me when run on its own, but fails consistently when the whole class is run. This assertion is not safe since `isMadeProgressLastPoll()` can be true for various reasons including the key being writable - key may be writable for SSL handshake and so when the handshake completes, madeProgress is set. You could make the flag more conservative in the implementation, but not sure that is worthwhile - you could just remove this assertion from the test."
97354296,2330,radai-rosenblatt,2017-01-23T16:19:05Z,"thats odd. the loop above explicitly waits for both handshakes to complete, and there should only ever be those 2 connections. I will remove the offending check, but i dont think its the handshake"
97360992,2330,rajinisivaram,2017-01-23T16:44:24Z,"I think the loop waits for handshakes to complete from the client point of view, so the server has done its final writes. But Kafka's SslTransportLayer code updates its handshake status a bit lazily, so there is a small window where the server has not yet updated its status after the final write."
97428810,2330,rajinisivaram,2017-01-23T22:10:53Z,SslSender?
97453785,2330,radai-rosenblatt,2017-01-24T00:59:21Z,fixed
97744192,2330,rajinisivaram,2017-01-25T10:01:45Z,"@radai-rosenblatt This needs to be ""TLSv1.2"" to work with Java 7 since the server side properties in tests explicitly set ""TLSv1.2"" and the default TLS version in Java 7 is lower."
97749640,2330,rajinisivaram,2017-01-25T10:29:09Z,"Minor typo (doesn't impact the test, but is confusing). I think you want to use `sslServerConfigs` here and remove `sslClientConfigs` setting just above since only one server channel builder is used in the test?"
97924111,2330,junrao,2017-01-26T02:43:40Z,This seems never used?
97924116,2330,junrao,2017-01-26T02:43:46Z,Does oomTimeSensor need to be volatile?
97924129,2330,junrao,2017-01-26T02:43:54Z,sizeBytes = > sizeInBytes  maxSingleAllocationSize => maxSingleAllocationBytes?
97924148,2330,junrao,2017-01-26T02:44:08Z,"Probably better with ""requested size "" + sizeBytes + "" <=0 ""? "
97924251,2330,junrao,2017-01-26T02:45:33Z,"In the case when the memory pool is full for a long time, we may not be able to update oomTimeSensor for a long period of time, which can make metric inaccurate. We could probably update the sensor periodically (e.g., based on the window size of Sensor) when the allocation is unsuccessful?"
97924265,2330,junrao,2017-01-26T02:45:45Z,Could we just iterate explicitlyMutedChannels directly?
97924301,2330,junrao,2017-01-26T02:45:58Z,Perhaps we can use a better name for keysWithBytesFromSocket since selectedKeys() include keys ready for writes too.
97924332,2330,junrao,2017-01-26T02:46:38Z,"When will keysHandled and selectionKeys have different size? If that happens, it seems that we still need to remove all keys in selectionKeys to clear the ""ready for selection table"" in the nio selector. Also, do you know if selectionKeys.clear() clears the ""ready for selection table""?"
98138485,2330,junrao,2017-01-27T03:01:49Z,This constructor seems never used?
98138501,2330,junrao,2017-01-27T03:01:59Z,"Not very clear on the above comment. Is ""do we do not"" a typo? Is the comment in the right place?"
98138532,2330,junrao,2017-01-27T03:02:26Z,"Is the check (madeProgressLastPoll && dataInBuffers) necessary? dataInBuffers is caused by no memory in the memory pool. It seems that it's simpler to wait for the default selector poll time, which is what we do when the pool is out of memory in other cases."
98138539,2330,junrao,2017-01-27T03:02:32Z,Could we just clear the set to avoid recreation overhead?
98138545,2330,junrao,2017-01-27T03:02:40Z,"Is this test needed? If a channel is explicitly muted, it won't be selected by the selector, right?"
98138556,2330,junrao,2017-01-27T03:02:52Z,Is the change needed since it seems memoryPool is never null from the caller?
98138581,2330,junrao,2017-01-27T03:03:15Z,"Since this is a server side metric, it's probably better to use a Yammer metric to be consistent. Currently, we try only using the client metric on the server side if it needs additional functionality from the client metric (e.g., quota)."
98138585,2330,junrao,2017-01-27T03:03:18Z,Would MemoryPoolUsed be better?
98138605,2330,junrao,2017-01-27T03:03:33Z,"We are not blocking the network threads, right?"
98138611,2330,junrao,2017-01-27T03:03:38Z,"This is optional. So, it probably should be of MEDIUM instead of HIGH?"
98791618,2330,radai-rosenblatt,2017-01-31T22:53:09Z,probably yes.
98792151,2330,radai-rosenblatt,2017-01-31T22:56:07Z,"i dont understand. the sensor is updated on every single tryAllocate call - successful or not.
only way for the sensor to stop being updated is if the server id idle, in which case there should be plenty of memory available?

if you want better accuracy i could update the sensor when calling release() - this by definition means we have memory, so i could zero-out the OOM time"
98793590,2330,radai-rosenblatt,2017-01-31T23:04:19Z,"there's a (hypothetical) corner case where there's data in the ssl app buffer but the underlying socket is done. this means the socket will never come back from a poll call, and you may wait 300ms for no reason instead of servicing the buffer immediately. this is why dataInBuffers exists. i agree its simpler to just wait a whole poll cycle, but this is an attempt to shave off the latency.

the made progress flag exists because the downside of the above condition is you may be stuck in a tight loop trying to service the buffer and so we dont try if no progress was made previous attempt."
98794090,2330,radai-rosenblatt,2017-01-31T23:07:31Z,"explicitlyMutedChannels are channels muted because they already have an outstanding request in progress. we never want to service them until they are (explicitely) unmuted and taken out of the set ?

so this loop iterates over _ALL_ channels, unmuting anything that _ISNT_ in explicit."
98794457,2330,radai-rosenblatt,2017-01-31T23:09:53Z,renamed int readyKeys --> numReadyKeys and keysWithBytesFromSocket to readyKeys
98795499,2330,radai-rosenblatt,2017-01-31T23:16:41Z,"not really - because then toPoll would be a copy ctr.
i need to iterate over keys with buffered data
i also need to record keys that (still?) have buffered data
these have to be different sets or i would be forced to use a thread-safe collection, as i'll be modifying the structure im iterating over?

this set gets ""cycled"" only when under memory pressure, so this is not expected to happen very often.

i could pre-allocate both sets as instance variables on the class, if you want"
98797148,2330,radai-rosenblatt,2017-01-31T23:26:50Z,"a channel can be in explicitelyMuted and in keysWithBytesBuffered at the same time - ssl may try and read several requests at once into stagedReceives. if it reads once request and then has no memory for the next the channel will be in keysWithBytesBuffered. the 1st request out of staged will be moved to completed, causing the channel to be muted when SocketServer picks it up (still on the same thread). 

under this condition channel.hasBytesBuffered() == true and also !hasStagedReceive(channel), causing data to be read for a channel that already has a request in progress"
98800869,2330,radai-rosenblatt,2017-01-31T23:52:08Z,"the sizes will differ only if some uncaught exception terminates the loop early (so never, unless bug?). the code path for different sizes is there to try and match what the previous code would produce under those conditions (which are, again, a bug).

under ""normal"" operating conditions the sizes should always be the same, which i think makes the clear() calls a faster implementation (N * arrayList.add() + 2*clear() < N * set.Iterator.remove()).

what do you mean by ""ready for selection table""? looking at the code for openjdk 8 selectors use a normal set, wrapped to disallow external add() calls. all selectors do is call add() on the set of keys, there's no special ""hook"" to react to removes/clears"
98805695,2330,radai-rosenblatt,2017-02-01T00:30:15Z,"i was trying to be safe, so i ""support"" nulls by translating them to NONE. if you want me to choose either null or NONE (instead of both) - just choose which."
98806646,2330,radai-rosenblatt,2017-02-01T00:38:25Z,would be much simpler if i could :-D
98813839,2330,radai-rosenblatt,2017-02-01T01:38:35Z,"the MemoryPool interface and implementations are in clients, which has no dep. on yammer.
i could either add a dep on yammer (probably bad idea) or introduce an intermediary interface ?"
98814575,2330,radai-rosenblatt,2017-02-01T01:44:32Z,also updated the KIP doc
115627162,2330,junrao,2017-05-09T23:40:23Z,madeProgressThisPoll seems unused?
115627211,2330,junrao,2017-05-09T23:40:51Z,"Selector is shared between client and server. So, it's better not to mention server here."
115627311,2330,junrao,2017-05-09T23:41:34Z, Is the comment accurate? It seems that the underlying socket may still have bytes when there is buffered data.
115627435,2330,junrao,2017-05-09T23:42:31Z,keysWithBytesBuffered => keysWithBufferedRead?
115627563,2330,junrao,2017-05-09T23:43:35Z,previous message => previous receive
115627970,2330,junrao,2017-05-09T23:47:05Z,"So it seems the only reason for this method is to optimize iterator.remove (by using keysHandled .clear())? If so, I am not sure if it's worth doing this optimization since this makes the code a bit harder to read."
115859559,2330,junrao,2017-05-10T21:39:58Z,"Would it be simpler to check channel.isMuted() instead of channel.isInMutableState()? Then, the latter can be a private method in KafkaChannel."
115883466,2330,junrao,2017-05-11T00:30:42Z,"I am wondering if we really need madeProgressLastPoll. In general, if the selector runs out of memory, selector.poll will just block for 300ms in SocketServer. If dataInBuffers is true, it's due to out of memory. So, it seems that it's more consistent and simpler to just wait for the default 300ms in SocketServer?"
115891794,2330,junrao,2017-05-11T01:58:19Z,"Hmm, not sure if this is very reliable since the bytes may still be in the client socket buffer. Perhaps a more reliable way is to do a waitUntil wrapping selector.poll() on the server side."
115892226,2330,junrao,2017-05-11T02:03:23Z,& => && ?
115892482,2330,junrao,2017-05-11T02:06:25Z,"Since this is only called in testMuteOnOOM, which is overridden in SslSelectorTest, perhaps the method can just be private?"
116027427,2330,junrao,2017-05-11T15:47:43Z,Could we track this at nano sec level and pass the value as double in ms for better accuracy?
116027727,2330,junrao,2017-05-11T15:48:49Z,Could we just set oomPeriodSensor in the constructor and get rid of this method?
116772044,2330,radai-rosenblatt,2017-05-16T15:13:48Z,:+1: 
116772303,2330,radai-rosenblatt,2017-05-16T15:14:37Z,:+1: left over from testing
116776829,2330,radai-rosenblatt,2017-05-16T15:29:45Z,"@junrao - the progress indicator was added to prevent a tight looping schenarion that @rajinisivaram spotted at jan 18 (see above discussion on an old version of selector, cant find a way to link to it). i believe the issue is that there may be bights in an underlying ssl buffer that would cause timeout = 0"
116777012,2330,radai-rosenblatt,2017-05-16T15:30:22Z,:+1: 
116777231,2330,radai-rosenblatt,2017-05-16T15:31:08Z,no because a channel can be muted for 2 reasons - 1 request at a time OR memory pressure.
116778212,2330,radai-rosenblatt,2017-05-16T15:34:34Z,we subtract the ready set from keysWithBufferedRead and to toPoll (set of keys we poll from under this condition) ends up being the set of keys for which there is data in buffers but NOT from the underlying socket (else they would be in the ready set)
116781172,2330,radai-rosenblatt,2017-05-16T15:44:42Z,:+1: 
116781268,2330,radai-rosenblatt,2017-05-16T15:45:04Z,:+1: 
116781973,2330,radai-rosenblatt,2017-05-16T15:47:30Z,"the sender only terminates after it completely flushes its output stream, so i would expect everything to have been written out? also, we accept() both incoming connections before the call to poll so that we know that at least handshaking has been done at that point. given that this is all local networking i think the timing is loose enough (also i've not see this fail in all the times that i've rebased and retested this branch)."
116783226,2330,radai-rosenblatt,2017-05-16T15:52:21Z,:+1: 
116784228,2330,radai-rosenblatt,2017-05-16T15:56:04Z,this assignment operator is defined differently for boolean operands (so it doesnt perform a bitwise operation) and so there is no &&=. i'll refactor the code to avoid this (as its rather obscure)
116801727,2330,radai-rosenblatt,2017-05-16T17:09:41Z,:+1: 
128666574,2330,junrao,2017-07-21T01:26:26Z,"Since dispose() can be called by both network threads and request handler threads, should we make buffer volatile?"
128906634,2330,junrao,2017-07-22T22:54:57Z,It seems that we can just check !keysWithBufferedRead.isEmpty?
128906636,2330,junrao,2017-07-22T22:55:15Z,"Hmm, it seems that madeProgressLastPoll needs to be set to false somewhere?"
128906639,2330,junrao,2017-07-22T22:55:28Z,"Since there is no guarantee when the server will receive those bytes, should we put this code block in a waitUntil loop?"
128906644,2330,junrao,2017-07-22T22:55:49Z,"Since there is no guarantee when the server will receive those bytes, should we put this code block in a waitUntil loop?"
128906645,2330,junrao,2017-07-22T22:55:56Z,MemoryPoolAvgDepletedPercent => MemoryPoolUtilization?
128906650,2330,junrao,2017-07-22T22:56:02Z,MemoryPoolAvgDepletedPercent-Avg => MemoryPoolAvgDepletedPercent
128906668,2330,junrao,2017-07-22T22:56:13Z,"Would it be better to name this queued.max.request.bytes? Otherwise, it's not obvious what queued bytes are for."
128906671,2330,junrao,2017-07-22T22:56:21Z,"Instead of defaulting it to null, should we default it to Defaults.QueuedMaxBytes?"
129095498,2330,radai-rosenblatt,2017-07-24T17:07:51Z,will fix
129099716,2330,radai-rosenblatt,2017-07-24T17:23:54Z,will fix
129099747,2330,radai-rosenblatt,2017-07-24T17:24:04Z,will fix
129107398,2330,radai-rosenblatt,2017-07-24T17:53:14Z,"the test asserts on the state of the progress flag, meaning we cant call poll() more than once (2nd+ call will wipe the progress flag).
will do


"
129107452,2330,radai-rosenblatt,2017-07-24T17:53:26Z,will do
129147307,2330,radai-rosenblatt,2017-07-24T20:39:15Z,will do
129147337,2330,radai-rosenblatt,2017-07-24T20:39:21Z,will do
129147672,2330,radai-rosenblatt,2017-07-24T20:40:48Z,will do
129148372,2330,radai-rosenblatt,2017-07-24T20:43:47Z,the null was the validator. i've simply removed it now
129403649,2330,junrao,2017-07-25T19:35:22Z,Could we add a comment to explain why madeReadProgressLastPoll is used for?
129403695,2330,junrao,2017-07-25T19:35:36Z,Is there a need to set madeReadProgressLastPoll here? It seems we only need to set when doing reads? 
129403711,2330,junrao,2017-07-25T19:35:43Z,Is there a need to set madeReadProgressLastPoll here? It seems we only need to set when doing reads? 
129408331,2330,junrao,2017-07-25T19:55:20Z,"Since on the server side, we release memory through RequestChannel.dispose(). However, memory is also released here through KafkaChannel.close(). Will this cause the same memory to be released more than once in certain cases?"
129412615,2330,radai-rosenblatt,2017-07-25T20:14:00Z,will do
129412879,2330,radai-rosenblatt,2017-07-25T20:15:14Z,"i was being cautious. but youre right, given the progress flag is only used in combination with dataInBuffers its probably impossible for a poll() round to involving the progress flag to consist solely of handshaking and connecting operations."
129412922,2330,radai-rosenblatt,2017-07-25T20:15:26Z,removed (see above comment)
129419356,2330,radai-rosenblatt,2017-07-25T20:40:32Z,"KafkaChannel.receive is a receive _in progress_. once its compete its read out (field is nulled) and the buffer passed to a RequestChannel.Request.
so any given buffer exists either as part of an in-progress receive or as part of a completed receive (that was transferred to request channel).
given the transition happens on the same thread that would close a KafkaChannel, i dont think this would be a problem?"
311655316,7170,mjsax,2019-08-07T16:46:14Z,nit: `creates` -> `create`
311655490,7170,mjsax,2019-08-07T16:46:41Z,"nit: `deserializers, [and] producer's`"
311657543,7170,mjsax,2019-08-07T16:51:17Z,"nit: `The number of partitions is determined based on the upstream topics partition numbers.`

One may use `merge()` and there may be multiple upstream topic[s] -- for this case, we use max-partitions over all upstream topics. Hence, I would be a little bit more fuzzy and avoid ""inherit"" as it implies it's the same number of partitions, but that only holds for the case of a single upstream topic. I would also use ""upstream"" instead of ""input"" because there might be an upstream repartition topic, too."
311658732,7170,mjsax,2019-08-07T16:54:06Z,"Do we need to have those two lines? We use `@see` usually to point to similar method, but not to point to overloads. For example, `map()` points to `mapValues()`, but `map()` would not point to another variant of `map()`.

The idea is to point people to different functionality, but if one know about `repartition` we assume they consider all overloads they can use."
311658960,7170,mjsax,2019-08-07T16:54:33Z,nit: `creates` -> `create`
311659340,7170,mjsax,2019-08-07T16:55:31Z,"nit: `partitions[,] and`"
311660226,7170,mjsax,2019-08-07T16:57:20Z,"nit: `name[,] and`

Why ""if repartitioning is required"" ? From my understanding, calling `repartition()` should always repartition, ie, enforce it.

That's why we included `groupBy()` in the KIP -- if one does not want to force repartitioning, but want to control repartition topic properties, one can pass in `Repartitioned` into `groupBy` but would not use `KStream#repartition`. At least, that was my understanding of the KIP? "
311660448,7170,mjsax,2019-08-07T16:57:52Z,Why `(and potentially repartitioned)` ? Should be removed IMHO
311660775,7170,mjsax,2019-08-07T16:58:39Z,"as above.

My comments form above also apply to the third overlaod. Not repeating them again."
311661094,7170,mjsax,2019-08-07T16:59:23Z,This will not render as expected in the JavaDocs. You need to you HTML markup to define bullet points.
311661405,7170,mjsax,2019-08-07T17:00:04Z,nit: remove empty lines between members
311661640,7170,mjsax,2019-08-07T17:00:37Z,"nit: remove ""if required""

nit: `{@link Repartitioned}` -> `{@code Repartitioned}` (no need to link to itself -- it's considered bad practice).

Both nits apply to other JavaDocs, too. Will not comment on the other ones, but please fix everywhere."
311663095,7170,mjsax,2019-08-07T17:04:09Z,Should we add an import to avoid the long package name?
311668500,7170,mjsax,2019-08-07T17:16:53Z,Why not `Repartitioned.as(null)` ? This way we can remove `empty()` -- it would align with the pattern we apply in existing code.
311670840,7170,mjsax,2019-08-07T17:22:03Z,"Compare my other comment: from my understanding, we would always repartition."
311672777,7170,mjsax,2019-08-07T17:26:27Z,Nit: Keep existing formatting or more `.withKeySerde(...)` into its own line.
311674136,7170,mjsax,2019-08-07T17:29:29Z,"As the parent class contains the corresponding member, both method should be added there"
311677453,7170,mjsax,2019-08-07T17:36:40Z,Seems do don't need `NamedInternal namedInternal` as it's own variable?
311693215,7170,mjsax,2019-08-07T18:12:47Z,We should extend existing `addInternalTopic` instead of having two method.
311696457,7170,mjsax,2019-08-07T18:20:30Z,Updating `repartitionTopicConfig` in this method may not be the best pattern. Could we pass the number of partitions into the constructor of `RepartitionTopicConfig` instead?
311708461,7170,lkokhreidze,2019-08-07T18:48:18Z,"yeah, that was something i wanted to verify actually. for example with DSL, user can do something like: `stream(...).mapValues().repartition()`. In this case repartition topic doesn't make much sense. So I chose to guard against situations like that. Open to suggestions."
311713308,7170,lkokhreidze,2019-08-07T18:59:38Z,"I added integration test to verify that repartition topic won't be created if key-changing operation isn't performed. If number of partitions is specified, repartition topic will be created though."
311803123,7170,mjsax,2019-08-07T23:27:48Z,"If we don't repartition if user calls `repartition()` what is the purpose of the operation? The new operator is similar to `through()`, with the difference that Kafka Streams manages the topic.

Note, that one motivation for adding `repartition()` was, to allow users to repartition data before `transform()`. Atm, this in only possible via `through()` forcing users to create the corresponding topic manually what is cumbersome.

If a user does `stream(...).mapValues().repartition()` I agree that repartitioning is not really required, but I would see this as a user error. At the same time, I see the potential to address this in the optimization layer: if we detect this case, we could remove the `repartition()` operator. It seems to be a subtle difference, but it's semantically two different approaches IHMO -- what you suggest is to keep the operator but to make it a no-op, while I suggest to _remove_ the operator. This would result in the same topology but the code how it is achieve is different and I believe it's an important difference."
311879654,7170,lkokhreidze,2019-08-08T06:54:26Z,I like the idea of addressing this on optimization layer. I think this depends on how end implementation would look like and what we gonna agree on in our main discussion thread on this PR. If we gonna have only `repartitioned` operator I guess it make sense to always force repartitioning. If we gonna go with `groupBy` and potentially `join` - optimization layer should be smarter about this in that case. I'll wait for an outcome of our discussion and update this accordingly.
312410620,7170,lkokhreidze,2019-08-09T09:58:28Z,"I've removed this check from here, but I'll investigate if we can do this in optimization layer as suggested in this thread: https://github.com/apache/kafka/pull/7170/files#r311660226. "
312427351,7170,lkokhreidze,2019-08-09T10:51:54Z,"none of the other members have corresponding methods in parent class. Do you think it's still okay to move only this two members? If that's the case, I would prefer moving all of the accessor methods there. I guess accessors are added only to `OptimizableRepartitionNode` because accessing members is needed only in case of optimization (`InternalStreamsBuilder#getFirstRepartitionTopicName`)"
312732526,7170,lkokhreidze,2019-08-11T10:18:23Z,"Based on comments from @vvcephei I've removed `repartition` operations from optimization logic altogether. Now, when calling `repartition` operations, corresponding repartition topic will be always created."
316772010,7170,vvcephei,2019-08-22T16:24:53Z,"Just as a general note, I 100% sympathize with the impulse to clean stuff up alongside your changes, but it would really help the reviewers if you made a pass over the PR and just removed all changes that aren't related to KAFKA-8611.

It's not a big deal with small changes, but when the PR is over a thousand lines of code, it really adds a lot of distraction when reviewers have to consider cleanup alongside substantial changes."
316781727,7170,vvcephei,2019-08-22T16:48:13Z,"I was looking at how this is used, and there are only two usages. In both cases, we create the builder, then call a method that populates the builder, then call `build()`. Maybe we can just ditch the builder and invoke the constructor from that static method?"
316783992,7170,vvcephei,2019-08-22T16:53:45Z,"This invocation has the side effect of incrementing the ""topology name counter"". In other words, this means that inserting a ""repartition"" node with a name will cause all the other processors, stores, and repartition topics in the topology to get renamed anyway.

To clarify (because it's confusing) this method increments the counter even if it's not generating a name.

Should we consider instead making it like Suppression, which does not increment the counter if you provide a name?"
316787124,7170,vvcephei,2019-08-22T17:01:14Z,"```suggestion
     * Materialize this stream to an auto-generated repartition topic and create a new {@code KStream}
```"
316787232,7170,vvcephei,2019-08-22T17:01:34Z,"```suggestion
     * from the auto-generated topic using default serializers, deserializers, and producers {@link DefaultPartitioner}.
```"
316787409,7170,vvcephei,2019-08-22T17:02:01Z,"```suggestion
     * Materialize this stream to an auto-generated repartition topic and create a new {@code KStream}
```"
316788481,7170,vvcephei,2019-08-22T17:04:49Z,"```suggestion
     * Materialize this stream to an auto-generated repartition topic and create a new {@code KStream}
```"
316788550,7170,vvcephei,2019-08-22T17:04:59Z,"```suggestion
     * from the auto-generated topic using default serializers, deserializers, and producers {@link DefaultPartitioner}.
```"
316788989,7170,vvcephei,2019-08-22T17:06:07Z,"```suggestion
     * Materialize this stream to an auto-generated repartition topic and create a new {@code KStream}
```"
316790193,7170,vvcephei,2019-08-22T17:09:02Z,It seems this method is unused. Are we missing test coverage?
316790517,7170,vvcephei,2019-08-22T17:09:48Z,"Likewise, this one is unused."
316790765,7170,vvcephei,2019-08-22T17:10:29Z,Thanks for avoiding mutable state in this class!
316794184,7170,vvcephei,2019-08-22T17:18:57Z,"It seems like this might be able to just replace `internalTopicNames`. We only add to `internalTopicNames` in one place, where we also (maybe) add to this map. We can't (shouldn't) make the value `null`, but I noticed that `InternalTopicProperties` allows its parameter (`numberOfPartitions`) to be null, which seems like it should have the same effect as a null `internalTopicProperties`...

What do you think about requiring `iternalTopicProperties` to be non-null in `addInternalTopic`, although it might have a null number of partitions. Then, we can get rid of `internalTopicNames` and just use `internalTopicNamesWithProperties.keySet()`?"
316795023,7170,vvcephei,2019-08-22T17:21:08Z,"If we add this to `equals`, we *must* add it to `hashCode` as well, and we _should_ also add it to `toString()`."
316795418,7170,vvcephei,2019-08-22T17:22:03Z,should be final
316795547,7170,vvcephei,2019-08-22T17:22:24Z,These three fields should be final as well.
316796336,7170,vvcephei,2019-08-22T17:24:14Z,generics can be inferred here.
316796825,7170,vvcephei,2019-08-22T17:25:17Z,"variable can be final. I won't comment on final-able variables anymore. Do the tests pass? There should be a check that fails on variables that aren't final, but could be."
316796993,7170,vvcephei,2019-08-22T17:25:43Z,method should be static
316797085,7170,vvcephei,2019-08-22T17:25:55Z,"likewise, this one can be static"
316798314,7170,vvcephei,2019-08-22T17:28:41Z,"Not sure what the intent is here, to increment the number between each test, or between each instance of this integration test class within the JVM... It actually does the latter."
316798925,7170,vvcephei,2019-08-22T17:30:02Z,"This can (and should) be a unit test, since we don't need to produce data or run Kafka to build and verify the topology."
316802639,7170,vvcephei,2019-08-22T17:38:10Z,"Maybe consider:

```suggestion
        final CountDownLatch latch = new CountDownLatch(1);
        kafkaStreams.setStateListener((newState, oldState) -> {
            if (oldState == KafkaStreams.State.REBALANCING && newState == KafkaStreams.State.RUNNING) {
                latch.countDown();
            }
        });
        kafkaStreams.start();
        try {
            latch.await(IntegrationTestUtils.DEFAULT_TIMEOUT, TimeUnit.MILLISECONDS);
        } catch (final InterruptedException e) {
            throw new RuntimeException(e);
        }
```

Then, this method won't return until Streams is actually started, which we've seen can increase test stability."
317388374,7170,lkokhreidze,2019-08-25T08:46:32Z,"interesting... yup, checkstyleTests pass. I think checkstyle don't cover `try with resources` usage. anyway, it should be final yes, will update this everywhere. thanks."
317389525,7170,lkokhreidze,2019-08-25T09:19:55Z,"I can change it, sure. but personally i would prefer to have the builder here because 1) it follows same standard as other `BaseRepartitionNode` implementations 2) static factory method for the `UnoptimizableRepartitionNodeBuilder` will have a lot of parameters and it'll make code uglier.
wdyt?"
317390219,7170,lkokhreidze,2019-08-25T09:41:47Z,so that each individual test has its own input/output topics. there's code in `@After` that increments `TEST_NUM`
317412260,7170,lkokhreidze,2019-08-25T19:50:33Z,Done
317412281,7170,lkokhreidze,2019-08-25T19:51:11Z,Added `StreamsGraphTest#shouldNotOptimizeWhenRepartitionOperationIsDone`
317412296,7170,lkokhreidze,2019-08-25T19:51:32Z,Done
317412304,7170,lkokhreidze,2019-08-25T19:51:38Z,Done
317412334,7170,lkokhreidze,2019-08-25T19:52:23Z,Done
317412354,7170,lkokhreidze,2019-08-25T19:53:02Z,Added tests.
317412360,7170,lkokhreidze,2019-08-25T19:53:11Z,Added tests
317412366,7170,lkokhreidze,2019-08-25T19:53:23Z,Done.
317412373,7170,lkokhreidze,2019-08-25T19:53:30Z,Done
317412377,7170,lkokhreidze,2019-08-25T19:53:38Z,Done
317412381,7170,lkokhreidze,2019-08-25T19:53:44Z,Done
317412451,7170,lkokhreidze,2019-08-25T19:55:45Z,"Very fair point, totally agree. Sorry about that. There're not many changes related to ""cleanup"" in this PR, but if you think it creates noise and makes it harder to do the review, I'll revert all cleanup related code."
320850937,7170,vvcephei,2019-09-04T16:17:30Z,Fair enough. Thanks for the reply.
320859172,7170,vvcephei,2019-09-04T16:35:57Z,"Thanks for your understanding. I just mentioned it because it seemed like there wasn't a ton of review activity. Just thought I'd share the tactic with you, since rightly or wrongly, the large ""diff"" numbers in the PR can scare off reviewers.

For myself, I didn't have trouble overlooking it."
320859948,7170,vvcephei,2019-09-04T16:37:42Z,"Ah, yeah, there are some limitations to the linter. Thanks for taking care of it."
320861613,7170,vvcephei,2019-09-04T16:41:31Z,"I see. But even though the number gets incremented after each test method, the string `inputTopic` is already fixed when the class is constructed, so it won't automatically get incremented. I think you need to make this a method to achieve the effect you intended."
320868518,7170,lkokhreidze,2019-09-04T16:57:19Z,"Hi @vvcephei thanks for the comment. Sorry if I'm missing something... but JUnit creates new instance of the class before each test case. Since those are non-static fields they'll be initialized during each test run with incremented `TEST_NUM`.
I've verified it one more time and during each individual test topic number gets incremented. Here are the screenshots just for the sake of clarity:

<img width=""1662"" alt=""Screen Shot 2019-09-04 at 7 47 11 PM"" src=""https://user-images.githubusercontent.com/8927925/64275423-29f79180-cf4e-11e9-8506-bff8d5c417cc.png"">
<img width=""1658"" alt=""Screen Shot 2019-09-04 at 7 47 03 PM"" src=""https://user-images.githubusercontent.com/8927925/64275424-29f79180-cf4e-11e9-9083-924d3b663818.png"">
"
321099321,7170,lkokhreidze,2019-09-05T06:51:57Z,"Makes total sense, thanks for sharing. I will definitely take this into account for the future PRs."
321394898,7170,vvcephei,2019-09-05T17:41:10Z,"Ok, I'm convinced :)

Thanks for clearing up my confusion."
329290021,7170,mjsax,2019-09-28T01:10:35Z,"Should we explain the difference to `through()`? Something like:
```
This operation is similar to {@link #through()}, however, Kafka Streams manages the used topic automatically. The created topic is considered and internal topic and hence to other application should read from the topic. Similar to auto-repartitioning, the topic will be created with infinite retention time and data will be purged by Kafka Streams explicitly.
```

We might also update the docs for `through()` and point to the new operator.

We should also cross-link using `@see` tags"
329290382,7170,mjsax,2019-09-28T01:18:11Z,"While I agree that immutability is great, I am wondering about consistency. The other configuration classes are mutable. Do we think that might be of any concern? should we just update all other configuration classes an make them immutable, too? (Of course not in this PR...) \cc @vvcephei "
329290536,7170,mjsax,2019-09-28T01:21:53Z,"nit: remove `this.` (we only use `this` is we must -- applies to other parts, too)"
329290692,7170,mjsax,2019-09-28T01:25:27Z,"This method share a lot of code with `repartition(Repartitioned)` -- we should create `private doRepartition(KeyValueMapper, Repartitioned)` and all 4 public method should call it and just have a block
```
if (selector != null) {
  // insert corresponding node...
}
```"
329291018,7170,mjsax,2019-09-28T01:33:09Z,"Seems we should add this to the parent class? And also use `OptimizableRepartitionNode` and `GroupedTableOperationRepartitionNode`.

As a side cleanup, we should remove the `get` prefix -> `keySerializer()` -- we should also remove the `get` from `BaseRepartitionNode#getKeySerializer()` (same for value)"
329291182,7170,mjsax,2019-09-28T01:37:28Z,Not sure why we need this? Couldn't we use `InternalTopicConfig`?
329291498,7170,mjsax,2019-09-28T01:45:23Z,"Wondering if we should treat `numberOfPartitions` as a regular config and add to `Map<String, String> topicConfigs` instead using `num.partitions` as parameter name? \cc @guozhangwang @vvcephei "
329291593,7170,mjsax,2019-09-28T01:48:31Z,It seems also a little inconsistent with `InternalTopicConfig` that uses `Optional` but not `int`...
329291663,7170,mjsax,2019-09-28T01:50:38Z,"The whole method could be simplified to (and hence removed and embedded):
```
return new RepartitionTopicConfig(decorateTopic(topic), internalTopicProperties.getNumberOfPartitions(), Collections.emptyMap));
```
if we let `#getNumberOfPartitions()` return an `Optional`."
329311990,7170,lkokhreidze,2019-09-28T14:19:49Z,"Thought about it, but it felt more natural to introduce separate class rather than using `RepartitionTopicConfig` in current design. `RepartitionTopicConfig` potentially can be package-private (haven't touched this in current PR) and I wanted to avoid leaking it through other packages. It felt like best to leave construction of `RepartitionTopicConfig` in `InternalTopologyBuilder` class since `InternalTopologyBuilder` ""knows the best"" how to construct it. Idea of this class is to provide bare minimum configs of internal topic properties. Hope my way of thinking around this makes sense. wdyt? "
329329123,7170,lkokhreidze,2019-09-28T22:01:50Z,Done.
329329133,7170,lkokhreidze,2019-09-28T22:02:31Z,Done
329329200,7170,lkokhreidze,2019-09-28T22:06:17Z,"Fixed, `numberOfPartitions` is now Optional"
329329236,7170,lkokhreidze,2019-09-28T22:07:25Z,"Did what you suggested. `BaseRepartitionNode` now has following methods:

```
    Serializer<V> valueSerializer() {
        return valueSerde != null ? valueSerde.serializer() : null;
    }

    Deserializer<V> valueDeserializer() {
        return valueSerde != null ? valueSerde.deserializer() : null;
    }

    Serializer<K> keySerializer() {
        return keySerde != null ? keySerde.serializer() : null;
    }

    Deserializer<K> keyDeserializer() {
        return keySerde != null ? keySerde.deserializer() : null;
    }
```

I've refactored `GroupedTableOperationRepartitionNode`, `OptimizableRepartitionNode` and `UnoptimizableRepartitionNode` accordingly."
329329258,7170,lkokhreidze,2019-09-28T22:08:26Z,Done
329329268,7170,lkokhreidze,2019-09-28T22:08:51Z,Done and added more javadocs
361869452,7170,lkokhreidze,2019-12-29T20:02:17Z,@mjsax I've created JIRA ticket for it: https://issues.apache.org/jira/browse/KAFKA-9342
368248756,7170,mjsax,2020-01-18T21:11:21Z,nit: you need to insert `<p>` markup if you want to get a new paragraph.
368248926,7170,mjsax,2020-01-18T21:15:07Z,I don't think that `{@link #through}` is correct markup. Should be `{@link #through(String)}`
368249035,7170,mjsax,2020-01-18T21:16:57Z,"`Created topic` -> `[The] created topic is considered [an] internal topic` ?

"
368249068,7170,mjsax,2020-01-18T21:18:01Z,"as above -> `<p>` (seems other comment from above apply here, too -- won't repeat them)"
368249232,7170,mjsax,2020-01-18T21:21:51Z,nit: `selector` -> `keySelector` (we recently did a cleanup PR that uses `keySelector` as name is all other method that set a new key -- would be nice to align the names)
368249741,7170,mjsax,2020-01-18T21:33:51Z,"Nit: in other classed, we just add `@see` tags instead of mentioning it in the text. Should we do the same here for consistency?"
368250067,7170,mjsax,2020-01-18T21:41:06Z,"We can do all `null` checks in a single place within `doRepartition()` -- also, all other methods just say ` <parameterName> can't be null` (ie, please remove `parameter` for consistency)"
368250271,7170,mjsax,2020-01-18T21:46:00Z,"I don't think we can use `this.keySerde` that should align to the type of the input key, ie, type `<K>` -- instead, we should set `keySerde` to `null` if not specified by `Repartitioned` and fall back to the default key serde from `StreamsConfig` during runtime. (This issue is also indicated by the ""unchecked"" warning that you suppress...)"
368250370,7170,mjsax,2020-01-18T21:48:15Z,Comment seems redundant
368250670,7170,mjsax,2020-01-18T21:54:42Z,Not sure if I understand why we need this? It seems also to be tricky to understand the code if one calls `RepartitionTopicConfig#setNumberOfPartitions` and nothing happens because the actual object is of type `ImmutableRepartitionTopicConfig`.
368251040,7170,mjsax,2020-01-18T22:02:16Z,"Should be better throw here as this should never be called?

What raised the question (we you actually do check the type already: should be flip the hierarchy as being mutable is a superset of being immutable and thus it should be `RepartitionTopicConfig extends ImmutableRepartitionTopicConfig` -- for this case, `ImmutableRepartitionTopicConfig` would not have this method at all what seems to be cleaner)"
368251147,7170,mjsax,2020-01-18T22:05:25Z,To what extent is this check different from the check above when we call `validateAndGetNumOfPartitionsOfImmutableTopics` -- or can we remove it here as it's redundant?
368251683,7170,mjsax,2020-01-18T22:16:36Z,"I am wondering, if we should really throw an exception for this case? Why do we not create this repartition topic with the same number of partitions as specified on the second input topic instead?

IIRC, for the following case we would also adjust the number of partitions:
```
topicWith2partitions = builder.stream(...).map(...);
topicWith4partitions = builder.stream(...);
topicWith2Partitions.join(topicWith4partitions,...)
```
This code above is very similar to:
```
topicWith2partitions = builder.stream(...).map(...);
topicWith4partitions = builder.stream(""topicWith1Partitions"").repartition(/*set 4 partitions*/);
topicWith2Partitions.join(topicWith4partitions,...)
```

In both cases, the number of partitions of one topic is fixed, while the second one has key-changing operation and thus we can just create a reparition topic that matches the number of partitions of the first topics?"
368421799,7170,lkokhreidze,2020-01-20T08:40:38Z,"Thanks for the suggestion. We can do null check in `doRepartition` for `repartitioned` parameter. null check for `selector` parameter should be on upper level since in some cases we do pass selector as `null`. For example:
```java
    @Override
    public KStream<K, V> repartition() {
        return doRepartition(Repartitioned.as(null), null);
    }
```

Will update the error msg as well.
  "
368425477,7170,lkokhreidze,2020-01-20T08:49:46Z,good call. done.
369771654,7170,lkokhreidze,2020-01-22T19:56:18Z,"Interesting point. I guess in that case idea would be if in the `copartitionGroup` there's one `ImmutableRepartitionTopicConfig` and rest are repartition topics, we will enforce number of partitions from `ImmutableRepartitionTopicConfig`. In cases when there're more than one `ImmutableRepartitionTopicConfig` we would still throw an exception. Does this make sense?"
369775730,7170,lkokhreidze,2020-01-22T20:05:04Z,"This check covers the case when number of partitions do not match between immutable repartition topics (aka created via `repartition` operation) and _ordinary_ repartition topics. But considering your comment below, different logic is needed here."
369784339,7170,lkokhreidze,2020-01-22T20:24:25Z,"Originally I've implemented this method with throwing an exception. But it seems like this method is being called from various places, like `StreamsPartitionAssignor#prepareTopic`. My thinking was - instead of each individual caller checking if `InternalTopicConfig` supports setting number of partitions, it makes more sense to delegate this to the ""builder"" that chooses appropriate implementation based on some logic.

`setNumberOfPartitions` is part of `InternalTopicConfig` so even i flip the hierarchy, I can't avoid `setNumberOfPartitions` method. So I don't think it gives any benefit if we flip the hierarchy. I also thought instead of adding new class, I could maybe enhance `RepartitionTopicConfig` to support ""immutability"" in cases when topics are for `repartition` operation, but in that case I have to add a flag to the class to indicate that this `RepartitionTopicConfig` is actually for `repartition` operation which seems a bit worse compared to introducing new class. Thoughts?

"
369788818,7170,lkokhreidze,2020-01-22T20:35:33Z,"Not ideal, agree. But tbh whole repartition topic management is complicated and is built around the idea of updating the number of partitions during different phases of Kafka Streams lifecycle. Seems like adding new concrete class that indicates ""immutability"" the easiest and safest solution for now without changing current implementation and logic too much. Would appreciate your ideas around this. Not sure how to accommodate and guarantee immutability of partitions in some other (without introducing some major changes current internal topic management logic. Maybe followup ticket is in order?)"
373849243,7170,lkokhreidze,2020-02-02T14:14:32Z,"Hi @mjsax this is now implemented. Logic is the following: If `RepartitionTopicConfig`s which have enforced number of partitions have the same value, non-enforced repartition topics (like for mapper) will be created with the num of partitions specified via `repartition` operation. `shouldDeductNumberOfPartitionsFromRepartitionOperation` integration tests verifies this case."
373849898,7170,lkokhreidze,2020-02-02T14:23:47Z,"Hi @mjsax 
gave it a bit more thought and decided to ditch this class altogether. `RepartitionTopicConfig` and `ImmutableRepartitionTopicConfig` are exactly the same, just one wouldn't allow setting num of partitions. Also, considering your comment about it being tricky to understand when num of partitions can be set or not (which is very valid concern) I've decided to encapsulate necessary logic into `RepartitionTopicConfig` and `InternalTopicConfig` classes. `InternalTopicConfig` now can accept in the constructor `enforceNumberOfPartitions` boolean flag:

```
InternalTopicConfig(
    final String name,
    final Map<String, String> topicConfigs,
    final int numberOfPartitions,
    final boolean enforceNumberOfPartitions
)
```

If `enforceNumberOfPartitions` is set as `true`, and somebody decides to call `setNumberOfPartitions` method, exception will be raised. I think this should make things much more clear. Looking forward to your feedback.
"
373849925,7170,lkokhreidze,2020-02-02T14:24:14Z,This class was removed. Check my comment here: https://github.com/apache/kafka/pull/7170#discussion_r373849898
373863902,7170,lkokhreidze,2020-02-02T18:02:01Z,Update: I've added `numberOfPartitions` as int this constructor overload to indicate that passing `numberOfPartitions` is mandatory when one wants to use this constructor. Corresponding InternalTopicConfig constructor also uses int.
392608571,7170,vvcephei,2020-03-14T18:24:49Z,"Looking at this again with fresh eyes, I can't remember what advantage this has over `selectKey(KeyValueMapper).repartition(Repartitioned)`. Can you remember why we decided to add this, @lkokhreidze ?"
397650119,7170,lkokhreidze,2020-03-25T07:24:30Z,"Hi @vvcephei 
As far as I remember we didn't have any specific discussion around this operation. Main reason why we have added it I think is because of the convenience (merging selectKey and repartition into single operation). Similarly how `groupBy(final KeyValueMapper<? super K, ? super V, KR> selector, final Grouped<KR, V> grouped)` does it."
398113069,7170,vvcephei,2020-03-25T19:25:52Z,"I see. IIRC, your initial thought was to replace `groupBy` with `repartition`, but we've gotten away from that design. Now, it's more like a managed-topic version of `through` (in fact, this is what the javadoc for the method says). Maybe this is why I was confused to see this overload, since it makes less sense to think of changing the key at the last minute before `though` or `to`.

Are you particularly attached to this convenience overload? I'm just thinking it's a safer bet to add it later if people really want it than to add it now and never really know if it's useful or not."
398366211,7170,lkokhreidze,2020-03-26T07:36:34Z,"I can't say I am particularly attached to it, but I think it's useful one. From personal experience, we, at our company, often write topologies similar to this:

```
streamBuilder.stream(""input-topic"")
.selectKey(...)
.through(...)
.transform(...) // transform is stateful 
```
here we need to explicitly use through in order to trigger repartitioning by selecting key (and we have to manage topic that we create in advance for `through` operation)

New way of doing same thing is quite nice I think:

```
streamBuilder.stream(""input-topic"")
.repartition(...) // select key here
.transform(...) // transform is stateful 
```

For us, it's quite common use-case. And.my _guess_ is it will be common use-case for anyone using complex `transform` operations in DSL. Does this make sense? On the other hand, if you believe that it's safer to remove `KeyValueMapper` overloads, I'll definitely do it."
398927435,7170,vvcephei,2020-03-26T22:20:37Z,"I certainly agree that it would be common to do a `selectKey` before a `repartition`. There's a trade-off to strike between the number of different operations you need and the number of options on a single operation you have to choose from.

I guess my hesitation is that it's still two different operations. For example, it also seems like it would be common to do a `map` before a `repartition`, but it's clearly now too much piled on if we have a fourth `repartition` overload also folding in the `map` operation. When I imagine coming to the API as a user, especially for the first time, I worry that I'd already have a lot of documentation to read to understand the implications of `repartition`, and each new overload adds linearly to the amount I have to learn to use the API. Plus, I just feel like I would be puzzled about the exact same question I asked above: is this overload just the same as `selectKey().repartition()`, or does it do something subtly different? As you can tell, I worry quite a bit about how we can make sure the API stays as simple as possible while we still add new functionality.

I guess this is just a long-winded way of saying that, yes, I would prefer to remove it :) Hopefully, this isn't too disappointing for you, since the main motivation was to save on managing the `through` topic, not necessarily to save on that extra `selectKey` operator."
399058852,7170,lkokhreidze,2020-03-27T06:30:00Z,"Thanks John, that's a valid point. Agree, I'll remove it."
399067977,7170,lkokhreidze,2020-03-27T06:59:31Z,Done. I'll resolve this conversation.
401253268,7170,vvcephei,2020-03-31T22:30:12Z,"```suggestion
            .selectKey(Integer::sum);
```

Looks like an accidental formatting change."
401349352,7170,mjsax,2020-04-01T04:28:23Z,typo: `producer's`
401349473,7170,mjsax,2020-04-01T04:28:51Z,`[c]reated`
401349644,7170,mjsax,2020-04-01T04:29:27Z,`by [the] current`
401349985,7170,mjsax,2020-04-01T04:30:58Z,`explicitly` -> `automatically` ? (not sure which one is better)
401350251,7170,mjsax,2020-04-01T04:32:09Z,"`[c]reated`
`by [the] current`"
401350309,7170,mjsax,2020-04-01T04:32:27Z,`automatically` ?
401350867,7170,mjsax,2020-04-01T04:34:46Z,Nit: do we need to add the `@see` tag to every method? Seems somewhat redundant (it's already mentioned in class JavaDocs above)? (similar below for other methods)
401351951,7170,mjsax,2020-04-01T04:39:27Z,Why would we not use an upstream `keySerde` (similar to `valueSerde = valSerde` L584 above) if `repartitionInternal` has a `null` key serde?
401352615,7170,mjsax,2020-04-01T04:42:42Z,Why do we need to duplicate this method? Might it be better to have just a single one and let caller set a `null` `StreamPartitioner` is they can't set it?
401354459,7170,mjsax,2020-04-01T04:50:33Z,"For a `GroupedTableOperationRepartitionNode` we should never have a customized `InternalTopicProperties` object, but it should always be `InternalTopicProperties.empty()` -- can we simplify this and not pass this parameter at all?"
401354751,7170,mjsax,2020-04-01T04:51:55Z,Similar as above: can we avoid this parameter?
401355550,7170,mjsax,2020-04-01T04:55:00Z,"`this.name = Objects.requireNonNull(name, ""name can't be null"");` ?

Also, I believe `topicConfig` should not be `null` either -- can we add a check (also for the existing constructor above?"
401358667,7170,mjsax,2020-04-01T05:08:44Z,"With parallel test runners, would it be better to call this as first line in `before()` method (and use the returned value instead of calling `get()` in addition -- otherwise, we might get multiple different numbers per test run)? Also wondering if we should assign the topic names that use the counter within before?

For a clean isolation, it might also be good to add the test number to the `application.id`"
401359192,7170,mjsax,2020-04-01T05:10:53Z,Should we setup all topics name within `before`?
401360195,7170,mjsax,2020-04-01T05:14:55Z,"For this particular test, it seems we could detect the issue during topology `build()` already? Ie, we could do an additional early check? If we think it's worth doing, we should do it in follow up PR to not drag this PR any longer. \cc @vvcephei (If yes, we could change this test from an integration test to a unit test)"
401361948,7170,mjsax,2020-04-01T05:21:56Z,"Why do we have a `map()` step here? Wouldn't this imply a repartition topic that would match whatever number of partitions is used on the other stream? Ie, only without the map(), we guarantee that `topicBStream` has a certain number of partitions?

With the `map()` step it seems to be the same test `shouldDeductNumberOfPartitionsFromRepartitionOperation` as above?"
401362636,7170,mjsax,2020-04-01T05:24:32Z,"Do we need an integration test for this? Using `Topology#describe()`, I think we could verify this with a unit test."
401362888,7170,mjsax,2020-04-01T05:25:34Z,Not sure why we need this test?
401363128,7170,mjsax,2020-04-01T05:26:37Z,"nit: in test code, the signature can always be simplified to `throws Exception` (there is no value to list exceptions) -- same for all test methods in this class (and maybe somewhere else?)"
401364227,7170,mjsax,2020-04-01T05:30:40Z,Similar to above: we should be able to test with via unit tests using `Topology#describe()`
401364387,7170,mjsax,2020-04-01T05:31:15Z,Seems to be unit-test able via `Topology#describe()` ?
401364523,7170,mjsax,2020-04-01T05:31:46Z,"Not sure what this test is about, ie, how does is relate to the `repartition()` feature?"
401365280,7170,mjsax,2020-04-01T05:34:11Z,Not sure what this test actually verifies?
401365442,7170,mjsax,2020-04-01T05:34:48Z,Using `MockProcessorSupplier` is an old test pattern -- we should use the new `TestOutputTopic` instead.
401368679,7170,mjsax,2020-04-01T05:46:09Z,"I realize that this contradicts a previous review comment, but I think that the older comment was incorrect, because `repartition()` might be called to just scale out without a key changing operation and thus for this case we should reuse the upstream `keySerde` (note that if there was an upstream key changing operation, `keySerde` would be set to `null` and we would still fall back to the default serdes from the config)."
401369054,7170,mjsax,2020-04-01T05:47:16Z,Why do we introduce a new type `<KR>`? The key type of the input and output KStream does not change during repartitioning.
401372458,7170,mjsax,2020-04-01T05:58:29Z,"@lkokhreidze Can you update the KIP wiki page accordingly and send an follow up email to the VOTE thread of the KIP to highlight the change as an FYI that the KIP was modified (just in case somebody would have an objection, what I don't expect -- it's just custom in the community to do this)."
402561217,7170,lkokhreidze,2020-04-02T19:32:00Z,"I've followed same standard as other configurations classes (Produced, Grouped, etc). To keep things consistent maybe worth cleaning up all the config classes with redundant `@see` tags? (in the follow up pr maybe) wdyt? "
403061232,7170,lkokhreidze,2020-04-03T14:51:10Z,"`application.id` already has test number.

Will do as you suggested."
403097472,7170,lkokhreidze,2020-04-03T15:44:39Z,You're right. This test is redundant. Removed it.
403098661,7170,lkokhreidze,2020-04-03T15:46:32Z,"Wanted to verify that key changing operation with `repartition` works as expected. I think it adds value, especially considering the fact that we've removed `repartition(KeySelector` overloads."
403109967,7170,lkokhreidze,2020-04-03T16:05:00Z,"This was the ""easiest"" way I could figure out to verify that custom partitioner is invoked when it's set"
403372355,7170,lkokhreidze,2020-04-03T22:48:53Z,It's related to this comment https://github.com/apache/kafka/pull/7170#issuecomment-522163447
403374045,7170,lkokhreidze,2020-04-03T22:54:37Z,"Thought about that, but somehow it felt ""safer"" with integration tests. Mainly because I was more comfortable verifying that topics actually get created when using repartition operation."
403374128,7170,lkokhreidze,2020-04-03T22:54:55Z,"Thought about that, but somehow it felt ""safer"" with integration tests. Mainly because I was more comfortable verifying that topics actually get created when using repartition operation."
403374331,7170,lkokhreidze,2020-04-03T22:55:41Z,I'll create followup ticket on that.
403374552,7170,lkokhreidze,2020-04-03T22:56:35Z,"@mjsax yes, thanks for reminding me. Was meaning to do it."
403449003,7170,lkokhreidze,2020-04-04T09:36:26Z,@mjsax done
404993384,7170,vvcephei,2020-04-07T17:39:46Z,"Yeah, I'd agree with checking as early as possible in the special cases where we can know the partition counts statically. But also agree with doing it in a follow-on ticket, since it's kind of a nice-to-have."
404994770,7170,vvcephei,2020-04-07T17:42:00Z,"I had a similar thought, that it looks like good fodder for unit testing, but I did like the safety blanket of verifying the actual partition counts. I guess I'm fine either way, with a preference for whatever is already in the PR ;) "
406532074,7170,mjsax,2020-04-09T23:34:00Z,"Yeah. Was just a general inquire and we don't really have a guideline for it... If you are interested, it would be great to draft some guidelines (maybe just for Kafka Streams first, and we could propose them for other client APIs, later) as a wiki page and we could discuss them on the dev mailing list?"
406532691,7170,mjsax,2020-04-09T23:36:34Z,Cool. @lkokhreidze did you create a ticket already? (Just want to make sure we don't drop this on the floor.)
406532989,7170,mjsax,2020-04-09T23:37:44Z,Ok. Thanks for clarifying.
406533537,7170,mjsax,2020-04-09T23:39:49Z,"> Mainly because I was more comfortable verifying that topics actually get created when using repartition operation.

I guess that is fair. (I just try to keep test runtime short if we can -- let's keep the integration test.)"
406533774,7170,mjsax,2020-04-09T23:40:52Z,Thanks for clarifying!
406535443,7170,mjsax,2020-04-09T23:47:20Z,"Seems unnesseary complex? A simple 
```
        return Arrays.asList(new String[][] {
            {StreamsConfig.OPTIMIZE},
            {StreamsConfig.NO_OPTIMIZATION}
        });
```
would do, too :)

(Feel free to ignore the comment.)"
406535968,7170,mjsax,2020-04-09T23:49:15Z,"A simple
```
    @Parameter
    public String topologyOptimization;
```

Would be sufficient instead of adding a constructor and those lines could go into `before()`.

(As above, feel free to ignore this comment.)"
406839201,7170,lkokhreidze,2020-04-10T16:39:43Z,"@mjsax yes, here it is https://issues.apache.org/jira/browse/KAFKA-9850"
406899069,7170,mjsax,2020-04-10T19:04:51Z,Thank you!
365485878,7884,junrao,2020-01-11T01:08:13Z,"ltc => logToClean ? Also, do we need to use another local val since ltc is only used once?"
365486280,7884,junrao,2020-01-11T01:12:19Z,Need to change the javadoc above to currentTime.
365486311,7884,junrao,2020-01-11T01:12:45Z,Could we add the new param to javadoc?
366049275,7884,junrao,2020-01-13T21:55:01Z,"We should make it clear the difference btw retainDeletesAndTxnMarkers and tombstoneRetentionMs. Also, it's probably better to put they as adjacent params."
366061210,7884,junrao,2020-01-13T22:23:48Z,"Hmm, isControlBatchEmpty is a bit misleading since batch is not always a control batch."
366064567,7884,junrao,2020-01-13T22:32:30Z,retainTxnMarkers is no longer used in shouldDiscardBatch().
366085895,7884,junrao,2020-01-13T23:36:42Z,"It's a bit awkward to have to pass in the same batch to two different methods isControlBatchEmpty and checkBatchRetention, during filtering. I was thinking that perhaps that we could just combine them into a single method checkBatchRetention(), which returns (BatchRetention, shouldSetHorizon). We could then extend shouldDiscardBatch() to sth like the following. The result of shouldDiscardBatch() can then be used to build the result for checkBatchRetention().

```
shouldDiscardBatch(): (Boolean, shouldSetHorizon) = {
  canRemoveBatch = false;
  if (batch.isControlBatch) {
    if (transactionMetadata.onControlBatchRead(batch)) {
      if (batch could be removed based on deleteHorizon, old or new way) 
         batchRetention = true
      else
         batchRetention = false
    }
    if (batch.magic() >= V2 and !batch.deleteHorizonSet)
      shouldSetDeleteHorizon = true;
  }  else
      canRemoveBatch=transactionMetadata.onBatchRead(batch)
(canRemoveBatch, shouldSetDeleteHorizon)
}
``` "
366086360,7884,junrao,2020-01-13T23:38:16Z,"We probably need to do the check based on the batch magic. If magic is >=V2, check based on the new deleteHorizonMs. Otherwise, check based on the old approach."
366090028,7884,junrao,2020-01-13T23:51:19Z,"It's probably better to have the logic to determine if deleteHorizonMs should be set here instead of MemoryRecords since it's log cleaner specific logic. I was thinking that we could extend checkBatchRetention() to return (Boolean, shouldSetHorizon)."
366093839,7884,junrao,2020-01-14T00:05:55Z,"Hmm, if deleteHorizonSet is not set, we shouldn't be deleting the tombstone. So, not sure what newBatchDeleteHorizonMs is intended for."
366093991,7884,junrao,2020-01-14T00:06:30Z,"Hmm, why are we passing in containsTombstonesOrMarker, which is always false?"
366095048,7884,junrao,2020-01-14T00:10:34Z,"Since deleteHorizonMs can be obtained from batch, it's not clear why we need to pass that in as a param."
366098944,7884,junrao,2020-01-14T00:25:10Z,"I am not sure about this. A round of cleaning can be expensive since we need to read in all existing cleaned segments. That's why by default, we only trigger a round of cleaning if the dirty portion of the log is as large as the cleaned portion. Not sure if it's worth doing cleaning more aggressively just to remove the tombstone. So, perhaps we can leave it outside of this PR for now."
367151238,7884,ConcurrencyPractitioner,2020-01-15T22:56:48Z,"Yep, done so.
"
367151276,7884,ConcurrencyPractitioner,2020-01-15T22:56:54Z,Done.
367152424,7884,ConcurrencyPractitioner,2020-01-15T23:00:16Z,"Oh, this is used as a means to help the tests in LogCleanerTest.scala pass. LogCleanerTest usually wants the tombstones removed in a single pass (but that pass is usually used for setting the delete horizon ms, which means without doing the above, we would be unable to remove tombstones). Therefore, by adding the ```newBatchDeleteHorizonMs``` argument (which is passed in by MemoryRecords), whenever LogCleaner calls clean log with the current time marked as ```Log.MAX_VALUE```, we will be able to remove the tombstones / control records in one pass."
367152628,7884,ConcurrencyPractitioner,2020-01-15T23:00:52Z,"Oh, I can remove that."
367153616,7884,ConcurrencyPractitioner,2020-01-15T23:03:46Z,"Oh, look in comment above. This delete horizon is used for the case where we want to remove the tombstones in a single pass. On the first iteration of Log Cleaner, we are unable to remove the tombstone because no delete horizon has not been set yet. Therefore, when we compute the delete horizon, we need to pass the delete horizon back into ```checkBatchRetention``` so that tombstones can be removed in one iteration.

On second thought, I think we don't need to add an extra parameter to the ```checkBatchRetention``` method. Such logic would only need to be restricted to LogCleaner. i.e. we store the delete horizon in another variable in the Record Filter we implemented in LogCleaner."
367154887,7884,ConcurrencyPractitioner,2020-01-15T23:07:25Z,"@junrao I did some thinking about this. The integration test I added does not pass without this part. Because what happens is that in logs with tombstones, there is the possibility that without further throughput, the cleanable logs will always be empty. Therefore, as I mentioned in the comment, since we are in a low throughput situation, LogCleaner's workload is relatively light anyways. In that case, we can clean tombstones since we don't have much else to do."
367675732,7884,junrao,2020-01-16T22:08:25Z,tombstoneRetentionMs is duplicated in the javadoc.
367675858,7884,junrao,2020-01-16T22:08:39Z,Could we add currentTime to the javadoc?
367679076,7884,junrao,2020-01-16T22:17:04Z,This seems never used?
367692941,7884,ConcurrencyPractitioner,2020-01-16T22:57:49Z,"There is a way to figure out whether if log cleaner has a heavy workload or not. If cleanable logs has remained empty for a long period of time (for a set threshold), then we can safely say that the log cleaner thread isn't busy since there is no logs to clean. After that threshold has passed, we can start processing logs with tombstones and removing them.

This should help us know exactly when we can go back and remove tombstones."
367711171,7884,junrao,2020-01-17T00:02:22Z,"Perhaps, we can keep track of the largest deleteHorizonMs in the cleaned portion. We can then trigger a round of cleaning when the current time has passed the largest deleteHorizonMs."
367731735,7884,junrao,2020-01-17T01:29:55Z,"I am not sure that I understand the need for overloading this and the other method. It seems that this is just so that we can remove the tombstone in one pass in the test? If so, could we just design/fix the test accordingly?"
367732948,7884,junrao,2020-01-17T01:35:23Z,"Hmm, I am still not sure why we need to remove a tombstone in one pass. If a tombstone's delete horizon is not set, it can't be removed in this round of cleaning."
371018776,7884,ConcurrencyPractitioner,2020-01-26T18:08:45Z,"Yep, I realized that was probably unnecessary, so I removed it. "
371018800,7884,ConcurrencyPractitioner,2020-01-26T18:08:57Z,"Yeah, will get rid of that.
"
371019104,7884,ConcurrencyPractitioner,2020-01-26T18:14:07Z,"Alright, acknowledged. I think thats a good point."
372143661,7884,ConcurrencyPractitioner,2020-01-29T01:09:37Z,"Yeah, I found that this approach probably is a lot better."
374441552,7884,junrao,2020-02-04T02:08:37Z,deleteHorizonMs in the next line is no longer present.
374442122,7884,junrao,2020-02-04T02:11:12Z,Could we move this up to below retainDeletesAndTxnMarkers?
375562926,7884,junrao,2020-02-05T23:18:14Z,It's probably better to name writeOriginalBatch here to sth like recordsFiltered since we combine other information to determine writeOriginalBatch later on.
375565104,7884,junrao,2020-02-05T23:24:38Z,It seems that the logic can be simplified a bit. It seems that we can do this branch if writeOriginalBatch is true and needToSetDeleteHorizon is false (`needToSetDeleteHorizon = (batch magic >= V2 && containsTombstonesOrMarker && batch's deleteHorizon not set)`). 
376105398,7884,junrao,2020-02-06T21:58:24Z,"This may not be the best place to track latestDeleteHorizon. Perhaps we can return the largest deleteHorizon in MemoryRecords.filterTo() and keep track of latestDeleteHorizon in the while loop in line 713. If we do that, I am not sure if we need retrieveDeleteHorizon() since MemoryRecords.filterTo() can obtain whether deleteHorizon is set from the batch and calculate the new deleteHorizon if needed."
376109770,7884,junrao,2020-02-06T22:08:23Z,"Hmm, it seems that we only want to pass in deleteHorizonMs if `containsTombstonesOrMarker && deleteHorizon is not set`."
376111679,7884,junrao,2020-02-06T22:12:46Z,isLatestVersion => supportDeleteHorizon?
376120400,7884,junrao,2020-02-06T22:34:58Z,Could we put the common logic into a shared method to avoid duplicating most of the code below?
376122666,7884,junrao,2020-02-06T22:41:04Z,This method seems unused?
376122878,7884,junrao,2020-02-06T22:41:37Z,This method seems unused?
376124221,7884,junrao,2020-02-06T22:45:04Z,It seems that we need to reinitialize this value at the start of each round of cleaning.
376138391,7884,ConcurrencyPractitioner,2020-02-06T23:23:32Z,"Oh, that's a good catch! Otherwise, we might end up cleaning the logs over and over again."
376142170,7884,ConcurrencyPractitioner,2020-02-06T23:35:23Z,"Oh, sure, that's fine. But we also still need to account for the control batch and check whether or not it is empty yet. "
376147542,7884,junrao,2020-02-06T23:53:27Z,"For a control batch, it's only removed at the batch level. So, if the batch can be deleted at the batch level, we won't get in here. If the batch can't be deleted at the batch level, the record within the batch will always be retained."
378016120,7884,ConcurrencyPractitioner,2020-02-12T02:54:11Z,"By current logic, this would actually break the code. Since we don't pass a ```deleteHorizonSet``` boolean flag into the MemoryRecordsBuilder constructor, the MemoryRecordsBuilder class's current logic actually relies on the passed in argument to tell if the delete horizon has been set or not. i.e. (if deleteHorizonMs > 0L, then we set delete horizon, else we assume that it has not been set). Should I change the code correspondingly to accomadate your comment? @junrao "
378017124,7884,ConcurrencyPractitioner,2020-02-12T02:58:14Z,"@junrao Is this always the case? If I remember correctly in the KIP, control batches, if it contains only tombstones, will be persisted in the logs for a set period of time i.e. we need to at some point remove the tombstones first _before_ the control batches can be deleted. Therefore, I think it would be very much possible that we need to check for ```isControlBatchEmpty``` here. "
378019351,7884,ConcurrencyPractitioner,2020-02-12T03:08:21Z,"Well, I think there is multiple problems we might need to think about:

1. We don't know what the current time is since MemoryRecords doesn't have access to a ```Time``` instance.
2. For control batches, ```retrieveDeleteHorizon``` serves a critical function: We call ```controlBatch.onTransactionRead``` there to determine if we can set a delete horizon for our batch.

In summation, I think that there are multiple dependencies (located in LogCleaner) which must be called from ```MemoryRecords#filterTo```. It would be more of a hassle I think if we need to figure out a way how to call all these methods from filterTo as well. "
378046269,7884,junrao,2020-02-12T05:26:10Z,"@ConcurrencyPractitioner :  A control batch has only a single marker record (either a commit or abort). When all records before the control batch are removed, we set the deleteHorizon for the control batch. When the time passes the deleteHorizon, the control batch is removed. A control batch never contains a tombstone."
379089454,7884,junrao,2020-02-13T20:03:44Z,"Good point on #2.  My concern is that the batch could be filtered after retrieveDeleteHorizon() is called. Then, the latestDeleteHorizon maintained here won't be very accurate."
379094481,7884,junrao,2020-02-13T20:14:36Z,"Yes, it's just that in this PR, retrieveDeleteHorizon() returns deleteHorizonMs > 0 even for batches where deleteHorizonMs doesn't need to be set. Then, we will be setting deleteHorizonMs for those batches unnecessarily."
381024939,7884,junrao,2020-02-19T01:06:47Z,This batch could be filtered later in MemoryRecords.filterTo(). So if we maintain latestDeleteHorizon here. It may not be accurate.
381031162,7884,junrao,2020-02-19T01:29:53Z,"You were correct earlier that for a control Marker, deleteHorizon should only be set after transactional records before the marker have already been removed. So, we can't just set needToSetDeleteHorizon based on containsTombstonesOrMarker. Also, I still feel that retrieveDeleteHorizon() is a bit weird since it mixes deleteHorizonMs that's already set with the deleteHorizonMs to be set.

So, perhaps it's clearer if we instead have a method containEmptyMarker() that simply passes along the return value of shouldDiscardBatch().

Then `needToSetDeleteHorizon = batch.magic() >= 2 && (containEmptyMarker || containsTombstones) && !batch.deleteHorizonSet())`.

If we need to set deleteHorizon, deleteHorizonMs can be computed off tombstoneRetentionMs, which can be passed into filterTo()."
381031270,7884,junrao,2020-02-19T01:30:19Z,It seems we should check needToSetDeleteHorizon ?
381454797,7884,ConcurrencyPractitioner,2020-02-19T18:15:32Z,"Alright, sounds cool. This actually makes sense. I got it done."
381551654,7884,junrao,2020-02-19T21:21:55Z,This check seems redundant since the caller has verified it already. We can just always return the expected deleteHorizon.
381552110,7884,junrao,2020-02-19T21:22:56Z,This should now be named containsTombstones.
381554557,7884,junrao,2020-02-19T21:27:44Z,These two lines are awkward. Could we pass them through the constructor of RecordFilter?
381598053,7884,junrao,2020-02-19T23:03:08Z,"I am not sure that I follow the logic here. To me, the easiest way is to reset log.latestDeleteHorizon at the beginning of each round of cleaning. Then, we update it with the latestDeleteHorizon remaining in each cleaned segment. "
381599279,7884,junrao,2020-02-19T23:06:12Z,"Could we just fold containsEmptyMarker() into this method and let checkBatchRetention() return (BatchRetention, containsEmptyMarker)?"
382869365,7884,junrao,2020-02-22T01:00:22Z,"cleanSegments() just cleans a portion of the log. So, we need to reset log.latestDeleteHorizon in the caller doClean()."
383572781,7884,junrao,2020-02-24T23:20:23Z,This seems to be only used in tests. Could we just create a util method in test?
383572822,7884,junrao,2020-02-24T23:20:32Z,firstClean is unused.
383573289,7884,junrao,2020-02-24T23:21:50Z,"It would be useful to indicate that trackedHorizon is to cover tombstones in legacy message format. So, perhaps we could name it sth like legacyDeleteHorizonMs?"
383574205,7884,junrao,2020-02-24T23:24:38Z,discarding tombstones => discarding legacy tombstones ?
383575250,7884,junrao,2020-02-24T23:27:41Z,deletion horizon => legacy deletion horizon ?
383576155,7884,junrao,2020-02-24T23:30:28Z,No need for space before (.
383577646,7884,junrao,2020-02-24T23:34:52Z,containsEmptyMarker => containsMarkerForEmptyTxn ?
383578258,7884,junrao,2020-02-24T23:36:54Z,"We can just do 

```
val batchRetention: BatchRetention = if () ...
                   else ...
                   else ...
```"
383579725,7884,junrao,2020-02-24T23:41:45Z,"This can be 

```
val shouldRetainDelete = if () ...
   else ...

```"
383580041,7884,junrao,2020-02-24T23:42:55Z,"It seems this can be simplified to the following?

`shouldRetainDeletes = !batch.deleteHorizonSet() || currentTime < batch.deleteHorizonMs()`"
383580335,7884,junrao,2020-02-24T23:43:50Z,retainDeletes => retainDeletesForLegacyRecords ?
383581756,7884,junrao,2020-02-24T23:48:20Z,BatchRetentionAndEmptyMarker => BatchRetentionResult ?
383582417,7884,junrao,2020-02-24T23:50:19Z,the tombstones => the tombstones or txn markers 
383585273,7884,junrao,2020-02-24T23:59:07Z,"Not sure if we need these comments. If we do need them, it seems they should be added to the implementation in LogCleaner."
383585606,7884,junrao,2020-02-25T00:00:14Z,"Hmm, this comment seems out of place."
383619974,7884,junrao,2020-02-25T02:02:28Z,This doesn't look right. We need to track not only newly generated deletionHorizon but also existing one if the batch is kept.
383620136,7884,junrao,2020-02-25T02:03:11Z, batch and containsEmptyMarker are unused.
384268522,7884,ConcurrencyPractitioner,2020-02-26T04:25:16Z,@junrao Aren't we already keeping track of each individual delete horizon in each batch's first timestamp? My impression was that this result would just return the biggest delete horizon seen so far.
384284272,7884,junrao,2020-02-26T05:41:46Z,"@ConcurrencyPractitioner : If we get into the else branch in line 210, it seems that we still need to call filterResult.updateLatestDeleteHorizon() since the batch may contain deleteHorizon?"
384597604,7884,ConcurrencyPractitioner,2020-02-26T16:10:30Z,"Oh, I see. Makes sense. I misunderstood what the comment was suggesting. "
384611807,7884,ConcurrencyPractitioner,2020-02-26T16:30:59Z,"Just a note, I actually did resolve this comment with my previous push. Turns out I spotted this error while running over the code previously. Just didn't realize that it actually resolved this one as well. "
384793024,7884,junrao,2020-02-26T21:59:46Z,retainDeletesAndTxnMarkers => retainLegacyDeletesAndTxnMarkers ?
384795228,7884,junrao,2020-02-26T22:04:17Z,batchRetentionAndEmptyMarker => batchRetentionResult ?
384804732,7884,junrao,2020-02-26T22:24:41Z,"If we get in here, it could be that this batch already has deleteHorizon set. If we pass in RecordBatch.NO_TIMESTAMP to buildRetainedRecordsInto(), we will lose the deleteHorizon. So, we need to pass in the existing deleteHorizon to buildRetainedRecordsInto() and also reflect that deleteHorizon in filterResult."
384806582,7884,junrao,2020-02-26T22:28:41Z,typo Thoroughput
384810439,7884,junrao,2020-02-26T22:37:28Z,We probably don't need to assert this since we explicitly inserted some tombstones.
384810766,7884,junrao,2020-02-26T22:38:17Z,"To avoid transient failures, we probably want to give long enough maxWaitMs, sth like 5 secs."
384811017,7884,junrao,2020-02-26T22:38:53Z,This seems unnecessary since we are waiting in cleaner.awaitCleaned() already later.
384811420,7884,junrao,2020-02-26T22:39:55Z,This seems to be a complicated way of getting latestOffset. We could just do log.logEndOffset.
384818440,7884,junrao,2020-02-26T22:56:55Z,"The value of the map is an offset. So, it's weird to put in deleteHorizon as the value.

Also, there seems to be an existing issue with the test. It seems that shouldRemain in line 90 should be computed before line 89."
384827619,7884,junrao,2020-02-26T23:22:15Z,"Why do we need to set current time to Long.MaxValue - tombstoneRetentionMs - 1? For verifying the removal of the tombstone, it's clearer if we set the currentTime in mockTime before the first round of cleaning and then explicit set current time to be tombstoneRetentionMs longer than that currentTime in a subsequent round of cleaning to verify that the tombstone is removed. Ditto below."
384829410,7884,junrao,2020-02-26T23:27:43Z,Could we add a comment on why we need two passes? 
384835414,7884,junrao,2020-02-26T23:46:13Z,"Since there is no marker, It seems that containsMarkerForEmptyTxn should be false."
384835749,7884,junrao,2020-02-26T23:47:16Z,"Since there is no marker, It seems that containsMarkerForEmptyTxn should be false."
384835973,7884,junrao,2020-02-26T23:48:03Z,"Since there is no marker, It seems that containsMarkerForEmptyTxn should be false."
384837927,7884,junrao,2020-02-26T23:54:01Z,Could we use RecordBatch.NO_TIMESTAMP instead of -1L?
384848752,7884,ConcurrencyPractitioner,2020-02-27T00:28:59Z,"This was one part of the test which I had some problems with. Notably, what happens is that we will try to calculate the delete horizon using Long.MaxValue as the current time. Inherently, an integer overflow error will occur (and we end up with some very low negative number). Therefore, I thought that we can get around it by setting the delete horizon to some value which  would not have problems with overflow (hence largeDeleteHorizon having the above value you mentioned.)"
384854011,7884,ConcurrencyPractitioner,2020-02-27T00:46:54Z,"Yeah, it definitely is inconsistent with other tests in that there is a Thread.sleep(). Problem is that this test seems prone to be somewhat flaky. Without the sleep, at the present state, it definitely fails."
384855964,7884,ConcurrencyPractitioner,2020-02-27T00:53:07Z,"Also, about setting mock time. Mock time is in fact never called in doClean. It is called in just clean(). The currentTime supplied to doClean is from clean(). So what you stated probably only applies to methods which call the regular clean() method."
385391824,7884,junrao,2020-02-27T21:50:31Z,"Instead of sleeping, it's more reliable to just do cleaner.awaitCleaned() and assert the return value to be true."
385393585,7884,junrao,2020-02-27T21:54:21Z,"Hmm, in the previous round of cleaning, the dirty offset is already moved to log.logEndOffset. So, this call seems to also hit the timeout. Another way is to do TestUtils.waitUntilTrue(log.size() == 0). Then, we don't need the code in line 214 to 216."
385402234,7884,junrao,2020-02-27T22:13:38Z,"If we set currentTime to largeDeleteHorizon in the previous round of cleaning, we need to set the current time  to Long.MaxValue - 1 in order for the marker to be removed."
385409940,7884,junrao,2020-02-27T22:31:37Z,It doesn't seem that we need to convert this to runTwoPassClean().
385409985,7884,junrao,2020-02-27T22:31:43Z,It doesn't seem that we need to convert this to runTwoPassClean().
385410725,7884,junrao,2020-02-27T22:33:33Z,It seems this is a case that we should use runTwoPassClean().
385412064,7884,junrao,2020-02-27T22:37:09Z,It seems that currentTime should be set to Long.MaxValue - 1 to make sure the record still remains after the deleteHorizon.
385414150,7884,junrao,2020-02-27T22:42:40Z,"It's clearer if we set currentTime to largeDeleteHorizon here and in the second round of cleaning, set currentTime to Long.MaxValue - 1. We also want to change the comment above accordingly. "
385414334,7884,junrao,2020-02-27T22:43:11Z,"In this case, it seems that one round of doClean() is enough."
385416143,7884,junrao,2020-02-27T22:48:06Z,"Similar here. If we set currentTime to largeDeleteHorizon in the first round cleaning, we can just do one round of cleaning with currentTime set to Long.MaxValue - 1 and the first marker should be removed."
385418628,7884,junrao,2020-02-27T22:55:11Z,No need for this change since the intention is to put MaxValue as the offset in the map.
385419500,7884,junrao,2020-02-27T22:57:42Z,No need for this change since the intention is to put MaxValue as the offset in the map.
385419989,7884,junrao,2020-02-27T22:58:59Z,No need for this change since the intention is to put MaxValue as the offset in the map.
385420266,7884,junrao,2020-02-27T22:59:48Z,No need for this change since the intention is to put MaxValue as the offset in the map.
385420579,7884,junrao,2020-02-27T23:00:44Z,No need for this change since recovery point is not related to deleteHorizon.
385420800,7884,junrao,2020-02-27T23:01:15Z,No need for this change since the intention is to put MaxValue as the offset in the map.
385420835,7884,junrao,2020-02-27T23:01:21Z,No need for this change since the intention is to put MaxValue as the offset in the map.
385420853,7884,junrao,2020-02-27T23:01:26Z,No need for this change since the intention is to put MaxValue as the offset in the map.
385422158,7884,junrao,2020-02-27T23:05:04Z,"Perhaps it's clearer with ""On the first run, set the delete horizon in the batches with tombstone or markers with empty txn records."""
385429375,7884,junrao,2020-02-27T23:27:13Z,"With this PR, we can set currentTime to largeDeleteHorizon and the marker should still be preserved. We can change the comment above accordingly."
385841515,7884,junrao,2020-02-28T18:03:18Z,Let's assert that the return value is true.
385841874,7884,junrao,2020-02-28T18:04:07Z,This is unnecessary given the waitUntilTrue() below.
385843938,7884,junrao,2020-02-28T18:09:10Z,"We can be more generous with waitTimeMs. So, using the defaults for both waitTimeMs and pause is probably fine."
385848945,7884,junrao,2020-02-28T18:20:20Z,"This is still a bit confusing. Could we define 2 vals, beforeDeleteHorizon and afterDeleteionHorizon? The former takes Long.MaxValue - tombstoneRetentionMs - 1 and the latter takes Long.MaxValue. The comment can be changed to sth like ""current time is still before deleteHorizon"". It would be useful to do this consistently across other tests."
385851682,7884,junrao,2020-02-28T18:26:00Z,The previous comment was not addressed. It doesn't seem that we need to convert this to runTwoPassClean().
385854943,7884,junrao,2020-02-28T18:32:59Z,"In this case, it seems that one round of doClean() is enough as long as we set currentTime to postDeleteHorizon."
385855310,7884,junrao,2020-02-28T18:33:46Z,"Similar here. If we set currentTime to largeDeleteHorizon in the first round cleaning, we can just do one round of cleaning with currentTime set to Long.MaxValue - 1 and the first marker should be removed."
386758173,7884,junrao,2020-03-03T01:56:04Z,"Could we add the following comment above? 
""The deleteHorizon for {Producer2: Commit} is still not set yet."""
386758582,7884,junrao,2020-03-03T01:57:36Z,"Could we add the following comment above?
""In the first pass, the deleteHorizon for {Producer2: Commit} is set. In the second pass, it's removed."""
388490888,7884,junrao,2020-03-05T18:49:32Z,"Could we add the following comment?

""In the first pass, deleteHorizon is set for the abort marker. In the second pass, the abort marker is removed."
388507941,7884,junrao,2020-03-05T19:18:44Z,We could just use one pass of cleaning with currentTime = Long.MaxValue.
388509040,7884,junrao,2020-03-05T19:20:36Z,"Could we add the following comment?

""In the first pass, the deleteHorizon for the commit marker is set. In the second pass, the commit marker is removed, but the empty batch is retained for preserving the producer epoch."""
388511801,7884,junrao,2020-03-05T19:25:20Z,"Could we change the comment to the following?

""Aborted records are removed, but the abort marker is still preserved."""
388512379,7884,junrao,2020-03-05T19:26:18Z,"Could we change the comment to the following?

""In the first pass, the delete horizon for the abort marker is set. In the second pass, the abort marker is removed."""
388514252,7884,junrao,2020-03-05T19:29:45Z,"Could we change the comment to the following?

""In the first pass, the delete horizon for the first marker is set. In the second pass, the first marker is removed."""
388516416,7884,junrao,2020-03-05T19:34:06Z,This can just be one pass cleaning with currentTime = largeTimestamp.
388517353,7884,junrao,2020-03-05T19:35:57Z,"Could we add the following comment?

""In the first pass, the delete horizon for the abort marker is set. In the second pass, the abort marker is removed."""
391180372,7884,hachikuji,2020-03-11T18:32:07Z,Why don't we move these into `AbstractLegacyRecordBatch`?
391184841,7884,hachikuji,2020-03-11T18:40:03Z,nit: can we use `hasDeleteHorizonMs`. Another option would be to make `deleteHorizonMs` return an optional long.
391187776,7884,hachikuji,2020-03-11T18:45:22Z,"Hmm.. It seems a bit brittle to rely on documentation for this. I'm considering if we should change names to better reflect this. For example, maybe we should call this `baseTimestamp` and add a new method for `firstRecordTimestamp` or something like that."
391189939,7884,hachikuji,2020-03-11T18:49:05Z,"nit: why don't we initialize the variables here? e.g.
```java
boolean containsTombstones = iterationResult.containsTombstones();
```"
391190505,7884,hachikuji,2020-03-11T18:50:08Z,nit: no need for parenthesis
391194791,7884,hachikuji,2020-03-11T18:57:46Z,Why do we pass `writeOriginalBatch` here? Its value is always `true`.
391198263,7884,hachikuji,2020-03-11T19:04:26Z,"If we are not retaining this record, then records have been filtered, so shouldn't `recordsFiltered` be true? The original code used `writeOriginalRecord` instead of `recordsFiltered`, which seems clearer to me. Even `BatchIterationResult` still preserves the original name."
391199350,7884,hachikuji,2020-03-11T19:06:43Z,I think we can name this more specifically to its usage in filtering. Perhaps call it `BatchFilterResult` or something.
391199694,7884,hachikuji,2020-03-11T19:07:21Z,nit: fix alignment
391200796,7884,hachikuji,2020-03-11T19:09:30Z,Maybe we can call this `filterBatch`
391203869,7884,hachikuji,2020-03-11T19:15:37Z,"I guess this should take into account the magic version? If the magic version is older than v2, I think this should return false?"
391204835,7884,hachikuji,2020-03-11T19:17:28Z,Not sure why current time needs to be passed through here. Are you trying to save an extra call to `time.milliseconds()` or something?
391205039,7884,hachikuji,2020-03-11T19:17:53Z,I think `deleteRetentionMs` would be a better name since it is more general than tombstone cleanup.
391224340,7884,hachikuji,2020-03-11T19:54:15Z,I'm trying to understand why we need to collect this from `checkBatchRetention`. Why don't we collect this in `iterateOverBatch` as we do for `containsTombstones`?
391228346,7884,hachikuji,2020-03-11T19:58:33Z,We are only updating `firstTimestamp` when a record gets appended. Does that mean we cannot create an empty batch with the delete horizon set? I would expect that the constructor would initialize `firstTimestamp` to `deleteHorizonMs` if it is greater than 0.
391239196,7884,hachikuji,2020-03-11T20:10:52Z,"Note that `BatchRetention` is an enum. If there is some state that it is not sufficient to capture, then we can add a new state."
391331852,7884,ConcurrencyPractitioner,2020-03-11T23:46:09Z,"Yeah, Record Filter seemed to be the most convenient medium through which we can pass the current time. I don't want to pass in the time instance, so I just passed the time here."
391334367,7884,ConcurrencyPractitioner,2020-03-11T23:54:47Z,"Ah, perhaps I should've some comments to indicate what is going on. If you would look through the ```LogCleaner``` implementation, you would note that ```TransactionMetadata#onControlBatchRead``` must be called to determine if the control batch is empty. And the content of that call is stored in containsMarkerForEmptyTxn. Furthermore, this value is crucial for ```checkBatchRetention``` to function correctly (as it needs to know if the control batch can be removed).  Therefore, what we decided to do, is that we call onControlBatchRead at the beginning of checkBatchRetention and return it along with the BatchRetention enum (as we will need to use containsMarkerForEmptyTxn later for checking whether or not we retain individual records.)

"
391335870,7884,ConcurrencyPractitioner,2020-03-12T00:00:22Z,Acknowledged. The name is a bit contradictory with its value assignments.
391336806,7884,ConcurrencyPractitioner,2020-03-12T00:03:40Z,"@hachikuji If I am understanding this correctly, an empty batch does not contain tombstones, right? If we append a tombstone as the first record, then the delete horizon will be set. But if there isn't any tombstones, there isn't any delete horizon to set. So how would we set a delete horizon for an empty batch?"
393876656,7884,ConcurrencyPractitioner,2020-03-17T18:12:54Z,"@hachikuji  Oh, sorry about the misunderstanding. I see what you mean by that now.  You're right. I should take this into account."
393880203,7884,ConcurrencyPractitioner,2020-03-17T18:18:41Z,"Good point, version checking would be needed."
108020004,2735,hachikuji,2017-03-24T23:58:22Z,I think we should consider turning off `ParameterNumber` check if we're just going to keep increasing it.
108020604,2735,hachikuji,2017-03-25T00:07:22Z,The name seems like it could be a source of confusion. I wonder if we should rename this to something like `PidState` or `ProducerIdState` and maintain the actual transaction state separately? Do you think the coupling will be so tight that they will need to be tracked in the same class?
108021095,2735,hachikuji,2017-03-25T00:15:06Z,I think this comment is out of date.
108021197,2735,hachikuji,2017-03-25T00:16:57Z,"I wonder if we should require the string to be null or non-empty. In Guozhang's current TC patch, we treat the empty string the same as null, but maybe we shouldn't actually allow the client to send an empty string? Seems doing so would be more likely to cause problems than not."
108027736,2735,apurvam,2017-03-25T04:24:37Z,"Yes. We should not allow empty strings IMO. My transactional producer patch treats an empty transactionalId as being 'unset', and I think it makes sense to enforce that across the board."
108027961,2735,apurvam,2017-03-25T04:39:37Z,"I don't know what a good name is for this. It currently maintains the `PidAndEpoch` and the PID->Sequence number mappings. Eventually, it will also store the transactional id, whether there is an active transaction, and the partitions belonging to the currently active transaction. 

There is no real coupling between the latter transactional state and the sequence number tracking except for the `PidAndEpoch`.  

As such, if we want to separate them, then a clean separation would require 3 classes. My preference would be to keep them all together, and just call it `TransactionState` or `TransactionalState`."
108027985,2735,apurvam,2017-03-25T04:40:57Z,"I think there is still some value in keeping this check. The reason the number is being bumped here is because the Sender constructor has added an argument. The solution would be to have a builder, but then a builder doesn't make sense for the Sender. 

It may make sense to just exempt the `Sender` for this check, but I am not sure if that is possible."
108027989,2735,apurvam,2017-03-25T04:41:04Z,"I think we should modify this check and throw an `IllegalStateException` if we try to set producer state after the batch is closed, as that should never happen with the current code."
108296196,2735,hachikuji,2017-03-27T22:29:39Z,"This should either be `>` or `>= 0`. We could also move this check to the caller. Either way, we probably need a test case."
108311921,2735,apurvam,2017-03-28T00:32:34Z,This is now fixed.
108311947,2735,apurvam,2017-03-28T00:32:54Z,I have addressed this.
108320177,2735,hachikuji,2017-03-28T02:00:26Z,We seem to have lost this comment.
108558825,2735,ijuma,2017-03-28T23:06:29Z,"Hmm, are we going to remove these methods before we merge this PR?"
108558953,2735,ijuma,2017-03-28T23:07:18Z,This seems unused?
108559235,2735,apurvam,2017-03-28T23:08:55Z,"Yes.. I am about to push changes that removes these, here and in other places. "
108559389,2735,apurvam,2017-03-28T23:10:03Z,good catch. It was added during the initial implementations to generate PIDs before the server side code was ready. 
108571037,2735,junrao,2017-03-29T00:43:59Z,It doesn't seem time is being used.
108571055,2735,junrao,2017-03-29T00:44:11Z,"Log entry => record batch.

Perhaps it's clearer to say records received in the follower, instead of replication."
108571063,2735,junrao,2017-03-29T00:44:17Z,"Hmm, not sure why we need to expire the ids before the dirty offset."
108571083,2735,junrao,2017-03-29T00:44:25Z,"It seems that the snapshots are created under a dir named topic-partition. So, it seems we don't need to include ${topicPartition.topic}-${topicPartition.partition} here?"
108571107,2735,junrao,2017-03-29T00:44:31Z,Is this needed since we already did that during initialization of the class?
108571120,2735,junrao,2017-03-29T00:44:35Z,"It doesn't seem that we are returning a value. If so, we want to remove =."
108571134,2735,junrao,2017-03-29T00:44:41Z,Probably log initPidRequest too?
108571140,2735,junrao,2017-03-29T00:44:43Z,We will need to check if the request is authorized.
108577375,2735,junrao,2017-03-29T01:54:04Z,"In ReplicaFetcherThread, we probably want to log a warning if LogAppendInfo.isDuplicate is true after the append() call since it's not expected."
108577383,2735,junrao,2017-03-29T01:54:12Z,Do we need this? It seems that we already validate this in ProduceRequest.validateRecords() when the broker receives the produce request.
108577397,2735,junrao,2017-03-29T01:54:23Z,Could we make it clear that the latter epoch is the server epoch?
108577429,2735,junrao,2017-03-29T01:54:37Z,Should we do the same check for expiration when loading a snapshot?
108577433,2735,junrao,2017-03-29T01:54:41Z,What is a base name?
108577476,2735,junrao,2017-03-29T01:55:14Z,It doesn't seem that we have the logic to take snapshots periodically since this method is only called from tests?
108577487,2735,junrao,2017-03-29T01:55:18Z,remove after?
108593416,2735,apurvam,2017-03-29T05:23:32Z,"It is actually the producer epoch which is stale in this case, but will clarify the exception message."
108593532,2735,apurvam,2017-03-29T05:24:58Z,"Good catch, will add a periodic cleaner task."
108593569,2735,apurvam,2017-03-29T05:25:31Z,I added a JIRA to track this in a future PR: https://issues.apache.org/jira/browse/KAFKA-4970
108730187,2735,junrao,2017-03-29T16:57:16Z,"It would be inconvenient for a user to have to configure 2 other properties after enabling idempotence. Perhaps we could set these 2 values to a reasonable default (e.g., 3 retries) if the user doesn't configure these properties explicitly. If the user explicitly set those properties with an incorrect value, we can then throw an exception."
108730238,2735,junrao,2017-03-29T16:57:24Z,"Hmm, in sendAndAwaitInitPidRequest(), we just initiate a pid request w/o checking if we actually can send to the node. Not sure if this is safe. NetworkClient also sends internal metadata request. So, it's possible that an ongoing metadata request is still pending on the same node and the send of pid request will hit an IllegalStateException in NetworkClient.doSend()."
108746944,2735,hachikuji,2017-03-29T18:07:27Z,nit: `is True` seems redundant.
108747353,2735,hachikuji,2017-03-29T18:09:15Z,Discussed offline. We agreed it's not actually necessary to block here since the sequence number is not assigned until the batch is ready to be sent anyway.
108751964,2735,apurvam,2017-03-29T18:28:51Z,It seems to be called from `KafkaServer.scala:236`
108758172,2735,apurvam,2017-03-29T18:53:31Z,"Not sure I follow. The `ProduceRequest.validateRecords` just checks that the right message format goes with the right version. This check further validates that there is exactly one `RecordBatch` in a produce request with the new message format.

Seems to me that the checks complement each other."
108763087,2735,junrao,2017-03-29T19:15:17Z,"ProduceRequest.validateRecords() has the following, right?

```
            MutableRecordBatch entry = iterator.next();
            if (entry.magic() != RecordBatch.MAGIC_VALUE_V2)
                throw new InvalidRecordException(""Produce requests with version "" + version + "" are only allowed to "" +
                        ""contain record batches with magic version 2"");

            if (iterator.hasNext())
                throw new InvalidRecordException(""Produce requests with version "" + version + "" are only allowed to "" +
                        ""contain exactly one record batch"");

```"
108763450,2735,junrao,2017-03-29T19:16:51Z,"Apply() is being called, but is time actually being used?"
108764029,2735,apurvam,2017-03-29T19:19:39Z,You are right. Will delete this check.
108764889,2735,apurvam,2017-03-29T19:23:53Z,"Ah. So this is a reduced version of the class in the transactions branch. `time` is used in the `TransactionStateManager`, which is instantiated in the apply method."
108766424,2735,apurvam,2017-03-29T19:31:23Z,Good catch. Deleted.
108783378,2735,hachikuji,2017-03-29T20:49:21Z,This is no longer used.
108784158,2735,hachikuji,2017-03-29T20:52:32Z,"Now that we've merged the CRC32C patch, we may as use that."
108788282,2735,apurvam,2017-03-29T21:10:42Z,I think this comment is outdated. I updated with the actual logic. The snapshot files will be located inside a `pid-mapping` subdirectory of the log directory. The files themselves will be named with the pattern `$lastOffset.snapshot`.
108793143,2735,junrao,2017-03-29T21:34:12Z,"Not sure if we strictly needs to isWriteable. Currently, during append(), if the current producerBatch is full, we just create a new batch."
108822679,2735,apurvam,2017-03-30T01:09:32Z,"Actually, I undeleted these lines as they are needed. The `loadSnapshot` can be called in two cases: During initial start, and also when the log is truncated. In the latter case, if there is no previous snapshot, we need to reset to the start offset, which is what this code does."
108841803,2735,apurvam,2017-03-30T05:05:10Z,"I have implemented this. The silght modification is that since the only invalid retries config is 0, which is the default, I just override the default to 3 when idempotence is enabled. "
108842422,2735,apurvam,2017-03-30T05:14:16Z,I just deleted this block from the producer.
108844057,2735,apurvam,2017-03-30T05:35:50Z,"Hmm. You may be right. The reason I introduced `isWritable` is because we can no longer close a batch when it is full. we can only close it at the point of sending in order to set the right sequence number. So I introduced `isWritable` to denote the state where it can no longer take appends, but is not closed. 

Your point is valid: once a batch is considered full for a particular append, no future appends should go to it since there will be another batch at the tail which should get the new appends. "
108844701,2735,apurvam,2017-03-30T05:42:24Z,"So, looking over it a bit more, I now know why I introduced `isWritable`. The `isFull` method is not only used during append. It is also used to wake the sender up: if the deque is of size one and the only batch in there is not full, the sender will not be woken up to drain until it hits the linger ms. By introducing `isWritable` we can get the batch to be drained slightly quicker.

Not sure if the extra state is worth that optimization though. "
108957136,2735,junrao,2017-03-30T15:26:11Z,"Hmm, in RecordAccumulator.append(), we return the following. So if the current batch doesn't have enough space, we will create another one. Then dq.size() will be > 1 and isFull will be true anyway.

`return new RecordAppendResult(future, dq.size() > 1 || batch.isFull(), true);`"
108990172,2735,apurvam,2017-03-30T17:40:56Z,Yes. That makes sense. I will drop `isWritable`.
108999545,2735,apurvam,2017-03-30T18:18:17Z,I have added this log line.
109008262,2735,apurvam,2017-03-30T18:53:33Z,"Synced offline, and we agreed this isn't a real problem since both `getReadyNode` and `sendAndAwaitInitPidRequest` call poll. This will mean than any outstanding requests will get a chance to be processed there will be no deadlock."
109035640,2735,junrao,2017-03-30T20:57:34Z,A more reliable way to check if a user explicitly sets the config is to check from config.originals(). Ditto for configureRetries().
109035825,2735,junrao,2017-03-30T20:58:23Z,Should we add the comment for isDuplicate too?
109035966,2735,junrao,2017-03-30T20:58:57Z,Perhaps we should default to 2 to increase the chance that we can rebuild from a snapshot after truncation?
109036428,2735,junrao,2017-03-30T21:01:07Z,It seems this method  is never called?
109036442,2735,junrao,2017-03-30T21:01:10Z,"lastTimestamp => maxTimestamp? Also, probably add some comments to make it clear that firstSeq, lastSeq lastOffset and lastTimestamp refer to what's in the last appended batch?"
109036449,2735,junrao,2017-03-30T21:01:12Z,Remove () to be consistent with how we call other methods?
109045847,2735,hachikuji,2017-03-30T21:46:55Z,We should probably mention in the docs that enabling idempotence will change the default configurations for retries and in-flight requests. I think it might also be worth adding at least a `debug` level log message in the code that we are overriding the defaults. 
109046583,2735,hachikuji,2017-03-30T21:51:11Z,Do we have a test case for this?
109047045,2735,hachikuji,2017-03-30T21:53:33Z,"nit: we might want to spell out ""ProducerId"" in log messages, so there's no potential for confusion."
109047372,2735,hachikuji,2017-03-30T21:55:32Z,"Given that this class will be used to maintain transactional state as well, perhaps we should use a more explicit name. For example, `resetProducerId`, or maybe `invalideProducerId`."
109047769,2735,hachikuji,2017-03-30T21:57:49Z,We shouldn't need a placeholder for the exception unless the intention is to not print the stack trace.
109048485,2735,hachikuji,2017-03-30T22:01:48Z,Maybe we can add the PIDs to the exception message?
109048510,2735,hachikuji,2017-03-30T22:01:59Z,nit: move to previous line
109048622,2735,hachikuji,2017-03-30T22:02:28Z,nit: this could be `else if`?
109049954,2735,hachikuji,2017-03-30T22:10:34Z,"We don't have to do it here, but we should make these messages a bit more user-friendly. For example, here we should mention the fact that an old epoch means that this process is probably a zombie and another producer has taken over."
109050497,2735,hachikuji,2017-03-30T22:13:57Z,Replace log entries with record batches
109050709,2735,ijuma,2017-03-30T22:15:09Z,"I don't have all the context, but isn't `3` pretty low? We don't do exponential back-offs, so the recommendation for no data loss is typically higher."
109050957,2735,apurvam,2017-03-30T22:16:37Z,What is the recommendation for no data loss?
109051026,2735,hachikuji,2017-03-30T22:17:01Z,Can we list the possible errors in a comment like we do for other responses?
109051149,2735,hachikuji,2017-03-30T22:17:52Z,"We can assert the epoch also? Also, this is backwards: the expected value should be listed first."
109051741,2735,hachikuji,2017-03-30T22:21:48Z,"I was looking for a test case which verified that the producer PID, epoch, and sequence number are set correctly in the records included with the produce request (e.g. using a `RequestMatcher`). Do we have one?"
109051933,2735,hachikuji,2017-03-30T22:22:54Z,What are we aborting? Can you clarify in the name?
109052070,2735,hachikuji,2017-03-30T22:23:48Z,Missing a test case for `reset`?
109052173,2735,hachikuji,2017-03-30T22:24:29Z,nit: the convention we're using elsewhere is `Pid`. Same below.
109052264,2735,hachikuji,2017-03-30T22:25:01Z,nit: we could probably `import NetworkClientUtils._`
109052858,2735,hachikuji,2017-03-30T22:28:31Z,Might be a good idea to keep this private. We could add an `increment` method instead of writing to the field directly from external classes.
109053045,2735,hachikuji,2017-03-30T22:29:47Z,nit: can we make this `ProducerId Manager`?
109053126,2735,hachikuji,2017-03-30T22:30:15Z,nit: no need for the type on the left-hand side.
109054302,2735,hachikuji,2017-03-30T22:37:56Z,nit: no need for type on lhs.
109054659,2735,hachikuji,2017-03-30T22:40:34Z,nit: we can assign `batch.lastoffset - batch.baseOffset +1` to a local variable so it's easier to understand.
109054856,2735,hachikuji,2017-03-30T22:41:57Z,Why was the comment moved here?
109054973,2735,hachikuji,2017-03-30T22:42:36Z,Probably we should add something to the comment above about doing PID validation.
109055788,2735,hachikuji,2017-03-30T22:47:51Z,"Might be worth mentioning that the loop will only iterate once for a duplicate client request to be clear that the values below will not be overwritten. Would be nice if we could just break, but alas."
109055870,2735,hachikuji,2017-03-30T22:48:25Z,`warn` seems a bit high. Could this be `debug`?
109056087,2735,apurvam,2017-03-30T22:49:50Z,"Actually, this is called from `ProducerIdMappingTest.checkAndUpdate`. Not sure why the IDE doesn't find the usage, but the compiler definitely does!"
109056104,2735,apurvam,2017-03-30T22:49:57Z,Changed the name and added a comment.
109056127,2735,hachikuji,2017-03-30T22:50:08Z,Is this config part of the KIP? Do we really need it? I know we had discussed at one point just using a reasonable default.
109056174,2735,apurvam,2017-03-30T22:50:31Z,Good point. I think an `info` level log would be even more appropriate.
109056729,2735,hachikuji,2017-03-30T22:54:13Z,"Not clear what a ""valid"" entry is. Maybe the check should be inverted and phrased as `hasEntryExpired`?"
109057025,2735,hachikuji,2017-03-30T22:56:19Z,Seems this loop would be a little clearer if we just built a list of the snapshot files sorted by offset and iterated over it?
109057203,2735,hachikuji,2017-03-30T22:57:30Z,"One thing I was thinking about (for another patch) is whether we should have like a minimum number of messages before it's worth doing another snapshot. If only 5 messages have been written since the last snapshot, for example, maybe we can just skip the new snapshot."
109057493,2735,hachikuji,2017-03-30T22:59:29Z,Was this intentional?
109057570,2735,hachikuji,2017-03-30T23:00:02Z,Not sure why the variable name was changed: `error` is more accurate.
109058568,2735,hachikuji,2017-03-30T23:06:48Z,nit: you can use `fail` instead
109058879,2735,hachikuji,2017-03-30T23:09:10Z,nit: seems convention is to have spaces before and after `=`?
109062426,2735,ijuma,2017-03-30T23:38:20Z,"I don't think we should split the logic between this and `shouldRetainMessage`. For example, the latter already checks `record.isControlRecord`."
109065402,2735,apurvam,2017-03-31T00:05:33Z,Just added one.
109065473,2735,apurvam,2017-03-31T00:06:10Z,ok will update as and when I see them. Updated this one.
109066830,2735,apurvam,2017-03-31T00:20:04Z,"In this case, we want to swallow the exception and try again, since we can't do anything without a Pid when idempotence is enabled."
109067904,2735,apurvam,2017-03-31T00:32:14Z,"Hmm. The only real error codes are when there are transactions, ie. `CoordinatorNotAvailable`, `InvalidTransactionTimeout`, and `NotCoordinatorForTransactionalID`. I wonder if it makes sense to add these in this patch, or wait till we add transactions."
109070585,2735,apurvam,2017-03-31T01:03:23Z,I just added this test.
109071152,2735,apurvam,2017-03-31T01:09:52Z,I actually prefer to avoid wildcard imports.
109086998,2735,apurvam,2017-03-31T04:42:24Z,Not sure how that moved around like that.  I think it must have been a fat fingered cut/paste from last night. Moved it back now. 
109087940,2735,apurvam,2017-03-31T04:58:38Z,"I'll bump it down to info. debug seems too low, since this should happen fairly rarely."
109088180,2735,apurvam,2017-03-31T05:02:32Z,"Not sure if it would be more clear, but probably more efficient. I made the change this way so that we retain the existing logic for picking the latest snapshot less than the given offset."
109088358,2735,apurvam,2017-03-31T05:05:11Z,Nope. seems to have been there since fpj's time. I fixed it.
109088745,2735,apurvam,2017-03-31T05:10:57Z,"Hmm. It is not part of the KIP. We agreed that 2 would be a good static value, I think. Will update."
109089311,2735,apurvam,2017-03-31T05:19:20Z,"The reason to choose a positive name is that we use it to retain unexpired entries. If we choose something like `hasEntryExpired` we would have to negate the usage everywhere.

Seems like both have their tradeoffs, so I would prefer to leave it as is."
109090712,2735,apurvam,2017-03-31T05:37:14Z,Added the comment.
109090729,2735,apurvam,2017-03-31T05:37:25Z,Added the comment.
109090769,2735,apurvam,2017-03-31T05:37:42Z,Added a test case.
109090797,2735,apurvam,2017-03-31T05:38:10Z,Added a line for `isDuplicate`
109092828,2735,apurvam,2017-03-31T06:02:38Z,I consolidated all the logic into `shouldRetainMessage`.
109131783,2735,ijuma,2017-03-31T10:27:02Z,Wouldn't `awaitLeastLoadedNodeReady` be a clearer name?
109132404,2735,ijuma,2017-03-31T10:31:37Z,Nit: it's a bit nicer if we return `Long` here.
109132472,2735,ijuma,2017-03-31T10:32:05Z,"For my benefit, when we do we use `producer_id` and when do we use `pid`?"
109133247,2735,ijuma,2017-03-31T10:36:33Z,"We can avoid the `currentTimeMillis` if `timestampType` is `CREATE_TIME`. In that case, we can simply use `NO_TIMESTAMP`. The same applies for a couple of other methods."
109134063,2735,ijuma,2017-03-31T10:41:54Z,It would probably be useful to include some data in this message. Maybe the existing pid/epoch/baseSequence and the new proposed values?
109134528,2735,ijuma,2017-03-31T10:45:09Z,Please add a simple unit test in ByteUtilsTest for this.
109134743,2735,ijuma,2017-03-31T10:46:43Z,Nit: long line.
109135119,2735,ijuma,2017-03-31T10:49:23Z,"Hmm, but this same file has been changed to use wildcard imports at the top (e.g. `import org.apache.kafka.common.network._`)?"
109135874,2735,ijuma,2017-03-31T10:54:46Z,"The 5 lines above can be written as:

```scala
val data: String = zkClient.readData(path, stat)
(Option(data), stat.getVersion)
```
"
109136849,2735,ijuma,2017-03-31T11:01:42Z,"Simpler perhaps:

```scala
val topicData = getPartitionAssignmentForTopics(Seq(topic))
Some(topicData(topic).size).filter(_ > 0)
```"
109137853,2735,ijuma,2017-03-31T11:09:11Z,"Nit: it seems odd to have ""Added this test"" as a test comment. Something like ""Verify behaviour of ZkUtils.createSequentialPersistentPath since PIDManager relies on it"" seems like the expected style."
109137937,2735,ijuma,2017-03-31T11:09:45Z,Why do we have this at error level?
109139892,2735,ijuma,2017-03-31T11:22:44Z,"Great question. :) Infinite is often what is said in talks. But that may not be the right value either. I think it's worth thinking about what the retries help us recover from and how long would we want to keep retrying for. Because with the default retry backoff of 100ms, 3 retries get used pretty fast. Say that we wanted to keep retrying for 10 seconds, that would be roughly 100 retries.

The other side of the coin is: what is the cost of having a high retry number?"
109161698,2735,ijuma,2017-03-31T13:41:03Z,"Nit: I personally find comments that just repeat what the code is doing not so useful. However, if we explained why we need to reset the pid in this case, that would be pretty useful."
109162930,2735,ijuma,2017-03-31T13:46:52Z,Have we done any performance tests to see the impact of this change? The change I made to close the memory records here made a huge difference to the amount of memory used by the producer due to temporary compression buffers.
109163235,2735,ijuma,2017-03-31T13:47:58Z,Nit: this could just be `recordsBuilder`. We generally avoid the `get` prefix in Kafka.
109261455,2735,apurvam,2017-03-31T22:29:59Z,"So I thought about this a bit more. I think it still make sense to validate this on the server side. I assume that the librdkafka clients may not have the request side validation, so it would be good to have server side validation before we write to the log. 
"
109266099,2735,apurvam,2017-03-31T23:19:18Z,I added this documentation to the `TransactionState.resetProducerId` method.
109266180,2735,apurvam,2017-03-31T23:20:06Z,Was used during debugging. reverted to debug level.
109267210,2735,apurvam,2017-03-31T23:33:57Z,"I think it is a bit arbitrary. It started with pid everywhere, but then started changing gradually to producerId or producer_id."
109269845,2735,apurvam,2017-04-01T00:10:55Z,"Actually, the whole error message is out dated. We should not be calling `setProducerState` on a closed batch any more. Doing so is a bug on the client. Updated the message to indicate that."
109270153,2735,apurvam,2017-04-01T00:16:23Z,"Done.
"
109277377,2735,junrao,2017-04-01T04:26:50Z,"Similar to this, it seems the default acks=1 doesn't make sense when idempotence is enabled. This is because with acks=1, acked messages could be lost during leader change. Then, the producer will be out of sequence. Perhaps if idempotence is enabled, we should enforce acks=all."
109290030,2735,apurvam,2017-04-01T17:45:08Z,"Sounds good. Of course, we also depend on some topic level settings like: replication.factor >= 3, min.isr >= 2, unclean.leader.election=false.

But I agree, we should do what we can on the client."
109290465,2735,apurvam,2017-04-01T18:06:22Z,"I added the override for acks. Will make a note to update the KIP as well. 
"
109333086,2735,junrao,2017-04-03T02:43:38Z,entry => record
109333094,2735,junrao,2017-04-03T02:43:48Z,I thought we agreed that this check is unnecessary given the check in ProduceRequest?
109481971,2735,apurvam,2017-04-03T17:52:20Z,"We did, initially, but then I followed up on that thread (which is now impossible to find on github). Here is what I wrote: 

> So I thought about this a bit more. I think it still make sense to validate this on the server side. I assume that the librdkafka clients may not have the request side validation, so it would be good to have server side validation before we write to the log.

"
109488221,2735,junrao,2017-04-03T18:18:32Z,"Hmm, ProducerRequest.validateRecords() is called on the broker side when converting bytes from socket to a request object. So, even if librd client has an issue, the broker should still be able to capture this when constructing ProducerRequest."
36813449,130,hachikuji,2015-08-11T23:46:46Z,"Can you add some documentation for some of these interfaces?
"
37058213,130,ijuma,2015-08-14T08:32:18Z,"Since this class is used a lot, making the name short would help (as long as clarify is maintained). How about calling it `Pair`?
"
37058443,130,ijuma,2015-08-14T08:36:39Z,"This makes `compareTo` inconsistent with `equals`. Is that intentional?
"
37058480,130,ijuma,2015-08-14T08:37:14Z,"Maybe explain why?
"
37058865,130,ijuma,2015-08-14T08:43:02Z,"Wouldn't this be better as `return new HashSet<>(Arrays.asList(elems))`?
"
37059011,130,ijuma,2015-08-14T08:45:43Z,"There is already a `join` method in this class. Can you not use that?
"
37059086,130,ijuma,2015-08-14T08:47:15Z,"Rely on auto-boxing for less verbosity?
"
37146536,130,rhauch,2015-08-16T14:46:09Z,"Do you mean ""better"" to be more readable? Or more efficient? The current code is does less work than `new HashSet<>(Arrays.asList(elems))`.
"
37147003,130,rhauch,2015-08-16T15:37:38Z,"The `run()` and `close()` methods should not be synchronized. Because they are, then once `run()` is called it will block any other synchronized method, including `close()`, and because `run()` only completes when `close()` is called, `run()` will never complete. In other words, the thread will never stop.

You should be able to simply remove the `synchronized` keywords with the current code and maintain thread safety of the `running` volatile boolean field: the only method that reads that field is the private `stillRunning()` (called via private `runLoop()` which is called via public `run()`), while the only method that writes to the field is `close()`.
"
37147025,130,rhauch,2015-08-16T15:40:33Z,"This `recordsProcessed` field is never changed. If it were, it'd probably need to be made volatile, or better yet changed to be `final AtomicLong` so that operations are atomic.
"
37147089,130,rhauch,2015-08-16T15:44:05Z,"Add `lastCommit = now' as a last line in this method?
"
37169892,130,ijuma,2015-08-17T09:13:30Z,"@rhauch, I meant both. By passing the collection to the copy constructor of the `HashSet`, the initial size of the internal array is big enough to contain the elements of the collection. This avoids reallocation and rehashing. Note that `Arrays.asList` doesn't copy elements, it's just a view over the array. If this view is deemed too expensive (seems doubtful), we could keep the existing code, but then we should pass the correct sizing parameters to the `HashSet` constructor.
"
37339719,130,rhauch,2015-08-18T19:05:31Z,"The `ProducerRecord` class has a constructor that takes the partition number, yet that doesn't appear to be exposed in these two `send(...)` methods. Am I missing how to specify the partitioning logic for each of the sent messages?

UPDATE: Okay, it's pretty obvious you can set the `partitioner.class` property in the producer's configuration to the name of the `Partitioner` implementation class. Doing this makes the `KafkaProducer` pass the message key to the `Partitioner` to determine the partition number. Is this a best practice, or is it still logical for our `Processor` implementation to determine the partition, perhaps based upon something other than they key. If so, then it'd be great to have additional `send(...)` methods that take the partition number.
"
37673148,130,rhauch,2015-08-21T20:09:58Z,"The `createSensor(...)` method called on lines 68-75 uses the `this.metrics` field, and because `this.metrics` is not set until line 76 the result is a `NullPointerException`. To fix, simply move the  `this.metrics = context.metrics();` line before the first call to `createSensor(...)`.
"
37675103,130,rhauch,2015-08-21T20:32:23Z,"These two lines should get the **DE**serializer from the context:

```
        final Deserializer<K> keyDeserializer = (Deserializer<K>) context.keyDeserializer();
        final Deserializer<V> valDeserializer = (Deserializer<V>) context.valueDeserializer();
```
"
37711404,130,rhauch,2015-08-23T22:38:16Z,"This would be easier to implement if the parameter to this method were an `Iterable<Entry<K,V>>` than a `List<Entry<K,V>>`. For example, the current `RocksDBKeyValueStore` uses `byte[]` for the keys and values, and it's pretty easy to wrap that with a parameterized class that uses provided `Serializer` and Deserializer`instances for the keys and values -- except that the`putAll`method cannot be easily implemented as a delegate if it takes a`List`. (In essence, the list has to be fully-copied before the delegation can be made. 

I'd be happy to provide a patch with this fix.
"
38231414,130,guozhangwang,2015-08-28T18:47:48Z,"Ack.
"
38231825,130,guozhangwang,2015-08-28T18:51:45Z,"Does compareTo have to be consistent with equals? I though compareTo is supposed to be used as Comparable for PriorityQueue, etc while equals is for identity matching in Map, etc?
"
38232147,130,guozhangwang,2015-08-28T18:54:20Z,"Ack.
"
38232260,130,guozhangwang,2015-08-28T18:55:31Z,"Ack.
"
38232422,130,guozhangwang,2015-08-28T18:57:01Z,"Ack.
"
38232732,130,guozhangwang,2015-08-28T19:00:00Z,"Not sure if we can use auto-boxing here, since need to explicitly transform string to integer here.
"
38234637,130,guozhangwang,2015-08-28T19:21:50Z,"Ack.
"
38237786,130,ijuma,2015-08-28T19:55:28Z,"My bad, I misread.
"
38237961,130,ijuma,2015-08-28T19:56:57Z,"It's generally a good idea. The documentation for `Comparable` says:

```
 * It is strongly recommended (though not required) that natural orderings be
 * consistent with equals.  This is so because sorted sets (and sorted maps)
 * without explicit comparators behave ""strangely"" when they are used with
 * elements (or keys) whose natural ordering is inconsistent with equals.  In
 * particular, such a sorted set (or sorted map) violates the general contract
 * for set (or map), which is defined in terms of the <tt>equals</tt>
 * method.<p>
```
"
38252600,130,guozhangwang,2015-08-28T23:00:23Z,"OK makes sense, however after a second thought I feel by ""consistency"" we want:

1) if compareTo() returns none-zero, equals() should return false;
2) if equals() returns true, compareTo() should return zero.

but:

3) if equals() returns false, compareTo() does not necessarily returns none-zero.
4) if compareTo() returns zero, equals() does not necessarily returns true.

If we enforce 3) and 4) as well, it means sorted set / map will not allow two records who are comparably same to each other as the docs stated; but for our case, we actually want two  Stamped objects with the same timestamp to still be stored at the same time as keys if we ever want to do so.
"
38252639,130,guozhangwang,2015-08-28T23:01:09Z,"Ack.
"
38252646,130,guozhangwang,2015-08-28T23:01:20Z,"Ack.
"
38252729,130,guozhangwang,2015-08-28T23:02:51Z,"Ack.
"
38252996,130,guozhangwang,2015-08-28T23:07:55Z,"I think we can wrap the producer / consumer configs in the streaming / processor congis as you mentioned.
"
38253008,130,guozhangwang,2015-08-28T23:08:11Z,"Ack.
"
38589091,130,rhauch,2015-09-02T21:48:22Z,"When a `Processor` instance is started, the framework calls `init(ProcessorContext)`, but with the most recent changes it is no longer possible for a `Processor` implementation to get the configuration from the `ProcessorContext`. How can one pass in the configuration into the `Processor`? For example, my processor implementation might require several configuration properties to control or alter the default behavior. Using the same configuration sure seemed like a natural way to do this.

One option is to pass the `StreamingConfig` object into the `ProcessorDef` constructor, which would change line 94 to be something like:

```
builder.addProcessor(""PROCESS"", new MyProcessorDef(config), null, ""SOURCE"");
```

The `ProcessDef` could then pass the configuration into the constructor of the `Processor` implementation. While that works, it seems like this would then require the `TopologyBuilder` to contain objects that are dependent upon a configuration, and that might not be the same configuration passed into `new KafkaStreaming(builder, config);` (line 98 above).

It sure seems better and far simpler to instead allow `Processor.init(ProcessorContext)` access to the same configuration that KafkaStreaming has when it is initializing the processor. IOW, either change `ProcessorContext` to expose the configuration (as before), or (better yet IMO) add a second parameter to `Processor.init(...)` so it then becomes:

```
public interface Processor<K, V> {
    void init(ProcessorContext context, StreamingConfig config);
    ...
}
```

Thoughts? Am I missing something more obvious?
"
38593399,130,rhauch,2015-09-02T22:31:44Z,"Can `addSource`, `addSink`, and `addProcessor` be changed to return `TopologyBuilder` instance so that the builder's methods can be chained together. If so, then this:

```
TopologyBuilder builder = new TopologyBuilder();
builder.addSource(""SOURCE"", new StringDeserializer(), new IntegerDeserializer(), ""topic-source"");
builder.addProcessor(""PROCESS"", new MyProcessorDef(), null, ""SOURCE"");
builder.addSink(""SINK"", ""topic-sink"", new StringSerializer(), new IntegerSerializer(), ""PROCESS"");
return builder;
```

becomes:

```
return new TopologyBuilder().addSource(""SOURCE"", new StringDeserializer(), new IntegerDeserializer(), ""topic-source"")
                            .addProcessor(""PROCESS"", new MyProcessorDef(), null, ""SOURCE"")
                            .addSink(""SINK"", ""topic-sink"", new StringSerializer(), new IntegerSerializer(), ""PROCESS"");
```

Might not be useful in all situations, but in some cases it works quite beautifully. I'd be happy to submit a pull-request for this.

UPDATE: Here's the very simple PR: https://github.com/confluentinc/kafka/pull/32
"
38677576,130,guozhangwang,2015-09-03T18:11:44Z,"That is a good point, will review the PR.
"
38678884,130,guozhangwang,2015-09-03T18:21:45Z,"StreamingConfig is supposed to only include config values that are used by the KafkaStreaming runtime but not in the user logic, if users do want to modify the behavior of their processors based on some StreamingConfig values they can either do:

```
builder.addProcessor(""PROCESS"", new MyProcessorDef(config.get(CONFIG_NAME1), config.get(CONFIG_NAME2)), null, ""SOURCE"");
```

Or they can also pass-in the whole StreamingConfig object, which of course can be different from the one they passed into KafkaStreaming, into their instantiated ProcessorDef constructors, although I personally would not recommend this way. Generally I think StreamingConfig should not be exposed to the processor interface layer, since it is designed to be used only at the runtime level.
"
38683389,130,rhauch,2015-09-03T19:00:04Z,"Okay, that sounds reasonable.
"
38684688,130,rhauch,2015-09-03T19:12:28Z,"The casting done on line 53 from `ProcessorContext` to `ProcessorContextImpl` makes testing difficult, as the tests would require a `ProcessingContextImpl` rather than an alternative. (See how `KStreamTestDriver` uses a `MockProcessorContext` for an example.) I assume that `recordCollector()` was removed from `ProcessorContext` to hide it from `Processor` implementations, so this cast can either be kept and the test classes required to use a subclass of `ProcessorContextImpl`, or the `ProcessorContextImpl` implements another internal interface that `SinkNode` can use here when casting and the test classes can choose to implement. Thoughts?

(It looks like tests that use `KStreamTestDriver` do so by implementing a tail-end mock processor. I'm trying to create a similar test driver for testing a `Processor` and a `Topology`. Right now the only problem is that the `SinkNode` throws a ClassCastException on line 53.)
"
38686619,130,rhauch,2015-09-03T19:30:48Z,"Another option might be to break the current `ProcessorContextImpl` into two classes: one that holds the objects that all impls would need (e.g., the (de)serializers, the `RecordCollector`, the `StreamingConfig`, the `ArrayDeque<ProcessorNode>`, and maybe the `Metrics`), and a subclass that adds `StreamTask` and `ProcessorStateManager`. The `SinkNode` could cast to the base impl, and the test drivers could have impls that extend the base impl. If this sounds interesting, let me know and I'll create a PR for easier evaluation and comparison.
"
38795321,130,rhauch,2015-09-04T21:21:44Z,"The fact that the `ProcessorContextImpl` class is creating its own Consumer turns out to be a fairly significant problem for test cases, especially those that directly use `StreamTask`. I'd love to introduce a `ConsumerSupplier` (in Java 8 it'd simply be `Supplier<Consumer>`, but alas) and pass an implementation into this constructor and actually into the `StreamTask` constructor. This would then move the creation of this `ConsumerSupplier` into the `StreamThread`, which is already creating the `KafkaProducer` and `KafkaConsumer` instances used to consume records to pass to the `StreamTask`. And if this is acceptable, is it still desirable to use a _separate_ `KafkaConsumer` instance for the `ProcessorStateManager`?
"
38795471,130,rhauch,2015-09-04T21:24:00Z,"I've been able to work around this problem by directly using `StreamTask`, which internally creates its `ProcessorContextImpl` instance. However, the only roadblock I have is that the `ProcessorContextImpl` is explicitly creating a `KafkaConsumes`. See the details in [this comment](https://github.com/apache/kafka/pull/130/#discussion_r38795321).
"
38967999,130,guozhangwang,2015-09-08T19:20:06Z,"Yeah I agree this is kinda awkward.

The reason we need a separate consumer for restoring state is that the other consumer is 1) created for the thread and shared among its tasks, and 2) its subscribed topics is determined by the topology statically. While for local state the topic name is defined dynamically and the restoration is only one-time: once it is done you do not need to keep subscribing to it anymore.

I think one thing we can do here is to move the creation of the restoration consumer into the processor-state-manager, and set a flag into the processor-state- manager's constructor indicating whether we need to create this consumer (set to false for unit tests, for example).
"
38968103,130,guozhangwang,2015-09-08T19:21:12Z,"BTW are you working on adding some unit test classes? Since I am also working on some of them, would like to avoid any duplicate work or conflicts :)
"
38974963,130,rhauch,2015-09-08T20:22:45Z,"Actually, I've written a `ProcessingTopologyTestDriver` class under `src/test` that takes a TopologyBuilder and will make it very easy for projects that use Kafka Streams to test their topologies with unit tests that do not use a real Kafka. Each test method can set up the driver, pass one or more methods to the driver (which then forwards them to the appropriate source), and finally check the messages output by the sinks. It's pretty simple, and it uses mock consumers and producers along with a single `StreamTask` to do all the heavy lifting. (This has the benefit of also testing the bulk of the `StreamTask` implementation.) 

So, it'd be great if this test driver could pass the consumer for state manager into the `StreamTask` constructor took a consumer (or consumer supplier) so that the tests can inject mocks instead; the `StreamTask` constructor already takes a consumer and producer, so taking a second consumer for state management seems consistent. Right now `StreamTask` is the only thing that constructs a `ProcessingContextImpl`, so passing the consumer down also seem reasonable. This approach also seems to have minimal impact on other code, and IMO is better than passing a flag into the processor state manager's constructor or calling a method on the processor state manager, since right now the the `ProcessorStateManager` is constructed within the `ProcessingContextImpl` class which itself is constructed within the `StreamTask` constructor.

BTW, the test driver class and a unit test is ready for a PR, except that the tests that use state management are failing right now because it's trying to create a real consumer for state management. I'll go ahead and make the aforementioned change to pass in the consumer, fix my tests, and submit a PR for review first thing tomorrow.
"
39053621,130,rhauch,2015-09-09T15:00:22Z,"The PR is now available: https://github.com/confluentinc/kafka/pull/37
"
39055740,130,rhauch,2015-09-09T15:16:23Z,"@guozhangwang, how should I proceed with new unit tests. Any suggestions so we don't duplicate effort? Is there a better way to coordinate other than comments in this PR?
"
39055851,130,rhauch,2015-09-09T15:17:13Z,"Added a PR with the simple correction: https://github.com/confluentinc/kafka/pull/38
"
39056201,130,rhauch,2015-09-09T15:19:40Z,"With this configuration of `stream`, the test JARs are not build and uploaded to Maven. I created a PR (https://github.com/confluentinc/kafka/pull/34) that corrects this so that the test JARs are created and uploaded similarly to how `client` does it.
"
39059471,130,eribeiro,2015-09-09T15:44:26Z,"See, here you should evaluate what is the semantic that `paused` should have. If we want to return a **snapshot** of the paused `TopicPartition` then it's better to do:

```
return Collections.unmodifiableSet(new HashSet<>(paused));
```

If we want to return a **dynamic** view of the `paused` then it's better to use:

```
return Collections.unmodifiableSet(paused);
```

In either case, we should return an unmodifiable view of the set, because it's not very nice to expose a mutable field directly to callers as above.
"
39059935,130,eribeiro,2015-09-09T15:47:34Z,"nit: rename this method to `remove` or `delete`
"
39060300,130,eribeiro,2015-09-09T15:50:39Z,"""Returns an empty collection if this list is empty or null""
"
39060354,130,eribeiro,2015-09-09T15:51:00Z,"It is best practice to use `Collections.emptyList()` instead of `Collections.EMPTY_LIST`

Also, would make a difference to return `Collections.emptyList()` if the `other` is empty? I mean, like this:

`
return other == null || other.isEmpty() ? Collections.emptyList() : other;
`
"
39061914,130,eribeiro,2015-09-09T16:02:27Z,"private **final**?
"
39062322,130,eribeiro,2015-09-09T16:05:23Z,"Why not use `((Long) key);` or even `((Integer) key)`. No need to call `longValue()` as the autoboxing is called automatically, afaik. 
"
39062772,130,eribeiro,2015-09-09T16:08:56Z,"nit: can remove this blank line.
"
39062866,130,eribeiro,2015-09-09T16:09:42Z,"It's nice to expose those as:
``public final List<K> keys = new ArrayList<>()`

same for line 35
"
39062965,130,eribeiro,2015-09-09T16:10:20Z,"It's nice to expose those as:
`public final List<String> processed = new ArrayList<>()

same for line 29
"
39063280,130,eribeiro,2015-09-09T16:12:57Z,"Paraphrasing Joshua Bloch, prefer Collections to arrays so I would use:

```
private final List<StreamThread> threads;
```
"
39063561,130,eribeiro,2015-09-09T16:15:18Z,"Better to make `state` volatile too.
"
39063741,130,eribeiro,2015-09-09T16:16:51Z,"Still in this mood, this method could be rewritten as:

```
int size = config.getInt(StreamingConfig.NUM_STREAM_THREADS_CONFIG);
threads = new ArrayList<>(size);
for (int i = 0; i < size; i++) {
     threads.append(new StreamThread(builder, config));
}
```
"
39064142,130,eribeiro,2015-09-09T16:20:17Z,"If `state` is different from `CREATED` then lines L#93 and L#94 are unreachable. Therefore, why not move them to end of the `if` block?

```
if (state == CREATED) {
      for (StreamThread thread : threads)
           thread.start();
      state = RUNNING
      log.info(""Started Kafka Stream process"");
}
```
"
39064231,130,eribeiro,2015-09-09T16:20:52Z,"Same as above: lines 121 and 123 can be moved to inside the if block.
"
39065910,130,eribeiro,2015-09-09T16:35:20Z,"nit: `private List<K> list = new LinkedList<K>();`
"
39067165,130,eribeiro,2015-09-09T16:47:24Z,"As all the methods synchronize on the whole body why not make all the methods `synchronized`, like:

`public synchronized void close() {` ?
"
39067236,130,eribeiro,2015-09-09T16:48:12Z,"formatting: break line as:

```
if (elem == null) 
   return null;
```
"
39067357,130,eribeiro,2015-09-09T16:49:14Z,"nit: prefer interfaces on field declaration as: 

`private final Deque<ProcessorNode> nodeStack = new ArrayDeque<ProcessorNode>();`
"
39067577,130,eribeiro,2015-09-09T16:51:20Z,"tip: you can rewrite as `final int[] expectedKeys = {1, 10, 100, 1000};`
"
39067625,130,eribeiro,2015-09-09T16:51:56Z,"tip: you can rewrite as final `String[] expected = {""1:1"", ""10:2"", ""100:3"", ""1000:4""};`
"
39067732,130,eribeiro,2015-09-09T16:52:53Z,"tip: you can rewrite as `String[] expected = {""0:v0"", ""0:V0"", ""1:v1"", ""1:V1"", ""2:v2"", ""2:V2"", ""3:v3"", ""3:V3""};`
"
39067794,130,eribeiro,2015-09-09T16:53:25Z,"tip: you can rewrite as `final int[] expectedKeys = {0, 1, 2, 3};`
"
39089821,130,guozhangwang,2015-09-09T20:11:31Z,"@rhauch I am working on calling for a review round and push the first patch to OS now. Once that is done further PRs can be submitted directly to apache/kafka. Could you hold the current changes and rebased them to the apache trunk once this patch is checked in?
"
39090342,130,guozhangwang,2015-09-09T20:16:00Z,"Ack.
"
39090441,130,guozhangwang,2015-09-09T20:16:43Z,"Ack.
"
39090503,130,guozhangwang,2015-09-09T20:17:14Z,"Ack.
"
39090892,130,guozhangwang,2015-09-09T20:20:34Z,"Good point, changed to `Collections.emptyList()`.

The semantics is only to return an empty list if `other` is null, if it is empty then we will still return itself.
"
39090991,130,guozhangwang,2015-09-09T20:21:28Z,"ack.
"
39091129,130,guozhangwang,2015-09-09T20:22:51Z,"The value Integer cannot be cast directly to long.
"
39091209,130,guozhangwang,2015-09-09T20:23:27Z,"ack.
"
39091297,130,guozhangwang,2015-09-09T20:24:14Z,"ack.
"
39091428,130,guozhangwang,2015-09-09T20:25:18Z,"ack.
"
39092030,130,guozhangwang,2015-09-09T20:30:19Z,"ack.
"
39092290,130,guozhangwang,2015-09-09T20:32:35Z,"Since the start() / close() are synchronized the states will not be accessed concurrently. So I think it is not necessary?
"
39092328,130,guozhangwang,2015-09-09T20:32:54Z,"ack.
"
39092500,130,guozhangwang,2015-09-09T20:34:28Z,"ack.
"
39092546,130,guozhangwang,2015-09-09T20:34:53Z,"ack.
"
39092616,130,guozhangwang,2015-09-09T20:35:33Z,"We used LinkedList's offerLast / etc functions later so we have to declare it as LinkedList.
"
39092736,130,rhauch,2015-09-09T20:36:21Z,"It should be volatile so that different threads see the actual value at the same time.
"
39098298,130,eribeiro,2015-09-09T21:22:58Z,"Oh, OK. Excuse me for overlooking this. You right.
"
39197241,130,eribeiro,2015-09-10T18:40:16Z,"I would suggest to use a more modern approach that is replace int constants by enums. Therefore, it becomes:

```
public enum State {
   CREATED, RUNNING, STOPPED
}

private volatile State state = State.CREATED;
```
"
39197493,130,eribeiro,2015-09-10T18:42:27Z,"nit: a nifty trick to use here (reduces the scope of `iter` would be):

```
for (KeyValueIterator<String, Integer> iter = kvStore.all(); iter.hasNext(); ) {
      Entry<String, Integer> entry = iter.next();
      (...)
}
```
"
39198066,130,eribeiro,2015-09-10T18:47:38Z,"Using lock objects is considered a old fashioned approach. Better to use a ReentrantLock as below:

```
private final Lock lock = new ReentrantLock();

(...)

public void foo() {
   lock.lock();
   try {
          // do your computation
   } finally {
        lock.unlock();
   }
}
```
"
39198676,130,eribeiro,2015-09-10T18:52:36Z,"nit: maybe name this method as `of`, like `KeyValue.of(10, ""Hello"")`
"
39198875,130,eribeiro,2015-09-10T18:54:24Z,"This line and line below can be : `public final List<String> <field-name> = new ArrayList<>();`, as well as line below.
"
39199065,130,eribeiro,2015-09-10T18:56:02Z,"lines 34-36 can be simplified as:

`return timestamp - otherTimestamp;`

a nifty trick ;) 
"
39199114,130,ijuma,2015-09-10T18:56:33Z,"This is not really true. There are advantages and disadvantages when it comes to choosing between `synchronized` and `ReentrantLock`. For low contention cases, `synchronized` tends to do better, in fact.
"
39199275,130,eribeiro,2015-09-10T18:58:03Z,"It's usually advisable to 
1) make pq final; or
2) create a lock field or
3) synchronize the whole method 

only don't synchronize on a **non final** field.
"
39199569,130,eribeiro,2015-09-10T19:00:44Z,"Yeah, you right. My fault. For low contention synchronized is better.
"
39212009,130,eribeiro,2015-09-10T20:51:24Z,"I feel maybe we need a method to check the status of this class, that is: 

```
public State getState() {
   return State;
}

OR 

public boolean isRunning() {
   return state == State.RUNNING;
}
```

WDYT?
"
39214847,130,eribeiro,2015-09-10T21:15:09Z,"typo: 'coresponds' should be `correponds`
"
39215090,130,eribeiro,2015-09-10T21:17:13Z,"nit: **I** would name this method as `nullToEmpty` to let it clear what it does, but up to you. :)
"
39215276,130,eribeiro,2015-09-10T21:18:57Z,"nit: **I** would name this method `newSet`and make it return `Set` instead of a `HashSet`, but, again, up to you. :) 

ps: btw, once Guava is incorporated into Kafka project, the `Sets` helper class has methods to replace this one. ;)
"
39228036,130,eribeiro,2015-09-10T23:42:05Z,"as above
"
39228037,130,eribeiro,2015-09-10T23:42:05Z,"declare as `public final List<K> keys = new ArrayList<>();`, as well as line below
"
39228134,130,eribeiro,2015-09-10T23:43:18Z,"I suppose this is a leftover, right? 
"
39228245,130,eribeiro,2015-09-10T23:45:08Z,"Declare as `private final Deque<ProcessorNode> nodeStack = new ArrayDeque<>();`
"
39228751,130,eribeiro,2015-09-10T23:52:50Z,"That Kafka convention:

`
if (condition)
   statement
`

On the following line too.
"
39228814,130,eribeiro,2015-09-10T23:53:34Z,"**final** for `keySerializer` and `valSerializer`?
"
39228849,130,eribeiro,2015-09-10T23:54:01Z,"Define as `private final Deque<StampedRecord> fifoQueue;`
"
39471570,130,eribeiro,2015-09-15T03:33:29Z,"Hey, why don't we return a empty `Iterator` here? You can do this with `Collections.<V>emptyList().iterator()`. Returning null is usually a code smell. wdyt?
"
39472217,130,eribeiro,2015-09-15T03:50:47Z,"can be simplified as `for (int i : expectedKeys) {` 
"
39472298,130,eribeiro,2015-09-15T03:53:05Z,"format:

```
if (exception != null)
   throw exception;
```
"
39472324,130,eribeiro,2015-09-15T03:53:37Z,"formatting: no need for curly braces here
"
39472333,130,eribeiro,2015-09-15T03:53:50Z,"format: no need for curly braces
"
39472370,130,eribeiro,2015-09-15T03:54:44Z,"lines L#53 to L#55 can be simplified as:

```
return time1 - time2;
```
"
39472425,130,eribeiro,2015-09-15T03:55:57Z,"I like to rewrite those if-else condition as: `return (stamped == null) ? lastKnownTime : stamped.timestamp`, but up to you.
"
39472674,130,eribeiro,2015-09-15T04:02:25Z,"formatting: no need for curly braces
"
39472691,130,eribeiro,2015-09-15T04:02:50Z,"formatting: no need for curly braces.
"
39472770,130,eribeiro,2015-09-15T04:05:04Z,"That **snapshot** vs **dynamic view** intention: what do we want here? If it's the current snapshot then it's
`return new HashSet<>(partitionQueues.keySet());`
"
39472806,130,eribeiro,2015-09-15T04:06:00Z,"formatting: no need to curly braces
"
39472985,130,eribeiro,2015-09-15T04:10:55Z,"Doesn't it need a `new HashSet<>(sourceByTopics.keySet())` to return a snapshot as line 46?
"
39473139,130,eribeiro,2015-09-15T04:14:03Z,"`List<String> result = new ArrayList<>();`
"
39473154,130,eribeiro,2015-09-15T04:14:24Z,"`List<String> expected = new ArrayList<>();`
"
39473162,130,eribeiro,2015-09-15T04:14:41Z,"`List<String> expected = new ArrayList<>();`
"
39473198,130,eribeiro,2015-09-15T04:15:47Z,"Can be simplified as `for (int i : expectedKeys) {`
"
39679109,130,rhauch,2015-09-16T20:15:28Z,"It does not appear that `KafkaConsumer` (or rather the `SubscriptionState` class it uses) allows using both `subscribe(...)` and `assign(...)`. Given that line 133 was recently changed to `assign`, then shouldn't line 167 be changed as well to:

```
restoreConsumer.assign(Collections.<TopicPartition>emptyList());
```
"
39904068,130,guozhangwang,2015-09-18T21:41:29Z,"Ack.
"
39904149,130,guozhangwang,2015-09-18T21:42:37Z,"Ack.
"
39904467,130,guozhangwang,2015-09-18T21:46:39Z,"Ack.
"
39904581,130,guozhangwang,2015-09-18T21:47:56Z,"The topology's immutable, such that the processorNodes and sourceByTopics will not be modified.
"
39904795,130,guozhangwang,2015-09-18T21:50:39Z,"Yeah, it will be replaced with the unsubscribe() API introduced in https://github.com/apache/kafka/pull/128
"
39912569,130,onurkaraman,2015-09-19T00:12:25Z,"So I have very little context in this KIP. Is there any sort of expected size for the inner iterator? You may get stuck in the constructor's call to findNext() for a long time. It might be less surprising to postpone calling findNext() in hasNext() and next().
"
39916442,130,rhauch,2015-09-19T05:09:32Z,"Sounds good. The proposed `unsubscribe` will clear the state set by `assign` and `subscribe` methods, so that will address my concern. Thanks!
"
40004856,130,guozhangwang,2015-09-21T18:12:02Z,"The iterator will be constructed at around the same time when it is used to get next() (usually in process() call), so I feel postponing findNext() would not change much. But if we encountered some cases that this did become an surprise for users we can change it then.
"
40112156,130,onurkaraman,2015-09-22T16:58:21Z,"Minor, but this can alternately be called filterNot just like in scala's collections.
"
40122437,130,ijuma,2015-09-22T18:14:01Z,"We should use `$junit` here.
"
40123138,130,ijuma,2015-09-22T18:19:27Z,"There are no plans to incorporate Guava by the way.
"
40124198,130,ijuma,2015-09-22T18:26:12Z,"We may not care in this case, but nio.2 in Java 7 provides a mechanism to do this in a more efficient way: http://www.deepakgaikwad.net/index.php/2013/08/12/recursive-delete-directory-contents-java-7-nio.html
"
40125291,130,ijuma,2015-09-22T18:33:44Z,"What is the reasoning for having different prefixes in `KafkaStreaming`, `StreamingConfig` and `KStream`?
"
40126053,130,ijuma,2015-09-22T18:39:10Z,"Also, is it actually useful enough? Particularly if there is a `negate` method on `Predicate`. In Scala, one benefit of `filterNot` is when using the underscore notation to make lambdas more concise. In Java, that seems less useful.
"
40126098,130,ijuma,2015-09-22T18:39:28Z,"`valuesa` -> `values`
"
40127893,130,ijuma,2015-09-22T18:51:40Z,"Is this actually needed?
"
40128094,130,ijuma,2015-09-22T18:53:14Z,"Some of this are private while others are public, is there a reason for that?
"
40128168,130,ijuma,2015-09-22T18:53:46Z,"Is there a reason why these are not final?
"
40128440,130,ijuma,2015-09-22T18:55:36Z,"We should probably have a method that takes the name as a parameter and returns the new name using `INDEX`. Less error-prone and easier to change in the future.
"
40128852,130,ijuma,2015-09-22T18:58:43Z,"The suggestion to have a method that does this concat becomes even more appealing when seen in the light of subclasses like this one.
"
40128890,130,ijuma,2015-09-22T18:59:07Z,"Also, have we considered using an enum instead of strings?
"
40128968,130,rhauch,2015-09-22T18:59:38Z,"Currently, since Kafka can't use Java 8 functions nor static methods on interfaces, there is a `Predicate` interface in this package that is semantically equivalent to `java.util.function.BiPredicate`. Unfortunately, without static methods, it's difficult to create a `not( Predicate...)` method. So using `filterOut` seems to be the easiest approach.
"
40129636,130,rhauch,2015-09-22T19:04:42Z,"If the `apply` method were changed to `test`, then when Kafka moves to Java 8 this interface could extend `java.util.function.BiPredicate<K,V>`. That wouldn't really benefit anyone that simply supplied their own lambdas, but it might make it a bit easier or more obvious that existing `BiPredicate` implementations could be passed in. Regardless, aligning with Java 8 might be a good thing in and of itself.
"
40129686,130,ijuma,2015-09-22T19:05:10Z,"I was thinking of adding a default method to `Predicate` itself, but I now realise that we can't do that in Java 7 without changing it to an abstract class. Fair enough.
"
40149196,130,guozhangwang,2015-09-22T21:50:45Z,"ack.
"
40149364,130,guozhangwang,2015-09-22T21:52:36Z,"KafkaStreaming / StreamingConfig is aligned with KafkaProducer (KafkaConsumer) / ProducerConfig (ConsumerConfig), and KStream is just a name of the higher-level DSL API.
"
40149430,130,guozhangwang,2015-09-22T21:53:07Z,"ack.
"
40149490,130,guozhangwang,2015-09-22T21:53:49Z,"ack.
"
40149757,130,guozhangwang,2015-09-22T21:56:22Z,"ack.
"
40149780,130,guozhangwang,2015-09-22T21:56:35Z,"Some of them are used outside KStream, for example Source is used in KStreamBuilder, and Join is in KStreamWindowed, etc.
"
40150351,130,guozhangwang,2015-09-22T22:02:45Z,"Not sure I got the motivation: when we moved to Java 8 and removed this interface with BiPredicate directly, even with test() existing users still need to make code change, right?
"
40167379,130,rhauch,2015-09-23T02:55:58Z,"Renaming `apply` to `test` now gives us a slow, non-breaking migration path so that when moving to Java 8 we could:
- change `Predicate<K,V>` to extend `java.util.function.BiPredicate<K,V>` and remove the `apply` method from `Predicate<K,V>` since it would unnecessarily override `BiPredicate.test(...)`;
- change `KStream` methods to use `BiPredicate<K,V>` instead of this `Predicate<K,V>`;
- deprecate `Predicate<K,V>` and tell people to use `BiPredicate` instead.

No users would have to change their code, although anyone using `Predicate<K,V>` directly would get deprecation warnings and could but would not be required to change. In a later release, we can remove `Predicate<K,V>`, and anyone _still_ using it would then have to change their code.

OTOH, if we keep `apply` as it is now, then when we moved to Java 8 and change to  `java.util.function.BiPredicate<K,V>`, all users directly using the interface would _have_ to change.

Of course, neither of these options affects those users who are already on Java 8 and using lambdas rather than `Predicate<K,V>` implementation classes.
"
40215978,130,rhauch,2015-09-23T15:17:20Z,"@guozhangwang, why is the `RocksDBKeyValueStore` not parameterized like `InMemoryKeyValueStore`? I have a version of this that is parameterized (yes, it does require passing in key and value serializers and deserializers), and it makes using it very similar to `InMemoryKeyValueStore`. Any interest in it?
"
40237923,130,guozhangwang,2015-09-23T18:17:30Z,"Agree, do you want to create a new PR?
"
40239203,130,rhauch,2015-09-23T18:27:47Z,"@guozhangwang, yes I will create a new PR shortly.
"
40239945,130,ymatsuda,2015-09-23T18:32:40Z,"I am not sure if we want to migrate to `java.util.function.BiFunction` at this point. But, by this change, we can maintain the source level compatibility of user code if we want to migrate. A good idea. What happens to the binary level compatibility? Does a user using Predicate have to recompile?
"
40240252,130,guozhangwang,2015-09-23T18:35:25Z,"I think recompilation are not avoidable anyways..
"
40240731,130,rhauch,2015-09-23T18:39:28Z,"Inserting new class or interface types in the type hierarchy is listed as a [binary-compatible change in Java SE7](https://docs.oracle.com/javase/specs/jls/se7/html/jls-13.html) and [Java SE8](https://docs.oracle.com/javase/specs/jls/se8/html/jls-13.html). So if we rename the method now, then if/when `Predicate<K,V>` is changed to extend `java.util.function.BiPredicate<K,V>` and released clients using `Predicate<K,V>` should not have to recompile.
"
40243229,130,guozhangwang,2015-09-23T19:00:05Z,"Correct: http://wiki.eclipse.org/Evolving_Java-based_APIs_2
"
131572299,3621,becketqin,2017-08-07T04:52:30Z,"Not sure if we want to have this interface added while we have not supported the intra broker replica move yet. Also, we probably don't want to expose the implementation detail to the users about how the replicas are moved . So it would be better to not ask users to manually specify the replica dir before they do the partition reassignment.

So the end state of AdminClient after KIP-179 and KIP-113 would be having two methods for partition movement:
1. A method for partition reassignment in general, include both inter and intra broker reassignment. (e.g. `alterTopics()`)
2. A method to only move replicas within a broker. (e.g. `alterReplicaDir`)

BTW, I still feel that having (1) is sufficient. It would be useful to explore possibility of only having (1) so users don't need to deal with two different interfaces for replica movement.

With that, regarding this patch, I was thinking doing the following:
1. Let `ReassignPartitionCommand` take new input format which includes the log dir.
2. Do a sanity check in the `ReassignParititionCommand` to ensure no intra broker replica movement is specified. Otherwise throw `UnsupportedOperationException`.
3. In `ReassignPartitionCommand`, it should just send `ChangeReplicaDirRequest` to the related brokers before the partition movement.
4. Do not add `alterReplicaDir()` to the AdminClient until the broker supports that.

Would the above way be sufficient for what we want to achieve for this patch?"
131573994,3621,lindong28,2017-08-07T05:17:37Z,"@becketqin Thanks for your comment! Here is what I think:

- I think the interface is useful. The AlterReplicaDirRequest (renamed from ChangeReplicaDirRequest per Ismael's comment) is used to 1) create replica in the specified log directory later and 2) move replica to the specified log directory if it has already been created yet. This patch supports the first part and thus the new API `alterReplicaDir` in AdminClient is useful and well-defined. If we don't add this method and its implementation in AdminClient, we would have to implement it in `ReassingPartitionCommand` and move the implementation to AdminClient later, which seems like unnecessary work.

- Can you clarify a bit what implementation detail is exposed to user in this patch?

- I have thought about the possibility of using only one method in AdminClient to do both partition reassignment and replica -> log directory reassignment. My conclusion is that it is still better to put them into two separate methods. For example, if we do both using one method, the method would probably look like `reassignPartition(Map<TopicPartition, List<Int>> newPartitionAssignment, Map<TopicPartitionReplica, String> newReplicaDirAssignment)`. This method merges two inherently different parameters and functionality into one method, which seems less clean than using two methods. Do you any suggestion on how that one method would look like if we were to use only one method for this?

- I agree with the tasks 1-3. I still think it is better to put `alterReplicaDir` in AdmClient as long as we documents it properly. But it also works for me to move this code to `ReassignPartitionCommand`.

"
131574245,3621,lindong28,2017-08-07T05:21:42Z,And good point about checking that no intra broker replica movement is specified. I will add this logic in `ReassignPartitionCommand`
131777843,3621,lindong28,2017-08-07T22:18:18Z,Discussed with @becketqin offline. I will not add `alterReplicaDir()` API in AdminClient in this patch. I will add `alterReplicaDir()` as a public method in KafkaAdminClient so that this method can be used by ReassignPartitionsCommand in this patch.
132356130,3621,becketqin,2017-08-10T03:18:39Z,We have agreed the next major version would be 1.0.0.
132356186,3621,becketqin,2017-08-10T03:19:25Z,Should this be `describeLogDirs`? Also why this method is abstract?
132356414,3621,becketqin,2017-08-10T03:22:35Z,"Not sure if it is worth having this query granularity. It seems that when we query a broker, the cost to query all log directories and query some particular log directories are pretty much the same. And I think typically users would have to query all log dirs in order to get the dir names first. If so, would it be simpler to just use `Collection<Integer>` instead?"
132359600,3621,becketqin,2017-08-10T04:03:56Z,It seems a little verbose to have all these options with a timeout. It seems cleaner to merge them to a `TimeoutOption` and extends from that.
132359818,3621,becketqin,2017-08-10T04:06:38Z,This method does not exist.
132359891,3621,becketqin,2017-08-10T04:07:13Z,This method does not exist.
132832597,3621,becketqin,2017-08-13T05:19:26Z,This comment seems a little too verbose. Do we need to mention this or the API of `AlterReplicaDirResult` is already clear enough?
132832605,3621,becketqin,2017-08-13T05:19:52Z,1.0.0
132832726,3621,becketqin,2017-08-13T05:32:05Z,"Hmm, in which case would the future be null?"
132832899,3621,becketqin,2017-08-13T05:50:43Z,LogDirNotAvailableException?
132833418,3621,lindong28,2017-08-13T06:36:04Z,Good point. I have changed it to take `Collection<Integer>`.
132833428,3621,lindong28,2017-08-13T06:36:38Z,Thanks. I have changed it to 1.0.0.
132833468,3621,lindong28,2017-08-13T06:40:22Z,"I think `LogDir` may be a bit more verbose than Dir. It seems OK to be use `dir` instead of `logDir` as long as logDir will be the only ""dir"". If it is necessary to use `dir` instead of `logDir`, do you think we should rename `DescribeDirsRequest` to `DescribeLogDirsRequest`?"
132833487,3621,lindong28,2017-08-13T06:41:42Z,"Besides, this is an abstract method because its implementation is in AdminClient. This follows the same pattern as existing APIs in `AdminClient`."
132833512,3621,lindong28,2017-08-13T06:42:55Z,"Thanks. I replaced it with `{@link KafkaAdminClient#alterReplicaDir(Map, AlterReplicaDirOptions)}`"
132833528,3621,lindong28,2017-08-13T06:44:07Z,"Thanks. I replaced it with `{@link KafkaAdminClient#alterReplicaDir(Map, AlterReplicaDirOptions)}`"
132833575,3621,lindong28,2017-08-13T06:48:34Z,Sure. I removed the sentence `Updates are not transactional...` from this comment. Does this address the problem?
132833579,3621,lindong28,2017-08-13T06:48:52Z,Sure. Fixed now.
132833626,3621,lindong28,2017-08-13T06:51:20Z,"This follows the same pattern of the implementation of other existing APIs in KafkaAdminClient. I don't think future will be null unless there is bug. I think it is OK for the KafkaAdminClient to have extra check to protect itself form server side's bug. But if you don't like it, I can also remove it and in the worse case we just see NPE if future is null."
132833666,3621,lindong28,2017-08-13T06:54:21Z,Sure. Renamed to `LogDirNotAvailableException`.
132833688,3621,lindong28,2017-08-13T06:55:55Z,"And if we were to rename it to `describeLogDirs`, should we also rename `describeReplicaDir` to `describeReplicaLogDirs`?"
132833755,3621,lindong28,2017-08-13T07:00:42Z,"How about this: I added an abstract class named `ConfigOptions`. This class will have a timeMs variable and `Integer timeoutMs()` API. This abstract class can be used to hold all future APIs that are shared among all (or most) ""*ConfigOptions"" classes.

On the other hand I also think it is OK to keep it as is given that there are is only one common API (i.e. timeMs()) shared among those classes."
132857316,3621,becketqin,2017-08-13T23:56:05Z,"If we do not expect this to happen. Shouldn't we throwI IllegalStateException? In this case, if the broker returned a replica that is not in the request, the broker may have somehow misplaced a replica. We should probably alert in this case."
132868453,3621,becketqin,2017-08-14T03:29:15Z,"Personally I prefer avoiding the potential confusion, especially given dir is a pretty commonly used name in many places. I am not sure if in the future we will have some other dirs, but describeDir() itself seem lacking some necessary context to me. And yes, I do feel `DescribeLogDirRequest` is a better name. `describeReplicaDir` sounds OK though, as ReplicaDir must be a log dir."
132868590,3621,becketqin,2017-08-14T03:31:37Z,I am not sure what is the best solution here either. It just feels a little silly that we have all those classes that are essentially identical except the class name.
132868863,3621,becketqin,2017-08-14T03:36:25Z,Should this comment be updated?
132868995,3621,becketqin,2017-08-14T03:38:36Z,ditto above.
132869443,3621,becketqin,2017-08-14T03:47:23Z,"Technically speaking we are query the log directory in which the replica locates. i.e. the ROOT_LOG_DIR, not ROOT_LOG_DIR/replica_dir. Speaking of this, it might still be better to change the method name to `describeReplicaLogDir`."
132871530,3621,becketqin,2017-08-14T04:24:17Z,Is the left parenthesis missing?
132872895,3621,becketqin,2017-08-14T04:42:22Z,The default lag should probably be something like -1 and preferably a macro. It is also a little weird to allow those non-final fields to be tweaked. See the other comment in `KafkaAdminClient`
132873131,3621,becketqin,2017-08-14T04:44:52Z,Would it be better to construct the result after the response is returned instead of create the result beforehand and modify it later?
132873593,3621,becketqin,2017-08-14T04:53:18Z,"Given this change, is the log dir field still needed in the `DescribeDirRequest`? It seems that we can just leave the replica list there and remove the log dir list?"
132874067,3621,becketqin,2017-08-14T04:59:49Z,"Not found and not available seems slightly different. It would be useful to distinguish between ""exist but not available"" VS ""does not exist""."
132874436,3621,becketqin,2017-08-14T05:06:23Z,"By ""remaining"" do you mean ""available""?"
132874540,3621,becketqin,2017-08-14T05:08:34Z,"See the other comment, maybe we don't need the log dir list anymore."
132876992,3621,becketqin,2017-08-14T05:44:15Z,"Hmm, why are we do the validation after executing the reassignment?"
132878069,3621,becketqin,2017-08-14T05:59:02Z,"The comment is a little confusing. Maybe change to ""Remove the preferred log dir since it has already been satisfied."". BTW, should we remove it after the log has actually been created? Otherwise if storage exception happens during the log creation we may lose the preferred log dir information."
132878098,3621,becketqin,2017-08-14T05:59:27Z,Can you explain the reason of these changes?
132879332,3621,becketqin,2017-08-14T06:13:27Z,"I know this is in the KIP, but it still seems a little weird to throw exception in this case as the request is kink of legitimate. Not sure if there is a better way though."
132880098,3621,becketqin,2017-08-14T06:22:33Z,"Why should this be -1 instead of 0? This is essentially caught up, right?"
133014020,3621,lindong28,2017-08-14T17:43:13Z,"Sure. Good point. I have updated the patch with the following changes:

- Renamed DescribeDirsRequest to DescribeLogDirsRequest (same for response)
- Renamed AdminClient API to describeLogDirs
- Renamed AdminClient API to describeReplicaLogDirs

"
133014235,3621,lindong28,2017-08-14T17:44:04Z,I have added the class `AbstractOptions` that provides APIs to set and get timeoutMs.
133014767,3621,lindong28,2017-08-14T17:46:14Z,Good point. I have updated the patch to throw IllegalArgumentException if the future is not found.
133015323,3621,lindong28,2017-08-14T17:48:30Z,Sure. I replaced this comment with `Query the information of all log directories on the given set of brokers`.
133015335,3621,lindong28,2017-08-14T17:48:33Z,Sure. I replaced this comment with `Query the information of all log directories on the given set of brokers`.
133015631,3621,lindong28,2017-08-14T17:49:36Z,Good point. I have updated the protocol to remove `log_dirs` field from `DescribeLogDirsRequest`.
133015745,3621,lindong28,2017-08-14T17:50:00Z,Sure. I have renamed `describeReplicaDir` to `describeReplicaLogDir`.
133021670,3621,lindong28,2017-08-14T18:13:58Z,Good point. I replaced `-1` with `DescribeLogDirsResponse.INVALID_OFFSET_LAG`.
133021896,3621,lindong28,2017-08-14T18:14:54Z,Left parenthesis actually exists.
133022382,3621,lindong28,2017-08-14T18:16:48Z,Yeah. I have removed this field from DescribeLogDirsRequest.
133023112,3621,lindong28,2017-08-14T18:19:57Z,Good point. I have renamed this exception to `LogDirNotFoundException`. I think we can assume `not found` indicates `does not exist` given that we already have `NotFoundException`.
133023274,3621,lindong28,2017-08-14T18:20:43Z,I removed the `remaining` from this comment since it appears unnecessary.
133026257,3621,lindong28,2017-08-14T18:32:37Z,This is just an extra verification to confirm that we are only sending AlterReplicaDirRequest for replicas that have not been created yet.
133026969,3621,lindong28,2017-08-14T18:35:29Z,"Good point. I have updated the comment as suggested. And it is only removed after the log has been successfully created.

"
133027534,3621,lindong28,2017-08-14T18:37:25Z,"This is because we receive logDir as a strong from AlterReplicaDirRequest and it is inserted into `preferredLogDirs` as a strong. When we get it from `preferredLogDirs` it will be a strong not a file.

It is a minor change. Alternatively I can insert preferred log directory into that map as a file. I just feel the current approach is simpler."
133028201,3621,lindong28,2017-08-14T18:40:00Z,"IMO it is reasonable to throw exception here. This is because it is not guaranteed that the replica will be moved to the destination log directory if the replica is not already on the broker. For example, if the broker restarts after it receives AlterReplicaDirRequest but before it receives the upcoming LeaderAndIsrRequest, it will forget the cache in the memory."
133028267,3621,lindong28,2017-08-14T18:40:15Z,Discussed offline. I have updated the patch to remove this logic.
133028981,3621,lindong28,2017-08-14T18:42:54Z,"Are you talking about the `futures` map or the `replicaDirInfoByPartition`? The former needs to be constructed in advance as does the implementation of other APIs in AdminClient. The latter needs to be constructed in advance because its result maybe a combination of two entries in the response, i.e. primary and temporary replica of the same partition."
134098378,3621,becketqin,2017-08-19T18:52:50Z,Can we add a Java doc for this class?
134098547,3621,becketqin,2017-08-19T19:01:19Z,version should be 1.0.0.
134098552,3621,becketqin,2017-08-19T19:01:27Z,ditto.
134098889,3621,becketqin,2017-08-19T19:19:36Z,Should we change this name to AlterReplicaLogDirOptions as well?
134100952,3621,becketqin,2017-08-19T21:13:57Z,Do we need to have the `ReplicaLogDirInfo` printed? It seems the output format would be pretty verbose.
134101186,3621,becketqin,2017-08-19T21:28:44Z,We are still not throw exception here? Am I missing something?
134101387,3621,becketqin,2017-08-19T21:43:08Z,This assumes the only possibility of error is partition does not exist. Is this always true?
134101520,3621,becketqin,2017-08-19T21:51:45Z,The class also includes the broker id.
134101861,3621,becketqin,2017-08-19T22:14:29Z,AlterReplicaLogDirRequest?
134101922,3621,becketqin,2017-08-19T22:18:15Z,AlterReplicaLogDirResponse?
134101991,3621,becketqin,2017-08-19T22:21:31Z,Nit: DIr -> Dir
134102436,3621,becketqin,2017-08-19T22:51:34Z,I see. We probably need more comments to explain this. From the code itself it is weird that we always expect an exception from a future.
134102479,3621,becketqin,2017-08-19T22:54:52Z,We may also need to update the command help message.
134102637,3621,becketqin,2017-08-19T23:08:57Z,"Do we also want to look into the `preferredLogDirs`? In another word, do we want to have the preferred dir shown as temporary dir in the DescribeLogDirResponse even if the replica has not been created yet?"
134102674,3621,becketqin,2017-08-19T23:13:23Z,Should we also assert the log dir is non-null?
134102720,3621,becketqin,2017-08-19T23:17:23Z,Can we define a val for the number of log dirs?
134102789,3621,becketqin,2017-08-19T23:21:43Z,"This test seems a little overlapping with the test in `AdminClientIntegrationTest.testAlterReplicaLogDirBeforeTopicCreation`, do we need both?"
134103147,3621,lindong28,2017-08-19T23:50:04Z,Sure. I added this comment `This class implements the common APIs that are shared by Options classes for various AdminClient commands`.
134103158,3621,lindong28,2017-08-19T23:50:34Z,"Sorry, my bad. It is fixed now."
134103164,3621,lindong28,2017-08-19T23:50:58Z,Fixed now.
134103288,3621,lindong28,2017-08-20T00:00:20Z,"If we were to change it, we probably want to rename `AlterReplicaDirRequest` to `AlterReplicaLogDirReqeust`. But this seems a bit verbose to me. I think `AlterReplicaDirRequest` is probably OK and should not cause any confusion going forward. Do you think this may cause confusion if we don't name it `AlterReplicaLogDirReqeust`?"
134103332,3621,lindong28,2017-08-20T00:04:53Z,"The current patch doesn't not print `ReplicaLogDirInfo` anywhere to user except possibly in debug log. As far as the implementation of the `ReplicaLogDirInfo` is concerned, I think it makes sense to print all its fields as does in the current patch. Do you see any log statement that prints the `ReplicaLogDirInfo` in an unnecessary manner?"
134103367,3621,lindong28,2017-08-20T00:07:38Z,oops.. I must have made some mistake such that the change is lost.. I have fixed it now.
134103401,3621,lindong28,2017-08-20T00:10:36Z,"I added this `throw new IllegalArgumentException(""The partition "" + tp + "" in the response from broker "" + brokerId + "" is not in the request"");`"
134103422,3621,lindong28,2017-08-20T00:12:57Z,"This assumes that if `logDirInfo.error != Errors.NONE`, then the log directory must be offline on this broker and the `replicaInfos` in this `logDirInfo` will be empty. This is a valid assumption as of the current design."
134103428,3621,lindong28,2017-08-20T00:13:52Z,"Thanks a lot for the detailed review! I have changed it to `The topic name, partition number and the brokerId of the replica`."
134103435,3621,lindong28,2017-08-20T00:14:46Z,Thanks! Fixed now.
134103699,3621,lindong28,2017-08-20T00:38:03Z,"I don't think we should do that. Including this information in `DescribeLogDirRequest` will complicates the design of related APIs and expose an internal optimization detail to user. On the other hand, I don't think it provides any benefit to the user. The purpose of DescribeLogDirRequest is to get the load distribution of log directories on the broker so that user can determine a good reassignment replicas across log directories. Is there any information in `preferredLogDirs` that can help with this purpose of the `DescribeLogDirRequest`?

"
134103805,3621,lindong28,2017-08-20T00:50:43Z,"Sure. I added the following check:

```
logDirInfos.asScala.foreach { case (logDir, logDirInfo) =>
  logDirInfo.replicaInfos.asScala.keys.foreach(tp =>
    assertEquals(server.logManager.getLog(tp).get.dir.getParent, logDir)
  )
}
```"
134103834,3621,lindong28,2017-08-20T00:55:17Z,Not sure if I fully understand your question. Do you mean something like `protected def logDirCount: Int = 1` which is added by this patch in `BaseRequestTest.scala`? I think we only need to add this when we need to vary the logDirCount based on the test. It seems OK and simpler to just set logDirCount to 2 for all tests in `AdminClientIntegrationTest`.
134103883,3621,lindong28,2017-08-20T01:01:18Z,"I think it is OK to have both. If `AlterReplicaDirRequestTest.testAlterReplicaDirRequestBeforeTopicCreation` passes but `AdminClientIntegrationTest.testAlterReplicaLogDirBeforeTopicCreation` fails, it suggests that something is wrong in the KafkaAdminClient. This seems to follow the existing pattern -- tests such as `DeleteTopicsRequestTest` and `CreateTopicsRequestTest` probably would not be needed for the same reason given that we already have tests for higher level APIs that use them (if not then we should add the test for higher level APIs that uses them).

But I am not strong on this. I will remove this test if you think it is not necessary."
134104058,3621,lindong28,2017-08-20T01:19:24Z,"Good point. I added the following comments in the code:

```
/*
 * Before KIP-113 is fully implemented, user can only specify the destination log directory of the replica
 * if the replica has not already been created on the broker; otherwise the log directory specified in the
 * json file will not be enforced. Therefore we want to verify that broker will return ReplicaNotAvailableException
 * for this replica.
 *
 * After KIP-113 is fully implemented, we will not need to verify that the broker returns this ReplicaNotAvailableException
 * in this step. And after the reassignment znode is created, we will need to repeated send AlterReplicaDirRequest to broker
 * if broker returns ReplicaNotAvailableException for any replica in the request.
 */
```

And I updated the help message for option `--reassignment-json-file` to the following:

```
The JSON file with the partition reassignment configurationThe format to use is -                         

{""partitions"":                        
	[{""topic"": ""foo"",                    
	  ""partition"": 1,                    
	  ""replicas"": [1,2,3],               
	  ""log_dirs"": [""dir1"",""dir2"",""dir3""] 
  }],                                 
""version"":1                           
}                                     

Note that ""log_dirs"" is optional. When it is specified, its length must equal the length of the replicas list. The value in this list can be either ""any"" or the absolution path of the log directory on the broker.  If absolute log directory path is specified, it is currently required that the replica has not already been created on that broker. The replica will then be created in the specified log directory on the broker later.                       

```"
134106014,3621,becketqin,2017-08-20T04:27:38Z,"I think we usually do not print the field name of a map value, we usually not not print the name field of the value. Otherwise shouldn't we print the field name of the key as well? So for maps, it would simply be [key, value]. In this case, each entry could be printed as something like [topic-0-1, (currentLogDir=XXX, temporaryLogDir=YYY, temporaryReplicaOffsetLag=ZZZ)], which is probably clear enough.

BTW, I forgot to comment that I think we should add documentation about what are the value combination of those fields mean. For example is it possible I get a currentLogDir=null, tempLogDir=XXX, offsetsLage=-1? If so what does that mean."
134124244,3621,lindong28,2017-08-20T19:32:23Z,"I see. I didn't understand the question previously.

I wasn't aware that the conversion is to not print the name fild of the value. Sure. I just changed both the `ReplicaLogDirInfo.toString()` and `ReplicaInfo.toString()` so that they don't print the class name in the string.

BTW, I assume that this conversion only applies to internal classes, which I think is reasonable. For public classes such as `AlterReplicaDirRequest`, the patch will still print the class name in the `toString()` method as we do in e.g. `DeleteTopicsRequest`.

I also added comment to fields in the `ReplicaLogDirInfo` as show below. I think each fields can be interpreted independently and there is no need to explain how to interpret various combinations. For the example you mentioned, if currentLogDir=null, tempLogDir=XXX, offsetsLage=-1, it means the primary replica is not found and there is only a temporary replica for this partition on the given broker, which is possible if the primary replica is in an offline log directory.

```
// The log directory of the primary replica of this partition on the given broker.
// Null if the primary replica is not found for this partition on the given broker.
public final String currentReplicaLogDir;
// The log directory of the temporary replica of this partition on the given broker.
// Null if the temporary replica is not found for this partition on the given broker.
public final String temporaryReplicaLogDir;
// The LEO of the primary replica  - LEO of the temporary replica.
// -1 if either primary or the temporary replica is not found for this partition on the given broker.
public final long temporaryReplicaOffsetLag;
```"
134592277,3621,junrao,2017-08-22T20:26:50Z,unused import ReplicaInfo. Should we add the package name in front of DescribeLogDirsResponse?
134594816,3621,junrao,2017-08-22T20:36:53Z,"Would it be better to change the schema to have log_dir at the top level. Sth like

log_dirs => [log_dir [partitions]]
partitions => [topic [int32]]

This representation is more concise and is more consistent with how DESCRIBE_LOG_DIRS_RESPONSE_V0 is structured."
134616464,3621,junrao,2017-08-22T22:21:47Z,"In handleTopicMetadataRequest(), if topics() is null, we treat it as for all topics. If topics() is empty, we just treat it as no topics. It would be useful to use the same approach here for consistency."
134632381,3621,junrao,2017-08-23T00:23:26Z,"Hmm, if we are representing the dir for the permanent and the temporary replica together here, shouldn't we further keep the lag for both permanent and the temporary replicas?

Also, it's not clear to me why we don't return the same info in DescribeLogDirsResult."
134632961,3621,junrao,2017-08-23T00:28:39Z,"It's probably better to include alterReplicaDir() here too. If we think the api may change, we can mark it as unstable."
134633840,3621,junrao,2017-08-23T00:37:27Z,This seems no longer used?
134638546,3621,junrao,2017-08-23T01:21:01Z,"""on the broker ${replica.brokerId()}"" => ""on broker ${replica.brokerId()}"" "
134639735,3621,junrao,2017-08-23T01:34:04Z,"With this, it's possible for a created log not to be in logs. Then, we won't find this log in LogManager.getLog(). When serving requests like OffsetByTimestamp, we will probably return UnknownTopicPartitionException, but ideally we want to return KafkaStorageException. "
134661753,3621,lindong28,2017-08-23T05:46:53Z,My bad. It is removed now. I will go over the patch again to look for unused import.
134662104,3621,lindong28,2017-08-23T05:50:00Z,Sure. The original schema is motivated by the idea that alterReplicaDir operation naturally maps a replica to a log directory. I am not very sure AlterReplicaDirRequest needs to be consistent with DescribeLogDirRequest. But I think what you suggested will make the request more compact and smaller in size. I have made the change as suggested. Thanks!
134663181,3621,lindong28,2017-08-23T05:59:49Z,"The class `ReplicaLogDirInfo` doesn't include that currentReplicaLag because this information is not currently used in this patch. Previously I think we can include `currentReplicaLag` in this when it is needed, i.e. to track reassignment progress in KIP-179. I have updated the patch to include `currentReplicaLag` in `ReplicaLogDirInfo`.

I am not sure I fully understand the second question. What information should we add or remove from `DescribeLogDirsResult`? I think `DescribeLogDirsResult` and `DescribeReplicaLogDirResult` has different format because they are used to serve two different use-cases. These two classes are structured in a way that makes the respective AdminClient APIs easier to use."
134663259,3621,lindong28,2017-08-23T06:00:48Z,Sure. I have added it back to AdminClient.
134663285,3621,lindong28,2017-08-23T06:01:03Z,Ah.. my bad. It is removed now.
134663350,3621,lindong28,2017-08-23T06:01:36Z,I have fixed this as suggested in multiple places in this method.
134663939,3621,lindong28,2017-08-23T06:06:44Z,"After double checking the logic, I think we will actually return `KafkaStorageException` for `OffsetByTimestamp` query if log creation failed. This logic is enforced in KIP-112. More specifically, after broker receives LeaderAndIsrRequest to create the log, if `LogManager.getOrCreateLog()` throws `KafkaStorageException`, `Partition.getOrCreateReplica()` will also throw exception without putting this replica in the `Partition.assignedReplicaMap`. 

Later we will execute the following code in `ReplicaManager.becomeLeaderOrFollower()`:

```
leaderAndISRRequest.partitionStates.asScala.keys.foreach( topicPartition =>
  /*
   * If there is offline log directory, a Partition object may have been created by getOrCreatePartition()
   * before getOrCreateReplica() failed to create local replica due to KafkaStorageException.
   * In this case ReplicaManager.allPartitions will map this topic-partition to an empty Partition object.
   * we need to map this topic-partition to OfflinePartition instead.
   */
  if (getReplica(topicPartition).isEmpty && (allPartitions.get(topicPartition) ne ReplicaManager.OfflinePartition))
    allPartitions.put(topicPartition, ReplicaManager.OfflinePartition)
)
```

This logic makes sure that this partition will map to `ReplicaManager.OfflinePartition` when this broker receives any request from user, including requests for OffsetByTimestamp. The response will return `KafkaStorageException` to the user.
"
134683583,3621,lindong28,2017-08-23T08:09:29Z,Sure. I have updated the patch as suggested.
134794883,3621,cmccabe,2017-08-23T15:57:43Z,"We should use an accessor function for the following fields-- for all the usual reasons we do this in Java.  (Unlike in Scala, you cannot write an accessor later to transparently switch over users.)"
134795363,3621,cmccabe,2017-08-23T15:59:15Z,"@becketqin is right-- you should handle this case.  Perhaps the server sent back bad data.  The way to handle it is not to throw an exception, but to complete the relevant future(s) with an error.  There are a few other cases where we handle bad server data by completing a future with failure in AdminClient."
134795858,3621,cmccabe,2017-08-23T16:01:02Z,I don't think we should map zero responses to CLUSTER_AUTHORIZATION_FAILED.  What if we need to return different error codes later?  We should have an error code per log dir response.
134807975,3621,lindong28,2017-08-23T16:50:37Z,"@cmccabe Currently the only request level error for DescribeLogDirResponse is if the CLUSTER_AUTHORIZATION_FAILED. This error will happen if and only if the error there is no log directories in the response. This is because if a broker is online and the user is authorized to describe cluster resource, then it is guaranteed that DescribeLogDirResponse should return some log directories. Also note that DescribeLogDirResponse does include per-logDir error in the response.

Thus we don't need request level error for DescribeLogDirResponse at this moment. Is there any scenario that will request this error field in the future? If we don't have specific use-case for this field, can we add it only when we need it in the future?"
134809797,3621,lindong28,2017-08-23T16:58:18Z,@cmccabe Good point. I have updated the code to complete all futures with IllegalArgumentException when this happens.
134815324,3621,lindong28,2017-08-23T17:21:46Z,@cmccabe I noticed that classes such as `UpdateMetadataRequest.PartitionState` makes its fields public instead of using accessor fields. Are accessor fields needed by ReplicaLogDirInfo because it is exposed to user via `AdminClient.describeReplicaLogDir()`?
135162431,3621,junrao,2017-08-25T00:33:41Z,Should we guard the case that the same partition or the same log dir is specified more than once and error out?
135165084,3621,junrao,2017-08-25T01:01:20Z,getPermanentReplicaLogDir seems to match getTemporaryReplicaLogDir better. Ditto on getCurrentReplicaOffsetLag.
135166115,3621,junrao,2017-08-25T01:12:42Z,"Instead of using (_._2), could we use case to define named variables to make it clear? Ditto on line 233."
135167112,3621,junrao,2017-08-25T01:22:57Z,"Hmm, intuitively, if a replica doesn't exist, setting a log dir for it should succeed, instead of getting an error. Perhaps we should return success if the replica doesn't exist or is in the right dir, and throw an unsupported exception for now otherwise."
135167447,3621,junrao,2017-08-25T01:26:43Z,Do you mean broker 101?
135167654,3621,junrao,2017-08-25T01:29:04Z,Do you mean broker 102?
135168551,3621,junrao,2017-08-25T01:38:48Z,Is the comment accurate? It seems the test has invalid log dir.
135168609,3621,junrao,2017-08-25T01:39:23Z,Is the comment accurate? 
135168812,3621,junrao,2017-08-25T01:41:35Z,"Could we add a test case that the length of ""log_dirs"" doesn't match that in ""replicas""?"
135168953,3621,junrao,2017-08-25T01:42:54Z,unused import
135169300,3621,junrao,2017-08-25T01:46:53Z,"Hmm, the event will be processed asynchronously after this call returns. Should we guarantee that this event is processed before sending DESCRIBE_LOG_DIRS request?"
135182887,3621,lindong28,2017-08-25T04:45:59Z,"I think we don't need to add this logic in `AlterReplicaDirRequest` because the current implementation will always provide unique partition and log when instantiating `AlterReplicaDirRequest`. `AlterReplicaDirRequest` will only be instantiated in `AdminClient.alterReplicaDir()`. Because `AdminClient.alterReplicaDir()` takes `Map<TopicPartitionReplica, String> replicaAssignment` as input, it guarantees that the partition in the `AlterReplicaDirRequest` will be specified only once.

Also, `AlterReplicaDirRequest.toStruct()` will group `TopicPartitionReplica` by logDir and thus the logDir in the `AlterReplicaDirRequest` will be specified only once.
"
135183018,3621,lindong28,2017-08-25T04:48:13Z,I am bit concerned that `getPermanentReplicaLogDir` may mislead user into thinking log directory of this partition will never change. How about we rename `getTemporaryReplicaLogDir` to `getFutureReplicaLogDir`?
135183254,3621,lindong28,2017-08-25T04:51:51Z,"Sure. I replaced them with the following:

```
val partitionsByBroker = leaderByPartition.groupBy { case (partitionId, leaderId) => leaderId }.mapValues(_.keys.toSeq)`

val replicaInfos = logDirInfos.asScala.flatMap { case (logDir, logDirInfo) => logDirInfo.replicaInfos.asScala }.filterKeys(_.topic == topic)
```"
135184008,3621,lindong28,2017-08-25T05:03:03Z,"I think it depends on the semantics of AlterReplicaDirRequest. In my understanding ""alter"" means ""change the proper of something that already exists"". Thus broker should not create replica for this partition if this replica does not already exist. And it is reasonable to throw exception to user because user requested to alter log directory of a replica that doesn't exist.

Does this make sense?"
135184035,3621,lindong28,2017-08-25T05:03:35Z,My bad. Fixed now. Thanks.
135184065,3621,lindong28,2017-08-25T05:04:11Z,Ah.. fixed now. Thanks!
135184249,3621,lindong28,2017-08-25T05:07:10Z,My bad.. I replaced the comment with `When we execute an assignment that specifies an invalid log directory`.
135184640,3621,lindong28,2017-08-25T05:13:13Z,"No... I have replaced the comment with the following:

```
// When we execute an assignment that specifies log directory for an existing replica on the broker
// This test can be removed after KIP-113 is fully implemented, which allows us to change log directory of existing replicas on a broker
```"
135184811,3621,lindong28,2017-08-25T05:16:04Z,Good point. I have changed the code to do `servers.head.replicaManager.handleLogDirFailure(offlineDir)`.
135193414,3621,lindong28,2017-08-25T06:48:56Z,Sure. I have added test `shouldFailIfProposedHasInconsistentReplicasAndLogDirs` for this.
135372907,3621,junrao,2017-08-25T23:51:52Z,"Yes, the java client behaves as you described. I was mostly concerned about how the broker handles requests from non-java clients."
135372918,3621,junrao,2017-08-25T23:52:05Z,Perhaps use getCurrentReplicaLogDir and get getNewReplicaLogDir?
135372984,3621,junrao,2017-08-25T23:53:05Z,"Hmm, I think AlterReplicaDirRequest just means the intention to alter the dir. It doesn't mean the change has to be completed. As long as the intention is remembered by the broker, it seems it's reasonable to return success. The weird thing right now is that in ReassignPartitionsCommand, the happy path is actually based on an exception in the response of AlterReplicaDirRequest. Normally, the happy path should be when the response has no error."
135374404,3621,lindong28,2017-08-26T00:14:00Z,"Thanks for the comment @junrao !

By non-Java clients, do you mean the clients written by third-party and not maintained by in Apache Kafka repository? Here is my thought:

- If a thirty party client constructs the `AlterReplicaDirRequest` using the API `AlterReplicaDirRequest.Builder(Map<TopicPartition, String> partitionDirs)`, we still guarantee that both log directory and the topicPartition in the resulting `AlterReplicaDirRequest` will be unique. Thus server doesn't have to worry about it.

- If a third party client constructs `AlterReplicaDirRequest` without using our builder, and it constructs `AlterReplicaDirRequest` with duplicated topicPartition by mistake, the server won't be affected by this. This is because the `AlterReplicaDirRequest(Struct struct, short version)` will generate the `Map<TopicPartition, String> partitionDirs` which contains well-defined and unique partition to logDir mapping. Also, since user constructs `AlterReplicaDirRequest` without using our builder, we won't help user detect this mistake by changing the code here.

BTW, this issue seems to also exist in `AlterConfigsRequest`, which may contain duplicated entries for the same resource type and resource name. It may be reasonable to handle them in the same way as this patch does.

Does it make sense?"
135374956,3621,lindong28,2017-08-26T00:23:36Z,"I agree it doesn't have to be completed as long as it is remembered. However, currently this is only remembered in the memory which may be lost if broker restarts after it receives AlterReplicaDirRequest but before it receives LeaderAndIsrRequest to create the replica. I think we probably don't want to return success to user and create replica is a different log directory later (if restart happens in the above case).

@becketqin has similar comment regarding the weirdness that we expect response to throw exception. But this weirdness exists only because this patch represents the first part of KIP-113. After we fully implement the KIP-113, we won't expect to see exception in the happy path. The logic in the reassignment will look like this:

- Call `adminClient.alterReplicaDirRequest(replicaAssignment)` without waiting for response
- Create the reassignment znode so that controller can start replica reassignment across brokers.
- Call `adminClient.alterReplicaDirRequest(replicaAssignment)` and verify that there is no error in the response. The implementation of `adminClient.alterReplicaDirRequest` will treat `ReplicaNotAvailableException` as a retriable error and retry up to the user-specified timeout.

I think it is OK for this weirdness to exist for a short period of time before KIP-113 is fully implemented. User won't be affected by this weirdness. Does this make sense?"
135375065,3621,lindong28,2017-08-26T00:25:20Z,Sure. I will update the patch to use `getNewReplicaLogDir` and rename other fields as appropriate.
135376799,3621,lindong28,2017-08-26T01:08:36Z,"@junrao After discussing with @becketqin, I think it may be better to use `getFutureReplicaLogDir`.

If we were to use `getNewReplicaLogDir`, do you think we should rename `is_temporary` field in `DESCRIBE_LOG_DIRS_RESPONSE_V0` to `is_new`? If so, it kinds of collide with the `is_new` field in `LEADER_AND_ISR_REQUEST_PARTITION_STATE_V1`. It seems a bit confusing.

Also, if we were to use `getFutureLogDir`, are you OK with renaming the field `is_temporary` to `is_future` in `DESCRIBE_LOG_DIRS_RESPONSE_V0`?"
135377833,3621,junrao,2017-08-26T01:36:04Z,"Ok, if doesn't do harm on the server, we can punt on this."
135377868,3621,junrao,2017-08-26T01:37:01Z,"Ok, future is fine then. Changing is_temporary to is_future also sounds good."
135377870,3621,junrao,2017-08-26T01:37:03Z,"Hmm, I thought for verification, we want to use describeLogDirs not alterReplicaDirRequest? Also, for the first adminClient.alterReplicaDirRequest(), I am not sure that we want to completely ignore the response. For example, if the request fails with authentication error, we probably want to error out, right?

Overall, are you saying that AlterReplicaDirRequest only returns no error if the log dir is in the target log dir? That means most of the time, the request will return error. This seems unintuitive since error should be the exception, not the norm. "
135378158,3621,lindong28,2017-08-26T01:47:38Z,"Yes, we only use `describeLogDirs` for verification. In the current patch, `ReassignPartitionsCommand` only checks for `ReplicaNotAvailableException` using `alterReplicaDirRequest` during execution (i.e. when --execute is specified).

You are right. I simplified the logic of the first `adminClient.alterReplciaDir()` in my previous response. More specifically, after KIP-113 is fully implemented, the `ReassignPartitionsCommand` should use `adminClient.alterReplciaDir()` to send `AlterReplicaDirRequest` without retry. And it should verify that either there is no error or the error is `ReplicaNotAvailableException`. The second `adminClient.alterReplciaDir()` should retry `AlterReplicaDirRequest` upon `ReplicaNotAvailableException` until timeout.

`AlterReplicaDirRequest` can also return no error even if the log directory is not the target log directory, as long as replica already exists on the broker. This is because if the replica already exists on the broker, the broker will create a directory for the temporary replica in the destination log directory. Because this information is persisted on the disk rather than in memory, broker will continue to move replica to the destination log directory after restart.
"
136207465,3621,junrao,2017-08-30T22:37:28Z,"The name of the method seems a bit confusing. It's not really clear what WithoutDedup really means. Could it be just named parsePartitionReassignmentData()? Also, could we document the return value?"
136208723,3621,junrao,2017-08-30T22:45:52Z,"Ok, thanks for the explanation. This is fine then. Could we document the error code/exception in AlterReplicaDirResult?"
136209635,3621,lindong28,2017-08-30T22:52:06Z,"@junrao This name is used following the name of the existing method `ZkUtils.parsePartitionReassignmentDataWithoutDedup`. If we name it `parsePartitionReassignmentData` in `ReassignPartitionsCommand`, should we also rename the method in `ZkUtils` for consistency? Note that there is an existing method `parsePartitionReassignmentData(jsonData: String): Map[TopicAndPartition, Seq[Int]]` in ZkUtils which justifies the use of `parsePartitionReassignmentDataWithoutDedup` in ZkUtils."
136210274,3621,lindong28,2017-08-30T22:56:29Z,"@junrao Sure. Previously the error is only documented in `AlterReplicaDirResponse`. Just now I added the error code documentation in `AlterReplicaDirResult` as follows:

```
* LOG_DIR_NOT_FOUND (57)
* KAFKA_STORAGE_ERROR (56)
* REPLICA_NOT_AVAILABLE (9)
* UNKNOWN (-1)
```"
136211210,3621,junrao,2017-08-30T23:03:06Z,Perhaps we can just get rid of ZkUtils.parsePartitionReassignmentDataWithoutDedup since the only caller is ZkUtils.parsePartitionReassignmentData?
136211869,3621,lindong28,2017-08-30T23:08:07Z,Sure. I have updated the patch as suggested. Thanks for taking time to review the patch!
136690443,3621,becketqin,2017-09-02T06:25:28Z,"Can we add a comment here for this? Also, if the only possible error is log dir offline, would it be clearer to only check for that error and log an error message or throw exception in other cases?"
136703340,3621,becketqin,2017-09-02T20:31:09Z,"Maybe worth adding a comment? Also usually the clients do not infer an error code while the broker did not return it. Maybe it is better to let the server to return CLUSTER_AUTHORIZATION_FAILED. We are doing the same for `DescribeAclsRequest`. If we do that we will need to add response level error code, which probably makes sense."
136710394,3621,ijuma,2017-09-03T07:28:17Z,"@lindong28, Jun was referring to clients that don't use our code at all. librdkafka, kafka-python, etc."
136711246,3621,ijuma,2017-09-03T08:21:42Z,"Sorry for being late on this. Why is this `alterReplicaDir` instead of `alterReplicaDirs` (or some other version that indicates the batch nature)? This is a batch api like every other API so it should indicate that via the name, right?"
136711278,3621,lindong28,2017-09-03T08:23:58Z,@ijuma I see. Thanks for the information.
136711440,3621,lindong28,2017-09-03T08:32:55Z,"@ijuma I think one reason to use alterReplicaDir is that it presents a map from replica -> dir. It is probably a bit different from other names such as describeConfigs, which represents a collection of configs.

@ijuma Do you like me to submit a patch to change it to a different name? @junrao @becketqin What do you think about this name?
"
136711492,3621,ijuma,2017-09-03T08:36:14Z,Why did we remove this?
136711557,3621,ijuma,2017-09-03T08:39:15Z,"Thanks for the quick reply. Right, it's a map from replica to dir instead of a single replica to dir. The name sounds like the latter to me. Before doing a PR, let's see if we get consensus amongst yourself and the reviewers."
136711582,3621,lindong28,2017-09-03T08:40:42Z,"Originally I added a similar comment to the new API. @becketqin commented that this is unnecessary. I agree with @becketqin this is unnecessary because `AlterConfigsResult` returns `Map<ConfigResource, KafkaFuture<Void>>` which suggests that some configs may be updated successfully while others fail. Thus there is no need to have extra comment in the API documentation to specify this. Does this make sense?"
136712129,3621,ijuma,2017-09-03T09:10:32Z,"While I understand the sentiment, we have to remember that these APIs will be used by people who are not familiar with Kafka in the same way we are. We often get support questions because things that seem obvious to us are not clear to users. I disagree that we should remove clarification comments like the above in public APIs."
136712322,3621,lindong28,2017-09-03T09:20:39Z,"@ijuma I see. Sure, I can submit a minor patch tomorrow for this (or please feel free to just commit a minor patch if you prefer). To keep the API document consistent and for the same reason you described, maybe we should have this comment (i.e. API is not transactional) for all those APIs in the AdminClient which may partially succeed, e.g. deleteTopics?"
136712993,3621,ijuma,2017-09-03T09:58:32Z,"Yes, I think that's a good idea. If you are OK with submitting a PR, that would be great. It's not urgent, we should aim to do it before 1.0.0 is released."
136741892,3621,tedyu,2017-09-04T04:16:18Z,Is IllegalStateException more approriate for this situation ?
136742443,3621,tedyu,2017-09-04T04:27:46Z,nit: the 'else' can be omitted.
136772853,3621,lindong28,2017-09-04T08:52:23Z,Thanks much for catching this @tedyu. You are right. I will fix this in https://github.com/apache/kafka/pull/3781.
136772976,3621,lindong28,2017-09-04T08:53:01Z,Originally I think the if/else may be better. The difference is very minor to me. I will remove `else` in https://github.com/apache/kafka/pull/3781.
399149713,7898,OneCricketeer,2020-03-27T09:54:44Z,"IMO, why not rewrite against `org.slf4j`?"
399149940,7898,OneCricketeer,2020-03-27T09:55:09Z,"Based on the comments, how about `2.13.x`?"
399333888,7898,dongjinleekr,2020-03-27T15:08:24Z,"@cricket007 Yes, I am now working with `2.13.1` and it seems like good."
399335778,7898,dongjinleekr,2020-03-27T15:11:10Z,"`Log4jController` provides a dynamic `Logger` querying functionality, not logging itself. It is why it uses log4j `Logger`s directly.

In contrast, the streams module does not provide those kinds of functionality so it uses slf4j fascade."
400595313,7898,OneCricketeer,2020-03-31T01:48:42Z,"Is Zookeeper duplicated here??

Also, does Zookeeper transitively bring in log4j anywhere?"
400595736,7898,OneCricketeer,2020-03-31T01:50:16Z,It seems `org.apache.kafka.*` imports used to be first 
400596128,7898,OneCricketeer,2020-03-31T01:51:49Z,"Is the `.map()` needed?

```suggestion
            .collect(Collectors.toMap(logger -> logger.getName(), logger -> configLevelToMap(logger));
```"
400596367,7898,OneCricketeer,2020-03-31T01:52:44Z,"Also, `Collectors.toCollection(Treeset::new)` might be useful"
400596620,7898,OneCricketeer,2020-03-31T01:53:34Z,can `logger == null`?
400597136,7898,OneCricketeer,2020-03-31T01:55:15Z,It seems these imports used to be first
400597441,7898,OneCricketeer,2020-03-31T01:56:30Z,"personally, rather than rely on src/test/resources, I would pull from the classpath... `LoggingResourceTest.class.getClassLoader().getResource(""log4j2.properties"")`"
400598244,7898,OneCricketeer,2020-03-31T01:59:26Z,remove this?
400598312,7898,OneCricketeer,2020-03-31T01:59:40Z,remove this?
400598447,7898,OneCricketeer,2020-03-31T02:00:11Z,This pattern looks different
400598659,7898,OneCricketeer,2020-03-31T02:00:56Z,nit: these got rearranged
401709877,7898,dongjinleekr,2020-04-01T15:35:11Z,Fixed. Please have a look at [this PR](https://github.com/apache/kafka/pull/8404). :)
401713158,7898,dongjinleekr,2020-04-01T15:39:40Z,"This duplication has been addressed in [this PR](https://github.com/apache/kafka/pull/8130).

Since this setting contols only direct imports only, it does not bring `log4j` transitively."
401713391,7898,dongjinleekr,2020-04-01T15:39:59Z,Great. I will apply it. :)
401713720,7898,dongjinleekr,2020-04-01T15:40:27Z,No. log4j2 does not allow `logger == null`.
401714074,7898,dongjinleekr,2020-04-01T15:40:58Z,Agree. I will have a try.
401714449,7898,dongjinleekr,2020-04-01T15:41:30Z,Not yet. `log4j-appender` is still using log4j; `log4j2-appender` is under progress. :)
401714669,7898,dongjinleekr,2020-04-01T15:41:48Z,Ditto :smiley: 
401715288,7898,dongjinleekr,2020-04-01T15:42:40Z,"Yes, but the reason is that it follows deleted `quickstart/java/src/main/resources/archetype-resources/src/main/resources/log4j.properties`; it has different pattern so I followed it."
497371849,7898,tombentley,2020-09-30T09:30:32Z,"Shouldn't `connect.log.pattern` actually be something like this:

```
appender.stdout.layout.pattern=[%d] %p %m (%c:%L)%n
# and 
appender.connectAppender.layout.pattern=[%d] %p %m (%c:%L)%n
```
?"
497376858,7898,tombentley,2020-09-30T09:38:53Z,"Is the `policies` really necessary if we're only using a single triggering policy? I t_hink_ you could just say 

```
appender.connectAppender.policy.type=TimeBasedTriggeringPolicy
appender.connectAppender.policy.interval=1
appender.connectAppender.policy.modulate=true
```"
497378983,7898,tombentley,2020-09-30T09:42:20Z,Use a `\` escaped newline to avoid the very long line.
497380746,7898,tombentley,2020-09-30T09:45:17Z,"Also since log4j2 I think it's often unnecessary to have to specify` loggers` and `appenders` upfront, they can be determined during parsing. Unfortunately the docs don't bother saying when it _is_ necessary, but it would be easy for users to add their `logger....name` and `logger....level` properties only to find it didn't work because they forgot about adding the name to `loggers`, so it would be good to only specify `loggers` if we really need to.

Same applies to appenders and the other log4j config files too, of course."
497381662,7898,tombentley,2020-09-30T09:46:48Z,Why was `log4j.rootLogger=OFF` before but `WARN` now?
497385774,7898,tombentley,2020-09-30T09:53:23Z,"collect has a overload which takes a supplier of empty maps (I think you also have to provide a lamda for duplicate keys, annoyingly). That would allow you to avoid needing the `new TreeMap` in the `return`. Alternatively just use `forEach` as previously. "
497387063,7898,tombentley,2020-09-30T09:55:23Z,You can avoid the `if` using `found.orElse(null)`
497387624,7898,tombentley,2020-09-30T09:56:17Z,Was changing the parameter name really necessary? It makes the diff noisier and the old name wasn't _so_ bad.
497391073,7898,tombentley,2020-09-30T10:02:20Z,"It's a pre-existing issue, but I think this is slightly incorrect since it would tread `com.foo` as an ancestor of `com.foobar`. Really we should be using `startsWith` with a logger name that we know ends with a `.`."
497392952,7898,tombentley,2020-09-30T10:05:18Z,"This is the second time you've got a `.equals("""")`. It might be worth factoring into a `isRootLogger(Logger)` method."
497393514,7898,tombentley,2020-09-30T10:06:13Z,Why was this necessary?
497396302,7898,tombentley,2020-09-30T10:11:25Z,"The default when the level is not set should be the ancestor logger's level, rather than the root logger level, But I guess you're waiting for my PR to be merged, right?"
497400286,7898,tombentley,2020-09-30T10:18:56Z,"We probably need better coverage in the alter case (`testIncrementalAlterConfigsForLog4jLogLevels()`, below). It tests inheritance from the root logger, but not an ancestor logger. But I guess this is something I should add to my PR."
497402528,7898,tombentley,2020-09-30T10:23:06Z,"`s""${classOf[ControllerIntegrationTest]}#testControllerMoveOnTopicCreation""`, and if not then there should be no need for the `toString`"
497402639,7898,tombentley,2020-09-30T10:23:18Z,same comment.
497402786,7898,tombentley,2020-09-30T10:23:37Z,same comment.
497402868,7898,tombentley,2020-09-30T10:23:48Z,same comment
497410244,7898,tombentley,2020-09-30T10:37:28Z,"It's a shame about the new asynchrony here, but I don't see an obvious we of avoiding it."
497412007,7898,tombentley,2020-09-30T10:40:48Z,Again it's not really clear to me why this is added
497413476,7898,tombentley,2020-09-30T10:43:42Z,"I can see this being a source of difficult to maintain tests, if you have to tweak the latch every time logging statements are added or removed. "
497415319,7898,tombentley,2020-09-30T10:47:28Z,"Do we really need the caller to supply a `name`? It seems to force you to have to construct a unique name each time you want to use it, based on the test class and method name. But all that's needed is uniqueness and the removal of the appender at the end of the test, AFAICS. So just using a UUID or similar generated name would be sufficient and make call sites rather easier to read."
497416562,7898,tombentley,2020-09-30T10:50:02Z,"I wonder if it would simplify the tests if this had methods for asserting the existence of messages (optionally within a timeout) rather than having to use the `setLatch(), await(), getMessages()` pattern in every test."
497417343,7898,tombentley,2020-09-30T10:51:31Z,Was this really necessary?
500806583,7898,dongjinleekr,2020-10-07T07:50:01Z,"Oh, I don't know why it is shown as a modification in diff view. in `config/tools-log4j.properties ` root logger level is `WARN` by `log4j.rootLogger=WARN, stderr`."
500807752,7898,dongjinleekr,2020-10-07T07:51:52Z,Great. I will improve the formatting and lining. Let me see.
500821253,7898,dongjinleekr,2020-10-07T08:14:05Z,"No. `connect.log.pattern` is get referenced at the lines below:

```
appender.stdout.layout.pattern=${connect.log.pattern}
appender.connectAppender.layout.pattern=${connect.log.pattern}
```"
500829824,7898,dongjinleekr,2020-10-07T08:27:30Z,"Is this possible? All examples I saw explicitly state `Policies`, like:

- https://stackoverflow.com/questions/33611411/setting-rollover-strategy-in-log4j2-properties
- https://howtodoinjava.com/log4j2/log4j2-rollingfileappender-example/"
500830940,7898,dongjinleekr,2020-10-07T08:29:14Z,"Oh, I thought making it consistent with `LoggingResource#setLevel` would be better."
500838684,7898,dongjinleekr,2020-10-07T08:41:15Z,"It is related to a issue between log4j2 2.13.x and powermock; in short, it causes `java.lang.LinkageError` as of present.

Please see:

- https://stackoverflow.com/questions/41609436/powermock-after-log4j2-3-upgrade-could-not-reconfigure-jmx-java-lang-linkageerro
- https://stackoverflow.com/questions/16520699/mockito-powermock-linkageerror-while-mocking-system-class"
500840081,7898,dongjinleekr,2020-10-07T08:43:23Z,Right. Fixed.
500849730,7898,dongjinleekr,2020-10-07T08:58:12Z,"Oh my, your approach is much simpler and more clear. Okay, I will take this approach."
500870442,7898,dongjinleekr,2020-10-07T09:30:27Z,"As you can see below, `new StreamsConfig(props);` is called in every test method of this suite. So, this line is redundant.

More importantly, it generates duplicated log messages and makes log message related test cases hard to validate."
500882458,7898,dongjinleekr,2020-10-07T09:49:59Z,"Good proposal. Actually, it is the first approach I have taken.

However, while running the tests repeatedly, I found that the log messages are not forwarded in designated timeout properly, and the tests go so flaky. Each test runs correctly when I run them individually, but 3 ~ 5 tests were randomly failed when I run them in bulk, with `./gradlew :streams:test`. It seems like this symptom is related to the busy wait implementation of `ListAppender#getMessages` but I can't certain yet.

After numerous trial and error, I found that the current approach is a little bit verbose but makes the test suites sustainable. It is the background of this API design."
501043518,7898,dongjinleekr,2020-10-07T14:09:17Z,"> But all that's needed is uniqueness and the removal of the appender at the end of the test, AFAICS.

Exactly. However, I thought introducing randomness to the test cases is worse than some verbosity. Let's wait for others' opinions."
501046635,7898,dongjinleekr,2020-10-07T14:13:18Z,Agree. Let's add a additional test about that case.
501048158,7898,dongjinleekr,2020-10-07T14:15:19Z,"> But I guess you're waiting for my PR to be merged, right?

Exactly."
501054972,7898,dongjinleekr,2020-10-07T14:23:52Z,"Agree. I am now thinking about a utility class that determines whether a given logger name is a child, descendent, parent, or ancestor of another logger."
501060061,7898,dongjinleekr,2020-10-07T14:30:04Z,"Well, I found a similar method to what you described in `Collectors`, not in `Stream#collect`. Is this what you mean?

```
  ...
  .collect(Collectors.toMap(Logger::getName(), logger -> levelToMap(logger), (o1, o2) -> o1, TreeMap::new))
```"
769044309,7898,rafalmag,2021-12-14T21:04:51Z,Please use 2.15.0 instead to avoid noise about CVE-2021-44228
769083607,7898,amuraru,2021-12-14T22:07:30Z,Actually 2.16 please
769193981,7898,showuon,2021-12-15T02:24:53Z,"We should say, please use the **latest** release of log4j2, please! :)"
779703586,7898,ispringer,2022-01-06T17:05:23Z,Can this be bumped to `2.17.1` (https://logging.apache.org/log4j/2.x/changes-report.html#a2.17.1)?
780114085,7898,dongjinleekr,2022-01-07T08:59:50Z,"Yes, I did [LOG4J2-3256](https://issues.apache.org/jira/browse/LOG4J2-3256) for [KAFKA-12399](https://github.com/apache/kafka/pull/10244) and it will be also applied to this PR. :+1:"
783853620,7898,viktorsomogyi,2022-01-13T11:03:14Z,I think renaming in this case makes the code a bit clearer and consistent.
783871962,7898,viktorsomogyi,2022-01-13T11:29:10Z,nit: instead of `.map(_._2)` you could use `.values`
785830903,7898,viktorsomogyi,2022-01-17T10:32:27Z,I think I would also prefer generating a name in the `apply()` method. If someone uses the same name twice it might introduce an error but it's indeed easier to read without the `name` as Tom proposed.
785843643,7898,viktorsomogyi,2022-01-17T10:48:28Z,"Ran the tests and it seems like the PowerMock class loader isn't able to load these classes (as they have previously been loaded?). It doesn't cause a test failure but it's ugly and deferring to the system classloader fixes the issue. Maybe @dongjinleekr has a more specific answer but I think it's fine to have this annotation here.
```
2022-01-17 11:43:22,581 Test worker ERROR Could not reconfigure JMX java.lang.LinkageError: loader constraint violation: loader (instance of org/powermock/core/classloader/javassist/JavassistMockClassLoader) previously initiated loading for a different type with name ""javax/management/MBeanServer""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:756)
	at org.powermock.core.classloader.javassist.JavassistMockClassLoader.loadUnmockedClass(JavassistMockClassLoader.java:90)
	at org.powermock.core.classloader.MockClassLoader.loadClassByThisClassLoader(MockClassLoader.java:104)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass1(DeferSupportingClassLoader.java:147)
	at org.powermock.core.classloader.DeferSupportingClassLoader.loadClass(DeferSupportingClassLoader.java:98)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at org.apache.logging.log4j.core.jmx.Server.unregisterAllMatching(Server.java:337)
	at org.apache.logging.log4j.core.jmx.Server.unregisterLoggerContext(Server.java:261)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:165)
	at org.apache.logging.log4j.core.jmx.Server.reregisterMBeansAfterReconfigure(Server.java:141)
	at org.apache.logging.log4j.core.LoggerContext.setConfiguration(LoggerContext.java:637)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:699)
	at org.apache.logging.log4j.core.LoggerContext.reconfigure(LoggerContext.java:716)
	at org.apache.logging.log4j.core.LoggerContext.start(LoggerContext.java:270)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:155)
	at org.apache.logging.log4j.core.impl.Log4jContextFactory.getContext(Log4jContextFactory.java:47)
	at org.apache.logging.log4j.LogManager.getContext(LogManager.java:196)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getContext(AbstractLoggerAdapter.java:137)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getContext(Log4jLoggerFactory.java:55)
	at org.apache.logging.log4j.spi.AbstractLoggerAdapter.getLogger(AbstractLoggerAdapter.java:47)
	at org.apache.logging.slf4j.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:33)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:363)
	at org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:388)
	at org.apache.kafka.streams.processor.internals.StateDirectory.<clinit>(StateDirectory.java:67)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:348)
	at org.easymock.cglib.core.ReflectUtils.defineClass(ReflectUtils.java:467)
	at org.easymock.cglib.core.AbstractClassGenerator.generate(AbstractClassGenerator.java:339)
	at org.easymock.cglib.proxy.Enhancer.generate(Enhancer.java:492)
	at org.easymock.cglib.core.AbstractClassGenerator$ClassLoaderData$3.apply(AbstractClassGenerator.java:96)
	at org.easymock.cglib.core.AbstractClassGenerator$ClassLoaderData$3.apply(AbstractClassGenerator.java:94)
	at org.easymock.cglib.core.internal.LoadingCache$2.call(LoadingCache.java:54)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at org.easymock.cglib.core.internal.LoadingCache.createEntry(LoadingCache.java:61)
	at org.easymock.cglib.core.internal.LoadingCache.get(LoadingCache.java:34)
	at org.easymock.cglib.core.AbstractClassGenerator$ClassLoaderData.get(AbstractClassGenerator.java:119)
	at org.easymock.cglib.core.AbstractClassGenerator.create(AbstractClassGenerator.java:294)
	at org.easymock.cglib.proxy.Enhancer.createHelper(Enhancer.java:480)
	at org.easymock.cglib.proxy.Enhancer.createClass(Enhancer.java:337)
	at org.easymock.internal.ClassProxyFactory.createProxy(ClassProxyFactory.java:175)
	at org.easymock.internal.MocksControl.createMock(MocksControl.java:107)
	at org.easymock.internal.MocksControl.createMock(MocksControl.java:80)
	at org.powermock.api.easymock.PowerMock.doCreateMock(PowerMock.java:2023)
	at org.powermock.api.easymock.PowerMock.doMock(PowerMock.java:1970)
	at org.powermock.api.easymock.PowerMock.createMock(PowerMock.java:84)
	at org.powermock.api.extension.listener.AnnotationMockCreatorFactory$1.createMockInstance(AnnotationMockCreatorFactory.java:34)
	at org.powermock.api.extension.listener.EasyMockAnnotationSupport.createMock(EasyMockAnnotationSupport.java:109)
	at org.powermock.api.extension.listener.EasyMockAnnotationSupport.injectMock(EasyMockAnnotationSupport.java:98)
	at org.powermock.api.extension.listener.EasyMockAnnotationSupport.inject(EasyMockAnnotationSupport.java:90)
	at org.powermock.api.extension.listener.EasyMockAnnotationSupport.injectDefaultMocks(EasyMockAnnotationSupport.java:70)
	at org.powermock.api.extension.listener.EasyMockAnnotationSupport.injectMocks(EasyMockAnnotationSupport.java:56)
	at org.powermock.api.extension.listener.AnnotationEnabler.beforeTestMethod(AnnotationEnabler.java:76)
	at org.powermock.tests.utils.impl.PowerMockTestNotifierImpl.notifyBeforeTestMethod(PowerMockTestNotifierImpl.java:82)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.executeTest(PowerMockJUnit44RunnerDelegateImpl.java:308)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTestInSuper(PowerMockJUnit47RunnerDelegateImpl.java:131)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.access$100(PowerMockJUnit47RunnerDelegateImpl.java:59)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner$TestExecutorStatement.evaluate(PowerMockJUnit47RunnerDelegateImpl.java:147)
	at org.junit.rules.TestWatcher$1.evaluate(TestWatcher.java:61)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.evaluateStatement(PowerMockJUnit47RunnerDelegateImpl.java:107)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit47RunnerDelegateImpl$PowerMockJUnit47MethodRunner.executeTest(PowerMockJUnit47RunnerDelegateImpl.java:82)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$PowerMockJUnit44MethodRunner.runBeforesThenTestThenAfters(PowerMockJUnit44RunnerDelegateImpl.java:298)
	at org.junit.internal.runners.MethodRoadie.runTest(MethodRoadie.java:87)
	at org.junit.internal.runners.MethodRoadie.run(MethodRoadie.java:50)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.invokeTestMethod(PowerMockJUnit44RunnerDelegateImpl.java:218)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.runMethods(PowerMockJUnit44RunnerDelegateImpl.java:160)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl$1.run(PowerMockJUnit44RunnerDelegateImpl.java:134)
	at org.junit.internal.runners.ClassRoadie.runUnprotected(ClassRoadie.java:34)
	at org.junit.internal.runners.ClassRoadie.runProtected(ClassRoadie.java:44)
	at org.powermock.modules.junit4.internal.impl.PowerMockJUnit44RunnerDelegateImpl.run(PowerMockJUnit44RunnerDelegateImpl.java:136)
	at org.powermock.modules.junit4.common.internal.impl.JUnit4TestSuiteChunkerImpl.run(JUnit4TestSuiteChunkerImpl.java:117)
	at org.powermock.modules.junit4.common.internal.impl.AbstractCommonPowerMockRunner.run(AbstractCommonPowerMockRunner.java:57)
	at org.powermock.modules.junit4.PowerMockRunner.run(PowerMockRunner.java:59)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.runTestClass(JUnitTestClassExecutor.java:110)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:58)
	at org.gradle.api.internal.tasks.testing.junit.JUnitTestClassExecutor.execute(JUnitTestClassExecutor.java:38)
	at org.gradle.api.internal.tasks.testing.junit.AbstractJUnitTestClassProcessor.processTestClass(AbstractJUnitTestClassProcessor.java:62)
	at org.gradle.api.internal.tasks.testing.SuiteTestClassProcessor.processTestClass(SuiteTestClassProcessor.java:51)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:36)
	at org.gradle.internal.dispatch.ReflectionDispatch.dispatch(ReflectionDispatch.java:24)
	at org.gradle.internal.dispatch.ContextClassLoaderDispatch.dispatch(ContextClassLoaderDispatch.java:33)
	at org.gradle.internal.dispatch.ProxyDispatchAdapter$DispatchingInvocationHandler.invoke(ProxyDispatchAdapter.java:94)
	at com.sun.proxy.$Proxy2.processTestClass(Unknown Source)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker$2.run(TestWorker.java:176)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.executeAndMaintainThreadName(TestWorker.java:129)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:100)
	at org.gradle.api.internal.tasks.testing.worker.TestWorker.execute(TestWorker.java:60)
	at org.gradle.process.internal.worker.child.ActionExecutionWorker.execute(ActionExecutionWorker.java:56)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:133)
	at org.gradle.process.internal.worker.child.SystemApplicationClassLoaderWorker.call(SystemApplicationClassLoaderWorker.java:71)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.run(GradleWorkerMain.java:69)
	at worker.org.gradle.process.internal.worker.GradleWorkerMain.main(GradleWorkerMain.java:74)
```"
786045157,7898,dongjinleekr,2022-01-17T14:14:41Z,"Got it. I will update the PR to generate the context name automatically, like `LogCaptureContext-XXXXXX`."
786067964,7898,dongjinleekr,2022-01-17T14:41:52Z,"Oh yes, initially `KafkaStreamsTest` needed `@PowerMockIgnore({""javax.management.*"", ""org.apache.log4j.*""})` like `WorkerSourceTaskTest` but, it does not need it anymore. It would be better to remove this annotation."
788006560,7898,viktorsomogyi,2022-01-19T17:58:25Z,"Why don't you change this (and all the similar cases) to the log4j2 property? If I get it right then after the upgrade `-Dlog4j.configuration` won't work anyway so while I see the value of notifying the user, I think we should just use the log4j2 property outright. Or is there a backward compatibity, so does `log4j.configuration` work under log4j2?"
788034050,7898,rgoers,2022-01-19T18:34:38Z,"Yes, if you set log4j.configuration to reference a log4j.xml (Log4j 1 configuration) and you have log4j-1.2-api on the classpath then Log4j 2 will process the log4j 1 configuration. We improve this support with every release. Substantial improvements will be in 2.17.2 which should come out by the end of the month."
788207067,7898,dongjinleekr,2022-01-19T22:42:42Z,@rgoers Thanks for the clarification. It seems like we also have to upgrade to log4j 2.17.2 soon.
793571163,7898,dongjinleekr,2022-01-27T12:48:30Z,"For `Level#toLevel` semantics, see:

- [`Level` (log4j 1.x)](https://logging.apache.org/log4j/1.2/apidocs/org/apache/log4j/Level.html)
- [`Level` (log4j 1-2 compatibility module)](https://logging.apache.org/log4j/2.x/log4j-1.2-api/apidocs/org/apache/log4j/Level.html)
- [`Level` (log4j 2.x)](https://logging.apache.org/log4j/2.x/log4j-api/apidocs/org/apache/logging/log4j/Level.html)

As you can see here, all of above follows the same semantics - they fallback to `DEBUG` with unknown level."
794332702,7898,dongjinleekr,2022-01-28T09:24:29Z,"FYI: [KIP-817: Fix inconsistency in dynamic application log levels](https://cwiki.apache.org/confluence/display/KAFKA/KIP-817%3A+Fix+inconsistency+in+dynamic+application+log+levels)

cc/ @mimaison @stanislavkozlovski"
797627738,7898,mimaison,2022-02-02T13:53:12Z,nit: Can we get rid of the extra spaces and have `name=value` for all these lines like in the other files?
803805924,7898,mimaison,2022-02-10T15:36:58Z,Is the plan to remove this and use the log4j2 file by default in the next major release?
803816355,7898,mimaison,2022-02-10T15:46:17Z,nit: this is not aligned
815384279,7898,showuon,2022-02-27T03:54:18Z,"nit: can be simplified to:
```java
logger = found.orElseThrow(() -> new NotFoundException(""Logger "" + loggerName + "" not found.""));`
return Response.ok(effectiveLevelToMap(logger)).build();
```"
815400322,7898,showuon,2022-02-27T07:36:10Z,+1
815400528,7898,showuon,2022-02-27T07:38:25Z,"Is the additional ending ""space"" expected? And why?"
815400630,7898,showuon,2022-02-27T07:39:32Z,Is the catch block expected? 
815401329,7898,showuon,2022-02-27T07:47:35Z,nice refactor
815401515,7898,showuon,2022-02-27T07:49:17Z,"again, why additional ending space?"
815401739,7898,showuon,2022-02-27T07:52:04Z,Why did we need these 2 supplier? I didn't see them get used in this test.
815402457,7898,showuon,2022-02-27T07:59:29Z,"I'm thinking that we should add a clear javadoc in `LogCaptureContext`, to explain when we should add `setLatch`, and when we should do `await`. I can see sometimes we don't set latch, but sometimes we set, and sometimes we do `await`, but sometimes not. Could you help other developer with it? "
815403276,7898,showuon,2022-02-27T08:07:11Z,"Since we only care about `warn` level in this test, we could create `KTableSource.class` with `warn` log as before, right?"
815403282,7898,showuon,2022-02-27T08:07:20Z,ditto
815403425,7898,showuon,2022-02-27T08:08:34Z,Any reason why we change the expected log message here?
815404230,7898,showuon,2022-02-27T08:16:29Z,Why can't we verify whole messages?
815404252,7898,showuon,2022-02-27T08:16:46Z,ditto
815404272,7898,showuon,2022-02-27T08:16:53Z,ditto
815404434,7898,showuon,2022-02-27T08:18:21Z,nit: could we create log with warn only?
815404547,7898,showuon,2022-02-27T08:19:35Z,+1
815404639,7898,showuon,2022-02-27T08:20:46Z,"And, also please add docs for what latch size should be set."
815407543,7898,showuon,2022-02-27T08:47:35Z,"Why did we remove `finally` block for `rocksDBStore.close()`? Is there possible that we have resource leak here? 
"
815407744,7898,showuon,2022-02-27T08:50:03Z,ditto: potential resource leak?
829746139,7898,dongjinleekr,2022-03-18T07:22:54Z,"Because of `streams/src/test/resources/log4j2.properties`:

```
appender.console.layout.pattern=[%d] %p %m (%c:%L)%n
```"
829748108,7898,dongjinleekr,2022-03-18T07:26:53Z,No. This is a debris from the old code. Removed.
829777640,7898,dongjinleekr,2022-03-18T08:19:44Z,Documentaton on `LogCaptureContext` added.
830011584,7898,dongjinleekr,2022-03-18T13:42:48Z,The try block with `LogCaptureAppender` resource was removed; That's the reason.
461887873,9039,ableegoldman,2020-07-28T21:23:12Z,"Not sure if you did this or your IDE did it automatically, but nice  "
462622608,9039,ableegoldman,2020-07-29T22:18:47Z,"```suggestion
     * Create a new {@link TimeWindowedCogroupedKStream} instance that can be used to perform sliding
```"
462625072,9039,ableegoldman,2020-07-29T22:24:57Z,nit: call this `timeDifferenceMs` to be in sync with `graceMs`. Also it can be private
462626773,9039,ableegoldman,2020-07-29T22:29:17Z,nit: also rename the method `timeDifferenceMs` to be consistent with `gracePeriodMs`
462627186,9039,ableegoldman,2020-07-29T22:30:26Z,I think we can remove this suppression (and all the ones below)
462627276,9039,ableegoldman,2020-07-29T22:30:39Z,nit: extra space after `return` 
462640050,9039,ableegoldman,2020-07-29T23:06:31Z,"Seems like this should have also had a check for `sessionWindows != null`, right? Can we add that as well?"
462646319,9039,ableegoldman,2020-07-29T23:26:36Z,nit: alignment is off by one on the parameters
462647029,9039,ableegoldman,2020-07-29T23:29:01Z,"I think it's ok to skip this; since it's a new operator, there's no old topology to be compatible with"
462647656,9039,ableegoldman,2020-07-29T23:30:52Z,I wonder why we have to do this for `count` but not for `aggregate` and `reduce`? Is this intentional or an oversight? cc @mjsax @vvcephei @guozhangwang 
462653088,9039,ableegoldman,2020-07-29T23:48:39Z,"Can we add an `else` here with `builder.withCachingDisabled()`? It doesn't make a difference logically, it just seems easier to understand (again, also in `SlidingWindowedCogroupedKStreamImpl`)"
462653237,9039,ableegoldman,2020-07-29T23:49:12Z,You should be able to remove this suppression and comment (here and in `SlidingWindowedCogroupedKStreamImpl`)
462653505,9039,ableegoldman,2020-07-29T23:50:01Z,let's just remove this comment since it's the only style retention here (also in `SlidingWindowedCogroupedKStreamImpl`)
462653753,9039,ableegoldman,2020-07-29T23:50:47Z,We can remove this 
462655858,9039,ableegoldman,2020-07-29T23:58:03Z,"I was wondering what this method is actually used for so I checked out the callers of `KStreamWindowAggregate#windows`. There's a method called `extractGracePeriod` in `GraphGraceSearchUtil` where we might actually need to make a small addition to include the new sliding window processor.

I think it's for Suppression, which needs to figure out the grace period of the upstream operator since grace period doesn't get passed in directly to `suppress`"
462658903,9039,ableegoldman,2020-07-30T00:08:03Z,"Do you think we actually need to enforce that the retention period be a little longer for sliding windows? I was just thinking that since the range scan starts at `timestamp - 2 * windows.timeDifference()`, maybe we should actually enforce that the retention period be >= `2 * timeDifference + gracePeriod` in case we need to get the aggregate value from some older window that has technically expired. 
Haven't checked the math so I'm not sure that's the correct value exactly, but it seems like it might need to be a little bigger. Any thoughts?"
462660320,9039,ableegoldman,2020-07-30T00:13:10Z,"In general it's better to use a more descriptive variable name than a shorted one with a comment. It's not always possible to describe a variable exactly in a reasonable length, but I think in this case we can say `curLeftWindowAlreadyExists` or `curLeftWindowAlreadyCreated` or something

Might be better to use `AlreadyCreated` when we're specifically talking about whether or not a window already exists in the window store, and can use `Exists` when we're talking about whether a window is possible regardless of whether it currently has been created or not"
462661922,9039,ableegoldman,2020-07-30T00:19:14Z,"Does that make sense? In particular I feel like we're using `Exists` to mean one thing for `left/RightWindowExists`, and then we mean another thing entirely in `prevRightWindowExists`. ie `prevRightWinAlreadyCreated` is more similar to what we mean by the `left/RightWindowExists` variables"
462662132,9039,ableegoldman,2020-07-30T00:19:57Z,this comment doesn't seem quite correct
462662498,9039,ableegoldman,2020-07-30T00:21:20Z,Can we also name this variable a bit more clearly instead of the comment? Like `foundClosestStartTimeWindow` or something. Same with `foundFirstEndTime`
462663297,9039,ableegoldman,2020-07-30T00:24:13Z,maybe add a comment saying that this condition will only be hit on the very first record. Or it might be reasonable to pull this one condition out of the loop and just handle it before entering the loop
462663911,9039,ableegoldman,2020-07-30T00:26:22Z,"Actually maybe `foundRight/LeftWindowAggregate` would be good, since that's what ""the window with the closest start/end time to the record"" actually means to us"
462667387,9039,ableegoldman,2020-07-30T00:38:53Z,"Should this be inside the `if (!foundFirst)` condition above? We only want to save the aggregate of the first window we find with a start time less than the timestamp right?

Also, I think we might need to check that the max timestamp of this window is greater than the current record's timestamp. If not, then the right window will be empty.
For example, we have a record A at 10 and a record B at 11 and then process a record at 15. Obviously, the new right window will be empty. But the first window we'll find with a start time less than 15 will be [11, 21] with agg B."
462667988,9039,ableegoldman,2020-07-30T00:40:56Z,nit: put each parameter on its own line
462669250,9039,ableegoldman,2020-07-30T00:45:20Z,"Seems like we're aggregating with the new value twice; we call `aggregator.apply` once in this if/else branch but then also call it again in `putAndForward`, right? "
462677427,9039,ableegoldman,2020-07-30T01:16:27Z,Can we add a comment to clarify that we're checking whether it's a left window because that tells us there was a record at this window's end time
462677774,9039,ableegoldman,2020-07-30T01:17:55Z,"I feel like I'm just way overthinking this, but I keep getting these variables confused. Maybe we could call this guy `prevRightWindowCanExist`? Does that seem to get at its underlying purpose?"
462678777,9039,ableegoldman,2020-07-30T01:21:38Z,Can we remove this and just `break` out of the loop immediately at the end of the `isLeftWindow` condition block? 
462679991,9039,ableegoldman,2020-07-30T01:26:22Z,"I think we need to pass in the new maximum window timestamp here, not the window start time"
462682266,9039,ableegoldman,2020-07-30T01:32:45Z,Ok I may have lost the trail of logic here...are we just checking `prevRightWinExists` as an indicator of whether we actually found any records to the left of our record within range? Could/should we check `foundFirstEndTime` instead?
462682718,9039,ableegoldman,2020-07-30T01:34:10Z,"Since it's a left window, the max timestamp should always be `timestamp`, right? "
462683258,9039,ableegoldman,2020-07-30T01:36:00Z,"Can we put this condition into a method and give it a clear name to describe what this means? eg

```
private boolean rightWindowHasNotBeenCreatedAndIsNonEmpty(..) {
    return !rightWindowExists && (foundLeftFirst || prevRightWinAlreadyCreated)
}
```"
463026843,9039,lct45,2020-07-30T14:13:34Z,"The original just had the check for `sessionMerger != null`, are there scenarios where the sessionMerger would be null but the sessionWindows wouldn't? I did think it was kind of inconsistent to check 'sessionMerger' just that one time and check 'sessionWindows' the other times so maybe it was a mistake"
463036048,9039,lct45,2020-07-30T14:26:20Z,done!
463039985,9039,lct45,2020-07-30T14:31:32Z,"That's a good point. I think that `>= 2* timeDifference + gracePeriod` makes sense. Adding gracePeriod is just to help with out-of-order records, right? Since for a normal record we won't need anything beyond 2*timeDifference"
463046306,9039,lct45,2020-07-30T14:40:00Z,"Yeah this makes sense, the boolean naming has definitely been a struggle. I think it's clearer actually if I change `prevRightWinExists` to `prevRightWinPossible` since that's what we're saying. I agree that for `leftWinExists` and `rightWinExists`, `alreadyCreated` makes more sense."
463047237,9039,lct45,2020-07-30T14:41:16Z,"Yeah that's much clearer, and differentiates between the other bools better"
463056420,9039,lct45,2020-07-30T14:53:37Z,"I'm not sure if it can be moved out of the loop unless we also move a check for if the first is a window whose aggregate we need to update, which is easy to do but gets back to the redundant code that the algorithm had before having one while loop. I did update the comment though"
463063406,9039,lct45,2020-07-30T15:03:02Z,"Yeah I think you're right that the aggregate should be in the `if()`, good catch.

We could check max timestamp, but this scenario should be covered already. Before we create a right window we do the boolean checks and since any in-order-record won't have either a `foundLeftWinFirst` or the `prevRightWinAlreadyCreated` and one of them needs to be true for the right window to get created after the `while()`.

I haven't fully thought through checking with maxTimestamp but it seems like that would work, if that way seems clearer I can alter the algorithm and run through the examples to make sure that covers everything"
463073842,9039,lct45,2020-07-30T15:17:48Z,"Aha, I thought there was a scenario where the `putAndForward` function wouldn't be so simple. Yeah you're right, I updated it so the value of the record is only added in `putAndForward`"
463076565,9039,lct45,2020-07-30T15:21:23Z,"updated above! changed to `prevRightWindowPossible` , lmk if that still seems confusing. I definitely kept getting them all mixed up so I think this will help"
463077196,9039,lct45,2020-07-30T15:22:21Z,100%
463078058,9039,lct45,2020-07-30T15:23:36Z,good catch
463083346,9039,lct45,2020-07-30T15:31:00Z,"Yeah that's what `prevRightWinExists` is doing right now, we could store the full `valueAndTimestamp` for `foundFirstEndTime` and check to see if the max timestamp is within the range of recordTime-timeDifference"
463083821,9039,lct45,2020-07-30T15:31:38Z,yes!
463088582,9039,lct45,2020-07-30T15:38:57Z,"changed to
`                if (leftWinAgg.timestamp() < timestamp && 
                      leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()) {
                    valueAndTime = ValueAndTimestamp.make(leftWinAgg.value(), timestamp);
                } else {
                    //left window just contains the current record
                    valueAndTime = ValueAndTimestamp.make(initializer.apply(), timestamp);
                }`"
463091437,9039,lct45,2020-07-30T15:42:56Z,"To go with your above comment about the maxTimestamp, I changed this to be `if (!rightWinAlreadyCreated && rightWinAgg.timestamp() > timestamp)` is this clearer or should it still be in a new method?"
463092579,9039,lct45,2020-07-30T15:44:41Z,"Changing both of the `if()` for creating new windows cut down like half of the booleans too, which I think is good"
464728426,9039,mjsax,2020-08-04T00:19:22Z,"The build should actually fail on wildcard imports... Do we have some checkstyle gaps? @lct45 can you maybe look into that (if not, also ok)."
464728901,9039,mjsax,2020-08-04T00:21:03Z,This PR is rather larger. Would it maybe make sense to split it into 2 and add co-group in it's own PR?
464729126,9039,mjsax,2020-08-04T00:21:44Z,nit: double `/**`
464729311,9039,mjsax,2020-08-04T00:22:28Z,"nit: if you want to have a new paragraph, you need to insert `<p>` tag -- otherwise, the empty line is just ignored all it's going to be one paragraph. -- If you don't want a paragraph, please remove the empty line."
464729348,9039,mjsax,2020-08-04T00:22:35Z,as above.
464729534,9039,mjsax,2020-08-04T00:23:14Z,type `[w]indows`
464729743,9039,mjsax,2020-08-04T00:23:56Z,`and [a] given` ?
464730686,9039,mjsax,2020-08-04T00:26:58Z,"We must use HTML list markup to get bullet points rendered, ie, `<ul>` and `<li>` (cf https://www.w3schools.com/html/html_lists.asp)

"
464731092,9039,mjsax,2020-08-04T00:28:25Z,"`are processed` -> sounds like processing time semantics; maybe better `occur in the stream (i.e., event timestamps)`"
464731554,9039,mjsax,2020-08-04T00:29:58Z,Reference to `CogroupedKStream` is missing
464731762,9039,mjsax,2020-08-04T00:30:46Z,as above (won't comment on this again) -- please address throughput the whole PR.
464732204,9039,mjsax,2020-08-04T00:32:24Z,"`timeDifference (timeDifference)` (redundant) should be `time difference (timeDifference)`

`must be larger than zero.` -> `must not be negative.`"
464732462,9039,mjsax,2020-08-04T00:33:30Z,I guess a `timeDifference` of zero should be allowed to define a sliding window of size `1ms`
464732572,9039,mjsax,2020-08-04T00:33:53Z,as above -> should be `timeDifferenceMs < 0`
464733014,9039,mjsax,2020-08-04T00:35:30Z,"For consistency: `Grace period (grace) must` ? Frankly, I am not sure if we need to have ""natural language"" and the parameter name in those error messages -- also above. But we should do it in a consistent manner IMHO."
464734041,9039,mjsax,2020-08-04T00:39:23Z,I agree with Sophie that his check seems a little weird. We should check that either both (sessionWindows and sessionMerger) are null or not null.
464735173,9039,mjsax,2020-08-04T00:43:50Z,"I think we should do this check in `init` and use a `Runnable` that we just call blindly (ie, depending on the check, we instantiate the one or other `Runnable` and each `Runnable` implements a different algorithm."
464735252,9039,mjsax,2020-08-04T00:44:11Z,Why do we suppress instead of fix the issue? (or add an exception to the `suppress.xml` file if we really need it)
464736052,9039,mjsax,2020-08-04T00:47:16Z,"I think we should also drop if `value == null` ? (It seem this `null` check is missing in the existing time/session-window aggregate processors, too)"
464736550,9039,mjsax,2020-08-04T00:49:24Z,"Maybe we have the same issue in other processors, too (we might even have a ticket for it?) but won't we need to preserve `observedStreamTime` across restarts? It's transient atm... (Just want to confirm -- maybe it's ok as other processor do it the same way and we need to fix if for all of them at once?)"
464736969,9039,mjsax,2020-08-04T00:51:12Z,The comment seems redundant -- it just says exactly what the next line of code says.
464737602,9039,mjsax,2020-08-04T00:53:54Z,"No need to pass in an `Instant` -- we should just pass in the `long` directly.

It might not be clear from the type hierarchy, but the overloads that accept `long` are only deprecated for the `ReadOnlyXxx` store, but are still available on the ""read/write"" stores to avoid unnecessary runtime overhead."
464738212,9039,mjsax,2020-08-04T00:56:11Z,Why `+ 1` ?
464738932,9039,mjsax,2020-08-04T00:59:18Z,"as above. (also, this code seems to be duplicated; we should move it into `process()` before we call the the actual `processReverse` or `processInOrder` methods."
464739084,9039,mjsax,2020-08-04T00:59:53Z,as above
464739153,9039,mjsax,2020-08-04T01:00:07Z,nit: move `key` to the next line
464739977,9039,mjsax,2020-08-04T01:03:19Z,why `* 2` ?
464740108,9039,mjsax,2020-08-04T01:03:51Z,why `* 2` ?
464740325,9039,mjsax,2020-08-04T01:04:45Z,Why this?
464740712,9039,mjsax,2020-08-04T01:06:18Z,"we should not use this annotation (even if we still have code that used it... we are working on migrating test away lazily). We should instead use `assertThrows` and also verify the exception error message.

Same below."
464740960,9039,mjsax,2020-08-04T01:07:06Z,"Beside the fact, that zero should be valid IMHO, what do we gain by testing `0` and `-1` ?"
464741103,9039,mjsax,2020-08-04T01:07:51Z,This line seems to be unnecessary for this test?
464741202,9039,mjsax,2020-08-04T01:08:16Z,This is also a pattern we try to move off. Use `assertThrows` instead.
464741688,9039,mjsax,2020-08-04T01:10:08Z,"Why do we need three tests? If you want to ""randomize"" it, maybe just use `Random` to generate `difference` and `grace` input instead of hard coding them?"
464741801,9039,mjsax,2020-08-04T01:10:33Z,As above.
464741983,9039,mjsax,2020-08-04T01:11:13Z,What is the difference between `verifyInEquality` and `assertNotEquals` ?
464742330,9039,mjsax,2020-08-04T01:12:30Z,"This should be two test:
 - `shouldNotBeEqualForDifferentTimeDifference`
 - `shouldNotBeEqualForDifferentGracePeriod`"
464744504,9039,mjsax,2020-08-04T01:20:48Z,Why is `endTime = Long.MAX_VALUE`? Should it not be `firstBatchTimestamp` ?
464744963,9039,mjsax,2020-08-04T01:22:32Z,"`firstBatchLeftWindow` -> `firstBatchLeftWindowStart`
Maybe also introduce `firstBatchLeftWindowEnd = firstBatchTimestamp`"
464746013,9039,mjsax,2020-08-04T01:26:26Z,"Maybe add comment to clarify which input should trigger which output:
```
// process A @ 2000ms
new KeyValueTimestamp<>(new Windowed<>(""A"", new TimeWindow(firstBatchLeftWindow, Long.MAX_VALUE)), ""A"", firstBatchTimestamp),
// process A @ 2500ms
new KeyValueTimestamp<>(new Windowed<>(""A"", new TimeWindow(firstBatchRightWindow, Long.MAX_VALUE)), ""A"", firstBatchTimestamp),
new KeyValueTimestamp<>(new Windowed<>(""A"", new TimeWindow(secondBatchLeftWindow, Long.MAX_VALUE)), ""A:A"", secondBatchTimestamp),
// process A @ 2900ms
...
```
"
464746981,9039,mjsax,2020-08-04T01:30:05Z,"We should add a fourth batch with ts like 10K to get the windows when the second batch drops outs, too."
465073439,9039,lct45,2020-08-04T14:02:56Z,"Would checking both be redundant? It looks like the method that ultimately calls this one will check that sessionMerger is not null for session windows, so I think either both of these will be null or neither will be null"
465079294,9039,lct45,2020-08-04T14:10:57Z,"We want to be able to find the furthest window for which we can create a corresponding right window, so for any record the furthest window we will ever need will start at `timestamp - 2 * timeDifference`, but we will need to have these around to calculate new windows, hence the longer retention time."
465094963,9039,lct45,2020-08-04T14:30:49Z,"Because the windows are sorted, the windows created by each record aren't consecutive, so I added comments describing each window, but only did it for A since all the other keys are processed the exact same way. Sample comment: `// A @ secondBatchTimestamp left window created when A @ secondBatchTimestamp processed`"
465097558,9039,mjsax,2020-08-04T14:34:22Z,"Well, yes and no. We can follow two strategies: (1) we rely on the user to only set all `null` (for the non-windowed aggregation case) or either one of `windows`, `slidingWindow`, or `sessionWindow+sessionMerger` and we don't do any verification if the method is called correctly or not. However, for this case, we don't need to do any redundant not-null check and we could just write:
```
if (windows != null) {
  ...
} else if (slidingWindow != null) {
  ...
} else if (sessionWindow != null) { // we blindly assume that `sessionMerger` is not-null, too, for this case
 ...
} else { // all `null`: non-winowed aggregation
  ...
}
```

Or, (2) we do not ""trust"" the caller and do a proper check that the provided arguments make sense. And the existing code already has such a safe guard and does checks that not multiple windows are passed in and throws an `IllegalArgumentException` if the caller makes a mistake. I personally prefer to have a safe guard (especially on the non-hot code path) as it may prevent bugs. However, the current check is not complete, as it does not verify that `sessionMerger` must be not-null when `sessionWindows` is not-null; this may lead to a potentially cryptic `NullPointerException` later that is harder to understand. If we do the check and throw a proper `IllegalArgumentException(""sessionMerger cannot be null for sessionWindows"")` and `IllegalArgumentException(""Unexpected sessionMerger parameter: should be null because sessionWindows is null"");` help to identify the issue quickly."
465099913,9039,mjsax,2020-08-04T14:37:23Z,"Ack. Makes sense. Might be worth to add a comment why we need an ""unexpected"" large retention time."
465101059,9039,mjsax,2020-08-04T14:39:00Z,@lct45 With regard to above: the error message to does align to the required minimum retention time. It says `must be no smaller than its window time difference plus the grace period.`...
465111665,9039,lct45,2020-08-04T14:53:23Z,"Good catch, will do"
465191505,9039,lct45,2020-08-04T16:51:54Z,"There seems to be a bug in `TimeWindowedDeserializer` related to [this ticket](https://issues.apache.org/jira/browse/KAFKA-4468) that ends up setting the windowSize to `Long.MAX_VALUE`. For the purposes of testing, I don't think having it as the max value is totally awful (just somewhat awful) and the window end calculations are all tested in a different set of tests done through topology driver. I'll make a ticket for this bug and try to get it fixed when I'm done with testing"
465193735,9039,lct45,2020-08-04T16:55:32Z,"I think just confirming that the correct error will be thrown when someone sets a `timeDifference` we don't want. I'll update all the `windowSize` to be `timeDifference` and I agree, no need to check that it isn't 0"
465194845,9039,lct45,2020-08-04T16:57:19Z,"Re-examining the test, it looks like it does the same thing as `gracePeriodMustNotBeNegative()` so I think the test can be removed entirely"
465205088,9039,lct45,2020-08-04T17:15:08Z,"Whoops, not on purpose. Thanks for the check"
465219522,9039,lct45,2020-08-04T17:40:13Z,"To clarify, are you wanting to add records that would fall after the third batch _outside_ of all the existing windows, or so that it will fall into the third batch's windows but not the second batch's windows?"
465334577,9039,lct45,2020-08-04T21:15:01Z,"We could, but it would only pull out 3ish classes and not very many lines, so I don't think it would make this PR feel much smaller"
465336976,9039,lct45,2020-08-04T21:20:12Z,So we can check to see if the record that is being processed has an already existing right window (that would start at timestamp+1) without doing another call to the store
465339322,9039,lct45,2020-08-04T21:25:19Z,"fixed for both
"
465348679,9039,lct45,2020-08-04T21:46:37Z,"They look to be fairly similar, and it seems like the tests use both consistently. `verifyInEquality` seems to be more thorough, and to be consistent with the above `equalsAndHashcodeShouldBeValidForPositiveCases` I think I'll use `verifyInEquality` for this test, unless someone has an objection"
465369865,9039,ableegoldman,2020-08-04T22:40:42Z,"@mjsax why do we have a single method that accepts all three window types and then checks them all individually to enforce that only one type of window is actually ""set""? Seems like we could enforce this implicitly by having a separate method for time, session, and non-windowed aggregates and then just calling the correct signature. ie `SessionWindowedCogroupedKStreamImpl` calls `build(...SessionWindows, SessionMerger) and so on.

Maybe I'm missing something here because I wasn't following the cogroup KIP that closely, but is this even exposed to the user in any way? My understanding is that there's no way for this check to be violated by any kind of user input, because this method is only ever called directly by Streams internal code with `null` hardcoded for the unused window types. I think it's more of an internal consistency check for Streams than an input validation for the user (and it seems unnecessary: see above)"
465371635,9039,ableegoldman,2020-08-04T22:45:29Z,"Seems like the cogroup stuff makes up a pretty small amount of the overall PR, but up to Leah"
465374004,9039,ableegoldman,2020-08-04T22:52:21Z,"We definitely have the issue in all processors right now lol. It's not any more of a problem for this sliding windows algorithm as for any other operator that defines a grace period, at least. We might end up not dropping a late record that we should have; for sliding windows we'd get one extra window (with this record at the window end) whereas for a hopping/tumbling window we'd get N extra windows (however many overlaps there are)"
465379071,9039,ableegoldman,2020-08-04T23:07:35Z,Not saying all this needs to be cleaned up in this PR. If we check one thing (eg `sessionMerger`) then we should check everything (eg `sessionMerged != null && sessionWindows != null`). We can decide whether we really need to check anything as followup 
465379566,9039,ableegoldman,2020-08-04T23:09:14Z,Maybe we can add this answer as a comment in the code for future readers
465380753,9039,ableegoldman,2020-08-04T23:13:03Z,"Just to clarify, we don't get any additional output when the stream time is advanced and older windows drop out of the grace period. We've already forwarded their final state when the last record to update that window was processed. Not sure if that's what you meant by ""get the windows when the batch drops out"" or not?"
465382189,9039,ableegoldman,2020-08-04T23:17:51Z,"Not sure, I think `and given window grace` makes grammatical sense. But either way "
465382663,9039,ableegoldman,2020-08-04T23:19:29Z,"It took me a second to understand the structure of this sentence, can we insert an `and` after the `record's timestamp`?"
465399031,9039,ableegoldman,2020-08-05T00:13:33Z,extra `/*` here
465399853,9039,ableegoldman,2020-08-05T00:16:39Z,Technically this is an `XOR` not an `OR`  
465400403,9039,ableegoldman,2020-08-05T00:18:42Z,"can you revert the line changes here and below? Nothing wrong with them, but the fewer lines/classes changed in the PR, the better"
465401189,9039,ableegoldman,2020-08-05T00:21:40Z,"Think you missed changing this in the cogrouped class, this should be 2*timeDifference right?"
465402473,9039,ableegoldman,2020-08-05T00:26:16Z,I think we need a null check here like we have down in `SlidingWindowedKStreamImpl#materialize` 
465407448,9039,ableegoldman,2020-08-05T00:45:26Z,+1 to add a comment on the extra retention (here and in SlidingWindowedCoGroupedKStreamImpl)
465407638,9039,ableegoldman,2020-08-05T00:46:16Z,Should be `no smaller than twice its window time difference...`
465408876,9039,ableegoldman,2020-08-05T00:50:59Z,"+1 to using a random number instead of multiple lines. If it does happen to fail on a specific random number, we should be sure to print that number for reproducing it later. See TaskAssignorConvergenceTest#runRandomizedScenario for example"
465409330,9039,ableegoldman,2020-08-05T00:52:44Z,"Took me a second to understand this test, ""NegativeCases"" made me think the timeDifference/grace were supposed to be negative. +1 to Matthias's suggestion for naming (and splitting into two tests)"
465412482,9039,ableegoldman,2020-08-05T01:03:59Z,"Can you leave a TODO here to make sure we remember to change this to `reverseFetch`? Seems unlikely we'd forget, but you never know"
465414316,9039,ableegoldman,2020-08-05T01:11:09Z,">  unless we also move a check for if the first is a window whose aggregate we need to update

Which check? I was just thinking that, since the window starting at record.timestamp + 1 is basically a special case, we can just pull it out of the loop completely. We don't have to update anything since the record doesn't fall into this window, right? Basically just before entering the loop we check `if next.key.window().start() == timestamp + 1` and if so set `rightWinAlreadyCreated` and then skip to the next record"
465414863,9039,ableegoldman,2020-08-05T01:13:02Z,"We don't technically need `continue` at the end of each condition, right? "
465416585,9039,ableegoldman,2020-08-05T01:19:27Z,"Awesome! I might still recommend pulling the `rightWinAgg != null && rightWinAgg.timestamp() > timestamp` check out into a method called `rightWindowIsNonEmpty` or something, but it's definitely a lot easier to understand now even without that  "
465418942,9039,ableegoldman,2020-08-05T01:28:01Z,"We need to check `leftWinAgg` for null, right? Also, is it ever possible for `leftWinAgg` to be non-null but not satisfy this condition? Maybe we can just check `leftWinAgg != null` and if so, then assert that `leftWinAgg.timestamp() < timestamp && leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()` is always true (eg throw an `IllegalStateException` if it's not)"
465419531,9039,ableegoldman,2020-08-05T01:30:09Z,Can we remove the `Math.max` thing for now and just drop records that are too early for us to process for now? 
465421190,9039,ableegoldman,2020-08-05T01:36:28Z,"Just a note to other reviewers: we're planning to revisit the issue of ""early"" records later and are just dropping them for now to make the general algorithm easier to review and understand. It needs some special handling for the edge case of records that arrive earlier than the full sliding window due to the inability to store windows with negative start times"
465421265,9039,ableegoldman,2020-08-05T01:36:42Z,nit: put this on one line
465421514,9039,ableegoldman,2020-08-05T01:37:36Z,"```suggestion
            // keep the left type window closest to the record
```"
465421657,9039,ableegoldman,2020-08-05T01:38:18Z,comment doesn't seem to match the query bounds (missing a +1?)
465421880,9039,ableegoldman,2020-08-05T01:39:08Z,Is there any reason this wouldn't just be `window.start + timeDifferenceMs`? 
465424346,9039,ableegoldman,2020-08-05T01:48:06Z,"Same here, let's not pin the start time to 0 for now and just drop the early records"
465425090,9039,ableegoldman,2020-08-05T01:50:32Z,"Yeah especially since we use the same condition for both the forward and reverse case, let's just pull the `rightWinAgg != null && rightWinAgg.timestamp() > timestamp` out into a separate method"
465725011,9039,lct45,2020-08-05T13:26:48Z,"Yeah that works unless the first window isn't the right window, in which case we would need to process it (save as the rightWinAgg, update its aggregate if the current record falls into it, etc) before going to the next record at the top of the while loop. It definitely works and is what we had before, it just makes the code a little less clean."
465751597,9039,lct45,2020-08-05T14:05:28Z,"Hmmmm yeah, I think if there's something in the left window then we will always initialize `leftWinAgg` to something other than null, good catch. We essentially check `leftWinAgg.timestamp() < timestamp` in the while loop, so I don't think that should cause a problem, and `leftWinAgg.timestamp() > timestamp - windows.timeDifferenceMs()` will never be true because that window would be out of range, right? And we only take the first left agg. Long way of saying, I think we can do the null check and nothing else but will know slightly more when we can test it"
465756356,9039,lct45,2020-08-05T14:12:12Z,"Nope, since we aren't storing truncated windows"
466026007,9039,mjsax,2020-08-05T21:54:05Z,The ticket is already fixed. You need to pass in the `windowSize` into the the constructor of `TimeWindowDeserializer` to get rid of the problem.
466028866,9039,mjsax,2020-08-05T22:00:39Z,"I guess testing both cases would be good. Even if testing the former (fall outside of all existing windows) was my original intent.

And thank for comment Sophie: I tend to forget that we should produce all (non-empty) right windows already upfront/eagerly (and not delayed/lazily when stream-time advances beyond window-end time). In any case, it seems to be a good test case to make sure we don't (re-)emit an (unexpected) window if stream-time jumps ahead?"
466029116,9039,mjsax,2020-08-05T22:01:14Z,Ack. Was just a thought.
466032331,9039,mjsax,2020-08-05T22:09:37Z,"We pass in all parameter to sharing to code that creates the `StatefulProcessorNode` -- not sure if it's the best way to structure the code and I am happy to split it up into multiple methods call (as long as we avoid code duplication). And yes, you are right, it's internal and the checks are just for us to avoid programming errors. Users should never be exposed to it. I personally tend to make a lot of mistakes and the more checks we have in place the better IMHO :)

If @lct45 want's she can just do a side cleanup PR to fix it, and rebase this PR after the cleanup PR was merged? Or we do it as follow up. Whatever works best for you."
466032804,9039,mjsax,2020-08-05T22:10:51Z,Thanks for confirming. Let keep this issue out for this PR than.
466033381,9039,mjsax,2020-08-05T22:12:25Z,I am not a native speaker... Don't ask me... Mr.John should know -- he has the proper education for it.
466042505,9039,lct45,2020-08-05T22:38:04Z,"That should be covered in `KStreamSlidingWindowAggregateTest`, which goes through more of the edge cases using the `TopologyTestDriver` which is a little easier to manipulate than this set up"
466050097,9039,ableegoldman,2020-08-05T23:00:38Z,"Well, if the first window isn't the right window then we just wouldn't call `iterator.next` again, right? So we wouldn't have to do anything at all "
466051779,9039,ableegoldman,2020-08-05T23:05:45Z,"Ok cool, just checking. Either `endTime = window.start + timeDifferenceMs` or `endTime = window.end` is fine as long as we're consistent (I guess we only define it in two places, the forward and reverse algorithms?) "
466052825,9039,ableegoldman,2020-08-05T23:08:41Z,"Sounds good. We definitely need the null check just to avoid getting an NPE, but whether we _only_ need the null check is something we should put to the test"
466063583,9039,ableegoldman,2020-08-05T23:42:35Z,"No she's right, this problem is not resolved at all. You can pass in `windowSize` to the constructor for `TimeWindowedDeserializer` all you want but it just gets ignored because the actual deserializer object you instantiate is thrown away. Whether you're reading in records through a Java Consumer or the console consumer (for some reason this test does both), the actual deserializer is always constructed within the consumer based on the configs. There's a config for the windowed inner class which is properly set in `TimeWindowedDeserializer#configure` but no config for the `windowSize` so there's no way to set it at the moment.

tl;dr there's no point in having serde constructors accept parameters, they need to be set through `configure`"
466070317,9039,lct45,2020-08-06T00:05:50Z,Created a ticket for this here: https://issues.apache.org/jira/browse/KAFKA-10366 let me know if the description isn't clear
466425932,9039,lct45,2020-08-06T13:49:17Z,"I'm happy to do a PR! Looking into it now though, `getStatefulProcessorNode` is called by `build`, so I think to really separate it by type we'd need a different `build` _and_ `StatefulProcessorNode`, otherwise we'd be moving the null checks into `build` and then calling the correct `getStatefulProcessorNode`, which does't seem to really fix anything. Thoughts? It's easy to create new `build` functions but I figured this might fall under not avoiding code duplication :)"
466618450,9039,ableegoldman,2020-08-06T18:52:07Z,"I do think we'd need separate `build` methods, since that's where we originally accept multiple windows as arguments (where all but one type is set to null in each caller). But most of `build` doesn't touch the windows arguments so you could probably factor out all the window-independent code into a single method and just have each `build` method call that"
467231847,9039,mjsax,2020-08-07T19:33:46Z,Thanks. I missed the point that this trick to pass in the windowSize only works for KafkaStreams when we pass in `Serdes` object that are used as provided...
467260318,9039,lct45,2020-08-07T20:44:35Z,"Sounds good, I'll change that when I implement the reverse iterator in the next PR"
467296111,9039,ableegoldman,2020-08-07T21:50:49Z,Clearly Kafka Streams is superior to the plain Consumer  
468214782,9039,vvcephei,2020-08-10T22:11:47Z,"Haha, my specialty!

The distillation of this sentence is ""Windows are defined based on a record's timestamp, window size, and window grace period."" I think the meaning is pretty clear, so no need to change anything.

Just to point it out, there's structural ambiguity about whether the sentence is saying ""a record's (timestamp, window size, window grace period)"" (I.e., three properties of the record), or whether there are three top-level things that define the window. The latter was intended. I think actually inserting ""the"" before ""window"" both times would clear it up: ""Windows are defined based on a record's timestamp, the window size, and the window grace period.""

Another note is that because the second item in the list is so long, the structure of the list gets a little lost. It would be better in this case to use the Oxford comma to clearly delineate the boundary between the second and third items.

So, although I think this is fine as-is, if you want me to break out the red pen, I'd say:
```
 * Sliding Windows are defined based on a record's timestamp, the window size based on the given maximum time difference (inclusive) between
 * records in the same window, and the given window grace period.
```"
468216620,9039,ableegoldman,2020-08-10T22:16:48Z,Comment on the reverse case left behind
468217358,9039,ableegoldman,2020-08-10T22:18:45Z,https://github.com/apache/kafka/pull/9157
468217437,9039,ableegoldman,2020-08-10T22:19:00Z,nit: extra line breaks
468217987,9039,ableegoldman,2020-08-10T22:20:29Z,You should check to make sure all of these are still needed. In particular I bet we can get rid of the CogroupedStreamAggregateBuilder suppression once your cleanup PR is merged and this one is rebased
468218540,9039,ableegoldman,2020-08-10T22:22:02Z,I think we usually leave the arguments on the same line as the method declaration (even if that line ends up way too long)
468219113,9039,ableegoldman,2020-08-10T22:23:34Z,"Kind of hard to tell, but is the alignment in this method a bit off?  Might be good to just highlight and auto-indent everything, intellij will take care of any issues if it's configured properly"
468290069,9039,ableegoldman,2020-08-11T02:28:07Z,nit: extra spaces after the `->`
468291563,9039,ableegoldman,2020-08-11T02:34:01Z,"The input is the same for each test so the output is too, right? Maybe we can we pull all the output verification into a single method"
468292711,9039,ableegoldman,2020-08-11T02:38:04Z,"Can we add some tests to verify the other Materialized properties, specifically the retention? You can just pick a single operator (eg `reduce`) and write a test to make sure data is available (only) within the retention period.
 Also, do you think we can write a test to verify that the default retention is as expected when we don't specify it? "
468293244,9039,ableegoldman,2020-08-11T02:40:05Z,"This comment needs to be updated, looks like we do allow a grace period of zero in the code/tests"
468293930,9039,ableegoldman,2020-08-11T02:42:39Z, `assertThrows`  
468294181,9039,ableegoldman,2020-08-11T02:43:41Z,"Awesome, thanks for cleaning up some of these older tests  "
468653951,9039,lct45,2020-08-11T15:07:03Z,"I pulled it out for all except one, because there's one call to `windowStore` that returns a `<Windowed<String>, Long>` and the other calls to `windowStore` return a `<Windowed<String>, String>`"
468690429,9039,lct45,2020-08-11T15:58:35Z,"I'm not sure if I'm just missing something, but it doesn't look like there's a way to check what retention is. I created a test to make sure anything lower than our bound throws an exception, but I can't find anywhere the retention time is exposed for me to check what it's set to"
468796354,9039,lct45,2020-08-11T18:53:42Z,Update: the windows themselves are the same but the value is different for each test
468814368,9039,ableegoldman,2020-08-11T19:27:39Z,"Yeah sorry I should have been more clear, I just meant push some data through and try to query the store to make sure it is/isn't there according to the retention period. You're right, it's not directly exposed anywhere"
471797422,9039,ableegoldman,2020-08-17T21:57:26Z,Do we still need this one after the cleanup you did?
471798256,9039,ableegoldman,2020-08-17T21:59:31Z,"```suggestion
            final Set<Long> windowStartTimes = new HashSet<>();
```
Also I think this set is pretty clearly named, so we probably don't need a comment for it "
471798801,9039,ableegoldman,2020-08-17T22:00:50Z,nit: can we use the full word `Window` in method names at least
471799698,9039,ableegoldman,2020-08-17T22:03:01Z,"```suggestion
    public void shouldDropWindowsOutsideOfRetention() {
```"
471801086,9039,ableegoldman,2020-08-17T22:06:28Z,"```suggestion
        final WindowBytesStoreSupplier storeSupplier = Stores.inMemoryWindowStore(""aggregated"", ofMillis(1200L), ofMillis(100L), false);
```"
471801743,9039,ableegoldman,2020-08-17T22:08:09Z,"nit: you could use the version of `fetch` that just takes a single key instead of a key range, since there's only one key here"
471820115,9039,ableegoldman,2020-08-17T23:01:12Z,"Can we insert one that's like right on the border of the retention period? So if the streamtime at the end is 2,000 then the window cut off is 800 (or start time of 700), and verify that anything starting before 699 is gone and everything after that is there."
471823061,9039,ableegoldman,2020-08-17T23:09:53Z,"For readability, could we mark the final results for each window? We want to make sure all the intermediate results are as expected, but what we really care about is what we got in the end. It would just help to have the critical output easier to find and get oriented in the tests"
471824209,9039,ableegoldman,2020-08-17T23:13:31Z,It might be nice to use different values for each record (at least within the same key). I don't think there are really any edge cases we should worry about when records have the same value so we may as well use a distinct one to make the tests a bit easier to read
471825457,9039,ableegoldman,2020-08-17T23:17:24Z,"I still don't exactly understand why we have a join test in the `KStreamXXWindowAggregateTest`, but thanks for adding it for sliding windows. I'm sure there was a good reason for it, probably long ago"
471827371,9039,ableegoldman,2020-08-17T23:23:35Z,"Sorry that I only just got to looking through this class  . The tests here look good but can we add some more test coverage of possible edge cases? I know we can't test early records until the next PR, but we should probably have more than just the one test of the core functionality.

I know it's really annoying to have to think through all the intermediate output, so maybe you can write a helper method that just grabs the final result of each window in the output? Then we could have a number of tests that go through a larger number of input records without you having to spend all day manually processing them yourself  "
472436400,9039,lct45,2020-08-18T19:41:02Z,We don't!
472440045,9039,lct45,2020-08-18T19:48:20Z,"That `fetch` only returns a `WindowStoreIterator` instead of a `KeyValueIterator`, which I don't think is a huge deal but we wouldn't get the start/end time of the window which is nice to have for the test"
472441802,9039,ableegoldman,2020-08-18T19:51:48Z,"Oh right, forgot that it doesn't have the window times either. Nevermind then"
474966465,9039,ableegoldman,2020-08-21T20:59:10Z,"Can we actually wrap the whole `testProcessorRandomInput` test in the try-catch? Or at least, everything after the initial setup? Would be nice to have the seed in case something weird happens during the processing itself"
474977836,9039,ableegoldman,2020-08-21T21:28:42Z,"Just a minor note, can we order the expected results by window timestamp?"
475919413,9039,ableegoldman,2020-08-24T22:02:05Z,"nit: can you call this something a bit more direct, eg `verifyRandomTestResults` ?"
475922761,9039,ableegoldman,2020-08-24T22:10:32Z,nit: `testAggregateRandomInput` to match up with other test names
475923788,9039,ableegoldman,2020-08-24T22:13:19Z,Can you leave a brief comment here explaining why we're doing something slightly more complicated in the aggregator for this test
478518760,9039,vvcephei,2020-08-27T15:45:49Z,"I'm reviewing this whole PR as-is, so there's no need to do anything now, but @mjsax 's specific suggestion is beside the point. The general feedback is that this PR is too large, which it is. We shoot for under 1K, and it's the PR author's responsibility to figure out the best way to break it up.

This policy isn't just ""reviewers complaining,"" it's an important component of ensuring AK's quality. Long PRs overwhelm any reviewer's cognitive capacity to pay attention to every detail, so oversights are more likely to slip through into the codebase, and once they're there, you're really at the mercy of the testing layers to catch them. When the oversights are very subtle, they wind up getting released and then surface as user-reported bugs. Reviewers can't guarantee to notice every problem, but our capacity to notice problems is inversely proportional to the length of the PR."
478588602,9039,vvcephei,2020-08-27T17:41:25Z,"```suggestion
 *     <li>window {@code [7400;12400]} contains [1,2,3] (created when third record enters the window)</li>
 *     <li>window {@code [8001;13001]} contains [2,3] (created when the first record drops out of the window)</li>
 *     <li>window {@code [9201;14201]} contains [3] (created when the second record drops out of the window)</li>
```"
478616571,9039,vvcephei,2020-08-27T18:32:25Z,"```suggestion
        final String msgPrefixGrace = prepareMillisCheckFailMsgPrefix(grace, ""grace"");
```"
478617147,9039,vvcephei,2020-08-27T18:33:32Z,"```suggestion
                          (Aggregator<? super K, ? super Object, VOut>) aggregator);
```"
478627800,9039,vvcephei,2020-08-27T18:53:31Z,"Is this condition supposed to be checking whether records are ""early"" with respect to now? It looks like it should be:
```suggestion
            if (timestamp < (observedStreamTime - windows.timeDifferenceMs())) {
```"
478664149,9039,vvcephei,2020-08-27T20:03:13Z,minor: this could be declared `final` at the assignment on line 161
478669986,9039,vvcephei,2020-08-27T20:14:32Z,Might not be a bad idea to have an assertion here that the timestamp is actually in the window boundaries.
478671190,9039,vvcephei,2020-08-27T20:16:55Z,"Is it already guaranteed that this window actually contains the current record? It doesn't look like we're checking that `endTime >= timestamp` anywhere, and it seems like the start of the range (`timestamp - 2 * windows.timeDifferenceMs()`) could give back a window that starts and ends before the current record's timestamp."
478702517,9039,vvcephei,2020-08-27T21:20:46Z,"The Achilles heel of implementing new KTable features has historically been that we forgot to test them in a context that required the ValueGetter to work properly, of which Join is a notable use case. I'd actually say it should be required for every KTable operator to have a test where it's the source of a Join. For stateless operators, we should test both with and without a Materialized argument on the operator."
478703452,9039,vvcephei,2020-08-27T21:22:51Z,"I'd normally say we should have a test also to verify we log properly on early records, but you already opened the PR to add early record handling, so we're good."
478704359,9039,vvcephei,2020-08-27T21:24:45Z,Awesome test. Thanks!
478705942,9039,vvcephei,2020-08-27T21:28:18Z,"Aside from join, forgetting to test new operators in front of Suppress has also been an issue. It's great to see this test here!"
478707997,9039,vvcephei,2020-08-27T21:32:52Z,"Coming back to this after completing the review, I'd say the biggest advice I'd share is to avoid whitespace changes and cleanups on the side when the PR is so long already. In fact, for my own super-complex PRs, I tend to go back over the whole diff and back out anything that's not critically important, just to lighten the load on the reviewers.

Cleanups are nice to have, but it's better to keep them in their own PRs or in more trivial ones."
478715664,9039,ableegoldman,2020-08-27T21:50:34Z,"No, the condition is correct. In this context ""early"" just means ""within timeDifferenceMs of the zero timestamp"". We need some special handling to cover this full range of all record timestamps due to the inability to store negative timestamps. This algorithm works correctly for all records outside of this regardless of ""now"""
478716769,9039,ableegoldman,2020-08-27T21:53:00Z,"To be honest, it might not be so bad to just leave things as is and drop early records, since any sensible timestamps are unlikely to be that close to the epoch. But I do believe users may want to use lower timestamps in their unit testing (`1598565116374` is not a very human readable number) and would be surprised to see these records just dropped. "
478717127,9039,ableegoldman,2020-08-27T21:53:52Z,"Aha, so there was a good reason for it  "
478726427,9039,lct45,2020-08-27T22:17:40Z,Is that possible? It's reassigned for every iteration of the `while()`
478728829,9039,lct45,2020-08-27T22:24:19Z,"While the range might give a window that starts and ends before the current record's timestamp, the current record would fall into the right window of the records _within_ those windows.
EX: TimeDifference = 10, record @ 30, range from (10,31). The earliest start time of a window we can have is 10, so the earliest `leftTypeWindow` we can find is from [10,20]. If there's a record at 2, it's right window would be [21,31], which our record @ 30 would fall within. Because this is true for the furthest possible record, it'll be true for the others that we find."
478730796,9039,lct45,2020-08-27T22:29:46Z,"I don't think that would be true all the time, since the current record's right window wouldn't contain the current record and is created through this method. If it helps for clarity, I can add a check that if we're _not_ creating the right window then the timestamp needs to be within the window, and otherwise confirm that we're creating the right window"
478751787,9039,ableegoldman,2020-08-27T23:35:06Z,"I think he means, instead of declaring it once up here and then reassigning it every iteration, we can just do `final KeyValue<> next = iterator.next()` down on line 161. We don't need it outside the loop"
478753568,9039,lct45,2020-08-27T23:41:39Z,"Aha, that makes sense"
478756346,9039,ableegoldman,2020-08-27T23:51:15Z,"Now that you bring it up, that's kind of a weird case for this method, and it's currently handled in a pretty subtle way. For example down on line 233 we are effectively checking for this case, and line 234 just happens to work correctly for it. But it's not at all obvious that we're even handling this case. Can we avoid the ternary operator when setting `newAgg` and `newTimestamp` and just use a normal if/else to explicitly set both of these for the special case? (ie `if (windowStart == timestamp + 1)`...)"
479344784,9039,vvcephei,2020-08-28T14:32:35Z,"Thanks. From the other thread, it sounds like I misunderstood `putAndForward` as adding the `value` to the `window`."
479362665,9039,lct45,2020-08-28T15:01:33Z,"Yeah it's definitely vague, I'll update"
480429410,9039,vvcephei,2020-08-31T22:16:28Z,Thanks for the confirmation! I agree with your thinking.
1655160528,16456,apoorvmittal10,2024-06-26T16:15:11Z,Is it possible to use `DEFAULT_CLIENT_RACK` already defined in Configs?
1655173636,16456,apoorvmittal10,2024-06-26T16:25:12Z,"nit:
```suggestion
     * either as part of shareFetch request or shareAcknowledge request.
```"
1655178863,16456,apoorvmittal10,2024-06-26T16:27:31Z,Query: Can you please help what issue actually occurs and why we need to update the cache i.e. how can the cache eviction be prevented with the cache update?
1655181963,16456,apoorvmittal10,2024-06-26T16:28:54Z,Query: Is it always guranteed that a non-null share session will exist?
1655189309,16456,apoorvmittal10,2024-06-26T16:31:53Z,nit: should we declare the helper methods post `handle` methods? 
1655198343,16456,apoorvmittal10,2024-06-26T16:35:14Z,nit: do we need this or can work with existing `randomBytes`?
1655203506,16456,apoorvmittal10,2024-06-26T16:37:46Z,Where does this mocked version used later? If we don not define the setter of SharPartitionManager then ll it make a difference?
1656904716,16456,chirag-wadhwa5,2024-06-27T10:37:57Z,It should not make a difference. I removed the mock and passed Optional.empty() in setSharePartitionManager method. Did the same in MetadataRequestBenchmark.java as well
1656905232,16456,chirag-wadhwa5,2024-06-27T10:38:27Z,"Didn't realise the existence of randomBytes. Made the amends, thanks for the review"
1660544030,16456,chirag-wadhwa5,2024-07-01T06:40:32Z,"Upon going through the code again, I guess with the new changes it doesn't make sense at all to update the cache here at all. With no asynchronous process happening between the 2 updates, the second update could be removed altogether. I have made the required changes in the most recent commit. Thanks"
1660544678,16456,chirag-wadhwa5,2024-07-01T06:41:12Z,Removed this method in the latest commit. Pls refer to the reply for above comment for further context. Thanks !
1662479127,16456,apoorvmittal10,2024-07-02T13:04:06Z,"Time is already defined in the class, can we please re-use that:
```suggestion
        time,
```"
1662482911,16456,apoorvmittal10,2024-07-02T13:06:44Z,"I don't see `Option` suffix for other optionals in this class:
```suggestion
                val sharePartitionManager: Option[SharePartitionManager],
```"
1662487187,16456,apoorvmittal10,2024-07-02T13:09:35Z,@AndrewJSchofield Should we be relying on this config for `Share Groups`?
1662490711,16456,apoorvmittal10,2024-07-02T13:11:53Z,"Are we introducing this config to AK, I do not see the config in the KIP `group.share.enable`.
cc: @AndrewJSchofield "
1662494555,16456,apoorvmittal10,2024-07-02T13:14:08Z,Can we move this line after checking whether `sharePartitionManagerOption` has a value?
1662497106,16456,apoorvmittal10,2024-07-02T13:15:51Z,"Do we need `()` or can work without them as like elsewhere, can we please be consistent.
```suggestion
    val memberId = shareFetchRequest.data.memberId
```"
1662497519,16456,apoorvmittal10,2024-07-02T13:16:09Z,Same as above and elsewhere.
1662550023,16456,apoorvmittal10,2024-07-02T13:39:36Z,Can we delay the fetch of this when actually it's required later?
1662556294,16456,apoorvmittal10,2024-07-02T13:42:22Z,"It's odd to see that we need to expose `cachedTopicIdPartitionsInShareSession` externally. We should not have the release API to have `List<TopicIdPartition> topicIdPartitions`, only `String groupId, String memberId` should be sufficient. Can we please change the `releaseAcquiredRecords` API to below, and use `cachedTopicIdPartitionsInShareSession` internally only?
cc: @adixitconfluent 

```
public CompletableFuture<Map<TopicIdPartition, ShareAcknowledgeResponseData.PartitionData>> releaseAcquiredRecords(String groupId, String memberId) {
``` "
1662566850,16456,apoorvmittal10,2024-07-02T13:47:01Z,"I hardly see much of `breakable` usage in entire `Kafka` repository. How does the other part of code `breaks` in scala, any idea?"
1662567425,16456,apoorvmittal10,2024-07-02T13:47:13Z,"```suggestion
              break
```"
1662638990,16456,apoorvmittal10,2024-07-02T14:26:48Z,@omkreddy Any advise what's the best way to write such code in scala and Kafka?
1662644808,16456,apoorvmittal10,2024-07-02T14:29:43Z,"Can we declare variables when needed. It's anyways tough to read scala code with `def` inside `def`, declaration of variables without order makes it harder."
1662653794,16456,apoorvmittal10,2024-07-02T14:34:04Z,"I am not sure what does this case means i.e. is this check required? If yes, then what handling is done when `ShareFetchResponse` is not returned by the method call i.e. how it further gets handled?

Shouldn't we complete the request and return right away? "
1662666354,16456,apoorvmittal10,2024-07-02T14:40:39Z,"Why the validation of the request is done later prior initializing context and other processing, shouldn't that be the first step?"
1662673852,16456,apoorvmittal10,2024-07-02T14:44:40Z,Query: Why do we only authorize in subsequent request i.e. when acknowledge data is present?
1662675191,16456,apoorvmittal10,2024-07-02T14:45:14Z,Shouldn't this be an `async` call?
1663148002,16456,apoorvmittal10,2024-07-02T20:43:48Z,@AndrewJSchofield @adixitconfluent @omkreddy Can I get review on this please: https://github.com/apache/kafka/pull/16513
1663964233,16456,chirag-wadhwa5,2024-07-03T10:26:58Z,"Hi, thanks for the review. Yes this check is required here, because `sharePartitionManager.newContext` might throw errors during its execution. Initially, `shareFetchResponse` is defined as a null there. If an error is thrown, then its value is set as an error response. After this check, the acknowledging and fetching only proceeds if `shareFetchResponse` is null. If it is not null, then a final `ShareFetchResponse` object is prepared with the error code and appropriate values for the acknowledgements and sent back to the user"
1663972187,16456,chirag-wadhwa5,2024-07-03T10:33:18Z,"Hi, thanks for the review. According to my knowledge, this auth check is only required for acknowledgement and not for fetching. So, if there's nothing to acknowledge, there won't be any need for this auth check."
1663976290,16456,chirag-wadhwa5,2024-07-03T10:36:55Z,"Thanks for the review. `handleAcknowledgements` would internally call `acknowledge` in `sharePartitionManager`, which is an asyn function returning a future. According to the code that we already have in kip-932 branch, `handleAcknowledgements` method returns the value by waiting for the future to execute completely. So, this piece is synchronous. We can maybe return a future from `handleAcknowledgements` and wait for its completion here here, but I'm not sure how would that help us in any way."
1664101522,16456,apoorvmittal10,2024-07-03T12:21:03Z,"Are these intended changes, if yes then we shall have it in separate PR."
1664119319,16456,chirag-wadhwa5,2024-07-03T12:34:51Z,"Yes, the commit history got changed a bit. It should be fine now with the latest push in place"
1664122712,16456,apoorvmittal10,2024-07-03T12:37:31Z,Is this method being used anywhere now?
1664131801,16456,apoorvmittal10,2024-07-03T12:44:00Z,"My concern is on below code, should we complete the API call if `errorResponse` is constructed?

```
case e: Exception => shareFetchResponse = shareFetchRequest.getErrorResponse(AbstractResponse.DEFAULT_THROTTLE_TIME, e) match {
        case response: ShareFetchResponse => response
        case _ => null"
1664144313,16456,chirag-wadhwa5,2024-07-03T12:53:07Z,"nope, already taken care of in the latest commit. Thanks"
1664178535,16456,adixitconfluent,2024-07-03T13:17:19Z,"I agree with @apoorvmittal10 , seeing the `newContext` function, the possible errors are `INVALID_REQUEST`, `SHARE_SESSION_NOT_FOUND` and `INVALID_SHARE_SESSION_EPOCH`. In all such cases, we should be completing the API call with a top level error code there itself."
1664193628,16456,adixitconfluent,2024-07-03T13:27:03Z,We can remove this check if we return the API response with top level error code wherever it occurs. Wdyt @chirag-wadhwa5 @apoorvmittal10  ?
1664202015,16456,adixitconfluent,2024-07-03T13:32:33Z,"Comment should say ""Share"" instead of ""Regular"""
1664266986,16456,adixitconfluent,2024-07-03T14:14:03Z,"can we not club all of this ""if"" and the below ""if"" conditions into a single if?"
1664271714,16456,adixitconfluent,2024-07-03T14:17:13Z,Not sure if these dummy values are the ones we want to go ahead. Perhaps @apoorvmittal10 / @omkreddy would know better which values to use here?
1664273428,16456,adixitconfluent,2024-07-03T14:18:21Z,"nit: instead of partitions.size, maybe use partitions size"
1664275271,16456,adixitconfluent,2024-07-03T14:19:23Z,"nit: Add a ""."" at the end of comment. Can you do this for other added comments as well?"
1665550477,16456,AndrewJSchofield,2024-07-04T11:08:41Z,"I think so, at least for now. The eventual switch will be `group.version=2` as the second version of the new group coordinator feature. Unfortunately, `group.version` has been backed out and will not be re-introduced until 4.0. This is an internal (undocumented) config for the broker. It does the necessary thing, which is to enable the new GC, so it seems like a safe temporary answer here."
1665552665,16456,AndrewJSchofield,2024-07-04T11:10:49Z,"It's an internal (undocumented) configuration. As mentioned above, the configuration we are using is temporary for now. I'm happy with it being used until we get the real configs in place."
1665590294,16456,chirag-wadhwa5,2024-07-04T11:45:24Z,"That makes sense. Thanks for the review, will make the changes in the next commit"
1665592992,16456,chirag-wadhwa5,2024-07-04T11:47:48Z,"Thanks for the review ! Yes we can but I did that for better readability, without affecting the execution at all."
1665594694,16456,chirag-wadhwa5,2024-07-04T11:49:18Z,"yes, I wanted to have a discussion regarding these dummy values with @apoorvmittal10 as well"
1665597002,16456,chirag-wadhwa5,2024-07-04T11:51:28Z,Thanks for the review. I think I copied this comment from the regular fetch request implementation. But yeah its very trivial. I will make the change in the next commit.
1665633847,16456,apoorvmittal10,2024-07-04T12:22:03Z,"I find following line from the KIP which says following, hence shouldn't we authorize for READ on fetch as well?

```
Operations which change information about a share group (such as consuming a record) need permission to perform the READ action on the named group resource"
1665637301,16456,apoorvmittal10,2024-07-04T12:25:03Z,"Also, do we need to re-authorize on every request in session? What behaviour we have on regular fetch? Does it authorize everytime or once in session?

If we do it only at session establishment, I do get that if an API key is unauthorized then existing session might continue reading/acknowledging, but re-authorization is taxing as well. Hence checking what's the flow looks like on regular fetch.
cc: @AndrewJSchofield  "
1665642977,16456,apoorvmittal10,2024-07-04T12:29:40Z,"Hmmm, this is not clean.

Why can't we have `handleAcknowledgements` API to receive `mutable.Map[TopicIdPartition, util.List[ShareAcknowledgementBatch]]`? Then `ShareFetch` and `ShareAcknowledgeRequest` can send respective data to `handleAcknowledgements` method and we need not to use `asInstanceOf` or any type casting."
1665645855,16456,apoorvmittal10,2024-07-04T12:32:05Z,Isn't the `sharePartitionManager.acknowledge` already implemented?
1665661239,16456,apoorvmittal10,2024-07-04T12:44:44Z,"I do not see any futures handled here hence this maked the processing synchronous, irrespective if other API calls to sharepartitionmanager are async. Moreover I am failed to understand the behaviour of `handleAcknowledgements` method i.e. I do see `shareAcknowledgeResult` is returned right away but there might be a delay in getting reposne from `sharePartitionManager.acknowledge` method hence how that's handled?"
1665668978,16456,apoorvmittal10,2024-07-04T12:51:03Z,"Should we throw an exception here of complete the request by unsupported version error? What's followed in other APIs? For KIP-714, I added something below:

```
case None =>
        info(""Received get telemetry client request for zookeeper based cluster"")
        requestHelper.sendMaybeThrottle(request, subscriptionRequest.getErrorResponse(Errors.UNSUPPORTED_VERSION.exception))"
1665672326,16456,apoorvmittal10,2024-07-04T12:53:41Z,I see this is a copy from `fetch` but do we need to define same functionality again or create a common `def/method` for both?
1665676287,16456,apoorvmittal10,2024-07-04T12:56:43Z,"Shouldn't this be like below?

```suggestion
      return CompletableFuture.completedFuture[Unit](())
```"
1665689981,16456,apoorvmittal10,2024-07-04T13:07:43Z,"Do you need `asJava` conversion just ofr iteration? If yes, then aren't there better way in scala?
```suggestion
       acknowledgeResult.foreach { case (tp, partitionData) =>
```"
1665702943,16456,apoorvmittal10,2024-07-04T13:18:08Z,"Isn't the response is already returned and this is async call, what does `requestHelper.handleError(request, throwable)` does then?"
1665708594,16456,apoorvmittal10,2024-07-04T13:22:52Z,"Hmm the whole processing is `synchronous`, why can't we work with callbacks i.e. when complete?"
1665714440,16456,apoorvmittal10,2024-07-04T13:27:37Z,Can you please write comment regaridng what exaclty is happening and why we are going to modify the `shareFetchResponse` later? I am not sure if there is a better way of combinig 2 responses and construct `shareFetchResponse` just once rather first generating it in fetch and then modifying same.
1665777016,16456,chirag-wadhwa5,2024-07-04T14:18:10Z,"As per my knowledge the regular fetch does not authorize for READ operation on the named group. Only the topics from where data is to fetched, are authorized for READ operation"
1665987792,16456,apoorvmittal10,2024-07-04T18:19:48Z,Should it be in `error`?
1665988915,16456,apoorvmittal10,2024-07-04T18:22:26Z,should it be `private def`?
1665990669,16456,apoorvmittal10,2024-07-04T18:26:06Z,"I am not sure how costly the API for `asScala` on Map is, but should have some cost, so do you want to have conversion in forEack loop?"
1665993309,16456,apoorvmittal10,2024-07-04T18:31:49Z,"Does `topicIdNames` seems better? It was hard to relate later in the code what this variable holds, seems more like just name of topics."
1665994321,16456,apoorvmittal10,2024-07-04T18:33:51Z,Do we validate somewhere that partition index exists for the topic i.e. what if client request for partition 5 when there exists only 4 partitions for topic?
1665995004,16456,apoorvmittal10,2024-07-04T18:35:15Z,What exception is being thrown?
1665995891,16456,apoorvmittal10,2024-07-04T18:37:21Z,"Yeah it's much readable this way, I agree."
1665996271,16456,apoorvmittal10,2024-07-04T18:38:21Z,Will it not be better to mock `sharePartitionManager`?
1665997840,16456,apoorvmittal10,2024-07-04T18:41:18Z,"This change will be not needed if we use mock, that we should."
1665998426,16456,apoorvmittal10,2024-07-04T18:42:41Z,nit: would it better to have these methods defines later when used in tests?
1666000193,16456,apoorvmittal10,2024-07-04T18:46:23Z,"Seems the methods are same as defined in `SharePartitionTest`, shall we move them to common test utils (in java)?"
1666000611,16456,apoorvmittal10,2024-07-04T18:47:18Z,"I see we are using mock here, why not to define it on the top itself?"
1666001021,16456,apoorvmittal10,2024-07-04T18:48:11Z,Is `asJava` required? Isn't it already a java API?
1666723067,16456,chirag-wadhwa5,2024-07-05T12:00:46Z,I think it should be just return. `CompletableFuture.completedFuture[Unit](())` shouldn't be there at all because return type of `handleShareFetchRequest` is `Unit` and not `CompletableFuture[Unit]`
1666735395,16456,chirag-wadhwa5,2024-07-05T12:14:10Z,"Yep, this doesn't make sense. If there is an error here, the broker will send out 2 responses for that. I have changed this to the following logic - 
1) if throwable is not null, it will simply throw the throwable.
2) I have surrounded the call of `combineShareFetchAndShareAcknowledgeResponses` with a try catch. If error is not thrown then `requestChannel.sendResponse` with appropriate arguments. If error is thrown, then `requestHelper.handleError(request, throwable)` with appropriate arguments.

Also change the log from debug to error here."
1668159303,16456,chirag-wadhwa5,2024-07-08T07:55:34Z,"This was a mistake, will remove the try catch block. Thanks!"
1668342690,16456,chirag-wadhwa5,2024-07-08T09:55:53Z,"Thanks for the review. Actually yes it is required, since the List() is a scala list. There are other ways that do not use asJava, but they span over multiple lines, and would require new variable initialisations, reducing the code readability."
1669768302,16456,chirag-wadhwa5,2024-07-09T05:47:45Z,Hi thanks for reviewing. I think the first occurrence of these methods is in the test just underneath them. Isn't that what you mean here ?
1669815168,16456,chirag-wadhwa5,2024-07-09T06:31:01Z,"Yep, the true place for these methods should be in TestUtils file, but both SharePartitionTest and KafkaApisTest import separate TestUtils files. SharePartitionTest use the Java one and KafkaApisTest use the scala one. As far as I know, java does not support aliasing and neither do the older versions of scala."
1669817534,16456,chirag-wadhwa5,2024-07-09T06:33:40Z,Not currently. I think that can be done using the MetadataCache which is accessible in the KafkaApis. I will create a separate JIRA for this. Thanks !
1672098926,16456,apoorvmittal10,2024-07-10T11:24:59Z,I raised a query earlier regaridng why we only want to authorize when acknowledgements exist?
1672102704,16456,apoorvmittal10,2024-07-10T11:28:08Z,Why do we require `.get` here? Will it not block the calls?
1672103745,16456,apoorvmittal10,2024-07-10T11:28:57Z,Why this exception is only about `Release`?
1672106900,16456,apoorvmittal10,2024-07-10T11:31:48Z,I can find this comment is yet not addressed. I think we discussed that the code shall be asynchronous. In case we are finding it difficult in this PR then please log a jira and work on it post merge of this PR.
1672107334,16456,apoorvmittal10,2024-07-10T11:32:14Z,Again we have a blocking call here.
1672108309,16456,apoorvmittal10,2024-07-10T11:33:07Z,@chirag-wadhwa5 Did you get a chance to work on this?
1672109076,16456,apoorvmittal10,2024-07-10T11:33:49Z,Why do we need `breakable` here?
1672111524,16456,apoorvmittal10,2024-07-10T11:36:00Z,it's not that important but generally the helper method will come post the usage i.e. test -> helper method.
1672112684,16456,apoorvmittal10,2024-07-10T11:37:11Z,So why not to have either in java or scala test utils which can be used by both?
1672113400,16456,apoorvmittal10,2024-07-10T11:37:47Z,Do we have the jira now? Can we please link here?
1672117974,16456,apoorvmittal10,2024-07-10T11:41:46Z,Yeah but KIP-932 defines the behaviour for auth during fetch.
1675361989,16456,chirag-wadhwa5,2024-07-12T06:05:25Z,"I think this log is not in the right place. The only thing that throws an exception in the try block is the releasing the acquired records part, moved this log to that position. "
1675585257,16456,chirag-wadhwa5,2024-07-12T09:30:47Z,Created a separate JIRA to track this - https://issues.apache.org/jira/browse/KAFKA-17126
1675597142,16456,chirag-wadhwa5,2024-07-12T09:40:35Z,"Hey yep, the JIRA has been created - https://issues.apache.org/jira/browse/KAFKA-17127"
1675597743,16456,chirag-wadhwa5,2024-07-12T09:41:04Z,Created a JIRA - https://issues.apache.org/jira/browse/KAFKA-17126
1690482997,16456,junrao,2024-07-24T21:37:14Z,fetchOnComplete => onFetchComplete?
1690501820,16456,junrao,2024-07-24T21:54:18Z,"For consistency, could we remove `this`?"
1690504262,16456,junrao,2024-07-24T21:56:13Z,This only happens in ZK mode. We probably want to improve logging to reflect that.
1690509290,16456,junrao,2024-07-24T22:00:30Z,We could get rid of {}.
1690510398,16456,junrao,2024-07-24T22:01:36Z,We could get rid of {}. Ditto in a few other places.
1690517906,16456,junrao,2024-07-24T22:08:19Z,"Instead of forEach, perhaps you could do `shareFetchRequest.data.topics.stream().anyMatch` ?"
1690528671,16456,junrao,2024-07-24T22:24:41Z,The convention is to have no space before `:`. Ditto in a few other places below.
1690529065,16456,junrao,2024-07-24T22:25:21Z,space after `if`. Ditto in a few other places below.
1690566819,16456,junrao,2024-07-24T23:13:41Z,"Hmm, should we get the partitions for fetch from shareFetchContext instead from the request? The request may not include all partitions in the session since it can be incremental."
1690594851,16456,junrao,2024-07-24T23:46:04Z,Could this be private?
1690595889,16456,junrao,2024-07-24T23:47:38Z,This comment seems out of place.
1690596169,16456,junrao,2024-07-24T23:48:07Z,Datas is weird since data is already the plural form of datum.
1690599349,16456,junrao,2024-07-24T23:53:51Z,interestingWithMaxBytes => interestedWithMaxBytes ?
1690612220,16456,junrao,2024-07-25T00:14:57Z,"Could this block of code be replaced with sth like the following using lamda?

```
      val interestingTopicPartitions: util.List[TopicIdPartition] = interestingWithMaxBytes.entrySet().stream()
        .map(_.getKey)
        .collect(Collectors.toList());
```"
1691763015,16456,junrao,2024-07-25T16:17:40Z,ShareFetch is implemented on the latest version of the client and understands all versions of the message format. Why do we need to down convert here?
1691773793,16456,junrao,2024-07-25T16:25:50Z,Should we use CONSUMER_REPLICA_ID?
1691814103,16456,junrao,2024-07-25T16:45:35Z,"Could this code just be `requestHelper.throttle(quotas.fetch, request, maxThrottleTimeMs)`?"
1691823423,16456,junrao,2024-07-25T16:53:11Z,"If the response is not ready immediately, `combineShareFetchAndShareAcknowledgeResponses()` needs to be called asynchronously, right?"
1691825148,16456,junrao,2024-07-25T16:54:42Z,We could get rid of {} here.
1691839730,16456,junrao,2024-07-25T17:06:48Z,We are doing this check for fetching already. Do we need to do this again?
1691841163,16456,junrao,2024-07-25T17:08:02Z,Combine with the previous line?
1691841383,16456,junrao,2024-07-25T17:08:12Z,topicIdNames and clientID seem unused?
1691842743,16456,junrao,2024-07-25T17:09:17Z,We can get rid of {}.
1693588726,16456,junrao,2024-07-26T20:42:20Z,merge with previous line?
1693601483,16456,junrao,2024-07-26T20:59:28Z,merge with previous line?
1693602221,16456,junrao,2024-07-26T21:00:34Z,space after `if`
1693628707,16456,junrao,2024-07-26T21:41:26Z,This is an existing issue. Why do we need to pass in both interestingTopicPartitions and interestingWithMaxBytes? They have the same keyset. Could we just pass in interestingWithMaxBytes?
1693631675,16456,junrao,2024-07-26T21:45:46Z,"Hmm, I am not sure that I understand the logic here. The partition set for topicPartitionAcknowledgements is a subset of that for shareFetchResponse, right? If so, there is no remaining acknowledgements."
1695140156,16456,apoorvmittal10,2024-07-29T12:37:56Z,"Yeah as per suggestion here: https://github.com/apache/kafka/pull/16456#discussion_r1665668978, if we have it consistent across other logs then it would be good:

```
info(""Received share fetch request for zookeeper based cluster"")
```"
1695145736,16456,apoorvmittal10,2024-07-29T12:42:08Z,Is `()` required or we can just write `if(tp.partition == partition) {`
1695595126,16456,junrao,2024-07-29T17:23:53Z,`thenApply` => `.thenApply` ?
1695660706,16456,junrao,2024-07-29T18:09:47Z,"`isInvalidShareFetchRequest()` checks `isPartitionPresent`. If there is a partition level error, we should send a Errors.UNKNOWN_TOPIC_OR_PARTITION at the partition level, instead of Errors.INVALID_REQUEST at the request level."
1695831694,16456,junrao,2024-07-29T20:30:49Z,Why do we need `partitionDatas`? Could we just iterate `erroneousAndValidPartitionData.validTopicIdPartitions` directly?
1695851702,16456,junrao,2024-07-29T20:37:31Z,"This can a bit simpler like the following.
```
.thenApply{result => 
  ...
}
```"
1695877000,16456,junrao,2024-07-29T20:44:46Z,The convention is to combine with the previous line.
1695885553,16456,junrao,2024-07-29T20:47:08Z,Could this be private?
1695923179,16456,junrao,2024-07-29T20:57:29Z,It seems cleaner if we just create `erroneous` inside `handleAcknowledgements()`?
1695953996,16456,junrao,2024-07-29T21:05:50Z,indentation
1695961725,16456,junrao,2024-07-29T21:08:14Z,Why does this return a mutable map? Ditto for `handleAcknowledgements()`.
1695973614,16456,junrao,2024-07-29T21:15:32Z,Why do we need to copy the entries to `partitions`? We could just keep using `responsePartitionData`?
1696012525,16456,junrao,2024-07-29T21:59:12Z,Could this be private?
1696053083,16456,junrao,2024-07-29T22:51:55Z,There is some validation inside `sharePartitionManager.newContext`. Should we just fold this logic there?
1696057694,16456,junrao,2024-07-29T22:59:49Z,Why is this request invalid?
1696066333,16456,junrao,2024-07-29T23:14:22Z,This seems unnecessary since we already tested that the array is empty above.
1696077755,16456,junrao,2024-07-29T23:20:21Z,Should we set records to `MemoryRecords.EMPTY`?
1696078584,16456,junrao,2024-07-29T23:21:26Z,Merge into previous line?
1696079560,16456,junrao,2024-07-29T23:22:33Z,This is not the first request.
1696081720,16456,junrao,2024-07-29T23:24:45Z,"Is this test useful? In both cases, we mock `sharePartitionManager.newContext` to return Errors.SHARE_SESSION_NOT_FOUND regardless whether there is a wrong member ID or group ID. Should we mock `sharePartitionManager.newContext` to return a ShareSessionContext with the wrong group/member ID?"
1696083957,16456,junrao,2024-07-29T23:27:07Z,"Merge into previous line? Also, should we mock sharePartitionManager.newContext to return a ShareSessionContext with the wrong epoch instead of directly throwing an exception?"
1696094681,16456,junrao,2024-07-29T23:35:48Z,"Hmm, the expected epoch should be 2 for the last call, right?"
1696108097,16456,junrao,2024-07-30T00:00:53Z,This is not the first fetch request.
1696112503,16456,junrao,2024-07-30T00:09:07Z,`shareFetchData` seems unused?
1696113299,16456,junrao,2024-07-30T00:10:46Z,shareFetchData seems unused?
1696113985,16456,junrao,2024-07-30T00:12:00Z,Quite a long name. Could it be sth like `testHandleShareFetchFetchMessagesReturnErrorCode`?
1696114858,16456,junrao,2024-07-30T00:13:37Z,shareFetchData seems unused?
1696116555,16456,junrao,2024-07-30T00:17:09Z,merge with previous line?
1696116942,16456,junrao,2024-07-30T00:17:53Z,Could this be private?
1696116973,16456,junrao,2024-07-30T00:17:58Z,Could this be private?
1696118317,16456,junrao,2024-07-30T00:20:55Z,"I understand the logic better now. So, this is fine."
1696118888,16456,junrao,2024-07-30T00:22:11Z,I understand the code better now. This is fine.
1696517878,16456,chirag-wadhwa5,2024-07-30T08:07:31Z,"Thanks a lot for the review. I actually had some unit tests in place for this method, that is why left it as public. Should I add a comment saying `//Visible for testing` ?"
1696558877,16456,chirag-wadhwa5,2024-07-30T08:33:53Z,"Hi, thanks a lot for the review. I guess this comment refers to the code before the last commit. I believe all the issues with asynchronous code have been resolved with that. Let me know if you find any other gaps. Thanks !"
1696677665,16456,chirag-wadhwa5,2024-07-30T09:50:14Z,"Thanks for the review ! With the new code in place, I believe this problem has also been resolved."
1697309981,16456,chirag-wadhwa5,2024-07-30T17:10:39Z,"Thanks for the review. Actually the code does not down convert anything, its just the variable names and the comments that suggest that. Made the required changes"
1697405173,16456,chirag-wadhwa5,2024-07-30T18:33:05Z,"Thanks for the review. You are correct here. But the code actually has that check already, I think this is not needed at all. I will remove this in the next commit"
1697422741,16456,chirag-wadhwa5,2024-07-30T18:48:42Z,"Thanks for the review. Actually, the upcoming PR for shareAcknowledgeRequest would make it clear why it has been one this way. The acknowledgement data sent to `handleAcknowledgements` is retrieved using different methods in case of a fetch request and an acknowledge request. These methods can themselves identify some erroneous topic partitions, so that is why the map is being passed on to the method"
1697447600,16456,chirag-wadhwa5,2024-07-30T19:07:17Z,"Thanks for the review. Given that a single fetch from a partition can sometimes contain significant amount of data, copying the entire map again whenever a new element is added could be a little extensive. But I don't think this choice of using a mutable.map or simply a map would make a huge difference. Do you have any better suggestions though ?"
1697502447,16456,chirag-wadhwa5,2024-07-30T19:53:44Z,"Thanks for the review. Yes we could use the same, but the definition of some methods of shareFetchContext require a util.LinkedHashMap, so we would anyways require a new variable to store the converted map as it is required as an argument to multiple methods. Talking about why do we need a util.LinkedHashMap altogether, maybe we could change those method signatures to use a scala map as well, but I think that would out of scope for this PR as it would include making changes to others code as well."
1697672536,16456,junrao,2024-07-30T22:29:09Z,"It seems that we never add/remove elements in the returned mutable.Map? If the map doesn't need to be mutated, returning just Map reduces potential side effect."
1697988104,16456,chirag-wadhwa5,2024-07-31T06:48:01Z,"Thanks for the review. The logic dictates, that a final share fetch request should only be sent for acknowledging previously fetched records, and not for fetching new records (the new records wouldn't be acknowledged since this is the final request). So, if the the partition Max Bytes field is non zero for any share partition in the request, that mens the client expects records as a result, which should not be the case, since it is a final fetch request. Hence this is an invalid request. i know it is very trivial, but in the long run, the final epoch would be sent via a shareAcknowledge request and not a shareFetch request, thereby resolving this altogether"
1698763204,16456,junrao,2024-07-31T16:03:31Z,Thanks for the explanation. Make sense. Could we add a comment about this?
1699474736,16456,chirag-wadhwa5,2024-08-01T06:05:14Z,"Thanks for the review. I'm sorry but I don't understand how returning a ShareSessionContext with the wrong group/member ID will help. The entire logic to handle the groupID/memberID is in newContext and if a wrong groupID/memberID is provided in the request, an newContext throws an error, which is what this test tries to mimic. Pls let me know if my understanding is wrong anywhere, thanks !"
1699483505,16456,chirag-wadhwa5,2024-08-01T06:09:02Z,"Thanks for the review. Again, I'm sorry but I don't see how that is going to help. The expected behaviour is that the newContext method should throw an exception in case of wrong epoch. If it successfully returns a ShareSessionContext, then the fetching would proceed without any issues, because that does not care about epochs at all. I have tried to simulate the expected behaviour in the test. Pls let me know if my understanding is wrong anywhere, thanks !"
1699487912,16456,chirag-wadhwa5,2024-08-01T06:10:20Z,"Thanks for the review. Yes, you are right. Actually, the ShareFetchMetadata would include the current epoch in the request, but the ShareSession would contain the next epoch, as the epoch is bumped in the newContext method. I have made the change here, as well as in the other functions. Thanks again for pointing it out !"
1699568098,16456,chirag-wadhwa5,2024-08-01T07:22:47Z,"Thanks for the review. I already changed this piece of code in the last commit, it does not use any breakable now."
1699569869,16456,chirag-wadhwa5,2024-08-01T07:24:21Z,Thanks for the review. I think there is a difference between both the if and else cases. I actually used the code from the normal fetch request and the throttling logic is the same as it is there.
1699572051,16456,chirag-wadhwa5,2024-08-01T07:26:11Z,"Thanks for the review. I think this comment was addressing an issue in the previous version of the PR. I pushed a commit later, which I believe resolved all issues related to asynchronous code. If you find any other gaps, pls let me know. Thanks !"
1700521738,16456,junrao,2024-08-01T17:01:25Z,Move the statement to a separate line since this case has multiple statements.
1700530866,16456,junrao,2024-08-01T17:09:57Z,Could we make `fetchResult` a val by calling `handleFetchFromShareFetchRequest` here?
1700533859,16456,junrao,2024-08-01T17:11:42Z,merge into previous line
1700558567,16456,junrao,2024-08-01T17:32:45Z,Is the TODO still needed?
1700562536,16456,junrao,2024-08-01T17:35:39Z,"This can be a bit simpler like
```
thenApply{ result => 
  ...
}
```"
1700566242,16456,junrao,2024-08-01T17:39:01Z,no space before (
1700574089,16456,junrao,2024-08-01T17:46:10Z,interesting => interested ?
1700580952,16456,junrao,2024-08-01T17:50:48Z,shareAcknowledgeResult seems unused?
1700583206,16456,junrao,2024-08-01T17:52:10Z,"This can be a bit simpler like
```
thenApply{ result => 
  ...
}
```"
1700589809,16456,junrao,2024-08-01T17:55:30Z,merge into previous line
1700595263,16456,junrao,2024-08-01T18:00:08Z,"There is no conversion, right? So `converted` is inaccurate."
1700596185,16456,junrao,2024-08-01T18:01:01Z,Add an extra new line below.
1700596862,16456,junrao,2024-08-01T18:01:27Z,There is no down conversion.
1700603141,16456,junrao,2024-08-01T18:06:24Z,"Throttling is done inside this method. So, `invoked before throttling` is inaccurate."
1700604916,16456,junrao,2024-08-01T18:08:13Z,Could this be private?
1700605031,16456,junrao,2024-08-01T18:08:20Z,indentation
1700609451,16456,junrao,2024-08-01T18:12:26Z,"This can be a bit simpler like
```
forEach{topic => 
 ...
}
```"
1700619173,16456,junrao,2024-08-01T18:17:45Z,Could this be private?
1700620774,16456,junrao,2024-08-01T18:19:16Z,Could we add the new param to javadoc?
1700655193,16456,junrao,2024-08-01T18:42:27Z,We could use a mutable map locally for better efficiency and return it as `Map`.
1700657160,16456,junrao,2024-08-01T18:43:49Z,We could use a mutable map locally for better efficiency and return it as `Map`.
1700662501,16456,junrao,2024-08-01T18:48:30Z,"I mean that since we mock `sharePartitionManager.newContext` to return Errors.SHARE_SESSION_NOT_FOUND, we are not really testing whether `sharePartitionManager.newContext` could handle incorrect group/member ID properly. We are just testing if the caller can behave properly when `sharePartitionManager.newContext` returns an error. So, testing just one of incorrect group/member ID seems enough."
1700679813,16456,junrao,2024-08-01T18:59:08Z,"Why is isAcknowledgeDataPresent true? There is no acknowledgement, right? Ditto in a few other cases below."
1701337300,16456,chirag-wadhwa5,2024-08-02T06:19:08Z,"Understood, thanks a lot"
1701341077,16456,chirag-wadhwa5,2024-08-02T06:23:02Z,It's not. I have removed it. Thanks !
1701409939,16456,chirag-wadhwa5,2024-08-02T07:22:33Z,"Thanks for the review ! During the invocation of `sharePartitionManager.newContext` we don't pass any acknowledgements, so there's no way to know whether acknowledgements are present or not, only the variable `isAcknowledgeDataPresent` provides that information. My understanding says that in the general case, only the first fetch request (with request epoch 0) will not contain any acknowledgements, but all the subsequent requests would. Going by that logic, I have set this variable to false in case request epoch is 0, and true in all the other cases."
1702138634,16456,junrao,2024-08-02T17:37:00Z,"This should be 

```
      case e:  Exception => 
         requestHelper.sendMaybeThrottle(request, 
```"
1702139817,16456,junrao,2024-08-02T17:38:28Z,indentation
1702140987,16456,junrao,2024-08-02T17:39:53Z,merge with previous line?
1702141861,16456,junrao,2024-08-02T17:40:47Z,extra new line
1702159875,16456,junrao,2024-08-02T17:59:37Z,"`ShareFetchMetadata(Uuid.ZERO_UUID, -1)` could just be `newReqMetadata`?"
1702169187,16456,junrao,2024-08-02T18:09:59Z,This is fine. We can leave it as it is.
1704485591,16456,junrao,2024-08-05T18:24:21Z,This is unnecessary since `sharePartitionManager.close()` already calls `persister.stop()`.
1704494394,16456,chirag-wadhwa5,2024-08-05T18:33:45Z,"Ohh yes, you are right, missed it. Thanks. I have pushed the change @junrao "
1325138384,14364,philipnee,2023-09-13T22:32:26Z,just to create a coherent format
1325368170,14364,philipnee,2023-09-14T05:21:01Z,We should submit a patch to combine groupState into memberState. @lianetm 
1325381139,14364,philipnee,2023-09-14T05:41:03Z,"It might be helpful to read the test: `public void testHeartbeatResponse_errorHandling(final Errors error, final boolean isFatal)`"
1325382420,14364,philipnee,2023-09-14T05:43:03Z,I wonder if this can be moved to the memberStateManager.
1325383326,14364,philipnee,2023-09-14T05:44:36Z,Current I'm unable to reference the assignor config in memberStateManager because it is always null.
1325999433,14364,lianetm,2023-09-14T13:59:12Z,"Totally, but is there a reason why we couldn't just use the `membershipManager` here already? I though that was the point of unblocking the state PR that includes all that's needed here."
1326031428,14364,lianetm,2023-09-14T14:21:24Z,Makes sense. Let's move it on this same PR I would suggest
1326089090,14364,lianetm,2023-09-14T14:56:57Z,"It's null here simply because it wasn't defined when creating the `membershipManager` in the DefaultBackgroundThread.

My understanding is:

- If the client specifies an assignor (client or server), we should make sure to set it in the `membershipManager` when creating it in the DefaultBackgroundThread [here](https://github.com/apache/kafka/blob/d15659a4027698f0f231828315752991575e471c/clients/src/main/java/org/apache/kafka/clients/consumer/internals/DefaultBackgroundThread.java#L193). This will ensure that it is available here to send it on the HeartbeatRequest.

- If the client did not specified any assignor, then it will be OK to have null here, and we don't need to include anything for it in the HeartbeatRequest. We let the group coordinator on the server select the default assignor for the member from the `group.consumer.assignors` config. Correct me here @dajac if I'm missing something. 
"
1326105802,14364,dajac,2023-09-14T15:08:30Z,"That's right. The (server) assignor should be null be default as defined in the [KIP](https://cwiki.apache.org/confluence/display/KAFKA/KIP-848%3A+The+Next+Generation+of+the+Consumer+Rebalance+Protocol#KIP848:TheNextGenerationoftheConsumerRebalanceProtocol-ConsumerConfigurations). When null, the server uses the first one in the list on the server side."
1326205537,14364,philipnee,2023-09-14T16:14:48Z,thanks for the clarification.
1326228037,14364,philipnee,2023-09-14T16:28:49Z,actually - my bad: I think we can just get rid of the groupState. 
1326232358,14364,philipnee,2023-09-14T16:31:52Z,"There's a bit of refactoring needed, to keep things in scope for this PR - I will be submitting another PR to address this."
1326463429,14364,lianetm,2023-09-14T20:09:54Z,"The subscription state already has it, and it is a component that is all over, so I would try to keep the regex in that single place"
1326478655,14364,lianetm,2023-09-14T20:24:07Z,"Just for the record, that regex that we keep in the client (now on the SubscriptionState), will need to be updated to move away from the java `Pattern` and use the new `SubscriptionPattern` defined in the protocol I expect. "
1327513916,14364,philipnee,2023-09-15T16:09:28Z,"Sounds good thanks!

FWIW, a bit out of the scope here: We discussed the plan to split subscriptionState to remove (some of) the synchronization locks.  As users don't need to access the regex pattern directly (aside from submitting one), it might just live in the background thread once this refactor happens."
1329227271,14364,lianetm,2023-09-18T20:14:47Z,This is not using the state from params so I expect the `else` part will never be executed (member should be always in the default UNJOINED). Is it missing mocking the state value using the param I guess?
1329251065,14364,philipnee,2023-09-18T20:34:22Z,you are right! I will rewrite this test just mocking the notInGroup response.
1329477649,14364,philipnee,2023-09-19T02:12:01Z,"I wonder if we could just call this ""shouldHeartBeat"" as the server side protocol is tight to the heartbeat"
1330246922,14364,lianetm,2023-09-19T14:38:43Z,"Sure, sounds good to me"
1330274787,14364,dajac,2023-09-19T14:57:39Z,nit: I think that we usually use `()` instead of `{}` in to Strings.
1330275873,14364,dajac,2023-09-19T14:58:23Z,"Should we add javadoc to attributes, classes and methods? I think that we usually do it for all the new java code these days."
1330276646,14364,dajac,2023-09-19T14:58:52Z,This is incorrect. The heartbeat interval comes is provided in the response.
1330277639,14364,dajac,2023-09-19T14:59:37Z,nit: Let's add javadoc to be consistent with the other methods.
1330280744,14364,dajac,2023-09-19T15:01:22Z,"I am not sure to follow the `membershipManager.notInGroup()` part here. If we are not in the group, shouldn't we heartbeat to join (or rejoin) it?"
1330281420,14364,dajac,2023-09-19T15:01:42Z,nit: We could use `ifPresent`.
1330282085,14364,dajac,2023-09-19T15:02:04Z,nit: This empty line could be removed.
1330286961,14364,dajac,2023-09-19T15:04:58Z,"When we transition to Failed in updateState, I think that it analogous to a non retriable error. Is our plan to capture all the non-retriable errors before we reach this? We also do some error handling in updateState. The responsibilities are not clear here."
1330288247,14364,dajac,2023-09-19T15:05:56Z,nit: The naming does not respect our conventions here. We should use camel case.
1330319691,14364,lianetm,2023-09-19T15:27:08Z,"Agree that we need to better define responsibilities. As I see it for now, the membershipMgr should only come into play here when it's time to `updateState`, and that would be : 

- when no errors in HB response (to extract relevant state info from the response and transition to `Stable`)
- when there is a non-retriable error (to transition to `Failed`)

Any retriable error received in the HB response would be handled in the heartbeat manager here, while the state remains unchanged (ex. UNJOINED).

With this approach, there would error handling in both, the heartbeat manager and the membership manager but for different purposes:
- heartbeatManager handles errors to continue retrying the request if needed (until it gets a success or fatal error, in which case it would stop retrying and call `updateState`)
- membershipManager `updateState` internally handles fatal errors only, just to update state info accordingly (ex. reset epoch on Fence) and do the right transition. 

Thoughts?
"
1330451339,14364,philipnee,2023-09-19T17:04:22Z,I see - I think we've been consistently using { in the refactor.  Maybe we should change that @kirktrue @lianetm 
1330458175,14364,philipnee,2023-09-19T17:11:10Z,Will do. Sorry for completely missing this part.
1330460386,14364,philipnee,2023-09-19T17:13:24Z,"thanks, just note it here: `group.consumer.heartbeat.interval.ms is defined on the server side and the member is told about it in the heartbeat response.`"
1330475695,14364,philipnee,2023-09-19T17:25:35Z,"As previously commented, maybe let's use `shouldHeartbeat` to be more explicit"
1330480862,14364,philipnee,2023-09-19T17:28:37Z,"I've seen it in quite a few places so I thought we don't have a conventions  
"
1330940223,14364,philipnee,2023-09-20T03:45:13Z,"Thanks, I made some update to the manager, in particular, I split the fatal error out of the updateState, LMK if you like the change, or we could do better."
1331809403,14364,lianetm,2023-09-20T15:26:35Z,"This `onFatalError` does update the state for the member, so separating it from the `updateState` leads to having the update logic and transitions in 2 places (which I think is harder to follow/troubleshoot). 

What about we go back to a single `updateState` responsible for updating state (aka. member info and transitions) . And if we make this single `updateState` return the Optional<Error> that it may find in the response, then we could leave the error handling only in the MembershipManager, and the HeartbeatManager could be much simplified. Take a look at [this](https://github.com/apache/kafka/pull/14413) draft PR and let me know your thoughts"
1331811249,14364,lianetm,2023-09-20T15:27:56Z,This whole func would completely disappear if we agree on the something like the draft PR [here](https://github.com/apache/kafka/pull/14413)
1331828912,14364,lianetm,2023-09-20T15:40:02Z,"Again brainstorming based on the [draft](https://github.com/apache/kafka/pull/14413), this would be much simplified with the move of the error handling more into the MembershipManager I expect. Here, instead of having to paths to update state (now there are 2 calls, one to membershipManager.updateState and another for all the error handling), we could simply have something like:

```
private void onResponse(final ConsumerGroupHeartbeatResponse response, long currentTimeMs) {
        if (response.data().errorCode() == Errors.NONE.code()) {
            // Heartbeat manager specifics for success - nothing affecting state
        } else {
            // Heartbeat manager specifics for failure - nothing affecting state & no error handling other than identifying when coordinatorRequestManager.markCoordinatorUnknown is needed, which should be here I think
            }
        }
        // Update state - single point of interaction with the membershipMgr
        Optional<Errors> error = membershipManager.updateState(response.data());
        if (error.isPresent()) {
            nonRetriableErrorHandler.handle(error.get().exception());
        }
    }
```"
1331895217,14364,philipnee,2023-09-20T16:21:45Z,"Why don't we let the heartbeat request manager to handle all the errors?  Technically, these are heartbeat errors, and it would be more centralized to handle them on the manager.

The membershipManager really could just ensure if the transition is valid."
1331944957,14364,lianetm,2023-09-20T17:05:56Z,"Sounds good, actually better to move it all to the Heartbeat manager, given that it is the more concerned about the HB errors. The membershipMgr in the end only needs to know about what affects the state (success, fencing and fatal failures) "
1331948509,14364,lianetm,2023-09-20T17:09:14Z,"This could still be simplified a lot like I was suggesting in the comment above. Not handling all errors, only the fencing/fail ones. For all the rest is a common action that could be done with a single `nonRetriableErrorHandler.handle(error.get().exception());`"
1331997791,14364,lianetm,2023-09-20T17:56:41Z,This will be invoked on any non-retriable error I expect (not only the UNRELEASED_MEMBER_ID)
1332028195,14364,lianetm,2023-09-20T18:27:58Z,This could be final now
1332033298,14364,lianetm,2023-09-20T18:33:35Z,nit nit: I find it a better format to read the code if adding the separators at the end of the previous line for better alignment (having all the added `propId=` at the beginning of each line)
1332033928,14364,lianetm,2023-09-20T18:34:17Z,Indentation
1332038622,14364,lianetm,2023-09-20T18:39:21Z,Indentation? (I guess it shouldn't be the same as in the requestManager down below)
1332041024,14364,lianetm,2023-09-20T18:41:52Z,This is only needed when there is a groupId defined so I would move it completely to the `if (groupState.groupId != null)` block
1332064354,14364,lianetm,2023-09-20T19:06:17Z,"I think we should explain a bit here about the timing of the heartbeat requests, which is also managed by this class. It would be good to explain the timing logic based on the interval as max waiting time, but also mentioning that the manager may send out a HB request without waiting for the interval, ex. when completing processing an assignment. "
1332069325,14364,lianetm,2023-09-20T19:11:45Z,"Given that the `canSendRequest` checks the heartbeat interval, this means that we'll be only sending heartbeats on the interval (heartbeatTimer.isExpired()), but we also need to have a mechanism for sending heartbeat requests ""on-demand"" (ex. when rebalance callbacks execution finishes, we should send a HB request right away , without waiting for the interval timer to expire)  "
1332077647,14364,lianetm,2023-09-20T19:20:50Z,Do we need this at the class level? Seems to only be needed in the constructors for initializing the heartbeatTimer
1332093119,14364,lianetm,2023-09-20T19:38:01Z,final
1332115559,14364,lianetm,2023-09-20T20:03:05Z,"I think we should still validate here that the response contains no error (and throw IllegalArgument if so), as this func now is only expected to be called on successful responses. Without such validation, an erroneous call to this func in the case of an error would end up going unnoticed and transition the member to stable."
1332168444,14364,kirktrue,2023-09-20T20:50:48Z,"```suggestion
        logger.warn(""Failed to send heartbeat to coordinator node {} due to error: {}"", coordinatorRequestManager.coordinator(), exception.getMessage());
```"
1332177568,14364,kirktrue,2023-09-20T21:00:24Z,Can we resolve the error code to an `Errors` object via `Errors.forCode()`?
1332178673,14364,kirktrue,2023-09-20T21:01:15Z,"Same question here, can we use and compare errors based on `Errors` (which we can get via `Errors.forCode()`)?"
1332180195,14364,kirktrue,2023-09-20T21:02:22Z,"```suggestion
            logInfo(""Coordinator node {} is either not started or not valid. Retrying"", coordinatorRequestManager.coordinator(), response, currentTimeMs);
```"
1332182234,14364,kirktrue,2023-09-20T21:03:50Z,"```suggestion
            logInfo(String.format(""Heartbeat was not successful because the coordinator node %s is loading. Retrying"", coordinatorRequestManager.coordinator()), response, currentTimeMs);
```"
1332183105,14364,kirktrue,2023-09-20T21:04:48Z,Same request here: use `Errors` to remove unnecessary use of raw error `code`.
1332190917,14364,kirktrue,2023-09-20T21:11:24Z,Can we rename `failMember` to be more descriptive?
1332191970,14364,kirktrue,2023-09-20T21:12:39Z,"Per the related comment, can we rename `failMember` something that is more descriptive of what action _happened_ vs. that action's _result_?"
1332443272,14364,philipnee,2023-09-21T04:40:54Z,"This is a pretty common pattern to override toString. Do you mean making it doing?

```
AssignorSelection(
type=TYPE,
serverAssignor=assignor,
...);
```

Currently everything is in a single line. Though - this can be used for logging, so i wonder what would it look like it if we add line separator."
1332444068,14364,philipnee,2023-09-21T04:41:41Z, 
1332446857,14364,philipnee,2023-09-21T04:44:25Z,make sense.
1332454913,14364,philipnee,2023-09-21T04:54:32Z,"I thought it would be more clear on how to implement this after the revocation is implemented or implement it with the assignment logic - can we punt it to a separated PR?
"
1332455723,14364,philipnee,2023-09-21T04:55:43Z,"this is left non-final intentionally - as sometimes we might want to spy the membership class, so the test function can override it."
1332466756,14364,philipnee,2023-09-21T05:11:46Z,maybeTransitionToFailure ?
1333624789,14364,lianetm,2023-09-21T21:33:13Z,"Agree with the pattern, I was only referring to having the nit of having props aligned :
```
return ""AssignorSelection("" +
                ""type="" + type + "","" +
                ""serverAssignor='"" + serverAssignor + '\''
                ...
```"
1333629206,14364,lianetm,2023-09-21T21:39:58Z,"Ok with having it in a separate PR but let's maybe add a comment/TODO here, and think about how to define the interaction between the AssignmentReconciler and this HB manager. They need to somehow communicate to trigger a HB request when the callbacks complete successfully (exactly what came out in the AssignmentReconciler PR review [here](https://github.com/apache/kafka/pull/14357/files#r1333628852))"
1334626047,14364,lianetm,2023-09-22T16:57:01Z,typo membershipManager
1334633010,14364,lianetm,2023-09-22T17:04:49Z,I think this is not only for revocation. I expect members should send a heartbeat request as soon as they complete processing an assignment without waiting for the interval (for both cases: new partitions being added and partitions being revoked). Let's double check with @dajac 
1334636930,14364,lianetm,2023-09-22T17:09:23Z,unused since the class level var was removed
1334645635,14364,lianetm,2023-09-22T17:19:05Z,"I find it a bit confusing to say that the member won't send HB when it left the group. Agree that it holds true when a member intentionally leaves a group (ex. when the consumer is closed), but it's not true for when a member is left out of the group by the server (ex. all fencing scenarios). When left out of a group because of a fencing situation, the member will release its assignment and send HB again to rejoin.    "
1334652824,14364,lianetm,2023-09-22T17:27:10Z,"I think we're still missing important info in the doc about the HB interval and how it is applied. (The heartbeat sent on the heartbeat interval, that is received from the server on the first HB response. If the member finishes processing an assignment (partitions assigned/revoked) the interval is not honored and the HB request is sent out right away)"
1334657428,14364,lianetm,2023-09-22T17:32:14Z,uhm we're using a 0 as default `heartbeatIntervalMs` here. This will only get updated when we get the value from the server in the first HB response. Thinking about the case where we send an initial HB request but never get a response...does this 0 then mean that we'll continue to send a HB on every poll iteration? 
1334659219,14364,lianetm,2023-09-22T17:34:12Z,nit: review punctuation marks usage
1334668800,14364,lianetm,2023-09-22T17:45:30Z,"Agree that we'll ""update the timer when the response is received"", but I see here that the timer is only used on the case where it is not time to send HB yet (if block above, ln 112). 

In the case of this return, which is the case when the HB request manager is polled and notices it is time to send the HB, we're always passing  `Long.MAX_VALUE` as `timeMsTillNextPoll`. Shouldn't we use the timer here too (that will have its default value if this is a first req, or the one provided by the server in a previous HB response)? "
1334734541,14364,philipnee,2023-09-22T19:08:25Z,"the canSendRequest should take care of the inflight request.  If the request has been sent w/o a response/error, it won't try to send again.  For the initial state, I really just need to set to a specific number as we get the HB response from the server. I set to 0 because I think the client should quickly poll the manager against to see if it can send a heartbeat.  Another alternative is to use `backoffs` to prevent a tight loop there. WDYT?"
1335289660,14364,lianetm,2023-09-25T01:02:54Z,"Got it, seeing that the `canSendRequest` considers inflight requests then it makes sense to set an initial value of 0 I would say, so that we send the first HB as soon as the HM manager starts. I would only suggest to add some tests for the interval, including this case where we might not get a response to our first HB request. "
1335295791,14364,lianetm,2023-09-25T01:20:46Z,`shouldSendHeartbeat` returning false when UNJOINED does not seem right. We do need to send HB when UNJOINED to be able to join the group. I would say FAILED is the only state we we shouldn't send HB.
1335297785,14364,lianetm,2023-09-25T01:27:16Z,"Given that the current `shouldSendHeartbeat` returns false when UNJOINED, I expect the second part of this condition will be true when a members starts for the first time and we'll return a pollResult with empty request list, so we'll never be sending the first HB request?"
1335304701,14364,lianetm,2023-09-25T01:46:06Z,"This is a non-retriable exception, so I expect we should be calling `membershipManager.transitionToFailure()`? (same for all other fatal exceptions up to the `UNRELEASED_INSTANCE_ID`, which is properly doing the transition)"
1335307959,14364,lianetm,2023-09-25T01:55:36Z,"From the HB request manager point of view, this means that when polled, it won't return any request right? Could we assert that to ensure that the request manager is actually not generating requests at this point?"
1335309021,14364,lianetm,2023-09-25T01:58:34Z,"If we agree on the [comment](https://github.com/apache/kafka/pull/14364/files#r1335304701) regarding  missing transitions when handling fatal errors, I expect this will be updated to reflect align with it and check transition to FAILED on all fatal errors other than the 2 fencing ones."
1336389075,14364,philipnee,2023-09-25T21:06:06Z,i think you are right.
1336390801,14364,philipnee,2023-09-25T21:08:17Z,the updated PR should invoke maybeTransitionToFailureState() on a few fatal exceptions.
1336429379,14364,philipnee,2023-09-25T22:03:13Z,"so when we startup the manager, the timer will be set to 0 until the first heartbeatInterval response is received. Which means, we will get a heartbeat request on the first poll.  I added a test for this."
1337234985,14364,lianetm,2023-09-26T13:42:35Z,Wrong placeholder {}
1337235694,14364,lianetm,2023-09-26T13:43:05Z,ditto
1337238424,14364,lianetm,2023-09-26T13:44:57Z,"tries ""to"" rejoin"
1337239851,14364,lianetm,2023-09-26T13:45:54Z,"and ""try"""
1337258265,14364,lianetm,2023-09-26T13:57:51Z,"This format won't show as a proper list in the java doc, we should use `<ol>` tags (similar for the empty lines)"
1337263803,14364,lianetm,2023-09-26T14:00:56Z,"True, but the poll is much more than just determining the wait time, so I would add to this something like "" it builds the heartbeat request, including the logic for handling the responses"""
1337265790,14364,lianetm,2023-09-26T14:02:19Z,comma after expired
1337269007,14364,lianetm,2023-09-26T14:04:32Z,These empty lines won't show as such in the java doc so let's add tags to ensure we have the separation we want
1337275381,14364,lianetm,2023-09-26T14:08:55Z,"Just for my understanding, what's the idea behind this TODO? I thought we had inflight req handling in the parent `RequestState`, that already identifies `log.trace(""An inflight request already exists for {}"", this);`. And what would be the concurrent scenario if there is a single background thread sending heartbeats and it is not resending while there is one inflight?"
1337278130,14364,lianetm,2023-09-26T14:10:40Z,Extra space after state.
1337278315,14364,lianetm,2023-09-26T14:10:47Z,ditto
1337282271,14364,lianetm,2023-09-26T14:13:22Z,final
1337282860,14364,lianetm,2023-09-26T14:13:46Z,extra line
1337295428,14364,lianetm,2023-09-26T14:20:46Z,seems we're not using `memberAssignment` in the test anymore? let's remove if unused
1337309504,14364,lianetm,2023-09-26T14:28:25Z,"High level comment, I do see this test covering the timing logic for sending, and the response handling on error, but nothing for the successful HB response handling (important to ensure that it is updating the target assignment so that it can be processed by other components). Also it would be helpful to have some tests around HB timeouts, mainly to validate the retry logic around that. 
(Just suggestions for better coverage of core actions, OK for me if we prefer to target that in a separate PR)"
1337418980,14364,philipnee,2023-09-26T15:37:20Z,This is just for comment formatting.
1337420489,14364,philipnee,2023-09-26T15:37:58Z,"i think it was a note from before, removed."
1337423659,14364,philipnee,2023-09-26T15:39:35Z,we don't make these var final in tests because we could change them later
1337428627,14364,lianetm,2023-09-26T15:43:11Z,"Ok, that's fine but not enough. I think here we also need the tags. If you look at the java doc it shows as a giant block, which I expect it is not what we want."
1337639317,14364,lianetm,2023-09-26T18:42:23Z,"Agree, with the fix for the `shouldSendHeartbeat` this should now work as expected.   "
1340176254,14364,lianetm,2023-09-28T13:40:51Z,Seems this is still wrong? same for the following comment
1340351740,14364,philipnee,2023-09-28T15:44:05Z,for some reason the change didn't get pushed.
1341528842,14364,dajac,2023-09-29T15:42:23Z,I actually missed this in the other PR but the assignor selection should be based on the new `group.remote.assignor` config which should be null by default. null means that the server selects the assignor.
1341531725,14364,dajac,2023-09-29T15:44:30Z,Don't we need to rejoin with epoch 0 when the member is kicked out of the group? We should actually remove all partitions and trigger the partition lost callback as well before doing so.
1341532755,14364,dajac,2023-09-29T15:45:21Z,What's the delay for the next event loop?
1341535159,14364,dajac,2023-09-29T15:47:25Z,I wonder if we really need this. I would expect retriable exception to inherit from `RetriableException` and the fatal ones to not inherit from it. Is it possible to leverage this somehow?
1341535814,14364,dajac,2023-09-29T15:47:51Z,Should we have javadoc for all those attributes?
1341539358,14364,dajac,2023-09-29T15:50:34Z,"When the reconciliation of the local assignment is completed, we need to send the heartbeat request immediately to ack it. Is this going to be another condition here?"
1341540248,14364,dajac,2023-09-29T15:51:17Z,This answers my previous comment :). Note that we also need to do this when partitions are assigned.
1341547285,14364,dajac,2023-09-29T15:56:40Z,"Most of the fields in the ConsumerGroupHeartbeat Req/Rsp are optional. Our aim was to avoid having to send unnecessary information when the group is stable. In this case, the request/response should be as lightweight as possible.

There are basically three requires fields: groupId, memberId, memberEpoch. Those must be set all the time. I am also debating whether groupInstanceId should also be. For all the others, they should only be set if they have changed. We should also send out a full request when we recover from a recoverable error (e.g. fenced, network issues, timeouts, etc.)."
1341548905,14364,dajac,2023-09-29T15:57:54Z,"On the regex topic, keep in mind that we will support both the java regex and the new sever side one for a while. When the java regex is used, the resolution must be done locally. When the server side regex is used, we must pass it to the server."
1341588720,14364,dajac,2023-09-29T16:40:07Z,I don't really get the value of this map given that we handle all (known) errors anyway. Should we just move `transitionToFailure` to there?
1341590757,14364,dajac,2023-09-29T16:42:44Z,"The name `onFatalErrorResponse` is incorrect here, isn't it? We handle recoverable errors (e.g. FENCED_MEMBER_EPOCH). Personally, I would prefer to re-group all errors in `onErrorResponse` and have a `switch` covering all of them there.

This would simplify the code and avoid repetition such as `Errors.forCode(response.data().errorCode())` which is in both places.

Thoughts?"
1341591777,14364,dajac,2023-09-29T16:44:00Z,We also pass the `errorMessage` to `exception()` here in order to give it back to the user.
1341592473,14364,dajac,2023-09-29T16:44:54Z,Don't we need to propagate those unknown errors to the user as well? What's our strategy here?
1341592782,14364,dajac,2023-09-29T16:45:16Z,nit: We usually declare final attributes before the others.
1341593245,14364,dajac,2023-09-29T16:45:51Z,nit: Couldn't it be final as well?
1341596109,14364,dajac,2023-09-29T16:49:23Z,"Why do we need this try catch here? If we remove it, where would the exception be caught?"
1341597457,14364,dajac,2023-09-29T16:50:52Z,nit: `transitionToFenced` to be aligned with the other one?
1341597673,14364,dajac,2023-09-29T16:51:08Z,nit: Should it be `transitionToFailed`?
1341662210,14364,lianetm,2023-09-29T18:00:53Z,"Agree, I missed this too. We agreed that we would have no default on the client side, and would let the server choose. "
1341666380,14364,lianetm,2023-09-29T18:06:37Z,"Agree too. It's only when the member leaves the group intentionally (ex. when consumer closes) that I expect this applies, no more HB. (addressed also on [this](https://github.com/apache/kafka/pull/14364#discussion_r1334645635) comment)"
1341669179,14364,philipnee,2023-09-29T18:10:28Z,"yeah possible - but i'll need to encode the 2 exceptions, i.e. unknown member id and fenced epoch, somewhere"
1341690694,14364,philipnee,2023-09-29T18:37:33Z,"it is a little bit tricky here, because the background thread loop is blocked until either it receives some responses, or the timer of the minimum of the heartbeat, metadata, or request timeout is expired.  What we would do is to reset the HeartbeatRequestState so that we could ensure the next event loop will trigger a heartbeat.

The timing, however, is a little tricky here, because it can be non-deterministic of when the next heartbeat will be sent."
1341699215,14364,philipnee,2023-09-29T18:49:09Z,left a TODO.  Will have a separated PR to fix this.
1341767013,14364,philipnee,2023-09-29T20:27:16Z,"Actually i think we should, thanks for the catch."
1341775832,14364,philipnee,2023-09-29T20:41:42Z,"We don't, it is purely for logging purposes.  For the case of this exception, maybe we should fail the request and retry."
1341778036,14364,philipnee,2023-09-29T20:45:03Z,we probably don't need it after all. because error is thrown when an error is presented in the response. 
1342665614,14364,dajac,2023-10-02T13:03:52Z,"My understanding is that we basically retry on all exceptions here. Am I correct? It seems to me that we could also get non-retriable exceptions here (e.g. UnsupportedVersionException, etc.). How do we handle those? "
1342666388,14364,dajac,2023-10-02T13:04:24Z,"We also need to handle `UnsupportedVersionException` error, I think."
1342667683,14364,dajac,2023-10-02T13:05:20Z,"Would it make sense to just call `membershipManager.transitionToFailed();` in all errors instead of having this one? At the movement, the handling is a little inconsistent. "
1342669966,14364,dajac,2023-10-02T13:06:52Z,nit: We usually put an empty line between cases. It makes it a bit more readable.
1342674863,14364,dajac,2023-10-02T13:10:14Z,"When we get this one, I understand that we will mark the coordinator as unknown to rediscover it. Is it going to apply the exponential backoff after that?"
1342677484,14364,dajac,2023-10-02T13:12:26Z,This message is not consistent with the others. Should it also start with `GroupHeartbeatRequest failed due to...`? I would also replace `Retrying` by something like `Will attempt to find the coordinator again and retry`.
1342678720,14364,dajac,2023-10-02T13:13:33Z,"nit: This one is also inconsistent. `+ ""Retrying""` could be merged with the previous string. A space misses between `loading.` and  `Retrying`."
1342681301,14364,dajac,2023-10-02T13:15:56Z,"I don't really understand how the error message is handled here. Is it going to be added as the message of the exception later on? When I mentioned this in my earlier comment, I means doing this `Errors.INVALID_REQUEST.exception(errorMessage)`. This uses the provided error message."
1342682876,14364,dajac,2023-10-02T13:17:24Z,Does the  mean that you will add it?
1342683440,14364,dajac,2023-10-02T13:17:54Z,Should we also log something here?
1342689429,14364,dajac,2023-10-02T13:23:06Z,I also wonder if having a tailored error message for each error is needed. An alternative would be to group errors and have a generic message for the fatal ones for instance. What do you think?
1342694336,14364,dajac,2023-10-02T13:27:18Z,"I also noticed that we have the following in the current implementation:

```
            if (e instanceof DisconnectException) {
                markCoordinatorUnknown(true, e.getMessage());
            }
```"
1342778930,14364,philipnee,2023-10-02T14:37:07Z,"The heartbeat request depends on the state of the state machine, so for most of the non-retriables, it should transition the state to a failure state to prevent additional requests being sent.

The non-retriable also send an error to the handler (`nonRetriableErrorHandler`) to send the error to the user to handle the failure case."
1342781938,14364,philipnee,2023-10-02T14:39:25Z,"But we don't want errors like `COORDINATOR_NOT_AVAILABLE` to be fatal, no? "
1342783380,14364,philipnee,2023-10-02T14:40:41Z,really?  Most cases I see don't have an empty line.  But I'll add a line anyway.
1342785385,14364,philipnee,2023-10-02T14:42:21Z,"yap - `this.heartbeatRequestState.onFailedAttempt(currentTimeMs);`

basically markets the `lastReceivedMs` as the response receive time.  Then it would trigger the backoff mechanism in the Requeststate.  I think expo backoff was tested in the test."
1342899010,14364,philipnee,2023-10-02T16:14:38Z,"Yap that's a good idea to make the code easier to follow. We do want to provide some context such as group id or group instance id to GROUP_AUTHORIZATION_FAILED and UNRELEASED_INSTANCE_ID respectively. So I'm grouping the following 4 errors

```
case INVALID_REQUEST:
case GROUP_MAX_SIZE_REACHED:
case UNSUPPORTED_ASSIGNOR:
case UNSUPPORTED_VERSION:
```"
1342910020,14364,philipnee,2023-10-02T16:25:52Z,"Added some comments directly above the var - Is this enough or you actually want this to be presented in the ""javaDoc"" of the class section?"
1342915079,14364,philipnee,2023-10-02T16:31:09Z,I understand the concern about inconsistency but different errors can put the member in a different state
1342920121,14364,philipnee,2023-10-02T16:36:57Z,"Yap I see the DisconnectException handling, it is implemented by all requests going to the coordinator.  Here we handle them independently in the request mangers.  Maybe we could modularize these reaction to make it more unified. "
1344205855,14364,dajac,2023-10-03T14:29:18Z,@philipnee My point was that the `exception` received here may not be re-triable (e.g. UnsupportedVersionException) and that we may have to take actions on them (e.g. DisconnectException). Are you saying that we handle those in another component? My current understanding is that we don't update the state machine based on those exceptions at the moment because `onErrorResponse` is not called. Do we understand all the exceptions that we could receive here? We need to handle them all appropriately.
1344221907,14364,dajac,2023-10-03T14:36:18Z,"It seems that we could get the following ones:

```
@Override
        public void onComplete(final ClientResponse response) {
            if (response.authenticationException() != null) {
                onFailure(response.authenticationException());
            } else if (response.wasDisconnected()) {
                onFailure(DisconnectException.INSTANCE);
            } else if (response.versionMismatch() != null) {
                onFailure(response.versionMismatch());
            } else {
                future.complete(response);
            }
        }
```

+ `TimeoutException`"
1344230989,14364,dajac,2023-10-03T14:38:36Z,"Sorry, I was not clear. I was trying to say that we should just call `membershipManager.transitionToFailed();` in the relevant errors in the switch."
1344232148,14364,dajac,2023-10-03T14:39:14Z,"Do we want to backoff in this case though? It seems to me that we want to retry immediately when the new coordinator is discovered, no?"
1344236644,14364,dajac,2023-10-03T14:42:12Z,Could we please file a jira for this?
1344336463,14364,philipnee,2023-10-03T15:50:54Z,I see.  Sure we could certainly do that.
1344462484,14364,philipnee,2023-10-03T17:17:27Z,"I think we might want to refactor the coordinator requests per what you indicated above. A jira is filed: https://issues.apache.org/jira/browse/KAFKA-15531

Otherwise, in the OnFailure() method, I added the logic to fail the state machine and propagate the error when the error is non-retriable"
1344497810,14364,philipnee,2023-10-03T17:43:27Z,See `testHeartbeatOnStartup`  in the test. The first poll returns a request.
1344512748,14364,philipnee,2023-10-03T17:56:32Z,Here you go David: https://issues.apache.org/jira/browse/KAFKA-15533
1344515152,14364,philipnee,2023-10-03T17:58:41Z,"Thanks - the seems like the right way to do this to only backoff for COORDINATOR_LOAD_IN_PROGRESS error.  onFailedAttempt is removed. 

In the test `testHeartbeatResponseOnErrorHandling`: next heartbeat is verified
` assertEquals(0, heartbeatRequestState.nextHeartbeatMs(mockTime.milliseconds()));`"
1344560113,14364,lianetm,2023-10-03T18:40:40Z,duplicated above
1344567777,14364,lianetm,2023-10-03T18:47:54Z,"I expect this will be the place where we'll need to make sure to release any assignment we may have, right? Whenever a transitionToFailed/Fence occurs, the HB manager should call the reconciler to trigger the onPartitionsLost/Revoked as needed. Let's add a TODO just as we did for the missing bit of triggering the HB before the interval. "
1344589907,14364,philipnee,2023-10-03T19:03:45Z,This is actually invalid because the response can be null in this case. I filed a jira KAFKA-15278 to propagate the time from the networkClientDelegate
1344603478,14364,philipnee,2023-10-03T19:08:57Z,I added a test for the heartbeat timeout. See `testBackoffOnHeartbeatTimeout`
1345652576,14364,dajac,2023-10-04T11:38:27Z,I would add real javadoc to all attributes or none of them. This is what we have been doing for all the new java code recently.
1345661869,14364,dajac,2023-10-04T11:46:16Z,"nit: It would be good to be consistent in the log messages. They almost all start with `GroupHeartbeatRequest failed due...`. Why don't we do the same here? I think that this is important to know that the GroupHeartbeatRequest failed in this case as well. The same applies to COORDINATOR_LOAD_IN_PROGRESS.

`GroupHeartbeatRequest failed because the group coordinator %s is incorrect. Will attempt to find the coordinator again and retry`."
1345662992,14364,dajac,2023-10-04T11:47:13Z,nit: `GroupHeartbeatRequest failed because the group coordinator %s is still in the process of loading state. Will retry`.
1345667909,14364,dajac,2023-10-04T11:51:33Z,"It would be great to split those two in order to have separate error messages.

FENCED_MEMBER_EPOCH:
`GroupHeartbeatRequest failed because member epoch %s is invalid. Will abandon all partitions and rejoin the group`

UNKNOWN_MEMBER_ID:
`GroupHeartbeatRequest failed because member id %s is invalid. Will abandon all partitions and rejoin the group`"
1345669627,14364,dajac,2023-10-04T11:53:03Z,Should we revert this? It seems that we don't need it anymore.
1345670931,14364,dajac,2023-10-04T11:54:19Z,nit: Constants are usually formatted as follow: `HEARTBEAT_INTERVAL_MS`.
1345680555,14364,dajac,2023-10-04T12:01:21Z,nit: We can remove empty line here.
1345683631,14364,dajac,2023-10-04T12:03:20Z,So by default `heartbeatRequestState` is a mock but we don't prefix it with `mock`. This is inconsistent...
1345684050,14364,dajac,2023-10-04T12:03:37Z,Could we use `createManager()` here? 
1345687969,14364,dajac,2023-10-04T12:06:18Z,nit: Bring back on previous line.
1345688824,14364,dajac,2023-10-04T12:07:04Z,nit: final to be consistent with the others?
1345689109,14364,dajac,2023-10-04T12:07:21Z,nit: Remove `this.` to be consistent with the others.
1345692568,14364,dajac,2023-10-04T12:10:13Z,Should we have a test which verifies that fatal errors update the state machine?
1345695035,14364,dajac,2023-10-04T12:12:10Z,Should we add a unit which verifies the generated request? We have none doing this...
1345696189,14364,dajac,2023-10-04T12:13:08Z,nit: `Manually...`. Could you please verify all the other comments as well? I don't really care if they start with a capital letter or not but I do care about consistency...
1345697251,14364,dajac,2023-10-04T12:14:00Z,nit: Is this really needed? It seems that using `code` would work as well.
1345703205,14364,dajac,2023-10-04T12:18:13Z,"It is a bit weird to statically import those two but not the others (also used in this class), no?"
1345704121,14364,dajac,2023-10-04T12:18:58Z,nit: Could we align `errorCode` on `errorCode`?
1345704966,14364,dajac,2023-10-04T12:19:40Z,Should we add unsupported version as well?
1345951445,14364,philipnee,2023-10-04T14:55:54Z,Pretty sure I reverted in the previous commit....hmm
1346233433,14364,philipnee,2023-10-04T17:32:22Z,"Thanks, i'll just get rid of the mock there.  Probably unnecessary."
1346244285,14364,philipnee,2023-10-04T17:41:57Z,i refactored the if else block using switch.  I think it should make it a bit more readable.
1346245842,14364,philipnee,2023-10-04T17:43:22Z,See above - This section is refactored into swtich.
1346258637,14364,philipnee,2023-10-04T17:54:40Z,`ensureFatalError()` already ensure the transitionToFailed is invoked.  `testTransitionToFailure` in the membershipManagerImplTest also verifies the state transition.
1346463825,14364,philipnee,2023-10-04T21:05:36Z,Added `testValidateConsumerGroupHeartbeatRequest` - which validate the fields in the the requestBuilder.build(version) - Is that what do you mean?
1347439165,14364,dajac,2023-10-05T13:38:54Z,nit: Sorry but I missed those yesterday. Could we also align those log messages to the other format used: `GroupHeartbeatRequest failed....`?
1347441115,14364,dajac,2023-10-05T13:40:12Z,nit: empty line.
1347441582,14364,dajac,2023-10-05T13:40:31Z,nit: empty line.
1347447016,14364,dajac,2023-10-05T13:44:05Z,Hum... I don't follow. My point was that we don't have any tests verifying that we actually update the state machine when a fatal __exception__ is received [here](https://github.com/apache/kafka/pull/14364/files#diff-0eb94945bb75c3b1bda527708a48af59eafafc5bd9f85136fff0d0c405590497R209). Or did I miss it?
1347448120,14364,dajac,2023-10-05T13:44:49Z,"Yep, thanks."
1347449738,14364,dajac,2023-10-05T13:45:50Z,Should we verify all the fields? We also set others [here](https://github.com/apache/kafka/pull/14364/files#diff-0eb94945bb75c3b1bda527708a48af59eafafc5bd9f85136fff0d0c405590497R166).
1347455966,14364,dajac,2023-10-05T13:49:46Z,nit: Assure...
1347728174,14364,philipnee,2023-10-05T16:57:22Z,Done - I left a todo to verify client and server side assignors and pattern regex.
1348690378,14364,dajac,2023-10-06T12:58:35Z,"I am not really satisfied by the logging on failures:
* When there is a fatal error, it logs a warning follower by an error.
* The messages don't really follow the format of the other messages.

Should we just drop the warning on top? Could we update the remaining debug message to follow the structure of the other log messages?"
1348885178,14364,philipnee,2023-10-06T15:35:26Z,"thanks, let's drop the warn.  I also updated the debug message."
1788067718,17373,mumrah,2024-10-04T17:36:06Z,These (and other similar `testRuntimeOnly`) should be put into the `runtimeTestLibs` definition
1788068473,17373,mumrah,2024-10-04T17:36:26Z,What's this dependency for?
1788082764,17373,mumrah,2024-10-04T17:47:01Z,"This could break some existing Kafka installations. If users are extracting in place or copying previous config files to a new installation directory, they will be expecting the log4j.properties to still work."
1788846414,17373,frankvicky,2024-10-06T03:28:54Z,"Hi @mumrah 
I add this to fix the warning during build:
```
/home/frankvicky/.gradle/caches/modules-2/files-2.1/org.apache.logging.log4j/log4j-api/2.24.1/7ebeb12c20606373005af4232cd0ecca72613dda/log4j-api-2.24.1.jar(/org/apache/logging/log4j/Level.class): warning: Cannot find annotation method 'value()' in type 'BaselineIgnore': class file for aQute.bnd.annotation.baseline.BaselineIgnore not found
```
"
1810596051,17373,showuon,2024-10-22T12:04:05Z,"We should remove this file, right?"
1810598884,17373,showuon,2024-10-22T12:06:12Z,"Is this right? It's still possible it runs with log4j 1.x, right?"
1810600249,17373,showuon,2024-10-22T12:07:15Z,We still need reload4j here?
1810616327,17373,showuon,2024-10-22T12:18:04Z,"In[ previous config](https://github.com/apache/kafka/blob/trunk/config/connect-log4j.properties#L32-L39), we also set the pattern to ConnectAppender, right?"
1810618105,17373,showuon,2024-10-22T12:19:08Z,Should we remove them?
1810618615,17373,showuon,2024-10-22T12:19:30Z,Should we remove them?
1810625227,17373,showuon,2024-10-22T12:23:44Z,I don't understand the changes in the file. We move a log4j.properties in `connect/runtime` to `connect/mirror`? Why?
1810705740,17373,frankvicky,2024-10-22T13:13:55Z,"Yes, there are still some modules (like `tools`) that directly depend on `reload4j`, which we can't remove at the moment. Removing it would cause build errors.

```
Execution failed for task ':tools:compileTestJava'.
> Could not resolve all files for configuration ':tools:testCompileClasspath'.
   > Could not resolve ch.qos.reload4j:reload4j:1.2.25.
     Required by:
         project :tools
      > Cannot find a version of 'ch.qos.reload4j:reload4j' that satisfies the version constraints:
           Dependency path 'org.apache.kafka:tools:4.0.0-SNAPSHOT' --> 'ch.qos.reload4j:reload4j:1.2.25'
           Constraint path 'org.apache.kafka:tools:4.0.0-SNAPSHOT' --> 'ch.qos.reload4j:reload4j:{strictly 1.2.19}' because of the following reason: version resolved in configuration ':tools:compileClasspath' by consistent resolution
           Dependency path 'org.apache.kafka:tools:4.0.0-SNAPSHOT' --> 'org.slf4j:slf4j-reload4j:1.7.36' (compile) --> 'ch.qos.reload4j:reload4j:1.2.19'
```"
1810711392,17373,frankvicky,2024-10-22T13:17:09Z,"Yes, we should. However, I'm thinking that removing the zk configurations might be better handled as a follow-up PR, since this one is already quite large. 

WDYT ?"
1810722170,17373,frankvicky,2024-10-22T13:23:19Z,Hmm... I think it's just an issue with GitHub detection. I modified these two files in place.  
1810744889,17373,frankvicky,2024-10-22T13:35:57Z,"Yes, I think the `ConnectAppender` setting in log4j1 is equivalent to the following, if I haven't misunderstood:

https://github.com/frankvicky/kafka/blob/8ec412ded2272e6c251539b926cfc584a1f0e4ec/config/connect-log4j2.properties#L25-L34"
1811786044,17373,showuon,2024-10-23T03:53:04Z,"It looks like it's our test depends on reload4j. In this case, we should try to fix it."
1811787630,17373,frankvicky,2024-10-23T03:54:44Z,"I see. 
I will try to deal with this issue."
1811789962,17373,showuon,2024-10-23T03:57:47Z,"Oh, I missed that! Thanks."
1811959867,17373,showuon,2024-10-23T06:31:13Z,"I'm not familiar with log4j2, so I don't understand the empty name here. Why can't we use `getRootLogger` as above?"
1811961770,17373,showuon,2024-10-23T06:33:03Z,Is this behavior change expected?
1811968367,17373,showuon,2024-10-23T06:39:10Z,This is for which method?
1811977005,17373,showuon,2024-10-23T06:45:28Z,We don't need `resolveLevel` now because log4j2 will do that for us? Do we have test for it?
1811982931,17373,showuon,2024-10-23T06:48:50Z,This change is for ZK removal?
1811990279,17373,showuon,2024-10-23T06:53:19Z,Why can't we change the root log level now?
1811993482,17373,showuon,2024-10-23T06:54:51Z,"OK, this is replaced with `reconfigure()`, right?"
1811996534,17373,showuon,2024-10-23T06:56:07Z,"I think our goal in this PR, is to remove the reload4j above. Thanks for looking into how we can completely remove it!"
1812025899,17373,showuon,2024-10-23T07:12:23Z,"I'm not familiar with log4j2, just want to confirm will it confuse the setting?"
1812137033,17373,frankvicky,2024-10-23T08:08:49Z,"Yes, exactly.
`reconfigure()` provides a more convenient way to hot reload log4j configurations.  "
1812147748,17373,frankvicky,2024-10-23T08:14:04Z,"Yes, the name `R` might be confusing. My intention was to purely transform from log4j1 to log4j2 without making any further unnecessary changes. However, it can definitely be renamed if needed. "
1812168338,17373,frankvicky,2024-10-23T08:23:53Z,"Yes, I intentionally made this change to align with the removal of zk tests."
1812209524,17373,frankvicky,2024-10-23T08:40:17Z,"We could, but to make the test more precise, we should avoid having the test depend on the root logger. We are now focusing on changing the log level of the `kafka` logger, which acts as the top-level logger for the `kafka.server.ControllerServer`, `kafka.log.LogCleaner`, and `kafka.server.ReplicaManager` components. This change ensures that we can still control Kafka's logging behavior without relying on the root logger."
1812297584,17373,frankvicky,2024-10-23T09:12:49Z,"In this test case, we focus on setting the level for the root logger. IMHO, it would be ideal if we could have a ""clean"" configuration to conduct this test case. If I understand correctly, we can create a new configuration programmatically at the beginning in this way to ensure that the test case is not affected by existing Log4j configurations."
1812309503,17373,frankvicky,2024-10-23T09:17:09Z,"Yes.
In Log4j2, the `getLevel` method returns either the explicitly set level or the effective level. We can roughly think of it as a combination of two methods (`getLevel` and `getEffectiveLevel`) from Log4j1."
1812370163,17373,showuon,2024-10-23T09:49:10Z,I see. But I still want to see if we can change the root logger to make sure we didn't break existing behavior. Maybe we can create one more test for it?
1812371827,17373,frankvicky,2024-10-23T09:50:14Z,"Yes, log4j2 will do it for us.
In Log4j2, when we retrieve a Logger instance from the `LoggerContext`, the `getLevel` method returns the effective log level, which includes any levels inherited from parent loggers up to the root logger. This means that if a logger doesnt have an explicit level set, `getLevel` will provide the inherited level, so we dont need to manually traverse the logger hierarchy as we did in the `resolveLevel` method."
1812372018,17373,showuon,2024-10-23T09:50:22Z,"OK, could we add some comments for these changes?"
1812372375,17373,frankvicky,2024-10-23T09:50:36Z,"Sadly, we don't have a test for it"
1812372794,17373,frankvicky,2024-10-23T09:50:55Z,"Oops, I will do some clean-up"
1812374332,17373,frankvicky,2024-10-23T09:51:54Z,"Sure, I will do that."
1812375243,17373,frankvicky,2024-10-23T09:52:28Z,Sure.  
1812406378,17373,showuon,2024-10-23T10:10:40Z,Let's add some tests for this class. Thanks.
1812696648,17373,frankvicky,2024-10-23T12:54:26Z,"Ah, I have just found that we already have test cases for it.
https://github.com/apache/kafka/blob/trunk/core/src/test/scala/kafka/utils/LoggingTest.scala"
1813031963,17373,frankvicky,2024-10-23T15:24:28Z,"https://github.com/apache/kafka/blob/2d896d9130f121e75ccba2d913bdffa358cf3867/build.gradle#L3354-L3375
Since `log4j-appender` is depending on `clients`, we will need to delete `log4j-appender` module if we want to entirely get rid of `log4j`.
https://issues.apache.org/jira/browse/KAFKA-17860

cc. @chia7712 "
1813405127,17373,mimaison,2024-10-23T19:17:30Z,The link also needs updating (as well as the line number)
1813427142,17373,mimaison,2024-10-23T19:36:09Z,"I'm not convinced we need that dependency. Also it seems to complain about an annotation so at least we should not need it at runtime, so we should not include it in our distribution package. Currently it's included in the artifact generated by `releaseTarGz`."
1813429859,17373,mimaison,2024-10-23T19:38:40Z,Let's keep the newline
1813430705,17373,mimaison,2024-10-23T19:39:25Z,We probably don't need this zookeeper logger
1814229869,17373,frankvicky,2024-10-24T04:31:35Z,"Fair enough, I will try to solve this one."
1814232672,17373,frankvicky,2024-10-24T04:36:33Z,"Yes,
We need to clean up the zk-related configurations. I plan to split this into a separate JIRA and PR, as previously discussed with @showuon.
WDYT ?

https://github.com/apache/kafka/pull/17373#issuecomment-2430832465"
1814233350,17373,frankvicky,2024-10-24T04:37:34Z,I have opened a PR for it #17588 
1814236919,17373,frankvicky,2024-10-24T04:43:17Z,"Since this line is directly linked to a file in the `trunk` branch, a better solution might be to modify the document after this PR gets merged. I can file a JIRA to track this modification if you agree.
WDYT?"
1819719894,17373,ppkarwasz,2024-10-28T20:35:35Z,"The BND annotations are intentionally in the `provided` Maven scope of all Log4j artifacts, so that these annotations with `CLASS` retention do not end up in the runtime classpath. You can do the same and add them as `compileOnly` in Gradle.

The compiler warnings should disappear once [JDK-8342833](https://bugs.openjdk.org/browse/JDK-8342833) is fixed.
Untile then we will remove the outdated ones (see apache/logging-log4j2#3133) in the next Log4j release, which should remove the warning on `Level`."
1819769400,17373,ppkarwasz,2024-10-28T21:12:45Z,"The switch from the legacy to the new configuration format can be based on the presence of specific files:

```shell
if [ -f ""$base_dir/../config/log4j.properties"" ]; then
    echo DEPRECATED: Using Log4j 1.x configuration file \$KAFKA_HOME/config/log4j.properties >&2
    echo To use a Log4j 2.x configuration, create a \$KAFKA_HOME/config/log4j2.xml file and remove the Log4j 1.x configration. >&2
    echo See https://logging.apache.org/log4j/2.x/migrate-from-log4j1.html#Log4j2ConfigurationFormat for details about Log4j configuration file migration. >&2
    export KAFKA_LOG4J_OPTS=""-Dlog4j.configuration=file:$base_dir/../config/log4j.properties""
elif [ -f ""$base_dir/../config/log4j2.xml"" ]; then
    export KAFKA_LOG4J_OPTS=""-Dlog4j2.configurationFile=$base_dir/../config/log4j2.xml""
fi
```"
1819834263,17373,ppkarwasz,2024-10-28T22:07:16Z,"Have you considered switching to a structured configuration format like XML or YAML?
The properties configuration format is not the default one and is not even one of the original ones (it appeared in version 2.4). It has [a lot of quirks](https://logging.apache.org/log4j/2.x/manual/configuration.html#java-properties-features) to make it easier to read, but also harder to understand.

The XML format does not require additional dependencies. YAML only requires [`jackson-dataformat-yaml`](https://mvnrepository.com/artifact/com.fasterxml.jackson.dataformat/jackson-dataformat-yaml/2.16.2) that will only take an additional 400 KiB in Kafka's distribution. In YAML the configuration file would look like:

```yaml
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the ""License""); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an ""AS IS"" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

# Unspecified loggers and loggers with additivity=true output to server.log and stdout
# Note that INFO only applies to unspecified loggers, the log level of the child logger is used otherwise
Configuration:
  Properties:
    Property:
      # Fallback if the system property is not set
      - name: ""kafka.logs.dir""
        value: "".""
      - name: ""logPattern""
        value: ""[%d] %p %m (%c)%n%ex""

  # Appenders configuration
  # See: https://logging.apache.org/log4j/2.x/manual/appenders.html
  Appenders:
    Console:
      name: STDOUT
      PatternLayout:
        pattern: ""${logPattern}""

    RollingFile:
      - name: KafkaAppender
        fileName: ""${sys:kafka.logs.dir}/server.log""
        filePattern: ""${sys:kafka.logs.dir}/server.log.%d{yyyy-MM-dd-HH}""
        PatternLayout:
          pattern: ""${logPattern}""
        TimeBasedTriggeringPolicy: { }
      # State Change appender
      - name: StateChangeAppender
        fileName: ""${sys:kafka.logs.dir}/state-change.log""
        filePattern: ""${sys:kafka.logs.dir}/stage-change.log.%d{yyyy-MM-dd-HH}""
        PatternLayout:
          pattern: ""${logPattern}""
        TimeBasedTriggeringPolicy: { }
      # Request appender
      - name: RequestAppender
        fileName: ""${sys:kafka.logs.dir}/kafka-request.log""
        filePattern: ""${sys:kafka.logs.dir}/kafka-request.log.%d{yyyy-MM-dd-HH}""
        PatternLayout:
          pattern: ""${logPattern}""
        TimeBasedTriggeringPolicy: { }
      # Cleaner appender
      - name: CleanerAppender
        fileName: ""${sys:kafka.logs.dir}/log-cleaner.log""
        filePattern: ""${sys:kafka.logs.dir}/log-cleaner.log.%d{yyyy-MM-dd-HH}""
        PatternLayout:
          pattern: ""${logPattern}""
        TimeBasedTriggeringPolicy: { }
      # Controller appender
      - name: ControllerAppender
        fileName: ""${sys:kafka.logs.dir}/controller.log""
        filePattern: ""${sys:kafka.logs.dir}/controller.log.%d{yyyy-MM-dd-HH}""
        PatternLayout:
          pattern: ""${logPattern}""
        TimeBasedTriggeringPolicy: { }
      # Authorizer appender
      - name: AuthorizerAppender
        fileName: ""${sys:kafka.logs.dir}/kafka-authorizer.log""
        filePattern: ""${sys:kafka.logs.dir}/kafka-authorizer.log.%d{yyyy-MM-dd-HH}""
        PatternLayout:
          pattern: ""${logPattern}""
        TimeBasedTriggeringPolicy: { }

  # Loggers configuration
  # See: https://logging.apache.org/log4j/2.x/manual/configuration.html#configuring-loggers
  Loggers:
    Root:
      level: INFO
      AppenderRef:
        - ref: STDOUT
        - ref: KafkaAppender
    Loggers:
      # Zookeeper logger
      - name: org.apache.zookeeper
        level: INFO
      # Kafka logger
      - name: kafka
        level: INFO
      # Kafka org.apache logger
      - name: org.apache.kafka
        level: INFO
      # Kafka request logger
      - name: kafka.request.logger
        level: WARN
        additivity: false
        AppenderRef:
          ref: RequestAppender
      # Uncomment the lines below and change log4j.logger.kafka.network.RequestChannel$ to TRACE
      # for additional output related to the handling of requests
#      - name: kafka.network.Processor
#        level: TRACE
#        additivity: false
#        AppenderRef:
#          ref: RequestAppender
#      - name: kafka.server.KafkaApis
#        level: TRACE
#        additivity: false
#        AppenderRef:
#          ref: RequestAppender
      # Kafka network RequestChannel$ logger
      - name: kafka.network.RequestChannel$
        level: WARN
        additivity: false
        AppenderRef:
          ref: RequestAppender
      # KRaft mode controller logger
      - name: org.apache.kafka.controller
        level: INFO
        additivity: false
        AppenderRef:
          ref: ControllerAppender
      # ZK mode controller logger
      - name: kafka.controller
        level: TRACE
        additivity: false
        AppenderRef:
          ref: ControllerAppender
      # LogCleaner logger
      - name: kafka.log.LogCleaner
        level: INFO
        additivity: false
        AppenderRef:
          ref: CleanerAppender
      # State change logger
      - name: state.change.logger
        level: INFO
        additivity: false
        AppenderRef:
          ref: StateChangeAppender
      # Authorizer logger
      - name: kafka.authorizer.logger
        level: INFO
        additivity: false
        AppenderRef:
          ref: AuthorizerAppender
```"
1820021534,17373,frankvicky,2024-10-29T03:08:13Z,"Hi @ppkarwasz,  
Thanks for your feedback!   
As you mentioned, the `.properties` file format indeed has a drawback of understand. I actually struggled when trying to transform the `.properties` file from log4j1 to log4j2 -- it was really painful to understand its meaning and transform them at same time. 

The yml format looks nice and is more readable, but changing the configuration format might require further discussion, especially since it would introduce additional dependencies to the project.  
I will file a jira to initiate a discussion on this."
1820025144,17373,frankvicky,2024-10-29T03:14:32Z,"https://issues.apache.org/jira/browse/KAFKA-17889

c.c. @showuon @mimaison @mumrah "
1820029662,17373,frankvicky,2024-10-29T03:22:31Z,"Thanks for the information.  
I have already changed its scope to compile time.
PTAL  "
1820546264,17373,mimaison,2024-10-29T10:36:02Z,"As long as we still support the old properties format, we can consider switching to a new format if users provide a log4j2 configuration file. I think it's worth starting a thread on the dev list to explain our options and gather some feedback. "
1820760649,17373,ppkarwasz,2024-10-29T13:06:21Z,"@mimaison,

To be precise, users will **always** be able to use the configuration format of their choice, regardless of the format adopted by Kafka. The choice of the configuration file format mostly concerns the **default** configuration files shipped in the `*.tar.gz` archive. If Kafka ships with a `log4j2.properties` file, users will feel forced to use that one and that is IMHO a terrible format to work with.

I have opened a [thread on `dev@kafka`](https://lists.apache.org/thread/khm0jn9f0vgp30pfyoy6jc0qy46sbklp) to start a discussion about the subject.

**PS**: There is currently a primitive [`Log4j1ConfigurationConverter` CLI tool](https://logging.apache.org/log4j/2.x/migrate-from-log4j1.html#Log4j2ConfigurationFormat) that allows users to automatically convert a `log4j.properties` files into a `log4j2.xml` file. I am currently working on extending the list of formats that can be automatically converted (cf. https://github.com/apache/logging-log4j2/issues/2080), but I will probably not have time to support the quirky `log4j2.properties` format."
1820849781,17373,mimaison,2024-10-29T13:52:44Z,"I understand what you mean. My point was that some users may have built custom `log4j.properties` files and run clusters with those, and we want that to continue working. For the new log4j2 files, then yes it makes sense to evaluate the different formats.

Thanks for opening a thread, it's very useful to get input from an Apache Logging PMC member to help us make decisions."
1820856701,17373,frankvicky,2024-10-29T13:55:50Z,"I have file a jira for it.
https://issues.apache.org/jira/browse/KAFKA-17892"
1824026165,17373,ppkarwasz,2024-10-31T07:51:37Z,"This looks pretty much as a maintenance headache for the Apache Kafka team. What will happen if the user switches logging implementation (at least 3 logging implementations are supported by the Log4j API, see [logging implementations](https://logging.apache.org/log4j/2.x/manual/installation.html#impl))?

It looks to me that you only use this for JMX. If that is the case, Log4j Core provides an [out-of-the-box JMX support](https://logging.apache.org/log4j/2.x/manual/jmx.html). You just need to enable it, since JMX is a potential source of security problems and is disabled by default.

If you need to get and set the levels for other reasons, please open a thread on `dev@logging`. Users like to change logger levels programmatically so often, that we'd better offer an implementation independent API for that."
1824185851,17373,mimaison,2024-10-31T10:00:05Z,This is used by Kafka Connect. We have a REST API that allows changing the log level of all instances in a Kafka Connect cluster. See https://cwiki.apache.org/confluence/display/KAFKA/KIP-495%3A+Dynamically+Adjust+Log+Levels+in+Connect for the details.
1824901568,17373,ppkarwasz,2024-10-31T17:41:09Z,"We can probably reach a consensus in the Logging PMC to release a new Log4j Configuration API, that you can use to abstract from the internals of the logging implementation (see [this thread on `dev@logging`](https://lists.apache.org/thread/ktdlo5br6jc39xo33vdsfsj2s200mm93)).

What is the planned release date for Kafka 4.x? If you wait until the end of the year, this class might not be necessary."
1825368063,17373,frankvicky,2024-11-01T02:42:16Z,"Hi @ppkarwasz 
Currently, AK 4.0 release is scheduled at January 29th 2025.
For further details, you can refer to the release plan:
https://cwiki.apache.org/confluence/display/KAFKA/Release+Plan+4.0.0
"
1832983990,17373,ppkarwasz,2024-11-07T16:26:28Z,"```suggestion
    public void append(final LogEvent event) {
        synchronized (events) {
            events.add(event.toImmutable());
```
By default log events are mutable and bound to one thread. They are cleared as soon as the logging call exits.
There is a [`log4j2.enableThreadlocals` kill switch](https://logging.apache.org/log4j/2.x/manual/systemproperties.html#log4j2.enableThreadlocals) that you can store in a [`log4j2.component.properties`](https://logging.apache.org/log4j/2.x/manual/systemproperties.html#property-sources) on the test classpath, but it is easier to just take an immutable snapshot.

You can also replace `LogCaptureAppender` with [`ListAppender`](https://github.com/apache/logging-log4j2/blob/2.x/log4j-core-test/src/main/java/org/apache/logging/log4j/core/test/appender/ListAppender.java) from the [`log4j-core-test` artifact](https://logging.apache.org/log4j/2.x/components.html#log4j-core-test). You can set it up with a config like:

```xml
<Configuration>
  <Appenders>
    <List name=""LIST""/>
  </Appenders>
  <Loggers>
    <Root level=""DEBUG"">
      <AppenderRef ref=""LIST""/>
    </Root>
  </Loggers>
</Configuration>"
1833659727,17373,frankvicky,2024-11-08T03:51:30Z,"Very appreciate!
I have applied it and tested it locally; it works like a charm.
As for replacing `LogCaptureAppender`, I think it's a great idea. 
IMHO, leveraging an existing tool is better than building our own. 
I will file a JIRA to initiate a discussion on this."
1833673224,17373,frankvicky,2024-11-08T04:17:59Z,https://issues.apache.org/jira/browse/KAFKA-17963
1843386874,17373,showuon,2024-11-15T08:39:46Z,nit: I thought we'll honor log4j2.properties when both log4j2.properties and log4j.properties exist. No? 
1843393551,17373,frankvicky,2024-11-15T08:45:31Z,"Make sense. 
Yes, we can change the order of the if-case to achieve that. I will update it in a follow-up PR."
1843491362,17373,ppkarwasz,2024-11-15T09:44:53Z,"I think that `log4j.properties` should have a higher priority than `log4j2.properties`:

- Fresh installations of Kafka 4.x will only have a `log4j2.properties` file.
- If we find a `log4j.properties` file, it means that it is either an upgraded installation of Kafka or the user copied their customized configuration."
1843502542,17373,showuon,2024-11-15T09:51:16Z,"Hmm... it makes sense. Already, let's keep the log4j.properties as the highest priority. Thanks."
1843597095,17373,mimaison,2024-11-15T11:01:29Z,"Why do you want to do it in a separate PR?
If we merge as is we instruct users to go check `clients/src/test/resources/log4j2.properties` but instead link to another file. If we update the comment we need to update the link."
1843599782,17373,mimaison,2024-11-15T11:04:00Z,Why do we recommend creating an XML file? Should we point to the migration guide and to the log4j2 example file Kafka will have under `config`
1843600763,17373,mimaison,2024-11-15T11:05:00Z,Same in the other scripts
1843603382,17373,mimaison,2024-11-15T11:07:32Z,Would `ConnectConfig` be a better name?
1843606711,17373,mimaison,2024-11-15T11:10:35Z,"We since removed ZooKeeper from the existing log4j properties files in trunk (https://github.com/apache/kafka/commit/085b27ec6e65565cd41336b14aed2824a6e154db), so let's not re-add ZooKeeper stuff to remove it again later."
1843610478,17373,mimaison,2024-11-15T11:14:24Z,Let's keep the newline
1843613244,17373,mimaison,2024-11-15T11:17:00Z,We can remove this new line to make it clearer the comment applies to both `org.apache.kafka.consumer` and `org.apache.kafka.coordinator.group`
1843628839,17373,mimaison,2024-11-15T11:26:47Z,Why are we adding this method? This looks like a rebase issue.
1843629078,17373,mimaison,2024-11-15T11:27:02Z,Let's keep the new line.
1843630758,17373,mimaison,2024-11-15T11:28:36Z,Is this file still used? 
1843631728,17373,mimaison,2024-11-15T11:29:38Z,Can we import `LogManager`?
1843633651,17373,mimaison,2024-11-15T11:31:29Z,Nit: I know dependencies are not fully ordered but can we insert it roughly where it should be in the list instead of appending at the end.
1843640613,17373,mimaison,2024-11-15T11:38:23Z,Let's keep the new line. Same in a few other files 
1843652254,17373,mimaison,2024-11-15T11:50:16Z,Do we really need this as `implementation`? This is make it part of our release artifact.
1843962004,17373,mimaison,2024-11-15T15:02:54Z,"This effectively changes the behavior of the `/admin/loggers` endpoint of the Connect REST API.
The endpoints accept the logger name in the path `/admin/loggers/{name}`. If the root logger is the empty string, it's not possible to query it anymore. I wonder if we should still expose the root logger as `root` (I assume it's possible to rename it somewhere here or in `LoggingResource`). cc @gharris1727 WDYT
"
1843981241,17373,frankvicky,2024-11-15T15:14:27Z,I will see if we could avoid it be included in release artifact 
1844010558,17373,frankvicky,2024-11-15T15:24:06Z,"Yes, it looks have a module name prefix is a little bit silly."
1844089662,17373,frankvicky,2024-11-15T15:51:09Z,Make sense. I will apply it in next commit
1844097109,17373,frankvicky,2024-11-15T15:56:43Z,Yes... It seems that this method has been removed in `trunk`. I will remove it in next commit.
1844099567,17373,frankvicky,2024-11-15T15:58:30Z,It seems that it's not in use based on ide hint. I will remove it and build to see if we could remove this one.
1845315012,17373,dongjinleekr,2024-11-17T07:20:30Z,"@mimaison @frankvicky @gharris1727

**We should retain the root logger's name as `root`, against log4j2's naming change.** Here is the comment from [the original implementation](https://github.com/apache/kafka/pull/7898/commits/fd34fbc8c454ddf1558a05acc932e44601ef9f87#diff-eafb580880c7f5ea6aa6a710cecf3acd4c0ede5ec5ac0998c60cd9e92abc5176L51):

> Note: In log4j, the root logger's name was ""root"" and Kafka also followed that name for dynamic logging control feature.
> 
> The root logger's name is changed in log4j2 to empty string (see: [[LogManager.ROOT_LOGGER_NAME]]) but for backward-compatibility. Kafka keeps its original root logger name. It is why here is a dedicated definition for the root logger name.

To maintain compatibility with Kafka Connect's REST API, we need to use `root` to indicate the root logger.

Since the log4j2 logger names are generally `{package}.{class}` form, defining a `root` named logger is almost not reasonable. So, we don't need to be concerned about it.

@showuon Long time no see :smiley: Since it is not mentioned in the [KIP documentation](https://cwiki.apache.org/confluence/display/KAFKA/KIP-653%3A+Upgrade+log4j+to+log4j2), so I will add a subsection explaining this design decision. I got some vacation this week! "
1845315796,17373,frankvicky,2024-11-17T07:24:38Z,"@dongjinleekr 
Very appreciate your explanation. I will modify it in the next commit.  "
1846332278,17373,mimaison,2024-11-18T10:31:56Z,Should we remove connect-log4j.properties?
1846332506,17373,mimaison,2024-11-18T10:32:07Z,Should we remove log4j.properties?
1846355549,17373,frankvicky,2024-11-18T10:43:48Z,"I think it's fine now since our script has sufficient protection. It should work well when upgrading from the old version. 
I will delete these two configurations in the next commit."
1846417125,17373,mimaison,2024-11-18T11:20:49Z,"Yes because otherwise we default to the log4j file so you get the warning everytime you run a command:
```
DEPRECATED: Using Log4j 1.x configuration file $KAFKA_HOME/config/log4j.properties
```"
1846550136,17373,chia7712,2024-11-18T13:04:11Z,"we don't need to recreate collection -
```java
        return loggerConfigs.stream()
                .map(LoggerConfig::getName)
                .distinct()
                .map(LogManager::getLogger)
                .collect(Collectors.toUnmodifiableList());
```"
1846556605,17373,chia7712,2024-11-18T13:09:17Z,"why don't we keep the `root` compatibility here? After this PR, users can't set 'root=warn' to change the root level."
1846632146,17373,chia7712,2024-11-18T13:55:54Z,we need to keep the null handle to avoid NPE
1847704612,17373,chia7712,2024-11-19T06:20:06Z,Do they use the same reference? or we should use `!equals` instead of `!=`?
1847704673,17373,chia7712,2024-11-19T06:20:11Z,ditto
1847752728,17373,chia7712,2024-11-19T07:11:00Z,"Please fix the `connect.py`

https://github.com/apache/kafka/blob/trunk/tests/kafkatest/services/connect.py#L367
https://github.com/apache/kafka/blob/trunk/tests/kafkatest/services/connect.py#L424"
1847767385,17373,chia7712,2024-11-19T07:24:24Z,`compileOnly` is good enough I think as we don't use the annotation in runtime
1847776149,17373,chia7712,2024-11-19T07:28:48Z,Could you please remove this method also?
1847864958,17373,chia7712,2024-11-19T08:12:33Z,"`ReplicationQuotasTestRig` has similar code 
https://github.com/apache/kafka/blob/trunk/tools/src/test/java/org/apache/kafka/tools/other/ReplicationQuotasTestRig.java#L98

Could we delete it in this PR too?"
1847870812,17373,chia7712,2024-11-19T08:17:09Z,Do we really need `log4j1Bridge2Api`? 
1847872407,17373,chia7712,2024-11-19T08:18:21Z,please remove it from https://github.com/apache/kafka/blob/trunk/LICENSE-binary#L257 too
1847873077,17373,chia7712,2024-11-19T08:18:50Z,"ditto 

https://github.com/apache/kafka/blob/trunk/LICENSE-binary#L316"
1847874109,17373,chia7712,2024-11-19T08:19:37Z,please add it to `LICENSE-binary` file
1847964777,17373,frankvicky,2024-11-19T09:21:34Z,"Some APIs rely on it, such as `PropertyConfigurator`. However, since we are removing it, we might not need the Log4j bridge anymore. I will test this locally."
1848323271,17373,dongjinleekr,2024-11-19T13:03:22Z,"@chia7712 @frankvicky When I worked on this issue last, it was required to support the log4j 1.x configuration file."
1848672753,17373,chia7712,2024-11-19T16:23:18Z,"in the e2e we could run kafak on different version, so we must check the version before applying the config file."
1848672880,17373,chia7712,2024-11-19T16:23:22Z,ditto
1848672960,17373,chia7712,2024-11-19T16:23:25Z,ditto
1850373880,17373,chia7712,2024-11-20T14:02:53Z,"it seems connect e2e does not run different version for workers, so you can just change them to `connect_log4j2.yaml`. However, please change the `-Dlog4j.configuration` to `-Dlog4j2.configurationFile`"
1850375006,17373,chia7712,2024-11-20T14:03:41Z,Please noted that the previous version should use `-Dlog4j.configuration` and trunk version should use `-Dlog4j2.configurationFile`
1850378074,17373,chia7712,2024-11-20T14:05:39Z,"```
        Policies:
          TimeBasedTriggeringPolicy:
            modulate: true
            interval: 1
```"
1850378575,17373,chia7712,2024-11-20T14:05:59Z,please add `filePattern`
1850514477,17373,ppkarwasz,2024-11-20T15:21:56Z,"Since there is only one triggering policy, there is no need to wrap it in a [`Policies` wrapper](https://logging.apache.org/log4j/2.x/manual/appenders/rolling-file.html#Policies).

BTW: there is a typo in the plugin name: it should be `Policies`, instead of `Polices`."
1850545236,17373,frankvicky,2024-11-20T15:35:32Z,Thanks for information  
1850570634,17373,frankvicky,2024-11-20T15:50:47Z,"It seems that tons of code needs to apply this change.
I will prepare it asap."
1860858017,17373,mimaison,2024-11-27T15:23:24Z,"With the current code, updating Connect loggers does not work. For example, I get:
```
$ curl -X PUT -H ""Content-Type: application/json""  -d '{""level"": ""DEBUG""}'  \ 
     localhost:8083/admin/loggers/root
{""error_code"":500,""message"":null}
```

We should not return an UnmodifiableList here. In the `loggers()` method above we call `add()` on the `List`. Here is the stack trace:
```
[2024-11-27 16:17:39,224] INFO Setting level of namespace root and children to DEBUG (org.apache.kafka.connect.runtime.Loggers:131)
[2024-11-27 16:17:39,234] ERROR Uncaught exception in REST call to /admin/loggers/root (org.apache.kafka.connect.runtime.rest.errors.ConnectExceptionMapper:65)
java.lang.UnsupportedOperationException: null
	at java.base/java.util.ImmutableCollections.uoe(ImmutableCollections.java:142) ~[?:?]
	at java.base/java.util.ImmutableCollections$AbstractImmutableCollection.add(ImmutableCollections.java:147) ~[?:?]
	at org.apache.kafka.connect.runtime.Loggers.loggers(Loggers.java:156) ~[connect-runtime-4.0.0-SNAPSHOT.jar:?]
	at org.apache.kafka.connect.runtime.Loggers.setLevel(Loggers.java:132) ~[connect-runtime-4.0.0-SNAPSHOT.jar:?]
	at org.apache.kafka.connect.runtime.AbstractHerder.setWorkerLoggerLevel(AbstractHerder.java:1186) ~[connect-runtime-4.0.0-SNAPSHOT.jar:?]
	at org.apache.kafka.connect.runtime.rest.resources.LoggingResource.setLevel(LoggingResource.java:132) ~[connect-runtime-4.0.0-SNAPSHOT.jar:?]
```"
1864432648,17373,chia7712,2024-11-30T18:52:38Z,"`get_log4j_config_for_connect(node)` does not point to the ""full"" path, so all related services can't start up as it fails to find the log4j2 config. Please use `os.path.join(self.PERSISTENT_ROOT, get_log4j_config_for_connect(node))` instead"
1864432755,17373,chia7712,2024-11-30T18:53:07Z,"ditto. `get_log4j_config_for_connect(node)` is a file name rather than full path. please use `os.path.join(self.PERSISTENT_ROOT, get_log4j_config_for_connect(node))` instead"
1864432782,17373,chia7712,2024-11-30T18:53:14Z,ditto
1864435374,17373,chia7712,2024-11-30T19:03:51Z,I don't think those configs file are existent. please remove it
1864435870,17373,chia7712,2024-11-30T19:06:10Z,"ditto. use `os.path.join(self.PERSISTENT_ROOT, get_log4j_config(node))` instead"
1864435899,17373,chia7712,2024-11-30T19:06:25Z,"ditto. use `os.path.join(self.PERSISTENT_ROOT, get_log4j_config(node))` instead"
1864456124,17373,chia7712,2024-11-30T19:42:15Z,why do we override the config path? it breaks all e2e since the custom log4j config can't be used.
1864456149,17373,chia7712,2024-11-30T19:42:24Z,ditto
1864456157,17373,chia7712,2024-11-30T19:42:27Z,ditto
1864457267,17373,chia7712,2024-11-30T19:46:59Z,"Im not sure why we override the `KAFKA_LOG4J_OPTS` here. We typically allow users to define custom `KAFKA_LOG4J_OPTS`. Moreover, overriding `KAFKA_LOG4J_OPTS` can break many end-to-end tests, as they often create log4j configurations dynamically and pass them through `KAFKA_LOG4J_OPTS`

Noted that we do not require users to strictly use the path `$base_dir/../config/log4j2.xml`."
1864898855,17373,chia7712,2024-12-01T13:45:14Z,@frankvicky please update it as well. the path is incorrect
1893936536,17373,ijuma,2024-12-20T13:23:56Z,Why did we do this in many files instead of kafka-run-class?
1893938757,17373,ijuma,2024-12-20T13:25:55Z,How come these are `implementation` while `slf4jLog4j` is `testImplementation`?
1893939705,17373,ijuma,2024-12-20T13:26:48Z,Why is this needed?
1893947193,17373,ijuma,2024-12-20T13:34:14Z,"Two questions:
1. Have we tested that this log configuration results in the same output as the previous one? In particular, we should avoid anything that requires collecting a stacktrace to log (we made sure of that for the previous configuration).
2. Have we benchmarked the system to make sure there aren't any regressions due to the new logging library? I saw a JIRA/PR saying that log4j2 has a particularly costly `getLogger` implementation."
1894092841,17373,chia7712,2024-12-20T15:45:05Z,[`Log4jController`](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/utils/Log4jController.scala#L21) needs it to configure the log level at runtime
1894121803,17373,frankvicky,2024-12-20T16:12:03Z,"I added `spotbugs` to address the compiler warnings. While we could suppress these warnings using `-Xlint:all,-classFile`, I prefer not to relax compiler checks.

Ideally, we should avoid depending on core-specific methods altogether. However, that would require thorough pass to our code. I suggest we create a separate JIRA ticket to track this architectural improvement as a future enhancement.

For further details: https://github.com/apache/logging-log4j2/issues/2144#issue-2061148244
"
1894124425,17373,chia7712,2024-12-20T16:14:07Z,"`jacksonDatabindYaml` is required to parse yaml config - see https://logging.apache.org/log4j/2.x/manual/installation.html#impl-core-config

however, it seems we can do a bit cleanup for `jacksonDatabindYaml` as not all modules need to add explicit reference"
1894164970,17373,chia7712,2024-12-20T16:50:29Z,"> Have we tested that this log configuration results in the same output as the previous one?

I will use log4j-2 transform tool to check the config later (https://logging.staged.apache.org/log4j/transform/cli.html#log4j-transform-cli)

>  In particular, we should avoid anything that requires collecting a stacktrace to log (we made sure of that for the previous configuration).

Could you please share the commit to me? I trace the history (https://github.com/apache/kafka/commits/3.9/config/log4j.properties) and fails to see the fix about ""avoid anything that requires collecting a stacktrace"". 

> Have we benchmarked the system to make sure there aren't any regressions due to the new logging library?

According to official docs (https://logging.apache.org/log4j/2.3.x/performance.html), the performance of ""logging"" has no obvious regression. 

>  I saw a JIRA/PR saying that log4j2 has a particularly costly getLogger implementation.

it seems the story is about `getLogger` - https://issues.apache.org/jira/browse/KAFKA-18046 and https://issues.apache.org/jira/browse/KAFKA-15141 - there is already a PR to fix it #17896 - we can discuss the improvement on it.

"
1894238151,17373,ppkarwasz,2024-12-20T18:05:23Z,"At Apache Logging we had several other issue reports regarding our usage of annotations in the `provided` scope (see https://github.com/apache/logging-log4j2/issues/3110#issuecomment-2423586754 for example).

Regarding the [`@SuppressFBWarnings` annotation](https://javadoc.io/doc/com.github.spotbugs/spotbugs-annotations/latest/edu/umd/cs/findbugs/annotations/SuppressFBWarnings.html) that causes this particular problem:

- IMHO the compiler should not issue any warnings if it is missing, since the annotation has a retention of `CLASS` and is totally invisible at runtime. I submitted [JDK-8342833](https://bugs.openjdk.org/browse/JDK-8342833) to change the compiler's behavior.
- Log4j Core could theoretically move Spotbugs Annotations from the `provided` to the `compile` scope, but this could cause legal problems, since the annotation library is licensed under LGPL and this can not be changed (see https://github.com/spotbugs/spotbugs/issues/3144). This is one of the reasons we keep the library in the `provided` scope, so it does not propagate to consumers."
1894553541,17373,ijuma,2024-12-21T04:35:02Z,"Yeah, the licensing issue is the reason I wanted to avoid using this library at all. Can we use a different library for annotations, one that is Apache licensed?"
1894553658,17373,ijuma,2024-12-21T04:36:54Z,"We should perhaps bite the bullet and just make the slf4jLog4j choice for the server (i.e. include it as `implementation`). After all, unless you choose this, some functionality won't work."
1898304621,17373,ppkarwasz,2024-12-27T06:48:34Z,"IMHO the `kafka_<scala_version>` artifact should not have these dependencies, these dependencies should be added **only** to the binary Kafka distribution. Otherwise Kafka will leak the Log4j Core dependencies to its consumers, similarly to what was happening with Zookeeper (see [ZOOKEEPER-4820](https://issues.apache.org/jira/browse/ZOOKEEPER-4820)).

To add dependencies only to the binary distribution, you could use something similar to apache/eventmesh#4719, i.e. a separate `distOnly` Gradle configuration.

`Log4jController` can be rewritten to use Log4j Core if present or a no-op implementation otherwise, so the Log4j Core can be declared as an [optional Gradle dependency](https://blog.gradle.org/optional-dependencies).

**Note**: I am working on a `org.apache.logging:logging-admin` artifact (see [`ppkarwasz/logging-admin` repo](https://github.com/ppkarwasz/logging-admin)) that would provide the same functionality as `Log4jController`, but in a logging implementation independent way. Unfortunately I have a long TODO list before I can publish it, so probably it won't be ready for Kafka 4."
1898313661,17373,chia7712,2024-12-27T07:06:32Z,"`Log4jController` is used exclusively by the server, so exposing its dependencies should be acceptable."
1898356316,17373,ppkarwasz,2024-12-27T08:27:49Z,"In Kafka 3.9.0, the dependency on `ch.qos.reload4j:reload4j` is declared `compileOnly`:

https://github.com/apache/kafka/blob/84caaa6e9da06435411510a81fa321d4f99c351f/build.gradle#L970

The dependency is added to the binary distribution in a separate task:

https://github.com/apache/kafka/blob/84caaa6e9da06435411510a81fa321d4f99c351f/build.gradle#L1027-L1037

These dependencies should probably be handled in a similar way."
1898357031,17373,ppkarwasz,2024-12-27T08:29:30Z,"**Note**: No modern library uses Log4j 1 in code (they use JCL, SLF4J or Log4j API), so my guess is that `libs.log4j1Bridge2Api` could be dropped entirely."
1898365578,17373,chia7712,2024-12-27T08:44:31Z,"> These dependencies should probably be handled in a similar way.

Yes, we can declare them as compileOnly and then add them to the distribution. However, the flexibility of replacing the SLF4J provider at runtime may break the functionality of Log4jController (similar to [Juma's comment](https://github.com/apache/kafka/pull/17373#discussion_r1894553658)). KIP-1064 is attempting to use SLF4J2's system variable to choose the provider more effectively.

> No modern library uses Log4j 1 in code (they use JCL, SLF4J or Log4j API), so my guess is that libs.log4j1Bridge2Api could be dropped entirely.

Pardon me, in the #18290 we decide to allow users to use log4j.properties - so we still need `log4j-1.2-api`, right?

 "
1898478552,17373,ppkarwasz,2024-12-27T12:16:27Z,"> Yes, we can declare them as compileOnly and then add them to the distribution. However, the flexibility of replacing the SLF4J provider at runtime may break the functionality of Log4jController (similar to [Juma's comment](https://github.com/apache/kafka/pull/17373#discussion_r1894553658)). KIP-1064 is attempting to use SLF4J2's system variable to choose the provider more effectively.

As far as I can tell, this is the way `Log4jController` worked in Kafka 3.x: if the optional `ch.qos.reload4j` dependency was absent, the class didn't work.

> Pardon me, in the #18290 we decide to allow users to use log4j.properties - so we still need `log4j-1.2-api`, right?

Sorry, my mistake."
1898534270,17373,chia7712,2024-12-27T14:12:27Z,"> As far as I can tell, this is the way Log4jController worked in Kafka 3.x: if the optional ch.qos.reload4j dependency was absent, the class didn't work.

Yes, you're right. Perhaps we should consider offering similar functionality for other popular SLF4J providers, such as Logback and JUL. @ijuma WDYT?"
1898950768,17373,ijuma,2024-12-28T16:39:13Z,"@ppkarwasz It is true that we previously tried hard to avoid making the logging choice for the Maven artifact while making it for the distributed binaries. However, this is brittle and only worked partially. When I tried to fix it in #12148, it caused problems and it was partially reverted (#16260, #16559).

Also, it's actually bad to silently not support the dynamic logging functionality for the broker (this is _incredibly_ useful in production).

So, I think the simplest thing is to make the logging choice explicit for the server modules (the rare user who doesn't want that can still override it with exclusions via their build file) and leave it up to the applications for the client modules. In the future, if there is a way to address these issues, we can change it again. There are two promising and complementary paths:

1. Your logging admin library.
2. slf4j2 makes it possible to choose the logging library dynamically instead of via classpath tricks."
1898983229,17373,ppkarwasz,2024-12-28T20:04:38Z,"> 1. Your logging admin library.

I pushed the draft to Apache Logging ([`apache/logging-admin`](https://github.com/apache/logging-admin)) and I'll start to actively work on it. Probably you can expect a release by end of January/February.

In the meantime I can make a PR for Kafka, so that `Log4jController` fails softly if Log4j Core is not present.

> 2. slf4j2 makes it possible to choose the logging library dynamically instead of via classpath tricks.

Note that choosing the SLF4J implementation does not really tell you which logging implementation is being used: except Logback, all the other SLF4J implementation are bridges between logging APIs. If you use `slf4j-jdk14` you don't know which JUL implementation is being used and if you use `log4j-slf4j2-impl` you don't know which Log4j API implementation is being used."
1899049780,17373,chia7712,2024-12-29T02:16:54Z,"> In the meantime I can make a PR for Kafka, so that Log4jController fails softly if Log4j Core is not present.

Yes, it would be great to display accurate error messages!"
1166622209,13561,divijvaidya,2023-04-14T09:56:51Z,"Should this operation be performed in a separate thread pool which can have a defined quota? (similar to how we perform cleaning for local log using separate cleaner/background threads).

I am concerned that this may impact the rate of copy to remote if amount of cleaning is large. Also, it's perhaps better to have different scaling characteristics for cleaning from remote vs. copying. Copying maybe considered urgent since slowness in copying can potentially fill up disk whereas cleaning from remote may be a lower priority activity."
1166661016,13561,divijvaidya,2023-04-14T10:28:00Z,"The current logic may cause deletion of more data than anticipated.

This is because it is possible to have remote segments satisfying this condition which are not part of the current leadership epoch chain. Calculation of totalSizeEarlierToLocalLogStartOffset may include these segments as well and hence, the calculation of totalSize will include local segments + all remote segments (< local log start offset). 
The calculated TotalSize will be actually larger than the actual total size (where actual total size = size of remote + local log for the active epoch chain). This will lead to higher value of remainingBreachedSize than actual and hence, more data gets deleted than necessary.

Is this making sense? Else I can provide an example to explain it better.

"
1166767890,13561,divijvaidya,2023-04-14T12:20:05Z,"We don't need to calculate this for time based retention. Right? If yes, can we refactor the code here so that we perform size calculation (since it requires a full scan over all log segments) only when it's required i.e. for size based retention."
1166839800,13561,divijvaidya,2023-04-14T13:30:56Z,"we are only interested in segments in state COPY_SEGMENT_FINISHED or DELETE_SEGMENT_STARTED here. Right? (DELETE_SEGMENT_STARTED to clean up any stragglers)

Can we make this more explicit by filtering on them?"
1166850232,13561,divijvaidya,2023-04-14T13:40:18Z,could we check for `if (isCancelled() || !isLeader())` here again please to short circuit this expensive loop during shutdown.
1166854317,13561,divijvaidya,2023-04-14T13:44:11Z,"`isCancelled() || !isLeader()`

Also, please add a log which can help operator understand that this was actually cancelled. "
1166856223,13561,divijvaidya,2023-04-14T13:45:51Z,please guard this update with `isLeader()`
1166868212,13561,divijvaidya,2023-04-14T13:56:20Z,"Unreferenced segments are not only the ones which have a lower epoch that earliest known epoch, they could also be ones which have an epoch that is not part of active epoch chain. How are we handling that?"
1166876777,13561,divijvaidya,2023-04-14T14:03:57Z,"I would appreciate your response on https://lists.apache.org/thread/pc6x3xr4b8y84g43dcbdzh170kb5oz1v where we discussed that even with caching mechanism, warm up of the cache is going to slow down the copying. I agree that this can be discussed outside the scope of this PR but adding the above thread as FYI."
1166892667,13561,divijvaidya,2023-04-14T14:13:29Z,Do we need this at a warn level?
1166896440,13561,divijvaidya,2023-04-14T14:17:00Z,I would suggest to have address local retention in a separate PR. We can limit this PR to handling remote log retention only.
1167074461,13561,Hangleton,2023-04-14T16:53:04Z,"Hmm, I don't think it is enough to evict the data for epochs < than the current leader's smallest epoch. With unclean leader election, it is possible to have divergence in-between a log prefix and suffix shared by two replicas."
1167086128,13561,Hangleton,2023-04-14T17:01:09Z,"Hmm, it seems we are iterating over all remote segment metadata every time the expiration task is executed. This could become costly if the RLMM implementation does not cache the said metadata. That could be an explicit implementation constraint for plugin providers. Maybe we could also add a small layer a memoization here to avoid traversing the log metadata every time."
1180118575,13561,showuon,2023-04-28T08:50:56Z,"> Should this operation be performed in a separate thread pool which can have a defined quota? (similar to how we perform cleaning for local log using separate cleaner/background threads).

Good suggestion. But we don't include this part in the original KIP, we need another KIP to improve it. "
1180121259,13561,showuon,2023-04-28T08:53:12Z,"We might need to add logs here to describe why we need to update highest offset in remote storage for followers. I think that's for fetch from follower replica feature, right?"
1180123353,13561,showuon,2023-04-28T08:55:09Z,nit: assume that segments contain size >= 0
1180137735,13561,showuon,2023-04-28T09:06:15Z,I agree this could be costly if the RLMM implementation doesn't cache the metadata. But I don't think there's an implementation constraint for plugin providers. They can always cache them in the plugin. I'm thinking it should be enough if we add something about it in the RLMM#listRemoteLogSegments javadoc.
1180254846,13561,divijvaidya,2023-04-28T10:41:45Z,"That is fair. As I mentioned in the [review overview](https://github.com/apache/kafka/pull/13561#pullrequestreview-1385125558), I am fine (and would actually prefer) with creating JIRAs and tackling the perf related comments outside this PR. With this comment, I wanted to make sure we are aware and are tracking things that need fixing in this code."
1181520975,13561,satishd,2023-05-01T11:24:07Z,"This is different from local log deletion. It requires the deletion of segments from local storage which need to really delete the files. 
But incase of remote storages, it does not wait for the data to be deleted but it marks the file or object for deletion in their respective metadata stores. Respective garbage collectors in those storages will take care of deleting the data asynchronously. There is no perf impact for these delete calls as they take a much shorter time than copying segments. It is very unlikely that copying segments get affected because of the deletion of segments. Deletion checks are happening in every iteration so there will not be many segments that need to be deleted. 
Anyways, we can discuss this separately in a separate JIRA. On another note, all this logic will go to UnifiedLog in future.
"
1181526976,13561,satishd,2023-05-01T11:41:52Z,"log.retention.<> configs indicate the total amount of log segments that can be stored in remote storage. So, it is not just about the segments only related to the current leader epoch lineage. We need to be careful of removing any unreferenced segments and also should not have any segment leaks in the remote storage incase of unclean leader elections. So, it cleans up any unreferenced segments beyond the earliest leader epoch that are also available for retention checks."
1181527119,13561,satishd,2023-05-01T11:42:21Z,We do not need any specific check here as we want to clean up any segment that is not yet deleted including COPY_SEGMENT_STARTED
1181528820,13561,satishd,2023-05-01T11:46:37Z,Unreferenced segments within the current leader epoch chain will eventually move earlier to the earliest epoch of the current leader epoch chain after a few retention checks. That will take care of those kinds of segments. 
1181529149,13561,satishd,2023-05-01T11:47:19Z,"Sure, let us discuss this out side of this PR. "
1181532348,13561,satishd,2023-05-01T11:56:03Z,Unreferenced segments within the current leader epoch chain will eventually move earlier to the earliest epoch of the current leader epoch chain after a few retention checks. That will take care of those kinds of segments. 
1181545427,13561,showuon,2023-05-01T12:31:09Z,"It's confusing we update `_localLogStartOffset` twice with different value. I think the one in L982 should be removed, right?"
1182338603,13561,Hangleton,2023-05-02T09:53:16Z,"Hmm, I am not sure this is the right thing to do, because including segments which are not part of a log yields a size which is not truly that of the log. It is possible to design scenarios with a log chronology which allows for premature deletion of data under size-based retention.

While I understand that deleting unreferenced segments is consistent with the local log use case, where some data can be lost too, a key difference here between this approach and the current retention semantics applied for local logs is that in the latter case, all segments belong to the current log when the log size is calculated, so that the size-based retention policy always apply to the current log. Eviction of unreferenced segments/data in the local case happens via truncation separately from the enforcement of retention policies. But here, both are retention-based and truncation-driven eviction are _de facto_ combined.

What are the benefits of diverging from these semantics with tiered segments?"
1197648143,13561,Hangleton,2023-05-18T10:16:16Z,"I see, yes that's right those segments will eventually fall out of the range of active leader epochs. That should be fine, as long as users know there is no specific enforcement on the time those unreferenced segments will be cleaned up."
1203191600,13561,junrao,2023-05-24T00:10:33Z,unnecessary semicolon. Ditto in KafkaServer.
1203192234,13561,junrao,2023-05-24T00:11:04Z, missing javadoc for new param.
1203194554,13561,junrao,2023-05-24T00:12:15Z,Should we exit the loop the first time a remote segment offset passes localLogStartOffset?
1203195682,13561,junrao,2023-05-24T00:12:59Z,What about overlapping segments between remote and local? Do we double count them?
1203196737,13561,junrao,2023-05-24T00:13:53Z,Are we iterating the same remote segment multiple times since a segment could have multiple epochs?
1203197683,13561,junrao,2023-05-24T00:14:43Z," Loggering uses $ notation, which is for scala."
1203198445,13561,junrao,2023-05-24T00:15:23Z,"Hmm, not sure that I follow the comment."
1203200216,13561,junrao,2023-05-24T00:16:50Z,"Hmm, logStartOffset could be larger than base offset of the first local segment. So, it seems that we can't just switch to the base offset of the first local segment if remote log is not enabled."
1203200822,13561,junrao,2023-05-24T00:17:24Z, Should we return here to actually ignore?
1203201676,13561,junrao,2023-05-24T00:18:10Z,"Yes, agreed. Not sure why we need to update _localLogStartOffset here again."
1203202960,13561,junrao,2023-05-24T00:19:13Z,"Hmm, local retention needs to be bound by last tiered offset, right?"
1203204099,13561,junrao,2023-05-24T00:20:07Z,"retention time depends on remote storage being enabled, right? Ditto in line 2284."
1205425110,13561,showuon,2023-05-25T12:13:00Z,"debug(""Update {} with remoteLogStartOffset: {}"", topicPartition, remoteLogStartOffset)"
1205437354,13561,showuon,2023-05-25T12:20:51Z,additional `$` sign: remote log segment [$]{} ...
1209013637,13561,showuon,2023-05-29T07:33:46Z,nit: less than
1213100593,13561,satishd,2023-06-01T12:47:30Z,"`listRemoteLogSegments(TopicIdPartition topicIdPartition)` are not returned in any specific order. 
"
1213102668,13561,satishd,2023-06-01T12:49:15Z,"`totalSizeEarlierToLocalLogStartOffset` computes only the log segments in remote storage beyond local-log-start-offset. The remaining local log segments size is computed separately. So, there will be no overlapping segments."
1213107255,13561,satishd,2023-06-01T12:51:17Z,"No, it is taken care of. When we remove a remote log segment, it also updates that entry in RLMM in synchronous manner. So, RLMM store will remove the entry from respective epoch states."
1213107956,13561,satishd,2023-06-01T12:51:51Z,Updated the comment to make it more clear.
1213112946,13561,satishd,2023-06-01T12:55:34Z,"We already set the localLogStartOffset as max of passed logStartOffset and the first segment's base offset. 
When remote log is not enabled, `logStartOffset` is set as `localLogStartOffset` as computed above."
1213113458,13561,satishd,2023-06-01T12:56:00Z,This is addressed with the latest commits. 
1222183059,13561,junrao,2023-06-07T21:12:47Z,extra new line
1222183119,13561,junrao,2023-06-07T21:12:51Z,extra new line
1222183188,13561,junrao,2023-06-07T21:12:57Z,extra new line
1222189063,13561,junrao,2023-06-07T21:20:34Z,"Let's say there is a remote segment with startOffset 100 and endOffset 200. If the localLogStartOffset is 150, we exclude the remote segment. This means that we are undercounting the size, right?"
1222205974,13561,junrao,2023-06-07T21:43:25Z,"Hmm, why do we need a separate retention based on leader epochs? Is that not already covered by size/time/startOffset based retention?"
1222283938,13561,junrao,2023-06-07T23:42:27Z,This is already done in `updateLogStartOffset`. Do we need to do it here again?
1222315217,13561,junrao,2023-06-08T00:38:00Z,We already log the completion of the load loading in LogManager. Could we fold this there to avoid double logging?
1223300322,13561,junrao,2023-06-08T16:35:58Z,"Hmm, this logic doesn't look right. If a client calls `deleteRecords`, we call `maybeIncrementLogStartOffset` with onlyLocalLogStartOffsetUpdate=false. So, we will go through this branch and update _localLogStartOffset. This will be incorrect if remote log is enabled."
1223303905,13561,junrao,2023-06-08T16:39:51Z,Is highestOffsetInRemoteStorage inclusive or exclusive? It would be useful to document that.
1223312004,13561,junrao,2023-06-08T16:47:45Z,extra new line
1224374293,13561,divijvaidya,2023-06-09T14:22:38Z,please add a debug log here (and  other places where we are exiting this function) so that we know while debugging where did we exit the function from.
1224420683,13561,divijvaidya,2023-06-09T14:57:29Z,please add an info log on why we exited the function prior to it's completion. It greatly helps debugging when we don't have to guess where the return point was.
1226259513,13561,divijvaidya,2023-06-12T08:15:09Z,"please add the following check. We don't want to construct an object for RetentionSizeData if not required.

```
if (totalSize > retentionSize) {
    long breachedSize = totalSize - retentionSize
    return Optional.of(new RetentionSizeData(retentionSize, remainingBreachedSize));
}
```"
1226261499,13561,divijvaidya,2023-06-12T08:16:47Z,"please perform an argument validation here. If retentionSize < remainingBreachedSize, then IllegalArgumentException.

Same for RetentionTimeData"
1226268535,13561,divijvaidya,2023-06-12T08:21:09Z,This comment has been addressed in the latest code
1226280821,13561,divijvaidya,2023-06-12T08:31:03Z,"nit

unnecessary else"
1226315828,13561,divijvaidya,2023-06-12T08:53:45Z,"we need to restore the original value of remainingBreachedSize when remainingBreachedSize < 0?

May I suggest re-writing this entire predicate here as:
```
long segmentSize = metadata.segmentSizeInBytes()
remainingBreachedSize -= segmentSize
if (remainingBreachedSize < 0) {
    remainingBreachedSize += segmentSize
    return false
}

return true
```

Note that remainingBreachedSize is a member of the class and you don't need to do `retentionSizeData.get().remainingBreachedSize`. Also the earlier `if (retentionSizeData.get().remainingBreachedSize > 0) {` is made redundant by the code I suggested."
1226433742,13561,divijvaidya,2023-06-12T10:25:03Z,"nit

s/Log size after deletion/Local log size after deletion

Asking so that the reader can disambiguate between Log size (which is tiered + local) and local log size."
1226467554,13561,divijvaidya,2023-06-12T10:50:38Z,should we ensure that we have acquired the partition `lock` first? 
1226471102,13561,divijvaidya,2023-06-12T10:53:59Z,this should probably be a error level log because we don't expect to call this method when remote storage is disabled. Isn't that right?
1226517527,13561,divijvaidya,2023-06-12T11:33:37Z,"We are assuming that the state of local log will remain same from this point to the time we use the information computed here (i.e. totalSizeEarlierToLocalLogStartOffset ) to delete the segments. But that is not true since local retention threads are running concurrently and might have moved the localLogStartOffset by the time we use the `totalSizeEarlierToLocalLogStartOffset` computed here. As an example:

### Time instant: T1
LocalLSO = 10
LSO = 0
LSE = 20
TieredEO = 15

In this case we will calculate `totalSizeEarlierToLocalLogStartOffset` as the size from 0-10. 

### Time instant: T2
Local log retention thread deletes some stuff and updates the LocalLSO=14

### Time instant: T3
When we calculate `long totalSize = log.validLocalLogSegmentsSize() + totalSizeEarlierToLocalLogStartOffset;` at `buildRetentionSizeData`,  
validLocalLogSegmentsSize returns data from 14-20 and we say that the total size = totalSizeEarlierToLocalLogStartOffset ( i.e. 0-10) + validLocalLogSegmentsSize (i.e. 14-20).

This leads to data from 11-13 not being counted anywhere. This looks like a bug! We need to re-use the values stores at the beginning of the retention calculation otherwise other threads (local retention threads) may change the values behind the scenes.

Thoughts?
"
1226536841,13561,divijvaidya,2023-06-12T11:50:51Z,"Sorry, I am a bit confused here. Earlier in the comment https://github.com/apache/kafka/pull/13561#discussion_r1181526976 you mentioned that retention size/time configuration applies across all epochs. i.e. if I say retention is 3GB and the total log as per current epoch is 2 GB, but the total data stored in remote +local = 7GB, then I will delete (7-3) = 4GB of data as part of this cleanup. Is my understanding correct? If yes, then we seem to be deleting only the current leadership chain here BUT we are using the breached size from ALL the epochs calculated earlier. Isn't this contradictory?
"
1226538752,13561,divijvaidya,2023-06-12T11:52:40Z,"even if we are not the leader at this stage, we have deleted the logs in remote. Shouldn't we still update the metadata?"
1234978872,13561,satishd,2023-06-20T09:12:40Z,"It is inclusive, updated with the doc describing about the variable."
1234980098,13561,satishd,2023-06-20T09:13:45Z,I guess that is fine as retention size is more about the minimum size available in the topic partition. That segment will be deleted when the local-log-start-offset moves in later cycles.
1234980957,13561,satishd,2023-06-20T09:14:25Z,"It was updated based on log-start-offset with `updateLogStartOffset`, but local-log-start-offset can be more than that and it will be updated if needed."
1246792402,13561,jeqo,2023-06-29T15:29:31Z,"I also find it strange to repeat the mutations of HWM and local log recovery point in both updates, we can pull those two updates into a single method and call it once?"
1246831654,13561,jeqo,2023-06-29T15:52:44Z,"is it correct to update localLogStartOffset directly, but use updateLogStartOffset method to also update related values (HWM and local log recovery)?"
1247609605,13561,jeqo,2023-06-30T08:46:28Z,"found this a bit confusing. In main operation `onlyLocalLogStartOffsetUpdate` is false by default, but here we are overriding with `onlyLocalLogStartOffsetUpdate` as true, and methods signature are mainly the same. Wouldn't be clearer to use the default method with `onlyLocalLogStartOffsetUpdate=true` instead of creating this private method?"
1247799637,13561,jeqo,2023-06-30T12:19:02Z,"nit: 
```suggestion
                            BiConsumer<TopicPartition, Long> updateLogStartOffsetFromRemoteTier) {
```"
1247807726,13561,jeqo,2023-06-30T12:28:17Z,"> even if we are not the leader at this stage, we have deleted the logs in remote. 

If I'm reading the call path correctly, this is not the case. `handleLogStartOffsetUpdate` function is called only at the end of `cleanupExpiredRemoteLogSegments` that filters out calls from followers.
I guess we could either remote the `isLeader` validation here, or move this logic within the lambda itself?"
1247839591,13561,jeqo,2023-06-30T13:03:12Z,maybe worth adding a log message here stating why cleanup is not happening? or maybe just a comment explaining why this scenario may never happen given the low prob that recordVersion < 2 is used. 
1247868877,13561,jeqo,2023-06-30T13:30:32Z,"Could we add a log info here similar to copy?
```suggestion
                if (predicate.test(segmentMetadata)) {
                    logger.info(""Deleting remote log segment {}"", metadata.remoteSegmentId());
```"
1247878833,13561,jeqo,2023-06-30T13:40:08Z,"@satishd , could you elaborate a bit more what do you mean by
> Respective garbage collectors in those storages will take care of deleting the data asynchronously.

?
Is this relying on some specific storage backend implementation? "
1281564595,13561,showuon,2023-08-02T08:20:58Z,"@jeqo , What Satish meant, is in most remote storage case, the deletion API won't wait until data deleted in remote storage, instead, it'll mark file as deleted and return immediately. And run background GC in remote storage to delete the deleted flagged file."
1281568519,13561,showuon,2023-08-02T08:24:15Z,"I agree with @divijvaidya 's suggestion. In this scenario:
1. replica 1 is the leader, and doing remote log segment deletion
2. leadership changed to replica 2
3. replica 1 entering this `handleLogStartOffsetUpdate` method

Under current implementation, we won't update log start offset since it is not the leader anymore. But we should update it! @satishd , thoughts?"
1281575922,13561,showuon,2023-08-02T08:30:07Z,"+1, or at least a debug level."
1281648556,13561,showuon,2023-08-02T09:28:21Z,"As commented above, there might be chances that the leadership change during the segment deletion, I think we should update the log start offset before exiting the `cleanupExpiredRemoteLogSegments` method since if there's no deletion happened, the `logStartOffset` will be empty. WDYT?"
1281657244,13561,showuon,2023-08-02T09:35:36Z,Should we log partition info as below did here?
1281660832,13561,showuon,2023-08-02T09:38:41Z,"The comment is not clear:
`// Segment's first epoch's offset [should] be more than or equal to the respective leader epoch's offset.`

The log is not correct:
`""[{}]  Remote segment {}'s first epoch {}'s offset is [less] than leader epoch's offset {}."",`"
1281664514,13561,showuon,2023-08-02T09:41:56Z,+1
1281667417,13561,showuon,2023-08-02T09:44:36Z,Why don't we log `reason` here?
1281671130,13561,showuon,2023-08-02T09:47:59Z,nit: It's weird to see `local log retention size` when user is not enabled the tiered storage. Could we add a if check to see if remote storage is enabled or not and print the log accordingly?
1281672320,13561,satishd,2023-08-02T09:48:59Z,"All the size/time/startOffset handlers run based on the current leaders leader epochs. Here, we are removing the segments which have leader epochs earlier to the lowest leader epoch on this broker(partition leader)."
1281675518,13561,showuon,2023-08-02T09:51:35Z,Nice test!
1281844159,13561,satishd,2023-08-02T12:37:14Z,Good point! Addressed in the latest commits to keep the logic simpler.
1283024088,13561,satishd,2023-08-03T10:50:15Z,It is not mandatory to update it when this node becomes a follower as the existing follower fetch protocol makes sure that the follower truncates their log-start-offset based on the leader's log-start-ffset. 
1284241897,13561,satishd,2023-08-04T10:10:50Z,This is not required as the updated code does not use this method. 
1284278935,13561,satishd,2023-08-04T10:54:02Z,Good catch.
1284280124,13561,satishd,2023-08-04T10:55:05Z,Replied in the [comment](https://github.com/apache/kafka/pull/13561#discussion_r1283024088).
1287757841,13561,junrao,2023-08-08T22:37:26Z,Identation doesn't match other places in this file.
1287764297,13561,junrao,2023-08-08T22:50:50Z,"Since newLocalLogStartOffset is larger than localLogStartOffset(), could we just assign newLocalLogStartOffset to _localLogStartOffset?"
1288778006,13561,junrao,2023-08-09T15:52:12Z,"> All the size/time/startOffset handlers run based on the current leaders leader epochs. Here, we are removing the segments which have leader epochs earlier to the lowest leader epoch on this broker(partition leader).

Hmm, I still don't quite understand this part. The leader's epoch chain only gets trimmed from the beginning when segments are deleted due to retention or the advancement of the startOffset by `deleteRecord()` call. These are covered by the size/time based retention and logStartOffset based retention. So what additional cases does the following code cover?"
1288852233,13561,junrao,2023-08-09T16:18:28Z,"Here is a corner case. Let's say remote log is enabled, but there is no remote segment (all have been deleted due to retention). The new logic will do retention based on `localRetentionBytes`, but it should actually do the retention based on `retentionSize`. If that happens, we need to advance logStartOffset, in addition to localLogStartOffset."
1288860833,13561,junrao,2023-08-09T16:25:57Z,"Hmm, this should be false, right? Do we have a test case to cover that?"
1288884433,13561,junrao,2023-08-09T16:41:47Z,This is an existing issue. But there is one direct reference to `_localLogStartOffset` in `fetchOffsetByTimestamp()`. Should we change that to use `localLogStartOffset()` instead?
1288898898,13561,junrao,2023-08-09T16:54:23Z,"This doesn't look right. If remote log is not enabled, it seems that we should delete based on logStartOffset, not localLogStartOffset."
1289488513,13561,satishd,2023-08-10T03:08:13Z,"No, this should be true if the remote storage is not enabled as this segment should be eligible based on other checks like `highWatermark >= upperBoundOffset && predicate(segment, nextSegmentOpt)`. Existing tests in `UnifiedLogTest`, `LogOffsetTest`, `LogLoaderTest`, `LogCleanerTest` already cover those scenarios."
1289566534,13561,satishd,2023-08-10T05:25:05Z,"Local log size is based on the local retention configs and those are always less than or equal to the complete log retention.

I'm unclear about the rationale behind retaining data in local storage using an overall retention size where there are no remote log segments. Please provide clarification.
"
1289577612,13561,satishd,2023-08-10T05:42:01Z,Nice catch! Missed it while merging the conflicts.
1289595441,13561,satishd,2023-08-10T06:03:07Z,"This covers scenarios where unclean leader election happens and the remote storage contains segments that are earlier to the current leader's leader-epoch-lineage. 

For ex:

The current leader has the current leader-epoch-cache.
```
-----------------------------
leader-epoch | start-offset |
-----------------------------
     3              700
     4              780
     6              900
     7              990 					
-----------------------------
```
But the earlier broker which got replaced with a new broker which has the current leader's leader-epoch lineage.
```
-----------------------------
leader-epoch | start-offset |
-----------------------------
     0              0
     1              180
     2              400
-----------------------------
```
But these segments did not expire retention and they were not deleted in the remote storage. But these leader epochs are not there in the current leader's leader epoch as it was chosen with unclean leader election. In this case, we need to remove the segments, that exist beyond the current leader epoch lineage. Otherwise, they will never be cleaned up and will continue to accumulate in remote storage.

"
1291679491,13561,junrao,2023-08-11T18:47:15Z,"While you are here, could you also add the missing javadoc for brokerTopicStats?"
1291681393,13561,junrao,2023-08-11T18:49:53Z,Thanks for the explanation. It makes sense to me. Could you add a comment that this is needed for unclean leader election?
1291690729,13561,junrao,2023-08-11T19:02:54Z,"Here is what I mean. Ideally, the retention behavior should be unchanged with remote storage. Consider the following case without remote storage. Let's say retentionSize is 100MB and we have only 1 segment of 90MB. The retention logic won't trigger the deletion of the last segment.

Now, consider the same situation with remote storage enabled, but no remote segments. If localRetention is 20MB, the retention logic will delete last segment of 90MB. Since the data is not in remote storage. We have deleted the data a bit earlier than expected.

A similar issue exists for time-based retention. If remote storage is enabled, but no remote segments, the time-based retention is now based on localRentionTime, not retentionTime. Since the former can be smaller than the latter, it means that we could delete the data earlier than expected."
1291693299,13561,junrao,2023-08-11T19:06:35Z,"In that case, the name `isSegmentTieredToRemoteStorage` is a misnomer. If remote storage is disabled, there shouldn't be any segment tiered to remote storage, yet we are setting this val to true."
1291698612,13561,junrao,2023-08-11T19:14:23Z,"space after `if`.

Also, this logic still doesn't look quite right. If remote log is enabled, it seems that we still want to delete local segments whose offset is smaller than logStartOffset."
1292935520,13561,showuon,2023-08-14T03:23:22Z,"> In that case, the name isSegmentTieredToRemoteStorage is a misnomer.

Nice catch!"
1292967294,13561,satishd,2023-08-14T04:41:20Z,"When remote log is enabled, it deletes the local segments whose offset is <= local-log-start-offset. The existing condition without tiered storage is to delete the local log segments <= log-start-offset."
1292967405,13561,satishd,2023-08-14T04:41:39Z,It was implicit from the condition that it is relevant only when remote storage is enabled. I removed the value and added a condition and the respective comments for better clarity.
1293066497,13561,showuon,2023-08-14T07:23:37Z,Good point! I think it's worth filing a bug in JIRA. WDYT @satishd ?
1293071660,13561,showuon,2023-08-14T07:29:21Z,"So you mean, all the segment deletion will happen again in the new leader, and update the log start offset there. OK, make sense."
1293081560,13561,kamalcph,2023-08-14T07:39:13Z,"In the FETCH response, the leader-log-start-offset will be piggy-backed. But, there can be a scenario:

1. Leader deleted the remote log segment and updates it's log-start-offset
2. Before the replica-2 update it's log-start-offset via FETCH-request, the leadership changed to replica-2.
3. There are no more eligible segments to delete from remote.
4. The log-start-offset will be stale (referring to old log-start-offset but the data was already removed from remote)
5. If the consumer starts to read from the beginning of the topic, it will fail to read.

 I realised the case mentioned by @divijvaidya and this one is different. Both of them can be handled by the new leader gracefully. We can take this task in a follow-up PR if required."
1293086543,13561,kamalcph,2023-08-14T07:44:56Z,"To ensure consistency, similar to local, which marks the segment for deletion (renames the file to .delete) and deletes it after 1 minute. (segment.delete.delay.ms). Should we move the log-start-offset before the remote log segment deletion?

One way to do this is not to delete the remote log segments in `deleteRetentionTimeBreachedSegments` and `deleteRetentionTimeBreachedSegments` and only move the `logStartOffset`. In the next iteration, those remote-log-segments will be removed via `deleteLogStartOffsetBreachedSegments`.

WDYT? @satishd"
1293158752,13561,kamalcph,2023-08-14T08:31:22Z,Statements in L1065 and L1057 are same. Typo error?
1293179175,13561,satishd,2023-08-14T08:50:26Z,@kamalcph The case mentioned by you can be addressed in a followup PR. Please file a JIRA.
1293185679,13561,kamalcph,2023-08-14T08:56:07Z,"nit:

```suggestion
        TreeMap<Integer, Long> leaderEpochToStartOffset = new TreeMap<>();
        leaderEpochToStartOffset.put(0, 0L);
        leaderEpochToStartOffset.put(1, 10L);
        leaderEpochToStartOffset.put(2, 20L);
        leaderEpochToStartOffset.put(3, 30L);
        leaderEpochToStartOffset.put(4, 40L);
        leaderEpochToStartOffset.put(5, 50L);
        leaderEpochToStartOffset.put(7, 70L);
```"
1293189650,13561,kamalcph,2023-08-14T08:58:39Z,"`Optional` is not recommended as parameter in Java:

https://stackoverflow.com/a/31923105"
1293224397,13561,divijvaidya,2023-08-14T09:28:33Z,@satishd can you please address this comment. Multiple folks have asked me why this code of line exists which makes me believe that a comment explaining the purpose here would be nice.
1293244517,13561,divijvaidya,2023-08-14T09:46:56Z,"could we please store the value of log.logEndOffset() at the beginning of clean up process and use the stored value for all calculations? Asking because endOffset may move behind the scenes while we are processing cleaning. 

The overall idea is that this cleanup should be executing on a snapshot of log state."
1293286722,13561,divijvaidya,2023-08-14T10:26:10Z,"Isn't it possible for older epoch chain to become the current chain after another unclean election?

For example:

Time T1: Leader epoch chain
```
-----------------------------
leader-epoch | start-offset |
-----------------------------
     0              0
     1              180
     2              400
-----------------------------
```
Time T2: Unclean leader election occurs where the new leader loses all existing data and starts with new leader epoch
```
-----------------------------
leader-epoch | start-offset |
-----------------------------
     3              700
     4              780
     6              900
     7              990 					
-----------------------------
```
Time T3: Unclean leader election occurs again but the old leader from T1 becomes new leader (epoch 8). In this case, the current epoch chain will be 0->1->2->8. But we have deleted data from remote already pertaining to 0,1 and 2, even if it was not eligible for deletion based on retention. 

To remedy this situation, may I suggest that we delete the unreferenced segments ""only"" if we definitely know that they can be cleaned i.e. when they have exceeded the retention time or when the size in remote itself is greater than retention size. I have to check but I believe that local log solves it in a similar manner.
"
1293305304,13561,divijvaidya,2023-08-14T10:43:56Z,"Note that same segment may span across multiple epochs. Hence, same segment ID will be returned multiple times here and we will count it's size multiple times. 

May I suggest:

```
epochEntries.navigableKeySet().iterator
    .flatMap(epochEntry => remoteLogMetadataManager.listRemoteLogSegments(tpId, epochEntry.epoch).asScala)
    .filter(isRemoteSegmentWithinLeaderEpochs(epochEntries, _, logEndOffset))
    .distinctBy(_.remoteLogSegmentId.id())
    .map(segment => segment.segmentSizeInBytes())
    .reduceOption((a, b) => a.add(b))
    .getOrElse(0)
```


Also, if you agree that this was a bug, please add a unit test that should have failed."
1293325721,13561,divijvaidya,2023-08-14T11:06:09Z,you need to use this to correctly filter out segments at `findOffsetByTimestamp` method as well please.
1293327365,13561,divijvaidya,2023-08-14T11:08:05Z,"I believe we already have public accessor functions in LogConfig for these.

See LogConfig.localRetentionMs(), LogConfig.localRetentionBytes() and LogConfig.remoteStorageEnable()"
1293329291,13561,divijvaidya,2023-08-14T11:10:27Z,"You can instead use similar methods already present in LogConfig.

see LogConfig.localRetentionBytes() and LogConfig.localRetentionMs()

(you will probably have to modify them to add new case of `if (config.remoteLogConfig.remoteStorageEnable)`

"
1293332177,13561,divijvaidya,2023-08-14T11:14:01Z,The code in this PR still uses this method. No? What am I missing?
1293336126,13561,divijvaidya,2023-08-14T11:19:06Z,The current code uses the leader epoch chain to calculate the size. This comment is resolved in latest code.
1293345266,13561,satishd,2023-08-14T11:29:59Z,"Let me rephrase what you mentioned here

retention.bytes= 100MB
segment1 - 90MB

When remote storage is not enabled, then this segment is not deleted from local log segments becuas eof the retention size check.

retention.bytes= 100MB
local.retention.bytes= 20MB
segment1 - 90MB

When remote storage is enabled, and there are no segments uploaded to remote storage. That means it will not allow this segment to be deleted as it is not yet copied to remote storage based on the introduced check in this PR.

If it is copied to remote storage, that means it is not an active segment and there are one or more local segments after this segment. This segment will be eligible for deletion based on the local retention policy as it is already copied to remote storage earlier.

@junrao Am I missing anything here?"
1293402836,13561,nikramakrishnan,2023-08-14T12:36:13Z,+1. We should add this check to [findOffsetByTimestamp](https://github.com/apache/kafka/blob/43751d8d0521b1440a823a9430fdb0659ce7c436/core/src/main/java/kafka/log/remote/RemoteLogManager.java#L434C12-L434C12) to ensure we select the segment with correct leader lineage.
1293823449,13561,junrao,2023-08-14T18:24:26Z,Should we do the same for RetentionMsBreach to log whether the retention time is for local retention or not?
1293851913,13561,junrao,2023-08-14T18:55:48Z,"We want to be a bit careful of using this method. LeaderEpochCache is mostly derived from the data in the log. However, on new leader epoch from a leader change, the new leader also appends the new epoch to LeaderEpochCache before any record is appended for the epoch. This could cause a slight mis-match between the epoch chain in the remote segment and LeaderEpochCache. For example, it's possible for a LeaderEpochCache to have

10 100
11  200 //no record appended for epoch 11
12 200

where a segment's epoch chain only has 
10 100
12 200

We don't want to prevent the remote segment from being deleted through the retention logic because of this slight mismatch on leader epoch chain. Does the code allow for this?"
1293857043,13561,junrao,2023-08-14T19:00:04Z,"Yes, it just means that the segment won't be deleted until it's uploaded to the remote store. But this is probably ok."
1293857107,13561,junrao,2023-08-14T19:00:09Z,"@satishd : Sorry, I didn't give the right example. This is the case.

Without remote storage,
  retention.bytes= 100MB
  segment1 - 200MB
We will delete segment1 (even if it's the active segment).

With remote storage, 
  retention.bytes= 100MB
  local.retention.bytes= 20MB
  segment1 - 200MB
If segment1 is the active segment, it won't be deleted until it rolls and is uploaded to the remote store.

It's a very subtle difference."
1294602887,13561,satishd,2023-08-15T13:33:11Z,"I do not find a strong reason not to use Optional as an argument. :) In the same SO link, few other opinions on why it is a weak argument. 

Optional as an argument is used in several other places within this project. I do not have strong opinions and I am fine if we decide to go with that rule across the project when there is a consensus. We can revisit it when we do that."
1294603142,13561,satishd,2023-08-15T13:33:25Z,Good catch!
1294604024,13561,satishd,2023-08-15T13:34:15Z,What is the rationale for this suggestion?
1294618085,13561,jeqo,2023-08-15T13:45:28Z,"Could we elaborate what's the purpose of this validation? IIUC `(totalSize - retentionSize) > retentionSize`, are we validating that totalSize is not higher than 2 times `retentionSize`?"
1294828839,13561,jeqo,2023-08-15T16:11:10Z,"```suggestion
                throw new IllegalArgumentException(""retentionSize ["" + retentionSize + ""] must be greater than remainingBreachedSize ["" + remainingBreachedSize + ""]"");

```"
1294829317,13561,jeqo,2023-08-15T16:11:34Z,"```suggestion
                throw new IllegalArgumentException(""retentionMs ["" + retentionMs + ""] must be greater than cleanupUntilMs ["" + cleanupUntilMs + ""]"");
```"
1294843843,13561,satishd,2023-08-15T16:25:07Z,This method is used only from `locally` block and it does not require taking any lock. We moved this method inside the locally block to avoid any confusion and future usage outside of that.
1295387916,13561,satishd,2023-08-16T05:15:50Z,"Good catch! It was changed while refactoring, added UTs to cover that in the latest commits."
1295394486,13561,kamalcph,2023-08-16T05:28:16Z,Filed KAFKA-15351 and KAFKA-15352 to track the cases.
1295402786,13561,kamalcph,2023-08-16T05:42:48Z,"For clean code, it creates an anonymous extra class at every usage and we should try to avoid this pattern.

https://www.baeldung.com/java-initialize-hashmap"
1295607067,13561,satishd,2023-08-16T09:09:02Z,"Thanks @junrao for the clarification. 

In the above case with remote storage enabled, it will eventually be deleted from local and remote storages, and updates log-start-offset and local-log-start-offset respectively. "
1295670320,13561,satishd,2023-08-16T10:02:58Z,"Thanks @kamalcph for the clarification, good to know about that."
1295701347,13561,satishd,2023-08-16T10:33:17Z,"Thanks Jun for pointing it out.
Currently, segment epochs are created from leader epoch cache truncated with start and end offsets. But I added defensive checks to filter the epoch with empty records as they will not have any records/messages in the segments. These changes with UTs added in the latest commits. "
1295713103,13561,satishd,2023-08-16T10:45:11Z,"Follower replicas do truncation based on leader epoch lineage and catch up with the leader. It is hard to know whether a particular lineage can exist in any of the replicas as replicas can fail and it is hard to say whether a particular replica can come back with in a specific duration. That may cause leakages in remote storage. Follower replicas can not delete the remote segments as these may be part of the current leader and it may delete the data that is expected by the leader.  

The tradeoff taken in case of unclean leader election here is to clean up the epoch lineage earlier to the current leader epoch instead of creating segment leakages in remote storage.
"
1296218455,13561,junrao,2023-08-16T17:19:11Z,This can be simplified a bit to `.foreach{ log => ...}`. Ditto for the same code in BrokerServer.
1296221075,13561,junrao,2023-08-16T17:22:01Z,"Yes, I agree that it's not a large and common issue. So, we can leave it as it is for now."
1296264710,13561,divijvaidya,2023-08-16T18:04:46Z,">It is hard to know whether a particular lineage can exist in any of the replicas as replicas can fail and it is hard to say whether a particular replica can come back with in a specific duration.

Correct, that is why we should not be deleting data that we are unsure about. It's a durability loss! In a trade-off situation, wouldn't we want to trade-off in favour of durability instead of remote storage leak (which can be GC'ed by RSM implementation for such cases).

One way to solve is it is to delete the data that we know for sure is ready for deletion, e.g. if we have 10MB of data in remote store for non-active lineage and retention size is 2MB, then we can safely delete the rest of the 8MB. This is because even if this leadership chain becomes active, it will adhere to retention size.

In other words, I am not saying that we should not delete non-active lineage data in remote store. I am saying that the non-active lineage data should only be deleted if it when it is violating the retention policies. If we have time based retention, this will ensure that there are no leaks. If we have size based retention, then we can do what you are suggesting.

I will not consider this comment as blocking to merge this PR since this is in early access but we should document this risk of data loss as part of release notes and try to arrive at a conclusion before production release.

Thoughts @showuon @junrao @ivanyu @jeqo ?"
1296731885,13561,satishd,2023-08-17T06:27:15Z,">One way to solve is it is to delete the data that we know for sure is ready for deletion, e.g. if we have 10MB of data in remote store for non-active lineage and retention size is 2MB, then we can safely delete the rest of the 8MB. This is because even if this leadership chain becomes active, it will adhere to retention size.

It is hard or impossible to find the non-active lineage deterministically as the failed host can have any subset of the non active lineage. Determining which epoch/segments can be marked for deletion under such circumstances is not feasible. "
1296732476,13561,satishd,2023-08-17T06:27:58Z,"In case of unclean leader election, there is already a durability loss when a non in-sync replica needs to be chosen as a leader and given preference to availability. The approach taken in this PR uses the current tradeoff of durability loss and avoids remote log segment leaks. This is slightly different from local log cleanup which we can clarify in the release notes. 

Retention/cleanup logic spread across multiple layers(outside of Kafka) poses significant risks and could lead to more extensive problems. So, it is better that to be handled by Kafka's retention mechanism.

We will discuss further on finalizing the approach before we make this feature production ready. "
1297576757,13561,junrao,2023-08-17T18:11:51Z,"Hmm, should we use `remoteLogEnabled()` instead of `config.remoteLogConfig.remoteStorageEnable`? Ditto for `localRetentionSize`."
1297580311,13561,junrao,2023-08-17T18:15:41Z,"@divijvaidya has a good point. When doing unclean leader election, the new leader (even if unclean) should still have access to the remote data. So, it probably should never lose that portion of the data?"
1298029817,13561,satishd,2023-08-18T05:47:39Z,"That is a fair point. If we want to take that approach, we should not delete any segments beyond the current leader's leader epoch lineage. We need to take the risk of RSM plugins having cleanup mechanisms for the segment leaks in the remote storage. These leaks may accumulate over time and create operational issues. It is hard even for RSM plugin owners to deterministically find out whether a segment is unreferenced when there are out-of-sync/offline replicas.

I filed [KAFKA-15376](https://issues.apache.org/jira/browse/KAFKA-15376) to continue the discussion and take a final call later before productionizing it."
1298640349,13561,junrao,2023-08-18T16:25:11Z,What's the impact to 3.6.0? Do we need to outline any limitation with unclean leader election in the release notes?
1299184981,13561,satishd,2023-08-19T13:09:53Z,This is only applicable with tiered storage enabled topics. We will add that in the release notes of tiered storage section about the change in the behavior.
1299185257,13561,satishd,2023-08-19T13:12:33Z,Good point. Added the required filtering check `findOffsetByTimestamp` API. 
1299221576,13561,junrao,2023-08-19T16:58:45Z,What's the behavior of unclean leader election when tiered storage is enabled?
1299640804,13561,satishd,2023-08-21T05:55:11Z,"The remote storage retention cleanup mechanism considers cleaning up the remote log segments that have all the records that are created with a leader epoch precedes to the earliest leader epoch in the current leader's leader epoch lineage. 

In case of unclean leader election, the earlier leader replica may delete the segments that are copied to remote storage but those are not part of its leader epoch lineage but they may be part of out-of-sync or offline follower replicas and they will not be available for consumption."
1299988151,13561,jeqo,2023-08-21T11:33:03Z,"Similar to the previous comment on RentetionSizeData: `cleanupUntilMs` represents a point in time (`now - retentionMs`), while `retentionMs` represents a duration (e.g. 1 week in millis). Is this comparison correct/needed? If I'm reading this right, this will always be true."
1300055517,13561,divijvaidya,2023-08-21T12:39:26Z,"This takes an assumption that the partition has continuous monotonically increasing offsets. But it is not true for a topic that was historically compacted (i.e. compaction is turned off now, that is why TS is enabled).

I would suggest to read the next segment and set the startOffset as the start offset of the next segment."
1300176481,13561,satishd,2023-08-21T14:16:28Z,Good point. I think we discussed this earlier also. Let us address this in a followup PR covering topics changing their retention from compaction to delete only retention. Filed https://issues.apache.org/jira/browse/KAFKA-15388
1300178242,13561,satishd,2023-08-21T14:17:49Z,This check will be true when using system time. But added this defensive check if we have tests setting the mock time to set any long values.
1300199098,13561,divijvaidya,2023-08-21T14:32:48Z,"Sure. We can address this separately but I think that should be a blocker Jira for 3.6. Otherwise we are shipping this PR with a known bug which I am not very comfortable with. This bug is also not very edge case-y as others for which we have started Jira items such as bugs related to performance instead this bug impacts correctness. 

Do you agree?"
1300279331,13561,satishd,2023-08-21T15:23:52Z,"Yes, this is planned for 3.6.0. I did not want to block this PR with that as we want to unblock other dependent PRs, especially integration test PRs.  "
1300314011,13561,jeqo,2023-08-21T15:52:59Z,"> This check will be true when using system time.

Yeah, but the part I'm missing is why should we throw an exception when this is true. If retention is 1 hour, and `cleanupUntil` is at any point in system time, we are throwing an exception.  "
1300372620,13561,junrao,2023-08-21T16:40:19Z,"Hmm, I was just asking about how unclean leader election with tiered storage is handled in 3.6.0. It seems that this PR has removed the logic for retention by leader epoch. In that case, when an unclean leader is elected, does it just use its logEndOffset to start writing new data? In that case, do we just hide those remote segments with offsets higher than the new leader's starting logEndOffset? Will those hidden remote segments be cleaned up eventually? "
1301361619,13561,satishd,2023-08-22T09:38:39Z,"Right, I fixed the validation check. Thanks. "
1304597735,13561,satishd,2023-08-24T16:34:49Z,"Syncedup with Jun to understand the comment here and clarified them. The retention logic deletes the segments with leader epochs preceding the earliest leader epoch in the current leader. Any epochs/offsets which are not there in the current leader epoch lineage but they are within the range, those will be eventually deleted when the current leader's earliest leader epoch moves beyond that.

> In that case, when an unclean leader is elected, does it just use its logEndOffset to start writing new data?

Right, it will start writing with its logEndoffset with the new epoch.

> In that case, do we just hide those remote segments with offsets higher than the new leader's starting logEndOffset? Will those hidden remote segments be cleaned up eventually?

Right, they will be eventually removed. 
"
1307788374,13561,dopuskh3,2023-08-28T18:52:30Z,"@satishd it seems I'm reaching that codepath when running reassignments on my cluster and segment are deleted from remote store despite a huge retention (topic created a few hours ago with 1000h retention). 
It seems to happen consistently on some partitions when reassigning but not all partitions.


My test: 

I have a test topic with 30 partition configured with 1000h global retention and 2 minutes local retention
I have a load tester producing to all partitions evenly
I have consumer load tester consuming that topic
I regularly reset offsets to earliest on my consumer to test backfilling from tiered storage.

My consumer was catching up consuming the backlog and I wanted to upscale my cluster to speed up recovery: I upscaled my cluster from 3 to 12 brokers and reassigned my test topic to all available brokers to have an even leader/follower count per broker.

When I triggered the reassignment, the consumer lag dropped on some of my topic partitions: 
<img width=""2426"" alt=""Screenshot 2023-08-28 at 20 57 09"" src=""https://github.com/apache/kafka/assets/86608/14e8f43d-da77-40b9-83c6-bb930fe2763d"">

Later I tried to reassign back my topic to 3 brokers and the issue happened again. 

Both times in my logs, I've seen a bunch of logs like:
```
[RemoteLogManager=10005 partition=uR3O_hk3QRqsn4mPXGFoOw:loadtest11-17] Deleted remote log segment RemoteLogSegmentId{topicIdPartition=uR3O_hk3QRqsn4mPXGFoOw:loadtest11-17, id=Mk0chBQrTyKETTawIulQog} due to leader epoch cache truncation. Current earliest epoch: EpochEntry(epoch=14, startOffset=46776780), segmentEndOffset: 46437796 and segmentEpochs: [10]
```

Looking at my s3 bucket. The segments prior to my reassignment have been indeed deleted."
1308148879,13561,showuon,2023-08-29T03:14:54Z,"@dopuskh3 , thanks for reporting this issue. I've created [KAFKA-15414](https://issues.apache.org/jira/browse/KAFKA-15414) for this issue. Let's discuss it in JIRA."
1308153773,13561,satishd,2023-08-29T03:26:59Z,Thanks @dopuskh3 for bringing the observed issue here. There are a few more pending changes to be merged which are in review/planned related to this change. I will followup on [JIRA](https://issues.apache.org/jira/browse/KAFKA-15414). 
1433944692,13561,iit2009060,2023-12-21T11:26:05Z,"@divijvaidya  @satishd  @showuon  I gone through the specific code and realised this is actually not impacting the logic
1. While copying the remote segments , remotelogsegmentmetadata stores  endoffset using value from the nextSegment base offset.
https://github.com/apache/kafka/blob/5785796f985aa294c12e670da221d086a7fa887c/core/src/main/java/kafka/log/remote/RemoteLogManager.java#L693
2.  In my understanding it will be safe to use same logic for historically compacted topics. 
Let me know  If my  analysis is correct or not ?
"
1434040959,13561,divijvaidya,2023-12-21T12:52:59Z,Yes that is correct. Copying functionality is not impacted as discussed in https://issues.apache.org/jira/browse/KAFKA-15388. It's only the read-from-remote that is impacted for the historically compacted topic.
194783912,5201,bbejeck,2018-06-12T15:27:28Z,"This class is created to contain common information for repartition operations.  There are two types of repartitioning currently.  When changing a key in a `KStream`s method (`map`, `flatMap` `selectKey` etc) or when performing a `KTable.groupBy` operation.  The former is eligible for optimization, while the `KTable.groupBy` is not. "
194784561,5201,bbejeck,2018-06-12T15:28:57Z,There are 2 difference between the repartition operations the first is the `Serializer` and `Deserializer` required 
194784918,5201,bbejeck,2018-06-12T15:29:54Z,The second difference between repartition operations is the name used to wire up the repartitioning processor.
194785793,5201,bbejeck,2018-06-12T15:32:15Z,This class was created to represent the _**non optimizable**_ repartition resulting from a `KTable.groupBy` operation
194786010,5201,bbejeck,2018-06-12T15:32:50Z,Same class just moved to `graph` package
194786478,5201,bbejeck,2018-06-12T15:33:53Z,This class represents a repartition operation that _**is eligible**_ to get optimized away.
194787418,5201,bbejeck,2018-06-12T15:36:26Z,"The class is the same just moved to a new package. This is the same for all graph objects, so I won't continue to repeat this comment."
194790053,5201,bbejeck,2018-06-12T15:43:10Z,"Created to represent `KTable` operations (`filter`, `transformValues`, `mapValues`)"
194791681,5201,bbejeck,2018-06-12T15:47:39Z,"For now, this includes the recent changes from @guozhangwang (PR #5163) for optional re-use of source topic as changelog topic.  This optimization will get folded into this PR in a follow-up push."
198283461,5201,vvcephei,2018-06-26T20:18:55Z,"It seems like this is happening twice; once inside `addChildNode` and once outside it.

(also applies to other occurrences)"
198285554,5201,vvcephei,2018-06-26T20:26:02Z,nit: could be final ;)
198287289,5201,vvcephei,2018-06-26T20:31:43Z,I think these are never referenced. Is there still more to do?
198289563,5201,vvcephei,2018-06-26T20:39:02Z,"This is a bit beside the point, but do we actually need this? It's unused in our codebase. Do we expect users to subclass `AbstractStream`? I think the question is applicable; I'm still hoping we could finish disentangling the InternalTopologyBuilder from the StreamsGraph and InternalStreamsBuilder."
198289936,5201,vvcephei,2018-06-26T20:40:14Z,"Also, on Line 76, we're taking note of which nodes need to be copartitioned. Do we need to capture this information in the StreamsGraph as well?"
198290842,5201,vvcephei,2018-06-26T20:43:08Z,"Aside from these two usages, the only other purpose that the internalTopologyBuilder serves in the StreamsBuilder hierarchy is to add state stores. I think that if we add this to the logical plan first, then we could completely decouple the InternalTopologyBuilder from the InternalStreamsBuilder."
198291738,5201,vvcephei,2018-06-26T20:45:53Z,"Might be nice to throw an exception if this check fails. It would clearly be a mistake, and it might be nicer for it to break than to do nothing. (I'm specifically thinking if the builder is created, built, modified, and built again; the second modification would just be lost.)"
198293629,5201,vvcephei,2018-06-26T20:52:24Z,Nit: many variables in here can be final.
198294005,5201,vvcephei,2018-06-26T20:53:34Z,This field is never used.
198294951,5201,vvcephei,2018-06-26T20:56:39Z,"I wasn't sure why the `NodeIdComparator` implements `Serializable`. If it doesn't need to, then you can get rid of that class and that field and just do:
```
final PriorityQueue<StreamsGraphNode> graphNodePriorityQueue = new PriorityQueue<>(5, Comparator.comparing(StreamsGraphNode::id));
```"
198294955,5201,mjsax,2018-06-26T20:56:39Z,"If we remove the `null` check, should we add one in the constructor?"
198295985,5201,mjsax,2018-06-26T21:00:04Z,Why remove `final`?
198297056,5201,mjsax,2018-06-26T21:04:17Z,nit: add `final`
198298276,5201,mjsax,2018-06-26T21:08:19Z,Don't understand the log statement? Something missing there?
198298348,5201,mjsax,2018-06-26T21:08:34Z,nit: add `final`
198298794,5201,mjsax,2018-06-26T21:10:23Z,nit: add `final` (seems some more in the next lines)
198299478,5201,mjsax,2018-06-26T21:12:21Z,nit: move after the following `if` (we not' need to get the `keyChangingNode` if we `continue`
198299936,5201,mjsax,2018-06-26T21:14:03Z,"which ""StreamsGraphNode"" ?"
198301439,5201,vvcephei,2018-06-26T21:19:52Z,Super minor nit: I thought the code style says to always put arguments on a new line when splitting args over multiple lines. I only bring this up because I've been doing it that way...
198302388,5201,vvcephei,2018-06-26T21:23:31Z,final?
198304292,5201,mjsax,2018-06-26T21:30:43Z,Why do we need this here?
198306976,5201,mjsax,2018-06-26T21:41:28Z,nit: can we use variable instead of getting the names multiple times?
198307864,5201,mjsax,2018-06-26T21:45:01Z,Why this renaming?
198308379,5201,mjsax,2018-06-26T21:47:18Z,"nit: simplify `multipleParentNames` -> `parentNames` (`Names` is already plural, should be good enough).

Im wondering, it it might be better to track the parents in `StreamsGraphNode` ?"
198308755,5201,mjsax,2018-06-26T21:49:00Z,When could this be `null`?
198308922,5201,mjsax,2018-06-26T21:49:33Z,When would this be not empty?
198310099,5201,mjsax,2018-06-26T21:54:40Z,"Why doe we assume, that a stateful operator has always one parent? What about a materialized table-table join?"
198310662,5201,mjsax,2018-06-26T21:57:09Z,nit: remove empty line
198311099,5201,mjsax,2018-06-26T21:59:19Z,nit: introduce variable to get name only once?
198311844,5201,mjsax,2018-06-26T22:02:37Z,"This is done for both join, right? (ie, update comment?)"
198312036,5201,mjsax,2018-06-26T22:03:28Z,nit: `Steam - Table join [only]`
198312756,5201,mjsax,2018-06-26T22:06:45Z,"Why do we need this check? It seem that `parentNode` cannot be `null`, and that it would be `this` always, too? (assuming one parent node, what is not generic)"
198312832,5201,mjsax,2018-06-26T22:07:10Z,as above.
198313052,5201,mjsax,2018-06-26T22:08:18Z,"Why not extend `StatefulProcessorNode` ? Why remove the generic type? Can't it be windowed, too? What about custom stores in `transform()` ?"
198313486,5201,mjsax,2018-06-26T22:10:19Z,nit: add `final`
198314691,5201,mjsax,2018-06-26T22:15:52Z,"Would it be better to put a generic `<S>` her and change to `TableSourceNode<K, V, S extends StateStore>`? Also, should we take `builder.windowedTable()` into account already? Pretty sure the KIP will be accepted."
198319640,5201,mjsax,2018-06-26T22:40:51Z,nit: `final`
198320164,5201,mjsax,2018-06-26T22:43:53Z,as above
198368837,5201,guozhangwang,2018-06-27T05:14:07Z,"This is added for users that want to extend `AbstractStream`, one use case of it is https://github.com/fhussonnois/kafkastreams-cep.

Regarding `InternalTopologyBuilder` and `InternalStreamsBuilder`: we need to pass in `InternalTopologyBuilder` into the `StreamsGraphNode` because their `writeToTopology` (this is the one that translates the logical node into one or more physical nodes) needs it, and those graph nodes are accessed from the `InternalStreamsBuilder`, so I think we still need to let it hold a reference of the `InternalTopologyBuilder` anyways."
198368989,5201,guozhangwang,2018-06-27T05:15:10Z,+1
198369331,5201,guozhangwang,2018-06-27T05:18:00Z,"If they are used for the optimization rules that are to be added later, we should remove them from this PR. I'm now not so sure if @bbejeck wants to add the optimizations in this PR, but I'd suggest we do it in a forth one given the current PR is pretty large already."
198369849,5201,guozhangwang,2018-06-27T05:22:25Z,"If users call `StreamsBuilder#build()` multiple times, which we cannot forbid programmatically, we should still only call this function once; on the other hand, if users add a few more operations into the streams topology and then call `build()` again we should probably re-run optimization and generate a new topology, i.e.:

```
1. stream1 = builder.stream(..); stream1.groupby().aggrege();...
2. topology1 = builder.build();   // generate physical plan from the logical plan once.
3. topology2 = builder.build();   // nothing changed, we should return the same topology, hence making sure all the optimizations are deterministic.
4. stream2 = builder.stream(..); stream1.groupby().aggrege();...
5. topology1 = builder.build();   // generate a new physical plan from the logical plan again
```
"
198369989,5201,guozhangwang,2018-06-27T05:23:37Z,"+1, if we are not going to write / serialize the logical plan anywhere we do not need to do that."
198370646,5201,guozhangwang,2018-06-27T05:29:03Z,KGroupedTableImpl also have a duplicated `createRepartitionNode`.
198370821,5201,guozhangwang,2018-06-27T05:30:05Z,May need some more explanation of this optimization rule.
198370939,5201,guozhangwang,2018-06-27T05:31:08Z,"Meta comment: if we are going to add more optimization rules in `optimize()`, should we keep this PR as a plain one that do not enforce any optimizations, so that we can then consider each function separately, that helps more concentrated reviews and reduce large PR burdens."
198371994,5201,guozhangwang,2018-06-27T05:38:31Z,This is nice cleanup.
198372212,5201,guozhangwang,2018-06-27T05:40:13Z,Meta comment: seems we have migrated the node classes in a previous PR so could you update the description of this PR to remove the statement that we changed the package here?
198372226,5201,guozhangwang,2018-06-27T05:40:20Z,+1
198372742,5201,guozhangwang,2018-06-27T05:44:19Z,Should we override this for all the subclasses so that we can have a more informative intermediate logical plan representation for debugging purposes?
198372787,5201,guozhangwang,2018-06-27T05:44:40Z,+1.
198372943,5201,guozhangwang,2018-06-27T05:45:50Z,"I cannot tell how this includes the source topic reuse logic, could you explain a bit?"
198933994,5201,bbejeck,2018-06-28T18:04:46Z,ack
198937951,5201,bbejeck,2018-06-28T18:18:14Z,ack
198938045,5201,bbejeck,2018-06-28T18:18:33Z,ack merge mistake
198938944,5201,bbejeck,2018-06-28T18:21:36Z,ack. Removed for now as part 3 is for writing physical plan using graph. Part 4 will contain optimization only
198939184,5201,bbejeck,2018-06-28T18:22:19Z,same as above will clean up in part 4
198939337,5201,bbejeck,2018-06-28T18:22:47Z,same as above
198939460,5201,bbejeck,2018-06-28T18:23:08Z,"ack, will clean up in part 4"
198940600,5201,bbejeck,2018-06-28T18:26:37Z,"The repartition node just created, will clarify in part 4"
198941179,5201,bbejeck,2018-06-28T18:28:21Z,ack removed
198942912,5201,bbejeck,2018-06-28T18:33:21Z,ack
198944739,5201,bbejeck,2018-06-28T18:39:12Z,"During work on this PR, I started to consider this name was a better fit if you insist I can revert."
198945962,5201,bbejeck,2018-06-28T18:43:21Z,"ack on the name.

For now, I'd prefer to leave `parentNames` in the `ProcessorNode` as we specifically add multiple parent names in `KStream#merge`. Or maybe refactor always to take a list `parentNames` that would simplify the logic 

EDIT: I take that back, we want to rely on getting the parent name for the current graph node by calling `parentNode.name()` the multiple parent names comes from the case mentioned above and are needed for the `InternalStreamsBuilder` to complete the merge processor,  so I'll leave as is for now."
198950446,5201,bbejeck,2018-06-28T18:57:39Z,"I've removed this line, left over from a previous refactoring."
198951509,5201,bbejeck,2018-06-28T19:00:48Z,"Only used for merge node, so it's empty most of the time. I'll look to see if I can refactor internally."
198952083,5201,bbejeck,2018-06-28T19:03:02Z,"table-table joins are represented in a separate node, `KTableKTableJoinNode`.  I used a specific node for this case IMHO there is too much information needed in the table-table join to generalize."
198952415,5201,bbejeck,2018-06-28T19:04:17Z,ack
198954002,5201,bbejeck,2018-06-28T19:10:24Z,ack
198955006,5201,bbejeck,2018-06-28T19:14:43Z,ack
198955044,5201,bbejeck,2018-06-28T19:14:52Z,ack
198955718,5201,bbejeck,2018-06-28T19:17:29Z,"Since the optimization is pushed out to a 4th PR, this method is removed.  I'll clean up in 4th PR. 

The original idea was to prevent any errors by calling `clearChildren` _*after*_ the children of a given parent node have migrated to another parent.  Probably better to eliminate the checks and document the calling order when updating the graph."
198955766,5201,bbejeck,2018-06-28T19:17:40Z,same comment from above
198960187,5201,bbejeck,2018-06-28T19:33:56Z,ack
198963636,5201,bbejeck,2018-06-28T19:47:40Z,"I removed the generic type from the class definition and changed the `materializedInternal` from `S` to `KeyValueStore<Bytes, byte[]` as it matches the `MaterializedInternal` parameter used in `KTable#filter`, `KTable#mapValues`, `KTable#transformValues`.  I can revert that if you want.

Looking at `StatefulProcessorNode` it's only used from `KStream` for `process` and `transform`, but those methods never pass a strore builder or materialized, just store names.

 What makes sense to me is to refactor `StatefulProcessorNode` to remove the store builder then have `TableProcessorNode` extend `StatefulProcessorNode` . WDYT?

I'm not sure what you mean custom stores in `transform()`, `KStream#transform` takes a list of store names which is captured in the `StatefulProcessorNode` is that what you are referring to?"
198964111,5201,bbejeck,2018-06-28T19:49:32Z,ack
198970765,5201,bbejeck,2018-06-28T20:14:03Z,"Ack, required some other minor changes, you'll have to let me know what you think"
198972710,5201,bbejeck,2018-06-28T20:20:48Z,"EDIT: I've put the generic type back, but requires a cast internally, you have to let me know what you think"
198976922,5201,bbejeck,2018-06-28T20:35:09Z,Removed from this PR and delayed to 4th PR with optimization
198977076,5201,bbejeck,2018-06-28T20:35:46Z,ack
198977160,5201,bbejeck,2018-06-28T20:36:09Z,removed until 4th PR with optimization
198977315,5201,bbejeck,2018-06-28T20:36:42Z,This is required by findbugs.
198977471,5201,bbejeck,2018-06-28T20:37:15Z,ack
198981153,5201,bbejeck,2018-06-28T20:49:36Z,"That was the intent, build once then subsequent calls return the same physical plan from rebuilding.  

As for updates, I'm thinking the use case would be to build the topology incrementally and call build at the end versus incremental calls to build,  in which case the current approach still works."
198982348,5201,bbejeck,2018-06-28T20:53:43Z,"This is intentional. 
Today, when a repartition is required, the `InternalTopologyBuilder` creates a repartition operation and immediately writes it to the physical plan.  So we need to capture the repartition as a graph node and pass the new node to the `InternalStreamsBuilder` for possible metadata collection for optimization.  

In other cased graph nodes are created in classes that don't subclass the `AbstractStream`."
198982978,5201,bbejeck,2018-06-28T20:56:08Z,Ack removed optimization in favor of pushing a 4th PR with optimizations only once 3rd PR is merged.
199006843,5201,bbejeck,2018-06-28T22:31:33Z,"Ack will try to collapse the two in 4th PR, but for now, the `createReparitionNode` is removed from `InternalStreamsBuilder` until 4th PR"
199012496,5201,guozhangwang,2018-06-28T23:02:02Z,"Hmm.. for step 4) / 5), now the `topologyBuilt` would still be true and hence we would return the same topology, but that is incorrect right?"
199014145,5201,guozhangwang,2018-06-28T23:12:05Z,"Could you explain to us a bit more? I'm still scratching my head now on when do we call 

`AbstractStream#addGraphNode` 

v.s. 

`parentNode.addChildNode; builder.maybeAddNodeForOptimizationMetadata(repartitionNode);`

Because their logic are just the same, right?"
199272791,5201,mjsax,2018-06-29T20:30:11Z,"I am 51:49 to keep the old name (so, I don't insist in reverting). \cc @guozhangwang @vvcephei WDYT?"
199274082,5201,mjsax,2018-06-29T20:36:06Z,Ack. Makes sense that `transform` does not apply here as this is a `*Table*ProcessorNode`.
199282205,5201,bbejeck,2018-06-29T21:16:21Z,"yep, I'll have to put some thought into what's relevant without being too spammy"
199286587,5201,bbejeck,2018-06-29T21:40:26Z,ack updated
199288962,5201,bbejeck,2018-06-29T21:53:38Z,"What I was thinking of was the developer would build an initial topology with the last statement being the `builder.build()...` Then at that point execute the program and observe the printed topology.  Then go back and update the topology again and run the program a separate time and watch the results.

But thinking about it more, that is an opinionated/subjective view of how to develop, and we should not restrict to one style.  I'll put something to detect if the topology has changed then rebuild if true."
199289400,5201,bbejeck,2018-06-29T21:56:25Z,"I try to follow that as well, but maybe I'm missing something because I thought the args were all on one line here."
199291907,5201,bbejeck,2018-06-29T22:11:22Z,"Yes, the logic is the same.  But `GroupedStreamAggregateBuilder` does not subclass `AbstractStream` so we need to make those two calls separately.

Secondly, in the example above we are building 2 graph nodes.  The repartition node and the stateful processor node for the aggregation.  

Today when we need to repartition for an aggregation, the `InternalTopologyBuilder` creates a repartition operation and it becomes the parent of the aggregation operation.  So the repartition node needs to be a child of the current `StreamsGraphNode` in the `GroupedStreamAggregateBuilder` but it must be the parent of the aggregation graph node. 

Does this make sense?

"
205591823,5201,bbejeck,2018-07-26T20:28:02Z,"For removing the restriction of only building topology once, I've decided on the ""traditional"" approach when traversing a graph to only visit (in this case write its contents to the `InternalTopologyBuilder`) graph nodes not already visited.

I feel a similar approach should work with optimization. But since this PR does not include the optimization, I'd prefer to defer any discussions on what will and won't work for multiple `build()` calls until we have the PR for applying the optimization pushed."
206391213,5201,guozhangwang,2018-07-31T04:22:32Z,nit: you can configure the IDE to turn auto-newline-beyond-column-limit off :) 
206392683,5201,guozhangwang,2018-07-31T04:36:58Z,nit: this.streamsGraphNode can be replaced with parentNode.
206393083,5201,guozhangwang,2018-07-31T04:40:49Z,Why change Collection to Set? The former is more generalized right?
206393265,5201,guozhangwang,2018-07-31T04:42:32Z,The `StreamsGraphNode#internalStreamsBuilder()` seems not used anywhere?
206393456,5201,guozhangwang,2018-07-31T04:44:33Z,"Ack, lgtm."
206395841,5201,guozhangwang,2018-07-31T05:06:50Z,"I know it is cherry-picked from @vvcephei 's PR, but it seems we drops this type information in the logical StreamsGraphNode anyways. Maybe @vvcephei can comment whether we still need this?"
206395968,5201,guozhangwang,2018-07-31T05:07:45Z,Why we pass `null` before and now we need to pass in the `transformNode`? Was it a bug before and we fixed it here?
206396093,5201,guozhangwang,2018-07-31T05:08:34Z,nit: `processNode`.
206397008,5201,guozhangwang,2018-07-31T05:16:05Z,"The comments here are a bit confusing: line 63 is for both globaltable-stream join and table-stream join, and line 66 is for table-stream join only."
206397146,5201,guozhangwang,2018-07-31T05:17:28Z,nit: selectKeyMapNode.
206397274,5201,guozhangwang,2018-07-31T05:18:43Z,"We did not add a new logical node here anymore in this PR, is this intentional?"
206397509,5201,guozhangwang,2018-07-31T05:20:37Z,nit: rename to `parentGraphNode`.
206398136,5201,guozhangwang,2018-07-31T05:26:19Z,"These four graph nodes: thisWindowedStreamsNode, thisStreamsGraphNode, otherWindowedStreamsNode, otherStreamsGraphNode, seems not needed any more since we now use an ""umbrella"" joinGraphNode that contains all the information, right? Ditto for other join implementations.

If it is indeed the case, then I'm wondering why our unit test does not fail, as it will unnecessarily add more processor nodes, and hence should cause some unit test to fail."
206399370,5201,guozhangwang,2018-07-31T05:36:22Z,nit: groupByMapNode.
206399786,5201,guozhangwang,2018-07-31T05:39:58Z,Should be `OptimizableRepartitionNode{ + super.toString() + }`?
206400130,5201,guozhangwang,2018-07-31T05:42:46Z,Ditto here.
206400597,5201,guozhangwang,2018-07-31T05:46:48Z,nit: Since we have another class named `ProcessorNode` already could we still name it `StatelessProcessorNode`?
206401093,5201,guozhangwang,2018-07-31T05:50:22Z,"The `StreamsGraphNode#parentName` and here `parentNames` relationship is a bit awkward, as the only difference is for `merge`. Could we just use `parentNames` on the `List<String> StreamsGraphNode` directly, and replace `setParentNode` with `addParentNode`. And in all other nodes we just call `addParentNode` only once, while for `merge` we call it twice for each fo the merging streams."
206401149,5201,guozhangwang,2018-07-31T05:50:49Z,See my other comment above: we can replace this logic with the `addParentNode`.
206401318,5201,guozhangwang,2018-07-31T05:52:07Z,I think this coding restyle is really not necessary.
206401445,5201,guozhangwang,2018-07-31T05:53:16Z,"Just to be general enough, we should just loop over all parent nodes from the list and add all their node names here."
206401533,5201,guozhangwang,2018-07-31T05:53:56Z,Why changing collection to list?
206401753,5201,guozhangwang,2018-07-31T05:55:37Z,For line 35 above: see my other comment: `internalStreamsBuilder` seems not used anywhere.
206402166,5201,guozhangwang,2018-07-31T05:58:28Z,"Ditto above: if we take the underlying `StreamsGraphNode` to have `list of parentNodes`, then for all such callers we will generalize to loop over all parent nodes and add their node names. Although for now we will only have two parents for `merge`, and one parent for any other types."
206402321,5201,guozhangwang,2018-07-31T05:59:35Z,The `StoreBuilder` should be templated as `StoreBuilder<S>`. Ditto elsewhere.
206402481,5201,guozhangwang,2018-07-31T06:00:44Z,Same here.
206402673,5201,guozhangwang,2018-07-31T06:02:11Z,Should we just do `final ProcessorTopology topology = builder.build();` and remove the line below? Same elsewhere.
206402910,5201,guozhangwang,2018-07-31T06:03:58Z,nit: add final.
206566399,5201,vvcephei,2018-07-31T15:06:46Z,nit: alignment is off.
206566569,5201,vvcephei,2018-07-31T15:07:13Z,nit: alignment is off.
206567717,5201,vvcephei,2018-07-31T15:10:00Z,nit: alignment is off.
206567780,5201,vvcephei,2018-07-31T15:10:10Z,nit: alignment is off.
206571206,5201,vvcephei,2018-07-31T15:18:46Z,nit: alignment is off.
206571401,5201,vvcephei,2018-07-31T15:19:13Z,nit: alignment is off.
206574955,5201,bbejeck,2018-07-31T15:28:16Z,Ack
206575220,5201,vvcephei,2018-07-31T15:28:59Z,"Yeah, in this case, the type parameter is unused, and we're also suppressing warnings. I think we can either ditch the parameter or remove the suppression.

FWIW, I have a todo after this PR is merged to make another pass to eliminate warnings and unnecessary suppressions, so feel free to just drop the type parameter, and I'll figure out whether the suppression is necessary later on. So as to not expand this PR further."
206576264,5201,vvcephei,2018-07-31T15:31:34Z,"We were never actually using the graph before, right? So a missed node might go unnoticed until now, when we actually build the topology from the graph."
206576886,5201,vvcephei,2018-07-31T15:33:19Z,This is (and was) a weird line break.
206577284,5201,vvcephei,2018-07-31T15:34:10Z,`notNullKeyPredicate`?
206586295,5201,bbejeck,2018-07-31T15:57:32Z,ack
206586526,5201,bbejeck,2018-07-31T15:58:11Z,"Can't recall, I think it was related to changes for optimization implementation, I'll revert for now."
206587699,5201,bbejeck,2018-07-31T16:01:11Z,"ack, removed

EDIT: put back for now to avoid findbugs error."
206591585,5201,bbejeck,2018-07-31T16:13:05Z,looking from the history looks like it was a bug but fixed here.
206591870,5201,bbejeck,2018-07-31T16:14:02Z,ack
206596270,5201,bbejeck,2018-07-31T16:27:39Z,"ack, updated"
206597007,5201,bbejeck,2018-07-31T16:29:54Z,ack
206597490,5201,vvcephei,2018-07-31T16:31:23Z,"If so, then this comment would apply to all the other nodes as well."
206599239,5201,vvcephei,2018-07-31T16:37:04Z,"I thought it was a little weird previously that `StatefulProcessorNode` extends `StatelessProcessorNode`, since subclassing is generally an ""is a"" relationship. So it was previously saying ""StatefuProcessorNode is a StatelessProcessorNode"", which is silly of course.

Maybe we can call it `ProcessorGraphNode` to avoid a collision?"
206600066,5201,vvcephei,2018-07-31T16:39:49Z,:+1: I think this was from my misinterpretation of the code style.
206601158,5201,bbejeck,2018-07-31T16:43:32Z,"Yes, it's intentional. What I found during testing is that we don't need to create a node here.

I initially had a logical node at this point, but it never rendered any details for the physical plan, as it's methods on the `KGroupedStreamImpl` that provide details for the next operation of the physical plan, thus we only need to create a new logical node when one of those operations are specified.

When I did create a new logical plan node here, it contained no details to render, so I needed to put in checks for `null` `ProcessorParameters`. Having a placeholder node was somewhat probalmatic, so instead of a ""dummy"" node which I found to be confusing as well, I removed creating a new node at this point, and IMHO is better off this way.
"
206603371,5201,vvcephei,2018-07-31T16:50:47Z,duplicate test line?
206613580,5201,bbejeck,2018-07-31T17:21:48Z,ack
206616617,5201,bbejeck,2018-07-31T17:30:27Z,ack
206620359,5201,bbejeck,2018-07-31T17:41:30Z,"ack, reverted"
206620751,5201,bbejeck,2018-07-31T17:42:42Z,ack
206640577,5201,guozhangwang,2018-07-31T18:41:11Z,"Actually, my point is that the private `internalStreamsBuilder` field is not used anywhere either, so we can remove both this field as well as the getter function."
206640980,5201,guozhangwang,2018-07-31T18:42:26Z,Thanks for confirming :) just want to make sure it is not a regression.
206641195,5201,guozhangwang,2018-07-31T18:43:13Z,Thanks!
206672496,5201,bbejeck,2018-07-31T20:28:50Z,"Hmm, I'm not sure. While I agree with you that the `StreamsGraphNode#parentName` and here `parentNames` relationship is a bit awkward but I think changing `setParentNode` to `addParentNode` can be equally as awkward. 

 While I'm up for changing this in some way I'd prefer to leave how we establish the parent-child relationship the same, as we'd be making a cross-cutting change for just the `merge` case, and here it's isolated for just `merge`.


EDIT: I get what you are saying, I'll try to change and see if I can get it to work.

EDIT part II:  Updated and implemented as you suggested, good call."
206672618,5201,bbejeck,2018-07-31T20:29:12Z,replied above
206674028,5201,bbejeck,2018-07-31T20:33:46Z,ack. But I think we need to come together on this as a team.
206729312,5201,bbejeck,2018-08-01T01:15:36Z,"ack, done "
206729719,5201,bbejeck,2018-08-01T01:19:01Z,"out of convenience, reverted"
206729744,5201,bbejeck,2018-08-01T01:19:15Z,"ack, updated"
206730023,5201,bbejeck,2018-08-01T01:21:47Z,"I think it's needed for optimization PR, but I'll remove here and will add back if necessary"
206731569,5201,bbejeck,2018-08-01T01:34:23Z,ack
206732565,5201,bbejeck,2018-08-01T01:41:57Z,"Actually, we still need both The `StreamsBuilder.build()` call returns a `Topology` while the `InternalTopologyBuilder.build()` returns a `ProcessoryTopology`.   


While the method names seem to be a bit overloaded, the terminology and methods pre-date this PR, so maybe we can do a follow-up PR to look at renaming and clarifying things some."
206733189,5201,bbejeck,2018-08-01T01:47:36Z,ack
206737233,5201,bbejeck,2018-08-01T02:20:55Z,"@guozhangwang  the graph node is indeed required, and the tests are passing correctly.  Implementing the join for optimization was possibly the most ""intricate"" detail to get correct.

Previously when we generated the entire physical plan upfront, the parent names for the two `KStreamJoinWindow` instances were generated inside the `KStreamImplJoin.join` method and passed to the InternalStreamsBuilder to wire up the processors.  

The first pass of the `StreamStreamJoinNode` followed this approach, and everything was wired together in one operation.  But the problem is with that approach we lose the ability to optimize any join nodes much for the same reason as before, we write everything required for the join to the `InternalTopologyBuilder` and lose any concept if optimization has occurred and what the new parent node is. 

 Either stream involved in a join can be eligible for optimization concerning repartitioning.  In that case, we need to be able to use the updated parent node names for either or both of the `KStreamJoinWindow` instances involved in the join.

So what has been done here is to add the two `KStreamJoinWindow` instances explicitly as child nodes of the two `KStream` instances passed as parameters from the `doJoin` method.  The key point here is that one or both of the original `KStream` instances may have required a repartitioning.  

This fact requires us to explicitly attach the `KStreamJoinWindow` as a child node of the passed in `KStream` instance so if a repartition optimization does occur; the correct parent name is used by the `KStreamJoinWindow` when the physical plan is written.

As for your question of adding too many processors, when traversing the graph the nodes representing the `KStreamJoinWindow` processors have their details written, but in the `StreamStreamJoinNode` the previous calls writing the details for the `KStreamJoinWindow` processors out for the physical plan have been removed. Thus the number of processors written out in the physical plan is correct."
206737456,5201,bbejeck,2018-08-01T02:22:56Z,ack
206737468,5201,bbejeck,2018-08-01T02:23:04Z,ack
206737593,5201,bbejeck,2018-08-01T02:24:13Z,ack
206737755,5201,bbejeck,2018-08-01T02:25:41Z,ack
206737805,5201,bbejeck,2018-08-01T02:26:04Z,ack
206737992,5201,bbejeck,2018-08-01T02:27:35Z,ack
206738254,5201,bbejeck,2018-08-01T02:29:41Z,"ack fixed, need to update IntelliJ formatting"
206738426,5201,bbejeck,2018-08-01T02:30:29Z,ack
206739044,5201,bbejeck,2018-08-01T02:33:00Z,ack
206750808,5201,guozhangwang,2018-08-01T04:13:11Z,"Ack, thanks for clarifying."
206751900,5201,guozhangwang,2018-08-01T04:24:07Z,"My question is that, in this function we call the following `addGraphNode` calls:

```
builder.addGraphNode(thisStreamsGraphNode, thisWindowedStreamsNode);
builder.addGraphNode(otherStreamsGraphNode, otherWindowedStreamsNode);
builder.addGraphNode(this.parentGraphNode, joinGraphNode);
```

And in the returned `KStreamImpl` we passed in `joinGraphNode`. Note the `parentGraphNode` is passed as the latest node added to the topology for now, and hence it looks like although `thisStreamsGraphNode` and `otherStreamsGraphNode` each added a child, they do not have any parents, and hence these two ""branches"" are sort-of dangling as they are not connected to the topology graph at all.. Could you maybe explain a bit more on this logic?"
206930443,5201,bbejeck,2018-08-01T15:38:42Z,">it looks like although thisStreamsGraphNode and otherStreamsGraphNode each added a child, they do not have any parents, and hence these two ""branches"" are sort-of dangling as they are not connected to the topology graph at all

@guozhangwang  great question.  

In building the topology graph, it's not possible for a graph node to not have a parent.  

It is easier for me to explain the logic best with some examples. 

First, let's consider no repartitioning
```
   KStream<String, String> streamA = builder.stream(""topicA"");
   KStream<String, String> streamB = builder.stream(""topicB"");
   KStream<String, String> joined = streamA.join(streamB, vj, JoinWindows.of(5000));
```

In this case we a repartition did not occur in `doJoin`, so the `thisStreamsGraphNode` and `otherStreamsGraphNode`, from the `lhs` and `other` `KStream` instances respectively, represent the graph nodes for `streamA` and `streamB` and the parent node is `root`. Anything created directly from the `StreamBuilder` always has the parent node of `root`.   If a `KStream`  instance is created by another upstream operation like `builder.stream(""topicB"").filter(...)` , then its parent would be the graph node representing the `filter` operation.

In this case in the `thisStreamsGraphNode` and `parentGraphNode` are the same instance, and it ends up with two child nodes, the `thisWindowedStreamsNode` and the `joinGraphNode`.  I prefer to keep these two nodes separate as we don't need to keep track of repartitioning, I find the logic more clear to use the graph nodes from the `lhs` and `other` `KStream` instances.

Next repartitioning is required:
```
  KStream<String, String> streamA = builder.stream(""topicA"");
  KStream<String, String> streamB = builder.stream(""topicB"");
  KStream<String, String> newKeyStream = streamA.selectKey((k,v) -> k+v);
  KStream<String, String> joined = newKeyStream.join(streamB, vj, JoinWindows.of(5000));
```

Now a repartition occurred in `doJoin` and the `thisStreamsGraphNode` is now a repartition graph node, with its parent being the `selectKey` graph node and the grandparent node is the original `streamA` graph node.  

If `streamB` had required a repartition the parent-child structure would be the same, the `otherStreamsGraphNode` would be a repartition node with the parent being the graph node representing the key changing operation and the grandparent node representing the original `streamB` graph node.

So at all times the `thisStreamsGraphNode` and `otherStreamsGraphNode` have parents of either `root` or some other upstream `KStream` operation and the `thisStreamsGraphNode` and `otherStreamsGraphNode` need to add the respective window stream processor graph nodes as children, so the physical plan is rendered correctly.

Does this make sense?"
1187976451,13639,jeffkbkim,2023-05-08T23:11:16Z,will this be changed to all groups once we begin implementing the old apis?
1187980069,13639,jeffkbkim,2023-05-08T23:19:59Z,"nit: ""at least a subset"""
1187980194,13639,jeffkbkim,2023-05-08T23:20:15Z,"nit: ""the member"""
1187981608,13639,jeffkbkim,2023-05-08T23:23:38Z,i'm wondering if this would hide the fact that ownedTopicPartitions should not be null.
1187987623,13639,jeffkbkim,2023-05-08T23:38:27Z,"nit: ""has a larger member epoch""

a bug in setting the member epoch in the client side or in storing the epoch in the server side would result in the consumer never finding a coordinator right? if the client is expected to update its member epoch only from server side response then it seems that a server side bug would be more likely. "
1187994938,13639,jeffkbkim,2023-05-08T23:56:47Z,"(adding comment to this line since it's related)

might be confusing things but shouldn't we include the partitions to revoke in the heartbeat response? i think i remember something along the lines that the consumer will calculate the diff from its view of the owned partitions vs. this heartbeat response and will try to revoke. "
1188015174,13639,jeffkbkim,2023-05-09T00:50:17Z,aren't these thrown in `throwIfConsumerGroupHeartbeatRequestIsInvalid` if it's not the first heartbeat request?
1188024098,13639,jeffkbkim,2023-05-09T01:14:37Z,"I think adding what reconciliation we're doing at this stage would be helpful.

Also to confirm, target assignment records are actual diffs from a previous state whereas current assignment record holds the entire state right?"
1190415641,13639,jolshan,2023-05-10T21:35:49Z,Maybe we can explain the mapping here.
1190416089,13639,jolshan,2023-05-10T21:36:34Z,(I guess it is just name and assignor though)
1190417026,13639,jolshan,2023-05-10T21:38:00Z,Or will we have another method for generic groups?
1190428691,13639,jolshan,2023-05-10T21:56:05Z,What happens if we send an unsupported assignor on any request besides the first?
1190433019,13639,jolshan,2023-05-10T22:03:23Z,"maybe slightly off topic, but just for my understanding TopicPartitions is a data structure that contains
topic ID + partitions for that topic? 

Reading this name, it was not immediately clear that these all belonged to the same topic. Not a huge deal, but maybe something in naming that we missed and can think about amending in the future."
1190436747,13639,jolshan,2023-05-10T22:10:11Z,Could we also say we don't match if there is one or more owned partition that is not in the target set?
1190445435,13639,jolshan,2023-05-10T22:26:47Z,I'm also a bit concerned by saying this error because it assumes we got a bump from another coordinator and could obscure bugs. It's good the thrown error at least mentions the epoch though.
1190447106,13639,jolshan,2023-05-10T22:30:17Z,How do we continue from this state? Do we eventually get a bumped epoch response? 
1190454574,13639,jolshan,2023-05-10T22:46:37Z,"It should be null on every request besides the first one right? 

This method handles both the first heartbeat and future ones though right? So it needs to handle null and non null fields?"
1190455545,13639,jolshan,2023-05-10T22:48:44Z,Do we expect these changes to happen fairly often? Is info a bit high of a log level?
1190477811,13639,jolshan,2023-05-10T23:38:47Z,"Is the incrementing on line 497 the only way groupEpoch will be greater than the targetAssignmentEpoch?

This is a little confusing to me since I would expect the target to be higher. I think saying generic ""groupEpoch"" also confuses me about the source of the data."
1192164806,13639,clolov,2023-05-12T09:55:14Z,"I am most certainly missing a crucial piece of information, but the only mention I could find of a partition epoch is in the rejected alternatives of [KIP-848](https://cwiki.apache.org/confluence/display/KAFKA/KIP-848%3A+The+Next+Generation+of+the+Consumer+Rebalance+Protocol#KIP848:TheNextGenerationoftheConsumerRebalanceProtocol-ConsumerGroupStates). Has the need for this been discussed someplace else so I can have a further read?"
1192170321,13639,dajac,2023-05-12T10:00:38Z,"In this case, I need a method which returns a ConsumerGroup. If we can reuse it, I am fine with this. Otherwise, we can use another method and extracts some logic from this one."
1192173471,13639,dajac,2023-05-12T10:03:46Z,Reworked the comment.
1192174161,13639,dajac,2023-05-12T10:04:29Z,`ownedTopicPartitions` can be null.
1192179523,13639,dajac,2023-05-12T10:10:14Z,"This scenario could happen if a zombie coordinator is kept around. The consumer could get a higher epoch from the new coordinator and end up talking to the zombie by mistake (based on stale metadata). We did something similar in the controller in the part so I used the same approach here.

This seems safe to me as the consumer should only learn a new epoch from the coordinator. Of course, in case of a bug, this could become an issue. The alternative would be to be defensive and to fence the member in this case and force it to restart at epoch 0. The benefits of this option is that it is more robust but also more disruptive. Should we be conservative in this case? "
1192180394,13639,dajac,2023-05-12T10:11:09Z,"I think that providing the expected assigned partitions is actually more robust because it forces the consumer to revoke all the other partitions, even the ones that we would not know about on the server side."
1192180723,13639,dajac,2023-05-12T10:11:31Z,Correct. Those could be `null` here as well.
1192182531,13639,dajac,2023-05-12T10:13:24Z,Reworked the comment.
1192184441,13639,dajac,2023-05-12T10:15:39Z,Good point. It should be checked there as well.
1192188074,13639,dajac,2023-05-12T10:19:34Z,"Correct. `TopicPartitions` contains a Topic Id and a list of partition ids. Sure, we could consider renaming this."
1192194658,13639,dajac,2023-05-12T10:27:10Z,Right. The member will get a response with the correct epoch.
1192195549,13639,dajac,2023-05-12T10:28:12Z,They should be occasional. We log similarly in the current protocol and those logs are really helpful. We can lower them down in the future if they become an issue.
1192198495,13639,dajac,2023-05-12T10:31:30Z,"> Is the incrementing on line 497 the only way groupEpoch will be greater than the targetAssignmentEpoch?

At the moment, this is the only way but other ways will come. I think that it is safe to trigger the computation whenever the group epoch is larger than the target assignment epoch.

> This is a little confusing to me since I would expect the target to be higher. I think saying generic ""groupEpoch"" also confuses me about the source of the data.

We bump the group epoch when the group has changed (e.g. new members, new subscriptions, etc.). Then, we update the target assignment when we detect that it is older than the group metadata. However, the member epoch should always be lower or equals to the target assignment epoch.

Regarding ""groupEpoch"", I am not sure to understand your point. Are you saying that the name is confusing?"
1192204200,13639,dajac,2023-05-12T10:37:34Z,This is indeed not in the KIP because this is an implementation detail. We need to track epoch of partitions in order to know if they are free or not. An epoch with an old epoch basically means that the partition has not been revoked yet.
1194138883,13639,jeffkbkim,2023-05-15T17:24:33Z,that's a great point. thanks
1194141365,13639,jeffkbkim,2023-05-15T17:27:01Z,maybe TopicAndPartitions? topic partitions generally is used to refer to topic-partition tuples
1194145192,13639,jeffkbkim,2023-05-15T17:31:00Z,"maybe it would be good to include a comment on the relationship between the group, target assignment, and member epochs. "
1194151379,13639,jeffkbkim,2023-05-15T17:37:05Z,"actually the ConsumerGroup fields, groupEpoch and assignmentEpoch, already have comments"
1194171285,13639,jeffkbkim,2023-05-15T17:55:33Z,is there a reason we use expectedSize = 1?
1194188439,13639,jolshan,2023-05-15T18:10:17Z,"I think the comment 
> We bump the group epoch when the group has changed (e.g. new members, new subscriptions, etc.). 

makes things a little clearer. But I guess my confusion was it seems like this should always cause the assignment to change. I think I also got a bit confused because on earlier PRs I thought that the target assignment epoch was the epoch we get we at at when the assignment is complete. But maybe I am confused somewhere."
1194189770,13639,jeffkbkim,2023-05-15T18:10:57Z,should we throw an illegal state exception if new member is null?
1194198701,13639,jolshan,2023-05-15T18:17:35Z,is this still a todo? Ditto to above.
1194201125,13639,jolshan,2023-05-15T18:19:32Z,This method is a bit long and complicated. In the comments (and maybe the javadoc for the method) could we break it up into logical chunks.
1194201173,13639,jeffkbkim,2023-05-15T18:19:34Z,"""An"" immutable Map"
1194207204,13639,jeffkbkim,2023-05-15T18:24:06Z,"nit: ""or -1 if the partition does not exist."""
1194218869,13639,jolshan,2023-05-15T18:36:19Z,"could we be a bit more specific in what each of these replays do? Ie. for this one we update or remove a member depending on if the value is null. Others set subscription metadata etc.

I don't think the class names are enough to explain what each is doing -- especially since the names are so similar."
1194225263,13639,jolshan,2023-05-15T18:42:15Z,"When we are in the state between making the assignment and installing it, how do we guard against generating new assignments? Is this part of the state transitions?"
1194246241,13639,jolshan,2023-05-15T18:58:27Z,nit: An
1194249412,13639,jolshan,2023-05-15T18:59:35Z,"Can we say ""keyed by __"""
1194251237,13639,jolshan,2023-05-15T19:01:20Z,"This is ""updated"" because the member had changes and we want to use the assignor to make a new assignment?"
1194252036,13639,jolshan,2023-05-15T19:02:16Z,"nit ""replaces"""
1194368824,13639,jolshan,2023-05-15T21:13:28Z,I thought the java doc for TimelineHashMap doesn't allow null values.
1194380191,13639,jolshan,2023-05-15T21:28:27Z,can we reuse computeSubscriptionMetadata here?
1194394646,13639,jeffkbkim,2023-05-15T21:47:43Z,nit: replaces
1194394856,13639,jolshan,2023-05-15T21:48:02Z,Are the topic partitions not empty here? (Like empty is different than null?)
1194395409,13639,jeffkbkim,2023-05-15T21:48:58Z,"how's ""Computes a new subscription metadata with a member's updated topic subscriptions""?"
1194397156,13639,jolshan,2023-05-15T21:51:41Z,I see we check for null or !empty.
1194404457,13639,jeffkbkim,2023-05-15T21:58:32Z,"nit: memberId is more readable for me. also from https://github.com/apache/kafka/pull/13476#discussion_r1162006613

> it's a bit unfortunate that we are iterating through the entire members map every time a single member changes its subscription. i wonder if we can keep a map of TopicMetadata to a set of member ids

have we looked into this?
"
1194427256,13639,jeffkbkim,2023-05-15T22:38:09Z,nit: javadoc on params
1194427427,13639,jeffkbkim,2023-05-15T22:38:29Z,nit: javadoc on params
1194427551,13639,jeffkbkim,2023-05-15T22:38:45Z,nit: javadoc on params
1194427668,13639,jeffkbkim,2023-05-15T22:38:59Z,ditto on javadoc
1194427886,13639,jeffkbkim,2023-05-15T22:39:21Z,nit: we can remove this
1194430597,13639,jeffkbkim,2023-05-15T22:44:50Z,nit: does this have to be in its own line?
1194432103,13639,jeffkbkim,2023-05-15T22:47:50Z,(not completely related) older record types i.e. GroupMetadataValue have camelcase field names. when did we change the format to upper camelcase?
1194435189,13639,jeffkbkim,2023-05-15T22:54:06Z,"nit: ""It does not"""
1194437607,13639,jeffkbkim,2023-05-15T22:58:42Z,"what happens if the member id is not empty but the member epoch is 0 to indicate it's a new member?

does this mean that the client can choose its member id? i don't think it should right?"
1194440425,13639,jeffkbkim,2023-05-15T23:04:18Z,"nit: i think we should differentiate the member from the memberEpoch.

the member points to the existing member stored in the consumer group whereas the memberEpoch points to the epoch of the member from the heartbeat request"
1194446025,13639,jeffkbkim,2023-05-15T23:16:30Z,nit: can we add a newline between the if statement and updatedMember?
1194447475,13639,jeffkbkim,2023-05-15T23:19:39Z,can we do `for (ConsumerGroupMember member : members.values()) {`?
1194449829,13639,jeffkbkim,2023-05-15T23:25:01Z,"more of a comment for `ConsumerGroup.preferredServerAssignor()`, but we're iterating through all members to get the preferred assignor, every time we need bump the target assignment epoch. 

have we considered saving the count?"
1194455250,13639,jeffkbkim,2023-05-15T23:37:27Z,"we don't perform a reconciliation phase on leave group because once the existing members heartbeat, the metadata manager will notice the bumped groupEpoch (L619) and so targetAssignmentEpoch < groupEpoch will trigger the reconciliation. is this correct?

why do we need to create target assignment records here then?

"
1194457005,13639,jeffkbkim,2023-05-15T23:41:03Z,"i think targetAssignments size does represent the number of members but shouldn't this be ""assignments"" or ""target assignments""?"
1194459578,13639,jeffkbkim,2023-05-15T23:46:13Z,i think we would want at least a way to detect that something is wrong since with the current approach the bug could go unseen. 
1194463847,13639,jeffkbkim,2023-05-15T23:56:59Z,"> it seems like this should always cause the assignment to change

what's ""this""?

> the target assignment epoch was the epoch we get we at at when the assignment is complete

that's right. the target assignment epoch is what every member will try to converge to by revoking/assigning partitions. once we build the target assignment, we bump the target assignment epoch (L532)"
1194464178,13639,jeffkbkim,2023-05-15T23:57:46Z,i also think that would be very helpful
1194465375,13639,jeffkbkim,2023-05-16T00:00:46Z,david can correct me but the coordinator will continue to compute new target assignments even if we're in the process of installing an existing one and the group would try to converge to the latest assignment
1194477394,13639,jolshan,2023-05-16T00:27:22Z,"When we bump the group epoch, it seems like we will also always reassign. So I guess I was just trying to figure out if there is a case where the group epoch is different from the assignment epoch. "
1194479029,13639,jolshan,2023-05-16T00:31:08Z,Ok. That's what I was expecting to happen. So if that's expected it is fine. I just wonder if it is extra work.
1200087476,13639,dajac,2023-05-22T07:39:20Z,"Yes, it is. This is part of another PR."
1200129689,13639,dajac,2023-05-22T08:15:50Z,Ack. Let me expand the comments.
1200134115,13639,dajac,2023-05-22T08:18:37Z,Ack. I plan to rename those records but this will come when my main PRs are merged.
1200138816,13639,dajac,2023-05-22T08:22:14Z,That's correct.
1200143037,13639,dajac,2023-05-22T08:25:37Z,"Right. This is ""updated"" because the member may have been updated so we need to use the latest (non-persisted) information from the member to select the correct assignor."
1200145207,13639,dajac,2023-05-22T08:27:19Z,Right but `compute` gives you `null` if the key is not present.
1200149162,13639,dajac,2023-05-22T08:30:23Z,`computeSubscriptionMetadata` is slightly different so I can't reuse it directly here.
1200149897,13639,dajac,2023-05-22T08:30:59Z,The topic partitions should be an empty list when joining or re-joining.
1200153306,13639,dajac,2023-05-22T08:33:53Z,I think that I should use `assignedPartitions.size()`.
1200154967,13639,dajac,2023-05-22T08:35:12Z,We could.
1200227960,13639,dajac,2023-05-22T09:12:37Z,`exist` is a bit misleading here as `-1` is returned if the partition does not have an epoch set.
1200233059,13639,dajac,2023-05-22T09:16:45Z,I put it like this to follow the style of the other methods in this file.
1200233985,13639,dajac,2023-05-22T09:17:32Z,I think that we have always been using upper camelcase in requests/responses. GroupMetadataValue is likely wrong here.
1200235936,13639,dajac,2023-05-22T09:19:07Z,epoch equals to zero does not only indicate a new member. it also indicates a rejoining member. the member could indeed set the member id when it joins but it is not supposed to. it is a small quirk in the protocol.
1200237262,13639,dajac,2023-05-22T09:20:08Z,That makes sense. Let me use `receivedMemberEpoch`.
1200239503,13639,dajac,2023-05-22T09:21:58Z,Let me put the condition on one line.
1200293273,13639,dajac,2023-05-22T10:07:04Z,`memberId` is already used... Let me check if keeping a map is worth it.
1200402995,13639,dajac,2023-05-22T11:47:16Z,"I think that keeping a map of TopicMetadata to member ids does not work because the content of TopicMetadata could change as well. However, we could keep a map of topic name to number of subscribers. That would reduce the computation. I did the change. Let me know what you think."
1200415899,13639,dajac,2023-05-22T11:58:51Z,That's fair. We could remove it.
1200462722,13639,dajac,2023-05-22T12:41:30Z,that makes sense. done.
1200467372,13639,dajac,2023-05-22T12:45:37Z,It may be better to not do this after all. Let me remove it for now in order to be on the safe side. We can always bring it back if we find it useful in the future.
1202837929,13639,jolshan,2023-05-23T18:36:53Z,"So across all types of groups (generic, consumer) we can only have one type of group with a given name. We can't have a foo consumer group and a foo generic group. Are we also enforcing this when we create groups? 

Looks to be the case if we only create groups via getOrMaybeCreateConsumerGroup"
1202846284,13639,jolshan,2023-05-23T18:43:31Z,"I see in the json comments for instance ID:
`null if not provided or if it didn't change since the last heartbeat; the instance Id otherwise.`

But this seems inconsistent with what we are saying here. It should only be provided in the first request?"
1202846585,13639,jolshan,2023-05-23T18:43:45Z,ditto for rack id
1203077539,13639,jolshan,2023-05-23T22:08:38Z,"I'm not sure I follow `Note the member is the persisted member anytime in this method`
Are we saying that the member is written to disk? Or something else? I think the word ""anytime"" is confusing me."
1203079411,13639,jolshan,2023-05-23T22:10:12Z,do we want a log message for the first time joining the group?
1203083017,13639,jolshan,2023-05-23T22:13:51Z,Do we add a newMemberSubscriptionRecord when the member is updated (not new) too?
1203084630,13639,jolshan,2023-05-23T22:15:36Z,"I suppose the naming here with ""new"" and the usage below for `newGroupSubscriptionMetadataRecord` confused me."
1203119594,13639,jolshan,2023-05-23T22:52:09Z,"Rereading this and the KIP -- is the only time we have a different assignment epoch from the group epoch is when assignment fails? I also assume that other members should learn about this assignment. I think I'm missing how that is done -- is this through the records methods?

So for the next heartbeat request of another member, line 552 would see updatedMember.nextMemberEpoch() != targetAssignmentEpoch as true since the group epoch and assignment epoch would be equal, but the member epoch would still be behind?"
1203132909,13639,jolshan,2023-05-23T23:08:24Z,would it be useful to have a helper method to delete a member?
1203150079,13639,jolshan,2023-05-23T23:29:41Z,If the partition did not exist it would also be -1 though right? There's just two possible cases?
1203152667,13639,jolshan,2023-05-23T23:32:36Z,"should we also mention in the java doc that we update the assignors here. I know we say in method, but maybe good to mention at the top too.
(I was a little confused earlier why we needed both the new and the old members as parameters when this was called in the other class)"
1203158830,13639,jolshan,2023-05-23T23:40:12Z,Is there a reason we use this method instead of put? Shouldn't each topic name be absent?
1203181594,13639,jolshan,2023-05-24T00:01:53Z,"if we compute and return null, isn't that trying to set null values? Or am I missing something."
1203184877,13639,jolshan,2023-05-24T00:04:45Z,I guess I'm also wondering why we don't use empty set. Unless this is because we want to overload null as mentioned in the getEpoch method.
1203192959,13639,jolshan,2023-05-24T00:11:42Z,Ok so null is not ok -- it must be explicitly empty.
1203222949,13639,jolshan,2023-05-24T00:35:51Z,do we also test that the other members also update their assignments?
1203224577,13639,jolshan,2023-05-24T00:37:10Z,nit: member 2?
1203225900,13639,jolshan,2023-05-24T00:38:17Z,I guess we sort of duplicate the code and test in testReconciliationProcess
1203578132,13639,dajac,2023-05-24T07:17:06Z,That's right. We enforce this in `getOrMaybeCreateConsumerGroup`.
1203601651,13639,dajac,2023-05-24T07:33:47Z,Good catch. Let me fix this.
1203610206,13639,dajac,2023-05-24T07:39:48Z,I wanted to callout that `member` is different from `updatedMember` in this method but I think that the names are clear. Let me remove that comment. It is more confusing than anything else.
1203610761,13639,dajac,2023-05-24T07:40:12Z,Epoch 0 means that a new member joins or an existing member rejoins. Let me update the log.
1203611994,13639,dajac,2023-05-24T07:41:02Z,`new` means that a new record is created. A new record is created when the member is created or updated.
1203615232,13639,dajac,2023-05-24T07:43:16Z,"We already have `consumerGroup.removeMember(memberId)` but it does not apply here. In this case, we really want to update the member with sentinel values."
1203623275,13639,dajac,2023-05-24T07:46:37Z,I have clarified the javadoc.
1203625806,13639,dajac,2023-05-24T07:47:54Z,It is a mistake. We can use `put` as you suggested.
1203627084,13639,dajac,2023-05-24T07:48:37Z,That's right.
1204465047,13639,jeffkbkim,2023-05-24T16:15:47Z,"nit: The number of members supporting each server assignor name.

is more readable to me"
1204467377,13639,jeffkbkim,2023-05-24T16:17:36Z,nit: The metadata associated with each subscribed topic name
1204468514,13639,jeffkbkim,2023-05-24T16:18:36Z,nit: The target assignment per member
1204471728,13639,jeffkbkim,2023-05-24T16:21:27Z,"nit: it removes its partition epochs from this map. When a member gets a partition, it adds the epoch to this map. "
1204475716,13639,jeffkbkim,2023-05-24T16:25:01Z,"for this line and the following couple lines, does expected size of 0 create a hash table with size 1 as it's a power of 2?

i'm confused because the expected size intuitively should not be 0."
1204477868,13639,jeffkbkim,2023-05-24T16:27:01Z,"I think justine or someone else may have mentioned in another PR but for getters, just having the `@return The current group epoch` makes more sense. i'm not sure if this line adds much value as well as for other getter methods"
1204492401,13639,jolshan,2023-05-24T16:40:28Z,"Ok -- so the idea is member stays the same from when it is first ""gotten"", but updatedMember is the one we change through the method?"
1204493167,13639,jolshan,2023-05-24T16:41:11Z,I slowly figured this out as I read the PR.  Did we also say earlier we were changing the record names?
1204494016,13639,jolshan,2023-05-24T16:42:05Z,"Yeah, I suppose I was meaning a method to add the sentinel values. But if it is only done here, maybe it is not useful."
1204494893,13639,jeffkbkim,2023-05-24T16:42:57Z,nit: Returns the existing
1204509686,13639,jeffkbkim,2023-05-24T16:54:37Z,"do you think we should log something if topicImage == null? 
can both the topicsImage and/or subscribed topic names be outdated here?"
1204522037,13639,dajac,2023-05-24T17:06:23Z,"Yeah, let me try to re-explain it with my words.

Basically, we have three epochs: the group epoch, the assignment epoch, and each member has its own epoch.
- When the group changes (e.g. new member, updated subscription, etc), the group epoch is bumped in order to note that the topology of the group has changed.
- When the assignment epoch is smaller than the group epoch, it means that the assignment for the group is stable so we need to recompute it. We generate a new assignment with the epoch matching the group epoch in order to note that the assignment is for the latest group topology.
- When a member has a smaller epoch than the assignment epoch, it means that its assignment is stale so we need to reconcile it to converge it to that epoch.

All the epochs are equal when the group is stable. This means that all the members have converged to the desired assignment.

> Rereading this and the KIP -- is the only time we have a different assignment epoch from the group epoch is when assignment fails? I also assume that other members should learn about this assignment. I think I'm missing how that is done -- is this through the records methods?
So for the next heartbeat request of another member, line 552 would see updatedMember.nextMemberEpoch() != targetAssignmentEpoch as true since the group epoch and assignment epoch would be equal, but the member epoch would still be behind?

Yeah, we could have a difference between the assignment epoch and the group epoch when the assignment is not computed immediately. In the current implementation, we almost never have this but keep in mind that we will support client side assignors. In this case, the computation will take some time so we will see it more.

The member learn about their assignment when they heartbeat. This is when we do the reconciliation process. We reconcile a member in two cases: 1) the member is not stable; 2) the target assignment has changed since the last time we reconciled the member. This is done [here](https://github.com/apache/kafka/pull/13639/files#diff-00f0f81cf13e66781777d94f7d2e68a581663385c37e98792507f2294c91bb09R569)."
1204523103,13639,dajac,2023-05-24T17:07:24Z,That's right.
1204523524,13639,dajac,2023-05-24T17:07:52Z,I will do this but only when all my code is merged. It will create a mess otherwise...
1204535515,13639,jolshan,2023-05-24T17:19:36Z,I assume if we just got member1's partitions we would still be in assigning state.
1204538473,13639,jolshan,2023-05-24T17:22:14Z,Or I guess can we just get member1's partitions while member2 is still revoking?
1204556978,13639,jolshan,2023-05-24T17:38:55Z,"Just for my understanding, we will send the same request again when we receive the error.
We will already have a group ID and member ID server side, and we will reuse them. 

Ideally, the assignment works and we continue. Is this correct?"
1204564082,13639,jolshan,2023-05-24T17:46:03Z,Would it be helpful to use a new epoch here to see that it changes?
1204571671,13639,jeffkbkim,2023-05-24T17:52:01Z,"after updating member 1 in L315, we have
```
range: 1
uniform: 1
```
in serverAssignors. preferredServerAssignor() in L317 decrements ""range"" so the copy should have
```
range: 0
uniform: 1
```
shouldn't this be
`assertTrue(assignor.equals(Optional.of(""uniform"")));`?"
1204579244,13639,jolshan,2023-05-24T17:58:03Z,Is there ever a time the next member epoch is not the same as the member epoch?
1204582235,13639,jolshan,2023-05-24T18:00:53Z,"Would it make sense to also check member states here? I know we had something similar in the other file, but we are also testing the group states here."
1204597635,13639,jolshan,2023-05-24T18:17:53Z," If we have more than one assignor with the same max value, we choose one randomly (based on map iteration order). Is this intended?"
1204599222,13639,jolshan,2023-05-24T18:19:39Z,Ah I see this below.
1204607029,13639,jolshan,2023-05-24T18:26:24Z,preferredServerAssignor not modifying the state was really throwing me off here. Not sure if comments will help. 
1204613085,13639,jolshan,2023-05-24T18:32:41Z,We aren't actually removing member1 here but computing what would happen if we did. This is a bit confusing to follow.
1204613762,13639,jolshan,2023-05-24T18:33:21Z,did we mean to have null instead of member3 here?
1204623123,13639,jolshan,2023-05-24T18:43:16Z,We have member 2 as range and member 3 as uniform so that's why it is 50/50
1204872093,13639,jeffkbkim,2023-05-25T00:13:04Z,"i am curious as well. i notice 3 places
```
CurrentAssignmentBuilder#maybeTransitionFromAssigningToAssigningOrStable
CurrentAssignmentBuilder#transitionToNewTargetAssignmentState
CurrentAssignmentBuilder#maybeTransitionFromRevokingToAssigningOrStable
```
that both set member epoch and next member epoch to `targetAssignmentEpoch`"
1204874200,13639,jeffkbkim,2023-05-25T00:18:00Z,"i'm confused because i was referring to member 2 here:
range: 1 (member 2)
uniform: 1 (member ~~1~~ 3)

after `consumerGroup.updateMember(updatedMember1);` in L315.

in L317, we call `preferredServerAssignor()` which gets a copy, then will decrement the count of ""range"" since member1 (range) is passed in as the oldMember argument. So shouldn't the count for ""range"" be 0?

i think i'm missing something"
1204875457,13639,jeffkbkim,2023-05-25T00:21:10Z,are we using this variable?
1204877037,13639,jeffkbkim,2023-05-25T00:25:13Z,are we using this?
1204878528,13639,jeffkbkim,2023-05-25T00:29:01Z,"can we use `targetAssignment` and explain the mapping of member id to its assignment? ""assignments"" is ambiguous since we also have current assignments throughout the new protocol. "
1204879910,13639,jeffkbkim,2023-05-25T00:32:29Z,"btw, i was wondering if we could name this targetAssignmentEpoch. similar to my comment on `assignments`. "
1204881034,13639,jeffkbkim,2023-05-25T00:35:20Z,i think we can use put here too right?
1204882557,13639,jeffkbkim,2023-05-25T00:39:25Z,are we using this?
1204882590,13639,jeffkbkim,2023-05-25T00:39:32Z,are we using this?
1204883474,13639,jeffkbkim,2023-05-25T00:41:49Z,"should these be `Epoch`s? to me, offset seems more partition related. "
1204883836,13639,jeffkbkim,2023-05-25T00:42:51Z,i'm noticing a bunch of unused methods. are we planning to use these in the following PRs?
1204885916,13639,jeffkbkim,2023-05-25T00:48:17Z,we use a record key's raw version in RecordHelpers. should we choose a convention to follow?
1204889028,13639,jeffkbkim,2023-05-25T00:56:16Z,i think adding a comment above this line to indicate we're now testing a non-new/rejoining member will be helpful. and maybe a comment when we first start testing new/rejoining member's heartbeat requests
1204889641,13639,jeffkbkim,2023-05-25T00:57:56Z,this is `TopicsImage.EMPTY` right?
1204903807,13639,jeffkbkim,2023-05-25T01:29:38Z,"curious, what happens if the member acknowledges but does not actually revoke i.e. a buggy client? would that member and the new owner of the partition fetch from the same partition?

i'm guessing there's no guards against that"
1204907417,13639,jeffkbkim,2023-05-25T01:38:31Z,shouldn't the metadata manager respond with `assignedTopicPartitions`? what happens if the previous heartbeat respond was lost?
1204909101,13639,jeffkbkim,2023-05-25T01:42:27Z,ditto on assignedTopicPartitions (and pendingTopicPartitions for member3)
1204909313,13639,jeffkbkim,2023-05-25T01:42:58Z,the revocation is acknowledging the new assigned partitions revoked from member 1 right?
1204910503,13639,jeffkbkim,2023-05-25T01:45:42Z,why is the topic id ordering different here?
1204914720,13639,jeffkbkim,2023-05-25T01:55:28Z,"i think i'm getting confused here - do we create a new current assignment record whenever we respond back to a member?

i might be remembering incorrectly but i thought the current assignment was created when the member transitions to Stable (after revoking partitions and acknowledging assigned partitions)"
1205093788,13639,dajac,2023-05-25T07:16:03Z,"From the javadoc of `compute`:

```
If the remapping function returns null, the mapping is removed (or remains absent if initially absent). If the remapping function itself throws an (unchecked) exception, the exception is rethrown, and the current mapping is left unchanged.
```

We don't keep the empty set because we want to clean the Map. Otherwise, it would keep sets for non-existing partitions for instance."
1205138023,13639,dajac,2023-05-25T07:57:02Z,Right.
1205155041,13639,dajac,2023-05-25T08:11:49Z,Sure.
1205155348,13639,dajac,2023-05-25T08:12:06Z,Right.
1205156348,13639,dajac,2023-05-25T08:12:57Z,That's correct. -1 is returned when the partition is not in the map. This could be because the partition does not exist or because the partition does not have an epoch yet.
1205157172,13639,dajac,2023-05-25T08:13:38Z,"As it is used only once here, it is not worth it in my opinion."
1205165820,13639,dajac,2023-05-25T08:21:08Z,"> I assume if we just got member1's partitions we would still be in assigning state.

Correct.

> Or I guess can we just get member1's partitions while member2 is still revoking?

Yes, we can. Member 3 gets member 1's partitions when the are available but it does not have to wait on member 2."
1205169546,13639,dajac,2023-05-25T08:23:21Z,Correct. The request will be retried with the group id and the member id (if the member already has one).
1205178313,13639,dajac,2023-05-25T08:30:26Z,"That's right. There is only one case where it is not. When a member must revoke partitions, it stays in its current epoch so member epoch is different from the next epoch in this case.

The rational of keeping track of the next epoch here is to basically prevent recomputing the state while the member is in revoking state. Without it, we would have to recompute it on every heartbeat."
1205203837,13639,dajac,2023-05-25T08:51:33Z,Make sense.
1205211076,13639,dajac,2023-05-25T08:57:20Z,The capacity will be 2 (the min capacity).
1205218229,13639,dajac,2023-05-25T09:03:13Z,"> do you think we should log something if topicImage == null?

I would not because it could be common to not have metadata about a topic yet. This will spam the logs.

> can both the topicsImage and/or subscribed topic names be outdated here?

We will always use the latest topics image that we have got. The subscribed topic names is never outdated as it always represent the subscriptions provided by the consumer. However, it may not be known/exist yet."
1205219753,13639,dajac,2023-05-25T09:04:28Z,Correct.
1205238390,13639,dajac,2023-05-25T09:20:29Z,Updated the test. I had a mistake in these.
1205239034,13639,dajac,2023-05-25T09:21:03Z,I have renamed it to `computePreferredServerAssignor`. Now `preferredServerAssignor` only returns what the group has.
1205239532,13639,dajac,2023-05-25T09:21:29Z,I have put more comments. I hope it helps.
1205247698,13639,dajac,2023-05-25T09:25:36Z,Nope. Let me remove it.
1205248818,13639,dajac,2023-05-25T09:26:06Z,Nope. Removed.
1205252133,13639,dajac,2023-05-25T09:28:54Z,nope.
1205252262,13639,dajac,2023-05-25T09:29:01Z,nope.
1205253148,13639,dajac,2023-05-25T09:29:44Z,right but this is what they are in the end in our context. keeping offset is better here.
1205254844,13639,dajac,2023-05-25T09:31:08Z,i have removed all of them but this one. i will use it in the future.
1205256385,13639,dajac,2023-05-25T09:32:26Z,"Using the constants is fine here. In RecordHelpers, I did not use them in order to not change the version by mistake."
1205264178,13639,dajac,2023-05-25T09:39:07Z,Added comments.
1205264648,13639,dajac,2023-05-25T09:39:30Z,Right.
1205266764,13639,dajac,2023-05-25T09:41:24Z,"That's right. This is basically a violation of the protocol. Note that the previous owner won't be allowed to commit offsets.

We can't do much at this protocol level. However, we could imagine passing the member epoch while fetching so the leader could reject stale member epoch. That would strengthen the overall protocol."
1205271089,13639,dajac,2023-05-25T09:45:14Z,"The assignment is only provided in the following cases:
1. The member reported its owned partitions;
2. The member just joined or rejoined to group (epoch equals to zero);
3. The member's assignment has been updated.

In the case of a lost response, the client would hit a timeout/network error. In this case, the client is expected to send a ""full"" heartbeat with the owned partitions set so it will get a ""full"" response."
1205271405,13639,dajac,2023-05-25T09:45:29Z,See my previous reply.
1205273140,13639,dajac,2023-05-25T09:46:58Z,"member 3 has nothing to revoke here. this comment is wrong, let me update it."
1205275881,13639,dajac,2023-05-25T09:49:21Z,hmm.. i don't know. let me check this.
1205278404,13639,dajac,2023-05-25T09:51:27Z,it is the other way around. we persist the current assignment when it changes based on the reconciliation. we provide the assignment to the client when the current assignment change.
1206048854,13639,jolshan,2023-05-25T22:10:08Z,Clarified offline -- null returned in compute removes the entry.
1206050414,13639,jolshan,2023-05-25T22:13:07Z,That's totally fair. Just confirming :) 
1206053946,13639,jolshan,2023-05-25T22:19:58Z,Is nextMemberEpoch a bit confusing here? It seems more like a target epoch.
1206060783,13639,jeffkbkim,2023-05-25T22:33:47Z,nit: group size
1206063165,13639,jeffkbkim,2023-05-25T22:39:03Z,"do we have a test case for this?

i expect that when we accept a request with the previous epoch, we compute the diff from the request's owned partitions and the target assignment and respond to the consumer (assignedTopicPartitions, pending assignment if exists). which would be identical to what we do for the expected member epoch."
1206064411,13639,jolshan,2023-05-25T22:41:41Z,This and the server assignor test are much more readable. Thanks!
1206066309,13639,jeffkbkim,2023-05-25T22:45:49Z,nit: existing and the new target assignment
1206317753,13639,dajac,2023-05-26T07:02:21Z,"Yeah, I agree. Let me use `targetMemberEpoch`."
1206322259,13639,dajac,2023-05-26T07:07:55Z,"This is covered in `testConsumerGroupMemberEpochValidation`. If the member comes with the previous epoch and its owned partitions is a subset of its assigned partitions, we accept it and it goes through the regular process. If nothing has changed since the last heartbeat, it will just receive the current assignment/epoch."
178980194,4812,guozhangwang,2018-04-03T22:27:42Z,"I think you can still use the log4j format here, e.g.

```
""Skipping record due to deserialization error. topic={} partition={} offset={}"", rawRecord.topic(), rawRecord.partition(), rawRecord.offset(), deserializationException
```

With four parameters, the last one is auto interpreted as the exception; maybe we can validate if this is the case."
178981229,4812,guozhangwang,2018-04-03T22:32:42Z,"If it is a per-thread metric, I'd suggest we pre-register them at the beginning of the application. This way some other tools like `JMXTool` do not need to wait for the object name to show up. WDYT?"
179188828,4812,vvcephei,2018-04-04T15:42:45Z,"Sounds good.

I meant to make a comment before you read this to say that there had been a concern in the discussion about having metrics reported from processor nodes (when the proposal was at the node level) that would never actually skip records, thereby polluting the metrics. I thought I'd throw the lazy registration pattern in just to see what you all thought.

I'll switch it back to pre-registration."
179312182,4812,vvcephei,2018-04-04T23:24:35Z,"Confirmed, switching to the variant you mentioned still prints:
```
[2018-04-04 18:23:20,310] WARN task [0_1] Skipping record due to deserialization error. topic=[topic1] partition=[1] offset=[1] (org.apache.kafka.streams.processor.internals.RecordDeserializer:80)
org.apache.kafka.common.errors.SerializationException: Size of data received by IntegerDeserializer is not 4
```"
179586187,4812,guozhangwang,2018-04-05T20:09:33Z,"Same here, we can get rid of `String.format`."
179587232,4812,guozhangwang,2018-04-05T20:13:34Z,"We are stripping the prefix for this sensor: is it intentional? Note that for JMX reporter, the sensor name would not be included in any fields."
179587354,4812,guozhangwang,2018-04-05T20:13:59Z,"`skippedRecordsSensor` should not be null, right?"
179589855,4812,guozhangwang,2018-04-05T20:24:03Z,"nit: flattening to a very long single line, is it intentional?"
179589896,4812,guozhangwang,2018-04-05T20:24:11Z,Ditto below and in other tests like `testPauseResume`
179590319,4812,guozhangwang,2018-04-05T20:25:54Z,nit: alignment.
179590486,4812,guozhangwang,2018-04-05T20:26:32Z,Ditto here.
179590561,4812,guozhangwang,2018-04-05T20:26:52Z,Ditto here.
179590584,4812,guozhangwang,2018-04-05T20:26:58Z,Here.
179591415,4812,guozhangwang,2018-04-05T20:30:09Z,Why do we remove this sensor?
179592552,4812,guozhangwang,2018-04-05T20:34:32Z,Should we record the thread-level `skipped record` sensor here?
179592840,4812,guozhangwang,2018-04-05T20:35:43Z,"Hmm.. I did not see we have recorded the sensor for deserialization error here, why this test passed?"
179626142,4812,vvcephei,2018-04-05T23:17:50Z,"heh, what a coincidence! I think so, and that's actually part of the motivation for this change I'm proposing to the metrics. "
179627246,4812,vvcephei,2018-04-05T23:26:31Z,"I disabled these tests because part of their function is to verify the number of metrics we register. This currently fails because we're registering a lot more metrics. If we decide to go with this overall strategy, I'll rethink these tests."
179627503,4812,guozhangwang,2018-04-05T23:28:22Z,"Just `implements InternalStreamsMetrics` should be sufficient, since `InternalStreamsMetrics` extends `StreamsMetrics`?"
179771434,4812,bbejeck,2018-04-06T14:16:43Z,For `testLatencyMetrics` and `testThroughputMetrics` maybe use `@Ignore` instead ? Not a big deal but by getting an `ignored` test count there's a better chance these two tests won't fall through the cracks.
179774999,4812,bbejeck,2018-04-06T14:28:05Z,For the `task.addRecords` with a long list of `ConsumerRecord<>` seems like the only difference with each record is the offset.  Maybe create a method that takes an `int[]` with offsets and returns a `List<ConsumerRecord>`? 
179775146,4812,vvcephei,2018-04-06T14:28:31Z,"Ah, yeah, in an earlier pass they were independent interfaces."
179828056,4812,mjsax,2018-04-06T17:42:13Z,nit: remove `this`
179828169,4812,mjsax,2018-04-06T17:42:41Z,nit: move `topology` to next line
179830023,4812,mjsax,2018-04-06T17:49:58Z,"nit: add `final` to the parameters to cleanup code ""on the side"""
179830080,4812,mjsax,2018-04-06T17:50:08Z,nit: add `final`
179830701,4812,mjsax,2018-04-06T17:52:31Z,"when would `skippedRecordsSensor` be `null`? (I know this is just ""move"" code but still wondering why we need this)"
179831780,4812,vvcephei,2018-04-06T17:56:31Z,"ah, that's how you do it. I tried `@Test(ignore=true)` like testng, but that obviously doesn't work..."
179833851,4812,mjsax,2018-04-06T18:04:32Z,"Because `INFO` level is default, should we remove `Sensor.RecordingLevel.INFO` in the above calls?"
179835194,4812,mjsax,2018-04-06T18:09:51Z,nit: Is this suppression necessary? I don't think that gradle builds put a warning -- might be your local IDE setting only?
179835363,4812,mjsax,2018-04-06T18:10:36Z,nit: do we need this? (cf. my other comment about `SuppressWarnings`)
179835676,4812,mjsax,2018-04-06T18:11:38Z,as above.
179835819,4812,mjsax,2018-04-06T18:12:14Z,as above.
179836521,4812,mjsax,2018-04-06T18:15:06Z,nit: `topic` -> `partitionsForTopic`
179836693,4812,mjsax,2018-04-06T18:15:47Z,nit: `partitions` -> `partitionsForChangelog`
179838008,4812,mjsax,2018-04-06T18:20:42Z,Very nice!
179872407,4812,vvcephei,2018-04-06T20:49:38Z,It won't. That's an artifact that I need to fix.
179872830,4812,vvcephei,2018-04-06T20:51:34Z,"meh. I personally favor explicit settings, so if anything, I'd actually add it here, but I'm happy to do whichever you all prefer."
179873240,4812,vvcephei,2018-04-06T20:53:25Z,"yeah, I have my IDE set on paranoid mode. I can disable this inspection if you don't want to see supressions like this. Or I can inline the parameter value, which is what the inspection was complaining about."
179873711,4812,vvcephei,2018-04-06T20:55:39Z,"Double-brace initialization is actually not great for a number of reasons. I've been terrraforming it whenever I encounter it, but for some reason I decided to suppress this one instead. I'll plan to remove the suppression and the double-brace initialization in the final draft."
179873858,4812,vvcephei,2018-04-06T20:56:24Z,Thanks!
179885340,4812,vvcephei,2018-04-06T21:56:56Z,"during `thread.runOnce(-1);`, it'll encounter an exception ""asdfasdfasdf"" as an integer and increment the metric."
179885776,4812,vvcephei,2018-04-06T21:59:23Z,it was not. I've fixed it.
179886374,4812,vvcephei,2018-04-06T22:03:30Z,fixed.
179886390,4812,vvcephei,2018-04-06T22:03:38Z,it was when I made it lazy. I've fixed it.
179893704,4812,mjsax,2018-04-06T22:56:44Z,"I am fine with removing the overload that has a default and add INFO explicitly here. (To me, it's more about consistency---I immediately assume that this sense is different to the others, even if it's not if the code pattern is different). "
179893837,4812,mjsax,2018-04-06T22:58:03Z,"I personally would prefer getting rid of the annotation -- to me, annotations are noise in the code and distracting."
179893976,4812,mjsax,2018-04-06T22:59:10Z,"> not great for a number of reasons

For my own education: why?

Refactoring is fine with me."
179929221,4812,mjsax,2018-04-07T22:12:01Z,why this change?
179929312,4812,mjsax,2018-04-07T22:17:23Z,Nit: do we need to `[]` around each value? `[]` is used for collections or list -- might be confusing to add them?
179929318,4812,mjsax,2018-04-07T22:17:38Z,as above.
179929326,4812,mjsax,2018-04-07T22:18:01Z,as above.
179929377,4812,mjsax,2018-04-07T22:18:23Z,nit: remove space
179929389,4812,mjsax,2018-04-07T22:19:24Z,nit: indention
179929538,4812,mjsax,2018-04-07T22:26:03Z,"nit: move `new File` to new line for consistent formatting (note, that `ProcessorStateManager.CHECKPOINT_FILE_NAME)` is second parameter of `File` constructor."
179929549,4812,mjsax,2018-04-07T22:26:46Z,as above.
179929590,4812,mjsax,2018-04-07T22:28:52Z,Did this slip? Or did you leave it intentionally?
179929609,4812,mjsax,2018-04-07T22:29:56Z,Did this slip?
179929657,4812,mjsax,2018-04-07T22:33:29Z,"Adding this implies, that we have to maintain the same code twice. Should we extract this into some internal method that we can call here to avoid code duplication?

What about other thread-level metrics?"
179929685,4812,mjsax,2018-04-07T22:35:09Z,For my own education: what is this? (btw: can we remove `this` below?)
180160009,4812,bbejeck,2018-04-09T16:50:49Z,"nit: I realize this was pre-existing in a single line, but since there are several parameters,  maybe put each param on its own line."
180167095,4812,bbejeck,2018-04-09T17:16:08Z,"super nit: what about `assertTrue((Double)metrics.metric(skippedRateMetric).metricValue() > 0.0);` however, I don't have a strong opinion in this one."
180172747,4812,vvcephei,2018-04-09T17:35:53Z,It's a private method with an unused return value. Making it void helps the reader to understand the code without having to trace through usages.
180176820,4812,vvcephei,2018-04-09T17:50:26Z,"Good question. I have developed the habit of delimiting variables in log messages, as it disambiguates the structure of the message for the reader. Without delimiters, there are several edge cases that would make the log message difficult to read.

For example, if the key were `""value""` and the value were `""""` with the old format, you get:
```
Error sending records (key value value  timestamp 1234)
```
Whereas, if the key were `""""` and the value were `""value""`, you get
```
Error sending records (key  value value timestamp 1234)
```
The only difference between these strings is where the extra space is. With delimiters, you have:
```
Error sending records (key=[value] value=[] timestamp=[1234])
Error sending records (key=[] value=[value] timestamp=[1234])
```

It's the kind of thing that saves people from #1 making a bad assumption about the nature of the problem and burning hours before they realize their mistake, or #2 being unable to clearly understand the error message and having to load it in a debugger just to understand what the values of the arguments actually are.

It sounds like your concern is about the ambiguity of `[]` as delimiters, since they already indicate a list. Can we keep delimiters but pick a different character? Other paired delimiters are `<>` and `{}`, and `""""` and `''` also come to mind. WDYT?"
180180359,4812,vvcephei,2018-04-09T18:02:43Z,"I've been mulling over the same thing, and that was part of what I was trying to achieve with my experiment before. I think I have a better solution now, so maybe you can take another look after my next update and see what you think."
180183446,4812,vvcephei,2018-04-09T18:13:40Z,"That also works, but `assertNotEquals` is a little nicer in that it'll print the actual value on failure, whereas `assertTrue` only tells you that it was `false` on failure. I suppose I could add a utility method `assertGreater` that prints the values on failure, but in this case, I'm really just making sure that the metric got moved. I don't care that much to assert what it got moved to, or I would override the time implementation and assert the exact expected value."
180183779,4812,vvcephei,2018-04-09T18:14:53Z,"we can remove `this` below.

The WeakerAccess inspection tells you that it's possible to restrict the access scope of `cancel()`. I think this particular case was warning me that `cancel()` could be package-private instead of public. But the static analyzer can only look at the code in the project. We know that we do want the method to be public, so I added a supression for this inspection. An alternative would be to write black-box tests in a different package (just like real user tests would be), and the static analyser wouldn't warn us anymore, since it would have an example of a usage requiring public access."
180195011,4812,mjsax,2018-04-09T18:54:07Z,"I personally thank, that having `=` (without `[]`) is good enough as the `=` makes it clear:
```
Error sending records (key=value value= timestamp=1234)
```
Thus, it's not ambiguous to me (I agree that having no delimiter at all would be bad).

It's just that I like uniform formatting, and this would introduce a new style -- I am fine with change to this style, but we should agree on one style and rewrite code (on the side) if it does not fit the 'style guide'.

\cc @bbejeck @dguy @guozhangwang "
180196390,4812,mjsax,2018-04-09T18:59:05Z,"Will do, after you pushed an update :)"
180238210,4812,guozhangwang,2018-04-09T21:34:54Z,"I'm do not feel very comfortable to define the metrics name in scattered places, because it means whenever we'll update the name we have to remember to update all the places (for this sensor the other place we declared it is

```
skippedRecordsSensor = metrics.sensor(prefix + "".skipped-records"", Sensor.RecordingLevel.INFO);
skippedRecordsSensor.add(createMeter(metrics, new Sum(), ""skipped-records"", ""skipped records""));
```
So which line gets called first, it will create the sensor, while keeping the other just as an no-op.

), and that's why I liked @vvcephei 's proposal for wrapping the sensor names in the leveled metrics, and passing those metrics across different modules than re-declaring the sensors in different places.

This makes me feel more urgent to do the refactoring of the metrics hierarchy."
180239208,4812,guozhangwang,2018-04-09T21:39:14Z,"I tend to prefer `key=[value]`, but I do not have a scientific reason for that: I just feel it is more ""vivid"" :P"
180239876,4812,guozhangwang,2018-04-09T21:41:57Z,"Again, if we could pass around the `threadMetrics` here, it will make the code more readable: we can make it very clear at which places we record some task metrics like `taskMetrics.sensorA.record()` and where do we record thread-level metrics like `threadMetrics.skippedRecordsSensor.record()`.

But I think it is better to be left as a follow-up PR as this one is already pretty big."
180240234,4812,guozhangwang,2018-04-09T21:43:20Z,Nice improvement
180243194,4812,vvcephei,2018-04-09T21:56:21Z,"Yeah, I keep getting wrapped around the axle thinking about stuff like this. Hopefully, I'll be able to deliver a reasonable implementation for this PR, and I'll continue to mull about a way to pass the right metric context around the code base."
180243791,4812,vvcephei,2018-04-09T21:59:20Z,"I'm about to push a commit to put the skipped-records sensor in particular in a common place, since it winds up getting accessed from so many different places in the code. I'm hoping that will be good enough for now, and we can seek an elegant enclosing-scope metrics implementation in the future."
180246639,4812,vvcephei,2018-04-09T22:12:53Z,"@mjsax I can dig the desire to have uniform style on log messages. I'll also point out that the logs are part of the public API, so we can't just go terraform them willy-nilly, but instead we'd have to change them only in scope of the relevant KIPs, which makes it difficult to change, or even establish, a log style.

Nevertheless, if we don't already have a clear style for streams logs, I'll advocate for some kind of enclosing delimiter on substitutions. I continue to agree that square brackets are confusing w.r.t. common `List#toString()` formats, so I think we should agree on a different enclosing delimiter.

I agree that `=` is better than nothing, but it's still ambiguous when the substitution is 0 or more whitespace characters, while `[]` vs `[ ]` gives you more of a clue. No choice here is going to be perfect, but my experience is that this format saves enough debugging time to be worth the visual noise."
180576030,4812,guozhangwang,2018-04-10T21:39:40Z,"What's the purpose of keep track of the metric names? If it is for preventing double-registering, I think relying on maintaining the metrics name inside the sensor would not always work, since multiple sensors would be added into the `metrics` registry, and we still cannot prevent different sensors trying to register the same metrics."
180576708,4812,guozhangwang,2018-04-10T21:42:38Z,We do not need the non-arg constructors since it will be defined by default.
180577126,4812,guozhangwang,2018-04-10T21:44:29Z,No callers seem to provide any non-empty `tags`?
180577282,4812,guozhangwang,2018-04-10T21:45:10Z,Ditto here.
180577454,4812,guozhangwang,2018-04-10T21:45:56Z,"If we always create the skipped record sensor upon creating the thread, then we should always get the sensor right? If that case, should we simply throw if the `getSensor` returns null?"
180578491,4812,guozhangwang,2018-04-10T21:50:26Z,"This is a meta comment:

I think we have seen two approaches here:

1. Pass along the metrics objects across different modules (in some classes, we will pass multiple metrics objects for different levels, like threadMetrics and taskMetrics) in order to record their sensors.

2. In the current PR: only pass alone the metrics registry (i.e. the `Metrics` object) along different modules, but standardize the sensor name construction, and get the sensor by its raw name directly whenever necessary to record the sensor.

I am slightly in favor of the second one since we could pass long fewer parameters, i.e. only a single `Metrics` object which can be accessed 1) from ProcessorContext, 2) in multiple internal classes."
180578769,4812,guozhangwang,2018-04-10T21:51:35Z,This is a detailed comment: what's the rantionale of naming it `CommonStreamsMetrics`? Is it for thread-level metrics only? I.e. should we just move this static function into `ThreadMetrics`?
180607694,4812,mjsax,2018-04-11T00:47:33Z,"> if we don't already have a clear style for streams logs

We never discussed this explicitly; it's just a matter of fact that we use `key=value` so far from what I can remember. The question is, how much we gain if we start to rewrite to a different format and how much work it it.

With regard to ambiguity: you can always construct an (academic?) example for which any formatting strategy ""break"" and is ambiguous... If we agree on `key=[value]` I am fine with it. Still not sure, if we gain much (but if you think we do, it's fine with me to change)"
180815261,4812,vvcephei,2018-04-11T16:20:42Z,"I have pulled this change into a separate PR: https://github.com/apache/kafka/pull/4853

The intent is to make it a no-op if you add the same metric to the same sensor twice, as opposed to the current behavior, in which the `registry.registerMetric(metric)` throws an exception if the metric is already registered.

With this change, you'll still get an exception if the metric is already registered in another sensor, but if it's already in the same sensor, you just get a no-op success."
180815535,4812,vvcephei,2018-04-11T16:21:38Z,"This privatizes the constructor, guaranteeing that the class cannot be instantiated. It's a way of enforcing that the class be used only for its static members."
180815880,4812,vvcephei,2018-04-11T16:22:44Z,"It's laying the groundwork for a future change in callers can compose thread-level tags, task-level tags, etc."
180816071,4812,vvcephei,2018-04-11T16:23:20Z,"To do this properly, though, the class should also be final. I'll make that change."
180816181,4812,vvcephei,2018-04-11T16:23:38Z,same rationale.
180817574,4812,vvcephei,2018-04-11T16:28:17Z,"This method is used to idempotently create or retrieve the sensor. In other words, the mechanism by which we create the sensor when we create the thread is that it calls this method, and the sensor is null, so it creates it.

You're correct in that if we do that, then all other usages will just return the existing sensor.

I'm not sure I see the value in separating creation from retrieval so that we can throw an exception if you retrieve it without creating it first."
180824861,4812,vvcephei,2018-04-11T16:51:39Z,"Yeah, I'm still undecided on whether approach 1 or 2 is better. But I did decide that I don't want to make a call on it in this PR. If you'd like me to resolve this sooner rather than later, I can follow up immediately with another PR to reorganize the metrics.

The reason I pulled skipped-records out into a CommonStreamsMetrics class is that that metric is common across all components in Streams. It's accessed by both our framework-level code and also by the user-space DSL-provided processors. Thus, it needs to live in a spot where all those components have visibility on it.

There's a distinction between metrics that are aggregated at the thread level and metrics that belong to `StreamThread`. It'll be difficult to really do a good job in this PR with that distinction, though, without refactoring the whole metrics hierarchy. Since we're already over 2,000 LOC, I'd really like to move such a refactoring to another PR.

What I have done in this PR is as minimal as I can manage to expose the skipped-records sensor to our Processors."
180866308,4812,bbejeck,2018-04-11T19:09:05Z,"I also prefer `key=[value]`, but can't say it's for any specific reason other than personal preference."
180870195,4812,bbejeck,2018-04-11T19:23:52Z,is this line intentional?
180870791,4812,bbejeck,2018-04-11T19:26:16Z,"The `hasItem` matcher is new to me, nice one! "
180876468,4812,bbejeck,2018-04-11T19:47:38Z,"Do we need a separate class for this?   We could add the `getMetricByName` method to `StreamsTestUtils` instead, additionally, it doesn't access anything package private  so it should be fine to make the method public "
180878303,4812,bbejeck,2018-04-11T19:54:39Z,"meant to say this before, nice addition!"
180930916,4812,vvcephei,2018-04-11T23:50:42Z,oops. I put it there when I needed a place for a breakpoint. Sorry!
180931250,4812,vvcephei,2018-04-11T23:53:06Z,"Thanks! It's predicated on us using log4j as the implementation for slf4j in the unit tests, so if that changes, we'll have to put in a different log appender. But that seems unlikely, and it's handy to have this in the mean time."
180932520,4812,vvcephei,2018-04-12T00:01:29Z,"I can move it to StreamTestUtils if you like.

I made `getMetricByName` package-private to prevent other code from calling it, since it's intended for these tests only. Making it public would open this method up to be called beyond the intended scope. I'd rather defer that until we have a use case we think calls for broadening the scope."
181250474,4812,guozhangwang,2018-04-12T23:26:24Z,"Thanks for the explanation, that makes sense."
181250828,4812,guozhangwang,2018-04-12T23:28:41Z,"I had another meta question while discussing offline, and I'll leave it here for discussion: since different meters belong to the different metrics levels, should we move these static functions to the corresponding `XXMetrics` instead? I'm thinking about that since we have some meters that exist in multiple levels, like commit rate. In the future if we move them all to this class then we need to get one function for each level as their tags naming conventions are different, and hence it is not really `common` streams metrics."
181251041,4812,guozhangwang,2018-04-12T23:30:03Z,Similar to my other comment on `CommonStreamsMetrics`: could we move these static functions to the corresponding level metrics class?
181252004,4812,guozhangwang,2018-04-12T23:36:39Z,"@vvcephei @bbejeck @mjsax I'd suggest in the future try to only piggy-back different changes into the same PR if we think they are either correlated or if they are really trivial. Having a single PR mingled with multiple changes has several drawbacks:

1. It makes git history a bit harder to trace: think, ""git blame"" would be tricker to reason.
2. It tends to generate bigger PRs than necessary, making reviewer less willing to start working on them :P
3. If multiple rounds of reviews are needed, even requiring major code refactoring, it will surprisingly introduce regressions during those iterations as by-products of the multiple changes."
181252540,4812,guozhangwang,2018-04-12T23:40:36Z,I'm not sure why we need to pass around `streamsMetricsImpl` from StreamThread to everywhere else now?
181410527,4812,vvcephei,2018-04-13T14:43:21Z,"Previously, we passed around the interface only to cast it back to `StreamsMetricsImpl` in line 80 of this file. All I did was make TaskMetrics declare that it really does want a `StreamsMetricsImpl` instead of a `StreamsMetrics` explicitly.

If we're going to cast it anyway, why not just use the type system?"
181412307,4812,vvcephei,2018-04-13T14:49:06Z,"I thought it would be nice if the metrics naming conventions were all in one place, to help us maintain consistency. Right now, we have one (internal) naming convention enforced via our (public) metrics API: `StreamsMetrics`, but we also have a bunch of metrics with no defined naming convention declared, e.g., in `StreamsMetricsThreadImpl`.

I think it'll be simpler for people to use the metrics if we maintain consistency throughout the whole streams code-base, and it'll be simpler to maintain consistency if we keep all the naming conventions in one file."
181424139,4812,vvcephei,2018-04-13T15:25:18Z,"There are two different concepts of belonging in play here.
1. grouping: a metric ""belongs"" to the Thread level if it's aggregated at that level
2. ownership: a metric ""belongs"" to the Thread level if StreamThread needs a reference to it.

Grouping and Ownership are orthogonal here.

There are several metrics that are aggregated at the thread level and are also owned by StreamThread, like the commitTimeSensor. Grouping is easy to determine; all you have to do is look at the metric name. To tell that commitTimeSensor is Owned by StreamThread, you can trace the references to the sensor. It's never passed outside of StreamThread, so it's owned by StreamThread.

Other metrics are still Grouped by thread but Owned by other classes, such as the taskCreatedSensor. All of StreamThread, AbstractTaskCreator, TaskCreator, and StandbyTaskCreator have references to this metric, so they all share the ownership. Now that I'm looking at it, StreamThread never uses the reference, so it could be owned only by AbstractTaskCreator and its children.

Finally, we get to the skippedRecordsSensor. This metric is still Grouped by thread, but Ownership is shared among 14 classes. Two of them are framework classes (GlobalStreamThread and StreamThread), and 12 of them are user-space Processor classes.

Since Ownership for skippedRecordsSensor is so dispersed, I decided to just make its ownership global, aka common. Anything in the entire Streams codebase can get a reference to the skippedRecordsSensor.

I wouldn't move all the metrics into the global ownership space just because I moved one, and I wouldn't give StreamThread a reference to a metric just because that metric happens to be aggregated by thread.

Am I making sense?"
182273476,4812,guozhangwang,2018-04-17T23:43:48Z,"I understand the separation of ownerships of metrics, my question is: right now the `skippedRecordsMeter` assumes this metric is grouped at the thread-level, and hence the second parameter is a `threadName`. What if in the future we add a per-task level `skippedRecordsMeter(final Metrics, final TaskId)`? Would that be put in this `CommonStreamsMetrics` as well?

Practically speaking either is fine, and I think I was originally leaning towards not having a `CommonStreamsMetrics` conceptually since that for each meter, it is going to be aggregated at a specific layer anyways. But if people think that it is better of having a `CommonStreamsMetrics` where we put these metrics so that all dependent classes would just depend on `o.a.k.streams.processor.internal.metrics`, I'm fine with it as well."
182274054,4812,guozhangwang,2018-04-17T23:47:54Z,Ditto.
182274130,4812,guozhangwang,2018-04-17T23:48:31Z,"Since we will have a different KStreamWindowReduceProcessor for each topology, each task, each thread, the Thread.currentThread() will always be the same; it is okay to get the meter at the constructor and cache it, than trying to search for it in the registry each time.

Ditto below."
182274177,4812,guozhangwang,2018-04-17T23:48:46Z,Ditto.
182274189,4812,guozhangwang,2018-04-17T23:48:53Z,Ditto.
182274204,4812,guozhangwang,2018-04-17T23:49:00Z,Ditto.
182274440,4812,guozhangwang,2018-04-17T23:50:36Z,Ditto.
182274531,4812,guozhangwang,2018-04-17T23:51:12Z,Ditto.
182274558,4812,guozhangwang,2018-04-17T23:51:21Z,Ditto.
182275004,4812,guozhangwang,2018-04-17T23:54:24Z,Ditto.
182275021,4812,guozhangwang,2018-04-17T23:54:31Z,Ditto.
182275061,4812,guozhangwang,2018-04-17T23:54:41Z,Ditto.
182275088,4812,guozhangwang,2018-04-17T23:54:53Z,Ditto.
182275337,4812,guozhangwang,2018-04-17T23:56:46Z,Do we need to pass the sensor all the way down here? Could we fetch it from the `CommonStreamsMetrics` directly as well? Note that each thread will create its own `RecordDeserializer` objects that are exclusively owned by the thread itself.
182275382,4812,guozhangwang,2018-04-17T23:57:02Z,Please see my comment in `RecordDeserializer`
182275534,4812,guozhangwang,2018-04-17T23:58:07Z,This import is not needed?
182275652,4812,guozhangwang,2018-04-17T23:58:50Z,"Similar as above, do we still need to pass this sensor along, than getting it from `CommonStreamsMetrics` directly?"
182275731,4812,guozhangwang,2018-04-17T23:59:27Z,As above in `RecordDeserializer`.
182275826,4812,guozhangwang,2018-04-18T00:00:09Z,Ah right. THanks.
182275969,4812,guozhangwang,2018-04-18T00:01:17Z,"Same as above, could we just get it from `CommonStreamsMetrics` now?"
182276400,4812,guozhangwang,2018-04-18T00:04:31Z,Could you point to me where do we remove this sensor upon shutting down now?
182276649,4812,guozhangwang,2018-04-18T00:06:46Z,Why passing `threadClientId` twice?
182277627,4812,guozhangwang,2018-04-18T00:14:01Z,Actually I've been thinking .. could we move the construction of the `TaskManager` and its `taskCreators` into the constructor of `StreamThread` directly from `create` call? Then we can get the threadName from `currentThread.name()` directly and do not need to pass this parameter around any more.
182278148,4812,guozhangwang,2018-04-18T00:18:07Z,"For test util classes, they are usually put in `org.apache.kafka.test` package. This allows a larger scope of utilization by non-streams unit tests."
182278220,4812,guozhangwang,2018-04-18T00:18:37Z,And you can put them in `streams.test.o.a.k.test` folder.
182278443,4812,guozhangwang,2018-04-18T00:20:20Z,This looks like a general test util function than a streams-specific test util function. How about moving it to `org.apache.kafka.test.TestUtils`?
182826571,4812,vvcephei,2018-04-19T17:34:08Z,"Using the interface is really only useful in our public ProcessorContext interface. Using the StreamsMetrics interface in our internals just forces us to cast it back to StreamsMetricsImpl all over the place.

I've changed it to S.M.I. here and elsewhere to cut down on the casting. The only things that should need to cast now are components that get the metrics via ProcessorContext, and they should always perform that cast as early as possible to prevent post-initialization runtime exceptions."
182827178,4812,vvcephei,2018-04-19T17:36:18Z,Override to refine the type from StreamMetrics to StreamMetricsImpl in support of internal usages.
182829844,4812,vvcephei,2018-04-19T17:45:17Z,"It's used by the NodeMetrics implementation (it might have been unused when you made this comment, though)"
182830330,4812,vvcephei,2018-04-19T17:46:54Z,"I've removed CommonStreamsMetrics, so we need either to pass the sensor or the whole StreamsMetricsImpl. The sensor is smaller scope, so I just did the sensor for now."
182830606,4812,vvcephei,2018-04-19T17:47:51Z,The same rationale from `RecordCollectorImpl` applies.
182834003,4812,vvcephei,2018-04-19T17:58:48Z,"This is an internal class, and this javadoc doesn't say anything that the method signature doesn't say. I added a new constructor and changed the existing one, so I just removed the doc rather than updating it."
182834401,4812,vvcephei,2018-04-19T18:00:09Z,"This existed only so a test could override it. Instead, I added a constructor arg for the test to pass and removed this method."
182837476,4812,vvcephei,2018-04-19T18:10:55Z,"I refactored these to flatten the metric definition, since it was super hard to figure out what metrics were actually being created.

Maybe you can forgive me for this because I actually found a bug: The description of the total metrics say that it counts the number of calls, but it previously summed the recorded values. (was via createMeter -> new Meter -> new Total)"
182840375,4812,vvcephei,2018-04-19T18:20:40Z,"I'd like to add taskLevel names and tags, but it doesn't work with the current way most of the corresponding sensors get created.

Ultimately, I'd like to flatten all the metrics definitions like I did with StreamThreadMetricsImpl, which would make it possible to define the task and node level conventions here as well. But I don't want to do that in this PR."
182840948,4812,vvcephei,2018-04-19T18:22:39Z,"defining and providing skippedRecordsSensor here now, since it's now needed in contexts where the implementation is not a StreamThreadMetricsImpl."
182843117,4812,vvcephei,2018-04-19T18:30:00Z,"Similar to the metrics in StreamThread, by getting rid of the Meter and flattening these metrics, I realized that the total computation was incorrectly a Total rather than a Count."
182843558,4812,vvcephei,2018-04-19T18:31:36Z,These metrics never get removed. Is that ok?
182845636,4812,vvcephei,2018-04-19T18:38:54Z,"This is required per the docs, but we previously only added it in production code paths. Now we add it in all code paths."
182846028,4812,vvcephei,2018-04-19T18:40:13Z,"This is required per the docs, but we previously only added it in production code paths. Now we add it in all code paths."
182846606,4812,vvcephei,2018-04-19T18:42:13Z,"The skipped records metrics are now always present. Rather than updating the hard-coded value, I did this to make the test less brittle."
182847809,4812,vvcephei,2018-04-19T18:46:25Z,"I can move them there. The reason I put them here was to restrict the scope in which log4j was an allowed import (I had to add an exception). Co-locating this class in any other package will allow other code to accidently depend on log4j when it should depend on slf4j instead.

I was also uncertain about whether it would be a good idea to expose this class for general use in Kafka tests... WDYT?

If you're not concerned about supporting this class for the whole project, maybe `o.a.k.test.log4jappender` package would be the best of both worlds?"
182849325,4812,vvcephei,2018-04-19T18:51:39Z,I replaced the override-with-capture strategy in this test with just a regular StreamsMetrics and verifying the invocation by checking that the total metric is == 1.
182850930,4812,vvcephei,2018-04-19T18:57:02Z,"""virtual"" just in case people go looking for the actual thread in the thread dump. I also thought about using `Thead.currentThread()`, but it wouldn't necessarily be the same thread when the tests run."
182851149,4812,vvcephei,2018-04-19T18:57:37Z,"same thinking regarding ""virtual"""
182914835,4812,guozhangwang,2018-04-19T23:45:56Z,"Ah I see, overlooked `log4j` dependency; let's keep it as is then."
182915655,4812,guozhangwang,2018-04-19T23:51:21Z,Why this need to be public now?
182915744,4812,guozhangwang,2018-04-19T23:52:01Z,"Good point, let's add a TODO marker and remove them in a follow-up PR: so we do not drag too long on this one."
182916297,4812,guozhangwang,2018-04-19T23:56:06Z,Sounds good!
182916817,4812,guozhangwang,2018-04-20T00:00:03Z,"I guess my previous comment was a bit misleading :P Actually I'm not against the `CommonStreamsMetrics` and `ThreadMetricsConventions` classes, but I think we could have one such class for each different layer than having a `common` class, for the reason I mentioned before. But since you have removed it I'm also fine with passing along the sensors as well.

We can consider which one is better in the near future and if we can do another code refactoring, but let's not block on this PR for too long."
182917077,4812,guozhangwang,2018-04-20T00:02:10Z,Sounds good!
182917552,4812,guozhangwang,2018-04-20T00:06:12Z,Good call!
182917825,4812,guozhangwang,2018-04-20T00:08:20Z,Cool.
182918255,4812,guozhangwang,2018-04-20T00:11:55Z,"Hmm.. why we create the sensor in `StreamsMetricsmpl` while removing it in `StreamsMetricsThreadImpl`? It seems a bit inconsistency.. could we still create in `StreamsMetricsThreadImpl`, and let `StreamsMetricsImpl` to get a hold of the sensor object assuming it is already created then (i.e. set `sensor == null` in constructor, and in `skippedRecordsSensor(): if (sensor == null) try get it from the metrics`)?"
182918502,4812,guozhangwang,2018-04-20T00:14:29Z,Not sure I understand this comment: it was indeed defined as a `Count()` before as well right?
183065396,4812,vvcephei,2018-04-20T14:18:25Z,"No, it was previously:
```
sensor.add(new Meter(new Count(), rateMetricName, totalMetricName));
```
But the `Count` there is only used for updating the rate. Here's the Metric constructor:
```
public Meter(SampledStat rateStat, MetricName rateMetricName, MetricName totalMetricName) {
        this(TimeUnit.SECONDS, rateStat, rateMetricName, totalMetricName);
}

public Meter(TimeUnit unit, SampledStat rateStat, MetricName rateMetricName, MetricName totalMetricName) {
        this.total = new Total();
        this.rate = new Rate(unit, rateStat);
        this.rateMetricName = rateMetricName;
        this.totalMetricName = totalMetricName;
}
```

You can see that it's using `Total` for the ""total"" metric, which I guess makes sense given the parameter name. But according to the description of our ""total"" metric, what we really wanted to do was keep a count of occurrences, which should be a `Count` stat."
183075990,4812,vvcephei,2018-04-20T14:52:56Z,"Ok, when I took another look, I found that `hitRatioSensor` does get removed, but its parent doesn't. Also neither sensors have scoped names, so every `record()` will actually update *all* hit ratio metrics for *all* caches.

That seems like a bigger deal, so I've already prepared a follow-up PR. I'll send it next once this one is merged."
183081167,4812,vvcephei,2018-04-20T15:08:22Z,"Because I moved StreamsMetricsImpl to the `...internal.metrics` package, but I took a look, and it was only used (properly) by `ProcessorNode`.

`StreamTask` also used it, but only by specifically wrapping the operation in a `Runnable` and passing it to the metric to immediately be run.

I've added https://github.com/apache/kafka/pull/4812/commits/18358f30445cd27c5155be7d73b8d5a98410886a to simplify `StreamTask`'s usage to not need this method, and move the method to `ProcessorNode`."
183091285,4812,vvcephei,2018-04-20T15:40:35Z,"I tried that, but it wound up making things messier than I expected (because there are other callers to StreamsMetricsImpl, and because it makes the ownership of this sensor ambiguous).

Instead, I added https://github.com/apache/kafka/pull/4812/commits/36c065fa93b922668af87c8c7cc00460a7282ced, which gives SMI the ability to remove its own sensors, and then I called to it from the two places (StreamThread and GlobalStreamThread) that actually need to unload metrics when they shut down.

WDYT?"
183114250,4812,guozhangwang,2018-04-20T17:09:29Z,"I see. That makes sense.

I think it was not introducing any issue only because we only call that `sensor.record()` not `sensor.record(n)` so `Count` and `Total` are actually the same: `record()` is the same as `record(1)`, but I agree that it should really be `Count`, to avoid any potential bugs."
183114732,4812,bbejeck,2018-04-20T17:11:29Z,nit: Just thinking if this change is necessary as `NodeMetrics` is an internal class so a cast here from `ProcessorContext` to `InternalProcessorContext` should not be a big deal and keeps `ProcessorNode` more generic. EDIT: NM read a comment below about using type system and I agree.
183116528,4812,bbejeck,2018-04-20T17:18:41Z,why remove this?
183130635,4812,guozhangwang,2018-04-20T18:13:14Z,"I did not completely get your explanation re: `because there are other callers to StreamsMetricsImpl, and because it makes the ownership of this sensor ambiguous`. Could you elaborate a bit more?"
183161064,4812,bbejeck,2018-04-20T20:27:06Z,nit: `shouldLogAndMeterOnSkippedRecords` -> `shouldLogAndMeterOnSkippedRecordsWithNullValue` ?
183164652,4812,bbejeck,2018-04-20T20:43:09Z,Line 503 the JavaDoc should change as the constructor for `StreamsMetricsImpl` only takes `Metrics` and a `String` parameter.
183166451,4812,bbejeck,2018-04-20T20:51:34Z,Maybe consider replacing `Stack` with `Deque` as `Stack` is synchronized and `ownedSensors` only adds in the constructor and removes values in `synchronized` block already.
183182512,4812,vvcephei,2018-04-20T22:29:21Z,"This is just used in one spot. It's easier to read the code if the log message is located at the spot where it gets logged rather than at the top of the file.

In earlier versions of Java, code like this was beneficial for performance, since the string is an object that can just get allocated and instantiated once statically, rather than dynamically on every invocation. But nowadays, the compiler and JIT compiler are smarter than that, so there really no benefit to coding this way, and you still pay the comprehensibility cost of having to follow the indirection.

I didn't want to make it a ""thing"", though, so I only inlined the message I needed to change."
183182785,4812,vvcephei,2018-04-20T22:31:55Z,"Ah, good eye. I might just ditch the Javadoc, since it's an internal class and its function is pretty obvious."
183182905,4812,vvcephei,2018-04-20T22:33:10Z,"Ooh, good call. I will do that and probably avoid using Stack again."
183185297,4812,vvcephei,2018-04-20T22:54:13Z,"Sorry; I misread your suggestion. I thought you wanted the StreamsMetricsImpl to take the sensor as a constructor argument. 

Aside from the StreamThread (via StreamThreadMetricsImpl), several other classes directly invoke the StreamsMetricsImpl constructor and thus obtain a StreamsMetricsImpl that is not a StreamThreadMetricsImpl. Namely, GlobalStreamThread, MockProcessorContext, and TopologyTestDriver.

When the code paths downstream of these points need to record a skipped record, they will get a null sensor back. It wouldn't be possible to get it from the Metrics registry at that point, though, because the skipped records sensor is scoped by thread (or ""virtual"" thread for the test-utils), and the sensor would never have been created for GlobalStreamThread, MockProcessorContext, or TopologyTestDriver. So the only way to get it at that point is to have either the caller of the SMI constructor or the SMI itself create the sensor (either at construction or at call-time).

The previous implementation with the public static getter was effectively saying that the one who wants it in a particular context first creates it, but it's problematic because no-one owns it. And indeed, in my implementation, the sensor never got destroyed. In practice I think it's not a huge deal because I'm sure it's rare for a streams app to shut down and start up again with the same Metrics registry, and I think the threads live as long as the app. But still, we have this model of unloading metrics when they're out of scope, and I think it's a good one.

So that brings us to the current implementation. In the current implementation, the skipped records metric is clearly owned by the StreamsMetricsImpl, which it may as well be, since that is the smallest scope in which it's needed. It's the first metric to be owned by SMI, so I had to create a removeAll method, and make sure it's invoked in the right places. But that seems appropriate; every other scope that owns metrics has such a method."
329311213,7378,jukkakarvanen,2019-09-28T13:57:09Z,"This is still using deprecated pipeInput because there is partition verification.
Can we drop here partition verification which is null here?"
329311302,7378,jukkakarvanen,2019-09-28T14:00:55Z,"This is still using deprecated pipeInput because there is partition verification.
Can we skip partition assertion here or do we need some other way to get ProducerRecord?"
330105303,7378,vvcephei,2019-10-01T14:55:39Z,"yes, it seems to be not what this test is checking on. I think we can drop it here."
330108688,7378,vvcephei,2019-10-01T15:01:12Z,"It seems like the purpose of this test class is to verify the test driver. Since the partition is effectively not part of the test driver's (non-deprecated) API, I think we can drop it.

Technically, as long as the deprecated interface is still in the public API, we should still exercise it, but I think in this case it's extremely unlikely we would break the partition field in the future, and it's probably not that risky anyway.

I'd vote just to skip the partition assertion here."
330248099,7378,jukkakarvanen,2019-10-01T20:01:43Z,Moved to use TestOutputTopic and partition assertion removed.
330248275,7378,jukkakarvanen,2019-10-01T20:02:10Z,Moved to use TestOutputTopic and partition assertion removed.
330345901,7378,vvcephei,2019-10-02T01:55:47Z,"Note: the unused suppression indicates that we're missing test coverage. If we want to add test coverage, we can at the same time address the WeakerAccess warning by putting the test in a different package than this class. I.e., a true test of a class's public API should be located outside that class's package, so it wouldn't have access to package-private members."
330346143,7378,vvcephei,2019-10-02T01:57:20Z,"It might be a good idea to note that this only advances the _stream_ time, not the wall-clock time, and therefore doesn't trigger punctuations."
330346457,7378,vvcephei,2019-10-02T01:59:28Z,"would it be handy to also print out the other constructor argument fields here? (serializers in particular, not sure if the times are that interesting)"
330346561,7378,vvcephei,2019-10-02T02:00:05Z,should we also require non-null deserializers?
330347517,7378,vvcephei,2019-10-02T02:07:04Z,"I'm curious why these suppressions were necessary. I'd have thought that the package-private members would all have been here because they were needed by the I/O Topic classes.

Is it because IDEA thinks that protected is ""stronger"" than package-private?"
330347991,7378,vvcephei,2019-10-02T02:10:28Z,"You should double-check, but I think Java will automatically do this in string concatenation. (It was news to me when I learned it a couple of months back)"
330348273,7378,vvcephei,2019-10-02T02:12:15Z,Probably should use a (slf4j) Logger instead of stdout here.
330348480,7378,vvcephei,2019-10-02T02:13:29Z,These tests are beautiful
330348611,7378,vvcephei,2019-10-02T02:14:17Z,There seems to be a lot of overlap between these and the input tests. Could they just be one test class that covers both?
330348785,7378,vvcephei,2019-10-02T02:15:25Z,"```suggestion
    //Factory and records for testing already deprecated methods
```

Thanks for keeping the test coverage for deprecated methods."
330369097,7378,jukkakarvanen,2019-10-02T04:37:38Z,SuppressWarnings removed and comment clarified
330369142,7378,jukkakarvanen,2019-10-02T04:37:54Z,Added
330369166,7378,jukkakarvanen,2019-10-02T04:38:07Z,Added
330369686,7378,jukkakarvanen,2019-10-02T04:42:09Z,Not sure the reason. 
330369748,7378,jukkakarvanen,2019-10-02T04:42:38Z,Replaced with another style toString
330369769,7378,jukkakarvanen,2019-10-02T04:42:49Z,Dobe
330369794,7378,jukkakarvanen,2019-10-02T04:43:02Z,Thanks
330370023,7378,jukkakarvanen,2019-10-02T04:44:51Z,Merged
330370105,7378,jukkakarvanen,2019-10-02T04:45:29Z,Typo fixed
331089257,7378,bbejeck,2019-10-03T14:59:10Z,"nit: I **_think_** that streams convention now is to mark methods as `@Deprecated` now vs. using `@SuppressWarnings`.  My reasoning for this is that these test methods will go away once the deprecated method under test is removed.  This applies here and other deprecated tests below.  However, this is a minor point.

\cc @vvcephei "
331091087,7378,bbejeck,2019-10-03T15:02:10Z,super nit:  `deprecatd`  -> `deprecated` here and below
331099773,7378,bbejeck,2019-10-03T15:18:38Z,nit: `configure topic` -> `configure the topic`
331122381,7378,bbejeck,2019-10-03T16:02:51Z,Why did we eliminate the partition check in the `assertNextOutputRecord` method?
331124616,7378,bbejeck,2019-10-03T16:07:47Z,nit: `You need to have own TestInputTopic object` -> `You need a TestInputTopic object`
331125926,7378,bbejeck,2019-10-03T16:10:37Z,as above
331126553,7378,bbejeck,2019-10-03T16:12:02Z,nit: `send` -> `sent`
331131776,7378,bbejeck,2019-10-03T16:23:46Z,"Do we still need this comment? Seems to me the other topic is expecting a `String` as well, but I could be missing something."
331134784,7378,bbejeck,2019-10-03T16:30:39Z,"A minor point, but isn't this supposed to swap the key and value on output?"
331189754,7378,vvcephei,2019-10-03T18:33:18Z,"Ah, yes, good catch, @bbejeck . Adding the deprecation annotation will also satisfy the compiler, and it is indeed better. As you say, it documents that this method will also be removed when the other one is removed. It's not such a concern for a test, but the other big benefit is that it prevents ""deprecation laundering"": suppressing would make it ok for another method to call this one, whereas deprecating this method would also notify all callers that _this_ method is using deprecated functionality."
331191034,7378,vvcephei,2019-10-03T18:36:17Z,"Partitions are not part of the `TestRecord` API in the KIP, so it's not possible to make assertions on it in the new API. We can add the field later on if requested.

I don't remember offhand why we didn't include it; maybe something to do with the symmetry of the API."
331212689,7378,jukkakarvanen,2019-10-03T19:27:54Z,"Ok, modified"
331212945,7378,jukkakarvanen,2019-10-03T19:28:27Z,Fixed
331214169,7378,jukkakarvanen,2019-10-03T19:31:26Z,added
331215421,7378,jukkakarvanen,2019-10-03T19:34:39Z,Fixed
331215484,7378,jukkakarvanen,2019-10-03T19:34:49Z,Fixed
331216194,7378,jukkakarvanen,2019-10-03T19:36:24Z,Removed
331225577,7378,jukkakarvanen,2019-10-03T20:00:01Z,"There are two stream. This is using this:
        builder.stream(INPUT_TOPIC).to(OUTPUT_TOPIC);

OUTPUT_TOPIC_MAP is where values are swapped
"
331226741,7378,jukkakarvanen,2019-10-03T20:02:58Z,Fixed
331233340,7378,jukkakarvanen,2019-10-03T20:19:25Z,"I removed all unnecessary field from TestRecord compared to ProducerRecord and ConsumerRecord to keep it simple. This way we can utilize standard assertions and do not need to ignore the partition and use OutputVerifier kind of contructions.

I don't see many use cases in normal Stream Application verification where partion is needed as we see here where this is only test class checking partition.

If there are need to verify partition, I would add for example extra method readProducerRecord and use deprecated method until this is possible added if there is need for it."
331344364,7378,mjsax,2019-10-04T05:22:30Z,nit: `inputTopic` -> `rightInputTopic`
331344534,7378,mjsax,2019-10-04T05:23:36Z,"Why do we need to pass `null` as third parameter? Saw this in some other tests, too."
331344660,7378,mjsax,2019-10-04T05:24:33Z,nit: `inputTopicRight`
331345200,7378,mjsax,2019-10-04T05:27:59Z,nit: add `assertTrue(outputTopic2.isEmpty());` (as done in the original test code)
331345336,7378,mjsax,2019-10-04T05:28:48Z,as above
331345361,7378,mjsax,2019-10-04T05:28:57Z,as above
331346249,7378,mjsax,2019-10-04T05:35:02Z,nit: `final Instant initialWallClockTime = Instant.ofEpochMilli(0)`
331346966,7378,mjsax,2019-10-04T05:39:36Z,"nit:
```
{@code TestInputTopic}` is used to pipe record into a {@link TopologyTestDriver} source topic.
```"
331347564,7378,mjsax,2019-10-04T05:43:28Z,nit: `create [a new instance via]`
331347785,7378,mjsax,2019-10-04T05:44:37Z,"nit: `message` -> `record`
nit: `{@link KeyValue} pairs.` (not add `.` at the end."
331347943,7378,mjsax,2019-10-04T05:45:33Z,"nit: `If you have multiple source topics, you need to create a {@code TestInputTopic} for each.`"
331348093,7378,mjsax,2019-10-04T05:46:36Z,`Kafka` -> `record` (same next line for value)
331348378,7378,mjsax,2019-10-04T05:48:14Z,nit: `messages` -> `records`
331348642,7378,mjsax,2019-10-04T05:49:54Z,"We can omit JavaDocs for non-public methods -- similar below -- wondering if we need this constructor? It's internal and hence, TopologyTestDriver can always use the one with all parameters."
331349765,7378,mjsax,2019-10-04T05:56:06Z,"TopologyTestDriver supports event/stream time punctuations -- hence, this comment is a little bit miss leading. I would suggest:
```
Advances the internally tracked event time of this input topic. Each time a record without explicitly defined timestamp is piped, the current topic event time is used as record timestamp.
<p>
Note: advancing the event time on the input topic, does not advance the tracked stream time in {@link TopologyTestDriver} as long as no new input records are piped. Furthermore, it does not advance the wall-clock time of {@link TopologyTestDriver}.
```"
331349882,7378,mjsax,2019-10-04T05:56:40Z,"In the KIP, the method is still called `advanceTimeMs` -- can you update the KIP?"
331351096,7378,mjsax,2019-10-04T06:02:48Z,"nit `Send input record to the topic and then commit the record.` (note, in KafkaStreams we use the abstractions of records, that are typed, while messages are untyped lower lever `byte[]` arrays).

Similar below."
331351274,7378,mjsax,2019-10-04T06:03:41Z,`key` -> `value`
331351440,7378,mjsax,2019-10-04T06:04:44Z,"Should we add ""May auto advance topic time"" to the other methods?"
331351655,7378,mjsax,2019-10-04T06:05:49Z,"nit: `{@link KeyValue}`

nit: remove double whitespace (2 times -- also more below)"
331351999,7378,mjsax,2019-10-04T06:07:47Z,"`time will auto advance` -- well, only if the advance is not zero. Should we be more precise?"
331352367,7378,mjsax,2019-10-04T06:09:56Z,`keySerializer.getClass().getSimpleName()` (similar for value)
331352477,7378,mjsax,2019-10-04T06:10:34Z,"nit: `{@code TestOutputTopic}`
nit `from [a] topic`"
331352569,7378,mjsax,2019-10-04T06:10:58Z,nit: `create [a new object via]`
331352610,7378,mjsax,2019-10-04T06:11:17Z,nit: `message` -> `record`
331352696,7378,mjsax,2019-10-04T06:11:46Z,same as for input topic
331352981,7378,mjsax,2019-10-04T06:13:21Z,"nit:
```
 * Using {@link #readKeyValue()} you get a {@link KeyValue} pair, and thus, don't get access to the record's timestamp or headers.
```"
331353098,7378,mjsax,2019-10-04T06:13:57Z,"nit:
```
 * Similarly, using {@link #readValue()} you only get the value of a record.
```"
331353157,7378,mjsax,2019-10-04T06:14:19Z,`Kafka` -> `record` (same for value)
331353208,7378,mjsax,2019-10-04T06:14:39Z,we can omit JavaDocs here
331353431,7378,mjsax,2019-10-04T06:15:48Z,"nit `[r]ecord`
nit `from [the] output topic and return the record's value.`"
331353468,7378,mjsax,2019-10-04T06:15:58Z,I think we can omit this
331354197,7378,mjsax,2019-10-04T06:19:42Z,`Read one record from the output topic and return its key and value as pair.`
331354239,7378,mjsax,2019-10-04T06:20:00Z,nit: `{@link KeyValue}`
331355498,7378,mjsax,2019-10-04T06:26:03Z,`Kafka` -> `{@link TopologyTestDriver}`
331355674,7378,mjsax,2019-10-04T06:26:53Z,"`A key/value pair, including timestamp and record headers, to be sent...`"
331355924,7378,mjsax,2019-10-04T06:28:09Z,"`If [a] record does` 

`{@link TestInputTopic} will auto advance it's time when the record is piped.`"
331356123,7378,mjsax,2019-10-04T06:29:01Z,remove `with a specific Instant` (unclear what this means).
331356208,7378,mjsax,2019-10-04T06:29:26Z,the key of the record
331356259,7378,mjsax,2019-10-04T06:29:38Z,the value of the record
331356323,7378,mjsax,2019-10-04T06:29:55Z,the record headers
331356516,7378,mjsax,2019-10-04T06:30:54Z,"remove `as Instant` (that is clear from the parameter type).

nit `If {@code null}`"
331356647,7378,mjsax,2019-10-04T06:31:29Z,It's unclear when `now()` or internally tracked time is used -- we should be more specific?
331357009,7378,mjsax,2019-10-04T06:33:10Z,`with specified timestamp` -> sounds is if there would not be anything else specified. Simply to `Create a new record.`?
331357212,7378,mjsax,2019-10-04T06:34:07Z,"nit: `timestampMs` ?

nit: `since [the beginning of the] epoch` ?

nit: `{@code null`}"
331357301,7378,mjsax,2019-10-04T06:34:32Z,as above: explain when which case is used?
331357761,7378,mjsax,2019-10-04T06:36:36Z,nit: add braces
331357774,7378,mjsax,2019-10-04T06:36:40Z,nit: add braces
331358003,7378,mjsax,2019-10-04T06:37:37Z,Similar comments as above -- also applies to other constructors
331358093,7378,mjsax,2019-10-04T06:38:00Z,`with {@code null} key`
331358460,7378,mjsax,2019-10-04T06:39:30Z,Should we add: `Objects.requireNonNull(record)`?
331358507,7378,mjsax,2019-10-04T06:39:43Z,as above
331358629,7378,mjsax,2019-10-04T06:40:15Z,nit: `Create a {@code TestRecord} from a {@link ConsumerRecord}.`
331358675,7378,mjsax,2019-10-04T06:40:26Z,as above.
331359572,7378,mjsax,2019-10-04T06:44:10Z,"nit: `{@link #createInputTopic(String, Serializer, Serializer) create}` (make `create` the link directly)"
331359756,7378,mjsax,2019-10-04T06:44:55Z,"`and use the` -> `and use a`

`to supply input records` (plural)"
331359883,7378,mjsax,2019-10-04T06:45:33Z,"as above -> make `create` the link

`and use a`"
331359975,7378,mjsax,2019-10-04T06:45:52Z,`any output records of the topology`
331360311,7378,mjsax,2019-10-04T06:47:19Z,`outputTopic2`
331361415,7378,mjsax,2019-10-04T06:52:06Z,"if `key == null`, size should be `ConsumerRecord.NULL_SIZE` (same for value)"
331361500,7378,mjsax,2019-10-04T06:52:31Z,as above
331362053,7378,mjsax,2019-10-04T06:54:47Z,Seems we encoded the size incorrectly...
331364079,7378,mjsax,2019-10-04T07:02:53Z,nit: add braces (preferred for all blocks)
331364419,7378,mjsax,2019-10-04T07:04:17Z,remove
331365084,7378,mjsax,2019-10-04T07:06:48Z,"if `time == null && record.timestamp() == null` we pass `timestamp==0`; is this intended? Sounds like an error case to me (should we throw an exception, or can this never happen anyway?)"
331365575,7378,mjsax,2019-10-04T07:08:41Z,Why do we handle this case differently?
331366050,7378,mjsax,2019-10-04T07:10:41Z,"I know the context of the KIP, but I think it's hard to understand for users what this means.

`This method can be used if the result is considered a stream. If the result is considered a table, the list will contain all updated, ie, a key might be contained multiple times. If you are only interested in the last table update (ie, the final table state), you can use {@link #readKeyValuesToMap()} instead.`"
331366098,7378,mjsax,2019-10-04T07:10:51Z,`Map` ?
331366171,7378,mjsax,2019-10-04T07:11:05Z,As above
331367846,7378,mjsax,2019-10-04T07:17:13Z,"We should point out in the JavaDocs, that null-values don't have delete semantics! Ie, if the last update to a key is a delete/tombstone, the key will still be in the map (with null-value).

Also, I think we should not allow `null` keys, but throw an exception for this case."
331368233,7378,mjsax,2019-10-04T07:18:39Z,`keyDeserializer.getClass().getSimpleName()` (same for value)
331368853,7378,mjsax,2019-10-04T07:20:50Z,we can omit javadoc for non-public methods
331368920,7378,mjsax,2019-10-04T07:21:02Z,we can omit javadoc for non-public methods
331368970,7378,mjsax,2019-10-04T07:21:14Z,we can omit javadoc for non-public methods
331369932,7378,mjsax,2019-10-04T07:24:27Z,use `final IllegalArgumentException exception = assertThrows(...)` instead of try-catch
331554734,7378,jukkakarvanen,2019-10-04T15:16:23Z,Renamed
331556004,7378,jukkakarvanen,2019-10-04T15:19:02Z,"Reason for those null parameter in TestRecord contructor is limited variation of constructors with long. It would not require header parameter (null) if Instant would be used, which is the prefered way for the future.
"
331557898,7378,jukkakarvanen,2019-10-04T15:23:07Z,renamed
331559215,7378,jukkakarvanen,2019-10-04T15:25:51Z,fixed
331559316,7378,jukkakarvanen,2019-10-04T15:26:05Z,fixed
331560205,7378,jukkakarvanen,2019-10-04T15:28:05Z,fixed and order aligned
331561533,7378,jukkakarvanen,2019-10-04T15:31:12Z,Changed
331562321,7378,jukkakarvanen,2019-10-04T15:33:02Z,Changed
331562657,7378,jukkakarvanen,2019-10-04T15:33:48Z,changed
331568443,7378,jukkakarvanen,2019-10-04T15:47:06Z,Done
331568499,7378,jukkakarvanen,2019-10-04T15:47:14Z,done
331568797,7378,jukkakarvanen,2019-10-04T15:47:54Z,fixed
331570268,7378,jukkakarvanen,2019-10-04T15:51:22Z,this is removed
331570450,7378,jukkakarvanen,2019-10-04T15:51:47Z,Removed contructor and JavaDoc
331571591,7378,jukkakarvanen,2019-10-04T15:54:24Z,Changed
331572126,7378,jukkakarvanen,2019-10-04T15:55:35Z,Updated
331573167,7378,jukkakarvanen,2019-10-04T15:57:47Z,Replaced all messages
331573595,7378,jukkakarvanen,2019-10-04T15:58:46Z,fixed
331578731,7378,jukkakarvanen,2019-10-04T16:11:15Z,changed
331578775,7378,jukkakarvanen,2019-10-04T16:11:21Z,removed
331578924,7378,jukkakarvanen,2019-10-04T16:11:44Z,removed
331579069,7378,jukkakarvanen,2019-10-04T16:12:06Z,removed
331580264,7378,jukkakarvanen,2019-10-04T16:14:56Z,changed
331582622,7378,jukkakarvanen,2019-10-04T16:21:05Z,Added
331585568,7378,jukkakarvanen,2019-10-04T16:28:45Z,Fixed
331585636,7378,jukkakarvanen,2019-10-04T16:28:58Z,Clarified
331585726,7378,jukkakarvanen,2019-10-04T16:29:12Z,changed
331587844,7378,jukkakarvanen,2019-10-04T16:34:56Z,"To my understanding null can happen either topic does not exist or no input piped to that topic. 
This is to able to Throw error if topic does not exist at all."
331590978,7378,jukkakarvanen,2019-10-04T16:43:40Z,I needed to modify like that to get some old test to work. It might be some non valid test.
331591058,7378,jukkakarvanen,2019-10-04T16:43:55Z,removed
331591666,7378,jukkakarvanen,2019-10-04T16:45:28Z,added
331592830,7378,jukkakarvanen,2019-10-04T16:48:43Z,Fixed
331592861,7378,jukkakarvanen,2019-10-04T16:48:50Z,Fixed
331593078,7378,jukkakarvanen,2019-10-04T16:49:29Z,"@mjsax, What this means, is actions needed from my side"
331593792,7378,jukkakarvanen,2019-10-04T16:51:26Z,Changed record2 to read from outputTopic2
331594180,7378,jukkakarvanen,2019-10-04T16:52:29Z,changed
331596166,7378,jukkakarvanen,2019-10-04T16:57:43Z,done
331596244,7378,jukkakarvanen,2019-10-04T16:57:54Z,done
331596343,7378,jukkakarvanen,2019-10-04T16:58:10Z,done
331596809,7378,jukkakarvanen,2019-10-04T16:59:24Z,done
331596839,7378,jukkakarvanen,2019-10-04T16:59:29Z,done
331598905,7378,jukkakarvanen,2019-10-04T17:05:12Z,added
331598928,7378,jukkakarvanen,2019-10-04T17:05:16Z,added
331599147,7378,jukkakarvanen,2019-10-04T17:05:53Z,replaced
331600525,7378,jukkakarvanen,2019-10-04T17:09:46Z,changed
331600579,7378,jukkakarvanen,2019-10-04T17:09:54Z,done
331603570,7378,jukkakarvanen,2019-10-04T17:17:58Z,replaced in all places
331604119,7378,jukkakarvanen,2019-10-04T17:19:22Z,"I removed time generator logic text, it is functionality of TestInputLogic, not needed here."
331604682,7378,jukkakarvanen,2019-10-04T17:20:59Z,done
331604752,7378,jukkakarvanen,2019-10-04T17:21:08Z,done
331604866,7378,jukkakarvanen,2019-10-04T17:21:25Z,same as above
331604966,7378,jukkakarvanen,2019-10-04T17:21:43Z,same as above
331605927,7378,jukkakarvanen,2019-10-04T17:24:12Z,changed
331606011,7378,jukkakarvanen,2019-10-04T17:24:23Z,changed
331606681,7378,jukkakarvanen,2019-10-04T17:26:01Z,changed multiple occurences
331607233,7378,jukkakarvanen,2019-10-04T17:27:15Z,removed
331608046,7378,jukkakarvanen,2019-10-04T17:29:11Z,Replaced
331608817,7378,jukkakarvanen,2019-10-04T17:31:05Z,changed
331610034,7378,jukkakarvanen,2019-10-04T17:34:27Z,done
331610133,7378,jukkakarvanen,2019-10-04T17:34:42Z,done
331610298,7378,jukkakarvanen,2019-10-04T17:35:10Z,removed
331610965,7378,jukkakarvanen,2019-10-04T17:36:58Z,changed
331611197,7378,jukkakarvanen,2019-10-04T17:37:26Z,removed
331611349,7378,jukkakarvanen,2019-10-04T17:37:50Z,done
331612150,7378,jukkakarvanen,2019-10-04T17:39:42Z,changed
331612202,7378,jukkakarvanen,2019-10-04T17:39:49Z,changed
331612485,7378,jukkakarvanen,2019-10-04T17:40:27Z,replaced
331613185,7378,jukkakarvanen,2019-10-04T17:42:15Z,instance as in InputTopic
331613689,7378,jukkakarvanen,2019-10-04T17:43:33Z,done
331613912,7378,jukkakarvanen,2019-10-04T17:44:03Z,done
331614099,7378,jukkakarvanen,2019-10-04T17:44:31Z,done
331615408,7378,jukkakarvanen,2019-10-04T17:47:52Z,clarified
331615881,7378,jukkakarvanen,2019-10-04T17:49:09Z,done
331617799,7378,jukkakarvanen,2019-10-04T17:53:54Z,added
331621072,7378,mjsax,2019-10-04T18:02:07Z,Ack.
331622341,7378,mjsax,2019-10-04T18:05:40Z,"No -- I just acknowledged that you basically move code that was already ""wrong"", so not your fault to use `0` instead of `Consumer.NULL_SIZE` :) -- no action needed."
331623203,7378,mjsax,2019-10-04T18:08:08Z,Ack.
331634638,7378,mjsax,2019-10-04T18:36:08Z,This must be `OUTPUT_TOPIC_2`
331638698,7378,jukkakarvanen,2019-10-04T18:46:45Z,I don't understand the original test. Why it is checking OUTPUT_TOPIC_2 if it is not in Topology at all.
331643505,7378,jukkakarvanen,2019-10-04T18:59:40Z,I pushed the version with outputTopic2 removed
331648099,7378,mjsax,2019-10-04T19:12:52Z,"Well. Overwriting the timestamp is fine, however, if we don't pass in `final Instant time`, we should just pass `record.timestamp()` into `pipeRecord()` -- I checked out the code applied the following change an run test (and they passed---hence, I think it fine to be more strict)

```
long timestamp;
if (time != null) {
    timestamp = time.toEpochMilli();
} else if (record.timestamp() != null) {
    timestamp = record.timestamp();
} else {
    throw new IllegalStateException(""Provided `TestRecord` does not have a timestamp and no timestamp overwrite was provided via `time` parameter."");
}
```"
331654063,7378,mjsax,2019-10-04T19:30:36Z,"The original intent of the test was to ensure, we don't write into non-exiting topics, ie, create a topic out of nowhere -- but with the new abstraction that cannot happen anyway I guess."
331659853,7378,jukkakarvanen,2019-10-04T19:47:53Z,Some tests failing if making change like this.
221138500,5709,bbejeck,2018-09-28T04:25:41Z,Added for access to contents of `Grouped`. I added this class to follow the pattern we currently use for configuration classes.
221138726,5709,bbejeck,2018-09-28T04:28:29Z,I did this to be consistent  with `SessionWindowedKStreamImpl`
221138924,5709,bbejeck,2018-09-28T04:30:40Z,Needed to change the topology as we now use the topic name of the first merged/replaced repartition topic when performing repartition topic optimization.
221138954,5709,bbejeck,2018-09-28T04:31:00Z,same as above
221139121,5709,bbejeck,2018-09-28T04:32:38Z,Same here and below needed to update the expected topology as now we use the topic name of the **_first_** merged repartition topic for the new optimized repartition topic.
221139255,5709,bbejeck,2018-09-28T04:34:12Z,"Most of the LOC in this class are boilerplate to create the topologies for the tests, and the expected optimized and non-optimized topologies."
221141350,5709,bbejeck,2018-09-28T04:58:47Z,Required to keep the merge nodes in the same order as they are added to the graph.  They may be used later when performing an optimization if the merged node represents key-changing operations up-stream.
221141409,5709,bbejeck,2018-09-28T04:59:37Z,Keep `OptimizableRepartitonNodes` in the same order as they are added.
221141573,5709,bbejeck,2018-09-28T05:01:13Z,Now grab the topic name from the first existing repartition topic that will get merged/replaced. 
221142005,5709,bbejeck,2018-09-28T05:05:25Z,Keep the key-changing parent nodes for the merge node in order.
221142221,5709,bbejeck,2018-09-28T05:07:11Z,Keep the `OptimizableRepartitionNode`s in order as they are mapped to merge nodes
221284385,5709,vvcephei,2018-09-28T15:03:18Z,"maybe a nit: you could alternatively return `return new Grouped<>(name, keySerde, valueSerde);` from this and the following methods and then make the three variables `final`.

Personally, I'd feel more comfortable making them `protected` (for `GroupedInternal`'s access) if they were final."
221284484,5709,vvcephei,2018-09-28T15:03:37Z,"I assume this is in reference to https://issues.apache.org/jira/browse/KAFKA-7435.
Just jotting down some thoughts: if this were iface/impl, all the state would be private, and there'd be no need for the `protected` copy constructor.

But if we decide we like KAFKA-7435, we'd apply it to all the config objects, and converting this one would be low incremental cost.

I'm :+1: for defaulting to the common pattern."
221287987,5709,vvcephei,2018-09-28T15:13:45Z,nit: maybe just call this `name` for consistency?
221288250,5709,vvcephei,2018-09-28T15:14:23Z,nit: formatting
221288991,5709,vvcephei,2018-09-28T15:16:34Z,"hmm. I failed to notice this in the kip...
Should this be `named` like `Grouped.named`?

Seems like a minor kip update that would be ok at this point, but valuable to establish consistency now."
221289641,5709,vvcephei,2018-09-28T15:18:32Z,"nit: maybe just call the constructor. The other static method doesn't apply any defaults, so there's no benefit to the chained method call."
221291992,5709,vvcephei,2018-09-28T15:25:33Z,"""xxx"" could also be configured via `Grouped`, right?"
221293611,5709,vvcephei,2018-09-28T15:30:39Z,"Did you mean to also insert a `<p>` here, so the html would be formatted the same as this javadoc?"
221294337,5709,vvcephei,2018-09-28T15:32:58Z,"maybe we can also add note to the javadoc, like `@deprecated since 2.1. Use {@link groupByKey(Grouped)} instead.` ?"
221294537,5709,vvcephei,2018-09-28T15:33:25Z,ditto on the deprecation notice in the javadoc.
221295115,5709,vvcephei,2018-09-28T15:35:12Z,"I didn't follow:
> (which should be of the same type)

It seems like there's no restriction that `KR` is the same as `K`. Or did you mean something else?"
221295243,5709,vvcephei,2018-09-28T15:35:40Z,Similar question re: `<p>`
221295702,5709,vvcephei,2018-09-28T15:36:53Z,"similar question re: ""xxx"" and `Grouped.named`"
221296087,5709,vvcephei,2018-09-28T15:38:09Z,nit: missing the `@param` for `grouped`
221296127,5709,vvcephei,2018-09-28T15:38:16Z,nit: missing the `@param` for `grouped`
221296539,5709,vvcephei,2018-09-28T15:39:41Z,"This one should be `@Deprecated`, right?"
221296735,5709,vvcephei,2018-09-28T15:40:24Z,similar comment re: `<p>`
221296798,5709,vvcephei,2018-09-28T15:40:37Z,super nit: alignment ;)
221297424,5709,vvcephei,2018-09-28T15:42:48Z,"Similar question about:
> (with should both have unmodified type)"
221299721,5709,vvcephei,2018-09-28T15:50:08Z,"similar questoin re ""xxx"""
221299939,5709,vvcephei,2018-09-28T15:50:52Z,similar question re: `@deprecated` javadoc
221301291,5709,vvcephei,2018-09-28T15:55:07Z,"Sounds good.

If this is an important semantic property of `mergeNodes`, perhaps we should declare the variable as a `LinkedHashSet` as well?"
221301788,5709,vvcephei,2018-09-28T15:56:52Z,"Similar to my question about `mergeNodes`, should we go ahead and declare `keyChangingOperationsToOptimizableRepartitionNodes` as a `LinkedHashMap<StreamsGraphNode, LinkedHashSet<OptimizableRepartitionNode>>` to document that the insertion order is preserved at both levels?"
221303796,5709,vvcephei,2018-09-28T16:03:59Z,ditto
221303825,5709,vvcephei,2018-09-28T16:04:07Z,ditto
221304091,5709,vvcephei,2018-09-28T16:05:09Z,"Would you be ok with renaming ""name"" to ""nodeName"" to disambiguate?"
221304694,5709,vvcephei,2018-09-28T16:07:18Z,Did we want to give priority to the explicitly named repartition topics?
221304981,5709,vvcephei,2018-09-28T16:08:16Z,nit: alignment
221305467,5709,vvcephei,2018-09-28T16:09:52Z,"nit: Since the `GroupedStreamAggregateBuilder` needs to know everything that's in `GroupedInternal`, maybe we can just pass the whole config object in to cut down on the param list?"
221305778,5709,vvcephei,2018-09-28T16:10:58Z,"Thanks! I just noticed this yesterday, and it did trip me up a little."
221307699,5709,vvcephei,2018-09-28T16:18:09Z,What's the code path that leads to `repartitionTopicBaseName.endsWith(REPARTITION_TOPIC_SUFFIX)`? I couldn't find it.
221309541,5709,vvcephei,2018-09-28T16:24:52Z,"I think if you mark this method as `@Deprecated` as well, it will also suppress the warnings, which might be better because it preserves the deprecation notice from the interface."
221309956,5709,vvcephei,2018-09-28T16:26:13Z,ditto
221310171,5709,vvcephei,2018-09-28T16:27:05Z,ditto to `KStreamImpl`
221310476,5709,vvcephei,2018-09-28T16:28:07Z,Ditto to `KStreamImpl`: it might be better to mark this class `@Deprecated`
221310586,5709,vvcephei,2018-09-28T16:28:31Z,Thank you!
221314587,5709,vvcephei,2018-09-28T16:44:00Z,Should this be `shouldKeepRepartitionTopicNameForGroupByKeyNoWindows`?
221353167,5709,bbejeck,2018-09-28T19:06:35Z,ack
221356038,5709,bbejeck,2018-09-28T19:18:35Z,"I can but this change isn't specific to the semantics of merge nodes themselves, it's more about bookkeeping and keeping them in the same order as they are added when building the graph.

Since this is private variables I'll make the change.

"
221356652,5709,bbejeck,2018-09-28T19:21:19Z,"ack, same as above"
221356700,5709,bbejeck,2018-09-28T19:21:33Z,"ack, same as above"
221356733,5709,bbejeck,2018-09-28T19:21:46Z,"ack, same as above"
221358640,5709,bbejeck,2018-09-28T19:29:32Z,ack
221361859,5709,bbejeck,2018-09-28T19:43:47Z,ack
221361903,5709,bbejeck,2018-09-28T19:43:55Z,ack
221369563,5709,bbejeck,2018-09-28T20:15:44Z,"ack, copy-paste error"
221369692,5709,bbejeck,2018-09-28T20:16:16Z,I'll add but I believe that was pre-existing
221369921,5709,bbejeck,2018-09-28T20:17:13Z,ack
221370454,5709,bbejeck,2018-09-28T20:19:29Z,ack
221371198,5709,bbejeck,2018-09-28T20:22:39Z,"pre-existing, fixed"
221371933,5709,bbejeck,2018-09-28T20:25:25Z,"pre-existing, you'll notice the other java doc has the same thing, I'll update though"
221373333,5709,bbejeck,2018-09-28T20:30:45Z,fixed
221378081,5709,bbejeck,2018-09-28T20:51:02Z,ack
221378533,5709,bbejeck,2018-09-28T20:52:52Z,ack
221379140,5709,bbejeck,2018-09-28T20:55:35Z,ack
221379486,5709,bbejeck,2018-09-28T20:57:04Z,ack
221379642,5709,bbejeck,2018-09-28T20:57:43Z,ack
221380525,5709,bbejeck,2018-09-28T21:01:54Z,"legacy comments, fixed"
221380620,5709,bbejeck,2018-09-28T21:02:14Z,ack
221380780,5709,bbejeck,2018-09-28T21:03:07Z,ack
221382112,5709,bbejeck,2018-09-28T21:09:24Z,ack
221383257,5709,bbejeck,2018-09-28T21:15:18Z,"I had the same thought, but so far we've only discussed grabbing the first repartition topic name. 
 I'm inclined to leave as is because 1) users don't care about the name as much as it doesn't change and break the topology and 2) IMHO will add some complexity without a significant benefit"
221383379,5709,bbejeck,2018-09-28T21:15:47Z,"I had the same thought, I think this just slipped."
221386486,5709,bbejeck,2018-09-28T21:32:45Z,"It comes from the `repartitionTopicNamePrefix` passed as a parameter when calling `createRepartitionedSource`.

Since we may get an existing repartition topic name, we don't want to append `-repartition` at the end and change it.  

For example, we may get `KSTREAM-AGGREGATE-STATE-STORE-0000000005-repartition` for a repartition topic name resulting from an optimization operation and appending another `-repartition` would break the topology.  

As the java docs and our other docs state, we append as single `-repartition` to the repartition topic name we wouldn't want to double append.  But I'll update this with a comment explaining why we do this check."
221386734,5709,bbejeck,2018-09-28T21:33:53Z,ack
221386797,5709,bbejeck,2018-09-28T21:34:13Z,ack
221387336,5709,bbejeck,2018-09-28T21:36:49Z,ack
221387515,5709,bbejeck,2018-09-28T21:37:46Z,"Yeah, especially as `Serialized` is deprecated, good catch!"
221388695,5709,bbejeck,2018-09-28T21:44:18Z,good catch actually `shouldKeepRepartitionTopicNameForGroupByNoWindows`
221431415,5709,vvcephei,2018-09-29T15:28:25Z,"Ack.

It's probably also likely that the user who names some repartition topics names them all; another reason the extra complexity wouldn't buy anything."
221431492,5709,vvcephei,2018-09-29T15:31:49Z,I agree we shouldn't append multiple suffixes. It just wasn't clear where the pre-suffixed string could come from. Thanks for the clarification!
221442359,5709,mjsax,2018-09-29T23:12:46Z,"This is ok, because not part of 2.0, right?"
221442399,5709,mjsax,2018-09-29T23:15:37Z,"missing `, or ` and missing space before `operations`"
221442444,5709,mjsax,2018-09-29T23:18:45Z,"`set the name` -- the specified name is part of the topic name -- think we should be more precise (note: I would not describe the used naming pattern in JavaDocs though)

Similar below in method JavaDocs."
221442610,5709,mjsax,2018-09-29T23:30:15Z,nit: missing `.` at the end
221442612,5709,mjsax,2018-09-29T23:30:21Z,nit: missing `.` at the end
221442618,5709,mjsax,2018-09-29T23:30:47Z,"nit: missing `.` at the end

Oxford comma?

nit: `{@code name}, {@code keySerde}, and {@code valueSerde}.` (similar below)"
221442626,5709,mjsax,2018-09-29T23:31:33Z,"nit: `{@code Grouped}`

Similar below."
221442659,5709,mjsax,2018-09-29T23:34:34Z,"Agreed with John. We should keep immutability. Existing code also creates new objects. (similar below)

Also update JavaDocs `return this` above."
221442678,5709,mjsax,2018-09-29T23:36:03Z,nit: fix indention
221442709,5709,mjsax,2018-09-29T23:38:20Z,return `new Joined(...)`
221442733,5709,mjsax,2018-09-29T23:40:00Z,nit: missing space
221442771,5709,mjsax,2018-09-29T23:43:03Z,nit: update `XXX` to `<name>` (or `&lt;name&gt;` to be more precise)
221442809,5709,mjsax,2018-09-29T23:46:11Z,"`the name for a repartition topic`: it's not the name, but part of the name only"
221442824,5709,mjsax,2018-09-29T23:47:42Z,"nice catch! (Can you double check other JavaDocs, too. Could be c&p error.)"
221442832,5709,mjsax,2018-09-29T23:48:24Z,nit: missing space
221442841,5709,mjsax,2018-09-29T23:49:16Z,nit: missing space
221442855,5709,mjsax,2018-09-29T23:50:04Z,nit: remove one space before `and the name...`
221442858,5709,mjsax,2018-09-29T23:50:29Z,remove `<`
221442872,5709,mjsax,2018-09-29T23:51:51Z,`maybe being` sounds a little odd to me... (not a native speaker though)
221442895,5709,mjsax,2018-09-29T23:54:38Z,nit: why does left hand side needs to specify classed instead of interface?
221442905,5709,mjsax,2018-09-29T23:55:17Z,Why is `Set` not sufficient? 
221442912,5709,mjsax,2018-09-29T23:55:43Z,Why not `Set` left hand side?
221442930,5709,bbejeck,2018-09-29T23:57:12Z,"Yes, that's correct. I checked out the `2.0` branch to confirm."
221442957,5709,mjsax,2018-09-29T23:59:41Z,"nit: this is the user specified name, that is part of the repartition topic name -- we should rename this"
221442960,5709,mjsax,2018-09-30T00:00:39Z,nit: the use specified name is part of the repartition topic only -- we should rename this to `name` or `userName`?
221443004,5709,mjsax,2018-09-30T00:04:03Z,Could we set `name` instead of `null` here and simplify other code (cf. my comments below) ?
221443009,5709,mjsax,2018-09-30T00:04:28Z,remove `name` parameter ? (cf. comment above)
221443020,5709,mjsax,2018-09-30T00:05:18Z,I think we can remove `repartitionTopicBaseName` entirely (cf. my comments from above)
221443050,5709,mjsax,2018-09-30T00:07:57Z,"Cannot follow here... Do you aim for existing topologies with generated names, and user update code to ""pin"" names? For this case, user would pass it name, without `-repartition` suffix? User, would also need to drop `<application.id>` prefix in the name she passed to `Grouped`."
221443054,5709,mjsax,2018-09-30T00:08:40Z,nit: remove empty lines
221443072,5709,mjsax,2018-09-30T00:09:37Z,"Do we need this annotation again? Though we would need a `@SuppressWarning(""deprecation"")` here instead?"
221443096,5709,mjsax,2018-09-30T00:10:07Z,nit: 4 space indention only
221443107,5709,mjsax,2018-09-30T00:10:22Z,as above
221443114,5709,mjsax,2018-09-30T00:10:54Z,nit: 4-space indention plus move `builder` down one line
221443122,5709,mjsax,2018-09-30T00:11:19Z,as above
221443126,5709,mjsax,2018-09-30T00:11:37Z,nit: 4-space indention
221443137,5709,mjsax,2018-09-30T00:12:20Z,"No need to deprecate an internal class IMHO. Maybe add `@SuppressWarning(""deprecation"")` instead?"
221443198,5709,mjsax,2018-09-30T00:16:18Z,"Should this not fail here in `groupByKey()` already?

Maybe use `try-fail-catch` pattern here."
221443199,5709,mjsax,2018-09-30T00:16:39Z,nit: missing `t`
221443236,5709,mjsax,2018-09-30T00:20:43Z,"Not sure why this should not be allowed? To be more precise: I understand why the code fails, however, it's very unintuitive for the user why this would fail -- looks like valid code and IMHO, users should be allowed to write this code: Both operations can reuse the same repartition topic anyway (and with optimization turned on, they will, if I don't miss anything).

Not sure if we can fix this easily to be honest, but accepting this as ""by design"" would not be user friendly. Maybe we can merge both repartition topics into one for this case, too?"
221461753,5709,vvcephei,2018-09-30T14:39:15Z,"I suggested this.

While it is normally better to use an interface on the LHS, it should only be done if the interface provides the correct semantics. I.e., you should be able to swap out any two implementations of the interface and maintain correct behavior.

Normally, when we work with Maps or Sets, we do indeed need just the semantics they promise (i.e., a k/v mapping, or the set property), and we could in theory use any implementation without changing the correctness of the program.

But in this case, it seemed like the correct behavior of this class depends on maintaining these collections in insertion order. Unfortunately, Java does not have an interface for an ordered Map or Set. Therefore, the most general ""interface"" that provides the correct semantics is actually just the implicit interface of LinkedHashMap/Set itself."
221461770,5709,vvcephei,2018-09-30T14:39:48Z,This is also my fault... see https://github.com/apache/kafka/pull/5709#discussion_r221461753
221461780,5709,vvcephei,2018-09-30T14:40:14Z,This is also my fault... see https://github.com/apache/kafka/pull/5709#discussion_r221461753
221462040,5709,vvcephei,2018-09-30T14:51:38Z,"IMHO, it's better to pass along the deprecation instead of suppressing it.

They both cause the compiler not to issue warnings about the use of deprecated APIs in the method body.

This difference is that if we suppress it here, then any `groupBy` calls on a `KStreamImpl` reference *will not* issue a warning, whereas calls on a `KStream` reference will issue the warning as desired."
221462114,5709,vvcephei,2018-09-30T14:54:50Z,This is the same thinking as https://github.com/apache/kafka/pull/5709#discussion_r221462040 . 
221470735,5709,guozhangwang,2018-09-30T19:41:53Z,"Also we should indicate that it is for setting the repartition topic ""if necessary: Streams will not always create the repartition topic for grouped operation""."
221470841,5709,guozhangwang,2018-09-30T19:44:56Z,This just occurred to me that `Grouped.named()` is a bit weird when writing it down. Could we rename it to `Grouped.as()` or `Grouped.for`? WDYT @mjsax @vvcephei 
221471094,5709,guozhangwang,2018-09-30T19:51:00Z,"`will be` -> `may need to be created in Kafka if a later operator depends on the newly selected key.`

Ditto elsewhere."
221471436,5709,guozhangwang,2018-09-30T20:00:14Z,"`repartitionTopicName` and `repartitionTopic` is a bit confusing. I'd suggest just keeping the `GroupedInternal` as a field to replace key/valueSerde and `repartitionTopicName` in the constructor and retrieve its fields later.

Ditto for other internal class's constructors (you already replaced serdes with the object in some classes, just trying to suggest consistency here)."
221471570,5709,guozhangwang,2018-09-30T20:03:37Z,"I cannot follow here too.. the `createRepartitionedSource` should always be called before the optimization kicks in, so the passed in name should always be the raw names right?"
221471689,5709,guozhangwang,2018-09-30T20:07:31Z,+1
221471765,5709,guozhangwang,2018-09-30T20:09:21Z,Not clear what is this test used for?
221471837,5709,guozhangwang,2018-09-30T20:11:46Z,"I think the reason is that we do not check for unique names at Grouped / Joined, and hence only when later when the repartition topics are indeed going to be created the exception will be thrown. This looks fine to me."
221471891,5709,guozhangwang,2018-09-30T20:14:32Z,"Seems for Joined we do not have a test to check for naming uniqueness yet, could we add one?"
221476261,5709,mjsax,2018-09-30T22:37:37Z,"I don't think that suppress works for any callers of `KStreamImpl#groupBy` -- from my understanding, there will be a warning for all callers independently of a suppress annotation -- callers would need to add their own annotation to suppress the warning for them. A `SuppressWarning` only suppressed warning from the body/implementation of this method (ie, if we would call any other deprecated method).

I also don't think we need `@Deprecated` as this annotation is inherited anyway.

However, this is an internal class anyway, and thus, not public. Thus, I don't have a strong opinion on this."
221476305,5709,mjsax,2018-09-30T22:39:26Z,Ack. Thanks for clarification.
221476351,5709,mjsax,2018-09-30T22:41:33Z,That's a good point. `Grouped.as()` sounds ok (`Grouped.for()` sounds weird to me though).
221476723,5709,bbejeck,2018-09-30T22:56:31Z,ack
221477923,5709,bbejeck,2018-09-30T23:41:23Z,ack
221477926,5709,bbejeck,2018-09-30T23:41:26Z,"Ack, Changed the suggested order a bit, but IMHO the message is the same."
221477927,5709,bbejeck,2018-09-30T23:41:28Z,ack
221477928,5709,bbejeck,2018-09-30T23:41:30Z,ack
221477929,5709,bbejeck,2018-09-30T23:41:32Z,ack
221477932,5709,bbejeck,2018-09-30T23:41:35Z,ack
221477934,5709,bbejeck,2018-09-30T23:41:37Z,ack
221477937,5709,bbejeck,2018-09-30T23:41:40Z,"ack, updated other methods as well."
221477940,5709,bbejeck,2018-09-30T23:41:43Z,ack
221477941,5709,bbejeck,2018-09-30T23:41:46Z,ack
221477959,5709,bbejeck,2018-09-30T23:41:52Z,ack
221477965,5709,bbejeck,2018-09-30T23:41:55Z,ack
221477967,5709,bbejeck,2018-09-30T23:41:57Z,ack
221477968,5709,bbejeck,2018-09-30T23:42:01Z,ack
221477969,5709,bbejeck,2018-09-30T23:42:03Z,ack
221477999,5709,bbejeck,2018-09-30T23:43:22Z,"Yeah I agree, updated."
221478087,5709,bbejeck,2018-09-30T23:46:58Z,"Additionally, I would in most circumstances agree with specifying the interface, but since these are private variables on an internal class, there is no ""leaking"" of an implementation."
221478133,5709,bbejeck,2018-09-30T23:49:12Z,ack
221478190,5709,bbejeck,2018-09-30T23:50:40Z,ack
221482476,5709,bbejeck,2018-10-01T01:40:31Z,ack
221482479,5709,bbejeck,2018-10-01T01:40:35Z,ack
221483107,5709,bbejeck,2018-10-01T01:51:12Z,ack already done
221483292,5709,bbejeck,2018-10-01T01:54:33Z,ack
221483725,5709,bbejeck,2018-10-01T02:02:28Z,ack
221484247,5709,bbejeck,2018-10-01T02:11:42Z,ack
221484591,5709,bbejeck,2018-10-01T02:17:43Z,ack
221485045,5709,bbejeck,2018-10-01T02:24:39Z,"As @guozhangwang pointed out, the error does not occur until we go to build the topology and the duplicate topic name is detected and at that point, the error is thrown.

I can update with the `try-fail-catch` pattern, but have we established this as a convention of unit tests vs using the `@Test(expected=...)` approach?"
221485076,5709,bbejeck,2018-10-01T02:25:06Z,ack
221490175,5709,bbejeck,2018-10-01T03:40:05Z,"Just asserting that the different repartition topic base names resulted in successfully building the topology, but this is covered from other tests, so I'll remove it"
221490839,5709,mjsax,2018-10-01T03:48:48Z,"I personally highly prefer the try-fail-catch pattern, because it allows to narrow down which operation throws the exception. The current test would pass if the same exception is thrown one line above. IMHO, the `expected` annotation should only be used, if no other part in the test code could potentially throw the same exception (what is rarely the case)."
221490975,5709,bbejeck,2018-10-01T03:50:34Z,ack
221491369,5709,bbejeck,2018-10-01T03:55:57Z,"ack, I believe I've cleaned this up."
221492650,5709,bbejeck,2018-10-01T04:14:00Z,"ack, `Grouped.as` is better, I'll update."
221496830,5709,bbejeck,2018-10-01T05:12:28Z,"I agree, but IMHO it's exposing some unintuitive behavior with respect to creating multiple repartition topics.  And yes with optimizations turned on users will be able to write code in this form. 

But as it stands now, by explicitly naming a repartition I don't see how we can re-use a single `KGroupedStream` instance, as before we relied on the auto-generated names to handle the creation of multiple repartition topics.  

>Maybe we can merge both repartition topics into one for this case, too?

I'm not sure I follow, do you mean in this case we do an automatic optimization and merge repartition topics ""in-line""?   If so, I'm inclined to say yes we can, but I'm thinking this may be done best in a follow-on PR.

WDYT?"
221509793,5709,mjsax,2018-10-01T07:08:07Z,"Something like this -- the idea would be to set a ""flag"" on the `KGroupedStream` (same for `KGroupedTable`?) after the first `count()/reduce()/aggregate()` is executed and to remember the created repartition topic. And for this case, consecutive `count()/reduce()/aggregate()` would skip creating a new changelog topic but reuse the already created one.

For backward compatibility, we would only do this if `Grouped.as` is specified. It might be a little bit hacky, but might be worth it...

Thoughts?"
221629926,5709,bbejeck,2018-10-01T14:29:31Z,"I like the idea; I'll try and implement that now

EDIT: Looking at this  I have some more thoughts.

Why limit to just when people name the repartition topic?  Since we have a graph now, we can keep a reference to the repartition graph node and at this point in the code always re-use this node for repartitioning.  But this could be tricky as this will still affect an existing topology. 

For example, consider a user with multiple `KGroupedStream` calls where a repartition is required.  While this means we have created multiple repartition topics, this also means that we have incremented the processor counter N times (N being the number of repartition topics).  If we adopt this approach, and the user names the repartition topic, and we reuse the first created repartition topic, we'll change the number of all downstream operations including changelog topics and any other repartition topics.  This ""skipping incrementing"" is similar to what happened when re-using a source topic for source `KTable` changelogs.

While I realize most users will probably name all repartition topics, by doing so, they'll have to ensure they name any changelog topics as well if we reuse the repartition topics in-line.  With the current optimization approach the numbering isn't affected, we move the nodes around.

Additionally,  I""m not sure how this will affect the current optimization approach (maybe change it, as I think if we keep repartition node references as we go we could have ""automatic"" partial merging ?)

I'm thinking this approach is could worth looking into, but as an immediate follow-on PR to this one as this requires some thought.

WDYT?



"
221679349,5709,mjsax,2018-10-01T16:47:28Z,"> Why limit to just when people name the repartition topic?

For backward compatibility.

For new topologies, we should not need to care, because I would assume that users turn on optimization."
221817992,5709,mjsax,2018-10-02T03:36:39Z,nit: `create [a] repartition topic` -- or `create repartition topic[s] for`
221818181,5709,mjsax,2018-10-02T03:38:51Z,nit `uses [as] part` ?
221818570,5709,mjsax,2018-10-02T03:42:41Z,`for` -> `as` ?
221818628,5709,mjsax,2018-10-02T03:43:25Z,`@{Grouped}` -> `{@link Grouped}` or `{@code Grouped}`
221818784,5709,mjsax,2018-10-02T03:45:18Z,`{@code Grouped}` and `{@link Grouped}` is mixed comparing different methods -- we should unify.
221819043,5709,mjsax,2018-10-02T03:48:20Z,"Comparing JavaDocs with `Joined`: there we point out that `null` is ok for `Serdes` and the usage from config `Serdes` -- we should do this here, too."
221819137,5709,mjsax,2018-10-02T03:49:40Z,"Comparing JavaDocs with `Joined`: there we point out that `null` is ok for `Serdes` and the usage from config `Serdes` -- we should do this here, too.

Also for `@param` docs -- check other methods, too, please."
221819376,5709,mjsax,2018-10-02T03:52:36Z,nit: `..` -> `.`
221819613,5709,mjsax,2018-10-02T03:55:02Z,`XXX` -> `<name>`
221820266,5709,mjsax,2018-10-02T04:02:10Z,nit: remove var `newJoined` (also not used for left-hand-side code)
221820519,5709,mjsax,2018-10-02T04:05:01Z,"Was this a bug, to pass in `null` as value Serde? Did Guozhang's PR introduce this?"
221820603,5709,mjsax,2018-10-02T04:06:08Z,Similar here?
221820654,5709,mjsax,2018-10-02T04:06:39Z,nit: remove `this.`
222098442,5709,bbejeck,2018-10-02T20:22:58Z,ack
222099260,5709,bbejeck,2018-10-02T20:25:29Z,ack
222099946,5709,bbejeck,2018-10-02T20:27:48Z,ack
222102316,5709,bbejeck,2018-10-02T20:35:30Z,ack
222103740,5709,bbejeck,2018-10-02T20:40:05Z,"ack, going with `{@link Grouped}`"
222110689,5709,bbejeck,2018-10-02T21:01:45Z,ack
222115262,5709,bbejeck,2018-10-02T21:16:56Z,ack
222115412,5709,bbejeck,2018-10-02T21:17:29Z,ack
222115625,5709,bbejeck,2018-10-02T21:18:15Z,ack
222117740,5709,bbejeck,2018-10-02T21:26:03Z,"We need this right now to work with generics as the `repartitionForJoin` signature is `<K, V> repartitionForJoin(final Joined<K, V, ?> ` but the right-hand side is `<K, V1>` and the left-hand side is `<K, V>`. 

I know it's a bit of a hack, but I think it's worth the trade-off for being able to pass a single `Joined` parameter, vs. all of the required components of `Joined`.  Having the single `Joined` parameter was introduced from the Serdes inheritance PR.  If you insist I can revert to what it was before."
222117869,5709,bbejeck,2018-10-02T21:26:26Z,Introduced by the Serdes inheritance PR
222117907,5709,bbejeck,2018-10-02T21:26:35Z,same as above
222118272,5709,bbejeck,2018-10-02T21:27:49Z,ack
222129874,5709,mjsax,2018-10-02T22:15:53Z,Ack. Makes sense.
51845565,812,xiaotao183,2016-02-04T09:11:53Z,"@rajinisivaram SASL_CALLBACK_HANDLER_CLASS must be part of `addClientSaslSupport` in order to take effect
"
52296579,812,rajinisivaram,2016-02-09T11:36:07Z,"@xiaotao183 Yes, of course. Thank you, will add the missing line.
"
54384085,812,ijuma,2016-02-29T09:11:37Z,"Don't we have to update this to have a separate `LoginManager` per mechanism?
"
54396485,812,rajinisivaram,2016-02-29T11:11:12Z,"@ijuma The implementation uses a single login context with multiple login modules to support multiple mechanisms. Since Login is associated with login context rather than login module, one LoginManager per login type is sufficient.
"
58483193,812,junrao,2016-04-05T03:44:57Z,"Could we just use configs.getString and avoid casting? There are a few other places like that.
"
58483211,812,junrao,2016-04-05T03:45:09Z,"In the server mode, should we even check hasKerberos since SaslConfigs.GSSAPI_MECHANISM is for the client?
"
58483235,812,junrao,2016-04-05T03:45:26Z,"It seems that we have an existing issue in the handling of case INITIAL. If we can't completely write all bytes of the sasl token, we have to rely on the next call of authenticate() to finish writing the remaining bytes. However, when the write completes, we will go to the initial state and try to send the token again. It seems that we should be transitioning to the INTERMEDIATE state after the write completes. The same issue seems to exist when transitioning from SEND_MECHANISM to RECEIVE_MECHANISM_RESPONSE, if we can't write all bytes in SaslMechanismRequest in one send call.
"
58483242,812,junrao,2016-04-05T03:45:35Z,"Should we always return the enabled mechanism list?
"
58483259,812,junrao,2016-04-05T03:45:51Z,"Could we handle an explicit exception due to the first packet not being a MechanismRequest? I was thinking that we can catch all exceptions from SaslMechanismRequest(ByteBuffer buffer) and convert that to a SchemaException.
"
58483268,812,junrao,2016-04-05T03:46:00Z,"Hmm, the client may not understand mechanismResponse if it doesn't send mechanismRequest in the first place.
"
58483270,812,junrao,2016-04-05T03:46:09Z,"Similar to SaslClientAuthenticator, it seems that we need to deal with the case that not all bytes in netOutBuffer can be sent in a single authenticate() call.
"
58483294,812,junrao,2016-04-05T03:46:39Z,"Could you update the security section of the documentation on the support of new mechanism, how to specify and plug in PlainLoginModule, and what it takes to enable multiple mechanisms on the broker side?
"
58537309,812,ijuma,2016-04-05T13:30:28Z,"This is just a `Map`, so we can't use `getString`. I wanted to change `configure` to take a `Config` type for this reason, but it would break API classes unfortunately.
"
58747063,812,rajinisivaram,2016-04-06T17:28:42Z,"@junrao You are right, server doesn't need to check the SASL mechanism. But it can disable Kerberos if GSSAPI is not included in `enabledMechanisms`. Have updated the check.
"
58747189,812,rajinisivaram,2016-04-06T17:29:29Z,"@junrao Thank you, I have fixed setting of SASL state.
"
58747357,812,rajinisivaram,2016-04-06T17:30:17Z,"@junrao Have updated to include enabled mechanisms in response for successful response.
"
58747425,812,rajinisivaram,2016-04-06T17:30:35Z,"@junrao Done.
"
58747950,812,rajinisivaram,2016-04-06T17:33:26Z,"@junrao Yes, I wasn't sure whether to send the response in this case. But I thought it would be useful to send it before the connection is closed since it may be useful if you are looking at the bytes returned for debugging purposes.
"
58747999,812,rajinisivaram,2016-04-06T17:33:41Z,"Done, same as before.
"
58748697,812,rajinisivaram,2016-04-06T17:38:14Z,"I have opened another JIRA (KAFKA-3517) to update the docs. Will submit a PR.
"
59741966,812,ijuma,2016-04-14T15:51:06Z,"`ApiKeys.forId` (which is called by `getRequest` and by ourselves) throws `IllegalArgumentException` if the api key is not within range. Should we be catching that and throwing a more informative error?
"
59842206,812,rajinisivaram,2016-04-15T08:30:30Z,"@ijuma Thank you for the review. Since the first GSSAPI token starting with 0x60 (when handshake request is omitted) can also be an invalid API key (unlikely since `RequestHeader.parse` will probably throw `SchemaException`, but still possible I suppose), I changed the code to revert to GSSAPI for `IllegalArgumentException` as well.
"
60461201,812,junrao,2016-04-20T18:21:17Z,"Could we add the place-holder for those two error codes in ErrorMapping?
"
60461220,812,junrao,2016-04-20T18:21:22Z,"Should we also add the IllegalSaslSate error code?
"
60461542,812,junrao,2016-04-20T18:23:15Z,"Should we include throws KafkaException in the signature of configure() and close()? It's uncaught exception anyway.
"
60461708,812,junrao,2016-04-20T18:24:18Z,"It seems that for both client and server, Login uses the same ClientCallbackHandler. The existing code works like that. Is that correct? If so, perhaps we should rename ClientCallbackHandler to sth more generic?
"
60461752,812,junrao,2016-04-20T18:24:33Z,"Hmm, it's a bit weird that we instantiate a ClientCallbackHandler here and also in DefaultLogin. Could we just create it once and reuse?
"
60461827,812,junrao,2016-04-20T18:24:56Z,"Hmm, should we do that? So for, we only guarantee old version of java client can talk to new version of server. But there is no guarantee that new version of java client can talk to old version of server. So, it seems simpler to always let the new client send SaslHandshakeRequest. This also makes it easier to add ApiVersionRequest in the future (KIP-35).
"
60461839,812,junrao,2016-04-20T18:25:02Z,"Would it be better to rename this to receiveResponseOrToken()?
"
60461878,812,junrao,2016-04-20T18:25:14Z,"Since we can receive both Sasl tokens or a response, perhaps we should rename serverToken to sth more general?
"
60461893,812,junrao,2016-04-20T18:25:19Z,"For consistency, should we rename ClientCallbackHandler to SaslClientCallbackHandler?
"
60461901,812,junrao,2016-04-20T18:25:24Z,"The comment seems obsolete.
"
60461930,812,junrao,2016-04-20T18:25:30Z,"Should we change INIT to sth like HANDSHAKE_REQUEST to match what's in the client?
"
60461969,812,junrao,2016-04-20T18:25:41Z,"Could we rename the above to plainSaslProducer and plainSaslConsumer to distinguish from plain text port?
"
60499829,812,rajinisivaram,2016-04-20T22:37:50Z,"Done.
"
60499881,812,rajinisivaram,2016-04-20T22:38:21Z,"Yes, added.
"
60499925,812,rajinisivaram,2016-04-20T22:38:42Z,"Removed exception from signature.
"
60500580,812,rajinisivaram,2016-04-20T22:43:47Z,"@junrao Yes, it was like that with Kerberos, and I imagine the class was reused to avoid code duplication. But actually I think it is better to use a different class for Login to make the logic clearer and more readable. I have added a different callback handler in DefaultLogin with just the callbacks for Login. There is some overlap with the client callback handler. Let me know what you think.
"
60500774,812,rajinisivaram,2016-04-20T22:45:22Z,"@junrao See note above.
"
60501307,812,rajinisivaram,2016-04-20T22:49:28Z,"@junrao We need this for rolling upgrade from 0.9.0.x to 0.10.0 when SASL is used for inter-broker communication. We can remove this in the release that follows (the next minor release perhaps), thus providing a non-disruptive upgrade path. Will that be ok?
"
60501331,812,rajinisivaram,2016-04-20T22:49:37Z,"Done.
"
60501345,812,rajinisivaram,2016-04-20T22:49:46Z,"Done.
"
60501397,812,rajinisivaram,2016-04-20T22:50:17Z,"Renamed and moved to top-level class, consistent with SaslServerCallbackHandler.
"
60501417,812,rajinisivaram,2016-04-20T22:50:28Z,"Removed comment.
"
60501435,812,rajinisivaram,2016-04-20T22:50:34Z,"Done.
"
60501449,812,rajinisivaram,2016-04-20T22:50:44Z,"Done.
"
60517713,812,junrao,2016-04-21T02:13:03Z,"Very good point. For backward compatibility, we can probably just guard that by inter.broker.protocol version. If the version is >= 0.10.0, we will use the new protocol. Otherwise, use the old one.
"
60558377,812,rajinisivaram,2016-04-21T10:25:56Z,"@junrao Thank you, that makes sense. I have updated the code and the KIP. Since the version comparison code is in `core`, to avoid duplicating too much logic in `clients`, I am checking for 0.9.0 rather than 0.10.0. Hope that is ok.
"
60589714,812,junrao,2016-04-21T14:27:50Z,"Hmm, we want to check inter.broker.protocol.version >= 0.10.0. This is easier if we can use the case object in core. Since we only need to use the old protocol when SaslClientAuthethicator is used at the broker side. Perhaps, we can check  inter.broker.protocol.version in the broker code and pass a flag into inter.broker.protocol.version. The places where we use SaslClientAuthethicator are in ReplicaFetcherThread, ControllerChannelManager, and KafkaServer (for controlled shutdown). When used in clients (producer/consumer), SaslClientAuthethicator will always use the new protocol.
"
60611214,812,rajinisivaram,2016-04-21T16:16:29Z,"@junrao Thank you for the review. I have moved the version check to `core`.
"
60831292,812,ijuma,2016-04-23T16:54:18Z,"I wonder if we should be adding server-only configs here, it doesn't seem like there is much benefit (although I understand that we may have done that for some configs in the past).
"
60831331,812,ijuma,2016-04-23T16:55:54Z,"Is it worth mentioning the following as a reference for mechanism names in a comment?

http://www.iana.org/assignments/sasl-mechanisms/sasl-mechanisms.xhtml
"
60831526,812,ijuma,2016-04-23T17:06:02Z,"Nevermind, we do actually need this because we use this property in `common` classes.
"
60831568,812,ijuma,2016-04-23T17:10:29Z,"Why do we need these as fields?
"
60831627,812,ijuma,2016-04-23T17:13:32Z,"Nit: is there a `the` missing between `supported` and `requested`?
"
60831641,812,ijuma,2016-04-23T17:14:15Z,"Nit: replace `in` with `given`?
"
60831687,812,ijuma,2016-04-23T17:17:28Z,"Nit: `the` missing before `mechanism`?
"
60831696,812,ijuma,2016-04-23T17:17:52Z,"This should be final.
"
60831745,812,ijuma,2016-04-23T17:20:01Z,"This should be final and the `ArrayList` should be assigned only after it's fully constructed (this ensures thread-safety).
"
60831757,812,ijuma,2016-04-23T17:20:46Z,"Nit: space missing before `:`
"
60831862,812,ijuma,2016-04-23T17:28:38Z,"Would this not be slightly better if we used `Errors.forCode` and then did a switch on the enum? Also, we should not compare to `0`, we should use `Errors.NONE`.
"
60831892,812,ijuma,2016-04-23T17:30:51Z,"This is the same code that is in `NetworkClient.correlate`. Maybe we can make that a static public method and reuse it.
"
60831968,812,ijuma,2016-04-23T17:34:26Z,"Can we please group final fields first and then the non final fields? It makes easier to understand what gets set during construction versus mutable fields.
"
60832031,812,ijuma,2016-04-23T17:36:29Z,"I think @junrao suggested that we should send the enabled mechanisms in the successful case, but I don't understand the purpose. This bloats the response for the common case without any benefit that I can see. Thoughts @junrao @rajinisivaram?
"
60832070,812,ijuma,2016-04-23T17:38:14Z,"Nit: for fields that are initialised during `configure`, I think it's better to keep them `null` until `configure` is called. It makes it easier to debug if something goes wrong.
"
60832315,812,ijuma,2016-04-23T17:54:52Z,"I'm wondering if this is really necessary. Could we instead add the sasl properties to the properties returned by this method via a utility method that added them only if necessary? It seems like we don't gain much by doing it this way and it adds one more parameter to a very large number of parameters already.
"
60832411,812,ijuma,2016-04-23T18:02:18Z,"Did you really mean to have different indenting between lines? I think it would be nicer if these 3 lines were at the same indentation. We can change `JaasSection.toString` if it's relying on the current behaviour.
"
60833139,812,ijuma,2016-04-23T18:50:31Z,"Maybe we don't need `Option` here. `Option` is useful when we want the behaviour of `None` to be different than the empty case.
"
60833170,812,ijuma,2016-04-23T18:52:29Z,"It's more readable if we write this as `map { case (user, password) => (s""user_$user"" -> password }` or something like that.
"
60833256,812,ijuma,2016-04-23T18:58:26Z,"You can do something like

``` scala
mechanisms.map {
  case ""GSSAPI"" =>...
  ...
  case mechanism => throw new...
}
```
"
60833263,812,ijuma,2016-04-23T18:59:29Z,"It's generally preferable to use `getOrElse` with an appropriate message for the case when one passes a `None` when a `Some` is expected.
"
60833297,812,ijuma,2016-04-23T19:02:24Z,"Hmm, maybe the way you did is better since subsequent lines are a continuation of previous lines.
"
60833355,812,ijuma,2016-04-23T19:07:30Z,"It seems that this is used for inter-broker communication. Shouldn't we be using the same pattern we used for `InterBrokerSecurityProtocol`?
"
60833773,812,ijuma,2016-04-23T19:39:34Z,"Not worth having an empty `return` annotation.
"
60833788,812,ijuma,2016-04-23T19:40:57Z,"I think it would be nicer if we had a separate `AbstractLogin` that `KerberosLogin` inherits from. Inheritance from concrete classes is good to avoid as it tends to be brittle.
"
60834053,812,ijuma,2016-04-23T19:56:47Z,"Because these tests take a while to run, would it make sense to only have `SaslMultiMechanismConsumerTest`?
"
60834178,812,ijuma,2016-04-23T20:05:49Z,"Do we need some negative tests (eg clients connects with unsupported mechanism and client tries sasl handshake after connection is established).
"
60847448,812,junrao,2016-04-24T16:37:46Z,"Could we add some comments here and SaslClientCallbackHandler to distinguish between the two (e..g, which callbacks are expected in each handler)? Also, it seems that PasswordCallback is never supposed to be called during login?
"
60847587,812,junrao,2016-04-24T16:45:41Z,"My feeling is that always returning enabledMechanisms makes the protocol a bit simpler. Also, the client can always know what the available mechanisms are.
"
60852892,812,ijuma,2016-04-24T22:07:27Z,"We have `auth` and `authenticator` packages. What's the thinking regarding when to use one versus the other?
"
60853278,812,ijuma,2016-04-24T22:42:18Z,"Is the plan to allow users to provide their own `Login`? Is that why we have a `configure` method instead of passing the parameters via the constructor?
"
60853355,812,ijuma,2016-04-24T22:46:25Z,"Nitpick: is it worth having this as a field? Seems like we could just create it and pass it to the `LoginContext` constructor.
"
60853535,812,ijuma,2016-04-24T23:01:36Z,"This should be in the ""assigned in `configure`"" section of fields.
"
60853576,812,ijuma,2016-04-24T23:05:17Z,"I think I'd configure this right after creating the callback handler instead of in this method.
"
60853631,812,ijuma,2016-04-24T23:09:48Z,"We should use interpolation instead of string concat here.
"
60853650,812,ijuma,2016-04-24T23:11:20Z,"`send` is already doing this, right?
"
60853663,812,ijuma,2016-04-24T23:13:41Z,"Can you please add a comment on how the `pendingSaslState` is used? It seems correct to me, but it will be helpful for others reading the code.
"
60853704,812,ijuma,2016-04-24T23:16:52Z,"I wonder if more of this code is generic and should be pushed somewhere else.
"
60853743,812,ijuma,2016-04-24T23:21:02Z,"This cast is redundant.
"
60853821,812,ijuma,2016-04-24T23:25:22Z,"If the server is expecting GSSAPI, would it not disconnect the client? If we want to wrap any `SchemaException` into an `AuthenticationException`, we should probably include the rest of the code in this method into the `try` block. And we would probably want to catch `IllegalArgumentException` too.
"
60853909,812,ijuma,2016-04-24T23:32:00Z,"Is there some other information we can provide for the non Kerberos case?
"
60853932,812,ijuma,2016-04-24T23:33:39Z,"Calling `getPrivateCredentials` and `getPublicCredentials` twice is a bit messy. Not sure if we can make it better though.
"
60854042,812,ijuma,2016-04-24T23:39:23Z,"This can still be final right?
"
60854807,812,ijuma,2016-04-25T00:29:16Z,"Does this imply that we should not ship this with our production-ready code?
"
60855186,812,ijuma,2016-04-25T00:51:30Z,"It may be worth saying that only `PLAIN` is supported.
"
60855233,812,ijuma,2016-04-25T00:53:44Z,"Perhaps it would be good to include some more information (ie the number of tokens was not correct)
"
60855240,812,ijuma,2016-04-25T00:54:12Z,"Style nit: I think this should be `authorizationId`.
"
60855258,812,ijuma,2016-04-25T00:54:48Z,"We should use `isEmpty` instead of `length == 0`
"
60855285,812,ijuma,2016-04-25T00:55:58Z,"Is it worth concatenating the message given that we are passing the exception as the cause anyway?
"
60855298,812,ijuma,2016-04-25T00:56:58Z,"Should we be returning `null` here?
"
60855309,812,ijuma,2016-04-25T00:57:37Z,"I guess it's OK to leave as is because the method that returns this needs to match the name provided by `SaslServer`.
"
60855865,812,ijuma,2016-04-25T01:12:23Z,"It seems to me that this should be the other way around:

``` text
For example, if <tt>props</tt>
     * contains the <tt>Sasl.POLICY_NOPLAINTEXT</tt> property with the value
     * <tt>""true""</tt>, then the factory must not return any SASL mechanisms
     * that are susceptible to simple plain passive attacks.
```
"
60856113,812,ijuma,2016-04-25T01:25:21Z,"Indenting.
"
60856117,812,ijuma,2016-04-25T01:25:34Z,"Indenting.
"
60856366,812,ijuma,2016-04-25T01:32:18Z,"Can we group final fields please?
"
60856398,812,ijuma,2016-04-25T01:32:56Z,"As for the client, it would be good to have a comment explaining why we need a `pendingSaslState`
"
60856514,812,ijuma,2016-04-25T01:35:50Z,"There's an extra space after `=`.
"
60856547,812,ijuma,2016-04-25T01:37:35Z,"I would configure the callback handler after creating it.
"
60856567,812,ijuma,2016-04-25T01:38:38Z,"Not sure about this magic, I don't think we do that for inter-broker communication. It may be better to add some validation to `KafkaConfig` for consistency.
"
60856584,812,ijuma,2016-04-25T01:39:18Z,"I think it would be better to leave this as `null` until it is initialised in `configure`.
"
60856700,812,ijuma,2016-04-25T01:43:58Z,"We should use interpolation instead of string concatenation.
"
60857015,812,ijuma,2016-04-25T01:55:44Z,"Do we actually need this `removeInterestOps`? It seems to me that we remove it in `flushNetOutBufferAndUpdateInterestOps` before we reach here.
"
60857041,812,ijuma,2016-04-25T01:56:04Z,"Can you please explain why we need this now? It may be worth a comment.
"
60894701,812,rajinisivaram,2016-04-25T10:48:10Z,"Done.
"
60894764,812,rajinisivaram,2016-04-25T10:48:51Z,"Removed fields.
"
60894790,812,rajinisivaram,2016-04-25T10:49:00Z,"Added.
"
60894807,812,rajinisivaram,2016-04-25T10:49:10Z,"Added.
"
60894823,812,rajinisivaram,2016-04-25T10:49:21Z,"Done.
"
60894837,812,rajinisivaram,2016-04-25T10:49:33Z,"Done.
"
60894885,812,rajinisivaram,2016-04-25T10:49:48Z,"Added space.
"
60894913,812,rajinisivaram,2016-04-25T10:50:01Z,"Done.
"
60895052,812,rajinisivaram,2016-04-25T10:51:30Z,"Couldn't find a good place for the common code. Added a static method in `NetworkClient` for use in both `NetworkClient` and `SaslClientAuthenticator`.
"
60895106,812,rajinisivaram,2016-04-25T10:52:03Z,"Leaving as is as-per Jun's suggestion.
"
60895309,812,rajinisivaram,2016-04-25T10:54:01Z,"@ijuma Agree that there are too many parameters already. But a similar pattern is used for producers and consumers as well. Rather than change them all to call additional methods, leaving as-is for now. This keeps it consistent with the way SSL properties are set.
"
60895385,812,rajinisivaram,2016-04-25T10:54:42Z,"Updated.
"
60895402,812,rajinisivaram,2016-04-25T10:54:53Z,"Done.
"
60895427,812,rajinisivaram,2016-04-25T10:55:06Z,"Done.
"
60895436,812,rajinisivaram,2016-04-25T10:55:15Z,"Done.
"
60895583,812,rajinisivaram,2016-04-25T10:56:36Z,"@ijuma Since the property is used by common code rather than broker-specific code, the same property is used for both clients and broker.
"
60895608,812,rajinisivaram,2016-04-25T10:56:48Z,"Removed.
"
60895633,812,rajinisivaram,2016-04-25T10:56:59Z,"Done.
"
60895764,812,rajinisivaram,2016-04-25T10:58:19Z,"@ijuma I think it is useful to have at least one test for SASL/PLAIN that doesn't use GSSAPI since some codepaths are not enabled when GSSAPI is disabled. I have removed one of the SASL/PLAIN tests and left one in.
"
60895960,812,rajinisivaram,2016-04-25T11:00:26Z,"@ijuma I have another changeset with unit tests for SASL in the `clients` project. I will rebase and submit that once this is committed.
"
60896246,812,rajinisivaram,2016-04-25T11:03:27Z,"@junrao I have added comments. `PasswordCallback` shouldn't be called with the standard login modules during login. But since you may want to plugin custom login modules, especially for PLAIN, it makes sense to throw an appropriate exception if it was called.
"
60896528,812,rajinisivaram,2016-04-25T11:06:31Z,"@harshach is probably the best person to answer the question. Since `auth` already contained interfaces, I added interfaces to that package (the new ones are currently not exposed externally, but we may want to make these externally configurable at some point). `authenticator` contained SASL authentication implementation and it continues to hold the same.
"
60896686,812,rajinisivaram,2016-04-25T11:07:58Z,"@ijuma Yes, the original KIP was proposing to expose `Login` to plugin new authentication mechanisms. The cut-down KIP no longer needs that, but it made sense to keep it configurable for the future.
"
60896698,812,rajinisivaram,2016-04-25T11:08:13Z,"Removed.
"
60896709,812,rajinisivaram,2016-04-25T11:08:22Z,"Done.
"
60896753,812,rajinisivaram,2016-04-25T11:08:45Z,"Done.
"
60896777,812,rajinisivaram,2016-04-25T11:08:54Z,"Done.
"
60896799,812,rajinisivaram,2016-04-25T11:09:07Z,"Yes, removed.
"
60896815,812,rajinisivaram,2016-04-25T11:09:16Z,"Done.
"
60896823,812,rajinisivaram,2016-04-25T11:09:26Z,"Removed.
"
60896983,812,rajinisivaram,2016-04-25T11:11:18Z,"@ijuma Client would get disconnected, but I am not sure if GSSAPI includes an error response in that case. Catching exception from the whole block now.
"
60897075,812,rajinisivaram,2016-04-25T11:12:25Z,"@ijuma Can't think of anything that is generic and useful for other mechanisms in this case.
"
60897383,812,rajinisivaram,2016-04-25T11:15:28Z,"@ijuma Could move it into another variable, but it adds more code, so leaving as is.
"
60897403,812,rajinisivaram,2016-04-25T11:15:38Z,"Yes, updated.
"
60897700,812,rajinisivaram,2016-04-25T11:18:42Z,"@ijuma Rewrote the comment. It is simlar to Digest-MD5 implementation in Zookeeper where clear passwords are stored on disk. It can be used in production, but you may want to write your own.
"
60897719,812,rajinisivaram,2016-04-25T11:18:54Z,"Done.
"
60897732,812,rajinisivaram,2016-04-25T11:19:03Z,"Done.
"
60897762,812,rajinisivaram,2016-04-25T11:19:21Z,"Done.
"
60897773,812,rajinisivaram,2016-04-25T11:19:30Z,"Removed.
"
60898494,812,rajinisivaram,2016-04-25T11:27:33Z,"@ijuma Empty response indicates to the client that the authentication has completed successfully. Null response would possibly need changes to the client code.
"
60898519,812,rajinisivaram,2016-04-25T11:27:51Z,"Updated.
"
60898535,812,rajinisivaram,2016-04-25T11:28:01Z,"Done.
"
60898552,812,rajinisivaram,2016-04-25T11:28:08Z,"Done.
"
60898566,812,rajinisivaram,2016-04-25T11:28:17Z,"Done.
"
60898573,812,rajinisivaram,2016-04-25T11:28:24Z,"Done.
"
60898589,812,rajinisivaram,2016-04-25T11:28:34Z,"Removed space.
"
60898751,812,rajinisivaram,2016-04-25T11:30:11Z,"@ijuma It was written this way when callback handler class was configurable in the original KIP. Since it is no longer configurable, I have moved the construction to `createSaslServer` when the mechanism is known.
"
60898799,812,rajinisivaram,2016-04-25T11:30:35Z,"Replaced with error check and validation in `KafkaConfig`.
"
60898811,812,rajinisivaram,2016-04-25T11:30:44Z,"Done.
"
60898835,812,rajinisivaram,2016-04-25T11:30:54Z,"Removed.
"
60900182,812,rajinisivaram,2016-04-25T11:46:04Z,"Have added a comment.  `SaslServerAuthenticator.complete()`  used to check `saslServer.isComplete` earlier and didn't rely on the SASL state. It is useful to keep the state up-to-date for debug anyway, and now that it is kept uptodate, `SaslServerAuthenticator.complete()` can use it too.
"
60916629,812,ijuma,2016-04-25T13:57:25Z,"Thanks.
"
60916703,812,ijuma,2016-04-25T13:57:50Z,"OK, fine.
"
60917166,812,ijuma,2016-04-25T14:00:30Z,"Thanks. I guess what I was trying to say is that I don't know if we will ever get the schema exception since the server will just disconnect us. But it would be good to verify that via tests. It can be done in a separate PR though.
"
60917208,812,ijuma,2016-04-25T14:00:43Z,"OK, fine.
"
60917226,812,ijuma,2016-04-25T14:00:49Z,"OK.
"
60917392,812,ijuma,2016-04-25T14:01:46Z,"OK.
"
60917610,812,ijuma,2016-04-25T14:03:10Z,"My comment was based on:

``` text
@return The possibly null challenge to send to the client.
     * It is null if the authentication has succeeded and there is
     * no more challenge data to be sent to the client.
```

However, if our code expects an empty byte array, it's fine to leave as is.
"
60917684,812,ijuma,2016-04-25T14:03:45Z,"Sounds good.
"
60917800,812,ijuma,2016-04-25T14:04:27Z,"Sounds good.
"
60917907,812,ijuma,2016-04-25T14:05:16Z,"Thanks.
"
60918260,812,ijuma,2016-04-25T14:07:32Z,"Can you please file a JIRA for that and also think about system tests that we need? Some of it is just a matter of using the PLAIN mechanism in some tests. The other important case is testing upgrades and different broker/client versions. We already have many of these tests, so it may just be a matter of expanding them.
"
60918411,812,ijuma,2016-04-25T14:08:23Z,"The fact that it's common code is an implementation detail. We can pass the parameter via `ChannelBuilders` to deal with that. Let's see what @junrao thinks.
"
60918481,812,ijuma,2016-04-25T14:08:48Z,"OK.
"
60918614,812,ijuma,2016-04-25T14:09:37Z,"Rajini did this.
"
60918664,812,ijuma,2016-04-25T14:09:55Z,"Looks good, thanks.
"
60919159,812,ijuma,2016-04-25T14:13:13Z,"This may seem a bit nitpicky, but it makes a difference from a Java Memory Model perspective, the assignment to the final field should happen after the list has been populated.
"
60919232,812,ijuma,2016-04-25T14:13:39Z,"Would it make sense to move this down to `DefaultLogin`?
"
60919291,812,ijuma,2016-04-25T14:13:57Z,"Maybe we don't need this here either (subclasses can implement it).
"
60919588,812,ijuma,2016-04-25T14:16:03Z,"Nitpick: there should be a space before `{`.
"
60919616,812,ijuma,2016-04-25T14:16:13Z,"Nitpick: there should be a space before `{`.
"
60949617,812,ijuma,2016-04-25T17:06:25Z,"This doesn't seem to be changed?
"
60956336,812,rajinisivaram,2016-04-25T17:48:32Z,"Done
"
60956492,812,rajinisivaram,2016-04-25T17:49:28Z,"Sorry, there were two sets of code changes here, I had missed one, but have updated  now.
"
60957054,812,rajinisivaram,2016-04-25T17:52:45Z,"@ijuma Agree. I suppose it depends on whether SASL mechanism is treated as a property of the SASL protocol or a higher level protocol itself. There are other instances of client-side properties (eg. service name) which don't have an ""inter.broker"" prefix when used in the broker. I am happy with either, so will wait and see what @junrao thinks.
"
60957655,812,ijuma,2016-04-25T17:56:06Z,"Good point about service name.
"
60957839,812,rajinisivaram,2016-04-25T17:57:03Z,"Will add a test in KAFKA-3617 along with the other unit tests.
"
60958025,812,rajinisivaram,2016-04-25T17:58:09Z,"Done.
"
60958055,812,rajinisivaram,2016-04-25T17:58:19Z,"Moved.
"
60958076,812,rajinisivaram,2016-04-25T17:58:28Z,"Done.
"
60958107,812,rajinisivaram,2016-04-25T17:58:40Z,"Done.
"
60959856,812,rajinisivaram,2016-04-25T18:09:05Z,"Done.
"
60963638,812,ijuma,2016-04-25T18:30:32Z,"I asked @junrao offline about this and he suggested the `sasl.mechanism.inter.broker.protocol`. The reason why I think this is different than `sasl.kerberos.service.name` is that it's easy to be confused when one sees `sasl.mechanism` and `sasl.enabled.mechanisms`.
"
60993752,812,rajinisivaram,2016-04-25T21:36:01Z,"@ijuma @junrao OK, makes sense. I have added updated the PR.
"
61186244,812,harshach,2016-04-27T00:14:38Z,"authenticator for pluggable authenticator that we use.
"
37835370,165,onurkaraman,2015-08-25T06:09:30Z,"`votes.maxBy(_._2)._1`
"
37835755,165,onurkaraman,2015-08-25T06:18:59Z,"`allMemberMetadata.toMap` converts to an immutable map
"
37836136,165,onurkaraman,2015-08-25T06:27:03Z,"given that the session timeouts are defined per member, should this instead be:

```
val MemberMinSessionTimeoutMsProp = ""member.min.session.timeout.ms""
val MemberMaxSessionTimeoutMsProp = ""member.max.session.timeout.ms""
```
"
37836633,165,onurkaraman,2015-08-25T06:38:14Z,"Another minor point, but I think it's cleaner to keep all group-specific checks in doJoinGroup similar to how it was before.
"
37842937,165,onurkaraman,2015-08-25T08:21:18Z,"I think we can replace collect with the less obscure map:
`supportedProtocols.find{ case (p, d) => protocol == p }.map{ case (p, d) => d }`
"
37890199,165,onurkaraman,2015-08-25T16:58:37Z,"`return metadata != null ? metadata.equals(that.metadata) : that.metadata == null;`
"
37895582,165,onurkaraman,2015-08-25T17:44:30Z,"`def candidateProtocols = {`
"
37895610,165,hachikuji,2015-08-25T17:44:46Z,"I was thinking that ""member"" at the root seemed a little vague in configuration. 
"
37895692,165,hachikuji,2015-08-25T17:45:28Z,"Agreed.
"
37895693,165,onurkaraman,2015-08-25T17:45:28Z,"Same as above:
`def currentMemberMetadata: Map[String, Array[Byte]] = {`
"
37896333,165,hachikuji,2015-08-25T17:50:59Z,"Intellij generated this line! Wonder why their template is so weird...
"
37898046,165,onurkaraman,2015-08-25T18:05:02Z,"Your addGroup change now takes in (groupId, protocolType). It probably won't change the outcome of the tests, but better for clarity to match up with the intended usage.
"
37901060,165,onurkaraman,2015-08-25T18:27:58Z,"github annoyingly picked the diff window for me. Just to clarify, I was only suggesting you move the following check:
`else if (group.protocolType != protocolType)`
"
37904617,165,ewencp,2015-08-25T18:56:51Z,"This confused me a bit since previously this class was purely additive wrt the set of topics. Is there a case where you can actually lose a subscription by doing this? I'm thinking if the user does something to the subscriptions during a callback?
"
37904628,165,ewencp,2015-08-25T18:56:54Z,"Nit, but since this is a nested class that will probably always be referred to by `Metadata.MetadataListener`, `Listener` would probably be more concise/less redundant.
"
37904632,165,ewencp,2015-08-25T18:56:57Z,"I think this is the right default assignor, but just to be clear this is changing a default. We're still ok with this since we haven't released this yet, but are there any concerns with inconsistency with the old consumer?
"
37904642,165,ewencp,2015-08-25T18:57:03Z,"This comment doesn't seem useful...
"
37904654,165,ewencp,2015-08-25T18:57:07Z,"We could make this private since the `success` and `failure` methods seem to be the preferred way of constructing these objects and ensure you construct a valid combination (i.e. guarantees the right fields are null depending on the value of `succeeded`).
"
37904666,165,ewencp,2015-08-25T18:57:11Z,"Can we may be document what `groupSubscription` is more clearly somewhere else, maybe even just on this class's javadoc? It took me a bit of digging to figure out what it was -- I was confused why we were getting (and using) something called group subscription when a join group had failed.

Maybe a bit of renaming could also help? aggregateGroupSubscription or aggregateRequestedGroupSubscription?
"
37904681,165,ewencp,2015-08-25T18:57:16Z,"This doesn't currently build for me because Iterator's `remove()` method is not implemented.
"
37904688,165,ewencp,2015-08-25T18:57:21Z,"Is there a reason for this extra level that just contains the subscription struct field? I get that other implementations might want subscription info + other metadata, but does AbstractPartitionAssignor need this?
"
37904699,165,ewencp,2015-08-25T18:57:23Z,"This could be a static final -- doesn't need to be a separate object for every call to `schema()`
"
37904706,165,ewencp,2015-08-25T18:57:25Z,"I'm not sure I understand how versioning will work for this data format. Do we need to also include a schema version number as part of this `write()` call? Otherwise, how would we introduce a V1? It doesn't look like this is handled in PartitionAssignmentProtocol since the version of the schema isn't exposed anywhere in this interface.
"
37904713,165,ewencp,2015-08-25T18:57:29Z,"Why not just `subscription.toArray()`
"
37904720,165,ewencp,2015-08-25T18:57:32Z,"Same question, why not `topics.toArray()`
"
37904726,165,ewencp,2015-08-25T18:57:36Z,"Any other ideas besides Controller for naming these classes? This is a different part of the code, but we've already got KafkaController on the broker.
"
37916318,165,hachikuji,2015-08-25T20:38:44Z,"I don't think it's possible since we await all pending requests to the coordinator before sending a new join group request, but I'll have to look into it. This is definitely one of the messier aspects of this patch. It may get easier after KAFKA-2388 since subscriptions are no longer additive, but the hard part is ensuring that the Metadata topic list matches the union of all topics subscribed by the group. One option might be to only update the Metadata topic list based on the subscriptions defined in the join group response. That would mean we'd always need two rounds of the rebalance when subscriptions change, which seems unfortunate. I'll see if there are any nicer options.
"
37917400,165,hachikuji,2015-08-25T20:47:58Z,"As long as we document the difference, it's probably fine. I can't imagine any cases where users would have a hard dependence on the range assignor. And since a lot of users probably won't override the default, I think we should have it set to the better strategy.
"
37917723,165,hachikuji,2015-08-25T20:50:41Z,"Yep, I didn't catch it locally because Java 8 provides a default implementation.
"
37918344,165,ewencp,2015-08-25T20:55:35Z,"Not a big deal, but `Collections.singletonMap` is a nice shortcut for building maps like this.
"
37918767,165,hachikuji,2015-08-25T20:58:29Z,"Initially I was trying to include a generic byte array in the protocol which extensions could provide a schema to support their own custom metadata. That got a little too complex to manage, so I decided to leave it up to custom assignors to define the full metadata schema (including subscriptions) that they depend on. In short, the extra level can be removed now.
"
37919217,165,hachikuji,2015-08-25T21:02:08Z,"Manager?
"
37919626,165,ewencp,2015-08-25T21:05:40Z,"Test for inconsistent metadata? Or are we just assuming the other test is good enough since that's handled in AbstractPartitionAssignor?
"
37922433,165,ewencp,2015-08-25T21:32:02Z,"So this basically means that not only do we enable rolling upgrades where you add a new protocol, restart everything, and can then later remove the new protocol, but we actually require it (unless you shut down the entire group and then start again from scratch)?
"
37922449,165,hachikuji,2015-08-25T21:32:13Z,"This has probably not been thought all the way through, but any version embedded in the metadata itself cannot really be leveraged in the protocol. New versions of an assignor can support old versions of the metadata, but the opposite won't generally work. If the user wants to have a change to the metadata without bringing the cluster down, then they'd have to provide separate assignors supporting the different metadata versions. 

The protocol allows each assignor to provide a single version which the coordinator can use to ensure compatibility. I think the question is whether this version should identify the version of the metadata, the version of the assignor, or whether we need an additional version field to be able to express both. In general, the only thing the coordinator can do with these versions is check that they match for all members, so it would seem a little unfortunate to have to add another.

It is also possible to use the name of the assignor to communicate version differences. For example, instead of ""roundrobin,"" this should be ""roundrobin-v0."" Then if we need to change the metadata, we would implement AbstractPartitionAssignorV1 and a RoundRobinAssignorV1 which uses the name ""roundrobin-v1.""
"
37924821,165,hachikuji,2015-08-25T21:57:27Z,"That is right. If you attempt to upgrade the protocol without providing both versions, then the new group members (with the new protocol) would be rejected until all old members have left. This is consistent with the current implementation. I proposed previously to have the coordinator choose the protocol which the largest number of members support, but you seemed concerned that this would effectively halve the group's capacity for a short duration in a rolling upgrade. I still think that might be a more useful update mechanism in practice, but the implementation might be tricky since rejected members would have to retry at a later time which could lead to unnecessary (and costly) rebalancing. The advantage of the approach implemented here is that the code is simple.
"
37927101,165,ewencp,2015-08-25T22:23:44Z,"I think this is fine -- the rolling upgrade with 2 protocols should be the common and suggested path if you need to do this anyway. Since this is only an issue if you don't configure your consumers for a seamless upgrade, I actually don't think it's bad to have pretty harsh behavior like this. If someone screws up and misconfigures something, they'll figure it out a lot faster if they bounce consumers and they can't even get back into the group, whereas the majority vote would work but halve the capacity. I think either way is fine, but I agree this code is simpler and has reasonable results.
"
37927178,165,ewencp,2015-08-25T22:24:43Z,"That works. We have a ton of XManager classes on the broker, but it's a generic enough name component that it shouldn't be confusing.
"
37927752,165,ewencp,2015-08-25T22:31:56Z,"Sorry, I think we've already discussed this like 3 times and I just keep getting confused about it.

The reason this code confused me is because we have `CONSUMER_METADATA_V0`, which implies at some point we could add `CONSUMER_METADATA_V1` to this class and support both/upgrading. I think I see now how we could actually still (potentially) only have `AbstractPartitionAssignor`, but then versioned concrete classes.
"
37927905,165,ewencp,2015-08-25T22:33:48Z,"Do we even want the range assignor anymore then? Is it needed for anything that the other assignors we'd want to implement wouldn't?
"
37929097,165,hachikuji,2015-08-25T22:48:28Z,"Haha, it still confuses me too. I think I sort of blindly applied the same pattern that was used for schema definitions in the Protocol class. I can drop the V0, but the trouble with assignor versioning won't go away quite that easily. One concrete suggestion might be to make the protocol version a string instead of an int to allow more information to be embedded in the version. We could then use it to track both the protocol and metadata versions. I can also add some documentation on the GroupProtocol and PartitionAssignor interfaces to try to make intended usage clearer.
"
37929523,165,hachikuji,2015-08-25T22:53:56Z,"Yeah, that's a good point. Maybe we leave it for now and address it in a separate JIRA? I can default to RangeAssignor for consistency with the current version.
"
37932986,165,ewencp,2015-08-25T23:40:09Z,"Yeah, I think that's fine. We probably need a JIRA for any other built-in assignors we want to ship with 0.8.3 anyway, e.g. I assume we'll have a copartitioning implementation for kstreams.
"
38051685,165,guozhangwang,2015-08-27T00:29:55Z,"Which version did you use and which edit feature did you turn on? Your Intellij is definitely overly-smart ;)
"
38051966,165,guozhangwang,2015-08-27T00:35:56Z,"I would prefer defaulting to range just for consistency. We have seen similar cases in the producer where the behavior of partitioner's hashing function changes a bit, causing offset manager migrated for mirror-makers and hence resetting offset and data duplicates.
"
38602589,165,ewencp,2015-09-03T00:32:28Z,"Why are we splitting the handling of metadata between both `Metadata` and `Fetcher` now? Is this just so that this topic-partition metadata is not persistent in `Metadata` since calling `partitionsFor` doens't really imply anything about whether you'll continue to need updated metadata for the topics passed in here? Even so, this split seems less than ideal...
"
38602592,165,ewencp,2015-09-03T00:32:32Z,"Any reason for using an empty list here rather than `null` as a sentinel? The empty list approach seems like it could lead to confusing results if you have a programmatically generated list which can sometimes be empty.

Right now it's not a problem since we only expose `listTopics` and `partitionsFor(oneTopic)`. But wasn't there a proposal for something like `partitionsFor(String... topics)`, in which case this could affect the public API.
"
38602607,165,ewencp,2015-09-03T00:32:44Z,"Does this really make sense? For this to occur, we'd need to have everyone agree on the metadata since that was checked above, but then they'd have to be missing metadata on one of those topics. Wouldn't at least the one consumer that included that topic have metadata for it?

Is the goal here that we continue processing the ones we have metadata for so we can at least make progress? If we do this, is there anything that's forcing the metadata to be refreshed (like the inconsistentMetadata result does)? If not, wouldn't this cause us to sometimes knowingly ignore some topics which we might be able to make progress on immediately if we refreshed and rejoined the group?
"
38602625,165,ewencp,2015-09-03T00:33:07Z,"You might want to rename this for clarity. I saw `HashSets` being passed into the `MetadataSnapshot` constructor in `ConsumerGroupController` and since I knew `hash()` is called on that, I was worried the `HashSets` might have been a mistake. They're not, and that makes sense given the protocol we came up with, but the current method names aren't clear about what's being hashed. My first thought was `metadataHash()`, but that doesn't exactly help... maybe `topicMetadataHash()`?
"
38602735,165,ewencp,2015-09-03T00:35:35Z,"We can always go the `max.in.flight.requests.per.second` route with maximum verbosity and prefix with `group.member.`!
"
38661233,165,hachikuji,2015-09-03T15:46:57Z,"Since we depend on the metadata matching the subscription set in this patch, I wanted to remove any other calls which can affect it. The alternative would be to always intersect the subscription and metadata before computing the hash sent along in the join group. Either way should work, but I actually think it's a good thing not having the persistent metadata state updated by partitionsFor(). Can you explain a little more why this is less than ideal?
"
38661343,165,hachikuji,2015-09-03T15:47:56Z,"Fair enough. I can use null instead.
"
38664475,165,ewencp,2015-09-03T16:15:09Z,"It's just not great having multiple paths that make the same type of request if possible. But I see why we at least want different handling of this request/response, and there was another JIRA to make this exact change anyway.
"
38664724,165,hachikuji,2015-09-03T16:17:49Z,"The case I was trying to handle is non-existing topics. The current behavior of the consumer is to keep trying to fetch metadata for the non-existing topic in the hope that it will eventually be created, and to continue consuming from other subscribed topics. If one of the consumers does have metadata (perhaps because the topic was just created), then the metadata hash will be inconsistent and members will refetch. If the topic is later created, we will discover it when we update metadata and the consumers will rejoin.
"
38667583,165,ewencp,2015-09-03T16:46:31Z,"Ok, that makes sense in that it allows consumers to make progress even if a topic needs to be created. If auto topic creation is not enabled, then this might be the only way to make progress if one of the topics doesn't yet exist. But doesn't this also mean that if you have auto topic creation enabled and the consumer group starts up before any producers, then it might have to wait an entire metadata refresh timeout before any data is consumed from that topic, even if producers start sending data to it immediately?

I think this is fine (and it's a one-time delay, 5 or 10 minutes by default IIRC), just want to make sure we understand the implications of doing this.
"
38669112,165,hachikuji,2015-09-03T17:01:48Z,"Yeah, that's right, and I agree it's a little unfortunate to have to wait the full refresh interval, though I assume most cases would have the topic already created or created by producers. Probably the main case where this would be encountered is in testing. One thing we could try to do is stagger metadata refreshes randomly in the group so that they are not all synchronized around the join group completion. That would reduce the expected time to discover metadata changes in general. It might also make sense to set the default metadata refresh rate a little lower for client-side assignment since we can't depend any longer on the broker discovering changes for us anymore.
"
41659964,165,ewencp,2015-10-09T18:04:02Z,"This is out of date, right? No more MetadataHashes in this version?
"
41661913,165,ewencp,2015-10-09T18:20:56Z,"Is this version for the consumer protocol itself? That can't be in the struct, can it? Doesn't it need to prefix the struct so you can decide which Schema to decode with?
"
41662735,165,ewencp,2015-10-09T18:28:24Z,"Is this just temporary until we add better support in the configs for multiple assignors? I'd imagine we need to think through the exact semantics, if ordering matters at all, etc. Is the plan to eventually just switch this to a comma-separated list of class names?

One thing I found with Copycat was that the more things that needed to be configured via the same config dictionary, the more problematic Kafka's standard approach to configuration became because you could easily hit cases where there were conflicting settings. Not sure if a) that'll be an issue here or b) if we even want to support assignors that have _that_ much config, but something worth thinking about before committing to this specific approach to specifying assignors.
"
41664701,165,hachikuji,2015-10-09T18:45:40Z,"Yes, you are right. I wrote it that way initially, but changed it several times as I was considering compatibility implications. Assuming forward compatibility, for example, you just parse blindly even if the version is higher. However, newer versions would need to check the version before doing any parsing, so it should still be parsed separately. I'll update the patch. In general, the versioning problem is complex enough to merit its own discussion, so the goal here is to keep things as simple as possible.
"
41664757,165,ewencp,2015-10-09T18:46:10Z,"Seems like this comment is not true now? Intentional change or are we missing a check here?
"
41664863,165,ewencp,2015-10-09T18:47:24Z,"Agreed. As long as we address it in a blocker follow up patch, I'm happy to defer that discussion until after this patch.
"
41665659,165,ewencp,2015-10-09T18:55:06Z,"Nice. It took me a minute to figure out how the combination of join group and sync were working, but the futures made this work out very nicely. This is quite a bit cleaner than I thought it was going to be.
"
41666054,165,ewencp,2015-10-09T18:58:56Z,"Can probably remove this comment since #290 is addressing this as part of KAFKA-1843.
"
41666648,165,ewencp,2015-10-09T19:05:01Z,"Minor, but it'd be good to document the `M` and `S` type parameters. The current javadoc doesn't explain what ""state"" means here.
"
41666750,165,ewencp,2015-10-09T19:06:01Z,"What does `Gen` in `GenType` stand for? This is a very minor naming issue, but since I understand the generalized group functionality and am unsure what is trying to be communicated with this name, I imagine it might confuse others as well.
"
41670166,165,hachikuji,2015-10-09T19:40:29Z,"Haha, Gen is short for Generic. Awful name, I know. I'll take any suggestions. Basically I just wanted to have the type information ride along with the schema. Maybe GenericType? ParamType?
"
41671402,165,ewencp,2015-10-09T19:54:43Z,"Minor cleanup, but there seem to be a few cases like this where the response to two cases is identical and you could collapse them into one conditional block.
"
41671988,165,ewencp,2015-10-09T20:00:59Z,"Yeah, I was going to suggest `ValidatedType`, but I see that validate is already part of `Type` and you've just refined the return type. It's unfortunate that `Type` isn't already generic. I'd just expand the name to `GenericType`.
"
41674067,165,hachikuji,2015-10-09T20:23:43Z,"Honestly, Type could probably be made generic without a huge impact. The usage kind of suggests that intention anyway. That might be worthwhile refactoring for a follow-up patch, but probably not a good idea to jam in here. I'll change to GenericType for now.
"
41676221,165,guozhangwang,2015-10-09T20:46:53Z,"nit: rename assignors to assignorsMap? it's a bit misleading to have two variables with the same name here.
"
41678401,165,guozhangwang,2015-10-09T21:10:21Z,"Is this indentation intentional?
"
41678754,165,hachikuji,2015-10-09T21:14:18Z,"Not intentional, just Intellij getting a little aggressive trying to ""fix"" the indentation. I'll fix it in the next commit. 
"
41679809,165,guozhangwang,2015-10-09T21:26:55Z,"Why does consumers need to fetch the metadata for group subscription? If it is not subscribed to some topics their partitions will never to assigned to itself right?
"
41680449,165,guozhangwang,2015-10-09T21:34:04Z,"This class should be in clients/src/test instead of clients/src/main.
"
41680691,165,hachikuji,2015-10-09T21:37:09Z,"The leader is the only member who sees the group subscription; for everyone else, groupSubscription() will just return that member's subscription. We depend on the leader's metadata in order to set assignments, so it must have metadata available for the full group's subscription. We could just issue one metadata request when the leader is preparing to do the assignment, but that opens the door to the leader missing metadata changes affecting topics it itself is not subscribed to. More concretely, suppose the leader is subscribed to [a], but one follower is subscribed to [a,b]. If the follower notices an increase in b's partitions, it can trigger a rebalance, but there's no guarantee that the partition change will be visible by the leader when it fetches topic metadata. Therefore we register the leader to watch for changes to b as well. Eventually, when the leader sees the change, it can rebalance. Does that make sense? 
"
41682690,165,guozhangwang,2015-10-09T22:02:14Z,"Could this ever happen in the normal case? I think the key-set of consumersPerTopic should always be a super-set of key-set of partitionsPerTopic?
"
41682946,165,hachikuji,2015-10-09T22:05:44Z,"Yeah, you are right.
"
41683260,165,guozhangwang,2015-10-09T22:09:55Z,"This can be private; also what is the benefit of making it templated instead of using String directly?
"
41684230,165,guozhangwang,2015-10-09T22:24:14Z,"There are some overlap between this GenType with org.apache.kafka.common.protocol.types.Schema. But Schema.validate() returns a Struct instead of a generic T type. Could we try to merge these two?
"
41685576,165,guozhangwang,2015-10-09T22:47:08Z,"A related question to Ewen's comment above: how will this version() function be used, for both Assignment and Subscription?
"
41687689,165,hachikuji,2015-10-09T23:34:03Z,"The leader provides compatible versions of assignments given the respective versions specified in member subscriptions. For this patch, I have assumed full forwards and backwards compatibility of the embedded group protocol, which allows any member to be elected leader during an upgrade scenario and still perform correctly. There are two cases to consider:
1. The leader is on the old version: In this case, forwards compatibility allows the leader to go ahead and parse subscriptions from all members (even those on newer versions) and generate assignments corresponding to its older version. Backwards compatibility ensures that the newer members will be able to parse the assignments with older versions. For example, if the leader is on version 0 and a follower is on version 1, the leader will parse the version 1 subscription as a version 0 subscription and provide it a version 0 assignment.
2. The leader is on the new version: In this case, the leader can parse both new and old subscription versions. Depending on the protocol, it can choose to send assignments to members corresponding to their respective subscriptions, or it can choose the oldest version and send assignments based on it. Using a similar example as above, if the leader is on version 1 and the follower is on version 0, then the leader can parse the version 0 subscription and send a version 0 assignment.

Obviously this strict compatibility model might be challenging to implement in practice, depending in particular on the protocol format (it would be relatively straightforward if we chose JSON, for example). Alternatively, if we exposed version information in the JoinGroup protocol, then the coordinator would be able choose the member with the highest version, which would let us weaken the compatibility assumption. The tradeoff is that we'd need to complicate the protocol a bit to carry version information from each member along with their respective subscriptions and assignments. 

This gets even trickier if you allow assignment strategies to include their own embedded data format, so I was hoping to get a simple approach in place first before having a more complete discussion. Perhaps for now I can just add some documentation on the GroupProtocol class to clarify the current compatibility assumptions?
"
41709461,165,ijuma,2015-10-11T11:44:04Z,"This seems like a generic method and not specific to strings, so it seems right to me to do it generically (unless we can implement a faster version by making it specific, which doesn't seem to be the case here). A few questions:
- Should it be in `Utils` instead of here?
- `consumers` parameter name should be `collection` probably
- Would `<T extends Comparable<? super T>>` be a better bound? That's what `Collections.sort` uses.
"
41724298,165,guozhangwang,2015-10-12T05:24:35Z,"That makes sense, could you rephrase the above a little bit as comments to groupSubscription?
"
41724379,165,guozhangwang,2015-10-12T05:27:38Z,"The only place sorted() is called is in the above RoundRobinAssignor where T is String, so I am wondering if this function is specific to RoundRobinAssignor only or not; If it is general I agree with you that we should keep it with generics and move it to Utils, otherwise we can just make it private to RoundRobinAssignor and with String only.
"
41727098,165,guozhangwang,2015-10-12T06:49:25Z,"Add some explanations about generic M(etadata) and S(tate).
"
41775541,165,guozhangwang,2015-10-12T16:39:54Z,"I'm wondering if we can put the parse() function in a centralized place since their implementations are all similar?
"
41778171,165,guozhangwang,2015-10-12T17:08:05Z,"Could we avoid having two Metadata.Listener, one in KafkaConsumer and one here? If not, I would prefer to let Coordinator have a variable field Listener instead of letting itself implement the interface itself.
"
41778848,165,guozhangwang,2015-10-12T17:15:34Z,"What scenario would require leaderId in doSync?
"
41781687,165,guozhangwang,2015-10-12T17:45:31Z,"A general comment: maybe we can rename GroupCoordinator to Coordinator, and Coordinator to ConsumerCoordinator, and the server-side ConsumerCoordiantor to GroupCoordinator. Do you think the names are more clear that way?
"
41783220,165,hachikuji,2015-10-12T18:00:32Z,"Yeah, that sounds reasonable. Maybe instead of Coordinator, we can call it AbstractCoordinator?
"
41786233,165,guozhangwang,2015-10-12T18:29:47Z,"Update the comments for this function.
"
41787479,165,guozhangwang,2015-10-12T18:41:13Z,"This metadataSnapshot is only written upon metadata update, but not read in doSync: we still use the above metadata object, and actually it seems not used anywhere. Is this intentional?
"
41791203,165,guozhangwang,2015-10-12T19:18:58Z,"Good catch.
"
41791349,165,guozhangwang,2015-10-12T19:20:17Z,"If we are not assigning a different error code here, maybe we want to shift the rest error codes left? And same above for 23 / 24 / 25.

EDIT: actually 23 and 25 are still used, scratch that part.
"
41792646,165,guozhangwang,2015-10-12T19:36:12Z,"Is this still a possible error code? We may need to update the possible error codes for Join / Sync / Commit / FetchOffset responses.
"
41793002,165,guozhangwang,2015-10-12T19:39:52Z,"Add a comment pointing out this is for leader id.
"
41793506,165,guozhangwang,2015-10-12T19:45:41Z,"This is not introduced in this patch, but why we are returning GROUP_COORDINATOR_NOT_AVAILABLE for OffsetCommit if (!isActive.get), while return NOT_COORDINATOR_FOR_GROUP for OffsetFetch for the same condition? Do you know if there is any motivation?
"
41793618,165,guozhangwang,2015-10-12T19:46:52Z,"What are the possible error codes for this response? I ask this since moving forward we will move this into the protocol as well so it's better keep track of those for now.
"
41797594,165,hachikuji,2015-10-12T20:27:34Z,"@ewencp @guozhangwang I've added a commit to clarify current versioning assumptions of the embedded metadata/assignment format. Let me know if that is sufficient for now. I think we'll still want to consider this in a little more detail in a follow-up issue after this is checked in.
"
41801442,165,hachikuji,2015-10-12T21:07:40Z,"I think I had made it generic initially because the consumer had an object representation, but I agree it's a little silly here. Since it is a general function, perhaps I'll go ahead and relocate it to Utils.
"
41802182,165,guozhangwang,2015-10-12T21:16:15Z,"Why do we remove the check on subscriptionListener.revoked?
"
41802812,165,guozhangwang,2015-10-12T21:22:55Z,"generation id is not incremented here?
"
41803001,165,guozhangwang,2015-10-12T21:24:49Z,"Why error code is 1 instead of 0? Also we probably should use Errors.OFFSET_OUT_OF_RANGE.code() for readability.
"
41804236,165,guozhangwang,2015-10-12T21:38:31Z,"When we receive a join group in this state from a single member, does that mean we will transit to PreparingRebalance and then immediately back to AwaitingSync?
"
41804868,165,guozhangwang,2015-10-12T21:45:30Z,"Do we still have this event in the FSM diagram now?
"
41805021,165,guozhangwang,2015-10-12T21:47:38Z,"Is syncErrorCode always NONE.code?
"
41805157,165,guozhangwang,2015-10-12T21:49:32Z,"Update comments.
"
41807478,165,guozhangwang,2015-10-12T22:19:36Z,"Maybe we do not need to have the <action> item here in the FSM, but just list for all possible ""input"": join, sync, heartbeat, offset-commit, offset-fetch, and events like ""member failed"", ""leader timed out while syncing""; and what are the possible ""output"" (i.e. the response) and the state-change (i.e. the transition). Some input may not trigger state-change while some other may trigger, and the same input may also end in different state change (e.g. the last join-group request from the group or not). That will make the state machine diagram more clear.
"
41808846,165,guozhangwang,2015-10-12T22:37:29Z,"I think we should, probably piggy-backing on LOAD_BALANCE_IN_PROGRESS.
"
41809554,165,guozhangwang,2015-10-12T22:47:44Z,"Since maybePrepareRebalance and proporgateAssignment are all synchronized on the group, when the group is still in awaiting sync either all members still have their callback yet to trigger, or all of them should have sent the response in callback right?

Also the comments are not very accurate: if the group is in the awaiting sync, cancel the sync response for all of them if possible to have all its members rejoin.
"
41809935,165,guozhangwang,2015-10-12T22:53:42Z,"In the current implementation we can return the sync response multiple times to members' requests as long as they have the right generation-id and are within the group. I think it does not have any side-effects for now but just want to clarify with you.
"
41810084,165,guozhangwang,2015-10-12T22:56:11Z,"Do we need to call completeAndScheduleNextHeartbeatExpiration in both doJoin and doSync? Why we need to reschedule in doSync?
"
41810875,165,guozhangwang,2015-10-12T23:08:04Z,"And with the FSM, we can write handleXXX in Coordinator in a way that is aligned with the FSM as the following (personally I feel it may make future implementation and reviews easier):

handleXXX:
1. check coordinator arability and group availability,
2. if group is available call doXXX.
3. otherwise proper error code, or create group first then call doXXX.

doXXX:
   switch (group.state)
       case State1: this request should not be received in this state, return some error-code.
       case State2: do some actions based on the group metadata, and probably transit group to another state.
       case StateX: ... etc

And similarly for events like failed consumers, call onYYY:

onYYY:
   switch (group.state)
       case State1: this event should not happen in this state, throw exception.
       case State2: do some actions based on the group metadata, and probably transit group to another state.
       case StateX: ... etc
"
41822373,165,hachikuji,2015-10-13T02:47:56Z,"It looks like the offset fetch is the only request where we return NOT_COORDINATOR_FOR_GROUP for that case. It's a little unclear which error code should be preferred, but I would tend to think that NOT_COORDINATOR_FOR_GROUP would be the right one since it would force the client to rediscover the coordinator, which is probably what we want when we're shutting down. On the other hand, when we're starting up, we'd want to use GROUP_COORDINATOR_NOT_AVAILABLE. What do you think?
"
41822535,165,hachikuji,2015-10-13T02:53:23Z,"Woops, I'll add the assertion back. I think at one point I had added code to only invoke the revocation callback when we were leaving a valid generation, so the expected revokeCount was actually zero and the check on revoked was unneeded. Later on, I removed the check to be consistent with the current code, but forgot this assertion.
"
41822701,165,hachikuji,2015-10-13T02:58:23Z,"Seems like it's just testing serialization, so 1 was probably chosen for convenience. I'll change to Errors.NONE instead.
"
41823037,165,hachikuji,2015-10-13T03:07:32Z,"If we already transitioned to AwaitingSync, then the members of the current generation have already been sent to the leader and the other members have received the generationId. If a new JoinGroup arrives before the leader has sent SyncGroup, then we can either reject the JoinGroup and continue waiting, or we can just transition to PreparingRebalance and have all members rejoin. I chose the latter since there didn't seem much point in stabilizing the group only to have the rejected member force a rebalance anyway after we transitioned to Stable. 
"
41823511,165,hachikuji,2015-10-13T03:21:59Z,"I think this is a good idea. It is difficult in the existing implementation to check where each case fits and whether all cases have been covered.
"
41895691,165,hachikuji,2015-10-13T17:14:07Z,"The transition from AwaitingSync to Stable happens when the leader submits the assignment for the generation in its SyncGroup request. Other members (followers) may submit SyncGroup before or after this transition occurs. If before, we hold onto the request until the leader has synced; if after, we return immediately.

An alternative would be to await all SyncGroup requests before returning any of them, but that seems unnecessary since the group already synchronized in the JoinGroup barrier and the generation has been incremented. It is possible that some members will receive the assignment and others may crash before receiving anything, but the worst thing that happens in that case is that a rebalance is triggered. The generation protects us from inconsistent assignment.
"
41901890,165,guozhangwang,2015-10-13T18:02:07Z,"I am OK either way.
"
41902685,165,guozhangwang,2015-10-13T18:08:32Z,"I think we would better return GROUP_COORDINATOR_NOT_AVAILABLE for (!isActive), and NOT_COORDINATOR_FOR_GROUP for (!isCoordinatorForGroup(groupId)) for all requests / conditions, but on the server side we should check the latter first before the former, so that consumers will get NOT_COORDINATOR_FOR_GROUP if it is ever the case and rediscover.
"
41907724,165,guozhangwang,2015-10-13T18:46:55Z,"That is fine, then in PreparingRebalance state we could get SyncGroup requests from leader / followers, and need to return an error code to let them re-send the JoinGroup. Is that the case?
"
41908099,165,guozhangwang,2015-10-13T18:50:10Z,"rep. first paragraph: yeah makes sense.

rep. second paragraph: I am not favoring in adding another synchronization barrier unless we have to. Let's sync up some time about the state transition again.
"
42170845,165,guozhangwang,2015-10-15T19:45:25Z,"Wondering if we can do the following:
1. Assignment / Subscriptions extend Assignor.Assignment / Subscriptions interface AND extend Struct.
2. The Schema object of Assignment / Subscriptions will be relying on CONSUMER_PROTOCOL_HEADER_SCHEMA, ASSIGNMENT_V0 and SUBSCRIPTION_V0 (maybe we can move them to a ConsumerProtocol sub-class inside AbstractPartitionAssignor).
3. Use Schema.write / read to serialize / deserialize the Assignment / Subscriptions.
4. Remove GenericType class and subscriptionSchema / assignmentSchema functions.

Is there any blockers for that?
"
42174780,165,hachikuji,2015-10-15T20:20:19Z,"@guozhangwang  Wouldn't it be a little restrictive to require all assignor implementations to use Kafka structures for serialization?
"
42175026,165,ewencp,2015-10-15T20:22:18Z,"@guozhangwang Isn't the drawback with that is that you _must_ use Kafka's struct then? That seems pretty inconvenient if there's a chance your data format will change but you can guarantee compatibility, e.g. a resource-based assignor is likely to evolve the set of configs it has over time.

In particular, I think this would mean that if you wanted to make any change that could be handled compatibly by your code but that doesn't fit into Struct's compatibility rules, you would have to change assignor name/sub protocol name, which also means going through the multiple-assignors + 2 rolling bounces upgrade process.
"
42175057,165,guozhangwang,2015-10-15T20:22:33Z,"I feel it is general enough. What kind of serdes are not compatible with Kafka structures? 
"
42176271,165,hachikuji,2015-10-15T20:32:23Z,"@guozhangwang We might be able to make that work if we change the format to something like this:

```
Subscription => Version Topics UserData
  Version => Int16
  Topics => [String]
  UserData => Bytes
```

Then users can provide any metadata they need in the UserData field.
"
42178482,165,ewencp,2015-10-15T20:50:20Z,"What's the real benefit of requiring Kafka's structs and doing so directly? It has the drawback I mentioned and it also keeps you from being able to manually handle multiple versions if you wanted to (you could check the version number before trying to decode, but only if you have control over the deserialization process). It looks to me like it just saves a few lines of code for us, and _maybe_ is a bit simpler for implementers of assignors since GenericType will be removed.
"
42179615,165,hachikuji,2015-10-15T20:58:41Z,"@ewencp One benefit of having a common serialization for subscriptions/assignment is that admin tooling can then depend on it. I think it might not be too bad if we implemented the UserData suggestion above. Then we could actually keep the struct serialization hidden from users and give them control over UserData, which lets them do whatever they want.
"
42180627,165,guozhangwang,2015-10-15T21:07:42Z,"Could this function ever be triggered? RequestFutureCompletionHandler only have the onComplete() API which will always trigger fireSuccess().
"
42181904,165,ewencp,2015-10-15T21:18:59Z,"@hachikuji I kind of buy that argument, but that tooling can't do anything useful with Kafka structs that it doesn't already know the format of since the schema is only recorded in code. It might make writing the tooling simpler since it only has to deal with one format, but with Kafka structs I don't think it actually enables anything that you couldn't get even with the more general form.

I'm fine if we go with requiring Schema/Struct, I'm mostly wary because Copycat is relying on this as well and I don't think we have as clear an idea of the exact requirements there as we do with a lot of the assignors -- right now its really simple, just the config topic offset that the worker is currently on, but I'm not sure what else might eventually make it in there. I personally like having the possibility to make certain incompatible schema changes without requiring config changes + rolling bounces (i.e. the possibility to keep those changes seamless for the user).
"
42182802,165,hachikuji,2015-10-15T21:26:49Z,"@ewencp This makes me want to add back the ProtocolType field in the JoinGroup request. Then if the protocol type is ""consumer,"" tools would be able to parse metadata directly by using ConsumerProtocol. They wouldn't be able to touch UserData, but that seems reasonable.
"
42185251,165,hachikuji,2015-10-15T21:48:56Z,"ConsumerNetworkClient does fail some requests before they are sent, but I don't think DisconnectException is possible.
"
42189757,165,hachikuji,2015-10-15T22:34:12Z,"Ok, so here's a summary of the changes I'm suggesting:

1. Change consumer metadata schema to the following:

```
Subscription => Version Topics UserData
  Version => Int16
  Topics => [String]
  UserData => Bytes
```

Assignment stays as it currently is (are there any cases where an assignor implementation would need to propagate additional information in the assignment)?

2. Migrate the subscription/assignment schemas currently in AbstractPartitionAssignor back into ConsumerProtocol.

3. Change the PartitionAssignor interface to look something like this:

``` java
interface PartitionAssignor {
  Subscription subscription(List<String> topics);
  Map<String, List<TopicPartition>> assign(Cluster metadata, Map<String, Subscription> subscriptions);

  class Subscription {
    ByteBuffer userData;
    List<String> topics;
  }
}
```

4. Add ProtocolType field to JoinGroup request:

```
JoinGroupRequest => GroupId SessionTimeout MemberId ProtocolType Protocols
  GroupId => String
  SessionTimeout => int32
  MemberId => String
  ProtocolType => String
  Protocols => [Protocol MemberMetadata]
    Protocol => String
    MemberMetadata => Bytes
```

For the consumer, the protocol type will be ""consumer"" and this will allow tools to use ConsumerProtocol to parse subscription/assignment metadata (assuming we will eventually expose it in DescribeGroup or something similar).

@guozhangwang @ewencp What do you think?
"
42190485,165,ewencp,2015-10-15T22:42:19Z,"So then does that include the changes @guozhangwang was talking about? The basic approach sounds fine, and the organization is similar to how I updated the Copycat code when I rebased onto this code -- I kept a CopycatProtocol class, though it ended up just holding a few static methods + inner classes.

But this is a solution for making generic tooling for consumers possible. You could do all of this and still restrict the format to Kafka Structs as @guozhangwang proposed, right?
"
42193354,165,guozhangwang,2015-10-15T23:15:10Z,"Is this class needed?
"
42195737,165,guozhangwang,2015-10-15T23:49:11Z,"@hachikuji  I was originally mainly think about code simplicity, not about tooling connivence much. I am wondering for CopyCat, is a topics field always necessary and representative of the resource partitions?

I get @ewencp 's point that enforcing the whole protocol to be restricted to Kafka structures would be less flexible in terms of compatibility, and if we only do that by extracting `topics` from subscriptions while keep `userData` and  `assignment` still generic types then it does not buy us much regarding code simplicity either. So I am now OK with the current approach of keeping `GenericType`. The only other comment is as below: we probably can save the AbstractPartitionAssignor as its only `assign` function is more like a helper function, hence putting the schemas in a `ConsumerProtocol` class along with the helper functions, and only keep one `PartitionAssignor` interface for users to instantiate.
"
42195884,165,hachikuji,2015-10-15T23:51:18Z,"@guozhangwang My suggestion is limited in scope to the new consumer. Copycat can still implement metadata however it wants.
"
42197214,165,ewencp,2015-10-16T00:11:39Z,"Following up, I got a clearer explanation from @hachikuji offline. The change is specific to the consumer groups, Copycat still has control over its serialization via its AbstractCoordinator subclass since that class just deals with `ByeBuffer`s. So I think this plan with the extra user data makes sense.
"
42197590,165,guozhangwang,2015-10-16T00:17:50Z,"Talked to @hachikuji offline, my previous understanding was incorrect that other services like CopyCat and KafkaStreams also need to implement `PartitionAssingor`, which is actually only gonna be used by the new consumer. Hence for others they can always decide to implement their only schemas that is not restricted to Kafka Schema / Struct.
"
42278727,165,guozhangwang,2015-10-16T19:07:24Z,"We can remove the package prefix.
"
42281759,165,hachikuji,2015-10-16T19:40:04Z,"Yes, that's right, since the generation is required, I think it should be ok. Of course this implies that the coordinator keeps sync state around while in the Stable state. One thing we've discussed is allowing the coordinator to discard this state some time after the group becomes Stable (maybe after one session timeout). This would reduce the memory footprint of the group on the broker in the steady state. This is probably worth looking at once this patch is out of the way.
"
42284440,165,guozhangwang,2015-10-16T20:08:39Z,"Totally agree.
"
42426323,165,guozhangwang,2015-10-19T21:10:14Z,"Could you also update the comments here according to the discussed FSM? Basically we only need the responding logic for each event within a state, and the transit logic between across states.
"
42427091,165,guozhangwang,2015-10-19T21:17:31Z,"I think we said in this case we will also return REBALANCE_IN_PROGRESS, and hence we can remove UNKNOWN_MEMBER_ID from possible error codes in SyncResponse?
"
42427602,165,guozhangwang,2015-10-19T21:21:52Z,"In the current protocol the leader should never submit the sync request twice, or in otherwords submit a sync group request without receiving a join group response; and if they do it would be due to a re-send upon ack failed, etc. Hence I think in the Stable state we should not distinguish leader with others any more and just treat everyone as the same.
"
42428212,165,guozhangwang,2015-10-19T21:27:19Z,"I think it's better the follow the same pattern as we do in handleXXX here, e.g. move the checking such as `rebalancePurgatory.checkAndComplete()` if the state is in `RepareRebalance``, or``maybePrepareRebalance``if state is in``AwaitingSync``or``Stable``.

Also with the current implementation we will transit from AwaitingSync to PrepareRebalance if any of the members failed, but with my proposed protocol we could only trigger that transition if the leader fails. Is there any problem doing this?
"
42428592,165,guozhangwang,2015-10-19T21:30:37Z,"I forgot to add this event in the FSM before, it should be treated as similar to onConsumerFailure.
"
42429156,165,guozhangwang,2015-10-19T21:36:07Z,"Move this condition after the first one as we want to send REBALANCE_IN_PROGRESS before ILLEGAL_GENERATION: they may not cause difference for now but just be careful for future updates. Or just use `match case` on states and only do generation / member-id checks for `Stable` state.
"
42429312,165,guozhangwang,2015-10-19T21:37:34Z,"If the state is in `PrepareRebalance` we can accept if generation is from the previous one?
"
42429493,165,guozhangwang,2015-10-19T21:39:13Z,"nit: onExpireRestablaize.
"
42429521,165,guozhangwang,2015-10-19T21:39:31Z,"nit: onExpireHeartbeat.
"
42429678,165,guozhangwang,2015-10-19T21:40:58Z,"Also according to what we discussed offline, maybe we can rename ""Restabilize"" to ""Join"" since it is only for the first phase of rebalance.
"
42437210,165,hachikuji,2015-10-19T23:02:31Z,"I think that is right. The generation is incremented after the join completes, so we allow commits from the current generation in PrepareRebalance.
"
42437479,165,guozhangwang,2015-10-19T23:06:06Z,"Ah right, my bad.
"
42469221,165,onurkaraman,2015-10-20T08:45:08Z,"It may be more intuitive to return UNKNOWN_MEMBER_ID when group == null.
"
42531198,165,guozhangwang,2015-10-20T18:04:34Z,"I think I agree, it is also for consistency with HB and Leave requests.
"
42575975,165,becketqin,2015-10-21T01:50:05Z,"Should this be getConfiguredInstances()?
"
42576026,165,becketqin,2015-10-21T01:51:12Z,"Minor, Kafka convention is not using brackets for single line statement.
"
42576335,165,becketqin,2015-10-21T01:58:16Z,"Can we put some of the methods in a separate util class?
"
42576376,165,guozhangwang,2015-10-21T01:59:01Z,"Currently our configs are still going to pass-in a single partitioner which is then be used as a singleton list, hence we use `getConfiguredInstance` here.
"
42577420,165,hachikuji,2015-10-21T02:20:40Z,"I think @becketqin might be right. Providing multiple instances is to support changes in a rolling upgrade scenario.
"
42578537,165,becketqin,2015-10-21T02:44:19Z,"The name is a little bit misleading here. When rebalance occurs, no one actually leaves the group, right? Also, according to the comments for this method: _Invoked when the group is left (whether because of shutdown, metadata change, stale generation, etc.)_ At very least it looks metadata change actually will not kick anyone out of the group, but only trigger a rebalance.
"
42578745,165,becketqin,2015-10-21T02:47:22Z,"Do we have any use case in mind for this method? It looks somewhat overlapping with ConsumerRebalanceListener.onPartitionsAssigned().
"
42578814,165,becketqin,2015-10-21T02:49:10Z,"Empty comments.
"
42578959,165,hachikuji,2015-10-21T02:52:02Z,"Two potential use cases I know of. One is sticky partitioning where the partitioner sends the old assignment in the subscription for the next round so that the leader can keep partition movement minimal. The other is for kafka streams (@guozhangwang knows more about this use case).
"
42642563,165,hachikuji,2015-10-21T15:52:10Z,"Yeah, that's a fair point. Perhaps beforeRejoin would be more accurate?
"
42643752,165,hachikuji,2015-10-21T16:01:01Z,"Actually I don't think it's too inaccurate since you are leaving the generation. Perhaps onLeaveGeneration?
"
42653027,165,becketqin,2015-10-21T17:15:33Z,"I'm not sure how it would work for sticky partition case. To make the leader honor the sticky partition (move as less partition as possible), the current leader needs to know the global assignment from the previous rebalance. But here we only have the assignment of this particular consumer. 
If we want to have sticky partition assignor perhaps we can let every consumer include their current assignment in the JoinGroupRequest and let the leader parse them.
"
42653813,165,hachikuji,2015-10-21T17:21:38Z,"The idea is to have the local assignment returned in onAssignment included in the user data of the next subscription. All subscriptions are forwarded to the leader, so it would be able to use the previous assignments found in the member's subscription userData to compute the next assignments.
"
42654165,165,guozhangwang,2015-10-21T17:24:16Z,"`onAssignment` is only triggered after the syncing phase by everyone, not only by the leader. It is different from `ConsumerRebalanceListener.onPartitionsAssigned()` such that the latter only gives the list of partitions, while the former contains extra userData that can be associated with the assigned partitions. For Kafka Streams it is important to use this userData to infer the further mapping from consumer's partitions to task's partitions.
"
42683684,165,becketqin,2015-10-21T21:18:44Z,"@hachikuji I kind of think we should be careful about those terminologies. Otherwise we can easily confuse people. Another example is `doSync` and `performSync`, from the name it is hard to tell the difference. I suggest we do the following for AbstractCoordinator:
1. rename `doSync` to `generateAssignments`, and there is no need to pass in `leaderId` because it is always going to be itself.
2. separate `performSync` to `onElectedAsGroupLeader` and `onEelectedAsGroupMemeber`. We can invoke them in JoinGroupResponseHandler.handle()
3. rename `onLeave` to `onGroupJoinStart`
4. rename `onJoin` to `onGroupJoinFinish`
5. rename `sendJoinGroupRequest` to `performGroupJoin`

So the sequence becomes:
1. onGroupJoinStart
2. performGroupJoin
3. onGroupJoinFinish

In performGroupJoin():
1. sendJoinGroupRequest
2. JoinGroupResponseHandler.handle()
    \* onAssignedAsGroupLeader (Only valid for leader)
      \* generateAssignments
      \* sendSyncGroupRequest() with assignments
OR
    \* onAssignedAsGroupMember (Only valid for member)
      \* sendSyncGroupRequest
1. SyncGroupResponseHandler.handle()
   - getPartitionAssignment and complete the future.

Except for that, the client side code structure looks good to me. I will continue to review the server side code.
"
42685152,165,becketqin,2015-10-21T21:30:55Z,"@guozhangwang @hachikuji Got it. Thanks for the explanation.
"
42689244,165,hachikuji,2015-10-21T22:05:54Z,"@becketqin Thanks for the suggestions! A few notes:
1. I agree with renaming doSync. Initially I was thinking of this phase as ""state synchronization,"" but in the end, it seemed that the assignment terminology was easier to understand, so I did some renaming, but didn't catch all. I think, however, that leaderId is needed in the general case because the member doesn't know its own id (the copycat patch is already depending on this).
2. Yeah, we can do this. Maybe more concisely, we can use `onBecomeLeader` and `onBecomeFollower`?
3. This suggestion is equally confusing in my mind. It seems more natural to phrase this in terms of the generation. Instead of `onLeave`, my suggestion would be `onGenerationEnd`.
4. Similarly, `onJoin` becomes `onGenerationBegin`.
5. I guess that's fair since it's not only sending the join group.
"
42718108,165,becketqin,2015-10-22T07:27:54Z,"@hachikuji About (3) and (4), I think the confusion comes from we are reusing the term **Join** here while we already have a JoinGroupRequest. But I am also a little concerned about exposing the concept of **generation**. **Generation** is a purely implementation detail so it would be nice not to expose that to user.

My original thinking was `onRebalanceBegin()` and `onRebalanceEnd()`. It is a precise description and avoids reusing JoinGroup, but it exposes the concept of **rebalance** to user. That was why I switched to `onGroupJoinBegin` and `onGroupJoinEnd` - because it is very easy for user to understand. People who overrides those two methods don't need to have ideas about internal details such as JoinGroupRequest, Generation, Rebalance, etc. They only need to know:
1. **onGropuJoinStart** - Do something before taking action to join/rejoin the group.
2. **generateAssignments** - Run the customized resource assigning algorithm.
3. **onGroupJoinFinish** - Do something after you joined the group

From user's point of view they are only doing a Group Join (it has nothing to do with JoinGroupReqeust). Group Join looks more intuitive than names related to rebalance or generation.

I just want to reason a little bit on the names. Because those method names are essentially public API that user will override, I feel making them easy to understand is important.
"
42779406,165,hachikuji,2015-10-22T17:35:35Z,"@becketqin Yep, totally agree on the importance of naming. This issue is clearly related to the discussion on KAFKA-2674. I think there are two ways we can approach this:
1. The callbacks are used to indicate phases of rebalance. There is an initial call when the rebalance begins and another call when the rebalance ends. Maybe we could change the names in this case to `onJoinBegin` and `onJoinFinish`.
2. The callbacks are used to indicate group membership. In this case, the first call would always be onJoin and every onJoin would be paired with a corresponding onLeave. In this case, onLeave would get invoked in close if there is an active group.

I think you are favoring 1., while the current RebalanceListener used by the consumer and the AbstractCoordinator is suggesting 2. to the user. Whichever we decide, we need to make the naming consistent. I actually don't have a strong preference either way, but I think it comes down to the following question: are there any cases where the onPartitionsRevoked() function is going to do anything different than what would be done on close(). If they always do the same thing, then we should probably make user's life easier and implement 2. If not, then we should keep 1.
"
42831631,165,becketqin,2015-10-23T04:03:50Z,"@hachikuji Good point about the correlation with rebalance listener methods. You are right, we should make them consistent. 

It looks we are trying to address two different kinds of programmers:
1. People who are using KafkaConsumer.
2. People who are developing their clients(producer/consumer/processor, etc) and extending AbstractCoordinator.

Majority of the programmers belongs to (1). What they need to provides today are:
- ConsumerRebalanceListener
  - `onParitionsRevoked()`
  - `onParitionsAssigned()`
- PartitionAssignor
  - `assign()`
  - `onAssignment()`
  - `subscription()`

The programmers in case (2) need to think about:
- `onGroupJoinStart()`
- `generateAssignment()`
- `onGroupJoinEnd()`

To make everything consistent, perhaps we can change the ConsumerRebalanceListener interface to:
- ConsumerRebalanceListener
  - `onGroupJoinStart()`
  - `onGroupJoinEnd()`

As said in KAFKA-2674, my only concern about this approach is that if later on we want to add a `beforeCommittingOffset()` to rebalance listener, that's going to be a bit weird. But there is always a workaround to turn off auto commit and let user have full control over the state when `onGroupJoinStart` is called. So I guess this is less a problem.

Does this approach make sense?

BTW, WRT **leave**, here is my understandings about onLeave() if we have one.
If we have an onLeave(), it should only be called when a consumer has been kicked out of the group, i.e. its generation id is no longer valid. Otherwise, the consumer is technically still in the group. That means onLeave() should not be called during a normal rebalance because the generation id is still valid.
"
42897321,165,hachikuji,2015-10-23T18:21:35Z,"Hey @becketqin, can you have a look at #354? Maybe we can continue discussion there? My current position is basically to make the naming consistent with the implementation. Right now, the implementation of onLeave is actually closer to onGroupJoinStart (or onJoinPrepare as I named it in that patch). I think it's worth discussing whether onLeave would be more useful in practice, but we need to think it through. It seems to me like onJoin and onLeave, if implemented according to their naming, would be a more intuitive paradigm for developers to build off of, but it is a little inconsistent with prior usage of rebalance callbacks.
"
43426682,165,becketqin,2015-10-29T18:35:45Z,"What would happen if coordinator shuts down after this test passes?
"
43427203,165,becketqin,2015-10-29T18:39:49Z,"Minor, it might be slightly cleaner if we throw exception when some error occurs and catch the exception in handleJoinGroup() and call responseCallback.
We might also want to add error log message when exception is thrown.
"
43428250,165,becketqin,2015-10-29T18:48:07Z,"Do we need to pass in member.memberId? Can we simply pass in member?
"
43442917,165,hachikuji,2015-10-29T20:54:28Z,"Yep, good point.
"
43444426,165,hachikuji,2015-10-29T21:07:18Z,"Nod, I was following the pattern of the other handlers. Throwing exceptions might let us remove a little boilerplate, but either approach works for me. And I agree on logging.
"
43449219,165,becketqin,2015-10-29T21:50:26Z,"It seems fine. We always shutdown SocketServer and KafkaApis before shutting down coordinator.
"
43451430,165,becketqin,2015-10-29T22:12:19Z,"The comments here seems not accurate.
"
43457097,165,becketqin,2015-10-29T23:24:31Z,"What if the leader send SyncGroupRequest late? Should we start counting down after we send SyncGroupResponse because the consumer only starts to heartbeat after SyncGroupResponse is received.
"
43664832,165,hachikuji,2015-11-02T18:50:00Z,"Actually I think it's right. If the follower's metadata is different, then we force a rebalance.
"
43665262,165,hachikuji,2015-11-02T18:53:14Z,"We start the clock after join group completion. If the leader fails to send SyncGroup within one session timeout after the join group phase has completed, then we will transition to preparing rebalance (I think I have a test case for this). If the leader sends SyncGroup after this transition, then it will be given a REBALANCE_IN_PROGRESS error.
"
1806596264,17539,apoorvmittal10,2024-10-18T14:25:57Z,"Question for my understanding: Do we need to this calculation? As I can see fetch params already has minBytes in request to replica manager hence isn't the response from replica manager should be empty if minBytes criteria is not satisfied?

So the question arise that how do we differentiate between empty reponse from replica manager log read, if that's beacus of min bytes or there is no data in the log? In either case we should continue holding the request in purgatory? Wdyt? "
1806619158,17539,adixitconfluent,2024-10-18T14:39:49Z,"Hi @apoorvmittal10 , IIUC, minBytes is utilized in `replicaManager.fetchMessages` functionality [here](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1729) not in `replicaManager.readFromLog`. The way it calculates the `accumulatedBytes` is the same way I have done it in my code ([original code reference](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaManager.scala#L1718)). I don't see the usage of `params.minBytes` in `readFromLog` functionality"
1806639708,17539,apoorvmittal10,2024-10-18T14:53:04Z,"I think you are right. I also see only reference of param minBytes in fetchMessages and not in readFromLog. Also the readFromLog says upto maximum in description and nothing about minBytes.

Then the PR change sounds good but I was wondering why do we accept complete fetchParams in readFromLog when we don't utilize something like minBytes there. Not sure if we should have minBytes support in readFromLog itself. Maybe out of scope of this PR.

@junrao can help is with more context."
1806669871,17539,apoorvmittal10,2024-10-18T15:15:54Z,"Hmmm, is this same for regular fetch operations as well?"
1806671945,17539,apoorvmittal10,2024-10-18T15:17:28Z,And why don't we want to release the partition locks from onComplete?
1806680209,17539,apoorvmittal10,2024-10-18T15:24:01Z,"In most scenarios the request might have minBytes, hence do you always want to initialize a hash map? Mostly it will be overriden with `responseData` map. So can't it be null? Moreover can't it be simpy a boolean variable i.e. 

boolean minBytesSatisfied = false

if (accumulatedBytes.get() >= shareFetchData.fetchParams().minBytes)
                replicaManagerFetchSatisfyingMinBytes = responseData;
=>
if (accumulatedBytes.get() >= shareFetchData.fetchParams().minBytes)
                minBytesSatisfied = true;

if (replicaManagerFetchSatisfyingMinBytes.isEmpty() && !hasRequestTimedOut) { 
=>
if (!minBytesSatisfied && !hasRequestTimedOut) {

return replicaManagerFetchSatisfyingMinBytes;
=>
return Collections.emptyMap()
"
1806682789,17539,apoorvmittal10,2024-10-18T15:26:07Z,Now we can come to this code path getting no result from `replicaManagerFetchData`. Hence is the log line still correct?
1806685722,17539,apoorvmittal10,2024-10-18T15:28:25Z,"Do you need this extra variable or can just write later, if needed? And then no need of else block below.

replicaManagerFetchDataFromTryComplete = replicaManagerFetchData(topicPartitionData, true);"
1806689153,17539,apoorvmittal10,2024-10-18T15:31:10Z,"fetchResponseData can still be empty, though processFetchResponse handles the empty check, is it intended? Though no harm, just checking with you."
1806690387,17539,apoorvmittal10,2024-10-18T15:32:10Z,What about if partitions were locked but no response in data aarived then will the lock be correctly released?
1806716182,17539,adixitconfluent,2024-10-18T15:53:21Z,"in this case, we want to call `ShareFetchUtils.processFetchResponse(shareFetchData, fetchResponseData, sharePartitionManager, replicaManager)` before we want to release the locks. That part is in `onComplete`, hence we don't release the lock"
1806722986,17539,adixitconfluent,2024-10-18T15:59:04Z,"yeah, it makes sense. I'll make the change."
1806725239,17539,adixitconfluent,2024-10-18T16:01:03Z,"Now that I think again,  I should return a map with key as topic partition and value as `fetchPartitionData` object containing 0 records and since we have not been able to satisfy all the fetch request criterias. @junrao your thoughts?"
1806729605,17539,adixitconfluent,2024-10-18T16:04:48Z,"yeah, since `processFetchResponse` can handle it, that's why I didn't add any check here"
1806735371,17539,adixitconfluent,2024-10-18T16:10:17Z,"for that case, even when the data is not received from replica manager, the fetchResponseData should still have keys as the locked topic partitions and values as empty data, so it should work. Am I wrong in that understanding?"
1807312461,17539,adixitconfluent,2024-10-19T12:35:26Z,"I can get rid of it, but then the variable name `replicaManagerFetchDataFromTryComplete` won't make sense if it is getting some values in `onComplete`. I just feel that the code is more readable this way."
1807413812,17539,adixitconfluent,2024-10-19T16:31:47Z,"Hi @apoorvmittal10, I went through the code for onComplete of `DelayedFetch` ([code reference](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/DelayedFetch.scala#L176)), it seems that it returns whatever it gets from `replicaManager.readFromLog`, so the current code is correct in that case. Please let me know if I am wrong. cc - @junrao "
1809360360,17539,apoorvmittal10,2024-10-21T19:07:23Z,Should it be AtomicLong?
1809365166,17539,apoorvmittal10,2024-10-21T19:10:32Z,"info seems to be too much here, how helpful this log would be in info mode? Can we move it to debug please."
1809366048,17539,apoorvmittal10,2024-10-21T19:11:07Z,Wouldn't this be too common? Should we move it trace?
1809378335,17539,apoorvmittal10,2024-10-21T19:17:15Z,Will it not depend on `readFromLog` API response i.e. if you sent 3 partitions then is it guaranteed that replica manager will return all 3 partitions in response?
1809417014,17539,apoorvmittal10,2024-10-21T19:44:11Z,Should we maintain the order by `LinkedHashMap` as earlier?
1809422393,17539,adixitconfluent,2024-10-21T19:47:48Z,"Reading `readFromLog` functionality, I don't see an indication where it doesn't return all the partitions sent to it, however I'll still make the change to use `topicPartitionData` to be on the safe side."
1809669520,17539,junrao,2024-10-21T23:53:47Z,"1. This approach is ok, but probably not the most efficient. `replicaManager.readFromLog` is relatively expensive. To avoid calling it on every HWM change, the DelayedFetch maintains the file position of the fetch offset and compares it with the file position of HWM to estimate the fetchable bytes. We could potentially do the same thing here. This requires us to maintain the file position for SharePartition.endOffset.
2. Ideally, we need to take into account the size of those non-acquirable batches in SharePartition when estimating the fetchable bytes."
1810373760,17539,apoorvmittal10,2024-10-22T09:40:25Z,"Thanks a lot for suggestion @junrao. This is good. 

I have aquestion on 2.
> Ideally, we need to take into account the size of those non-acquirable batches in SharePartition when estimating the fetchable bytes.

Though it's an ideal solution but the next fetch offset being prior to endOffset should be rare i.e. when some records are released or timedout. So I think we can avoid calculating non-acquirable and proceed to fetch anyways if our criteria from end offset to HWM meets. We can have the min bytes check after the fetch as currently in the PR. Wdyt?"
1810810655,17539,adixitconfluent,2024-10-22T14:11:40Z,"Hi @junrao , agreed this is a much better approach. Just one clarification - 
> This requires us to maintain the file position for SharePartition.endOffset

You mean the file position of the latest offset that was fetched for the share partition, right?"
1811067709,17539,junrao,2024-10-22T16:44:46Z,"> Though it's an ideal solution but the next fetch offset being prior to endOffset should be rare i.e. when some records are released or timedout. So I think we can avoid calculating non-acquirable and proceed to fetch anyways if our criteria from end offset to HWM meets. We can have the min bytes check after the fetch as currently in the PR. Wdyt?

Yes, this sounds reasonable. We can just ignore the non-acquirable records for now.

> You mean the file position of the latest offset that was fetched for the share partition, right?

Basically, we need to maintain endOffset as a `LogOffsetMetadata`, which contains segment position. We can then use `LogOffsetMetadata.positionDiff` to calculate the available bytes."
1812510188,17539,apoorvmittal10,2024-10-23T11:13:22Z,Can it reside in share module under `fetch`?
1812512229,17539,apoorvmittal10,2024-10-23T11:14:53Z,"Isn't by default it will be null, do we need to define it here?"
1812513907,17539,apoorvmittal10,2024-10-23T11:16:09Z,"You made a new class for this, shouldn't we pass that here?"
1812514360,17539,apoorvmittal10,2024-10-23T11:16:29Z,Is it need to be default scoped?
1812516782,17539,apoorvmittal10,2024-10-23T11:18:26Z,"So in a fetch result of 10 batches we should store the last batch info, is that correct? If yes then shouldn't name of variable be appropriately defined?"
1812517165,17539,apoorvmittal10,2024-10-23T11:18:45Z,Merge with previous line.
1812518313,17539,apoorvmittal10,2024-10-23T11:19:39Z,Why do we need this?
1812520317,17539,apoorvmittal10,2024-10-23T11:21:10Z,Will this log line print anything meaningful? I don't see toString in `FetchPartitionData`. Should we return what's required here?
1812521637,17539,apoorvmittal10,2024-10-23T11:21:49Z,Should it be default scoped? If used for tests then please write // Visible for testing.
1812523220,17539,apoorvmittal10,2024-10-23T11:22:45Z,How the exception will be handled with this method? Do we have a test case when exception is thrown by this replica manager call?
1812747750,17539,adixitconfluent,2024-10-23T13:20:10Z,I've moved it to protected. It cannot be private since it is utilized in `DelayedShareFetch`
1812784514,17539,adixitconfluent,2024-10-23T13:36:51Z,"we store the last fetched offset's properties in `LogOffsetMetadata` class like the message offset, file position etc. Hence, naming it `latestFetchOffsetMetadata` makes more sense than `latestFetchBatchMetadata`. Wdyt?"
1812833979,17539,AndrewJSchofield,2024-10-23T13:56:54Z,"nit: I hesitate to correct your Greek, but the singular of ""criteria"" is ""criterion"". Maybe ""isMinBytesSatisfied"" would be simpler than worrying about Greek grammar."
1812914606,17539,adixitconfluent,2024-10-23T14:33:31Z,I've added the handling and test case for this in my latest commit
1813543008,17539,junrao,2024-10-23T20:50:30Z,"We need to check if the two offset metadata are on the same segment first before using `positionDiff`. Also, we need to handle the case when HWM doesn't have offset metadata. We can just follow the logic in `DelayedFetch`. "
1813549471,17539,junrao,2024-10-23T20:53:30Z,We need to check `shareFetchData.fetchParams().isolation` to decide whether to use HWM or lastStableOffset.
1813657687,17539,junrao,2024-10-23T21:31:18Z,"It's kind of late to set the offset metadata here since we will acquire the fetch records and move the next fetch offset. This means the cached offset metadata doesn't match the next fetch offset.

I was thinking of doing the following. In `tryComplete`, if the offset metadata doesn't exist, we call `replicaManager.readFromLog` to populate the offset metadata. We can then proceed with the minByte estimation. If SharePartition.endOffset moves, we invalidate the offset metadata."
1815509020,17539,junrao,2024-10-24T18:07:15Z,"The `acquire` call always comes after `DelayedShareFetch.tryComplete`, which already updates  `latestFetchOffsetMetadata`. So, it seems that we don't need to update `latestFetchOffsetMetadata` in `acquire`?"
1815510878,17539,junrao,2024-10-24T18:09:01Z,We need to reset `latestFetchOffsetMetadata` every time `endOffset` changes.
1815515373,17539,junrao,2024-10-24T18:12:55Z,"It's possible that `maybeUpdateFetchOffsetMetadataForTopicPartitions` calls `readFromLog`, which returns enough bytes. In that case, it's more efficient to reuse the fetched result instead of calling `readFromLog` again in `onComplete`."
1815520277,17539,junrao,2024-10-24T18:17:19Z,Perhaps it's clearer to make sharePartition.latestFetchOffsetMetadata() an Optional?
1815531648,17539,junrao,2024-10-24T18:27:27Z,"In DelayedFetch, if a partition no longer exists, we complete the operation immediately."
1815542651,17539,junrao,2024-10-24T18:37:09Z,Could this just be a `long`?
1815548051,17539,junrao,2024-10-24T18:42:00Z,"Hmm, we don't want to read all partitions if only one partition's offset metadata is missing, right?"
1815562399,17539,junrao,2024-10-24T18:55:14Z,"1. Perhaps ""We maintain the latest fetch offset metadata to estimate the minBytes requirement more efficiently.""? 
2. Also, could we keep it together with `endOffset` since they are related."
1815586235,17539,adixitconfluent,2024-10-24T19:18:47Z,"yeah, but in case there are multiple partitions for which offset metadata is missing, I thought that its better to make a single `readFromLog` call instead of multiple `readFromLog` calls. Thus, using all the topic partitions in one call itself. Wdyt?"
1815592778,17539,adixitconfluent,2024-10-24T19:25:07Z,"`updateLatestFetchOffsetMetadata()` is only called from `tryComplete` when we find that there is a share partition whose offset metadata is not present (because in that case only, we call `readFromLog`). However, for any other case, we can only update the `latestFetchOffsetMetadata` via `acquire` method only. Else we'll have to call `readFromLog` from `tryComplete` in all scenarios to directly update share partition's `latestFetchOffsetMetadata`. Anything wrong in that understanding?"
1815599413,17539,adixitconfluent,2024-10-24T19:31:09Z,"IIUC, everytime we fetch new records from `readFromLog`, I utilize the `LogOffsetMetadata` object from the response to update the `latestFetchOffsetMetadata` object while doing the acquire. Hence, this way I have the most recent fetched offset's metadata that I always update through `acquire` method. Most of the time, it is going to be the endOffset's metadata itself, as soon as new data is produced to the topic partition. Anything else that I need to do here?"
1815731593,17539,junrao,2024-10-24T21:50:37Z,"Hmm, if a previously acquired batch times out, SharePartition will change endOffset. This means that the cached latestFetchOffsetMetadata no longer matches the next fetch offset, right?"
1815739504,17539,junrao,2024-10-24T21:56:47Z,"Hmm, to me `acquire` seems to be the wrong place to update `latestFetchOffsetMetadata`. We update `latestFetchOffsetMetadata` with the fetch offset and then acquire a few batches after the fetch offset. This typically moves the next fetch offset, which makes `latestFetchOffsetMetadata` useless for the next fetch."
1815741519,17539,junrao,2024-10-24T21:59:31Z,Maybe we can first make a pass to collect all partitions missing offset metadata and then make a single `readFromLog` with those partitions?
1816504391,17539,apoorvmittal10,2024-10-25T11:20:03Z,Wouldn't `FetchOffsetMetadataUpdateResult` be a better name?
1816509367,17539,apoorvmittal10,2024-10-25T11:24:10Z,Do we need the variable name to have suffix `TryComplete`? I don't find the suffix is any helpful.
1816510686,17539,apoorvmittal10,2024-10-25T11:25:23Z,"Why to have method names with such suffix, are they helping?

```suggestion
    Map<TopicIdPartition, FetchPartitionOffsetData> combineLogReadResponse(Map<TopicIdPartition, FetchRequest.PartitionData> topicPartitionData) {
```"
1816519200,17539,apoorvmittal10,2024-10-25T11:33:01Z,@adixitconfluent Is it handled?
1816530978,17539,apoorvmittal10,2024-10-25T11:43:00Z,"How frequent it is to see missing fetch offset information, mostly at start only right? They why to initialize this variable, can't it be lazily loaded, if required?"
1816619673,17539,apoorvmittal10,2024-10-25T12:46:27Z,So the update should be safe with multiple threads as we have acquired the lock on share partition which guards us from 2 threads trying to update the offset metadata. But we should write comments on the share partition update method that the caller of the method should ensure that share partition fetch lock is acquired prior invoking the updateLatestFetchOffsetMetadata.
1816620754,17539,apoorvmittal10,2024-10-25T12:47:18Z,Can it be in a separate method i.e. divide methods.
1816621221,17539,apoorvmittal10,2024-10-25T12:47:39Z,Merge the lines.
1816657075,17539,apoorvmittal10,2024-10-25T13:09:30Z,"So this iteration will always be executed for every share fetch when the `missingFetchOffsetMetadataTopicPartitions` will rarely be true, only when a new SharePartition is created. Hence, I was thinking why not to have such update only on SharePartition initialization. Though I understand that current `readFromLog` API requires fethchParams but is there an API which can supply the LogOffsetMetadata when requested with topic partition and specific offset(start offset of share partition)? @junrao wdyt? "
1816661560,17539,apoorvmittal10,2024-10-25T13:12:17Z,Can it go in a method please.
1816667130,17539,apoorvmittal10,2024-10-25T13:16:10Z,Why do we satisfy the min byte criteria if share partition is null?
1816668262,17539,apoorvmittal10,2024-10-25T13:17:03Z,"Should the varibale in SharePartition be Optional? We always should have that, correct?"
1816670642,17539,apoorvmittal10,2024-10-25T13:18:48Z,"Sorry, I didn't understand when we can have the offset in share partition > partition end offset?"
1816678081,17539,apoorvmittal10,2024-10-25T13:22:04Z,Again this will be rare hence shall we delay initiliazing LinkedHashMap.
1816687484,17539,apoorvmittal10,2024-10-25T13:25:21Z,Can it be non-null and empty ever i.e. do you require your second condition?
1816691162,17539,apoorvmittal10,2024-10-25T13:27:47Z,"Can it ever occur that you have non-empty `logReadResponseFromTryComplete` but `topicPartitionData` came from fresh aquisition from line 98 (topicPartitionData = acquirablePartitions();). I think never, can you just write comments for this."
1816693860,17539,apoorvmittal10,2024-10-25T13:29:27Z,Please correct the alignment of comments.
1816695469,17539,apoorvmittal10,2024-10-25T13:30:31Z,Why it's optional?
1816703702,17539,apoorvmittal10,2024-10-25T13:35:20Z,So if we are fetching from offset 0 and gets response from log for 0-1000 offsets then `logResult.info().fetchOffsetMetadata` contains information for 0 offset or 1000th offset i.e. which offset metadata does it hold?
1816704806,17539,apoorvmittal10,2024-10-25T13:36:05Z,Class comments please.
1816705073,17539,apoorvmittal10,2024-10-25T13:36:17Z,Empty line break please.
1816711409,17539,apoorvmittal10,2024-10-25T13:40:35Z,That's my queustion with this comment https://github.com/apache/kafka/pull/17539#discussion_r1816703702. Do we ever get the lastFetchOffsetMetadata or we always update with fetchOffsetMetadata?
1816800887,17539,adixitconfluent,2024-10-25T14:34:41Z,I can confirm this will return information about 0th offset.
1816813489,17539,apoorvmittal10,2024-10-25T14:41:03Z,So this suggestion is not apt as we need the refreshed information https://github.com/apache/kafka/pull/17539#discussion_r1816800887
1816818571,17539,adixitconfluent,2024-10-25T14:44:21Z,"Hi @junrao , I understand your point now. Here's my proposed solution changes- 
1. Remove update of `latestFetchOffsetMetadata` via `acquire` method
2. In `onComplete`, after I complete the call of `ShareFetchUtils.processFetchResponse`(which internally completes the call of `acquire` method), I will do a `readFromLog` for `topicPartitionData` with their latest fetch offset and update the share partition's `latestFetchOffsetMetadata`. Note this call to `readFromLog` will always happen despite the `tryComplete` `readFromLog` happens or not.

Please let me know what you think of this approach.
"
1816827231,17539,adixitconfluent,2024-10-25T14:49:08Z,"Hi @junrao, understood, so if there is any calls to acknowledge/acquisition lock timeout/release acquired records on session close, I should update the latestFetchOffsetMetadata to `Optional.empty()`. Then when the next tryComplete call comes, it will update the `latestFetchOffsetMetadata` and we will have the most recent result. Or is my understanding incorrect?"
1816829826,17539,adixitconfluent,2024-10-25T14:50:57Z,"I did it because the value can be null, so we thought it would be better to keep it as Optional https://github.com/apache/kafka/pull/17539#discussion_r1815520277"
1816830605,17539,adixitconfluent,2024-10-25T14:51:32Z,"you're right, I don't need it."
1816832978,17539,adixitconfluent,2024-10-25T14:52:57Z,"not necessary, it can be null as well. Hence, we use optional here."
1816851362,17539,adixitconfluent,2024-10-25T15:02:51Z,"It might not be true ever in case we are using `FetchIsolation.HIGH_WATERMARK`, but I think it can be true in case we use `FetchIsolation.LOG_END` and `FetchIsolation.TXN_COMMITTED` which might be used in the future in share fetch requests."
1817193502,17539,junrao,2024-10-25T18:39:55Z,"I was thinking about an alternative approach by maintaining `latestFetchOffsetMetadata` every time we update `endOffset`. In the common case, we move `endOffset` to `lastOffset` of an acked batch. The file position in `latestFetchOffsetMetadata` can just be updated by adding the batch size to the file position of the previous `latestFetchOffsetMetadata`. This way, in the common case, we only need to call `readFromLog` once per fetch request, instead of twice (once for getting `latestFetchOffsetMetadata` and another for getting the data) in the current approach."
1817933790,17539,adixitconfluent,2024-10-26T21:36:55Z,"Hi @junrao , i am not sure if I understand the approach completely. IIUC, 
1. I agree with the common case where during `onComplete` we can update the file position by directly adding the batch size. But, there are cases where we return true from `isMinBytesSatisfied` if `fetchOffsetMetadata` was on a different segment than the `endOffsetMetadata` ([code reference](https://github.com/apache/kafka/pull/17539/files#diff-d835cdc01e77905316584ce9e6e21a060cb3d36efa717d4b822b16744e4d713aR296)). In those case, we don't know the number of bytes that got accumulated in the response. Same goes for the case `fetchOffsetMetadata.messageOffset > endOffsetMetadata.messageOffset`, how do we know whta is the number of bytes to add?
2. I am assuming that with this approach, we do not need to do any handling during acquisition lock timeout/acknowledgements/release acquired records on session close. Please correct me if I am wrong.
PS - Thanks a lot for taking out the time to explain me and reviewing this PR."
1817943465,17539,junrao,2024-10-26T22:31:36Z,"@adixitconfluent : Here is the rough idea.
1. If endOffset advances forward, we incrementally update its file position by the size of batches going forwarded.
2. The tricky thing is how the offset metadata picks up a new segment being rolled. As we increase the file position, endOffset will eventually reach the baseOffset of the next segment. This means that the next fetch request will be satisfied immediately since the HWM is on a different segment. When we acquire the data (for batches at the beginning of the segment), we can check if the offset metadata in the fetch data has the same offset as endOffset but on a different segment. If so, we update the offset segment of endOffset.
3. If the endOffset goes backward (due to timeout/acknowledgements/release) or endOffset is being initialized for the first time, we just call readFromLog to get the offset metadata.
4. `tryComplete` will have the same logic to deal with the uncommon cases where the offset metadata is not available or the offset metadata is on the same segment. The only difference is that it won't update latestFetchOffsetMetadata any more since the update happens when endOffset changes."
1818144589,17539,junrao,2024-10-27T17:18:30Z,"While this alternative approach is more efficient, it's probably also more complicated. So, it's also ok to just take the current approach to start with. In the current approach, (1) if any call moves `endOffset`, we reset the `latestFetchOffsetMetadata` to Optional.empty(). In `tryComplete`, if `latestFetchOffsetMetadata` is empty, we call `readFromLog` and update `latestFetchOffsetMetadata`."
1818157944,17539,adixitconfluent,2024-10-27T18:40:29Z,"Yes, it is handled here https://github.com/apache/kafka/pull/17539#discussion_r1816519200"
1818158187,17539,adixitconfluent,2024-10-27T18:41:37Z,The explanation is present in this conversation thread https://github.com/apache/kafka/pull/17539#discussion_r1815531648
1818159160,17539,adixitconfluent,2024-10-27T18:47:48Z,"Hi @junrao , agreed with the simpler approach. I have made the following changes in my latest commit- 
> In the current approach, (1) if any call moves endOffset, we reset the latestFetchOffsetMetadata to Optional.empty()

I reset the `latestFetchOffsetMetadata` to Optional.empty() if -
1. `acquire()` results in non-empty acquired records in `ShareFetchUtils`.
2. acquisition lock timeout is called.
3. release acquired records on session close is called.
I haven't made the change in `acknowledge()` method of `SharePartition`, since in the common case all the `ACQUIRED` records will moved to `ACKNOWLEDGED` state and endOffset doesn't change then.

> In tryComplete, if latestFetchOffsetMetadata is empty, we call readFromLog and update latestFetchOffsetMetadata

This functionality has been added in previous commits.

Please review my PR whenever you can. Thanks!
"
1819587266,17539,junrao,2024-10-28T18:52:02Z,Could we make fetchOffsetMetadata Optional instead of relying on `null`?
1819590569,17539,junrao,2024-10-28T18:54:40Z,This is getting a bit hard to track since we need to make this call in all places where endOffset changes. Could we have a method for updating both `endOffset` and `latestFetchOffsetMetadata`? Then we can replace all code that changes `endOffset` with this method.
1819592494,17539,junrao,2024-10-28T18:55:41Z,latestFetchOffsetMetadata => fetchOffsetMetadata?
1819610603,17539,junrao,2024-10-28T19:10:15Z,It's probably clearer to put those in an `else` clause?
1819618734,17539,junrao,2024-10-28T19:16:42Z,We could just initialize `missingFetchOffsetMetadataTopicPartitions` with `new LinkedHashMap<>()`. Ditto in `combineLogReadResponse`.
1819637938,17539,junrao,2024-10-28T19:27:56Z,"Hmm, it would be better for `logReadResponse` to only be empty, but never `null`."
1819645608,17539,junrao,2024-10-28T19:34:59Z,"Do we need this wrapper class `FetchPartitionOffsetData`? It seems that it's simpler for `readFromLog` to `return Map<TopicIdPartition, LogReadResult>`. We can then convert `LogReadResult` to `Map<TopicIdPartition, ShareFetchResponseData.PartitionData>` in `onComplete`."
1819664109,17539,junrao,2024-10-28T19:48:44Z,"Every passed in partition to `readFromLog` will be included in the response. So, there is no need to pass in both `missingFetchOffsetMetadataTopicPartitions` and `replicaManagerReadResponseData`. We do want to check the error code for each partition. In regular fetch, if any partition has an error code, we send a response immediately. We can just do the same here. "
1819665350,17539,junrao,2024-10-28T19:49:52Z,I thought we agreed that we want to send a response immediately if a sharePartition can't be found. Is that handled?
1819666995,17539,junrao,2024-10-28T19:51:27Z,Could we  just make a single `sharePartitionManager.sharePartition` call per `tryComplete` to avoid having to check null repeatedly?
1819670828,17539,junrao,2024-10-28T19:54:58Z,"Ideally, we need to handle the exception at the partition level in the caller."
1819677154,17539,junrao,2024-10-28T20:01:02Z,"This condition seems unnecessary. We need to set `logReadResponse` as long as `fetchOffsetMetadataUpdateResult.replicaManagerReadResponse` is not empty, right?"
1824747173,17539,adixitconfluent,2024-10-31T16:04:06Z,"Hi @junrao , I had updated it at a different place but now I've added that as the first step in tryComplete ([code reference](https://github.com/apache/kafka/pull/17539/files#diff-d835cdc01e77905316584ce9e6e21a060cb3d36efa717d4b822b16744e4d713aR151)). As you had also mentioned, this prevents us from making multiple share partition null checks at different places in the code."
1824903292,17539,junrao,2024-10-31T17:42:42Z,"We are still calling `sharePartitionManager.sharePartition` in multiple places (`anySharePartitionNoLongerExists`, `acquirablePartitions`, `isMinBytesSatisfied` and `maybeUpdateFetchOffsetMetadataForTopicPartitions`), each of which needs to handle null SharePartition since the sharePartition could disappear any time. I was thinking that we could get all sharePartitions once at the beginning and pass them around to other methods. This way, the null handling is only done once."
1824914771,17539,junrao,2024-10-31T17:52:44Z,maybeUpdateFetchOffsetMetadataForTopicPartitions => maybeReadFromLogAndUpdateFetchOffsetMetadata ?
1824931761,17539,junrao,2024-10-31T17:59:44Z,"Could we just return an empty map? This way, the caller doesn't need to do the null check."
1824937560,17539,junrao,2024-10-31T18:01:45Z,"topicPartitionDataFromTryComplete => partitionsToComplete ?
logReadResponse => partitionsAlreadyFetched ?"
1824959019,17539,junrao,2024-10-31T18:09:14Z,"This code can be a bit more concise.

```
topicPartitionData.forEach((topicIdPartition, partitionData) -> {
    if (!logReadResponse.containsKey(topicIdPartition)) {
        missingLogReadTopicPartitions.put(topicIdPartition, partitionData);
    }
});
```"
1826092331,17539,adixitconfluent,2024-11-01T17:19:18Z,"Hi @junrao, I have changed the exception handling to a top level exception handling in `tryComplete` to combat with this scenario."
1826157479,17539,junrao,2024-11-01T18:24:00Z,"There is no need to pass in `fetchOffsetMetadata` since it's always empty.

updateEndOffsetAndFetchOffsetMetadata => updateEndOffsetAndResetFetchOffsetMetadata?"
1826160028,17539,junrao,2024-11-01T18:26:20Z,Let's be consistent with the usage of `this`. Most other places don't use `this`. 
1826160780,17539,junrao,2024-11-01T18:27:06Z,All callers hold the lock. So we could remove the locking here and add a comment that the caller is expected to hold the lock when calling this method.
1826166921,17539,junrao,2024-11-01T18:33:29Z,This problem is still there?
1826169337,17539,junrao,2024-11-01T18:35:52Z,This check is unnecessary since partitionsAlreadyFetched initializes to empty.
1826174394,17539,junrao,2024-11-01T18:41:05Z,Should we reset `partitionsToComplete` and `partitionsAlreadyFetched` too when we release the locks?
1826180030,17539,junrao,2024-11-01T18:47:20Z,Should we just assign the return value to partitionsToComplete directly? We already acquired the locks for those partitions and partitionsToComplete is the only place to track them for releasing.
1826183958,17539,junrao,2024-11-01T18:51:35Z,"This code can be a bit simpler.

```
topicPartitionData.forEach((topicIdPartition, partitionData) -> {
    SharePartition sharePartition = sharePartitions.get(topicIdPartition);
    if (sharePartition.fetchOffsetMetadata().isEmpty()) {
        missingFetchOffsetMetadataTopicPartitions.put(topicIdPartition, partitionData);
    }
});
```"
1826185041,17539,junrao,2024-11-01T18:52:48Z,updateFetchOffsetMetadataForMissingTopicPartitions => updateFetchOffsetMetadata ?
1826187006,17539,junrao,2024-11-01T18:54:51Z,Perhaps do this in the caller? Then the purpose of the method is simpler and the method name can just be `maybeReadFromLog`.
1826561286,17539,adixitconfluent,2024-11-02T12:32:50Z,"Hi @junrao , we can do that but then once we are in the `else` of this check -` if (anyTopicIdPartitionHasLogReadError(replicaManagerReadResponse) || isMinBytesSatisfied(topicPartitionData))`, after we have release the partitions lock, we also need to do `partitionsToComplete.clear()`. Hence, I've tried to avoid doing this by  assigning `partitionsToComplete` once we are sure that we can do a `forceComplete()`"
1826562798,17539,adixitconfluent,2024-11-02T12:43:59Z,"Hi @junrao, I am not actually too sure if I definitely need this check. Ideally, I don't think there can be any value in `sharePartitions` which can be null, but this [TODO](https://github.com/apache/kafka/blob/trunk/core/src/main/java/kafka/server/share/SharePartitionManager.java#L629) in `SharePartitionManager` is confusing me, plus there is this JIRA https://issues.apache.org/jira/browse/KAFKA-17510 where we will be refactoring share partition initialization. So, this can act as a safety check for now, and I can remove this in a future PR once the refactor is complete, and we are sure we don't send null share partitions. What do you think?
cc - @apoorvmittal10 "
1828181101,17539,junrao,2024-11-04T18:14:18Z,"shareFetchData => partitionsToComplete ?
partitionsToComplete => partitionsAcquired ?"
1828190239,17539,junrao,2024-11-04T18:22:08Z,It's kind of weird for this method to return the input. It's more natural for this method to return nothing.
1828196542,17539,junrao,2024-11-04T18:26:13Z,"Hmm, when we hit an exception, do we guarantee that `partitionsToComplete` has been set?"
1828254033,17539,junrao,2024-11-04T19:14:00Z,"We need to return an optional `fetchOffsetMetadata` if the value returned from `nextFetchOffset` changes. If `findNextFetchOffset` is false, `nextFetchOffset` returns a value based on endOffset. This case is already covered in this PR. If `findNextFetchOffset` is true, `nextFetchOffset` returns a value not depending on endOffset. So, we should return empty here if `findNextFetchOffset` is true."
1828280371,17539,junrao,2024-11-04T19:36:40Z,"It seems that sharePartitions is always a subset of shareFetchData.partitionMaxBytes()? If that's the case, I agree that we don't need anySharePartitionNoLongerExists. However, it would be useful to make sure that the caller passes in sharePartitions and shareFetchData.partitionMaxBytes() with the same set of partition keys."
1828303188,17539,junrao,2024-11-04T19:55:05Z,This comment doesn't match the code.
1828307832,17539,junrao,2024-11-04T19:59:05Z,Quite a long name. How about sth like testTryCompleteReturnsFalseWhenMinBytesNotSatisfied?
1828314722,17539,junrao,2024-11-04T20:04:40Z,"Hmm, not sure how this test is different from the next one. If this is testing fetching for the first time, sp0.fetchOffsetMetadata() should return empty, right?"
1828321808,17539,junrao,2024-11-04T20:10:46Z,"Could we change `combineLogReadResponse` to also take `partitionsAlreadyFetched`? This way, we can get rid of `delayedShareFetch.updateLogReadResponse`."
1828532884,17539,apoorvmittal10,2024-11-04T23:28:40Z,We initialize the variable in constructor then re-assign while creating another LinkedHashMap in acquirablePartitions() method. Are we are initializing `partitionsToComplete` here to save null check? Can't we re-use already initialized LinkedHashMap()?
1828533813,17539,apoorvmittal10,2024-11-04T23:30:01Z,Again we reset `partitionsAlreadyFetched` to response from `replicaManagerReadResponse`. Why to have such instances created when anyways we have to re-assign?
1828538998,17539,apoorvmittal10,2024-11-04T23:37:46Z,"We have now passed `sharePartitions` map which contains topicIdPartition and sharePartition itself then why are we iterating on `shareFetchData.partitionMaxBytes().keySet()` and doing a null check, why not to iterate in `sharePartitions` map itself? Also make the map of `sharePartitions` in SPM as of type LinkedHashMap then."
1828539471,17539,apoorvmittal10,2024-11-04T23:38:32Z,Same elsewhere.
1828542474,17539,apoorvmittal10,2024-11-04T23:43:20Z,"Shouldn't we iterate on `sharePartitions` map passed in delayed share fetch which will guarantee that SharePartition cannot be null it can only be fenced (fenced handling has been separate), hence no null check is required. Also no `anySharePartitionNoLongerExists()` method call is required."
1828547060,17539,apoorvmittal10,2024-11-04T23:50:42Z,"nit: I personally find the code hard to read with nested if/else blocks, same is the case here. Though I leave it on you.

```
if (topicPartitionData.isEmpty()) {
     log.trace(""Can't acquire records for any partition in the share fetch request for group {}, member {}, "" +
                        ""topic partitions {}"", shareFetchData.groupId(), shareFetchData.memberId(),
                    shareFetchData.partitionMaxBytes().keySet());
    return false;
}

// In case, fetch offset metadata doesn't exist for one or more topic partitions, we do a
// replicaManager.readFromLog to populate the offset metadata and update the fetch offset metadata for
// those topic partitions.
Map<TopicIdPartition, LogReadResult> replicaManagerReadResponse = updateFetchOffsetMetadata(maybeReadFromLog(topicPartitionData));
if (!anyTopicIdPartitionHasLogReadError(replicaManagerReadResponse) && !isMinBytesSatisfied(topicPartitionData)) {
       log.debug(""minBytes is not satisfied for the share fetch request for group {}, member {}, "" +
                            ""topic partitions {}"", shareFetchData.groupId(), shareFetchData.memberId(),
       shareFetchData.partitionMaxBytes().keySet());
       releasePartitionLocks(topicPartitionData.keySet());
       return false;
}

partitionsToComplete = topicPartitionData;
partitionsAlreadyFetched = replicaManagerReadResponse;
....
....
"
1828550191,17539,apoorvmittal10,2024-11-04T23:56:01Z,There are 2 checks in the if condition (anyTopicIdPartitionHasLogReadError and isMinBytesSatisfied) but the log says that minBytes criteria is not satified. I this correct log statement?
1828552140,17539,apoorvmittal10,2024-11-04T23:59:15Z,We are re-assigning the already initalized variables in constructor. I would say we should have null check handling in `onComplete` rather creating resources which never gets utilized.
1828561128,17539,apoorvmittal10,2024-11-05T00:14:37Z,So did we not chose to implement https://github.com/apache/kafka/pull/17539/files#r1816530978 rather initialize with LinkedHashMap which will hardly be filled?
1828563750,17539,apoorvmittal10,2024-11-05T00:18:54Z,Shouldn't the name be `maybeUpdateFetchOffsetMetadata` as it depends on the log read result?
1828564729,17539,apoorvmittal10,2024-11-05T00:20:32Z,nit: Will `forEach` be more convenient here then you don't need to call entry.getKey and entry.getValue?
1828566030,17539,apoorvmittal10,2024-11-05T00:23:01Z,Isn't the log incorrect as it says the the log does not contain topicIdPartition rather the response exists but it errored. Also do you need to log `replicaManagerLogReadResult` or complete `replicaManagerReadResponseData`? Shouldn't we be logging former which corresponds to topic id partition?
1828571399,17539,apoorvmittal10,2024-11-05T00:31:55Z,Seems an incorrect error handling of release acquired partitions to me. Say line 155 acquires partitions and `topicPartitionData` is set. And we get an exception in `isMinBytesSatisfied` method (which anyways call getPartitionOrException method) or elsewhere then the locks released at line 193 will not release any locks as they are invoked on `partitionsToComplete` which is not yet set. Moreover if forceComplete call is successful then the acquired partitions will anyways not be released.
1828573589,17539,apoorvmittal10,2024-11-05T00:35:25Z,What @junrao meant was that it should not be a top level rather partition level error.
1828574063,17539,apoorvmittal10,2024-11-05T00:36:22Z,"```suggestion
        // extend it to support other FetchIsolation types.
```"
1828579315,17539,apoorvmittal10,2024-11-05T00:45:30Z,I understand there can't be concurrent calls to tryComplete but I didn't get this comment. Though forceComplete() cannot be called twice. But forceComplete() on expiration can be on different thread and tryComplete() on different (that's my understanding as per DelayedOperation code I have seen) can this change any behaviour here?
1828580776,17539,apoorvmittal10,2024-11-05T00:47:59Z,"Can be merged together.

```
 shareFetchData.future().complete(ShareFetchUtils.processFetchResponse(shareFetchData, fetchPartitionsData, sharePartitionManager, replicaManager));"
1828758874,17539,adixitconfluent,2024-11-05T05:45:48Z,"my bad, I've corrected it."
1828775451,17539,adixitconfluent,2024-11-05T06:04:52Z,"my bad, you're right, I've adjusted the mock to return `Optional.empty()` for the first time and `Optional.of(new LogOffsetMetadata(0, 1, 0))` for the second time (post `readFromLog` call)"
1828820632,17539,adixitconfluent,2024-11-05T06:55:13Z,"Hi @junrao, you're right, I've changed the line to `releasePartitionLocks(topicPartitionData.keySet())`"
1828824435,17539,adixitconfluent,2024-11-05T06:59:40Z,"Yes, there have been comments above where I left some variables as null, and it was pointed out that I need to initialize them to avoid null checks at different places, then we just need to do empty checks.. Hence, I've implemented it in this manner."
1828825732,17539,adixitconfluent,2024-11-05T07:01:05Z,"Same reason as above, we are avoiding any null checks. We just check for empty scenarios by doing this."
1828831006,17539,adixitconfluent,2024-11-05T07:06:21Z,"I have been asked to add else blocks in the above comments on this PR, hence I don' think I should change it again. https://github.com/apache/kafka/pull/17539#discussion_r1819610603"
1828836294,17539,adixitconfluent,2024-11-05T07:12:01Z,"Now, we reset fetchOffsetMetadata everytime the `endOffset` changes (see the usages of function `updateEndOffsetAndResetFetchOffsetMetadata` in `SharePartition`). So, it can be frequent now that the fetchOffsetMetadata is empty. https://github.com/apache/kafka/pull/17539#discussion_r1824931761"
1828845131,17539,adixitconfluent,2024-11-05T07:20:53Z,"Since, I am using `continue` in the loop, I prefer to do it using for instead of forEach."
1828852012,17539,adixitconfluent,2024-11-05T07:27:04Z,"yes, the log statement is correct because because `isMinBytesSatisfied` can run only if `anyTopicIdPartitionHasLogReadError` returns false. We should only check `isMinBytesSatisfied` if `anyTopicIdPartitionHasLogReadError` returns false. If `anyTopicIdPartitionHasLogReadError` return true, then we do a `forceComplete`."
1828853882,17539,adixitconfluent,2024-11-05T07:28:50Z,"my bad, you're right about both."
1828860825,17539,adixitconfluent,2024-11-05T07:35:27Z,"you're right, I've changed the line to `releasePartitionLocks(topicPartitionData.keySet())`. If the `forceComplete` is successful, then the partition locks are released from `onComplete` finally block and it doesn't concern here."
1828877645,17539,adixitconfluent,2024-11-05T07:50:46Z,"What I mean is that `partitionsAlreadyFetched` value can't be changed once we enter this point in `onComplete` via `forceComplete` either by expiration or by a `tryComplete` successful call. IIUC, `forceComplete` uses this `AtomicBoolean` variable `completed` which is used as locking mechanism for `forceComplete`. This should ensure atomicity of global variables between `tryComplete` and `forceComplete` we use in `DelayedShareFetch`. I'll change the comment to explain this better."
1828902626,17539,adixitconfluent,2024-11-05T08:10:20Z,"yes, I'll remove `anySharePartitionNoLongerExists()` and also iterate on `sharePartitions` rather than using `shareFetchData.partitionMaxBytes()`. Will remove the null checks from our code."
1829020183,17539,adixitconfluent,2024-11-05T09:31:37Z,"hey @apoorvmittal10 , we are doing partition level error handling in `onComplete` using `sharePartitionManager.handleFetchException`. When we get an exception in this line `replicaManager.getPartitionOrException`, we directly call `forceComplete` (mentioned in above comments), and that does a partition level handling."
1829235324,17539,apoorvmittal10,2024-11-05T12:03:46Z,"If the concern is with additional null check then I would recommend general helper methods. My concern is with creating additional maps when they are always re-referenced.

```
    private <K,V> void addToNullableMap(Map<K, V> map, K key, V value) {
        if (map == null) {
            map = new LinkedHashMap<>();
        }
        map.put(key, value);
    }

    private boolean isMapEmpty(Map<?,?> map) {
        return map == null || map.isEmpty();
    }"
1829237621,17539,apoorvmittal10,2024-11-05T12:05:33Z,I leave it on @junrao to decide then.
1829240438,17539,apoorvmittal10,2024-11-05T12:07:57Z,"It's missing yet, you can log a jira for me to fix as I am doing handling anyways."
1829256794,17539,adixitconfluent,2024-11-05T12:19:58Z,logged a JIRA https://issues.apache.org/jira/browse/KAFKA-17943 for the same.
1830198129,17539,junrao,2024-11-05T23:15:54Z,Why is this a LinkedHashMap instead of just a HashMap?
1830199934,17539,junrao,2024-11-05T23:18:33Z,"This is actually a super set of the partitions to complete. So, it's better to just keep the name shareFetchData. It would be useful to add a comment that the partitions to be completed are given by sharePartitions and is a subset of shareFetchData."
1830201006,17539,junrao,2024-11-05T23:20:05Z,Why is this a LinkedHashMap instead of just a HashMap?
1830203044,17539,junrao,2024-11-05T23:23:08Z,missingFetchOffsetMetadataTopicPartitions => partitionsMissingFetchOffsetMetadata?
1830208462,17539,junrao,2024-11-05T23:31:16Z,"This can be a bit simpler.

```
return replicaManagerReadResponse.values().stream()
    .anyMatch(logReadResult -> logReadResult.error().code() != Errors.NONE.code());
```"
1830209083,17539,junrao,2024-11-05T23:32:13Z,anyTopicIdPartitionHasLogReadError => anyPartitionHasLogReadError ?
1830230584,17539,junrao,2024-11-06T00:06:59Z,"This part of the exception is still quite confusing to me. If we hit the exception, should we release the locks before calling forceComplete()? Otherwise, `forceComplete` will try to acquire the same locks again, but can't. It will also help to narrow the try/catch to where the exception can be thrown."
1830231877,17539,junrao,2024-11-06T00:09:06Z,"We can skip clearing these two maps here since the operation is completed at this point. In `onComplete()`, we don't clear these two maps. So, this will make the behavior more consistent."
1830238005,17539,junrao,2024-11-06T00:19:06Z,"This is an existing issue. In the line below, we are logging for each partition, but shareFetchData contains the full request.

```
                log.trace(""Acquired records for topicIdPartition: {} with share fetch data: {}, records: {}"",
                    topicIdPartition, shareFetchData, shareAcquiredRecords);
```"
1830242539,17539,junrao,2024-11-06T00:26:42Z,testTryCompleteReturnsFalseWhenMinBytesNotSatisfiedOnFirstFetch => testTryCompleteWhenMinBytesNotSatisfiedOnFirstFetch?
1830242838,17539,junrao,2024-11-06T00:27:17Z,"testTryCompleteReturnsFalseWhenMinBytesNotSatisfiedOnLatestFetch =>
testTryCompleteWhenMinBytesNotSatisfiedOnSubsequentFetch ?"
1830269891,17539,junrao,2024-11-06T01:14:59Z,"Hmm, Apoorv had a good point in his comment (https://github.com/apache/kafka/pull/17539#discussion_r1828579315). There seems to be a potential problem. It's possible that thread1 calls `tryComplete`, finds `completed` to be false, and is about to set `partitionsAlreadyFetched`. The expiration thread then calls `forceComplete` and sets `completed` to true and proceeds to here. Now, thread1 continues and updates `partitionsAlreadyFetched`. The expiration thread will pick up the wrong `partitionsAlreadyFetched`."
1830395752,17539,adixitconfluent,2024-11-06T05:07:55Z,"yes, I think changing the log line to below makes more sense.
```
log.trace(""Acquired records: {} for topicIdPartition: {}"", shareAcquiredRecords, topicIdPartition);
```"
1830410313,17539,adixitconfluent,2024-11-06T05:31:34Z,"makes sense, I've changed the catch block to 
```
catch (Exception e) {
            log.error(""Error processing delayed share fetch request"", e);
            releasePartitionLocks(topicPartitionData.keySet());
            return forceComplete();
}
```"
1830454251,17539,adixitconfluent,2024-11-06T06:27:58Z,"Hi Jun, reading online regarding the performance of LinkedHashMap and HashMap - LinkedHashmap offers better performance when iterating through elements since they maintain an ordered entry list, while a HashMap offers better performance when accessing large datasets. Furthermore, when storing objects, LinkedHashmap stores objects in key-value pairs, while HashMap stores them in hash table. The type of key used also affects performance. 

Since now we are iterating over `sharePartitions` within the function `acquirablePartitions`, I thought it would be more efficient to use LinkedHashMap over HashMap."
1830461947,17539,adixitconfluent,2024-11-06T06:36:58Z,"Hi @junrao , I've created a ticket https://issues.apache.org/jira/browse/KAFKA-17948 to track this issue and if it fine to you, I would prefer to address the issue in a future PR."
1830468964,17539,adixitconfluent,2024-11-06T06:44:58Z,"Similar reason as [above](https://github.com/apache/kafka/pull/17539#discussion_r1830454251), I did it for performance efficiency. We are iterating over `partitionsAcquired` in `releasePartitionLocks`, hence I thought it would be more efficient to use LinkedHashMap over HashMap."
1831062681,17539,apoorvmittal10,2024-11-06T13:56:10Z,"The reason I suggested to use LinkedHashMap was to maintain the fetch order of partitions.

As per KIP - https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=255070434#KIP932:QueuesforKafka-Fetchingandacknowledgingrecords

> In order to ensure no share-partitions are starved from records being fetched, the share-partition leader rotates the order of share-partitions for which it returns partition information. This ensures that it eventually returns data about all partitions for which data is available.

I will add the rotation in SharePartitionManager to ensure the behaviour."
1831731314,17539,junrao,2024-11-06T21:26:53Z,"If ordering is important, should we explicitly define it as LinkedHashMap in all the places?"
1831742440,17539,junrao,2024-11-06T21:37:58Z,Should we reset partitionsAcquired and partitionsAlreadyFetched?
1831747806,17539,junrao,2024-11-06T21:43:40Z,Let's add a comment that the minBytes estimation currently assumes the common case where all fetched data are acquirable. 
1831878254,17539,apoorvmittal10,2024-11-07T00:08:42Z,"Yeah, good point. We should."
1832911918,17539,junrao,2024-11-07T15:47:22Z,"Since the ordering is important, let's use LinkedHashMap."
1832912856,17539,junrao,2024-11-07T15:47:55Z,"Since the ordering is important, let's use LinkedHashMap."
1832915380,17539,junrao,2024-11-07T15:49:24Z,"Since the ordering is important, let's use LinkedHashMap."
1832921605,17539,junrao,2024-11-07T15:52:49Z,This means `tryComplete` will never get non-empty `fetchOffsetMetadata` and its calculation of minBytes will be off. We need to think through how to address this.
111506681,2849,mjsax,2017-04-13T23:26:33Z,Why not using validateTransactionalId here ?
111517794,2849,junrao,2017-04-14T01:40:50Z,"Every time the epoch advances, it seems that we need to write the TransactionalId mapping message to the transactional log and we want to write that in epoch order since the transactional log is a compacted topic. "
111517806,2849,junrao,2017-04-14T01:41:01Z,"In the design, the epoch will wrap around."
111517818,2849,junrao,2017-04-14T01:41:12Z,Could we rename the method to sth like getTransactionState?
111517825,2849,junrao,2017-04-14T01:41:19Z,"Hmm, we need to add the transactionalId -> pid mapping to the transactional log, right? "
111517832,2849,junrao,2017-04-14T01:41:26Z,A few unused imports such as  KafkaException and Random.
111517837,2849,junrao,2017-04-14T01:41:32Z,$error => errors: $error   Ditto for line in 1412.
111528069,2849,junrao,2017-04-14T04:32:31Z,"Hmm, how is TransactionMetadata.timestamp used? If it's intended to expire a transaction, should we keep the time when the first partition is added? We probably also want to rename the field to sth like txnStartTime to make it clear."
111528074,2849,junrao,2017-04-14T04:32:39Z,"Hmm, is it intentional to use acks =1 instead of acks=-1? If so, could we add a comment how the potential data loss is dealt with?"
111528088,2849,junrao,2017-04-14T04:33:00Z,"Hmm, in the design doc, the key for transactional status message is the following. Is the doc outdated? Is it true that we are combining the transactional status message and the TransactionalId mapping message into a single one?

Key => Version Type PID
  Version => 0 (int16)
  Type => 1 (int16)
  PID => int64	"
111528094,2849,junrao,2017-04-14T04:33:07Z,"In the design doc, the value for transactional status message is the following. Is the doc outdated?

Value => Version Epoch Status [Topic [Partition]]
  Version => 0 (int16)
  Epoch => int16
  Status => byte
  Topic => bytes
  Partition => int32
"
111528102,2849,junrao,2017-04-14T04:33:14Z,"The comment in line 38 doesn't include TXN_TIMESTAMP_FIELD. Also, could we add a doc for each field?"
111528120,2849,junrao,2017-04-14T04:33:41Z,"Hmm, what's the default txnTimeout in the producer? The server side default is 15 minutes, definitely too long for a timeout for the callback during log append. "
111653276,2849,junrao,2017-04-15T00:34:08Z,Do we still need this object?
111653293,2849,junrao,2017-04-15T00:34:35Z,"Hmm, it seems that if the connection is not ready, we should just wait until it's ready or until the request timeout has been reached, instead of sending an error back immediately."
111653317,2849,junrao,2017-04-15T00:35:10Z,"Hmm, I am wondering if checks like this are synchronized properly. For example, the following sequence seems possible.
(1) handleAddPartitionsToTransaction() is called and validateTransactionalId() check passes. 
(2) Leader of the partition changes to a different broker and changes back.
(3) Now txnManager.appendTransactionToLog() may succeed but TransactionStateManager may still be loading the transaction state. "
111653333,2849,junrao,2017-04-15T00:35:27Z,"Hmm, is it possible for txnManager.getTransaction() to return None because leader change in the transaction topic?"
111653336,2849,junrao,2017-04-15T00:35:33Z,"map { case (topic, partitionIds) .. } ?"
111653338,2849,junrao,2017-04-15T00:35:37Z,"""pid mapping message"" seems no longer valid"
111653340,2849,junrao,2017-04-15T00:35:40Z,PidMessageFormatter => TransactionLogFormatter ?
111653344,2849,junrao,2017-04-15T00:35:45Z,key => transactionalId ?
111653357,2849,junrao,2017-04-15T00:35:56Z,"Currently, there are a couple of cases when the following IllegalStateException is thrown. (1) A producer times out on a commit/abort request and resends the request on a different socket channel. (2) A different producer has initialized the pid on the same transactional id. 

It seems that in both cases, perhaps we want to send back a retriable error (e.g., CoordinatorBusy) to that the client so that it can retry until successful? "
111653362,2849,junrao,2017-04-15T00:36:02Z,This is not the replica fetcher.
111653364,2849,junrao,2017-04-15T00:36:06Z,Do we need this tag since we know this thread is from this broker.
111653370,2849,junrao,2017-04-15T00:36:17Z,"Since both the request and the response will be small, perhaps we could just use the default receive buffer."
111653378,2849,junrao,2017-04-15T00:36:30Z,"Not sure if this is needed, but should we ensure that we send WriteTxnMarkersRequest with a lower coordinatorEpoch before a higher one?"
111653405,2849,junrao,2017-04-15T00:36:48Z,"(1) The reason for the disconnect could be that the current leader is down and a new leader is elected. So, we should go through the path dealing with Errors.NOT_LEADER_FOR_PARTITION to rediscover a potential new broker to send the request to. (2) Not sure if this truly needed, but do we need to ensure that the ordering of controller epoch is preserved during re-enqueue?"
111653412,2849,junrao,2017-04-15T00:36:58Z,"Since there are different types of epochs now, could we name this coordinatorEpoch to make it clear?"
111653448,2849,junrao,2017-04-15T00:37:22Z,partitionLock gives people the impression that this is a lock for a transaction topic partition. Perhaps rename this to sth like stateLock?
111653453,2849,junrao,2017-04-15T00:37:30Z,There doesn't seem be a corrupted list?
111653475,2849,junrao,2017-04-15T00:37:53Z,"Hmm, shouldn't we load up to log end offset instead of HW? The latter can be slightly smaller than log end offset and in that case, we may miss the portion of the log between log end offset and HW."
111653484,2849,junrao,2017-04-15T00:38:10Z,"Hmm, ownedPartitions is updated in the scheduler, which means when this method returns, there is no guarantee that ownedPartitions has been updated. Then, a client could still update the transaction log even after handleTxnEmigration() is called? Ditto in loadTransactionsForPartition(0."
111653491,2849,junrao,2017-04-15T00:38:17Z,"Hmm, INVALID_FETCH_SIZE seems to be for fetch requests, will log append throw this?"
111653493,2849,junrao,2017-04-15T00:38:25Z,"Not clear what "" since the metadata does not match anymore"" is."
111653508,2849,junrao,2017-04-15T00:38:56Z,"We synchronize on metadata in different classes like TransactionCoordinator, TransactionMarkerChannelManager, TransactionStateManager and DelayedTxnMarker. This makes a bit hard to reason about concurrency. Would it be better to consolidate all concurrent accesses to TransactionStateManager or a new class and only do synchronization on methods inside that class?"
111653523,2849,junrao,2017-04-15T00:39:23Z,We need to add the ACL check for each of the new request. This can be done in a separate PR if needed.
111653527,2849,junrao,2017-04-15T00:39:32Z,We need to set TransactionsTopicReplicationFactor and TransactionsTopicMinISR to 1 in config/server.properties so that local testing could work.
111781296,2849,junrao,2017-04-17T17:40:42Z,"Hmm, if pollTimeout is maxLong, networkClient.poll() could be blocked for a long time waiting for the response to come back. This means if there are new requests for other brokers coming in, their processing will be delayed."
111781327,2849,junrao,2017-04-17T17:40:51Z,"Hmm, in general, it's useful not to reconnect on a failed connection, is setting reconnectBackoff to 0 intentional?"
111781351,2849,junrao,2017-04-17T17:40:59Z,"Should we use metadataToWrite instead of metadata? Also, could we just fold maybeAddPendingRequest() into addRequestToSend()?"
111781396,2849,junrao,2017-04-17T17:41:13Z,Perhaps it's better to make TransactionMetadata.topicPartitions a private val and expose methods for manipulation so that it's easier to track when the state is changed?
111829832,2849,junrao,2017-04-17T21:49:29Z,"Hmm, shouldn't we be using the transaction timeout in the producer config instead of Integer.MAX_VALUE?"
111829857,2849,junrao,2017-04-17T21:49:40Z,"This may throw BrokerEndPointNotAvailableException and we probably need to handle this explicitly. Otherwise, some request handler threads will fail unexpectedly."
111829897,2849,junrao,2017-04-17T21:49:52Z,"We have to be a bit careful here. It's possible when a broker is restarted, it's ip and port have changed. So, we need to update the ip/port in brokerStateMap if needed. "
111830152,2849,junrao,2017-04-17T21:51:22Z,It seems that we need to call this during TransactionCoordinator.handleTxnEmigration() as well?
111830246,2849,junrao,2017-04-17T21:51:57Z,"Here, we are draining the requests to every broker whether the broker channel is ready or not. It's probably better to only take requests from brokers whose connection is ready."
111830264,2849,junrao,2017-04-17T21:52:04Z,max.transaction.timeout.ms => transaction.max.timeout.ms
111932482,2849,dguy,2017-04-18T11:43:14Z,Because this case is handled differently
111933023,2849,dguy,2017-04-18T11:46:43Z,It will be used to expire the transactions from the `transactionMetadataCache` in `TransactionManager`. It was my understanding that the timestamp should be updated to the the current timestamp (at least is supposed to  be in the EndTxnRequest case)
111933363,2849,dguy,2017-04-18T11:48:48Z,"According to point 2 https://docs.google.com/document/d/11Jqy_GjUGtdXJK94XGsEIK7CP1SnQGdp2eF0wSw9ra8/edit#bookmark=id.3af5934pfogc

>When all the responses have been received, append a COMPLETE_COMMIT transaction message to the transaction topic. We do not need to wait for this record to be fully replicated since otherwise we will just redo this protocol again."
111933574,2849,dguy,2017-04-18T11:50:12Z,Yes - this was initially missing in the `initPidRequest` i've updated it so it does add it to the log
111934136,2849,dguy,2017-04-18T11:53:42Z,@guozhangwang ?
111966880,2849,dguy,2017-04-18T14:23:40Z,"Not sure i completely agree. It says in the javadoc for `RequestCompletionHandler`:
```
/**
 * A callback interface for attaching an action to be executed when a request is complete and the corresponding response
 * has been received. This handler will also be invoked if there is a disconnection while handling the request.
 */
```"
111967313,2849,dguy,2017-04-18T14:25:15Z,"Yep, i'm not sure what the thinking is/was behind that. @guozhangwang ?"
111972450,2849,dguy,2017-04-18T14:43:30Z,"So, i think you are saying we need to always call `metadataCache.getAliveEndpoint(...)` even if we have a node for the given brokerId already in `brokerStateMap`?"
111978219,2849,dguy,2017-04-18T15:03:43Z,I am not sure it is a retriable error in this case. Once the previous `EndTxnRequest` has completed then a subsequent `EndTxnRequest` for the same transactionalId should also fail - right?
111997154,2849,dguy,2017-04-18T16:13:24Z,"hmmm. interesting! I don't think we want to clear everything, but just those transactionalIds where the partition has emigrated? Though, that is not immediately clear to me based on the current design.
Also there may well be in-flight-requests corresponding to the emigrated partitions. Not sure which error response should be sent back for those. "
111997381,2849,dguy,2017-04-18T16:14:27Z,Not sure - @guozhangwang ?
112013590,2849,junrao,2017-04-18T17:23:17Z,"That sounds good. However, it seems that TransactionMetadata needs 2 different timestamps. (1) The last timestamp when there is some activity from the pid. (2) The timestamp when the last open transaction is started. (1) will be used to expire pid and (2) will be used to abort a long transaction."
112013865,2849,junrao,2017-04-18T17:24:25Z,"Ok, could we add a comment. Sth like ""It's possible for a COMPLETE message to be lost with acks=1 when the leader for the transaction coordinator changes. If so, we will re-add the COMPLETE message during handleTxnImmigration()"""
112014229,2849,junrao,2017-04-18T17:25:53Z,"There are 2 cases when a channel is not ready: (1) The channel was ready and is disconnected now. (2) The channel is not ready and is being connected. In case (1), the callback will be called. In case (2), the current usage of networkClient is that the caller shouldn't send requests until the channel is ready. Otherwise, we will be unnecessarily taking requests out of the queue, submitting it to networkClient and only to re-enqueue the failed requests. "
112014271,2849,junrao,2017-04-18T17:26:05Z,"Yes, every time that we add a new request to a broker id, it's possible for the broker's ip/port to change. We need to update the cached ip/port in brokerStateMap."
112014403,2849,junrao,2017-04-18T17:26:42Z,"Yes, we should fail the subsequent EndTxnRequest. But it seems that we want the client to backoff a bit and keep retrying until success. If we are not sending back a retriable error, the client will think the EndTxnRequest actually permanently failed, which is not the case here."
112014452,2849,junrao,2017-04-18T17:26:54Z,"Yes, only clearing transactionalIds where the partition has emigrated. For in-flight-requests, we probably want to return a NotTransactionCoordinator error."
112073658,2849,sriramsub,2017-04-18T21:53:30Z,@apurvam @hachikuji 
112073751,2849,sriramsub,2017-04-18T21:54:04Z,@apurvam @hachikuji 
112073854,2849,sriramsub,2017-04-18T21:54:35Z,@apurvam @hachikuji 
112138268,2849,dguy,2017-04-19T08:04:51Z,Not sure -  @apurvam ?
112138822,2849,dguy,2017-04-19T08:08:13Z,"So, i think in the second case we probably want to check if the broker is ready before it gets to this point. As we've already drained the queue and will need to re-enqueue?"
112142569,2849,ijuma,2017-04-19T08:27:58Z,"As long as the `NetworkClient` is instantiated with an appropriate request timeout, this code is fine. The actual timeout here will be `the minimum of timeout, request timeout and metadata timeout` (as specified in the docs for`NetworkClient.poll`)"
112143022,2849,ijuma,2017-04-19T08:30:17Z,"Unless we want a timeout that's lower than `requestTimeout`, of course."
112282085,2849,apurvam,2017-04-19T18:38:13Z,"We synced up on this with Jun offline. The doc is outdated. We havea ticket here to track updates we need to make to the doc: https://confluentinc.atlassian.net/browse/CPKAFKA-616

"
112286609,2849,apurvam,2017-04-19T18:57:54Z,"I think @guozhangwang would have the most context on that, but I don't think we should not set it to 0. This is the connection to the leaders to write the abort marker. If the connection is disconnected, there is little reason to try to establish it again. I think the default of 50ms is reasonable here."
112286690,2849,apurvam,2017-04-19T18:58:18Z,That makes sense to me.
112286926,2849,apurvam,2017-04-19T18:59:24Z,"The comment is out of date. The original version of the patch had a notion of 'corrupted partitions', but the definition what is corrupted was not clear and we dropped the notion altogether."
112287416,2849,apurvam,2017-04-19T19:01:30Z,The default on the producer as of now is 60 seconds. I think perhaps 120seconds on the broker is reasonable?
112300739,2849,apurvam,2017-04-19T20:05:37Z,The config is being added as part of my patch. Shall we introduce an overload of `InitPidRequest.Builder` which just takes a `transactionalId` and assigns some default timeout? My patch will use the two parameter Builder and pass in the timeout from the producer config. 
112302162,2849,apurvam,2017-04-19T20:12:25Z,"We can avoid this by using the `NetworkClientUtils.awaitReady` method, as used in https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/ReplicaFetcherBlockingSend.scala#L87

That will ensure that the destination node is ready before sending the initial request. So you could peek into the queue to get the destination, ensure that it is ready, and if so, send the request. 

However, if it is not ready after a particular time, you will have to rediscover the correct node to send the request to (presumably because the old broker is down and the leader has moved)."
112306365,2849,apurvam,2017-04-19T20:31:43Z,"Yes, we have to do this across the board. I filed a JIRA so that we can make the change in one PR so that it is easier to make sure we are consistent. https://confluentinc.atlassian.net/browse/CPKAFKA-671"
112311456,2849,junrao,2017-04-19T20:55:25Z,"The broker has a request timeout, but it's probably better for the client could pass along the timeout in the RPC request?"
112311549,2849,junrao,2017-04-19T20:55:48Z,"Ideally, we only want to take requests off a queue when the channel to the corresponding broker is ready. However, if one broker's connection is not ready, we don't want to delay the sending of requests for other brokers. One way to achieve this is to have a separate queue per broker."
112311627,2849,junrao,2017-04-19T20:56:05Z,"Hmm, waiting for a 30 secs request timeout is still too long if there is another request in the queue ready to be processed."
112317720,2849,apurvam,2017-04-19T21:25:58Z,"Jun raises a good point. Right we should have two timestamps in the messages we write to the transaction log. One is the `entryTimestamp` which is the current time and which is used to expire the transactionalId. 

The other is the `transactionStartTime` which is the timestamp of the first `AddPartitionsToTransaction` request for a transaction. This is used to expire transactions on the broker.

As it stands, we have only the `entryTimestamp` and so a transaction could be open for ever if it gets updates very slowly. This would hold up the progress of the LSO on all the partitions involved in the transaction, which is not a desirable outcome.

@dguy would you be able to add the new timestamp and add a note to update doc on this ticket? https://confluentinc.atlassian.net/browse/CPKAFKA-616"
112415717,2849,dguy,2017-04-20T10:05:21Z,Seems to make sense to me. @guozhangwang @apurvam any reasoning behind this?
112417500,2849,dguy,2017-04-20T10:14:35Z,"It seems so. It is handled like this in `GroupMetadatamanager#prepareStoreGroup`, too"
112419986,2849,dguy,2017-04-20T10:28:20Z,"@apurvam there already is an overload, but it also sets the timeout to `Integer.MAX_VALUE`"
112426248,2849,dguy,2017-04-20T11:05:31Z,"Actually, i think this should be `producerEpoch`? AFAIK `coordinatorEpoch` is `partition.getLeaderEpoch`"
112462355,2849,dguy,2017-04-20T14:08:30Z,I'm not sure.
112472596,2849,dguy,2017-04-20T14:47:19Z,"we achieve that now as we just don't bother sending the request if the broker isn't ready. It gets re-enqueued, but it might need to be as the broker may now be down. I guess i'm missing something here, but i don't really see what."
112494867,2849,junrao,2017-04-20T16:09:46Z,"Hmm, I don't see INVALID_FETCH_SIZE or InvalidFetchSizeException being generated in the code base. If we don't want to clean this up in this patch, could we at least file a followup jira to track this?"
112498072,2849,junrao,2017-04-20T16:23:54Z,"My point is on efficiency. If a channel is not ready to begin with, keep re-enqueuing the same request again and again just adds overhead. So, it's better to wait until a channel is ready and then start processing the requests intended on that channel.

On the other hand, if a channel is ready, we send a request to that channel and the channel is disconnected before the response is received, we need to re-enqueue the requests for reprocessing. But that's done in the callback in the clientRequest already."
112554067,2849,apurvam,2017-04-20T20:33:47Z,"The transaction timeout is passed in in the InitPidRequest. It will apply to all transactions in that session. On the producer, the default for this timeout is 60s. "
112562609,2849,apurvam,2017-04-20T21:14:42Z,"So the transaction coordinator is colocated with the leader the topics in the transaction log which it owns. When we do a `txnManager.appendTransactionToLog` and the coordinator has moved, the leader for the topic should also have moved. In that case, we will get a `NOT_LEADER_FOR_PARTITION` error, and send back a `NOT_COORDINATOR` error code, in which case the client will send another `FindCoordinator` request. "
112572188,2849,apurvam,2017-04-20T22:13:35Z,"I agree with @junrao that this is inefficient. The producer model where we have a background sender thread and one queue per broker is more efficient, since it avoids busy waiting. However, we agreed that we can make this improvement in the future, after the initial integration is done.

I filed this ticket to keep track of the work item: https://confluentinc.atlassian.net/browse/CPKAFKA-673"
112572768,2849,apurvam,2017-04-20T22:17:41Z,"Followed up with @junrao offline. The main case to guard against is when the partition moves to another broker, and then moves back between the call to `validateTransactionalId` an `txnManager.appendTransactionToLog`. In this case, when the call back for `appendTransaction` executes, the coordinator may still be loading the cache, and it is unclear what we should do .

We agreed that the best thing probably is that when a partitino emigrates, we should track down in flight operations (and requests sitting in the purgratory), and error them out with a `NOT_COORDINATOR` code.

@dguy is it feasible to do the latter in a simple fashion?"
112622713,2849,dguy,2017-04-21T07:09:13Z,"Ok, so that is what we are already using."
112623812,2849,dguy,2017-04-21T07:17:40Z,"Yes it would be inefficient in the case that there is only a single broker or all brokers are not ready. In other cases it will not necessarily be so inefficient as the request will be re-enqued, but not immediately retried. Other requests, to potentially, other brokers will be tried etc, before the requests for the not ready broker is retried. 

Anyway, that said, having a queue per broker is better."
112623904,2849,dguy,2017-04-21T07:18:24Z,So should we just default this to something much lower? What would be a reasonable timeout?
112651788,2849,dguy,2017-04-21T09:44:05Z,@apurvam thanks. Yeah i should be able to do that
112805167,2849,dguy,2017-04-22T08:05:58Z,"@junrao, correct it doesn't look it is generated anywhere. I'll remove it from here and i've filed: https://issues.apache.org/jira/browse/KAFKA-5113"
112806295,2849,dguy,2017-04-22T09:31:16Z,"@apurvam @junrao i already changed `handleTxnEmigration` to remove the inflight operations that we know about, i.e., the ones in purgatory. Once they complete they will error with `NOT_COORDINATOR`. However this is only useful for the case where we writing the Txn Markers.  
For the other cases, if the coordinator is still loading the cache and the `transactionalId` is not yet cached we will respond with `NOT_COORDINATOR`. However, if the cache is  is loading  it may have an old record for the `transactionalId`. Should we do a check in the callback, i.e.,
```
if (!isCoordinatorLoadingInProgress(transactionalId))
          responseError = Errors.NOT_COORDINATOR
else
 // complete
````"
113041521,2849,junrao,2017-04-24T19:59:12Z,Be consistent on whether to use () when calling clientId() ?
113042021,2849,junrao,2017-04-24T20:01:29Z,This still needs to be addressed. The loading in GroupCoordinator has the same issue.
113083208,2849,junrao,2017-04-25T00:00:43Z,"I am a bit worried about all those independent checks on transactional state w/o any coordinator level locking. For example, in theory, a coordinator emigration and immigration could have happened after the check in line 104. Then, the appendMetadataToLog()/initPidWithExistingMetadata() call could mess up some state.

I was thinking that another way of doing this is to maintain a read/write lock for the coordinator partition. Immigration/emigration will hold the write lock while setting the state. Other calls like initPid will hold the read lock, do the proper coordinator state check, initiate the process like appending to the log and then release the read lock (we already have such a partition level lock in Partition, not sure if it's easily reusable). This will potentially give us better protection and make things easier to reason about."
113083220,2849,junrao,2017-04-25T00:00:50Z,"Hmm, appendMetadataToLog() is not doing exactly the same as if the metadata has existed. It seems that we will be missing all those checks on metadata state in initPidWithExistingMetadata()?"
113083240,2849,junrao,2017-04-25T00:01:00Z,Should we just do the eq check on reference instead?
113083247,2849,junrao,2017-04-25T00:01:04Z,"I assume that entryTimestamp is used for expiring a transactional id if there is no activity. If so, this needs to be updated on any activity related to a transactional id such as addPartitions, abort/commit and initPid. Also, could we rename this to sth like lastAccessTimestamp? It would also be useful to document the usage of the two timestamps in TransactionMetadata."
113083250,2849,junrao,2017-04-25T00:01:05Z,"Hmm, transactionStartTime needs to be set when we add the first partition to the transaction."
113083255,2849,junrao,2017-04-25T00:01:08Z,"Hmm, in this case, the coordinator is not really in a loading state. It seems that we need a new error like CONCURRENT_TRANSACTIONS?"
113083261,2849,junrao,2017-04-25T00:01:11Z,"This can be tricky to handle completely. (1) Should we also cancel any ongoing loading due to the previous immigration? (2) There could be outstanding transactional requests (e.g., initPid) waiting in producer purgatory after the log append call. Ideally, we should mark the coordinator as not available, trigger a check on requests associated with the coordinator's partition in the purgatory so that they can responds with a coodinator_not_available error."
113083269,2849,junrao,2017-04-25T00:01:17Z,"If we are doing this optimization, we need to make sure that the handleAddPartitionsToTransaction() call first completes any pending transaction, i.e, the markers from a previous transaction must have been sent successfully and the complete transaction entry has been added to the transaction log. Otherwise, the producer may start publishing the data for the next transaction before the transaction marker has been added for the previous transaction."
113083272,2849,junrao,2017-04-25T00:01:18Z,It seems that we should just keep retrying until successful or the broker is no longer the transaction coordinator.
113083277,2849,junrao,2017-04-25T00:01:22Z,Good question. We could either keep retrying or mark the coordinator in a bad state.
113083280,2849,junrao,2017-04-25T00:01:26Z,The only place that brokerStateMap is needed outside of this class is in TransactionMarkerChannelManager for draining events. Perhaps we can make brokerStateMap private and expose a method like drainQueuedTxnMarkers() instead? This makes it a bit easier to track the accesses to the map.
113083290,2849,junrao,2017-04-25T00:01:30Z,"Hmm, what about those pending requests already in the Selector()? Ideally we need to drain them too. Not sure what's the best way to do that."
113083300,2849,junrao,2017-04-25T00:01:33Z,"Hmm, not sure why we need to do flatMap instead of just map."
113083312,2849,junrao,2017-04-25T00:01:36Z,metadataPartition can be a bit confusing. How about coordinatorPartition?
113083333,2849,junrao,2017-04-25T00:01:53Z,This can be in a future patch. It would be useful to record this in some metric so that we know the state of transaction coordinator.
113083342,2849,junrao,2017-04-25T00:01:55Z,Should we do the eq test to only test for reference equal?
113083350,2849,junrao,2017-04-25T00:02:00Z,"Is that enough? For transactions in the prepare state, shouldn't we try to write the txn markers to added partitions and bring those transactions to the complete state too?"
113083356,2849,junrao,2017-04-25T00:02:03Z,Should we clear out loadingPartitions and stop any ongoing loading on that partition?
113083359,2849,junrao,2017-04-25T00:02:05Z,This method seems cheap. Is there a reason for running this in the scheduler?
113083367,2849,junrao,2017-04-25T00:02:09Z,The above issue is still not resolved.
113083375,2849,junrao,2017-04-25T00:02:11Z,"Hmm, do we need this check if we make sure all outstanding transactions are aborted on emigration and new transactions can only be started after the coordinator loading completes?"
113083384,2849,junrao,2017-04-25T00:02:16Z,Inaccurate comment. Internal topics are not just offset topic now.
113088713,2849,junrao,2017-04-25T01:01:00Z,"If the thread is blocked in networkClient.poll and some new requests are added to the queue, it seems that we need to wake up networkClient so that new requests can be processed immediately."
113088732,2849,junrao,2017-04-25T01:01:10Z,We should just wait until the target broker is available.
113088785,2849,junrao,2017-04-25T01:01:42Z,"Hmm, instead of throwing IllegalStateException, it seems that we should just keep retrying until successful?"
113089004,2849,junrao,2017-04-25T01:04:11Z,"Since the timeout in DelayedTxnMarker is infinite. I am wondering if we really need a txnMarkerPurgatory. In TransactionMarkerRequestCompletionHandler, we are already updating the pending partitions as the client response comes back. The response that removes the last pending partition can just trigger the calling of completionCallback."
113090851,2849,junrao,2017-04-25T01:26:07Z,"If the broker's message format is < V2, currently, when appending to the log, we simply convert it to an old format. In this case, we want to error out and respond to the client with a TransactionNotSupported error."
113091081,2849,junrao,2017-04-25T01:28:07Z,"Since we are sending new type of requests across the brokers, we need to check inter broker protocol and error out if the new request is not supported."
113123510,2849,dguy,2017-04-25T07:28:27Z,Yep @guozhangwang any reason why we are using HW?
113124537,2849,dguy,2017-04-25T07:34:51Z,It isn't clear to me what this is supposed to be saying either. @guozhangwang ?
113125360,2849,dguy,2017-04-25T07:40:11Z,Yes. We need to do that. Discussed this with @guozhangwang offline yesterday and we thought it might be better to do in a follow up patch as this patch is already quite large. Thoughts?
113125813,2849,dguy,2017-04-25T07:43:13Z,I guess it is because `loadTransactions` is run in the scheduler. So they won't interleave. @guozhangwang ?
113127361,2849,dguy,2017-04-25T07:52:28Z,"Ok. i can do that by adding a check in the while loop in `loadTransactionMetadata`, i.e, 

`while(currOffset < highWaterMark && loadingPartitions.contains(partitionId) && ...)`

will that be ok?"
113127705,2849,dguy,2017-04-25T07:54:19Z,@junrao i'm not really sure what i'm supposed to be checking here?
113128786,2849,dguy,2017-04-25T08:00:21Z,I'm not sure how i can cancel all inflight requests. Say for instance `TransactionStateManager.appendTransactionToLog(..)` is called and then on another thread `handleTxnEmigration` is called and removes the partition. How do i abort the inflight `TransactionStateManager.appendTransactionToLog(..)` call.
113129466,2849,dguy,2017-04-25T08:04:29Z,"@junrao thanks - yeah that makes sense. I was thinking about locking, too, but wasn't sure of the correct level to do it at, but the partition level seems ok. Will look into it. Thanks for the suggestion"
113130366,2849,dguy,2017-04-25T08:09:55Z,"I'm not sure what was meant by the comment, but i think you are correct in that we should do `initPidWithExistingMetadata()` in the case that they aren't the same. @guozhangwang any thoughts?"
113165211,2849,dguy,2017-04-25T11:03:40Z,in which case should i default it something like -1 here?
113165318,2849,dguy,2017-04-25T11:04:22Z,True. @guozhangwang any thoughts on this?
113165961,2849,dguy,2017-04-25T11:08:10Z,"@junrao i think we can handle (1) by:
1. in `removeTransactionsForPartition` we remove the partition from `loadingPartitions`
2. in `loadTransactionMetatdata` we only perform the loop when `loadingPartitions.contains(partition)

I have to think a bit more about 2"
113166353,2849,dguy,2017-04-25T11:10:21Z,I think this case is already handled by `handleInitPid()`
113167929,2849,dguy,2017-04-25T11:19:30Z,They will error out with `NOT_COORDINATOR` on completion. Will that suffice? I'm not sure how we can cancel inflight requests in the `NetworkClient`
113168889,2849,dguy,2017-04-25T11:25:14Z,i probably should just use flatten. My scala knowledge is not the best
113169486,2849,dguy,2017-04-25T11:28:53Z,In `initPidWithExistingMetadata` we also need to wait on the transaction to complete if there is an inflight transaction in the `PrepareAbort` or `PrepareCommit` phase.
113172390,2849,dguy,2017-04-25T11:45:47Z,How would i do that?
113173083,2849,dguy,2017-04-25T11:49:47Z,ok i guess i'll need todo that in `TransactionMarkerChannel#addRequestToSend`
113178448,2849,ijuma,2017-04-25T12:15:40Z,"`flatMap` is preferable to `map` and then `flatten`. I am guessing Jun's question is because `txnMarkerEntry` sounds like an element, but it's actually a `List`. Maybe it should be renamed?"
113178626,2849,ijuma,2017-04-25T12:16:27Z,"Also, you can write it a bit more concisely by doing `buffer.flatMap(_.txnMakerEntry).asJava` (x doesn't add anything over the underscore)."
113195654,2849,dguy,2017-04-25T13:34:40Z,"@junrao the TC maintains multiple partitions, so we'd need to have a lock per partition. You mentioned that there is a read/write lock on partition  - i believe you are referring to `leaderIsrUpdateLock`... I can't see any other locks in `Partition`. Anyway, do we want to expose this for other classes to use? I'd probably think not.

If we maintain a lock per partition then perhaps it should be done by the `TransactionStateManager` and then we'd need to add/remove locks in the immigration/emigration. I think we'd also need to add another method on `TransactionStateManager`, say `partitionLock(partitionId)` that returns an `Option[ReentrantReadWriteLock]`. The calls in `TransactionCoordinator` to `isCoordinatorFor` could then be replaced with calls to `partitionLock(partitionId)` - if the lock exists they take a read lock. If it doesn't exist then respond with `Errors.NOT_COORDINATOR`

Does this seem sensible?

@guozhangwang "
113330271,2849,guozhangwang,2017-04-25T22:48:31Z,In that case do we really need this change in this PR? Maybe we can just remove this change as it is actually doing the same still.
113330372,2849,guozhangwang,2017-04-25T22:49:15Z,Are these changes intentional? The original ordering seems OK to me.
113331290,2849,guozhangwang,2017-04-25T22:55:50Z,"Thinking about this a bit more, I wonder if the `coordinatorEpoch` should also be in the internal entry as well, since different txn log partition leader's epoch hence the coordinator epoch would be different?"
113331531,2849,guozhangwang,2017-04-25T22:57:33Z,"EDIT: in the existing branch I have already made those changes a while back: https://github.com/guozhangwang/kafka/blob/KEOS-transactions-coordinator-network-thread/clients/src/main/java/org/apache/kafka/common/requests/WriteTxnMarkerRequest.java

The design doc however is not updated.

I saw you did a groupBy on the `coordinatorEpoch` instead, so that each write marker request will only contain one `coordinatorEpoch`, but since on the broker side, this coordinator epoch is checked inside the `Log` layer anyways I felt it is better to change this field as a per-marker-entry field in the protocol."
113332359,2849,guozhangwang,2017-04-25T23:03:23Z,"I did this in the original commit but: since this thread is owned by the broker only, we do not need this tag. Instead we can just pass an empty tag map."
113347218,2849,guozhangwang,2017-04-26T01:15:54Z,Nice catch. Currently we do not have a broker-side `reconnect.backoff` config yet so different modules just hand-code different values. But moving forward I felt we may want to introduce a new config for inter-broker reconnect backoff.
113348424,2849,guozhangwang,2017-04-26T01:29:27Z,nit: indentation on the comment line 69.
113348812,2849,guozhangwang,2017-04-26T01:33:43Z,Sounds good to me.
113351582,2849,guozhangwang,2017-04-26T02:00:13Z,nit: new line after condition
113352248,2849,guozhangwang,2017-04-26T02:07:55Z,"Yeah I was ""piggy-backing"" on this error code since this is the only retriable error code that the client recognizes in my incomplete patch. And I do agree that we should introduce another retriable error code here."
113352808,2849,guozhangwang,2017-04-26T02:14:28Z,"Since we return immediately after the `prepareXX` marker is written. In both `initPid` and `addPartition` request handling the metadata state could be in `prepareXX` or `Ongoing`, so we need to handle them in both these two places, better in a consistent way.

In the design doc we said the request will be held on the broker side indefinitely until the current transaction is rolled-forward or rolled back complete. We did this in `initPid`, but here we are directly returning a non-retriable error code. I was planning to do these handling on both places in a separate PR but since @dguy is already adding the logic for `initPid` now maybe we can discuss about that now: after thinking about it a bit more I felt maybe its better to return a retriable exception either immediately (with the new error code we need anyways for the above case) or after some timeout (with the `timeout` error code) on both places, and let the client to back-off and retry. Doing this we can avoid ever increasing the in-memory structures like like the per-broker queues and the purgatory.

Thoughts? @junrao @apurvam @hachikuji @dguy "
113353056,2849,guozhangwang,2017-04-26T02:17:12Z,"I'm wondering if we should bound the size of this blocking queue or it will cause TC-broker OOM when some of the txn involved partition leaders are temporarily / permanently available. My feeling is that we do not and here is my reasoning (cc @junrao @dguy ):

1. We have two in-memory structures whose size is unbounded, this blocking queue and the txn purgatory.
2. When there are some partitions unavailable, hence the current prepareXX transaction cannot be completed, they will take one slot on each of the above structures;
3. Then when new request is coming from the same Pid, either `initPid` or `addPartitions`, they will not proceed. Hence the above structures will have at most one parked item for each producer.

See my other comment about handling the `initPid` and `addPartitions` request while the previous txn has not completed."
113353166,2849,guozhangwang,2017-04-26T02:18:28Z,"As long as we clear the txn purgatory, even if there are inflight request during the time. When they come back as responses, either succeeded or failed, since the corresponding delayed operation has already gone its callback will effectively be reduced to a no-op. So I think that is fine."
113353266,2849,guozhangwang,2017-04-26T02:19:50Z,Chatted with @junrao offline. I did this mainly to just mimic group coordinator's loading behavior. But after discussing with him I think it is safer to read up to LEO (we probably need to do the same for GC as well)
113353374,2849,guozhangwang,2017-04-26T02:21:23Z,Yup. Let's do that in follow-up PRs.
113353582,2849,guozhangwang,2017-04-26T02:23:52Z,"The log entry was wrong, maybe just ""since the the appended log did not successfully replicate to all replicas"". Does that sound better?"
113353750,2849,guozhangwang,2017-04-26T02:26:03Z,"Since we now have one queue per broker, and 1) we drain all the elements in the queue whenever trying to send; 2) we wake up the client whenever we are adding new elements to the queue; I think it is not as critical to set lower values?"
113353816,2849,guozhangwang,2017-04-26T02:26:52Z,"Agree, maybe we can have a read-write lock on the txn metadata cache and only release the read lock after the txn log has been appended locall?"
113354061,2849,guozhangwang,2017-04-26T02:29:31Z,I think @junrao 's comment is that we did some checking on the txn metadata's state in `initPidWithExistingMetadata` whereas we did not do such checking before calling `appendMetadataToLog`. Have explained to him that it is because at line 129 we are assured that the metadata is just newly created and hence it's always `Ongoing`. Maybe the comment itself has been outdated after the addition of the `initPidWithExistingMetadata` logic.
113354470,2849,guozhangwang,2017-04-26T02:33:57Z,"@dguy @junrao What's the motivation of trying to drain all the queued elements? Since the max inflight request is only 1 in the network client, even if we construct multiple requests for a certain destination only the first request will succeed in sending right? In that case could just do the 1) peek-first 2) if-ready-send-and-pop pattern?"
113379063,2849,dguy,2017-04-26T07:07:11Z,@guozhangwang we don't have one queue per broker yet. That was going to be in a follow up PR
113379863,2849,dguy,2017-04-26T07:12:57Z,"actually, i don't think we need this debug message as the error cases are all logged previously. I'll just remove it."
113386204,2849,dguy,2017-04-26T07:51:34Z,@guozhangwang per partition? or a global lock?
113388847,2849,dguy,2017-04-26T08:05:39Z,Will the KIP need to be updated with the new Error?
113392736,2849,dguy,2017-04-26T08:26:59Z,"So this should change back to what you previously had? We originally had your code, but during the merge with other changes it was probably removed. That is why i did the groupBy on `coordinatorEpoch`."
113395115,2849,dguy,2017-04-26T08:40:00Z,"Question more for my own understanding than anything else: 
If `initPid` is holding on to the request on the broker until the transaction is completed, then i think `addPartitions` should never be called when the transactionId is in a `PrepareXX` state. Is that correct?"
113395718,2849,dguy,2017-04-26T08:43:18Z,@guozhangwang this is largely a refactoring of your code from here: https://github.com/guozhangwang/kafka/blob/KEOS-transactions-coordinator-network-thread/core/src/main/scala/kafka/coordinator/transaction/TransactionMarkerChannelManager.scala#L157 :-P
113454320,2849,dguy,2017-04-26T13:43:12Z,"On second thoughts, i'll add a single read/write lock in the coordinator as it is much simpler than having to maintain multiple. If that is not ok, we can revisit."
113457671,2849,dguy,2017-04-26T13:55:37Z,@junrao i'm not sure i understand (2). When you refer to there being outstanding requests in the producer purgatory is that w.r.t `replicaManager.appendRecords(...)`?
113499063,2849,apurvam,2017-04-26T16:22:42Z,@dguy @guozhangwang we should probably have one or more JIRA's to track these. 
113501224,2849,guozhangwang,2017-04-26T16:32:02Z,"This is about the inter-broker protocol version. Details are here:

http://kafka.apache.org/documentation/#upgrade_10_1

Maybe just leave a TODO marker and I can address it in a follow-up PR, so we would not drag too long for this one?"
113501426,2849,guozhangwang,2017-04-26T16:32:56Z,Responded in another comment. Let's do this incrementally in another PR and just leave a TODO in this PR. Otherwise we would be looking at a 10K diff
113506336,2849,dguy,2017-04-26T16:53:53Z,Ok i've filed: https://issues.apache.org/jira/browse/KAFKA-5128
113506744,2849,dguy,2017-04-26T16:55:31Z,I've filed https://issues.apache.org/jira/browse/KAFKA-5128
113510250,2849,junrao,2017-04-26T17:10:10Z,"Yes, we can probably also just check if the partition is still in ownedPartitions."
113511035,2849,junrao,2017-04-26T17:13:41Z,"If you set the coordinator state first and trigger a check in the purgatory, the checking of the isComplete() logic should realize that the coordinator is no longer valid and error out."
113511326,2849,dguy,2017-04-26T17:15:02Z,"https://issues.apache.org/jira/browse/KAFKA-5131
"
113512025,2849,junrao,2017-04-26T17:18:18Z,"Most operations just need to hold a read lock. Only emigration/immigration need to hold a write lock. So, perhaps having a single lock per broker is also fine as long as we don't hold the lock for too long (i.e., we should mostly be just setting critical states while holding the lock. Any expensive stuff should be done outside the lock)."
113512989,2849,junrao,2017-04-26T17:22:29Z,"We can set it to -1, but it may not matter. transactionStartTime is only useful when aborting a transaction that's started. If the transaction is in empty state, transactionStartTime is probably not going to be used. It's more important to set transactionStartTime on adding the first partition."
113513257,2849,junrao,2017-04-26T17:23:39Z,"Yes, we can remember this and update the KIP with all the changes in a batch."
113513973,2849,junrao,2017-04-26T17:26:53Z,"Yes, I was referring to delayed events in the producer purgatory after the replicaManager.appendRecords(...) call. We can trigger a check of those delayed events and let them error out (since the check will find out that the broker is no longer the coordinator)."
113515117,2849,junrao,2017-04-26T17:31:52Z,"Hmm, is it? handleInitPid() seems to just call appendMetadataToLog(). There is no guarantee that abort/commit marker from the previous transaction has been sent by the inter broker thread."
113515688,2849,junrao,2017-04-26T17:34:14Z,"Well, if the response fails because of disconnection, we shouldn't keep retrying right?"
113517731,2849,dguy,2017-04-26T17:42:47Z,In `handleInitPid` if metadata exists it calls `initPidWithExistingMetadata` that waits for the previous transaction to complete
113537029,2849,dguy,2017-04-26T19:05:32Z,@junrao is calling `delayedProducePurgatory.checkAndComplete(topicPartitionOperationKey)` the correct thing to do in this case?
113538220,2849,dguy,2017-04-26T19:11:47Z,"Yep, it does get set when the first partition is added."
113550754,2849,guozhangwang,2017-04-26T20:12:19Z,"@dguy `initPid` is only called only for the lifetime of a producer, when the producer client completed the current txn and is about to start the next one, it will not call `initPid` but just `addPartitions` as the first request. Only when producer client fails-over the new instance (i.e. in its next life) will send an `initPid` again."
113594739,2849,apurvam,2017-04-27T00:45:09Z,"The code as it is seems correct to me. If you get `addPartitions`, you should be in an Ongoing state. I also think the `EMPTY` state is invalid in this case. "
113594825,2849,apurvam,2017-04-27T00:45:54Z,"Or is `EMPTY` just signally `NO_ONGOING_TRANSACTION`? If so, it is a valid state."
113595652,2849,apurvam,2017-04-27T00:54:41Z,"I guess you mean 

```java
if (isCoordinatorLoadingInProgress(transactionalId)) 
   responseError = Errors.NOT_COORDINATOR
```

(note the absence of the negation in my if).

I think this is reasonable. More generally, if we don't find the transaction metadata in the `appendToTransactionLog` callback, or if the coordinatorEpoch is different from what we expect, we should just return NOT_COORDINATOR. The client will retry and eventually succeed. "
113595956,2849,apurvam,2017-04-27T00:57:49Z,"Also, cc @guozhangwang.."
113596678,2849,apurvam,2017-04-27T01:06:30Z,"I guess @guozhangwang current point was that a valid state here could also be `PREPARE`, if the previous transaction is still completing, in which case we should return `CONCURRENT_TRANSACTIONS`, and have the client retry with backoff. "
113634970,2849,dguy,2017-04-27T07:54:21Z,"@guozhangwang @apurvam - cool makes sense now. So i guess we should do the same thing in `initPid` and `addPartitions`, i.e., respond with `CONCURRENT_TRANSACTION` or block until the previous transaction has completed"
113831631,2849,guozhangwang,2017-04-28T00:32:12Z,This is already addressed.
114487301,2964,ijuma,2017-05-03T07:46:13Z,There's no need to have the `Errors` type parameter here. That check always succeeds.
114488311,2964,ijuma,2017-05-03T07:53:55Z,"This can be written more clearly as:

```scala
transactionMetadataCache.get(partitionId).flatMap { cacheEntry =>
  cacheEntry.metadataPerTransactionalId.get(transactionalId)
}.map { txnMetadata =>
  CoordinatorEpochAndTxnMetadata(txnMetadataCacheEntry.coordinatorEpoch, txnMetadata)
}
```
"
114488522,2964,ijuma,2017-05-03T07:55:26Z,"Hmm, as discussed previously, it's preferable to use a case class instead of many unnamed parameters."
114505601,2964,dguy,2017-05-03T09:40:51Z,"Do we need to update these tuples to be `(error, null, null)`? i.e., we are expecting a triple as the result"
114507297,2964,dguy,2017-05-03T09:50:44Z,Looks like this is not used anymore?
114509240,2964,dguy,2017-05-03T10:02:25Z,Why do we need to do this?
114510129,2964,dguy,2017-05-03T10:07:39Z,Should we not just retry in this case? We've already written `PrepareXX` to the log and i thought we need to complete the transaction? How will the transaction be completed?
114510837,2964,dguy,2017-05-03T10:09:49Z,The comment is incorrect. I don't think we are returning anything to the client. If the transaction has been emmigrated than it the commit should be completed by the new partition leader
114511374,2964,dguy,2017-05-03T10:12:53Z,"I think we also need a check `case Errors.NOT_COORDINATOR` in which case we should just log an move on. 
Also, in this case i think we need to retry? "
114512933,2964,dguy,2017-05-03T10:22:15Z,How are we cleaning up cases like this where it has failed? What are the implications for consumers etc?
114513958,2964,dguy,2017-05-03T10:28:55Z,why do we need to update the `txnStartTimestamp` here and in the `CompleteCommit` case?
114514040,2964,dguy,2017-05-03T10:29:26Z,could this be `case PrepareAbort | PrepareCommit` ? I think they are identical
114514118,2964,dguy,2017-05-03T10:30:00Z,`case CompleteAbort | Complete Commit` ? I think they are identical 
114515680,2964,dguy,2017-05-03T10:41:41Z,"We have the situation here that when this method returns nothing has actually happened yet, i.e., the removal is done on a different thread. So what happens if we get a request at the same time for another transactionalId that is in the same partition? Are we just relying/hoping it will eventually fail?"
114515990,2964,dguy,2017-05-03T10:44:02Z,Is this an exceptional condition? Could we just log it and move on?
114516182,2964,dguy,2017-05-03T10:45:16Z,Should we change this to have `CoordinatorEpochAndTxnMetadata` as a param rather than the 2 separate params?
114675298,2964,guozhangwang,2017-05-03T23:20:50Z,ack
114675858,2964,guozhangwang,2017-05-03T23:24:58Z,"EDIT: the reason I made it as unnamed parameters is that the `sendTxnMarkers` function is not long used as a parameter but also directly called elsewhere. And it has been reduced from 8 params to 4, so I feel this is better cost-effective?"
114677405,2964,guozhangwang,2017-05-03T23:37:23Z,"ack. this needs to be tweaked a bit since the inner map is a `Pool`, let me know if you like the new pattern or not."
114677702,2964,guozhangwang,2017-05-03T23:40:09Z,ack.
114677859,2964,guozhangwang,2017-05-03T23:41:42Z,ack
114677949,2964,guozhangwang,2017-05-03T23:42:25Z,this is needed as `groupBy` to generate the grouped map keyed by the node object.
114678445,2964,guozhangwang,2017-05-03T23:46:55Z,"From `appendTransactionToLog` the only error codes we would pass in the callback are:

```
COORDINATOR_NOT_AVAILABLE
NOT_COORDINATOR
UNKNOWN
other
```

And anything falling into `other` should be considered fatal as they should not happen (if they happen that should be a bug), and `UNKNOWN` comes from `MESSAGE_TOO_LARGE` or `RECORD_LIST_TOO_LARGE` which are also doomed as fatal."
114678501,2964,guozhangwang,2017-05-03T23:47:30Z,ack. updating the comment.
114678943,2964,guozhangwang,2017-05-03T23:51:37Z,"From `DelayedTxnMarker` when we call `completionCallback` we will only return `NONE` and hence any other error codes would not be expected and considered as fatal right? Note that the write-marker-response could contain error codes like `UNKNOWN_TOPIC_OR_PARTITION`, but this is handled in the `TransactionMarkerRequestCompletionHandler`, in which case we will retry."
114679087,2964,guozhangwang,2017-05-03T23:53:00Z,"Generally this default is just for any un-expected error codes (i.e. there is a bug in the protocol, that broker returns some error codes not defined) to fail-fast."
114679260,2964,guozhangwang,2017-05-03T23:54:19Z,"good point, ack."
114679267,2964,guozhangwang,2017-05-03T23:54:22Z,"good point, ack."
114679565,2964,guozhangwang,2017-05-03T23:57:09Z,"I have not reviewed your expiration PR so my understanding might be inconsistent, but here is how I read the existing code: `txnStartTimestamp` is only set when there is an active txn ongoing, otherwise it should always be set to -1 (i.e. the above check on the new metadata). And expiration will simply skip the metadata when this field is -1."
114680315,2964,guozhangwang,2017-05-04T00:03:32Z,"This will be in completed in another PR for the locking mechanism, and here is a sketch:

1. we will add a per-broker read-write lock on TC.
2. All request-handling logic need to grab the read lock before accessing the cache for checking state until appending to the local log call returns.
3. the loading / removing thread will grab the write lock when removing / adding the sub-map for that partition into the cache.
4. implementation-wise, we will modify `stateLock` for that read-write lock. The main purpose is to avoid log appending out-of-order, e.g. consider the following order:

```
a. check on cache passed.
b. write to local txn log

between a and b, long GC happens and emigration happens, followed by an immigration so that the broker re-become the tc for that txn partition. So step b) will succeed but there could be more entries already appended after checking step a)
```"
114680536,2964,guozhangwang,2017-05-04T00:05:22Z,"Hmm, that is a good point. I think it can happen that `onFollower` called twice but the second call's triggered thread gets executed before the first call. So yeah we could just log it and move on."
114681378,2964,guozhangwang,2017-05-04T00:13:26Z,"EDIT: similarly for `addLoadedTransactionsToCache`, but slightly different: we only have one background thread inside `scheduler` and we are checking on `loadingPartitions` so that no requests will be handled while loading so no new entries will be appended to the log. So even we have two loading tasks scheduled,  it is safe for the second loaded metadata to replace the first loaded sub-map via `put`, so if `isDefined` we can just log it."
114682211,2964,guozhangwang,2017-05-04T00:22:34Z,"I have been thinking about it, the main reason that I keep it separately is that it can be called in two paths: handler thread call it directly and the callback from writeMarkerSender call it separately. In the latter case the two values are separate so I ended up keep them as is."
114723919,2964,dguy,2017-05-04T08:41:09Z,"In step 2 isn't the log append callback happening on another thread? So how do we release the read lock? I guess i'm missing something but i'm reading it like this:
```
inReadLock(lock) {
   // do some stuff
   txnManager.appendToLog(...)
}
```
Which would mean the read lock would be given up as soon as the `appendToLog` call returns, but that doesn't mean the write to the log has actually completed as it might go into purgatory and be completed by another thread. I guess you have some other idea?"
114724163,2964,dguy,2017-05-04T08:42:42Z,My understanding is that it only needs to be set when we add the first partition to a new transaction.
114724204,2964,hachikuji,2017-05-04T08:43:01Z,"Why is it safe to access `transactionMetadataCache` without a lock? It seems we protect mutations with the state lock, but since it is not a concurrent collection, don't we need to protect reads as well?"
114724432,2964,dguy,2017-05-04T08:44:33Z,"ok - that makes sense. However, if this does happen how would we recover from it? We'd have uncommitted data in the logs which would block consumers using read committed. Do we have any plans to deal with this?"
114724503,2964,dguy,2017-05-04T08:45:02Z,yeah - duh! I missed that!
114724840,2964,hachikuji,2017-05-04T08:47:07Z,"Not from this patch, but should this be `TransactionStateManager`?"
114725329,2964,hachikuji,2017-05-04T08:50:12Z,nit: this is not aligned.
114726156,2964,hachikuji,2017-05-04T08:54:40Z,"Hmm... It doesn't seem right to synchronize on the instance of `CoordinatorEpochAndTxnMetadata`. We construct a new one in every call to `addTransaction`, right? Also, we're still synchronizing on the metadata elsewhere in this file."
114726879,2964,hachikuji,2017-05-04T08:58:33Z,"nit: `error`? In spite of the name of `Errors`, there is only one error. A few more of these in this class if you're keen."
114728807,2964,hachikuji,2017-05-04T09:09:58Z,"This is a comment throughout, but it seems we rarely check the result of `prepareTransitionTo`. Perhaps that function should just raise an invalid state exception if the state transition is invalid? Otherwise we should get in the habit of adding the checks."
114729513,2964,hachikuji,2017-05-04T09:13:50Z,"nit: Not from this patch, but should this be `responseCallback`? It's nice to keep naming consistent and we use this name in `handleInitPid` and `initPidWithExistingMetadata`. It's also helpful that the name expresses that this callback returns to the user."
114730909,2964,hachikuji,2017-05-04T09:21:29Z,Shouldn't we be using `newMetadata` somehow?
114732285,2964,hachikuji,2017-05-04T09:28:44Z,"nit: Not from this patch, but couldn't we add a couple constructors to `InitPidResult` and remove these functions? Or maybe just add default values for PID and epoch."
114737164,2964,hachikuji,2017-05-04T09:57:30Z,The reuse of `TransactionMetadata` here is super confusing. It seems we just need a struct to propagate the new state to the completion callback. Maybe we could do this with an immutable case class instead? Maybe `TransactionStateTransition` or something.
114739243,2964,hachikuji,2017-05-04T10:09:36Z,"typo: ""Competed"""
114740699,2964,hachikuji,2017-05-04T10:18:53Z,"This is pretty confusing stuff. My understanding is that once this append completes, we'll invoke `TransationMetadata.completeTransitionTo`. In this case, it will be the same `TransactionMetadata` object passed to itself. Might not be incorrect, but it's definitely weird.

By the way, you can replace `epochAndMetadata.transactionMetadata` with `metadata`."
114749671,2964,ijuma,2017-05-04T11:19:41Z,"Similar to a comment that I left in a different PR, adding type annotations here triggers a pattern match and it's brittle if there are cases where the match can fail, but the compiler can't prove it (any time there is a subclass involved). It's one of the cases where relying on type inference is safer (it will always infer a safe type)."
114750267,2964,ijuma,2017-05-04T11:24:05Z,"Btw, a better way to represent this would be using `Either[Errors, (Int, TransactionMetadata)]`. Even better would be to have a type for the tuple on the right, but even without it, it would make the flow way clearer IMO."
114807366,2964,guozhangwang,2017-05-04T15:12:07Z,"When a transaction is completed, should we reset the startTimestamp to -1 again to avoid being expired?"
114807926,2964,guozhangwang,2017-05-04T15:14:05Z,ack.
114877797,2964,guozhangwang,2017-05-04T20:20:46Z,"When this happens usually ops people need to be involved to manually truncate the log before starting. But I think this code can also be improved a bit for a more graceful shutdown.

Do you have any ideas?"
114878149,2964,guozhangwang,2017-05-04T20:22:13Z,"As I said we only need to make sure the the log entry is appended to the local data segment (even not required to flushed on disk), not necessarily replicated complete."
114879314,2964,guozhangwang,2017-05-04T20:27:35Z,"1. Please see my previous comment: we will modify the `statelock` into a read-write lock and the only operations that could mutate the top-level map `transactionMetadataCache` will be covered (the checking / append-to-log will be covered in read lock).

2. For the lower-level map `Pool[String, TransactionMetadata]`, its operations are thread safe and any modifications to the inner `TransactionMetadata` will be covered by the object `synchronized` itself and all modifications will be in-place than object override."
114879417,2964,guozhangwang,2017-05-04T20:28:03Z,ack.
114879969,2964,guozhangwang,2017-05-04T20:30:42Z,"I have also thought about that, e.g. having sth. like

```
case class InitPidResult(pid: Long = 
RecordBatch.NO_PRODUCER_ID, epoch: Short = RecordBatch.NO_PRODUCER_EPOCH, error: Errors)
```

and call it without overriding defaults upon checking failure cases, but I found it is less readable. If you feel strong about it I can change."
114880965,2964,guozhangwang,2017-05-04T20:35:07Z,"Yeah that is a good question, ideally we should sycnrhonize on `txnMetadata` only, since coordinator epoch will only likely be changed during loading / removal of the metadata. Will refactor on this part."
114902795,2964,guozhangwang,2017-05-04T22:33:08Z,ack.
114903066,2964,guozhangwang,2017-05-04T22:35:06Z,"We check it in `TransactionMetadata`, that the new state is equal to the pending state."
114903584,2964,guozhangwang,2017-05-04T22:38:28Z,"I just refactored the code a bit around `initPid`, since it is a special case compared with others:

1. In other cases, we first create a new metadata as a place holder of all the pending updates to the original object and then only update it in the callback after append / send marker completes;

2. In `initPid` case, we would try first inserting a dummy into the cache but set its pending state, as `Empty -> Empty`; if there is an existing entry already we would either a) abort its existing txn first b) return retriable error code or c) update epoch / txn timeout and do the same as case 1) above.

The new code path looks better to me know. Let me know what do you think."
114903764,2964,guozhangwang,2017-05-04T22:39:39Z,ack.
114903788,2964,guozhangwang,2017-05-04T22:39:51Z,done.
114903866,2964,guozhangwang,2017-05-04T22:40:32Z,"Not sure I understand this comment, could you elaborate a bit?"
114904861,2964,guozhangwang,2017-05-04T22:47:50Z,ack.
114905004,2964,guozhangwang,2017-05-04T22:48:54Z,ack.
114906261,2964,hachikuji,2017-05-04T22:58:09Z,"But if we know the transition is invalid here, then we could skip appending to the log, right? I feel we should just make `prepareTransitionTo` raise an exception if the attempted transition is invalid."
114916468,2964,guozhangwang,2017-05-05T00:32:04Z,Discussed offline.
114916529,2964,guozhangwang,2017-05-05T00:32:46Z,"EDIT: discussed offline, realized it is a different issue than I thought about. Refactored this part a bit as well."
114917535,2964,guozhangwang,2017-05-05T00:45:44Z,"EDIT: @ijuma I realized that in many cases I would return a `null`, and it will cause the annotated type to be `Any` at compilation time. So I think it is actually better to enforce type annotations in this case."
114917644,2964,ijuma,2017-05-05T00:47:01Z,"Yeah, you should never use `null` in Scala generally. Using an `Either` here avoids all the issues."
114920333,2964,junrao,2017-05-05T01:27:36Z,Not sure checking coordinatorEpoch here is enough since coordinatorEpoch could change when we append the log.
114921338,2964,guozhangwang,2017-05-05T01:42:57Z,"Got it, thanks!"
114973551,2964,dguy,2017-05-05T11:06:35Z,Ok  - i missed the local bit. Thanks
115104932,2964,junrao,2017-05-05T23:36:30Z,"In scala, == tests object equality. I think we want to test reference equality here. If so, we should use eq for testing."
115106324,2964,junrao,2017-05-05T23:57:42Z,"Hmm, this means that in prepareTransitionTo(), we need to transition from one state to itself. Not sure if all states are allowed to transition from itself."
115106407,2964,junrao,2017-05-05T23:59:24Z,"Since now there are different types of epoch, would it be better to name this prepareIncrementProducerEpoch?"
115106983,2964,junrao,2017-05-06T00:10:02Z,Should we include pendingState in hashCode() and equals()?
115107794,2964,junrao,2017-05-06T00:26:22Z,"Hmm, when transitioning from empty to ongoing, it seems it's ok for newMetadata.txnStartTimestamp to be larger than current txnStartTimestamp. It's only when transitioning from ongoing to ongoing that we don't want txnStartTimestamp to change."
115107921,2964,junrao,2017-05-06T00:29:38Z,"Yes, it seems Damian's understanding makes sense."
115108280,2964,junrao,2017-05-06T00:39:21Z,It seems the comment should be the same as in line 185: let client backoff and rety instead of retry immediately.
115147118,2964,junrao,2017-05-07T16:09:38Z,highWaterMark below should be renamed to logEndOffset?
115147657,2964,junrao,2017-05-07T16:32:52Z,"Since removeTransactions() is cheap, not sure if this needs to be run in a scheduler. Also, Perhaps it's useful to wait for the scheduler to have no pending task here before return? Any pending loadTransaction task should be completed very quickly after removeTransactions() is called."
115163421,2964,junrao,2017-05-08T02:03:38Z,Does BrokerRequestQueue.destination need to be volatile?
115163433,2964,junrao,2017-05-08T02:03:50Z,"Hmm, the while loop may tie up a request handler thread, which is not ideal."
115163443,2964,junrao,2017-05-08T02:04:02Z,"Since the timeout for delayedTxnMarker is infinite, do we need a purgatory or just a map?"
115163467,2964,junrao,2017-05-08T02:04:23Z,"Hmm, in the disconnected case, shouldn't we check txnStateManager.getTransactionState as well in case an emmigration has happened?"
115163478,2964,junrao,2017-05-08T02:04:41Z,"Since multiple immigration/emigration could have happened when the response comes back, a more reliable way is to check if the current coordinator epoch is still the same as what's in the request."
115163488,2964,junrao,2017-05-08T02:04:51Z,Should we also remove the entries keyed by transactionalId in purgatory?
115163567,2964,junrao,2017-05-08T02:06:01Z,"Structure wise, maybe it's better to do this as part of TransactionMetadata.completeTransitionTo() so that we can limit the place where internals of TransactionMetadata are modified?"
115163598,2964,junrao,2017-05-08T02:06:23Z,"Hmm, is this right? We watch on transactionalId, not producerId. Also, since txnMarkerPurgatory is passed around to different classes, it's bit hard to track who is calling checkAndComplete on this purgatory. An alternative way is to expose an access method to txnMarkerPurgatory in TransactionChannelManager and let all classes call that method."
115294821,2964,hachikuji,2017-05-08T16:43:35Z,No strong preference if you already rejected the option.
115317861,2964,hachikuji,2017-05-08T18:23:05Z,nit: the `toShort` is not needed.
115317895,2964,hachikuji,2017-05-08T18:23:14Z,Maybe we could use the `Ongoing` state explicitly?
115320070,2964,hachikuji,2017-05-08T18:32:24Z,nit: the whole body of `handleInitPid` is misaligned.
115321540,2964,hachikuji,2017-05-08T18:38:41Z,"Hmm.. If the user is trying to add partitions before the previous transaction has completed, shouldn't that be INVALID_TXN_STATE?"
115322180,2964,hachikuji,2017-05-08T18:41:26Z,nit: I think this is more than an optimization: it is necessary for correctness because there is no guarantee that the client will receive the result of the EndTxnRequest.
115322387,2964,hachikuji,2017-05-08T18:42:17Z,"nit: comment misaligned. Also, pity we can't merge this branch with `CompleteCommit` somehow."
115323214,2964,hachikuji,2017-05-08T18:45:52Z,"Hmm... Isn't it possible that a client resends an EndTxnRequest while we are still in `PrepareCommit` or `PrepareAbort`. As long as the outcome matches, it seems we should accept those requests and perhaps return CONCURRENT_TRANSACTIONS?

Also, can we list the states we're catching here explicitly?"
115323957,2964,hachikuji,2017-05-08T18:49:13Z,Maybe we should log something in the else case?
115324352,2964,hachikuji,2017-05-08T18:51:00Z,"Could we check for the case explicitly with `txnManager.isCoordinatorFor(transactionalId)`? If the transactionalId did not migrate, then maybe it should be an illegal state."
115325045,2964,hachikuji,2017-05-08T18:53:47Z,nit: add newline
115337072,2964,hachikuji,2017-05-08T19:51:23Z,This should be `epochAndMetadata.transactionMetadata`?
115337702,2964,hachikuji,2017-05-08T19:54:22Z,Same here: `epochAndMetadata.transactionMetadata`.
115338461,2964,hachikuji,2017-05-08T19:57:45Z,Seems this class doesn't depend on the containing instance of `TransactionMarkerChannel`. Can we move it outside or to the companion object?
115339106,2964,hachikuji,2017-05-08T20:01:02Z,nit: `private[transaction]`?
115339988,2964,hachikuji,2017-05-08T20:05:42Z,"Might be worth noting that `size` is O(n). Also, maybe `Queued` is redundant given the name of the class. Maybe it could just be `totalRequests`?"
115340279,2964,hachikuji,2017-05-08T20:06:54Z,Could this be `private[transaction]`? Also I found the name a bit misleading: would `TransactionMarkerQueue` or `TransactionMarkerAccumulator` be closer?
115340733,2964,hachikuji,2017-05-08T20:09:05Z,Maybe `brokerRequestQueues`?
115350244,2964,hachikuji,2017-05-08T20:51:55Z,"It would be nice to decouple `TransactionStateManager` from `TransactionMarkerChannel`. One way to do this is to move the building of the `RequestAndCompletionHandler` objects into `requestGenerator`. You can let `drainQueuedTransactionMarkers` return something like `Map[Int, List[TxnIdAndMarkerEntry]]`. Then `TransactionMarkerChannel` would not need a reference to `TransactionStateManager`. Also, the generator pattern is a little odd. Could we just make `InterBrokerSendThread` an abstract class with an abstract method `generateRequests`? In that case, you could let `TransactionMarkerChannelManager` extend directly from `InterBrokerSendThread` and implement the request generation."
115351637,2964,hachikuji,2017-05-08T20:58:10Z,Kind of unfortunate that we have a dependence on `NetworkClient` only in order to invoke this `wakeup()`. I'm wondering if the callers could do it instead and we can remove the dependence? That will help to decouple the objects which will make testing easier.
115352392,2964,hachikuji,2017-05-08T21:01:56Z,Why do we do this? Maybe worth a comment.
115353412,2964,hachikuji,2017-05-08T21:07:07Z,Also it seems like a good idea to verify that the state of the transaction is still what we expect.
115353802,2964,hachikuji,2017-05-08T21:09:02Z,I think `REQUEST_TIMEOUT` might be another possibility.
115354587,2964,hachikuji,2017-05-08T21:13:06Z,I wonder if you can add a comment explaining when this case can happen.
115356024,2964,hachikuji,2017-05-08T21:20:23Z,Sounds good. Will that be part of this patch?
115356574,2964,hachikuji,2017-05-08T21:23:11Z,The cast to `FileRecords` is not totally safe. I fixed the same problem in `GroupMetadataManager` recently.
115356894,2964,hachikuji,2017-05-08T21:24:41Z,nit: pattern match?
115359708,2964,hachikuji,2017-05-08T21:37:56Z,"A cache change due to emigration/immigration would be handled by the epoch check, right? Are there any cases where `completeTransitionTo` itself could fail?"
115377273,2964,guozhangwang,2017-05-08T23:25:55Z,ack.
115377857,2964,guozhangwang,2017-05-08T23:30:27Z,"> Any pending loadTransaction task should be completed very quickly after removeTransactions() is called.

We could be loading for partition X while removing for partition Y in this case, so waiting for the removing to complete may still take long time right?

Another benefit to let all three operations (loading, removing-partition, expiring-txns) to be execute by the same back ground thread is that locking mechanism would be a bit easier to reason (this is not part of this PR): only background thread would grab the write lock, and the handler thread will only try to grab read lock, and handler thread would not add / remove any entries from the maps but just modify the objects in-place."
115378256,2964,guozhangwang,2017-05-08T23:33:08Z,"Not sure I understand your question? `coordinatorEpoch` is the value passed from the caller, which is the epoch of the cached metadata object before append is called, and here we are checking if it is still the same epoch by reading from the cache again. Plus inside `completeTransitionTo` we also check that other fields inside metadata are expected."
115378411,2964,guozhangwang,2017-05-08T23:34:33Z,ack.
115378474,2964,guozhangwang,2017-05-08T23:35:07Z,ack.
115378959,2964,guozhangwang,2017-05-08T23:38:52Z,"We only allow `Empty` -> `Empty` transition, and it is a special case for adding a txnID for the first time, the inserted entry will be Empty and its pending state also Empty. Only when the pending state is cleared after the txn log write returns it is considered ""created succesfully"".

As for this function itself, it is only used when `initPid` is received while the current PID has an ongoing txn, i.e. it is `Ongoing`, and later we check if it is `Ongoing` we will abort the txn first and return `CONCURRENT_TXN` to let user retry."
115378997,2964,guozhangwang,2017-05-08T23:39:07Z,ack.
115379242,2964,guozhangwang,2017-05-08T23:41:29Z,"That is true: when the pid is first created, there is no txn yet, the startTime is set to ""-1"", when we received the first addPartitions we will update the startTime to `now`, and it will not be updated until it has completed, and then a new addPartition is received indicating the start of a new txn for this pid."
115379741,2964,guozhangwang,2017-05-08T23:45:48Z,ack.
115379921,2964,guozhangwang,2017-05-08T23:47:17Z,Good point! ack.
115380508,2964,guozhangwang,2017-05-08T23:51:53Z,"yeah I have thought about that too.. The approach is that if we cannot locate the broker to offer to its queue then in order for this thread to return we likely need to put it in an ""unknown broker"" queue first, and periodically we can check the queue and migrate to the found brokers. Does that make sense?"
115381202,2964,guozhangwang,2017-05-08T23:57:25Z,"I have not made up my mind about infinite timeout, and would like to have another discussion with you (as I left some comments in Damian's previous patch: 

https://github.com/apache/kafka/pull/2849#discussion_r113353056

For now I'll add a TODO marker and do that in a follow-up once we have a conclusion."
115381919,2964,guozhangwang,2017-05-09T00:02:41Z,"We check that in the `appendToLogCallback` callback in `txnMarkerChannelManager`. Generally speaking here is the reasoning:

1. for the handler logic, the only error code that we should not retry is ""coordinatorEpochNotValid"", in which we should complete the delayed operation immediately with a meaningful error code (currently it is always `NONE` ). 

2. then in `appendToLogCallback`, we check the error code, and if it is due to received `coordinatorEpochNotValid` we can simply ignore the rest of the operations."
115382316,2964,guozhangwang,2017-05-09T00:05:42Z,"ack. I have re-factored the handling logic here to cover all the error codes, please take a look at the modified file."
115382553,2964,guozhangwang,2017-05-09T00:07:44Z,"Not sure I understand the comment, we are indeed canceling for the transactionalId right?"
115382668,2964,guozhangwang,2017-05-09T00:08:47Z,ack.
115388039,2964,guozhangwang,2017-05-09T00:57:01Z,ack.
115388145,2964,guozhangwang,2017-05-09T00:57:55Z,ack.
115388336,2964,guozhangwang,2017-05-09T00:59:38Z,"Well, since we return to the user right after we have appended the log, in which case the state has updated to `PrepareXX`, the client code is possible to send another `addPartitions` while the sending markers are still on the flight. In this case we should not return a fatal error but let client retry."
115388370,2964,guozhangwang,2017-05-09T01:00:07Z,"Okay, will update the comments."
115388416,2964,guozhangwang,2017-05-09T01:00:37Z,ack.
115388582,2964,guozhangwang,2017-05-09T01:02:29Z,Good point! ack.
115388842,2964,guozhangwang,2017-05-09T01:05:18Z,"EDIT: actually I think for most cases this case should be covered in `txnMetadata.pendingTransitionInProgress`, the only edge case that we have migrated to `prepareXX` but have not prepareTo `completeXX` (note they should happen fairly consecutively). Anyways, will add them to cover it as well."
115389069,2964,guozhangwang,2017-05-09T01:08:13Z,ack.
115389114,2964,guozhangwang,2017-05-09T01:08:46Z,"good point, ack."
115536083,2964,hachikuji,2017-05-09T16:28:18Z,"Ack, makes sense."
115614545,2964,guozhangwang,2017-05-09T22:09:10Z,ack.
115614791,2964,guozhangwang,2017-05-09T22:10:33Z,ack.
115614938,2964,guozhangwang,2017-05-09T22:11:25Z,ack. This will be part of the minor refactoring that exhausts all the possible error codes in the writeTxnMarker responses.
115616278,2964,guozhangwang,2017-05-09T22:20:04Z,"Actually I just realized we do not need to check the coordinator epoch anymore, since it is already checked in `appendTransactionToLog#updateCacheCallback`, which is executed before calling this callback, so the error code returned is already reflecting the fact if 

```
(epochAndMetadata.coordinatorEpoch == coordinatorEpoch && metadata.completeTransitionTo(newMetadata))
```

So we can directly go head with the error code here."
115624264,2964,guozhangwang,2017-05-09T23:17:10Z,"ack. After thinking about this, I'm going to merge the `markerChannel` into `markerChannelManager` since it is always a one-to-one mapping."
115624364,2964,guozhangwang,2017-05-09T23:17:56Z,Ack.
115624544,2964,guozhangwang,2017-05-09T23:19:24Z,Ack.
115624880,2964,guozhangwang,2017-05-09T23:21:56Z,ack.
115624941,2964,guozhangwang,2017-05-09T23:22:30Z,ack. Let me know WDYT about the after-refactoring.
115625053,2964,guozhangwang,2017-05-09T23:23:25Z,ack.
115625404,2964,guozhangwang,2017-05-09T23:26:02Z,"I'm not sure either, it is from the old code. My understanding is that `networkClient` is only pollable from the `sendThread`, and hence `wakeup` is not required. But maybe I'm missing something here, cc @dguy "
115628649,2964,guozhangwang,2017-05-09T23:52:22Z,rebased this already.
115628821,2964,guozhangwang,2017-05-09T23:53:43Z,Could you elaborate a bit? We only have logic for the `if` condition?
115629030,2964,guozhangwang,2017-05-09T23:55:31Z,"Good point, we could consider throw exception directly instead of returning false.

@junrao @hachikuji There are a few of those suggestions that we can do in a follow-up PR. I have marked those places with `TODO`s."
115636450,2964,guozhangwang,2017-05-10T01:06:03Z,"I checked the source code, `TimeoutException` can only be thrown from producer / consumer internals, but they will never be returned from the broker side. So this should not be a possibility? @hachikuji "
115643581,2964,hachikuji,2017-05-10T02:26:46Z,@guozhangwang Did you also look for uses of `Errors.REQUEST_TIMEOUT`? 
115647476,2964,guozhangwang,2017-05-10T03:21:13Z,You mean `REQUEST_TIMED_OUT`? There is no `REQUEST_TIMEOUT` in Errors. For the latter case I searched the code and found no return errors from broker-side.
116116648,2964,hachikuji,2017-05-11T22:25:56Z,"After we remove `TransactionMarkerChannel`, maybe we can claim its name for this class?"
116124062,2964,hachikuji,2017-05-11T23:17:38Z,"Can we mention the partition number? Also, ""may likely has emigrated"" -> ""has likely emigrated""?"
116124812,2964,hachikuji,2017-05-11T23:23:57Z,"If the epoch has changed, is it still necessary or safe to continue with the logic below? In particular, we could still remove the partitions from `TransactionMetadata`. Maybe we should just return?"
116125417,2964,hachikuji,2017-05-11T23:28:33Z,We discussed offline changing this to assert a valid transition instead of returning a boolean. We could do this in a follow-up if you prefer.
116125544,2964,hachikuji,2017-05-11T23:29:31Z,"Should we return here? Otherwise, the call to `completeSendMarkersForTxnId` will be invoked below. Not sure it's a problem, but seems odd.

Also, I'm not sure I fully understand the chain of operations that this should trigger. It seems that `removeMarkersForTxnId` cancels the `DelayedTxnMarker`, which means its callback won't get invoked. But what happens to the state of the `TransactionMetadata`? It seems like it will just remain indefinitely in one of the Prepare states. For CoordinatorFenced, that seems OK; we rely on the partition to ultimately be evicted. How about ProducerFenced? I'm actually having a tough time imagining a scenario where we would hit that error. The only one that comes to mind is if the coordinator has become a zombie, in which case we should get blocked by the coordinator epoch. Are there any others? In any case, I think it would be helpful to add some comments here clarifying the scenario and the handling expectation."
116155904,2964,guozhangwang,2017-05-12T05:39:45Z,"It is done already, but was not pushed somehow, could you check again now?"
116156002,2964,guozhangwang,2017-05-12T05:40:42Z,This call is just for removing the delayed operation in the txn marker purgatory. I think it is still safe.
116156202,2964,guozhangwang,2017-05-12T05:44:07Z,"`TransactionMetadata` should be removed by the emmigration handler thread, and in the future the PID expiration scheduler's thread. Other handling logic should never remove the entry, but just to update the entry in-place.

And about returning here: good catch! yeah we should not call `completeSendMarkersForTxnId` in this case. Will fix now."
116312856,2964,junrao,2017-05-12T20:00:37Z,reaperEnabled doesn't seem be be used.
116323993,2964,junrao,2017-05-12T21:03:33Z,Is appending w/o synchronization on coordinator epoch safe? We don't want to write to the log if the coordinator epoch has changed. The most reliable way is probably to hold a read lock on TransactionStateManager.stateLock() while doing the log append.
116338170,2964,junrao,2017-05-12T22:54:25Z,"Hmm, not sure if this is completely safe. In removeTransactionsForTxnTopicPartition(), we modify loadingPartitions in the scheduler and here, we modify loadingPartitions in the method directly. This means that if an emmigration is followed immediately by an immigration, the updating of loadingPartitions by emmigration could happen after the updating of loadingPartitions by immigration, which will leave the state incorrect."
116339508,2964,junrao,2017-05-12T23:08:39Z,"The issue of changing the transaction states in the scheduler is that when this call returns, the new transaction states are not necessarily reflected. So, another request after this call may still see the old states?"
116340857,2964,junrao,2017-05-12T23:24:13Z,"txnMarkerPurgatory registers each item under transactionalId and txnTopicPartition. We are only removing the item registered under transactionalId. So, which process will be removing the item registered under txnTopicPartition, especially the reaper thread is not running in txnMarkerPurgatory?"
116342175,2964,junrao,2017-05-12T23:42:41Z,"Since we always send None as error in DelayedTxnMarker, it seems that we can just get rid of error?"
116343892,2964,junrao,2017-05-13T00:10:43Z,"Yes, it's just that in the case that a transaction coordinator's epoch has changed, there is no need to keep resending the WriteMarker request to the brokers."
116638694,2964,guozhangwang,2017-05-16T01:44:41Z,ack.
116638797,2964,guozhangwang,2017-05-16T01:46:02Z,"Yup, it is going to be done in the locking PR that I'm also working on now. Just trying to keep each PR small to make it easier for reviews."
116638956,2964,guozhangwang,2017-05-16T01:47:58Z,Good catch! ack.
116639242,2964,guozhangwang,2017-05-16T01:51:08Z,It is in the `TransactionMarkerChannelManager#removeMarkersForTxnTopicPartition`.
116639395,2964,guozhangwang,2017-05-16T01:52:49Z,"Hmm that is right, but I feel we will probably need to add other possible error codes as we as fixing various error cases under integration tests, so I'd rather keep it as is for now. If after exactly-once has been quite stable and we still do not have any other error code we can remove it. 

But let mw add a TODO for now in case we forgot.."
116650638,2964,guozhangwang,2017-05-16T03:53:57Z,Makes sense. I will add the check.
116653014,2964,guozhangwang,2017-05-16T04:29:03Z,"Got it. However, if we just do the removal in the handler thread, then the immigrate-then-emigrate issue may occur. On the other hand, we cannot depend on scheduler do not have pending request since we are periodically schedule txn-expiration and pid-expiration with that scheduler as well.

So here is what I will do: add a removal partitions which will be modified in the handler thread, then get txn metadata will check if the txn partition is in the removal partitions set.

Then in the locking PR, I will add the read-write-lock so that checking will be monitored by the read lock. WDYT?"
79560389,1884,dguy,2016-09-20T08:35:42Z,"private?
"
79560800,1884,dguy,2016-09-20T08:38:32Z,"delegate to the other constructor:
`this(config, 0, WINDOW_CHANGE_LOG_ADDITIONAL_RETENTION_DEFAULT)`
"
79562570,1884,dguy,2016-09-20T08:49:18Z,"If the topic exists we need to make sure the number of partitions is the same. If it isn't the same then we should throw an exception.
"
79563507,1884,dguy,2016-09-20T08:54:49Z,"Doesn't look like this method is used anywhere?
"
79566464,1884,dguy,2016-09-20T09:12:18Z,"Is this needed? It is not used anywhere
"
79566488,1884,dguy,2016-09-20T09:12:26Z,"As above
"
79566646,1884,dguy,2016-09-20T09:13:21Z,"This doesn't need to be a field. Could be a local in the constructor
"
79566686,1884,dguy,2016-09-20T09:13:42Z,"As above
"
79566702,1884,dguy,2016-09-20T09:13:50Z,"As above
"
79567055,1884,dguy,2016-09-20T09:15:58Z,"nit: Using an instance variable to reference a static. Should be: 
`this.metadata = new Metadata(streamsConfig.getLong(StreamsConfig.RETRY_BACKOFF_MS_CONFIG), streamsConfig.getLong(StreamsConfig.METADATA_MAX_AGE_CONFIG));`
"
79568023,1884,dguy,2016-09-20T09:21:29Z,"Should this just use `StreamsConfig.METRIC_REPORTER_CLASSES_CONFIG`? And then I don't see why we need line 100
"
79568120,1884,dguy,2016-09-20T09:22:09Z,"`StreamsConfig.CONNECTIONS_MAX_IDLE_MS_CONFIG`
"
79568305,1884,dguy,2016-09-20T09:23:19Z,"As above. Next 4 lines should use the class to reference static fields, i.e., `StreamsConfig.RECONNECT_BACKOFF_MS_CONFIG`
"
79568817,1884,dguy,2016-09-20T09:25:55Z,"`private static final MAX_ITERATIONS` ?
"
79569302,1884,dguy,2016-09-20T09:28:55Z,"nit: 
My preference is that all immutable parameters and fields are `final`.  
So:
`StreamsKafkaClient(final StreamsConfig streamsConfig)`
and below:
`final Time time = ...`
etc
"
79569654,1884,dguy,2016-09-20T09:31:03Z,"Should we make this into a final static field? i.e., `MAX_INFLIGHT_REQUESTS` or similar?
i.e., what is 100?
"
79569944,1884,dguy,2016-09-20T09:32:32Z,"What is the 0 for? Can we make it into a static final field, so it has a name? 
"
79570181,1884,dguy,2016-09-20T09:33:38Z,"Do we need this? Don't think it is used anywhere
"
79570479,1884,dguy,2016-09-20T09:35:10Z,"I don't currently see this being used anywhere. Is it going to cause shutdown problems if this isn't closed properly? i.e., will there be non-daemon threads hanging around that cause the JVM to not shutdown?
"
79570684,1884,dguy,2016-09-20T09:36:23Z,"`if (...) { .. }`
"
79570848,1884,dguy,2016-09-20T09:37:04Z,"this should probably throw `StreamsException` as that is the top-level exception for streams
"
79571533,1884,dguy,2016-09-20T09:40:52Z,"We should change `internalTopicConfig.toProperties(...)` to return a `Map<String, String>` then we won't need to copy the properties into the `topicConfig` map below
"
79571804,1884,dguy,2016-09-20T09:42:36Z,"nit: as mentioned above. my preference is for params and locals to be final if possible.
I know it i adds few extra characters, but it shows intent and can prevent unnecessary bugs
"
79571936,1884,dguy,2016-09-20T09:43:23Z,"Move this down to line 149 and do the init and assignment on a single line
"
79572482,1884,dguy,2016-09-20T09:46:38Z,"I don't think you need this callback. You can just pass `null` if the callback isn't necessary
"
79573826,1884,dguy,2016-09-20T09:53:54Z,"It would be good if we could say why it failed. Is there a mapping from the error code to a string we could use?
"
79573949,1884,dguy,2016-09-20T09:54:37Z,"Do we need this method?
"
79575520,1884,dguy,2016-09-20T10:02:57Z,"I don't think this should be a field. It probably needs to be done before each request as
1. the least loaded node will change.
2. this node might not be up when the request is made.
"
79576598,1884,dguy,2016-09-20T10:08:42Z,"perhaps maxIterations should be a timeout instead? Also i think it reads cleaner like so:

```
final long timeout = systemTime.milliseconds + MAX_WAIT_TIME;
while (!kafka.ready(...)  && systemTime.milliseconds() < timeout) {
   kafkaClient.poll(...)
}
if (!kafkaClient.ready(...) {
    throw new StreamsException(...)
} 
kafkaClient.send(...)
```
"
79576919,1884,dguy,2016-09-20T10:10:44Z,"As above we should probably change `maxIterations` for a timeout
"
79577022,1884,dguy,2016-09-20T10:11:19Z,"why not just re-use the `systemTime` from above?
"
79577074,1884,dguy,2016-09-20T10:11:44Z,"Do we need this? It isn't used anywhere.
"
79577318,1884,dguy,2016-09-20T10:13:16Z,"Move this down to line 279
"
79577973,1884,dguy,2016-09-20T10:17:14Z,"nit: looks like the formatting has been unnecessarily changed.
"
79578001,1884,dguy,2016-09-20T10:17:27Z,"nit: formatting?
"
79578105,1884,dguy,2016-09-20T10:18:09Z,"Do we need this? I don't think we ever delete topics? Also the test is commented out
"
79578334,1884,dguy,2016-09-20T10:19:20Z,"Test commented out. Also we may not need this if we merge `StreamsKafkaClient` and `InternalTopicManager` into a single class.
"
79650885,1884,hjafarpour,2016-09-20T16:16:12Z,"Yes, made it private :)
"
79650926,1884,hjafarpour,2016-09-20T16:16:26Z,"Removed this constructor.
"
79652102,1884,hjafarpour,2016-09-20T16:22:05Z,"Added the check for the number of partitions.
"
79652577,1884,hjafarpour,2016-09-20T16:24:21Z,"Made them local in the constructor.
"
79652760,1884,hjafarpour,2016-09-20T16:25:17Z,"They were there from the previous iteration. Not being used anymore, removed them.
"
79655027,1884,hjafarpour,2016-09-20T16:35:23Z,"I used the similar way of creating NetworkClient in KafkaConsumer. For the StreamKafkaClient I can make it a constant as you mentioned. It would be good to update the KafkaConsumer too.
"
79655809,1884,hjafarpour,2016-09-20T16:38:26Z,"Yes, corrected them.
"
79656213,1884,hjafarpour,2016-09-20T16:40:12Z,"Good point, moved it to the request method as a local variable.
"
79657266,1884,hjafarpour,2016-09-20T16:44:47Z,"We were thinking about having StreamsKafkaClient available for other uses too. I'm removing this for now and we can add it later if we need it.
"
79657548,1884,hjafarpour,2016-09-20T16:46:06Z,"This was here from the previous version and I kept it. I'm going to remove it now since it is not being used.
"
79658172,1884,hjafarpour,2016-09-20T16:49:07Z,"That's a good practice. Made all of the params that won't change final.
"
79659263,1884,hjafarpour,2016-09-20T16:54:20Z,"Made it null.
"
79659681,1884,hjafarpour,2016-09-20T16:56:25Z,"I could get the error name or message. I am adding error name to the message.
"
79659793,1884,hjafarpour,2016-09-20T16:56:59Z,"It's not being used but we can have it in the StreamsKafkaClient for future use.
"
79666863,1884,hjafarpour,2016-09-20T17:28:40Z,"Changed them to timeouts.
"
79667197,1884,hjafarpour,2016-09-20T17:30:15Z,"Needed for checking the partition number.
"
79667509,1884,hjafarpour,2016-09-20T17:31:26Z,"I'm removing these tests then.
"
79777362,1884,dguy,2016-09-21T07:54:43Z,"I'd probably use the same Importance level as was used in the Consumer or Producer Config
"
79777945,1884,dguy,2016-09-21T07:58:22Z,"I know i said make this private...  Looking again it isn't actually used anywhere apart from the constructor, so it can be removed.
"
79778569,1884,dguy,2016-09-21T08:02:37Z,"I have a PR to remove this method, https://github.com/apache/kafka/pull/1888, as it is unused. If you want to remove it i'll close the PR? Also, if you do remove it then you can also remove line 37. 

EDIT:
Sorry @hjafarpour the PR was merged before I'd seen your update. 
"
79779302,1884,dguy,2016-09-21T08:07:32Z,"A better way to do this would be to pass in the StreamsKafkaClient, i.e.,
`public InternalTopicManager(final StreamsKafkaClient streamsKafkaClient, final int replicationFactor, final long windowChangeLogAdditionalRetention)`

Why? well the `StreamsConfig` is only used to construct the `StreamsKafkaClient` and it would mean in unit tests we can pass in a Stub or Mock  for `StreamsKafkaClient`. We can then remove the no-arg constructor on line 60 (It is only used in `MockInternalTopicManager`)
"
79782495,1884,dguy,2016-09-21T08:27:12Z,"Strictly not part of this PR, but might as-well make these params `final` :-)
"
79783003,1884,dguy,2016-09-21T08:29:39Z,"`final`
"
79785267,1884,dguy,2016-09-21T08:42:57Z,"I'm thinking in `StreamsKafkaClient.getTopicMetadata(..)` we could just return the `MetadataResponse.TopicMetadata` regardless of the `error().code()`.  So then i think we will never get `null` here as i believe we will always get a response. We can then throw an exception with a more meaningful  message, i.e., based on the `error().code()` - rather than just ""Topic metadata is corrupted""
"
79785576,1884,dguy,2016-09-21T08:44:40Z,"Looks like this can be a local now?
"
79785702,1884,dguy,2016-09-21T08:45:29Z,"Again, i'd make all the immutable locals `final` - it is a good habit to get into.
So, `metricsTags`, `metadata`,  `metricConfig`, `reporters`, `channelBuilder`, `selector`
"
79786661,1884,dguy,2016-09-21T08:50:25Z,"Again, `final` for all locals. 
Also, as per my previous comment on this. I think we should change `InternalTopicConfig.toProperties(...)` to return `Map<String, String>` and then we don't need to copy the props into another `Map`
"
79787573,1884,dguy,2016-09-21T08:55:18Z,"I still think if this is not being used anywhere we should remove it. If we really want to keep it then we need a test for it.
"
79787831,1884,dguy,2016-09-21T08:56:41Z,"""Deleting topic {} from brokers ..."" ?
"
79787999,1884,dguy,2016-09-21T08:57:38Z,"We can make all of these locals `final`
"
79788308,1884,dguy,2016-09-21T08:59:11Z,"maybe we should make this a better name, like `readyTimeout` ?
"
79788383,1884,dguy,2016-09-21T08:59:40Z,"Same here, maybe sth like: `requestTimeout`
"
79788859,1884,dguy,2016-09-21T09:02:16Z,"Probably need a more descriptive message, i.e.,
""Timed out waiting for node="" + brokerNode + "" to become available""
"
79789101,1884,dguy,2016-09-21T09:03:54Z,"I think this is more like:
""Failed to get response from node="" + brokerNode + "" within timeout""
"
79789452,1884,dguy,2016-09-21T09:06:15Z,"We can remove this as the callback isn't required. We can also remove the `callback` param from `sendRequest(..)` as we don't need it.
"
79789680,1884,dguy,2016-09-21T09:07:42Z,"private? is only used in this class, so there is no need to make it public.
Also, as per comment below, we can probably remove the `callback` arg as we don't really need it, i.e., we can just pass `null` for the callback.
"
79790026,1884,dguy,2016-09-21T09:10:00Z,"See my comment above in `InternalTopicManager`; we can probably just return the matching `MetadataResponse.TopicMetadata` instance here. As far as i know it will never be null.
If we don't find one then we should probably throw an exception
"
79790100,1884,dguy,2016-09-21T09:10:25Z,"`final` locals...
"
79790238,1884,dguy,2016-09-21T09:11:16Z,"`final` param and locals
"
79790517,1884,dguy,2016-09-21T09:13:01Z,"`props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, Serdes.String().deserializer().getClass())`

Same on next line
"
79790921,1884,dguy,2016-09-21T09:15:01Z,"nit: `new KafkaConsumer<>(props);`
"
79791157,1884,dguy,2016-09-21T09:16:14Z,"nit: formatting
i.e., `for (String topicNameInList : topics.keySet())`
"
80079829,1884,hjafarpour,2016-09-22T16:21:32Z,"Changed it to MEDIUM.
"
80080356,1884,hjafarpour,2016-09-22T16:24:26Z,"Removed it.
"
80080581,1884,hjafarpour,2016-09-22T16:25:36Z,"Removed it, please close the PR.
"
80081058,1884,hjafarpour,2016-09-22T16:28:15Z,"Good point, made the change accordingly!
"
80081221,1884,hjafarpour,2016-09-22T16:29:01Z,"They are final now :)
"
80082570,1884,hjafarpour,2016-09-22T16:36:00Z,"Made the change, include error code in the message.
"
80082828,1884,hjafarpour,2016-09-22T16:37:24Z,"Yes, after the previous changes it only is used in one method. Changed it to local.
"
80083158,1884,hjafarpour,2016-09-22T16:39:06Z,"All are final now :)
"
80083950,1884,hjafarpour,2016-09-22T16:43:10Z,"The method is being used in quite a few tests too. If I change it I should change those tests too. Do you want me to go ahead and make the change?
"
80084032,1884,hjafarpour,2016-09-22T16:43:33Z,"Removed it.
"
80084885,1884,hjafarpour,2016-09-22T16:48:09Z,"Made the change!
"
80200474,1884,dguy,2016-09-23T08:21:18Z,"Can remove this as it is no longer used
"
80200580,1884,dguy,2016-09-23T08:22:06Z,"`final` params?
"
80204324,1884,dguy,2016-09-23T08:47:49Z,"I think we can remove this constructor now. It is only used in the constructor of `MockInternalTopicManager`.
So, we could change the `MockInternalTopicManager` constructor to also take a `StreamsConfig` as a param. The `StreamsConfig` could be minimal, i.e, just `APPLICATION_ID_CONFIG` and `BOOTSTRAP_SERVERS_CONFIG` would need to be set.
Then in `MockInternalTopicManager` constructor we just do:
`super(new StreamsKafkaClient(streamsConfig), 0, 0)`

And then we can remove this constructor as it is not needed. Also means we can mark `streamsKafkaClient` on line 39 as `final` 
"
80204544,1884,dguy,2016-09-23T08:48:56Z,"Is there a mapping from the `short` code to a String somewhere? 
"
80204699,1884,dguy,2016-09-23T08:49:56Z,"Can remove this TODO as we need these 2 config properties. All the others have been removed.
"
80205835,1884,dguy,2016-09-23T08:57:19Z,"Can remove this as it is not used.
"
80206000,1884,dguy,2016-09-23T08:58:35Z,"`final`
"
80206022,1884,dguy,2016-09-23T08:58:43Z,"`final`
"
80207103,1884,dguy,2016-09-23T09:06:04Z,"`final`
"
80207117,1884,dguy,2016-09-23T09:06:12Z,"`final`
"
80207158,1884,dguy,2016-09-23T09:06:30Z,"`final`
"
80207228,1884,dguy,2016-09-23T09:07:00Z,"`final`
"
80207325,1884,dguy,2016-09-23T09:07:37Z,"`Collections.singletonList(topic)`
"
80213799,1884,dguy,2016-09-23T09:48:38Z,"It would be great if you do, but we can probably do it as another PR if you'd prefer. Up to you
"
80285978,1884,hjafarpour,2016-09-23T17:10:59Z,"Removed it.
"
80286072,1884,hjafarpour,2016-09-23T17:11:32Z,"Removed it.
"
80288512,1884,hjafarpour,2016-09-23T17:25:49Z,"Couldn't find it in the docs. In the code we have ""org.apache.kafka.common.protocol.Errors"". I can also print message() or exceptionName() instead of the code. Adding the message().
"
80288656,1884,hjafarpour,2016-09-23T17:26:32Z,"Removed it.
"
80295804,1884,dguy,2016-09-23T18:03:55Z,"Do we need to add the `JmxReporter` here? The `TODO` suggest we should be doing something differernt
"
80505313,1884,hjafarpour,2016-09-26T15:42:06Z,"@dguy in KafkaStreams.java JMX_PREFIX is defined private:
private static final String JMX_PREFIX = ""kafka.streams"";

I could either make it public and use it here or define a new field here. Which one would you suggest?
"
80983996,1884,guozhangwang,2016-09-28T18:19:59Z,"Do we still need this since we already have `testCompile project(':core')` in line 704 which should bring in zkClient jars transitively?
"
80986713,1884,guozhangwang,2016-09-28T18:32:04Z,"nit: Instead of declaring a new doc variable, could we just refer to `CommonClientConfigs.XXX_DOC` in the `.define`?
"
80987096,1884,guozhangwang,2016-09-28T18:33:35Z,"We could use a smaller default value as this is only used for admin requests that are mostly small.
"
80987442,1884,guozhangwang,2016-09-28T18:35:14Z,"`final`
"
80987926,1884,guozhangwang,2016-09-28T18:37:27Z,"Are these two props `CLEANUP_POLICY_PROP` and `RETENTION_MS` used anywhere any more?
"
80990290,1884,guozhangwang,2016-09-28T18:48:09Z,"Also should we remove `ZOOKEEPER_CONNECT_CONFIG` above as well?
"
80990884,1884,guozhangwang,2016-09-28T18:50:49Z,"@dguy @hjafarpour In there future we will have an `AdminClient` as part of completing KIP-4 which is used for all such admin requests, and that can be used in KStream, KConnect, Replicator, MM, etc. And whoever is about to implementing it would be suggested to borrow from this class.

So I think it is OK to keep it as is for internals, and replacing it with the `AdminClient` in the future.
"
80992348,1884,guozhangwang,2016-09-28T18:57:47Z,"Currently the embedded client: producer, consumer, and admin, have their own metrics and reporters, and we are only correlating them with the `clientId` in the tags. It is better to be improved with hierarchical metrics moving forward. For now I think we can just follow this way but change the prefix in `JmxReporter` from `kafka.streams` to `kafka.admin`?
"
80992958,1884,guozhangwang,2016-09-28T19:00:21Z,"The comment is a bit misleading: `Polls the request for a given number of iterations to receive the response.`

Isn't it `keep polling until the corresponding response is received`?
"
80996029,1884,guozhangwang,2016-09-28T19:16:18Z,"If the broker node is not ready, should we consider picking a different broker instead of tie-ing up one thread doing the while loop here?
"
80996757,1884,guozhangwang,2016-09-28T19:20:27Z,"Is it a good behavior that we are simply dropping all other responses on the floor while waiting for the corresponding response? I think today we will not encounter such issues since we always send one request, and block on its response and then send another one. But this is less efficient since with N topics to create we have to go N round trips now, suppose moving forward we will do that in a more batched manner where multiple in-flight requests exist, then I this will be an issue.

So instead of checking if the response, could we use the `RequestCompletionHandler` interface with sth. similar to consumer where we poll until the handler set the `Future` indicating it is received and processed?
"
80996887,1884,guozhangwang,2016-09-28T19:21:07Z,"Using another temporary consumer is very inefficient, could we just use `ListOffsetRequest` with the admin client here?
"
81010527,1884,guozhangwang,2016-09-28T20:33:33Z,"Related to the comment below: since only one request is sent at a time, the `leastLoadedNode` function is not taking any load into consideration actually; so I think we could just iterate over the nodes and find one that is ready, and if all destination nodes are not ready, backoff based on the configured value and retry again.
"
81013284,1884,dguy,2016-09-28T20:46:42Z,"@guozhangwang will sending `ListOffsetRequest` result in creating the topic when `auto.topic.create` is true?
"
81014530,1884,guozhangwang,2016-09-28T20:52:50Z,"Sorry I meant `MetadataRequest`, not `ListOffsetRequest`.

If you specify the topics to be the empty list (i.e. `ALL_TOPICS_REQUEST`), then the broker will not create any topics even with `auto.topic.create` is true. 
"
81044949,1884,mjsax,2016-09-29T00:22:36Z,"This class is still there. I did not follow the whole discussion, but I agree with @dguy that we might want to remove this class... What is the reason for keeping it?
"
81047430,1884,guozhangwang,2016-09-29T00:47:43Z,"In `StreamPartitionAssignor`, there is a while loop checking that the metadata has been refreshed with the right number of partitions:

```
// wait until the topic metadata has been propagated to all brokers
                List<PartitionInfo> partitions;
                do {
                    partitions = streamThread.restoreConsumer.partitionsFor(topic.name());
                } while (partitions == null || partitions.size() != numPartitions);
```

which is error-prone. Could we remove that logic and check that the topic metadata has propagated to to the broker with the same `streamsKafkaClient.topicExists(topic.name()`, i.e.:

```
while (!streamsKafkaClient.topicExists(topic.name()) {
   ...
}

if (...) {
    // check num partitions is correct, etc.
}
```
"
81047544,1884,guozhangwang,2016-09-29T00:48:38Z,"Just FYI @hachikuji and I found the original while-loop may be the culprit of the recent unit test hanging issue.
"
81155529,1884,enothereska,2016-09-29T14:52:59Z,"So we still need to connect to zookeeper directly to do this verification? We can't get rid of the ZK dependency in tests?
"
81163558,1884,dguy,2016-09-29T15:25:40Z,"@guozhangwang it is a combination of that and auto topic creation
"
84377963,1884,guozhangwang,2016-10-20T21:18:21Z,"Can we merge these two functions `filterExistingTopics` and `getTopicsToBeDeleted` into a single one, or just in-line this logic in the `makeReady` function as this seems specific in the InternalTopicManager, not in the KafkaClient.
"
84378019,1884,guozhangwang,2016-10-20T21:18:43Z,"This function seems not used any more.
"
84378082,1884,guozhangwang,2016-10-20T21:19:06Z,"Ditto below.
"
84378389,1884,guozhangwang,2016-10-20T21:21:11Z,"This can be private.
"
84378437,1884,guozhangwang,2016-10-20T21:21:27Z,"Not used any more.
"
84378526,1884,guozhangwang,2016-10-20T21:22:03Z,"Not used any more.
"
84378580,1884,guozhangwang,2016-10-20T21:22:22Z,"the function name needs to be updated to `createTopics`.
"
84379587,1884,guozhangwang,2016-10-20T21:28:19Z,"Do we want to use the same value for `NetworkClient.poll()` timeout, and as well as the timeout value of `create/deleteTopic` requests? In addition the `MAX_WAIT_TIME_MS` is 30 seconds which is even lower than the default value of timeout, so it is likely that the `client.poll()` will not return even when `MAX_WAIT_TIME_MS` has elapsed.
"
84737925,1884,hjafarpour,2016-10-24T17:24:12Z,"@guozhangwang pushed a new version with the updates you mentioned.
"
94757625,1884,ijuma,2017-01-05T12:12:49Z,We should not add this file back. We removed it intentionally.
94757650,1884,ijuma,2017-01-05T12:13:06Z,We should not add this file back. We removed it intentionally.
94757707,1884,ijuma,2017-01-05T12:13:39Z,Is this dependency still needed? The comment seemed to imply that it wasn't.
94758189,1884,ijuma,2017-01-05T12:17:50Z,"Instead of instantiating `SystemTime`, `Time.SYSTEM` should be used. Same applies for other instances where we are creating `SystemTime` instances."
95036491,1884,guozhangwang,2017-01-06T23:04:12Z,I think `jackson` is not needed either since it was only for JSON parsing.
95036696,1884,guozhangwang,2017-01-06T23:06:15Z,@mjsax You mean we should just inline this class inside `StreamPartitionAssignor`? Personally I feel that class is already quite large and the functionalities in this class is self-contained.
95036764,1884,guozhangwang,2017-01-06T23:06:53Z,can be private.
95036888,1884,guozhangwang,2017-01-06T23:08:00Z,Better add a log entry here since otherwise the error message in `StreamsException` will never be shown anywhere.
95037077,1884,guozhangwang,2017-01-06T23:09:48Z,Can we just inline this function in the other `makeReady` since its only caller is the other function?
95037147,1884,guozhangwang,2017-01-06T23:10:18Z,can be private.
95037232,1884,guozhangwang,2017-01-06T23:11:02Z,"Why not catch exceptions thrown here as well? Also we should move it as well as the metadata fetching requests (line 60 - 63) inside the retry block as well since each time we retry, the metadata may have changed, right?"
95037434,1884,guozhangwang,2017-01-06T23:13:04Z,nit: this line not needed.
95038339,1884,guozhangwang,2017-01-06T23:22:14Z,"I think this is not a good pattern, since AFAIK unlike the other clients the only `throwable` is actually `IOException` (your error message also indicates that doesn't it :P `KafkaStreamClient`), and it is only thrown from the metricsReporter.close(), capturing all throwable may hide some issues. Instead, we can just let `InternalTopicManager.close()` to capture and log if there is any `IOException` and not throw it all the way here."
95038406,1884,guozhangwang,2017-01-06T23:22:46Z,@hjafarpour Could you reply to this comment as well?
95038495,1884,guozhangwang,2017-01-06T23:23:42Z,BTW I think this class will eventually be merged into o.a.k.common admin package when KIP-4 is completed. cc @ijuma 
95039652,1884,hjafarpour,2017-01-06T23:35:28Z,Deleted the file!
95039666,1884,hjafarpour,2017-01-06T23:35:37Z,Deleted the file.
95039719,1884,hjafarpour,2017-01-06T23:35:59Z,"Removed the line ""compile libs.jacksonDatabind""."
95039868,1884,hjafarpour,2017-01-06T23:37:29Z,Replaced it with Time.SYSTEM.
95040218,1884,hjafarpour,2017-01-06T23:41:29Z,Done!
95040389,1884,hjafarpour,2017-01-06T23:43:19Z,Good point! Moved all in the try/catch block.
95040644,1884,ijuma,2017-01-06T23:46:09Z,"@guozhangwang, yeah, we will have to refactor it to make it more generic, but good to have non-test code using the protocol. :)"
95040864,1884,hjafarpour,2017-01-06T23:48:44Z,Added a log message.
95041682,1884,hjafarpour,2017-01-06T23:57:25Z,Handling the IO Exception in InternalTopicManager.close() now.
95678659,1884,xvrl,2017-01-11T21:42:16Z,"This breaks backwards compatibility. Until now Streams did not delete existing topics. Until Streams offers a way to configure partition count and min.isr for internal topics, it should never attempt to delete topics. Even then it might be dangerous to delete existing topics without warning."
95980233,1884,enothereska,2017-01-13T11:34:33Z,I noticed this path is gone from the new code.
96026577,1884,guozhangwang,2017-01-13T16:38:25Z,"I thought @hjafarpour wrote it down on the KIP wiki but seems he's not, we discussed about this issue while proposing KIP-90. The problem is that even in the case that existing number of partitions is less than expected, we can not safely add partitions and reuse the existing ones for repartition topics or changelog topics, due to hashing."
96035255,1884,mjsax,2017-01-13T17:27:38Z,This is an issue that we did miss. :(
203197427,5379,stanislavkozlovski,2018-07-17T22:08:12Z,"`ScramMessages`'s value regex is `""[\\x01-\\x7F&&[^,]]+""`. I tried using it but could not get my tests to pass. 
To be frank, I don't understand it at all. Specifically the `&&[^,]` part.

[Regex101](https://regex101.com/r/ED5kuX/1) says 
> &&[^, matches a single character in the list &[^, (case sensitive)
but I doubt that that is the case, otherwise I think SCRAM would not work as well.
Is this some feature in the Java Regex engine I'm not familiar with?"
203203016,5379,stanislavkozlovski,2018-07-17T22:34:53Z,"I kind of want to have this in a more general space where every SaslClient will have this code.
An idea could be extend the SaslClient interface and provide this default method, or move it to some utils resource.
I'm not sure if it is worth the effort."
204061226,5379,rondagostino,2018-07-20T14:23:35Z,"Is the returned map supposed to be modifiable or unmodifiable?  The default value (set via Collections.emptyMap) is unmodifiable.  But if someone sets the map it isn't copied, so whether it will be modifiable or unmodifiable is non-deterministic.  I think it would be best to state in the Javadoc that the returned map is always unmodifiable, and when setting the map the input map should be copied and wrapped so as to be unmodifiable."
204063618,5379,rondagostino,2018-07-20T14:30:45Z,"I would be careful to state that ""You can also add custom unsecured SASL extensions when using the default, builtin{@link AuthenticateCallbackHandler} implementation using..."" because it is really the AuthenticateCallbackHandler instance that determines what kinds of callbacks are supported rather than this class itself."
204066800,5379,rondagostino,2018-07-20T14:40:10Z,"The token and the extensions must not be added until commit() is called as per the JAAS specification.  I would add a field ""extensionsRequiringCommit"" to mirror how the token is handled between the login() and commit()  methods.  Also, note that this LoginModule supports calling commit() when the Subject is shared with another instance of this class associated with a separate LoginContext and that other instance has not yet had its logout() method called (see the last paragraph of the Javadoc for details).  You will need to support this for the extensions as well as for the token.  You can mirror the existing code for the token and treat the extensions the same way.  Failure to do this will result in the extensions being deleted from the Subject when the original LoginContext has its logout() method called."
204068531,5379,rondagostino,2018-07-20T14:45:35Z,"Also, I am wondering if maybe we shouldn't simply attach a Map to the public credentials but should instead attach something more precise and fit-for-purpose.  The reason is because once we attach a Map we can't ever use a Map on the public credentials again; if we wanted to attach something else in the future it could not implement Map.  Map is very broad and limits flexibility in the future.  I am wondering if we should make the SaslExtensions class part of the public API and make it immutable.  Then we can attach an instance of that class specifically rather than just a Map and we don't constrain ourselves going forward.  @rajinisivaram what do you think?  SaslExtensionsCallback would return a SaslExtensions instance instead of a Map if we decide to do this, and it also removes the confusion about the modifiability/immutabililty of what SaslExtensionsCallback actually returns -- it will always be immutable."
204070121,5379,rondagostino,2018-07-20T14:50:10Z,Copy the map and store it immutably?
204071530,5379,rondagostino,2018-07-20T14:54:20Z,Don't need both properties and saslExtension -- probably just saslExtensions.
204073705,5379,rondagostino,2018-07-20T15:00:49Z,Please update Javadoc for the class to also state that the instance of {@code AuthenticateCallbackHandler} can optionally handle an instance of {@link SaslExtensionsTokenCallback} to return any extensions generated by the {@code login()} event on the {@code LoginContext}.
204074321,5379,rondagostino,2018-07-20T15:02:43Z,Typically catch Exception rather than Throwable since Error generally should not be caught as it denots an event that the code can't really do anything about.
204074903,5379,rondagostino,2018-07-20T15:04:34Z,Is there a reason to return the Callback?   I would think the calling code is interested in the extensions themselves rather than the callback.
204075829,5379,rondagostino,2018-07-20T15:07:37Z,Please update Javadoc to state that this class also recognizes {@link SaslExtensionsCallback} and retrieves any SASL extensions that were created when the {@code OAuthBearerLoginModule} logged in by looking for an instance of {@code Map} in the {@code Subject}'s public credentials. (or an instance of SaslExtensions rather than a Map if we decide to make SaslExtensions part of the public API)
204076627,5379,rondagostino,2018-07-20T15:10:19Z,"Can you name this in the same way as handleCallback(OAuthBearerTokenCallback) -- i.e. handleCallback(SaslExtensionsCallback)?  Unless there is a reason to do it differently?  Maybe you are foreshadowing a move of the method onto the OAuthBearerLoginModule class at some point?  If so, then maybe make the method static, name it handleCallback(), and make the handleCallback(OAuthBearerTokenCallback) symmetric by also accepting a Subject parameter?  I think there is value in making the two methods look very much the same except for the type of callback they accept, so whichever you decide let's make them look the same as much as possible."
204080059,5379,rondagostino,2018-07-20T15:21:26Z,"It is possible that the extensions could be set and then the process() method either returns an error response to the client or throws an exception.  It probably isn't too much of an issue if this happens, but best to set the extensions at the same point where tokenForNegotiatedProperty is set, so probably should add the extensions as a new parameter on the process() call."
204082009,5379,rondagostino,2018-07-20T15:27:35Z,"Probably good to state above ""A {@code CallbackHandler} that recognizes {@link OAuthBearerTokenCallback} to return an unsecured OAuth 2 bearer token and {@link SaslExtensionsCallback} to return  SASL extensions."""
204088230,5379,stanislavkozlovski,2018-07-20T15:47:49Z,Should be unmodifiable come to think of it. Updated docs and code
204088701,5379,stanislavkozlovski,2018-07-20T15:49:24Z,Agreed
204091548,5379,stanislavkozlovski,2018-07-20T15:59:31Z,That would be the best approach I think. I also found this way non-ideal but decided to stick with the implementation as with `ScramExtensions`
204093826,5379,stanislavkozlovski,2018-07-20T16:07:19Z,I see. Thanks for the clarification
204099967,5379,stanislavkozlovski,2018-07-20T16:29:38Z,"You're right, even the method name implies you get the extensions"
204100861,5379,stanislavkozlovski,2018-07-20T16:33:10Z,I wanted to document the code via more explicit names
204101263,5379,stanislavkozlovski,2018-07-20T16:34:38Z,Good catch
204110927,5379,rajinisivaram,2018-07-20T17:11:42Z,"@rondagostino @stanislavkozlovski Yes, it makes sense. We currently use `Map` for delegation tokens extension for SCRAM. Since we are only adding custom extensions to OAuth in this KIP, perhaps we should add `SaslExtensionsCallback` and `SaslExtensionsCallbackHandler` in `org.apache.kafka.common.security.auth` and use it only for `OAuth` for now. For SCRAM, we should probably stick to the `Map` for now, but we could have the interfaces extend the public interface. What do you think?"
204136508,5379,rondagostino,2018-07-20T18:43:20Z,"Agreed, we can focus on OAUTHBEARER for now, take the right steps (adding 2 classes to the public API instead of just the Callback class), and start to leverage the added public API classes in other mechanisms (i.e. SCRAM-related) if/when the time seems right.  I am less familiar with the SCRAM-related code, so I defer to whatever/whenever you feel is best."
204138659,5379,stanislavkozlovski,2018-07-20T18:51:36Z,Why should we not change SCRAM? Was the delegation token publicly accessible and that would break? I feel like we should change it outright if we can
204139145,5379,rondagostino,2018-07-20T18:53:16Z,"This is the only place where the separator field is used, and I'm not clear on the semantics of this method.  I think maybe the separator field can be dropped and eliminate that parameter from the constructor that takes a Map?"
204140072,5379,rondagostino,2018-07-20T18:56:56Z,Make unmodifiable via Collections.emptyMap()
204140395,5379,rondagostino,2018-07-20T18:58:19Z,Javadoc on public API classes and their methods is very helpful
204140849,5379,rondagostino,2018-07-20T19:00:23Z,Should this constructor accept the same parameters as Utils.parseMap()?
204141019,5379,rondagostino,2018-07-20T19:01:03Z,Probably remove separator parameter (and the field) as mentioned below
204141410,5379,rondagostino,2018-07-20T19:02:49Z,"Might want to add an extensionEntries() method that returns a Map.Entry<String, String>"
204142058,5379,rondagostino,2018-07-20T19:05:23Z,"Probably no need to make a copy since the map is unmodifiable; Javadoc can say it returns an unmodifiable Map view of the extensions.  This raises the question of whether all of the Map-related methods isEmpty(), extensionNames(), extensionEntries(), and extensionValue() are actually needed.  If returning the map is cheap -- which it would be -- then all of those methods feel like clutter rather than good API additions.  What do you think?"
204142539,5379,rondagostino,2018-07-20T19:07:43Z,I'm not sure this method needs to exist.  If someone want to parse a String and get a Map they can either call Utils.parseMap() directly or they can construct an instance of this class and call extensionsMap() on it.
204142913,5379,rondagostino,2018-07-20T19:09:31Z,I think this class should accept and return instances of SaslExtensions rather than a Map now that SaslExtensions is part of the public API.
204143400,5379,rondagostino,2018-07-20T19:11:45Z,Is there a reason why we shouldn't always use SaslExtensions rather than Map?
204143672,5379,rajinisivaram,2018-07-20T19:13:00Z,"@stanislavkozlovski For this KIP, I think we should do one of these:
1. Implement custom extensions for OAUTHBEARER alone, but add common callback classes to enable reuse for other mechanisms in future. This means leaving SCRAM alone.
2. Support custom extensions for both SCRAM and OAUTHBEARER, changing the delegation token mechanism to use the custom extensions code path treating `token` as a property in the map.

I don't think we should do a half-change for SCRAM, changing a public contract without actually providing a good reason to do so (i.e. change the way extensions are propagated without supporting custom extensions).
Does that make sense?"
204144544,5379,rondagostino,2018-07-20T19:16:39Z,I think this will end up being `extensions == myCommitedExtensions` ?
204144759,5379,rondagostino,2018-07-20T19:17:38Z,Same -- use SaslExtensions instead of Map?
204157244,5379,stanislavkozlovski,2018-07-20T20:11:28Z,Nope - https://docs.oracle.com/javase/8/docs/api/java/util/AbstractMap.html#equals-java.lang.Object-
204157659,5379,stanislavkozlovski,2018-07-20T20:13:12Z,Actually - no. My bad
204158880,5379,stanislavkozlovski,2018-07-20T20:18:23Z,Fair enough - I figured to have it there for the sake of completeness
204159227,5379,stanislavkozlovski,2018-07-20T20:19:54Z,"Is it bad to have it, even if unused in code (only tested)?"
204159567,5379,rondagostino,2018-07-20T20:21:20Z,"Yeah, the use of == instead of .equals() is certainly unusual, but it is necessary in this case because we need to make sure we remove our instance as opposed to an instance put there via another LoginContext.  I did not comment it when I did it for the token but should have given that it is unusual."
204160970,5379,stanislavkozlovski,2018-07-20T20:27:27Z,I find they're still useful to keep. It's always good to abstract away the implementation behind an interface in my opinion
204164892,5379,rondagostino,2018-07-20T20:45:33Z,"Good question.  The biggest mistake I made in doing this OAUTHBEARER implementation was trying to do too much.  The community reigned me in over time :-)  My guess would be to keep things as simple and minimalistic as possible; there is less that can go wrong, less to debate over/review/test/fix, and ultimately a shorter time to when the code actually gets merged.  At least that's the lesson I took from the experience."
204165198,5379,stanislavkozlovski,2018-07-20T20:47:07Z,Agreed and done
204165223,5379,stanislavkozlovski,2018-07-20T20:47:15Z,Done
204165380,5379,stanislavkozlovski,2018-07-20T20:48:02Z,That's true in general. I honestly believe this is a useful constructor to have though
204165803,5379,stanislavkozlovski,2018-07-20T20:50:01Z,Is there a need to explicitly make it unmodifiable since `extensionsMap()` returns an unmodifiable version? Come to think of it - maybe the other constructors shouldn't make it unmodifiable as well
204498442,5379,rondagostino,2018-07-23T17:55:21Z,"Should be ""Check whether your callback **handler** is explicitly..."""
204499789,5379,rondagostino,2018-07-23T17:59:35Z,"Need to also state: ""The {@code OAuthBearerLoginModule} instance also asks its configured {@link AuthenticateCallbackHandler} implementation to handle an instance of {@link SaslExtensionsCallback} and return an instance of {@link SaslExtensions}.  The configured callback handler does not need to handle this callback, though -- any {@code UnsupportedCallbackException} that is thrown is ignored, and no SASL extensions will be associated with the login."""
204500558,5379,rondagostino,2018-07-23T18:02:11Z,"This method doesn't actually attach the token -- it identifies the token that should be attached if/when commit() is called.  The method needs a better name.  Maybe ""identifyToken()""?"
204500716,5379,rondagostino,2018-07-23T18:02:31Z,Same here -- need a better method name (and also update Javadoc) to reflect the fact that it is identifying the extensions that should be attached if/when commit() is called.
204501651,5379,rondagostino,2018-07-23T18:05:39Z,Should be == so we know it is literally the instance we committed
204502047,5379,rondagostino,2018-07-23T18:07:11Z,This check is unnecessary; extensionsRequiringCommit cannot be null if tokenRequiringCommit is non-null.
204505373,5379,rondagostino,2018-07-23T18:17:50Z,"This isn't actually parsing the custom extensions; it is retrieving the ones that have already been parsed and stored somewhere.  It needs a better name.  Maybe ""retrieveExtensions()""?"
204506803,5379,rondagostino,2018-07-23T18:22:20Z,"We need to make sure the ""auth"" key isn't defined here since that is generated from the token's compact serialization.  Should also add to the Javadoc above that all token keys that meet the regex criteria are valid except ""auth""."
204507438,5379,rondagostino,2018-07-23T18:24:12Z,Attaches the first SaslExtensions (not Map anymore)
204507634,5379,rondagostino,2018-07-23T18:24:52Z,Can add static modifier since it doesn't refer to anything in the instance
204509972,5379,rondagostino,2018-07-23T18:32:13Z,Stylistically it is better to write it as `this.saslExtensions = validateExtensions(extensions)` (and of course make that method return the extensions passed in if an exception is not raised).
204510197,5379,rondagostino,2018-07-23T18:32:52Z,"Make it return the extensions as per above, and also throw an exception if the ""auth"" extension is specified."
204512923,5379,rondagostino,2018-07-23T18:41:53Z,`this.saslExtensions = validateExtensions(new SaslExtensions(properties))`
204513594,5379,rondagostino,2018-07-23T18:44:17Z,I actually think this method is no longer needed.
204513736,5379,rondagostino,2018-07-23T18:44:43Z,Can/should delete this method as per above.
204514223,5379,rondagostino,2018-07-23T18:46:18Z,"Is this constructor ever used?  If not, probably best to eliminate it; if it is used, then it should invoke `extensionsMap = Collections.emptyMap()`"
204515380,5379,rondagostino,2018-07-23T18:49:54Z,This method is not needed since map() is an inexpensive call; anybody wanting an extension value can simply call `theSaslExtensions.map().get(theName)`.
204515582,5379,rondagostino,2018-07-23T18:50:28Z,Same here -- unnecessary method due to the ability to invoke `theSaslExtensions.map().keySet()`
204515709,5379,rondagostino,2018-07-23T18:50:57Z,Same here -- unnecessary method due to the ability to invoke `theSaslExtensions.map().isEmpty()`
204517898,5379,rondagostino,2018-07-23T18:58:09Z,"Since the isGSSAPI variable is only used to determine if we should return, why not just this?
`if (SaslConfigs.GSSAPI_MECHANISM.equals(mechanism)) return; // extensions are not supported for GSSAPI`"
204518784,5379,rondagostino,2018-07-23T19:01:10Z,"""auth"" is not allowed."
204532330,5379,stanislavkozlovski,2018-07-23T19:50:05Z,"Could you elaborate on why this should be the case? 
I tend to agree but cannot explicitly define why that's better - maybe it's just more obvious?"
204534985,5379,stanislavkozlovski,2018-07-23T20:00:08Z,Should I? What do we win by that - it's private.
204535795,5379,stanislavkozlovski,2018-07-23T20:03:04Z,Sure
204537047,5379,stanislavkozlovski,2018-07-23T20:07:42Z,"No, it's not. I kept it since I did not call `super(map)` in `ScramExtensions` which should not have been the case"
204542803,5379,stanislavkozlovski,2018-07-23T20:27:04Z,"I agree with the others but for this most common use case I propose we keep the method name. It is shorter and more concise to write. Also reads better than `map().get(theName)`. Glancing at `extensionValue` you immediately understand what we're taking - in the other way, it's still obvious but not as much"
204545192,5379,stanislavkozlovski,2018-07-23T20:35:00Z,Done
204557034,5379,rajinisivaram,2018-07-23T21:18:22Z,`should be attached` => `may be added`
204557800,5379,rajinisivaram,2018-07-23T21:21:10Z,Why was this change made? I think the single `if` statement is better than returning here.
204558311,5379,rajinisivaram,2018-07-23T21:23:16Z,"Same as before - check the mechanism in the `if` statement below. Also, I think we could check for Scram mechanism in the check above and check for OAUTHBEARER here."
204562743,5379,rajinisivaram,2018-07-23T21:40:24Z,Make this `private final`?
204563236,5379,rajinisivaram,2018-07-23T21:42:27Z,Personally I think we s should create a copy of the map.
204563539,5379,rajinisivaram,2018-07-23T21:43:45Z,"Personally, I would get rid of this and use `extensionValue` and `extensionNames`. Otherwise, as @rondagostino said below, we should remove `extensionValue`."
204564358,5379,rajinisivaram,2018-07-23T21:46:58Z,"I thought we weren't supporting `SaslExtensionsCallback` for SCRAM. We should either update `ScramSaslClient` to process `SaslExtensions` and add tests for that  or not deprecate this now. In any case, I am not sure why the javadoc was removed."
204565761,5379,rajinisivaram,2018-07-23T21:52:51Z,Make `final`?
204567218,5379,stanislavkozlovski,2018-07-23T21:59:13Z,Doesn't `new HashMap<>(extensionsMap)` do exactly that?
204567896,5379,stanislavkozlovski,2018-07-23T22:02:02Z,"It would then require its initialized on the spot or in the constructor. The appropriate callback handler initializes it using `#extensions(...)`, so making it `final` wouldn't work"
204569668,5379,stanislavkozlovski,2018-07-23T22:10:29Z,"`OAuthBearerClientInitialResponse` uses this to iterate over all values and build the extensions string using `Utils.mkString`. If we removed `map()`, we would need to iterate through `extensionNames`, fetch and validate each value one by one.
We would also need to rebuild the map in `#extensionsMessage()` so that we could call `Utils.mkString`.
This is all more complex to write and slower to execute, thus I believe we should keep `map()`.

I do not understand what is inherently wrong with having an `extensionValue` method. It keeps it consistent with `ScramExtensions`' usage, does not bloat the API (it's a single method) and offers a more concise and readable way to fetch a value from the extensions. 

Can you elaborate why you believe we should remove `extensionValue()` ?"
204570041,5379,stanislavkozlovski,2018-07-23T22:12:32Z,It's `protected` so `ScramExtensions` can have access to it. I made it `final` now
204570220,5379,stanislavkozlovski,2018-07-23T22:13:25Z,"Come to think of it, I'll outright remove `extensionNames` and use `map().keySet()`. Did not realize `ScramExtensions` is not a public class"
204571187,5379,stanislavkozlovski,2018-07-23T22:18:34Z,"Removing the javadoc was a mistake.
I will look into making `ScramSaslClient` work with `SaslExtensions`. Last time I tried some tests kept failing for a reason I could not debug even after significant effort. If it happens to be the case again I'll simply remove the deprecated tag"
204575201,5379,stanislavkozlovski,2018-07-23T22:39:32Z,Personal preference. I find this more readable than a bigger if check. Changed back to one `if` and now checking for the correct mechanism in each callback
204575952,5379,stanislavkozlovski,2018-07-23T22:43:28Z,@rajinisivaram is this the correct way to check for the mechanism? I'm wondering why the previous code only checked for `!SaslConfigs.GSSAPI_MECHANISM.equals(mechanism)` and not other mechanisms as well
204583187,5379,rondagostino,2018-07-23T23:24:58Z,I agree with @rajinisivaram; since we have map()  there is no need to provide any shorthand methods for functionality that the return value of map() provides.
204583275,5379,rondagostino,2018-07-23T23:25:33Z,Probably a good idea to add toString() as well.
204584539,5379,rondagostino,2018-07-23T23:33:27Z,"The SASLClient callback handler for the OAUTHBEARER mechanism needs to handle OAuthBearerTokenCallback as well as SASLExtensionsCallback (with the last one optional, but the first one is definitely mandatory).  If we are going to put this code here for OAUTHBEARER then the only way the code is going to ever be invoked in a successful runtime scenario is if 1) we also add code to handle OAuthBearerTokenCallback; and 2) somehow this class is set as the SASLClient callback handler.  (2) will happen if the config explicitly specifies this class, or, alternatively, we can delete the OAuthBearerSaslClientCallbackHandler class and make this class the default SASLClient callback handler for the OAUTHBEARER mechanism (that decision is made at line 330 of org.apache.kafka.common.network.SaslChannelBuilder; that line would have to change).  I'm okay with either migrating to a fully-functional (for OAUTHBEARER) SaslClientCallbackHandler class or deleting these lines; keeping them without also handling OAuthBearerTokenCallback doesn't make sense, though."
204585957,5379,rondagostino,2018-07-23T23:41:47Z,"Sure.  We want to remove the instance that we put there, so we use == instead of .equals().  The .equals() method may identify another instance rather than the one we added.  Frankly I don't think it would be a problem due to the existence of the `break` statement below, but if that `break` statement were to be removed for some reason then we would iterate through the entire collection and remove everything that satisfied .equals() -- and that could be multiple instances.  So using `==` makes the semantics very clear and acts as an insurance policy at the same time."
204586766,5379,rondagostino,2018-07-23T23:46:27Z,This needs to be `getPublicCredentials()` rather than `getPublicCredentials(SaslExtensions.class)` because the former returns the actual public credentials (see [Subject#getPublicCredentials()](https://docs.oracle.com/javase/8/docs/api/javax/security/auth/Subject.html#getPublicCredentials--)) whereas the latter returns a new set that doesn't propagate changes through (see [Subject#getPublicCredentials(Class)](https://docs.oracle.com/javase/8/docs/api/javax/security/auth/Subject.html#getPublicCredentials-java.lang.Class-)).
204588760,5379,rondagostino,2018-07-23T23:59:28Z,need to invoke `extensions = null` here as well.
204589027,5379,rondagostino,2018-07-24T00:01:18Z,"Need to state that the extension name must match th required regex but cannot be the reserved value ""auth""."
204589338,5379,rondagostino,2018-07-24T00:03:42Z,Probably a good idea to wrap in a try {} catch (KafkaException e) {} block as is done above for the OAuthBearerTokenCallback.
204589592,5379,rondagostino,2018-07-24T00:05:17Z,"Should be `extensions.put(extensionName, extensionValue)`"
204595693,5379,stanislavkozlovski,2018-07-24T00:51:10Z,"I do not know what it should return, though"
204595909,5379,stanislavkozlovski,2018-07-24T00:52:52Z,"Oops, yes. This should not be here at all"
204596760,5379,stanislavkozlovski,2018-07-24T01:00:26Z,That is a big gotcha! Thanks!
204597201,5379,stanislavkozlovski,2018-07-24T01:04:00Z,"And just swallow the exception? I guess it boils down to:
Do we want to stop authentication on invalid extension value or just not use extensions?"
204599621,5379,rondagostino,2018-07-24T01:23:20Z,No need to check instanceof and cast it since `==` will return false if it isn't an instanceof.  See line 331-338 above for what this should look like.
204599894,5379,rondagostino,2018-07-24T01:25:53Z,Is there a reason why this is protected and not private?
204599996,5379,rondagostino,2018-07-24T01:26:29Z,`extensionsMap.toString()` seems appropriate
204600596,5379,rondagostino,2018-07-24T01:31:44Z,Lines 344-347 are unnecessary; the statement `if (myCommittedExtensions == credential)` will be correct regardless.  See lines 332-333 above.
204601020,5379,rondagostino,2018-07-24T01:35:15Z,Probably should use `{@link OAuthBearerClientInitialResponse.AUTH_KEY}` instead of `{@code OAuthBearerClientInitialResponse.AUTH_KEY}`
204601155,5379,rondagostino,2018-07-24T01:36:22Z,"No, don't swallow, propagate it wrapped in an IOException as is done a few lines up."
204601217,5379,rondagostino,2018-07-24T01:36:48Z,`{@link` instead of `{@code`
204601696,5379,rondagostino,2018-07-24T01:40:46Z,duplicate line
204602098,5379,rondagostino,2018-07-24T01:44:07Z,Why delete these comments?
204602841,5379,rondagostino,2018-07-24T01:50:04Z,"I think what you want to do here is accept an array of SaslExtensions objects; if an array element is null then the handler would throw UnsupportedCallbackException on that iteration, otherwise it returns the element.  This test is making sure the simultaneous login/logout functionality doesn't get confused.  basically follow what is going on with the tokens and do the same thing with the SaslExtensions.  You might need separate indexes for tokens and SaslExtensions (i.e. `tokenIndex` instead of `index`, and then add `extensionsIndex`)"
204603396,5379,rondagostino,2018-07-24T01:54:34Z,"Same thing here; create an array of SaslExtensions mocks, one element of which should be null, etc."
204603442,5379,rondagostino,2018-07-24T01:54:57Z,"Same thing here; create an array of SaslExtensions mocks, one element of which should be null, etc."
204603529,5379,rondagostino,2018-07-24T01:55:36Z,"Same thing here; create an array of SaslExtensions mocks, one element of which should be null, etc."
204603724,5379,rondagostino,2018-07-24T01:57:11Z,This test becomes unnecessary after weaving SaslExtensions into the above tests.
204604012,5379,rondagostino,2018-07-24T01:59:30Z,"Create an array of SaslExtensions mocks, one element of which should be null, etc."
204604217,5379,rondagostino,2018-07-24T02:01:00Z,This test becomes unnecessary after weaving SaslExtensions into the above tests as long as you include at least one null element in the array for each one.
204604435,5379,rondagostino,2018-07-24T02:02:51Z,Why delete these 2 lines?  Can you just call `response.extensions().map().get()` instead of `response.propertyValue()`?
204604726,5379,rondagostino,2018-07-24T02:05:33Z,Why delete this test?  Can you just call `response.extensions().map().get()` instead of `response.propertyValue()`?
204605136,5379,rondagostino,2018-07-24T02:09:00Z,"Indicate that ""auth"" is reserved and cannot be used."
204672879,5379,rajinisivaram,2018-07-24T08:49:05Z,"sorry, my mistake."
204673218,5379,rajinisivaram,2018-07-24T08:50:14Z,"sorry, I was looking at the `unmodifiableMap` and didn't see the copy."
204673925,5379,rajinisivaram,2018-07-24T08:52:31Z,We should make this `private`.
204674806,5379,rajinisivaram,2018-07-24T08:55:08Z,`ScramMechanism.isScram(mechanism)`
204676556,5379,rajinisivaram,2018-07-24T09:00:47Z,"In this line and the similar one for retrieving tokens, could we says `An internal error occurred while doing xxx`? Also, perhaps `log.error(""Error occurred while doing xxx"", e)`."
204680030,5379,rajinisivaram,2018-07-24T09:11:57Z,"Looks like duplicate code. Couldn't we just use one static `OAuthBearerClientInitialResponse.validateExtensions(Map<String, String>)` method for validation?"
204789092,5379,stanislavkozlovski,2018-07-24T14:54:13Z,I wrote a similar test (copied this one) to this that didn't make the PR. I must have deleted the comments from the wrong test in the end
204795761,5379,stanislavkozlovski,2018-07-24T15:10:39Z,"No, we'd need this. The null element won't test out this backwards-compatible behavior"
204818374,5379,stanislavkozlovski,2018-07-24T16:09:58Z,I'll add the functionality and have one of the commit/login tests use it. I vote we keep the two tests I wrote - I don't see anything wrong with unit testing functionality in a more fine-grained way
204819005,5379,stanislavkozlovski,2018-07-24T16:11:55Z,"Yes. My bad, sorry"
204821362,5379,stanislavkozlovski,2018-07-24T16:18:23Z,Yes we can. This will result in a bit more complicated code in `OAuthBearerUnsecuredLoginCallbackHandler#handleExtensionsCallback()` since it needs to unprefix the extensions first
204907936,5379,rondagostino,2018-07-24T20:54:55Z,Can mirror the way it is done for tokens just call `getPublicCredentials()` instead of `getPublicCredentials(SaslExtensions.class)`.  This also eliminates the need to keep calling the method to calculate a new set -- the original set will always be accurate.
205113394,5379,rondagostino,2018-07-25T13:44:17Z,Can remove this line after making the change mentioned in line 127 above
205114012,5379,rondagostino,2018-07-25T13:45:52Z,Can remove this line after making the change mentioned in line 127 above
205114191,5379,rondagostino,2018-07-25T13:46:17Z,Can remove this line after making the change mentioned in line 127 above
205114596,5379,rondagostino,2018-07-25T13:47:15Z,Can remove this line after making the change mentioned in line 127 above
205114906,5379,rondagostino,2018-07-25T13:48:03Z,Can remove this line after making the change mentioned in line 127 above
205115117,5379,rondagostino,2018-07-25T13:48:34Z,Can remove this line after making the change mentioned in line 127 above
205115297,5379,rondagostino,2018-07-25T13:49:00Z,Can remove this line after making the change mentioned in line 127 above
205115775,5379,rondagostino,2018-07-25T13:50:16Z,Same as above -- can mirror the way it is done for tokens just call getPublicCredentials() instead of getPublicCredentials(SaslExtensions.class). This also eliminates the need to keep calling the method to calculate a new set -- the original set will always be accurate.
205116317,5379,rondagostino,2018-07-25T13:51:35Z,"SaslExtensions array length should be the same as token array length -- 2, not 3."
205116655,5379,rondagostino,2018-07-25T13:52:19Z,Can remove this line after making the change mentioned in line 230 above
205116789,5379,rondagostino,2018-07-25T13:52:38Z,Can remove this line after making the change mentioned in line 230 above
205116910,5379,rondagostino,2018-07-25T13:52:57Z,Can remove this line after making the change mentioned in line 230 above
205117003,5379,rondagostino,2018-07-25T13:53:12Z,Can remove this line after making the change mentioned in line 230 above
205117091,5379,rondagostino,2018-07-25T13:53:28Z,Can remove this line after making the change mentioned in line 230 above
205117236,5379,rondagostino,2018-07-25T13:53:49Z,Can remove this line after making the change mentioned in line 230 above
205117560,5379,rondagostino,2018-07-25T13:54:33Z,Same as above -- can mirror the way it is done for tokens just call getPublicCredentials() instead of getPublicCredentials(SaslExtensions.class). This also eliminates the need to keep calling the method to calculate a new set -- the original set will always be accurate.
205118094,5379,rondagostino,2018-07-25T13:56:01Z,Can remove this line after making the change mentioned in line 296 above
205118194,5379,rondagostino,2018-07-25T13:56:16Z,Can remove this line after making the change mentioned in line 296 above
205118319,5379,rondagostino,2018-07-25T13:56:38Z,Can remove this line after making the change mentioned in line 296 above
205118397,5379,rondagostino,2018-07-25T13:56:50Z,Can remove this line after making the change mentioned in line 296 above
205118520,5379,rondagostino,2018-07-25T13:57:08Z,Can remove this line after making the change mentioned in line 296 above
205118748,5379,rondagostino,2018-07-25T13:57:43Z,Same as above -- can mirror the way it is done for tokens just call getPublicCredentials() instead of getPublicCredentials(SaslExtensions.class). This also eliminates the need to keep calling the method to calculate a new set -- the original set will always be accurate.
205119069,5379,rondagostino,2018-07-25T13:58:29Z,Can remove this line after making the change mentioned in line 353 above
205119183,5379,rondagostino,2018-07-25T13:58:45Z,Can remove this line after making the change mentioned in line 353 above
205119367,5379,rondagostino,2018-07-25T13:59:11Z,Can remove this line after making the change mentioned in line 353 above
205119483,5379,rondagostino,2018-07-25T13:59:29Z,Can remove this line after making the change mentioned in line 353 above
205119824,5379,rondagostino,2018-07-25T14:00:16Z,Can remove this line after making the change mentioned in line 353 above
205119994,5379,rondagostino,2018-07-25T14:00:41Z,Can remove this line after making the change mentioned in line 353 above
205120125,5379,rondagostino,2018-07-25T14:00:59Z,Can remove this line after making the change mentioned in line 353 above
205120889,5379,rondagostino,2018-07-25T14:02:54Z,I believe this test adds no value and should be eliminated because the case is covered above.
205122848,5379,rondagostino,2018-07-25T14:08:10Z,"This test is checking the same thing that was checked via passing in RAISE_UNSUPPORTED_CB_EXCEPTION_FLAG (null) at line 133 and checking for EMPTY_EXTENSIONS at lines 191 and 200.  This test should be deleted.
"
205125175,5379,rondagostino,2018-07-25T14:14:12Z,`Collections.emptyMap()` instead of `new HashMap<>()`
205125573,5379,rondagostino,2018-07-25T14:15:07Z,"Oops, delete this unintended change"
205178909,5379,stanislavkozlovski,2018-07-25T16:33:59Z,"Hm, that is strange. I initially went with this approach (obviously sprinkling `getPublicCredentials()` before every assert is bad) but hit some problem.
I assumed that the public credentials had another value in them and changed the test with what you just reviewed. The test passed afterwards so I didn't go into investigating what the problem was. 
Now that I removed the calls, test still pass. I'm not sure what I initially missed there"
205185439,5379,stanislavkozlovski,2018-07-25T16:55:30Z,"Sorry about my initial comment of ""No, we'd need this. The null element won't test out this backwards-compatible behavior"". I unfortunately commented prematurely before completely understanding your suggestion.

I acknowledge that this is verified in the above tests.
It's just that from what I've read from TDD books, the overall approach experts recommend is to rely on single, small tests that test concrete functionality. This way, when a problem occurs you immediately know what the cause is - `commitPopulatesExtensions` - oh, my extensions weren't populated.

Where as if you get an error in test `login1Commit1Login2Abort2Login3Commit3Logout3` you need to investigate the test well and figure out where the problem is. 
While such big tests are always useful, I believe a test suite comprised of more smaller tests is better.
Tests comprised of more methods serve as better documentation. You can then just read the method names and get a general feeling of what the tested subject should do.

Please share your thoughts on this"
205185773,5379,stanislavkozlovski,2018-07-25T16:56:38Z,"Take a look at my comment below for test `commitDoesNotThrowOnUnsupportedExtensionsCallback`. I do not feel as strongly about this test as I feel on the one below, but I also tentatively think it doesn't hurt to have one more test"
205197478,5379,rondagostino,2018-07-25T17:32:48Z,"Still need this adjusted: SaslExtensions array length should be the same as token array length -- 2, not 3."
205198355,5379,rondagostino,2018-07-25T17:35:34Z,"I agree with your point below about lots of simple tests being better than one big one.  Let's eliminate the null value here, replace it with a mock, and rely on your test below to verify that UnhandledCallbackException is ignored."
205198824,5379,rondagostino,2018-07-25T17:37:01Z,"Yes, let's eliminate this one and keep the one below."
205199240,5379,rondagostino,2018-07-25T17:38:17Z,Good point -- I agree.  Let's keep this test and eliminate the null value in `login1Commit1Login2Abort2Login3Commit3Logout3`.
207815285,5379,rajinisivaram,2018-08-06T08:47:03Z,"Can you move the link to the next line and include the full name including package since that class has not been imported in this class:
`{@link org.apache.kafka.common.security.oauthbearer.internals.OAuthBearerClientInitialResponse#AUTH_KEY}`"
207818595,5379,rajinisivaram,2018-08-06T08:58:44Z,"Was like this earlier, but will be good to update anyway. `String.format` not required. Could use:
`log.info(""Login failed {} : {} (URI={}"",....`"
207819149,5379,rajinisivaram,2018-08-06T09:00:41Z,"`log.info(""CallbackHandler {} does not support..."", callbackHandler.getClass().getName())`?"
207819502,5379,rajinisivaram,2018-08-06T09:02:03Z,We could have a constant like `EMPTY_EXTENSIONS` for this case?
207820729,5379,rajinisivaram,2018-08-06T09:06:27Z,"Now that we can use Java8, we could use `removeIf` here and in the block above for tokens?
```
if (subject.getPublicCredentials().removeIf(e -> myCommittedExtensions == e))
  myCommittedExtensions = null;
```"
207822201,5379,rajinisivaram,2018-08-06T09:12:06Z,`{@link OAuthBearerClientInitialResponse.AUTH_KEY}` => `{@link OAuthBearerClientInitialResponse#AUTH_KEY}`
207822476,5379,rajinisivaram,2018-08-06T09:13:06Z,`get` => `containsKey`?
207824202,5379,rajinisivaram,2018-08-06T09:19:09Z,`OAuthBearerClientInitialResponse.AUTH_KEY` => OAuthBearerClientInitialResponse#AUTH_KEY
207824332,5379,rajinisivaram,2018-08-06T09:19:34Z,`OAuthBearerClientInitialResponse.AUTH_KEY` => `OAuthBearerClientInitialResponse#AUTH_KEY`
207827945,5379,rajinisivaram,2018-08-06T09:31:55Z,Could just use `Collections.emptyMap()` here since type can be inferred?
207830135,5379,rajinisivaram,2018-08-06T09:39:45Z,`static final`?
207841511,5379,rajinisivaram,2018-08-06T10:20:11Z,`values` => `value`?
207842331,5379,rajinisivaram,2018-08-06T10:23:43Z,Not used?
207842366,5379,rajinisivaram,2018-08-06T10:23:51Z,Not used?
207859716,5379,stanislavkozlovski,2018-08-06T11:32:50Z,That is very cool
38921717,191,ijuma,2015-09-08T12:59:25Z,"We should probably use 2.7.1, right?
"
38921790,191,ijuma,2015-09-08T13:00:12Z,"Indenting doesn't look right in the new `allow` lines.
"
38923719,191,ijuma,2015-09-08T13:20:24Z,"Is the current plan not to support `auth-int` and `auth-conf` qops?
"
38924187,191,ijuma,2015-09-08T13:25:00Z,"What is this change for? `MaxFDLimit` seems to be a deprecated flag (https://bugs.openjdk.java.net/browse/JDK-8010126?focusedCommentId=13315747&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13315747)
"
38924482,191,ijuma,2015-09-08T13:27:45Z,"Instead of having one method per security protocol, why not just take the security protocol as a parameter? Then 3 methods would become one. The implementation seems exactly the same apart from the security protocol passed to `boundPort`.
"
38924900,191,ijuma,2015-09-08T13:31:31Z,"I agree that this is useful. I think there are a number of places where we use `milliseconds()` where we should really be using a method like this one (I noticed that recently). It may be worth thinking about the naming so that the differences are clear. Maybe `milliseconds()` should be removed in favour of `currentWallTime()` introduced below?
"
38925126,191,ijuma,2015-09-08T13:33:25Z,"Not sure whether we should be returning `java.util.Date` as it's a somewhat deprecated class as of Java 8. Maybe we can return a long and the caller can decide to create a `java.util.Date` or anything else?
"
39592806,191,junrao,2015-09-16T04:25:38Z,"Could we document the handshake protocol in the comment? It seems that for each token, we first send a 4-byte size, followed by the bytes in the token itself?
"
39592812,191,junrao,2015-09-16T04:25:48Z,"It seems that we can just use one level of if/else.
"
39592816,191,junrao,2015-09-16T04:25:56Z,"Could we just use a ByteBuffer instead of a NetworkSend?
"
39592823,191,junrao,2015-09-16T04:26:07Z,"Could we get that through the config property instead of a jvm system property?
"
40359478,191,Parth-Brahmbhatt,2015-09-24T19:08:40Z,"I think this could just be if(!SecurityProtocol.values().contains(securityProtocol)) 
"
40359495,191,Parth-Brahmbhatt,2015-09-24T19:08:47Z,"Shouldn't this come from a config? 
"
40359539,191,Parth-Brahmbhatt,2015-09-24T19:09:12Z,"is this needed?
"
40359628,191,Parth-Brahmbhatt,2015-09-24T19:09:57Z,"anyways to avoid this vendor spicific thing? Can we just make this a config that defaults to sun.security.krb5.Config? 
"
40362188,191,Parth-Brahmbhatt,2015-09-24T19:33:44Z,"Let's also add ""socketChannel.socket().getInetAddress().getHostName() must match the hostname in principal/hostname@realm""
"
40375211,191,Parth-Brahmbhatt,2015-09-24T21:24:39Z,"probably better to just create a method that returns the principal name and host. might be easier to extract all of it using a simple pattern matcher instead of going through bunch of indexofs and substrings.
"
40375463,191,Parth-Brahmbhatt,2015-09-24T21:27:06Z,"I am guessing this is all part of GSS API magic but a link to doc or some explanation on what we are doing here might help with future maintenance. 
"
40575583,191,ijuma,2015-09-28T16:52:14Z,"That would not be right because of `SecurityProtocol.TRACE` (the fact that TRACE exists is the reason why we do the check in the first place).
"
40575758,191,ijuma,2015-09-28T16:53:55Z,"`sun.security.krb5.Config` is also vendor-specific and it won't work in Java 9 (see http://openjdk.java.net/jeps/260). Is there no way to avoid this?
"
40669700,191,ijuma,2015-09-29T13:22:29Z,"I think it would be more readable if `listeners` were a `Seq` and we can build the `String` at the end with `mkString`
"
41590473,191,harshach,2015-10-09T01:48:04Z,"@ijuma enabling qop on sasl proven to cause lot of perf issues . It was discussed before hence the reason I went with proposal ssl+sasl.
"
41590525,191,harshach,2015-10-09T01:49:06Z,"@junrao The reason to use networksend is to have length encoded token . I can use ByteBuffer have it encoded with length. Let me know if you prefer that.
"
41627145,191,ijuma,2015-10-09T13:03:41Z,"I removed it in my PR.
"
41627200,191,ijuma,2015-10-09T13:04:24Z,"OK, thanks. Should the name of this be `SASL_PLAINTEXT` and `SSLSASL` should be `SASL_SSL`? I guess this is a bit subjective, but seems a bit clearer to me.
"
41628653,191,ijuma,2015-10-09T13:22:05Z,"I removed this in my PR as it's not used anywhere.
"
41628772,191,ijuma,2015-10-09T13:23:38Z,"I filed KAFKA-2607 to modify the `Time` interface. In the meantime, maybe we can `nanoseconds` and `milliseconds` instead of introducing these methods? I can make the change if you agree.
"
41628927,191,ijuma,2015-10-09T13:25:12Z,"@junrao do we have anything in Kafka that does something similar to this?
"
41629059,191,ijuma,2015-10-09T13:26:18Z,"I changed this in my PR.
"
41629081,191,ijuma,2015-10-09T13:26:33Z,"I removed this change in my PR.
"
41719366,191,junrao,2015-10-12T02:00:42Z,"2 components -> 3 components ?
"
41719367,191,junrao,2015-10-12T02:00:43Z,"with out => without
"
41719374,191,junrao,2015-10-12T02:00:50Z,"Would it be better to name this to sth like TICKET_RENEW_WINDOW_FACTOR?
"
41719381,191,junrao,2015-10-12T02:01:01Z,"Should that be made configurable?
"
41719400,191,junrao,2015-10-12T02:01:11Z,"Could we use Utils.newThread() so that we can give it a proper name and register the uncaught exception handler?
"
41719401,191,junrao,2015-10-12T02:01:15Z,"Should we test equals and after?
"
41719402,191,junrao,2015-10-12T02:01:20Z,"not be => not be able to
"
41719404,191,junrao,2015-10-12T02:01:24Z,"newuntil => newUntil
"
41719407,191,junrao,2015-10-12T02:01:32Z,"Since we just want to exit, should we change break to return?
"
41719410,191,junrao,2015-10-12T02:01:39Z,"Would it be better to get kafka.init from kafka_jaas.conf file instead of another system property?
"
41719414,191,junrao,2015-10-12T02:01:47Z,"Since we are waiting for this thread to finish during shutdown, it seem that it shouldn't be a daemon thread?
"
41719415,191,junrao,2015-10-12T02:02:07Z,"It seems that t is never null. So perhaps it's simpler to just start the thread after t is created.
"
41719416,191,junrao,2015-10-12T02:02:19Z,"Is this test needed? It seems that loginContextName can never be null.
"
41719421,191,junrao,2015-10-12T02:02:26Z,"Could we make ""java.security.auth.login.config"" a constant and reuse?
"
41719433,191,junrao,2015-10-12T02:03:00Z,"I am wondering how well this works when the broker is enabled to also authenticate to ZK through sasl. Will the global Configuration be set twice (once here and another time potentially in zookeeper client)? Will that affect the login logic? @fpj , do you know?
"
41719435,191,junrao,2015-10-12T02:03:07Z,"It seems that we need to set the login time during the initial login as well. 
"
41719438,191,junrao,2015-10-12T02:03:13Z,"It seems that we should setLastLogin() in setLogin() instead of here.
"
41719441,191,junrao,2015-10-12T02:03:20Z,"It seems that both loginContext and mode can just be a local variable.
"
41719446,191,junrao,2015-10-12T02:03:39Z,"Do we need to make serviceName configurable? Could that just be hardcoded as Kafka?
"
41719449,191,junrao,2015-10-12T02:03:44Z,"It doesn't seem that the client needs principalBuilder.
"
41719453,191,junrao,2015-10-12T02:03:51Z,"It seems that we need to pass in the config properties that may be specified in the jaas config file?
"
41719460,191,junrao,2015-10-12T02:04:04Z,"We need to turn off OP_WRITE when sasl state is complete, right?
"
41719461,191,junrao,2015-10-12T02:04:09Z,"Would it be enough to just check saslState?
"
41719464,191,junrao,2015-10-12T02:04:27Z,"Could you add some comments on when and what types of callbacks could be called?
"
41719465,191,junrao,2015-10-12T02:04:31Z,"This is a no op.
"
41722182,191,junrao,2015-10-12T04:31:32Z,"Could you add a comment on why we need to exclude this?
"
41722186,191,junrao,2015-10-12T04:31:45Z,"Should we specify those through Kafka config file or just the jaas config file? It seems that the latter is more natural since it consolidates all SASL related stuff in one file?
"
41722188,191,junrao,2015-10-12T04:31:56Z,"Need to add the new param configs.
"
41722190,191,junrao,2015-10-12T04:32:03Z,"Is the test transportLayer.ready() necessary?
"
41722191,191,junrao,2015-10-12T04:32:08Z,"Why does this need to be public?
"
41722196,191,junrao,2015-10-12T04:32:19Z,"Do we need to pass in the config properties that may be specified in the jaas config file?
"
41722202,191,junrao,2015-10-12T04:32:25Z,"Do we need to set OP_READ? It seems it's always on.
"
41722205,191,junrao,2015-10-12T04:32:30Z,"We need to turn off OP_WRITE when saslServer is complete.
"
41731553,191,ijuma,2015-10-12T07:54:25Z,"I agree that it's not needed in its current state, but it makes sense with the code as it was before, that is:

```
        if (!transportLayer.ready())
            transportLayer.handshake();
        if (transportLayer.ready() && !authenticator.complete())
            authenticator.authenticate();
```

Which version do we prefer?
"
41735085,191,ijuma,2015-10-12T08:45:58Z,"It doesn't, I'll change this back.
"
41752308,191,ijuma,2015-10-12T12:47:41Z,"Fixed locally.
"
41752351,191,ijuma,2015-10-12T12:48:16Z,"That's right, changed it locally.
"
41752705,191,ijuma,2015-10-12T12:53:31Z,"Done this locally.
"
41752835,191,ijuma,2015-10-12T12:55:01Z,"I don't understand why we are doing this. We call this method if `authid.equals(authzid)` and set it to the value of `ac.getAuthorizationID`, so it looks like a no-op?
"
41752875,191,ijuma,2015-10-12T12:55:31Z,"Removed locally.
"
41753910,191,ijuma,2015-10-12T13:08:19Z,"Agreed. And we should handle parsing errors properly (at the moment we are ignoring the case where `indexOf` returns -1). I haven't done this yet, but I added it to my list.
"
41754232,191,ijuma,2015-10-12T13:12:02Z,"Fixed locally.
"
41754241,191,ijuma,2015-10-12T13:12:09Z,"Fixed locally.
"
41754749,191,ijuma,2015-10-12T13:17:33Z,"Maybe we can use `KerberosName` for this?
"
41755037,191,ijuma,2015-10-12T13:21:11Z,"Done locally.
"
41755928,191,ijuma,2015-10-12T13:31:32Z,"Changed it locally (and in one other similar place).
"
41756215,191,ijuma,2015-10-12T13:34:58Z,"There is the following in the constructor, so the thread can be null.

```
if (!isKrbTicket) {
    // if no TGT, do not bother with ticket management.
    return;
}
```
"
41756262,191,ijuma,2015-10-12T13:35:32Z,"Changed it locally.
"
41756269,191,ijuma,2015-10-12T13:35:39Z,"Changed it locally.
"
41756390,191,ijuma,2015-10-12T13:37:05Z,"I think so, changed it locally.
"
41756657,191,ijuma,2015-10-12T13:39:56Z,"Changed it locally.
"
41756681,191,ijuma,2015-10-12T13:40:09Z,"Changed it locally.
"
41759052,191,ijuma,2015-10-12T14:05:12Z,"Done locally.
"
41815169,191,harshach,2015-10-13T00:13:09Z,"I've a config property in sasl will replace that with this.
"
41815315,191,harshach,2015-10-13T00:15:47Z,"Yes. will change that.
"
41815592,191,harshach,2015-10-13T00:20:18Z,"@junrao no its not needed. User needs to add another section kafka_jaas.conf with ""Client"" section. Here is the vagrant setup that I've for kafka kerberos.  Example here https://github.com/harshach/kafka-vagrant/blob/master/configs/kafka_jaas3.conf
"
41817433,191,harshach,2015-10-13T00:57:11Z,"@junrao  this is not hardcoded. Users need to come up with serviceName and its equivalent to the principal name of the KafkaServer.
"
41817530,191,harshach,2015-10-13T00:58:37Z,"didn't understand , are you saying we should pass jaas config file as part of client config properties?
"
41817534,191,harshach,2015-10-13T00:58:46Z,"Yes. will fix it.
"
41817553,191,harshach,2015-10-13T00:59:04Z,"Yes that should be enough.
"
41817601,191,harshach,2015-10-13T01:00:11Z,"Jaas config special file in that it needs a different syntax like sections that we define. So it should only need to have login details like keytab files not kafka specific configs.
"
41817692,191,harshach,2015-10-13T01:02:26Z,"Don't understand. What you mean by config properties in jaas config file. Jaas should only contain sections and it has specific syntax to them we shouldn't be treating it as generic config file.
"
41817707,191,harshach,2015-10-13T01:02:40Z,"will take it out.
"
41820027,191,junrao,2015-10-13T01:54:20Z,"Do we need the StringBuilder?
"
41820042,191,junrao,2015-10-13T01:54:40Z,"Could you add some examples of the rules and Keberos names? In particular, how match, fromPattern, toPattern, etc are used to convert Keberos names to user names?
"
41820061,191,junrao,2015-10-13T01:54:51Z,"It's a bit weird that # of params doesn't match numOfComponents. Could you add a comment?
"
41820068,191,junrao,2015-10-13T01:55:03Z,"Should we get this from a system property or from the jaas conf file?
"
41820075,191,junrao,2015-10-13T01:55:14Z,"Would it be better to pass in timeout through the constructor?
"
41820084,191,junrao,2015-10-13T01:55:22Z,"What does it mean to have a negative interval? Also, do we need to support interval? It seems that we have no use case to run a command periodically.
"
41822597,191,harshach,2015-10-13T02:55:10Z,"@junrao we initializing lastTime to negative of interval and in run method we are checking lastTime + interval > Time.currentElapsedTime()) so it guarantees at least one execution of runCommand.
"
41823048,191,harshach,2015-10-13T03:07:50Z,"we've ShellCommandExecutor in the same file that takes in timeout from constructor. Let me know if you want to change this for Shell as well.
"
41833260,191,ijuma,2015-10-13T07:13:33Z,"I fixed this in my PR that Harsha merged some minutes before you made this comment.
"
41875295,191,ijuma,2015-10-13T14:46:55Z,"I agree that it's clearer to receive the parameter via the constructor instead of assigning it directly in the subclass (it also avoids initialisation ordering issues). I've changed this locally with a few other `Shell` changes.
"
41879955,191,ijuma,2015-10-13T15:16:57Z,"As far as I can see, we don't need to support interval. I have removed it locally as it simplifies the class.
"
41881028,191,ijuma,2015-10-13T15:23:37Z,"Harsha changed this to be:

`transportLayer.removeInterestOps(SelectionKey.OP_WRITE);`

I think this also addresses the ""turn off OP_WRITE"" comment.
"
41881164,191,ijuma,2015-10-13T15:24:25Z,"Harsha address this, I believe.
"
41881443,191,ijuma,2015-10-13T15:26:12Z,"Changed it locally.
"
41883325,191,ijuma,2015-10-13T15:37:52Z,"That's right, changed it locally. With the current code, this doesn't make much of a difference in practice, but it could lead to bugs in the future.
"
41884709,191,ijuma,2015-10-13T15:47:19Z,"There was this code in the constructor before the `login` call:

`this.lastLogin = time.currentElapsedTime() - this.minTimeBeforeRelogin;`

I've changed it to:

`this.lastLogin = time.currentElapsedTime()`

`minTimeBeforeRelogin` is only relevant for the relogin case. However, we are still setting the `lastLogin` time before we actually execute `loginContext.login` (in both the first and subsequent logins). Do you think we should be updating that value after the `loginContext.login` call?
"
41884856,191,ijuma,2015-10-13T15:48:21Z,"Harsha has done this.
"
41884906,191,ijuma,2015-10-13T15:48:42Z,"It looks like we don't.
"
41885077,191,ijuma,2015-10-13T15:49:55Z,"We now create the appropriate one (Plaintext or SSL) based on whether it's SSLSASL or PLAINTEXTSASL.
"
41886074,191,ijuma,2015-10-13T15:56:39Z,"I removed this as it wasn't being used.
"
41886456,191,ijuma,2015-10-13T15:59:25Z,"I don't understand what you mean, could you elaborate please?
"
41886529,191,ijuma,2015-10-13T15:59:57Z,"This was fixed by Harsha.
"
41886544,191,ijuma,2015-10-13T16:00:05Z,"I fixed this.
"
41886627,191,ijuma,2015-10-13T16:00:34Z,"I removed this.
"
41886671,191,ijuma,2015-10-13T16:00:51Z,"Harsha did this.
"
41887608,191,ijuma,2015-10-13T16:07:42Z,"Checked with Jun and this is fine.
"
41889019,191,ijuma,2015-10-13T16:18:02Z,"These new methods are only used in the `Login` class, so I will move them there and make them private for now.
"
41889636,191,ijuma,2015-10-13T16:22:52Z,"Will propose SASL_PLAIN and SASL_SSL in a PR (checked with Jun).
"
41890445,191,ijuma,2015-10-13T16:30:14Z,"Harsha did this.
"
41890652,191,ijuma,2015-10-13T16:31:39Z,"Harsha removed the check.
"
41890663,191,ijuma,2015-10-13T16:31:50Z,"Harsha did this.
"
41898819,191,ijuma,2015-10-13T17:38:35Z,"I moved `transportLayer.removeInterestOps(SelectionKey.OP_WRITE);` from `case COMPLETE` to here in my latest PR.
"
41907669,191,ijuma,2015-10-13T18:46:29Z,"I will add a comment explaining the `Oid` line, which is particularly bizarre.
"
41922109,191,fpj,2015-10-13T20:43:32Z,"This class is surprisingly similar to org.apache.zookeeper.Login, have we copied from the same source? ;-)
"
41928675,191,rajinisivaram,2015-10-13T21:32:51Z,"@ijuma @junrao SASL/PLAIN is typically used to refer to SASL with mechanism PLAIN. And SASL/PLAIN is usually used with SSL as transport layer. Since the protocols here are referring to the transport layer and the plain transport layer is called PLAINTEXT, it would be less confusing to have SASL_PLAINTEXT and SASL_SSL.
"
41929181,191,rajinisivaram,2015-10-13T21:37:21Z,"Can the mechanism be made a configuration option? I haven't looked through the code yet to see if the implementation relies on this mechanism, but it will be good if it was configurable.
"
41929415,191,rajinisivaram,2015-10-13T21:39:25Z,"Same question as for client - can the SASL mechanism be made configurable?
"
41929710,191,rajinisivaram,2015-10-13T21:42:15Z,"Is there a reason why this isn't simply using `Configuration.getConfiguration()` to get the default configuration since it is using the standard Java property to get the Jaas config file anyway? I think `JavaLoginConfig` is provided by the Sun provider, dont think it is available with all vendors.
"
41930386,191,ijuma,2015-10-13T21:47:53Z,"OK, I can change my PR to use that instead. To check: is the proposed name better than what we have at the moment (SSLSASL and PLAINTEXTSASL).
"
41930669,191,ijuma,2015-10-13T21:49:59Z,"I think it would be good if we could do that in a separate PR. Which other mechanisms are important for you?
"
41931374,191,rajinisivaram,2015-10-13T21:55:36Z,"@ijuma  I do prefer SASL_PLAINTEXT and SASL_SSL since it is clearer (more readable) than SSLSASL and PLAINTEXTSASL.
"
41931566,191,ijuma,2015-10-13T21:57:05Z,"Ok, great.
"
41937626,191,rajinisivaram,2015-10-13T23:00:31Z,"The one we are keen on is PLAIN. We will be using SASL with SSL, so PLAIN gives us the simplest secure authentication without having to distribute certificates for mutual client auth. Yes, a separate PR makes sense so that this one can be committed soon. I will raise another JIRA.
"
41948372,191,junrao,2015-10-14T02:03:31Z,"Since this tests both the producer and consumer, probably this can be called SaslIntegrationTest. Also, could we parameterize the test to test SASL_SSL port too?
"
41948393,191,junrao,2015-10-14T02:03:58Z,"Perhaps it's clearer to also specify the param name for the third value (false).
"
41948396,191,junrao,2015-10-14T02:04:04Z,"Since we only use 1 consumer, do we need to create multiple consumers during setup?
"
41948402,191,junrao,2015-10-14T02:04:10Z,"Probably better to use foreach instead map.
"
41948404,191,junrao,2015-10-14T02:04:14Z,"Could we rename this to consumeAndVerifyRecords?
"
41948407,191,junrao,2015-10-14T02:04:21Z,"Should we verify the content of the consumed messages too?
"
41948415,191,junrao,2015-10-14T02:04:29Z,"It seems that SaslConsumerTest.scala covers what's being tested in this file. So, perhaps we don't need this test.
"
41948446,191,junrao,2015-10-14T02:05:07Z,"Could we make sun.security.jgss.native a property in the broker/client config file? In general, it seems that other than the jaas config file, it's better to specify other properties from config file instead of system properties.
"
41948452,191,junrao,2015-10-14T02:05:21Z,"It seems that we need to turn off OP_WRITE after completing the send of each token. Otherwise, the server will be busy looping over the selector while waiting for the next token to be received.
"
41979803,191,ijuma,2015-10-14T10:59:00Z,"I don't think so. I am adding the following comment to the codebase that should explain it:

// As described in http://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/jgss-features.html:
// ""To enable Java GSS to delegate to the native GSS library and its list of native mechanisms,
// set the system property ""sun.security.jgss.native"" to true""
// ""In addition, when performing operations as a particular Subject, for example, Subject.doAs(...)
// or Subject.doAsPrivileged(...), the to-be-used GSSCredential should be added to Subject's
// private credential set. Otherwise, the GSS operations will fail since no credential is found.""
"
41995662,191,junrao,2015-10-14T13:56:11Z,"It is probably not enough to just turn off OP_WRITE at SASL completion time. After completely sending a challenge token, the client needs to turn off OP_WRITE. Otherwise, while waiting to receive the next token from the server, the client will be busy checking in the selector.
"
42009871,191,ijuma,2015-10-14T15:35:04Z,"Looking at the documentation, this only needs to be called if the value passed to `setAuthorizedID` is different than the value of `getAuthorizationID` which is not the case here. Having said that, Hadoop does the same thing so I'll leave it in case it's needed due to non-compliant implementations (unless others disagree).
"
42062496,191,harshach,2015-10-14T22:30:08Z,"@junrao yes. I'll fix it.
"
42062570,191,harshach,2015-10-14T22:30:57Z,"@ijuma it will be helpful in case of doAs which we are not supporting int this case. But will be added in future. leaving as it is would be better.
"
42062645,191,harshach,2015-10-14T22:31:37Z,"@fpj Yes. Did take it from zookeeper.
"
42063676,191,harshach,2015-10-14T22:43:28Z,"@rajinisivaram yes will make it configurable we can implement additional callbacks and digest implementation as part of another PR.
"
42117788,191,ijuma,2015-10-15T12:33:14Z,"Is it right that we always set it to authorized here (instead of checking if authenticationID and authorizationID are the same like in the client)?
"
42119040,191,ijuma,2015-10-15T12:47:05Z,"I changed it to do as you say and it seems to work fine. Will include it in my next PR so that Harsha can integrate it if he agrees.
"
42119247,191,ijuma,2015-10-15T12:49:12Z,"What is the reason that we log here, but don't throw an exception?
"
42126516,191,ijuma,2015-10-15T13:55:38Z,"OK, looking deeper into this, there is a difference: if someone else had called `setConfiguration`, `getConfiguration` would return that while here we override the value of configuration with the JAAS file. Neither option is ideal, but that's because of the global nature of this setting. I think just using `getConfiguration` is probably better, but I thought I'd mention it here for completeness.
"
42127823,191,ijuma,2015-10-15T14:05:24Z,"@rajinisivaram, do you know a way of doing this without using proprietary classes?
"
42134970,191,rajinisivaram,2015-10-15T14:59:25Z,"@ijuma Sorry, I don't know of a standard way of doing this,
"
42186791,191,ijuma,2015-10-15T22:03:44Z,"@harshach, what is the reason that we refresh TGT ourselves instead of using `renewTGT=true` in the JAAS file?
"
42187629,191,rajinisivaram,2015-10-15T22:12:12Z,"Why is serviceName a property inside JaaS config? Could this be made one of the Kafka Sasl configuration properties instead? Presumably it is used only by Kafka code and hence doesn't belong in jaas.conf? IBM JDK Kerberos module throws an exception because it doesn't recognize this property.
"
42198931,191,harshach,2015-10-16T00:42:28Z,"renewTGT=true doesn't mean it does the renewal on its own. if its a keytab you don't set it to renewTGT but if its kinit and the tgt in cache than we need to do the renewal.
"
42199039,191,harshach,2015-10-16T00:44:03Z,"@rajinisivaram serviceName always been used in jaas config and it has to match the keytab prinicpal name . Since keytab is configured in the jaas config and it makes sense to keep it there.
And all other projects from Zookeeper, hdfs to everywhere else uses serviceName in jaas config. I don't want to make that as an exception.
"
42200491,191,ijuma,2015-10-16T01:13:49Z,"OK, I read from the following that it did:

> TGT Renewals
> The Java Authentication and Authorizaton Server (JAAS) Kerberos login module in JDK 5.0, Krb5LoginModule, now supports Ticket Granting Ticket (TGT) renewal. This allows long-running services to renew their TGT automatically without user interaction or requiring the services to restart.
> With this feature, if Krb5LoginModule obtains an expired ticket from the ticket cache, then the TGT will be automatically renewed and be added to Subject of the caller who requested the ticket. If the ticket cannot be renewed for any reason, then Krb5LoginModule will use its configured callback handler to retrieve a username and password to acquire a new TGT.
> 
> To use this feature, configure Krb5LoginModule to use the ticket cache and set the newly introduced renewTGT option to true. Here is an example of a JAAS login configuration file that requests TGT renewal.

Particularly this part ""With this feature, if Krb5LoginModule obtains an expired ticket from the ticket cache, then the TGT will be automatically renewed and be added to Subject of the caller who requested the ticket""

http://docs.oracle.com/javase/8/docs/technotes/guides/security/jgss/jgss-features.html

You are saying that this doesn't actually happen and we have to provide the implementation that does the actual renewal?
"
42230500,191,ijuma,2015-10-16T10:55:40Z,"If all those projects use this property and the IBM JDK fails when it sees it, are they doing something to make it work with the IBM JDK? I looked at the ZooKeeper codebase and I couldn't find any code that retrieves a `serviceName` from a JAAS configuration file:

https://github.com/apache/zookeeper/search?utf8=%E2%9C%93&q=serviceName
"
42259558,191,rajinisivaram,2015-10-16T16:08:24Z,"Shouldn't this be a daemon thread? Otherwise it would prevent client applications from terminating.
"
42261859,191,junrao,2015-10-16T16:31:51Z,"It seems that we need the logic to turn off OP_WRITE here too. Suppose that the client tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the client receives the next token from the broker.
"
42261864,191,junrao,2015-10-16T16:31:56Z,"This seems to have the same issue as in SaslClient in that we need the logic to turn off OP_WRITE here too. Suppose that the server tries to send a token, but couldn't completely flush the writes. We get in here and completely flush the output buffer. Now, if the OP_WRITE is not turned off, the selector will be woken up all the time before the server receives the next token from the client.
"
42262890,191,ijuma,2015-10-16T16:42:30Z,"To make sure I understand, if we completely flush here, we continue executing the method. There are a few code paths where we call `sendSaslToken` which will turn off `OP_WRITE`. However, if we are in the INTERMEDIATE state and we read to the `netInBuffer` but it's not complete, we could end up returning with the OP_WRITE on even though it should be off. Is that the case you are outlining?
"
42263662,191,junrao,2015-10-16T16:50:07Z,"That's right. If we are still waiting for a new token to be completely received, we will need to turn off OP_WRITE.
"
42343337,191,ijuma,2015-10-19T07:52:15Z,"I believe this is fixed in my next PR.
"
42343338,191,ijuma,2015-10-19T07:52:19Z,"I believe this is fixed in my next PR.
"
42343484,191,ijuma,2015-10-19T07:54:15Z,"@junrao suggested that it shouldn't be because we wait for it to terminate on `shutdown`. And if consumers are closed, then it won't prevent client applications from terminating. But it may cause this problem when consumers are not closed, so I am tempted to change it back to a daemon thread. What are your thoughts Jun?
"
42366614,191,ijuma,2015-10-19T12:53:41Z,"I added a comment explaining this in my latest PR.
"
42366668,191,ijuma,2015-10-19T12:54:17Z,"I added a TODO about this, we probably need to solve it in a subsequent release.
"
42366763,191,ijuma,2015-10-19T12:55:21Z,"@harshach, this is not actually used at the moment. Can you please point me to where it should be used and I can quickly address it?
"
573093783,10070,rondagostino,2021-02-09T17:42:50Z,"Should the second check appear within the first `if` as it does below in `touch()`?  And assuming yes, maybe refactor that common logic out into a `private void removeFromActiveAndUnfenced(BrokerHeartbeatState broker)` method?"
573094685,10070,rondagostino,2021-02-09T17:44:04Z,`public void remove(...)`?
573095059,10070,rondagostino,2021-02-09T17:44:38Z,`boolean hasValidSession(...)`?
573095621,10070,rondagostino,2021-02-09T17:45:26Z,`public void touch(...)`?
573098075,10070,rondagostino,2021-02-09T17:48:45Z,"This seems to imply that it is impossible for a broker to be doing a controlled shutdown and be fenced.  I guess that means any controlled shutdown gets cancelled?  A comment explaining the implications would be helpful.  Actually, from further down in `shouldShutDown()` it appears it can shutdown immediately if it is fenced -- so I think it's about leaders moving away?  Again, a comment would help.
"
573113436,10070,rondagostino,2021-02-09T18:03:42Z,"At first I was confused as to why these two operations were necessary, then I realized it is because the instance is mutable and its places in the ordered list and `TreeSet` are going to change.  A comment here would be helpful to make this apparent (I know there is a comment in the list and set declarations, but a reminder here would be helpful nonetheless)."
573115965,10070,rondagostino,2021-02-09T18:07:27Z,"`public void beginBrokerShutDown(...)`?  Javadoc would be helpful, especially to explain what `deferred` is about.  Or, if `private` rather than `public`, at least a comment."
573116733,10070,rondagostino,2021-02-09T18:08:34Z,What is supposed to happen if it is already shutting down and this is invoked?  Will it matter if `deferred` is different in the second call?
573117364,10070,rondagostino,2021-02-09T18:09:30Z,`public` or `private`?  Same with methods below.
573122112,10070,rondagostino,2021-02-09T18:16:01Z,"Is this the case because fenced implies leadership is already moving away?  If so, a comment to that effect (or some additional wording in the log line) would be helpful."
573123473,10070,rondagostino,2021-02-09T18:17:45Z,You seem to sometime use `shutdown` and other times use `shutDown` -- not sure if that is on purpose or there is a lack of consistency?
573125162,10070,rondagostino,2021-02-09T18:20:10Z,`currLowestActiveOffset` a better name?
573146291,10070,rondagostino,2021-02-09T18:50:32Z,What's the difference between `beginBrokerShutDown()` and `updateShutdownOffset()`?  Why would the offset at which it can shutdown change?  A comment would be helpful.
573234880,10070,cmccabe,2021-02-09T20:57:26Z,"Hmm, are you suggesting that it should be public?  I'd rather not make this public because it's only accessed from within the controller package"
573235167,10070,cmccabe,2021-02-09T20:57:55Z,"Hmm, I'm not sure I understand the question...."
573235373,10070,cmccabe,2021-02-09T20:58:15Z,"Hmm, are you suggesting that it should be public?  I'd rather not make this public because it's only accessed from within the controller package"
573236986,10070,cmccabe,2021-02-09T21:01:06Z,"Thanks... that is a good catch.  Yes, it's a bit more efficient if the statements are nested.  I will refactor this out into a separate function."
573238990,10070,cmccabe,2021-02-09T21:04:59Z,"Good question.  A fenced broker will not have leaders, so there should be no leaders to move away.  More specifically, if any fenced broker tries to enter controlled shutdown, it will be shut down immediately.  I'll add a comment."
573239810,10070,rondagostino,2021-02-09T21:06:23Z,"> only accessed from within the controller package

Ok, I was just checking.  These are all fine then."
573240204,10070,rondagostino,2021-02-09T21:07:00Z,"Sorry, was asking if it should be public -- but I assume not.  Was just checking.  It's fine now."
573254808,10070,cmccabe,2021-02-09T21:29:21Z,If it's already shutting down nothing happens
573254866,10070,cmccabe,2021-02-09T21:29:28Z,package-private is OK
573256321,10070,cmccabe,2021-02-09T21:32:00Z,"In general it doesn't make sense to wait for controlled shutdown if the broker is already fenced, because in that case its leaders have already been moved away.  I will add a comment."
573256612,10070,cmccabe,2021-02-09T21:32:32Z,I wanted to standardize on shutDown.  I will fix the inconsistency.
573256853,10070,cmccabe,2021-02-09T21:33:00Z,I'll switch to `lowestActiveOffset`
573342973,10070,junrao,2021-02-10T00:08:53Z,typo hwne
573345382,10070,junrao,2021-02-10T00:15:25Z,The returned map is not keyed on partition.
573345428,10070,junrao,2021-02-10T00:15:31Z,The returned map is not keyed on partition.
573931818,10070,junrao,2021-02-10T17:32:46Z,Do we also need to update the metric for processing time?
573934771,10070,junrao,2021-02-10T17:36:55Z,Could we move those private methods after all the internal classes?
573952836,10070,junrao,2021-02-10T18:01:33Z,The return type is not a tuple.
574130720,10070,junrao,2021-02-10T22:42:20Z,Could we log lastKey too?
574142240,10070,junrao,2021-02-10T23:00:23Z,Is this used only for test?
574147362,10070,junrao,2021-02-10T23:09:32Z,Could we add a bit comment explaining logManagers and batches?
574158801,10070,junrao,2021-02-10T23:34:34Z,Could we add a comment for maxReadOffset? Is it the committed offset?
574162564,10070,junrao,2021-02-10T23:37:53Z,"We already replay the message when it's first appended to the log and here we replay the same message again after commit. This could temporarily revert the state. For example, the latest (uncommitted) config could be overwritten by a previously committed config. "
574202545,10070,junrao,2021-02-11T01:33:46Z,This can throw StaleBrokerEpochException. It would be useful for KafkaEventQueue.run() to log the event associated with the exception. 
574204032,10070,junrao,2021-02-11T01:39:11Z,"In the ZK case, we use the ZK version to do conditional updates. In Raft, could we associated each partitionState with the offset in the Raft log and use that as partitionEpoch for conditional updates? This way, we don't need to explicitly maintain a separate partitionEpoch field and the epoch is automatically bumped up for any change to the partition record, not just for leader and isr."
574204838,10070,junrao,2021-02-11T01:42:24Z,I thought the raft leader epoch is an int since we store only int as leader epoch in the log?
574208116,10070,junrao,2021-02-11T01:51:20Z,"Currently, the follower never removes the leader out of ISR. So, perhaps we should just throw an exception if this is not the case."
574209960,10070,junrao,2021-02-11T01:56:44Z,"This is in response to a heartBeat request. So, it should generate a response in ControllerResult?"
574213985,10070,junrao,2021-02-11T02:13:48Z,Some of the replay (e.g. UNREGISTER_BROKER_RECORD) could throw exceptions. We probably need to turn the exception into an error response. Are we handling that already?
574216140,10070,junrao,2021-02-11T02:23:07Z,We probably should name this sth like removeFromIsrAndMaybeChooseLeader.
574216583,10070,junrao,2021-02-11T02:25:19Z,We need to choose at least a live replica.
574765157,10070,junrao,2021-02-11T19:22:01Z,Is that temporary?
574773300,10070,junrao,2021-02-11T19:35:06Z,"In the ZK based code, we also take live brokers into consideration when selecting a new leader."
574843007,10070,junrao,2021-02-11T21:35:53Z,"Hmm, not all partitions with ISR containing the shutting down need to change the leader."
574845213,10070,junrao,2021-02-11T21:39:46Z,We already did this check and the one below in the caller through `clusterControl.checkBrokerEpoch`.
574846943,10070,junrao,2021-02-11T21:43:04Z,This seems unnecessary.
574869742,10070,junrao,2021-02-11T22:26:32Z,"If we want to log the shutting down the broker, it seems it's more consistent if we always log it. Now, it seems we log it only when leaders need to be moved."
574872303,10070,junrao,2021-02-11T22:31:36Z,"It seems that we if the request wants to shut down, we should always remove the shutting down broker from isr, just like moving the leader off the shutting down broker?"
575409674,10070,junrao,2021-02-12T18:00:36Z,"When a broker is unfenced, some of the partitions without leader could have a new leader now. So, it seems that we need to trigger a leader election here."
575433620,10070,junrao,2021-02-12T18:26:12Z,"This call generates a leader change record before the following fencedBroker record. Ordering wise, it seems that we should record the fencedBroker first. 

Also, I am wondering what's the best place to trigger leader election and removing from ISR due to fenced broker. There are multiple cases when a broker can be fenced (e.g. broker controlled shutdown, broker fenced due to no heartbeat). Instead of of doing leader election and isr removal in all those cases, another option is to tigger them in a single place when a fencedBroker record is replayed."
575435491,10070,junrao,2021-02-12T18:28:10Z,"Since we use IsrChangeRecord for changing the leader too, could we name it to sth more general?"
575451381,10070,junrao,2021-02-12T18:47:30Z,"Should we also trigger leader elections here? Also, should we allow broker decommission when it still has replicas?"
575459759,10070,junrao,2021-02-12T19:03:12Z,"This should tigger leader election too, right?"
575475691,10070,junrao,2021-02-12T19:35:05Z,"Is the intention of `shouldShutDown` to wait until the metadata of the shutting down broker is received by other brokers? If so, not sure why `beginBrokerShutDown` sets `broker.shutDownOffset` to either MAX_LONG or 0.

Also, if `shouldShutDown` returns false, when do we get another opportunity to mark the shutting down broker as fenced?
"
575478386,10070,junrao,2021-02-12T19:40:05Z,"In the ZK based logic, on receiving a controlled shutdown request, the controller tries to move the leaders off the broker. If this is successful, the controller sends a successful return for the broker to proceed with shutdown. Here, it seems that the controller will initially return shouldShutDown as false to the broker if there are leaders moved off the broker and require the broker to heartbeat again to be able to shut down."
575480618,10070,junrao,2021-02-12T19:44:40Z,I thought we want to allow topics to be created even when there is not enough live brokers now?
575514366,10070,junrao,2021-02-12T20:55:11Z,Do we need to use `this`? Ditto below.
575516660,10070,junrao,2021-02-12T20:59:54Z,"Hmm, should we return the future from `appendWriteEvent` ?"
575525508,10070,junrao,2021-02-12T21:20:15Z,"Sometimes, we update the in-memory state after the record is appended to the log. Here, it seems that we do the reverse. Should we make that consistent?"
575527291,10070,junrao,2021-02-12T21:24:36Z,It seems the broker can shut down immediately if this is false?
575535738,10070,junrao,2021-02-12T21:45:14Z,Could we just update `partitions` in place?
575541127,10070,junrao,2021-02-12T21:58:46Z,What triggers this on a hard controller failure?
575542308,10070,junrao,2021-02-12T22:01:47Z,Should we trigger the logic for leader election/isr removal since there could be unhandled fencedBroker records when the new controller takes over?
575546009,10070,junrao,2021-02-12T22:11:02Z,Should this be changed to unregisterBroker to match the KIP?
575547292,10070,junrao,2021-02-12T22:14:39Z,Should we call this sth like waitForShutdownComplete to match `beginShutdown`?
575548758,10070,junrao,2021-02-12T22:18:13Z,Is this still needed with Raft metadata?
575548839,10070,junrao,2021-02-12T22:18:24Z,This class seems unused.
575549084,10070,junrao,2021-02-12T22:19:01Z,This class seems unused.
575549246,10070,junrao,2021-02-12T22:19:28Z,Inaccurate comment.
575549741,10070,junrao,2021-02-12T22:20:43Z,Could we add some comment for this class?
575552471,10070,junrao,2021-02-12T22:25:10Z,It might be useful to log both the old and the new value.
575554284,10070,junrao,2021-02-12T22:28:11Z,Are we deprecating the state-change log and the controller log that we had before?
577117523,10070,cmccabe,2021-02-16T20:31:01Z,"Yes, it is.  I will move it to the test directory."
577125115,10070,cmccabe,2021-02-16T20:45:16Z,This code is only executed by the followers.  It is true that the leader already applied these log messages but these nodes are not the leader.
577127231,10070,cmccabe,2021-02-16T20:48:46Z,`handleEventException` handles logging exceptions thrown by events.
577135430,10070,cmccabe,2021-02-16T21:02:02Z,"Yes, I think that could work for partition epoch.  Let's do that once we have the initial code in, though..."
577136289,10070,cmccabe,2021-02-16T21:03:31Z,"It is an int and will be in 2.8, but I think that's a mistake (as discussed in the mailing list) and we should plan to make this 64 bit in the near future to avoid overflow issues.

So the controller code treats it as a long... cc @hachikuji "
577143791,10070,cmccabe,2021-02-16T21:17:09Z,ok.  We can make this an invalid request then.
577146421,10070,cmccabe,2021-02-16T21:21:53Z,"Hmm, this comment shows up for me as being left in the `decommissionBroker` function, which is called in response to the decomission broker RPC, not the heartbeat RPC.  Maybe this is a Github UI issue?  Did you mean to leave this comment for a different function?"
577147155,10070,rondagostino,2021-02-16T21:23:19Z,"I believe after this we need to invoke something to cover the case where a topic has this broker as its only ISR member:

```
replicationControl.maybeMakeLeader(request.brokerId(), result.records());
```

The implementation might look like this:

```
    void maybeMakeLeader(int brokerId, List<ApiMessageAndVersion> records) {
        Iterator<TopicPartition> iterator = brokersToIsrs.iterator(brokerId, false);
        while (iterator.hasNext()) {
            TopicPartition topicPartition = iterator.next();
            TopicControlInfo topic = topics.get(topicPartition.topicId());
            if (topic == null) {
                throw new RuntimeException(""Topic ID "" + topicPartition.topicId() + "" existed in "" +
                    ""isrMembers, but not in the topics map."");
            }
            PartitionControlInfo partition = topic.parts.get(topicPartition.partitionId());
            if (partition == null) {
                throw new RuntimeException(""Partition "" + topicPartition +
                    "" existed in isrMembers, but not in the partitions map."");
            }
            // make it the leader if it is the only ISR member
            if (partition.isr.length == 1 && partition.isr[0] == brokerId && partition.leader != brokerId) {
                int newLeader = brokerId;
                int newLeaderEpoch = partition.leaderEpoch + 1;
                IsrChangeRecord message = new IsrChangeRecord().
                    setPartitionId(topicPartition.partitionId()).
                    setTopicId(topic.id).
                    setIsr(Replicas.toList(partition.isr)).
                    setLeader(newLeader).
                    setLeaderEpoch(newLeaderEpoch).
                    setPartitionEpoch(partition.partitionEpoch + 1);
                records.add(new ApiMessageAndVersion(message, (short) 0));
            }
        }
    }
```
"
577148107,10070,cmccabe,2021-02-16T21:25:13Z,If an exception is thrown here we will end up in `handleEventException`.  Since the exception won't be a subclass of `ApiException` we will resign as controller and return an `UnknownServerException`.
577148212,10070,rondagostino,2021-02-16T21:25:26Z,"I believe we should surround this section of code with the following to be sure we never drop the last ISR member:

```
            // never remove the last ISR member
            if (partition.isr.length > 1) {
                int[] newIsr = ...
                etc...
            }
```"
577163457,10070,cmccabe,2021-02-16T21:52:59Z,I changed it to `removeFromIsrAndLeaderships`
577167409,10070,cmccabe,2021-02-16T21:58:24Z,"good point, will fix"
577167941,10070,cmccabe,2021-02-16T21:59:17Z,This has to be handled by the individual brokers.  It's not persisted anywhere currently (currently it is not stored in ZK I believe)
577176042,10070,cmccabe,2021-02-16T22:14:05Z,"In this function we are iterating only over the partitions that the given broker is a leader for.

( We obtained the iterator from `BrokersToIsrs#iterator(brokerId=brokerId, leadersOnly=true)` )"
577178197,10070,cmccabe,2021-02-16T22:18:19Z,ok
577187004,10070,cmccabe,2021-02-16T22:34:34Z,"I will improve the logging a bit here.  I agree that we should log when a broker is told it can shut down or be (un) fenced, since those are major events."
577188202,10070,cmccabe,2021-02-16T22:36:59Z,"It seemed safer to leave it in the ISR until it's ready to shut down for good.  Also, if we take it out, it might just get re-added if it catches up... ?"
577214559,10070,cmccabe,2021-02-16T23:36:58Z,"Good point. The appropriate place to handle this will be in the broker heartbeat handling code, since that is where the active controller unfences brokers."
577217605,10070,cmccabe,2021-02-16T23:44:42Z,"> This call generates a leader change record before the following fencedBroker record. Ordering wise, it seems that we should record the fencedBroker first.

Hmm... it should be OK to remove the leaderships first.

Kip-500 Controlled shutdown works this way, for example... the shutting-down broker is not actually fenced at all until all the other brokers have removed it as a leader.

Also, wouldn't it be a bit weird to be in a state where a broker is still marked as the leader for some partition, but doesn't show up in the list of brokers given in the MetadataResponse?  That would happen if we put the fencing record first.  I don't think clients or brokers would handle this well.

> Also, I am wondering what's the best place to trigger leader election and removing from ISR due to fenced broker. There are multiple cases when a broker can be fenced (e.g. broker controlled shutdown, broker fenced due to no heartbeat). Instead of of doing leader election and isr removal in all those cases, another option is to tigger them in a single place when a fencedBroker record is replayed.

Replaying a record cannot trigger the creation of any additional records.  This would not work since the standby controllers can't create records, after all... only the active controller.

I have created a `handleBrokerFenced` function in `ReplicationControlManager` which does most of what needs to be done for fencing, though... aside from creating the actual fencing record."
577259797,10070,junrao,2021-02-17T01:23:48Z,Sounds good. Could we add a comment to make that clear?
577262652,10070,junrao,2021-02-17T01:31:42Z,"It seems that we are logging at the debug level. I am wondering if we should log at WARN as before in ZK based appoach.

```
        if (exception instanceof ApiException) {
            log.debug(""{}: failed with {} in {} us"", name,
                exception.getClass().getSimpleName(), deltaUs);
            return exception;
        }

```"
577263866,10070,junrao,2021-02-17T01:35:14Z,"I thought the issue was that if changing leaderEpoch to Long requires a log format change for user data, it has significant performance impact such as down conversion."
577268593,10070,junrao,2021-02-17T01:48:41Z,"Sorry, I meant `decommissionBroker`. It seems that decommissionBroker request should we a response too, right?"
577278241,10070,junrao,2021-02-17T02:16:16Z,"Well, in ZK based approach, in response to a controlled shutdown, the controller changes ISR and the leader. Here, it seems that `result.response().isFenced()` is not always true if the broker heartbeat indicates the intention to shut down?"
577865782,10070,cmccabe,2021-02-17T18:57:08Z,"I normally do group private methods after internal classes, but in this case, it seemed better to keep them together.  Otherwise you'd have to jump around a lot when reading the code.  What do you think?"
577866850,10070,cmccabe,2021-02-17T18:58:41Z,added
577868843,10070,cmccabe,2021-02-17T19:01:51Z,"We can get here just because the user made an invalid RPC, so I don't know if WARN is appropriate.  I'll change it to INFO for now.  "
577879960,10070,junrao,2021-02-17T19:19:29Z,Sounds good too.
577894071,10070,cmccabe,2021-02-17T19:42:26Z,"Note: this has been changed to unregisterBroker as per the mailing list discussion.

It's OK to have a future that returns void.  That just means you either get success, or an error (there is no other result).  Which is consistent with UnregisterBrokerResponse.json, etc."
577895640,10070,cmccabe,2021-02-17T19:45:07Z,Hmm... We only fence the broker once controlled shutdown has completed.  If we fenced it immediately that would be disruptive to clients since they wouldn't be able to continue fetching from it until the leaderships have moved. Basically immediate fencing is the NON-controlled shutdown path....
577914392,10070,cmccabe,2021-02-17T20:16:22Z,I have renamed it to `PartitionChangeRecord`
577915565,10070,cmccabe,2021-02-17T20:18:09Z,"> Should we also trigger leader elections here? 

Good catch.  Yes, this should be moving leaders.  I fixed this in the latest version of the PR.

> Also, should we allow broker decommission when it still has replicas?

Yes, this is allowed."
577916179,10070,cmccabe,2021-02-17T20:19:10Z,"I restructured this code a bit.  But yes, it does move leaders if needed (it's clearer in the new version I think)"
577916633,10070,cmccabe,2021-02-17T20:19:56Z,"This got refactored, hopefully the new version is clearer.  The new version avoids the MAX_LONG / 0 hack and other ugliness that was in the initial version."
577916988,10070,cmccabe,2021-02-17T20:20:33Z,that is correct.  the broker must send another heartbeat before it is allowed to shut down.
577918400,10070,cmccabe,2021-02-17T20:23:00Z,"good point.  Since we don't have much time for 2.8, I will add a TODO for now."
577919102,10070,cmccabe,2021-02-17T20:24:17Z,good catch.
577919831,10070,cmccabe,2021-02-17T20:25:33Z,"The heartbeat manager is special because it stores soft state which is not in the metadata log (when each broker last heartbeated, for example)."
577920156,10070,cmccabe,2021-02-17T20:26:04Z,"this got refactored. hopefully it is clearer now.  yes, a broker can shut down immediately in some cases"
577920610,10070,cmccabe,2021-02-17T20:26:52Z,since this is stored in a `TimelineHashMap` it must be treated as immutable.  We can't modify the past.
577921275,10070,cmccabe,2021-02-17T20:27:54Z,this comes out of the Raft layer and is invoked when the Raft election has succeeded and produced a new leader node.
577922334,10070,cmccabe,2021-02-17T20:29:46Z,"the standby controller must replay all the committed records before becoming active.  so, there is no unfinished work to be done at this point."
577922831,10070,cmccabe,2021-02-17T20:30:39Z,"The name close comes from `AutoCloseable`, which makes some of the tests nicer to write (because we can use the Java try-with-resources syntax)."
577929118,10070,cmccabe,2021-02-17T20:41:57Z,"I don't think the state change log can scale to the number of partitions we need.  It gets too verbose.  Also, this information is available in the metadata log itself."
577929403,10070,cmccabe,2021-02-17T20:42:29Z,"Thanks, @rondagostino .  In the latest version, I do not remove brokers from singleton ISRs."
577961646,10070,rondagostino,2021-02-17T21:36:39Z,`MockControllerMetrics` is a test class?
577971959,10070,rondagostino,2021-02-17T21:54:59Z,"Maybe it would be better to check for null and exit out if it is unset -- otherwise we see this, which is not ideal:

```
Fatal error during controller startup. Prepare to shutdown (kafka.server.ControllerServer)
java.lang.ClassNotFoundException: org.apache.kafka.controller.MockControllerMetrics
```"
577993008,10070,junrao,2021-02-17T22:34:45Z,"In the old controller, EventQueueTimeMs is used to measure the amount of time an event is sitting in the queue before being processed. There is a separate timer metric per controller state that measures the processing time per event type."
578003900,10070,junrao,2021-02-17T22:58:11Z,It seems that using long for leaderEpoch in the log requires a bigger discussion. Could we use int for now?
578014387,10070,junrao,2021-02-17T23:22:16Z,"If a broker wants to shut down and is only included in ISR, we still want to remove the broker from ISR before allowing it to shutdown. Otherwise, a new published record needs to wait for the session timeout before it can be committed."
578015133,10070,junrao,2021-02-17T23:24:13Z,What about the case when request.wantFence() is true?
578019439,10070,junrao,2021-02-17T23:35:12Z,"(1) In ZK-based approach, we do leader election a bit differently for controlled shutdown. If we can't select a leader from the remaining ISR, we just leave the current leader as it is. This gives the shutting down broker a chance to retry controlled shutdown until the timeout.

(2) In ZK-based approach, we also remove the broker from isr for other partitions whose leader is not on the shutting down broker.

> It seemed safer to leave it in the ISR until it's ready to shut down for good. Also, if we take it out, it might just get re-added if it catches up... ?

That's true and is an existing problem. One way to address this is to include partitionEpoch in the follower fetch request. The leader could then reject a follower request if the partitionEpoch doesn't match. This can be done in a followup PR."
578026068,10070,junrao,2021-02-17T23:52:30Z,Could we add a TODO for handling the preferred leader election?
578028795,10070,junrao,2021-02-17T23:59:48Z,Should this be named unregisterBroker?
578047059,10070,junrao,2021-02-18T00:51:31Z,Is this check already implied since we are iterating `brokersToIsrs`?
578054305,10070,junrao,2021-02-18T01:08:09Z,Is there any benefit/enough to only allow the broker to shutdown when all brokers have caught up to controlledShutDownOffset? The ZK-based criteria is that a broker is allowed to shut down as long as all leaders have been moved off the shutting down broker.
578062730,10070,junrao,2021-02-18T01:31:49Z,We are generating an UnregisterBrokerRecord.
578065045,10070,junrao,2021-02-18T01:36:55Z,Are all records generated in a single ControllerWriteEvent appended to the metadata log atomically?
578067236,10070,junrao,2021-02-18T01:43:22Z,"Hmm, the comment seems to be the same as shouldShutDown."
578071150,10070,junrao,2021-02-18T01:55:21Z,testFindStaleBrokers ?
578076422,10070,junrao,2021-02-18T02:09:31Z,Could we make it private?
578079100,10070,junrao,2021-02-18T02:17:13Z,testUnregister?
578080050,10070,junrao,2021-02-18T02:19:53Z,testPlaceReplicas ?
578603931,10070,cmccabe,2021-02-18T17:19:36Z,"Good point. I will fix it so that EventQueueTimeMs has its original meaning.

For now, I have added a metric called EventQueueProcessingTimeMs which deals with processing time.  I do want to do the per-state tracking but I don't think we have time right now"
578606545,10070,junrao,2021-02-18T17:23:17Z,An empty topic name currently results in an INVALID_REQUEST error.
578613805,10070,junrao,2021-02-18T17:32:32Z,An empty broker currently results in an INVALID_REQUEST error.
578619727,10070,cmccabe,2021-02-18T17:40:46Z,"OK, that makes sense.  I will remove it from all non-singleton ISRs as well as removing it from all leaderships."
578621621,10070,cmccabe,2021-02-18T17:43:39Z,"Good question. The broker doesn't currently request fencing once it is unfenced.  But for completeness, it is simple to support this and it makes the code more intuitive, so I'll add it."
578625531,10070,cmccabe,2021-02-18T17:49:18Z,"As I mentioned above, I changed it so that it now removes the broker from all non-singleton ISRs, as well as removing it from leaderships.

It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline).  If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1.  Maybe we can tune this a bit later?

I like the idea of including the partition epoch in the follower fetch request."
578627766,10070,cmccabe,2021-02-18T17:52:32Z,"Hmm, I thought this already handles preferred leader election (there are only two options, PREFERRED and UNCLEAN, so far...)"
578628736,10070,cmccabe,2021-02-18T17:53:57Z,Good catch. This has been superseded by `ReplicationControlManager#unregsiterBroker`.
578629900,10070,cmccabe,2021-02-18T17:55:39Z,"We're iterating over the partitions with no leader, which may or may not have the newly activated broker in their ISR."
578630655,10070,cmccabe,2021-02-18T17:56:43Z,"Basically it lets us know that the other brokers have successfully taken over as leader (where needed) which avoids having a period of unavailability, ideally"
578648887,10070,cmccabe,2021-02-18T18:23:21Z,fixed
578664492,10070,junrao,2021-02-18T18:47:22Z,Do we allow a heartbeat request to set both the fence and wantShutDown flag?
578667056,10070,junrao,2021-02-18T18:51:14Z,"> It seems like the remaining behavioral difference is that the new code will, if no other leader can be chosen, set the leader to -1 (offline). If we don't do this, controlled shutdown easily gets stuck if there are any partitions with replication factor = 1. Maybe we can tune this a bit later?

It's fine to revisit that later. The tradeoff is that if we wait, it slightly increases the probability of availability since another replica could join isr."
578669898,10070,junrao,2021-02-18T18:55:42Z,"I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2."
578693886,10070,junrao,2021-02-18T19:35:29Z,"This can be revisited later. When finalizing a feature, should be consider other controller's supported features too?"
578696957,10070,junrao,2021-02-18T19:40:18Z,This class seems never used?
578699530,10070,junrao,2021-02-18T19:44:45Z,Could we add some comments for this class?
578700824,10070,junrao,2021-02-18T19:46:48Z,"To make this more intuitive, perhaps we could add a method isActive in QuorumController?"
578722110,10070,junrao,2021-02-18T20:23:39Z,"Hmm, why is a replication factor of 1 invalid?"
578817527,10070,cmccabe,2021-02-18T23:24:41Z,"A Resource with type BROKER and an empty name represents a cluster configuration that applies to all brokers.  I'll add a comment
"
578817611,10070,cmccabe,2021-02-18T23:24:57Z,"yes, it can set both"
578850598,10070,cmccabe,2021-02-19T00:54:40Z,"Hmm... right now, we don't have a good way of finding out what features the other controllers support.  Maybe we will have to think more about this when we support rolling upgrade in kip-500."
578850804,10070,cmccabe,2021-02-19T00:55:21Z,It's used in unit tests
578851481,10070,junrao,2021-02-19T00:57:07Z,"Yes, that's the problem. From a consistency perspective, it seems that we should use the supported features from either all controller nodes or none."
578857828,10070,cmccabe,2021-02-19T01:15:22Z,"There are no unfenced brokers (as you mentioned earlier, we should change this so that it places on the fenced broker).  I will add a TODO"
578876029,10070,junrao,2021-02-19T02:10:09Z,Could we add some comments to this class?
578877349,10070,junrao,2021-02-19T02:14:23Z,"> > I think we need to handle preferred leader election in a special way. For example, if the assigned replicas are 1,2,3, isr is 2,3 and the current leader is 3, when doing preferred leader election, we want to keep the leader as 3 instead of changing it to 2.


> 
> Hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred?

Well, currently the contract is just that if every broker picks the preferred replica (i.e. 1st replica), the leaders will be balanced among brokers. If not, all other replicas are equivalent. Moving leaders among non-preferred replicas just creates churns without benefiting the balance."
578878282,10070,junrao,2021-02-19T02:17:34Z,"As Jason pointed out, in ZK based approach, the controller bumps up the leader epoch for removing replica from ISR too.

Also, since the broker is no longer receiving the leaderAndIsr requests, we need some logic for the broker to ignore the new partition record (for follower fetching) once it starts the controlled shutdown process."
579374802,10070,cmccabe,2021-02-19T18:04:52Z,"This is resolved in the latest version of the code, where we disable metadata updates on the shutting down broker before starting controlled shutdown, and bump the leader epoch of all partitions."
579378306,10070,cmccabe,2021-02-19T18:10:42Z,"I changed this so that the leader epoch is bumped if and only if there is a leader present in the PartitionChange record.  (It is possible to bump the epoch without changing the leader by including the same leader again in the record.)

We now use this during controlled shutdown to unconditionally bump the leader epochs.  Otherwise, we only bump the leader epochs if the leader changed."
579403297,10070,junrao,2021-02-19T18:50:24Z,"Hmm, does Integer.MIN_VALUE have any special meaning? If so, could we use a more intuitive constant? "
579406849,10070,junrao,2021-02-19T18:56:29Z,"Hmm, if the leader is already -1 and we can't change ISR, there is no need to generate a new PartitionChangeRecord just to bump up the leader epoch. It won't help controlled shutdown since there is already no leader."
579410463,10070,junrao,2021-02-19T19:01:26Z,"Currently, for controller initiated ISR change (controlled shutdown or hard failure), we always bump up the leader epoch. Also, the name alwaysBumpLeaderEpoch is a bit weird since the code in handleNodeDeactivated() doesn't directly bump up leader epoch."
579411851,10070,junrao,2021-02-19T19:03:59Z,"Currently, for leader initiated AlterIsr request, the controller doesn't bump up the leader epoch. If we change that, it will slightly increase unavailability since all clients have to refresh the metadata in this case."
579414289,10070,junrao,2021-02-19T19:08:23Z,"If we do this, does `brokerMetadataListener.close() `still need to call `beginShutdown()`."
579421318,10070,junrao,2021-02-19T19:21:16Z,"Hmm, it seems that we should only do `newLeader != partitionInfo.preferredReplica()` if this is a preferred leader election. "
579438984,10070,junrao,2021-02-19T19:54:01Z,"Hmm, merge bumps up the leaderEpoch. It seems that this needs to be persisted in the metadata log?"
579458466,10070,cmccabe,2021-02-19T20:31:19Z,No special meaning.  It's just a constant that can't be a valid leader.  We could use -2 if that seems nicer.
579462462,10070,cmccabe,2021-02-19T20:38:40Z,"Hmm... I don't completely understand why we would want to bump the leader epoch when the controller removes a non-leader broker B but not when an AlterIsrRequest removes a non-leader broker B from the ISR.  It seems like we should either bump in both scenarios or neither.

Is the fact that we bump in the first scenario just an artifact of the fact that otherwise we could not send out a leader and isr request that had a new epoch and thereby caused a  change?"
579463797,10070,cmccabe,2021-02-19T20:41:39Z,"Hmm... ReplicationControlManager should not allow this to happen during an alter isr request.  There is some code that checks if the alter isr request is attempting to remove the current leader from the isr, and returns an error if so.  So the leader should not be changed by an alter isr request and therefore the leader epoch will not be.

```
                if (!Replicas.contains(newIsr, partition.leader)) {
                    // An alterIsr request can't remove the current leader.
                    responseTopicData.partitions().add(new AlterIsrResponseData.PartitionData().
                        setPartitionIndex(partitionData.partitionIndex()).
                        setErrorCode(Errors.INVALID_REQUEST.code()));
                    continue;
                }
```"
579464755,10070,cmccabe,2021-02-19T20:43:43Z,It does need to be because there are some paths through the code that don't go through here.  In general calling `beginShutdown` or `close` multiple times is harmless-- only the first time has an effect.
579496903,10070,junrao,2021-02-19T21:52:36Z,"Yes, the alterIsr doesn't change leader, but generates a PartitionChangeRecord. On replaying this record, the code following code bumps on leaderEpoch?

`        PartitionControlInfo newPartitionInfo = prevPartitionInfo.merge(record);`"
579497748,10070,junrao,2021-02-19T21:54:30Z,"Ok, maybe the check can be `record.leader()< -1`?"
579522201,10070,cmccabe,2021-02-19T22:47:15Z,"Even in an unclean leader election, we don't want to change the leader unless we need to."
579522827,10070,cmccabe,2021-02-19T22:48:43Z,"The leader epoch is managed implicitly -- every time a PartitionChangeRecord appears, the epoch is bumped if the leader is not NO_LEADER_CHANGE."
579536414,10070,junrao,2021-02-19T23:28:15Z,"Hmm, we should set the leader to NO_LEADER_CHANGE, right?"
579538752,10070,junrao,2021-02-19T23:37:14Z,Could you file a separate jira to follow up on PartitionEpoch post 2.8?
579547098,10070,cmccabe,2021-02-20T00:07:59Z,That is the default so we don't need to set it unless we're changing it
579547404,10070,cmccabe,2021-02-20T00:09:24Z,filed KAFKA-12349
579547731,10070,cmccabe,2021-02-20T00:10:42Z,"merge only bumps the epoch if the leader was set.

```
        PartitionControlInfo merge(PartitionChangeRecord record) {
            int[] newIsr = (record.isr() == null) ? isr : Replicas.toArray(record.isr());
            int newLeader;
            int newLeaderEpoch;
            if (record.leader() == NO_LEADER_CHANGE) {
                newLeader = leader;
                newLeaderEpoch = leaderEpoch;
            } else {
                newLeader = record.leader();
                newLeaderEpoch = leaderEpoch + 1;
            }
            return new PartitionControlInfo(replicas,
                newIsr,
                removingReplicas,
                addingReplicas,
                newLeader,
                newLeaderEpoch,
                partitionEpoch + 1);
        }
```"
579547825,10070,cmccabe,2021-02-20T00:11:10Z,"As per our discussion outside github, let's just use the old behavior for now."
579547910,10070,cmccabe,2021-02-20T00:11:40Z,I added a constant.  I think it looks a little nicer...
579553251,10070,junrao,2021-02-20T00:35:14Z,"We have NO_LEADER_CHANGE as the default for the serialized data. However, the active controller replays the PartitionChangeRecord created in memory, which defaults leader to NO_LEADER_CHANGE, right?"
579564268,10070,cmccabe,2021-02-20T01:15:01Z,let's revisit this after 2.8
579564579,10070,cmccabe,2021-02-20T01:16:31Z,RIght now the answer is yes. Eventually we plan on supporting multiple batches.
579564654,10070,cmccabe,2021-02-20T01:17:04Z,"I do think we should harmonize this, but I think it would be better to do that when we get rid of MetaLogShim. We've had a plan to get rid of the shim layer for a while but we just didn't have time to do it this week. So let's plan to do it then, if that makes sense

"
579565540,10070,cmccabe,2021-02-20T01:21:30Z,ack.  I fixed this
580411938,10070,junrao,2021-02-22T16:54:21Z,Do we have a jira to track this?
136670512,3765,junrao,2017-09-01T21:42:22Z,Could you carry over the comments in ReplicaStateMachine about the possible state changes and the corresponding actions? Ditto for the new PartitionStateMachine.
136672966,3765,junrao,2017-09-01T22:00:28Z,"I am actually not sure if we need to read the partition state from ZK in this case. The transition to NewReplica is only used in 2 places: (1) starting a new replica in partition reassignment, (2) when a new topic is created. In (1), if there is a leader, it will already be cached by the controller. In (2), currently, it seems it's guaranteed there is not a leader at this point. A subsequent transition to OnlinePartition in onNewPartitionCreation() will do the leader election part. 

So, it seems that we can (a) just read the cached leader info, (b) call brokerRequestBatch.addLeaderAndIsrRequestForBrokers if leader is available and doesn't equal to current replica, (c) move the replica to NewReplica state unless the leader equals to the current replica, (d) we probably also want to log an error if the new replica happens to be the leader."
136675513,3765,junrao,2017-09-01T22:24:23Z,It will be useful to document the response since it is a bit complicated. 
136678656,3765,junrao,2017-09-01T22:59:27Z,"It seems that it may be possible for the response to have a sequence of CONNECTIONLOSS, followed by a sequence of OK when the session was disconnected and then reconnected again. So, we may want to just do filter instead of takeWhile."
136679343,3765,junrao,2017-09-01T23:07:29Z,It will be useful to explicitly define the return type in each of the public methods to make it clear.
136679765,3765,junrao,2017-09-01T23:13:11Z,"Hmm, will KafkaControllerZkUtils.getTopicPartitionStates ever throw an Exception?"
136872413,3765,junrao,2017-09-04T21:52:57Z,"It seems that we only need to check the topic config if the isr is empty after replicaId is removed from isr, not before."
136872700,3765,junrao,2017-09-04T22:00:02Z,"For those partitions whose isr can't be shrunk due to making isr empty, we simply keep the current isr. So, it seems in this case, we can just do an optimization by not sending the LeaderAndIsrRequest for those partition since neither the leader nor the isr will change."
136873950,3765,junrao,2017-09-04T22:29:21Z,"Hmm, do we need the logic here? It seems that we are refreshing the topic partition state when retrying doRemoveReplicasFromIsr. So, reading the topic partition state here seems redundant."
136874953,3765,junrao,2017-09-04T22:57:06Z,We will need to handle NoNode error here by creating the missing parent path if needed.
136875070,3765,junrao,2017-09-04T23:00:51Z,Check the error code?
136875311,3765,junrao,2017-09-04T23:06:24Z,It seems that we should do this in a while loop since we do conditional update to the leaderAndIsr path in ZK and need to retry on BadVersion.
136875649,3765,junrao,2017-09-04T23:15:27Z,"partitionsWithUncleanLeaderElectionState includes partitions that don't need unclean leader election. So, the name seems a bit miss-leading."
136876376,3765,junrao,2017-09-04T23:35:34Z,"Ideally, we want to select the preferred replica if it's alive, in-sync and not shutting down as ControlledShutdownLeaderSelector does."
136876429,3765,junrao,2017-09-04T23:36:33Z,unused import
136876599,3765,junrao,2017-09-04T23:40:15Z,"If a create operation retries due to Code.CONNECTIONLOSS, it's possible for the retry to receive a NodeExist error since the previous create may have succeeded. Perhaps on NodeExist error during retry, we can read the value back and only return NodeExist error if the value is different from that to be created."
136876729,3765,junrao,2017-09-04T23:44:05Z,Do we need to add some general handling for errors like NoAuth?
136903037,3765,onurkaraman,2017-09-05T06:23:29Z,"Yeah I noticed this as well. I mainly just wanted to keep the existing behavior the same and worry about tweaking the existing behavior in later patches.

The only reason I can come up with for this zk lookup is to notice if another controller takes over while this current controller is in the process of doing this transition."
136904968,3765,onurkaraman,2017-09-05T06:39:48Z,"It seems that in this scenario today, the leadership changes to LeaderAndIsr.NoLeader (-1), the leader epoch increments, and the zkVersion increments too. We send LeaderAndIsrRequests to the full replica set (not just ISR) and also send UpdateMetadataRequests to the whole cluster with this updated LeaderAndIsr with leader = -1, isr = old isr with the single replica, the incremented leader epoch, and the incremented zkVersion.

In fact, whenever we send a LeaderAndIsrRequest, we also broadcast UpdateMetdataRequests to the whole cluster. Are you suggesting we do neither of these things? Skipping these steps means that:
1. non-isr replicas will not be aware of the incremented leader epoch and zkVersions.
2. the cluster will not receive the updated metadata."
136905187,3765,onurkaraman,2017-09-05T06:41:27Z,"It's possible since it internally calls ZookeeperClient.waitUntilConnected, which can throw exceptions (ZookeeperClientAuthFailedException and ZookeeperClientExpiredException)."
136906744,3765,onurkaraman,2017-09-05T06:53:16Z,"It attempts to mimic the existing behavior in ReplicationUtils.checkLeaderAndIsrZkData (which is called from ReplicationUtils.updateLeaderAndIsr).

Strictly speaking I don't think this logic is required. I think it just attempts to make progress on a version conflict (for instance if the partition leader concurrently updated isr) instead of starting over."
137011097,3765,junrao,2017-09-05T14:53:43Z,"Hmm, since we are fixing the issue with ZK session expiration, there shouldn't be more than 1 controller accessing ZK at the same time. In general, I agree that we don't want to make major changes to Partition/ReplicaStateMachine while refactoring ZK accesses. However, if this simplifies how we use ZK, it may be worth doing."
137011133,3765,junrao,2017-09-05T14:53:51Z,"Ah, ok. So, we did change the leader in this case."
137011245,3765,junrao,2017-09-05T14:54:07Z,"Hmm, for both ZookeeperClientAuthFailedException and ZookeeperClientExpiredException, we probably don't want to retry forever in removeReplicasFromIsr(). It seems that we should just log an error and move on."
137011321,3765,junrao,2017-09-05T14:54:22Z,"Ok, that logic is just to handle the possibility that a previous write has succeeded on a ConnectionLossException. It would be useful to document that."
137052511,3765,onurkaraman,2017-09-05T16:58:18Z,It's not terribly clear but that's actually the behavior in the PR.
137159386,3765,onurkaraman,2017-09-06T02:34:25Z,This falls in the category of suggestions that differ in current behavior. Again I'm not sure if we want to overload this PR with more than just porting existing behavior.
137159912,3765,onurkaraman,2017-09-06T02:40:39Z,"In the case of `KafkaControllerZkUtils.createTopicPartitionStates`, we're creating the state znode for the first time, in which case the following parent znodes almost definitely will not exist:
* /brokers/topics/\<topic\>/partitions
* /brokers/topics/\<topic\>/partitions/\<id\>
* /brokers/topics/\<topic\>/partitions/\<id\>/state

So perhaps in this case, we should push the creation of these parent znodes into `KafkaControllerZkUtils.createTopicPartitionStates`."
137160199,3765,onurkaraman,2017-09-06T02:43:52Z,"Yeah as stated in one of my earlier PR comments, no effort has been put into PartitionStateMachineV2 yet to do error handling and retries. The idea was to first validate that the overall strategy would work in ReplicaStateMachineV2 and then apply error handling and retries to PartitionStateMachineV2."
137167391,3765,onurkaraman,2017-09-06T04:07:12Z,"There was one typo in getTopicPartitionStatesFromZk:
`val candidateIsr = leaderIsrAndControllerEpoch.leaderAndIsr.isr.filterNot(_ != replicaId)`
should be:
`val candidateIsr = leaderIsrAndControllerEpoch.leaderAndIsr.isr.filter(_ != replicaId)`

but otherwise we actually do the topic config checks only on the partitions whose isr is empty after removal. Note that the `candidateLeaderAndIsrs` returned is used to find `partitionsWithEmptyIsr` which is what we pass into `leaderAndIsrBasedOnLogConfigs`."
137349319,3765,junrao,2017-09-06T18:18:11Z,"Yes, that sounds good."
138211018,3765,junrao,2017-09-11T22:39:08Z,"It seems that we will need to set ACL based on whether ZK security is enabled or not, like what ZkUtils.defaultAcls() does."
138212390,3765,junrao,2017-09-11T22:47:47Z,This method can be private.
138451688,3765,junrao,2017-09-12T20:06:03Z,Perhaps we should also log an error for any other error code?
138454615,3765,junrao,2017-09-12T20:19:12Z,"Hmm, there is a bit of inconsistency here in how we deal with exceptions from ZK calls. In general, it seems that exceptions (e.g. authorization error, session closed, etc) are not retriable. So, we probably should return them in a failed partition map as what we do in getLogConfigs()."
138478606,3765,junrao,2017-09-12T22:03:50Z,"I think we can simplify this logic a bit. BADVERSION can happen because of updates from another client (e.g., leader) or retries from a connection loss. Technically, for the latter, we can do a read and avoid updating the path again if the new value is already in place. However, it seems that it's simpler to just return any partition with BADVERSION as updatesToRetry and the let the caller retry. The caller already has the logic to read the latest value from ZK on retry. Since connection loss is rare, doing an extra write when it happens is probably ok."
138480660,3765,junrao,2017-09-12T22:14:37Z,"Hmm, it seems any other error is not really retriable and should be returned in a failed partition map."
138489383,3765,junrao,2017-09-12T23:09:11Z,"""Controller %d epoch %d initiated state change for partition %s from %s to %s failed"" => ""Controller %d epoch %d failed to change state for partition %s from %s to %s"" ?"
138490985,3765,junrao,2017-09-12T23:20:44Z,"Hmm, it seems the convention should be that any exception is a non-retriable error and we will just log an error on those partitions, and then move on w/o retry."
138495827,3765,junrao,2017-09-12T23:55:43Z,"We want to pick the preferred replica if possible. isr in general is not ordered. So, we want to go through assigned_replicas in order as ControlledShutdownLeaderSelector does."
138496088,3765,junrao,2017-09-12T23:57:37Z,Would the code be easier to read if we just return failed partitions to electLeaderForPartitions() and log the error there?
138499176,3765,junrao,2017-09-13T00:23:11Z,Same comment here. Would the code be easier to read if we just return failed partitions to removeReplicasFromIsr() and log the error there?
138501599,3765,junrao,2017-09-13T00:46:26Z,It's a bit inconsistent to return currentLeaderAndIsrs since it includes candidateLeaderAndIsrs. It seems it's easier to understand if we return 4 disjoint sets. The first two parts could then be named partitionsWithoutReplicaInIsr and partitionsWithReplicaInIsr
138502217,3765,junrao,2017-09-13T00:52:27Z,It seems that we should extract the topic set before passing into KafkaControllerZkUtils.getLogConfigs()?
138502301,3765,junrao,2017-09-13T00:53:17Z,Could topics be a Set?
138505415,3765,junrao,2017-09-13T01:22:10Z,"The method name is a bit confusing. It sounds like that we are just reading the partition state from ZK, but we actually also remove the isr here. So, we probably want to use a more accurate method name. Also, would it be better to change the isr in leaderAndIsrBasedOnLogConfigs() instead of here?"
138505545,3765,junrao,2017-09-13T01:23:35Z,"Hmm, do we need those wrappers? Could be just change ControllerEvent directly?"
138675875,3765,onurkaraman,2017-09-13T16:46:26Z,"Anything with a V2 suffix is just temporary. It's there to make the diff easier to read. Once we agree on the changes, I'll remove the suffix and make the new code replace the old."
138677309,3765,onurkaraman,2017-09-13T16:52:16Z,"This had also crossed my mind but decided to just try to replicate existing behavior in the first pass.

However I agree that doing so simplifies the logic here quite a bit and agree that the change should be made in this PR."
138684045,3765,onurkaraman,2017-09-13T17:20:35Z,"The behavior in the PR just tries to imitate `ZkUtils.conditionalUpdatePersistentPath` which marks the update success as false when it receives any exception other than BADVERSION. `ZkUtils.conditionalUpdatePersistentPath` is called by `ReplicationUtils.updateLeaderAndIsr` which is called by `KafkaController.removeReplicaFromIsr`. That update success value is what determines if we should retry in the while loop of `KafkaController.removeReplicaFromIsr`.

That being said, the existing behavior isn't necessarily right."
138807407,3765,onurkaraman,2017-09-14T06:35:39Z,Suppose we collect all failed partitions and return it to `electLeaderForPartitions`. How will `electLeaderForPartitions` log relevant information? I think you'd basically have to return error messages or exceptions back to `electLeaderForPartitions`. So the end result would just consolidate all the calls to `logFailedStateChange` into one place. Is this what you had in mind?
139002874,3765,junrao,2017-09-14T20:28:40Z,"Yes, I was thinking of returning partition -> error_code map to the caller."
139269156,3765,junrao,2017-09-15T23:39:22Z,Should we do that in every test?
139270862,3765,junrao,2017-09-16T00:01:44Z,"It will be useful to add a test for the controlled shutdown case, which involves setting controllerContext.shuttingDownBrokerIds and making the state transition to online."
139271124,3765,junrao,2017-09-16T00:05:51Z,Could we consolidate all the logFailedStateChange() calls here?
139271360,3765,junrao,2017-09-16T00:08:47Z,Could this be private?
139271478,3765,junrao,2017-09-16T00:10:43Z,Could we add a comment on the return value?
139271708,3765,junrao,2017-09-16T00:14:25Z,It seems that we should return failedUpdates to the caller so that we can log the error.
139272320,3765,junrao,2017-09-16T00:25:57Z,Could we consolidate all the logFailedStateChange() calls here?
139272482,3765,junrao,2017-09-16T00:29:45Z,"Hmm, not sure that I really understand this comment. Also, could we document the return value?"
139273133,3765,junrao,2017-09-16T00:43:56Z,"I think this part of the logic could be simplified a bit from the original implementation. If isr only contains replicaId, it seems that we can just always leave the last replica in isr independent of the UncleanLeaderElection config. This is because the leader will be set to NoLeader. So, no new data could be written to this partition and replicaId will remain in sync."
139273316,3765,junrao,2017-09-16T00:48:15Z,leaderAndIsrs.keys.map(_.topic).toSeq => leaderAndIsrs.keys.map(_.topic).toSet ?
139298824,3765,onurkaraman,2017-09-17T00:54:22Z,"It doesn't seem like we should relax the EasyMock strictness in general. I only had to do it because I had issues specifying expectations on some scala methods with defaults (specifically, the isNew param for `mockControllerBrokerRequestBatch.addLeaderAndIsrRequestForBrokers`). "
139298830,3765,onurkaraman,2017-09-17T00:55:08Z,Your above earlier comment suggests we can get rid of this method entirely. So problem solved!
139298835,3765,onurkaraman,2017-09-17T00:55:31Z,Done.
139298837,3765,onurkaraman,2017-09-17T00:55:38Z,Agreed.
139298838,3765,onurkaraman,2017-09-17T00:55:46Z,Done.
139298862,3765,onurkaraman,2017-09-17T00:56:46Z,Done.
139298884,3765,onurkaraman,2017-09-17T00:58:58Z,"This ends up getting surprisingly ugly due to the Option[Int] return values of `PartitionLeaderElectionAlgorithms` as well as the leader election helper methods, specifically `leaderForOffline` which also does its own logging."
139298901,3765,onurkaraman,2017-09-17T01:00:52Z,I agree that it should be logged. It's just a bit tricky to cleanly consolidate all of the logging in PartitionStateMachineV2. I'll give it another try in a follow-up update to this PR.
139299269,3765,onurkaraman,2017-09-17T01:37:07Z,Done.
139316087,3765,junrao,2017-09-17T17:20:41Z,"Structure-wise, it's probably better to do line 234 to 238 in electLeaderForPartitions()?"
139316113,3765,junrao,2017-09-17T17:22:09Z,Could we add a comment on what will fail if this is not added?
139317183,3765,onurkaraman,2017-09-17T18:07:56Z,"We could, but in doing so, we'd have to return more state to `electLeaderForPartitions`.

Currently, `doElectLeaderForPartitions` returns `(Seq[TopicAndPartition], Seq[TopicAndPartition], Map[TopicAndPartition, Exception])` where the successes are just that first `Seq[TopicAndPartition]`.

If we delegate the work to `electLeaderForPartitions`, we'd have to pass back not only the successful partitions, but their LeaderAndIsr as well as the intended recipients of that partition's share of LeaderAndIsrRequest.

The code could end up messier as a result."
139318096,3765,onurkaraman,2017-09-17T18:48:30Z,"Sure.

Alternatively, one option is to just get rid of scala default arguments in ControllerBrokerRequestBatch. If we actually want to keep something resembling default arguments, we can just do it the java way. Split the methods with default arguments into two:
1. one with all of the arguments
2. one without the default argument that fills in the default value for the user.

I'm fine with either commenting why or converting to java-style default arguments."
139323536,3765,junrao,2017-09-17T22:57:40Z,"Ok, we can leave it as it is then."
139323553,3765,junrao,2017-09-17T22:58:00Z,Perhaps we can just do 2?
140394457,3765,junrao,2017-09-22T01:14:39Z,"Could we change p to case (tp, _) to get rid of ._1 for better readability? Ditto in a few other places."
140394568,3765,junrao,2017-09-22T01:15:53Z,This comment is probably not longer valid?
140394943,3765,junrao,2017-09-22T01:19:44Z,Could we explicitly define the return type for all methods in the class?
143092927,3765,junrao,2017-10-06T01:14:59Z,"It's possible that the ControllerEventThread has just taken an event out of the queue before eventManager.clearAndPut(Expire) is called. So, it seems that we need to wait for ControllerEventThread to return back to idle state before creating a new ZK session. Otherwise, ControllerEventThread may use the newly create ZK session to process an event that happens before the session expiration."
143096688,3765,junrao,2017-10-06T01:57:13Z,Perhaps it's better to close kafkaControllerZkUtils before shutting down the controller to avoid it blocked on any ZK operation.
143225895,3765,junrao,2017-10-06T15:47:52Z,"Hmm, not sure that we really need to read the controllerId from ZK again. It seems that we could just always set activeControllerId to -1, call onControllerResignation(), and then call elect."
143226529,3765,junrao,2017-10-06T15:50:36Z,I am wondering if we could just remove line 1242 to 1251 and alway try to create the controller path in ZK.
143230568,3765,junrao,2017-10-06T16:07:42Z,unused import org.I0Itec.zkclient.IZkStateListener and org.apache.zookeeper.Watcher.Event.KeeperState
143231117,3765,junrao,2017-10-06T16:10:22Z,"Over time, other components will be using this class, should we name this sth more general that's not tied to controller?"
143232047,3765,junrao,2017-10-06T16:14:59Z,This line seems unnecessary since it's done in the previous line.
143232631,3765,junrao,2017-10-06T16:17:57Z,Could replicaStateMachine.handleStateChanges() take a set of replicas instead of a sequence?
143234078,3765,junrao,2017-10-06T16:24:49Z,We probably don't intent to check Code.OK again?
143237031,3765,junrao,2017-10-06T16:38:57Z,"It's possible that the controller is still working on an event when expire() is called. In this case, it seems that we want to wait until the controller returns to idle state before we create a new ZK session. Otherwise, the controller may process an old event using the new session."
143255144,3765,onurkaraman,2017-10-06T17:47:49Z,Good catch. This must've been some leftover code while refactoring.
143255167,3765,onurkaraman,2017-10-06T17:47:57Z,Good catch.
143262946,3765,junrao,2017-10-06T18:22:32Z,ControllerState.TopicChange is incorrect.
143263715,3765,junrao,2017-10-06T18:26:03Z,Is V2 needed?
143292558,3765,junrao,2017-10-06T20:56:33Z,Is v2 needed?
143316120,3765,onurkaraman,2017-10-07T00:20:06Z,"From an offline discussion, I think we agreed to leave this logic as is and change it in a later patch."
143316125,3765,onurkaraman,2017-10-07T00:20:11Z,"From an offline discussion, I think we agreed to leave this logic as is and change it in a later patch."
143318695,3765,onurkaraman,2017-10-07T01:17:23Z,Good catch.
143318696,3765,onurkaraman,2017-10-07T01:17:26Z,Good catch.
143318830,3765,onurkaraman,2017-10-07T01:21:51Z,"That's the current behavior and I think it's right. PartitionModifications is the event used for adding partitions to a topic.

 If you look at the other ControllerState states, it's the most accurate option."
143319274,3765,onurkaraman,2017-10-07T01:38:52Z,"I was hoping to do a later ""cleanup"" PR that would contain:
1. renaming KafkaControllerZkUtils
2. removing state change logger entirely
3. doing your above suggestion of removing the ith tuple element notation ex: x._1"
143598970,3765,junrao,2017-10-09T23:14:27Z,"Instead of exposing countDownLatch directly, perhaps it's better to add a waitUntilProcessed() method in ExpireEvent and call countDownLatch.await() there?"
143601978,3765,junrao,2017-10-09T23:38:37Z,I am not sure why replicas needs to be a Seq instead of a Set. It seems that all callers are converting a Set to a Seq.
143604811,3765,junrao,2017-10-10T00:03:03Z,Could we use case inside map to avoid unnamed reference _._2? Ditto in the line below.
143605920,3765,junrao,2017-10-10T00:12:17Z,"This matches the existing code. However, in this case, it seems that we probably want to throw an exception instead of proceeding w/o a new controller epoch."
143607644,3765,junrao,2017-10-10T00:28:05Z,Should we just pass the locally created brokerRequestBatch to both the ReplicaStateMachine and PartitionStateMachine?
143611081,3765,junrao,2017-10-10T01:01:22Z,"This will be called when a topic is deleted. So, we probably want to make this a no-op instead of throwing an exception."
143611266,3765,junrao,2017-10-10T01:03:36Z,"This will be called when the partition reassignment completes. So, we probably want to make this a no-op instead of throwing an exception."
143611398,3765,junrao,2017-10-10T01:04:56Z,"This will be called when the preferred leader election completes. So, we probably want to make this a no-op instead of throwing an exception."
143612243,3765,junrao,2017-10-10T01:14:49Z,Could we use case inside filter to avoid unnamed reference p._2?
143612493,3765,junrao,2017-10-10T01:17:19Z,Could we use case inside partition to avoid unnamed reference _._2? There are a few other places like that.
143612617,3765,junrao,2017-10-10T01:18:49Z,It seems that failedUpdates.toMap can just be failedUpdates.
143612849,3765,junrao,2017-10-10T01:21:12Z,Could we use case to avoid unnamed reference _._2? There are a few other places like that in this file.
143613461,3765,junrao,2017-10-10T01:28:23Z,"Could we explicitly define the return type in this and the following private method? Otherwise, it's a bit hard to follow the logic."
143613534,3765,junrao,2017-10-10T01:29:06Z,"The class is a bit harder to read now with the new ZK wrapper. To make that a bit easier, could we add a comment to describe the return type and this method does? It may be worth doing that on a few other methods in this class as well."
143614244,3765,junrao,2017-10-10T01:37:17Z,"Add a new line above.

Could we explicitly define the return type of this method?"
143614282,3765,junrao,2017-10-10T01:37:44Z,Could we explicitly define the return type explicitly of this method and describe a bit the return type and the method? Ditto for the method below.
143614860,3765,junrao,2017-10-10T01:44:35Z,Could we add a comment to describe the return value?
143614998,3765,junrao,2017-10-10T01:46:20Z,Could we add a comment to describe the return value?
143615041,3765,junrao,2017-10-10T01:46:52Z,Could we explicitly specify the return type of this method?
143615065,3765,junrao,2017-10-10T01:47:09Z,Could we explicitly specify the return type of this method?
143615317,3765,junrao,2017-10-10T01:50:48Z,"We probably want to wait for  ZookeeperClient to be connected within the connection timeout. Otherwise, fail the restart of the broker."
143621204,3765,onurkaraman,2017-10-10T03:04:22Z,Sounds good. I'll make the change.
143864242,3765,onurkaraman,2017-10-10T21:59:18Z,This is definitely something I've had in mind for a while now and I've mentioned it in the redesign doc. We agreed offline to do this change in a follow-up patch to minimize regressions.
143867505,3765,onurkaraman,2017-10-10T22:16:03Z,"That's not quite right. TopicDeletionManager's `completeDeleteTopic` first unregisters the PartitionModificationsHandler before deleting the topic znode.

In addition to looking at the code, I did a topic deletion and didn't find any errors in any of the logs.

I think there are 3 options here:
1. change unimplemented methods to be no-ops on a case-by-case basis.
2. change all of the controller's unimplemented handle methods to be no-ops.
3. change the traits themselves to make these methods no-ops by default. You'd only override if you need to.

I am leaning towards option 3 since it'll make the code more concise but am okay with any option."
143869274,3765,onurkaraman,2017-10-10T22:25:19Z,That's a good point. See my above comment describing several ways we can address this.
143870540,3765,onurkaraman,2017-10-10T22:32:38Z,That's a good point. See my above comment describing several ways we can address this.
143871821,3765,junrao,2017-10-10T22:39:53Z,"Option 3 sounds reasonable. The thing with unregistering a watcher is that it only gets reflected during the next read. So, it may not happen immediately."
143880980,3765,onurkaraman,2017-10-10T23:42:31Z,"Watches are tricky with zookeeper. The key point is that ZookeeperClient handler unregistration is not the same thing as watcher removal.

Prior to zookeeper 3.5, there actually [isn't a way to explicitly remove a session's watcher from the ensemble](https://issues.apache.org/jira/browse/ZOOKEEPER-442). Watchers can only get removed after they get triggered.

However, from the ZookeeperClient's perspective, handler unregistration is local and immediate. It's possible for the zookeeper ensemble to send you a notification for a watcher after you've unregistered that watcher's corresponding handler, but it won't have any effect. ZookeeperClient maintains local mappings from paths to registered handlers and updates these mappings immediately upon handler registration and unregistration. When ZookeeperClient receives a watcher notification, its ZookeeperClientWatcher first looks up a handler in its local mappings and only if one exists does it actually trigger the corresponding handle method."
143885696,3765,junrao,2017-10-11T00:17:20Z,"Ok, then, it's probably covered in this case."
143898041,3765,onurkaraman,2017-10-11T02:18:37Z,"This is already exactly what happens. The ZookeeperClient constructor calls `waitUntilConnected(connectionTimeoutMs, TimeUnit.MILLISECONDS)`."
143908207,3765,onurkaraman,2017-10-11T04:07:48Z,Made the change in the latest update.
144068415,3765,junrao,2017-10-11T16:43:53Z,It seems this should really be called onReconnectionTimeout?
144133096,3765,junrao,2017-10-11T20:54:11Z,I am not sure if we need to explicitly have this callback. It seems that this can just be fold into the logic of waiting for the connection to be ready during initial connect and reconnect?
144136815,3765,junrao,2017-10-11T21:08:57Z,We probably want to mention that the new leaders will be written to ZK.
144136971,3765,junrao,2017-10-11T21:09:28Z,identation
144150290,3765,junrao,2017-10-11T22:13:41Z,This is redundant given what's in line 115.
144151962,3765,junrao,2017-10-11T22:23:45Z,We probably want to document that new isr will be written to ZK.
144160128,3765,onurkaraman,2017-10-11T23:12:25Z,That indentation's actually what intellij suggested to me.
144165799,3765,junrao,2017-10-11T23:54:29Z,This seems unnecessary since we log in the first statement in replicaStateMachine.startup() already.
144165817,3765,junrao,2017-10-11T23:54:44Z,This seems unnecessary since we log in the first statement in partitionStateMachine.startup() already.
144177295,3765,onurkaraman,2017-10-12T01:41:48Z,Done.
144177305,3765,onurkaraman,2017-10-12T01:41:53Z,Done.
144482045,3765,onurkaraman,2017-10-13T07:27:27Z,"`StateChangeHandler.onAuthFailure` only gets used in our ZookeeperClient's custom ZookeeperClientWatcher:
```
if (event.getState == KeeperState.AuthFailed) {
  info(""Auth failed."")
  stateChangeHandler.onAuthFailure()
}
```
`StateChangeHandler.onAuthFailure` would only get called when the raw zookeeper client transitioned from the CONNECTING state to the AUTH_FAILED state as shown in the state transition diagram below:
https://zookeeper.apache.org/doc/trunk/zookeeperProgrammers.html#ch_zkSessions

There are three scenarios that could have caused you to be in the CONNECTING state in the first place:
1. initial ZookeeperClient instantiation
2. transient disconnect from the zookeeper ensemble
3. ZookeeperClientWatcher initializing a new session after session expiration

If you get rid of `StateChangeHandler.onAuthFailure`, then only 1 and 3 will react to the auth failure on their own:
* For scenario 1, assuming you want to keep the thrown ZookeeperClientAuthFailedException, then 1 will throw that exception in waitUntilConnected.
* For scenario 3, eventually the connection timeout will be hit and the `StateChangeHandler.onConnectionTimeout` will get called.
* However for scenario 2, without `StateChangeHandler.onAuthFailure`, a reaction to the auth failure for 2 can only occur if the user calls waitUntilConnected or if they observe the return code from any requests that were in-flight or sent after the auth failure. So you now risk a scenario where the application is just sitting around indefinitely with a client in the AUTH_FAILED state."
144585764,3765,ijuma,2017-10-13T15:29:17Z,"Out of curiosity, what is the reasoning for the `Listener` -> `Handler` rename?"
144588126,3765,ijuma,2017-10-13T15:38:31Z,"Btw, there are some really long lines in this PR. Our convention is that lines should not be longer than the GitHub review window."
144624196,3765,junrao,2017-10-13T18:18:07Z,"Hmm, in case 2, wouldn't the ZK session expire eventually?"
144668085,3765,junrao,2017-10-13T22:18:38Z,"We probably want to delete the log dir event first and then register the handler. Otherwise, we may be processing those events triggered by deletion unnecessarily. Ditto for deleteIsrChange."
144669429,3765,junrao,2017-10-13T22:29:17Z,It seems that it's better to batch the call to handleStateChanges across all replicas like before?
144669526,3765,junrao,2017-10-13T22:30:08Z,"We only need to do this on newTopics, right?"
144676732,3765,onurkaraman,2017-10-13T23:42:37Z,Good catch.
144677353,3765,onurkaraman,2017-10-13T23:50:05Z,Done.
144680808,3765,onurkaraman,2017-10-14T00:45:15Z,"The diagram seems to indicate that you can either hit auth failure or session expiration, but not both."
144681072,3765,junrao,2017-10-14T00:51:03Z,"Hmm, that could be true. Perhaps we could keep it and just log an error for now."
144681238,3765,onurkaraman,2017-10-14T00:54:51Z,"1. It's more concise.
2. It's also easier to read and verbalize, for me at least.
3. It let me keep both version of the classes side-by-side as a reference.
4. They pretty much mean the same thing in programming."
145007795,3765,junrao,2017-10-17T01:13:16Z,Instead of duplicating the comment. We could just refer it to the one in handle(requests: Seq[AsyncRequest])?
145007985,3765,junrao,2017-10-17T01:15:05Z,"registerZNodeChildChangeHandler(zNodeChildChangeHandler: ZNodeChildChangeHandler) calls this method. So, it's better to put the detailed comments here and let the former refer it here."
145008953,3765,junrao,2017-10-17T01:26:05Z,"It seems that in all state transitions, it's useful to know the assigned replica list, the leader and the isr. Perhaps, we can just do a generic logging at the end of the method?"
145010777,3765,junrao,2017-10-17T01:44:39Z,I had a comment on this before. Should onConnectionTimeout() be onReconnectionTimeout?
145206369,3765,junrao,2017-10-17T17:57:40Z,Could we explicitly define the return type of this method?
145206447,3765,junrao,2017-10-17T17:57:56Z,Could we explicitly define the return type of this method?
145212540,3765,onurkaraman,2017-10-17T18:19:03Z,"Now that we've decoupled handler registration from watcher registration, there's really no benefit to having `registerZNodeChangeHandlers` and `registerZNodeChildChangeHandlers` since these are now purely local operations and equivalent to registering one-at-a-time.

I'm going to remove these methods."
145215526,3765,onurkaraman,2017-10-17T18:29:16Z,"Hmm not sure if this would actually work. Some of these concepts don't even exist in certain states. 
- NewPartition has no leader or isr.
- NonExistentPartition has no replicas, leader, or isr."
145215623,3765,onurkaraman,2017-10-17T18:29:38Z,Sure.
145215655,3765,onurkaraman,2017-10-17T18:29:47Z,Makes sense.
145227489,3765,junrao,2017-10-17T19:12:45Z,"Ok. Maybe in the case where we transition to OnlinePartition, we can just log the whole leaderAndIsr instead of just the leader."
145250912,3765,onurkaraman,2017-10-17T20:45:25Z,Done
145250947,3765,onurkaraman,2017-10-17T20:45:34Z,Done.
145250967,3765,onurkaraman,2017-10-17T20:45:40Z,Done.
145483014,3765,tedyu,2017-10-18T17:20:24Z,"doWork() doesn't use putLock.
Is it possible that an event retrieved by doWork() is supposed to be cleared by this call ?
"
145488953,3765,tedyu,2017-10-18T17:42:14Z,updatesToRetry is not used.
145489072,3765,tedyu,2017-10-18T17:42:39Z,partitionsLeadByBroker -> partitionsLedByBroker
145664130,3765,ijuma,2017-10-19T10:46:03Z,"I think it's a good point that we should document the expected behaviour even if there's no bug. @onurkaraman, can we please do that in a follow-up?"
145664835,3765,ijuma,2017-10-19T10:49:39Z,Fixed in https://github.com/apache/kafka/pull/4088
145664852,3765,ijuma,2017-10-19T10:49:45Z,Fixed in https://github.com/apache/kafka/pull/4088
145820562,3765,junrao,2017-10-19T20:52:58Z,"The purpose of putLock is to make sure no other callers can put anything to the queue in the middle of a clearAndPut() call. It's ok for a reader to have taken an event out of the queue just before the queue is cleared since in KafkaController.expire(), we wait until the last event in the queue is processed before creating a new ZK session. We can probably document this to make it clear."
1104737066,13240,Hangleton,2023-02-13T16:38:01Z,"The sequence of validation chosen here reflects what is used on the fetch request path:

- If topic IDs are used and the given topic ID cannot be resolved (and no fallback name is provided), send back `UNKNOWN_TOPIC_ID`;
- If the topic name is valid but that name is not authorized, send `TOPIC_AUTHORIZATION_FAILED`;
- If the topic name is authorized but not present in the metadata cache (in which case, that topic will not have been resolved via its ID because in this case, we expect it to be in the cache), send `UNKNOWN_TOPIC_OR_PARTITION`."
1104738082,13240,Hangleton,2023-02-13T16:38:36Z,Note: topic ID must not be null for a request/response version >= 9 to be serialized. `ZERO_UUID` means no topic ID specified.
1104738961,13240,Hangleton,2023-02-13T16:39:12Z,Note: topic ID must not be null for a request/response version >= 9 to be serialized. `ZERO_UUID` means no topic ID specified.
1104740902,13240,Hangleton,2023-02-13T16:40:29Z,"Note 1: if the `topicName` is not null, we should also check if it resolves to the same UUID as we have cached locally."
1104760648,13240,Hangleton,2023-02-13T16:53:38Z,Note 2: We could fail partially - just for the given topic entry - rather than the entire response.
1105846605,13240,dajac,2023-02-14T13:48:17Z,"There is very likely a bug here. In this case, `topic.name` is `null` and the response builder uses a HashMap keyed by topic name. Therefore, all the topics with an unknown topic id will end up together."
1105854122,13240,dajac,2023-02-14T13:54:34Z,"We are calling `resolveTopicName` three times. I think that it would be better to iterate once over the topics to resolve the topic ids and build the list of topic names (if one was found) while doing this. Then, we can check the authorization and do the rest."
1105855843,13240,dajac,2023-02-14T13:55:54Z,I suppose that this check is not necessary if we are using topic ids. We already know that the name resolved based on the topic id is valid.
1105985661,13240,Hangleton,2023-02-14T15:31:23Z,"Apologies, you are right. This hints that perhaps we should reconstruct the list of `OffsetCommitRequestTopic` and use it internally to avoid any such mistake?"
1105987099,13240,Hangleton,2023-02-14T15:32:31Z,"You are right, yes. Let's make this explicit in the code."
1106229439,13240,Hangleton,2023-02-14T18:36:47Z,"I replaced this approach with a single iteration over the list of topic data, resolving and populating the topic name in place (line 455). I am concerned though because this involved mutating the request's body. But I am also concerned about the cost of creating a new ArrayBuffer, Sequence or another data structure to pre-filter. Without falling into premature optimization, what do you think about in-place mutation? 

I think the problem here is that we have the instantiation of the `OffsetCommitRequest` decoupled from the resolution of topic IDs. It makes sense since the former corresponds to the request deserialization while the latter corresponds to added semantics unconveyed by the request itself. In responsibility chains on server request handlers, one pattern sometimes adopted is to decorate a request with extraneous information which fall beyond the scope of ser/de. I wonder if topic id resolution could happen before passing it to the business request handler."
1113132735,13240,dajac,2023-02-21T14:21:42Z,nit: We probably don't need to duplicate `data` here. I understand why you are doing it but in practice we assume that `data` is owned by the builder once it is given to it.
1113142734,13240,dajac,2023-02-21T14:29:30Z,nit: InvalidRequestException would be more appropriate.
1113146018,13240,dajac,2023-02-21T14:32:07Z,I just realized that this is only used in tests. I wonder if we should just get rid of it and use the auto-generated classes in tests as well.
1113149061,13240,dajac,2023-02-21T14:34:29Z,"nit: You could replace this by the following:
```
    @ParameterizedTest
    @ApiKeyVersionsSource(apiKey = ApiKeys.OFFSET_COMMIT)
```"
1113151869,13240,dajac,2023-02-21T14:36:46Z,nit: ditto.
1113153482,13240,dajac,2023-02-21T14:37:59Z,nit: We could remove this empty line.
1113156030,13240,dajac,2023-02-21T14:39:58Z,"I think that there is a bug here for the case where multiple topic ids are unknown in a single request. For those, the topic name will be null so they will be aggregated in the same OffsetCommitResponseTopic and that one will have the topic id of the first unknown topic id seen."
1113156632,13240,dajac,2023-02-21T14:40:25Z,nit: I think that we could remove this comment. It does not bring much.
1113157135,13240,dajac,2023-02-21T14:40:49Z,The KIP also specifies new errors for this version. Could we mention them here?
1113160687,13240,dajac,2023-02-21T14:43:33Z,"At L1361 in this file, we construct `TopicPartition` based on the response data but we don't resolve the topic id. I think that we should add the resolution there as well, no? We probably need to extend tests to better cover this as well.

Regarding `UNKNOWN_TOPIC_ID`, would it make sense to place it after `UNKNOWN_TOPIC_OR_PARTITION` as they are quite similar?"
1113164928,13240,dajac,2023-02-21T14:46:18Z,"I would prefer to inline `resolveTopicName` and avoid allocating an `Option` which does not bring much here.

In the mean time, I would directly construct the list of topic names for the authorizer at L461. This way, we could save re-iterating over the topics and the `filter`. What do you think?

Moreover, the KIP states that an `INVALID_REQUEST` should be return if both a topic id and a topic name are provided. We could also handle this here."
1113175049,13240,dajac,2023-02-21T14:52:56Z,"This issue is still present. Yeah, we definitely need to update the response builder to support this. One way would be to change the semantic of `addPartitions` to directly add to the response when it is called and to only put the topic in the HashMap when `addPartition` is used."
1113176828,13240,dajac,2023-02-21T14:54:08Z,It would be great if we could extend the tests here. I think that we need to use multiple unresolvable topic ids in the same request and also check the different versions. I am not sure if we could extend this one or if we should add other ones.
1113178848,13240,dajac,2023-02-21T14:55:25Z,"What's the reason for this change? If we refactor this, it may be better to directly go with the auto-generated data structures. "
1113183283,13240,dajac,2023-02-21T14:58:39Z,We also need tests to check if the response is handled correctly. 
1113220631,13240,dajac,2023-02-21T15:26:05Z,nit: Could we add `.` at the end?
1113220793,13240,dajac,2023-02-21T15:26:12Z,nit: Could we add `.` at the end?
1116196039,13240,Hangleton,2023-02-23T20:10:09Z,"Sure, I used the auto-generated class in the unit test for the `OffsetCommitRequest`. I moved this method to the unit test class as it is used from other unit tests in for the consumer coordinator, from where using the full-fledged request object would be less convenient."
1116198012,13240,Hangleton,2023-02-23T20:12:23Z,"You are right, thanks for finding this bug (again!). I followed the approach you suggest here in the builder of the `OffsetCommitResponse`, please let me know if the semantics make sense."
1116200049,13240,Hangleton,2023-02-23T20:14:13Z,"That is right, thanks for pointing out. The resolution of topic name has been added to the response handler. If the topic is not defined, or the response topic is invalid because it contains neither an id or name, or contains both, that topic is ignored. The offset commit invocation is however not failed."
1116202452,13240,Hangleton,2023-02-23T20:16:23Z,"Thanks, I built the list of resolved topics and pass it to the authorizer, inlining name resolution. If a topic has both a name and id defined, the broker fails fast the request and returns an `INVALID_REQUEST`. Is this what you had in mind? Should we send more information to the client in that case?"
1116203414,13240,Hangleton,2023-02-23T20:17:14Z,"Sure, I added another unresolvable topic to the request/response. I will add more cases covering more of the possible code paths."
1116204122,13240,Hangleton,2023-02-23T20:17:54Z,"Sure, I reverted this refactoring and use the response class instead."
1116205901,13240,Hangleton,2023-02-23T20:19:26Z,"That is right, I added tests which invoke the sync and async offset commit method."
1119248386,13240,dajac,2023-02-27T19:58:06Z,"@Hangleton I discussed offline with a few committers and the consensus is that having both the topic name and the topic id in the same version is not the right way. They share the same concerns that we discussed last week. Could you update the PR to only have TopicId from version 9? We can also remove the nullableVersions for the Name and set the versions to 0-8. I suppose that both fields could be ignorable.

Regarding the admin client, which does not support topic ids, it cannot use version 9 at the moment. We need to handle this in the Builder (we can set the maximum allowed version).

Sorry for this late change."
1119798298,13240,Hangleton,2023-02-28T09:40:06Z,"Hi David, thanks for the follow-up and clarifying. This is all good, I am working on adapting the PR. Thanks!"
1121486904,13240,dajac,2023-03-01T10:29:55Z,"Now that we can rely on the version, we should use it here and simplify all this logic."
1121487731,13240,dajac,2023-03-01T10:30:29Z,It would be better to rely on the version of the request instead of the topic name here.
1121488495,13240,dajac,2023-03-01T10:31:00Z,I would move this up and do it in the first iteration.
1121489116,13240,dajac,2023-03-01T10:31:23Z,You could use `resolvedTopics` instead of `offsetCommitRequest.data.topics` here.
1121494379,13240,dajac,2023-03-01T10:35:07Z,I think that we could remove those checks now.
1121496085,13240,dajac,2023-03-01T10:36:34Z,I think that we could just set both the name and the id all the time as the fields are ignorable. The serialization framework will do the right thing based on the version. We could also remove `version` from the arguments.
1121499577,13240,dajac,2023-03-01T10:39:12Z,We could get this in the base class and always set both of them. The serialization framework knows what to do.
1121501980,13240,dajac,2023-03-01T10:41:03Z,"Is this change related to the PR? If not, I would rather do it in a separate PR."
1121502171,13240,dajac,2023-03-01T10:41:14Z,Same question here.
1121506569,13240,dajac,2023-03-01T10:44:43Z,"I am not a fan of all those attributes in test. One or two are fine if they are really re-used on all the tests. Otherwise, it may be better to check define what you need in tests. I would also use `TopicIdPartition` when relevant so you can basically group the name, id, and partition together."
1121510354,13240,dajac,2023-03-01T10:47:36Z,Is this really needed?
1121710275,13240,Hangleton,2023-03-01T13:14:50Z,Note: is this OK to break message round trip between < 9 and >= 9?
1121724366,13240,Hangleton,2023-03-01T13:24:49Z,"Adding the version to the response seems to be an anti-pattern as I haven't seen any other similar use in other responses. Semantically it should be OK because the response instance is supposed to be built against a given version. If another approach is advisable, I will remove it."
1121724819,13240,Hangleton,2023-03-01T13:25:09Z,Will add Javadoc.
1122096890,13240,Hangleton,2023-03-01T17:33:31Z,There is still a problem here if `topicName` and `topicId` are both undefined in which case we should do what was done before and add to the response without caching.
1122097709,13240,Hangleton,2023-03-01T17:34:20Z,Move the `TopicResolver` in the `MetadataCache` or create it without copying the map of topic ids as this is costly.
1122175488,13240,Hangleton,2023-03-01T18:52:25Z,"This case shouldn't be reachable because once we have proceeded with constructing the response via `addPartition` all topic ids are supposed to have been resolved successfully. Here, we choose to add the topic to the response with the error code `UNKNOWN_TOPIC_ID` if no error is already set. Any existing error is not overwritten."
1122188628,13240,Hangleton,2023-03-01T19:06:41Z,"At this point, topic ids should be always resolvable. However if some aren't, we should fallback to adding the topic ""as is"" to the response to avoid caching `ZERO_UUID` with risk of overwrites."
1122265994,13240,Hangleton,2023-03-01T20:27:38Z,Adding more tests to this class.
1122271424,13240,Hangleton,2023-03-01T20:34:15Z,Note - this duplicated invocation of the `Builder` constructor is to allow the resolution of the parameter type as either `Uuid` or `String`. Not graceful but...
1122830477,13240,Hangleton,2023-03-02T09:41:36Z,"Thinking about it, it seems unnecessary to adopt a different classification for v >= 9 since topic names should always be resolved when calling `addPartition`. Will remove all this logic and simplify."
1124559198,13240,Hangleton,2023-03-03T14:48:12Z,Not strictly needed. We can remove the condition and the logger as well.
1125501983,13240,dajac,2023-03-04T16:59:02Z,Should we add tests to cover this new logic?
1125687390,13240,Hangleton,2023-03-05T15:35:04Z,Sure! Added the tests. Thanks.
1126511918,13240,Hangleton,2023-03-06T14:38:48Z,"Hmm, we probably don't want to include a topic without id in the response version 9 here."
1128110858,13240,dajac,2023-03-07T16:01:47Z,nit: We usually don't leave such comment in our code base.
1128111731,13240,dajac,2023-03-07T16:02:18Z,nit: Should we use a boolean?
1128113386,13240,dajac,2023-03-07T16:03:22Z,nit: I usually prefer to use `ZERO_UUID.equals(...` as it is safe for null values.
1128117144,13240,dajac,2023-03-07T16:05:47Z,"nit: We usually don't break long lines like this. I personally prefer the following:

```
private OffsetCommitResponseHandler(
     Map<TopicPartition, OffsetAndMetadata> offsets,
     Generation generation,
     TopicResolver topicResolver
) {
```

You can find other ways in the code base."
1128122177,13240,dajac,2023-03-07T16:08:47Z,"Is this really true? As we keep the `TopicResolver` used to construct the request, all topics should be there. This case could happen if the server returns an unexpected topic id that was not in the request and that is not in the `TopicResolver`. Do I get this right?"
1128123974,13240,dajac,2023-03-07T16:09:50Z,"For my understanding, are we going to propagate this error back to the end user?"
1128126364,13240,dajac,2023-03-07T16:11:15Z,We don't really use those in our code base at the moment. We usually just mention those characteristics in the java doc.
1128126821,13240,dajac,2023-03-07T16:11:31Z,Should this be an InvalidStateException?
1128128673,13240,dajac,2023-03-07T16:12:33Z,I am not really happy with this name but I could not find a better one yet. My concern is that this class is really about resolving topic ids/names and not really topics per say. Have you considered any alternatives?
1128130754,13240,dajac,2023-03-07T16:13:43Z,Is this constructor still used?
1128132186,13240,dajac,2023-03-07T16:14:32Z,"nit: When we break the line like this, we usually align the arguments on the first one. Otherwise, you can use the style that I mentioned earlier."
1128133377,13240,dajac,2023-03-07T16:15:16Z,nit: Should we also add the other ones?
1128136075,13240,dajac,2023-03-07T16:16:52Z,Did you check how we did this for the FetchRequest?
1128141284,13240,dajac,2023-03-07T16:19:55Z,I am not sure about passing the `TopicResolver` here. My understanding is that we are doing this because topic ids are lost when we call the group coordinator. Wouldn't it better to update the group coordinator to preserve those topic ids? We may be able to handle this in the GroupCoordinatorAdaptor or we could switch to using TopicIdPartitions. We could also consider doing this in a separate PR as this one is already quite large.
1128148229,13240,dajac,2023-03-07T16:24:11Z,This does not look good. It would be better to place those helpers in `OffsetCommitRequestTest` for instance or to keep them where they are used.
1128149528,13240,dajac,2023-03-07T16:24:56Z,nit: You can omit the `()` after `topics` as we usually don't put them for getters in Scala. There are a few other cases in the PR.
1128151493,13240,dajac,2023-03-07T16:26:10Z,I think that there is a race condition here. You have no guarantee that both maps are consistent with each others.
1128154143,13240,dajac,2023-03-07T16:27:47Z,Should we just throw an illegale state exception if we end up having a topic without id? Ignoring it seems to be risky.
1128157560,13240,dajac,2023-03-07T16:29:43Z,I wonder if using Optional is necessary here given that we always use `orNull` and `orDefault`. What do you think?
1129066723,13240,dajac,2023-03-08T07:41:32Z,@Hangleton I had a deeper look into this and it seems that we could get the version with `this.response.requestHeader().apiVersion()`. Could you check if this would work?
1129167871,13240,Hangleton,2023-03-08T09:26:49Z,"I agree with you and am not satisfied either with `TopicResolver` but could not find a better name. `TopicIdResolver` would be misleading because this class treats topic ids and names symmetrically. One of the closest entity with similar purposes as this is in [`MetadataCache#L93`](https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/server/MetadataCache.scala#L93) where `topicIdInfo` is used to refer to the bidirectional mapping. The suffix `Info` could be used here as well although it is not strictly aligned with other uses of that suffix such as in [`TopicPartitionInfo`](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/TopicPartitionInfo.java).  Interestingly another entity for which may have had to be assigned a generic name is [`TopicCollection`](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/TopicCollection.java).

Using another name to refer to the dual name/id reference such as `TopicRefResolver` introduces yet another noun (_reference_) not used elsewhere in the codebase and which can be confusing.

So, I am not sure about what could be a better name but maybe `TopicInfoResolver` or `TopicIdInfoResolver` or `TopicIdInfo` or `TopicIdResolver` may sound better albeit still ambiguous and partially incorrect?"
1129194029,13240,dajac,2023-03-08T09:51:18Z,Another way would be to implement a minimal and generic BiMap that we could use here. Would it be an option?
1129266598,13240,Hangleton,2023-03-08T11:03:53Z,"With the synchronous API in the consumer, the error is not surfaced (only `true`/`false`). However, I added the missing tests to exercise the asynchronous API for this use case, and it did expose the `UnknownTopicIdException` to the user. Since it violates the API contract which exclusively relies on topic names, I raised the error `UNKNOWN_TOPIC_OR_PARTITION` when an `UNKNOWN_TOPIC_ID` is returned in the offset commit response. Do you think this is sensible?

I added the corresponding unit tests for the consumer coordinator."
1129267206,13240,Hangleton,2023-03-08T11:04:35Z,"Yes, that is right. Apologies, this is a fundamental misunderstanding/overlook."
1129272424,13240,Hangleton,2023-03-08T11:10:09Z,"I would tend to have a preference for a business type which conveys semantics versus a generic data structure, but that is not very important here especially since the entity exposing the bidirectional mapping is relatively short-lived when used in the code. One advantage of a generic DS is that it can be reused for other purposes. Another thing is that there is no functionality provided outside that of a bimap and since no extension is foreseen, there is no need to expose a specialized type. Very happy to expose it as a bimap. I could not find an existing implementation in the codebase or its dependencies, although there is a bidirectional multimap defined within restricted scope [here](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/admin/internals/AdminApiDriver.java#L459-L489)."
1129283573,13240,Hangleton,2023-03-08T11:22:44Z,"It was only used in tests, so best to have it removed."
1129311477,13240,Hangleton,2023-03-08T11:51:54Z,"Yes, this works. Thanks for the call-out."
1129371713,13240,Hangleton,2023-03-08T12:41:39Z,Oops...
1129407685,13240,Hangleton,2023-03-08T13:11:41Z,"Yes, there is a code smell here."
1129452978,13240,Hangleton,2023-03-08T13:46:03Z,"I see what you mean. I moved this logic in the callback of the future which merges the results from the coordinator with those created by the request handling method. I thought about extending the support id in internal layers (group coordinator) in a PR of its own. So, eventually, the coordinator will return results populated with topic ids when applicable.

Added [this JIRA](https://issues.apache.org/jira/browse/KAFKA-14793) to track this work, if that is ok?"
1129482786,13240,Hangleton,2023-03-08T14:05:41Z,"Modified the PR so that the server now sends an `UNKNOWN_SERVER_ERROR` when this happens, in the code moved to the future handler in `KafkaApis`. Would this behaviour be acceptable?"
1129710408,13240,Hangleton,2023-03-08T16:22:25Z,Maybe `TopicIdAndNameBiMap`?
1130670740,13240,dajac,2023-03-09T08:54:40Z,"I actually wonder if we should do it the other way around. We could do KAFKA-14793 first, merge it, and update this one accordingly. Without KAFKA-14793, the contract of the not really respected and it feels a bit weird to work around it here instead of fixing the real issue. Is KAFKA-14793 complicated? What do you think?"
1130671265,13240,dajac,2023-03-09T08:55:06Z,Both names are fine for me. I leave it up to you.
1130866578,13240,Hangleton,2023-03-09T11:36:23Z,"Hi David, thanks for the insight. I think you are right that implementing support of topic ids in the functional layer before exposing it in the API makes sense as it provides the guarantee that offsets and metadata belong to the partitions of the right topic in case of homonyms.

Now, one question is how deep we go in the integration of ids in this layer. Would you consider changing the data model authored by the group coordinator down to the `OffsetCommitValue ` as prescribed by KIP 848?"
1130942769,13240,dajac,2023-03-09T12:18:21Z,The OffsetCommitValue part is not possible at the moment because we dont have a way to downgrade. My colleague @jeffkbkim works on a proposal for this. We could start by either migrating from using TopicPartition to using TopicIdPartition or handling this in the GroupCoordinatorAdaptor layer. The former is likely simpler.
1131047710,13240,Hangleton,2023-03-09T13:41:44Z,"Thanks for the answer. If I understand correctly, we would then have a resolution of topic ids from topic-name-based persisted data, so this may not prevent offsets from a topic to be provided as those of another topic with the same name (defined at different point in time in the server)?

The resolution can be done in the group coordinator layer, assuming it has access to the topic id resolved upstream by the request handler. Because we want to preserve the same mapping used when request processing started, we need to ensure the right ids are used within the adaptor's `GroupCoordinator#commitOffsets` method(). Since the mapping returned from the metadata cache depends on the snapshot used at the time the mapping is requested, if the adaptor retrieves it from the metadata cache internally, at a different time from the request handler, there is no guarantee the metadata is the same hence that the topic IDs registered with the broker are the same.

This means that the topic ids need to be propagated from the request handler (`KafkaApis`) to the coordinator adaptor somehow. Without a change in the method and contract implemented by the coordinator, these ids could be transferred via the `OffsetCommitRequestData` DTO directly, which means a change in the API schema would be required prior to the change. Alternatively, we may want to change the interface of the coordinator and change the signature of the offset commit method to allow for the propagation of topic ids.

I may be missing the entire thing though?"
1131114261,13240,dajac,2023-03-09T14:33:59Z,"How about doing the following?

We change the signature of `GroupCoordinator.handleCommitOffsets` to the following:

```
  def handleCommitOffsets(groupId: String,
                          memberId: String,
                          groupInstanceId: Option[String],
                          generationId: Int,
                          offsetMetadata: immutable.Map[TopicIdPartition, OffsetAndMetadata],
                          responseCallback: immutable.Map[TopicIdPartition, Errors] => Unit,
                          requestLocal: RequestLocal = RequestLocal.NoCaching): Unit = {
```

Note the change from `TopicPartition` to `TopicIdPartition` for `offsetMetadata` and `responseCallback`.

Then, we have to adapt the implementation of `handleCommitOffsets` to get the `TopicPartition` from the `TopicIdPartition` where required. We can keep `pendingOffsetCommits` and `offsets` keyed by `TopicPartition` for now in `GroupMetadataManager`.

This allows the preservation of the topic ids provided to the GroupCoordinator but it does not provide any stronger guarantee for the offsets yet (as you pointed out). With this approach, we don't depend on the resolver at all."
1131139080,13240,Hangleton,2023-03-09T14:49:42Z,"Sounds good. Thanks for your guidance. As you mentioned, this PR is already quite large, so if you agree, I will go ahead and implement this change first, in a PR of its own. Thanks!"
1131241761,13240,dajac,2023-03-09T15:56:05Z,Sounds good to me. Thanks!
1168410473,13240,dajac,2023-04-17T09:17:52Z,"It looks like `topicIdAndNames` is only used if version >= 9. Should we move it that else branch? Moreover, it seems that we don't need the BiMap anymore here. Should we just get the mapping that we need and revert the BiMap think in the `MetadataCache`?"
1168412474,13240,dajac,2023-04-17T09:19:41Z,"Just to be sure. The addition of `true` is the only real change here, right? "
1168414850,13240,dajac,2023-04-17T09:21:50Z,I think that `TopicId` is optional so we could just set it here.
1168415581,13240,dajac,2023-04-17T09:22:24Z,"Is using `true` all the time correct here? I suppose that it should be `false` if `version` < 9, no?"
1168416665,13240,dajac,2023-04-17T09:23:22Z,nit: I think that we could set it all the time here as well.
1168423960,13240,dajac,2023-04-17T09:29:54Z,"Should this test be parameterized as well? With this change, it seems that we don't have any tests exercising the validation with topic names now."
1168430856,13240,dajac,2023-04-17T09:35:45Z,nit: `true` should be derived from the `version`.
1168433830,13240,dajac,2023-04-17T09:38:23Z,"Changing the code structure like this is really annoying during reviews. It explodes the diff for no reasons and distracts the reviewing from the more important changes. It would be better to keep those for separate PRs. In this case, we could just add the `true` and the `TopicId` to the previous code."
1168454144,13240,dajac,2023-04-17T09:56:31Z,Would you mind if we keep to keep those code refactoring in the tests for separate PR(s)? This PR is already extremely large and I would like to focus on getting the new code right. All those non-related changes are additional (unnecessary) distractions for now.
1168456953,13240,dajac,2023-04-17T09:59:08Z,I think that you could pass config overrides to `createConsumer` directly.
1168464261,13240,dajac,2023-04-17T10:05:22Z,I think that consumers created with `createConsumer` are closed automatically by the super class.
1168468628,13240,dajac,2023-04-17T10:09:03Z,nit: This could be private.
1168468929,13240,dajac,2023-04-17T10:09:20Z,nit: `topicNames.map(topic => {` -> `topicNames.map { topic => `
1168469544,13240,dajac,2023-04-17T10:09:58Z,Do we really need to use `NameAndId` here? This does not seem necessary.
1168470063,13240,dajac,2023-04-17T10:10:28Z,"nit: You could get `topicIds` with `getTopicIds(""topic1"", ""topic2"", ""topic3"")`."
1168472971,13240,dajac,2023-04-17T10:13:06Z,I would rather prefer to use the request/response data objects here.
1168473224,13240,dajac,2023-04-17T10:13:22Z,Could we parameterize the test instead of doing this?
1168473647,13240,dajac,2023-04-17T10:13:46Z,I wonder if we already have integration tests for the consumer covering this. Do we?
1168474784,13240,dajac,2023-04-17T10:14:55Z,This test does not seem to be at the right place. It seems to me that `OffsetCommitRequestTest` is more focused on testing the OffsetCommitRequest API.
1168475048,13240,dajac,2023-04-17T10:15:09Z,nit: Let's make all the private methods private.
1168475457,13240,dajac,2023-04-17T10:15:32Z,nit: Could we revert this change and just add the boolean?
1168671042,13240,dajac,2023-04-17T13:11:41Z,nit: Indentation seems to be off here.
1168675481,13240,dajac,2023-04-17T13:14:40Z,nit: `topicIdOrZero`?
1168676628,13240,dajac,2023-04-17T13:15:01Z,nit: `topicNameOrNull` and get rid of the `Optional`?
1168677142,13240,dajac,2023-04-17T13:15:24Z,nit: Should we remove `<p></p>`?
1168678603,13240,dajac,2023-04-17T13:16:32Z,nit: Should we replace `ofNullable` by a simple `if/else` statement? Allocating an optional does not seem necessary here.
1168679689,13240,dajac,2023-04-17T13:17:09Z,nit: This one is already in the list (L47).
1168688432,13240,dajac,2023-04-17T13:23:02Z,"nit: Could we try to combine those? `private static TopicIdPartition t1p = new new TopicIdPartition(Uuid.randomUuid(), 0, topic1)`?"
1168689266,13240,dajac,2023-04-17T13:23:38Z,nit: `topicIdAndNameBiMapping`?
1168693500,13240,dajac,2023-04-17T13:26:00Z,nit: This empty line could be removed.
1168694559,13240,dajac,2023-04-17T13:26:28Z,nit: This empty line could be removed.
1168696909,13240,dajac,2023-04-17T13:28:15Z,"If the outcome of the test is different in this case, isn't it a bit weird to combine them in the same unit test?"
1168703280,13240,dajac,2023-04-17T13:33:05Z,This goes a bit too far in my opinion. We usually prefer to have simpler parameterized tests. Could we simplify this somehow and bring stuck back in the main unit test?
1168735466,13240,dajac,2023-04-17T13:52:19Z,"This one made me think that we are probably not doing the right thing in the implementation. In this particular case, if we have only one committed offset and we don't have a response for it because the topic id is wrong, I think that `commitOffsetsSync` should not succeed because we actually don't know if the offset was committed or not. What do you think?

One way around this would be to verify that we have received a response for each topic-partitions."
1168742716,13240,dajac,2023-04-17T13:57:02Z,I am not sure to follow why we need this `Consumer` here. Couldn't we just have a matcher which verifies what we want/need?
1168815694,13240,dajac,2023-04-17T14:36:03Z,"This seems to be a quite complicated way to group `OffsetCommitRequestPartition` or `OffsetCommitResponsePartition` by `TopicIdPartition`, no? I would just write two methods to do just this."
1185002319,13240,clolov,2023-05-04T13:17:25Z,I believe t1p is abstracted because it is being used in 175 other places in this test class for test setup and assertions.
1185137613,13240,clolov,2023-05-04T14:51:40Z,"Sorry, could you elaborate, because I am not certain I follow? `getTopicIds(...)` will return a map but only if the topics requested have been created first. Are you suggesting that since all tests create these three topics we move the creation to the setup method and then we use `getTopicIds` everywhere else?"
1185779424,13240,Hangleton,2023-05-05T07:29:21Z,"That is right. Christo, maybe you can create two separate tests for these cases and factor in common code in a method?"
1185861417,13240,clolov,2023-05-05T09:04:20Z,"Yup, I will get to this today"
1197974839,13240,clolov,2023-05-18T15:36:59Z,"To be honest I would prefer if we leave it like this. If we parameterise it, this means we have to change `sendOffsetCommitRequest`. If we change `sendOffsetCommitRequest` then we need to come up with a different source for `testOffsetCommitWithUnknownTopicId`. Alternatively I can parameterise this test, but I would end up wrapping a single version in a Seq. Is the reason you want it parameterised here so that it breaks it down when running the tests in IntelliJ?"
1197988726,13240,clolov,2023-05-18T15:49:27Z,From my reading of the code this consumer is a captor. We validate some of the things in this method and we validate the overall captured value elsewhere in individual tests. I am not too certain how this can be simplified to just a matcher to be honest.
1203965540,13240,Hangleton,2023-05-24T11:46:23Z,"I think David probably hints at consolidating both in one defining object to ease future updates of topic-partition to topic ids. I updated the test class as per David's comment, I am happy to revert if this brings too many LoC changes."
1203992806,13240,Hangleton,2023-05-24T12:03:45Z,"I think I see what you mean, it is rather heavy-weight and lacks single scope which is preferable for unit tests. Updating accordingly."
1204079800,13240,Hangleton,2023-05-24T12:54:16Z,Did the change to provide higher cohesion to the tests. I split the initial test method in two separated test methods.
1204087015,13240,Hangleton,2023-05-24T12:58:43Z,That is true.
1204151150,13240,Hangleton,2023-05-24T13:34:57Z,"@dajac Sure, I removed the method `createTopics` and use `getTopicIds` instead."
1204162396,13240,Hangleton,2023-05-24T13:40:14Z,"@clolov Yes, I think the idea of parameterizing the test by version of request is it is faster to identify version-specific failures."
1204312762,13240,Hangleton,2023-05-24T14:52:34Z,Added a commit to build the DTOs directly. This removes the contingency on correctness of the test code which built these DTOs.
1204356253,13240,Hangleton,2023-05-24T15:12:20Z,"You are right, this is exercised in `OffsetFetchRequestTest`, `PlaintextConsumerTest`, `GroupCoordinatorIntegrationTest` and `AuthorizerIntegrationTest`. So, I removed this test to avoid duplication."
1204361177,13240,Hangleton,2023-05-24T15:14:16Z,Added parameterization as David suggested.
1204376110,13240,Hangleton,2023-05-24T15:20:43Z,"Agreed, I thought to put it there because the underlying RPC is used, but you are right, it is a different client-level API."
1204403475,13240,Hangleton,2023-05-24T15:33:49Z,I removed the test since this method is already exercised in `org.apache.kafka.clients.admin.KafkaAdminClientTest#testOffsetCommitNumRetries`.
1205089112,13240,Hangleton,2023-05-25T07:11:05Z,"Just to clarify, do you mean the commit offsets method should return false when at least 1 over n > 1 could not be committed due to topic id mismatch, or when n == 1 could not be committed for the same reason?"
1205119564,13240,Hangleton,2023-05-25T07:40:48Z,"@dajac Sure, that makes sense to get rid of the consumer, since it is mixing test design pattern and overlaps the responsibilities of the matcher as you pointed out."
1226607855,13240,dajac,2023-06-12T12:41:20Z,"As a second thought, I wonder if we should complete the future with an exception here. Being defensive would help us to catch bugs early one. What do you think?"
1226610273,13240,dajac,2023-06-12T12:43:20Z,Should we remove this one for now as it is not implemented yet?
1226612028,13240,dajac,2023-06-12T12:44:42Z,Should we remove STALE_MEMBER_EPOCH and UNKNOWN_MEMBER_ID for now?
1226632779,13240,dajac,2023-06-12T13:00:26Z,"My understanding is that we don't retry when `commitOffsetsAsync` is used. Is it correct? If it is, it may be better to split the test in two. It is really misleading otherwise."
1226634411,13240,dajac,2023-06-12T13:01:50Z,nit: It may be better to name this one `prepare....`.
1226634534,13240,dajac,2023-06-12T13:01:56Z,nit: It may be better to name this one `prepare....`.
1226636149,13240,dajac,2023-06-12T13:03:07Z,nit: I would inline this in the respective tests because it seems not related to what this method does.
1226638371,13240,dajac,2023-06-12T13:04:55Z,Is this used anywhere?
1226638660,13240,dajac,2023-06-12T13:05:09Z,Is this used anywhere?
1226647441,13240,dajac,2023-06-12T13:12:15Z,"I think that the method should return false if any mismatched topic id. If I commit foo-topic-id and bar-topic-id, the method should not succeed if we don't get a response for any of them, right?"
1226648857,13240,dajac,2023-06-12T13:13:25Z,This case is not correct as well in my opinion. The caller should get an exception in this case.
1226673880,13240,dajac,2023-06-12T13:32:08Z,nit: TopicNameAndId?
1226675517,13240,dajac,2023-06-12T13:33:23Z,It is a bit weird to have this class defined here but I cannot think of a better place for now. Thoughts?
1226676007,13240,dajac,2023-06-12T13:33:43Z,nit: this seems to be misaligned.
88262234,2140,ijuma,2016-11-16T15:45:33Z,"Is it intentional that this is `LOGBUFFER` instead of `LOG_BUFFER`? Same for the `toString` implementation.
"
88262970,2140,ijuma,2016-11-16T15:48:36Z,"Should this method be renamed as well?
"
88285663,2140,hachikuji,2016-11-16T17:25:22Z,"Ack. There are probably a few of these. I'll do another pass and try to find others.
"
89426622,2140,junrao,2016-11-24T02:13:36Z,Should we assert record.magic() > 0?
89426686,2140,junrao,2016-11-24T02:14:27Z,"To be consistent, perhaps this.size and this.channel should just be size and channel?"
89426710,2140,junrao,2016-11-24T02:14:45Z,"It seems that some of the changes are lost during rebase? For example, there was code in MemoryRecords for setting the buffer limit according to length, and cast position to int instead of creating a Long object."
89426721,2140,junrao,2016-11-24T02:14:55Z,"This seems to be an existing issue. For uncompressed messages, do we double count messagesRead since we already increased the count in line 98?"
89426736,2140,junrao,2016-11-24T02:15:05Z,This and line 144 don't seem to be correct. It seems that we should add the number of entries in retainedEntries?
89426742,2140,junrao,2016-11-24T02:15:10Z,Should we add slice.limit or slice.position?
89426755,2140,junrao,2016-11-24T02:15:19Z,It seems that this is only used in test now?
89680530,2140,junrao,2016-11-27T00:17:36Z,"In line 337, we get the deep iterator by constructing a LogBufferIterator with shallow set to false. To be consistent, it seems that if we want to get a shallow iterator, we should construct a LogBufferIterator with shallow set to true instead of call a separate static method? "
89680531,2140,junrao,2016-11-27T00:17:43Z,Is this comment at the right place? The following code doesn't directly allocate any buffer.
89680539,2140,junrao,2016-11-27T00:17:57Z,"Since logEntries is a deque, perhaps it's clearer if we explicitly use addLast() ?"
89680541,2140,junrao,2016-11-27T00:18:03Z,RecordsIterator.DeepRecordsIterator is no longer valid.
89698576,2140,junrao,2016-11-27T20:26:51Z,"Instead of having shallowEntries() and deepEntries(), would it be better to combine them into a logEntries(boolean isShallow)? This will make it consistent with how we get an iterator for records through AbstractLogBuffer.records(boolean isShallow)."
89698580,2140,junrao,2016-11-27T20:26:55Z,This can be private.
89698584,2140,junrao,2016-11-27T20:27:07Z,"Hmm, this seems like an existing issue. It seems that we should subtract the wrapper header and the record overhead from position() to get the compressed data size?"
89698589,2140,junrao,2016-11-27T20:27:17Z,Could this be private since it seems to be only used within the class?
89698592,2140,junrao,2016-11-27T20:27:35Z,"It seems that LogEntry.writeHeader() is only used inside this class. Perhaps we could just move the code from LogEntry to here as a private method. Once we do that, it seems that we could also make putLong() and putInt() private in this class."
89698597,2140,junrao,2016-11-27T20:27:41Z,It seems that we could just eliminate this line?
89931628,2140,junrao,2016-11-29T02:43:47Z,"With this change, it seems that we can make BufferPool.deallocate(ByteBuffer buffer, int size)  private?"
89931638,2140,junrao,2016-11-29T02:43:52Z,Should we just remove the commented out code?
89931661,2140,junrao,2016-11-29T02:44:09Z,Perhaps we can make the comment clearer by saying that this can happen when there is no full log entry in the log buffer.
89931696,2140,junrao,2016-11-29T02:44:35Z,Would it be necessary to cache the Record instance and reuse? It seems that a few methods like size() and setCreateTime() are calling record().
89931716,2140,junrao,2016-11-29T02:44:46Z,This maybe a bit confusing since our default compression type to the user is None. Could we let the callers use GZIP directly?
89931730,2140,junrao,2016-11-29T02:44:52Z,$targetTimestamp can only be used in scala.
89931765,2140,junrao,2016-11-29T02:45:09Z,Could we just always load records lazily and get rid of eagerLoadRecords? Not sure if we lose any performance by doing that.
89931793,2140,junrao,2016-11-29T02:45:18Z,"To be consistent, should we change record to message?"
89931855,2140,junrao,2016-11-29T02:45:53Z,Could we rename shallowEntries/deepEntries to shallowLogEntryIterator/deepLogEntryIterator so that it's clear that we are not buffering all entries in the call?
89931924,2140,junrao,2016-11-29T02:46:37Z,"The contract for LogInputStream seems to be that a null value from calling nextEntry() indicates normal completion of the iterator and any IOException indicates an error. So perhaps we should capture EOFException in DataLogInputStream since it's only expected there, and convert it to a null return value in nextEntry(). Then, here, we can just check null for ending the iterator like other places."
89931932,2140,junrao,2016-11-29T02:46:43Z,Do we want to add a separator between records?
89931956,2140,junrao,2016-11-29T02:46:53Z,Perhaps we could add a comment here that this class deals with the write path to MemoryLogBuffer while MemoryLogBuffer only deals with the read path?
89931968,2140,junrao,2016-11-29T02:47:03Z,Could this method just call appendUnchecked() to avoid code duplication?
89931980,2140,junrao,2016-11-29T02:47:09Z,It seems that estimatedBytesWritten() and numRecordsWritten() can be private?
89931983,2140,junrao,2016-11-29T02:47:11Z,bufferStream.buffer() can just be buffer().
89932016,2140,junrao,2016-11-29T02:47:36Z,"This also seems to be an existing issue. Until MemoryLogBufferBuilder is closed, buffer().position() is not necessarily accurate since the compressor may not have flushed compressed data to the output stream. Currently, RecordAccumulator.drain() calls this method before closing MemoryLogBufferBuilder. So, if builtLogBuffer != null, perhaps it's better to use estimatedBytesWritten()?"
89932051,2140,junrao,2016-11-29T02:47:59Z,"This also seems to be an existing problem. It seems that when generating those internal messages (tombstone, groups, etc), we assume the message timestamp type is always CREATE_TIME. However, the offset topic could be configured with LOG_APPEND_TIME."
89932067,2140,junrao,2016-11-29T02:48:08Z,It seems that we could get rid of ensureMatchingMagic since no caller is setting it?
90155476,2140,junrao,2016-11-30T01:52:30Z,"For compressed messageset, perhaps it's more consistent if we always return the lastOffset as offsetOfMaxTimestamp regardless of the timestamp type? We only need that for timestamp indexing. Indexing at the shallow offset level is good enough and will make the indexing logic consistent between the leader replica and the follower replica (which doesn't do decompression during log append)."
90155523,2140,junrao,2016-11-30T01:52:58Z,"To be future proof, should we pass in the timestamp in this record instead of NO_TIMESTAMP?"
90155541,2140,junrao,2016-11-30T01:53:10Z,"Since this is only used in MemoryLogBufferBuilder, perhaps it can be a private method there?"
90155548,2140,junrao,2016-11-30T01:53:14Z,It seems that this method can be private?
90155558,2140,junrao,2016-11-30T01:53:18Z,It seems that this method can be private?
90155589,2140,junrao,2016-11-30T01:53:35Z,"info.offsetOfMaxTimestamp returns the deep offset of the message with max timestamp. To be consistent with the other code path, if compression is enabled, it seems that we want to return the shallow offset? It maybe clearer to also rename ValidationAndOffsetAssignResult.offsetOfMaxTimestamp to sth like shallowOffsetOfMaxTimestamp."
90155602,2140,junrao,2016-11-30T01:53:42Z,It seems that toFormatVersion() is only used in test now?
90155611,2140,junrao,2016-11-30T01:53:46Z,It seems that convertToBuffer() is no longer used? 
90155619,2140,junrao,2016-11-30T01:53:50Z,unused import kafka.api.FetchResponsePartitionData
90155632,2140,junrao,2016-11-30T01:53:58Z,"""message set size"" is bit ambiguous. Perhaps we should say ""number of messages""?"
90155643,2140,junrao,2016-11-30T01:54:08Z,"Not sure what's ""byte offset""."
90155656,2140,junrao,2016-11-30T01:54:15Z,"Hmm, why do we have to change the expected size?"
90155674,2140,junrao,2016-11-30T01:54:24Z,Is there a reason that we only test non-compressed message conversion now?
90361346,2140,hachikuji,2016-12-01T00:35:21Z,"Hmm.. there was actually a reason for this. The static `shallowIterator` returns the more specific LogEntry type, which would not be possible if `shallow` is passed as an argument.  Having the more specific type in shallow iteration lets you do some operations to the shallow entries that are not possible with the deep entries (such as setting offsets or timestamps in-place)."
90363088,2140,hachikuji,2016-12-01T00:50:56Z,Ack. I'll fix this and the one below and update the test cases.
90377352,2140,hachikuji,2016-12-01T03:27:16Z,"To be honest, I'm not really sure why this comment is needed. It seems obvious that the key and value sizes in the inner messages are based on the uncompressed data (how could they be otherwise if we compress the inner message set as a whole?). "
90377503,2140,hachikuji,2016-12-01T03:29:28Z,"Thanks, I like this idea."
90378055,2140,hachikuji,2016-12-01T03:36:43Z,How about `shallowIterator` and `deepIterator`? 
90378753,2140,hachikuji,2016-12-01T03:45:52Z,"Related to my comment above. The reason to separate them is so that we can return a more specific type in the shallow iterator. For `MemoryLogBuffer`, `shallowEntries` returns `ByteBufferLogEntry`, which gives you hooks for writing over the offset and the timestamp. These methods do not make sense for the inner entries, so it is not desirable to add them to the general `LogEntry` interface. Similarly, with `FileLogBuffer`, we get shallow instances of type `FileChannelLogBuffer`, which provides its own custom hooks. The other thing I like about having the explicit names is that it makes the iteration type clear in the calling code (I don't have to remember whether `true` or `false` means shallow).

For consistency, we could change `records(boolean isShallow)` to support two variants. I added this method mainly for testing, but still it would be nice to have a consistent approach."
90378909,2140,hachikuji,2016-12-01T03:48:01Z,"As a matter of fact, the `shallow` option is unused for `records()`, so maybe I will change this to have it only return the deep records."
90381994,2140,hachikuji,2016-12-01T04:34:16Z,"Thanks for the suggestion. I modified `LogEntry.writeHeader` to work with the `DataOutputStream`. After doing so, I found that i no longer needed `putLong` and `putInt`."
90382244,2140,hachikuji,2016-12-01T04:38:04Z,Apologies... I often comment this out in testing and forget about it.
90382891,2140,hachikuji,2016-12-01T04:47:58Z,I think we can. The only cost is that we have to allow for the possibility of an exception thrown from `LogEntry.record()` instead of `LogInputStream.nextEntry()` (which already deals with IO errors).
90383775,2140,hachikuji,2016-12-01T05:02:32Z,"Hmm.. I think the test was broken or at least incomplete since `Message.toFormatVersion` only did shallow conversion. When I implemented this in the client code, I forbid shallow-only conversion because it results in bugs like we found in `LogCleaner`. We'll probably end up dropping this code after we remove `Message.toFormatVersion` as suggested above."
90383989,2140,hachikuji,2016-12-01T05:06:19Z,"It puzzled me for a while when writing this code why the size was coming out different only for snappy, but it turns out that we've overridden the block size in the client code, instead of using the default as was done for the server code."
90486643,2140,ijuma,2016-12-01T16:43:09Z,"Also, in Java, having named methods is clearer than using booleans since one cannot use named arguments. However, it can be a bit confusing to have both options."
90487017,2140,ijuma,2016-12-01T16:44:50Z,`shallowIterator` and `deepIterator` sounds good to me.
90487806,2140,ijuma,2016-12-01T16:48:26Z,Good catch. We probably don't want the change the buffer size in the server to be the same as the client. We may consider changing the client to be the same as the server. See KAFKA-3704 for details.
90805334,2140,guozhangwang,2016-12-05T05:23:18Z,Did we get rid of the re-allocation logic as a whole? Otherwise we cannot remove this additional check I think.
90805786,2140,guozhangwang,2016-12-05T05:31:37Z,"Is this private function better than previously in-lined, since it is private anyways?"
90806087,2140,guozhangwang,2016-12-05T05:36:06Z,"If we only deallocate the initial buffer, if re-allocation happens does that mean we will effectively have ""memory leaks""?"
90810199,2140,guozhangwang,2016-12-05T06:42:19Z,data -> memory?
90810687,2140,guozhangwang,2016-12-05T06:49:42Z,Is there any rationale for this magic number?
90813219,2140,guozhangwang,2016-12-05T07:22:36Z,"I'm following myself about renaming here: we could consider rename to RecordEntryInputStream, with T extends RecordEntry."
90814254,2140,guozhangwang,2016-12-05T07:36:51Z,This is not introduced in this patch: since we get the exact number of bytes returned from `log.append` could we use that in the trace logging?
90815062,2140,guozhangwang,2016-12-05T07:46:44Z,Where is this function used?
90978864,2140,hachikuji,2016-12-05T23:18:12Z,"This ended up a little ugly whichever way I cut it. I think I prefer the current location because it keeps the message format encapsulated in `Record` a bit better, but it comes at the cost of leaking the write optimization which is only used in `MemoryRecordsBuilder`. I could go either way here, so let me know if you feel strongly about moving it into `MemoryRecordsBuilder`. For now, I'll add a javadoc which explains the usage better."
90979896,2140,guozhangwang,2016-12-05T23:24:38Z,I have a comment on this function and it seems squashed. My question was since it is a private function do we really need this rather than having it in-lined?
90979998,2140,guozhangwang,2016-12-05T23:25:18Z,nit: data -> space?
90980061,2140,guozhangwang,2016-12-05T23:25:42Z,What is the rationale for this magic number for compressed set?
90981109,2140,guozhangwang,2016-12-05T23:32:53Z,We use `maxRecordSize` here and `maxMessageSize` in the other extended class.
90981197,2140,guozhangwang,2016-12-05T23:33:27Z,+1.
90986644,2140,guozhangwang,2016-12-06T00:12:47Z,"We are using `LogEntry` here for the message set wrapper, and in other places `LogEntry` is used for the internal message. We may need to update the JavaDoc on `LogEntry` accordingly in different extensions of `LogInputStream` and `Records` clarify if its `nextEntry` and `iterator` returns shallow or deep iterations."
90986838,2140,guozhangwang,2016-12-06T00:14:24Z,"This actually returns the message set as a `Record`, right?"
90986917,2140,guozhangwang,2016-12-06T00:15:09Z,nit: Also add the `FileRecords` reference here?
90987096,2140,guozhangwang,2016-12-06T00:16:37Z,nit: Indicate that this needs shallow iterations on the entries.
90987112,2140,guozhangwang,2016-12-06T00:16:45Z,nit: Indicate that this needs deep iterations on the entries.
90987175,2140,guozhangwang,2016-12-06T00:17:18Z,"This statement is a bit misleading, how about ""to the format indicated by the given magic value""."
90987669,2140,guozhangwang,2016-12-06T00:21:56Z,nit: unnecessary new lines.
90988043,2140,guozhangwang,2016-12-06T00:24:42Z,We can reuse RECORD_OVERHEAD_V0 and RECORD_OVERHEAD_V1 here.
90988718,2140,guozhangwang,2016-12-06T00:30:03Z,"Update the comment as well for `public`. Also I'm wondering if we could rename `wrapperXX` just to `XX` and add comments indicating that they are only used for old formatted messages with magic number > 0, and also add a check in constructor that the `magic()` field is consistent with its values: if it is larger than 0 these two fields should never be null; if it is 0 then these two fields should always be null etc."
90989533,2140,guozhangwang,2016-12-06T00:36:26Z,Is this function only used for unit tests?
90989804,2140,guozhangwang,2016-12-06T00:38:51Z,Is this function only used in tests?
90989898,2140,guozhangwang,2016-12-06T00:39:40Z,Why we keep its reverse function as private static in `Record` while making it in Utils?
90990467,2140,guozhangwang,2016-12-06T00:44:19Z,read from -> write to?
90991673,2140,guozhangwang,2016-12-06T00:54:36Z,Why we can use `Integer.MAX_VALUE` for deep iteration?
90991785,2140,guozhangwang,2016-12-06T00:55:33Z,Do we still need to override this function from `AbstractRecords` since we already override its calling `shallowIterator`?
90992078,2140,guozhangwang,2016-12-06T00:58:01Z,Ditto above. We use `maxMessageSize` and `maxRecordSize` interleavingly.
90992562,2140,guozhangwang,2016-12-06T01:02:27Z,Hmm... not sure I understand this: if compression is not use we will simply ignore the `shallow` flag and always go shallow??
90994228,2140,guozhangwang,2016-12-06T01:17:26Z,Could you elaborate a bit why shallow iterator allows extensible `LogEntry` while deep iterator does not?
90994615,2140,guozhangwang,2016-12-06T01:21:28Z,Why we need to re-construct the `LogEntry` if its magic number is larger than 0? Could we just set its corresponding record's timestamp directly?
90995329,2140,guozhangwang,2016-12-06T01:28:44Z,Good idea :)
90995731,2140,guozhangwang,2016-12-06T01:32:38Z,Those java docs need to be updated with the new class names. Ditto everywhere else.
90996571,2140,guozhangwang,2016-12-06T01:41:11Z,nit: group Kafka imports?
90997090,2140,guozhangwang,2016-12-06T01:45:58Z,"I think we do not need to make `builderWithEntries` public since this is the only caller here, and it is followed by a `build()` call immediately, so we can still use `withLogEntries`."
90997888,2140,guozhangwang,2016-12-06T01:53:42Z,"It is not introduced in this patch: I think its more clear to use two vals here, one named `trimedRecords` and one named `validRecords`."
90998010,2140,guozhangwang,2016-12-06T01:55:10Z,Where is this object used?
90998487,2140,guozhangwang,2016-12-06T02:00:39Z,"A meta clarification question: for all these classes, are we planning to get rid of them all at the same time when the old consumer is removed?"
90999554,2140,guozhangwang,2016-12-06T02:12:13Z,Looks good :)
90999912,2140,guozhangwang,2016-12-06T02:16:23Z,private?
91007176,2140,hachikuji,2016-12-06T03:45:33Z,Hmm... I may have misunderstood how this worked. I'll add it back.
91008613,2140,hachikuji,2016-12-06T04:05:02Z,"""Data"" is correct, but I'll clarify the comment since it does read a little awkwardly. We're trying to say that the client should raise an error if the size of the message exceeds the bytes returned in the response."
91008790,2140,hachikuji,2016-12-06T04:07:51Z,It was copied from the server code: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/message/ByteBufferMessageSet.scala#L161. I think perhaps the 1024 comes from the minimum block size for snappy encoding. I can add a comment about that part at least.
91009212,2140,hachikuji,2016-12-06T04:13:44Z,It's used in this file. It may not show in the diff because I moved it from `FileMessageSet` (which was deleted).
91009296,2140,hachikuji,2016-12-06T04:15:00Z,Inside `LogManager.asyncDelete`.
91009688,2140,hachikuji,2016-12-06T04:20:18Z,It's the maximum size of a record entry to read. I think the only place we use it is in `DumpLogSegments`.
91010122,2140,hachikuji,2016-12-06T04:27:05Z,"The main reason for extension of `LogEntry` is to enable optimization tricks like overwriting the offsets in place (see `ByteBufferLogEntry`) or reading the magic byte without loading the record data (see `FileChannelLogEntry`). These tricks are generally only possible for the shallow records. You can't modify the deep records in place since they have been decompressed, nor can you optimize which parts of the record to read (you have to read the whole thing). For the deep records, there's not much you can do aside from read the data, so extension seemed unnecessary."
91010212,2140,hachikuji,2016-12-06T04:28:14Z,"Haha, depends on whether it's compressed or not, right?"
91010306,2140,hachikuji,2016-12-06T04:29:41Z,I have a comment in `LogInputStream` which attempts to make the distinction clear. Let me know if more explanation is needed.
91011143,2140,hachikuji,2016-12-06T04:41:47Z,"The wrapper values can be null if either the magic is 0 or if the record is uncompressed. I'll add an assertion for this, but I'd prefer to keep the current names since it's otherwise harder to explain."
91011764,2140,hachikuji,2016-12-06T04:51:21Z,"It was necessary before because of the optimization in `FileChannelLogEntry.magic()`, but since I've factored that into `LogEntry`, I was able to remove it."
91011883,2140,hachikuji,2016-12-06T04:52:59Z,"If compression is not used, there are no deep entries."
91012046,2140,hachikuji,2016-12-06T04:55:40Z,"I followed what the current code did. Previously deep iteration was done here in `ByteBufferMessageSet.deepIterator`, which has no check for max message size."
91012622,2140,hachikuji,2016-12-06T05:02:47Z,"Hmm... Seems worth exploring.  We could add a `setWrapperTimestamp(TimestampType, long)` or something like that, but I'm not sure it's a good idea to make those fields mutable."
91012841,2140,hachikuji,2016-12-06T05:06:55Z,The public version of `builderWithEntries` is used in `LogValidator` and `ByteBufferMessageSet`.
91013183,2140,hachikuji,2016-12-06T05:12:28Z,"On second thought, doing so would change the behavior of this function."
91013213,2140,hachikuji,2016-12-06T05:12:48Z,"Yes, that is the hope. I've removed almost all other uses."
91017536,2140,hachikuji,2016-12-06T06:12:35Z,"Actually I had to remove these assertions. First, if the magic is 1, then we can't distinguish between a shallow uncompressed entry, and a deep entry, which would also not have its compression flag enabled. We could at least raise an error if magic is 0 and there is a provided wrapper timestamp., but we currently have test cases which allow us to create a Record with only a valid CRC (which means we can't check magic). We could probably be a little stricter: if there aren't enough bytes in the ByteBuffer to read the record overhead, then obviously the record can't be valid, so maybe those test cases are kind of silly and should be removed."
91149520,2140,guozhangwang,2016-12-06T19:11:31Z,"Aha, I was thinking about the new format :) NVM."
91149711,2140,guozhangwang,2016-12-06T19:12:25Z,"I was thinking if we can save one object creation, but it seems less worth optimizing since we are not re-creating the underlying buffer anyways. So it's your call."
91826419,2140,junrao,2016-12-10T02:22:04Z,deepEntries => deepIterator ?
91826423,2140,junrao,2016-12-10T02:22:10Z,"Is ""File-backed log buffer."" still needed?"
91826431,2140,junrao,2016-12-10T02:22:21Z,"Hmm, why do start and end need to be long?"
91826435,2140,junrao,2016-12-10T02:22:28Z,"""log buffer"" probably need to be changed?"
91826438,2140,junrao,2016-12-10T02:22:35Z,It seems that position should be int?
91826441,2140,junrao,2016-12-10T02:22:40Z,Could we just cast channel.size() to int?
91826447,2140,junrao,2016-12-10T02:22:45Z,shallowEntries in this and the next two methods should be shallowIterator?
91826461,2140,junrao,2016-12-10T02:23:01Z,"Is the reference to ""log buffer"" still valid?"
91826468,2140,junrao,2016-12-10T02:23:08Z,"It seems that the follower could call this more than once. So, perhaps it's worth caching."
91826477,2140,junrao,2016-12-10T02:23:21Z,It doesn't seem that last offset is being maintained here.
91826488,2140,junrao,2016-12-10T02:23:46Z,"Hmm, we are getting the last offset, which is not necessarily the offset for message with the max timestamp."
91826489,2140,junrao,2016-12-10T02:23:52Z,This and the next method probably should be deepIterator() too?
91826492,2140,junrao,2016-12-10T02:23:59Z,"I had the following comment earlier. Is that valid?

This also seems to be an existing issue. Until MemoryLogBufferBuilder is closed, buffer().position() is not necessarily accurate since the compressor may not have flushed compressed data to the output stream. Currently, RecordAccumulator.drain() calls this method before closing MemoryLogBufferBuilder. So, if builtLogBuffer != null, perhaps it's better to use estimatedBytesWritten()?"
91826501,2140,junrao,2016-12-10T02:24:05Z,Are the references to log buffer still valid?
91826508,2140,junrao,2016-12-10T02:24:13Z,"To be consistent, should we change entries to records?"
91826512,2140,junrao,2016-12-10T02:24:18Z,Should logEntries be records?
91830966,2140,ijuma,2016-12-10T07:53:07Z,"Btw, it's a bit of a shame to lose the enhanced foreach syntax. Is there a reason not to expose a `deepIterable()` method instead?"
91880865,2140,becketqin,2016-12-12T04:10:01Z,@junrao @hachikuji KAFKA-4497 reported an issue regarding this. There was a few other issues in `ByteBufferMessageSet.filterInto()` logic. I provided a patch in #2242 . We should probably fix the logic here as well.
92014373,2140,hachikuji,2016-12-12T18:57:41Z,"Yes, that makes sense. Apologies for missing the comment before."
92020118,2140,hachikuji,2016-12-12T19:24:26Z,"Ack. This goes back to LogCleaner actually, but I'll go ahead and fix here."
92072625,2140,junrao,2016-12-13T00:21:41Z,"a 4 byte size, an 8 byte offset => an 8 byte offset, a 4 byte size of the record"
92072954,2140,junrao,2016-12-13T00:24:22Z,"Is the reference to ""log buffer"" still valid?"
92072967,2140,junrao,2016-12-13T00:24:27Z,"Is the reference to ""log buffer "" still valid?"
92195306,2140,ijuma,2016-12-13T15:42:32Z,"It's a bit annoying that we create so much indirection (DataLogInputStream -> ByteBufferInputStream -> UnderlyingInputStream -> ByteBuffer -> byte[]). In an ideal world, we would not bother with `InputStream` at all and would just operate at the `ByteBuffer` level. However, the gzip case is hard to do that way."
92216997,2140,junrao,2016-12-13T17:09:21Z,Do we need to change the position of buffer? Perhaps we could instead just change the position in the slice passed to Record().
92217186,2140,junrao,2016-12-13T17:10:13Z,"""a 4 byte size,"" needs to be removed."
92225885,2140,hachikuji,2016-12-13T17:50:35Z,"@ijuma Haha, yeah. One of the layers is sort of fake (`DataInputStream` should be a mixin), but the point is still valid."
92227525,2140,hachikuji,2016-12-13T17:58:31Z,"Currently `Record` expects the position of the `ByteBuffer` to be at 0. I was tempted to change this assumption, but decided to leave it for now (it's a bit annoying to change all the accessors to assume relative positioning). We could accomplish the same result using `mark()` and `reset()` if that seems any better."
563471404,2140,dengziming,2021-01-25T05:30:44Z,"Should this be `Math.min(length, size.get() - offset)`?"
563909128,2140,junrao,2021-01-25T17:32:58Z,@dengziming : Thanks for the comment. This does seem like a bug. Would you be interested in submitting a separate PR to have this fixed?
564172786,2140,dengziming,2021-01-26T02:25:09Z,https://github.com/apache/kafka/pull/9970
179653282,4830,tedyu,2018-04-06T03:32:29Z,Should this be KAFKA_1_1_IV0 ?
179653551,4830,tedyu,2018-04-06T03:35:26Z,I think channelUnmutingCallback would be better name
179898815,4830,jonlee2,2018-04-06T23:49:03Z,"This was based on the comments in lines 31-36, which states ""When we change the protocol a second time while developing 0.10.0, we will add a new config value ""0.10.0-IV1"" and a corresponding case object KAFKA_0_10_0-IV1. We will change the config value ""0.10.0"" to map to the latest internal version object KAFKA_0_10_0-IV1."" Following the example, I set 1.1 to the latest internal version, KAFKA_1_1_IV1."
179919414,4830,lindong28,2018-04-07T15:22:58Z,`throttleDelayMs`?
179919602,4830,lindong28,2018-04-07T15:30:06Z,"If connection state is connecting (i.e. connectionDelay = Long.MAX_VALUE) and throttleDelay is 10 ms, should we poll this connection right after 10 ms, or should we wait until the state is connected?"
179919811,4830,lindong28,2018-04-07T15:38:07Z,Would it be more intuitive and consistent to let `ClusterConnectionStates.isReady()` return false if the connection is throttled?
179919877,4830,lindong28,2018-04-07T15:40:51Z,Maybe `throttleDelayMs`?
179919904,4830,lindong28,2018-04-07T15:41:38Z,"Maybe `pollDelayMs`? Also, can you update the param in the Java doc?"
179919911,4830,lindong28,2018-04-07T15:41:53Z,Maybe `pollDelayMs`?
179920062,4830,lindong28,2018-04-07T15:47:04Z,"According to the Java doc of log.trace(), it says `This form avoids superfluous string concatenation when the logger is disabled for the TRACE level`. Thus we probably don't need to explicitly check `log.isTraceEnabled`."
179920083,4830,lindong28,2018-04-07T15:48:11Z,nits: it may be simpler to just do `if (nodesWithClientSideThrottlingEnabled.contains(nodeId) && throttleTimeMs > 0)`
179920227,4830,lindong28,2018-04-07T15:53:25Z,We probably don't need to explicitly check log.isDebugEnabled(...) because `log.debug` will automatically check this before doing string operation.
179920433,4830,lindong28,2018-04-07T16:00:33Z,"It seem a bit unintuitive -- if a node's ApiVersionResponse's version is smaller than 2, why would the node be in `nodesWithClientSideThrottlingEnabled` in the first place?

It maybe more intuitive to remove this `else` branch and instead remove the node from `nodesWithClientSideThrottlingEnabled` in `handleConnections()`. What do you think?"
179920578,4830,lindong28,2018-04-07T16:04:56Z,"In order to be consistent with other comments, it may be better to say `Introduced ApiVersionsRequest V2 via KIP-219`. Also, can you move `""1.1"" -> KAFKA_1_1_IV1` to the last entry?"
179920783,4830,lindong28,2018-04-07T16:13:09Z,Can `throttledChannel` be `val` instead of `var`?
179920867,4830,lindong28,2018-04-07T16:17:45Z,It seems that the return value of `tryUnmuate` is only needed for trace level logging. Can we move the trace level logging into this method and make this method void to simplify the implementation?
179920958,4830,lindong28,2018-04-07T16:22:49Z,Can `throttledChannel` be `val`?
179920962,4830,lindong28,2018-04-07T16:22:56Z,Can `throttledChannel` be `val`?
179921259,4830,lindong28,2018-04-07T16:35:48Z,It is probably not necessary to check `throttleTimeMs > 0` since `maybeThrottle()` will check it anyway.
179921295,4830,lindong28,2018-04-07T16:37:03Z,"It maybe more readable to keep the code style consistent by moving `quotas.request.maybeThrottle(request, requestThrottleTimeMs)` to a new line with `{` and `}`."
179921553,4830,lindong28,2018-04-07T16:44:54Z,I think the word `maybe` in the original method name `maybeRecordAndThrottle` is mostly for `throttle`. It is probably more intuitive to name this method `getThrottleTimeMs`.
179921638,4830,lindong28,2018-04-07T16:48:10Z,It seems a bit redundant to check `quotasEnabled` in both `maybeRecord` and `maybeThrottle`. It is probably more intuitive to check `quotasEnabled` only in `maybeRecord` such that `throttleTimeMs` will be 0 if `quotasEnabled` is false. Then `maybeThrottle` can act solely based on `throttleTimeMs`.
179921729,4830,lindong28,2018-04-07T16:51:06Z,"nits: it may be more readable to use Math.max(bandwidthThrottleTimeMs, requestThrottleTimeMs) to be consistent with other code and also to show that these two variables are treated equally."
180278955,4830,jonlee2,2018-04-10T02:01:50Z,Done
180286163,4830,jonlee2,2018-04-10T03:09:53Z,Done
180286169,4830,jonlee2,2018-04-10T03:09:57Z,Done
180286176,4830,jonlee2,2018-04-10T03:10:00Z,Good point. I think throttleDelay should make sense only when connected (either CHECKING_API_VERSIONS or READY). Updated the code to explicitly check the connection state to determine which delay to return.
180286183,4830,jonlee2,2018-04-10T03:10:06Z,Done. I also updated ClusterConnectionStates.hasReadyNodes() to take the throttling state into account. Please review that change as well.
180286188,4830,jonlee2,2018-04-10T03:10:08Z,Done
180286192,4830,jonlee2,2018-04-10T03:10:10Z,Done
180286207,4830,jonlee2,2018-04-10T03:10:20Z,Done
180286219,4830,jonlee2,2018-04-10T03:10:26Z,"Based on your next comment, I think it still makes sense to call this maybeRecord() (or maybeRecordAndGetThrottleTimeMs?) because recording only happens when quotasEnabled == true. And also rename maybeThrottle() to throttle(). I made those changes. What do you think? "
180286234,4830,jonlee2,2018-04-10T03:10:32Z,"Makes sense assuming that maybeThrottle() always uses throttle time returned by maybeRecord(), which is the case.  "
180286244,4830,jonlee2,2018-04-10T03:10:34Z,Done
180286350,4830,jonlee2,2018-04-10T03:11:33Z,Done
180286480,4830,jonlee2,2018-04-10T03:12:56Z,This variant takes 3+ arguments after the format string and the comment says that it incurs a small overhead. Looks like the same check is used for other places where 3+ arguments are taken. Will keep unless you have other concerns.
180286492,4830,jonlee2,2018-04-10T03:13:04Z,Done
180286561,4830,jonlee2,2018-04-10T03:13:38Z,"For this, I removed the check because it take 1 or 2 args."
180286995,4830,jonlee2,2018-04-10T03:15:53Z,"It was to cover corner cases where a broker restarted with a lower version (like rollback).

I am not entirely sure how this can be in handleConnections(), though. It looks like just initiating api version fetch. Can you elaborate?  "
180287026,4830,jonlee2,2018-04-10T03:15:59Z,Done
180287053,4830,jonlee2,2018-04-10T03:16:05Z,Done
180287077,4830,jonlee2,2018-04-10T03:16:10Z,Done
180287095,4830,jonlee2,2018-04-10T03:16:14Z,Done
180287122,4830,jonlee2,2018-04-10T03:16:20Z,Done
180671354,4830,lindong28,2018-04-11T08:22:18Z,Is this method used only in test? It maybe unnecessary to add a method to a interface solely for test purpose.
180673624,4830,lindong28,2018-04-11T08:30:23Z,`isThrottled()` is only called once in non-test code (i.e. `ClusterConnectionStates.isReady(...)`). It maybe simpler to just call `throttleDelayMs(...) > 0` in `isReady()`. (similar to the code in `hasReadyNodes()`)
180675326,4830,lindong28,2018-04-11T08:36:50Z,How about just call `isReady()` here? I understand that existing code does not re-use the isReady(). It maybe better to improve it.
180678877,4830,lindong28,2018-04-11T08:49:44Z,"It seems that we can remove `isConnected(..)` and replace it with `isReady(...)` in `pollDelayMs(...)`. `isConnected()` will be different from `isReady()` only when state is `CHECKING_API_VERSIONS`. But we are sending ApiVersionRequest to broker in `handleInitiateApiVersionRequests()` anyway without checking whether the connection is throttled.

Alternatively we can update `handleInitiateApiVersionRequests()` so that it does not send `ApiVersionRequest` if the connection is throttled. Personally I would not do this because I don't think the first version ApiVersionRequest would overload the broker and thus the extra code/logic in client-side implementation is probably not worthwhile."
180679073,4830,lindong28,2018-04-11T08:50:31Z,"`throttleDeadlineMs` maybe be a bit ambiguous in how it is used -- does it mean the connection should be throttled or un-throttled after this deadline? How about `throttleUntilTimeMs`, `muteUntilTimeMs` or `earliestSendTimeMs`?"
180679291,4830,lindong28,2018-04-11T08:51:20Z,We probably don't need this method if we don't need `ClusterConnectionStates.isConnected()`.
180682137,4830,lindong28,2018-04-11T09:00:59Z,We can probably just use `now` without calling `time.milliseconds()` here.
180682340,4830,lindong28,2018-04-11T09:01:36Z,We can probably call `time.milliseconds` only once and share it with the existing `now = time.milliseconds()`.
180683981,4830,lindong28,2018-04-11T09:07:30Z,@jonlee2 Yeah `maybeRecordAndGetThrottleTimeMs()` sounds better.
180813450,4830,lindong28,2018-04-11T16:14:40Z,"Previously the way we throttle a request is pretty extensible: we first throttle based on byte rate, and if that passes, then we throttle based on produce rate. If in the future we have another throttle mechanism, e.g. based on CPU, we can easily stack this on top of byte-rate-based and request-rate-based throttling mechanism.

The way this patch implements throttling is kind of hard to extend (or appears to be difficult to read) due to the following reasons:

- The number of throttling mechanisms is hard-coded to be 2 in ThrottleChannel.tryUnmute()
- Instead of being able to distribute throttling mechanism in multiple places, now we have to determine the throttle time of all mechanism in one place (e.g. here) and use the one that has the largest throttle time. This will work for now. But it will be hard to extend and seems not-so-readable.
- We pass around Option[ThrottledChannel] and treats `None` separately from `Some()`. It will be cleaner to just pass one non-option object.

I am wondering if the following high-level solution would be better:

In class `KafkaChannel` we use an integer to keep track of the number of existing mechanisms that are throttling this channel. For example, if this request exceeds the byte-based quota, we increment this integer by one and enqueue an object into a delayed queue. After the corresponding throttle time is passed, we dequeue this object and decrement the value of this integer in the `KafkaChannel`. If the value is 0 after it is decremented, we unmute the channel.

This solution may be cleaner in the following sense:

- We no longer needs to pass Option[ThrottledChannel].
- We can implement byte-based and request-based throttling mechanism in the same manner as the existing Kafka implementation without having to put them together.
- The solution is more extensible since it does not assume there are exactly two throttling mechanisms.

What do you think?

"
182270273,4830,jonlee2,2018-04-17T23:21:30Z,Done
182270329,4830,jonlee2,2018-04-17T23:21:47Z,Done
182270349,4830,jonlee2,2018-04-17T23:21:58Z,Done
182270366,4830,jonlee2,2018-04-17T23:22:04Z,Done
182270438,4830,jonlee2,2018-04-17T23:22:32Z,Renamed to throttleUntilTimeMs.
182271668,4830,jonlee2,2018-04-17T23:31:08Z,"Now that isReady() checks for throttling status, I don't think it can be used directly for pollDelayMs(). pollDelayMs() needs to check if the state is READY and *throttled*. I can directly check if the state is READY, instead of isConnected(). Will that work?"
182271745,4830,jonlee2,2018-04-17T23:31:44Z,See the comment above. I'll remove once we agree on what to do for pollDelayMs().
182271764,4830,jonlee2,2018-04-17T23:31:52Z,Done
182271781,4830,jonlee2,2018-04-17T23:31:59Z,Done
182271956,4830,jonlee2,2018-04-17T23:33:16Z,"We discussed this offline and agreed to keep the current logic for using the max.

As for Option[ThrottledChannel], I used KafkaChannel to keep the ref count as suggested and got rid of the use of Option[ThrottledChannel]."
183115763,4830,lindong28,2018-04-20T17:15:27Z,"The Java doc seems to be inconsistent with the implementation. If the connection has been established and the throttle delay is 0, we actually return `Long.MAX_VALUE` instead of 0.

Also, can this Java doc follow the style of the existing Java doc for `connectionDelay`. For example, `Returns the number of milliseconds to wait, based on the connection state and the throttle time, before attempting to send data.`"
183115843,4830,lindong28,2018-04-20T17:15:45Z,The Java doc seems to be inconsistent with the implementation.
183145676,4830,lindong28,2018-04-20T19:16:12Z,"It seems that `decrementUnmuteRefCountAndGet` and `unmute` are always used together. Would it be simpler to just modify the existing method `unmute` to be `maybeUnmute`, which internally will decrement the reference count and umute the channel if the reference count is 0?"
183146203,4830,lindong28,2018-04-20T19:18:35Z,"Would it be simpler to just modify the existing method `mute` to be `maybeMute`, which internally will increment the reference count and mute the channel if the reference count is 1 after it is incremented?"
183147589,4830,lindong28,2018-04-20T19:24:59Z,Should this be debug level logging?
183150790,4830,lindong28,2018-04-20T19:40:01Z,"It maybe subjective. I am more inclined not to use super.*. Can we keep the existing Java method style, where we check `quotasEnabled` in `maybeRecordAndGetThrottleTimeMs`, and call `recordAndGetThrottleTimeMs` if `quotasEnabled` is true. `maybeRecordAndGetThrottleTimeMs` can be defined in both `ClientRequestQuotaManager` and `ClientQuotaManager`. `recordAndGetThrottleTimeMs` will be defined only in `ClientQuotaManager`."
183152661,4830,lindong28,2018-04-20T19:49:15Z,"This method is in `ClientRequestQuotaManager` but not in `ClientQuotaManager`. And it kind of overlaps with `throttle` and `maybeRecordAndGetThrottleTimeMs`. So the simplification of removing callback in KafkaApis.java comes at the cost of added methods and inconsistency in `ClientRequestQuotaManager` and `ClientQuotaManager`. I am usually conservative and prefer to keep the existing code style unless the code style is clearly superior. In this case there is pros and cons in the new code style. And since difference people may have different opinion, it may cause back-and-forth change in open source development.

Later the second reviewer can comment on this."
183153259,4830,lindong28,2018-04-20T19:52:02Z,"Previously the throttleTimeMs in FetchResponse is `bandwidthThrottleTimeMs + requestThrottleTimeMs`. Now it is changed to `max(bandwidthThrottleTimeMs, requestThrottleTimeMs)`."
183153543,4830,lindong28,2018-04-20T19:53:30Z,"Previously the throttleTimeMs in ProduceResponse is `bandwidthThrottleTimeMs + requestThrottleTimeMs`. Now it is changed to `max(bandwidthThrottleTimeMs, requestThrottleTimeMs)`."
183155316,4830,lindong28,2018-04-20T20:00:37Z,I am wondering if there is specific reason for the previous method signature.
183588951,4830,jonlee2,2018-04-24T02:20:45Z,We need to return max since we are not stacking throttling anymore.
183588956,4830,jonlee2,2018-04-24T02:20:49Z,We need to return max since we are not stacking throttling anymore.
183588969,4830,jonlee2,2018-04-24T02:20:54Z,This is because we don't need to pass this as a callback anymore.
183603418,4830,jonlee2,2018-04-24T04:43:04Z,Done
183603427,4830,jonlee2,2018-04-24T04:43:11Z,Done
183603515,4830,jonlee2,2018-04-24T04:43:59Z,I actually removed maybeRecordThrottle() from ClientRequestQuotaManager. It is now consistent with ClientQuotaManager.
183603548,4830,jonlee2,2018-04-24T04:44:21Z,Done. Thanks for catching this.
183603566,4830,jonlee2,2018-04-24T04:44:33Z,Updated
183604255,4830,jonlee2,2018-04-24T04:50:13Z,"I thought about it and I actually prefer to keep it this way. The reason why is that the ref count is used by SocketServer only and thus I want SocketServer to be the only one that updates the count. In other words, whatever layer that uses this ref count should update it within that layer. If we expose this to selector, I am concerned that there may be some misuse. What do you think? "
183604277,4830,jonlee2,2018-04-24T04:50:29Z,Please see the comment above.
183916192,4830,lindong28,2018-04-25T00:19:45Z,nits: can we use math.max(...)?
183917654,4830,lindong28,2018-04-25T00:33:09Z,It may be slightly better to share the code with the existing `setExpectedApiVersionsResponse()`.
183918381,4830,lindong28,2018-04-25T00:39:55Z,Do we need this change?
183919344,4830,lindong28,2018-04-25T00:48:28Z,"Typo requoest. Also, should the throttle time for request rate quota to be larger than 0, since the request rate quota is 0.01?"
183919358,4830,lindong28,2018-04-25T00:48:39Z,Typo requoest
183940157,4830,jonlee2,2018-04-25T04:27:12Z,Done
183940161,4830,jonlee2,2018-04-25T04:27:15Z,Refactored the code to maximize code reuse.
183940170,4830,jonlee2,2018-04-25T04:27:24Z,Forgot to remove. Updated.
183940176,4830,jonlee2,2018-04-25T04:27:28Z,"Fixed. The test is instrumented so that both quotas are violated, but throttle time is recorded for the max of the two only. That's why throttle time metrics for request quota is supposed to be 0."
183940180,4830,jonlee2,2018-04-25T04:27:31Z,Fixed
184196347,4830,rajinisivaram,2018-04-25T20:30:58Z,"There are a lot of comments like this that refer to KIP-219. Typically, we don't include KIP numbers and JIRAs in comments for code changes. It is preferable to have comments that are self contained so that developers don't have to find the KIP to follow the code."
184196826,4830,rajinisivaram,2018-04-25T20:32:58Z,"@ijuma had a comment on the KIP discuss thread about the approach to update only ApiVersions version. Can you respond on the thread, please?"
184197543,4830,rajinisivaram,2018-04-25T20:35:47Z,"I haven't gone through the PR in detail, but I think this callback is invoked on a different thread when throttle time expires. `Selector` is not thread-safe and we expect methods on the selector to be invoked from a single thread."
184199595,4830,rajinisivaram,2018-04-25T20:43:28Z,"I am not sure about this. Since quotas are calculated differently, not sure `max` gives the same result as the throttle times calculated separately as it was done earlier."
184753260,4830,jonlee2,2018-04-27T17:25:02Z,"I removed KIP-219 references, except for ApiVersion.scala."
184753299,4830,jonlee2,2018-04-27T17:25:10Z,"Yes, I replied to the vote thread for seeking comments. Could you also respond to that thread since you were one of the original voters? "
184753337,4830,jonlee2,2018-04-27T17:25:17Z,"Thank you for catching this. This is an important point. I made the following changes to address this.

- removed the callback from SocketServer to the API layer
- when throttling starts and ends, the API layer will put responses using new ResponseActions to the request channel queue to notify socket server of start/end throttling
- when SocketServer receives responses with these new actions, it will update reference count and try to unmute the channel

With these changes, reference count handling and mute/unmute channel will be handled in the same thread."
184754840,4830,jonlee2,2018-04-27T17:30:50Z,"With this KIP, a response needs to be sent out with a throttle time value before actually starting throttling, and thus we need to determine how long we should mute the channel first before sending out the response. In other words, if multiple quotas are violated, we can't really wait till throttling for one quota is over before computing throttle time for next quota.
 
Under this scenario, I thought using the max is reasonable. It will be same as before if only one quota is violated. If multiple quotas are violated, using max may not give the same throttle time
in some cases but I am not sure what will be a better alternative.

I am not entirely sure about how throttle times can be ""calculated separately"" with this KIP. Are you suggesting that on multiple quota violations, say, produce and request, we throttle for produce violation only and deal with any remaining request quota violation the next time the client sends another produce request after the initial throttling? But this is also different from the way it was and can be inefficient (in case we keep picking smaller throttle time).  "
184950779,4830,rajinisivaram,2018-04-30T10:10:15Z,"No, I am not suggesting that we do only one at a time. I think we need to see if we can do a better calculation than `max` to combine the two (or potentially many in future)."
185120383,4830,jonlee2,2018-04-30T21:49:07Z,"I still think using max is reasonable. In my understanding, that is the minimum amount of time we need to throttle anyway for the traffic at the time of quota violation. There may be other connections using the same client id/user while the throttling is going on, but that will only add more load and thus will not improve the throttle time. I also discussed with @becketqin and he agreed. Would you let me know if you have any specific suggestions?   "
186307847,4830,lindong28,2018-05-07T00:06:29Z,nits: can we replace `throttleDeadlineMs` with `throttleUntilTimeMs` for consistency?
186308119,4830,lindong28,2018-05-07T00:14:46Z,nits: can we align the second line of parameters with the first line in the same manner as `sendInternalMetadataRequest()`?
186308382,4830,lindong28,2018-05-07T00:21:40Z,"According to the Java doc of org.slf4j.Logger.trace, `This form avoids superfluous string concatenation when the logger is disabled for the TRACE level. However, this variant incurs the hidden (and relatively small) cost of creating an <code>Object[]</code> before invoking the method`. So it is probably OK to skip checking `log.isTraceEnabled`. It will also be more consistent with the existing invocation of `log.trace()` in Kafka."
186308601,4830,lindong28,2018-05-07T00:28:08Z,"Since this integer is decremented when `tryUnmuteChannel()` is called, would it be a bit more intuitive to name it `muteRefCount()`? If so, we may want to also rename methods such as `incrementUnmuteRefCount()`, `decrementUnmuteRefCountAndGet()`, `getUnmuteRefCount()` and `incrementChannelUnmuteRefCount()`."
186309712,4830,lindong28,2018-05-07T00:55:33Z,"For the same reason that this patch adds the throttleTimeMs field to the LeaderAndIsrResponse, should we also add this field to StopReplicaResponse?"
186313712,4830,lindong28,2018-05-07T02:03:46Z,"Broker's handling of FetchRequest becomes stateful after KIP-227 added support for incremental fetch response. It means that the state in broker will be inconsistent with the state in client if we replace a non-empty FetchResponse with an empty response. More specifically, in `CachedPartitions.updateResponseData()`, state (e.g. highWatermark) will be updated if a partition is assumed to be included in the FetchResponse.

In order to solve this problem, we probably need to change the implementation related to KIP-227. Currently `CachedPartitions.updateResponseData()` will 1) determine the partitions to be included and 2) update the state for those partitions that are included. We probably need to split this into two separate functions. The the state for those partitions should be updated only if the FetchResponse is not throttled.

Another problem is that `clientSensors.quotaSensor.record` has already been called in `recordAndGetThrottleTimeMs()` at this point. It means that we have already assumed that the resource is used to send the FetchResponse at this point. It will cause under-utilization if we only sends an empty FetchResponse after updating the quotaSensor.

The main motivation of this KIP is to address problem caused by ProduceRequest. FetchRequest is small and probably not a concern. If we don't have a simpler way to handle the above two problems, I would recommend not to touch the handleFetchRequest() logic in this patch, i.e. broker still sends the non-empty FetchResponse after the throttle time has passed.



"
187203528,4830,jonlee2,2018-05-09T23:23:32Z,"Per discussion with @rajinisivaram, I reverted the changes I made to cluster action responses."
187203579,4830,jonlee2,2018-05-09T23:23:56Z,Done
187203589,4830,jonlee2,2018-05-09T23:24:01Z,Done
187203602,4830,jonlee2,2018-05-09T23:24:09Z,Done
187203618,4830,jonlee2,2018-05-09T23:24:15Z,Done
187204068,4830,jonlee2,2018-05-09T23:27:20Z,"Thanks for catching this. I made the following changes to address these points:
1. added getResponseSize() to FetchContext to get the response size (for calculating throttle time) without updating the internal states
2. In case of fetch throttling, unrecord the recorded usage value by recording a negative value of the same quantity. 

I added comments stating more details about these changes. "
187911027,4830,rajinisivaram,2018-05-14T10:59:10Z,"`AtomicInteger` suggests that this field is updated from multiple threads. Since we rely on this to be updated and accessed only from a single thread, it would be better to make it an `int` (like the other fields in this class). Also, it is confusing to have a field `muted` and another `muteRefCount` and separate methods to go with each. Can we combine these two? Possibly even just have `mute/unmute` methods in `KakaChannel` and make the reference count internal rather than managed by `SocketServer`?"
187913359,4830,rajinisivaram,2018-05-14T11:09:53Z,This response as well as others without a throttle time don't need version bump.
187913661,4830,rajinisivaram,2018-05-14T11:11:03Z,I think it would be better to add a `throttleTimeMs()` method to `AbstractResponse` that returns throttle time for responses which contain the time and zero for others.
187914030,4830,rajinisivaram,2018-05-14T11:12:40Z,Should just return zero for this response as well as other responses which don't contain throttle time.
187924822,4830,rajinisivaram,2018-05-14T11:54:47Z,We don't usually use `get` prefix for getters.
187925371,4830,rajinisivaram,2018-05-14T11:56:57Z,Hmm.. This is not ideal. Metrics are externally visible entities that are used for monitoring. Recording and unrecording can be confusing. But agree that it is hard to fix. We should at least record using the same time (will also avoid an extra `System.currentTimeMillis()` per fetch request).
187925735,4830,rajinisivaram,2018-05-14T11:58:28Z,"This is the request quota manager, so it is always request processing time."
188146295,4830,jonlee2,2018-05-15T01:53:39Z,Done
188146303,4830,jonlee2,2018-05-15T01:53:44Z,Done
188146314,4830,jonlee2,2018-05-15T01:53:49Z,Done
188146336,4830,jonlee2,2018-05-15T01:53:56Z,Done
188146689,4830,jonlee2,2018-05-15T01:56:58Z,"I agree that it is not ideal, but the current implementation couples the reporting and quota checking a little too tightly. I used the same time for both record/unrecord as suggested."
188149319,4830,jonlee2,2018-05-15T02:21:38Z,"I changed AtomicInteger to int.

As for your other comments, the ref count is used by SocketServer and thus I think it should be updated by SocketServer only. I am concerned that the ref count combined with the existing KafkaChannel mute/unmute may cause some issues when misused by other KafkaChannel users. For example, what if someone calls mute() twice? The second mute() is supposed to be a no-op, but if we decide to increase the ref count as part of mute(), it is not actually a no-op. 

Also, SocketServer still needs to call some method to increase the count when StartThrottlingAction is received, so it won't be completely transparent. Another point is that unmute() will be effectively tryUnmute() because it only unmutes when the ref count is 0. 

With these reasons, I initially decided to separate the ref count from the existing mute/unmut logic. But I do agree that it is confusing to have both in KafkaChannel. Having said that, would it make more sense if I maintain the ref count in SocketServer (by using a per-processor map from channel id to ref count) instead of KafkaChannel? What do you think?
"
188299694,4830,rajinisivaram,2018-05-15T14:04:47Z,"The problem with mute is that we already have too many different ways of controlling and tracking it, making the code really confusing. There is `KafkaChannel.muted`, `KafkaChannel.muteRefCount`, `KafkaChannel.isInMutableState()`, `Selector.explicitlyMutedChannels` and the actual interest ops of the selection key in the transport layer. I don't think we want a per-processor map containing channel ids in `SocketServer` since managing two lists of channels is just more work and could result in inconsistencies. The particular problem with `KafkaChannel.muteRefCount` is that if you are looking at `KafkaChannel`, then that field and the methods that go with it make no sense since you can have `muted=true, refCount=0`.

Would it be possible to convert `KafkaChannel.muted` to `KafkaChannel.muteState` with enum states like `NOT_MUTED`, `MUTED`, `RESPONSE_PENDING`, `THROTTLED`, `THROTTLED_RESPONSE_PENDING` or something along those lines with clear state transitions?"
188508062,4830,jonlee2,2018-05-16T05:42:19Z,Thank you for the suggestion. Makes a good sense. I replaced KafkaChannel.muted with KafkaChannel.muteState and remove the ref count. Transition of the muteState of each channel will be controlled by mute-related events reported by SocketServer (details mentioned in the comments).
188706191,4830,tedyu,2018-05-16T17:21:10Z,"What if mute state becomes ChannelMuteState.MUTED after the above call ?
Should unmute still be carried out ?"
188729990,4830,jonlee2,2018-05-16T18:35:52Z,"Yes. This is a NoOp response case, so if the mute state transitions to MUTED after the above call (meaning that there's no throttling in progress), we should unmute the channel. "
189442592,4830,lindong28,2018-05-19T19:17:08Z,nits: Return the number ...
189442599,4830,lindong28,2018-05-19T19:17:19Z,nits: Return the number ...
189442674,4830,lindong28,2018-05-19T19:21:24Z,Can you make it private?
189442716,4830,lindong28,2018-05-19T19:23:11Z,nits: personally I feel the string can be in the same line.
189448493,4830,lindong28,2018-05-20T01:10:34Z,"It seems that we will mute a channel after receiving request from client, and maybe unmute a channel after sending the response to client. So should the first two entries be renamed to `REQUEST_RECEIVED` and `RESPONSE_SENT` (with updated docs) respectively?"
189448703,4830,lindong28,2018-05-20T01:27:00Z,Is this change in the LeaderAndIsrResponse needed?
189448829,4830,lindong28,2018-05-20T01:36:54Z,Is this change needed?
189448832,4830,lindong28,2018-05-20T01:37:28Z,Is this change needed?
189448835,4830,lindong28,2018-05-20T01:37:43Z,Is this change needed?
189448844,4830,lindong28,2018-05-20T01:38:49Z,Is this change needed?
189448845,4830,lindong28,2018-05-20T01:39:00Z,Is this change needed?
189448878,4830,lindong28,2018-05-20T01:41:22Z,nits: it maybe slightly simpler to skip the `if ` statement
189448885,4830,lindong28,2018-05-20T01:41:27Z,nits: it maybe slightly simpler to skip the `if ` statement
189448890,4830,lindong28,2018-05-20T01:41:48Z,nits: it maybe slightly simpler to skip the `if ` statement
189448893,4830,lindong28,2018-05-20T01:41:52Z,nits: it maybe slightly simpler to skip the `if ` statement
189449055,4830,lindong28,2018-05-20T01:57:35Z,nits: `${quotaType}` can be replaced with `$quotaType`
189449166,4830,lindong28,2018-05-20T02:09:07Z,"Since we only unrecord quota sensor if the request is throttled, it may be better to skip checking `quotasEnabled` (or throw exception if quotasEnabled = false) and rename this method to `unrecordQuotaSensor`."
189449289,4830,lindong28,2018-05-20T02:20:40Z,It could be `val`.
189449309,4830,lindong28,2018-05-20T02:22:42Z,nits: It seems that we typically put the the body of the if statement in a new line?
189449419,4830,lindong28,2018-05-20T02:34:35Z,`shouldBeIncludedInResponse ` name may be a bit confusing because it does not tell whether this method can change state or not. Can we name it `maybeUpdateResponseData`?
189449460,4830,lindong28,2018-05-20T02:39:48Z,"Can we make this class private? 

And would it be better to add `type RESP_MAP_ITER = Iterator[util.Map.Entry[TopicPartition, FetchResponse.PartitionData]]` in FetchSession object, and replace the first parameter with `val iter: RESP_MAP_ITER`. This seems to be more consistent with the existing code patter and make it more obvious that the new class is a wrapper around the original iterator."
189450020,4830,lindong28,2018-05-20T03:11:43Z,Would it make the code a bit more readable by initializing unconvertedFetchResponse to an empty map here. By doing this we initialize unconvertedFetchResponse in roughly the same place and createResponse() does not need to do `if (throttleTimeMs > 0)`.
189450100,4830,lindong28,2018-05-20T03:18:04Z,nits: can we move `2.0-IV0` to be after `2.0`?
189474811,4830,jonlee2,2018-05-20T23:55:52Z,Done
189474816,4830,jonlee2,2018-05-20T23:55:58Z,Done
189474821,4830,jonlee2,2018-05-20T23:56:02Z,Done
189474823,4830,jonlee2,2018-05-20T23:56:06Z,Done
189474841,4830,jonlee2,2018-05-20T23:56:53Z,"Hmm.. I thought I already removed it here and other cluster action responses, but obviously not. Done."
189474845,4830,jonlee2,2018-05-20T23:56:59Z,Done
189474848,4830,jonlee2,2018-05-20T23:57:05Z,Done
189474850,4830,jonlee2,2018-05-20T23:57:11Z,Done
189474855,4830,jonlee2,2018-05-20T23:57:17Z,Done
189474857,4830,jonlee2,2018-05-20T23:57:22Z,Done
189474864,4830,jonlee2,2018-05-20T23:57:29Z,Done
189474865,4830,jonlee2,2018-05-20T23:57:35Z,Done
189474868,4830,jonlee2,2018-05-20T23:57:41Z,Done
189474869,4830,jonlee2,2018-05-20T23:57:48Z,Done
189474873,4830,jonlee2,2018-05-20T23:57:57Z,Done
189474877,4830,jonlee2,2018-05-20T23:58:03Z,Done
189474880,4830,jonlee2,2018-05-20T23:58:10Z,Done
189474885,4830,jonlee2,2018-05-20T23:58:15Z,Done
189474890,4830,jonlee2,2018-05-20T23:58:22Z,Done
189474891,4830,jonlee2,2018-05-20T23:58:27Z,Done
189475769,4830,jonlee2,2018-05-21T00:23:40Z,Done
189476185,4830,jonlee2,2018-05-21T00:34:28Z,"REQUEST/RESPONSE in this enum and the previous one refer to request/response between SocketServer and the API layer, not between the client and SocketServer. One reason why I chose this way was because we don't send out a response to the client when acks = 0.

But you are right that we try unmuting after sending out to the response to the client, so I updated as suggested. I still use RESPONSE_SENT for acks=0 case, but I think it should not be confusing (mentioned that in the comments).  "
462738230,9103,abbccdda,2020-07-30T05:07:29Z,You commented on the previous PR about the style here. The reasoning is that this is a more common style than having period at the end in our codebase.
463919020,9103,abbccdda,2020-08-01T04:11:47Z,Moved to `AlterConfigsUtil`
464029227,9103,abbccdda,2020-08-02T04:18:20Z,Moved to `AlterConfigsUtil`
464029411,9103,abbccdda,2020-08-02T04:21:07Z,"This is the new test, the rest of changes in this file are just side cleanups."
464241946,9103,dajac,2020-08-03T07:34:55Z,Could we use Optional for these two as they are not always provided?
464242434,9103,dajac,2020-08-03T07:36:02Z,nit: I would actually keep the callback as the last argument as it is a bit more natural to have the callback last.
464244756,9103,dajac,2020-08-03T07:41:05Z,nit: empty line could be removed.
464245796,9103,dajac,2020-08-03T07:43:25Z,"I personally prefer the previous indentation which is, I believe, more common in our code base. Or do we plan to adopt a new formatting?"
464246880,9103,dajac,2020-08-03T07:45:52Z,nit: Could we move it after `clientInformation` to keep the order inline with the order in the constructor?
464247255,9103,dajac,2020-08-03T07:46:42Z,Shall we use Optional here as well?
464247740,9103,dajac,2020-08-03T07:47:47Z,"Actually, we will also use it for quota. I think that we could say that both `InitialPrincipalName` and `InitialClientId` will be used for logging and quota purposes."
464248637,9103,dajac,2020-08-03T07:49:43Z,"As 2.7 has not be release yet, we don't need to introduce a new version. We can reuse `KAFKA_2_7_IV0`."
464248862,9103,dajac,2020-08-03T07:50:12Z,Shall we use Option here?
464249275,9103,dajac,2020-08-03T07:51:05Z,nit: That was already present before your change but could we remove the extra space before the colon?
464255551,9103,dajac,2020-08-03T08:04:21Z,The usage of the square brackets and the colon looks weird here. The audit log does not look like a sentence anymore. I wonder if we could go with something like this instead: `Principal = A on behalf of Principal = B is allowed...`. We could also put the initial principal name only if it is set.
464260158,9103,dajac,2020-08-03T08:14:21Z,nit: Remove extra space before `authorizedResources`.
464271174,9103,dajac,2020-08-03T08:36:17Z,"* I presume that this does not work if we use the same listener for bother the control plane and the data plane.
* I also wonder if it is a good thing to have this extension here as it applies to all the authorization in the Api Layer. I think that we should be cautious and only do this for forwarded requests."
464271735,9103,dajac,2020-08-03T08:37:25Z,I presume that this does not work if the broker uses the same listener for the control plane and the data plane.
464274377,9103,dajac,2020-08-03T08:42:43Z,"nit: `as admin client doesn't know how to find the controller` is not relevant anymore. What about the following: `When IBP is smaller than XYZ, forwarding is not supported therefore requests are handled directly`?"
464276809,9103,dajac,2020-08-03T08:47:21Z,"It looks like that we will propagate the `NOT_CONTROLLER` error back to the client. Is it intentional? As clients don't send this request to the controller (and new ones won't get the controller id anymore), it sounds weird to return them this error. We could perhaps return another generic error."
464277479,9103,dajac,2020-08-03T08:48:34Z,Have we considered using Scala functions as callbacks? It would be more aligned with the other callbacks that we have in Scala and also would avoid having to define classes for each handler that support forwarding. What do you think?
464292019,9103,dajac,2020-08-03T09:15:10Z,"For my understanding, I suppose that we don't verify that redirection is enabled here to ensure that the controller can accept forwarded requests as soon as one broker in the cluster is configured with IBP 2.7. Am I getting this right?"
464567004,9103,abbccdda,2020-08-03T17:44:56Z,It is not necessary as we don't check nulls for these fields.
464569040,9103,abbccdda,2020-08-03T17:48:54Z,"Not necessary, as explained."
464570322,9103,abbccdda,2020-08-03T17:51:19Z,"I don't think we need initial client id for audit logging, is there some other logging you have in mind?"
464582897,9103,abbccdda,2020-08-03T18:16:19Z,Will requests only flow to data plane if they use the same listener?
464583310,9103,abbccdda,2020-08-03T18:17:08Z,Not this is propagating to the sender broker.
464584005,9103,abbccdda,2020-08-03T18:18:30Z,"Yes, the purpose is to always handle a forwarding request even if IBP is not 2.7 yet. This is because some brokers may already upgrade their IBP and they start sending forwarding requests, which is totally legitimate."
464585176,9103,abbccdda,2020-08-03T18:20:47Z,Sg!
465272340,9103,dajac,2020-08-04T19:16:28Z,"Yeah, I was actually thinking about the request log. I thought that it may be useful to print them out there as well: https://github.com/apache/kafka/blob/trunk/core/src/main/scala/kafka/network/RequestChannel.scala#L229."
465273597,9103,dajac,2020-08-04T19:18:54Z,"Sorry, I was not clear. If the control plane listener is not configured, control requests will go to the data plane listener. Based on your last commits, it seems that you have figured that out."
465273903,9103,dajac,2020-08-04T19:19:35Z,Ack. I have missed the handling of `NOT_CONTROLLER` in the `BrokerToControllerChannelManager`.
465274018,9103,dajac,2020-08-04T19:19:49Z,Ack. This is what I thought.
465275440,9103,dajac,2020-08-04T19:22:33Z,"Actually, we check nulls for these two in `isForwardingRequest` method. I don't feel strongly about this but I usually better to use Optional when such values are not always present."
465908873,9103,dajac,2020-08-05T18:05:16Z,"I wonder if this is correct. Usually, we use `CLUSTER_ACTION` action with the `CLUSTER` resource. For instance, this is how we authorize control requests:
```
authorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)
```
I thought that we would do the same in this case. Don't we?"
466582281,9103,abbccdda,2020-08-06T17:45:49Z,"I'm not sure either, cc @rajinisivaram @cmccabe "
466714423,9103,abbccdda,2020-08-06T22:14:07Z,"Actually I think you are right, will change here."
467214507,9103,cmccabe,2020-08-07T18:54:46Z,"Can we get rid of whitespace-only changes like this, or at least move them to another PR?"
467215647,9103,abbccdda,2020-08-07T18:56:53Z,Let me check around.
473158779,9103,abbccdda,2020-08-19T16:26:12Z,Add equality check for the sake of easymock verification
475710400,9103,cmccabe,2020-08-24T15:43:18Z,"Need to include:
```
private static final long serialVersionUID = 1L;
```"
475712573,9103,cmccabe,2020-08-24T15:46:38Z,"How about: ""A broker failed to authorize itself to another component of the system.  This indicates an internal error on the broker cluster security setup"".

This isn't specific to forwarding... there might be other reasons why a broker would need to authorize itself and fail"
475714230,9103,cmccabe,2020-08-24T15:49:13Z,In general we don't define equals or hashCode on these builders.  Why are we defining it here?
475725389,9103,cmccabe,2020-08-24T16:06:10Z,just as a note the alter isr PR may also have an object like this.  so maybe we want a name which is more specific to redirection.
476860796,9103,abbccdda,2020-08-25T23:33:02Z,The purpose is for the mock tests to compare the expected builder in `KafkaApisTest`
476864248,9103,abbccdda,2020-08-25T23:36:10Z,"Interesting, why does the `AuthorizationException` have no `serialVersionUID`? Is it because we never use that error code explicitly?"
489875283,9103,abbccdda,2020-09-17T01:59:50Z,I'm still trying to decide how to make sure we could turn off the redirection in 2.7. Having a separate IBP for 3.0 may not work. @cmccabe 
493160927,9103,hachikuji,2020-09-23T02:31:05Z,"nit: might be useful to document the expectation that `resources` is a subset of the key set of `configs`. The signature surprised me a little bit.

As an aside, this kind of convenience conversion seems more appropriate for `IncrementalAlterConfigsRequest.Builder` rather than a static class."
493165520,9103,hachikuji,2020-09-23T02:48:53Z,Typically responses are immutable after construction. It seems kind of a brittle pattern to rely on being able to mutate the response we receive from the other broker. For example we inherit the throttle time which is a bit weird. Are we saving that much by not creating a new response?
493168061,9103,hachikuji,2020-09-23T02:58:59Z,"In general, the forwarded request may have a different version than the client request. I'm wondering if we should keep the version the same in case there are semantic differences. As an example, a newer version of the API may introduce unexpected error codes. Unless we have logic to convert those error codes, then we might break compatibility unexpectedly."
493168697,9103,hachikuji,2020-09-23T03:01:28Z,Get rid of this TODO. We do not need to remove IBP internal versions.
493169192,9103,hachikuji,2020-09-23T03:03:34Z,"nit: why don't we add a case class and make this optional. for example:

```scala
case class InitialPrincipal(name: String, clientId: String)
```
In addition to reducing parameters, that makes the expectation that both are provided explicit.
"
493169273,9103,hachikuji,2020-09-23T03:03:54Z,nit: space after `if`
493169694,9103,hachikuji,2020-09-23T03:05:42Z,Can you explain why this change is needed?
493170132,9103,hachikuji,2020-09-23T03:07:28Z,The comment doesn't seem to make sense here. Seems like the logic doesn't have anything to do with the controller?
493170494,9103,hachikuji,2020-09-23T03:09:02Z,This function has 3 callbacks... It would be nice if we could figure out how to pass through the `ForwardRequestHandler` directly.
493170831,9103,hachikuji,2020-09-23T03:10:24Z,nit: this is misaligned
493171357,9103,hachikuji,2020-09-23T03:12:42Z,"We can't guarantee that this broker will still be the controller when we call `process` or that the broker we're forwarding to will still be the controller when it receives the request. In these cases, we need to return some retriable error to the client. Can you help me understand how this is implemented?"
493171799,9103,hachikuji,2020-09-23T03:14:28Z,"nit: this is subjective, but this style is a bit ugly. I would prefer the following:
```scala
override def resourceSplitByAuthorization(
  createTopicsRequest: CreateTopicsRequest
): (Map[String, CreatableTopic], Map[String, ApiError]) = {
```
That makes it easier visually to separate the return type and the function logic (again, in my opinion)."
493172457,9103,hachikuji,2020-09-23T03:17:08Z,"nit: seems `handle` doesn't really need to be part of `ForwardRequestHandler`. Instead we could pull it out:
```scala
private def handle(handler: ForwardRequestHandler): Unit = {
...
```
The advantage of this is that it allows us to pull the type out of `KafkaApis` without inheriting all of the dependencies that are needed by `handle`."
493173723,9103,hachikuji,2020-09-23T03:22:14Z,It would be helpful to have a comment explaining this. It does not seem obvious.
493174084,9103,hachikuji,2020-09-23T03:23:45Z,"Good to see the unit tests in here. I think we also need at least a couple integration tests. For example, could we add something to `CreateTopicsRequestTest` to ensure that forwarding works as expected?"
493206068,9103,abbccdda,2020-09-23T05:28:51Z,"The primary reason is that we would trigger the disallowed import if we do it in the request builder:
```
[ant:checkstyle] [ERROR] /Users/boyang.chen/code/kafka/clients/src/main/java/org/apache/kafka/common/requests/IncrementalAlterConfigsRequest.java:20:1: Disallowed import - org.apache.kafka.clients.admin.AlterConfigOp. [ImportControl]
```
Let me check if we could make exceptions here"
493209974,9103,abbccdda,2020-09-23T05:41:23Z,"But in case we release AK 2.7, wouldn't this flag give user the confidence to upgrade to, which we don't want to happen?"
493518004,9103,rajinisivaram,2020-09-23T12:15:37Z,nit: `SSL` => `Ssl`
493518728,9103,rajinisivaram,2020-09-23T12:16:25Z,Does this get reset somewhere or will we keep adding `/`?
493519587,9103,rajinisivaram,2020-09-23T12:17:26Z,`SSL` => `Ssl`
493520160,9103,rajinisivaram,2020-09-23T12:18:05Z,"This means update was requested, but not necessarily that file has changed?"
493520165,9103,rajinisivaram,2020-09-23T12:18:05Z,"This means update was requested, but not necessarily that file has changed?"
493522186,9103,rajinisivaram,2020-09-23T12:20:21Z,Can't we put this logic in `DynamicBrokerConfig`?
493742677,9103,hachikuji,2020-09-23T16:50:32Z,I'm not sure I follow. Do you not want redirection to be part of 2.7?
493751026,9103,abbccdda,2020-09-23T17:03:58Z,"The rational is to trigger a reload of ssl store file by the ZK notification. @cmccabe @rajinisivaram came out this idea to augment the path to
```
//path//to//ssl//store//file
```
when a reload is requested on the receiver broker, and by propagating such a path other brokers would see a difference and thus reload their corresponding store files as well. In the meantime, we need to trim the path back to single slash after handling the notification:
```
/path/to/ssl/store/file
```"
493752855,9103,abbccdda,2020-09-23T17:07:06Z,"The logic is needed when there is an AlterConfigRequest targeting at a specific broker. Since the non-controller node will no longer handle AlterConfigs, it is possible to see a redirected changing request with a broker.id different than the controller broker.id."
493771144,9103,abbccdda,2020-09-23T17:37:09Z,"Yes, we would trim it in `trimSslStorePaths`"
493771267,9103,abbccdda,2020-09-23T17:37:24Z,Yea
493773324,9103,abbccdda,2020-09-23T17:41:05Z,"I feel it's more explicit to do it in here, as zk notification is the only target case."
493782410,9103,abbccdda,2020-09-23T17:56:03Z,I guess we could get rid of it and do the merge in caller level.
493795436,9103,abbccdda,2020-09-23T18:17:46Z,"It's a bit hard since we are passing requestBuilder all the way to NetworkClient, so if we want a designated version to build the request, that may involve some non-trivial changes."
493823776,9103,hachikuji,2020-09-23T18:56:42Z,"As discussed offline, we can pass the expected version down to the Builder. The abstract builder already supports an explicit range of versions. In any case, it doesn't seem like we have a choice.

By the way, one potential edge case here is that the broker receiving the request has upgraded to a later version than the controller. This would be possible in the middle of a rolling upgrade. I don't think there's an easy way to handle this. We could return UNSUPPORTED_VERSION to the client, but that would be surprising since the client chose a supported API based on ApiVersions and is not aware of the controller redirection.

One idea to address this problem is to gate version upgrades to redirectable APIs by the IBP. Basically all of these APIs have become inter-broker APIs through redirection so they need the safeguard of the IBP. Feels like we might have to do this."
504286524,9103,hachikuji,2020-10-13T22:08:10Z,nit: why don't we call it `requestData` to be consistent with the name used in the api spec?
504287723,9103,hachikuji,2020-10-13T22:10:46Z,nit: I think it might be better to pull this out of the request class. The direction we're moving is toward dumber request/response classes. Eventually `EnvelopeRequest` will go away and we'll just use `EnvelopeRequestData`.
504288659,9103,hachikuji,2020-10-13T22:12:59Z,Not sure why we need this change. I think the convention is to include `NONE` in error counts.
504293144,9103,hachikuji,2020-10-13T22:24:33Z,I'm wondering if we really need the IBP to leak into the common library. It should really only be a broker concern. Seems like the only point is so that we can continue to use the factory methods defined below from the broker code. Is that right? Could we instead move the factories to the broker?
504295024,9103,hachikuji,2020-10-13T22:29:23Z,"In a similar vein, I think it's better to not include serialization logic in the response object. It tends to hide some of the details like byte buffer allocation that we might want to control at another level. "
504295874,9103,hachikuji,2020-10-13T22:31:40Z,Same here. We can return `ByteBuffer` and leave parsing to higher layers.
504298663,9103,hachikuji,2020-10-13T22:39:21Z,"It is strange to couple the serialization of the principal with the version of the envelope request. This might help us in the case of default principal builder, but users with their own custom builder are on their own, right? I think it is better to be consistent and always leave versioning to the principal builder. "
504298879,9103,hachikuji,2020-10-13T22:39:55Z,nit: maybe print `forwardingPrincipal` only if it is defined
504300234,9103,hachikuji,2020-10-13T22:43:56Z,Do we have a use case for this yet? I don't see that it gets used anywhere.
504327985,9103,abbccdda,2020-10-14T00:12:51Z,I guess there are some inconsistency between different RPCs as I spotted cases excluding NONE. I would initiate a separate JIRA for the cleaning and revert the change here.
504341071,9103,abbccdda,2020-10-14T01:01:55Z,https://issues.apache.org/jira/browse/KAFKA-10607
504345664,9103,abbccdda,2020-10-14T01:19:44Z,"Not yet, could be removed."
504408633,9103,abbccdda,2020-10-14T05:23:43Z,"The tricky thing here is that if we handle the api version constraints on the broker side, it means we need to either make changes directly to the returned ApiVersionsResponse or spawn a new instance with applied constraints. That means leaking of the internal architecture of ApiVersionsResponse to the broker level and redundant conversions IMHO. The current approach makes sure the broker level logic is clean with only the necessity of passing the IBP number. "
505765664,9103,hachikuji,2020-10-15T18:47:39Z,What is the benefit of using a different error code instead of `CLUSTER_AUTHORIZATION_FAILURE`?
505768901,9103,hachikuji,2020-10-15T18:53:21Z,"I believe we need to set `requiresDelayedAllocation` for this API. Typically we will release the underlying buffer allocated for a request when `RequestChannel.Request` is constructed. However, since we are using ""zeroCopy,"" we need to hold onto the `ByteBuffer` reference until the API has been handled."
505798234,9103,hachikuji,2020-10-15T19:47:58Z,"It seems like we're trying to reuse this handler from the previous patch, but I'm not sure it still makes as much sense. A simpler structure might be something like the following:

```scala
  private def maybeForward(
    request: RequestChannel.Request,
    handler: RequestChannel.Request => Unit
  ): Unit = {
    if (!controller.isActive && config.redirectionEnabled && request.context.principalSerde.isPresent) {
      redirectionManager.forwardRequest(sendResponseMaybeThrottle, request)
    } else {
      // When IBP is smaller than 2.8 or the principal serde is undefined, forwarding is not supported,
      // therefore requests are handled directly.
      handler(request)
    }
  }

  // then invoked like this
override def handle(request: RequestChannel.Request): Unit = {
    try {
      trace(s""Handling request:${request.requestDesc(true)} from connection ${request.context.connectionId};"" +
        s""securityProtocol:${request.context.securityProtocol},principal:${request.context.principal}"")

      request.header.apiKey match {
      ...
        case ApiKeys.ALTER_CONFIGS => maybeForward(request, handleAlterConfigsRequest)
...


  // unchanged
  def handleAlterConfigs(request): Unit 
```"
505842904,9103,abbccdda,2020-10-15T20:56:55Z,"`CLUSTER_AUTHORIZATION_FAILURE` normally indicates a client side security configuration error. We intentionally define a separate error code to let admin know that there is some security config trouble with the brokers, not the clients."
505852939,9103,abbccdda,2020-10-15T21:08:41Z,I think we do have that logic enforced by setting `zeroCopy` to true for request data field in the RPC json.
509528009,9103,hachikuji,2020-10-21T18:01:43Z,Not sure I follow. All current inter-broker APIs are gated by `ClusterAction` and will return `CLUSTER_AUTHORIZATION_FAILURE` if the principal does not have access. There is no distinction between clients and brokers. It's not clear to me why we need something different here.
509533968,9103,hachikuji,2020-10-21T18:06:33Z,"Rather than assuming highest supported version, we should include the version in the serialized data. The simple thing would be to write the version first, then write the payload."
509537587,9103,hachikuji,2020-10-21T18:09:44Z,nit: can we move this back to where the request parsing logic is. Otherwise it becomes a bit hidden.
509538545,9103,hachikuji,2020-10-21T18:10:43Z,"nit: add braces to all of these methods. Even though they are not required, braces make it easier to see the scope"
509540617,9103,hachikuji,2020-10-21T18:12:56Z,nit: use `match`
509541377,9103,hachikuji,2020-10-21T18:13:43Z,nit: use `match`
509545893,9103,hachikuji,2020-10-21T18:19:31Z,nit: this is misaligned. It might be better to pull the body here into a separate method (e.g. `parseEnvelopeRequest`)
509550124,9103,hachikuji,2020-10-21T18:24:02Z,"We should have a check at the beginning of `handle` to restrict the ""forwardable"" APIs. "
509550802,9103,hachikuji,2020-10-21T18:24:48Z,"We use 'forward' and 'redirect' interchangeably throughout the PR, but the names do suggest different behavior. In my mind 'redirection' suggests that we are telling the client to go somewhere else, while 'forward' suggests that the broker is passing the request through to its destination. So maybe we can stick with 'forward' consistently (e.g. `isForwardingEnabled`)?"
509553533,9103,hachikuji,2020-10-21T18:28:14Z,"As mentioned above, you can see the rest of the cases in this class where we check CLUSTER_ACTION and they all return `CLUSTER_AUTHORIZATION_FAILURE`."
509555177,9103,hachikuji,2020-10-21T18:29:41Z,nit: you can just use `channel.principalSerde.asScala`
509562013,9103,hachikuji,2020-10-21T18:35:40Z,"We want to avoid this serialization since it introduces the possibility for the request to be altered by the forwarding broker. The `RequestChannel.Request` object retains the reference to the original buffer, which we can use here, but we need to tell the channel to delay releasing the buffer using `ApiKeys.requiresDelayedAllocation` for all of the ""forwardable"" APIs."
509565018,9103,hachikuji,2020-10-21T18:38:58Z,Use `defineInternal`
509565555,9103,hachikuji,2020-10-21T18:39:36Z,How about `enable.metadata.quorum`?
509645115,9103,abbccdda,2020-10-21T20:09:07Z,Had a try but it seems java Optional doesn't have an `asScala` option
509670268,9103,abbccdda,2020-10-21T20:31:29Z,Sounds good.
510533897,9103,hachikuji,2020-10-23T00:37:04Z,"You probably need the following:
```scala
import scala.compat.java8.OptionConverters._
```"
510536894,9103,hachikuji,2020-10-23T00:49:41Z,"I was thinking a little bit about this and trying to decide if the envelope request should have a more literal representation of the client ip address. The way it is working right now, it looks like the following:

1) Use `Socket.getInetAddress` to populate `RequestContext.clientAddress`.
2) Use `InetAddress.getHostName` to populate the `clientHostName` field in the envelope request. This will do a reverse dns lookup based on the IP address from 1).
3) Now we send `clientHostName` over the wire. It gets unpacked here by doing a dns lookup to get to the `InetAddress` object.

So it seems we should be skipping the dns translation and just using the IP address from 1). The `InetAddress` class gives us `getAddress` and `getHostAddress`. The first provides the raw byte representation of the ip address, while the latter provides a textual representation. I am thinking we should use `getAddress` and let this field be represented as bytes. What do you think?"
510540720,9103,hachikuji,2020-10-23T01:06:42Z,"Can we move some of the checks from `maybeForward` here? This is the flow I'm thinking about:

1. First check authorization => CLUSTER_AUTHORIZATION_FAILURE
2. Verify forwarding is enabled => INVALID_REQUEST
3. Verify the api is forwardable => INVALID_REQUEST 

If all of these pass, then the request continues down the normal handling path."
510552124,9103,hachikuji,2020-10-23T01:56:06Z,"Quotas are one aspect of this work that need more consideration. What we don't want is for the inter-broker channel to get affected by the individual client throttle, which is what will happen with the current patch. What I'd suggest for now is that we allow the broker to track client quotas and pass back the throttle value in the underlying response, but we set the envelope throttle time to 0 and ensure that the inter-broker channel does not get throttled. 

For this, I think we we will need to change the logic in `KafkaApis.sendResponseMaybeThrottle`. If it is a forwarded request, we still need to check `maybeRecordAndGetThrottleTimeMs`, but we can skip the call to `ClientQuotaManager.throttle`. When the response is received on the forwarding broker, we will need to apply the throttle, which I think the patch already handles.

One challenging aspect is how this will affect quota metrics. Currently quota/throttling metrics are relatively simple because they are recorded separately by each broker. However, here the controller is the one that is tracking the throttling for the client across multiple inbound connections from multiple brokers. This means that the broker that is applying a throttle for a forwarded request may not have actually observed a quota violation. Other than causing some reporting confusion, I am not sure whether there are any other consequences to this.

cc @apovzner @rajinisivaram "
510554587,9103,hachikuji,2020-10-23T02:06:38Z,"One challenge we have here is that there are two levels of errors. The current patch seems to conflate the two, which makes it confusing. I think we need a structure which allows us to separate the errors possible at the envelope level and those possible at the request level. What I'm thinking is this:

1. For cluster auth and principal serde errors, we should return the envelope error and null response body.
2. For everything else, we return envelope error NONE and just pass through whatever error is in the response.

Does that make sense?"
510568706,9103,abbccdda,2020-10-23T03:06:57Z,"The question would be how the forwarding broker should do the error handling for auth & principal serde exceptions. To me we should get a vanilla error response with `UNKNOWN_SERVER_ERROR` and get back to the original client? Besides that, I think we could add a differentiation here to avoid passing the serde-type errors to the client."
511007228,9103,abbccdda,2020-10-23T16:39:28Z,"For pt2, if the forwarding is not enabled on the active controller, but it has the capability, should we just serve the request?"
511012913,9103,abbccdda,2020-10-23T16:49:50Z,So the proposal is simply for saving the unnecessary dns translation? Not sure if representing as bytes would also serve the security purpose as well.
511872018,9103,rajinisivaram,2020-10-26T10:52:19Z,"I guess the only quota that is affected for the RPCs we currently forward is request quotas. Totally agree that we shouldn't throttle inter-broker connections. There are a few other things to consider here:

1) Every forwarded request uses network thread and request handler time on two brokers. Are we saying that we can ignore the time spent on the forwarding broker because that is negligible? In a deployment with SSL on the external listener and PLAINTEXT on the inter-broker listener, there may be more network thread time used on the forwarding broker rather than the controller. Do we record these, but use the controller throttle time for throttling?
2)  Are we changing the semantics of quotas? For example, if a client sends a request1 to leastLoadedNode A which mutes the connection and then sends request2 to leastLoadedNode B that happens to be the controller, we would mute that connection too. Another client with the same principal would get muted on B, but not A because A's quota hasn't been violated. I think this should be ok, though a bit confusing.
3) Are these measures good enough to protect the controller? This is the one that needs some more thought. Request quotas are configured to allocate a percentage of thread usage to each principal. Our quotas aren't very good at protecting against DOS attacks, but they help to limit usage for normal clients using the APIs. So if we can make sure the implementation for forwarded requests can handle this case, it would be good enough. In the old world, a client doing a lot of config updates would have just distributed the load across brokers as each node was throttled. Now, we distribute the iniital request across brokers as controller decides to throttle. Total rate for these requests across the cluster is dramatically reduced because all load is now on the controller. But from the controller broker's point of view, we are now allowing more requests through for the same quota from every client because a client can forward through `n` brokers. @apovzner may have more context on whether these request types actually hit request quotas in real deployments."
513608319,9103,hachikuji,2020-10-28T16:56:18Z,"Hmm.. It looks like we do not serialize the response header, but I think we probably should. Today it only includes the correlationId, but who knows how it will evolve in the future? Since we do serialize the request header, it seems better to be consistent. "
513612935,9103,hachikuji,2020-10-28T17:02:38Z,"Since this is a public API, it's worth documenting that these apis should raise a consistent error, such as `SerializationException`, in case of an error."
513613435,9103,hachikuji,2020-10-28T17:03:23Z,nit: for the the purpose of inter-broker forwarding
513614262,9103,hachikuji,2020-10-28T17:04:30Z,We may as well add a check here for the version so that we get a useful error in case we receive a version that we do not support.
513615916,9103,hachikuji,2020-10-28T17:06:37Z,nit: use upper-case `TokenAuthenticated` for consistency with other fields
513616503,9103,hachikuji,2020-10-28T17:07:26Z,Might be worth mentioning `org.apache.kafka.common.security.authenticator.DefaultKafkaPrincipalBuilder` explicitly.
513617300,9103,hachikuji,2020-10-28T17:08:33Z,"Perhaps add a little more detail?

> Whether the principal was authenticated by a delegation token on the forwarding broker"
513619656,9103,hachikuji,2020-10-28T17:11:52Z,"Since principals should be small, it is tempting to just use simple byte arrays for this interface. This is typically simpler for users and gives us a stronger boundary between plugin and broker code. "
513635843,9103,hachikuji,2020-10-28T17:34:28Z,"It looks like these changes made it to 2.7. We need to revert them before the release or it will not be safe to remove them. The danger is that we might use these tag ids for another purpose in the future, which will break the request parsing."
513645474,9103,hachikuji,2020-10-28T17:48:22Z,I guess this shows an inconsistency between the envelope and the other inter-broker APIs. The throttle time field is only useful if we actually expect the forwarding broker to respect it and backoff. I wonder if we should just be consistent for now and leave this out. 
513646769,9103,hachikuji,2020-10-28T17:50:11Z,"Would it make sense to add a default rule? If the api is forwardable, then we can assert it requires delayed deallocation."
513647237,9103,hachikuji,2020-10-28T17:50:49Z,"In fact, the schema doc says that the response header should be included."
513648122,9103,hachikuji,2020-10-28T17:52:10Z,It's not clear to me why we need to do this now since we are not enabling forwarding yet.
513659883,9103,hachikuji,2020-10-28T18:09:37Z,"Hmm.. The request logging will not be too useful if we cannot see what is in the embedded request and response. I think we should print the envelope structures separately. Longer term, we should figure out how to incorporate the envelope into https://cwiki.apache.org/confluence/display/KAFKA/KIP-673%3A+Emit+JSONs+with+new+auto-generated+schema. "
513662914,9103,hachikuji,2020-10-28T18:14:32Z,Not sure why this was resolved. I don't see the check. Basically the first thing we should do in `handle` is check whether we have an envelope request and if it is authorized.
513667101,9103,hachikuji,2020-10-28T18:21:14Z,"Unless the internal config is present, I think we should treat the envelope as non-existing. Once we are ready to enable it in the IBP, then we will accept the envelope request even if the local IBP is not high enough."
513743473,9103,abbccdda,2020-10-28T20:35:38Z,I think it's ok to remove this flag for now.
513750141,9103,abbccdda,2020-10-28T20:48:19Z,"I was under the impression that byte buffer provides more information such as a read position and capacity/limits, which makes the deserialization easier. If given a byte[], I'm afraid they need to convert to byte buffer internally eventually."
513757978,9103,abbccdda,2020-10-28T21:02:19Z,"Sounds good, will remove the throttle time field from the Envelope
"
513759802,9103,abbccdda,2020-10-28T21:05:21Z,"Sg, but I guess we need to keep it as is for now to try using the correct api version."
513826471,9103,abbccdda,2020-10-28T23:44:21Z,"Sg, will initiate a PR for that."
515212449,9103,hachikuji,2020-10-30T16:11:00Z,"Probably the first thing we should check is `isForwardingEnabled`. If it is not, I suggest we close the connection, which is basically the broker's way of saying ""I don't know how to handle this."""
515215364,9103,hachikuji,2020-10-30T16:16:02Z,Can we add a description explaining what this is for?
515219198,9103,hachikuji,2020-10-30T16:22:25Z,We should duplicate the buffer instead of modifying it directly.
515219726,9103,hachikuji,2020-10-30T16:23:22Z,"We can leave this for a follow-up, but it would be nice if we could avoid this deserialization (and the subsequent re-serialization). "
515220480,9103,hachikuji,2020-10-30T16:24:37Z,Probably useful to explain why we do this. A debug log message with the original error would be helpful as well.
515229180,9103,hachikuji,2020-10-30T16:39:03Z,"I think this was one of my initial questions, but do we have a timeout for the request? Looking at the current logic in `handleResponse`, it seems like we will just retry indefinitely. That is probably what we want for requests generated by the broker (e.g. `AlterIsr`), but it is not so useful for client requests since the client itself will eventually give up and send a new request."
515230020,9103,hachikuji,2020-10-30T16:40:21Z,nit: can we create a helper for `request.envelopeContext.isEmpty`? Perhaps we can write this as `!request.isForwarded`?
515231023,9103,hachikuji,2020-10-30T16:42:03Z,"Hmm.. I had assumed we would be using the same channel manager. Can you explain why we need two? In the end, I think all of the requests get serialized on the controller, so I'm not sure we're buying much."
515245665,9103,hachikuji,2020-10-30T17:04:46Z,"As far as I can tell, the `callback` here is unused. Tracing this back to `KafkaApis`, the callback passed to `sendResponseMaybeThrottle` also appears to be unused. I think we can remove it from both APIs and simplify this a bit."
515276738,9103,abbccdda,2020-10-30T17:48:18Z,"I agree we don't have a prioritization system on the controller yet, but in long term having two separate managers mean we don't block AlterISR unnecessarily, which seems to be definitely a higher priority message. cc @mumrah @cmccabe "
515377453,9103,abbccdda,2020-10-30T20:51:51Z,Sg
515388258,9103,abbccdda,2020-10-30T21:20:47Z,"Yea, I think this could be done as a follow-up. Filed: https://issues.apache.org/jira/browse/KAFKA-10667"
515389068,9103,abbccdda,2020-10-30T21:23:12Z,Filed: https://issues.apache.org/jira/browse/KAFKA-10668
515395403,9103,abbccdda,2020-10-30T21:41:50Z,Got a follow-up ticket as well: https://issues.apache.org/jira/browse/KAFKA-10348
515413173,9103,hachikuji,2020-10-30T22:44:52Z,Can we use `closeConnection`. We do not want to even acknowledge that the api exists unless forwarding is enabled.
515414500,9103,hachikuji,2020-10-30T22:50:24Z,nit: drop parenthesis for simple getter 
515417787,9103,hachikuji,2020-10-30T23:05:00Z,"This begs the question whether the api should even be advertised from non-privileged listeners if users cannot access it. I am thinking we can make this case similar to the behavior if forwarding is not enabled.  Here we can use this logic:
```
if (!config.forwardingEnabled || !request.context.fromPrivilegedListener) {
  closeConnection(request, util.Collections.emptyMap())
} else if (!authorize(request.envelopeContext.get.brokerContext, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {
  throw new ClusterAuthorizationException(s""Envelope request $request is not authorized"")
} 
```
Similarly, we can change the check in `ApiVersion.apiVersionsResponse` so that it skips the ENVELOPE API if the request is not from a privileged listener."
515418503,9103,hachikuji,2020-10-30T23:08:15Z,Not sure I follow the point about the correct api version.
515421189,9103,hachikuji,2020-10-30T23:21:24Z,"I think this logic still conflates the envelope error and the inner response error. We might catch an exception raised from `validateForwardRequest` or from the request handler in `KafkaApis.handle`. Both paths flow through `KafkaApis.handleError`, so we do not have a way to distinguish the two cases. This means that an uncaught error from the underlying request will get sent back to the forwarded broker as an error in the envelope, which will cause us to translate it to UNKNOWN_SERVER_ERROR.

I think we should handle envelope errors explicitly through a separate method. We can define a method here such as `buildFailedEnvelope` which can be used inside `validateForwardRequest`. Then inside `buildResponse` here, we can always return `Errors.NONE` as the envelope error."
515421553,9103,abbccdda,2020-10-30T23:23:16Z,"It's a bit tricky to do it here since we rely on exception catching to skip all the rest of handling logic, not sure it is worth to add this special case and do `if-else` to incur a large code change."
515427912,9103,hachikuji,2020-10-30T23:57:37Z,"Perhaps it is obvious, but this logic does not give us any tight guarantees that the request will actually be handled by the broker that is currently the controller. For example, a new controller might get elected between the check in `validateForwardRequest` and the handler here. That is probably fine at the moment, because the zk logic in `AdminManager` can execute on any broker. 

If we imagine instead how this will work with the kip-500 controller, I think the incoming request will get put on the controller's queue. By the time the request gets dequeued, we will be able to know for sure whether this node is the controller or not, so we will be able to have a much better guarantee.

The only reason I bring this up is that we are currently assuming that the NOT_CONTROLLER gets propagated in the envelope error field. We'll have to keep this in mind when we adapt this logic for the new controller."
515428385,9103,hachikuji,2020-10-31T00:00:26Z,Perhaps we could return a boolean to indicate whether the handling logic should execute. I think it is important to avoid exposing this api until we're ready for it.
516344965,9103,hachikuji,2020-11-02T23:57:03Z,nit: seems this change was not needed
516349664,9103,hachikuji,2020-11-03T00:06:25Z,"nit: I feel `FailureException` is redundant. Can we just call it `PrincipalDeserializationException`?

Also, I am not sure about this extending `AuthorizationException`. I would consider it more of an invalid request than an authorization failure, though the effect is the same. I think it's probably better to avoid categorizing it and just let it extend `ApiException`."
516356565,9103,hachikuji,2020-11-03T00:21:28Z,nit: every other property name uses a capital first letter
516363883,9103,hachikuji,2020-11-03T00:37:20Z,"It is quite expensive to parameterize these test cases. I am not sure it is worthwhile. If forwarding works for one of these cases, why would the others be different? Since we are not planning to enable this feature yet, I think unit tests in `KafkaApisTest` and maybe one integration test are good enough."
516364497,9103,hachikuji,2020-11-03T00:38:40Z,"I think it would be simpler to short-cut return.

```scala
if (request.isForwarded && !validateForwardRequest(request))
  return
```"
516365803,9103,hachikuji,2020-11-03T00:41:17Z,nit: `validatedForwardedRequest`
516366191,9103,hachikuji,2020-11-03T00:42:10Z,"nit: we are doing more than building the response here, we are sending it. How about `sendFailedEnvelopeResponse`?"
516372020,9103,hachikuji,2020-11-03T00:54:27Z,"nit: instead of `original`, could we use `forwarded` in these names?"
516373262,9103,hachikuji,2020-11-03T00:56:55Z,nit: define return type
516375303,9103,hachikuji,2020-11-03T01:00:58Z,Can you add a javadoc for these methods and mention `@throws SerializationException`?
516378459,9103,hachikuji,2020-11-03T01:09:12Z,"> I was under the impression that byte buffer provides more information such as a read position and capacity/limits, which makes the deserialization easier.

Hmm, not sure I get your point. Nothing is simpler than a byte array. The main question is whether we want to expose the actual request buffer to the plugin, especially since we still plan on using it afterwards. The plugin is treated as a trusted component in any case, so it might not make a big difference. Probably we should optimize here for simplicity.

> If given a byte[], I'm afraid they need to convert to byte buffer internally eventually.

That may or may not be true. If it is, users can just use `ByteBuffer.wrap`."
516381090,9103,hachikuji,2020-11-03T01:19:18Z,nit: `network` prefix is not needed since we are already in this package
516933194,9103,hachikuji,2020-11-03T20:20:55Z,"This inherits all tests from `DynamicBrokerReconfigurationTest`, which doesn't look to be intended. Can we just remove it? We can add it back once we get to testing the ssl path changes. For now I think the simple integration test for CreateTopics is good enough.

(By the way, it's curious that `testTrustStoreAlter` still passes even after we have removed the path update logic.)"
516933962,9103,hachikuji,2020-11-03T20:22:34Z,Do we need this change anymore?
516934696,9103,hachikuji,2020-11-03T20:24:08Z,I don't think we want to make this the default until we are ready to enable it. I would suggest we create a new `ForwardRequestTest` which extends `BaseRequestTest`. Then we can move the test case from `CreateTopicsRequestTest`.
516938814,9103,hachikuji,2020-11-03T20:32:19Z,nit: is this change needed?
516942223,9103,hachikuji,2020-11-03T20:39:33Z,Is this change needed? I am not sure I follow the comment about the privileged listener. That shouldn't affect ACLs I think.
516950684,9103,abbccdda,2020-11-03T20:56:53Z,Seems ok to remove
516951766,9103,abbccdda,2020-11-03T20:59:06Z,"Yea, that's weird, let's move to the next PR for a discussion."
37318079,132,hachikuji,2015-08-18T16:09:35Z,"4,2,0?
"
37320190,132,hachikuji,2015-08-18T16:28:11Z,"It might make this code a little easier to follow if you split the rack-aware and default assignments into separate functions. What do you think?
"
37323035,132,hachikuji,2015-08-18T16:52:26Z,"Maybe getInverseMap instead?
"
37441970,132,allenxwang,2015-08-19T17:24:41Z,"It will be difficult to separate them out as they actually share quite a lot of common logic, specifically around choosing the leader of the partition. The code change may seem a lot but actually very little for the default assignment algorithm other than changing the name of `brokerList` to `arrangedBrokerList`. 

I can try separate out the logic of choosing followers into different functions for default vs. rack aware assignment and see how it looks like.
"
37442156,132,allenxwang,2015-08-19T17:26:15Z,"That's correct. Will fix.
"
37442594,132,allenxwang,2015-08-19T17:30:10Z,"sure.
"
37448769,132,hachikuji,2015-08-19T18:19:45Z,"Yep, there would probably be some redundancy, but at least the default path would be uncluttered with all the rack-aware logic. I don't think it's too bad as it is, but clearer separation would be nice if possible.
"
49287039,132,joestein,2016-01-11T02:48:05Z,"can you add some negative testing please, folks do weird and odd things in their properties by accident and we want to guard against that too, etc
"
49287098,132,joestein,2016-01-11T02:50:07Z,"something about the scala of this makes me want to say it should be an implicit, that is a much bigger topic and change so i would say maybe not introduce that now but here is one of a lot of places we could without losing readability or performance reduce code. Maybe even try it with this change as your converting type only in RacLocator
"
49287153,132,joestein,2016-01-11T02:52:34Z,"rack-locator might be a bit confusing to the user when just coming and looking at the new command  / api changes in a release. why not rack-aware or rack-placement-class (keep the SimplaRackLocator class as is) and then rack-placement-properties? or something of the sort?
"
49287181,132,joestein,2016-01-11T02:53:34Z,"what/why are we ignoring here? not looking at entire class just seeing the diff hard to say if this makes sense or not to ignore
"
54477992,132,hachikuji,2016-02-29T21:08:46Z,"nitpick: maybe should be RACK_KEY_NAME for consistency?
"
54478309,132,hachikuji,2016-02-29T21:10:57Z,"Also, the comment doesn't add much. Maybe you can just relocate under ""EndPoint key name""?
"
54489948,132,allenxwang,2016-02-29T22:26:45Z,"I don't think rack belongs to EndPoint. It is the same level as ""EndPoint"" as indicated in the JSON format of broker in ZooKeeper and UpdateMetadataRequest protocol. A broker can have multiple EndPoints but only one rack.
"
54490437,132,hachikuji,2016-02-29T22:30:12Z,"Yep, you are right. Guess it would make sense for it to go under broker key names then?
"
54498311,132,allenxwang,2016-02-29T23:28:28Z,"Sounds good.
"
54592298,132,granthenke,2016-03-01T16:26:46Z,"Do we need to support null? Would empty string work well enough and avoid null checks throughout the code?
"
54592311,132,granthenke,2016-03-01T16:26:50Z,"Related to my protocol question above. Would defaulting to empty string work?
"
54592316,132,granthenke,2016-03-01T16:26:51Z,"Could use the constructor that doesn't take a rack. 
"
54592322,132,granthenke,2016-03-01T16:26:53Z,"Could use the constructor that doesn't take a rack. 
"
54592337,132,granthenke,2016-03-01T16:26:58Z,"Is brokerList used here?
"
54592349,132,granthenke,2016-03-01T16:27:03Z,"Seams I may need to use this in KIP-4. Which means it would need to live in the clients library under the common package. Could this be a java enum there?
"
54592380,132,granthenke,2016-03-01T16:27:09Z,"Is this includeRack boolean used anywhere?
"
54592390,132,granthenke,2016-03-01T16:27:10Z,"Is this includeRack boolean used anywhere?
"
54592404,132,granthenke,2016-03-01T16:27:15Z,"Are there unsafe characters that could be in the rack string that would break the json read/write?
"
54624794,132,hachikuji,2016-03-01T19:52:38Z,"Is this only public for testing? Would protected or default also work?
"
54627527,132,hachikuji,2016-03-01T20:10:36Z,"Would ""Safe"" be a better default? Looks like the default is only used in test cases, so maybe it would be better to always require the argument?
"
54645170,132,allenxwang,2016-03-01T22:13:58Z,"Broker.sizeInBytes() and Broker.writeTo() was used for serialization and deserialization of UpdateMetdataRequest when I started the PR. That's why I have to add includeRack parameter for version compatibility.  It was recently changed to use the Java code in kafka.common for this. But Broker.readFrom and Broker.writeTo remain in the code. So I am not sure if they are still needed. If not, we can delete this part of code all together.
"
54645719,132,allenxwang,2016-03-01T22:17:44Z,"I am not sure. This is only used when replica assignment is needed. If the only change in client library is to be able to access rack in TopicMetadataRequest/Response, then this can stay in scala in kafka.admin.
"
54645900,132,allenxwang,2016-03-01T22:18:59Z,"I would think any character is fine.
"
54646069,132,granthenke,2016-03-01T22:20:14Z,"TopicCommand is taking this as a parameter when creating a topic. Assuming the options is important. When CreateTopic calls go through the broker I will need to pass this option in the request. 
"
54646230,132,allenxwang,2016-03-01T22:21:21Z,"No, it is used in ControllerChannelManager and has to be public.
"
54648019,132,allenxwang,2016-03-01T22:33:44Z,"""Safe"" is only used in auto topic creation. In command line tools, we would like to be strict about using rack (to catch mis-configured rack) unless the user wants to disable it. This was discussed in KIP process.

The reason to make this argument optional is that in most cases, user would supply rack for all brokers or no rack for any broker which can be handled automatically in ""Enforced"" mode. Then createTopic can remain the same signature so that caller of this method does not need to be concerned about rack aware.
"
54648159,132,allenxwang,2016-03-01T22:34:44Z,"sure.
"
54649101,132,allenxwang,2016-03-01T22:43:27Z,"I discussed this in KIP discussion. NULLABLE_STRING was recommended in the discussion.

I think it makes sense as rack itself is designed to be nullable (Option[String]). It is legal to define rack as an empty string. 

There isn't really any null checks in the code as far as I can tell. null just means no rack is defined.
"
54649841,132,hachikuji,2016-03-01T22:49:22Z,"Is that because we're depending on this constructor for version 1? I know we depend on choosing the right constructor in other request objects to get the right version, but I wonder if it would be better to have explicit static factory methods (e.g. `UpdateMetadataRequest.createV0()`)?
"
54649887,132,allenxwang,2016-03-01T22:49:41Z,"It's not. I will remove it.
"
54650027,132,granthenke,2016-03-01T22:50:37Z,"I checked the java doc for `Json.encode`. It says `This method does not properly handle non-ascii characters.` I am not sure how ""bad"" it fails. 

Some basic limitations/validation on available rack characters and length might help prevent unforeseen issues. Something similar to the limitations for a topic name maybe.
"
54663838,132,allenxwang,2016-03-02T00:57:52Z,"@granthenke If topic creation is available from clients, then we need to pass RackAwareMode in the request. In that case I agree this class should be in common package as enum. Do you want me to make this change? Does the protocol support enum?
"
54664542,132,allenxwang,2016-03-02T01:06:12Z,"We depend on this constructor to create version 1 and 2 UpdateMetadataRequest, and possibly for future versions as well.
"
54667264,132,hachikuji,2016-03-02T01:39:53Z,"Fair enough. I was only wondering if there was a way to keep the version better encapsulated (like all of the other requests). Perhaps at least there should be a check on the version to make sure it is greater than 1? I might even enforce only version 1 and 2 since we'll almost certainly have to touch this code anyway if there is another version bump.
"
54668363,132,hachikuji,2016-03-02T01:53:29Z,"Makes sense, thanks.
"
54807092,132,allenxwang,2016-03-02T22:53:29Z,"I will add the check for version. I believe 0 is still supported so 0, 1 and 2 should be allowed.
"
54811738,132,hachikuji,2016-03-02T23:30:47Z,"@allenxwang Actually it's probably fine as it is since we would raise an error in `ProtoUtils.requestSchema()`. I didn't notice that this also supports version 0, so would it make sense change ControllerChannelManager to use this constructor for all cases. (And apologies for all this nitpicking)
"
54812799,132,ijuma,2016-03-02T23:39:22Z,"By the way, for a bit of history, I initially proposed having a single constructor with a version when I introduced this class. However, @junrao preferred having separate constructors with all, but the most recent version deprecated.
"
54813357,132,hachikuji,2016-03-02T23:44:08Z,"I think my preference would probably be to have static factory methods with the versions included in the name. Using constructors is kind of annoying because you have to check the comment to make sure you get the right one. 
"
54813814,132,ijuma,2016-03-02T23:48:37Z,"Yeah, we should use more static factories and less overloaded constructors in Kafka.
"
54815295,132,allenxwang,2016-03-03T00:02:35Z,"I don't really know what would be the valid characters or length limit for rack. Looking at the implementation of Json.encode() there is nothing suspicious how characters are handled.

Note that in some cases, rack can be a logical name and used for grouping brokers for fault tolerance. So any character is possible.

Apache Cassandra does not seem to do any validation on rack name for their property file based configuration.

If there is no specification or usual convention for the rack name, I suggest we leave it unchecked.
"
54817202,132,allenxwang,2016-03-03T00:22:24Z,"@ijuma @hachikuji Would you mind if I leave this code refactoring of constructors to you guys? 
"
54817893,132,ijuma,2016-03-03T00:28:21Z,"Fine by me.
"
54817929,132,hachikuji,2016-03-03T00:28:39Z,"Ditto
"
54838116,132,junrao,2016-03-03T05:41:11Z,"Could we fix the alignment?
"
54838129,132,junrao,2016-03-03T05:41:23Z,"alignment
"
54838137,132,junrao,2016-03-03T05:41:37Z,"We should probably mark this as deprecated.
"
54838143,132,junrao,2016-03-03T05:41:40Z,"an -> a
"
54838189,132,junrao,2016-03-03T05:42:43Z,"By leader, do you mean preferred leader? The first replica is not always the leader. Perhaps it's clearer to just refer to them as 1st replica, the rest of replicas, etc.
"
54838197,132,junrao,2016-03-03T05:42:53Z,"Would it be better to combine brokerList and rackInfo and pass in a Seq of BrokerMetadata that includes id and rack?
"
54838204,132,junrao,2016-03-03T05:43:00Z,"Can reverseMap(rack).toIterator just be list.toIterator?
"
54838213,132,junrao,2016-03-03T05:43:13Z,"Could we use case instead of tuple to make it clearer? Ditto below.
"
54838217,132,junrao,2016-03-03T05:43:17Z,"Should we sort the broker list per rack?
"
54922060,132,allenxwang,2016-03-03T18:16:50Z,"rackInfo here can be different from the actual broker-rack mapping. In case some brokers have rack and some brokers do not have rack, AdminUtils.getBrokersAndRackInfo() will modify the mapping depending on how strict we want to be rack aware. It will also return empty Map if user does not want rack aware. I think it is better to have higher level API (like createTopic()) to be influenced by the rack aware mode depending on the situation and user input and leave this assignment API free of that influence.
"
54928901,132,junrao,2016-03-03T18:59:57Z,"We will still need a separate constructor for v1 of UpdateMetadataRequest since in ControllerChannelManager, we may need to send a v1 request depending on inter.broker.protocol.
"
54928926,132,junrao,2016-03-03T19:00:07Z,"Now that we are returning the assignment, it's a bit weird to print the assignment to stdout. Perhaps we should let the caller do that.
"
54928999,132,junrao,2016-03-03T19:00:36Z,"It seems that readFrom and Broker.writeTo are only used in tests now since the serialization of UpdateMetadataRequest is based on the one in o.a.k. Instead of maintaining the logic here, could we just remove readFrom and Broker.writeTo and the corresponding test code?
"
54929005,132,junrao,2016-03-03T19:00:38Z,"unused import
"
54929073,132,junrao,2016-03-03T19:01:01Z,"We will need to construct v1 and v2 request using different constructors. See comment in UpdateMetadataRequest.
"
54929083,132,junrao,2016-03-03T19:01:05Z,"unused import
"
54929093,132,junrao,2016-03-03T19:01:10Z,"unused import
"
54929109,132,junrao,2016-03-03T19:01:19Z,"Could we add the new params in the comment above?
"
54929134,132,junrao,2016-03-03T19:01:27Z,"This seems to be an expensive way to test auto topic creation since it needs to start a cluster. Could we just test AdminUtils.assignReplicasToBrokers() directly?
"
54929143,132,junrao,2016-03-03T19:01:32Z,"Do we need to start ZK for this test?
"
54929192,132,junrao,2016-03-03T19:01:42Z,"Is this useful since none of the 3 verifications are enabled by default. Ditto below.
"
54929198,132,junrao,2016-03-03T19:01:45Z,"unused import
"
54929228,132,junrao,2016-03-03T19:01:50Z,"Should those comments starting with ""ensure"" be here?
"
54929246,132,junrao,2016-03-03T19:01:56Z,"Do we need to start ZK in this test? It seems that we can just test AdminUtils.assignReplicasToBrokers() directly?
"
54929251,132,junrao,2016-03-03T19:02:00Z,"Similar to the above. Do we need to start ZK in this test? It seems that we can just test AdminUtils.assignReplicasToBrokers() directly?
"
54937233,132,allenxwang,2016-03-03T19:51:18Z,"@junrao Do you suggest using a case class to represent the tuple?
"
54937680,132,granthenke,2016-03-03T19:54:04Z,"I think just like you did in the map above is good:

``` scala
.groupBy { case(v, k) => v }
```
"
54954835,132,ijuma,2016-03-03T21:54:00Z,"The KIP says:

`case class Broker(id: Int, endPoints: Map[SecurityProtocol, EndPoint], rack: Option[String] = None)`

I prefer how you have it here, but we should update the KIP.
"
54955136,132,ijuma,2016-03-03T21:55:52Z,"Yes and the KIP should be updated to remove the point about updating `Broker.writeTo`.
"
54971611,132,allenxwang,2016-03-04T00:15:15Z,"@junrao I think we can use the same constructor for both v1 and v2 except for the version number. When the request is serialized, the rack in v1 request is ignored according to Protocol. In other words, regardless v1 or v2, we can have rack in UpdateMetadataRequest.Broker and have it handled differently only at serialization.

See the updated test RequestResponseTest where I added the test for v1 request to make sure it still works.
"
54971962,132,allenxwang,2016-03-04T00:19:06Z,"@junrao This may not be necessary. See my comment in ControllerChannelManager.
"
54972661,132,junrao,2016-03-04T00:26:36Z,"Yes, you are right.
"
54974078,132,allenxwang,2016-03-04T00:41:59Z,"The only caller is main(). I feel it is little bit over stretched to print from main or create another function just to print the result. 
"
54974433,132,allenxwang,2016-03-04T00:45:54Z,"Will update the KIP
"
54974451,132,allenxwang,2016-03-04T00:46:04Z,"Will update the KIP
"
54977110,132,allenxwang,2016-03-04T01:10:56Z,"We have unit tests for AdminUtils.assignReplicasToBrokers() that does not require starting a server. However, since the logic that governs the rack aware mode is separate from AdminUtils.assignReplicasToBrokers(), we need to have tests to make sure the right API is called from KafkaApi. The behavior we need to test is
- If all brokers have rack, rack aware assignment will be generated
- If some brokers have rack and some do not, it will be treated as none of the brokers have rack
"
54978198,132,allenxwang,2016-03-04T01:24:05Z,"There is a test case (testGetBrokersAndRacks) testing the behavior of what broker-rack mapping should be used under different rack aware mode.
"
54980762,132,allenxwang,2016-03-04T01:54:08Z,"It is a bug that by default the three verifications are disabled. I will fix it.
"
54981583,132,allenxwang,2016-03-04T02:06:34Z,"It ensures that user can disable rack aware in the command line in TopicCommand and the ReassignPartitionCommand can generate the assignment which is rack aware, if user does not disable rack aware when running ReassignPartitionCommand.
"
54981885,132,allenxwang,2016-03-04T02:11:04Z,"This covers the situation where replica assignment is rack aware for alter operation. Again, I added this test since user input in the command line can change how rack aware is enforced.
"
54983632,132,allenxwang,2016-03-04T02:38:55Z,"@granthenke Given the comment from @junrao that the old constructor should be deprecated, I think it is better to use the new constructor.
"
54996851,132,granthenke,2016-03-04T07:08:34Z,"Works for me
"
54997151,132,granthenke,2016-03-04T07:13:12Z,"The way protocols support enum is via some id field. Examples can be seen in existing enums and my KIP-4 PRs: [PermissionType.java
](https://github.com/granthenke/kafka/blob/acl-wire/clients/src/main/java/org/apache/kafka/common/security/auth/PermissionType.java)
"
55029807,132,ijuma,2016-03-04T13:35:25Z,"Note that this constructor is only used in tests. Does it even make sense to keep it?
"
55030014,132,ijuma,2016-03-04T13:37:23Z,"This constructor is only used in tests, does it make sense to keep it? I guess the question is whether the request classes are API. As I understand, they are not, but I would like to get @junrao's take.
"
55031610,132,ijuma,2016-03-04T13:52:14Z,"Can you please elaborate why you think it's not good to move the println to `main`? In fact, that is exactly what was done in the following PR that adds some tests to the reassign partitions command:

https://github.com/apache/kafka/pull/956/files#diff-15a36057cf9f21154f103a55757a7b59R99
"
55032389,132,ijuma,2016-03-04T14:00:22Z,"I think it would be good if we could elaborate a little more on the purpose of this property (with some brief examples, maybe).
"
55033837,132,ijuma,2016-03-04T14:14:35Z,"We say that we register the v3 format including rack, but the code only adds the rack if `apiVersion >= 3`. If we follow the same approach as what we did with V2, we should always add the rack as it will simply be ignored if `version < 3`, right?
"
55033946,132,ijuma,2016-03-04T14:15:36Z,"Also, the documentation here isn't accurate as we may register `v2` depending on the value of `apiVersion`.
"
55075041,132,allenxwang,2016-03-04T19:16:21Z,"Do you really need to serialize RackAwareMode from the client side? If you want to send a request to broker to create topic, the only thing related to rack aware is whether you want to disable it. And you can send a string for this purpose like the ''--disable-rackaware"" command line option, right?
"
55075472,132,granthenke,2016-03-04T19:19:51Z,"@allenxwang Why send a string and translate it in 2 places, with 2 pieces of logic, if instead I can have 1 common Enum that lets me send a small byte and ensure translation is the same on both sides. My understanding is this is not a boolean choice but there are 3 values to pick from.

I have not spent enough time digging into RackAwareMode and if its really required, but if I need to use it to communicate to a broker it should be in the clients code. 
"
55076734,132,allenxwang,2016-03-04T19:29:16Z,"It is more of a style issue. I noticed that most of the command class' `main` does not have println.

The above PR refactored the `generatedAssignement` function into two and there is no println in main. I am fine with this approach.
"
55082237,132,ijuma,2016-03-04T20:09:58Z,"That's fine too. The main aim is to keep the method that returns a value without `println` so that it can be tested without polluting the logs, for example.
"
55082518,132,ijuma,2016-03-04T20:12:28Z,"@allenxwang Are you suggesting that `Safe` mode would not make sense in this context? If so, then I agree that this enum would not make sense in `common`.
"
55083450,132,allenxwang,2016-03-04T20:20:07Z,"I will update the document. I think it is safer not to register the rack when version <  3 and having rack in v2 violates the specification anyway which might be confusing to users.
"
55084318,132,ijuma,2016-03-04T20:27:37Z,"OK, I think this has been discussed before and I apologise for asking again. Why do we need to write v2 at all? With the exception of 0.9.0.0, the parsing code reads any version and ignore fields that it doesn't know about, right? Is it to make it possible to upgrade from 0.9.0.0 (even if that will break old clients anyway)?
"
55088095,132,junrao,2016-03-04T21:00:54Z,"I think the reason that we have to design the broker registration json in a backward compatible way is that the old consumer and admin tools still read the json directly. Once we deprecate the old consumer and move the admin tools to the admin api, only the brokers need to read the json. Then, potentially we can make non compatible json changes as long as the broker writes the json according to inter.broker.protocol. For example, we make want to clean up the redundant host field in the future. So, it is true that at this moment, we can always write the latest version of the json that the broker understands since the change is backward compatible. However, following the inter.protocol convention is probably what we want to do in the future. So, we may want to just start doing this now.
"
55088361,132,allenxwang,2016-03-04T21:03:37Z,"Safe mode is currently only used for auto topic creation. It is not used for command line tools.

I think it is better for you guys to do the necessary changes when implementing the client side code since I cannot verify the changes I make here will work for clients in the future. And it will be more efficient to do this later since there might be unforeseen problems or design changes arise when client code is implemented.
"
55088556,132,granthenke,2016-03-04T21:05:20Z,"I am okay with that. 
"
55095191,132,ijuma,2016-03-04T22:00:10Z,"@junrao Thanks for explaining. If we want to do it this way, I think we need to add a comment explaining it because it's not obvious and it's inconsistent with how we did the V1 to V2 change. Having said that, I am not sure if it's worth doing this now for the following reasons:
- It will probably be a long time before we can actually drop support for the old consumer and admin tools in the broker (I think 3 non-bug fix releases is the absolute minimum, but it will probably be longer)
- It's inconsistent with how the parser code works (for compatibility as you mentioned). Without changing the parser code, older brokers will break if we suddenly remove fields.
- It doesn't seem to buy us anything in terms of what gets stored (people will bump inter.broker.protocol as part of the upgrade and then V3 will be stored anyway) for the time when we want to delete fields.

It seems to me that when we have a concrete plan to clean this up, we can add the necessary code for both writing and parsing. It will probably take a few releases before we can actually delete the fields, but that's probably OK.
"
55098730,132,junrao,2016-03-04T22:27:05Z,"@ijuma : Yes, we can probably just write the v3 json for now and clean things up later.
"
55102385,132,allenxwang,2016-03-04T23:02:57Z,"One thing I want to add is that we have to write v2 when protocol is 0.9.0.X because of KAFKA-3100. Having rack in v2 version is probably fine but I don't see any benefit of doing that.
"
55103536,132,ijuma,2016-03-04T23:16:52Z,"Right, so that was my original question, this helps with people who want to upgrade brokers from 0.9.0.0 to 0.10.0.0, right? That's a fair argument and worth mentioning in the code. With this approach, only old clients have to go to 0.9.0.1 before the 0.10.0.0 upgrade. Brokers can go straight from 0.9.0.0 to 0.10.0.0 (provided that they do the inter.broker.protocol dance).
"
55106909,132,allenxwang,2016-03-05T00:00:19Z,"OK. I like that approach too and it will simply my JUnit tests. I will borrow that in this PR. You still need to resolve the conflict through in the future.
"
55107138,132,ijuma,2016-03-05T00:03:20Z,"Sounds good.
"
55187222,132,ijuma,2016-03-07T10:47:20Z,"@allenxwang, if we use something like the following, we can still handle the scenarios you describe, right?

``` scala
case class BrokerMetadata(id: Int, rack: Option[String])
```

I am happy to try this out on a branch to see how it looks. Thoughts?
"
55188628,132,ijuma,2016-03-07T11:00:54Z,"I think we should also update the text around:

`There are 2 goals of replica assignment`

Either we need to add a third goal or we should make it clear that that description is for rack unaware and then explain what the goals are for the rack-aware case.
"
55402888,132,allenxwang,2016-03-08T18:24:57Z,"Updated the doc. Please take a look.
"
55404378,132,ijuma,2016-03-08T18:33:53Z,"Looks good, thanks.
"
55487522,132,ijuma,2016-03-09T08:52:56Z,"""Notes to clients"" maybe?
"
55488300,132,ijuma,2016-03-09T09:00:44Z,"Maybe: ""clients with a ZooKeeper dependency (old Scala high-level Consumer and MirrorMaker if used with the old consumer) will not work with 0.10.0.x brokers. Therefore, 0.9.0.0 clients should be upgraded to 0.9.0.1 <b>before</b> brokers are upgraded to 0.10.0.0.""?
"
55532555,132,granthenke,2016-03-09T15:15:02Z,"@ijuma @allenxwang This is a big deal right? It breaks Kafka's backwards compatibility guarantee. It should probably be moved from the performance impact section to the breaking section. Is there anything else we can do to fix this? Are we sure it does not break 0.8 clients? 

cc @gwenshap @junrao 
"
55534413,132,granthenke,2016-03-09T15:26:22Z,"I went back and looked. I see why its only an issue for 0.9.0.0. Its because a change went in that throws an exception for any version > 2 ([here](https://github.com/apache/kafka/blob/0.9.0.0/core/src/main/scala/kafka/cluster/Broker.scala#L60)), where in 0.8.x version isn't even used ([here](https://github.com/apache/kafka/blob/0.8/core/src/main/scala/kafka/cluster/Broker.scala#L36)). 

Can we leverage the fact that we are only adding a field, therefore this is a ""compatible"" update that does not require a version bump? That way we stay at version 2, 0.9.0.0 ignores the rack field, and everything works?

This should work because all of our json parsing just uses a Map[String, Any]. So adding fields doesn't break parsing. This PR actually acts as if version 2 & 3 are equal [here](https://github.com/allenxwang/kafka/blob/KAFKA-1215/core/src/main/scala/kafka/cluster/Broker.scala#L86-L93) anyway. 
"
55536850,132,ijuma,2016-03-09T15:41:09Z,"@granthenke, yes, it is a big deal. Your suggestion was discussed in the KIP thread, but @junrao said that we need to increase the version when we change the format. As I understand it, the idea is to move away from being limited to just adding fields once we stop supporting old clients (but that is a while away in my opinion).

Thinking more about this, we could also add a new version field and deprecate the old one (which can stay at `2` until we no longer support `0.9.0.0` many years from now) if we really care about having an incrementing version each time we change the format. Thoughts?
"
55538191,132,granthenke,2016-03-09T15:49:05Z,"I agree, we don't want to be limited to just adding fields, but when we are just adding fields a version bump may not be required. This patch assumes anything over version 1 will be compatible in the else statement, so even though our goal is being able to remove fields, this patch does not do that. 

I don't think adding another version makes sense, thats just adding another field to (old) version 2. We can just add the rack field and achieve the same goal. 
"
55539640,132,ijuma,2016-03-09T15:57:16Z,"I don't agree that we achieve the same goal. From a storage format documentation perspective, it's easier to track format changes if there is a version associated with them (even when doing compatible changes like adding fields). It is still useful to know when a particular set of fields was added ((in this case it's just a single field, but in others there will be more).

Yet another way to handle that is to tie the format with `ApiVersion`. This can be done implicitly or explicitly.
"
55541167,132,granthenke,2016-03-09T16:05:41Z,"@ijuma I completely agree. Versioning the the format is important. It acts as valuable documentation and (when used correctly) can help improve compatibility. 

Let me be clear about my intentions. I am making no comment on the future of zookeeper json compatibility, just this change and its impact to the upcoming release. Given that we still, even in this patch, don't support removing fields in future versions. Changing the version for that reason is a moot point. Therefore the only value the version has is documentation. 

If we weigh documentation and compatibility for 0.10 and choose documentation, then we can bump the version. If we think compatibility is the most important thing to maintain. That can be solved by keeping the version at 2. 
"
55604952,132,junrao,2016-03-09T22:50:20Z,"@granthenke : Yes, we could keep version 2 in json. But the the drawback is the following. If we want to support upgrading form 0.9.0.0 to any future releases post 0.10.0, we can't bump the version in json forever. Given that (1) versioning the json in ZK is useful and (2) the issue in 0.9.0.0 is a bug and relatively few people are using 0.9.0.0 yet, I think it's probably better to change the ZK version now, but require people to upgrade from 0.9.0.1.
"
55631206,132,granthenke,2016-03-10T03:57:23Z,"@junrao In future releases we expect to remove the old Scala clients. This means only the brokers will talk directly to Zookeeper and this should not longer be an issue. My understanding is the goal of 0.10.0 is to be a ""compatible"" release. Future releases may remove other deprecated things and choose to be breaking. I think that's the best time to break here as well. 

I don't want to push the issue too much. In the end, I am okay with either choice. I just want to be sure we are consciously choosing to break and for good reason. 
"
55909377,132,junrao,2016-03-12T00:37:13Z,"This doesn't seem to be 100% safe in that we can potentially assign 2 replicas to the same broker. Consider the following example.

rack   :  a   b  a  a
broker:  0  1  2  3

At some point, we assign the 1st replica to broker 1. Suppose that nextReplicaShift is 0. We then assign 2nd replica to broker 2. When assigning the 3rd replica, we will be skipping 3 and 0 and assign broker 2 to the 3rd replica again.

Before we don't have this issue since when assigning replicas other than the 1st, we cycle through the brokers sequentially without skipping. The new logic allows skipping. So, it's possible for us to hit the same broker.
"
55909620,132,junrao,2016-03-12T00:41:32Z,"I still think that this test and AdminRackAwareTest (except for testGetBrokerMetadatas) can still be simplified. For example, if we restructure the code a bit by wrapping assignReplicasToBrokers in another helper method that takes the following signature, then we should be able to test all kinds of rack/rackAwareMode combinations w/o needing to start broker/ZK, right?

(brokerMetadatas: Seq[Broker],
 rackAwareMode: RackAwareMode,
 nPartitions: Int,
 replicationFactor: Int,
 fixedStartIndex: Int = -1,
 startPartitionId: Int = -1)
"
55909641,132,junrao,2016-03-12T00:41:38Z,"Hmm, what does NotEnoughPartitions mean?
"
55909658,132,junrao,2016-03-12T00:41:48Z,"Is filling the same value expensive? Would it be more efficient to just iterate each size and do a check?
"
55909701,132,junrao,2016-03-12T00:42:30Z,"Should we also verify that no two replicas from the same partition are assigned to the same broker?
"
55909718,132,junrao,2016-03-12T00:42:41Z,"Is there anything special with 12 partitions?
"
55910465,132,ijuma,2016-03-12T00:56:40Z,"This is really cheap compared to other things we do in our tests and it gives better error messages.
"
55911136,132,allenxwang,2016-03-12T01:07:54Z,"I don't think that will help from test's perspective. Even if we add rackAwareMode here, we still need to make sure that for auto topic creation and command line tools (where you can disable rack aware) the right RackAwareMode is used. 

The tests that have dependency on broker/ZK make sure no matter how underlying API is structured, the end result is correct. So I think there are values in the tests.
"
55911374,132,allenxwang,2016-03-12T01:12:35Z,"I will fix the confusing name. The test makes sure the algorithm works when the number of partition is not multiple of brokers.
"
55911537,132,allenxwang,2016-03-12T01:15:43Z,"Probably not. :)

In general, these tests run very fast since all they do is operate on collections in memory. So I have not thought about reducing the number of tests.
"
55915045,132,allenxwang,2016-03-12T03:23:17Z,"That's a very good point. I will address this in my next update.
"
55917767,132,allenxwang,2016-03-12T07:16:04Z,"Thinking a little bit more on this, I think this situation is actually covered by the algorithm. In this case, there are three replicas and only two racks. Once replicas are assigned to 1 and 2, we know that all racks have replicas for the partition and skipping behavior will stop.
"
55922985,132,junrao,2016-03-12T15:49:24Z,"Right, this example actually works. But the following won't. Consider the following broker to rack mapping.

rack   : a  b  c  a  a
broker: 0  1  2  3  4

Let's say you want to have 4 replicas and the first replica is assigned to broker 2. Then you assign 2nd replica to 3. Then you skip broker 4 and 0 since both are on rack a and not all racks are filled yet. Then you assign 3rd replica to 1. Finally, you will assign 4th replica to broker 3 again.
"
55929323,132,allenxwang,2016-03-12T23:09:16Z,"Yes, I will add that check.
"
55929568,132,allenxwang,2016-03-12T23:31:03Z,"Excellent example. Added the logic to prevent assigning replica twice to the same broker for the same partition.
"
55933791,132,ijuma,2016-03-13T08:37:22Z,"Nitpick: we don't really need this `assertEquals` or the `brokerList` val since that is checking that `toBrokerMetadata` works correctly, which is not the purpose of this test.
"
55933813,132,ijuma,2016-03-13T08:39:50Z,"This was probably an accidental reformatting by IntelliJ.
"
55933814,132,ijuma,2016-03-13T08:39:55Z,"This was probably an accidental reformatting by IntelliJ.
"
56015924,132,junrao,2016-03-14T15:12:39Z,"Only this test needs ZK. Could we pull this test to a different class and remove the ZK dependency from this class? Otherwise, each test will unnecessarily start a ZK server, which will slow down the test.
"
56016010,132,junrao,2016-03-14T15:13:09Z,"Could we add an error message in assertEquals? Ditto in the assertEquals below.
"
56053157,132,allenxwang,2016-03-14T18:40:06Z,"Will do.
"
56054757,132,allenxwang,2016-03-14T18:50:00Z,"I think there is value in checking this to make sure test set up is correct. Otherwise if `toBrokerMetadata` is changed, there are two possibilities: 
- Test fails and it is difficult to debug why it fails
- Test passes but is actually weakened
"
56095654,132,ijuma,2016-03-14T23:45:37Z,"@allenxwang, we use `toBrokerMetadata` in many other tests and we don't check its behaviour in the other cases, so it looks a bit inconsistent. In my opinion, if we want to be sure about its behaviour, we should write a test for it instead of checking its behaviour inside other tests. In any case, this is a very minor point and I'm fine if we leave as is.
"
220379749,5693,vvcephei,2018-09-25T22:47:11Z,"This change (and similar changes below) are to make sure the serdes we need for suppression are available.
I sort of thought that we already merged a PR to do this, but perhaps it was only partial."
220380552,5693,vvcephei,2018-09-25T22:51:38Z,"I realized belatedly that I missed this (internal) interface when I renamed ""maxKeys"" to ""maxRecords"" in Part 1."
220380693,5693,vvcephei,2018-09-25T22:52:29Z,This wraps the value so that the buffer can store the whole record context for later forwarding.
220381174,5693,vvcephei,2018-09-25T22:55:02Z,"Since we don't actually store the value serialized in the in-memory impl, we annotate the value with its size so we can maintain the current footprint of the buffer. Alternatively, we could serialize it again on removal to idempotently re-compute its size, but this seemed cheaper."
220383032,5693,vvcephei,2018-09-25T23:04:56Z,"This could be configurable in the future, but for now, we enforce the time limit in the following fashion:
* start a timer when a key first enters the buffer
* that key and its most recent value will be emitted when the time limit expires, regardless of how recently it has been updated

The primary advantage of this implementation is that we guarantee that if you set a 5-minute limit, we delay emitting the key for no more than five minutes. If we instead re-set the timer on each update, you might never see a record that gets consistently updated more frequently than the time limit.

My judgement was that this is the most intuitive default and starting point for the feature. If people want to configure it, we can easily add that option later."
220383454,5693,vvcephei,2018-09-25T23:07:36Z,"As demonstrated by Part 1, we don't always need the buffer, so I thought it best to avoid allocating it and scheduling the associated punctuator until we first discover we need to buffer something."
220384765,5693,mjsax,2018-09-25T23:15:03Z,Why do we need this in `process` -- seem like moving it to `init()` should be sufficient?
220385505,5693,vvcephei,2018-09-25T23:19:53Z,"This was the punctuation concern @guozhangwang brought up.
I haven't optimized this yet because I wanted to discuss the available options first.

I'm thinking:
1. store the min timestamp in the buffer to make this function cheap when there's nothing to do
2. schedule just one punctuator for all the buffers. This would require more coordination in the topology builder, and I'm not sure if it would actually yield any benefit. Is iterating over buffers any better than iterating over an equal number of punctuators?
3. schedule the punctuator less frequently. This would improve performance for high-frequency topics, but not for medium to low frequency topics. On the downside, it would sacrifice resolution and make the tests a little tricky to reason about.
3a. we could probably make a reasonable approximation of the appropriate resolution based on the suppression time limit, like `min( max(1, suppressDuration / 10), 30 seconds)`, or even tie it to the commit interval.
3b. to mitigate the testing problem, we could add a private mechanism to directly set the resolution. (not sure this is needed; would like to see how awkward it is in practice once we decide on some optimizations)"
220385584,5693,mjsax,2018-09-25T23:20:27Z,`suppress.getTimeDefinition()` should return the same thing each time? Should we put it into a member variable?
220385995,5693,vvcephei,2018-09-25T23:22:55Z,"Come to think of it, this is probably insufficient to catch the wrong serde (due to erasure). I probably need to relocate this error message to the actual call to de/serialize"
220386097,5693,vvcephei,2018-09-25T23:23:35Z,oops. I'll take these out.
220386172,5693,mjsax,2018-09-25T23:23:58Z,"I am wondering about this: as we compute the byte-size later, and already pay the cost to serialize the record, should we not store `byte[]/byte[]` in the buffer? Of course, still will imply that we need to deserialize later, however, the keeping the actual deserialized objects around would haver more storage overhead and would not obey the buffer size IMHO. Thoughts?"
220386402,5693,mjsax,2018-09-25T23:25:24Z,We should resolve this before merging IMHO.
220386480,5693,vvcephei,2018-09-25T23:25:55Z,"Oh yeah, I was meaning to figure out the right exception to throw to achieve a nice shutdown (I think any runtime exception will do it, but is there a semantically best one?)"
220386796,5693,mjsax,2018-09-25T23:28:01Z,Do we need this check?
220387020,5693,vvcephei,2018-09-25T23:29:25Z,This is specifically for storing the keys sorted by timestamp in the buffer. I wasn't sure whether a more general or more specific name like `BufferKey` is better...
220387145,5693,mjsax,2018-09-25T23:30:17Z,nit: remove `this`
220387246,5693,vvcephei,2018-09-25T23:31:02Z,"aka, ordering of keys that share a timestamp is arbitrarily. If anyone cares, we can do ""better"" by requiring K to be Comparable (but I don't think anyone should care, so I kept it simple)"
220387263,5693,mjsax,2018-09-25T23:31:07Z,Should we check for `last == null` and set `last = null` ?
220387414,5693,mjsax,2018-09-25T23:31:56Z,Guess this should be removed?
220387906,5693,mjsax,2018-09-25T23:35:08Z,Not sure about this. See my other comment. Would be good to get input from @guozhangwang and @bbejeck about this.
220387946,5693,vvcephei,2018-09-25T23:35:24Z,"Aha. I was thinking of https://github.com/apache/kafka/pull/5521, which just isn't merged (yet)."
220390443,5693,mjsax,2018-09-25T23:50:35Z,"IMHO, scheduling a `1ms` punctuation would cause quite some overhead. Alsw, we only need this for ""time based"" eviction, not for buffer size (num records, num bytes), right?

We should also know, *when* we need to evict earliest -- thus, it should be sufficient to schedule accordingly? I think, we can also exploit cancellation to scheduled punctuation to be more flexible.

Also note, that during runtime, we don't check for punctuation execution after each record, but do this only after N records are processed (with N being adjusted dynamically during runtime). We also need to consider, that we fire a lot of punctuations if we ""jump ahead"" in time what seems to be inefficient."
220390754,5693,mjsax,2018-09-25T23:52:28Z,I agree with the described semantics.
220390935,5693,mjsax,2018-09-25T23:53:39Z,"Forgot to add this to my review: this seems to have large runtime overhead and IMHO, we should try to find a better way to handle this."
220391498,5693,mjsax,2018-09-25T23:57:09Z,Do we need to use punctuations to enforce record/byte limit? Might be better to check for record/byte limit on put and use punctuations only to evict time based?
220402141,5693,vvcephei,2018-09-26T01:19:05Z,"Ah, ok. I was hoping this is not how punctuations work (I'm ashamed to say I haven't looked at it yet). What I was hoping is that if I start at stream-time 0ms and then get a record at time 100ms, then my 1ms punctuator would be invoked just once, at time 100ms. I.e., I was thinking it would ""jump ahead"" (I thought I observed this, but maybe it was using the `TopologyTestDriver`).

One alternative is to ""brew my own"" schedule exactly as I described, checking during `process` if there are any old-enough records. This could be done in the same loop that evicts if we're over capacity. This implementation would be very cheap.

The tradeoff is that the punctuator will be fired on any advancement in stream time, whether or not that record actually reaches the buffer. But the hack I described would only ""tick"" when `process` is invoked. I *think* this would probably be satisfactory semantics."
220403434,5693,vvcephei,2018-09-26T01:29:19Z,"Regarding:
> We should also know, when we need to evict earliest -- thus, it should be sufficient to schedule accordingly? I think, we can also exploit cancellation to scheduled punctuation to be more flexible.

This is true, but it's slightly tricky (or at least it took me a while to realize it's not sufficient to trigger every `suppressDuration` ms). I guess that each time we have a new min buffered timestamp `m`, we'd schedule a punctuation, we could cancel the previous punctuation and schedule a punctuation for `m + suppressDuration` time from now.

The punctuation schedule doesn't let you schedule in the form of ""`x` ms from now"", (I'm guessing it's epoch aligned like the windows), so we'd do a little math to compute a punctuation interval that would next fire at the correct time.

I said ""each time we have a new min timestamp"". This can happen when we buffer new records or when we evict records.

Is this what you had in mind?"
220405269,5693,vvcephei,2018-09-26T01:44:03Z,"It's not so easy to tell when we really need to buffer records until we actually get some records. This is a consequence (maybe a downside) of my choice to use `TimeDefinition` to use the window-end time as ""now"" and the grace period as the `suppressDuration`. Because of this, within the buffering context, even with a `suppressDuration` of 0, we might still need to buffer, as the effective timestamp is in the future.

Thinking through this, we could try instead using the window start as ""now"" and using the window size + grace period as the suppress duration, but offhand it seems this wouldn't work too well with SessionWindows (or other variable-sized windows).

So instead what I chose to do is just do a lightweight check when I need the buffer and initialize it if it hasn't already been. I could even move the `if buffer == null` to right here, and jit branch prediction would ensure this lazy check is almost zero after buffer gets initialized.

Some alternatives:
1. discard the optimization and just always initialize it, in case I need it.
2. junk the (maybe unnecessarily) flexible `TimeDefinition` function and instead just use a ""time strategy"" enum that tells the processor whether it should use record time or window-end time:
In the former case, if the duration is zero, we know we'll never need a buffer. If it's > zero, we'll probably need one.
In the latter case, we'll probably need a buffer, regardless of the suppression duration.

WDYT?"
220413716,5693,vvcephei,2018-09-26T02:54:39Z,"Yeah, I think this is a reasonable thing to do. I've been going back and forth on it.

The downside of storing it serialized is then we need to deserialize it to emit it. This is a moot point for the (planned) on-disk implementation, but for in-memory it saves some CPU and possibly some GC pressure not to round-trip it through a byte array.

As is, we serialize it just once instead of serialize + deserialize. Plus we currently discard the produced array immediately, so it's easy on the gc, whereas if we keep it, then we have 3 medium-to-long term objects: the incoming record, the serialized array, and the (deserialized) outgoing record. Is this premature optimization? Possibly.

Some other factors to consider: when we send to the changelog, we'll need to serialize it anyway. But I'm planning to send only on `flush` and to keep the changelog buffer compact with a LinkedHashmap, so records that get updated or retracted several times within a commit interval would only get serialized once. Plus, for this purpose, we still only need the `serialize` side; we could hang onto the produced array after computing the size long enough to send it to the changelogger.

For changelogging purposes, we'd only need to deserialize when we recover on startup, not in steady-state operations, so I think it's still more economical to store the records as objects instead of serialized.

It is true that there's really no tight correlation between the heap used by an object and the heap used by its serialized form. So at the moment, we're only roughly obeying the size limit. For primitive data, it's probably pretty close, though.

I'm open to either way of doing it, but that was my thinking. What say you?"
220413812,5693,vvcephei,2018-09-26T02:55:41Z,"definitely. Should it just be a `KafkaException`, or something more specific?"
220415961,5693,vvcephei,2018-09-26T03:16:24Z,"Good idea, setting it to null after I use it will make it available for gc.

I can guard against null also, but fwiw, I'm not sure how that situation could arise. It's an `IllegalStateException` to invoke  `delegate.remove` without an intervening call `delegate.next`. Or to call it before `next`.
`delegate.next` could return null, but in that case, we'd get an exception in line 69... which I should check for there.

"
220416468,5693,vvcephei,2018-09-26T03:21:03Z,"probably not. I don't think this can happen unless this buffer is used across threads (which shouldn't happen), or unless we screw up the implementation in the future (which we could do in any number of ways, it doesn't mean we need guards everywhere).

WDYT?"
220416859,5693,vvcephei,2018-09-26T03:24:51Z,I think you're spot on. I'll check it out.
220424615,5693,vvcephei,2018-09-26T04:38:31Z,I think I like this better than my ideas 2 and 3 above. I'm on the fence about this vs just doing it as a part of `process`. I think we'll probably want to do idea 1 regardless.
220435089,5693,mjsax,2018-09-26T06:06:25Z,"Compare https://issues.apache.org/jira/browse/KAFKA-6323 and the corresponding PR for more details about punctuation semantics. `TopologyTestDriver` should not work differently (if it does, it's a bug in the test driver -- behavior must be the same to allow for unit testing -- would be bad if it would behave differently).

About the second point: yes, something like this. I did not think this through. Maybe it's also ""good enough"" to have something more coarse grained for first release.

Going with ""manual punctuation"" with ""process"" might also be a good first approach -- might still be better than `1ms` punctuation from an overhead point of view (of course, depends on the throughput... 1ms == 1000 records/second/task...)"
220435537,5693,mjsax,2018-09-26T06:09:25Z,Hard to say -- JIT branch prediction might make my concern invalid -- it's just because it's on the hot code path. Would be good to get input from @guozhangwang and @bbejeck 
220435742,5693,mjsax,2018-09-26T06:10:44Z,"Also, we should avoid pre-mature optimization..."
220436580,5693,mjsax,2018-09-26T06:16:03Z,"Agree with all trade-offs you mention. For KTable caches, we also went to storing `byte[]` to obey the size config. Also note, we don't need to deserialize all byte[] arrays, but only on eviction -- if we have a lot of suppression. many byte[] arrays would never the deserialized but overwritten. Depending on throughput and number if unique keys, this might happen quickly enough to still be young gen. Hard to say. Again, more input from @guozhangwang and @bbejeck would be helpful.

And as above, pre-mature optimization should be avoided. Could we do some prototyping and benchmarking of both approaches? Not sure if there is enough time. Also, it's an internal implementation and if performance becomes an issue, we ca also improve on it in 2.2."
220436698,5693,mjsax,2018-09-26T06:16:42Z,I guess `StreamException` or maybe a new sub-class would be a good idea.
220437021,5693,mjsax,2018-09-26T06:18:42Z,"I tend to think, that we don't need this guard because a bug that gives multi-threaded access seems to be very unlikely. But it's a personal opinion... My concern again is because this is the hot code path. But I am also ok to keep the check if somebody insists."
220437726,5693,mjsax,2018-09-26T06:22:08Z,"Ack. See your point that `delegate` does the check for us.

I was aware that it would imply incorrect API usage (ie, wrong call order or similar). Just wanted to make sure we catch a bug like this -- but seems it would crash anyway even if we don't add a check for `null`."
220585715,5693,vvcephei,2018-09-26T14:26:29Z,"Ok, I think the in-`process` approach sounds simple and low-overhead, so I'll do that for starters, and we'll see what we think."
220586765,5693,vvcephei,2018-09-26T14:28:52Z,"This is very true:
>  if we have a lot of suppression. many byte[] arrays would never the deserialized but overwritten

I won't do anything with it right now, but wait for more input (and take care of the other things we discussed)"
220587434,5693,vvcephei,2018-09-26T14:30:28Z,I also think it's unlikely to be useful. I'll remove it.
220675913,5693,bbejeck,2018-09-26T18:32:37Z,I also agree with the semantics for enforcing the time limit.
220687788,5693,vvcephei,2018-09-26T19:08:12Z,"Ok, I just confirmed that `TreeMap#entrySet().iterator().next()` can never return `null`, but we could theoretically store a null value in the map, which could still throw an NPE on this line. I'll guarded against it."
220689349,5693,bbejeck,2018-09-26T19:13:22Z,"While I also agree with the trade-offs mentioned by @vvcephei, we can't say exactly what the better approach will be without testing.  To me, the bigger savings potential would be in CPU  but again we can't say without testing.  

But we do need to serialize for sending to the changelog, and even if we only send on `flush` and couple that with the fact that a `byte[]`  coming in does not always get deserialized due to updates by key.  So I'm starting to think to go with either approach will be a wash.

So, for now, I'm leaning towards storing `byte[]`

1. That's what we currently use for `KTable`, while that by itself is not enough of a reason, IMHO we need to be careful about having different approaches for similar issues without a clear, demonstratable reason for doing so.
2. Benchmarking will really give us the answers we are looking for, but time is something we don't have right now for getting this into 2.1
3. I could be wrong about this but I think the biggest users of suppression are going to have several updates per key, so as @mjsax mentions, many of the `byte[] arrays` are going get overwritten."
220695412,5693,bbejeck,2018-09-26T19:33:58Z,left over debugging?
220695672,5693,bbejeck,2018-09-26T19:34:51Z,+1 for a sub-class of `StreamException` 
220701301,5693,bbejeck,2018-09-26T19:54:12Z,I think doing it in `process` is a good start as well.
220703951,5693,vvcephei,2018-09-26T20:02:12Z,"yeah, I noticed it late. It's gone now."
220707349,5693,vvcephei,2018-09-26T20:13:40Z,"Ok, it sounds like no one has a super strong performance intuition.

I think @bbejeck's point about uniformity is a good one. If anyone wants to insist on this, I'll change it right now.

Otherwise, if we're all comfortable making a performance-based decision, I think I'll propose to implement change-logging first and then do a comparative benchmark to make the final call."
220777672,5693,vvcephei,2018-09-27T02:29:33Z,"I've been mulling this over... It seems like byte arrays is the more normal choice in the code base, so it should be the default until proven otherwise by a performance test. The fact that I made the opposite choice in development is irrelevant.

So I'll go ahead and swap it out for byte arrays tomorrow."
220953221,5693,bbejeck,2018-09-27T14:44:59Z,"Can we add two cases to `KTableSuppressProcessorTest` to hit this branch? One for the `EMIT` case and another for the `SHUT_DOWN`

"
220954471,5693,bbejeck,2018-09-27T14:48:13Z,"we could use a test to hit this branch as well, but IMHO it's a lower priority than the others mentioned above."
220989558,5693,vvcephei,2018-09-27T16:21:32Z,"This line is actually gone now. But if it were still there, I'd agree with you."
221031086,5693,bbejeck,2018-09-27T18:31:04Z,"nit: this can be simplified to:
`TestUtils.waitForCondition(() -> driver.state() == KafkaStreams.State.ERROR, TIMEOUT_MS, ""Streams didn't shutdown in error state"");`"
221046943,5693,vvcephei,2018-09-27T19:21:55Z,"AH, right. I looked for something like that, but I was looking in `IntegrationTestUtils`. Thanks."
221048082,5693,vvcephei,2018-09-27T19:26:00Z,"Ok, I've updated it."
221336195,5693,guozhangwang,2018-09-28T18:04:25Z,"Could we merge #5521 (I think it is in pretty good shape) and rebase this PR on that? I felt a couple of the changes blew are a bit redundant, e.g. passing in the materializedInternal object as well as its serde fields."
221346202,5693,guozhangwang,2018-09-28T18:41:04Z,"Could we move this function to a single class, e.g. `WindowedSerdes` to avoid duplicates (we have the same function in `SessionWindowedKStreamImpl.java`). BTW in #5521 I just inlined each call, but I think extracting it is also fine."
221346600,5693,guozhangwang,2018-09-28T18:42:16Z,"Why only passing the windows object (for its length) here, but not in other callers below?"
221347082,5693,guozhangwang,2018-09-28T18:44:00Z,This reminds me of the `LRUCacheEntry` class used for caching.. could we consolidate these two?
221348235,5693,guozhangwang,2018-09-28T18:48:15Z,"Can we just use `org.apache.kafka.streams.processor.internals.Stamped`? They seem very similar (feel free to rename the class if you like other names better: since it is internal classes, we can change it whenever we want.)"
221350777,5693,guozhangwang,2018-09-28T18:57:17Z,Do we assume we will only remove the head of the iterator? If not I'm not clear why we can simply set the minTimestamp as the next key's timestamp.
221351979,5693,guozhangwang,2018-09-28T19:01:31Z,EDIT: it seems the above assumption is true from the other classes. In this case could we guard against the unexpected case if there are un-deleted entries before the current position?
221352155,5693,guozhangwang,2018-09-28T19:02:14Z,nit: I'd suggest putting the size calculation of `ContextualRecord` inside the `ContextualRecord` class instead of in this class.
221376160,5693,guozhangwang,2018-09-28T20:42:21Z,Do we ever expect the passed in not-null valueSerde is a `FullChangeSerde` already? If not we should wrap it with `FullChangeSerde` still.
221377817,5693,guozhangwang,2018-09-28T20:49:51Z,"We've encountered some issues related to the ordering of this before: https://issues.apache.org/jira/browse/KAFKA-4492

Could you read that ticket and double check if flush-first-remove-later would not cause any issues for re-entrant puts on the same buffer (say, if we have a loop in the topology)?"
221378172,5693,guozhangwang,2018-09-28T20:51:25Z,"Should we clear the buffer upon closing? Maybe it does not make a difference on correctness, but would it worthy for performance?"
221380146,5693,vvcephei,2018-09-28T21:00:17Z,"The one in SessionWindowedKStreamImpl is actually different (wraps it with a SessionWindowedSerde).

FWIW, I think inlining it is actually preferable to extracting it to a ""common"" location if it's actually just going to have one use."
221380482,5693,vvcephei,2018-09-28T21:01:45Z,This was an oversight. Thanks for the catch!
221381920,5693,vvcephei,2018-09-28T21:08:17Z,"It is similar, but the LRUCacheEntry tracks `isDirty` that would be confusing in this context, so I wouldn't use LRUCacheEntry here, but we could go the other way and make LRUCacheEntry wrap ContextualRecord instead of storing the value + context itself.

Let me know if this sounds good to you... I'll go ahead and optimistically code it up."
221382215,5693,vvcephei,2018-09-28T21:10:01Z,"Yeah, this sounds good."
221384089,5693,vvcephei,2018-09-28T21:19:35Z,"Hmm. Actually, Stamped has unusual implementations of equals, hashcode, and compareTo. They all disregard the stamped value and are only determined by the timestamp...

So, Stamped won't provide the semantics we need from TimeKey, and I'm afraid to change the equals/hashcode/compareTo of Stamped and messing up _its_ semantics...

WDYT?"
221384963,5693,vvcephei,2018-09-28T21:24:08Z,"aaah, yes. This min-timestamp update does depend on always removing the head of the iterator. I'll fix it.

Thanks."
221386195,5693,vvcephei,2018-09-28T21:31:16Z,"This computation makes use of the fact that this reference is a `ContextualRecord<byte[]>`, the value type is generic in ContextualRecord. Of course, this is the only usage of that class, so, I could just build the `byte[]` value type into ContextualRecord.

But I'm slightly in favor of keeping it as-is so we can use ContextualRecord in other contexts where we need both the value (not serialized) and the context in the future. WDYT?"
221387849,5693,vvcephei,2018-09-28T21:39:38Z,"This would mean that they have configured the `default.value.serde` as a FullChangeSerde, which is in the `internals` package.

Nevertheless, it doesn't hurt to guard it. Will do."
221389162,5693,guozhangwang,2018-09-28T21:46:57Z,"I meant to have `ContextualRecord` contains its only computeSize() function which caluclates the size of bytes ""except"" the value size, which can then be called by this function, and here we only need to calculate the key size and value size plus whatever returned from `ContextualRecord#computeSize`. Anyways, it is a nit comment and I do not feel strong about it."
221389407,5693,guozhangwang,2018-09-28T21:48:17Z,"Yeah I point is that is seems ""impossible"" that the passed in serde will be a `FullChangeSerde` but just the inner serde used for `FullChangeSerde`, so we should always wrap (either the default one from config, or the inherited one) with the `FullChangeSerde`, right?"
221390374,5693,guozhangwang,2018-09-28T21:54:10Z,"Do we need to require value ordering for `TimeKey` here? I thought it is not required as they are not following offset ordering to break ties anyways, right?"
221390459,5693,guozhangwang,2018-09-28T21:54:38Z,`make LRUCacheEntry wrap ContextualRecord` yeah that sounds good.
221390529,5693,guozhangwang,2018-09-28T21:55:07Z,ack.
221391967,5693,vvcephei,2018-09-28T22:03:46Z,"Interesting! That issue seems to be cache-specific: that two subsequent processors can be backed by the same cache (as in the join case).

I don't think loops are generally allowed in the subtopology, are they? If so, this code would indeed result in an infinite loop or possibly a concurrent modification exception.

I was concerned that the remove might be sent to the buffer's changelog record collector and maybe sent to the broker, and then some exception might happen before the forward, resulting in the record being forgotten upon restart.

I looked at some other processors, and they tend to do (logged) store operations first and then forward last. But then again, normal operations are forwarding a value that's a direct consequence of processing the _current_ record, which wouldn't have been committed and would therefore get re-processed upon restart.

But the buffer is forwarding some older record, which has already been committed. Reprocessing the new record (which caused the eviction the first time) won't cause us to remember the old record, which we were supposed to emit.

Under EOS, if we crash after the changelog update but before the forward, we'll be fine because the changelog update won't be visible (it'll be in an aborted transaction) on restart, so the buffer will go back to it's correct starting point for reprocessing the new record.

If we can't be sure that Streams subtopologies are acyclic, then I reckon we'd better swap these two lines and tell people they'd better use EOS if they want to be protected from all crash corruption (which I think is true anyway).
Otherwise, if subtopologies are acyclic, then I think it's better to leave it as is.

WDYT?"
221392110,5693,vvcephei,2018-09-28T22:04:40Z,"Yeah, I wasn't sure. I'll go ahead and do it."
221392505,5693,vvcephei,2018-09-28T22:07:08Z,"Actually, let's defer this to Part 4, where the buffer becomes a proper store, and has its own `close()` method."
221393704,5693,vvcephei,2018-09-28T22:15:25Z,"I don't _think_ that will work...

`Comparable` requires a total ordering and also specifies that `a.compareTo(b) == 0` iff `a.equals(b)`, which in turn requires that `a.hashCode() == b.hashCode()`.

But this would prevent us from inserting two different keys with the same time into our buffer map. It doesn't seem like `Stamped` is suitable for map keys or set entries for this reason."
221405676,5693,vvcephei,2018-09-28T23:59:18Z,"Ok, this is done now."
221408442,5693,vvcephei,2018-09-29T00:41:29Z,"Ok, I put in a guard.

I also refactored the interface to purely evict the head of the buffer while a condition holds, which cleans up the usage quite a bit.

Let me know what you think."
221408530,5693,vvcephei,2018-09-29T00:43:01Z,"Since the part of the ContextualRecord that isn't the value is just the ProcessorContext, I just added a `sizeBytes()` method there.

WDYT?"
221408651,5693,vvcephei,2018-09-29T00:45:27Z,"oh, I gotcha. The type of valueSerde is already a FullChangeSerde. In the case of an inherited serde, it gets wrapped in the constructor. The types ensure that the constructor arg is not already a FullChangeSerde."
221412551,5693,vvcephei,2018-09-29T02:23:50Z,"Yes, I think that's a good plan. I agree on the reduncancy, but I wanted to keep the serde-related perturbations to a minimum so we wouldn't distract from the PR."
221478048,5693,mjsax,2018-09-30T23:44:40Z,Just reviewed #5521 again -- left some more comments.
221478098,5693,mjsax,2018-09-30T23:47:24Z,"I actually think that forward before delete is correct. Compare: https://issues.apache.org/jira/browse/KAFKA-5315 and the corresponding PR, that we never finished."
221492758,5693,mjsax,2018-10-01T04:15:54Z,nit: `castOrWrap`
221492805,5693,mjsax,2018-10-01T04:16:41Z,Why this change? (Just for my own education.)
221493048,5693,mjsax,2018-10-01T04:19:29Z,Could we extend `wrapOrCast` to add a `null` check and return `null` for this case and use it here to make code more readable?
221493375,5693,mjsax,2018-10-01T04:24:31Z,"I think we need to call `put` only if `previousKey == null`? Ie, we could merge L103 ad L105 into an if-then block? Might be more readable?"
221493496,5693,mjsax,2018-10-01T04:26:03Z,"This check for `previousKey == null` could be merged with the check from above? (It's hot code path, so might be worth to unify.)"
221493640,5693,mjsax,2018-10-01T04:28:03Z,Different thought: why do we need to call `remove` above explicitly? `put` would return the old/replace value anyway if there is any -- would avoid one tree-traversal?
221493984,5693,mjsax,2018-10-01T04:32:40Z,"I don't see the advantage of using generics in `ContextualRecord` is it's only used once with `byte[]` types. As generic types are lost after compilation, I would prefer to remove the generic if not needed (AFAIK, generics have some runtime overhead as the compiler needs to insert casts that are evaluate during runtime.)"
221494361,5693,mjsax,2018-10-01T04:38:14Z,"This value should only be `0` or `1` -- maybe use a boolean instead? Also wondering, if we need this at all? Have the gut feeling, that `last != null` and `nextCount != 0` is the same thing?"
221494851,5693,mjsax,2018-10-01T04:45:43Z,"if `next()` is called twice in a row without `remove()` in between, `nextCount` could be larger than 1 and thus we should throw -- seems that the current code enforces a `next-remove-next-remove...` pattern? If yes, why?"
221494894,5693,mjsax,2018-10-01T04:46:12Z,Should this be set to `1` instead of incrementing?
221495017,5693,mjsax,2018-10-01T04:48:07Z,"See my other comments -- it's still unclear from the code that we want to enforce `next-remove-...` pattern -- might also be worth to add a JavaDoc to the iterator about correct usage, even if it's an internal class only."
221496039,5693,mjsax,2018-10-01T05:02:37Z,"I am wondering, if `suppressDurationMillis` is a valid config? I had problem to understand this part in the original PR already. Can you explain once more? (maybe it's an indicator that we should add a comment explaining the cases we are handling here?)"
221496235,5693,mjsax,2018-10-01T05:05:34Z,`and` -> `or` or `and/or` ?
221496719,5693,mjsax,2018-10-01T05:11:06Z,"Should we inline this method?

Also, I am wondering if we could/should call this unconditionally? If `overCapacity()` is true, we might or might not expire records here (same if called unconditionally). If `overCapacity()` is false, but `buffer.minTimestamp() <= expiryTime` is true, we would expire record (same if called unconditionally). If both are false, `drainExpriredRecords()` would not expire anything if called either, because it passed in the corresponding boolean predicate anyway?

Ie, I _think_ we can just remove the `if` condition and execute the `then` part always"
221496860,5693,mjsax,2018-10-01T05:13:00Z,nit: `next` -> `evictedRecord` or just `record` ?
221497240,5693,mjsax,2018-10-01T05:17:31Z,nit: `deserializedKey` -> `key` and `key` -> `rawKey` ?
221497342,5693,mjsax,2018-10-01T05:18:50Z,Should we cast here and keep `BufferConfig bufferConfig` as member type?
221497656,5693,mjsax,2018-10-01T05:22:47Z,"While I think, it's semantically fine, it might be nice to get the same eviction behavior for a reprocessing use-case... I am also realizing, that `TimeKey` is actually always used with `Bytes` -- thus, I would recommend to remove the generic type, and exploit that `Bytes` implements `Comparable` already."
221498033,5693,mjsax,2018-10-01T05:27:24Z,"`key` is always `Bytes()`, thus, this output is not very useful. Can we can hold on the deserialized for human readable output here?"
221498255,5693,mjsax,2018-10-01T05:30:19Z,"Each Java object has a natural overhead -- might be worth to add this here? would need to search the internet how many bytes, however, we would have it for `ProcessorRecordContext` itself, as well as `topic`, `headers` (including it's nested `Header` objects)."
221498409,5693,mjsax,2018-10-01T05:32:06Z,"a `String` also store the length (it's a `char[]` internally) -- should we add 4 more bytes here?

Also, has a `char[]` similar overhead than a regular object?"
221498827,5693,mjsax,2018-10-01T05:36:55Z,`value` is always `byte[]` -- can we get a handle on the deserializer to get human readable output here? (one more reason to avoid generic if not necessary -- those issues slip easily with missing type information).
221498993,5693,mjsax,2018-10-01T05:39:01Z,"Should we add 4 byte to store array size? Also, do we have object overhead for an array type?"
221499058,5693,mjsax,2018-10-01T05:39:49Z,Should we add object overhead for `context` itself? (might be included in `sizeBytes()` if we update is accordingly thought)
221640642,5693,vvcephei,2018-10-01T14:55:34Z,"It's just evidence of my mental slowness...

In the prior PR, Guozhang pointed out that my calling `buffer.array()` was incorrect, since the backing array isn't guaranteed to be exactly within the bounds we allocated. I fixed it at the time by delegating to the `ByteBufferSerializer`, which handles this.

Later on I realized that there is a more efficient solution available. By pre-creating the backing array and wrapping it, we know that `buffer.array()` returns what we needed. No need for the more general handling logic in `ByteBufferSerializer`."
221640973,5693,vvcephei,2018-10-01T14:56:22Z,I can and will.
221642753,5693,vvcephei,2018-10-01T15:00:59Z,"I've added that check because `context.valueSerde()` (called elsewhere) could return null.

If it's ok with you, though, I prefer the current code right here. This code ensures that `valSerde` is of the correct type (notice that no casting is necessary). In general, I think we should avoid casting unless we actually need it, as it makes regressions harder to catch."
221646019,5693,vvcephei,2018-10-01T15:09:46Z,"This is true about `put`, but we still need to choose a key to insert into `sortedMap`. If I don't declare the `nextKey` variable, I need to have a bunch of redundant code in the if and else blocks:
```java
final TimeKey<Bytes> previousKey = index.get(key);
        // non-resetting semantics:
        // if there was a previous version of the same record,
        // then insert the new record in the same place in the priority queue
        if (previousKey == null) {
            final TimeKey<Bytes> nextKey = new TimeKey<>(time, key);

            index.put(key, nextKey);
            sortedMap.put(nextKey, value);

            minTimestamp = Math.min(minTimestamp, nextKey.time());
            memBufferSize =
                memBufferSize
                    + computeRecordSize(key, value);
        } else {
            final ContextualRecord<byte[]> removedValue = sortedMap.remove(previousKey);
            sortedMap.put(previousKey, value);
            memBufferSize =
                memBufferSize
                    + computeRecordSize(key, value)
                    - (removedValue == null ? 0 : computeRecordSize(key, removedValue));
        }
```

IMHO, this is less readable than the linear version where we just reuse or construct the key in line 103."
221646437,5693,vvcephei,2018-10-01T15:10:56Z,"But if, after looking at it, you prefer the branching version, I'll change it."
221648943,5693,vvcephei,2018-10-01T15:17:41Z,"Please see my comment above. I agree it's more efficient to have just one branch, but I do think this version is easier to follow.

Regardless, you have a fresher perspective. If you prefer the branching version above, I'm happy to change it."
221650193,5693,vvcephei,2018-10-01T15:21:04Z,"Ok, apparently the way to convince me is to point out three reasons...

I'll switch it out for the branching version."
221655192,5693,vvcephei,2018-10-01T15:34:50Z,I didn't consider this runtime overhead. I'll go ahead and inline the generic type.
221655988,5693,vvcephei,2018-10-01T15:37:06Z,"It is ok to call next multiple times, but if you do, you can't subsequently call remove.

I don't think that we can learn whether next has been called twice by looking at any of the other fields we're maintaining."
221661624,5693,vvcephei,2018-10-01T15:52:56Z,"yes. This is an optimization to support maximal efficiency in:
* removing some unknown number records, each of which is currently the minimum in the buffer when it gets removed
* maintaining a correct value of `minTimestamp`.

As far as we know right now, we will only ever need to remove the min records from the buffer. I.e., I don't think we need to iterate for a while and *then* remove.

But we may need to remove more than one record, and we won't know if we need to remove the *next* record until after we remove *this* record.

Previously, I didn't have this guard, but in that case, we can't just set `minTimestamp` to the buffer time of the next record upon removing. Because we don't know whether the record we just removed is the leftmost record in the tree without traversing it again. Because of that, I had to avoid updating `minTimestamp` until you close the iterator (and therefore it had to be a `CloseableIterator`). This means that the KTableSuppressProcessor couldn't just keep popping records while the minTimestamp was less than the desired boundary, it had to get the ""buffer time"" from the TimeKey and make its decision from that.

All in all, it's way cleaner this way, with the expense of that one extra guard.

I could go one step further and make it like a ""predicated, consuming iterator"", which just pops records out as long as the predicate condition is true. Do you think this would be more straightforward?"
221661740,5693,vvcephei,2018-10-01T15:53:15Z,no; see the reply above.
221662088,5693,vvcephei,2018-10-01T15:54:14Z,"From your later comments, it seems like you would say it would be more straightforward. I'll go ahead and simplify it."
221667743,5693,vvcephei,2018-10-01T16:11:01Z,I think the complexity is due to my over-flexible time definition. I'll drop it and then we'll see if it's still non-obvious what's going on here.
221690047,5693,mjsax,2018-10-01T17:22:54Z,I guess it's personal taste -- don't insist on a change.
221690536,5693,mjsax,2018-10-01T17:24:31Z,"Think, even without the branching, this `remove` and the `put` below should be merged."
221690691,5693,vvcephei,2018-10-01T17:25:03Z,"Ok, I've decided that this optimization is premature and complex, so I've gone ahead and simplified it. (I'll let you know when I push the update).

I've also updated the TimeDefinition class to be less flexible (although it doesn't really simplify this particular method).

FWIW, though I think that ""suppress for 0ms"" is a perfectly valid way to disable a suppression operation. Note that this is also what we wind up with when you use final-results on a windowed stream with gracePeriod set to 0ms, which also seems perfectly fine."
221691013,5693,vvcephei,2018-10-01T17:26:10Z,I added the missing punctuation instead.
221691019,5693,mjsax,2018-10-01T17:26:11Z,"> It is ok to call next multiple times, but if you do, you can't subsequently call remove.

This breaks the iterator contract and should be well documented"
221692098,5693,vvcephei,2018-10-01T17:29:27Z,"I wanted to save on setting up the iterator, but your comment made me realize we can and should do that with an initial `if (predicate.get())` inside `evictWhile`. I did this and removed the condition as you recommended."
221692337,5693,vvcephei,2018-10-01T17:30:09Z,Good point. I called it `toEmit`.
221693619,5693,vvcephei,2018-10-01T17:34:29Z,"This made me realize that I named them `Impl` when I meant to name them `Internal`. In other words, both `SuppressedInternal` and `BufferConfigInternal` to indicate that these are the internal interfaces."
221694264,5693,vvcephei,2018-10-01T17:36:43Z,I didn't notice that. That is handy.
221696756,5693,vvcephei,2018-10-01T17:44:39Z,"I agree that this is an under-estimate, but I don't think there's much point in being exact.
The overhead is dependent on the JVM implementation, so we'd have to detect the JVM and maintain a mapping for each different implementation. Even then, we don't know how much extra memory we're using in the various garbage collectors, of which there are now three different implementations in the Oracle JDK alone...

I'd rather just make the best effort we reasonably can to live more-or-less within the desired boundary. For example, storing the `byte[]` value is much closer than storing the object. But beyond that, we get into diminishing returns for quickly increasing complexity.
"
221697255,5693,vvcephei,2018-10-01T17:46:16Z,"I believe arrays also store their types. But again, we are getting into JVM implementation details. There are too many JVM implementations for us to be expected to worry about this, IMHO."
221698960,5693,vvcephei,2018-10-01T17:51:38Z,"I didn't consider this overhead, and agree it would be good to get rid of it."
221699155,5693,vvcephei,2018-10-01T17:52:12Z,"I don't think the record needs to know how to deserialize itself.

Since `toString` is only for debugging, I'm fine printing out the `Arrays.toString` summary of the value.

If we wanted to print out the value in a log message, we would format it more specifically (including a deserialization if desired).

That said, I will go ahead and get rid of the generic type."
221701912,5693,vvcephei,2018-10-01T18:00:46Z,as above.
221702092,5693,vvcephei,2018-10-01T18:01:16Z,"It would be the responsibility of the context to account for its own overhead, but see my comments above."
221756263,5693,vvcephei,2018-10-01T21:04:23Z,"Roger that. It's moot now, since I've removed this iterator entirely."
221757521,5693,vvcephei,2018-10-01T21:09:03Z,"I think in sum, your points elevate it beyond personal taste. I've gone ahead and done the branching. After a little cleanup, it's not too shabby anyway."
221792731,5693,vvcephei,2018-10-02T00:01:39Z,"I had to add these so that suppress doesn't ""forget"" the window end time when it round-trips the record."
221793798,5693,mjsax,2018-10-02T00:08:26Z,Ack. That's fair. The existing caches also use rough estimates only. (Might be interesting how much we are off though... But this could be a follow up improvement.)
221794070,5693,mjsax,2018-10-02T00:10:24Z,"> only for debugging

My point is, that even for debugging it's not useful to print `byte[]` -- my argument is, to either ""fix this"" or don't overwrite `toString()` at all."
221794878,5693,mjsax,2018-10-02T00:16:14Z,Is this mentioned in the KIP? It's a public API change.
221795634,5693,mjsax,2018-10-02T00:21:39Z,Not sure if this is the best way to tack it? Requires public API change.
221796847,5693,mjsax,2018-10-02T00:29:57Z,I guess we can remove this generics?
221797359,5693,mjsax,2018-10-02T00:33:39Z,`nextKey.time()` -> `time`
221797795,5693,mjsax,2018-10-02T00:37:11Z,"Should we compute `bufferTime` within `buffer()` -- no need to pass it in, as both `internalProcessorContext` and `key` are available there, too?"
221797893,5693,mjsax,2018-10-02T00:38:08Z,So we need this here? No need to pass it into `enforceConstraints()` IMHO.
221798063,5693,mjsax,2018-10-02T00:39:28Z,Should this be `<` instead of `<=` ?
221798280,5693,mjsax,2018-10-02T00:41:14Z,Guess we can remove variable `key` (only used once).
221798347,5693,mjsax,2018-10-02T00:41:45Z,`key1` -> `key` and `key.get()` -> `toEmit.key.get()`
221798908,5693,mjsax,2018-10-02T00:46:05Z,"Similar argument as for `byte[]` value: Of course, here we still get the `time` information, but the `Bytes` `key` is useless."
221800768,5693,mjsax,2018-10-02T01:00:20Z,Why remove this comment? Seems to be valid?
221800971,5693,mjsax,2018-10-02T01:01:37Z,"The change makes sense -- test was bubby before, but we did not notice at it threw anyway?"
221801525,5693,mjsax,2018-10-02T01:06:01Z,What was the original intend of this part? And why don't we need it?
221802094,5693,mjsax,2018-10-02T01:10:39Z,Why `timestamp - 1L` ?
221805529,5693,vvcephei,2018-10-02T01:38:54Z,"Ah, no. When I did this before, I did it differently to keep it private. I thought this was a better way, but overlooked the public-ness of it.

I'll go back to private mode."
221805956,5693,vvcephei,2018-10-02T01:43:11Z,"Ah, yeah, it was previously used also here, but it's not needed anymore. Good catch."
221805975,5693,vvcephei,2018-10-02T01:43:21Z,same here. Thanks!
221806806,5693,vvcephei,2018-10-02T01:49:57Z,"It wouldn't be wrong, but I think `<=` is also right, and it's a tighter bound.

Let's say we have buffered an event with time 10 at stream time 10 and the suppressDuration is 1. The expiry time is `10-1 = 9`. minTimestamp is 10, and `10 <= 9` is false, so we don't evict.

Then, we get an event with time 11 at stream time 11. Now, the expiry time is `11-1=10`. minTimestamp is still 10, but now the check is `10 <= 10`, so we evict that first event.

I think this matches up to the intention of saying ""suppress for 1 ms""."
221807188,5693,vvcephei,2018-10-02T01:52:48Z,"It's not anymore. Now, we buffer the new event before we enforce the buffer constraints, so we return the more intuitive most recent state of `""v1"", 1L, 2L` right away, instead of later on."
221808272,5693,vvcephei,2018-10-02T02:01:52Z,"We didn't throw it away before, just emitted it later on. This is what the comment I removed was explaining."
221808394,5693,vvcephei,2018-10-02T02:02:54Z,"When we enforced constraints before buffering, we needed one extra tick to flush everything out. Now that we buffer first, everything happens more promptly, so we don't need this last cycle to witness all the results we're looking for."
221808532,5693,vvcephei,2018-10-02T02:04:09Z,"It doesn't matter for anything, it just seemed weird to have window start == window end. The window end is the time that matters for this test, which is why I made it the baseline."
221811667,5693,mjsax,2018-10-02T02:34:28Z,Ack
221812576,5693,mjsax,2018-10-02T02:44:07Z,"We set record timestamp to `timestamp` -- thus, the record will be put in window `[timestamp, timestamp+1)`, right? Seems weird to use the wrong window IMHO. Or do I miss something?"
221813111,5693,vvcephei,2018-10-02T02:49:17Z,"Ok, I wasn't thinking about it like this. It makes sense."
221813409,5693,mjsax,2018-10-02T02:52:09Z,I see. The comment focus on the second `v1` -- I applied it to the third `v1`. Seems the comment was ambiguous :)
221813838,5693,vvcephei,2018-10-02T02:56:43Z,Good thing it's gone!
221821211,5693,guozhangwang,2018-10-02T04:13:10Z,Makes sense.
231290134,5821,lindong28,2018-11-06T20:58:29Z,Exception and its corresponding error code is part of public interface. Can you update design doc as appropriate and reply to the email thread with this change?
231292242,5821,lindong28,2018-11-06T21:05:09Z,"Currently all epoch fields (e.g. controller epoch, leader epoch) uses int32. Would it be more consistent and space efficient to use int32 for broker epoch as well? Max int32 value is more than 2 billion which seems large enough for broker epoch.

If we change the type of broker epoch from int64 to int32, can you also update the design doc and reply to the email thread?"
231294160,5821,lindong28,2018-11-06T21:11:32Z,"Since we are modifying the schema here, it may be a good time to use the new way of specification as shown in FetchRequest for consistency. Then we can use `struct.getOrElse(...)` here. It is specifically preferred to make this refactor together with the change in this PR if the existing code footprint is small (e.g. `StopReplicaRequest.java`)."
231294372,5821,lindong28,2018-11-06T21:12:14Z,nits: can you add an empty line between these two methods?
231300243,5821,lindong28,2018-11-06T21:30:41Z,nits: it seems a bit more consistent with the existing style (e.g. `PRODUCE_RESPONSE_V4 = PRODUCE_RESPONSE_V3` in ProduceResponse.java) to do `LEADER_AND_ISR_RESPONSE_V2 = LEADER_AND_ISR_RESPONSE_V1`. It is probably more readable as well since we typically want to see how the new schema compares with the previous version.
231304928,5821,lindong28,2018-11-06T21:45:58Z,"nits: for consistency with the exiting style, can we use `UPDATE_METADATA_RESPONSE_V5 = UPDATE_METADATA_RESPONSE_V4`?"
231306661,5821,lindong28,2018-11-06T21:51:11Z,"It seems that even if we do this filter, the broker may still go offline after this step and before controller sends the request to the broker. So we still need to have this filter logic later. Could you explain the benefit of having this logic here?"
231319305,5821,lindong28,2018-11-06T22:33:29Z,Would this be more consistent and readable to move this logic to the class `ControlledShutdown`? This can also ensure that the `brokerEpochsCache` is accessed only by the controller event thread after controller is initialized.
231323345,5821,lindong28,2018-11-06T22:48:40Z,nits: there is one extra space after `=`
231329438,5821,lindong28,2018-11-06T23:13:47Z,"Currently `brokerEpoch` is a `var` and its internal state is also immutable. It is generally preferred to allow mutation in only one way.

Since `brokerEpoch` has its initial value from `KafkaServer.startup()` and it can be updated multiple times in `RegisterBrokerAndReelect`, would it make sense to define `@volatile var brokerEpoch: Int` in `KafkaController` similar to the existing `brokerInfo` field. `KafkaServer.startup()` can get the initial value of the broker epoch as integer and passes it to the `KafkaController` constructor as `initialBrokerEpoch`. This approach seems much simpler and we would not need the helper class `BrokerEpoch`."
231332131,5821,lindong28,2018-11-06T23:26:01Z,"I am wondering if it will be more intuitive and cleaner to move the logic of checking broker epoch from ReplicaManager to KafkaApis. Currently there is already logic such as `controller.isActive` in KafkaApis which is similar to the logic of checking broker epoch. And if we do that, we can keep ReplicaManager unchanged. KafkaApis can first compare the epoch from the StopRepliaRequest with the epoch in `controller.brokerEpoch` before invoking e.g. `ReplicaManager.stopReplicas(...)`.

Controller can have API such as `controller.isActiveBrokerEpoch(epoch)` to make the logic more explicit."
231339976,5821,lindong28,2018-11-07T00:02:54Z,"nits: `zookeepr` has typo. Would the message `s""$request does not need controller epoch check""` be more appropriate here?"
231343743,5821,lindong28,2018-11-07T00:23:49Z,"Now we have three methods named `retryRequestsUntilConnected(...)`. I am wondering if it would be more readable to keep the number still as two, one for single request and the other for sequence of requests. One thing that may be confusing to the reader is that, `retryRequestsUntilConnected[Req <: AsyncRequest](requests: Seq[Req])` does not take `expectedControllerZkVersion` as parameter and thus it is not clear what is the expected behavior with controller epoch check in this method."
231344408,5821,lindong28,2018-11-07T00:27:31Z,"Can we also throw `IllegalStateException` if `zkOpResults` does not match the pattern `Seq(ZkOpResult(checkOp: CheckOp, checkOpResult), zkOpResult)`?"
231348543,5821,lindong28,2018-11-07T00:50:29Z,"Now the patch is not longer using zookeeper transaction, will there be issue if e.g. controller znode is created but the controller epoch is not incremented?"
231352782,5821,hzxa21,2018-11-07T01:14:05Z,The reason why we use int64 for broker epoch is that the `czxid` we get back from zookeeper is of int64 type. I think it is not a good idea to convert it into int32 because we may lose the globally unique and monotonically increasing guarantee.
231365666,5821,hzxa21,2018-11-07T02:45:38Z,"After introducing broker epoch, we need to fill in the broker epoch we want to use for the control request we send out. Only brokers in `controllerContext.liveOrShuttingDownBrokerIds` will have a entry in the `controllerContext.brokerEpochsCache`.

The benefit of this change is to ensure that we can always get back the broker epoch from the cache in controller context when constructing the control requests. We can instead add extra logic in `controllerChannelManager.sendRequestsToBrokers` to check the existence of the broker epoch when constructing the request but I think it is cleaner and easier to reason about the code with this change because we will not send out requests to brokers that are not in `controllerContext.liveOrShuttingDownBrokerIds` anyway. 

It is true that the broker can become offline and this will cause RequestSendThread to fail to send out the request if controller doesn't process the broker change event before RequestSendThread sending out the request to the offline broker. This change will not affect this behavior and does not aim to solve this race condition. This change only acts as a pre-filter to ensure we can always construct the control request with broker epoch. Whether we can actually send out the request is a separate issue."
231397289,5821,lindong28,2018-11-07T06:58:52Z,"Sounds good. I am not sure when brokerId in the `brokerIds` will be negative. Since we don't expect any brokerId in `liveOrShuttingDownBrokerIds` to be negative, can we just do `brokerIds.filter(controllerContext.liveOrShuttingDownBrokerIds.contains)` to simplify the code here? Same for `addStopReplicaRequestForBrokers()` and `addUpdateMetadataRequestForBrokers()`."
231397355,5821,lindong28,2018-11-07T06:59:11Z,Yeah I forgot this reason. Sounds good.
231609250,5821,hzxa21,2018-11-07T17:47:41Z,That is a good point. I think we can simplify it.
231624498,5821,hzxa21,2018-11-07T18:28:48Z,Thanks for the suggestion. It makes sense and I will move it.
231629225,5821,hzxa21,2018-11-07T18:42:53Z,"Currently `brokerEpoch` can be updated in `KafkaServer.startup()` as well as `RegisterBrokerAndReelect` in the controller, and it can be read by `ReplicaManager` in order to reject outdated control requests. If we store `brokerEpoch` in `KafkaController`, it requires passing the `KafkaController` object to `ReplicaManager` just for reading `brokerEpoch`. I think use a helper class is simpler in this case. What do you think?"
231629808,5821,hzxa21,2018-11-07T18:44:39Z,"Miss your next comment. If we do the check in `KafkaApis`, then you are right. Please ignore the comment I just wrote down."
231629969,5821,hzxa21,2018-11-07T18:45:06Z,Makes sense. That is a good point.
231713300,5821,hzxa21,2018-11-07T23:09:38Z,"I am a little bit confused about your concern. There is no controller epoch check in `retryRequestsUntilConnected[Req <: AsyncRequest](requests: Seq[Req])` because this is the raw method that only does send requests as well as receive responses, and the epoch check happens outside of this method when calling `wrapRequestWithControllerEpochCheck` and `unwrapResponseWithControllerEpochCheck`. Maybe I understand you wrong, I think there is little confusion here.

Do you suggest only have `retryRequestsUntilConnected[Req <: AsyncRequest](requests: Seq[Req], expectedControllerEpochZkVersion: Int)` and `retryRequestsUntilConnected[Req <: AsyncRequest](request: Req, expectedControllerEpochZkVersion: Int)`?"
231724344,5821,hzxa21,2018-11-08T00:02:31Z,It is fine because we are using the zookeeper multi op directly right now. It is essentially the same as using zk transaction so we still provide the same guarantee.
232446499,5821,hzxa21,2018-11-10T08:53:01Z,Done.
232446521,5821,hzxa21,2018-11-10T08:54:07Z,Sure. I have adopted the new pattern in all control requests.
232446524,5821,hzxa21,2018-11-10T08:54:13Z,Fixed.
232446526,5821,hzxa21,2018-11-10T08:54:17Z,Fixed.
232446530,5821,hzxa21,2018-11-10T08:54:22Z,Fixed.
232446539,5821,hzxa21,2018-11-10T08:54:30Z,Done.
232446540,5821,hzxa21,2018-11-10T08:54:34Z,Done.
232446542,5821,hzxa21,2018-11-10T08:54:38Z,Fixed.
232446547,5821,hzxa21,2018-11-10T08:54:45Z,Fixed.
232446550,5821,hzxa21,2018-11-10T08:54:49Z,Done.
232446551,5821,hzxa21,2018-11-10T08:54:54Z,Fixed.
232446554,5821,hzxa21,2018-11-10T08:55:02Z,Sure. Done.
232477712,5821,lindong28,2018-11-11T08:02:17Z,"The exception name is inconsistent with name specified in KIP-380.

I feel that it is better to use the `STALE_CONTROLLER_EPOCH` which suggests that the broker epoch in the request is smaller than the expected value. Since we do not expect the epoch in the request to be larger than the expected value, it would be IllegalStateException if the epoch in the request is larger than the expected value."
232477749,5821,lindong28,2018-11-11T08:04:01Z,nits: can we follow the existing code style and move `BrokerEpochMismatchException::new` to a new line?
232477941,5821,lindong28,2018-11-11T08:11:11Z,"Unlike UpdateMetadataRequest, this field is named `live_leaders` rather than `live_brokers`."
232478024,5821,lindong28,2018-11-11T08:13:45Z,nits: there is an unnecessary space.
232479092,5821,lindong28,2018-11-11T08:50:14Z,nits: there is an unnecessary space.
232479565,5821,lindong28,2018-11-11T09:07:05Z,"Can we do `struct.setIfExists(OFFLINE_REPLICAS, offlineReplicas.toArray())` here?

Also, it seems that `setIfExists(Field.Array def, Object[] value)` and `setIfExists(Field.ComplexArray def, Object[] value)` in `Struct.java` should only set value if the field exists. Can you help fix that?"
232479847,5821,lindong28,2018-11-11T09:15:25Z,nits: would it be simpler to just do `brokerIds.filter(controllerContext.liveOrShuttingDownBrokerIds.contains)`? The extra variable name does not seem useful here.
232480199,5821,lindong28,2018-11-11T09:25:44Z,"We currently uses `controllerContext.brokerEpochsCache` in `sendRequestsToBrokers()` under the assumption that the `leaderAndIsrRequestMap`, `updateMetadataRequestPartitionInfoMap` and `stopReplicaRequestMap` only includes brokerId that is defined in `brokerEpochsCache`. However, the logic to guarantee this is in other methods such as `addUpdateMetadataRequestForBrokers`. 

Would it make the code more readable to put these logics closer together in `sendRequestsToBrokers()`? We can make the logic even more explicit by filtering the brokerId using `brokerEpochsCache` rather than `controllerContext.liveOrShuttingDownBrokerIds` in `sendRequestsToBrokers()`."
232480419,5821,lindong28,2018-11-11T09:32:08Z,Would it be more readable to have method `isBrokerEpochStale`? `isCurrentOrUnknownBrokerEpoch` is a bit verbose and it feels a bit weird to look for unknown broker epoch. `isBrokerEpochStale` would better match the name of `StaleBrokeEpochException`.
232480534,5821,lindong28,2018-11-11T09:34:41Z,Would it be simpler to just name the method `brokerEpoch`? The variable can be named `_brokerEpoch` similar to `_lastCaughtUpTimeMs` in `Replica.scala`.
232480834,5821,lindong28,2018-11-11T09:42:29Z,Currently most variables (e.g. `liveBrokersUnderlying`) in `ControllerContext` provide cached information. It will be a bit inconsistent and confusing if we just put the word `cache` for brokerEpoch. Can we just name it `brokerEpochs`?
232480935,5821,lindong28,2018-11-11T09:44:54Z,nits: can we rename `bid` to `brokerId`? `bid` is an english word and currently the existing code does not use `bid` as shortcut for broker id.
232481318,5821,lindong28,2018-11-11T09:55:51Z,"The log message itself raises concerns for user/developer without explaining why it is at warning rather than error level. Can we add comment that says why this is OK?

And since we expect this to happen normally when broker is restarted quickly, I am not sure we need to log it at warning level. We can ask other reviewer to comment on this later."
232481594,5821,lindong28,2018-11-11T10:04:18Z,"The code can probably be more readable with less nested if/else by doing this:

```
if (!isAuthorizedClusterAction(request)) {
  ...
} else if (!controller.isCurrentOrUnknownBrokerEpoch(...)) {
  ...
} else {
  ...
}
```

Same for other methods.
"
232481972,5821,lindong28,2018-11-11T10:17:56Z,"The code here is comparing the zkversion with epoch, which seems misleading.

Also, `expectedControllerZkVersion < 0`, will `expectedControllerZkVersion` be anything other than `MatchAnyVersion`? If not, it seems better to explicitly check `expectedControllerZkVersion == ZkVersion.MatchAnyVersion`. And if they are not equal, we can throw IllegalStateException if `expectedControllerZkVersion` is negative."
232482145,5821,lindong28,2018-11-11T10:24:33Z,nits: It seems that intellIj complains here. Can you change it to `getSortedBrokerList()`.
232482175,5821,lindong28,2018-11-11T10:25:54Z,"Do we expect `BrokerIdZNode.decode(...).broker` to return null? If not, it may be simpler to just do `Some(BrokerIdZNode.decode(brokerId, getDataResponse.data).broker, getDataResponse.stat.getCzxid)`."
232482475,5821,lindong28,2018-11-11T10:36:38Z,nits: can you add space between `case class` to be consistent with the existing code style?
232482769,5821,lindong28,2018-11-11T10:46:07Z,"Right, that is what I would suggest to reduce the overloaded methods number from 3 to 2. Now looking at it again, the current way also looks good."
232516981,5821,hzxa21,2018-11-12T01:37:08Z,That is a good point. I have changed it back to `STALE_BROKER_EPOCH`. I also move the broker epoch check helper function from `KafkaController` to `KafkaApis` and throw `IllegalStateException` when the broker sees the broker epoch in the request larger than the current epoch.
232516997,5821,hzxa21,2018-11-12T01:37:19Z,Sure. Done.
232517017,5821,hzxa21,2018-11-12T01:37:31Z,Good catch. Fixed.
232517029,5821,hzxa21,2018-11-12T01:37:37Z,Fixed.
232517036,5821,hzxa21,2018-11-12T01:37:42Z,Fixed.
232517050,5821,hzxa21,2018-11-12T01:38:00Z,Sure. Fixed.
232517068,5821,hzxa21,2018-11-12T01:38:20Z,Yes. Fixed.
232517104,5821,hzxa21,2018-11-12T01:38:45Z,Good suggestion. Done.
232517335,5821,hzxa21,2018-11-12T01:41:16Z,I have renamed it and moved this helper function to `KafkaApis` because it will only be called in `KafkaApis` and we will need to throw `IllegalStateException` when the epoch is larger than the expected one. I think it is more readable this way.
232517345,5821,hzxa21,2018-11-12T01:41:21Z,Fixed.
232517357,5821,hzxa21,2018-11-12T01:41:28Z,Sure. Done.
232517373,5821,hzxa21,2018-11-12T01:41:38Z,Sure. Done.
232517606,5821,hzxa21,2018-11-12T01:44:49Z,"Comments added. When the broker sees stale controller epoch in the request, we also log the message at warning level. So I think it is better to do the same thing for stale broke epoch to keep it more consistent."
232517635,5821,hzxa21,2018-11-12T01:45:15Z,That is a good point. Thanks for the suggestion. Done.
232517651,5821,hzxa21,2018-11-12T01:45:23Z,"Fixed. Btw, I think it is better to throw `IllegalArgumentException` if `expectedControllerZkVersion` is negative."
232517736,5821,hzxa21,2018-11-12T01:46:39Z,Fixed.
232517759,5821,hzxa21,2018-11-12T01:46:51Z,Good catch. Fixed.
232517773,5821,hzxa21,2018-11-12T01:46:57Z,Done.
232555865,5821,lindong28,2018-11-12T07:37:31Z,"It will be practically very rare to have `broker` that is not found in `controllerContext.brokerEpochs`. So this trace level logging is probably not useful. My understanding is that we only use trace level logging for something that is almost always triggered.

If we do not have good use-case for this trace level logging, can we simplify the code change here by just adding one line (relative to the original code) to filter the key for `leaderAndIsrRequestMap`. More specifically, we can change the code from

```
leaderAndIsrRequestMap.foreach { case (broker, leaderAndIsrPartitionStates) =>
  ...
}
```

to 

```
leaderAndIsrRequestMap.filterKeys(controllerContext.brokerEpochs.contains).foreach { case (broker, leaderAndIsrPartitionStates) =>
  ...
}
```

Same for `updateMetadataRequestPartitionInfoMap` and `stopReplicaRequestMap`."
232557832,5821,lindong28,2018-11-12T07:47:19Z,"nits: `not equal to current broker epoch` => `smaller than the current broker epoch`.

Same for other logs."
232562259,5821,lindong28,2018-11-12T08:08:05Z,"According to the zookeeper client Javadoc, the name in `CreateResponse` is expected to be `The name of the Znode that was created`. Also, it is mentioned that `On success, <i>name</i> and <i>path</i> are usually, equal, unless a sequential node has been created`.

On the other hand, the Javadoc for `CreateResult` says that, `A result from a create operation.  This kind of result allows the path to be retrieved since the create might have been a sequential create`.

We need to make sure that the `CreateResult.path` has the same value as the original value of the `name` in `CreateResponse` when a sequential node is created. The Javadoc  `CreateResult` suggests this is the case but the name of its variable, i.e. `path`, suggests they are different. Can you double check this by creating a sequential znode?

"
232566446,5821,lindong28,2018-11-12T08:27:51Z,"Should we also update `controllerContext.brokerEpochs` properly in `BrokerModifications.process()`?

To reduce the chance of missing such update in the future, it is probably good to make `brokerEpochs` a private variable. And expose a single method in `ControllerContext` to update `brokerEpochs`, `liveBrokersUnderlying` and `liveBrokerIdsUnderlying` together. This method can replace the existing method `liveBrokers_=(brokers: Set[Broker])` in `ControllerContext`."
232567122,5821,lindong28,2018-11-12T08:30:51Z,"Thinking about this more, it may be better to do the following to explicit show that we want to keep only broker ids that are in `liveOrShuttingDownBrokerIds`.

```
leaderAndIsrRequestMap.filterKeys(controllerContext.liveOrShuttingDownBrokerIds.contains).foreach { case (broker, leaderAndIsrPartitionStates) =>
  ...
}
```"
232764958,5821,hzxa21,2018-11-12T18:25:36Z,"Both zookeeper `CreateRequest` and `CreateResponse` use the field `path` to represent the resulting path (can handle the sequential create case). I think what you think it is confusing is that when we are using zookeeper async create, the `processResult` methond in `StringCallback` has a `path` field to represent the path included in the request and has a `name` field to represent the resulting path.

I double check zookeeper source code and zookeeper client (to be more specific, in `ClientCnxn.java`). The logic to invoke the callback is:
```
...
else if (p.response instanceof CreateResponse) {
                      StringCallback cb = (StringCallback) p.cb;
                      CreateResponse rsp = (CreateResponse) p.response;
                      if (rc == 0) {
                          cb.processResult(rc, clientPath, p.ctx,
                                  (chrootPath == null
                                          ? rsp.getPath()
                                          : rsp.getPath()
                                    .substring(chrootPath.length())));
                      } else {
                          cb.processResult(rc, clientPath, p.ctx, null);
                      }
                  }
...
```
The 2nd argument in `processResult` is `path` and the 4th argument is `name`. This confirms that we use `rsp.getPath()` for the `name` in `processResult`."
232767009,5821,hzxa21,2018-11-12T18:32:10Z,"I actually think of what you have suggested at the very beginning and the reason I didn't do that is that the only place we can update the broker epoch is in BrokerChange event. Broker epoch will only change when the broker ephemeral znode gets deleted and re-created so that is why we use czxid (create transaction id). czxid will only change when we create the broker znode, not when we modify it, so in `BrokerModification` event the czxid will not change. The only place we will capture the czxid change is in `BrokerChange` evnet because we are listening on the children nodes change."
232772273,5821,hzxa21,2018-11-12T18:49:51Z,Agree. Done.
232772296,5821,hzxa21,2018-11-12T18:49:56Z,Fixed.
232782780,5821,lindong28,2018-11-12T19:25:52Z,Sounds good. Thanks for the explanation.
232782818,5821,lindong28,2018-11-12T19:25:59Z,Sounds good. Thanks for the explanation.
233259207,5821,junrao,2018-11-13T23:11:36Z,Could we fix the javadoc above?
233260194,5821,junrao,2018-11-13T23:15:47Z,"It would be useful to avoid _1/_2 for better readability. We could do foreach { case (broker, epoch) => ...}"
233278007,5821,junrao,2018-11-14T00:43:32Z,It would be useful to avoid _1/_2 for better readability.
233278057,5821,junrao,2018-11-14T00:43:47Z,It would be useful to avoid _1 for better readability.
233279558,5821,junrao,2018-11-14T00:52:38Z,"Instead of bouncedBrokerIds(broker.id), it might be clearer to do bouncedBrokerIds.contains(broker.id)."
233281539,5821,junrao,2018-11-14T01:03:18Z,"Hmm, could we just change liveBrokersUnderlying to a Map[Broker, Long] instead of introducing a separate val?"
234024398,5821,junrao,2018-11-15T21:59:47Z,Could this be debug?
234025608,5821,junrao,2018-11-15T22:04:07Z,This probably should be info since we handle it as expected.
234028770,5821,junrao,2018-11-15T22:15:45Z,"Normally, when we call onBrokerFailure(), the passed in deadbrokers won't be in controllerContext.liveBrokers, which is used by onBrokerFailure() through PartitionStateMachine/ReplicaStateMachine. With this change, this may not be true. Will that have any impact?"
234029553,5821,junrao,2018-11-15T22:18:43Z,Should we log the broker epoch in addition to the broker id?
234033660,5821,junrao,2018-11-15T22:34:33Z,Could we use case for e to avoid _1/_2?
234055778,5821,junrao,2018-11-16T00:24:48Z,"This may be an existing problem. If a ZK multi fails because of one of the operations, does that error get reflected in the rc in the top level response or in individual ZkOpResult?"
234057258,5821,junrao,2018-11-16T00:34:27Z,"Do we need to add ()? In general, we only need () for methods with side effects."
234059863,5821,junrao,2018-11-16T00:50:39Z,This logic needs to be done after the response.resultCode match block as before the patch since getAfterNodeExists() can return Code.OK too. 
234061971,5821,junrao,2018-11-16T01:04:43Z,Is it still useful to log in the above line since codeAfterReCreate hasn't changed?
234062220,5821,junrao,2018-11-16T01:06:25Z,typo Abstarct
234064469,5821,junrao,2018-11-16T01:21:21Z,Perhaps those warn should be info since there is nothing for the user to act on this.
234065021,5821,junrao,2018-11-16T01:25:20Z,epoch => brokerEpochInRequest ?
234065222,5821,junrao,2018-11-16T01:26:51Z,There is logging in zkClient.registerBroker(). We could just log the broker epoch there.
234067161,5821,junrao,2018-11-16T01:40:40Z,Are LEADER_AND_ISR_REQUEST_TOPIC_STATE_V0 and LEADER_AND_ISR_REQUEST_PARTITION_STATE_V1 still valid? Perhaps it's simpler to just say we normalized partitions under each topic.
234069244,5821,junrao,2018-11-16T01:54:52Z,This is unnecessary since shutdown() is blocking.
234069643,5821,junrao,2018-11-16T01:56:25Z,"Since the propagation of the ZK event is async, we may need to put the checking logic in a waitUntilTrue() block. Ditto below."
234069804,5821,junrao,2018-11-16T01:57:13Z,The comment seems out of place.
234070541,5821,junrao,2018-11-16T02:02:40Z,Outdated comment?
234070606,5821,junrao,2018-11-16T02:03:07Z,We are not sending stale epoch anymore?
234070761,5821,junrao,2018-11-16T02:04:24Z,Does this need to be volatile?
234071333,5821,junrao,2018-11-16T02:08:28Z,The code in this method is quite similar to that in testControlRequestWithCorrectBrokerEpoch(). Should we merged them somehow?
234071751,5821,junrao,2018-11-16T02:11:44Z,Should this be volatile?
234073100,5821,junrao,2018-11-16T02:21:50Z,This is unnecessary. Great test!
234073572,5821,junrao,2018-11-16T02:25:08Z,indentation
234073709,5821,junrao,2018-11-16T02:26:06Z,indentation
234144660,5821,hzxa21,2018-11-16T09:48:12Z,"This is a very good point. I think it is fine because the bounced broker will reject the control requests anyway because the cached broker epoch has not been updated yet. 

However, this brings up another question: do we actually need to call `onBrokerFailure()` for bounced brokers? 
After a second thought, I think the answer is no because the end goal of controller handling bounced brokers in BrokerChange event is to make sure the quickly bounced brokers will be initialized correctly and the end partition/replica states will be the same with and without calling `onBrokerFailure` for the bounced brokers (if there are no new brokers and dead brokers). In this case, only calling `onBrokerStartup` is sufficient. Invoking `onBrokerFailure` first is a correct and safe option but it comes with some overhead because we need to perform leader election and send out the StopReplica/LeaderAndIsr/UpdateMetadata, which are not necessary. Previously I thought that missing `onBrokerFailure` will cause correctness issue because we might miss some state clean up but looks like it is not the case. Also note that if we use controlled shutdown to shutdown and restart the broker, the leadership election actually happens before processing the BrokerChangeEvent.

TL;DR:
To be more specific for your original question (why updating the live brokers first then invoke `onBrokerFailure` is fine), there are three places where we use the live brokers informartion in `onBrokerFailure`:
1. Determine whether we need to transition partition states to OfflinePartition: since at the time of the BrokerChange event processing, the bounced brokers are alive so there will not be offline partitions.
2. Determine which brokers we want to consider when performing leader election in `partitionStateMachine.triggerOnlinePartitionStateChange()`: since the bounced brokers are online at that time so we should consider them.
3. Determine which brokers we need to send out control requests and which brokers we need to include in the live brokers field in UpdateMetadataRequest: since the bounced brokers will not accept control requests anyway so the first point doesn't matter. For the second point, the bounced brokers are live so we don't want to exclude them in the UpdateMetadataRequest."
234152612,5821,hzxa21,2018-11-16T10:13:25Z,"Code from zookeeper (ClientCnxn.java):
```
...
else if (p.response instanceof MultiResponse) {
  MultiCallback cb = (MultiCallback) p.cb;
  MultiResponse rsp = (MultiResponse) p.response;
  if (rc == 0) {
          List<OpResult> results = rsp.getResultList();
          int newRc = rc;
          for (OpResult result : results) {
                  if (result instanceof ErrorResult
                      && KeeperException.Code.OK.intValue()
                          != (newRc = ((ErrorResult) result).getErr())) {
                          break;
                  }
          }
          cb.processResult(newRc, clientPath, p.ctx, results);
  } else {
          cb.processResult(rc, clientPath, p.ctx, null);
  }
}
...
```
This suggests that if we can get back the OpResult from zookeeper (no CONNECTION_LOSS), rc represents the first error in the operations. So using the rc in the top level response after unwrapping the multi response is fine becuase the first error will also be the actual error for create/setData/delete if we pass the controller epoch check.

This also suggests that List[OpResult] can be null in the callback and I don't handle this in our ZookeeperClient so I will fix it."
234307011,5821,lindong28,2018-11-16T18:36:20Z,"Regarding info vs. warn, I usually follow the summary in https://stackoverflow.com/questions/2031163/when-to-use-the-different-log-levels, which says that info level is for `Generally useful information to log (service start/stop, configuration assumptions, etc)`, and warn level is for `Anything that can potentially cause application oddities, but for which I am automatically recovering`. In this case the `brokerEpoch < cachedBrokerEpoch` can happen only under rare scenario when ControlledShutdownRequest is re-sent due to disconnection between broker and controller. This is similar to the scenarios captured in `NetworkClient.processDisconnection(...)` which are currently logged in warn level. And it is unlike all other info level logging in Kafka for normal broker start/stop.

So it seems that warn level is appropriate here?
"
236885202,5821,junrao,2018-11-27T23:14:43Z,"Yes, the question is whether rare == odd. To me, odd should be unexpected. brokerEpoch < cachedBrokerEpoch is rare, but is not unexpected. "
236902108,5821,junrao,2018-11-28T00:36:16Z,"@hzxa21 : Overall, I agree with your assessment that the onBrokerFailure() call seems redundant. The only thing is that it can force a leader epoch change. Suppose that broker 1 is a bounced broker and is the current leader. If we skip onBrokerFailure(), the controller just keeps broker 1 as the leader w/o bumping the leader epoch. This means that the follower won't go through leader epoch based log truncation, which maybe needed since broker 1 may not have all the data in its local log after the bounce. So, perhaps we can't skip onBrokerFailure(). The next question is should the live broker list exclude the bounced brokers when we call onBrokerFailure(). It seems that we should since live broker list influences which broker is the new leader. If the bounced brokers are still in the live broker list and are the current leaders, those leaders' epoch won't change. So, in summary, it seems that we should still call onBrokerFailure() but excluding bounced brokers from live broker list first. We then add the bounced brokers to live broker list and call onBrokerStartup()."
236906085,5821,hzxa21,2018-11-28T00:57:13Z,"I agree. Also after an offline discussion with Dong, we agree that the benefit of optimizing for quickly bounced brokers is minor and since in normal scenario we will go through onBrokerFailure and then onBrokerStartup for the bounce brokers, it is better to do the same thing here ( invoke onBrokerFailure() and then update live brokers).

Thanks for the comment. I will update the PR accordingly."
236907622,5821,junrao,2018-11-28T01:05:33Z,"Thanks. Since the code still uses the rc in the individual ops, it seems that we need to change it to check the top level rc?"
237354671,5821,hzxa21,2018-11-29T05:09:40Z,Fixed.
237354685,5821,hzxa21,2018-11-29T05:09:45Z,Fixed.
237354693,5821,hzxa21,2018-11-29T05:09:49Z,Fixed.
237354698,5821,hzxa21,2018-11-29T05:09:52Z,Fixed.
237354713,5821,hzxa21,2018-11-29T05:10:02Z,Done.
237354811,5821,hzxa21,2018-11-29T05:10:53Z,"Good suggestion. I have changed liveBrokerIdsUnderlying to a Map[Int, Long] to avoid introducing the val."
237355006,5821,hzxa21,2018-11-29T05:12:19Z,I think it is better to keep it info because the broker epoch information is as informative as the broker ids. We als log the broker ids in info log so we should keep it consistent.
237355104,5821,hzxa21,2018-11-29T05:13:13Z,I think info is fine. I have changed it to info. Thanks you guys for sharing the guideline.
237355138,5821,hzxa21,2018-11-29T05:13:31Z,Done.
237355290,5821,hzxa21,2018-11-29T05:14:32Z,The broker epoch is already logged at the end of the broker change event: https://github.com/apache/kafka/pull/5821/files/88e4eb606fc72761a17378ee6bdf6732765808ef#diff-ed90e8ecc5439a5ede5e362255d11be1R1305
237355315,5821,hzxa21,2018-11-29T05:14:43Z,Sure. Done.
237357526,5821,hzxa21,2018-11-29T05:27:45Z,"We need to use the rc in the individual ops (check and create/delete/set) because we need to differentiate whether the error happened in the controller epoch znode zkVersion check or in create/delete/set. The OpResut for the Check op will reflect whether it has succeeded or not.  If it succeeds, the top level rc will reflect the error happened in create/delete/set and we do use top level rc when constrcuting the response for create/delete/set. If the Check op fails, we will throw exception accordingly."
237357668,5821,hzxa21,2018-11-29T05:28:58Z,I don't think we need to add () here. The problem is we have () in the function definition which exists before this patch. I remove the () in the updated PR.
237357712,5821,hzxa21,2018-11-29T05:29:19Z,Good catch. Thanks for pointing it out. Fixed.
237357743,5821,hzxa21,2018-11-29T05:29:33Z,No. My bad. I have removed the log.
237357752,5821,hzxa21,2018-11-29T05:29:38Z,Fixed.
237357777,5821,hzxa21,2018-11-29T05:29:48Z,Sure. Done.
237357789,5821,hzxa21,2018-11-29T05:29:52Z,Done.
237357805,5821,hzxa21,2018-11-29T05:30:03Z,Yes. Done.
237357830,5821,hzxa21,2018-11-29T05:30:15Z,Thanks for the suggestion. Done.
237357840,5821,hzxa21,2018-11-29T05:30:23Z,Removed.
237357856,5821,hzxa21,2018-11-29T05:30:30Z,Good point.  Fixed.
237357868,5821,hzxa21,2018-11-29T05:30:35Z,Removed.
237357874,5821,hzxa21,2018-11-29T05:30:39Z,Removed.
237357953,5821,hzxa21,2018-11-29T05:31:19Z,Fixed.
237358000,5821,hzxa21,2018-11-29T05:31:37Z,Yes. Fixed.
237358049,5821,hzxa21,2018-11-29T05:32:02Z,Sure. I have merged them into a single function.
237358057,5821,hzxa21,2018-11-29T05:32:07Z,Yes. Fixed.
237358075,5821,hzxa21,2018-11-29T05:32:14Z,Remove. Thanks!
237358087,5821,hzxa21,2018-11-29T05:32:17Z,Fixed.
237358092,5821,hzxa21,2018-11-29T05:32:21Z,Fixed.
237710891,5821,junrao,2018-11-30T00:33:25Z,Could initialBrokerEpoch be right after initialBrokerInfo?
237716547,5821,junrao,2018-11-30T01:07:40Z,indentation
237718706,5821,junrao,2018-11-30T01:21:33Z,control request => controlled shutdown request
237719353,5821,junrao,2018-11-30T01:26:07Z,The controller part is not right. Non-controllers are registered through this api too.
237719687,5821,junrao,2018-11-30T01:28:31Z,It seems that this logging is redundant since registerBroker() logs the same info already?
237721321,5821,junrao,2018-11-30T01:40:07Z,Perhaps it's better to return a Map instead of a sequence of pairs?
237723919,5821,junrao,2018-11-30T01:58:10Z,normalize => normalizes
237724317,5821,junrao,2018-11-30T02:01:06Z,Perhaps add a comment on how v1 differs from v0?
237724569,5821,junrao,2018-11-30T02:02:47Z,normalize => normalizes
237725390,5821,junrao,2018-11-30T02:08:58Z,unused import
237725957,5821,junrao,2018-11-30T02:13:03Z,typo Reuest
237799160,5821,hzxa21,2018-11-30T09:42:03Z,Sure. Done.
237799446,5821,hzxa21,2018-11-30T09:43:00Z,Fixed.
237799723,5821,hzxa21,2018-11-30T09:43:54Z,"I mean LeaderAndIsr/UpdateMetadata/StopReplica requests here, not controlled shutdown request. I have updated the comment to avoid confusion."
237799747,5821,hzxa21,2018-11-30T09:44:00Z,Fixed.
237799890,5821,hzxa21,2018-11-30T09:44:27Z,Thanks for pointing out. I have removed the log here.
237799946,5821,hzxa21,2018-11-30T09:44:39Z,Agree. Fixed.
237799968,5821,hzxa21,2018-11-30T09:44:42Z,Fixed.
237799996,5821,hzxa21,2018-11-30T09:44:48Z,Sure. Added.
237800040,5821,hzxa21,2018-11-30T09:44:53Z,Fixed.
237800071,5821,hzxa21,2018-11-30T09:44:58Z,Removed.
237800090,5821,hzxa21,2018-11-30T09:45:02Z,Fixed.
238013840,5821,junrao,2018-11-30T21:53:05Z,Could we put return after param ?
238013931,5821,junrao,2018-11-30T21:53:30Z,Could we change the comment accordingly?
238020317,5821,hzxa21,2018-11-30T22:20:31Z,Sure. Done.
238020331,5821,hzxa21,2018-11-30T22:20:36Z,Done.
238021750,5821,junrao,2018-11-30T22:26:43Z,It seems that you fixed a different line?
238069509,5821,hzxa21,2018-12-01T16:57:44Z,Ah. My bad. I mislooked this one. Fixed.
205630399,5428,guozhangwang,2018-07-26T23:23:28Z,This is a bug found in MockProducer: we should never throw ProducerFenced in send() call as it should only be returned in the future callback.
205630643,5428,guozhangwang,2018-07-26T23:24:49Z,"This is the optimization on commit: we only execute commit when some processing has been done since last commit, either some records processed, or punctuation triggered.

For standby task commit will be triggered only when some update has been applied to the state store."
205630742,5428,guozhangwang,2018-07-26T23:25:28Z,This is the optimization we have done for partition stream time update.
205912746,5428,guozhangwang,2018-07-27T22:19:59Z,This is not intended and will be removed when rebasing on part II merged.
207436880,5428,guozhangwang,2018-08-03T04:17:16Z,This test is invalid (see the above comment).
207436995,5428,guozhangwang,2018-08-03T04:18:12Z,"If a producer is fenced, its ProducerFencedException is wrapped in the KafkaException."
207437074,5428,guozhangwang,2018-08-03T04:18:47Z,This function should only be called once within each iteration after records enqueued.
207437109,5428,guozhangwang,2018-08-03T04:19:08Z,Inline this function since it only have one caller.
207437286,5428,guozhangwang,2018-08-03T04:21:09Z,"This flaky test is found while working on the PR, so I'm piggy back the fix here. But itself is really independent of the PR, so if people wants to put it into a separate one I can also do that."
208295539,5428,bbejeck,2018-08-07T16:16:47Z,"I think we could simplify this block like so 
```java

catch (final Exception uncaughtException) {
            if (uncaughtException instanceof KafkaException &&
                uncaughtException.getCause() instanceof ProducerFencedException) {
                final KafkaException kafkaException = (KafkaException) uncaughtException;
                // producer.send() call may throw a KafkaException which wraps a FencedException,
                // in this case we should throw its wrapped inner cause so that it can be captured and re-wrapped as TaskMigrationException
                throw (ProducerFencedException) kafkaException.getCause();
            } else {
                throw new StreamsException(
                    String.format(
                        EXCEPTION_MESSAGE,
                        logPrefix,
                        ""an error caught"",
                        key,
                        value,
                        timestamp,
                        topic,
                        uncaughtException.toString()
                    ),
                    uncaughtException);
            }
        }
```

WDYT?"
208346664,5428,bbejeck,2018-08-07T18:54:08Z,"In trunk, the ordering of calls for the `producer` during a commit was broken up, but now they are all grouped together.  It seems ok to do this and is cleaner to follow, I just wanted to double check the change of ordering doesn't matter. 

Maybe we should run system tests to confirm?"
208347652,5428,bbejeck,2018-08-07T18:56:53Z,nit: we could return `commitNeeded` and get rid of `else` and return `false` directly if no punctuation occurred.
208347752,5428,bbejeck,2018-08-07T18:57:10Z,same as above
208354835,5428,bbejeck,2018-08-07T19:20:47Z,Nice addition!
208727056,5428,guozhangwang,2018-08-08T20:37:54Z,"Good point, I will run the system test accordingly."
208728232,5428,guozhangwang,2018-08-08T20:41:28Z,Yup! :)
208741696,5428,mjsax,2018-08-08T21:27:14Z,Should we check the root cause?
208745329,5428,mjsax,2018-08-08T21:41:04Z,"Isn't this a behavior change? IIRC, we had a discussion to do this change, or to maybe make it configurable if we want to interleave processing with recovery."
208745919,5428,mjsax,2018-08-08T21:43:28Z,should this be `timeSinceLastPoll >= maxPollTimeMs / 2` ?
208747346,5428,mjsax,2018-08-08T21:49:19Z,"We set poll interval to Integer.MAX_VALUE by default. Thus, if user does not change the default (most won't I assume), the condition will never be met. Should we rather consider to set a different default value (note, there is already a Jira for this)?"
208749613,5428,mjsax,2018-08-08T21:58:16Z,Isn't `timeSinceLastPoll < maxPollTimeMs` covered via `if (timeSinceLastPoll / 2 >= maxPollTimeMs) { break; }` and redundant?
208761166,5428,mjsax,2018-08-08T22:55:11Z,Could we actually remove this guard? We don't call `time. milliseconds()` as below.
208761888,5428,mjsax,2018-08-08T22:59:09Z,"Why do we need this? Wouldn't it be easier to remove `else` block and just call `return committed > 0;` after the `if`? If I understand correctly, we want to return `true` if committing happen, and currently, even if we commit we could return `false`"
208771114,5428,guozhangwang,2018-08-08T23:55:56Z,Good point!
208771638,5428,guozhangwang,2018-08-08T23:59:02Z,"Ah good point! I was actually not intentionally changing the behavior, I will revert it back to the old manner."
208772074,5428,guozhangwang,2018-08-09T00:01:52Z,"Yes, that was my plan. I'm aware that this line is basically a no-op because max.poll is Integer.MAX_VALUE, and want to do it in another PR. If people feel that we should reverse the ordering, i.e. change the default value first, then do this PR, I'm fine too."
208772154,5428,guozhangwang,2018-08-09T00:02:28Z,"> should this be timeSinceLastPoll >= maxPollTimeMs / 2 ?

Gosh, my bad."
208772446,5428,guozhangwang,2018-08-09T00:04:17Z,"Yes. In an old commit the code structure was a bit different and hence we may run over the check (assuming the maxPollTimeMs is not Integer.MAX_VALUE), but in this format we will always check for `timeSinceLastPoll >= maxPollTimeMs / 2` anyways in each loop, and hence we can remove this."
208772726,5428,guozhangwang,2018-08-09T00:06:00Z,"The intention is to save calling `taskManager.activeTaskIds(), taskManager.standbyTaskIds()` etc and pass them as parameters. It may not really introduce significant differences, but no harm to still keep them?"
208772966,5428,guozhangwang,2018-08-09T00:07:45Z,"You're right, and actually I should changed the above line to `committed += taskManager.commitAll();`."
208773153,5428,mjsax,2018-08-09T00:08:52Z,"I would personally prefer, to keep the condition in the `while` conditions instead of using `if() break` construct."
208773238,5428,mjsax,2018-08-09T00:09:30Z,"I see. Fine with my both ways -- as long as it's intentional and we know about it, it's ok."
208997552,5428,guozhangwang,2018-08-09T16:33:28Z,This is also a flaky test that I discovered here.
209042674,5428,mjsax,2018-08-09T18:56:36Z,Fine with me to keep the guard. Was just double checking.
209293245,5428,vvcephei,2018-08-10T15:13:11Z,I didn't follow why we need this now. Can you explain?
209300543,5428,vvcephei,2018-08-10T15:36:16Z,"Just checking my understanding: we are planning to replace this counter with a wall-clock timer in KIP-353.

I think that once we do that, we can actually move this logic into `isProcessable()` because the condition would no longer be dependent on the number of calls."
209300893,5428,vvcephei,2018-08-10T15:37:31Z,"It seems like it might be worth actually putting your remark ""This function should only be called once within each iteration after records enqueued."" in a comment so we can remember during refactoring later."
209310471,5428,vvcephei,2018-08-10T16:09:53Z,is this just because punctuations might result in context.forwards?
209313656,5428,vvcephei,2018-08-10T16:20:51Z,"I believe that Java (or the ALU) will do exactly the same thing whether you say `maxPollTimeMs >> 1` or `maxPollTimeMs / 2`, but your human colleagues might appreciate the latter ;)"
209336733,5428,vvcephei,2018-08-10T17:47:21Z,"Nice! Now that the condition is no longer dependent on the number of invocations, I think you can move it into `isProcessable()` and not need to call this method outside of this class."
209338385,5428,vvcephei,2018-08-10T17:53:27Z,"Actually, we should re-set the timer whenever we process, right? Imagine we have the following sequence:
```
max.idle=10

(some stuff happens)
... blocking
t60: processing is forced (set lastEnforcedProcess)
...
t68: process, draining a partition (do not set lastEnforcedProcess)
t69: no data on that partition, so we block
t70: processing is forced because it's been 10 since we last set lastEnforcedProcess at t60, even though we processed something at t68, so we have only been idle for 2
```"
209339003,5428,vvcephei,2018-08-10T17:55:50Z,"I think there's a risk of forcing processing on the very first iteration if too much time passes between construction and processing (like if the startup protocol takes a while).

Maybe we can initialize it to `Long.MAX_VALUE` instead, which should cause us never to force processing the first time."
209344830,5428,vvcephei,2018-08-10T18:16:44Z,"I take it this was the source of the flakiness. Can you explain why, for my education?"
209405125,5428,guozhangwang,2018-08-10T23:23:23Z,"This is following the same PR that @mjsax had: https://github.com/apache/kafka/pull/5389

The point is that when a XXXConfig is created, by default it will print `logAll` and hence swamped the logs (we can see the same lists to be printed multiple times whenever it is created). This function is to disable `log` for such cases."
209405232,5428,guozhangwang,2018-08-10T23:24:16Z,Actually it is because users can call `context.commit()` in either ` punctuate()` or `process()` calls.
209405249,5428,guozhangwang,2018-08-10T23:24:32Z,ack :)
209405308,5428,guozhangwang,2018-08-10T23:25:07Z,"Yup, good point."
209405511,5428,guozhangwang,2018-08-10T23:27:15Z,"That's a good catch, but if we move `isProcessable()` inside the iteration, it will mean that we will only enforce-process one record for every `max.idle.ms` right?

My original thought is that once we've decided to enforce process, we'll enforce for that whole thread iteration."
209405561,5428,guozhangwang,2018-08-10T23:27:49Z,"Hmm, good point, I'll see what can be done here."
209405825,5428,guozhangwang,2018-08-10T23:30:47Z,"The flakiness is actually that for this dedup integration test, we should check that ""for each key, the last record is the expected value"", while previously we just check that ""we retrieve N records, and check that these N records are exactly the expected values"". However even with dedup based on caching, it may not be the case that we ONLY produce N final records.

This PR increases the likelihood that we do not only produce N final records, and hence I updated the check logic accordingly."
209406887,5428,guozhangwang,2018-08-10T23:42:22Z,I will set the enforced process in the `initTopology` which will be triggered when the task transits to running state.
209757140,5428,vvcephei,2018-08-13T21:00:25Z,Ah! I misread this as turning `logAll` *on* instead of *off*. Now I get it :)
210646404,5428,bbejeck,2018-08-16T15:43:04Z,nit: can be package private
210646473,5428,bbejeck,2018-08-16T15:43:19Z,nit: can be package private
210689155,5428,bbejeck,2018-08-16T18:02:59Z,Why did this go from 2 to 1? other than not passing an arg to `runOnce` the test logic to this point hasn't changed
211453536,5428,guozhangwang,2018-08-21T01:16:54Z,"The logic does have changed: in the old code we will commit twice on producer, one during the rebalance and one from the elapsed time. In the new code, the optimization I added will realize that nothing has been generated since the last commit, and hence we will skip committing in this case. 

Thinking about it, this does have a side-effect though since for EOS if commit was not called in a long time then txn will be aborted, and if producer does not talk to txn coordinator even longer it could be removed as well. But personally I think it is okay for such scenario to happen, since really no data was generated, and hence committing an empty txn does not really make sense, and we should rather increase the txn expiration time in this case. WDYT? @mjsax @vvcephei "
211764912,5428,vvcephei,2018-08-21T21:23:35Z,"I guess that we always have a transaction open, not just when we have something to commit.

It seems like one solution is to open a transaction only when we have data to process. Although this might complicate things.

Alternatively, is there a way to periodically send a ""keep alive"" message to let the broker know we do still intend to use that transaction? It seems like either this or just abort/close the empty txn and re-open is better than a super-long expiration time. Otherwise, why is there even an expiration time?

Is there any tradeoff between having one transaction open for a super long time, vs periodically closing empty transactions and starting new ones?"
211773323,5428,guozhangwang,2018-08-21T21:57:21Z,"Completing a txn and starting a new one come with some cost, and hence is what we want to avoid generally.

On the other hand, we do not yet have a mechanism for ""keep alive"": with that, I think keeping a long lived EMPTY txn is okay, note that if the txn is not empty, then not committing it in time will increase the latency. Hence I'm only trying to optimize the case when the txn is empty."
211775968,5428,mjsax,2018-08-21T22:09:46Z,"I just talked to @hachikuji about this. Not committing is actually fine. Note, that beginTx() is a client local state transition -- nothing is written to the log (there are no ""begin tx markers"") and the TC state is also not modified. This implies, that the transaction timeout is not started on beginTx() -- the timeout only starts after the first record was written to the log. Thus, we don't need ""keep alive heartbeats"" and don't need to tell users to increase the tx timeout for low traffic topics that might have longer periods with no data."
212116777,5428,mjsax,2018-08-22T21:30:07Z,nit: `out-of-ordering` -> `out-of-order`
212120236,5428,mjsax,2018-08-22T21:44:01Z,nit: can be limited to be package private
212120925,5428,mjsax,2018-08-22T21:46:59Z,nit: `Failed to commit StreamTask {} due` -- to distinguish active and standbys as before.
212121159,5428,mjsax,2018-08-22T21:47:55Z,Why do we remove `taskTypeName` for the log statement?
212121272,5428,mjsax,2018-08-22T21:48:21Z,Why do we remove `taskTypeName` for the log statement?
212121781,5428,mjsax,2018-08-22T21:50:33Z,"I think we should increase `committed` after `task.commit()` returns -- otherwise, we over count if committing fails?"
212121812,5428,mjsax,2018-08-22T21:50:42Z,"I think we should increase `committed` after `task.commit()` returns -- otherwise, we over count if committing fails?"
212122618,5428,mjsax,2018-08-22T21:54:04Z,nit: `lastEnforcedProcess[ingTime[stamp]]` ?
212122704,5428,mjsax,2018-08-22T21:54:20Z,nit: `enforedProcess[ing]`
212123252,5428,mjsax,2018-08-22T21:56:36Z,Should we change the order of the order? Not sure atm if this has a semantic impact due to potential partial evaluation? Just want to double check.
212123631,5428,mjsax,2018-08-22T21:58:06Z,"Also, it seems that we might want to rename `enforcedProcess` to `enforceProcessing`  (without `d`) ?"
212123911,5428,mjsax,2018-08-22T21:59:11Z,This comments seems not to be addressed?
212125423,5428,mjsax,2018-08-22T22:05:14Z,"Both good points.

Basically, we should reset the timer when there is data for all input partitions. Thus, we should check after poll() if we can reset the timer (ie, in `StreamTask#addRecords()` each time all partitions have data)?"
212125967,5428,mjsax,2018-08-22T22:07:43Z,nit: formatting -> more `}` to next line
212126272,5428,mjsax,2018-08-22T22:09:07Z,Why do we not check `if(commitNeeded)` any longer?
212127138,5428,mjsax,2018-08-22T22:13:22Z,"It was broken apart because we checked if there is anything to commit in the first place (ie, do the check on one place)-- if we did not process any data, we don't need to commit.

This check now happens outside of `StreamTask` as pointed out by Guozhang https://github.com/apache/kafka/pull/5428/files#r212395430 Thus, regrouping makes sense. Code is cleaner this way."
212128396,5428,mjsax,2018-08-22T22:19:04Z,"Meta comment: Not sure if this is a good argument for inlining in general---code might be more readable if it's broken apart is smaller pieces and calling methods (with good names) actually self-documents the code. Also, shorter methods are easier to understand.

For this particular case, inlining is ok IMHO, as with `if (commitNeeded)` check, we don't loose anything."
212129095,5428,mjsax,2018-08-22T22:21:57Z,"Hmmm... if there is nothing to commit, it might also be fine to ignore the user commit request? It's a tricky question what to do for this case. Just follow what the user demands, or be smart? From a correctness point of view, it should not make a difference, would it?

Also, we set flag `commitRequested` for this case -- thus, it might be better to put this logic somewhere else? Eg: `AbstractTask` or overwrite in `StreamTask`:
```
public boolean commitNeeded() {
    return commitNeeded || commitRequested;
}
```

 (An alternative, that I like less would be to add a check if `commitRequested==true`)?"
212131579,5428,mjsax,2018-08-22T22:33:34Z,nit: remove `this` 
212133194,5428,mjsax,2018-08-22T22:41:50Z,nit: `maybeEnforceProcess[ing]` ?
212133245,5428,mjsax,2018-08-22T22:42:10Z,nit: `maybeEnforceProcess[ing]`?
212133339,5428,mjsax,2018-08-22T22:42:41Z,nit: `maybeEnforceProcess[ing]`?
212134225,5428,mjsax,2018-08-22T22:46:58Z,"Why this? The condition checks for `activeRunningTasks` -- why would we need to punctuate if there are not active tasks? Also, should we `maybeUpdateStandbyTasks()` before we do `maybeCommit()` to include the data that is processed by standbys in the commit?"
212135789,5428,mjsax,2018-08-22T22:55:13Z,This seems to contradict that we set `commitNeeded` after punctuations (cf. my comments below).
212136165,5428,mjsax,2018-08-22T22:57:17Z,"Should we move this into the and `else` branch of `if (commitTimeMs >= 0 && lastCommitMs + commitTimeMs < now)` -- if the condition is true, we call `taskManager.commitAll()` and thus `taskManager.maybeCommitActiveTasks()` seems to be redundant here?"
212386802,5428,guozhangwang,2018-08-23T17:09:26Z,Good point!
212387543,5428,guozhangwang,2018-08-23T17:12:06Z,"No specific reasons, I should add it back."
212387559,5428,guozhangwang,2018-08-23T17:12:11Z,Ditto.
212387745,5428,guozhangwang,2018-08-23T17:12:44Z,Yes!
212388642,5428,guozhangwang,2018-08-23T17:15:44Z,"The ordering thing: my old-school instinct is to put ""cheapest"" condition first for an `OR` operator, but in modern compiler / CPU it really not matter at all :)

Renaming: ack."
212389112,5428,guozhangwang,2018-08-23T17:17:20Z,My bad...
212395430,5428,guozhangwang,2018-08-23T17:37:20Z,"We check this in the AssignedTasks now: if no commit is needed, we skip the whole committing function, including commit offsets, flushing stores, etc."
212398936,5428,guozhangwang,2018-08-23T17:47:42Z,"Not sure I follow your comment here.. let me elaborate a bit on my logic:

We have two commits in places: commitAll (periodic) and maybeCommit (for user requested):

The latter checks

```
if (task.commitRequested() && task.commitNeeded()) 
                    task.commit();
```

While the former only checks:

```
if (task.commitNeeded()) 
                    task.commit();
```

I.e. the logic for the latter is that ""only if user have requested, and it is indeed needed to commit"": for example, if we have actually committed from the commit interval, and then user requested it as well, the second will be omitted.

I intentionally separated ""commitRequest"" (this is only set by user) and ""commitNeeded"" (this is determined by the library) because this way looks cleaner to me."
212399201,5428,guozhangwang,2018-08-23T17:48:27Z,I've removed this function as whole and only reset upon `addRecords` as you suggested.
212399262,5428,guozhangwang,2018-08-23T17:48:38Z,Ditto below.
212400548,5428,guozhangwang,2018-08-23T17:52:24Z,"Good catch.

The deliberation was that even though ""there is no data processed"", not ""there is no active tasks"" as the original check is `State == RUNNING` :) Note that although `taskManager.hasActiveRunningTasks()` returns true, we may still not process any data (i.e. `totalProcessed` == 0 and we break the loop immediately).

But with the new condition, we will always execute `if (maybePunctuate() || maybeCommit())` anyways, so we only need to do `maybeUpdate` followed by a `maybeCommit` again.
"
212401916,5428,guozhangwang,2018-08-23T17:56:27Z,"Actually thinking about this, I feel it is better to separate the standby tasks from active tasks in maybeCommit as otherwise we are doomed to waste some CPUs doing either one of them. Will refactor the code a bit more."
212402352,5428,guozhangwang,2018-08-23T17:57:37Z,Still not sure if I follow.. we do `maybePunctuate` before `maybeCommit` so this should be fine?
212402578,5428,guozhangwang,2018-08-23T17:58:25Z,"Yes we can, as I mentioned I felt it is better to separate committing for standby tasks and for active tasks."
212404222,5428,mjsax,2018-08-23T18:03:45Z,"So setting `commitNeeded` is a conservative approach, because we don't know what the user did within punctuation call? Might be better to set `commitNeeded` if user calls `context.forward` or `state.put()` -- not sure how hard this would be -- would also be out-of-scope for this PR. If we think it might be worth it, we should create a JIRA for this optimization."
212405498,5428,mjsax,2018-08-23T18:07:36Z,I think I miss understood the logic before. Please ignore this comment.
212779553,5428,mjsax,2018-08-25T00:05:14Z,"nit: the naming always confuses me -- maybe we could rename this to `checkForUserCommitRequest` or similar? The name should reflect that this method should be called to ""commit on user request only"" -- not for commit-interval purpose."
212779739,5428,mjsax,2018-08-25T00:07:31Z,Seems you missed this one :)
212779906,5428,mjsax,2018-08-25T00:09:27Z,"Wondering if this is redundant to 
```
} else if (partitionGroup.numBuffered() > 0 && now - lastEnforcedProcessingTime > maxTaskIdleMs) {
```"
212780227,5428,mjsax,2018-08-25T00:14:15Z,Thought on my last comment?
212780732,5428,mjsax,2018-08-25T00:22:19Z,"It seem we rely on `computeLatency()` above to advance `now` -- it seems ""dangerous"" to rely on a ""side effect"" for this. Should we advance time explicitly here? Or at least put a check if `now < lastPollMs || now > timeSinceLastPoll` ?"
212780848,5428,mjsax,2018-08-25T00:24:17Z,nit: should we rename to `taskManger.maybePunctuate()` as well as `AssignedStreamTasks#maybePunctuate()` to align naming?
212780939,5428,mjsax,2018-08-25T00:26:00Z,nit: add comment `// visible for testing`
212781336,5428,mjsax,2018-08-25T00:33:14Z,"`TaskManager#commitAll()` still commits both, active and standby tasks. What kind of separation do you mean? And even if we separate both, it seems to be orthogonal to my comment.

If we commit all tasks because commit time elapses, we don't need to check for user requested commits any longer. Atm, we might iterator over all tasks twice. First iteration is checking for user requested commits, and if commit interval is passed, we iterate over all tasks again. However, if we commit time passed, we commit all tasks anyway and thus can avoid checking for user requested commits (ie, we can put `int committed = taskManager.maybeCommitActiveTasks();` into the `else` of `if (commitTimeMs >= 0 && lastCommitMs + commitTimeMs < now)` ?

Or maybe I miss understood your comment?"
212781601,5428,mjsax,2018-08-25T00:38:36Z,"@vvcephei Note, that we call `waitUntilFinalKeyValueRecordsReceived` now instead of `waitUntilMinKeyValueRecordsReceived`.

@guozhangwang Not sure why we need to update the commit interval? Isn't `auto.commit=false` anyway? If not, should we set `auto.commit=false` instead of setting commit interval to ""infinite""?"
212781654,5428,mjsax,2018-08-25T00:39:41Z,nit: move `consumerProperties` to next line
212781954,5428,mjsax,2018-08-25T00:45:26Z,"simplify both lines to `finalAccumData.putIfAbsent(kv.key, new ArrayList<>()).add(kv);`"
212781963,5428,mjsax,2018-08-25T00:45:35Z,as above
212782085,5428,mjsax,2018-08-25T00:47:20Z,seem so to duplicate line above?
212782144,5428,mjsax,2018-08-25T00:48:36Z,"What do you mean by ""respect"" -- don't understand the test name"
212782288,5428,mjsax,2018-08-25T00:51:28Z,"Shouldn't commitRequest not be false by default? Also, did you intent to call `task.requestCommit()` above?"
212782318,5428,mjsax,2018-08-25T00:52:23Z,I think we need to initialize this with `false`? (compare my comment in the tests above)
212782384,5428,mjsax,2018-08-25T00:54:07Z,"If commit interval is 100ms, we might want to test the edge case 100 and 101 -- the test does not cover that we would force processing at 70L already."
212782504,5428,mjsax,2018-08-25T00:56:48Z,"why 202L? I cannot inver from the test, to what time the timer get's reset?"
212782744,5428,mjsax,2018-08-25T01:02:32Z,Why do we remove this test?
213046088,5428,vvcephei,2018-08-27T17:10:02Z,"Yeah, I can confirm that I just now got confused about the names. Can we maybe call this (and up the chain) `commitIfRequestedAndNeeded` or similar?

Specifically, the thing that confused me was differentiating the periodic commits on any dirty task vs. the on-demand commit driven by `ProcessorContext#commit`."
213052103,5428,vvcephei,2018-08-27T17:30:25Z,"Overall, the enforced-processing algorithm is unclear to me.

* Following on @mjsax 's comment, it seems strange to set this right before we return true anyway. Note that this is currently the only place we set `enforceProcessing` to true.

* Also, it still seems to me that `maxTaskIdleMs` should count from the last time we process at all, not the last time we forced processing (same basic scenario I pointed out last time). Is this right?"
213140235,5428,guozhangwang,2018-08-27T22:55:46Z,"Oops, my bad."
213140554,5428,guozhangwang,2018-08-27T22:57:20Z,Ack.
213141057,5428,guozhangwang,2018-08-27T22:59:59Z,"We still need to commit even if no records are processed: consider a topology which only contains a single source node, then no data processed at all, but we still want to commit so that we would not re-process them right?"
213152609,5428,guozhangwang,2018-08-28T00:15:02Z,"It is not: once `enforceProcessing` is set, we want to continue in that state until the next batch of records are enqueued and we not have all partitions buffered. Note that once we set the flag we update `lastEnforcedProcessingTime = now;` as well, but we do not want to disable enforce processing in the next run immediately."
213152646,5428,guozhangwang,2018-08-28T00:15:30Z,"It is not: once `enforceProcessing` is set, we want to continue in that state until the next batch of records are enqueued and we not have all partitions buffered. Note that once we set the flag we update `lastEnforcedProcessingTime = now;` as well, but we do not want to disable enforce processing in the next run immediately."
213153611,5428,guozhangwang,2018-08-28T00:24:58Z,"The motivation of advancing `now` in `computeLatency` is to save on `milliseconds()` call. I admit it is not ideal, if we want to change it to a different way, say: passing `now` along the calls than using a variable at all, then I'd suggest we do it in a separate PR as this PR has been dragging too long.

Regarding the check: that is a good idea, but I guess you mean `now - lastPollMs > timeSinceLastPoll` right? I will add that check."
213154023,5428,guozhangwang,2018-08-28T00:28:20Z,"I think `commit()` and `punctuate()` in TaskManager is okay, as they return the number of actual number of punctuation / commits triggered, while the `maybeXX` returns true or false."
213154784,5428,guozhangwang,2018-08-28T00:34:22Z,"I left a general comment before, copying here:

```
I tried about separating commit for active and standby tasks but after some investigation I realized it is related to https://issues.apache.org/jira/browse/KAFKA-6108 and hence I'd leave it to this JIRA / PR (also left a comment in the ticket for some clarification questions).
```

Regarding your question: yes I think switching the checking for time-based commits and then user-requested commits for now makes sense, I will update the code accordingly."
213155160,5428,guozhangwang,2018-08-28T00:37:36Z,"I think this is not needed, will remove this."
213156091,5428,guozhangwang,2018-08-28T00:44:52Z,"Note sure what do you mean? `task.needCommit()` sets the flag, `commitRequested()` checks the flag.

Do you suggest renaming `needCommit` to `requestCommit`?"
213156299,5428,guozhangwang,2018-08-28T00:46:21Z,"The default init value should be `false` anyways, but yeah I can make it explicit."
213156684,5428,guozhangwang,2018-08-28T00:49:15Z,"Note we are testing for max idle time as `now - lastEnforcedProcessingTime > maxTaskIdleMs` so 101 is necessary, ditto for below `202`."
213157946,5428,guozhangwang,2018-08-28T00:59:03Z,"Here is the rationale of this logic:

1. Once we decide to `enforceProcessing`, we will continue enforcing until we got new data enqueued and all the buffer become full, in this case we will in `normalProcessing` state.
2. But we will update the `lastEnforcedProcessingTime` the last time we decide to start enforce processing.
3. We know that once we decide to enforce processing, we will always process immediately as there are indeed some data buffered already.

So the logic above sets `lastEnforcedProcessingTime` at the time we decide to ""turn it on"", and only ""turn it off"" during records enqueuing and all buffers contain some data. And hence we will first check `enforceProcessing`: if it is true we just continue enforce processing.

LGTY?"
213162972,5428,guozhangwang,2018-08-28T01:39:44Z,Note that `putIfAbsent()` will return null if it does not contain the key previously. I can try to use `computeIfAbsent` though.
213173389,5428,mjsax,2018-08-28T03:11:36Z,Fair enough. Thanks for pointing it out.
213173805,5428,mjsax,2018-08-28T03:15:49Z,"Good point. Now I am wondering, if we should set `lastEnforcedProcessingTime = Long.MAX_VALUE`, too, when we set `enforceProcessing == false` when adding records to the buffers? "
213174290,5428,mjsax,2018-08-28T03:19:57Z,"Renaming helps -- `needsCommit` implies ""there is something to commit"" while `requestCommit` implies ""user request committing"" -- it's too different things and we need to keep naming separated to avoid confusion."
213800151,5428,vvcephei,2018-08-29T19:09:54Z,"I see. Thanks for explaining.

It still seems like the `enforceProcessing` variable isn't strictly necessary, it just saves calls to `partitionGroup.allPartitionsBuffered()`, `partitionGroup.numBuffered()`, and the comparison `now - lastEnforcedProcessingTime > maxTaskIdleMs`. These are all just cached field lookups, though, so I don't know if the performance boost is worth the algorithmic complexity.

Regarding `lastEnforcedProcessingTime`, consider this scenario.
```
maxTaskIdleMs := 2
lastEnforcedProcessingTime := 0 (init)
now=0; allPartitionsBuffered=true => isProcessable:=true
now=1; allPartitionsBuffered=true => isProcessable:=true
now=2; allPartitionsBuffered=true => isProcessable:=true
now=3; allPartitionsBuffered=false => 
  // 3 - 0 > 2, so we force processing
  lastEnforcedProcessingTime:=3
  isProcessable:=true
```

Two things to note here:
1. the expression should probably be `now - lastEnforcedProcessingTime >= maxTaskIdleMs` (with `>=` instead of `>`), otherwise you'll wait at least one extra ms _beyond_ the purported ""max task idle time"".
2. In the scenario above, we said we want to wait *2 ms* before forcing processing, but we actually force processing *immediately*. To fix this, we should be comparing against `lastProcessingTime`, which we should set every time we process."
213814276,5428,guozhangwang,2018-08-29T19:59:30Z,"Good point, I will try to address this along with @mjsax 's other comment:

> Good point. Now I am wondering, if we should set lastEnforcedProcessingTime = Long.MAX_VALUE, too, when we set enforceProcessing == false when adding records to the buffers?"
213815807,5428,vvcephei,2018-08-29T20:04:51Z,"Thanks. One final thought about whether the `enforceProcessing` optimization is worth it. It might be a good idea to benchmark it without the optimization, since branch prediction *should* eliminate any overhead from the checks on rarely used branches."
214207271,5428,guozhangwang,2018-08-30T23:10:55Z,"@vvcephei @mjsax this turns out to be harder than I thought. The tricky thing is that we originally want to 1) record the sensor only for the first time when we transit to ""enforced processing"" state, and 2) start the idleness timer only for the first time when we do not have all buffered but some buffered.

I tried to even implement a StreamTask State just like KafkaStreams and StreamThread, but that turns out to not be so elegant as well. So what I ended up now is this: we will record the sensor whenever we enforce processing, either for the first time or not, and hence we will only update idleStartTime once, and reset it whenever we have all buffered.

LMK WDYT."
215719543,5428,mjsax,2018-09-06T17:52:03Z,Not sure if I understand this. Why is a commit only required if we did not restore all records that were passed in? Don't we need to commit if we did a restore and updated `lastOffset` ?
215734428,5428,mjsax,2018-09-06T18:38:24Z,nit: should it be `>` instead of `>=` ?
215734587,5428,mjsax,2018-09-06T18:38:58Z,Nit: `NOT_KNOWN` -> `UNKNOWN` ?
215735099,5428,mjsax,2018-09-06T18:40:40Z,"nit: `fatal` is not a good name (was named like this before, no not introduced in this PR) -- this exception is not fatal but we can recover from it."
215737734,5428,mjsax,2018-09-06T18:49:08Z,"Why not just: `timeSinceLastPoll = now - lastPollMs` ? If we assume that `now` never goes backwards, I don't see the need for calling `Math.max`? And if we need the `Math.max` guard, why do we need both? Or do I miss something?"
215738983,5428,mjsax,2018-09-06T18:53:09Z,"Should we check for `timeSinceLastPoll < maxPollTimeMs / 2`, too? According to the comment above, if we break the while-loop, we want to half `numIterations`, too.

Ie, instead of checking in the `while` condition, add a check here and call `break` after reducing `numIterations`?"
215741170,5428,mjsax,2018-09-06T18:59:52Z,"nit: `lastCommitMs + commitTimeMs < now` -> `now - lastCommitMs > commitTimeMs`

IMHO, easier to read this way."
215741742,5428,mjsax,2018-09-06T19:01:48Z,Why another `if`? I thought this would be an `else` to the condition above?
215803459,5428,mjsax,2018-09-06T23:02:09Z,"> So what I ended up now is this: we will record the sensor whenever we enforce processing, either for the first time or not, and hence we will only update idleStartTime once, and reset it whenever we have all buffered.

Sounds good to me."
216033050,5428,guozhangwang,2018-09-07T17:32:21Z,There was an early comment on the test code that suggests `>=`. Personally I think it does not make a big difference at all.
216033754,5428,guozhangwang,2018-09-07T17:35:11Z,"Again, this comes from a previous comment that it is safer to make sure `timeSinceLastPoll` does not go backwards, in case `now` is reduced."
216037853,5428,guozhangwang,2018-09-07T17:49:27Z,Good point!
216039375,5428,guozhangwang,2018-09-07T17:54:27Z,"`!restoreRecords.isEmpty()` means we have non-empty records that are applied inside `stateMgr.updateStandbyStates` call, note it does not remove records that are applied after the call."
216067026,5428,vvcephei,2018-09-07T19:38:19Z,"I am to blame for this suggestion. I agree it doesn't make a big difference.
The reasoning was that if it's the ""maximum idle time"", then you shouldn't idle longer than it, otherwise, it's not really a maximum."
216141583,5428,mjsax,2018-09-08T21:50:01Z,"I agree that it does not matter too much :) (that why it's a nit)

However, I think that the maximum is inclusive, and only if we exceed it, we should force processing. From my understanding, ""maximum idle time"" is actually a lower bound (-> don't force processing until this time passed) because we cannot guarantee anyway to not exceed this threshold. I see your point why the name might be counter intuitive (even if I think the name is correct). If you interpret the name strictly, we would be allowed (or actually we would be required) to force processing before the time passed. This interpretation would make the parameter useless (ie, user tells us to idle max 5 minutes and we obey by forcing processing after 1 minute).

To me, the right interpretation is, ""wait until this time passed and force processing asap if the time is exceeded"". Chaning the name to `min.idle.time.ms` would be more precise, but I think it would be more confusing to users. "
216141611,5428,mjsax,2018-09-08T21:51:27Z,"> in case now is reduced

How could this happen? Seems to be impossible to me."
216141645,5428,mjsax,2018-09-08T21:53:28Z,Ack. I confused it with `remainingRecords`. All good :)
216399814,5428,guozhangwang,2018-09-10T17:05:51Z,"quoting your comment: 

```
It seem we rely on computeLatency() above to advance now -- it seems ""dangerous"" to rely on a ""side effect"" for this. Should we advance time explicitly here? Or at least put a check if now < lastPollMs || now > timeSinceLastPoll ?
```

The above change is for addressing this comment. Again I'd admit it is not ideal to rely on the side effect of `computeLatency()` to advance `now` but at the same time I want to avoid calling system time necessarily. If you feel strong about it, I can just go ahead and explicitly advance `now`, does it sound better to you?"
216400887,5428,guozhangwang,2018-09-10T17:09:01Z,"Okay guys, I'm going to make a final call here to end the discussion: I'm staying with `max.idle..` since I feel it is easier to understand for users, and be aware that this is not strictly respected in practice unless it is set to `0`. Also I'm staying with `>=` since again, it is easier to understand though not strictly sound mathematically."
216403879,5428,mjsax,2018-09-10T17:17:35Z,:) Fair enough.
216404303,5428,mjsax,2018-09-10T17:18:48Z,I see. I did not make the connection to the other discussion. I think we can leave as-is.
216520595,5428,guozhangwang,2018-09-11T01:14:46Z,"To make it clear, I 1) renamed the function, and 2) explicitly called it outside the `sensor.record` call."
216550162,5428,mjsax,2018-09-11T05:37:09Z,"Should we compute this, before we call `taskManager.updateNewAndRestoringTasks()` ? Also, do we need to update `now` after `updateNewAndRestoringTasks()` to compute `processLatency` correctly, below?"
216551625,5428,mjsax,2018-09-11T05:48:59Z,nit `stays at 2` seems to be correct -- it's `equalTo(2)` below.
216829644,5428,guozhangwang,2018-09-11T21:30:35Z,ack.
216853354,5428,guozhangwang,2018-09-11T23:15:27Z,ack
108246948,2743,hachikuji,2017-03-27T18:38:12Z,Sorry for the drive-by comment. Maybe this could be `PartitionLeaderEpoch` so there's no potential confusion with the producer epoch?
108319243,2743,junrao,2017-03-28T01:50:05Z,"To follow the existing convention, partition_id and error_id should be partition and error_code?"
108319252,2743,junrao,2017-03-28T01:50:11Z,epochs => leader epochs ?
108319291,2743,junrao,2017-03-28T01:50:33Z,"To follow the convention in other requests like FetchRequest, perhaps we can store Map<TopicPartition, Int>, where Int is for leaderEpoch?"
108319308,2743,junrao,2017-03-28T01:50:44Z,Could we consolidate the log here and in line 173 into a single one? Perhaps it's also useful to log the LEO at this point.
108319315,2743,junrao,2017-03-28T01:50:50Z,"It seems that during log recovery, we should recover the leader epoch cache as well?"
108319326,2743,junrao,2017-03-28T01:50:55Z,entry => batch 
108319349,2743,junrao,2017-03-28T01:51:11Z,"map { case (tp, state) => ...} ?"
108319571,2743,junrao,2017-03-28T01:53:41Z,"Currently, in AbstractFetcherThread, we try not to hold the partitionMapLock while making an RPC call. Otherwise, if an RPC call takes long for some reason, the becomingLeader/Follower call will be delayed while waiting for the partitionMapLock. Perhaps, we can structure the code like the following. 

```
    val leaderEpochRequest = inLock(partitionMapLock) {
      buildLeaderEpochRequest(states)
    }
    if (!leaderEpochRequest.isEmpty)
      processLeaderEpochRequest(leaderEpochRequest)

    val fetchRequest = inLock(partitionMapLock) {
      val fetchRequest = buildFetchRequest(states)
      if (fetchRequest.isEmpty) {
        trace(""There are no active partitions. Back off for %d ms before sending a fetch request"".format(fetchBackOffMs))
        partitionMapCond.await(fetchBackOffMs, TimeUnit.MILLISECONDS)
      }
      fetchRequest
    }
    if (!fetchRequest.isEmpty)
      processFetchRequest(fetchRequest)
```

In processLeaderEpochRequest(), we can do sth similar to processFetchRequest: send the LeaderEpochRequest w/o holding partitionMapLock; then hold onto partitionMapLock and do log truncation."
108319584,2743,junrao,2017-03-28T01:53:48Z,Merge 218-220 into the state line in 217?
108319612,2743,junrao,2017-03-28T01:54:02Z,"The reason for calling partitionMapCond.signalAll() is to wake up the AbstractReplicaThread. If the method is only called within AbstractReplicaThread, we know the thread is awake when making the call. So, there is no need to call signalAll()."
108319622,2743,junrao,2017-03-28T01:54:09Z,handleHandleOffsetForLeaderEpochRequest => handleOffsetForLeaderEpochRequest
108319626,2743,junrao,2017-03-28T01:54:15Z,It seems that OffsetsForLeaderEpoch.getResponseFor() should be a method in ReplicaManager?
108319633,2743,junrao,2017-03-28T01:54:19Z,missing license header
108319657,2743,junrao,2017-03-28T01:54:30Z,"We probably only want to resort to HW if the error is NoLeaderEpoch. Otherwise, we should probably just backoff a bit and then retry."
108319674,2743,junrao,2017-03-28T01:54:40Z,"If epochOffset.endOffset() is UNSUPPORTED_EPOCH_OFFSET, which can happen during the transition phase, we should fall back to HW?"
108319712,2743,junrao,2017-03-28T01:55:10Z,"If epochOffset.endOffset() >= replica.logEndOffset.messageOffset, perhaps we could avoid log truncation."
108319722,2743,junrao,2017-03-28T01:55:17Z,Perhaps we should change the file name to OffsetCheckpointFile?
108319742,2743,junrao,2017-03-28T01:55:31Z,"If epoch is < latestEpoch(), it might be useful to log a warning.

We probably also want to assert that offset is > the offset of the last epoch."
108319753,2743,junrao,2017-03-28T01:55:36Z,Is epochs.last() of O(1) cost? It will be called on every request.
108319812,2743,junrao,2017-03-28T01:56:21Z,"We want to be a bit careful here, especially during the transition phase when some existing messages may not have a leader epoch. So, if requestedEpoch is < the first epoch in epochs, we probably want to return UNSUPPORTED_EPOCH_OFFSET so that the follower can fall back to HW."
108319839,2743,junrao,2017-03-28T01:56:39Z,"Do we need retainMatchingOffset? It seems that in both Log.truncateTo() and Log.truncateFullyAndStartAt(), we want the offset to be inclusive, i.e., an epoch with offset will be removed. "
108319862,2743,junrao,2017-03-28T01:56:53Z,"Could we log the topic/partition too? Also, does this need to be info? Seems more like debug level. Ditto below in clearOldest()."
108319865,2743,junrao,2017-03-28T01:56:55Z,Is the comment accurate?
108319919,2743,junrao,2017-03-28T01:57:20Z,Not sure if we need retainMatchingOffset here either. It seems the caller always wants this to be inclusive. 
108379720,2743,benstopford,2017-03-28T09:52:12Z,"Yes, I'm using a ListBuffer: http://www.scala-lang.org/api/current/scala/collection/mutable/ListBuffer.html#last:A"
108408756,2743,benstopford,2017-03-28T12:41:13Z,Have changed to: `offset >= the offset of the last epoch.` as epoch can increment on the leader when the offset does not change. 
108652909,2743,benstopford,2017-03-29T11:31:33Z,"I've pulled the logic up into AbstractFetcherThread. What I've done is somewhat similar to your snippet, but it includes a lock around the truncation step. 

 ```
 override def doWork() {
    maybeTruncate()
...
...
  def maybeTruncate(): Unit = {
    val epochRequests = inLock(partitionMapLock) {buildLeaderEpochRequest(states)}
    if (!epochRequests.isEmpty) {
      val fetchedEpochs = fetchEpochsFromLeader(epochRequests)
      //Ensure we hold a lock during truncation.
      inLock(partitionMapLock) {
        //Check no leadership changes happened whilst we were unlocked, fetching epochs
        val leaderEpochs = fetchedEpochs.filter { case (tp, _) => partitionStates.contains(tp) }
        val truncationPoints = maybeTruncate(leaderEpochs)
        markInitialisationComplete(truncationPoints)
      }
    }
  }
```"
109215784,2743,junrao,2017-03-31T17:54:40Z,"It seems that $leaderEpoch is the same as ${partitionStateInfo.leaderEpoch}? Perhaps we can change the logging to ""$topicPartition starts leader epoch $leaderEpoch from offset ${getReplica().get.logEndOffset.messageOffset}""?"
109216215,2743,junrao,2017-03-31T17:56:39Z,"Given the logging in Partition.makeLeader(), it seems that we don't need the logging here."
109217691,2743,junrao,2017-03-31T18:03:39Z,indentation
109219494,2743,junrao,2017-03-31T18:12:35Z,"This needs to be logged before line 80? Otherwise, the new epoch is already the last epoch."
109225083,2743,junrao,2017-03-31T18:38:38Z,"We only want to return the offset for the epoch if the replica is still the leader. We probably want to call replicaManager.getLeaderReplicaIfLocal(), catch exceptions like NOT_LEADER_FOR_PARTITION and UnknownTopicOrPartitionException and convert it to an error code like what's in ReplicaManager.readFromLocalLog()."
109225474,2743,junrao,2017-03-31T18:40:24Z,"Hmm, if requestedEpoch == UNDEFINED_EPOCH, it seems that we should return UNDEFINED_EPOCH_OFFSET so that the follower can fall back to HW."
109225572,2743,junrao,2017-03-31T18:40:50Z,Inaccurate comment.
109225580,2743,junrao,2017-03-31T18:40:52Z,Inaccurate comment.
109226958,2743,junrao,2017-03-31T18:48:03Z,It seems that we should call leaderEpochCache.clear() in this case since all data is gone.
109227964,2743,junrao,2017-03-31T18:53:24Z,"In this case, we are removing all data starting at startOffset. So, we want to call leaderEpochCache.clearLatest instead. Even if there is no log recovery, it will be useful to make sure that leaderEpochCache is consistent with what's in the log. So, instead of doing it here, we could just call leaderEpochCache.clearLatest(nextOffset) immediately after loadSegments()."
109229094,2743,junrao,2017-03-31T18:59:10Z,"Legacy messages will have epoch < 0 and we don't want to flood the logging. So, we can probably only log a warning if epoch is >= 0?"
109230643,2743,junrao,2017-03-31T19:07:19Z,"Not sure if it matters, but we probably want to define leaderEpochCache before loadSegments() is called since log recovery needs access to leaderEpochCache."
109231519,2743,junrao,2017-03-31T19:12:46Z,"I think this also needs to be called during log recovery in LogSegment.recover(). Also, during this process, it's possible for an older epoch to be assigned again. To avoid the unnecessary logging in maybeWarn, on way is for the caller can only call assign() from latestOffset()."
109256482,2743,junrao,2017-03-31T21:47:17Z,The comment seems inaccurate. DeleteSegment only clearOldest.
109256659,2743,junrao,2017-03-31T21:48:39Z,Could we add epochCache to the comment above?
109256943,2743,junrao,2017-03-31T21:50:55Z,clearEarliest to match clearLatest?
109261174,2743,junrao,2017-03-31T22:26:58Z,No need for this logging?
109261760,2743,junrao,2017-03-31T22:33:01Z,indentation
109262199,2743,junrao,2017-03-31T22:37:36Z,allPartitions no longer used.
109263288,2743,junrao,2017-03-31T22:48:18Z,This can be private.
109264654,2743,junrao,2017-03-31T23:02:45Z,consumerId not used.
109265484,2743,junrao,2017-03-31T23:12:15Z,"We should probably add a new tag KAFKA_0_11_0_IV2 which corresponds to the introduction of LeaderEpoch request, and use it here."
109265507,2743,junrao,2017-03-31T23:12:29Z,"""Fetch from the leader"" can be a bit confusing. ""Issue LeaderEpochRequest to the leader""?"
109265520,2743,junrao,2017-03-31T23:12:34Z,remove ?
109268030,2743,junrao,2017-03-31T23:45:31Z,"Perhaps we can also change the line 945 from
    if(targetOffset > logEndOffset) {
to 
    if(targetOffset >= logEndOffset) {
"
109269015,2743,junrao,2017-03-31T23:58:16Z,This is the case the leader returned an offset >= LEO. It would be useful to log the topic/partition as well.
109269023,2743,junrao,2017-03-31T23:58:20Z,It would be useful to log the topic as well.
109269093,2743,junrao,2017-03-31T23:59:19Z,Could we add override?
109269313,2743,junrao,2017-04-01T00:02:35Z,This can be private.
109269322,2743,junrao,2017-04-01T00:02:44Z,This can be private.
109269876,2743,junrao,2017-04-01T00:11:25Z,It doesn't seem this method and the trait are used.
109311609,2743,junrao,2017-04-02T15:27:22Z,"Could we remove TODO? If we just want to test corrupted messages, there is no need to set includePartitionInitialisation to true."
109312409,2743,junrao,2017-04-02T15:58:38Z,This is because we bounce the leader epoch when the controller changes the ISR too so that the latest isr can be updated in the broker's cache.
109312786,2743,junrao,2017-04-02T16:17:18Z,"Hmm, there is a subtle question here, which is should the new epoch be added to epoch cache when the leader epoch advances or when there is actually a message added in the new epoch. The latter means that epoch will be more consistent after log recovery and be more consistent between the leader and the follower. So, perhaps it's better to do the latter. Then the flow will be (1) we remember the latest epoch in Partition.leaderEpoch when receiving leaderAndIsrRequests, but not updating the leader epoch cache yet; (2) we pass partition.leaderEpoch to log.append() and only update the leader epoch cache when there is a new message produced. This will also make the test a bit easier to understand since the epoch will always be consistent btw the leader and the follower."
109313719,2743,junrao,2017-04-02T16:54:09Z,This can be done using TestUtils.waitUntilTrue(() ?
109313817,2743,junrao,2017-04-02T16:58:00Z,"Since we have a large linger and batch size in the producer, does it matter whether we send those 100 messages in batches? It seems that all of them will be in the same batch after the flush() call."
109314127,2743,junrao,2017-04-02T17:10:49Z,unused method.
109314585,2743,junrao,2017-04-02T17:31:46Z,Put two statements in different lines and remove ;
109314603,2743,junrao,2017-04-02T17:32:27Z,remove ;
109314622,2743,junrao,2017-04-02T17:33:15Z,remove ; in the above 2 statements. Ditto in a few other places.
109315673,2743,junrao,2017-04-02T18:11:09Z,"deleteCorrespondingLeaderEpochs should only be set to true when we are deleting a prefix of the log segments. The only caller for that is def deleteSegments(deletable: Iterable[LogSegment]). Also, segment.nextOffset() needs scanning the log and can be a bit expensive. So, perhaps, we can call leaderEpochCache.clearOldest() in deleteSegments(deletable: Iterable[LogSegment]) with the recomputed logStartOffset, which is much cheaper."
109316101,2743,benstopford,2017-04-02T18:27:47Z,"This was a pretty big change, but the final one of your first round of comments. Committed now."
109316191,2743,junrao,2017-04-02T18:32:06Z,"The error message seem inaccurate. Here, we are just verifying the log for broker 0, not for broker 1."
109316509,2743,junrao,2017-04-02T18:47:39Z,Do we need TestSender or could we just reuse ReplicaFetcherBlockingSend?
109320017,2743,junrao,2017-04-02T21:05:12Z,"When clearEarliest() is called, it means that the first offset of the log starts at offset. So we want to (1) preserve an entry whose startOffset == offset; (2) if the last entry whose startOffset is < offset and the next entry's offset is > offset or is not present, we want to preserve that last entry and just set its startOffset to offset. We want to change the comment accordingly."
109320018,2743,junrao,2017-04-02T21:05:14Z,oldest => earliest?
109320084,2743,junrao,2017-04-02T21:07:48Z,Is this needed?
109320101,2743,junrao,2017-04-02T21:08:16Z,Should we use just use createProducer()?
109320117,2743,junrao,2017-04-02T21:08:55Z,A few methods like that can be made private.
109320127,2743,junrao,2017-04-02T21:09:21Z,"Hmm, why do we need to create a new producer here? If so, should we close the old one first?"
109320169,2743,junrao,2017-04-02T21:11:39Z,This is because we have to first change leader to -1 and then change it again to the live replica.
109320452,2743,junrao,2017-04-02T21:24:48Z,A few methods like that in this file can be private.
109320567,2743,junrao,2017-04-02T21:29:44Z,Is this needed? It seems that we create the dir when initializing Log in loadSegments().
109320709,2743,junrao,2017-04-02T21:34:25Z,The second param should be offset + 3 too?
109321139,2743,junrao,2017-04-02T21:50:17Z,This can be private.
109321508,2743,junrao,2017-04-02T22:04:12Z,"Hmm, if we are only deleting one segment, shouldn't the first offset be 5 and we should preserve EpochEntry(1,5)?"
109321591,2743,junrao,2017-04-02T22:06:53Z,This is actually not truncating the first segment. Ditto in line 1478.
109321653,2743,junrao,2017-04-02T22:09:46Z,"Hmm, we are already testing multiple lines in the previous test. Is this testing multiple partitions in the same topic?"
109321773,2743,junrao,2017-04-02T22:14:26Z,Should this be removed now that leader epoch is at the set level?
109321789,2743,junrao,2017-04-02T22:15:07Z,unused import
109321997,2743,junrao,2017-04-02T22:24:47Z,The code is for 3 times.
109322753,2743,junrao,2017-04-02T22:54:58Z,The part of calling assign() during log recovery still needs to be addressed.
109322961,2743,junrao,2017-04-02T23:02:18Z,"This probably can be done in a followup patch. If there is an error, we probably want to add a bit of delay to the partition before retrying the OffsetsForLeaderEpoch request (like what we do when the fetch request has an error)."
109333196,2743,junrao,2017-04-03T02:45:44Z,Should we uncomment this?
109447563,2743,benstopford,2017-04-03T15:29:09Z,"(2) is a good point. Thank you. Have altered and added appropriate tests. In a bit of a rush, coding this in the airplane lounge.  "
109447763,2743,benstopford,2017-04-03T15:29:50Z,I need this to get tests to pass locally. Feel free to remove as I have a PR to change these running  separately. 
109448157,2743,benstopford,2017-04-03T15:31:17Z,Hmm. This is covering a chicken/egg situation around the initialisation of Log (i.e. where we initialise leaderEpochCache). Needs changing. 
109547558,2743,junrao,2017-04-03T23:24:20Z,epochsByTopic => epochsByTopicPartition?
109547577,2743,junrao,2017-04-03T23:24:27Z,This doesn't seem to be used.
109547593,2743,junrao,2017-04-03T23:24:34Z,This doesn't seem to be used.
109547736,2743,junrao,2017-04-03T23:25:33Z,-1L = > EpochEndOffset.UNDEFINED_OFFSET ?
109547743,2743,junrao,2017-04-03T23:25:39Z,The method is not used.
109547807,2743,junrao,2017-04-03T23:26:02Z,"At this point, the log dir may not have been created. So, we probably need to make sure the log dir exists first."
109547834,2743,junrao,2017-04-03T23:26:16Z,We can set the deleteEpoch flag to false here since we call clearLatest() after loading the log.
109547857,2743,junrao,2017-04-03T23:26:26Z,Could we move line 943 to after line 949?
109547889,2743,junrao,2017-04-03T23:26:37Z,"Could we remove the TODO? Also, could we move this line to before line 960?"
109548130,2743,junrao,2017-04-03T23:28:14Z,"Hmm, if the broker is on a version before KAFKA_0_11_0_IV2 and we don't let the partition go through the initialization phase, then the followers won't be doing any truncation based on HW. 

We can probably always set the partition to need initialization. If the broker is on a version before KAFKA_0_11_0_IV2, in fetchEpochsFromLeader(), we don't do the actual leader epoch request, but simply set the response to UNKNOWN_OFFSET. Then the maybeTruncate() logic will just fall back to HW."
109548156,2743,junrao,2017-04-03T23:28:20Z,It would be useful to log the topic/partition as well.
109548176,2743,junrao,2017-04-03T23:28:27Z,Do maybeWarn() in else?
109548190,2743,junrao,2017-04-03T23:28:33Z,"It will be useful to log the topic/partition as well. Also, could this just be debug level logging?"
109548206,2743,junrao,2017-04-03T23:28:42Z,"If earliestOffset() == offset, it seems that we don't need to do anything."
109548223,2743,junrao,2017-04-03T23:28:49Z,"Similar here, we want to find entries with entry.startOffset < offset."
109548234,2743,junrao,2017-04-03T23:28:54Z,Not sure if we need the if test here.
109548266,2743,junrao,2017-04-03T23:29:09Z,"This should say ""epoch < latestEpoch"". an PartitionLeaderEpoch  => a PartitionLeaderEpoch "
109548287,2743,junrao,2017-04-03T23:29:19Z,It seems that we only append messages of format V2 and newer. So epoch is never expected to be <0 ?
109548308,2743,junrao,2017-04-03T23:29:29Z,"Hmm, it seems that only the first segment will be removed according to the retention policy?"
109548322,2743,junrao,2017-04-03T23:29:35Z,It's no longer doing this in batches.
109548330,2743,junrao,2017-04-03T23:29:39Z,This can be private.
109548343,2743,junrao,2017-04-03T23:29:47Z,Put two statements in different lines and remove ; There are quite a few other places using ;.
109651046,2743,benstopford,2017-04-04T12:42:53Z,"Oh, it's used by one of the tests. AuthorizerIntegrationTest. All the Request/Response classes have it. "
109651092,2743,benstopford,2017-04-04T12:43:09Z,as above
109651463,2743,benstopford,2017-04-04T12:45:10Z,see AuthorizerIntegrationTest
109781747,2743,benstopford,2017-04-04T21:26:54Z,"I think this is ok, but can remove if you feel strongly. "
109795882,2743,benstopford,2017-04-04T22:43:51Z,hmm. I think this actually correct. I would like a better way to express it but I can't see one. I'd encourage you to look at the tests in LeaderEpochFileCacheTest to see if you disagree with any of them.
109967050,2743,junrao,2017-04-05T16:41:06Z,"Yes, I agree that it's needed."
124910746,2743,lindong28,2017-06-29T20:55:49Z,It seems that the method's Java doc is inconsistent with its behavior if requestedEpoch is < the first epoch in epochs. I am wondering if we should update the Java doc or comment in this code to explain this. I only realized it is intentional after reading this @junrao's comment in this pull request.
124923278,2743,junrao,2017-06-29T22:01:02Z,@lindong28 : We can clarify that in the comment. Could you file a jira?
124927503,2743,lindong28,2017-06-29T22:27:39Z,Sure. I filed https://issues.apache.org/jira/browse/KAFKA-5542 and assigned it to @benstopford.
1890534015,18240,jsancio,2024-12-18T16:37:36Z,Can we try moving this to the internal module? Anything public in this package can be used outside the `raft` module.
1890534655,18240,jsancio,2024-12-18T16:38:08Z,Can we explicitly mark private any method that is not used outside of this class?
1890535623,18240,jsancio,2024-12-18T16:38:49Z,Please write Java doc for all public methods.
1890542820,18240,jsancio,2024-12-18T16:43:51Z,`IllegalArgumentException` seems like a better exception type.
1890544866,18240,jsancio,2024-12-18T16:45:11Z,Let's write Java doc for all public methods.
1890552560,18240,jsancio,2024-12-18T16:50:45Z,"This is not a warning. This is a valid state or condition. We should be able to log this message at INFO level. It should be rare because there are backoff/timeout logic in Candidate, Follower and Unattached which limit how quickly a replica transitions to Prospective. What do you think?"
1890554341,18240,jsancio,2024-12-18T16:52:04Z,Let's use the same word you used in the `NomineeState`: `isNomineeState()` and `nomineeStateOrThrow()`.
1890660800,18240,ahuang98,2024-12-18T18:22:03Z,"> This is a valid state or condition.

|I see your point here, there's nothing 'incorrect' about this happening. 
> It should be rare because there are backoff/timeout logic in Candidate, Follower and Unattached which limit how quickly a replica transitions to Prospective.

But I'm wondering about the case where a controller quorum is left partially upgraded on accident. Would having a warning log make this situation more discoverable?


"
1890678659,18240,jsancio,2024-12-18T18:37:30Z,"> But I'm wondering about the case where a controller quorum is left partially upgraded on accident. Would having a warning log make this situation more discoverable?

Yeah. This would be interesting to monitor but cluster are not really monitored by looking at the log. Clusters are monitored by collecting and comparing metrics.

This is an issue beyond KRaft and should be solved holistically. Kafka could have a metrics that fires if the API versions and supported features don't match across all of the replicas. This metric would require a KIP to implement.

Having said that, we should keep this message but it should not be logged at WARN. It should be logged at INFO."
1894260795,18240,ahuang98,2024-12-20T18:28:20Z,"it might not be expected that a state has both leader and voted key state, but I think it's better not to lose the state if it does happen to exist.

not an issue for backwards compatibility, older version would lose votedkey state but would have leader state and will reject standard votes correctly because of that."
1895898210,18240,ahuang98,2024-12-23T15:52:47Z,"open to what you think, this definitely isn't necessary but I thought this could help reduce the complexity of handleVoteResponse (this is called in a 3rd level of conditional statements)"
1895926505,18240,ahuang98,2024-12-23T16:24:52Z,"Found existing test which checks that observers with ids can vote - https://github.com/apache/kafka/blob/e30edb3eff0d2794854fa270ee1a4514dd983d6c/raft/src/test/java/org/apache/kafka/raft/QuorumStateTest.java#L1511 which was added in KAFKA-16526, so I've removed `Voted` for now (vs translating to `UnattachedVoted`)"
1896920647,18240,jsancio,2024-12-24T18:52:19Z,"The type should be `CandidateState`. Let's add the field `retries` back to the message.

How about replacing `voterStates` with `epochState` and implementing a `toString` method for `EpochState`?"
1896921039,18240,jsancio,2024-12-24T18:53:27Z,Let's move this method so that it doesn't show in the diff.
1896921206,18240,jsancio,2024-12-24T18:53:57Z,Let's move this method so that it doesn't show in the diff.
1896921449,18240,jsancio,2024-12-24T18:54:31Z,Let's move this method so that it doesn't show in the diff.
1896921655,18240,jsancio,2024-12-24T18:54:53Z,Let's move this method so that it doesn't show in the diff.
1896923862,18240,jsancio,2024-12-24T19:01:12Z,Let's revert this change. We should make it clear that this PR is not changing the persisted `quorum-state`.
1896926243,18240,jsancio,2024-12-24T19:07:42Z,Nice code removal! Thanks.
1896926826,18240,jsancio,2024-12-24T19:09:03Z,"I got the impression that you don't use this in `src/main`. If so, let's remove it."
1896930978,18240,jsancio,2024-12-24T19:21:53Z,"Let's just remove this method and have the tests use `transitionToUnattached(int, OptionalInt)`.

Also update the java doc to match the new signature and parameters."
1896932468,18240,jsancio,2024-12-24T19:26:02Z,"Let's remove this and update the callers to use `transtitionToUnattached(int, OptionalInt)`."
1896934629,18240,jsancio,2024-12-24T19:32:12Z,"How about:
```java
        if (topLevelError == Errors.UNSUPPORTED_VERSION && quorum.isProspective()) {
            logger.info(
                ""Prospective received unsupported version error in vote response in epoch {}, "" +
                ""transitioning to Candidate state immediately since entire quorum may not support PreVote."",
                quorum.epoch()
            );
            transitionToCandidate(currentTimeMs);
            return true;
        } else if (topLevelError != Errors.NONE) {
            return handleTopLevelError(topLevelError, responseMetadata);
```"
1896935574,18240,jsancio,2024-12-24T19:35:15Z,"Let's fix the indentation. In the raft module we using this formatting style:
```java
                logger.debug(
                    ""Ignoring vote response {} since we are no longer a VotingState "" +
                    ""(Prospective or Candidate) in epoch {}"",
                    partitionResponse,
                    quorum.epoch()
                );
```"
1896936282,18240,jsancio,2024-12-24T19:37:26Z,Let's fix this formatting. See my under examples on how we try to format code in the raft module.
1896936776,18240,jsancio,2024-12-24T19:39:06Z,This code only have one caller. How about just manually inline it at the call site.
1896939688,18240,jsancio,2024-12-24T19:47:01Z,Same here. Let's fix the indentation since you are already changing this part of the code.
1896941400,18240,jsancio,2024-12-24T19:52:07Z,I think this is too relax. The previous code assumed that if `leaderId.isPresent()` then `!leaderEndpoints.isEmpty()`.  That should not change in this PR.
1896944012,18240,jsancio,2024-12-24T20:00:05Z,When would raft hit this case?
1896944517,18240,jsancio,2024-12-24T20:01:45Z,"Minor but you can just inline the expression: `quorum.isProspective()`.

Also, the call site that calls this knows the `NomineeState` so it can use that information to determine if the request is a preVote request. No need to query `QuorumState` for this information.

For example, you can add `isPreVote` to the `NomineeState` interface and update the signature of this method to `buildVoteRequest(ReplicaKey, boolean)`."
1896952514,18240,jsancio,2024-12-24T20:29:38Z,This handle when the majority of the voters rejected the candidate. This needs to also handle when the majority of the voters reject the prospective candidate.
1896953461,18240,jsancio,2024-12-24T20:33:17Z,"This code shouldn't check `isVoteRejected` if it is handled in `handleVoteRespose`. I left a comment about this in that method.

This is an event driven programming model. The event that cause the majority of the voters to reject the prospective state is received in the vote response. And not when polling the prospective state."
1896955782,18240,jsancio,2024-12-24T20:40:24Z,"I see. Retries is only preserved when the prospective transition from prospective to candidate. The retries are lost if it transitions to unattached.

I think we should file a jira to remove this exponential backoff. I am convinced that it is starting to lose its value with this implementation and if we make the election timeout improvements you highlight in the KIP"
1896957160,18240,jsancio,2024-12-24T20:44:47Z,Let's also include the epoch election and add a `toString` method to the `EpochElection` type.
1896957660,18240,jsancio,2024-12-24T20:46:39Z,Let's write a comment explaining why this expression is needed.
1896958513,18240,jsancio,2024-12-24T20:49:40Z,Should this check that the epoch is not decreasing?
1896958745,18240,jsancio,2024-12-24T20:50:28Z,"Let's fix this formatting.
```java
                    ""Cannot add voted key (%s) to current state (%s) in epoch %d since it matches the local "" +
                    ""broker.id"",
```"
1896958960,18240,jsancio,2024-12-24T20:51:19Z,This should check that the epoch is not decreasing.
1896959037,18240,jsancio,2024-12-24T20:51:36Z,Fix formatting.
1896960167,18240,jsancio,2024-12-24T20:55:06Z,"How about this formatting:
```java
        durableTransitionTo(
            new ProspectiveState(
                time,
                ...
            )
        );
```"
1896961481,18240,jsancio,2024-12-24T21:00:19Z,Let's add a java doc comment to this method.
1896961706,18240,jsancio,2024-12-24T21:01:13Z,"```java
        return String.format(
            ""Unattached(epoch=%d, leaderId=%s, votedKey=%s, voters=%s, "" +
            ""electionTimeoutMs=%d, highWatermark=%s)"",
```"
1896962158,18240,jsancio,2024-12-24T21:02:46Z,Let's undo this change. It is good to keep the existing invariant to avoid persisting both `leaderId` and `votedKey`.
1896962548,18240,jsancio,2024-12-24T21:04:00Z,"```java
            .collect(
                Collectors.toMap(
                    ReplicaKey::id,
                    VoterState::new
                )
            );
```"
1896962727,18240,jsancio,2024-12-24T21:04:56Z,Let's add a `toString` method to this type so that its value is included in log messages.
1896962935,18240,jsancio,2024-12-24T21:05:52Z,This is a publicly visible change. Let's update the KIP if it doesn't include this change.
1897006321,18240,ahuang98,2024-12-25T00:07:15Z,"this method is solely for the case of adding voted state to unattached in the same epoch (if higher epoch, we take the path of transitionToUnattached)

the transitionToUnattached method has the following comment, I will duplicate it for this method too
```
     * Note, if we are transitioning from unattached and there is no epoch change, we take the path of
     * unattachedAddVotedState instead.
```"
1897013977,18240,ahuang98,2024-12-25T00:36:05Z,"I think I had convinced myself this was arguably not an invariant since it is not enforced (and perhaps just an unintentional quality of KRaft today). 

I know we chatted earlier about how KRaftVersion=2 should enforce that both votedKey and leaderId are persisted if they exist, which I agree with. I guess what we're not on the same page about is if we can start to persist all the information we have, now. I would argue it is fine to do now because it is backwards compatible, the additional info isn't necessary for correctness, and losing that additional information also doesn't affect correctness (e.g. currently, if Unattached with leaderId grants a standard vote, it transition to UnattachedVoted w/o leaderId. UnattachedVoted w/o leaderId and UnattachedVoted w/ leaderId behave the same way in rejecting votes. If Unattached has both leaderId and votedKey in electionState and then downgrades, it would only retain the votedKey and transition to UnattachedVoted w/o leader, which is the same transition that would have been taken in the past for an Unattached w/ leaderId that grants a vote)"
1897014715,18240,ahuang98,2024-12-25T00:39:01Z,"Retaining both leaderId and votedKey also can help other replicas find the leader faster (though not needed for correctness) -
UnattachedVoted w/ leader will reject vote requests w/ a leaderId  
UnattachedVoted w/o leader will reject vote request w/o a leaderId

If we only persist one, then on startup a replica just doesn't have all the information it _could_ have"
1899044657,18240,ahuang98,2024-12-29T01:12:26Z,"Fixed.
I added this because I thought it was an oversight not to check for empty endpoints given that we can initialize in UnattachedState when the leaderendpoints are not known. But I see now that `maybeTransition` is only called in places where the leaderendpoints are expected to be populated"
1899071006,18240,ahuang98,2024-12-29T06:28:07Z,Fixed
1899072269,18240,ahuang98,2024-12-29T06:41:32Z,we can use this Jira to track - https://issues.apache.org/jira/browse/KAFKA-18345
1899074255,18240,ahuang98,2024-12-29T06:59:55Z,I could also remove `epoch` as one of the params (method can use state.currentEpoch instead of parameter value). But I would prefer to keep `epoch` as a param so we can validate the method is being used correctly (without any unintentional epoch change).
1899074463,18240,ahuang98,2024-12-29T07:01:25Z,"same as above, this is meant to be called only to transition from prospectiveNotVoted in epoch X to prospectiveVoted in epoch X"
1899076157,18240,ahuang98,2024-12-29T07:16:33Z,"this is called within the third level of a conditional statement, adding this back violates checkstyle's cyclomatic complexity check"
1899077542,18240,ahuang98,2024-12-29T07:27:49Z,"for now, I've kept the helper but increased its scope to handle the case when prospective loses the election. I've renamed it as `maybeHandleElectionLoss` "
1899341945,18240,ahuang98,2024-12-30T07:39:52Z,"the existing raft event simulation tests picked up on a new bug in pollResigned - if we simply replace the transitionToCandidate(currentTimeMs) with transitionToProspective(currentTimeMs), a cordoned leader in epoch 5 could resign in epoch 5, transition to prospective in epoch 5 (with leaderId=localId), fail election and then attempt to become follower of itself in epoch 5. 

so far, these are the alternatives which seem reasonable to me:
- resigned voter in epoch X should transition to prospective in epoch X+1 
    - cons: need to create a special code path just for this case to allow becoming prospective in epoch+1 (would also add trivial complexity for determining if votedKey or leaderId should be kept from prior transition). transitioning to prospective in epoch + 1 is almost as disruptive as transitioning directly to candidate since it involves an epoch bump
    - pro: probably the option which follows intentions of past logic most closely
- resigned voter in epoch X should simply transition to unattached in epoch X+1 (current version)
    - con: resigned replica has to wait two election timeouts after resignation to become prospective
    - pro: simplified logic. unless this is the only replica eligible for leadership in the quorum (e.g. due to network partitioning), the impact of waiting two election timeouts after resignation is small - all other replicas should be starting their own elections within a single fetch timeout/election timeout 
- resigned voter in epoch X instead waits a smaller backoffTimeMs before transitioning to unattached in epoch X+1
    - con: scope creep - what should this backoff be? additional changes to resignedState
    - pro: resigned voter waits less time before becoming eligible to start a new election."
1899579689,18240,jsancio,2024-12-30T14:26:34Z,Yes. This is correct. Observers can vote for candidate (KIP-853) and prospective (KIP-996). This was changed as part of KIP-853 as documented [here](https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=217391519#KIP853:KRaftControllerMembershipChanges-Leaderelection).
1899610803,18240,jsancio,2024-12-30T15:11:33Z,"Got it. Thanks. I see now that kraft doesn't check if both the leader and voted field are set during iniialization.

During initialization, it does check the voted field first before checking the leader field. I think we should switch that order. If the leader and the leader endpoints are known, the replica should transition to follower immediately instead of needing to rediscover the leader.

Let's change this comment too as it is slightly inaccurate. There are many reason why the replica may not send the leader endpoints. Using an old version for the RPC is not the only reason why the replica may send the leader id but not the leader endpoint:
```java
      private Optional<Boolean> maybeHandleCommonResponse(
          Errors error,
          OptionalInt leaderId,
          int epoch,
          Endpoints leaderEndpoints,
          Node source,
          long currentTimeMs
      ) {
          if (leaderEndpoints.isEmpty() && leaderId.isPresent()) {
              // The response didn't include the leader endpoints because it is from a replica
              // that doesn't support reconfiguration. Look up the leader endpoint in the
              // voter set.
              leaderEndpoints = partitionState
                  .lastVoterSet()
                  .listeners(leaderId.getAsInt());
          }
```"
1899633981,18240,jsancio,2024-12-30T15:47:39Z,"> the existing raft event simulation tests picked up on a new bug in pollResigned

What is the exact error? Let's add an unittest to one of the `KafkaRaftClient*Test` suite that shows the bug.

> attempt to become follower of itself in epoch 5.

Let's add a check to `transtitionToFollower` that checks that `leaderId` is not equal to `localId`.

It makes sense to me that after the resign state the replica should always increase its epoch. The replica resigned from leadership at epoch X so eventually the epoch will be at least X + 1. Did you consider transitioning to candidate and relaxing the transition functions to allow both resigned and prospective to transition to candidate?"
1899726228,18240,ahuang98,2024-12-30T18:37:26Z,"discussed offline, transitionToUnattached has existing logic for assigning election timeouts which we can borrow - we can just add an additional if clause that if we came from resignedState, assign electionTimeout to resignedState.electionTimeout which is effectively 0"
1899726857,18240,ahuang98,2024-12-30T18:38:54Z,discussed offline that this method is only used for adding voted state to unattached (in same epoch)
1899727021,18240,ahuang98,2024-12-30T18:39:15Z,discussed offline
1899773951,18240,jsancio,2024-12-30T20:22:57Z,Sounds good to keep the epoch parameter and validating it against the current epoch.
1899778973,18240,jsancio,2024-12-30T20:34:54Z,Let's add an else  case and throw an illegal state exception.
1899780274,18240,jsancio,2024-12-30T20:38:21Z,"How about passing the `NomineeState` object, checking the subtype of that object and casting to the appropriate subtype."
1899782793,18240,jsancio,2024-12-30T20:44:33Z,How about just printing the entire `epochElection`? It may be useful to know the state of the entire voter set not just the rejecting voters.
1899783114,18240,jsancio,2024-12-30T20:45:25Z,How about just printing the entire `epochElection`? It may be useful to know the state of the entire voter set not just the rejecting voters.
1899783839,18240,jsancio,2024-12-30T20:47:12Z,"I think you can join these lines. If not, there should be a newline before `);`"
1899783973,18240,jsancio,2024-12-30T20:47:36Z,Add a newline before `);`.
1899785061,18240,jsancio,2024-12-30T20:50:11Z,Remove this line. Let's not have commented code.
1899785505,18240,jsancio,2024-12-30T20:51:24Z,Let add a comment that summarizes our discussion and conclusion. It is good to document and explain this decision.
1899790092,18240,jsancio,2024-12-30T21:03:06Z,"As we discussed offline, the resigned state also transitions to unattached with a greater epoch. Let's document that.

Having said that, let's also update the comment at the top of this file that documents the transitions from resigned:
> Resigned transitions to:

Let's also update the KIP."
1899791694,18240,jsancio,2024-12-30T21:07:39Z,Missing new line between these two lines.
1900259204,18240,jsancio,2024-12-31T20:30:38Z,Let's keep the previous pattern of using static methods to construct `ElectionState`. You can add `Optional<ReplicaKey>` parameter to `withElectedLeader`.
1900264788,18240,jsancio,2024-12-31T20:55:16Z,"Why do you need to call `maybeFireLeaaderChange`? Based on the inputs and since prospective doesn't increase the epoch, I would assume that the leader and epoch doesn't change when transitioning to prospective."
1900278415,18240,jsancio,2024-12-31T22:01:43Z,"`QuorumState` already logs all transitions. It logs the ""from"" and ""to"" state. Not sure this add any information."
1900278536,18240,jsancio,2024-12-31T22:02:33Z,Same here. QuorumState already logs all state transitions. 
1900279911,18240,jsancio,2024-12-31T22:09:59Z,"Transitioning to prospective is not really a durable transition since no persisted data should have changed, right? You can see this is the case since the function `transitionToProspective` doesn't take any inputs and it doesn't increase the epoch.

In other words, the information that is persisted is information that quorum state already knows and has already been persisted."
1900448685,18240,jsancio,2025-01-01T19:17:21Z,"You don't need this method, right? This method is declared by `EpochState`."
1900449219,18240,jsancio,2025-01-01T19:22:56Z,Can you remove this if it is not needed anymore?
1900449269,18240,jsancio,2025-01-01T19:23:30Z,Can you remove this if it is not needed?
1900451851,18240,jsancio,2025-01-01T19:45:23Z,Offline you mentioned that you added this because you didn't want to lose information when transitioning states. I agree with this goal but the voted key is lost when the replica transitions to the `LeaderState`. Do you agree? If so can you file a jira to fix this after this PR.
1900452510,18240,jsancio,2025-01-01T19:50:09Z,"Minor but let's just remove the `=` sign.
```java
                    ""Cannot transition to Unattached with epoch %d from current state %s"",
```"
1900453595,18240,jsancio,2025-01-01T19:59:59Z,State transition changes are already logged at `INFO` level.
1900455665,18240,jsancio,2025-01-01T20:18:33Z,"Isn't this the same
```java
                epoch == currentEpoch ? votedKey() : Optional.empty(),
```
If so, you can remove the variable `retainVotedKey`.

Similar to the unattached implementation, let's document why this is done."
1900458506,18240,jsancio,2025-01-01T20:42:39Z,"You should be able to remove the check for if it is the only voter by making that case transition to prospective instead. When the replica transitions to prospective, it already short-circuits that transition. When the replica transitions to prospective it checks if it can immediately transition to candidate."
1900459611,18240,jsancio,2025-01-01T20:52:49Z,"```java
    public ProspectiveState prospectiveStateOrThrow() {
        return maybeProspectiveState().orElseThrow(
            () -> new IllegalStateException(""Expected to be Prospective, but current state is "" + state)
        );
    }
```"
1900460262,18240,jsancio,2025-01-01T20:58:10Z,"```java
        VoterState voterState = getVoterStateOrThrow(voterId);
        boolean wasUnrecorded = voterState.state == VoterState.State.UNRECORDED;
```"
1900460957,18240,jsancio,2025-01-01T21:04:44Z,Looks like this doesn't need to be public. Looks like this method can be removed since it is not used.
1900461158,18240,jsancio,2025-01-01T21:06:15Z,"```java
                ""VoterState(replicaKey=%s, state=%s)"",
```"
1900461371,18240,jsancio,2025-01-01T21:08:36Z,"Why not just print the map?
```java
        return String.format(
            ""EpochElection(voterStates=%s)"",
            voterStates
        );
```"
1900540449,18240,ahuang98,2025-01-02T04:59:01Z,"it felt redundant to print the keys given that the replica ids are also contained in the values. since this is would only be used for debugging though, I'll take your suggestion and just print the entire map"
1900541138,18240,ahuang98,2025-01-02T05:01:16Z,"ReplicaKey's toString method contains the class name so I didn't want to be redundant - `String.format(""ReplicaKey(id=%d, directoryId=%s)"", id, directoryId);`"
1900546362,18240,ahuang98,2025-01-02T05:17:15Z,"Like the following?
```
    private void maybeHandleElectionLoss(NomineeState state, long currentTimeMs) {
        if (state instanceof CandidateState) {
            CandidateState candidate = (CandidateState) state;
        ...
        else if (state instanceof ProspectiveState) {
            ProspectiveState prospective = (ProspectiveState) state;
        ...
```
Is the intention of the additional parameter to make it clear this method should be called on NomineeState? This seems a bit redundant with the existing QuorumState helpers (e.g. isCandidate() and candidateStateOrThrow())."
1900571487,18240,ahuang98,2025-01-02T06:24:14Z,I'll also change Prospective and Unattached's election() to use the static methods.
1900580715,18240,ahuang98,2025-01-02T06:46:34Z,Replacing with code comments instead
1900583997,18240,ahuang98,2025-01-02T06:54:06Z, 
1900606997,18240,ahuang98,2025-01-02T07:40:28Z,https://issues.apache.org/jira/browse/KAFKA-18389
1900614259,18240,ahuang98,2025-01-02T07:53:35Z,Thanks! I'll add the logic for short-circuiting transitions for only-voters. This also allows our invariant - only Prospective can transition to Candidate - to remain simple w/o edge cases.
1900648700,18240,ahuang98,2025-01-02T08:43:45Z,Are you perhaps confusing `EpochElection` with `ElectionState` (the latter is what EpochState has declared)
1900951176,18240,jsancio,2025-01-02T14:48:09Z,"If the quorum has a size of one and since the replica votes for itself when transitioning to prospective, `isVoteGranted()` should always return true. If so, the replica doesn't need to check if it is the only voter.

Let's confirm we have a test for this in KafkaRaftClientTest. If not, let's add a test.

Let's also confirm that we have a test for this in ProspectiveStateTest and CandidateStateTest. If not, let's add tests for these cases."
1900954282,18240,jsancio,2025-01-02T14:51:23Z,"Why do you check that is not leader? In KRaft a replica should never start as a leader. KRaft throws and illegal state exception if it starts as leader. See line 545 above.
```java
        if (quorum.isLeader()) {
            throw new IllegalStateException(""Voter cannot initialize as a Leader"");
```"
1901113187,18240,jsancio,2025-01-02T17:43:07Z,Yes. We discussed this offline.
1901117479,18240,ahuang98,2025-01-02T17:48:39Z,Discussed
1901142655,18240,ahuang98,2025-01-02T18:17:36Z,"Discussed offline, technically the replica can transition to leader due to the above conditional. 
We can improve this conditional by directly checking if the replica is Unattached or Follower, and merge this conditional into the above conditional"
1901235462,18240,ahuang98,2025-01-02T20:27:29Z,"Added four tests for this, starting at `testInitializeAsOnlyVoterWithEmptyElectionState`

> confirm that we have a test for this in ProspectiveStateTest and CandidateStateTest

Confirmed!"
1902113084,18240,ahuang98,2025-01-03T19:49:45Z,"I've organized QuorumStateTest in the following way - misc tests were pulled to the front. All other tests are organized under banners (e.g. Initialization tests, Tests of transitions from state X)"
1902114352,18240,ahuang98,2025-01-03T19:51:26Z,The diff is misleading here. This test was just removed because I found it was a duplicate of `testInitializeAsResignedLeaderFromStateStore`
1902116235,18240,ahuang98,2025-01-03T19:54:13Z,@jsancio the HW drops to -1L after candidate transitions to leader - if you agree this is a bug I'll file a Jira for this
1915345255,18240,jsancio,2025-01-14T17:48:56Z,This feels like it needs a comment explaining what and why.
1915429555,18240,ahuang98,2025-01-14T18:45:41Z,"Clobbered `UnattachedStateWithVoteTest.java` into `UnattachedStateTest.java`. 

I didn't think it was necessary to have a separate file (both having votedKey state and leaderId state are tested in this one file). I wanted to prevent introducing a separate WithVoteTest for ProspectiveState as well"
1916914867,18240,jsancio,2025-01-15T15:55:11Z,Okay. I went through all of the possible combination of quorum state and this change seem to be backward compatible and correct.
1917004191,18240,jsancio,2025-01-15T16:43:54Z,"Please make sure that all of the constructors delegate to this constructor. There is at least one constructor (`this(OptionalInt, Uuid)`) that doesn't delegate construction to this constructor.

That may mean that the tests should not specify the voters through the constructor but instead use the `with...` methods.

In general the `Builder` constructor should be as small as possible and the user can override the configuration using the builder's methods before calling `build()`. I think that means that ideally we should delete this method or it should have this signature: `Builder(ReplicaKey)`."
1917022963,18240,jsancio,2025-01-15T16:55:41Z,Can we avoid this? Can we let the caller makes this decision? It is technically possible for the replicas to support new RPC (protocols) but their voter configuration to be static.
1917031492,18240,jsancio,2025-01-15T17:00:48Z,Let's create a followup Jira to remove this method.
1917037427,18240,jsancio,2025-01-15T17:05:00Z,"We are very close to being able to remove this method, `initializedAsLeader`. Do you want to do the honor and fix the last remaining test?"
1917046168,18240,jsancio,2025-01-15T17:11:28Z,Please use `ElectionStae.withElecterdLeader`.
1917046679,18240,jsancio,2025-01-15T17:11:51Z,Did you mean `votedKey`?
1917055500,18240,jsancio,2025-01-15T17:16:05Z,"This looks like a `static` method. It doesn't use any object fields or methods.
"
1917062525,18240,jsancio,2025-01-15T17:19:48Z,Can we use `isReconfigSupported()`? This will break when we add a new kraft.version.
1917064565,18240,jsancio,2025-01-15T17:21:20Z,Should this version check that the `preVote` field is `false`?
1917068135,18240,jsancio,2025-01-15T17:24:10Z,Hmm. How about having `RaftProtocol` implement `voteRpcVersion()`?
1917072892,18240,jsancio,2025-01-15T17:27:47Z,This is too relax. It should return 0 only for KIP_595_PROTOCOL and throw an illegal argument/state exception for the `else` case.
1917074839,18240,jsancio,2025-01-15T17:29:18Z,Do you want to update the exception messages to reference `withRaftProtocol` instead?
1917096432,18240,jsancio,2025-01-15T17:44:56Z,Do you need this since you are using BEGIN_QUORUM_EPOCH to propagate the leader and epoch.
1917097458,18240,jsancio,2025-01-15T17:45:46Z,The prospective candidate is the same as the leader. Was this done on purpose?
1917114855,18240,jsancio,2025-01-15T17:57:18Z,Is there a reason why you used 2 instead of 1 (like the other cases) for the LEO?
1917116456,18240,jsancio,2025-01-15T17:58:37Z,Same here. Why was the LEO changed to 2 in this case?
1917117528,18240,jsancio,2025-01-15T17:59:30Z,Same here. Why was the LEO changed to 2 in this case?
1917136327,18240,jsancio,2025-01-15T18:12:05Z,"I see. This actually depends on the kraft.version. For kraft.version 1 the local log will have an LEO of 3 (leader change message, kraft version record and voter set record). For kraft.version 0 the local log will have an LEO of 1, no?"
1917149169,18240,jsancio,2025-01-15T18:20:32Z,Is it intentional that this response has a leader for the epoch but the other one does not?
1917155747,18240,jsancio,2025-01-15T18:24:30Z,Interesting that unattached waits for election timeout to transition to prospective while follower waits for fetch timeout to transition to prospective. It is okay for now but maybe they should both wait for fetch timeout since unattached now sends fetch requests.
1917168666,18240,jsancio,2025-01-15T18:35:32Z,"You can just call `pollUntilRequest` since it calls poll at least once, no?"
1917177912,18240,jsancio,2025-01-15T18:43:51Z,"The most informative comparison in case of a failures is:
```java
        assertEquals(OptionalInt.of(leader.id()), context.client.quorum().leaderId());
```"
1917185173,18240,jsancio,2025-01-15T18:49:48Z,Does it need to sleep for 1 ms? Why? Is calling `poll` enough?
1917187753,18240,jsancio,2025-01-15T18:52:02Z,In other tests you sleep for `electionTimeoutMs * 2` why the difference?
1917190771,18240,jsancio,2025-01-15T18:54:11Z,Same here. Is calling poll enough since the remaining time is 0?
1917203942,18240,jsancio,2025-01-15T19:04:24Z,"Technically possible but it is odd that the replica is using a static voter set, with KIP_595_PROTOCOL and an elected leader or voted candidate that is not in the voter set.

Maybe it is less confusing if you limit these tests to KIP_853_PROTOCOL.

Minor but technically the protocol configuration is not needed since the replica doesn't need to send or handle RPCs to become leader."
1917213839,18240,jsancio,2025-01-15T19:13:37Z,Is this used? I couldn't find a caller for this method.
1917216983,18240,jsancio,2025-01-15T19:16:37Z,Checking the leader is not enough. It should also check that the epoch match.
1917220113,18240,jsancio,2025-01-15T19:19:21Z,"Let remove the ""todo"". How about:
```java
/* KAFKA-18439: Currently this just checks the leader is never changed after the first successful election.
 * KAFKA-18439 will generalize the invariant so it holds for all tests even if routing filters are changed.
 * i.e. if the current leader is reachable by majority, we do not expect leadership to change.
 */
```"
1917231251,18240,jsancio,2025-01-15T19:29:13Z,"How about this formatting:
```java
        return voterNode(
            replicaKey,
```"
1917235613,18240,jsancio,2025-01-15T19:33:19Z,Okay. I think it is fair to file a bug but assign it to yourself or me.
1917246184,18240,jsancio,2025-01-15T19:43:16Z,Not sure if idempotent is the correct description. I would just call this test: `testConsecutiveGrant`.
1917246675,18240,jsancio,2025-01-15T19:43:47Z,Not sure if idempotent is the correct description. I would just call this test: testConsecutiveReject.
1917255498,18240,jsancio,2025-01-15T19:52:23Z,What about the non-empty case?
1917258238,18240,jsancio,2025-01-15T19:54:56Z,We should have done this in a different PR. It is difficult for me to see what has change and what has move so I have to review almost the entire file.
1917352224,18240,ahuang98,2025-01-15T21:23:57Z,[KAFKA-18560](https://issues.apache.org/jira/browse/KAFKA-18560)
1917357351,18240,ahuang98,2025-01-15T21:29:22Z,https://issues.apache.org/jira/browse/KAFKA-18561
1917365519,18240,ahuang98,2025-01-15T21:36:42Z,I'll change the other variations of this method to do the same
1917376912,18240,ahuang98,2025-01-15T21:46:22Z,"Sorry, I wish github was a bit smarter with diffs :( It was difficult for me to figure out what coverage we were missing without the tests being more ordered (and I thought it would be difficult for you to tell what we might be missing as well) so I ended up deciding the re-order was worth it. 
We discussed this briefly before, but ideally each state will have its own file in the end - I decided not to make that change in this PR since it would make it even harder to tell what had changed."
1917380871,18240,ahuang98,2025-01-15T21:50:33Z,same with all the other `xyzRpcVersion()` methods?
1917398669,18240,jsancio,2025-01-15T22:08:40Z,You could but I didn't suggest it for the sake of keeping the diff smaller. You can file a Jira to fix this if you want.
1917398687,18240,ahuang98,2025-01-15T22:08:41Z,"yes, I had considered allocating a different node for local to make its voted candidate, but rationalized that the behavior won't change and that this is also a valid/common state for a follower to be in (votedCandidateKey=leaderId)"
1917401126,18240,jsancio,2025-01-15T22:11:13Z,I take it back. This implementation is fine if you want to keep it. Maybe just make it `else if (raftProtocol.isReconfigSupported()) {`.
1917404175,18240,jsancio,2025-01-15T22:14:51Z,"I think I miss spoke. How about moving this out of the constructors and adding a configuration method like `withStartingVoter(VoterSet, KRaftVersion)`. The implementation delegates to `withStaticVoters` or `withBootstrapSnapshot`."
1917419710,18240,ahuang98,2025-01-15T22:29:46Z,"hm, can't think of a reason. I'll standardize"
1917424027,18240,ahuang98,2025-01-15T22:35:05Z,"yep, I decided to just use an LEO of 3 for both cases as to not overcomplicate since it's valid for otherNodeKey to have a larger LEO than local anyways. The difference in LEO after gaining leadership between kraftVersion 0 and 1 is also highlighted and tested in other KafkaRaftClientTests which focus more on fetch/offset validation. 

I'll just add the conditional since it's easy enough"
1917434854,18240,ahuang98,2025-01-15T22:48:44Z,"yes, it was just for variation (since both could be valid responses)"
1917454524,18240,ahuang98,2025-01-15T23:10:33Z,I think I wanted to be more explicit with what happens when - I'll replace `pollUntilRequest()` with the necessary `poll()` calls
1917456559,18240,ahuang98,2025-01-15T23:13:38Z,yes!
1917459804,18240,ahuang98,2025-01-15T23:17:02Z,"no strong reason, I'll standardize"
1917512126,18240,ahuang98,2025-01-16T00:38:50Z,"ah, this actually clears the mock send queue (otherwise the following check `RaftRequest.Outbound fetchRequest = context.assertSentFetchRequest();` fails due to unexpected number of requests in send queue)

it works to remove the poll and clear the expected fetch later in the test, so I'll do that instead."
1917518120,18240,jsancio,2025-01-16T00:48:53Z,"Well, it is good to have self documented test (or code). You can make this connection clear by using the local log end offset if you expect the logs to match. E.g. `context.log.endOffset()`."
1917525983,18240,jsancio,2025-01-16T01:01:45Z,"Missing newline between parenthesis:
```java
            context.voteResponse(false, OptionalInt.empty(), epoch)
        );
```"
1917526225,18240,jsancio,2025-01-16T01:02:10Z,"Missing newline between parenthesis:
```java
            context.voteResponse(false, OptionalInt.empty(), epoch)
        );
```"
1917526506,18240,ahuang98,2025-01-16T01:02:35Z,"I introduced this constructor to remove some of the redundancy and conditionals I was seeing with test parameterization. Factoring in your next comment as well, maybe it makes more sense to remove this constructor and have a helper method do something similar instead in KafkaRaftClientPreVoteTest."
1917539718,18240,ahuang98,2025-01-16T01:25:06Z,I missed your last response with your suggestion about `withStartingVoter`. I'll leave this as is for now and we can discuss tomorrow
1917541053,18240,ahuang98,2025-01-16T01:27:15Z,https://issues.apache.org/jira/browse/KAFKA-18563
1918724919,18240,jsancio,2025-01-16T15:06:33Z,Can we make this an annotation that suppresses that check?
1918748224,18240,jsancio,2025-01-16T15:20:38Z,"Given this implementation it is also correct to just store it as `Endpoints leaderEndpoints` and changing the constructor to accept an `Endpoints` instead of an `Optional<Endpoints>`.

It looks like in the raft module we never use `Optional<Endpoints>` since `Endpoints.empty()` is a valid value."
1918750392,18240,jsancio,2025-01-16T15:22:06Z,This type doesn't write any log messages. We don't need to pass the log context to the object.
1918761915,18240,jsancio,2025-01-16T15:29:05Z,"I think we use this formatting in this case:
```java
                    ""Cannot transition to Candidate since the local id (%s) and directory id (%s) "" +
                     ""is not one of the voters %s"",
```"
1918763846,18240,jsancio,2025-01-16T15:30:18Z,"Okay but I would like us to standardize on using `String.format`.

I think his should be formatted as:
```java
            throw new IllegalStateException(
                ""Cannot transition to Candidate since the local broker.id="" + localId + "" is state "" + state
            );
```"
1918795087,18240,jsancio,2025-01-16T15:50:02Z,"Some code duplication can be removed with:
```java
        VoterState.State state = isGranted ? VoterState.State.GRANTED : VoterState.State.REJECTED;
        voterState.setState(state);
```
or
```java
        voterState.setState(
            isGranted ? VoterState.State.GRANTED : VoterState.State.REJECTED
        );
```
"
1918812401,18240,jsancio,2025-01-16T16:01:09Z,"Why would the replica send another request since it already sent a request [here](https://github.com/apache/kafka/pull/18240/files#diff-7e685d565d946f882cb6bd79d85bec3ca3fae64c137ab93684d939f1371196aeR120)?

Or is the issue that the replica change state from unattached (with voted) to follower [here](https://github.com/apache/kafka/pull/18240/files#diff-7e685d565d946f882cb6bd79d85bec3ca3fae64c137ab93684d939f1371196aeR121) and reset its connection and request manager?

Which means that it will send another FETCH request when it becomes a follower? If this is the case, maybe the test structure you had earlier is better where you assert a FETCH request is sent while in the unattached stated."
1918822107,18240,jsancio,2025-01-16T16:07:41Z,"Let's use this formatting:
```java
        List<RaftRequest.Outbound> voteRequests = collectPreVoteRequests(
            epoch,
            log.lastFetchedEpoch(),
            log.endOffset().offset()
        );
```"
1918823415,18240,jsancio,2025-01-16T16:08:31Z,"Missing newline character.
```java
            quorumStateStore.readElectionState().get()
        );
```"
1918831481,18240,jsancio,2025-01-16T16:13:56Z,"Let's use this formatting:
```java
            throw new IllegalStateException(
                ""Expected to be a NomineeState (Prospective or Candidate), "" +
                ""but current state is "" + state
            );
```"
1918838919,18240,jsancio,2025-01-16T16:19:08Z,"> since entire quorum may not support PreVote.

If just one of the voter doesn't support pre-vote this replica needs to transition to candidate. That because that voter that doesn't support pre-vote may be need to establish quorum with the majority.  I would change this working to:
```java
                ""transitioning to Candidate state immediately since at least one voter doesn't support PreVote."",
```"
1918852989,18240,jsancio,2025-01-16T16:28:12Z,"FYI, this shows the issue you highlighted in the metrics test. The known HWM is lost when transitioning to leader. This is odd from the client's (users of RaftClient) point of view.

This semantic turns out to be correct because the new HWM established by the leader is guarantee to be greater than the previous HWM. This is true because the leader first commits the current epoch before establishing the new HWM."
1918931718,18240,ahuang98,2025-01-16T17:24:19Z,It's needed for `unattachedOrProspectiveCanGrantVote` 
1918935598,18240,ahuang98,2025-01-16T17:27:18Z,I'll convert
1918980033,18240,ahuang98,2025-01-16T18:02:58Z,"english is hard  I meant ""not the entire quorum"" vs ""entire quorum does not""

"
1918980139,18240,ahuang98,2025-01-16T18:03:04Z,I'll add more details to the Jira - we can decide if it's worth changing this behavior
1919002962,18240,ahuang98,2025-01-16T18:22:42Z,"locally, checkstyle seems to take issue w/ this particular check. I'll give it a shot and see if it builds w/ CI"
1919024139,18240,ahuang98,2025-01-16T18:40:17Z,"CI doesn't like the change either :( 
```
Execution failed for task ':raft:checkstyleTest'.
> A failure occurred while executing org.gradle.api.plugins.quality.internal.CheckstyleAction
   > Checkstyle rule violations were found. See the report at: file:///home/runner/work/kafka/kafka/raft/build/reports/checkstyle/test.html
     Checkstyle files with violations: 1
     Checkstyle violations by severity: [error:1]
```
I recall spending some time trying to debug the issue, and the potential fix (SuppressionCommentFilter) seemed a bit more work than it was worth

JavaNCSS is mentioned in the existing Jira for addressing these Raft suppressions though - https://issues.apache.org/jira/browse/KAFKA-18332
"
1919031445,18240,jsancio,2025-01-16T18:45:36Z,Got it. Thanks.
92508146,2244,mjsax,2016-12-14T22:49:16Z,nit: either `Kafka Streams` or `{@link KafkaStreams}` (more ofter farther down)
92508580,2244,mjsax,2016-12-14T22:51:46Z,"nit: `{@link org.apache.kafka.streams.state.ReadOnlyKeyValueStore ReadOnlyKeyValueStore}`
applies to all links with package prefix"
92508710,2244,mjsax,2016-12-14T22:52:35Z,`@see KTable`
92509183,2244,mjsax,2016-12-14T22:55:15Z,"just a view, i.e., it is not materialized in a state store, on top"
92509708,2244,mjsax,2016-12-14T22:57:47Z,"JavaDoc missing
`replicatedTable` -> `globalKTable`"
92509786,2244,mjsax,2016-12-14T22:58:12Z,`table` -> `globalKTable`
92510046,2244,mjsax,2016-12-14T22:59:34Z,"An exception? Seems to align with `KTable` so not part of this PR -- but should we change this and just skip/drop those records? If we apply an aggregation to compute a KTable, we do the same, ie, just dropping those records. So there is an gap between aggregation and join -- even if for aggregation the input is a KStream... Nevertheless, we should think about this (\cc @enothereska )"
92514204,2244,mjsax,2016-12-14T23:27:03Z,nit. indention.
92514591,2244,mjsax,2016-12-14T23:30:05Z,nit `KStreamGlobalKTableJoin`
92514897,2244,mjsax,2016-12-14T23:32:20Z,"Can we unify this with inner class of `KStreamKTableJoin` (ie, extract both inner classes and make top level class)?
If not, maybe rename to `KStreamGlobalKTableJoinProcessor`."
92515229,2244,mjsax,2016-12-14T23:34:50Z,nit: rename `TheJoinProcessor ` ->`KTableGlobalKTableJoinProcessor`
92515479,2244,mjsax,2016-12-14T23:36:32Z,nit: rename `TheValueGetterSupplier` -> `KTableGlobalKTableJoinValueGetterSupplier`
92516392,2244,mjsax,2016-12-14T23:44:07Z,I just compared with `KTableKTableJoinProcessor` and the logic there is less nested thus easier to read. Maybe we can break this down into smaller pieces similar to `KTableKTableJoinProcessor`
92517207,2244,mjsax,2016-12-14T23:50:10Z,"If we can reuse `KTableKTableLeftJoinProcessor` why not the inner join processor, too?"
92517353,2244,mjsax,2016-12-14T23:51:16Z,Why renaming?
92518822,2244,mjsax,2016-12-15T00:02:57Z,comment does not apply anymore -- nodeGroup will never be null now (maybe empty though)
92573799,2244,dguy,2016-12-15T09:41:02Z,"Yeah, i think that is a discussion worth having."
92574547,2244,dguy,2016-12-15T09:45:15Z,"Thanks, i'll remember that going forward. You are my JavaDoc hero ;-)"
92577752,2244,dguy,2016-12-15T10:03:30Z,looks like a mistake. 
92578027,2244,dguy,2016-12-15T10:05:14Z,should be `topicGroupId` is null
92583292,2244,dguy,2016-12-15T10:36:26Z,Yep - thought i did that already.
92712542,2244,mjsax,2016-12-15T22:18:49Z,nit: root -> stateStore
92712675,2244,mjsax,2016-12-15T22:19:21Z,nit: name -> viewName
92713058,2244,mjsax,2016-12-15T22:21:49Z,"Should we check for `key == null` -- or is this checked before the call already?
We should start using assertions... would avoid those questions. (\cc @guozhangwang @enothereska)"
92714906,2244,mjsax,2016-12-15T22:32:40Z,"state stores are added via an supplier -- a suppliers is not required here, but it might be confusing for users if there are different method signature -- I think we should align both. WDYT?"
92716639,2244,mjsax,2016-12-15T22:42:58Z,"I am just wondering what abstraction we want to provide at PAPI level -- we never discussed this in detail -- and it's not part of the KIP either. We only talked about GlobalKTable.

For GlobalKTable we have the requirement that it is always populated from a source topic. Thus, this method mimics this -- but it this a PAPI concept? Should we have a method like this? And if it is a PAPI concept, should global stores always be used like this? If yes, we might want to drop `addGlobalStore(final StateStore store)`.

Not sure about the answers. We should discuss this."
92719445,2244,mjsax,2016-12-15T23:00:21Z,"I am still not sure about this design. Actually, the global part shout use a singleton pattern. It is kinda weird that we have two `ProcessorTopology` ""types"" -- the global one and the regular one.

As the KIP is still under discussion, we might want to think about this once more. Also from an PAPI vs DSL point of view -- I think, we need a better separation between both and not ""pollute"" PAPI with DSL concepts."
92719996,2244,mjsax,2016-12-15T23:03:55Z,Should we implement singleton pattern here?
92720200,2244,mjsax,2016-12-15T23:05:21Z,Should we release the lock here?
92720829,2244,mjsax,2016-12-15T23:09:57Z,Why not `.endOffsets()`? Could replace the whole method.
92723075,2244,mjsax,2016-12-15T23:26:58Z,Not strictly required -- only suppresses the log message. Is this intended?
92723476,2244,mjsax,2016-12-15T23:30:19Z,Are checkpoint files delete somewhere else? Or should be do this here?
92726236,2244,mjsax,2016-12-15T23:52:21Z,"As the restore consumer is shared, can we make sure no parallel restore operation messes with global state thread here? Applies to the whole class...

Also -- if we change partitions assignment -- would we need to restore a previous assignment or maybe better just extend the current assignment (instead or replacing it)? Or does every ""user"" of the consumer restores its own assignment each time it uses the consumer (this might also imply that doing the assignment in `initialize` is wrong)?


Btw: how does state recovery work today? single threaded?"
92773817,2244,dguy,2016-12-16T08:59:13Z,hmm - i'm don't think viewName really describes it any better. The class is already named `KeyValueStoreJoinView` - name seems appropriate
92773828,2244,dguy,2016-12-16T08:59:20Z,As above
92773952,2244,dguy,2016-12-16T09:00:20Z,This is what it is on the interface and is consistent with every other `StateStore`
92774161,2244,dguy,2016-12-16T09:02:15Z,It is worth adding a null check here. Though i will probably just return null rather than throwing exceptions etc. I'm not convinced that calling `get(null)` is worthy of raising an exception.
92774515,2244,dguy,2016-12-16T09:05:28Z,I don't think we should use a suppler here. We want a single instance of a `StateStore` and this shows that intent. A supplier indicates that there may be multiple instances.
92774899,2244,dguy,2016-12-16T09:08:42Z,"This is in the KIP.
Anyway, this method has been added so that the joins generated by `GlobalKTables` can be queried etc. As they are just views on top of other `GlobalKTables` they don't have their own source as such.
Also, from a users point of view - why shouldn't they be able to add a global store of whatever type they like? They might have a pre-populated table or something that they'd like available in all of there Processors. "
92775816,2244,dguy,2016-12-16T09:15:00Z,"I'm not 100% happy with this either, but as you know, the DSL just builds on the PAPI and the concepts are already mixed. I don't like this, but this is where we are at the moment. I'd much prefer that `KStreamBuilder` didn't extend `TopologyBuilder` - IMO there should be another class, lets say `PAPIBuilder` (name sucks), `TopologyBuilder` becomes package private. `KStreamBuilder` and `PAPIBuilder` are standalone classes, they don't inherit from `TopologyBuilder` they just use it to build the topology. Then we have no mixing of DSL and PAPI concepts at the API layer. Anyway, i don't want to do that as part of this task!
"
92776262,2244,dguy,2016-12-16T09:18:21Z,No. Not a fan of Singletons at all. We just create a single instance of it.
92777615,2244,dguy,2016-12-16T09:28:09Z,didn't know it existed! 
92778385,2244,dguy,2016-12-16T09:33:36Z,It probably should be deleted after it is first loaded. At least that is what we do elsewhere. 
92780091,2244,dguy,2016-12-16T09:44:14Z,"This consumer is not the restore consumer. It is a consumer just for this thread so it isn't shared at all. There are no parallel operations on it.
When the `StateStores` are restored, in `GlobalStateManagerImpl`, they each `assign` their own partitions, fetch the data up to the HW, then un-assign their partitions. During this process all of the `TopicPartition` for global stores are collected and that is what is assigned here. This is the set of partitions we need to consume to keep all global stores up-to-date.

Yes, recovery of StateStores is always single threaded."
92882610,2244,mjsax,2016-12-16T20:42:27Z,Agreed: this should definitely not be done in this PR! I am just afraid that this PR makes reworking and separating PAPI and DSL even harder.
92883151,2244,mjsax,2016-12-16T20:46:28Z,Hmmm... weird naming. Should we rename all?
92883382,2244,mjsax,2016-12-16T20:48:16Z,Sounds opinion based :) What's the problem with singletons?
92888694,2244,mjsax,2016-12-16T21:24:39Z,"I understand that argument. My point is more about API design -- we break an API pattern and thus reveal something to the user, that you can consider an implementation detail. The user does not care how ofter we do instantiate a store. If I write code for a store, and hand it in, I don't care about it -- I want to same API for regular and global stores. Why should I care about the number of instantiation as a user?

I personally dislike the whole handing in factories instead of stores from a user perspective completely -- even if I understand why it is necessary for Streams. (Flink for example has a very nice API for this and it's not a concern there.) As a user, I want to implement a store -- I don't want to bother with a store factory (I don't want to change this pattern because we have good reasons to enforce is, and one more wrapper for the store is an acceptable burden for the user IMHO). So we educate the user to implement factories and suddenly we change our mind and say -- ""not for global store"".

"
92889320,2244,mjsax,2016-12-16T21:29:12Z,"Understood. But than, we should maybe only keep `addGlobalStore(final StateStore store)` and remove the second (this) `addGlobalStore` method -- it's a DSL concept again -- I understand the ""I don't care, PAPI legacy"" argument, but everything we introduce in hard to remove later on.

We could just add a global store, and do the wiring with a topic in KStreamBuilder instead of TopologyBuilder?"
92890189,2244,dguy,2016-12-16T21:35:22Z,Not in this PR
93021658,2244,dguy,2016-12-19T12:24:23Z,"Everything in software development is opinion based! Singletons are ok for simple objects with no dependencies. 
As soon as you start adding dependencies in to the mix you end up with tightly coupled code. that is hard to test. "
93022514,2244,dguy,2016-12-19T12:31:19Z,"We could do that if we make some of the fields in `TopologyBuilder` protected, i.e., so they can be accessed from `KStreamBuilder`. I'm not a big fan of doing this, but i agree with your point about making things harder to remove later"
94648336,2244,mjsax,2017-01-04T19:28:48Z,"I had a discussion about when to set the state to RUNNING with @enothereska and we agreed that it is better to change the state before we start the threads -- can't remember the details of our discussion though. However, I would keep it as is. Maybe @enothereska can elaborate on it."
94648979,2244,mjsax,2017-01-04T19:32:16Z,"Joins with other GlobalKTable got removed, right?"
94649387,2244,mjsax,2017-01-04T19:34:30Z,"Nit: Better markup would be
```
<pre>{@code
    PUT CODE HERE
}</pre>
```"
94650151,2244,mjsax,2017-01-04T19:38:23Z,"weather [or] not

Seem like c&p error -- would you mind fixing the other typos in all JavaDocs, too?"
94650667,2244,mjsax,2017-01-04T19:41:14Z,"Remove this paragraph -- we did removed it for other JavaDocs, too."
94650786,2244,mjsax,2017-01-04T19:41:51Z,As above
94651214,2244,mjsax,2017-01-04T19:44:01Z,"I would prefer to use `GK` (like global key), `GV`, and `RV` (result value) instead of `K1`, `V1`, and `R` to have somewhat more meaningful names instead of numbering. (`RV` is used in other methods, too)"
94651248,2244,mjsax,2017-01-04T19:44:13Z,As above.
94675135,2244,mjsax,2017-01-04T22:05:41Z,"Should we not do this in `@Before`? If test fails and `@After` is never executed, next test run might fail."
94676111,2244,mjsax,2017-01-04T22:11:28Z,"I think that join result should not depend of other values being null or not. Line 102/103 might hit a race condition if global table get altered between both calls. Thus we might end up with wrong results IMHO.

Not 100% sure, but I think it's worth thinking about it."
94676192,2244,mjsax,2017-01-04T22:11:51Z,As above.
94679715,2244,mjsax,2017-01-04T22:33:41Z,"This test is only sufficient for Stream-GlobalTable join IMHO, but not for Table-GlobalKTable join for which we need to test `null` tombstone input record for KTable input.

Furthermore, I think we need a test that updates GlobalKTable in the background while processing -- this might be a separate test though (it's about the race condition I mentioned that we should test for)."
94686399,2244,mjsax,2017-01-04T23:18:45Z,Does this test anything that is not already covered by `GlobalKTableIntegrationTest` -- or the other way round?
94688157,2244,mjsax,2017-01-04T23:32:34Z,"The test behavior is ok, but I think the name is wrong.
-> `shouldNotForwardIfOldValueIsNull`
We should also have one more test, that test if `null` is emitted if `oldValue != null`"
94688915,2244,mjsax,2017-01-04T23:38:53Z,I think we should expect `a.newValue == null` because input `oldValue != null` -- we cannot know if GlobalTable was updated in between and thus previous oldValue might have joined.
94689054,2244,mjsax,2017-01-04T23:40:09Z,"IMHO, this test is redundant with my suggested version of `shouldNotForwardIfDeleteAndOldKeyNotInOtherStoreAndSendOldValues`"
94689208,2244,mjsax,2017-01-04T23:41:40Z,"From my understanding, if old and new value is `null` nothing should be forwarded -- independent of the content of GlobalKTable."
94689292,2244,mjsax,2017-01-04T23:42:27Z,I guess similar comments as above apply -- skipping this class for now.
94689865,2244,mjsax,2017-01-04T23:47:10Z,We should use `LockException` instead of `StreamsException` IMHO.
94690456,2244,mjsax,2017-01-04T23:52:15Z,Why not just one test for `shouldInitializeStateStores` and `shouldReturnInitializedStoreNames` ? Both test the same method.
94690596,2244,mjsax,2017-01-04T23:53:24Z,"Remove try-catch and fail and add `@Test(expected = IllegalArgumentException.class)`
(same below)

Or could `stateManager.initialize(context);` throw `IllegalArgumentException`, too?"
94691732,2244,mjsax,2017-01-05T00:03:39Z,:)
94692341,2244,mjsax,2017-01-05T00:09:17Z,"Nit: ""KABOOM!"" ;)"
94692983,2244,mjsax,2017-01-05T00:15:15Z,Should we apply a test timeout to check if `join()` got stuck because it did not stop running on `close()` ?
94693136,2244,mjsax,2017-01-05T00:16:40Z,Unify `shouldStopRunningWhenClosedByUser` and `shouldCloseStateStoresOnClose` ? Both do test `close()`
94695940,2244,mjsax,2017-01-05T00:43:44Z,Isn't this test covering `shouldUpdateStateWithReceivedRecordsForPartition` ?
94696136,2244,mjsax,2017-01-05T00:45:47Z,Unify `shouldFlushStoreWhenFlushIntervalHasLapsed` and `shouldNotFlushOffsetsWhenFlushIntervalHasNotLapsed` ?
94727438,2244,dguy,2017-01-05T08:17:54Z,sure - i'll put it back. Didn't really make sense to me to set the state to running before the threads have started. but whatever
94728105,2244,dguy,2017-01-05T08:24:48Z,Next test wont fail as it will use a different state directory.
94729425,2244,dguy,2017-01-05T08:37:42Z,"We should have both tests. They cover some of the same things, but this is much easier to write and debug issues. It runs much quicker. However, it doesn't cover the more end-to-end scenario that the integration tests cover."
94730687,2244,dguy,2017-01-05T08:49:00Z,"I don't agree with the name you have suggested, but i also think the name of the test is not completely correct"
94734155,2244,dguy,2017-01-05T09:15:44Z,"Yep. I was thinking that a direct key mapping could result in the join, but that was incorrect."
94735025,2244,dguy,2017-01-05T09:22:02Z,"Sure, but `LockException` didn't exist when i wrote this!"
94735566,2244,dguy,2017-01-05T09:25:54Z,"Yes, you could test them both in the same method. However, i prefer to have single focused tests where the test names describe what is happening. There is nothing wrong with having multiple tests for the same method and params, in-fact i'd encourage it."
94735880,2244,dguy,2017-01-05T09:28:02Z,"It doesn't now, but it, or something, it uses might in the future. I prefer this approach for these sort of scenarios as it guarantees that the exception was raised from the code i am trying to test"
94736396,2244,dguy,2017-01-05T09:31:37Z,Nope - they are testing different things that happen on close. I prefer it this way as it is easier to just read the test names to see what should be happening rather than have to read through the assertions.
94736604,2244,dguy,2017-01-05T09:33:14Z,Not quite - this is checking the multiple topic case. The other is just checking a single topic
94736696,2244,dguy,2017-01-05T09:33:57Z,See my other comments. This is how i'd prefer to see the tests written
94739678,2244,dguy,2017-01-05T09:53:44Z,"The `null` tombstone cases are already covered by `KTableGlobalKTableJoinTest` and `KTableKTableLeftJoinTest` - i don't think they need to be covered here again.
I'm not sure about the background thread. Yes there could be a race condition (i've updated the code as suggested below), but i'm not 100% sure how we could/should handle it."
94823111,2244,mjsax,2017-01-05T18:27:56Z,"If you apply this argument, you can never use `@Test(expected = ...)` as it would apply to all tests using this pattern. Wouldn't it?"
94823260,2244,mjsax,2017-01-05T18:28:47Z,"Yes. But if multi-topic works, single topic works, too. Or not?"
94829084,2244,mjsax,2017-01-05T19:00:26Z,I just thought about this once more. To me it seems that the race condition is because of sending oldValues. Why do we actually need this? Or could we just disable sending old values?
94832919,2244,mjsax,2017-01-05T19:22:06Z,"What is the different from this test to `shouldNotSendAnythingIfChangeIsNullNullAndKeyMapsToNullInOtherTable`?

Furhtermore, why do you setup a new `KTableGlobalKTableLeftJoin` ? If I did not miss anything, it is the same setup as the global member `join` variable."
94919202,2244,dguy,2017-01-06T09:23:13Z,Yeah they do look the same. brain fade!
94919538,2244,dguy,2017-01-06T09:26:11Z,"It is a question of how you write tests. I write tests and code starting from the simplest things and working out from that. So the test for the single topic comes first, and then the test for multiple topics. In this case the test for multiple topics might be enough, but maybe it isn't, maybe the code assume there is always >1 topic? It is good to have both"
94920231,2244,dguy,2017-01-06T09:32:06Z,"No, i disagree. Most tests i'd use `@Test(expected = ....)` in would either have a single method call in them, so you know that is the only method that can throw the exception. Or, it would be a `new Blah(..)` followed by testing the single method.  Of course the `new ..` could throw an exception, but hopefully most ctors are side-effect free."
94930979,2244,enothereska,2017-01-06T11:01:39Z,"""weather"" typo"
94931159,2244,enothereska,2017-01-06T11:03:28Z,Would be good to be consistent at least with the KTable.
94931291,2244,enothereska,2017-01-06T11:04:46Z,Currently we always materialize though.
94939282,2244,dguy,2017-01-06T12:29:21Z,"Hmmm - maybe we don't need to get the old value from the other table at all. As in `KTableKTableLeftJoin` we get the current value from the other table and then if `sendOldValues` we join `change.oldValue` with the value from the other table.
Thoughts?"
94950774,2244,enothereska,2017-01-06T14:16:25Z,I agree with both of you.
94980385,2244,mjsax,2017-01-06T17:04:46Z,"I would not send old value whatsoever -- because of async updates of GlobalKTable we cannot guarantee any semantical meaning -- we might miss multiple update from GlobalKTable and thus, me might send an ""old value"" that was never emitted as ""new value"" before."
94981984,2244,dguy,2017-01-06T17:14:29Z,"I've been looking a bit more and I'm still not certain what the correct thing to do in this situation is. We need to send an old value for the subtractors of the  aggregators/reducers, but the old value may not be the same value as previously seen. An example that doesn't work properly.
```
final GlobalKTable<String, String> g1 = builder.globalTable(Serdes.String(), Serdes.String(), ""g1"", ""g1"");
        final KTable<Integer, String> t1 = builder.table(Serdes.Integer(), Serdes.String(), ""t1"", ""t1"");
        t1.leftJoin(g1, (key, value) -> value, (v1, v2) -> v2)
           .groupBy((key, value) -> KeyValue.pair(value, value, Serdes.String(), Serdes.String()).count(""count"");
``` 
if i initialize g1 with:
(1, green)
(2, blue)
(3, yellow)
(4 red)
Then send to t1
(1, 1)
(2, 1)
(3, 1)
and flush state i get 
(green, 3)
all good so far.
However if i then send to g1
(1, orange)
and then to t1
(1, 4)
and flush state i get
(green, 3)
(orange, -1)
(red, 1)
which is obviously incorrect. The oldvalue of green never gets sent so the count for green doesn't reduce by 1, rather the oldvalue is orange, hence orange with a count of -1. HMMMM!"
95020150,2244,mjsax,2017-01-06T21:06:45Z,"That is exactly what I had in mind with my previous comment... I just thought we might be able to not send the old value at all -- but your example shows that we need to send it.

One way to fix it, is to remember the old value on the triggering KTable side (I guess we need another store for this...). Thus, instead of looking up old value in GlobalKTable, we look it up the the new store. Not sure if there is a better way to do it."
95133661,2244,dguy,2017-01-09T10:49:03Z,@guozhangwang @enothereska would appreciate your thoughts on this.
95287565,2244,guozhangwang,2017-01-10T01:57:06Z,Not sure if this interface is usefeul with `ProcessorStateManager`? Should it be in the extended `GlobalStateManager` only?
95287972,2244,guozhangwang,2017-01-10T02:01:50Z,Why we want to maintain an interface of `GlobalStateMaintainer`? Is it because of mocking in unit tests? Otherwise its only impl is `GlobalStateUpdateTask`.
95288306,2244,guozhangwang,2017-01-10T02:05:03Z,"My gut feeling is that we do not need to mimic a `ProcessorTopology` and `InternalProcessorContext` interface for this task, as it is a very special task whose topology will just be a list of source topics and a list of state stores, making them as generic interfaces would just introduce one-time classes like `GlobalProcessorContext` in which lots of its functions will not be required at all. Instead we can just e.g. pass into it a list of source topics, a map between topics to state stores, etc and let it run its only loop for fetching + updating stores, etc."
95288621,2244,guozhangwang,2017-01-10T02:09:14Z,"The usage of `SourceNodeAndSerializer` is a bit awkward to me here: we have wrapped the source node in order to get its corresponding deserializer in this class, and we again keep a map from topic name to this deserializer; so why don't we just use a map from topics to deserializer directly?"
95319354,2244,dguy,2017-01-10T08:42:06Z,It is used by `ProcessorContextImpl` it is needed on the interface
95319810,2244,dguy,2017-01-10T08:44:51Z,"Using interfaces is good OO design practice. It facilitates loose coupling, flexibility, better design of roles. Yes, it also helps with testing."
95324378,2244,dguy,2017-01-10T09:16:20Z,"Because we need both the sourceNode and the deserializer when we receive data from the topic. i.e., we need to deserialize the data and the `SourceNode` to process it"
95327582,2244,dguy,2017-01-10T09:35:35Z,"I disagree. Firstly it isn't mimicing a `ProcessorToplogy` - it is one! We have various classes in place already that do the the work needed to keep the table up-to-date, i.e, `SouceNode` and `KTableSourceProcessor`. They need a `ProcessContext`. If we don't go down this path then we will be duplicating the code in those classes as we need to do the same thing. 
Further `GlobalProcessorContext` is as one-time as `StandbyContextImpl`. I don't see any difference here. I also abstracted out all of the common `ProcessorContext` methods to `AbstractProcessorContext` to avoid having to avoid duplication and implementing of some unnecessary methods."
95420343,2244,guozhangwang,2017-01-10T17:56:25Z,"This is a meta comment about `Change<>`, not sure if my thoughts are correct:

Currently we propagate the need to send old values in any aggregate operators back-wards all the way to source nodes: if a KTable aggregate operator is observed it will be propagated to all its ancestors; with materialized results of KTable-KTable join, this propagation can be stopped at such operations.

In this case, KTableKTableXXJoin do not need to expect a `Change<>` at all as it will never need the old value any more. I'm not sure if we can immediately change its type from `Change<V>` to `V`, but I think technically it should be the case."
95420796,2244,guozhangwang,2017-01-10T17:58:32Z,nit: rename to `GlobalContextImpl` to be consistent?
95422432,2244,guozhangwang,2017-01-10T18:07:24Z,"It is a subjective thing, but I usually find such OO design most useful when the interface is public and the impl has some separate functions to be used in other internal classes (e.g. the refactoring @mjsax are doing to separate TopologyBuilder user-facing APIs from its internal functions used by `StreamThread`, etc). While this class is pure internal we can always just extend / override for testing."
95424323,2244,guozhangwang,2017-01-10T18:16:58Z,"Okay, make sense."
95425307,2244,guozhangwang,2017-01-10T18:22:24Z,"Instead of adding this interface, could we let `ProcessorContextImpl` to contain two `stateManagers`, while `standbyContextImpl` and `GlobalContextImpl` each contain one?

I know it is an internal interface so maybe it's not that important at all, but just wanted to throw my ideas here."
95425732,2244,guozhangwang,2017-01-10T18:24:39Z,This is not about this PR: in #1446 we are removing specific cache sensors since we are adding sensors for generic purposes already. There will be some major conflicts between these two.
95428000,2244,enothereska,2017-01-10T18:36:30Z,"There will be a conflict, but hopefully not major."
95428041,2244,dguy,2017-01-10T18:36:39Z,Yep :-(
95428151,2244,dguy,2017-01-10T18:37:14Z,sure
95429909,2244,dguy,2017-01-10T18:45:57Z,I'm not sure i follow. Not immediately clear to me how `KTableKTableXXJoin` can't expect a `Change<>`. That is what is will be sent from the previous `KTableXXXProcessor`
95455229,2244,dguy,2017-01-10T20:57:11Z,If we did that i think we'd need another `StateManager` implementation. As we wouldn't want the one used by `ProcessorContextImpl` to go through the initialization and restoration of global stores.
95455897,2244,dguy,2017-01-10T21:01:01Z,We'll have to agree to disagree on this one! 
95475213,2244,mjsax,2017-01-10T22:44:29Z,Created https://issues.apache.org/jira/browse/KAFKA-4613 for this.
85607023,2074,vahidhashemian,2016-10-28T20:44:23Z,"This is where the bug was introduced. If `state` is `None`, there is a possibility that the old consumer based group does not have any active members; so we need to check whether new consumer is used or not, and then proceed accordingly.
"
85607062,2074,hachikuji,2016-10-28T20:44:33Z,"Was there a KIP for this that I missed?
"
85607674,2074,vahidhashemian,2016-10-28T20:48:28Z,"Thanks for bringing this up. I wasn't totally sure what to process is for changing the protocol, and whether I'm actually correct to assume that the protocol has to be changed for this JIRA. I haven't opened a KIP yet, if you think that's eventually going to be needed for I'd be happy to create one. Thanks in advance for clarifying this.
"
85608617,2074,hachikuji,2016-10-28T20:54:01Z,"Yeah, protocol changes definitely need a KIP. Probably makes sense then to split the bug fix into a separate patch.
"
85608849,2074,vahidhashemian,2016-10-28T20:55:44Z,"Thanks. I'll submit the bug fix separately, and then work on creating a KIP. 
"
86647580,2074,vahidhashemian,2016-11-04T23:11:13Z,"@hachikuji I was hoping you could take a look at an issue I'm running into if and when you get a chance.
While KIP-88 is open for discussion I spent some time creating some unit tests for this PR.

This particular unit test simply mocks two consumers that consume from the same 1-partition topic and belong to the same consumer group. A similar unit test on old consumers exists earlier in the same file and runs fine. There are also other unit tests above using the new consumer that run fine but they mock only one consumer.

The problem I'm running into is this line that mocks the second consumer and takes a long time to run (I believe for the consumer to join the group that eventually times out) and the consumer group gets corrupted somehow. When I debug and check the status of the group down in the `waitUntilTrue` check, sometimes it is `Empty`, or `Dead`, or even `Stable` with only one of the consumers and it never gets into the expected state (`Stable` with two members). Where it gets stuck in a loop I think is [here](https://github.com/vahidhashemian/kafka/blob/5b1c5fc78477dc1c5cf9e48429d93fde15c0f4e6/clients/src/main/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkClient.java#L180) (after [trying to join the group](https://github.com/vahidhashemian/kafka/blob/5b1c5fc78477dc1c5cf9e48429d93fde15c0f4e6/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AbstractCoordinator.java#L307)).

I'm not sure if I'm doing something wrong with the unit test or if I'm hitting some bug. I thought you might know by just looking at it. Thanks.
"
92458550,2074,hachikuji,2016-12-14T18:34:17Z,nit: The `OffsetFetchRequest` suffix seems redundant. How about this `OffsetFetchRequest.forAllPartitions()`?
92458890,2074,hachikuji,2016-12-14T18:36:11Z,"Don't we need a null check in the loop above? Also, should we return a null array in the response if the requested partitions are null?"
92459503,2074,hachikuji,2016-12-14T18:39:37Z,One thing I'm realizing is that this schema gives us no way to indicate that a group doesn't exist when you fetch all partitions. That may be OK since we usually know ahead of time whether or not a group _should_ exist (e.g. by using ListGroups).
92460753,2074,hachikuji,2016-12-14T18:46:06Z,Seems like we could do these two lines with a `map` and `getOrElse` combo.
92461437,2074,hachikuji,2016-12-14T18:49:42Z,This is a little hard to follow. Maybe we could create two vals and do the append at the end?
92461907,2074,hachikuji,2016-12-14T18:52:02Z,Can we cover the error case also for a request for all partitions?
92462186,2074,hachikuji,2016-12-14T18:53:15Z,"Maybe we can just say ""versions 1 and above""?"
92462799,2074,hachikuji,2016-12-14T18:56:21Z,Is the version check necessary?
92463197,2074,hachikuji,2016-12-14T18:58:13Z, Maybe we could wrap the `Some` at the end to remove one level of nesting?
92463905,2074,hachikuji,2016-12-14T19:02:05Z,Hmm... I don't think we should be accessing the group directly at this layer. It's probably better to either overload `GroupCoordinator.handleFetchOffsets` or expose a new method.
92468547,2074,vahidhashemian,2016-12-14T19:24:18Z,"Sure, that sounds reasonable. I'll update the method name."
92477069,2074,hachikuji,2016-12-14T20:01:48Z,"Thinking about this a little more.. There is an edge case around coordinator failover. We may lookup the coordinator for some group, find that it is broker A, and then send the OffsetFetch for all partitions. Before the request arrives, it could happen that broker B becomes the coordinator (it may have already begun the transition even before we did the initial coordinator lookup), but we won't have a way to detect it. This will cause us to mistakenly report that there are no offsets for the group."
92478985,2074,vahidhashemian,2016-12-14T20:11:55Z,"Yes, I missed that. Thanks. I'll try to fix it in the next update."
92495078,2074,vahidhashemian,2016-12-14T21:39:40Z,"Could you please clarify your first comment above? With this change, I can still see the `Error: Consumer group '...' does not exist.` message if 1) the consumer group is never created. 2) the consumer group is created but its offsets are all expired.

Regarding the second comment, are you referring to double `findCoordinator(...)` calls in this use case (through [here](https://github.com/vahidhashemian/kafka/blob/498e8dff14fa1392dd70fcf0867496a2def993d2/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L399) and [here](https://github.com/vahidhashemian/kafka/blob/498e8dff14fa1392dd70fcf0867496a2def993d2/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L406))? If so, one improvement would be to somehow preserve the `coordinator` value found in `describeConsumerGroup(...)` for use in `listGroupOffsets(...)`. Please advise if I misunderstood the issue. Thanks."
92501254,2074,hachikuji,2016-12-14T22:10:14Z,"The basic issue is that the error codes are in the partition data of the response schema. If we have no partitions to return, then we cannot return any errors either. This is fine for most cases because we should already know if the group exists or not. However, there are (at least) two problematic edge cases:

1. The case I mentioned above. To use the OffsetFetch API, we must first lookup the coordinator for the group. It could happen that when we do so, we happen to get a stale coordinator. This is possible because it takes some time for metadata changes to propagate to all the brokers. 
2. When the coordinator first is started, it must read through the `__consumer_offsets` topic to populate the offset cache. Usually we return a `COORDINATOR_NOT_AVAILABLE` error in this situation which lets the consumer know it needs to retry a bit later. 

So if we happen to send an offset fetch for all partitions in either of these cases, then we could mistakenly believe that there are no offsets for the group."
92674520,2074,vahidhashemian,2016-12-15T18:52:08Z,"Thanks for explaining the issue. If I'm not mistaken, the first edge case (stale coordinator) could occur with the current code too ([this line](https://github.com/apache/kafka/blob/94909a8f83bfe214726f85130ad04d867e022894/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L394) that leads to [making a call to `findCoordinator(...)`](https://github.com/apache/kafka/blob/128d0ff91d84a3a1f5a5237133f9ec01caf18d66/core/src/main/scala/kafka/admin/AdminClient.scala#L134)).

So the main issue is that with the current protocol we cannot report error codes when there is no partition in the response. To me the options are

1. Making further changes to the protocol to address this issue too.
2. Rethinking how to solve the problem of KAFKA-3853 (with another option than proposed in KIP-88).
3. Accepting that limitation for now and continue with the current solution.

Is there another option? Which option do you recommend we take?"
92679066,2074,vahidhashemian,2016-12-15T19:14:10Z,Sounds good. I'll refactor the whole block a little bit.
92679685,2074,hachikuji,2016-12-15T19:17:13Z,"Adding an `error_code` at the top level in the response object seems like the cleanest solution. This error code could be used to communicate group-related errors, which is arguably a bit nicer than writing those errors into all the individual partition data.  It's a bit painful to reopen a KIP that has passed, but I don't think we can ignore this problem. Perhaps send a comment to the KIP-88 discussion thread explaining the problem and what you think the best option to fix it is?"
92680619,2074,vahidhashemian,2016-12-15T19:21:36Z,Sounds good. I'll do that. Thanks for your feedback and advice.
92697405,2074,vahidhashemian,2016-12-15T20:51:52Z,@hachikuji One more question before I re-open the KIP. The issue doesn't seem to be a side-effect of the suggested protocol change in KIP-88 (we are not modifying the response in KIP-88) and it would surface whenever there is no partition in the response. Do you think it can be addressed in a separate KIP? Or I am missing something here?
92702988,2074,hachikuji,2016-12-15T21:25:16Z,"I think it is a consequence of the changes to the request from this KIP though, right? Before we would always have at least one partition in the request, so we always had somewhere to pack an error code in the response. Now that's no longer true."
92706549,2074,vahidhashemian,2016-12-15T21:43:42Z,"OK. I see. Thanks. So before, it was guaranteed that whatever partition is in the request will be in the response. With the new protocol we could end up getting back an empty list if the group has no offset data. What about passing an empty array in the request with current protocol? Wouldn't this cause the same problem?"
92707749,2074,hachikuji,2016-12-15T21:50:10Z,"That is true, but the impact is different. If you request offsets for an empty list of partitions, the correct response, regardless of the state of the group or the coordinator, is to return an empty partition list."
92710994,2074,vahidhashemian,2016-12-15T22:08:45Z,"> ..., regardless of the state of the group or the coordinator, ... 

This makes it clear ( hopefully :) ). Thanks a lot. I'll work on that email and updating the KIP."
92729320,2074,vahidhashemian,2016-12-16T00:21:06Z,"@hachikuji A quick follow-up question: Is it possible for the current API to return an offset fetch response with various error codes associated with the partitions? I'm trying to think if we can remove the internal ""error_code"" from the schema. Thanks."
92730823,2074,hachikuji,2016-12-16T00:35:52Z,"I was wondering about this also. Unfortunately, it seems we can't remove the per-partition error code since we do authorization on the topics separately. "
92861504,2074,vahidhashemian,2016-12-16T18:30:06Z,"I guess not, since in previous versions it's not possible to pass in a null array. Thanks."
92876208,2074,vahidhashemian,2016-12-16T19:58:21Z,"Sure. I think I'll expose a new method (something like `GroupCoordinator.getPartitions(groupId)`), since the partitions have to go through authorization check and, after a quick look, I don't see a clean way of overloading `handleFetchOffsets` for this purpose that fits well with how `KafkaApis.handleOffsetFetchRequest` is implemented. Unless we want to refactor that method more extensively."
94959538,2074,ijuma,2017-01-06T15:14:34Z,This should be `KAFKA_0_10_2_IV0`.
94994319,2074,vahidhashemian,2017-01-06T18:30:04Z,Thanks @ijuma for catching this. I'll fix it shortly.
95102027,2074,ewencp,2017-01-09T04:36:55Z,"Is this still supposed to be included? It's not in the KIP (I can't remember if it existed in a previous version.) If it is supposed to be included, presumably it'd be an override of an interface method from `Consumer`?"
95103021,2074,ewencp,2017-01-09T05:08:34Z,style nit: normally we'd use braces around blocks unless they're a single line
95103761,2074,ewencp,2017-01-09T05:30:01Z,"Aren't you still missing setting the error code field on the struct in this case though?

The pattern that seems to be used elsewhere, e.g. in `MetadataResponse`, is to make the constructor that takes the version contain all the fields as arguments as well as the version. Then all the decoded fields are kept as member variables and written regardless of whether that version contains them, but only written to the struct conditionally. For example, `MetadataResponse` has some code that looks like this in its constructor:

```
        this.clusterId = clusterId;

        // This field only exists in v2+
        if (struct.hasField(CLUSTER_ID_KEY_NAME))
            struct.set(CLUSTER_ID_KEY_NAME, clusterId);
```
(after having constructed the `struct` with the correct schema). I think if the current code is working, it's just lucking out on `NONE`'s error code being `0` or something. I wouldn't think it would work as is since the field doesn't have a default value defined."
95104096,2074,ewencp,2017-01-09T05:38:53Z,"Shouldn't some of these change to remove the topic partition data since the v2 version will just include an empty list in that case?

Also, might be worth checking in on the patch(es) for https://cwiki.apache.org/confluence/display/KAFKA/KIP-97%3A+Improved+Kafka+Client+RPC+Compatibility+Policy to see how this will be impacted. I'd imagine you actually want to test both versions."
95104149,2074,ewencp,2017-01-09T05:40:32Z,"Since the is specific to v2+, the constructor used doesn't even really need the `responseData` parameter -- if there was a top-level error it seems there will never be response data so we can just use a dummy empty list in `OffsetFetchResponse`."
95106780,2074,ewencp,2017-01-09T06:43:47Z,comment can be cleaned up
95106900,2074,ewencp,2017-01-09T06:46:14Z,"Also, I noticed when reviewing this that `OffsetFetchRequest.handleError` doesn't use the request version when constructing the `OffsetFetchResponse`. This seems broken, but even more so now that the format differs between versions. (Strictly speaking I think it was already broken and a strict client could have potentially caught the issue.)"
95108544,2074,ewencp,2017-01-09T07:10:51Z,"I think you need to be careful about listing unauthorized topics. If `offsetFetchRequest.isAllPartitions()` is `true`, then you shouldn't reveal the existence of unauthorized topics. See `handleTopicMetadataRequest` for an example of what I mean."
95235045,2074,vahidhashemian,2017-01-09T20:06:14Z,Thanks for catching this. This method is not required anymore as it's part of rejected alternatives 1 and 2. I'll remove it.
95239706,2074,vahidhashemian,2017-01-09T20:32:26Z,That's fair. I can modify the condition of the `if` block before this `switch` statement to skip building `responseData` for version 2 and beyond.
95273042,2074,vahidhashemian,2017-01-09T23:43:03Z,You're right. I'll try to follow a similar pattern for `OffsetFetchResponse` in the next update.
95294702,2074,vahidhashemian,2017-01-10T03:25:11Z,"I'll update the expected responses as you suggested.

Regarding supporting both versions it seems that work would conflict with what is being implemented for KIP-97. Not sure what's the best way to handle it, wait for that to merge first, or move forward with this as is (assuming the latest API version), and then update as part of or after KIP-97."
95297660,2074,ewencp,2017-01-10T04:10:35Z,"Yeah, tbh I wasn't sure either since I hadn't reviewed those patches yet and wasn't sure of the state. I mentioned this to @hachikuji today as well. His thought was that since https://github.com/apache/kafka/pull/2264 (which is actually only 1 of a couple of patches for KIP-97) is quite large, it might make sense to get it merged first.

We're pretty sure this is the only KIP that will potentially be affected by it. @hachikuji has also taken a pass at that one, so if we merge it and you need guidance on updating the patch, he can probably give direction pretty easily. I just checked and there are some minor merge conflicts, but nothing too crazy, so my guess would be that it'd only be a bit more work to layer on the extra bit of compatibility work."
95298064,2074,vahidhashemian,2017-01-10T04:17:08Z,Sounds good to me. I'll work on the rest of items you found in the meantime that PR gets merged. Then we can revisit this piece.
95298512,2074,vahidhashemian,2017-01-10T04:24:36Z,Another good catch. Will try to address this it in the next update.
95431126,2074,vahidhashemian,2017-01-10T18:51:35Z,You're right. I'll try to fix this in the next update.
95485200,2074,hachikuji,2017-01-10T23:54:00Z,Kind of annoying that the response doesn't give us an instance of `Errors` directly.
95485372,2074,hachikuji,2017-01-10T23:55:28Z,Comment is out of date.
95488199,2074,hachikuji,2017-01-11T00:17:09Z,nit: not really sure we need two separate constants even though they are separate fields in the struct.
95488264,2074,hachikuji,2017-01-11T00:17:48Z,Perhaps useful to break down which of these are partition errors?
95489296,2074,hachikuji,2017-01-11T00:26:36Z,"This will be a little annoying to handle when we incorporate the client compatibility KIP since we'll have to check for the presence of these errors at both levels. One option might be to enhance the parsing of the response to check for the presence of one of the top-level errors in the partition data. If it is there, we could insert it at the top level as well. Currently I think we just put `Errors.NONE` at the top level for old versions."
95491518,2074,hachikuji,2017-01-11T00:45:18Z,"Couldn't we push this logic into the response constructor? Perhaps if the version is equal to 1, we take the top level error code and insert it into the partition data?"
95497258,2074,vahidhashemian,2017-01-11T01:40:34Z,I'll add a method to `OffsetFetchResponse` that returns the actual `Errors` value.
95497341,2074,vahidhashemian,2017-01-11T01:41:27Z,No problem. I'll use the same constant.
95498385,2074,vahidhashemian,2017-01-11T01:52:46Z,"Sure, I also used a constant below this comment to define and use that in the code."
95501741,2074,vahidhashemian,2017-01-11T02:24:15Z,Sure. And I think it would be safe to insert one top level error in case there are more than one.
95650676,2074,vahidhashemian,2017-01-11T19:20:32Z,"I think I'm missing something here. We are already iterating over all partitions here (for version 1) and injecting the proper error code. If we want to do the injection in `OffsetFetchResponse` constructor, we need to iteration over them again, which wouldn't be very efficient. Could you please clarify? Thanks."
95652522,2074,hachikuji,2017-01-11T19:29:38Z,"Mainly what I'm trying to achieve is keeping version handling logic out of `GroupCoordinator` as much as possible. So what I had in mind is a constructor or a factory which accepts a top-level error code and a list of the partitions. In the case of the old version, we take the top-level error code and override the partition-level errors. In the case of the new version, we can ignore the partition data and just return the top-level error code.

"
95659959,2074,vahidhashemian,2017-01-11T20:04:47Z,Thanks for clarifying. So the signature of this `GroupCoordinator` method would likely need to be modified to return a `OffsetFetchResponse` instance.
95660614,2074,hachikuji,2017-01-11T20:07:41Z,True. I'm not sure that's better or worse. It doesn't seem too bad given that we already return `OffsetFetchResponse.PartitionData` though.
95669374,2074,vahidhashemian,2017-01-11T20:52:43Z,"Yes, the only thing is after building the offset response here we'll have to later add response data for unauthorized topics."
95672186,2074,hachikuji,2017-01-11T21:08:28Z,"Good point... One option that comes to mind is to use exceptions to propagate top-level errors. This would rely on `OffsetFetchRequest.getErrorResponse` to build the response. But we don't do that for any of the other coordinator APIs, so I'd rather not make this case exceptional.

So how about this: in addition to returning the top-level error code directly in the tuple, we also use it to fill the partition-level error code. Then we don't need to pass the version into `GroupCoordinator` at all and we can let the handler in `KafkaApis` decide how to do the serialization. Basically if the top-level error code is not NONE, then ignore the partitions."
95672359,2074,hachikuji,2017-01-11T21:09:24Z,Can we use `Errors` instead of `Short` in the return type?
95673432,2074,hachikuji,2017-01-11T21:15:04Z,"Or perhaps even simpler: we could always return the error code and an empty map, and we could let `KafkaApis` expand the error code into the partition data when required by the fetch version?"
95677478,2074,vahidhashemian,2017-01-11T21:36:08Z,Thanks. I also like your last suggestion.
95700427,2074,hachikuji,2017-01-11T23:58:36Z,Could we just use `Errors` throughout? You can always get the code from `Errors` if you really need it.
95719434,2074,hachikuji,2017-01-12T03:17:37Z,I wonder if we ought to just assume that the error goes at the top-level. It's a little weird to receive a partition-specific error code here and then assume that it should be used for _all_ partitions.
95719458,2074,hachikuji,2017-01-12T03:18:01Z,"This is `errorCodeThrown`, right?"
95719646,2074,hachikuji,2017-01-12T03:20:34Z,Maybe we can remove this and force the use of `error()`?
95721130,2074,hachikuji,2017-01-12T03:43:01Z,nit: braces for multi-line branches
95721250,2074,hachikuji,2017-01-12T03:43:54Z,nit: I think this could be replaced by `offsets.get(topicPartition).map(_.offset)`
95721791,2074,hachikuji,2017-01-12T03:51:41Z,nit: right-hand side could be replaced by `new TopicAndPartition(offset._1)`
95722080,2074,hachikuji,2017-01-12T03:56:23Z,"A little easier to follow this if you deconstruct the tuple (i.e. use `case (topicPartition, partitionData)`."
95722163,2074,hachikuji,2017-01-12T03:57:36Z,"Where do we check errors in the response? If we push the error checking into `listGroupOffsets`, maybe this API could return `Map[TopicPartition, Long]` as you would probably expect."
95722389,2074,hachikuji,2017-01-12T04:00:57Z,Same as comment above: maybe we always treat this as a top-level error? 
95722449,2074,hachikuji,2017-01-12T04:01:54Z,nit: the `case` is unneeded.
95722624,2074,hachikuji,2017-01-12T04:04:27Z,"nit: slightly more natural if `Errors` is the first entry? Also, we don't need `apiVersion` anymore, right?"
95722933,2074,hachikuji,2017-01-12T04:08:46Z,We need to synchronize on the group to access its state.
95723097,2074,hachikuji,2017-01-12T04:10:06Z,Not sure about the name. How about `partitionsWithCachedOffsets`?
95723245,2074,hachikuji,2017-01-12T04:12:45Z,"Alternatively, we could allow `handleFetchOffsets` to return all offsets for the group, and we could filter out the partitions that the principal is not authorized to access. That seems a little bit better than exposing a new method in `GroupCoordinator`."
95723440,2074,hachikuji,2017-01-12T04:15:42Z,"If there should be a member, perhaps we should assert it?"
95723511,2074,hachikuji,2017-01-12T04:17:00Z,"nit: easy to miss the `&&` with this alignment. Perhaps this could go on the previous line? Also we can use `contains`. For example: `state.contains(""Dead"")` instead of `state == Some(""Dead"")`."
95723657,2074,hachikuji,2017-01-12T04:19:41Z,nit: Replace with `contains`
95723677,2074,hachikuji,2017-01-12T04:20:05Z,nit: We could use `count` here.
95723736,2074,hachikuji,2017-01-12T04:21:01Z,Maybe this should be in a `finally`? Similar below
95724623,2074,hachikuji,2017-01-12T04:36:09Z,"You can use `==` instead of `equals` for all of these. As it is, Intellij is complaining that the types are unrelated."
95725601,2074,hachikuji,2017-01-12T04:54:05Z,nit: pretty sure we shouldn't need this if we're throwing an exception above. More of these below.
95847224,2074,vahidhashemian,2017-01-12T17:55:09Z,So you mean if there is a top-level error code it should not be partition error? I'm okay with that. In that case the second check on this line would be redundant. Please advise if I misunderstood. Thanks.
95849478,2074,vahidhashemian,2017-01-12T18:06:20Z,And perhaps later we should remove this method from other `Response` classes.
95854631,2074,vahidhashemian,2017-01-12T18:32:55Z,Sounds good. I had missed the error check after the latest API change. Thanks.
95855842,2074,hachikuji,2017-01-12T18:39:06Z,"Haha, I'm not sure whether we're saying the same thing. My suggestion was to blindly treat the exception as a top-level error. In other words, take the error code from the exception and use it as the top-level error code for new versions, and as the partition-level error code for old versions."
95855851,2074,vahidhashemian,2017-01-12T18:39:08Z,"Right, and with the change to return type of `listGroupOffsets` this will become `offsets.get(topicPatition)`. Thanks."
95859684,2074,hachikuji,2017-01-12T18:57:39Z,Definitely. This is one of my favorite gripes. Using more specific types whenever possible allows the compiler to do more work for us.
95869619,2074,vahidhashemian,2017-01-12T19:45:04Z,"So even the older versions will have an error code at the top level? This would change it to
```
if (versionId < 2 && partitions != null) {
    for (TopicPartition partition: partitions) {
        responseData.put(partition, new OffsetFetchResponse.PartitionData(
                OffsetFetchResponse.INVALID_OFFSET,
                OffsetFetchResponse.NO_METADATA,
                errorThrown.code()));
    }
}

switch (versionId) {
    case 0:
    case 1:
    case 2:
        return new OffsetFetchResponse(responseData, errorThrown, versionId);
    default:
        throw new IllegalArgumentException(String.format(""Version %d is not valid. Valid versions for %s are 0 to %d"",
                versionId, this.getClass().getSimpleName(), ProtoUtils.latestVersion(ApiKeys.OFFSET_FETCH.id)));
}
```"
95875608,2074,vahidhashemian,2017-01-12T20:17:31Z,"Sure, this is a better approach. Thanks."
95886293,2074,vahidhashemian,2017-01-12T21:16:06Z,`contains` seems to be not supported in Scala 2.10. And builds are failing locally for me because of that. Don't we still support 2.10? Is there a way to get around it?
95891116,2074,vahidhashemian,2017-01-12T21:41:53Z,"Sure, I'm using eclipse and it doesn't complain about `equals`."
95912504,2074,hachikuji,2017-01-12T23:54:49Z,Don't worry about it if it's not supported.
95913606,2074,vahidhashemian,2017-01-13T00:04:00Z,"Well, this actually breaks the unit test, because we don't want to run the `try` block only once. We want to keep trying until the group stabilizes. If we move the `close` to `finally` we close the command after the first try and run into an error on the next try."
95937690,2074,vahidhashemian,2017-01-13T04:51:32Z,"So if we want to just check the top level error for any error in the response for partition level errors we lose the specific partitions that are erroneous. Also, if there are different partition level error types present we'll report only one. This just limits the error reporting but shouldn't affect the functionality. I hope I did not misunderstand your point."
95937720,2074,vahidhashemian,2017-01-13T04:52:12Z,"Also, do you happen to know why out of the 5 possible errors we just check only 3 here?"
95938741,2074,hachikuji,2017-01-13T05:09:43Z,Maybe we should enforce a minimum version number when querying all partitions? You can look at `ListOffsetRequest` for an example of this.
95939088,2074,hachikuji,2017-01-13T05:16:00Z,nit: add a space before the `:`.
95939338,2074,hachikuji,2017-01-13T05:20:11Z,Wonder if there's any harm retaining the top-level error regardless of the version. Seems more consistent with how we handle the case of constructing from a `Struct`.
95939575,2074,hachikuji,2017-01-13T05:23:09Z,Probably worth a comment explaining why we do this.
95939801,2074,hachikuji,2017-01-13T05:25:39Z,"Related to above comment. This method only makes sense for version 2, so maybe we should remove `version` and use 2 directly."
95939909,2074,hachikuji,2017-01-13T05:27:50Z,Do we need to check the partition errors also?
95939988,2074,hachikuji,2017-01-13T05:29:04Z,nit: space after comma.
95940177,2074,hachikuji,2017-01-13T05:31:54Z,You could also use `new TopicAndPartition(topicPartition)`
95940449,2074,hachikuji,2017-01-13T05:36:09Z,"Instead of using `null` as the sentinel, we could use an `Option`."
95940542,2074,hachikuji,2017-01-13T05:37:41Z,Another option would be to push the handling of all offsets into `getOffsets`. One small advantage is that you would only need to acquire the lock once instead of twice.
95942547,2074,hachikuji,2017-01-13T06:11:25Z,"You could do these assignments at once:
```scala
val (groupOffsets, partitions) = ...
```
We use this pattern just below."
95942666,2074,hachikuji,2017-01-13T06:13:31Z,"Seems like this and the other call to `handleFetchOffsets` below needs to go to the `else` case after the check for version 0. For version 0, we pull offsets from zookeeper. I'm wondering if your first approach, which collected all partitions from the coordinator first, may have been a little cleaner. Another option to consider, perhaps you could do the post-filtering for the `isAllPartitions` case separately, and continue doing pre-filtering when we are provided the partition list. "
95942941,2074,hachikuji,2017-01-13T06:17:07Z,"I was thinking we could handle both of these cases in the same constructor. The constructor would take a single error code and the list of requested partitions. If the version is greater than or equal to 2, the partitions are ignored; otherwise, the error code is written into the partition errors."
96034941,2074,vahidhashemian,2017-01-13T17:25:50Z,I think it should be okay to do that. Will update.
96036429,2074,vahidhashemian,2017-01-13T17:34:41Z,Will there be any partition error? In the case of all offsets unauthorized partitions are excluded and there won't be any unknown topic partition. Do you think we should throw an exception if there is any? 
96042948,2074,hachikuji,2017-01-13T18:12:16Z,Seems safer (and more future-proof) to check and throw an exception.
96043181,2074,vahidhashemian,2017-01-13T18:13:33Z,Great idea because `getOffsets` already gives us what we want ([here](https://github.com/apache/kafka/blob/6d6c77a7a9c102f7508e4bc48e0d6eba1fcbc9c6/core/src/main/scala/kafka/coordinator/GroupMetadataManager.scala#L346)).
96057467,2074,vahidhashemian,2017-01-13T19:29:20Z,Good idea. Thanks.
96058029,2074,vahidhashemian,2017-01-13T19:32:19Z,"Would it also work if one assignment depends on the other one? `partitions` uses `groupOffsets`. Also, it may not read easily since there is a big type definition for `groupOffsets`."
96059742,2074,hachikuji,2017-01-13T19:40:46Z,"Does type inference not work? I was thinking something like this:
```scala
        val (groupOffsets, partitions) =
          if (isAllPartitions) {
            val groupOffsets = coordinator.handleFetchOffsets(offsetFetchRequest.groupId, null)
            (groupOffsets, groupOffsets._2.keySet)
          } else {
            val requestPartitions = offsetFetchRequest.partitions.asScala.toList
            (coordinator.handleFetchOffsets(offsetFetchRequest.groupId, requestPartitions), requestPartitions)
          }
```"
96060015,2074,hachikuji,2017-01-13T19:42:08Z,nit: unneeded parenthesis. 
96062247,2074,vahidhashemian,2017-01-13T19:53:46Z,"Yes, it works perfectly. Sorry for the premature question!"
96085672,2074,vahidhashemian,2017-01-13T22:21:35Z,Still not too sure about this. Did you want to move this error check up in the first `if` block?
96088422,2074,hachikuji,2017-01-13T22:41:45Z,I'm not sure I see the problem. Would we ever see this at the top level?
96089207,2074,hachikuji,2017-01-13T22:46:58Z,"Actually I guess we could make this a little simpler. We know we need version 2, so maybe we can use it directly and remove `minVersion`?"
96089229,2074,vahidhashemian,2017-01-13T22:47:07Z,"I'm referring to [your earlier comment](https://github.com/apache/kafka/pull/2074#discussion_r95489296), and I'm not sure if it implied modifying this method too."
96089343,2074,vahidhashemian,2017-01-13T22:48:03Z,"Sure, I thought about it too, but thought to keep it in sync with `ListOffsetRequest`. I'll update."
96090049,2074,hachikuji,2017-01-13T22:53:44Z,"I was mainly concerned that we'd need to check errors in both places, but I think we're good now since we ensure that top-level error codes will always appear at the top level (even for older versions). "
96090198,2074,hachikuji,2017-01-13T22:54:55Z,"Yeah, we're still feeling out the best patterns for handling older versions."
96090442,2074,vahidhashemian,2017-01-13T22:56:45Z,"Great, thanks for clarifying."
96091324,2074,hachikuji,2017-01-13T23:04:02Z,Could we mention that we do this so that the client can depend on the top-level error code regardless of the offset fetch version?
96091481,2074,hachikuji,2017-01-13T23:05:30Z,Do we need another field if the errors are already contained in `PartitionData`?
96092241,2074,hachikuji,2017-01-13T23:10:41Z,"Talked to @ijuma about this, and I don't think we need to bump the internal version number since the brokers do not use offset fetches themselves."
96092616,2074,vahidhashemian,2017-01-13T23:13:53Z,I added this to keep track of partition errors that is needed by `AdminClient` [here](https://github.com/apache/kafka/pull/2074/files/ad48ee99f86cb526c0f7fb23c6e3ce9fd71847a8#diff-66eb7905a403b7fa05ff6bc90e60cd42R125). Unless it's okay to process `PartitionData` on the fly?
96092684,2074,vahidhashemian,2017-01-13T23:14:22Z,Sounds good. I'll remove the internal version.
96092918,2074,hachikuji,2017-01-13T23:16:39Z,"Seems just as efficient to me, especially since we only throw the first error. "
96093935,2074,vahidhashemian,2017-01-13T23:24:50Z,"Sure, then I think we perhaps need to at least have another `Errors` member for that first partition error. So we don't have to process `partitionData` multiple times for checking the existence and actually retrieving the error (in `hasPartitionErrors` and `partitionErrors`)."
96094359,2074,hachikuji,2017-01-13T23:28:38Z,"I think this is close, but it's a bit annoying that we have to call `handleFetchOffsets` in two places, right? Wouldn't it be better to delay the check for `isAllPartitions` and the filtering until after the call to `handleFetchOffsets` below. That gives us a clean separation of the Kafka and Zookeeper offset handling. So maybe the logic can be something like this:
1. Filter unauthorized partitions
2. Check if this is version 0
  a. Fetch from zk for version 0
  b. Check from kafka for versions 1 and above. After receiving the fetched offsets, check if `isAllPartitions` is set. If so, additionally filter out the fetched offsets for topics we are not authorized for.

Does that make sense?"
96094652,2074,hachikuji,2017-01-13T23:31:17Z,Hmm.. It just doesn't seem worth optimizing for. Processing the partition data means what? Looping over it and checking if error is NONE? Does it matter if we do that twice? We could also just leave off the `hasPartitionErrors` and do a single iteration and raise the error on the first exception.
96095174,2074,vahidhashemian,2017-01-13T23:36:50Z,"Would something like this work?
```
public Errors getPartitionError() {
    Collection<PartitionData> partitionsData = this.responseData.values();
    for (PartitionData data : partitionsData) {
        if (data.hasError())
            return data.error;
    }
     return null;
}
```"
96095469,2074,hachikuji,2017-01-13T23:40:03Z,"Sure, that would work. Maybe `getFirstPartitionError` is a clearer name? Or you could bundle the exception throwing as well into a single `maybeThrowFirstPartitionError`? Either way is fine with me, but I'd prefer not to additional fields without a clear case that they're needed."
96096333,2074,vahidhashemian,2017-01-13T23:48:54Z,"If I understand this correctly, for version 1 and above, to receive the fetched offsets we already need to check `isAllPartitions` to determine if `None` or `Some(partitions)` should be passed to `handleFetchOffsets`."
96096521,2074,hachikuji,2017-01-13T23:50:38Z,"Yeah, that's fair. The point is that it should happen in the Kafka branch of that `if` and not before."
96097187,2074,vahidhashemian,2017-01-13T23:58:29Z,I think we need to call `isAllPartitions` upfront anyway because we need to make sure `offsetFetchRequest.partitions` is not null before starting to filter.
96097302,2074,hachikuji,2017-01-13T23:59:40Z,Ack
96099907,2074,vahidhashemian,2017-01-14T00:28:49Z,I hope this is now closer to what you described.
138698389,3849,becketqin,2017-09-13T18:15:47Z,It seems better to say Producer.send() instead of send.
138699047,3849,becketqin,2017-09-13T18:18:06Z,We are passing `now` everywhere else. Maybe we can just keep the argument name the same.
138726608,3849,tedyu,2017-09-13T20:14:46Z,Should <= be used ?
138726989,3849,tedyu,2017-09-13T20:16:27Z,deliveryTimeoutMs should be mentioned
138787782,3849,becketqin,2017-09-14T03:06:17Z,Should we validate the delivery.timeout.ms is greater than request.timeout.ms?
138789918,3849,becketqin,2017-09-14T03:31:27Z,It is probably cleaner to have an explicit `EXPIRED` state.
138791125,3849,becketqin,2017-09-14T03:46:07Z,"The logic here probably needs more comments. We may have the following three cases that the state of a batch has been updated before the ProduceResponse returns:
1. A transaction abortion happens. The state of the batches would have been updated to `ABORTED`.
2. The producer is closed forcefully. The state of the batches would have been updated to `ABORTED`.
3. The batch is expired when it is in-flight. The state of the batch would have been updated to `EXPIRED`.

In the other cases, we should throw IllegalStateException."
138791482,3849,becketqin,2017-09-14T03:50:16Z,"The batches still needs to be expired in order if `max.in.flight.requests.per.connection` is set to 1. So we probably still want to check if the partition is muted or not. That said, if we guarantee that when `RecordAccumulator.expiredBatches()` returns non-empty list, all the earlier batches have already been expired, we can remove the muted check here.

BTW, I did not see the logic of expiring an in-flight batch in the current patch. Am I missing something?"
138791524,3849,becketqin,2017-09-14T03:50:53Z,isFull is no longer used.
138977901,3849,sutambe,2017-09-14T18:40:05Z,agreed
139014083,3849,sutambe,2017-09-14T21:19:24Z,"The actual argument is `now`. However, I like the formal argument name to be `createTime` because it's an immutable value while constructing a batch. `now`, is by definition, changing."
139014648,3849,sutambe,2017-09-14T21:22:07Z,I did some digging around. An expired batch's final state is `FAILED`. I don't feel great about adding yet another `finalState`. We already have `ABORTED` and `FAILED`. The `ProducerBatch.done` will get even more complicated.
139246402,3849,sutambe,2017-09-15T20:45:56Z,Please review the updated method documentation.
139768240,3849,tedyu,2017-09-19T17:53:22Z,This variable can be dropped.
139842619,3849,tedyu,2017-09-19T23:24:22Z,in not -> is not
139843016,3849,tedyu,2017-09-19T23:27:37Z,"The check 'if (deliveryTimeoutMs <= (now - this.createdMs))' inside maybeExpire() would be true.
Looks like another method can be created inside ProducerBatch which expires the batch."
139851577,3849,sutambe,2017-09-20T00:34:32Z,`maybeExpire` has a side-effect of setting `errorMessage` internally. Hence calling it again in `if`.
139852241,3849,tedyu,2017-09-20T00:40:18Z,"Understand.
That part can be refactored - goal is to reduce unnecessary comparison."
139852661,3849,sutambe,2017-09-20T00:43:47Z,@apurvam Those test don't even compile or run on my machine. What's up with those tests?
139853479,3849,apurvam,2017-09-20T00:51:42Z,They can't construct a kafka producer with the changes made in this PR. 
139871039,3849,becketqin,2017-09-20T04:01:43Z,Is this comment accurate? The new state is not necessarily SUCCEEDED.
139871499,3849,becketqin,2017-09-20T04:04:58Z,"Maybe it's not a big deal but just want to call out that this is a behavior change. Currently the producer will throw exception when transition from FAILED state to another state due to some reason other than expiration. If we change this logic, we may miss those cases which are not failed by expiration but still got state update twice. It may not be that important if we do not have programming bugs.

Personally I think it is better to clearly define the states of the batches even if additional complexity is necessary.

The comments should probably also cover the force close case for completeness."
139873674,3849,becketqin,2017-09-20T04:33:40Z,"Some typos in this comments. ""Expire the batch if no outcome is known within delivery.timeout.ms"""
139874459,3849,becketqin,2017-09-20T04:44:01Z,Does this have to be a per partition Map? Intuitively we just need a `TreeSet<ProducerBatch>` with a comparator?
139874583,3849,becketqin,2017-09-20T04:45:39Z,"Assuming `nFlightBatches` is a TreeSet suggested above, this code can be simplified to:
```
        while (!inFlightBatches.isEmpty() &&
               inFlightBatches.first().maybeExpire(deliveryTimeoutMs, now)) {
            expiredBatches.add(inFlightBatches.pollFirst());
        }
```"
139874633,3849,becketqin,2017-09-20T04:46:09Z,`tp` is not used anymore.
139874861,3849,becketqin,2017-09-20T04:49:12Z,No longer used.
139874980,3849,becketqin,2017-09-20T04:50:43Z,This logic would become `inFlightRequests.remove(batch)` when a `TreeSet` is used for this.
139875140,3849,becketqin,2017-09-20T04:52:44Z,This would be just `inFlightBatches.add(batch)`
139875288,3849,becketqin,2017-09-20T04:54:13Z,We usually just use `earliestDeliveryTimeout` in Kafka.
139875598,3849,becketqin,2017-09-20T04:58:47Z,It seems we don't need the `deliveryTimeoutMs` in the sender. It is only used as an argument passed to the accumulator. But the accumulator already has the config.
139876175,3849,becketqin,2017-09-20T05:06:51Z,It seems an existing issue. When we expire the batches here. The memory of those batches will be deallocated. It seems that we will deallocate the same batch again when the ProduceResponse returns?
140056222,3849,becketqin,2017-09-20T18:37:07Z,"Apparently the my understanding of `TreeSet` is not accurate. It uses the comparator to decide whether the entries are the same or not. We can use a TreeMap<Long, Set<ProducerBatch>> then. We may also want to bucket the timestamp a little bit, say one second to avoid huge amount of Sets created for each ms in the `TreeMap`."
140087965,3849,tedyu,2017-09-20T20:45:59Z,"I was thinking about this too. Using millisecond as unit for Map key is not prudent.

After the switch to second as unit, we may need to check the two adjacent buckets keyed by ts-1  (sec) and ts+1 (sec).
"
140133571,3849,becketqin,2017-09-21T01:50:36Z,This test has nothing to do with linger.ms anymore...
140133681,3849,becketqin,2017-09-21T01:51:43Z,Similar to above we should rename this.
140133773,3849,becketqin,2017-09-21T01:52:35Z,typo: timeout
140133809,3849,becketqin,2017-09-21T01:53:02Z,typo: timeout
140136188,3849,becketqin,2017-09-21T02:19:02Z,"Should we still expire the batches when they are expired instead of expiring all the bucket? Having a second granularity bucket does not prevent us from doing that, right?"
140303384,3849,sutambe,2017-09-21T17:11:05Z,"As we discussed, `TreeSet` does not cut it. The naming is consistent. A `TreeSet` is a set. It's just that equality criterion is different."
140304267,3849,sutambe,2017-09-21T17:14:47Z,It's there now
140367934,3849,hachikuji,2017-09-21T21:47:38Z,"Hmm.. Might not be too important, but it doesn't seem necessary to include the retry backoff in this check. If the user sets retries=0, then the backoff shouldn't matter."
140370206,3849,hachikuji,2017-09-21T21:58:50Z,"We are using the creation time of the batch to check for expiration. That will tend to expire some records which were added to the batch after creation earlier than the delivery timeout (by as much as linger.ms). Alternatively, we could use the time that the batch was closed, which will tend to expire records later than the delivery timeout (by as much as linger.ms), but maybe expiring late is bit safer than expiring early? This is equivalent to saying that the delivery timeout excludes linger time."
140371334,3849,hachikuji,2017-09-21T22:05:06Z,"Checking my understanding. With this change, it should no longer be possible to expire a batch before linger.ms has completed and the batch has been closed. If so, do we still need the logic to abort appends on expiration? (It might be safer to have it anyway, just checking if it is still needed for correctness)"
140373989,3849,hachikuji,2017-09-21T22:21:29Z,"After we reset `earliestDeliveryTimeoutMs`, it seems that we do not take into account the expiration times of in-flight batches."
140375038,3849,hachikuji,2017-09-21T22:29:00Z,Do we have any microbenchmarks that show this (potential) optimization is justified?
140377235,3849,hachikuji,2017-09-21T22:44:15Z,Why do we need this check? A comment would be helpful.
140392052,3849,sutambe,2017-09-22T00:47:58Z,Batch close may be arbitrrily delayed in some cases. See @junrao's explanation: http://search-hadoop.com/m/Kafka/uyzND1calgR1Udv2J?subj=Re+DISCUSS+KIP+91+Provide+Intuitive+User+Timeouts+in+The+Producer
140394635,3849,sutambe,2017-09-22T01:16:46Z,fixed. take a look.
140395587,3849,sutambe,2017-09-22T01:26:49Z,`RecordAccumulator.maybeUpdateEarliestDeliveryTimeout`
140560836,3849,apurvam,2017-09-22T18:10:06Z,"Shouldn't we also be removing the batches from the inflight set when the batch is completed (failed or successfully)? I might be missing something, but I don't see that code here."
140571976,3849,sutambe,2017-09-22T18:59:40Z,"@apurvam Right. Cleanup of `soonToExpireInFlightBatches` happens in two places (1) if a batch gets reenqueued and (2) when `Sender` looks for `expiredBatches`. In the second case, we cleanup the batches whose final state is known (success or failure) and there by ""removing"" them."
140590998,3849,hachikuji,2017-09-22T20:35:40Z,"As far as I can tell, it shouldn't be possible to abort a batch after it has been completed. Is this correct? If so, I think it might be better to continue to raise `IllegalStateException`. It's preferable to keep the allowable state transitions as narrowly defined as possible since it ensures faster failure for unexpected paths."
140591599,3849,hachikuji,2017-09-22T20:38:58Z,This is still not used
140591635,3849,hachikuji,2017-09-22T20:39:10Z,Still not used
140592411,3849,hachikuji,2017-09-22T20:43:10Z,"nit: ""for quick **access** to the oldest batch""?"
140594775,3849,hachikuji,2017-09-22T20:55:31Z,"If you are not implementing this, can you please remove it?"
140596621,3849,hachikuji,2017-09-22T21:05:43Z,The name is a little misleading given its proximity to similarly named fields. Maybe something like `nextExpirationTimestampMs` would be clearer?
140597138,3849,hachikuji,2017-09-22T21:08:29Z,Thanks. I synced with Jun and it seems reasonable. It would help to document somewhere why we use create time.
140603555,3849,hachikuji,2017-09-22T21:45:41Z,I think the answer to this question is that it is possible to expire while the batch is still being built because closing the batch can be arbitrarily delayed by inflight fetches.
140604152,3849,hachikuji,2017-09-22T21:49:42Z,Please respond to this. Is it necessary to include RETRY_BACKOFF in this check?
140604457,3849,hachikuji,2017-09-22T21:51:32Z,Do we want to mention the other case where a record is expired early because it was added to a batch which was already nearing expiration?
140606225,3849,hachikuji,2017-09-22T22:04:24Z,"The logic for updating this field seems to assume that the batch at the front of the deque will always be the next to expire, but I'm not sure that is true in the case of retries."
140608227,3849,hachikuji,2017-09-22T22:21:05Z,"To be honest, this lazy expiration seems like overkill. It should be a rare case where we actually have entries in `soonToExpireInFlightBatches` because of the other optimization to only add to it when the delivery timeout will expire prior to the request timeout. And if the producer is in a situation where batches are being expired, then the performance of removal for a particular batch is probably not a major concern. Maybe some benchmarking would show whether it is a worthwhile optimization."
140608868,3849,hachikuji,2017-09-22T22:26:51Z,Inadvertent commit I assume.
140609030,3849,hachikuji,2017-09-22T22:28:14Z,"If we're just returning `true` for `matches`, we don't need to provide a `RequestMatcher` at all."
140614799,3849,sutambe,2017-09-22T23:24:16Z,@hachikuji I think `retryBackOff` can be dropped. Perhaps we can do two tests based on whether `retries` is set or not.
140617891,3849,tedyu,2017-09-22T23:59:57Z,I don't see bucketing
140619943,3849,becketqin,2017-09-23T00:30:02Z,"@hachikuji I agree in most case we probably do not need this. It is probably only useful for large producers (e.g. mirror maker which has thousands of partitions to send). There may still be value to have this:

1. Since delivery.timeout.ms and request.timeout.ms are both user configurations. It would be good to guard against some configurations. (e.g. request.timeout.ms=delivery.timeout.ms, linger.ms=0). 

2. It seems that in some scenarios this would help. For example, when the brokers are being rolling bounced, there will be a lot of retried batches. Those batches may have `remainingDeliveryTimeoutMs` < `request.timeout.ms`. We may have even more than one batches per partition to insert if max.in.flight.request is greater than 1.

A benchmark would be useful. But in general I think it is safer to have this optimization given it does not increase too much complexity."
140625298,3849,becketqin,2017-09-23T04:01:04Z,I had a comment earlier that it seems we should still expire the batch at exact createTime + deliveryTimeoutMs even if we bucket them by seconds.
140625337,3849,becketqin,2017-09-23T04:04:48Z,"I agree with @hachikuji that using retry backoff is a little weird here. `retries` means that ""at most retry that many times within delivery.timeout.ms"". So even if `retries` is greater than 0, it does not mean there must be a retry. So we should probably just check the `delivery.tiemout.ms` is at least `linger.ms` + `request.timeout.ms`. "
140625355,3849,becketqin,2017-09-23T04:06:58Z,@sutambe I also think we should mention the scenario that a Record is added to a batch that is about to expire.
140625458,3849,becketqin,2017-09-23T04:14:29Z,Can we update the Java doc to explain the return value?
140625583,3849,becketqin,2017-09-23T04:24:43Z,Personally I still think a clear EXPIRED state would be clearer. We can let batch.done() method take a FinalState argument instead of inferring the state from the exception.
140625869,3849,becketqin,2017-09-23T04:46:59Z,"Good point. It would work if max.in.flight.requests.per.connection=1, or when we enable idempotence. But otherwise the first batch may not be the first to expire. Since we do not even have callback order guarantee in that case, maybe it is fine? But we should definitely document this."
140626273,3849,becketqin,2017-09-23T05:15:12Z,This test will pass whether line 76 throws IllegalStateException or not. Should we add a fail() statement after line 76?
140626284,3849,becketqin,2017-09-23T05:16:11Z,Is this needed? In what case could e be null?
140626361,3849,becketqin,2017-09-23T05:21:32Z,We should change the test name to something like testBatchExpiration. and the test below to testBatchExpirationAfterReenqueue.
140626370,3849,becketqin,2017-09-23T05:22:08Z,The typo is still there.
140626470,3849,becketqin,2017-09-23T05:23:40Z,"testSoonToExpire... (Upper case ""To"")"
140626641,3849,becketqin,2017-09-23T05:25:13Z,Do you mean it should NOT be included...
140626811,3849,becketqin,2017-09-23T05:38:05Z,typo in the test name.
140626831,3849,becketqin,2017-09-23T05:39:42Z,request1 and request 2 are not used.
140626885,3849,becketqin,2017-09-23T05:43:03Z,Should we check the completeness of request1?
140626907,3849,becketqin,2017-09-23T05:44:46Z,We may need to call sender.run() one more time to ensure the message is not reenqueued. The reqenqueued message won't be sent out again in the same sender.run().
140646338,3849,tedyu,2017-09-24T03:32:38Z,createTime -> creationTime
140646367,3849,tedyu,2017-09-24T03:35:40Z,nit: 'else' can be dropped
140671368,3849,sutambe,2017-09-24T23:48:00Z,@becketqin restored the test as it was before.
140864460,3849,sutambe,2017-09-25T18:47:43Z,@becketqin Just added a test to ensure that we don't do double deallocation. `SenderTest.testNoDoubleDeallocation`
140880631,3849,sutambe,2017-09-25T19:55:16Z,@becketqin The batch is expired at exact time but not removed from the `soonToExpireInflightBatches`. Based on some earlier comments grouping them to avoid pointer chasing?
140881985,3849,becketqin,2017-09-25T20:00:59Z,"This method either throw exception or return true, which indicates there is no need to have a return value."
140884890,3849,apurvam,2017-09-25T20:13:30Z,"Hmm. This seems a bit off. What this means that in the 'normal' case when responses are successful and there is no backlog in the accumulator, we will hang on to batches (and not garbage collect them) until the delivery timeout. 

Indeed, if you add the following at the end of sender tests where there are no more inflight requests (ie. all requests have completed and will never be retried) `        assertTrue(accumulator.soonToExpireInFlightBatches().isEmpty());` all the tests fail.

I think this should be fixed. We should clear the `soonToExpireInflightBatches` as soon as the batch is completely resolved (ie. failed or completed) so that we don't hang on to the reference unnecessarily."
140888794,3849,sutambe,2017-09-25T20:29:28Z,@becketqin Added the muted check back
140910557,3849,becketqin,2017-09-25T22:00:14Z,The method never returns false. We can keep it as void if so.
140914352,3849,becketqin,2017-09-25T22:20:07Z,Should probably add an expire case?
140918001,3849,becketqin,2017-09-25T22:41:20Z,The map is not used.
140918233,3849,becketqin,2017-09-25T22:42:40Z,Should we assert the pool is not deallocated after the expiration but before the response returns?
140918761,3849,becketqin,2017-09-25T22:46:05Z,How could the order be violated if we only append the first batch after the first one is expired?
140919067,3849,becketqin,2017-09-25T22:47:55Z,The map is not used.
140919353,3849,becketqin,2017-09-25T22:49:39Z,nit: Can we avoid reusing the argument name?
140927438,3849,becketqin,2017-09-25T23:46:36Z,"Actually, it seems we may release the memory before the the response returns?"
140928325,3849,apurvam,2017-09-25T23:53:15Z,"As I mentioned in my other comment, the memory utilization itself is not the issue as much as the fact that we are retaining the reference to `ProducerBatch` for at least `deliveryTimeoutMs`. This probably won't cause too much memory pressure, but is still an undesirable behavior."
140935275,3849,becketqin,2017-09-26T00:53:48Z,"Got it. Good catch. Yes, we should remove the completed batch."
142276754,3849,sutambe,2017-10-02T22:55:37Z,@becketqin return type restored to `void`
142484042,3849,sutambe,2017-10-03T18:29:10Z,@becketqin @apurvam Can some please clarify what suggestion is made here? Remove completed batch from where? 
150616359,3849,sutambe,2017-11-13T17:56:25Z,Fixed the test
151183810,3849,sutambe,2017-11-15T16:45:17Z,The new code seems to deallocate the batch right away. I'm not changing the behavior for now.
151194892,3849,sutambe,2017-11-15T17:21:41Z,added
151195752,3849,sutambe,2017-11-15T17:24:58Z,Right
151571211,3849,becketqin,2017-11-16T23:56:46Z,`verifyAndGetDeliveryTimeout()`?
151572566,3849,becketqin,2017-11-17T00:06:55Z,This check is a little flaky. What if deliveryTimeoutMs is `Long.MAX_VALUE - 1`?
159586946,3849,becketqin,2018-01-04T06:27:34Z,Still not used.
159587416,3849,becketqin,2018-01-04T06:34:00Z,"We don't need a PriorityQueue for this because the batches in the RecordAccumulator is already in order. So we just need to keep the draining order.
  "
159587789,3849,becketqin,2018-01-04T06:39:32Z,"If we always insert the batch to the inFlightBatches queue and there is no bug, the batch to be removed should always be the first batch. Can we assert on that?"
159588151,3849,becketqin,2018-01-04T06:44:30Z,"The original reason we have this optimization is because we used to have a big sorted data structure. So avoiding inserting elements to it makes sense. Given that now the batch order in the RecordAccumulator is already guaranteed. It seems we can just put all the drained batches to the inFlightBatches queue, which is simpler.
  "
159593877,3849,becketqin,2018-01-04T07:41:33Z,The while loop may break if the request size has reached. So there is no guarantee that it will iterate over all the partitions. One alternative is to find the nextBatchExpiryTimeMs in the expireBatches.
159594768,3849,becketqin,2018-01-04T07:50:21Z,It seems intuitively this should be the earliest batch in the entire record accumulator?
159600209,3849,becketqin,2018-01-04T08:36:38Z,"It seems we may release the memory for the expired batches before the response is returned. This means the underneath ByteBuffer is still referred by the ProducerBatch instance in the inFlightRequests. I am not sure if this would cause any problem, but it seems a little dangerous."
159602229,3849,becketqin,2018-01-04T08:50:18Z,Is the response preparation needed in this case?
67447081,1336,junrao,2016-06-17T00:34:31Z,"Reporting both owner and member-id can be a bit confusing. Also, for ZK based consumer, we get the following output. The member-id part is repeated in the owner part. Perhaps instead, we can report 3 fields: member-id, client-id, and client-ip. For Kafka-based consumer, we can fill in all 3 fields. For ZK-based consumer, we can just fill in member-id. We can leave the client-id and client-ip part empty since that are not stored explicitly. 

```
GROUP                          TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG             OWNER                                         MEMBER-ID
test-consumer-group            test                           0          2               2               0               test-consumer-group_Juns-MacBook-Pro.local-1466121481202-cb9a4849-0 test-consumer-group_Juns-MacBook-Pro.local-1466121481202-cb9a4849
test-consumer-group            test                                                                                                                                    test-consumer-gr
```
"
67447089,1336,junrao,2016-06-17T00:34:33Z,"Could all those scala.collection.mutable.Map be just mutable.Map?
"
67447119,1336,junrao,2016-06-17T00:34:51Z,"Could we rename this to describeMemberTopicPartitions() to make it clear?
"
67447129,1336,junrao,2016-06-17T00:35:00Z,"In this case, since the member has no associated partitions, do we need to pass in topic at all? Could we just pass in None?
"
67566895,1336,vahidhashemian,2016-06-17T19:59:57Z,"Sure. Will do in the next update.
"
67566945,1336,vahidhashemian,2016-06-17T20:00:20Z,"Yes, None should work too. Will update the PR.
"
67567026,1336,vahidhashemian,2016-06-17T20:00:57Z,"I'll make this change too in the next update.
"
67567292,1336,vahidhashemian,2016-06-17T20:03:09Z,"That's fair. I'll update the code to return the output in the format you suggested. I'm thinking of always returning an empty string for client id and ip for ZK-based consumer (as you suggested), and returning ""none"" for member id when no active consumer exists for the group.
"
67725952,1336,junrao,2016-06-20T17:00:35Z,"It doesn't seem that topic is being used. Also, could we fix the indentation?
"
67725986,1336,junrao,2016-06-20T17:00:48Z,"Could we change getOwner and getOwnerHost to getClientId and getMemberHost accordingly? Also, I am not sure why we getMemberId, getOwner and getOwnerHost need to be a function. Could we just pass in an Option and get rid of ""get""?
"
67726057,1336,junrao,2016-06-20T17:01:15Z,"Could we change owner to client-id? Could owner-host be member-host since not every member owns a partition? 
"
67726065,1336,junrao,2016-06-20T17:01:19Z,"Could we change ownerOpt and ownerHostOpt to clientIdOpt and memberHostOpt, respectively?
"
67726085,1336,junrao,2016-06-20T17:01:27Z,"If topicPartition doesn't exist, should we really pass in offsetOpt? It seems it's more intuitive to pass in None. 
"
67726141,1336,junrao,2016-06-20T17:01:51Z,"Not sure why we need to use ephemeral owner here. It seems that topicPartition->owner gives us the mapping from topic partition to group member id directly. consumerGroupDir + ""/ids gives us all members. From these two, we know members that don't own any partitions.
"
67726172,1336,junrao,2016-06-20T17:02:03Z,"This is the case for members not owning any topic partitions, right? If so, could we just set topicPartitions and partitionOffsets to empty?
"
67750840,1336,hachikuji,2016-06-20T19:27:44Z,"It seems weird to let this function accept `None` for either of these fields (why would I ever try to get the LEO if I don't have a topic or a partition?). Maybe the function can accept an instance of `TopicAndPartition` and the caller can make sure they have an instance prior to calling?
"
67753873,1336,hachikuji,2016-06-20T19:47:38Z,"Basically the same comment as above: it's weird to have a function `describePartition` where the partition is optional. Could we take the logic for handling that case out of this function?
"
67763714,1336,hachikuji,2016-06-20T20:50:42Z,"Minor: would it be helpful to echo the group back to the user in quotes (so that formatting errors are apparent)?
"
67780260,1336,vahidhashemian,2016-06-20T22:42:58Z,"You're right. I'll remove the `topic` parameter and fix indentation.
"
67780312,1336,vahidhashemian,2016-06-20T22:43:19Z,"Sure, I'll update in the next patch.
"
67780332,1336,vahidhashemian,2016-06-20T22:43:27Z,"Sure.
"
67780472,1336,vahidhashemian,2016-06-20T22:44:31Z,"Yup, and I'm going to switch the last two parameters so member related parameters are next to each other. I hope that's fine.
"
67780508,1336,vahidhashemian,2016-06-20T22:44:48Z,"Right, will change to `None`.
"
67781246,1336,vahidhashemian,2016-06-20T22:51:20Z,"I think I had tried that combination before to extract the info. The issue is when I try `get /consumers/group1/owners/test/0` the output looks like this: `cgroup1_kafka-1466461259713-759adaaa-0`. However, when I try `ls /consumers/group1/ids` the output is like `[cgroup1_kafka-1466462141465-ff19e1a5, cgroup1_kafka-1466461259713-759adaaa]`.

It seems the first call returns a client id, and the second one a list of member ids, and that's why the outputs do not quite match (`cgroup1_kafka-1466461259713-759adaaa-0 != cgroup1_kafka-1466461259713-759adaaa`). That's why I did not use this approach as I wasn't sure if there is anything further I can assume to connect the two outputs. If there is, please let me know.
"
67784301,1336,vahidhashemian,2016-06-20T23:18:06Z,"That's correct and makes sense. Will do.
"
67787649,1336,vahidhashemian,2016-06-20T23:52:01Z,"I understand the concern. Will make some changes to fix the issue.
"
67787692,1336,vahidhashemian,2016-06-20T23:52:27Z,"I'll try to fix this as suggested.
"
67788112,1336,vahidhashemian,2016-06-20T23:56:46Z,"That's a good suggestion. Will update this one and the one a few lines below to return the group name as part of the output message.
"
71457060,1336,hachikuji,2016-07-20T02:28:50Z,"minor: maybe we could just debug log the stack trace?
"
71457421,1336,hachikuji,2016-07-20T02:34:23Z,"nit: maybe something like `PartitionAssignmentState` would be more accurate?
"
71457470,1336,hachikuji,2016-07-20T02:35:35Z,"another small nit: the ""Opt"" suffix is kind of annoying. Could we drop it?
"
71457574,1336,hachikuji,2016-07-20T02:37:37Z,"Maybe we could change this to something like this:

``` scala
def handleError(e: Throwable)
```

That might make testing easier.
"
71458065,1336,hachikuji,2016-07-20T02:45:11Z,"Or maybe we keep the current name and just change the argument type since this is `OutputWriter`.
"
71458303,1336,hachikuji,2016-07-20T02:49:45Z,"I'm wondering if we can collapse these bottom 3 methods into a single `printAssignment(assignment: Array[ConsumerGroupAssignment])`. Doesn't seem like we're saving that much with the abstract for loop.
"
71458830,1336,hachikuji,2016-07-20T02:58:36Z,"nit: the name `describeGroup` no longer seems quite right. Maybe `assignmentState` would be more accurate since that's what the method returns.
"
71459346,1336,hachikuji,2016-07-20T03:07:25Z,"Maybe `describeGroup` could be `collectGroupAssignment` and this method could be `collectMemberAssignment`?
"
71459524,1336,hachikuji,2016-07-20T03:10:07Z,"suggestion: `getAllTopicPartitions`?
"
71459607,1336,hachikuji,2016-07-20T03:11:43Z,"Is this used anywhere?
"
71459749,1336,hachikuji,2016-07-20T03:14:11Z,"Actually I see that we print some context-specific error messages, so maybe we need to allow the message to come through. But perhaps we could pass the exception in an optional argument so that we can have a common place to log the stacktrace.
"
71580318,1336,vahidhashemian,2016-07-20T18:28:28Z,"I believe this is being handled in [this other PR](https://github.com/apache/kafka/pull/1548). If it's okay I would let it come through that PR. One of the two PRs would have to be rebased depending on which one goes in first.
"
71581180,1336,vahidhashemian,2016-07-20T18:32:45Z,"Both sound good. Will update in the next commit.
"
71592461,1336,vahidhashemian,2016-07-20T19:38:48Z,"Sure, I'll make necessary changes for this.
"
71592754,1336,vahidhashemian,2016-07-20T19:40:44Z,"That's fair. Would it make sense to use `Assignments`/`assignments` to imply all assignments, and `Assignment`/`assignment` to imply a single assignment row?
"
71593221,1336,vahidhashemian,2016-07-20T19:43:45Z,"Sure. I was wondering if we should use a `get` prefix (e.g. `getGroupAssignment`) to indicate there is some return value. But I don't see that respected as a convention everywhere in the code.
"
71593255,1336,vahidhashemian,2016-07-20T19:43:57Z,"Sounds good.
"
71593302,1336,vahidhashemian,2016-07-20T19:44:19Z,"No. Thanks for catching this. Will be removed.
"
71604687,1336,hachikuji,2016-07-20T20:54:15Z,"Not sure I understand the question. I was thinking that a single `printAssignment` method could accept the complete assignment for the full group. Then printing assignment rows or whatever is just an implementation detail. Does that make sense or not?
"
71606434,1336,vahidhashemian,2016-07-20T21:04:26Z,"I agree with merging those three methods. I was just curious about naming convention for the method and the variables used to implement them. Maybe [this line](https://github.com/apache/kafka/pull/1336/files#diff-f1b330624af520e6f9824bd308e21f24R105) helps, where I use `assignments` as the full list of assignments, and then each individual member is an `assignment`. I thought I could use this convention across the board. This is very minor and not a big deal though if it's confusing.
"
71608339,1336,hachikuji,2016-07-20T21:16:23Z,"Ah I see. maybe we could use `groupAssignment` and `memberAssignment`? As long as it's clear in its context, either way seems ok to me.
"
71609719,1336,vahidhashemian,2016-07-20T21:25:15Z,"Yeah, that works too, and would probably be more descriptive. Thanks.
"
72295665,1336,hachikuji,2016-07-26T17:19:46Z,"Minor: if we use debug(), do we need to extract the stack trace? Maybe you could do something like this:

``` scala
e.map(debug(""Exception in consumer group command"", _))
```
"
72296396,1336,hachikuji,2016-07-26T17:23:50Z,"It's a little weird to have describe() return the partition state. In the test case below using the mock of `OutputWriter`, could you replace the assertions on the result of describe() with `EasyMock.expect()` assertions?
"
72317288,1336,hachikuji,2016-07-26T19:14:02Z,"Seems like the only thing this method is contributing is the computation of lag. Maybe we could replace it with a method like this:

``` scala
protected def getLag(offset: Option[Long], logEndOffset: Option[Long])
```

What do you think?
"
72317749,1336,hachikuji,2016-07-26T19:16:39Z,"Based on the usage below, would `printMessage` be a more accurate name?
"
72318145,1336,hachikuji,2016-07-26T19:19:00Z,"Would it make sense to default `excludeInternalTopics` to true?
"
72321388,1336,vahidhashemian,2016-07-26T19:37:48Z,"This is what I originally wanted to do, but struggled with how to verify the actual result (what would be printed as a result of `describe()` call) against how I call `EasyMock.expect(...)`, which is something like `EasyMock.expect(outputWriterMock.printError(""The consumer group 'missing.group' does not exist""))` for the first unit test (`testDescribeNonExistingGroup`). How do I make sure that a call to `consumerGroupCommand.describe()` would actually make the call identified in `EasyMock.expect`?
"
72321525,1336,vahidhashemian,2016-07-26T19:38:35Z,"Sure, I'll make this change.
"
72324174,1336,vahidhashemian,2016-07-26T19:53:35Z,"Yup, that makes sense.
"
72324734,1336,vahidhashemian,2016-07-26T19:56:57Z,"Yes, it would be more appropriate. I'll change it. Thanks.
"
72325636,1336,vahidhashemian,2016-07-26T20:02:19Z,"I guess it would. There is an existing method before this one (`getConsumersPerTopic`) that also takes `excludeInternalTopics` and assumes no default value for it. I wanted to make the new method consistent with that one.
"
72375727,1336,hachikuji,2016-07-27T03:29:00Z,"That's kind of unfortunate. So here's another idea (feel free to dismiss it if it doesn't make sense). Maybe having `describe()` return the assignment is actually heading in the right direction. What if we get rid of `OutputWriter` and move the printing logic into the main method. Then the `ConsumerGroupService` becomes more functional (and testable). For `list()`, you can have it return the list of groups instead of printing them. The trickier one seems to be `delete()`. Maybe you can leave it as it is.
"
72523263,1336,vahidhashemian,2016-07-27T21:03:42Z,"Sure, I'll try that. Having `describe()` return the assignment would make testing much easier.
"
72666749,1336,hachikuji,2016-07-28T17:38:28Z,"This is looking promising. Maybe we could rename `list` to `listGroups` and `describe` to `describeAssignment`?
"
72688365,1336,vahidhashemian,2016-07-28T19:40:09Z,"Sounds good. Wouldn't `describeGroup` (singular) be more self-explanatory than `describeAssignment`?
"
72688649,1336,hachikuji,2016-07-28T19:41:51Z,"Yeah, that sounds good to me.
"
72688751,1336,vahidhashemian,2016-07-28T19:42:32Z,"And we could also rename `delete` to `deleteGroups`?
"
72691457,1336,hachikuji,2016-07-28T19:58:55Z,"Makes sense to me.
"
72722638,1336,vahidhashemian,2016-07-28T23:43:33Z,"@junrao I was wondering what your opinion is about the response above to your comment. This PR has gone through a few more rounds of reviews thanks to @hachikuji and this currently is the only outstanding item. Thanks in advance for looking into this.
"
72728458,1336,hachikuji,2016-07-29T00:53:38Z,"Looks like we would print this twice: once here and once in main. I'm wondering if we could remove `ConsumerGroupOutputWriter` from this class and do all the output in main. Maybe we just need to move the empty check that you have below in main?
"
72839362,1336,vahidhashemian,2016-07-29T18:28:54Z,"Thanks for catching this. I'll fix in the next update. And we can remove `ConsumerGroupOutputWriter` from that class, as you suggested.
"
83920039,1336,hachikuji,2016-10-18T18:28:43Z,"Is this needed for a consumer group? I think protocol type will always be ""consumer.""
"
83920161,1336,hachikuji,2016-10-18T18:29:15Z,"For consumer groups, the protocol is really the assignment strategy.
"
83921286,1336,hachikuji,2016-10-18T18:34:09Z,"As far as I can tell, the return type of this method doesn't need to be mutable. Maybe something like this would be a little nicer?

``` scala
    memberIds.map { memberId =>
      val topicCount = TopicCount.constructTopicCount(group, memberId, this, excludeInternalTopics)
      memberId -> topicCount.getTopicCountMap.keys.toList
    }.toMap
```

(Note I used `keys` instead of `keySet` since we throw away the set anyway.)
"
83922730,1336,hachikuji,2016-10-18T18:41:08Z,"Would it make a big difference if this method accepted `TopicPartition` instead of `TopicAndPartition` since it seems that's what we need anyway?
"
83923001,1336,hachikuji,2016-10-18T18:42:34Z,"Is this intentional? Same for the couple doc changes below.
"
83924024,1336,hachikuji,2016-10-18T18:47:48Z,"Since we didn't end up needing this for testing, maybe we can just get rid of it and keep its methods one level up?
"
83924432,1336,hachikuji,2016-10-18T18:49:47Z,"nit: Since you're using `map`, we don't need this check.
"
83924865,1336,hachikuji,2016-10-18T18:51:46Z,"Could we let this function handle the empty assignment case as well?
"
83925389,1336,hachikuji,2016-10-18T18:54:23Z,"I'm not actually sure this message is still correct. In KAFKA-2720, we introduced an `Empty` state for the group, which basically persists until all the offsets for the group have expired. In this case, the assignment will be empty, but it will not be rebalancing.
"
83925578,1336,hachikuji,2016-10-18T18:55:21Z,"Should this be `private`?
"
83926233,1336,hachikuji,2016-10-18T18:58:31Z,"Similar to other comment: maybe we could use `TopicPartition` here.
"
83926524,1336,hachikuji,2016-10-18T18:59:55Z,"Why not use `map`?
"
83932002,1336,hachikuji,2016-10-18T19:28:41Z,"Seems like this is another case where we don't actually need a mutable collection if we use `flatMap`.
"
83933934,1336,hachikuji,2016-10-18T19:39:34Z,"It seems the node of the partition owner includes the threadId. The pattern is always ""{consumerId}-{threadId}"", so checking the prefix of the partition owner would always get us the right id. Maybe it's a little nicer to use that than the ephemeral owner?
"
83934306,1336,hachikuji,2016-10-18T19:41:21Z,"Maybe change this to `map`, then I think `flatMap` as suggested above would work nicely.
"
83934935,1336,hachikuji,2016-10-18T19:44:51Z,"This looks odd. We create an option just so we can call `map`? Could we replace this with:

``` scala
eOwner -> memberId
```

Same for the loop below.
"
83935126,1336,hachikuji,2016-10-18T19:45:44Z,"Using `map` would be nicer?
"
83948295,1336,vahidhashemian,2016-10-18T20:52:35Z,"Yes, it'll be `consumer`, but there is a check [here](https://github.com/vahidhashemian/kafka/blob/904bcee920605e1fe5417fe39b5397194b665832/core/src/main/scala/kafka/admin/AdminClient.scala#L151) to verify that the protocol type os valid. If we remove this, I think that check has to be removed too. Should I still go ahead and remove it?
"
83948616,1336,vahidhashemian,2016-10-18T20:54:22Z,"Sure, I'll rename this field to `assignmentStrategy`.
"
83952330,1336,vahidhashemian,2016-10-18T21:14:02Z,"Makes sense. Thank you for the suggestion. I'll update the method.
"
83955679,1336,vahidhashemian,2016-10-18T21:33:13Z,"You're right. Would it be OK to change this message to `Consumer group ... has no active member or is rebalancing`?
"
83957107,1336,hachikuji,2016-10-18T21:42:04Z,"I would probably move that check into `describeGroup`. Now that I'm thinking about it, we should probably either rename `describeGroup` to `describeConsumerGroup`, or we should let `describeGroup` return a generic `GroupSummary` while `describeConsumerGroup` returns `ConsumerGroupSummary`.
"
83957355,1336,hachikuji,2016-10-18T21:43:17Z,"It would be more ideal if we could tell the user which is the case. Maybe we need to propagate the state of the group down to this method.
"
83960147,1336,vahidhashemian,2016-10-18T22:00:51Z,"Thanks. With the first suggestion, there already is a `describeConsumerGroup` method. Unless you prefer this suggestion (using a different method name) I'll go ahead with your second suggestion.
"
83963057,1336,vahidhashemian,2016-10-18T22:20:09Z,"Upon further consideration, I think I'm going to adopt your first suggestion, because I need the current `describeGroup` method to return the coordinator as part of its output; so I can report the coordinator's broker id. I'm going to use the method name `getConsumerGroupSummary` instead.
"
83973106,1336,vahidhashemian,2016-10-18T23:35:51Z,"Yup, will try to do this in the next patch.
"
83973553,1336,vahidhashemian,2016-10-18T23:39:43Z,"This `case class` is used in other classes in the same file. Since it's defined outside the scope of those classes `private` wouldn't work, but `protected` would. I hope I didn't misunderstood your point.
"
83974748,1336,vahidhashemian,2016-10-18T23:49:50Z,"Sure, sounds good.
"
83974783,1336,vahidhashemian,2016-10-18T23:50:11Z,"Thanks for catching this.
"
83975322,1336,vahidhashemian,2016-10-18T23:55:12Z,"We could, but we would need to pass more arguments now that we want to distinguish between empty group and rebalancing group. Plus, in the case of an empty assignment we are using the `printError` method, instead of actually printing assignments. Do you still think we should change it?
"
84141950,1336,vahidhashemian,2016-10-19T18:55:49Z,"I'll try to make use of `flatMap`.
"
84143779,1336,vahidhashemian,2016-10-19T19:05:03Z,"Sure I'll try to use that (by ignoring the threadId part) instead of ephemeral owner.
"
84150277,1336,vahidhashemian,2016-10-19T19:39:35Z,"This section will be removed now that we decided to use the owner info directly.
"
84157718,1336,vahidhashemian,2016-10-19T20:17:48Z,"I'll give it a try in the next patch.
"
84164530,1336,vahidhashemian,2016-10-19T20:53:37Z,"To be honest, it's been so long that I don't recall why I made these changes. But when I try the current command, I can't tell what the meaning of ""new consumer being the default"" is. It seems to me that we need to either provide `--zookeeper` or `--bootstrap-server`; it's not like if we use `--new-consumer` we don't have to provide `--bootstrap-server`. The only restriction around `--new-consumer` seems to be that it cannot be used along with `--zookeeper`. The descriptions in parenthesis are not very clear to me. But it might be just me. Your thoughts?
"
84167217,1336,hachikuji,2016-10-19T21:07:13Z,"I agree it's not super clear. Maybe we should just avoid saying it's required since it saves us from needing to qualify? You can try to fix this if you want, but I'd be ok just leaving it as it is since it seems orthogonal to the rest of this.
"
84173733,1336,vahidhashemian,2016-10-19T21:42:52Z,"Sure, I'll revert these changes. I'm OK with leaving the ""required"" text in since other tools have it too, but we need to better qualify them, as you mentioned. I may work on it separately later. 
"
84191293,1336,hachikuji,2016-10-19T23:53:22Z,"If we had `ConsumerGroupSummary` include a list of `ConsumerSummary` objects instead of `MemberSummary`, would we still need this function?
"
84192091,1336,hachikuji,2016-10-20T00:00:45Z,"Seems like this method isn't giving us much anymore. Maybe it's ok to use `println` directly?
"
84192990,1336,hachikuji,2016-10-20T00:10:00Z,"It's a little weird to locate this in `kafka.coordinator` since the coordinator is technically agnostic to group internals. The best alternative I can think of is maybe to put it in tools with `AdminClient`. What do you think?
"
84193326,1336,hachikuji,2016-10-20T00:13:17Z,"nitpick: since it's a simple statement, maybe parenthesis in `map` would be a little nicer than braces?
"
84193423,1336,hachikuji,2016-10-20T00:14:14Z,"nitpick: we usually don't put a space before the ':'.
"
84193800,1336,hachikuji,2016-10-20T00:18:18Z,"nitpick: I think parenthesis are a little nicer for simple one-liners like this.
"
84194583,1336,hachikuji,2016-10-20T00:26:11Z,"There are few other places in the patch where we could also change this.
"
84194947,1336,vahidhashemian,2016-10-20T00:30:38Z,"I'm not sure if I follow. Could you please elaborate a bit?
"
84195024,1336,hachikuji,2016-10-20T00:31:31Z,"Since the zookeeper service has no notion of consumer state, should we return `Option[String]` instead?
"
84195092,1336,vahidhashemian,2016-10-20T00:32:12Z,"Sure, it makes sense. I'll move it.
"
84195261,1336,vahidhashemian,2016-10-20T00:34:17Z,"I'll remove the space. There are a few other occurrences in this file that I'll fix too.
"
84195305,1336,hachikuji,2016-10-20T00:34:51Z,"Currently `ConsumerGroupSummary` has a field for the group members, which are represented as instances of `MemberSummary`. I'm wondering if it would make sense to use `ConsumerSummary` instead. Then we might only need a single method returning `ConsumerGroupSummary`.
"
84195485,1336,hachikuji,2016-10-20T00:36:58Z,"If we use normal `map` instead of `flatMap`, does this need to be a `Seq` anymore?
"
84195843,1336,hachikuji,2016-10-20T00:41:19Z,"Could this one be a `flatMap` like the one just above?
"
84195844,1336,vahidhashemian,2016-10-20T00:41:20Z,"No, it doesn't. Thanks for catching it.
"
84196169,1336,hachikuji,2016-10-20T00:44:54Z,"Looks like we're missing the 's' at the start of the string. Another few of these below.
"
84196393,1336,hachikuji,2016-10-20T00:47:07Z,"nitpick: I think the braces are unnecessary if it is a simple variable.
"
84196615,1336,vahidhashemian,2016-10-20T00:49:59Z,"Thanks, I noticed there were a few more.
"
84196895,1336,hachikuji,2016-10-20T00:53:01Z,"Looks like another case where we might be able to change the `foreach` to a `map` with a `toArray` at the end.
"
84197069,1336,hachikuji,2016-10-20T00:55:32Z,"nitpick: Maybe we could destructure on assignment?

``` scala
val (state, assignments) = consumerGroupService.describeGroup()
```
"
84197288,1336,vahidhashemian,2016-10-20T00:58:28Z,"Do you mean merging `getConsumerGroupSummary` and `describeConsumerGroup` into one method that returns a `ConsumerGroupSummary` object? If so, I thought about it when I was making the recent changes, and noticed that `getConsumerGroupSummary` is being used in a few other places. That's why I hesitated to make the unnecessary change. But I guess they could be merged and all calls to `getConsumerGroupSummary` would become calls to `describeConsumerGroup`. Please correct me if I'm misunderstood. Thanks.
"
84197311,1336,hachikuji,2016-10-20T00:58:45Z,"nitpick: should 'member' be plural? Also, a little surprising we don't have a variable for the groupId. Maybe we could add one above to make these messages a little easier to read.
"
84197388,1336,hachikuji,2016-10-20T00:59:50Z,"Yeah, that's what I meant. Don't bother if it's a ton of additional work, but seems like a nice cleanup.
"
84198268,1336,vahidhashemian,2016-10-20T01:08:24Z,"Good suggestion.
"
84198564,1336,vahidhashemian,2016-10-20T01:12:50Z,"No problem, I'll give it try in the next patch.
"
84349940,1336,hachikuji,2016-10-20T18:42:27Z,"nit: shouldn't need parenthesis for most of these getters, I think. There are a bunch of these around the patch.
"
84350479,1336,hachikuji,2016-10-20T18:45:11Z,"nit: might be nice to add a `require` to validate that the group assignment is not empty.
"
84352489,1336,hachikuji,2016-10-20T18:55:19Z,"This check needs to be updated since `state` is now an `Optional`.
"
84352847,1336,hachikuji,2016-10-20T18:57:03Z,"This should probably be `foreach` since we don't need any the return type.
"
84363117,1336,hachikuji,2016-10-20T19:55:17Z,"nit: no need for `new`.
"
84363280,1336,hachikuji,2016-10-20T19:56:14Z,"nit: maybe this could be

``` scala
.sortBy(_.partition)
```
"
84363729,1336,hachikuji,2016-10-20T19:58:46Z,"nit: maybe this could be a `foreach`?
"
84364647,1336,hachikuji,2016-10-20T20:04:04Z,"nit: no need for `new`
"
84364950,1336,hachikuji,2016-10-20T20:06:07Z,"nit: unneeded `toList`.
"
84365278,1336,hachikuji,2016-10-20T20:08:04Z,"nit: unneeded import
"
84365476,1336,hachikuji,2016-10-20T20:09:08Z,"Could this be `consumers`. The `Summaries` suffix seems a tad verbose.
"
84366002,1336,hachikuji,2016-10-20T20:12:08Z,"nit: since we've converted most of these to use string interpolation, maybe we could do the same here? I noticed a couple others.
"
84369348,1336,vahidhashemian,2016-10-20T20:30:13Z,"Sure, sounds fair.
"
84369482,1336,vahidhashemian,2016-10-20T20:30:56Z,"I'll try to remove them. I see that their occurrences are beyond what's in this patch.
"
84379640,1336,vahidhashemian,2016-10-20T21:28:38Z,"You're right. Thanks. I'll fix this in the next patch.
"
84388805,1336,vahidhashemian,2016-10-20T22:28:24Z,"Not sure if I follow this one?
"
84390074,1336,vahidhashemian,2016-10-20T22:38:02Z,"Never mind. I think you mean something like

```
memberId.foreach(id => groupMemberIds.filterNot(_ == id))
```
"
84391557,1336,hachikuji,2016-10-20T22:49:38Z,"Yeah, something like that. Kind of hard to tell sometimes when you're trying to get a little too cute, but using `map` and `foreach` seems to generally be preferred over an explicit check for `isDefined` or `isEmpty`.
"
84391755,1336,hachikuji,2016-10-20T22:51:12Z,"The general rule is to omit the parenthesis if the function does not mutate any state. No need to catch all such cases, it just stood out a bit on this line.
"
84513550,1336,hachikuji,2016-10-21T16:45:27Z,"I may have missed it, but how do we know this `get` is safe? Should we match using the option instead?
"
84520691,1336,vahidhashemian,2016-10-21T17:53:51Z,"I believe for the ZooKeeper-based consumers the `describeGroup` returns either `None` for `assignments` or some array (and the array cannot be empty because as soon as a group starts there is an assignment row). Therefore, this line would not be reached in that case. And for Java based consumer groups as far as I can tell `state` always has some value, and it cannot be `None`. So this line would be safe to call.

Having said that I'm ok with checking `state` here instead of `state.get`.

One more question. I tried creating a new topic and starting an old consumer consuming from that topic belonging to a new consumer group. When I tried `describe` with this patch I get this output for a few seconds, and then the error vanishes as the initializations are done. Are we OK with this behavior?

```
Note: This will only show information about consumers that use ZooKeeper (not those using the Java consumer API).

Error: Could not fetch offset from zookeeper for group 'cgroup3' partition '[test2,0]' due to missing offset data in zookeeper.
Consumer group: cgroup3

TOPIC    PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG    MEMBER-ID  ...
test2    0                                          0      cgroup3_kafka-1477072199030-5c817e19   ...
```
"
84525026,1336,vahidhashemian,2016-10-21T18:36:18Z,"This is the refactored code for `state` check. Please let me know if you see issues with it. Thanks. 
"
84536951,1336,hachikuji,2016-10-21T20:15:14Z,"nit: you can use `nonEmpty`.
"
84536970,1336,hachikuji,2016-10-21T20:15:23Z,"Couple naming suggestions:
1. `MEMBER-ID` -> `CONSUMER-ID`
2. `MEMBER-HOST` -> `HOST`

What do you think? Also, it seems neither of these options are available for the old consumer. Maybe it would reduce the noise if we leave them out of the output in that case? Not sure if there's a clean way to do that though.
"
84539540,1336,vahidhashemian,2016-10-21T20:33:45Z,"I can make the name changes. Regarding availability for old consumer, it's actually `MEMBER_HOST` and `CLIENT_ID` (the last two) that will be blank in the output.

I think I can leverage the `node` variable (that I introduced to report the coordinator broker id for new consumer option) and based on that decide whether the last two column should be printed or no. I'll submit an update shortly and you can take a look and let me know what you think. Thanks.
"
84542689,1336,vahidhashemian,2016-10-21T20:54:10Z,"I don't feel very happy about this repeating check. To me it was either this or totally separate print statements for old and new consumers in the `match` block of line 114 above. What do you think?
"
84543875,1336,hachikuji,2016-10-21T21:00:56Z,"It might be a little less annoying if you put the result in a val (e.g. `useNewConsumer`). Maybe you could even pass `opts.useOldConsumer` into this function to avoid the need to check the coordinator.
"
84544537,1336,vahidhashemian,2016-10-21T21:05:24Z,"BTW, I'm thinking printing ""-"" instead of """" when data is not available or does not apply would look better and more readable (especially when there are multiple rows with blank columns). Compare

```
TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID
test1                          0          0               0               0          consumer-1-7913cdd0-e177-409f-86fa-3fcf92ddd926
                                                                                     consumer-1-b2f82fe9-6c1a-4928-8e6d-ccfab0906788
```

with

```
TOPIC                          PARTITION  CURRENT-OFFSET  LOG-END-OFFSET  LAG        CONSUMER-ID
test1                          0          0               0               0          consumer-1-7913cdd0-e177-409f-86fa-3fcf92ddd926
-                              -          -               -               -          consumer-1-b2f82fe9-6c1a-4928-8e6d-ccfab0906788
```
"
84545114,1336,hachikuji,2016-10-21T21:09:20Z,"Good idea.
"
84545242,1336,vahidhashemian,2016-10-21T21:10:22Z,"Passing `opts.useOldConsumer` sounds good; but that doesn't avoid the need for this repeating `if` block. Or I misunderstood?
"
84545937,1336,hachikuji,2016-10-21T21:15:37Z,"Yeah, you still need it. Seems not too bad to me. You could move it outside the loop, but then you'd need to repeat the loop in both arms of the `if`, which seems worse.
"
84546160,1336,vahidhashemian,2016-10-21T21:17:00Z,"Right, I'll keep it as is then. Thanks.
"
84551578,1336,hachikuji,2016-10-21T21:58:36Z,"Really sorry to keep adding comments... Did we print this before? It seems unnecessary given that we require the group to be passed on the command line. One downside to having this line and the one below is that it's a little tougher to parse the output. On the other hand, having a way to get the coordinator seems useful for debugging. I wonder if it would make sense to add that to the `--list` option in a separate patch. What do you think?
"
84552130,1336,vahidhashemian,2016-10-21T22:03:39Z,"That's fine with me. I added these two lines (along with [this](https://github.com/vahidhashemian/kafka/blob/edb983f2614d4455b12628a698ea701b469dfab2/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L58) and [this](https://github.com/vahidhashemian/kafka/blob/edb983f2614d4455b12628a698ea701b469dfab2/core/src/main/scala/kafka/admin/ConsumerGroupCommand.scala#L61)) because there was a request on the corresponding JIRA asking for some clarification on the printed output.

I'm fine with removing them and have the `--list` option report the coordinator broker id.
Should I also leave out the other two lines I linked to above?
"
84553085,1336,hachikuji,2016-10-21T22:12:24Z,"Looked back at the JIRA. Maybe you could print the warning messages to stderr? I also saw Jun's comment about printing the coordinator. It's a little odd to print it as a column in the table as you suggested (since it will be the same for all members), but that might be a better choice. I'd be ok with either doing that or adding it to the `--list` option separately. The latter might be a little nicer since this already has a lot of columns and I think we should have the coordinator information already when using `--list`.
"
84553913,1336,vahidhashemian,2016-10-21T22:18:00Z,"I also think using `--list` to report the coordinator id is better for the same reasons you mentioned. I'll submit another update shortly, and open a JIRA for reporting the coordinator.
"
108825282,2744,junrao,2017-03-30T01:39:13Z,"Hmm, not sure if this is accurate to capture the network thread utilization. What we are recording is essentially the responseSendTime, which includes the time for waiting for the socket to be writable. That portion of the time actually doesn't tie up the network threads and shouldn't be accounted for in the request time. Also, this seems to only cover the network thread time for sending responses, not for reading requests (which could be significant for produce requests).

I was thinking that we may need to do the following. In Selector.pollSelectionKeys(), we will measure the time spent for reading/writing each KafkaChannel and propagate the time back to the caller. Then, we can account for both request/response time in network threads in SocketServer."
108825299,2744,junrao,2017-03-30T01:39:22Z,QuotaThreadPercentDefault => QuotaRequestPercentDefault ?
108825310,2744,junrao,2017-03-30T01:39:26Z,It seems that toInt is redundant.
108825313,2744,junrao,2017-03-30T01:39:29Z,It seems that toLong is redundant.
108825331,2744,junrao,2017-03-30T01:39:39Z,Do we need to make this protected? It doesn't seems to be customized in the subclass and it doesn't seem that we can change it to anything other than Rate().
108825337,2744,junrao,2017-03-30T01:39:42Z,unused import
108825350,2744,junrao,2017-03-30T01:39:48Z,utilizationThrottleTimeMs => requestThrottleTimeMs?
108825356,2744,junrao,2017-03-30T01:39:52Z,remove space after trace( ?
108825402,2744,junrao,2017-03-30T01:40:17Z,"Hmm, I am wondering if we really need to subtract bandwidthThrottleTimsMs from the throttle time. While the response is delayed for bandwidthThrottleTimsMs, some time has passed, which should bring down the value of the metric when we check the request quota. Then, naturally, this request will be delayed less for request quota violation."
108826148,2744,junrao,2017-03-30T01:49:43Z,We throttle LeaderAndIsrRequest if it's unauthorized. Should we do the same thing here?
108916491,2744,rajinisivaram,2017-03-30T12:49:37Z,Done.
108916544,2744,rajinisivaram,2017-03-30T12:49:54Z,Removed.
108916572,2744,rajinisivaram,2017-03-30T12:50:00Z,Removed.
108916610,2744,rajinisivaram,2017-03-30T12:50:13Z,Reverted.
108916623,2744,rajinisivaram,2017-03-30T12:50:18Z,Removed.
108917372,2744,rajinisivaram,2017-03-30T12:53:19Z,In this case `authorizeClusterAction` method throws an exception and the response is sent from the exception handler in `handle`. All responses sent from the exception handler are throttled.
108917401,2744,rajinisivaram,2017-03-30T12:53:29Z,Done.
108917419,2744,rajinisivaram,2017-03-30T12:53:34Z,Done.
108917772,2744,rajinisivaram,2017-03-30T12:55:10Z,"Yes, I wasn't sure which way to go. Either way, I think the throttle times will correct themselves over time. I have removed the subtraction."
108919055,2744,rajinisivaram,2017-03-30T13:00:59Z,"@junrao Thank you, I have reworked the code for recording network thread time as you suggested.  At the moment, SSL/SASL handshake time is included in the time for the first request. Is that reasonable? I can clear the time after authentication if it is confusing."
108923119,2744,rajinisivaram,2017-03-30T13:19:36Z,"Sorry, had to revert that since `toInt` was required for newer versions of scala."
109046976,2744,junrao,2017-03-30T21:53:07Z,"Could we add a comment somewhere so that people know that if they want to add a non-internal request in the future, they would need to include the throttle time field?"
109047035,2744,junrao,2017-03-30T21:53:31Z,This takes care of the time spent on receiving requests. We will need to do the same thing to track the time on sending responses.
109049180,2744,junrao,2017-03-30T22:05:52Z,"The approach in the patch works. But one issue is that the implementor of all future requests will have to deal with throttling the responses directly.

Another approach is to do the throttling early in handle(). If a request needs to be delayed, we first throttle it and then hand it over to the specific request handler. This way, we just need to implement the request throttling logic in one place and all future requests don't have to be aware of it. We probably need to mark whether a request is at the cluster level so that we can throttle unauthorized internal requests too. Also, not sure if there is an easier way for the request handler thread to pick up the requests after throttling is done. If there is, this may be a simpler approach?"
109049691,2744,junrao,2017-03-30T22:08:59Z,That seems fine for now. Perhaps we could add a comment in case we need to revisit in the future?
109146033,2744,rajinisivaram,2017-03-31T12:09:54Z,I have added unit test in `ApiKeysTest` that checks that all responses except those explicitly excluded contain a field named `throttle_time_ms`. There is a comment in the test as well to ensure that new requests either contain the field or are manually added to the test's exclusion list.
109146742,2744,rajinisivaram,2017-03-31T12:15:02Z,"The time is accumulated in `Selector` and includes the full time spent for each channel in `pollSelectionKeys`. For each request, the accumulated time is used and the value is reset. This time includes the time for write of the previous response and the time for read of the current request. I have added a comment in the code."
109149032,2744,rajinisivaram,2017-03-31T12:30:54Z,"@junrao Thank you for the review. 

It will be nice to handle throttling in a single place. However, handling of all new requests need to be aware of throttling, so that they add the throttling time to the response. The bigger issue is the exclusions. We need to authorize in a central place for `ClusterAction`. And worse, we need to handle `Produce` differently since we don't want to throttle until after the request is processed and the memory can be released. We probably want to throttle later for `Fetch` as well since we are relying on the bandwidth throttle time to reduce the request throttle time. We will need to record two more timestamps to take into account time spent before throttle in some cases and after throttle in others. And as you mentioned, we still want the request to be processed after throttling on the request handler thread. Taking all that into account, I am not  sure it is worthwhile to restructure the code to centralize the throttling logic.

I have updated `RequestQuotaTest` to ensure that unauthorized requests of all types are throttled. And also to check that all requests not explicitly excluded are throttled and return throttle time in response. This should catch any missing throttling in new requests.

Let me know if this is sufficient or whether I should try out the centralized approach."
109149244,2744,rajinisivaram,2017-03-31T12:32:32Z,Added comment in `SocketServer`.
109189436,2744,rajinisivaram,2017-03-31T15:42:41Z,"After writing the comment above, I realized that it sounds rather odd. So I have updated the code to record network thread time when request metrics are updated, so that receive+send are recorded together. This does also ensure that the last network time on each connection is recorded and it would work even if requests on a connection use different clientIds. Throttling is still performed only on subsequent requests."
110187347,2744,ijuma,2017-04-06T15:08:36Z,Why is this an `AtomicLong` instead of a plain long?
110188076,2744,ijuma,2017-04-06T15:11:24Z,"Hmm, it seems a bit unfortunate that we need to do a `System.nanoTime` per selection key. Have we done any measurements on the overhead?"
110188616,2744,ijuma,2017-04-06T15:13:33Z,I think we'd want to override `parseResponse` for `API_VERSIONS` only.
110189171,2744,ijuma,2017-04-06T15:15:19Z,I wonder if there's a way to add this to `ResponseHeader` in a compatible way. It seems a bit annoying to have to add that to every response.
110189630,2744,ijuma,2017-04-06T15:17:01Z,"Since we use `Ms`, should we not be using `Ns` instead of `Nanos`?"
110190164,2744,ijuma,2017-04-06T15:19:12Z,"Nit: the logic inside this catch has become a bit complicated. Can we perhaps extract methods? Also, something I was thinking about recently is that methods like `getErrorResponse` could throw an exception due to bugs. It would be nice for us not to leak connections in such cases."
110190689,2744,ijuma,2017-04-06T15:20:48Z,"If we could somehow add the throttle time in the response header, it would be easier to handle it in a generic way for the typical cases."
110422348,2744,rajinisivaram,2017-04-07T15:52:11Z,"@ijuma There are a few requests which don't have throttle time (ControlledShutdown, StopReplica etc. used for inter-broker and not by producer/consumer). So it made sense to add throttle time to individual requests. Error codes seem to be handled this way. But my main concern was compatibility. Even if we looked at request version to determine the response header version, there is still the issue of `ApiVersionsResponse` which needs to be handled. According to KIP-35, `ApiVersionResponse.ErrorCode is guaranteed to be the first int16 of the response for all future versions of ApiVersionRequest`. I didn't want to break that assumption."
110422668,2744,rajinisivaram,2017-04-07T15:53:33Z,I couldn't see any way around it. I will kick off system test runs to see the impact.
110422700,2744,rajinisivaram,2017-04-07T15:53:42Z,Done.
110423058,2744,rajinisivaram,2017-04-07T15:55:21Z,"Both `Ns` and `Nanos` are already used in the code. I chose `Nanos` everywhere in the PR since it stands out better from `Ms`, especially since I was changing some measurements from millis to nanos. But I am ok with switching to `Ns` if that is preferable."
110423731,2744,rajinisivaram,2017-04-07T15:58:29Z,"Have moved to a method. I think errors are propagated and logged. We won't close acks=0 connections if `getErrorResponse` threw an exception, but perhaps that is ok since that would still not be a leaked connection?"
110425748,2744,rajinisivaram,2017-04-07T16:08:22Z,Because the value is updated on the network thread and read-and-reset on the request handler thread. I have added a comment.
110442824,2744,junrao,2017-04-07T17:39:04Z,Our system test currently doesn't do perf validation well. It would be useful to just run ProducerPerformance and ConsumerPerformance and see if there is any noticeable degradation.
110478783,2744,rajinisivaram,2017-04-07T20:42:21Z,"@junrao @ijuma I ran `ProducerPerformance` and `ConsumerPerformance` on my laptop and didn't see any noticeable difference. This is the throughput in MB/s (average of three runs):

Test     (message size)    | trunk             |  with PR
-------------------------|--------------|------------
Producer (100 bytes)      | 158.68          | 160.18 
Producer (1000 bytes)    | 355.43         | 350.36
Consumer (100 bytes)    | 376.20          | 378.52 
Consumer (1000 bytes)  | 559.45          | 559.45 

   "
112594508,2744,junrao,2017-04-21T01:43:24Z,"Perhaps we should only fall back to version 0 of the request if the error is UNSUPPORTED_VERSION? For other kinds of error, just disconnect?"
112594514,2744,junrao,2017-04-21T01:43:29Z,"Instead of returning AtomicLong, could we just reset to 0 and return a long?"
112594523,2744,junrao,2017-04-21T01:43:37Z,"Hmm, not sure why we need this. It seems that the client should always use the requested version to parse the response of API_VERSIONS?"
112594538,2744,junrao,2017-04-21T01:43:44Z,"This makes things a bit more complicated. I was thinking of the following. In updateRequestMetrics(), we remember networkThreadTime as previousNetworkThreadTime. In KafkaApis, we can just add previousNetworkThreadTime to the throttler. That way we don't need this callback. Will that be better?"
112594555,2744,junrao,2017-04-21T01:43:54Z,It seems that we should update the instance level localCompleteTimeNanos instead of a local one?
112594610,2744,junrao,2017-04-21T01:44:36Z,apiKey in the comment needs to be changed accordingly.
112594983,2744,junrao,2017-04-21T01:49:17Z,will subsequently used => will subsequently be used
112726629,2744,rajinisivaram,2017-04-21T16:23:31Z,Done.
112726925,2744,rajinisivaram,2017-04-21T16:24:55Z,AtomicLong is returned so that the the value can be updated from the I/O thread when a request is complete without propagating `KafkaChannel` to the request handling code.
112727184,2744,rajinisivaram,2017-04-21T16:26:18Z,"If client sends ApiVersionsRequest with a higher version that client supports, broker responds with a version 0 response that indicates unsupported version."
112729035,2744,rajinisivaram,2017-04-21T16:36:20Z,"Hmm... Network thread time needs to be accumulated against the (user, client-id) and needs to include the time for the sending the response. The callback avoids having to propagate (user, client-id)."
112729071,2744,rajinisivaram,2017-04-21T16:36:30Z,Done.
112729097,2744,rajinisivaram,2017-04-21T16:36:38Z,Done.
112729130,2744,rajinisivaram,2017-04-21T16:36:46Z,Done.
112791899,2744,junrao,2017-04-21T23:13:34Z,Could we consistently add newThrottleTimeField() as the first field?
112791907,2744,junrao,2017-04-21T23:13:41Z,"OFFSET_FOR_LEADER_EPOCH_RESPONSE is an inter broker request. So, we shouldn't add a throttle field."
112791941,2744,junrao,2017-04-21T23:14:08Z,"It might be useful to report all the time still as ms, but up to micro sec level accuracy now that we track with nanosec."
112791964,2744,junrao,2017-04-21T23:14:21Z,"For request quota, since we are collecting request time in nanosecs already, it will be useful to create a Rate with nanosec as the time unit. This will make the measurement more accurate."
112791985,2744,junrao,2017-04-21T23:14:32Z,"Hmm, in this case, we probably only want to throttle if the exception is related to authorization. For any other exceptions, we should send an error immediately?"
112791992,2744,junrao,2017-04-21T23:14:40Z,"Hmm, this can be a bit tricky. Fetch requests from the follower are considered internal and shouldn't be throttled."
112792013,2744,junrao,2017-04-21T23:14:57Z,"Hmm, when there is no data, the consumer will wait for the timeout. So, not sure if this is enough to trigger the throttling. We probably need to either set a low maxWait in consumer config and set the quota to be really low."
112792018,2744,junrao,2017-04-21T23:15:02Z,"Hmm, not sure where the test is."
112792036,2744,junrao,2017-04-21T23:15:18Z,"Why is the replicaId 5000? That indicates it's from a follower. Also, I am wondering if 100 maxWait is enough to trigger throttling."
112792083,2744,junrao,2017-04-21T23:15:48Z,"Got it. An alternative is to call networkThreadTimeNanos() in SocketServer.processNewResponses() and processCompletedSends(). Then we can just reset and return the value, which is easier to understand?"
112792089,2744,junrao,2017-04-21T23:15:53Z,Thanks for the explanation. Could we add that as comment in the code?
112792094,2744,junrao,2017-04-21T23:15:56Z,Got it. This is fine then.
112908803,2744,rajinisivaram,2017-04-24T10:06:26Z,"According to KIP-35,
> ApiVersionResponse.ErrorCode is guaranteed to be the first int16 of the response for all future versions of ApiVersionRequest and is to be used to indicate that the client's ApiVersionRequest with version X (greater than 0) is not supported by the broker and the client should revert to version 0.

The Java clients don't rely on this, but just in case some other clients do, I have left `error_code` as the first field for `ApiVersionsResponse`."
112908831,2744,rajinisivaram,2017-04-24T10:06:36Z,Fixed.
112909448,2744,rajinisivaram,2017-04-24T10:09:51Z,The yammer metrics `Histogram` class that tracks time only takes long and not double. Hence the millisecond value is used.
112910225,2744,rajinisivaram,2017-04-24T10:14:24Z,"For request quota, values are recorded as double, so even though they use millisecond as unit to be consistent, they have higher precision. Isn't that sufficient?"
112910635,2744,rajinisivaram,2017-04-24T10:16:49Z,"Yes, `authorizeClusterAction` takes care of throttling for unauthorized request and all other paths including error path goes through this `sendResponseExemptThrottle` which does not perform throttling."
112910724,2744,rajinisivaram,2017-04-24T10:17:22Z,"Oops, you are right. Fixed."
112911096,2744,rajinisivaram,2017-04-24T10:19:27Z,Consumers in this test are configured with `fetch.max.wait.ms=0`. The quota is set very small as well to trigger throttling.
112911132,2744,rajinisivaram,2017-04-24T10:19:36Z,"Oops, fixed."
112911223,2744,rajinisivaram,2017-04-24T10:20:09Z,Copy-paste error. Fixed and set maxWait to zero.
112911309,2744,rajinisivaram,2017-04-24T10:20:38Z,Done.
112911522,2744,ijuma,2017-04-24T10:21:59Z,I think the AtomicLong comment should be moved to the field.
112912615,2744,ijuma,2017-04-24T10:28:34Z,"Is there a reason why we don't replace the 4 lines above with:

```scala
    request.recordNetworkThreadTimeCallback.foreach(record => record(openOrClosingChannel.getAndResetNetworkThreadTimeNanos())
```"
112917077,2744,rajinisivaram,2017-04-24T10:55:44Z,Done.
112917164,2744,rajinisivaram,2017-04-24T10:56:08Z,"@ijuma Thank you, done."
112965720,2744,junrao,2017-04-24T14:46:06Z,"Ok, could we add a comment that error_code has to be the first field in ApiResponse?"
112965772,2744,junrao,2017-04-24T14:46:20Z,I was actually referring to line 169 where we log the time components in trace logging. It's useful to see more precise time there since sometimes the time may take less than 1ms.
112965811,2744,junrao,2017-04-24T14:46:30Z,Thanks. That should be enough then.
112969979,2744,junrao,2017-04-24T15:00:57Z,"Here, we throttle independent of the exception type. Perhaps, it's better to only engage in throttling if the exception is ClusterAuthorizationException?"
113048200,2744,rajinisivaram,2017-04-24T20:30:53Z,Fixed to throttle only for `ClusterAuthorizationException` for broker-only requests.
113595344,2744,junrao,2017-04-27T00:51:18Z,This is an inter-broker request as well and clusterAction should be true.
113596378,2744,junrao,2017-04-27T01:02:52Z,"This is an inter-broker request. So, no throttling needed."
113597310,2744,junrao,2017-04-27T01:13:52Z,Perhaps it's better to pass the networkThread time to request.updateMetrics() and call the recordNetworkThreadTimeCallback there?
113597451,2744,junrao,2017-04-27T01:15:46Z,"Hmm, we probably don't want to call recordNetworkThreadTimeNanos here since it will be called in processCompleteSends() again. Instead, it seems that we want to call recordNetworkThreadTimeNanos() in all places where we call request.updateRequestMetrics()."
113598851,2744,junrao,2017-04-27T01:31:19Z,It's actually read and reset by the broker's network thread.
113599869,2744,junrao,2017-04-27T01:42:13Z,"For the trace logging in line 177, could we report all the time still as ms, but up to micro sec level accuracy?"
113600212,2744,junrao,2017-04-27T01:45:55Z,"Given this, should we just remove line 65?"
113694991,2744,ijuma,2017-04-27T13:29:57Z,"Sorry for the delay on this one. So, one way to do this would be the following:

1. Add two fields to ResponseHeader _if_ the request version is higher than the version before this PR: error_code, throttle_time_ms (in this order)
2. Remove any top-level error_code in the new version of all affected responses
3. Remove throttle_time_ms from FetchResponse and ProduceResponse in the new version
4. throttle_time_ms is always 0 for requests that are never throttled

As part of this, we would also solve the issue that we currently have no way to return generic errors via the protocol. Since we are bumping the protocol version for so many requests, it seems like it would be a good opportunity to fix both issues at the same time. Is there a reason why this is a bad idea or would not work?"
113730628,2744,ijuma,2017-04-27T15:38:43Z,"Discussed this with @junrao. The main challenge with this option is having the top level error field for every response. This would probably affect a lot of code:

1. We would need to handle this top level error code everywhere.
2. A bunch of protocols that currently have a top level error code would no longer have them, so a bunch of code would have to be updated as well.

So, it doesn't seem appropriate to do this as part of this KIP."
113731419,2744,rajinisivaram,2017-04-27T15:42:10Z,@ijuma Thank you for looking into this.
113735595,2744,junrao,2017-04-27T15:58:07Z,Could we make this and a few other methods in the class private?
113736431,2744,junrao,2017-04-27T16:01:12Z,"Hmm, is the test added? I don't see the code for submitTest that checks the throttling field in the response. "
114028254,2744,rajinisivaram,2017-04-28T22:05:14Z,Fixed.
114028270,2744,rajinisivaram,2017-04-28T22:05:25Z,Fixed.
114028303,2744,rajinisivaram,2017-04-28T22:05:39Z,Fixed.
114028317,2744,rajinisivaram,2017-04-28T22:05:47Z,Done.
114028402,2744,rajinisivaram,2017-04-28T22:06:32Z,"Yes, you are right. Replaced with `long` and updated comment."
114028417,2744,rajinisivaram,2017-04-28T22:06:39Z,Done.
114028438,2744,rajinisivaram,2017-04-28T22:06:52Z,Done.
114028518,2744,rajinisivaram,2017-04-28T22:07:28Z,"Sorry, had forgotten the tests, fixed now."
114028539,2744,rajinisivaram,2017-04-28T22:07:37Z,Done.
511148601,9487,wcarlson5,2020-10-23T20:48:43Z,This will call closeToError but I am testing if that has a problem. So far it does not
511149017,9487,wcarlson5,2020-10-23T20:49:47Z,moved into stream thread because of a concurrent operation exception that appeared
512320447,9487,wcarlson5,2020-10-26T23:03:45Z,Method was a few lines too long
514527605,9487,lct45,2020-10-29T19:57:51Z,Is this spacing on purpose?
514531597,9487,lct45,2020-10-29T20:03:00Z,Is this section going to be re-added after the other thread handling stuff gets figured out?
514535353,9487,lct45,2020-10-29T20:07:44Z,Supposed to be here?
514535737,9487,lct45,2020-10-29T20:08:33Z,two new lines in a row
514536366,9487,lct45,2020-10-29T20:09:41Z,extra line
514536509,9487,lct45,2020-10-29T20:09:56Z,extra line (: 
514538306,9487,lct45,2020-10-29T20:13:34Z,extra line
514540200,9487,lct45,2020-10-29T20:17:02Z,line!
514566700,9487,wcarlson5,2020-10-29T21:05:19Z,It will. I don't know if we should merge as comment or just add it later
514567028,9487,wcarlson5,2020-10-29T21:05:43Z,Same as the other use in KS
516984927,9487,vvcephei,2020-11-03T22:10:24Z,"```suggestion
     * Set the handler invoked when an {@link StreamsConfig#NUM_STREAM_THREADS_CONFIG internal thread}
```"
516994424,9487,vvcephei,2020-11-03T22:32:35Z,"```suggestion
     * @param streamsUncaughtExceptionHandler the uncaught exception handler of type {@link StreamsUncaughtExceptionHandler} for all internal threads
```

In L389, we say that we throw an exception if the handler is null, which sounds like a more reasonable API to me."
516995632,9487,vvcephei,2020-11-03T22:35:32Z,What's up with the `@NotNull` on this line? I don't think I've seen that before.
516996830,9487,vvcephei,2020-11-03T22:38:42Z,"It's normally kinda weird to merge commented-out code. I'd either delete it or instead have a todo, like `// TODO KAFKA-XXXX: add case REPLACE_STREAM_THREAD once KIP-??? is implemented`, where `KAFKA-XXXX` is a follow-up ticket you create to implement this feature."
516998180,9487,vvcephei,2020-11-03T22:41:56Z,"```suggestion
                log.error(""Encountered the following exception during processing "" +
                        ""and the registered exception handler opted to "" + action + "". The streams client is going to shut down now. "", e);
```

Just a little extra information, so we don't always have to pull up this code block to remember what exact response action this message corresponds to."
517008916,9487,vvcephei,2020-11-03T23:10:01Z,"```suggestion
                    log.error(""This option requires running threads to shut down the application,"" +
                            ""but the uncaught exception was an Error, which means this runtime is no longer in a well-defined state. Attempting to send the shutdown command anyway."", e);
```"
517009538,9487,vvcephei,2020-11-03T23:11:46Z,"```suggestion
            log.info(""Can not transition to error from state "" + state());
```

Didn't follow the prior message. Is this what you meant?"
517009778,9487,vvcephei,2020-11-03T23:12:30Z,"```suggestion
            log.info(""Transitioning to ERROR state"");
```

Similar confusion here..."
517012283,9487,vvcephei,2020-11-03T23:20:08Z,"This doesn't look like an ""error"". At best it's a ""warn"" log, but only if we think that this combination definitely looks like a misconfiguration. Even then, why wouldn't we check for the misconfiguration in KafkaStreams, since both the new and old handlers would be set over there?"
517012839,9487,vvcephei,2020-11-03T23:21:52Z,"```suggestion
            if (this.streamsUncaughtExceptionHandler.handle(e) = StreamsUncaughtExceptionHandler.StreamThreadExceptionResponse.SHUTDOWN_APPLICATION) {
                log.warn(""Exception in global stream thread cause the application to attempt to shutdown."" +
                        "" This action will succeed only if there is at least one StreamThread running on ths client"");
            }
```

This looked a bit off..."
517013547,9487,vvcephei,2020-11-03T23:24:09Z,It doesn't look like this needs to be shared outside of this thread. It seems like it just needs to be shared between the StreamThread and its Consumer?
517464968,9487,cadonna,2020-11-04T16:19:46Z,Could you please also add the needed changes to system test `streams_upgrade_test.py::StreamsUpgradeTest.test_version_probing_upgrade` to this PR.
517474129,9487,cadonna,2020-11-04T16:32:17Z,"I guess this should be 2.8.0, shouldn't it?"
517481718,9487,wcarlson5,2020-11-04T16:43:03Z,I don't remember putting it there so it was probably a mistake
517485257,9487,wcarlson5,2020-11-04T16:48:10Z,That works
517486135,9487,cadonna,2020-11-04T16:49:27Z,I would also remove the commented-out code.
517505173,9487,cadonna,2020-11-04T17:18:01Z,Wouldn't it also be possible to start a shutdown thread here which closes the client without timeout? I think the other shutdown thread in close is rather useless (or I do simply not get its value).
517507950,9487,cadonna,2020-11-04T17:22:26Z,"IMO, it would be better to extract code to methods instead of removing some lines. "
517543638,9487,ableegoldman,2020-11-04T18:22:10Z,Can you also leave a comment here reminding us to fix the version probing system test whenever this protocol number is bumped? Since we apparently always forget
517576758,9487,wcarlson5,2020-11-04T19:20:51Z,I think it is simpler to check in the Stream thread because we don't in KafkaStreams if the handlers have been set so we would have to check the stream thread a global thread so it would be much easier to just check in the thread. I do agree that it should be bumped down to warn through.
517621757,9487,wcarlson5,2020-11-04T20:47:05Z,You are right it seems that it is not necessary 
517627843,9487,wcarlson5,2020-11-04T20:59:16Z,"thanks for the reminder. I think I I under stood the test ad incrementing to the next version, as the version is now 9"
517909467,9487,cadonna,2020-11-05T09:32:23Z,Is this comment correct? In this code path we do NOT check that all threads have been stopped. 
517910538,9487,cadonna,2020-11-05T09:34:07Z,Could you also remove the commented-out code here.
517915038,9487,cadonna,2020-11-05T09:41:15Z,"```suggestion
        properties  = mkObjectProperties(
            mkMap(
                mkEntry(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, CLUSTER.bootstrapServers()),
                mkEntry(StreamsConfig.APPLICATION_ID_CONFIG, appId),
                mkEntry(StreamsConfig.STATE_DIR_CONFIG, TestUtils.tempDirectory().getPath()),
                mkEntry(StreamsConfig.NUM_STANDBY_REPLICAS_CONFIG, ""5""),
                mkEntry(StreamsConfig.ACCEPTABLE_RECOVERY_LAG_CONFIG, ""6""),
                mkEntry(StreamsConfig.MAX_WARMUP_REPLICAS_CONFIG, ""7""),
                mkEntry(StreamsConfig.PROBING_REBALANCE_INTERVAL_MS_CONFIG, ""480000""),
                mkEntry(StreamsConfig.NUM_STREAM_THREADS_CONFIG, 2),
                mkEntry(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.StringSerde.class),
                mkEntry(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.StringSerde.class)
            )
        );
```"
517916345,9487,cadonna,2020-11-05T09:43:22Z,The name is a bit ambiguous. I would go for `StreamsUncaughtExceptionHandlerIntegrationTest`
517993498,9487,cadonna,2020-11-05T11:51:47Z,"What is the benefit of using a latch versus simply sleeping here? 
Actually, you should use `StreamsTestUtils.startKafkaStreamsAndWaitForRunningState()` to avoid flakiness coming from the Kafka Streams client not being in state RUNNING before the verifications."
517995934,9487,cadonna,2020-11-05T11:55:58Z,"```suggestion
            log.error(""A Kafka Streams client in this Kafka Streams application is requesting to shutdown"");
```
An application is actually a group of Kafka Streams clients (or instances)."
518008710,9487,cadonna,2020-11-05T12:19:08Z,You could wait for this flag to become true with `TestUtils.waitForCondition()` before you verify the other criteria.
518010780,9487,cadonna,2020-11-05T12:23:05Z,Why do clean the state twice?
518014119,9487,cadonna,2020-11-05T12:29:10Z,Why do you need to set all these properties?
518021251,9487,cadonna,2020-11-05T12:41:57Z,I would remove these comments. 
518033723,9487,cadonna,2020-11-05T13:03:05Z,"I had a hard time to understand this. We write just one record to the topic, but we end up processing two records. This is true, because we use two stream threads and there is no commit between the processing of the record of the first stream thread and the processing of the second stream thread. Why do you use two stream threads here?"
518035301,9487,cadonna,2020-11-05T13:05:41Z,Most of the above comments also apply to the other tests.
518039024,9487,cadonna,2020-11-05T13:11:58Z,"Why are those fields all package-private instead of private? 
We usually define string constants as `private static final String IDEMPOTENT_TOPIC = ""idempotentTopic""`."
518042865,9487,cadonna,2020-11-05T13:18:06Z,I do not understand the motivation behind this topic. Could you clarify? 
518044472,9487,cadonna,2020-11-05T13:20:37Z,Unit tests for this case are missing.
518263368,9487,wcarlson5,2020-11-05T18:14:44Z,yes
518263937,9487,wcarlson5,2020-11-05T18:15:40Z,It might be but I do not think that it is necessary
518264927,9487,wcarlson5,2020-11-05T18:17:21Z,I don't think we actually need it either way so I will just remove it
518265803,9487,wcarlson5,2020-11-05T18:18:47Z,sure
518267956,9487,wcarlson5,2020-11-05T18:22:27Z,"Thats a good idea, I didn't see that option"
518269493,9487,cadonna,2020-11-05T18:25:11Z,"Why not? It would be much cleaner. We would close all stuff like admin client and the metrics, remove the client metrics and set the state to NOT_RUNNING which is not necessarily done with timeout zero (probably not because of the death lock). Additionally, we would get an nice info debug saying `Streams client stopped completely` instead of `Streams client cannot stop completely within the timeout`. ;-)"
518271215,9487,wcarlson5,2020-11-05T18:28:06Z,good idea
518271918,9487,wcarlson5,2020-11-05T18:29:17Z,we probably don't need all of them. I will trim them down
518274359,9487,wcarlson5,2020-11-05T18:33:24Z,I use 2 threads there to make sure the old behavior is being followed. Just one thread dies and then the next thread is tries. The second thread makes sure that the new path is not closing the client unintentionally.
518283539,9487,cadonna,2020-11-05T18:49:19Z,My last comment is not true! Sorry! Everything alright!
518326776,9487,wcarlson5,2020-11-05T19:55:10Z,Actually the latch ensures the rebalance gets processed
518335631,9487,wcarlson5,2020-11-05T20:11:16Z,it can be removed
518371511,9487,wcarlson5,2020-11-05T21:17:19Z,added unit test
518371722,9487,wcarlson5,2020-11-05T21:17:33Z,good questions
518478117,9487,ableegoldman,2020-11-06T01:55:29Z,"I had a little trouble following the `Handler` class. Some trivial things -- eg the handler in the StreamThread is named `streamsUncaughtExceptionHandler` but it's actually _not_ a `StreamsUncaughtExceptionHandler`. Also the usage of the return value; IIUC it's supposed to indicate whether to use the new handler or fall back on the old one. To me it sounds like if `handle` returns `true` that means we should handle it, ie we should _not_ rethrow the exception, but this looks like the opposite of what we do now. Honestly either interpretation is ok with me, as long as it's documented somewhere

Do we really need the `Handler` in the first place though? It's already pretty confusing that we have to deal with two types of handlers (old and new) so I'd prefer not to add a third unless it's really necessary. It seems like we can just inline the logic of whether to invoke the new handler or rethrow the exception, which would also clear up the confusion around the meaning of the return value. But I might be missing something here -- WDYT?"
518479524,9487,ableegoldman,2020-11-06T01:59:50Z,Seems like we can just pass in a Runnable with `KafkaStreams::closeToError` instead of adding a whole `ShutdownErrorHook` functional interface
518481887,9487,ableegoldman,2020-11-06T02:07:58Z,"```suggestion
                ""All clients in this app will now begin to shutdown"");
```"
518482832,9487,ableegoldman,2020-11-06T02:11:28Z,"```suggestion
                    log.error(""Exception in global thread caused the application to attempt to shutdown."" +
                            "" This action will succeed only if there is at least one StreamThread running on this client."" +
                            "" Currently there are no running threads so will now close the client."");
```"
518483194,9487,ableegoldman,2020-11-06T02:12:42Z,Should this be logged at error?
518484271,9487,ableegoldman,2020-11-06T02:16:48Z,"Looks like we call `setState(ERROR)` three times in this method, is that intentional?"
518485280,9487,ableegoldman,2020-11-06T02:20:24Z,"It probably doesn't matter too much since `handleRebalanceComplete` doesn't do anything that important at the mometn, but it seems like we should call it before shutting down, not after."
518485749,9487,ableegoldman,2020-11-06T02:22:02Z,This should probably stay `final` so we don't accidentally change it ever
518488577,9487,ableegoldman,2020-11-06T02:32:29Z,"This cast makes me kind of uncomfortable...either the `assignmentErrorCode` that we have in the AssignmentInfo is conceptually the same as the one we're adding to the SubscriptionInfo (in which case it should be the same type), or it's not the same, in which case we should use a different variable to track it.

Personally I think it's probably simpler to keep them the same, and just add an `int` errorCode field to the Subscription instead of a `byte` shutdownRequested field. But it's your choice"
518489261,9487,ableegoldman,2020-11-06T02:35:01Z,"I think we should mirror the `errorCode` in the AssignmentInfo here, both in terms of naming and type. If we're going to use the same AssignorError for both, then they should really be the same. And we may want to send other kinds of error codes in the subscription going forward: better to just encode a single `int` than a separate `byte` for every logical error code. I don't think we'll notice the extra three bytes since Subscriptions aren't sent that frequently"
518837250,9487,wcarlson5,2020-11-06T15:46:47Z,We could do the logic inline how ever this does make it slightly simpler. Also we only expose the `streamsUncaughtExceptionHandler` to the user and @vvcephei had a problem with the wrapping that again with the same type. So we introduced a wrapper class. if we renamed it from `Handler` to `streamsUncaughtExceptionHandlerWrapper` would that make it more clear?
518838421,9487,wcarlson5,2020-11-06T15:48:45Z,In the normal close method the corresponding log is also info. As multiple thread will be calling this at once I would rather not flood the logs with error unnecessarily.
518838586,9487,wcarlson5,2020-11-06T15:49:01Z,"No, I hadn't seen that"
518840121,9487,wcarlson5,2020-11-06T15:51:21Z,"We can do that, it doesn't seem make difference which order it is called. However if it is not called it will get stuck continually rebalancing. We return because setting the state to partitions assigned will cause an error"
518840602,9487,wcarlson5,2020-11-06T15:52:00Z,I was changing it intentionally but I think I can get away with not
518842747,9487,wcarlson5,2020-11-06T15:55:21Z,Yes we can
518850419,9487,wcarlson5,2020-11-06T16:08:05Z,"I think I agree on the name, I am not sure about the type. We should be able to fit thousands of different error code into the byte so we should not run out of space. The reason the errorCode. is an integer in the first place is because there is not `AtomicByte` that I know of."
518913514,9487,ableegoldman,2020-11-06T17:57:36Z,"Gotcha. In that case maybe we shouldn't log anything here at all? Or just reword it to clarify that this is expected (eg `""Skipping shutdown since we are already in ERROR""`) since ""Can not transition..."" kind of sounds like something went wrong"
518938852,9487,wcarlson5,2020-11-06T18:47:09Z,"That is a good idea, Ill change the log"
520077048,9487,ableegoldman,2020-11-09T19:47:31Z,"I'm not really worried that we'd run out of space, I just think it sends a signal that the Assignment and Subscription error codes are semantically distinct and don't refer to the same underlying concept. So it seems better to go with the simpler approach than over-optimize to save an occasional three bytes "
520104884,9487,wcarlson5,2020-11-09T20:38:03Z,"https://github.com/apache/kafka/pull/9273#discussion_r486597512
I originally had it at int32, but @vvcephei suggested int16, now it is int8.

would you be good with int16 or do you think int32 is the way?"
522596527,9487,ableegoldman,2020-11-13T03:51:22Z,This wording is a little difficult to parse
522597486,9487,ableegoldman,2020-11-13T03:55:34Z,"Just curious, what's the motivation for doing it like this vs just immediately throwing the exception?"
522598008,9487,ableegoldman,2020-11-13T03:57:50Z,nit: parameters unaligned
522598142,9487,ableegoldman,2020-11-13T03:58:34Z,That's a lot of line breaks  
522598707,9487,ableegoldman,2020-11-13T04:00:50Z,Is everything after this line the same as the code in the regular `close()`? Might be a good idea to move it to a separate method so we don't accidentally forget to update one of them if we ever need to make changes to how we close
522613334,9487,ableegoldman,2020-11-13T04:26:45Z,"It seems like we shouldn't both handle the exception in the catch block AND shut down the client in the finally block. If the new handler is used, then we've already shut down the client or possibly started to shut down the whole application. It's tricky, though, because if the old handler was used then we _do_ want to make sure that the global thread is all cleaned up before rethrowing the exception. 
Seems like we need some way to detect whether we're using the old or the new handler after all. But I think you can do it without too many changes, since basically the rule is ""if they set a new handler at all OR didn't set either handler, then use the new one"". So maybe you can just make the `StreamsUncaughtExceptionHandler` a local field instead of the `Consumer<>`, and leave it as `null` to indicate that the old handler should be used and therefore this shutdown logic should be invoked. Otherwise just call the new handler directly. Or something like that...you'd know this code better than me, WDYT?"
522616515,9487,ableegoldman,2020-11-13T04:30:37Z,"Hmm...this one seems like it should be a fatal error, so is it safe to just pass it along to the user and let them potentially just keep replacing the thread? (I know that option doesn't exist yet, but it will). There are some instances where we interpret errors as permanently fatal and choose to shut down the entire application, eg some errors during assignment. Should we do the same here? cc @abbccdda or @mjsax for more context on this error"
522622229,9487,ableegoldman,2020-11-13T04:37:49Z,I think we should add the `errorCode` parameter to the existing constructor rather than add a new one. It shouldn't be possible to construct a version 9 subscription that doesn't have an `errorCode`
522622882,9487,ableegoldman,2020-11-13T04:38:35Z,"Nice, thanks for the comment. Btw anytime we bump this protocol version we should add the corresponding unit tests, eg `SubscriptionInfoTest#shouldEncodeAndDecodeVersion8()`"
522626697,9487,ableegoldman,2020-11-13T04:43:11Z,"Does the comment relate to the `@deprecation` suppression? Either way this probably makes more sense as a comment on the PR than in the code.  Given how bad we are about updating comments, I'd try to avoid anything that describes a change and reserve code comments for describing what's currently going on (or better yet, ""why"")"
522627296,9487,ableegoldman,2020-11-13T04:43:39Z,"same here, what is the comment referring to? Also what does it mean for a test to be deprecated  "
522635059,9487,ableegoldman,2020-11-13T04:49:40Z,ditto here
522639947,9487,ableegoldman,2020-11-13T04:55:41Z,"Is the latch ever being counted down anywhere? You might want to take a look at some of the test utils, there's a lot of useful stuff so you don't have to implement everything from scratch. If you just want to make sure that the client gets to `CLOSED` within 15s then I'd recommend `TestUtils#waitForCondition` "
522641072,9487,ableegoldman,2020-11-13T04:57:10Z,Is this the only property that changed? Might be clearer if you just override what you need to here
522650704,9487,ableegoldman,2020-11-13T05:09:14Z,"We should probably use an actual handler here to make sure it works with the GlobalThread. Actually maybe we should add a few unit tests here to make sure that it closes down and rethrows when the old handler is used, but handles the exception internally when the new handler is used, etc"
522654080,9487,ableegoldman,2020-11-13T05:13:23Z,Why set the exception handler in this test and no others?
523028163,9487,cadonna,2020-11-13T15:35:28Z,"nit: usually we indent 4 spaces, not 8."
523034271,9487,cadonna,2020-11-13T15:45:11Z,"Are you sure this is the correct method to call? As far as I understand the the javadocs and the decompiled code, this method does not return the handler you can set on a `Thread` with `setUncaughtExceptionHandler()`."
523041454,9487,cadonna,2020-11-13T15:56:14Z,"I guess, you wanted to do this
```suggestion
                        ""and the registered exception handler opted to "" + action + ""."" +
```
"
523041842,9487,cadonna,2020-11-13T15:56:53Z,Please use a more meaningful parameter name.
523044111,9487,cadonna,2020-11-13T16:00:23Z,"I still have a question here. Since the stream thread is alive when it calls `close()` there will not be a deadlock anymore. So, why do we call `close()` with duration zero?"
523069611,9487,wcarlson5,2020-11-13T16:40:19Z,"changed to ` In order to get the thread uses use Thread.currentThread()`

Does that work better?"
523070844,9487,wcarlson5,2020-11-13T16:42:18Z,We have to do the casting in order to throw the exception. Otherwise the compiler complains about checked vs unchecked exceptions
523072781,9487,wcarlson5,2020-11-13T16:45:31Z,yes good catch
523073595,9487,wcarlson5,2020-11-13T16:46:49Z,that is a lot of line breaks
523079456,9487,wcarlson5,2020-11-13T16:56:05Z,Everything except the state we leave it in. We can move most of it to a helper
523079965,9487,wcarlson5,2020-11-13T16:56:55Z,we should be able to change it to `close()`
523080590,9487,cadonna,2020-11-13T16:57:57Z,"The name is a bit confusing. The best I could come up is `handleStreamsUncaughtExceptionByDefault()`, but I am sure there is a better name."
523089743,9487,vvcephei,2020-11-13T17:06:49Z,"If that's the case, then we really should just set a flag on KafkaStreams to indicate whether that handler has been set."
523090161,9487,wcarlson5,2020-11-13T17:07:04Z,"There is a logic to use the old handler if the conditions you laid out are true. The odd series of casts of exception types in `handleStreamsUncaughtExceptionDefaultWrapper` are what makes this happen. 

This is a bit tricky but I think we want to close the client either way. As we don't have plans to replace the global thread and shutting  down the application is best effort. We talked about this a while back and we decided the global handler was mainly for information and the return type we would try to follow but we need to make sure we at least close the client."
523095231,9487,wcarlson5,2020-11-13T17:10:31Z,I think this is fine for now. When we add replace thread as an option we can include overrides when handling the response that prevent the thread from being restarted in certain error cases.
523103147,9487,wcarlson5,2020-11-13T17:21:13Z,When we remove the old handler we either need to remove the test or remove the suppression. That is what I am hoping the comment will do
523109393,9487,wcarlson5,2020-11-13T17:28:41Z,"I'll add that to the comment, and add a test"
523136568,9487,wcarlson5,2020-11-13T18:14:59Z,how about `defaultStreamsUncaughtExceptionHandler`?
523138325,9487,wcarlson5,2020-11-13T18:17:29Z,We can just set a flag through to be safe
523141998,9487,wcarlson5,2020-11-13T18:21:29Z,same as above
523230140,9487,wcarlson5,2020-11-13T21:01:25Z,"So the problem that I am facing is that many tests are set up to work with the old handler. I was able to adapt most to use the new handler but not all. Some, like a few EOS tests, require one thread to die at a time. So I either suppress the deprecation or tag the test as deprecated, thus indicating it should be removed when the old handler is. 

Another problem is that a few tests rely on the threads dying one at a time or they test behavior in this case but they do not set an old handler. So I can either 1) set an old handler and mark for deletion or 2) adapt for the new out come. For the ones I could, I changed to the new flow but I could not do that with all of them.

@vvcephei @ableegoldman @cadonna How would you suggest updating these tests?"
523277374,9487,wcarlson5,2020-11-13T23:04:52Z,Because otherwise the task migrated exception sends it into a endless rebalance
523280093,9487,wcarlson5,2020-11-13T23:14:47Z,That is useful thanks. I went with `waitForApplicationState`
523280707,9487,wcarlson5,2020-11-13T23:17:00Z,Agree
523288678,9487,wcarlson5,2020-11-13T23:49:17Z,For the same reason I had to add to the other cases as the close from the new handler will not finish otherwise
523296833,9487,ableegoldman,2020-11-14T00:16:51Z,Is there an extra `uses` in there or am I not looking at this sentence from the right angle?
523302976,9487,ableegoldman,2020-11-14T00:47:17Z,"Ah ok I thought we executed this cleanup logic in the GlobalStreamThread's `shutdown` method but now I see that's not true. Sorry for the confusion there.
I do see some minor outstanding issues here, mainly around the state diagram. Let's say the user opts to `SHUTDOWN_CLIENT` in the new handler: the intended semantics are to end up in `NOT_RUNNING` 
But I think what would happen is that from the global thread we would immediately call `KafkaStreams#close` , which kicks off a shutdown thread to wait for all threads to join and then sets the state to `NOT_RUNNING`. Then when the handler returns, it would transition the global thread to `PENDING_SHUTDOWN` and then finally to `DEAD`. And during the transition to `DEAD`, we would actually end up transitioning the KafkaStreams instance to `ERROR`, rather than `NOT_RUNNING` as intended. So probably, we just need to update the `onChange` method in KafkaStreams.
This also reminds me of another thing, we need to update the FSM diagram and allowed transitions in KafkaStreams to reflect the new semantics we decided on for ERROR (which IIRC is basically just to make it a terminal state). Does that sound right to you?"
523303062,9487,ableegoldman,2020-11-14T00:47:52Z,"I suspect the tests didn't catch this because we would still transition out of ERROR to PENDING_SHUTDOWN and finally NOT_RUNNING in this case. But really, we shouldn't transition to ERROR in the first place"
523311219,9487,ableegoldman,2020-11-14T01:05:18Z,"What happens if we try to read the error code of an earlier subscription version? I genuinely don't know what the generated code does, but we should make sure it doesn't throw an NPE or something. Could you add a unit test for this case?"
523319677,9487,ableegoldman,2020-11-14T01:15:05Z,"I think any test that's trying to verify some unrelated behavior and just using the ""one thread dies at a time"" paradigm as a tool to do so should not be deleted. I'm sure in most if not all cases, there's some way to modify the test to verify that specific behavior either using the new handler or multiple apps or rewriting it altogether. 

But, there are a lot of tests that do this and a lot of them are pretty tricky, so I wouldn't want to stall this PR on waiting for all of these tests to be updated/adapted. I think we should file tickets for all of these tests and just try to pick up one or two of them every so often. Maybe that's being overly optimistic about our inclination to pick up small tasks even over a long period, but it's better than losing track of them altogether. WDYT?"
523322776,9487,ableegoldman,2020-11-14T01:18:45Z,But TaskMigratedException should never be thrown all the way up to the exception handler. Is that what you're seeing?
523327104,9487,wcarlson5,2020-11-14T01:23:46Z,I appreciate the benefit of the doubt :) but you are right there is an extra `uses`
523327338,9487,ableegoldman,2020-11-14T01:24:03Z,"Well it's not exactly a default, technically this method is always used to decide which handler to invoke (which may or may not invoke a default handler). Any of these would be fine by me but I'll throw one more idea out there: `invokeOldOrNewUncaughtExceptionHandler` "
523334335,9487,wcarlson5,2020-11-14T01:32:21Z,"I don't think it will actually transition to `ERROR` because the handler will call close before the global thread is dead, which will transition to PEDING_SHUTDOWN, there is no transition to ERROR from either PENDING_SHUTDOWN or NOT_RUNNING.

the FSM will be part of the add thread work as it doesn't really make sense to remove the change to error until we can add threads"
523334732,9487,wcarlson5,2020-11-14T01:35:22Z,I agree we shouldn't remove the valid test cases. Maybe the ones that are more complicated I can just set an idempotent old handler and mark as deprecated and we can file tickets to update. Either we work them down or when we go to remove the old handler they will fail and we need to fix them then.
523337226,9487,wcarlson5,2020-11-14T01:52:07Z,"Not quite. If I remove the handler and just run it there is an illegal state exception which runs endlessly until the handler can exit the loop. It looks like the thread hadn't started all the way before the TaskMigratedExcpetion is thrown

`INFO State transition from STARTING to PENDING_SHUTDOWN (org.apache.kafka.streams.processor.internals.StreamThread:223)
[`"
523337785,9487,wcarlson5,2020-11-14T01:56:35Z,Good idea. It does not seem to do anything. but good to have a test for it
523372464,9487,ableegoldman,2020-11-14T04:08:25Z,"Ah ok so there's some other IllegalStateException that would get swallowed if we just used `e -> {}` like in the other tests, so we need to explicitly rethrow it? That seems fine, although it makes me think that we should go ahead and use a ""real"" handler in _all_ of the tests, not just this one. Otherwise there could be some bug which causes an unexpected exception, but the test would just swallow it and silently pass.
Can we just use the default handler wrapper for all of these tests so they reflect realistic scenarios?"
523375614,9487,ableegoldman,2020-11-14T04:45:56Z,"Oh you're totally right, sorry for letting my paranoia start spreading conspiracy theories here   Given all this I'd still claim that the FSM is in need to being cleaned up a bit (or a lot), but if you'd prefer to hold off on that until the add thread work then I'm all good here. Thanks for humoring me and explaining the state of things. I just wanted/want to make sure we don't overlook anything, since there's a lot going on.

For example in the current code, if the global thread dies with the old handler still in use then we'll transition to ERROR. However the user still has to be responsible for closing the client themselves, and it will ultimately transition from ERROR to NOT_RUNNING. Whereas if we transition to ERROR as the result of a SHUTDOWN_APPLICATION error code, the user should NOT try to invoke close themselves, and the ERROR state will be terminal. That's pretty confusing eg for users who use a state listener and wait for the transition to ERROR to call close(). We should make sure that ERROR has the same semantics across the board by the end of all this work.

Anyways I'm just thinking out loud here, to reiterate I'm perfectly happy to merge this as-is. But for reasons like the above, I think it's important to tackle the FSM in the next PR and make sure it all gets sorted out by the next AK release"
524433881,9487,wcarlson5,2020-11-16T17:11:38Z,It's actually not always used. It is only used until a new handler is set in which it is over written. Once that happens we don't want the old handler to be set so we do not wrap a user provided handler with this method
524435241,9487,wcarlson5,2020-11-16T17:13:23Z,"The default is in KafkaStreams, but I see your point. We can make all of them rethrow then we will not have to worry about swallowing"
524437940,9487,wcarlson5,2020-11-16T17:17:31Z,"+1 to sorting out FSM before next release, I have a ticket to track the work. I started to change it and it ballooned out to be much more expansive than I thought. This PR is already complicated enough, so we can add is later."
524441515,9487,vvcephei,2020-11-16T17:22:55Z,"```suggestion
     * In order to get the thread that threw the exception, use Thread.currentThread().
```"
524447256,9487,vvcephei,2020-11-16T17:30:29Z,I think I'd personally still prefer the non-blocking version. It seems better to avoid blocking indefinitely when a thread is trying to shut itself down due to some unknown exception (or error).
524448160,9487,vvcephei,2020-11-16T17:31:54Z,"Likewise, here, it seems better to do a non-blocking close."
524470683,9487,vvcephei,2020-11-16T18:07:11Z,"```suggestion
                            ""and sent shutdown request for the entire application."", throwable);
```"
524475389,9487,vvcephei,2020-11-16T18:14:59Z,"Personally, as long as users have the information available to understand the nature of the error, it's fine to let them make their own decision about how to handle it. Maybe another team is in the middle of a broker upgrade, for example, and the owner of this app would like to just keep trying until the broker team gets it together."
524478717,9487,vvcephei,2020-11-16T18:20:34Z,I think I'd like to re-raise Sophie's concern here. It doesn't compute for me why we are casting an int to a byte here..
524487609,9487,wcarlson5,2020-11-16T18:35:22Z,"That is probably fine. We can really get into it when we add the replace option, as now all calls to the handler are fatal."
524490211,9487,wcarlson5,2020-11-16T18:39:34Z,"It doesn't really matter to me, though I think that non blocking is probably  preferable."
524540416,9487,wcarlson5,2020-11-16T20:06:47Z,I guess I must have misunderstood  your earlier comment. I thought you wanted it to stay a byte so that is why I pushed back. But if you have no objections I will just change it 
524814932,9487,ableegoldman,2020-11-17T00:56:33Z,"That's a fair point about broker upgrades, but don't we require the brokers to be upgraded to a version that supports EOS _before_ turning on eos-beta?
Anyways I was wondering if there was something special about this exception such that ignoring it could violate eos or corrupt the state of the program. I'll ping the eos experts to assuage my concerns"
524817135,9487,ableegoldman,2020-11-17T01:03:03Z,"> Since the stream thread is alive when it calls close() there will not be a deadlock anymore. So, why do we call close() with duration zero

@cadonna can you clarify? I thought we would still be in danger of deadlock if we use the blocking `close()`, since `close()` will not return until every thread has joined but the StreamThread that called `close()` would be stuck in this blocking call and thus never stop/join"
524819063,9487,ableegoldman,2020-11-17T01:08:55Z,"Just to clarify I think it's ok to leave this as-is for now, since as Walker said all handler options are fatal at this point "
524824649,9487,ableegoldman,2020-11-17T01:25:57Z,"Mm ok actually I think this should be fine. I was thinking of the handler as just ""swallowing"" the exception, but in reality the user would still let the current thread die and just spin up a new one in its place. And then the new one would hit this UnsupportedVersionException and so on, until the brokers are upgraded. So there shouldn't be any way to get into a bad state"
525161434,9487,cadonna,2020-11-17T13:41:49Z,">  I thought we would still be in danger of deadlock if we use the blocking close(), since close() will not return until every thread has joined but the StreamThread that called close() would be stuck in this blocking call and thus never stop/join

OK, I think you are right. I focused too much on 

```
if (!thread.isRunning()) {
    thread.join();
}
```

without considering that before the stream threads are shutdown which makes them not running.

In the meantime, I understood a bit better the motivation of the shutdown thread in `close()`. The shutdown thread ensures that the timeout is still consiered in case `close()` is called by a stream thread. I think we should revisit it. But that is outside the scope of this PR.

To unblock this PR, I am fine with `close(Duration.Zero)`, but I have the feeling we could do better."
525169791,9487,cadonna,2020-11-17T13:52:01Z,There is something wrong in this sentence.
525170922,9487,cadonna,2020-11-17T13:53:37Z,`oldHanlder` -> `oldHandler`
525194549,9487,cadonna,2020-11-17T14:26:06Z,nit: remove line
525442248,9487,wcarlson5,2020-11-17T19:41:19Z,oops
525444958,9487,wcarlson5,2020-11-17T19:43:34Z,need to remove `use`
525636554,9487,ableegoldman,2020-11-18T01:32:09Z,I think it makes more sense to transition to ERROR in this case than to NOT_RUNNING. But let's put this on file with the other FSM-related work planned for following PRs
525640088,9487,ableegoldman,2020-11-18T01:43:12Z,"Why do we shut down the global thread only after all stream threads have completed their shutdown? Seems like it would be more efficient to send the shutdown signal to everyone first, and then wait for all the threads to join. Can you try this out in the followup PR?"
525650632,9487,ableegoldman,2020-11-18T02:14:15Z,"I just realized that this is going to be a problem with the way the ERROR state is being used. IF we `closeToError` then we transition to ERROR and shut down, however `ERROR -> PENDING_SHUTDOWN` is still an allowed transition so there's nothing to prevent the shutdown from being triggered again when a user calls `close()`. And note that a lot of users most likely have a state listener at the moment which does exactly that, ie when it sees a transition to ERROR it immediately invokes close (because that's what you should do with the current semantics)
Just another thing that I think we can fix with some minor rewiring of the FSM. "
525658639,9487,ableegoldman,2020-11-18T02:23:10Z,"Hm ok this might be a problem. Since this is thrown from another catch block and not from the try block, it won't be caught by the catch block below and will slip through the exception handler. "
525663640,9487,ableegoldman,2020-11-18T02:28:30Z,We should remember to update the wording here when we add the REPLACE_THREAD functionality
525678234,9487,wcarlson5,2020-11-18T02:44:07Z,You are right I think. I just copied from the normal close method because I knew it worked. In a follow up we can maybe change both of these. Do you think that there should be a ak ticket to track it?
525680874,9487,wcarlson5,2020-11-18T02:46:55Z,I am on the fence about this. I do think its would be consistent to be not running but also it did shutdown cleanly. We made this choice when ERROR still meant all threads had died and that is not true now. In the end I just went with what we had in the KIP rather than try to change it. Though I could be swayed to leave this in ERROR.
525681642,9487,wcarlson5,2020-11-18T02:47:48Z,This is currently the plan to remove that transition. It is pretty much the only change we plan to make to the FSM.
525686843,9487,wcarlson5,2020-11-18T02:53:20Z,like in stream thread we can just add a call to the handler
525692960,9487,ableegoldman,2020-11-18T02:59:37Z,"Eh, I wouldn't bother with an AK ticket if this will be tackled in the next PR. I'll just make a list of all the minor followup work somewhere to keep track"
525701691,9487,ableegoldman,2020-11-18T03:06:40Z,"That's fair. I guess I was thinking less about the inherent meaning of ERROR vs NOT_RUNNING, and more about not behaving differently in this special case. ie if there _are_ still StreamThreads running when a user selects SHUTDOWN_APPLICATION, then we ultimately transition to ERROR. So it strikes me as a bit odd to transition to NOT_RUNNING just because we didn't happen to have any threads left."
525734417,9487,ableegoldman,2020-11-18T03:30:18Z,"WDYT about having both NOT_RUNNING and ERROR go through PENDING_SHUTDOWN, rather than just transitioning directly and permanently to ERROR? At a high level I think it just makes sense for ERROR and NOT_RUNNING to be symmetric. Also any benefit to having an intermediate PENDING_SHUTDOWN for the NOT_RUNNING case presumably applies to the ERROR case as well. eg, it indicates whether Streams has completed its shutdown or not: users know that an app in PENDING_SHUTDOWN should never be killed, its only safe to do so once it reaches NOT_RUNNING. We should provide the same functionality and only transition to ERROR after the shutdown is complete"
526211409,9487,wcarlson5,2020-11-18T16:08:54Z,"I do think that Error should not have direct transition. However I don't like using `PENDING_SHUTDOWN` , mostly because we can already distinguish between the two states and it would be best to inform right away. Also it could be a problem if we went to set Error and some how it went from PENDING_SHUTDOWN to NOT_RUNNING. I am in favor of adding something like `PENDING_ERROR` just to be more precise. "
526477258,9487,ableegoldman,2020-11-18T22:53:15Z,Sounds reasonable
665553313,10851,vlsi,2021-07-07T17:00:34Z,I guess it would be better to move the assignment to the field declaration to avoid duplication among constructors.
665561377,10851,vlsi,2021-07-07T17:12:31Z,"Can you please clarify why `TreeMap<UUID, ClientState>` is used here? Would `Map<UUID, ClientState>` suffice?"
669462506,10851,cadonna,2021-07-14T09:49:49Z,"```suggestion
        final Map<TaskId, Integer> tasksToRemainingStandbys =
            statefulTasks.stream().collect(Collectors.toMap(task -> task, t -> numStandbyReplicas));
```"
669473299,10851,cadonna,2021-07-14T10:05:18Z,I was wondering whether we can simply standby assignment if `configs.numStandbyReplicas == 0`. Here or as first step in the method body of  `assignStandbyReplicaTasks()`. In this way we can remove `NoopStandbyTaskAssignor`.
669496452,10851,cadonna,2021-07-14T10:41:19Z,"As far as I can see, this map is only used in `ClientTagAwareStandbyTaskAssignor` and it is only used to iterate over pairs (taskId, uuid). That can also be accomplished by iterating over the client states and for each client state  iterate over the assigned active tasks. I do not think that we need to modify the signature of `assignActiveStatefulTasks()`. Or am I missing something? "
669500118,10851,cadonna,2021-07-14T10:47:36Z,I think it should be `SortedMap` instead of `TreeMap`. I also saw that we sometimes missed to use `SortedMap` instead of `TreeMap` in some signatures. It needs to be a sorted map because the assignments should be stable otherwise it could happen that we compute different assignments for the same input which could lead to unnecessary state migrations.
669504412,10851,cadonna,2021-07-14T10:54:30Z,Why do we need this internal class? Wouldn't it be simpler to structure the code with methods directly under `ClientTagAwareStandbyTaskAssignor`?
669849371,10851,lkokhreidze,2021-07-14T18:20:28Z,"Thanks for the feedback Bruno.

I reasoned that, since internal states like `clientsPerTagValue`, `standbyTaskClientsByTaskLoad`, etc., have to be allocated per invocation of `assignStandbyTasks` method, it felt easier and more readable to create one single internal object rather than invalidating local caches in `ClientTagAwareStandbyTaskAssignor`."
669851397,10851,lkokhreidze,2021-07-14T18:23:36Z,"I tried to avoid unnecessary iterations. With that we would have to do separate iteration in the `ClientTagAwareStandbyTaskAssignor`, which felt redundant, since `assignActiveStatefulTasks` can return necessary mapping since it has to iterate over client states either way."
669853519,10851,lkokhreidze,2021-07-14T18:26:40Z,"I didn't give it much thought to be honest.

`TreeMap` for the `clientStates` was already used in the `HighAvailabilityTaskAssignor` and went with the same signature here. I think it makes sense to change the contract to be a `SortedMap`. Will do that."
669865915,10851,lkokhreidze,2021-07-14T18:45:48Z,"Sure! done.
Personal preference. Having all the strategies of standby task assignment implementations in a single class makes unit testing a bit easier. But I do agree that removing one extra class is indeed good idea."
670294370,10851,cadonna,2021-07-15T09:27:04Z,"Yes, the `TreeMap` in the signatures has been already there before this PR. Thank you for fixing this!"
670309425,10851,cadonna,2021-07-15T09:47:08Z,"As far as I can see, we would iterate only over the active tasks in both cases. The difference is that in case we have one loop and in the other we have two nested loops. In the nested loop case, the code in the innermost loop is executed the same number of times as in the one loop case. That is, as many times as the number of active tasks.
In general, I would not change too much code for a performance improvement before we hit a performance issue. You know, as Donald E. Knuth stated ""premature optimization is the root of all evil"".  "
670332282,10851,cadonna,2021-07-15T10:18:49Z,"Method `assignStandbyTasks()` is only invoked once per assignment, as far as I can see. I do not see the need to avoid invalidating caches. Or am I missing somethings?  "
670333813,10851,cadonna,2021-07-15T10:20:54Z,"I would prefer to have an interface instead of an abstract class. In the past, it turned out to be cleaner and easier maintainable even if we need to duplicate the `configs` field in the implementations of this interface."
670336504,10851,cadonna,2021-07-15T10:24:46Z,I think a factory method as used [here](https://github.com/apache/kafka/blob/f413435585a3ed735ea9d4c551aa4f4f533d6a13/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamsPartitionAssignor.java#L607) should suffice instead of an entire class.
671829899,10851,lkokhreidze,2021-07-18T11:47:01Z,"That is correct. Currently, `StandbyTaskAssignor` implementations are created once per `TaskAssignor#assign` method call, and `assignStandbyTasks` is called only once. 
I just didn't want to assume how and how many times `assignStandbyTasks` is called as I didn't want to leak the implementation details to the caller.
However, if you feel strongly that it's better to have the implementation in the `ClientTagAwareStandbyTaskAssignor`, I can refactor the code. Since it's internal contract of the assignment, maybe it's okay."
672023300,10851,lkokhreidze,2021-07-19T06:38:07Z,"Fair enough, done."
672023438,10851,lkokhreidze,2021-07-19T06:38:26Z,Done
672023741,10851,lkokhreidze,2021-07-19T06:39:04Z,Moved factory method in `StandbyTaskAssignor` interface itself. Hope it addresses your comment.
685807795,10851,cadonna,2021-08-10T08:31:19Z,"I see your point, but I do also not see the need for an internal state for which we need to avoid invalidation. Variables `numStandbyReplicas` and `numStandbyReplicas` are configs that can be stored as member fields of `ClientTagAwareStandbyTaskAssignor` or passed along to the methods that need them. Variables `tagKeyToTagValuesMapping`, `clientsPerTagValue`, `standbyTaskClientsByTaskLoad`, and `clientStates` can also be passed to the methods that need them. Avoiding state makes reasoning about code simpler and here it seems possible to avoid state. See `HighAvailabilityTaskAssignor`, it does not have any state."
685816278,10851,cadonna,2021-08-10T08:42:05Z,"I would prefer to make this interface independent of its implementations. If you put the factory method here, the interface is not independent anymore. I would prefer a factory method named `createStandbyTaskAssignor()` in `HighAvailabilityTaskAssignor` similar to the existing factory method `createTaskAssignor()` in `StreamsPartitionAssignor`. "
685822324,10851,cadonna,2021-08-10T08:49:41Z,"Minor: You could extend interface `TaskAssignor` and remove `assignStandbyTasks()` from this interface since `assign()` in `TaskAssignor` has almost the same signature. The difference is parameters `configs` and `allTaskIds`. You will need `configs` if you will not keep the config as a member variable as mentioned in my other comment. You will not need `allTaskIds`, but that would be OK, I guess."
685824044,10851,cadonna,2021-08-10T08:51:51Z,"I guess the initialization in the constructor on line 79 is only temporary. This will change in one of the next PRs. Nevertheless, I agree that it would also be fine to move the initialization to the field declaration for now.

I would even propose to pass the client tags to the constructor, since those are kind of constants coming from the config."
685865965,10851,cadonna,2021-08-10T09:48:09Z,I would prefer to just add two parameters -- `source` and `destination` -- to the `isValidMovement()` method in `StandbyTaskAssignor` and get rid of this class. 
685867438,10851,cadonna,2021-08-10T09:50:08Z,The task ID is never used. Could we remove it?
685871631,10851,cadonna,2021-08-10T09:56:06Z,Could you move the second condition to the `canMove` assignment on line 166? I think the condition is logically a part of `canMove`. 
685872701,10851,cadonna,2021-08-10T09:57:39Z,nit: I think `isAllowedTaskMovement()` reflects better the meaning of this method.
685877129,10851,cadonna,2021-08-10T10:03:55Z,Would it be possible to integrate the tag constraint as part of the constraint on the priority queue?
693133897,10851,lkokhreidze,2021-08-20T18:14:16Z,"It's already part of the poll constraint of the priority queue. Example:

```
final UUID polledClient = standbyTaskClientsByTaskLoad.poll(
    activeTaskId, uuid -> !clientsOnAlreadyUsedTagDimensions.contains(uuid)
);
```
I don't think it will be doable with constructor constraint, because we need to update constraint on each poll."
693150166,10851,lkokhreidze,2021-08-20T18:44:21Z,Good point about client tags being constant. Added it as constructor parameter.
693163634,10851,lkokhreidze,2021-08-20T19:09:26Z,Addressed with 9841d25
695463229,10851,lkokhreidze,2021-08-25T07:11:43Z,This method is needed in `ClientTagAwareStandbyTaskAssignor` and `DefaultStandbyTaskAssignor`. Was thinking to create `StandbyTaskAssignmentUtils` and extract this logic in there. Wdyt @cadonna ?
695630930,10851,cadonna,2021-08-25T10:49:00Z,"Yes, I think that makes sense. In this way you can also directly test the method.
BTW, you can simply pass `statefulTaskIds`  to this method instead of `statefulTasksWithClients`. The keys in `statefulTasksWithClients` should be the task IDs in `statefulTaskIds` and the values in `statefulTasksWithClients`  are never used. "
695633134,10851,cadonna,2021-08-25T10:52:08Z,"```suggestion
        final Map<TaskId, Integer> tasksToRemainingStandbys = computeTasksToRemainingStandbys(
            numStandbyReplicas,
            statefulTasksWithClients
        );
```"
695634093,10851,cadonna,2021-08-25T10:53:36Z,"```suggestion
        statefulTasksWithClients.forEach((taskId, clientId) -> assignStandbyTasksForActiveTask(
            numStandbyReplicas,
            taskId,
            clientId,
            rackAwareAssignmentTags,
            clients,
            tasksToRemainingStandbys
        ));
```"
695640768,10851,cadonna,2021-08-25T11:03:15Z,"Isn't this the same as:
```
tagKeyToTagValuesMapping.computeIfAbsent(tagKey, (ignored) -> new HashSet<>()).add(tagValue);
```"
695642889,10851,cadonna,2021-08-25T11:06:10Z,"Isn't this the same as:
```
clientsPerTagValue.computeIfAbsent(tagValue, (ignored) -> new HashSet<>()).add(clientId);
```"
695669488,10851,cadonna,2021-08-25T11:41:52Z,Why is the map from tag key to tag values computed for each active task? They should not change during the assignment and we can just compute it once in `assign()`. Do you agree?
695686709,10851,cadonna,2021-08-25T12:07:09Z,"```suggestion
            final Set<UUID> clientsOnAlreadyUsedTagDimensions = findClientsOnUsedTagDimensions(
                usedClients,
                rackAwareAssignmentTags,
                clientStates,
                clientsPerTagValue,
                tagKeyToTagValuesMapping
            );
```"
695690393,10851,cadonna,2021-08-25T12:12:07Z,"IMO, the code is easier readable if you name the variables consistently like `tagValueToClients` and `tagKeyToTagValues` or `clientsForTagValue` and `tagValuesForTagKey`. I prefer the former because it better visualises the mapping, but that is a matter of taste, I guess."
695717497,10851,lkokhreidze,2021-08-25T12:48:31Z,"Good catch. I missed it during refactoring, you're correct."
695735723,10851,lkokhreidze,2021-08-25T13:10:03Z,Done
695735854,10851,lkokhreidze,2021-08-25T13:10:15Z,Pushed changes
695735942,10851,lkokhreidze,2021-08-25T13:10:21Z,Fixed
695736026,10851,lkokhreidze,2021-08-25T13:10:27Z,Fixed
700864299,10851,cadonna,2021-09-02T08:25:40Z,"Although we never use the returned value from a standby task assignor, I would return `false` since a standby task assignment will never require a follow-up probing rebalance."
700905553,10851,cadonna,2021-09-02T09:18:37Z,"Map `statefulTasksWithClients` is only used to iterate over its entries. I think it would be better to use the following nested loops and remove `statefulTasksWithClients`:

```suggestion
        for (final TaskId statefulTaskId : statefulTaskIds) {
            for (final Map.Entry<UUID, ClientState> entry : clients.entrySet()) {
                final UUID clientId = entry.getKey();
                final ClientState clientState = entry.getValue();

                if (clientState.activeTasks().contains(statefulTaskId)) {
                    assignStandbyTasksForActiveTask(
                        numStandbyReplicas,
                        statefulTaskId,
                        clientId,
                        rackAwareAssignmentTags,
                        clients,
                        tasksToRemainingStandbys,
                        tagKeyToTagValues,
                        tagValueToClients
                    );
                }
            }
        }
```"
700910355,10851,cadonna,2021-09-02T09:25:03Z,I do not understand why you re-add `clientsOnAlreadyUsedTagDimensions`. Those clients were not modified and not polled for sure due to line 140. 
700917243,10851,cadonna,2021-09-02T09:33:57Z,"I think this map does not work for distinct tag keys that have overlapping tag values. For example, `key1` contains one of `{value1, value2}` and `key2` contains one of `{value2, value3}`."
701068792,10851,cadonna,2021-09-02T13:12:40Z,"Are you sure, because I cannot confirm the failure of the test on my side?"
701137236,10851,lkokhreidze,2021-09-02T14:26:05Z,"Yeah, sorry. You're right. This is not needed. "
701141903,10851,lkokhreidze,2021-09-02T14:30:51Z,"Sorry, can you elaborate more on this?
Currently, when deciding the distribution, algorithm takes into account both, tag key, as well as tag value. So it will treat `key1: value2` and `key2: value2` as different dimensions. Do you think it's something that has to be addressed?"
701142976,10851,lkokhreidze,2021-09-02T14:32:05Z,Pushed the changes.
701143273,10851,lkokhreidze,2021-09-02T14:32:26Z,I've removed this line and pushed the changes.
720193502,10851,cadonna,2021-10-01T12:12:59Z,"Let's assume you have two clients. `clientX` has tags `keyA:value1` and `keyB:value2` and `clientY` has tags `keyA:value2` and `keyB:value3`. Notice that `keyA` and `keyB` share `value2`. With your code, we will end up with a `tagValueToClients` map that looks like this:
```
value1 -> {clientX}
value2 -> {clientX, clientY}
value3 -> {clientY}
```  
Now, let's assume that an active task is assigned to `clientX`. It would be totally fine if we assign the standby task to `clientY` since each tag key of both clients do not share a value. However, your algorithm does not allow it, because on line 198 it also adds `clientY` to the clients that are not allowed to get the standby. The reason is that `tagValueToClients` only looks for clients that contain value `value2` and not for clients that contain it as a value for `keyA`.

The following test fails because of this:
```
    @Test
    public void shouldTestBrunosTheory() {
        final Map<UUID, ClientState> clientStates = mkMap(
            mkEntry(UUID_1, createClientStateWithCapacity(2, mkMap(mkEntry(ZONE_TAG, ZONE_1), mkEntry(CLUSTER_TAG, CLUSTER_1)), TASK_0_0)),
            mkEntry(UUID_2, createClientStateWithCapacity(2, mkMap(mkEntry(ZONE_TAG, CLUSTER_1), mkEntry(CLUSTER_TAG, CLUSTER_3))))
        );

        final Set<TaskId> allActiveTasks = findAllActiveTasks(clientStates);
        final AssignmentConfigs assignmentConfigs = newAssignmentConfigs(1, ZONE_TAG, CLUSTER_TAG);

        new ClientTagAwareStandbyTaskAssignor().assign(clientStates, allActiveTasks, allActiveTasks, assignmentConfigs);

        assertTrue(
            standbyClientsHonorRackAwareness(
                TASK_0_0,
                clientStates,
                asList(
                    mkSet(UUID_2)
                )
            )
        );
    }
```"
720196549,10851,cadonna,2021-10-01T12:17:41Z,Could you please use `tagValueToClients` and `tagKeyToValues` here as in the rest of the class?
720214125,10851,cadonna,2021-10-01T12:46:53Z,"Currently the code iterates over the active tasks and assigns all standby tasks for each active task. If the standby tasks cannot all be assigned, we might end up with all standby tasks assigned for some active task but none for others. What do you think about to assign one standby task for all active task and then assign the second standby task for all active task, and so on. In this way, it is more likely that at all active tasks have at least one standby task assigned.
I am aware that the default standby assignor has the same drawback."
741840561,10851,lkokhreidze,2021-11-03T11:18:42Z,"Thanks, good catch."
741896976,10851,lkokhreidze,2021-11-03T12:41:17Z,"Makes sense @cadonna.
Should I update default standby task assignor, or prefer to leave it out of scope of this PR?"
741897677,10851,lkokhreidze,2021-11-03T12:42:16Z,Solved by storing tuple of tag key and value as map key instead of just tag value.
741923941,10851,lkokhreidze,2021-11-03T13:15:03Z,"Also wondering if it's better to do this as a separate task altogether. Since, as you've mentioned, it's the same behaviour as with default standby task assignor.
But if you feel it's better to do it in current PR, happy to do so."
765909088,10851,cadonna,2021-12-09T15:39:09Z,For each standby of a single active task the set `clientsOnAlreadyUsedTagDimensions` is computed from scratch. I think this is not necessary since the clients on already used tag dimensions that we found for the first standby are still valid for the second standby and the clients on already used tag dimensions found for the second standby are still valid for the third standby and so on. This is true because we only add clients to the set `usedClients` but we never remove any. I think we can compute `clientsOnAlreadyUsedTagDimensions` incrementally for each standby of a single active task instead of computing it from scratch each time.
766104130,10851,cadonna,2021-12-09T19:48:26Z,"Something does not work as expected in this algorithm. According to this doc, the assignor should fall back to distributing tasks on least-loaded clients. However, the following test case fails:
```
    @Test
    public void shouldDistributeTasksOnLeastLoadedClientsWhenThereAreNoEnoughUniqueTagDimensions() {
        final Map<UUID, ClientState> clientStates = mkMap(
            mkEntry(UUID_1, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_1), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_0)),
            mkEntry(UUID_2, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_2)), TASK_0_1)),
            mkEntry(UUID_3, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_0_2)),
            mkEntry(UUID_4, createClientStateWithCapacity(3, mkMap(mkEntry(CLUSTER_TAG, CLUSTER_2), mkEntry(ZONE_TAG, ZONE_1)), TASK_1_0))
        );

        final Set<TaskId> allActiveTasks = findAllActiveTasks(clientStates);
        final AssignmentConfigs assignmentConfigs = newAssignmentConfigs(1, CLUSTER_TAG, ZONE_TAG);

        new ClientTagAwareStandbyTaskAssignor().assign(clientStates, allActiveTasks, allActiveTasks, assignmentConfigs);

        assertEquals(1, clientStates.get(UUID_1).standbyTaskCount());
        assertEquals(1, clientStates.get(UUID_2).standbyTaskCount());
        assertEquals(1, clientStates.get(UUID_3).standbyTaskCount());
        assertEquals(1, clientStates.get(UUID_4).standbyTaskCount());
    }
``` 
The standby task for active task 0_0 can be put on client UUID_2 and the standby task for active task 0_1 can be put on client UUID_1 without breaking rack awareness constraints. Standby tasks for active tasks 0_2 and 1_0 cannot be put on any client without breaking rack awareness, so they should be distributed on least-loaded clients. However, that does apparently not happen, because client UUID_3 and UUID_4 are not assigned any standby.    
"
789057202,10851,lkokhreidze,2022-01-20T18:46:16Z,"Fixed with e3aff39c7687a358cc8672accd5bbf6a27193a04.
Algorithm will try to achieve partial rack awareness as there are different `cluster` tag dimensions. 
"
789057904,10851,lkokhreidze,2022-01-20T18:47:15Z,"Fixed with e3aff39.
Now we only create `clientsOnAlreadyUsedTagDimensions` once and populate it for the each standby task assignment instead of re-creating it."
789060382,10851,lkokhreidze,2022-01-20T18:50:46Z,"Hi @cadonna
Would appreciate your feedback on this.
As of now, algorithm ignores a case when client has reached capacity and it will try to assign the standby task to it as long as it satisfies the rack awareness. There's a even test for it `shouldDistributeClientsOnDifferentZoneTagsEvenWhenClientsReachedCapacity`. For me it makes sense that rack awareness, if configured, takes precedence in this case. Added log to inform the user, just want to make sure if you think this is a valid approach.
It is not a lot of work to take capacity into account, so we can redo algorithm if you think that makes more sense."
789687365,10851,lkokhreidze,2022-01-21T14:09:22Z,"I reworked things a bit, check out this comment https://github.com/apache/kafka/pull/10851#issuecomment-1018535614"
789687883,10851,lkokhreidze,2022-01-21T14:10:01Z,"No longer relevant, I reworked things a bit, check out this comment https://github.com/apache/kafka/pull/10851#issuecomment-1018535614"
795434808,10851,lkokhreidze,2022-01-31T08:32:34Z,"Answering why do we need this:
I think with client tag aware standby task assignment, there's a much higher chance that we will overload some clients without this check. I think it's better to not to overload the clients and instead log the warning so users can do the needful of increasing the capacity in order to satisfy the rack awareness."
805156426,10851,showuon,2022-02-12T12:31:47Z,Could we add some java doc to this assign to briefly mention about the algorithm used in the assignor? Thanks.
805156641,10851,showuon,2022-02-12T12:34:04Z,"I know there was no any java doc for the default least load assignor. But do you think we can add some comments to it, just like in `ClientTagAwareStandbyTaskAssignor`? I believe not everyone knows default assignor algorithm is least loaded assignor."
805157124,10851,showuon,2022-02-12T12:40:02Z,"I'm wondering could we keep the original constructor and pass empty map into the new one? So that we don't have to make changes to the old caller. That is:

```
public ClientState() {
    this(0, Collections.emptyMap());
}

ClientState(final int capacity) {
    this(capacity, Collections.emptyMap());
}
```
WDYT?"
805199777,10851,cadonna,2022-02-12T20:12:02Z,"I see what you want to do. However, the capacity is the number of consumers on the Streams client, i.e., the number of stream threads running on the Streams client. With this check, you only allow to assign standby tasks to clients that have less tasks assigned as stream threads running. That is actually rather an unlikely case. Normally, you have more tasks assigned to a Streams client than the number of stream threads running on the client.
I would keep it simple and ignore the balance for now."
805203233,10851,cadonna,2022-02-12T20:51:21Z,Would it be possible to decrement the numbers in `tasksToRemainingStandbys` to maintain the remaining standbys to distribute instead of using `pendingStandbyTaskToNumberRemainingStandbys`? 
805571500,10851,showuon,2022-02-14T07:43:02Z,maybe add a comment here to mention we need to make sure the sourceClient tag matches to destinationClient tag if rack tag is enabled...something like that. 
805641183,10851,showuon,2022-02-14T09:19:10Z,Is it normal when this happened? Should we do anything to it? Or at least log something here?
805642565,10851,lkokhreidze,2022-02-14T09:20:51Z,"Thanks for the feedback. No objections from my side. The reason why I avoided that was to make sure that client tags are always passed. To emphasise that it's mandatory parameter when constructing the `ClientState` object. Please note that we have made `ClientState#clientTags` immutable; so there're no setters for the client tags.
But if you feel like it's better to default to empty map, happy to change it.
Will wait for your response on this."
805646942,10851,showuon,2022-02-14T09:25:59Z,"The variable name `polledClient` is unreadable. I think the variable is the client not having the same tag key/value, right? Could we give it a more meaningful name, ex: `clientUUIDNotOnUsedTagDimension`, or other better one if you have. "
805656409,10851,showuon,2022-02-14T09:36:51Z,"When reaching this point, we have tried our best to assign standby tasks with rack awareness to all clients. I think we should have a debug log here, to log some current status, like current assignment, `pendingStandbyTaskToNumberRemainingStandbys`, `pendingStandbyTaskToClientId`, and mention we're going to distribute the remaining tasks with least loaded assignor...etc, for better troubleshooting."
805659384,10851,showuon,2022-02-14T09:40:16Z,"TBH, I don't understand this method well before I read into the implementation. I think the method is trying to assign standby tasks to those clients without using the same tag key/value, right? If so, maybe we can change the name to `assignStandbyTasksToClientsWithoutSameTag`, or others you can think of. WDYT?"
805663941,10851,showuon,2022-02-14T09:45:26Z,"Looks like the `findClientsOnUsedTagDimensions` method keeps finding duplicated `usedClients`. That is, if we have 10 `numRemainingStandbys`, we'll run `findClientsOnUsedTagDimensions` with 1 `usedClients` at first. And then, add one more, to have 2 `usedClients` at 2nd run, and add one to 3, 4, 5, ... 10. Is my understanding correct? If so, could we improve it? "
805668921,10851,showuon,2022-02-14T09:50:57Z,"This is an internal class, so I think it won't be changed/used many times. I think change to my previous suggestion is better. Thanks."
805782141,10851,lkokhreidze,2022-02-14T12:13:21Z,I think having a warn log is a good call. We can add validation rules (if necessary) when doing last part of this KIP - updating streams configuration.
805830410,10851,lkokhreidze,2022-02-14T13:16:51Z,Good call. I don't know how I missed that...
807660376,10851,lkokhreidze,2022-02-16T08:24:19Z,Thanks! Renamed to `clientOnNotUsedTagDimensions` to be consistent with the rest of the codebase. Since we refer to client UUIDs as just `client` in the codebase. Hope this works.
807663879,10851,lkokhreidze,2022-02-16T08:28:41Z,Makes sense. Renamed to `assignStandbyTasksToClientsWithDifferentTags`. Hope this works too.
807738730,10851,lkokhreidze,2022-02-16T09:50:45Z,Refactored javadocs a bit. Moved some content from class level javadoc to the `assign` method. Hope this works.
807810480,10851,lkokhreidze,2022-02-16T11:07:20Z,Good call. Improved the code in a latest commit.
809711301,10851,showuon,2022-02-18T06:35:57Z,OK
809773328,10851,showuon,2022-02-18T08:29:41Z,"nit: the algorithm will fall back to the least-loaded clients without **taking** rack awareness constraints into consideration.
"
809797947,10851,showuon,2022-02-18T09:02:01Z,"I checked the use of `tagKeyToValues`. It is only used for total value count of each key. Is that right? If so, could we just store the `Map<String, Integer> tagKeyToValueCount` only?"
809798766,10851,showuon,2022-02-18T09:03:03Z,"Sorry, I didn't understand the reason why we can't filter out clients located on that tag when `allTagValues.size() <= countOfUsedClients`. Could you help explain to me? Thanks."
809972865,10851,showuon,2022-02-18T12:54:02Z,nit: `@see ClientTagAwareStandbyTaskAssignor` (no need the package name)
809980650,10851,showuon,2022-02-18T13:04:58Z,nit: close this bracket in the same line. That is: `private StandbyTaskAssignmentUtils() {}`
809983790,10851,showuon,2022-02-18T13:09:18Z,nit: indent issue. Could we add comment in front of `EMPTY_RACK_AWARE_ASSIGNMENT_TAGS`?
809984233,10851,showuon,2022-02-18T13:09:53Z,same as above.
810003432,10851,lkokhreidze,2022-02-18T13:35:38Z,"Can do, but also `Set` makes it easier to handle duplicate values as we are looking for distinct count values here. Not sure if refactoring is worth it though."
810019916,10851,lkokhreidze,2022-02-18T13:54:59Z,"Consider the following example

```
# Kafka Streams Client 1
client.tag.zone: eu-central-1a
client.tag.cluster: k8s-cluster1
rack.aware.assignment.tags: zone,cluster

# Kafka Streams Client 2
client.tag.zone: eu-central-1b
client.tag.cluster: k8s-cluster1
rack.aware.assignment.tags: zone,cluster
 
# Kafka Streams Client 3
client.tag.zone: eu-central-1c
client.tag.cluster: k8s-cluster1
rack.aware.assignment.tags: zone,cluster

# Kafka Streams Client 4
client.tag.zone: eu-central-1a
client.tag.cluster: k8s-cluster2
rack.aware.assignment.tags: zone,cluster
 
# Kafka Streams Client 5
client.tag.zone: eu-central-1b
client.tag.cluster: k8s-cluster2
rack.aware.assignment.tags: zone,cluster
 
# Kafka Streams Client 6
client.tag.zone: eu-central-1c
client.tag.cluster: k8s-cluster2
rack.aware.assignment.tags: zone,cluster
```

With the above we have following number of unique tag values:

```
zone: 3 (eu-central-1a, eu-central-1b, eu-central-1c)
cluster: 2 (k8s-cluster1, k8s-cluster2)
```

Now lets say we have standby replica count as `2` and we want to active task is located on `Client 1`

`usedClients=1` (because of the active task)

### 1st standby assignment

During 1st standby takes assignment, we will exclude clients on following dimensions:

```
zone: eu-central-1a
cluster: k8s-cluster1
```

Used clients will get incremented since we can allocate the client on different `zone` and `cluster`.
`usedClients=2`

### 2nd standby assignment

We will have to exclude `zone: eu-central-1a and (eu-central-1b || eu-central-1c)` tag values. We can do that, because after we exclude clients on the new tag, we still have clients on the one free tag value we can assign the next standby to.

We can't exclude `cluster` because we have already used two clients, and we just have two unique values for the `cluster` tag, so it's impossible to get the ideal distribution with this configuration and number of instances. So we can only achieve partial distribution.

So idea of this check is to ignore tags where we have less unique values than the clients we have already used. If we don't have this check, for the 2nd standby task assignment we would have excluded all the clients located
on `k8s-cluster1` and `k8s-cluster2`, and there wouldn't be any client left to assign the standby task to. We would fall back to the least loaded client, BUT there will be no guarantee that least loaded client assignment would honor partial rack awareness.

Hope this makes sense.
"
810020380,10851,lkokhreidze,2022-02-18T13:55:34Z,I added `shouldDoThePartialRackAwareness` test to verify this behaviour.
810462108,10851,showuon,2022-02-19T07:58:08Z,"nit: could we be consistent with other tests that make the UUID_seq in order? It makes me a little confused when reading this test. That is, 
```
           mkEntry(UUID_1, createClientStateWithCapacity(1, mkMap(mkEntry(ZONE_TAG, ZONE_1), mkEntry(CLUSTER_TAG, CLUSTER_1)), TASK_0_0)),
            mkEntry(UUID_2, createClientStateWithCapacity(1, mkMap(mkEntry(ZONE_TAG, ZONE_1), mkEntry(CLUSTER_TAG, CLUSTER_1)), TASK_1_0)),

            mkEntry(UUID_3, createClientStateWithCapacity(1, mkMap(mkEntry(ZONE_TAG, ZONE_2), mkEntry(CLUSTER_TAG, CLUSTER_1)), TASK_0_1)),
```"
810462515,10851,showuon,2022-02-19T08:02:43Z,"I can understand what you tried to assert here. But I think we should also assert that the standby tasks count in each client is as what we expected, because under current verification, we only focus on the tasks distributed with rack awareness. However, there is still possibility that standby tasks don't distribute evenly, right?

The following tests should also update. Thanks."
810462817,10851,showuon,2022-02-19T08:06:49Z,"A ha, you're right! We only need the distinct count values. No need to refactor it then. Thanks for the explanation. "
810464460,10851,showuon,2022-02-19T08:27:03Z,"In 1st standby assignment:
> Used clients will get incremented since we can allocate the client on different zone and cluster.
usedClients=2

I think this used client should be 5 or 6, right? But I got your idea.
Thanks for the explanation. Makes sense to me. 
"
810465176,10851,showuon,2022-02-19T08:35:58Z,"I'm thinking, we can make it much clear by adding comments, though I know this is hard to explain in simple words. How do you think we add comments like this:

            // If we have used more clients than all the tag's unique values,
            // we can't filter out clients located on that tag, because it'll excluded all the clients. 
            // Please check ClientTagAwareStandbyTaskAssignorTest#shouldDoThePartialRackAwareness test for more info.

And we can make more description in `shouldDoThePartialRackAwareness` test."
810485747,10851,showuon,2022-02-19T12:29:59Z,"We can add comments here like:

// We need 2 standby tasks (+ 1 active task) to distribute, but we only have 2 cluster tag, so we will won't exclude all clients when 2nd standby tasks assignment, and will try to distribute the 2nd standby tasks with taking partial rack tag into consideration.

WDYT?"
810487017,10851,showuon,2022-02-19T12:43:36Z,"After your explanation of partial rack awareness, I can understand the distribution will be `UUID_5, UUID_6`. But I don't know why it's possible with the result `UUID_5, UUID_3`? 
I thought after 1st standby task assignment, we'll exclude `CLUSTER_1` and `ZONE_1` tags clients. So the remaining clients will be `UUID_5, UUID_6`. Therefore, the only possible results will be `UUID_5, UUID_6`. Is my understanding correct? Anything I missed?
Thanks."
810487139,10851,showuon,2022-02-19T12:44:46Z,"And I think we should add some comments here, to have a simple explanation like we I did above to explain why we have these results. Thanks."
810857967,10851,lkokhreidze,2022-02-21T07:59:25Z,"`UUID_5` is essentially ""ideal"" distribution because it has both different cluster and zone compared to an active task.
However, when we assign 2nd standby, we can only choose client on different `zone`. `cluster` tag is excluded as we don't have enough unique values to exclude the `cluster`. So for the 3rd standby task, both `cluster1` and `cluster2` are valid. That means that clients with `UUID_3` (`cluster1`, `zone3`) and `UUID_6` (`cluster2`, `zone3`) are valid destinations. 
On the high level, idea is that, if any of the values of the `cluster` tag goes offline, no matter on which `cluster` we distribute the 2nd standby `cluster1` or `cluster2`, we either way will loose two clients at the same time. So from availability perspective it doesn't make difference where the 2nd standby will be assigned. One may argue that it would be better to always distribute to a different tags compared to an active task, but this will complicate algorithm even further, so I guess it's better to keep it simpler in a first iteration.

Hope this makes sense, I will add more info as a comment."
810894883,10851,lkokhreidze,2022-02-21T08:48:51Z,Added more explanation in `shouldDoThePartialRackAwareness` test.
811551306,10851,showuon,2022-02-22T03:33:07Z,Nice tests!
811556641,10851,showuon,2022-02-22T03:50:06Z,"nit: we can directly break the while when `numRemainingStandbys == 0`, so that we don't need to run the redundant `findClientsOnUsedClientTagDimensions` in the last run. Ex:

```java
countOfUsedClients++;
numRemainingStandbys--;
if (numRemainingStandbys == 0) {
  break;
}
clientsOnAlreadyUsedTagDimensions.addAll(
                findClientsOnUsedClientTagDimensions(
                    clientOnUnusedTagDimensions,
                    countOfUsedClients,
                    rackAwareAssignmentTags,
                    clientStates,
                    tagEntryToClients,
                    tagKeyToValues
                )
            );

```
"
811559765,10851,showuon,2022-02-22T03:59:48Z,"Thanks for the explanation for partial rack awareness assignment. I think that algorithm makes sense. However, I don't think the implementation matches what you described. You said in the `shouldDoThePartialRackAwareness` test, in 2nd standby assignment for task_0_0, we will only consider `zone`, but in current implementation, we will also consider `cluster`. That is, when entering the `while (numRemainingStandbys > 0) {` loop, the `clientsOnAlreadyUsedTagDimensions` already excluded the `cluster_1` and `zone_1`. And in the 1st standby assignment, `UUID_5` will be chosen, we'll exclude `zone_2` only, and not exclude `cluster_2`. So , the only client left is `UUID_6`. That's the current design, isn't it? I don't see where we only consider `zone` in 2nd assignment. Could you help elaborate more? Thank you."
811645496,10851,lkokhreidze,2022-02-22T07:30:21Z,"@showuon Ah, you're absolutely right! I'm very sorry for the confusion. It's been a while and got lost myself. I will update comments to reflect this. Do you think it makes sense to leave the implementation as is, or we should re-work it based on what I described before? Either is fine with me."
811651144,10851,showuon,2022-02-22T07:39:28Z,I think we should re-work the `assignStandbyTasksToClientsWithDifferentTags` method to match what you described. That makes more sense. Thanks.
811652014,10851,lkokhreidze,2022-02-22T07:40:50Z,Will get it done asap.
811918321,10851,lkokhreidze,2022-02-22T12:58:07Z,Updated implementation in https://github.com/apache/kafka/pull/10851/commits/5d1543eab455e5bc361800ee0976d4c7f88fdfe6
811964104,10851,cadonna,2022-02-22T13:48:33Z,"Wouldn't this be equivalent and maybe a bit more concise?

```suggestion
        UUID lastUsedclient = activeTaskClient;
        do {
            fillClientsOnAlreadyUsedTagEntries(
                lastUsedclient,
                countOfUsedClients,
                rackAwareAssignmentTags,
                clientStates,
                tagEntryToClients,
                tagKeyToValues,
                tagEntryToUsedClients
            );

            final UUID clientOnUnusedTagDimensions = standbyTaskClientsByTaskLoad.poll(
                activeTaskId, uuid -> !isClientUsedOnAnyOfTheTagEntries(uuid, tagEntryToUsedClients)
            );

            if (clientOnUnusedTagDimensions == null) {
                break;
            }

            clientStates.get(clientOnUnusedTagDimensions).assignStandby(activeTaskId);

            countOfUsedClients++;
            numRemainingStandbys--;

            lastUsedclient = clientOnUnusedTagDimensions;
        } while (numRemainingStandbys > 0);
```"
811981794,10851,cadonna,2022-02-22T14:06:40Z,"That is quite challenging to understand. After reading it a couple of times I understood that if we've used a number of clients that is equal to or greater than the number of unique values of the tag, we cannot guarantee that each standby is on a different tag value than the active and other standbys. So the rack-awareness becomes partial. Is that correct?
Could you reformulate it, so that it states that the rack-awareness guarantee does not hold anymore.
And why ""more clients than all tag's unique values""? When the number of used clients is equal to the unique tag values, we are already in the partial rack-awareness situation, right?
Maybe you should give here an example as in the mentioned test. I find referencing the test is a bit cumbersome, because if the test gets renamed this comment becomes useless."
812018771,10851,cadonna,2022-02-22T14:41:55Z,"I could not find where you decrement the number of remaining standbys. If you get a value from this map and put it into an `int` variable, you do not have a reference to the `Integer` value in the map anymore. This might become a problem in `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignRemainingStandbyTasks()`."
812081768,10851,lkokhreidze,2022-02-22T15:41:22Z,"Thanks Bruno, I'll add the example to the comment."
812519265,10851,showuon,2022-02-23T02:42:38Z,nit: lastUsedclient -> lastUsedClient
812525667,10851,showuon,2022-02-23T03:03:49Z,"Since we will remove tags, I think we can rename to `updateClieintsOnAlreadyUsedTagEntries`. WDYT?"
812527382,10851,showuon,2022-02-23T03:09:25Z,Nice catch! And maybe we should add a test to address this.
812527749,10851,showuon,2022-02-23T03:10:30Z,Good suggestion!
812626201,10851,cadonna,2022-02-23T07:48:45Z,"Yes, a test is absolutely necessary!"
812720897,10851,lkokhreidze,2022-02-23T09:54:23Z,Pushed the changes. I added detailed explanation with an example. Also tests have the similar example. Hopefully this change makes logic more clear.
812722151,10851,lkokhreidze,2022-02-23T09:55:49Z,"Updated tests for the `StandbyTaskAssignmentUtils#pollClientAndMaybeAssignAndUpdateRemainingStandbyTasks` and also added separate test for the `ClientTagAwareStandbyTaskAssignor`.

For the `ClientTagAwareStandbyTaskAssignor` I decided to make few things package private to be able to test this logic. As otherwise, there was no easy way to test if `tasksToRemainingStandbys` was getting updated properly. Hope this is okay. "
812884587,10851,showuon,2022-02-23T13:22:03Z,nit: additional space between `the` and `2nd`
817629787,10851,cadonna,2022-03-02T12:08:36Z,"I think info log would also be OK here. I imagine users that are wondering why their standbys are not distributed as they would expect. With this information they can at least try to fix it on the config level. This log message should only happen at rebalance time, which should usually be rather seldom.
If we decide to put the log message on info level, you should also change a bit the wording and not use variable names in it. Maybe some hints what the users can do to fix this would also be nice.

Is it possible to separate the concerns of this log message and the one on line 135? Something along the lines of here the rack-aware standby assignment did not work due the tag config and on line 135 the assignment did not work due to too low number of instances. We can then put both on warn or info (do not forget to also check the related log message in the default standby assignor).   "
817633715,10851,cadonna,2022-03-02T12:14:04Z,"```suggestion
            // for the 2nd standby task assignment would effectively mean excluding all the clients.
```"
817634423,10851,cadonna,2022-03-02T12:15:08Z,"```suggestion
            // This statement checks if we have used more clients than the number of unique values for the given tag,
```"
817650626,10851,cadonna,2022-03-02T12:37:25Z,"This can be done in a follow-up PR:
I am not a big fan of `// Visible for testing` because it often means that we missed to extract code to separate classes. Here I would definitely extract this code to a factory class. "
38823999,195,junrao,2015-09-06T17:45:05Z,"It seems that it will be cleaner if we split this into getAcls and getAclsFromZK. The former will just read from the cache and the latter always read from ZK. Only the ZK listener will use the latter.
"
38824003,195,junrao,2015-09-06T17:45:32Z,"Hmm, this may not work well if two ACL changes happen quickly. The second one will override the the resource of the previous one. If a broker hasn't finished processing the previous change, it will miss it after the override. I thought we wanted to use the same approach as in ConfigChangeListener where the updates are written as a sequential ZK node.
"
38824005,195,junrao,2015-09-06T17:45:44Z,"Could we document the format of values used in the new ZK paths in the comment?
"
38824017,195,junrao,2015-09-06T17:46:19Z,"Hmm, do we need this? It's simpler if we always rely on ZK watchers to propagate the changes. ZKClient will handle all the reconnects for us. If we lose a session, we probably have to read the AclChangedZkPath in case we missed a ZK notification.
"
38925968,195,ijuma,2015-09-08T13:40:40Z,"You can use `_` if you want a value to be initialised to the default one (eg `null`, `false`, `0`, etc.)
"
38926011,195,ijuma,2015-09-08T13:41:01Z,"Type annotation is redundant in cases like this.
"
38926146,195,ijuma,2015-09-08T13:42:22Z,"A less verbose and more idiomatic way to write this:

`import scala.collection._`
...
`private val aclCache = mutable.Map.empty[Resource, Set[Acl]]`
"
38926413,195,ijuma,2015-09-08T13:44:50Z,"What if `str` is not a `String`? That case is not being handled. Nicer code can be written if you use the scala version of the map (ie `config`). Then `get` will return an `Option` and you can use methods like `filter`, `map`, etc.

That applies to the other code that is reading from the config.
"
38926589,195,ijuma,2015-09-08T13:46:19Z,"Unnecessary type annotation
"
38926656,195,ijuma,2015-09-08T13:46:52Z,"In Scala, one should use `==` not `equals`.
"
38926768,195,ijuma,2015-09-08T13:47:41Z,"Instead of doing this, you should replace the for comprehension with `find`.
"
38926812,195,ijuma,2015-09-08T13:47:57Z,"No `var` needed, `inReadLock` returns a value.
"
38926927,195,ijuma,2015-09-08T13:49:10Z,"`aclJson.map(Acl.fromJson).getOrElse(Set.empty)`
"
39098212,195,Parth-Brahmbhatt,2015-09-09T21:22:02Z,"Done.
"
39098585,195,Parth-Brahmbhatt,2015-09-09T21:25:46Z,"I am confused. I thought you said a watch notification will never be missed or it will only be missed when a reconnection happens which is handled by zkClient,  which is why we don't need the sync thread. 

If client-A updated the /kafka-acl-chaged and set its data to ""topic-1"" (this should go throught zkQuorum) and then client-B updated the /kafka-acl-changed and set its data to ""topic-2"" are you saying we will only get notification for ""topic-2"" depending on how fast the change occurs? Doesn't that violate the first guarantee which led us to delete the hourly sync? 

I would like to point out that in the watch notification we are just relying on data sent as part of notification and not really reading /acl-changed one more time. 
"
39099520,195,Parth-Brahmbhatt,2015-09-09T21:33:56Z,"Done.
"
39099854,195,Parth-Brahmbhatt,2015-09-09T21:37:13Z,"Done.
"
39100044,195,Parth-Brahmbhatt,2015-09-09T21:39:00Z,"removed the whole scheduler.
"
39100138,195,Parth-Brahmbhatt,2015-09-09T21:39:47Z,"as soon as I add that import all the other places that expect Set starts complaining that ""Set[X] can not be converted to Set[X]"" which I am guessing is just scala compiler complaining that they expect the unmutable scala set but with the new import now they are all assumed to be mutable set. Did I mention scala is amazing :-). 
"
39100385,195,Parth-Brahmbhatt,2015-09-09T21:41:58Z,"Fixed.
"
39100572,195,Parth-Brahmbhatt,2015-09-09T21:43:37Z,"Done.
"
39103033,195,Parth-Brahmbhatt,2015-09-09T22:06:08Z,"apart from returning true , given I also want to log the debug statement as a side effect, the version with find looks less redable IMO. 
"
39103099,195,Parth-Brahmbhatt,2015-09-09T22:06:51Z,"function is deleted given Jun has recommended to assume that zkClient will handle reconnects and guarantee all the watchers are always delivered.
"
39103189,195,Parth-Brahmbhatt,2015-09-09T22:07:41Z,"Done.
"
39104580,195,Parth-Brahmbhatt,2015-09-09T22:21:52Z,"removed and added listener for reconnection handler. 
"
39105224,195,Parth-Brahmbhatt,2015-09-09T22:29:24Z,"Handled the case where its not string by just adding another case. 
"
39107202,195,junrao,2015-09-09T22:55:07Z,"Parth,

ZK watchers are actually one-timer watchers (http://zookeeper.apache.org/doc/r3.4.1/zookeeperProgrammers.html#sc_WatchRememberThese). When a watcher fires, the client has to register the watcher again (typically through a read) to get notification for future changes. Between the time that a watcher fires and the client does a read, multiple changes could have happened. The read will return the latest value and leave a watcher there. So, in general, there is no guarantee that every change triggers the firing of a watcher. In particular, if you update the value of a ZK path multiple times in between, only the latest value will be seen by the reader.

One way to capture all changes is to follow how we propagate the config changes (ConfigChangeListener). Whenever we change an ACL, we write the ACL under /kafka-acl and also write a sequential ZK node under /kafka-acl-changed. The value of the sequential ZK node indicates the resource for which the ACL has changed. The sequential ZK nodes are guaranteed to be unique and have a number suffix that's ever growing. The ACL manager registers a child change listener on /kafka-acl-changed and remembers the last number suffix that it has processed. When the watcher fires, the ACL manager reads all child nodes under /kafka-acl-changed (the read may pick up multiple ACL changes) and process new nodes after the last remembered number suffix. The processing will involve reading the latest ACL associated with the corresponding resource. Since every sequential node is unique, the ACL manager won't miss any ACL change. 

On initialization, the ACL manager will first register the watcher and read all existing ACLs.

Finally, we need a way to garbage collect old sequential nodes. This can be done by just removing sequential nodes that are say, more than 15 mins old (assuming every broker would have picked up the ACL changes by then).
"
39112549,195,Parth-Brahmbhatt,2015-09-10T00:08:18Z,"@junrao  Thanks for the explanation and it makes perfect sense. Will update the PR.
"
39349577,195,junrao,2015-09-13T17:06:25Z,"Typo immeditatly
"
39349578,195,junrao,2015-09-13T17:06:32Z,"Need to either remove those pirintlns or convert them to logging.
"
39349582,195,junrao,2015-09-13T17:06:39Z,"Perhaps we can include path and notifications in the error message.
"
39349583,195,junrao,2015-09-13T17:06:47Z,"We probably should rename this to /kafka-acl-changes.
"
39349584,195,junrao,2015-09-13T17:06:50Z,"Perhaps rename this to ""acl_changes_""?
"
39349586,195,junrao,2015-09-13T17:07:03Z,"Probably rename this as javaConfigs. Then we can define the scala one as configs.
"
39349587,195,junrao,2015-09-13T17:07:08Z,"Perhaps it's better to explicitly assign the initial values than using _.
"
39349588,195,junrao,2015-09-13T17:07:13Z,"To be consistent, use Set.empty[KafkaPrincipal] instead of Set.empty?
"
39349592,195,junrao,2015-09-13T17:07:27Z,"Perhaps we can allow zkConnectionTimeout and zkSessionTimeout to be configured explicitly for SimpleAuthorizer and default to those in KafkaConfig if not specified.
"
39349604,195,junrao,2015-09-13T17:08:01Z,"I had a comment in the jira. It seems that in SocketServer, when creating the session object, it's better to create a KafkaPrincipal instead of using KafkaChannel.principal(). The type in KafkaPrincipal should always be USER and the name should be KafkaChannel.principal().getName(). Then, we can just get the KafkaPrincipal from Session and don't need to create a new one.
"
39349608,195,junrao,2015-09-13T17:08:24Z,"This is useful for things like auditing. Perhaps we can log this in the end together with the decision on wether the operation is granted on not. Also, could we put that into a separate authorization log4j logger (take a look at the request logger in log4j.properties)? That way, if people want auditing, they can just enable the authorization logger.
"
39349609,195,junrao,2015-09-13T17:08:29Z,"Should this be removed?
"
39349610,195,junrao,2015-09-13T17:08:32Z,"Can this be private?
"
39349611,195,junrao,2015-09-13T17:08:35Z,"Can this be private?
"
39349613,195,junrao,2015-09-13T17:08:38Z,"Can this be private?
"
39349614,195,junrao,2015-09-13T17:08:39Z,"Can this be private?
"
39349615,195,junrao,2015-09-13T17:08:46Z,"This needs to be removed.
"
39349616,195,junrao,2015-09-13T17:08:54Z,"Since these two are accessed from different threads, do they need to be volatile?
"
39349633,195,junrao,2015-09-13T17:10:50Z,"Ideally, we want to reuse this class in DynamicConfigManager. If that's too much to do in this jira, could you file a follow-up jira to address this?
"
39439189,195,Parth-Brahmbhatt,2015-09-14T19:58:22Z,"I like the idea of changing type of principal in session as KafkaPrincipal. I dont agree with the Type being always user as different authentication schemes may want to change that and different authorizer implementations may actually use the values differently. Does that sound ok?
"
39445926,195,Parth-Brahmbhatt,2015-09-14T20:56:26Z,"Fixed
"
39445928,195,Parth-Brahmbhatt,2015-09-14T20:56:27Z,"Was planning to do the same. Filed KAFKA-2547
"
39445934,195,Parth-Brahmbhatt,2015-09-14T20:56:30Z,"removed debug statement.
"
39445938,195,Parth-Brahmbhatt,2015-09-14T20:56:32Z,"Done.
"
39445952,195,Parth-Brahmbhatt,2015-09-14T20:56:37Z,"Done
"
39445961,195,Parth-Brahmbhatt,2015-09-14T20:56:39Z,"Done
"
39445971,195,Parth-Brahmbhatt,2015-09-14T20:56:41Z,"Done.
"
39445975,195,Parth-Brahmbhatt,2015-09-14T20:56:43Z,"Done.
"
39445980,195,Parth-Brahmbhatt,2015-09-14T20:56:45Z,"Done.
"
39445984,195,Parth-Brahmbhatt,2015-09-14T20:56:47Z,"Done
"
39446002,195,Parth-Brahmbhatt,2015-09-14T20:56:55Z,"Any time we allow or deny we already log the decision and the reasoning behind it, the trace is actually just to mark the entry point. 

Modified log4j but I have set the default level at warn which will disable audit logging by default. Let me know if you want it to be enabled by default.
"
39446007,195,Parth-Brahmbhatt,2015-09-14T20:56:58Z,"Done.
"
39446014,195,Parth-Brahmbhatt,2015-09-14T20:57:01Z,"Done
"
39446020,195,Parth-Brahmbhatt,2015-09-14T20:57:03Z,"Done
"
39446024,195,Parth-Brahmbhatt,2015-09-14T20:57:05Z,"Done
"
39446032,195,Parth-Brahmbhatt,2015-09-14T20:57:06Z,"Done
"
39446038,195,Parth-Brahmbhatt,2015-09-14T20:57:09Z,"Done
"
39446049,195,Parth-Brahmbhatt,2015-09-14T20:57:14Z,"Done
"
39468051,195,junrao,2015-09-15T02:06:53Z,"Since we return early in a few places, it seems that not all accesses are logged.
"
39468196,195,junrao,2015-09-15T02:10:07Z,"Hmm, how do we get the type from the authentication layer? Currently, Authenticator only returns a Principal, which just has a name?
"
39538158,195,Parth-Brahmbhatt,2015-09-15T17:19:17Z,"I rechecked to assure it is logged in all cases (superuser, no acls, deny acl match(as part of matching function), allow acl match(as part of matching function),  no acl found).

If we want it to be logged at the end, I can change the logic so it does not return in middle. May be its just me, I think the current way looks cleaner.
"
39543626,195,Parth-Brahmbhatt,2015-09-15T18:00:01Z,"I was saying we change the type in Authenticator interface to KafkaPrincipal. Then each authenticator implementation can add its own type (SSL can set principalType=Certificate, and kerberos based sasl can set type= User) as long as the authorizer they use can handle different types they will be fine. This will specifically useful when there are multiple listeners.   

I guess we can just do what you are suggesting for now and if we encounter a real use case we can change the type in authenticator at that time. I have made the changes you suggested for now.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      
"
39591781,195,junrao,2015-09-16T03:54:26Z,"getAcls should be getAclsFromZk, right? Could we add a unit test to test the initial loading?
"
39630946,195,ijuma,2015-09-16T13:48:46Z,"Why not import `JavaConverters._` along with other imports at the top of the file and then simply do `changes.asJava` here and delete all local imports?
"
39631044,195,ijuma,2015-09-16T13:49:34Z,"We normally include `()` for side-effecting changes.
"
39631152,195,ijuma,2015-09-16T13:50:24Z,"It's generally better to do `notifications.nonEmpty` because it's O(1) even if the underlying implementation has a O(n) `size` (like scala.List).
"
39632552,195,ijuma,2015-09-16T14:00:47Z,"Early return inside a closure (and a for comprehension desugars to `foreach`) actually involves throwing a `NonLocalReturnControl` exception. You don't think the following is more readable?

```
 acls.find { acl =>
  acl.permissionType == permissionType
    && (acl.principal == principal || acl.principal == Acl.WildCardPrincipal)
    && (operations == acl.operation || acl.operation == All))
    && (acl.host.contains(host) || acl.host.contains(Acl.WildCardHost))
}.map {
  debug(s""operation = $operations on resource = $resource from host = $host is $permissionType.name based on acl = $acl"")
  acl.isDefined
}
```
"
39633336,195,ijuma,2015-09-16T14:06:56Z,"`unit.` should not be part of package name.
"
39633374,195,ijuma,2015-09-16T14:07:14Z,"`unit` should not be part of package name
"
39683091,195,Parth-Brahmbhatt,2015-09-16T20:48:48Z,"Done. Sorry about missing this in first place. added the test for load cache.
"
39683115,195,Parth-Brahmbhatt,2015-09-16T20:48:54Z,"Done.
"
39683121,195,Parth-Brahmbhatt,2015-09-16T20:48:56Z,"Done
"
39683126,195,Parth-Brahmbhatt,2015-09-16T20:48:58Z,"Done
"
39683134,195,Parth-Brahmbhatt,2015-09-16T20:49:03Z,"may be I am missing it but where do you see the side effect?
"
39683137,195,Parth-Brahmbhatt,2015-09-16T20:49:07Z,"Done.
"
39683204,195,Parth-Brahmbhatt,2015-09-16T20:49:40Z,"added a consistent audit log message in addition to the other logging statements.
"
39683466,195,Parth-Brahmbhatt,2015-09-16T20:51:45Z,"the original version is more readable to me, may be its just my java brain. I have assumed scala idioms make it more redable with your approach so changed it as you suggested. Still had to use a return inside the map as acl is not an Option but type ACL.
"
39684697,195,ijuma,2015-09-16T21:02:19Z,"`milliseconds()` returns a different result every time, so it's a side-effecting method (it gets data from the system clock typically). Side-effect free methods always return the same result given the same arguments, so a method that takes no parameters and doesn't return a constant result is a side-effecting method.
"
39769987,195,Parth-Brahmbhatt,2015-09-17T16:55:54Z,"my understanding of what side effect is different than what you described. I have added the () anyways given I don't really think it affects readability one way or another.
"
39771350,195,ijuma,2015-09-17T17:08:07Z,"It is true that the terminology would be clearer if we used pure versus impure functions. Thanks for making the change anyway.
"
39778562,195,Parth-Brahmbhatt,2015-09-17T18:06:46Z,"@ijuma I actually had to remove the bracket for compilation to succeed. With the beackets I get 
""/Users/pbrahmbhatt/repo/kafka/core/src/main/scala/kafka/common/ZkNodeChangeNotificationListener.scala:80: Long does not take parameters
      val now = time.milliseconds()""
"
39779399,195,ijuma,2015-09-17T18:13:07Z,"Sorry for this. The reason is that the Scala version of `Time` has a method called `milliseconds` so you can't use `()` at the end. The Java version of `Time` doesn't have this issue.
"
39812216,195,junrao,2015-09-17T23:45:04Z,"It seems that we don't need both imports on JavaConversions? import scala.collection.JavaConversions._ seems enough. Also, I think the previous version you had to just do the import inside processAllNotifications() is better since it makes it clear where the implicits are used.
"
39812243,195,junrao,2015-09-17T23:45:18Z,"@ijuma : I actually find the ""for-loop"" version that Parth wrote earlier easier to understand. map() is supposed to convert one collection to another and it seems it's weird to return from inside a map().
"
39812253,195,junrao,2015-09-17T23:45:25Z,"Why are we looping through resourceNames twice?
"
39812263,195,junrao,2015-09-17T23:45:32Z,"Could we put addAcls() and waitUntilTrue() in a private method and reuse?
"
39812389,195,ijuma,2015-09-17T23:47:33Z,"I agree that this version looks worse. It's not what I suggested. :) A `getOrElse` after the map is what I would do. Do you still prefer the previous version in that case?
"
39812505,195,ijuma,2015-09-17T23:49:14Z,"@junrao, why not simply use import JavaConverters._? It's the recommended way since it was introduced years ago. The main advantage is that it is both explicit (one has to use `asScala` or `asJava`) and you don't need scoped imports everywhere (which are quite verbose).
"
39813672,195,ijuma,2015-09-18T00:08:46Z,"To elaborate a bit, `return` is discouraged in Scala because it's not composable which makes it harder to refactor code safely. Once `return` is used, it is no longer safe to extract code into reusable functions. Another problem with `return` is that it uses exceptions for control flow once used inside closures (which are everywhere in Scala) as it is the only way to implement the specified behaviour in the JVM.

Unfortunately, return is used quite a lot in this PR. I'd be willing to submit a PR to this branch that removed all usages of `return` for comparison if there is interest.
"
39813673,195,junrao,2015-09-18T00:08:46Z,"Using explicit JavaConverters will be fine.
"
39813766,195,junrao,2015-09-18T00:10:24Z,"Could you post the exact syntax you had in mind?
"
39814098,195,ijuma,2015-09-18T00:16:15Z,"```
acls.find { acl =>
  acl.permissionType == permissionType
  && (acl.principal == principal || acl.principal == Acl.WildCardPrincipal)
  && (operations == acl.operation || acl.operation == All)
  && (acl.host == host || acl.host == Acl.WildCardHost)
}.map { acl =>
  debug(s""operation = $operations on resource = $resource from host = $host is $permissionType based on acl = $acl"")
  true
}.getOrElse(false)
```

or

```
acls.find { acl =>
  acl.permissionType == permissionType
  && (acl.principal == principal || acl.principal == Acl.WildCardPrincipal)
  && (operations == acl.operation || acl.operation == All)
  && (acl.host == host || acl.host == Acl.WildCardHost)
}.fold(false) { acl =>
  debug(s""operation = $operations on resource = $resource from host = $host is $permissionType based on acl = $acl"")
  true
}
```

The latter is more concise although we don't use it much in Kafka yet.
"
39862295,195,junrao,2015-09-18T14:46:40Z,"It would be better to move this inside processAllNotifications() so that we know exactly where the implicits are used. Alternatively, we can use the explicit conversion through JavaConverters as Isamel suggested.
"
39862320,195,junrao,2015-09-18T14:46:48Z,"Could we just return Boolean here?
"
39862560,195,junrao,2015-09-18T14:48:44Z,"It seems that if filteredAcls is empty, we want to remove the corresponding ACL path. Could we add a unit test for that? Also, we probably only need to update ZK if filteredAcls is different from existingAcls.
"
39862611,195,junrao,2015-09-18T14:49:15Z,"Could we make this more general to sth like the following? Then we can remove all individual usage of waitUntilTrue().

// return the new acl set after applying the changes to the original acl.
changeAclAndVerify(originalAcl: Set[Acl], addedAcl: Set[Acl], removedAcl: Set[Acl]): Set[Acl]
"
39862755,195,ijuma,2015-09-18T14:50:22Z,"Is this used anywhere?
"
39863387,195,ijuma,2015-09-18T14:55:29Z,"`exists` is probably better than `find` here as you are not using the value. Maybe something like

```
if (superUsers.exists(_ == principal)) {
  authorizerLogger.debug(...)
  true
}
else false
```
"
39863729,195,ijuma,2015-09-18T14:58:33Z,"A number of typing annotations that could be removed as they are easily inferred (`String`, `Set[Acl]`, `Acl`). It's OK to keep them if you think they help readability (as opposed to just Java habits ;)).
"
39864202,195,ijuma,2015-09-18T15:02:28Z,"Remove type annotation on the left-hand side? Also, you can replace `HashMap` with `Map` which is the static factory method.
"
39864888,195,ijuma,2015-09-18T15:08:31Z,"`aclCache.values.flatMap(_.filter(_.principal == principal)).toSet` should do the same as all of the lines above. The following is a bit more readable though:

`aclCache.values.flatMap(aclSet => aclSet.filter(_.principal == principal)).toSet`
"
39884655,195,Parth-Brahmbhatt,2015-09-18T18:15:57Z,"Done
"
39884657,195,Parth-Brahmbhatt,2015-09-18T18:16:00Z,"Done.
"
39885138,195,Parth-Brahmbhatt,2015-09-18T18:20:36Z,"we were not deleting the zookeeper path until an explicit call to remove(resource was made). I have changed that and added unit test. to read the zookeeper path I made the method toResourcePath public.
"
39885146,195,Parth-Brahmbhatt,2015-09-18T18:20:41Z,"Done
"
39885153,195,Parth-Brahmbhatt,2015-09-18T18:20:47Z,"In the initialize we register this zkClient so in case a reconnection happens zkClient will invoke this method and as part handling a new session it will process any missed notifications during the reconnection.
"
39885157,195,Parth-Brahmbhatt,2015-09-18T18:20:49Z,"Done.
"
39885181,195,Parth-Brahmbhatt,2015-09-18T18:21:03Z,"java habits, nothing really to do with readability :-). Guess it's going to take me sometime before I stop doing this.
"
39885186,195,Parth-Brahmbhatt,2015-09-18T18:21:08Z,"Removed the type. Trying to convert to Map actually gives compilation error.
"
39885345,195,Parth-Brahmbhatt,2015-09-18T18:22:42Z,"I actually realized one thing, given acl class itself does not have resource as acls are suppose to be attached to a resource, this method is pretty useless unless it returns a Map[Resource, Set[Acl]].  I have made the changes to both the interface and the implementation to reflect that.
"
39893276,195,junrao,2015-09-18T19:42:14Z,"empty set => empty map
"
39893280,195,junrao,2015-09-18T19:42:17Z,"space after if
"
39893304,195,junrao,2015-09-18T19:42:36Z,"@ijuma has a valid point. It doesn't seem reconnection is ever used? The handling of the reconnection logic is all in handleNewSession().
"
39893656,195,Parth-Brahmbhatt,2015-09-18T19:46:58Z,"sorry i though he was referring to ZkStateChangeListener object itself. removed.
"
39893666,195,Parth-Brahmbhatt,2015-09-18T19:47:01Z,"Done.
"
39893669,195,Parth-Brahmbhatt,2015-09-18T19:47:03Z,"Done.
"
39931619,195,junrao,2015-09-20T16:30:51Z,"startup() implies this class will have internal threads, but it doesn't. Would it be better to rename this to init()?
"
39931623,195,junrao,2015-09-20T16:31:00Z,"To be consistent with the line in 86, Set.empty => Set.empty[KafkaPrincipal]?
"
39931626,195,junrao,2015-09-20T16:31:27Z,"The ordering can be a bit subtle here. In order not to miss a session expiration event, we should probably use the following ordering.

  zkClient = ZkUtils.createZkClient(zkUrl, zkConnectionTimeoutMs, zkSessionTimeOutMs)
    ZkUtils.makeSurePersistentPathExists(zkClient, SimpleAclAuthorizer.AclZkPath)

```
loadCache

zkClient.subscribeStateChanges(ZkStateChangeListener)

aclChangeListener = new ZkNodeChangeNotificationListener(zkClient, SimpleAclAuthorizer.AclChangedZkPath, SimpleAclAuthorizer.AclChangedPrefix, AclChangedNotificaitonHandler)
aclChangeListener.startup()
```
"
39931632,195,junrao,2015-09-20T16:31:36Z,"We should add the Acl to aclCache, right? Could we add a unit test to cover this?
"
39931641,195,junrao,2015-09-20T16:32:10Z,"It seems that we expect notificationMessage to be non-empty. So, perhaps we can change the type of processNotifications to be just String and in ZkNodeChangeNotificationListener.processNotifications(), log a warning (with the path) if the data read from ZK is empty. 
"
39931644,195,junrao,2015-09-20T16:32:21Z,"simpleAclAuthorizer should be authorizer? Also, we should add a loadCache() tests where all the nodes in /acl_changes are gone.
"
40006553,195,Parth-Brahmbhatt,2015-09-21T18:24:46Z,"done.
"
40006570,195,Parth-Brahmbhatt,2015-09-21T18:24:52Z,"done.
"
40006588,195,Parth-Brahmbhatt,2015-09-21T18:25:01Z,"done. 
"
40006914,195,Parth-Brahmbhatt,2015-09-21T18:27:34Z,"Originally I did that but I figured given we havent added add,remove APIs in reality add/remove calls will be made from CLI so modifying cache did not make much sense. Also it seemed cleaner to just have a single path (notification) that updated the cache and also made it easy to test. 

I have made changes to update the cache as you suggested but I don't see given the same state can be modified by 2 components there is any easy way to add unit test to ensure which component actually made the state change.
"
40006927,195,Parth-Brahmbhatt,2015-09-21T18:27:42Z,"Done.
"
40006945,195,Parth-Brahmbhatt,2015-09-21T18:27:51Z,"Done.
"
40020498,195,junrao,2015-09-21T20:20:35Z,"This can just be 
private def loadCache() {
...
}

There are a few other places that we can get rid of "": unit ="".
"
40020520,195,junrao,2015-09-21T20:20:49Z,"We shouldn't be accumulating in acls, right?
"
40020555,195,junrao,2015-09-21T20:21:04Z,"Could we set up two resources with ACL to cover the new issue identified in loadCache()?
"
40020567,195,junrao,2015-09-21T20:21:09Z,"space after if
"
40022804,195,Parth-Brahmbhatt,2015-09-21T20:40:13Z,"Done.
"
40022808,195,Parth-Brahmbhatt,2015-09-21T20:40:14Z,"Done
"
40022814,195,Parth-Brahmbhatt,2015-09-21T20:40:18Z,"Yes, done.
"
40022826,195,Parth-Brahmbhatt,2015-09-21T20:40:24Z,"Done.
"
40027139,195,junrao,2015-09-21T21:16:44Z,"remove "": Unit =""
"
40027169,195,junrao,2015-09-21T21:17:02Z,"If a method has no return value, we need to define it as the following w/o =. Otherwise, it will pick up the value in the last statement as the return value. Ditto in a few other places.

def loadCache() {
}
"
40028686,195,Parth-Brahmbhatt,2015-09-21T21:28:35Z,"Done.
"
40028689,195,Parth-Brahmbhatt,2015-09-21T21:28:37Z,"Done.
"
40035419,195,junrao,2015-09-21T22:35:14Z,"remove =
"
40035425,195,junrao,2015-09-21T22:35:20Z,"remove =
"
40035432,195,junrao,2015-09-21T22:35:25Z,"remove =
"
40035437,195,junrao,2015-09-21T22:35:29Z,"remove =
"
40035443,195,junrao,2015-09-21T22:35:33Z,"remove =
"
40035455,195,junrao,2015-09-21T22:35:40Z,"testProcessNotification()
"
40035464,195,junrao,2015-09-21T22:35:47Z,"def testTopicAcl() {
"
40035473,195,junrao,2015-09-21T22:35:52Z,"def testDenyTakesPrecedence() {
"
40035480,195,junrao,2015-09-21T22:35:55Z,"def testAllowAllAccess() {
"
40035489,195,junrao,2015-09-21T22:36:01Z,"def testSuperUserHasAccess() {
"
40035496,195,junrao,2015-09-21T22:36:05Z,"def testNoAclFound() {
"
40035506,195,junrao,2015-09-21T22:36:13Z,"def testNoAclFoundOverride() {
"
40035514,195,junrao,2015-09-21T22:36:19Z,"def testAclManagementAPIs() {
"
40035523,195,junrao,2015-09-21T22:36:24Z," def testLoadCache() {
"
113242905,2910,ijuma,2017-04-25T16:19:38Z,Seems like passing `isolationLevel` explicitly is a good idea anyway. Maybe we don't need this comment.
113243316,2910,ijuma,2017-04-25T16:21:24Z,"If we don't use `buffer` in this path, do we still want to allocate it eagerly?"
113244042,2910,ijuma,2017-04-25T16:24:27Z,You could use `map` instead of pattern matching.
113244445,2910,ijuma,2017-04-25T16:26:03Z,Maybe this block should be in a separate method.
113244887,2910,ijuma,2017-04-25T16:27:41Z,Nit: just use `FileChannel.open` directly?
113244991,2910,ijuma,2017-04-25T16:28:05Z,How do we go about deciding whether to use Schema classes or writing to the buffer directly?
113246179,2910,ijuma,2017-04-25T16:32:39Z,"A general comment about default values: we should consider carefully when to use them as they are a common source of bugs. When we use them, the compiler no longer ensures that we think about what the value should be."
113247921,2910,hachikuji,2017-04-25T16:39:33Z,Ack. Was thinking the same thing when I was looking over this.
113248857,2910,hachikuji,2017-04-25T16:43:30Z,"Sure, we can push allocation into the other branch. This is technically a bug fix, by the way, since `Log.read` can return `MemoryRecords.EMPTY`."
113249530,2910,hachikuji,2017-04-25T16:46:28Z,"Yeah, it was a tough call. There's only a handful of use cases for `isFromClient = false`, and the default is the more restrictive option, so I thought it was reasonable. "
113253193,2910,hachikuji,2017-04-25T17:01:32Z,"Kind of a subjective call, but since we scan the abort index when handling fetches and the schema is so simple, I thought we could skip the need to go through the `Struct` object when reading and writing."
113829442,2910,junrao,2017-04-28T00:08:43Z,"During append, perhaps it's useful to assert that abortedTxn are inserted in ascending lastOffset order?"
113830001,2910,junrao,2017-04-28T00:13:54Z,"We don't set the position of channel after channel.truncate(), which seems ok. For consistency, we probably want to do the same in FileRecords.truncateTo()?"
113830270,2910,junrao,2017-04-28T00:16:40Z,"It seems that we can just allocate a single buffer and reuse, instead of reallocating?"
113831097,2910,junrao,2017-04-28T00:25:40Z,We throw an exception in the iterator below. Should we do the same thing here?
113832544,2910,junrao,2017-04-28T00:42:38Z,Could we add some comments on what's stored in this index?
113833158,2910,junrao,2017-04-28T00:49:14Z,It would be useful to document whether firstOffset and lastOffset are inclusive or exclusive.
113834767,2910,junrao,2017-04-28T01:05:00Z,It seems that we are missing the logic to fence off the request based on coordinatorEpoch?
113834965,2910,junrao,2017-04-28T01:07:18Z,We can probably use requestTimeoutMs.
113836150,2910,junrao,2017-04-28T01:21:01Z,Could we document the new field in the java doc above?
113838187,2910,junrao,2017-04-28T01:47:43Z,Could we add the new field to the comment above? Ditto for line 485.
113838217,2910,junrao,2017-04-28T01:48:05Z,"Now that we have a few different types of epoch, could we change epoch to producerEpoch?"
113972136,2910,junrao,2017-04-28T16:44:41Z,"Could we add a method in RecordBatch to indicate whether the batch is a controlled batch or not? Also, it seems that we use a special sequence number to indicate whether a batch is controlled or not. Would it be better to use another bit in the attribute for that?"
114010900,2910,junrao,2017-04-28T20:14:09Z,epoch => producerEpoch? Ditto in line 54.
114013479,2910,junrao,2017-04-28T20:29:42Z,"There can only be one opening transaction from a pid, right? If so, does startedTxns need to be a set?"
114016660,2910,junrao,2017-04-28T20:48:04Z,"Hmm, it seems because of this, we could be sending multiple responses for the same WriteTxnMarkersRequest, which won't be right?"
114018063,2910,junrao,2017-04-28T20:56:00Z,The spec says the value of the control record contains the coordinatorEpoch.
114018154,2910,junrao,2017-04-28T20:56:28Z,Could we be calculate the buffer size more precisely?
114035995,2910,junrao,2017-04-28T23:22:43Z,"Hmm, when this is called on the leader side, we haven't assigned the offset for the record yet. It seems that this means the offset returned in appendInfo.completedTransactions will be incorrect when we try to use it?"
114036286,2910,junrao,2017-04-28T23:25:54Z,"When firstOffset is set to controlRecord.offset, it seems that lastOffset could be smaller than firstOffset. Should we guard that?"
114038106,2910,junrao,2017-04-28T23:50:45Z,Do we need to call this here given that we are calling the same method in onHighWatermarkIncremented()?
114039345,2910,junrao,2017-04-29T00:09:55Z,Could we add isolationLevel to the comment above?
114039715,2910,junrao,2017-04-29T00:16:26Z,"We probably want to use the offset in fetchInfo instead of startOffset, which can be larger than startOffset?"
114040200,2910,junrao,2017-04-29T00:25:52Z,Should we call updateFirstUnstableOffset() in truncateTo() too?
114362367,2910,hachikuji,2017-05-02T16:29:01Z,I think we already do. It is in the call to `buildAndRecoverPidMap`.
114367045,2910,hachikuji,2017-05-02T16:48:37Z,The responses are accumulated and only sent after all callbacks have been received. I can change the name to clarify this.
114367825,2910,hachikuji,2017-05-02T16:52:00Z,Hmm.. yes that sounds right.
114400313,2910,hachikuji,2017-05-02T19:11:25Z,I've fixed this. We always use the offset of the control record as the last offset.
114460760,2910,junrao,2017-05-03T01:33:01Z,"Getting the position requires index lookup and may be a bit expensive. Do we really need to maintain position? It seems that when using firstUnstableOffset, we only need the offset, not the position?"
114461895,2910,junrao,2017-05-03T01:49:48Z,"Hmm, log.firstUnstableOffset is only advanced after the completeMarker has been fully replicated, which suggests that log.firstUnstableOffset is always going to be < highWatermark?"
114614159,2910,hachikuji,2017-05-03T18:08:06Z,"I realized that we need all started transactions in order to ensure that the LSO is updated correctly. We take all the started transactions from an append, add them to a sorted collection, and then remove the completed transactions in order of completion. This ensures that the LSO is always correct."
114662697,2910,junrao,2017-05-03T21:54:11Z,Could we add a comment whether fetchOffset and upperBoundOffset are inclusive or exclusive?
114678287,2910,junrao,2017-05-03T23:45:30Z,"When we call roll(), the base offset of the new segment could actually be larger than logEndOffset. So, in that case, the snapshot offset may not match the base offset of the next segment. Since in flush(), we delete snapshots based on the baseOffset of the active segment, we may delete the last snapshot by accident. One way to fix this is to call producerStateManager.updateMapEndOffset(newOffset) before taking the snapshot."
114679141,2910,junrao,2017-05-03T23:53:23Z,"Hmm, the comment in line 72 says completed txns are sorted by last offset, which seems to be what we want?"
114680532,2910,junrao,2017-05-04T00:05:19Z,The warn in line 312 now needs to include the tnx index.
114681599,2910,junrao,2017-05-04T00:15:57Z,"This can be done later. But reloading the snapshot when recovering every segment can be expensive. Since segments are recovered in order, it seems that we just need to load the snapshot on recovering the first segment."
114683318,2910,junrao,2017-05-04T00:35:04Z,"Hmm, in theory, it seems that we could have an abort marker with no open records before it?"
114685936,2910,junrao,2017-05-04T01:05:34Z,"Hmm, instead of using hard-coded READ_UNCOMMITTED, shouldn't we use the isolation_level in the Fetcher?"
114688209,2910,junrao,2017-05-04T01:37:56Z,"Hmm, not sure about the comment on HW. With KIP-101, typically the truncation point is >= the local HW."
114882654,2910,hachikuji,2017-05-04T20:42:54Z,"Ah, good catch. Forgot to update this after the transactional client patch was merged."
114899888,2910,hachikuji,2017-05-04T22:13:47Z,"Sorting by the first offset is what we want. This is used to find the first unstable offset which will always be the minimum first offset of all transactions. I couldn't find the comment you were referring to since the patch was updated. If it still exists, can you point me to it?"
114900931,2910,hachikuji,2017-05-04T22:20:48Z,"Hmm... good point. I had thought it was necessary since the first unstable offset could be lower than the high watermark, but we wouldn't be able to advance it any further from this write because any appended transaction markers could not have been replicated yet."
114904376,2910,junrao,2017-05-04T22:44:15Z,"Hmm, this logic may not be quite right for the follower. The follower could append batches for different pids in a single append call, if we reject all batches because a single batch is duplicated, the follower will miss records from other pids.

In theory, duplicates should never happen in the follower. If duplicates somehow already leaked into the leader, it's kind of hard to not take the duplicates in the follower since the follower's log is supposed to be identical with the leader. So, perhaps we can only return on duplicates if the append is on the leader. If duplicates are detect in the follower, just log a warning but proceed with the append?"
114906482,2910,junrao,2017-05-04T22:59:59Z,"It's a bit inconsistent that we serialize the value here, but the key inside appendControlRecord(). Perhaps it's better to serialize the value inside appendControlRecord() too?"
114916532,2910,junrao,2017-05-05T00:32:48Z,fetching up the the  => fetching up to the 
114918036,2910,hachikuji,2017-05-05T00:52:14Z,"The intent was to return a duplicate only if `isFromClient` is set (which would never be the case for the follower), but it looks like I left that out. I'll fix in the next commit."
114918859,2910,hachikuji,2017-05-05T01:04:36Z,"I was trying to keep `appendControlRecord` generic for future control record types. That said, it seems reasonable to add an `appendEndTxnMarker` which handles serialization of both the key and value."
115065007,2910,junrao,2017-05-05T18:51:10Z,Do we need to get position from segment.append()? We could also just get the size of the segment before appending.
115079600,2910,junrao,2017-05-05T20:17:21Z,It seems that undecidedFirstOffset should never be < unreplicatedFirstOffset?
115080853,2910,junrao,2017-05-05T20:24:56Z,Do we need to do this under lock since we are updating the state in ProducerStateManager?
115082326,2910,junrao,2017-05-05T20:33:58Z,Are the callers all properly synchronized on Log.lock?
115088857,2910,junrao,2017-05-05T21:14:23Z,epoch => producerEpoch ?
115090409,2910,junrao,2017-05-05T21:24:45Z,Should we print out an error in the else clause?
115094046,2910,hachikuji,2017-05-05T21:50:01Z,"I think it is possible. You may have something like this:

W1, W2, C2, C1

(where W1 is a write from producer 1, and C1 is a commit from producer 1).

Say the high watermark has reached C1 (which means the transaction from producer 1 is not yet visible). In this case, the first undecided offset will be W1 while the first unreplicated offset will be W2."
115097120,2910,hachikuji,2017-05-05T22:15:31Z,I added some code to print the unknown type id in the else case. Let me know if that seems reasonable.
115099271,2910,hachikuji,2017-05-05T22:35:37Z,"The example was a little wrong. The proper scenario is this:

W1, W2, C2

Say the high watermark is at C2. The first undecided offset will be W1, but the first unreplicated offset will be W2. Once the high watermark reaches C2+1, the first undecided offset will still be W1 and the first unreplicated offset will be empty."
634023546,10579,satishd,2021-05-18T04:06:08Z,`Config` definition will be moved to `KafkaConfig` later when default RLMM is integrated with the broker. 
640140543,10579,junrao,2021-05-26T21:40:19Z,"If the producer/consumer are configured incorrectly, we want to fail faster instead of retrying."
640140766,10579,junrao,2021-05-26T21:40:51Z,"Since producerManager and consumerManager are updated in a separate thread without holding lock, do they need to be volatile?"
640142495,10579,junrao,2021-05-26T21:44:24Z,Should we pass in time through the constructor?
640147224,10579,junrao,2021-05-26T21:54:14Z,Do we need this since it's in the caller already?
640148086,10579,junrao,2021-05-26T21:55:54Z,Waiting for each event to be consumed reduces throughput. Could we handle the expected metadata load with this?
640154886,10579,junrao,2021-05-26T22:08:46Z,remoteLogMetadataContext => remoteLogMetadata?
640157247,10579,junrao,2021-05-26T22:12:05Z,No need to cast to KafkaException.
640158657,10579,junrao,2021-05-26T22:15:25Z,Sending one event at a time reduces the batching benefit in the producer. Could this handle the expected metadata load?
640158995,10579,junrao,2021-05-26T22:16:09Z,Could we add a comment for this class?
640979693,10579,junrao,2021-05-27T21:28:06Z,typo ard
640996146,10579,junrao,2021-05-27T21:56:15Z,"Hmm, shouldn't we start with assignedMetaPartitions?"
640997181,10579,junrao,2021-05-27T21:57:24Z,"Hmm, assignedMetaPartitions may not change if assignedTopicPartitions changes. Should we still update assignedTopicPartitions in that case?"
641001043,10579,junrao,2021-05-27T22:04:49Z,From which offset does the consumer start fetching?
641029590,10579,junrao,2021-05-27T23:10:00Z,Should we only process events corresponding to assignPartitions?
641030221,10579,junrao,2021-05-27T23:11:42Z,"If a partition is moved to a different broker, we will need to bootstrap the remote state for the partition by consuming from the beginning of the remote metadata partition. How is that handled?"
641031767,10579,junrao,2021-05-27T23:15:58Z,The toString() method could change over time. Perhaps it's safer to compute a customized hashcode for topicIdPartition here.
641034498,10579,junrao,2021-05-27T23:24:16Z,"BOOTSTRAP_SERVERS_CONFIG should be prefixed with REMOTE_LOG_METADATA_CLIENT_PREFIX, right?"
641035399,10579,junrao,2021-05-27T23:26:54Z,Do we need this? It seems that it's easier to just duplicate the property for producer and consumer.
641093708,10579,junrao,2021-05-28T01:17:10Z,What's the purpose of waiting for consumption up to partitionToTargetEndOffsets?
642095193,10579,satishd,2021-05-30T15:53:03Z,Making volatile makes sense to me.
642095217,10579,satishd,2021-05-30T15:53:14Z,Done.
642095239,10579,satishd,2021-05-30T15:53:34Z,Removed the check as you suggsted.
642095245,10579,satishd,2021-05-30T15:53:40Z,This method is invoked from multiple threads for different topic partitions. This will not be a bottleneck as each partition's segments will be uploaded in a sequential manner.
642095258,10579,satishd,2021-05-30T15:53:47Z,It is needed as the method is declared with throws KafkaException. I am also fine with removing it as it is a RTE.
642095262,10579,satishd,2021-05-30T15:53:51Z,Multiple events are published by multiple threads and batching will occur in the producer. 
642095305,10579,satishd,2021-05-30T15:54:06Z,I will add that.
642095309,10579,satishd,2021-05-30T15:54:11Z,"Are you saying it should be initialized with `updatedAssignedMetaPartitions = new HashSet<>(assignedMetaPartitions);`. This is not needed as we are recomputing that set based on updatedReassignedPartitions.
"
642095314,10579,satishd,2021-05-30T15:54:14Z,"Nice catch, updated."
642095350,10579,satishd,2021-05-30T15:54:28Z,We have not yet added the code to store the consumed offset and start from those offsets whenever the consumer starts fetching from those partitions. We plan to add that in a subsequent PR.
642095355,10579,satishd,2021-05-30T15:54:33Z,"Right, I will add that check. "
642095359,10579,satishd,2021-05-30T15:54:36Z,This is not addressed in this PR. I planned to have a followup PR for these changes. I may use a different consumer for the newly subscribed partitions to build the state. 
642095367,10579,satishd,2021-05-30T15:54:44Z,"REMOTE_LOG_METADATA_CLIENT_PREFIX is just a prefix for generating client-ids for producer and consumer. If you are talking about the remote log metadata property prefix, It is assumed that the caller would have already removed those prefixes and sent the config. These prefixes are defined [here](https://github.com/apache/kafka/pull/10733/files/f185b4f86b457da35c21f1461fe1b1e1d73e4939#diff-31933b76c0578600db8744507e5c5ff840820d5a181a2a5d1e08c50bdc64ea6fR46)"
642095371,10579,satishd,2021-05-30T15:54:47Z,"This is to avoid duplicate entries for both the producer and consumer. We added that in the KIP earlier. 
If duplicating is the way we use at other places if any, I am fine with that."
642095384,10579,satishd,2021-05-30T15:54:49Z,This is leftover code for other changes that I was working on for handling partition moving to a new broker in another branch. I will remove it. 
642095431,10579,satishd,2021-05-30T15:55:05Z,Good point. I will add that.
643693257,10579,kowshik,2021-06-02T06:40:28Z,s/set in close state/closed
643693431,10579,kowshik,2021-06-02T06:40:47Z,s/metadataPartitionNo/metadataPartition
643694704,10579,kowshik,2021-06-02T06:43:10Z,It appears you could eliminate the additional `topicIdPartition` parameter and instead use the value returned by `remoteLogMetadata.topicIdPartition()` API. 
643695433,10579,kowshik,2021-06-02T06:44:35Z,It appears this class does not have unit tests currently. Is there a plan to add unit tests?
643698596,10579,kowshik,2021-06-02T06:50:35Z,Could you pls add a comment for this class?
643698781,10579,kowshik,2021-06-02T06:50:55Z,s/Millis/Ms
643705660,10579,kowshik,2021-06-02T07:02:30Z,I had the same question. It appears better to just duplicate the properties.
643707051,10579,kowshik,2021-06-02T07:04:58Z,"It seems that we have internal topics specified in `org.apache.kafka.common.internals.Topic` class.
Don't we want this new internal topic to be defined in the `Topic` class, together with other internal topics?"
643709758,10579,kowshik,2021-06-02T07:09:25Z,"Hmm, why do you need this exclusion to be true?"
643715269,10579,kowshik,2021-06-02T07:17:49Z,"Is this the timeout for how long you'd want the client to wait to consume the message that it produces to `__remote_log_metadata` topic? If yes, then don't we want this timeout to be unlimited i.e. we wait as long as it takes to consume the published event? "
643716426,10579,kowshik,2021-06-02T07:19:37Z,"`consumerProps` and `producerProps` are of type `Map`, therefore the `.toString()` is probably not readable. So you'd need to convert these into a comma-separated list sth like `K1=V1,K2=V2,...Kn=Vn`."
643725247,10579,kowshik,2021-06-02T07:32:18Z,"For readability, it'll be useful to place the positive case under `if` and negative case under `else`, such as:

```
if (callback.exception() == null) {
 ...
} else {
 ...
}
```"
643726313,10579,kowshik,2021-06-02T07:33:51Z,Could you pls document the state does this boolean represents?
643728635,10579,kowshik,2021-06-02T07:37:21Z,Could you pls mention what state does this boolean represent?
643730668,10579,kowshik,2021-06-02T07:40:20Z,Is it useful to assert that `record.key()` is empty before the key is ignored below?
643731568,10579,kowshik,2021-06-02T07:41:36Z,"Hmm, should you be setting `close` to true here? (it depends on the meaning of `close`, which I don't fully understand....)"
643736256,10579,kowshik,2021-06-02T07:48:14Z,"It appears that once this logic is implemented, there is probably no need to wait for `maybeWaitForPartitionsAssignment`. The reason is that whenever a partition is assigned, we will bootstrap the remote state by consuming from the beginning."
643738304,10579,kowshik,2021-06-02T07:51:00Z,"Do you really need this explicit lock? It seems you could just use `wait()` and `notify()` APIs on the `ConsumerTask` object instead, combined with `synchronized` keyword."
643740157,10579,kowshik,2021-06-02T07:53:28Z,s/noOfMetadataTopicPartitions/numMetadataTopicPartitions
643740418,10579,kowshik,2021-06-02T07:53:46Z,s/partitionNo/partition
643740610,10579,kowshik,2021-06-02T07:54:02Z,"s/No of.../Num of...

Also it feels overkill to me to log this message for each call."
643742067,10579,kowshik,2021-06-02T07:55:59Z,"Is topic necessary here, when UUID and partition is already sufficient input for the hash?"
643745043,10579,kowshik,2021-06-02T08:00:02Z,It seems these 2 attributes can be marked final if you call `Map.clear()` in `close()` instead of replacing the reference.
643746007,10579,kowshik,2021-06-02T08:01:17Z,s/logs/log ?
643749217,10579,kowshik,2021-06-02T08:05:53Z,I agree with the question here. This can become more expensive than it seems. The alternative is to pursue an asynchronous notification model to improve the throughput.
643754935,10579,kowshik,2021-06-02T08:14:07Z,Could you pls add a comment on what state does `close` represent?
648927106,10579,satishd,2021-06-10T07:36:25Z,"As we discussed offline, we see the benefit of keeping several common client config like security to be shared across producer and consumer props avoiding any copy/paste mistakes. 
User has an option not to use common client configs and use the respective producer and consumer configs. "
649387317,10579,satishd,2021-06-10T17:30:15Z,I plan to add this once RLMM is called from remote log layer classes. I wanted this change to be self contained for now.
649387744,10579,satishd,2021-06-10T17:30:37Z,We do not want this to be completely blocked as we want to release the remote log thread after a specific timeout in case of any intermittent issues so that other partitions tiring can proceed.  
649387805,10579,satishd,2021-06-10T17:30:40Z,"We are using HashMap for these instances and it prints k,v format. Are you suggesting that this map implementation may change as it is of type Map and need to put the right toString. We can change the reference type to HashMap for clarity if needed. "
649388017,10579,satishd,2021-06-10T17:30:51Z,I do not think that check is really needed here. 
649388066,10579,satishd,2021-06-10T17:30:53Z,"It indicates whether the closing process has been started or not. If it is set as true, consumer will stop consuming messages and it will not allow partition assignments to be updated.
Updated the java doc of close. 
"
649388122,10579,satishd,2021-06-10T17:30:57Z,I wanted to have a separate lock instance specifically for the assignments and the respective processing. It gives better clarity and separations even if we add any other logic by taking lock on this instance. 
649388430,10579,satishd,2021-06-10T17:31:13Z,Good point. We can skip topic as topic-id is sufficient here. 
649388497,10579,satishd,2021-06-10T17:31:16Z,I went with assigning empty map as Map.clear() needs to go through all the entries and dereference them. Another way is to leave the map as it is and set the cose state and do not allow any operation when it is closed but it will have a check for each call. 
649390143,10579,satishd,2021-06-10T17:32:50Z,Done
649390214,10579,satishd,2021-06-10T17:32:56Z,Done
649390292,10579,satishd,2021-06-10T17:33:03Z,Done
649390516,10579,satishd,2021-06-10T17:33:26Z,Done.
649390677,10579,satishd,2021-06-10T17:33:41Z,Done
650456266,10579,satishd,2021-06-13T01:49:18Z,Done
656734732,10579,ccding,2021-06-23T03:38:22Z,Can we avoid making the variable name the same as the function name?
656735781,10579,ccding,2021-06-23T03:42:06Z,"why is the variable name `partitions`, while the one above for `addAssignmentsForPartitions` is `allPartitions`?

also why one is `Set` and the other is `HashSet`?"
656737032,10579,ccding,2021-06-23T03:46:28Z,I think in the codebase we use `Ms` more often than using `Millis`
656737294,10579,ccding,2021-06-23T03:47:23Z,should this be fixed or configurable in `rlmmConfig`?
656737888,10579,ccding,2021-06-23T03:49:16Z,`Id` -> `If`
656738264,10579,ccding,2021-06-23T03:50:38Z,"We may want a better variable name here. e.g., `isClosing` or `closed` or something else."
656740556,10579,ccding,2021-06-23T03:57:00Z,"multiple threads are reading and writing `close`, which is not thread safe"
656740892,10579,ccding,2021-06-23T03:58:12Z,"why is this set to 30, rather than another number like 10, 50, 100?"
656741851,10579,ccding,2021-06-23T04:01:33Z,is the check necessary?
656743129,10579,ccding,2021-06-23T04:06:13Z,can you explain what this variable means?
656744938,10579,ccding,2021-06-23T04:12:18Z,also here. Why are the variable names different? One is `updatedPartitions` and the other is `partitions`
656747158,10579,ccding,2021-06-23T04:19:48Z,What is the cost of consuming from the beginning if the remote metadata partition grows huge?
656748094,10579,ccding,2021-06-23T04:22:44Z,"The function name is a little confusing with variable names in the same class, e.g., `assignPartitions`, `assignedTopicPartitions`. I am not sure if it would be better to rename `assignedPartition` to `isAssignedPartition`."
656748535,10579,ccding,2021-06-23T04:24:11Z,not good to use the same name for a variable and a function.
656749199,10579,ccding,2021-06-23T04:26:28Z,greater than or equal to?
656749604,10579,ccding,2021-06-23T04:27:45Z,will the exception be caught by your own catch in line 74? maybe move it out of the `try` block?
656751271,10579,ccding,2021-06-23T04:32:53Z,out of curiosity: why we don't do the check within the `new KafkaException(...)` call
656751707,10579,ccding,2021-06-23T04:34:24Z,is the check necessary?
656752286,10579,ccding,2021-06-23T04:36:07Z,new line at the end of the file
656761165,10579,ccding,2021-06-23T05:03:18Z,"function names `addRemoteLogSegmentMetadata`, `updateRemoteLogSegmentMetadata`, and `putRemotePartitionDeleteMetadata` don't seem very consistent. Is there a way to improve it?"
656764820,10579,ccding,2021-06-23T05:13:51Z,"Here has a race condition. It is possible `close=false` before calling `ensureInitializedAndNotClosed()` and the `close()` function call by another thread has completed before calling `remotePartitionMetadataStore.listRemoteLogSegments(topicIdPartition, leaderEpoch)`.

I think we should always grab a read lock before calling `ensureInitializedAndNotClosed();`, or do some other fancy things."
657655419,10579,satishd,2021-06-24T06:16:27Z,"`close/closing` is already volatile and its state is immediately reflected in other threads. 
In `close()` method, the consumer is invoked with `wakeup()`, and the other thread may receive `WakeupException` if it is executing `poll()` or earlier than that. If it is after `poll` then the next check of `close/closing` allows to come out and finally close the consumer.

`updateAssignmentsForPartitions` and `close()` methods can not be called concurrently as  it is alreadyhandled by `TopicBasedRemoteLogMetadataManager`."
657655825,10579,satishd,2021-06-24T06:17:20Z,"When the code was refactored, it went with the caller method arg names. thanks for catching these. "
657655966,10579,satishd,2021-06-24T06:17:37Z,"What about `must be less than the partition count`, conveys the intent clearly. "
657656102,10579,satishd,2021-06-24T06:17:55Z,"If you are asking about the earlier catch block, that will not cover the exceptions like receiving from broker etc.  The earlier catch block is applicable only until the record is added to the accumulator. "
657657314,10579,satishd,2021-06-24T06:20:42Z,I did not add the checks for these methods as they will not be invoked in parallel when close is called. But I agree to have the guards here with read lock. 
657670757,10579,satishd,2021-06-24T06:47:34Z,This is a temporary change. I plan to have a config with a default value. 
657671527,10579,satishd,2021-06-24T06:48:51Z,What about `isPartitionAssigned`?
657674359,10579,satishd,2021-06-24T06:53:57Z,"Javadocs explain the behaviorr in detail.
`addRemoteLogSegmentMetadata` - adds a new entry. 
`updateRemoteLogSegmentMetadata ` - updates an existing entry. 
`putRemotePartitionDeleteMetadata ` - adds or updates an existing entry, put is generally used for that purpose. If this is not so clear, another option can be `addOrUpdateRemotePartitionDeleteMetadata`.
"
657683814,10579,satishd,2021-06-24T07:10:29Z,"As we discussed offlime, this may not become a bottleneck but we will make respective RLMM APIs asynchronous so that the APIs are extensible and implementors can provide async behavior.FIled [KAFKA-12988](https://issues.apache.org/jira/browse/KAFKA-12988) to track this issue.  "
658361692,10579,satishd,2021-06-24T23:58:23Z,I plan to have a config with a default value in a follow-up PR. 
660802584,10579,junrao,2021-06-29T16:58:25Z,"Hmm, why is this a daemon thread? It seems that we want to coordinate the shutdown of the thread."
660810740,10579,junrao,2021-06-29T17:10:21Z,"Hmm, should we throw an exception in this case so that the caller knows the operation has failed?"
660812883,10579,junrao,2021-06-29T17:13:31Z,"Similar to the above,  should we throw an exception in this case so that the caller knows the operation has failed?"
660836440,10579,junrao,2021-06-29T17:45:05Z,We need to unblock the wait if we are closing the consumer.
660975970,10579,junrao,2021-06-29T21:28:57Z,"Is this necessary since immediately after this check, the consumer task could be closed. Ditto in other places."
660980922,10579,junrao,2021-06-29T21:37:50Z,"Is this necessary? Once producer is closed, send() will throw an exception."
660990345,10579,junrao,2021-06-29T21:54:51Z,How is this different from RemoteLogMetadataCache? It seems that it's just a wrapper over RemoteLogMetadataCache?
660991871,10579,junrao,2021-06-29T21:58:01Z,Could we add a comment for this class?
661000698,10579,junrao,2021-06-29T22:17:24Z,Why do we need to initialize in a separate thread?
661002879,10579,junrao,2021-06-29T22:22:17Z,"Since we have a lock, could we just make closing a boolean?"
661629252,10579,junrao,2021-06-30T16:20:30Z,MILLIS => MS to be consistent with other places. Ditto in a few other places.
661641738,10579,junrao,2021-06-30T16:36:54Z,Is this needed since we log all configs when creating KafkaConfig already?
661642633,10579,junrao,2021-06-30T16:38:10Z,"Other plugins on the broker may also need a bootstrap_server config. To distinguish them, it would be useful to add a prefix that's specific to remote storage."
661672097,10579,junrao,2021-06-30T17:20:36Z,"It seems that we need to automatically create metadata topic in RLMM implementation, not just in tests."
661673976,10579,junrao,2021-06-30T17:23:08Z,120s seems quite long. Do we need to wait that long?
661687251,10579,junrao,2021-06-30T17:42:56Z,typo nto
661687392,10579,junrao,2021-06-30T17:43:09Z,This sentence doesn't read well.
661693600,10579,junrao,2021-06-30T17:52:25Z,This should be for consumer?
661698279,10579,junrao,2021-06-30T17:59:31Z,Why are we testing against -1 here but 0 above?
663808257,10579,satishd,2021-07-05T10:06:16Z,"Good catch, updated it. "
663809083,10579,satishd,2021-07-05T10:07:34Z,There are two implementations about this class for both `RemoteLogMetadataCache` and `TopicBasedRemoteLogMetadataManager`. There are common tests in `RemoteLogSegmentLifecycleTest `that we want to run for both of them. 
663809683,10579,satishd,2021-07-05T10:08:27Z,It was required to be retried until the topic is successfully created. I added the logic to check for topic creation too. 
663811753,10579,satishd,2021-07-05T10:11:33Z,I was planning to add that later as mentioned earlier. I updated with the required changes in latest commit.
663813208,10579,satishd,2021-07-05T10:13:40Z,Updated with a comment in the code. 
663818344,10579,satishd,2021-07-05T10:21:39Z,Updated with a comment.
663819484,10579,satishd,2021-07-05T10:23:26Z,"I guess 60s may be sufficient, updated with that."
663821465,10579,satishd,2021-07-05T10:26:46Z,`closing` is accessed in `initializeResources` and we do not need to take a lock there.  I would like to keep this as `AtomicBoolean` which addresses that and it is easy to understand the semantics. 
664630725,10579,ccding,2021-07-06T14:53:56Z,Is it possible two threads call `close()` concurrently?
664632598,10579,ccding,2021-07-06T14:55:56Z,our of curiosity: why we have `L` here but not https://github.com/apache/kafka/pull/10579/commits/77da4ebff5228d1c8938105d50b6b15a5223b294#diff-9f35ec72323f704125b5ee6d35a230de7f28f8c448a706eec2946fa46a12cfedR63? 
664866782,10579,junrao,2021-07-06T20:45:42Z,The test for closing seem unnecessary since closing can't change while synchronized on assignPartitionsLock.
664868563,10579,junrao,2021-07-06T20:49:01Z,"Is RemoteLogSegmentLifecycleManager used for tests only? If so, could we move it under tests?"
664905089,10579,junrao,2021-07-06T22:00:41Z,"> It was required to be retried until the topic is successfully created. I added the logic to check for topic creation too.

I am still not sure why we need to initialize in a separate thread. If we can't create the metadata topic or instantiate the producer/consumer due to wrong configurations, we want to fail fast by throwing an error to shut down the broker."
664907494,10579,junrao,2021-07-06T22:06:20Z,Should we verify that the number of partitions in the existing topic matches the configuration?
664908297,10579,junrao,2021-07-06T22:08:11Z,"Hmm, we don't want to retry forever. If there is a configuration error, we want to fail fast."
664911197,10579,junrao,2021-07-06T22:14:50Z,"Hmm, why is this needed since initializeResources() does this already?"
664912870,10579,junrao,2021-07-06T22:19:02Z,It seems producer credentials are closer for the admin client.
664913921,10579,junrao,2021-07-06T22:21:34Z,Is this comment addressed?
664914536,10579,junrao,2021-07-06T22:22:55Z,createMetadataTopic() is no longer used.
664970217,10579,satishd,2021-07-07T01:11:27Z,`RemoteLogSegmentLifecycleManager` is already under tests.
664976317,10579,satishd,2021-07-07T01:32:10Z,"This loop can run after `assignPartitionsLock.wait()` call which might have been notified from `close()` method, we should have a `closing` check to get out of the loop. 

We can have a more aggressive check to return from here when `closing` is true, I will add that. "
664980913,10579,satishd,2021-07-07T01:46:58Z,`close()` will not be called concurrently here. 
664983511,10579,satishd,2021-07-07T01:53:00Z,"I prefer adding L for longs, which I missed at other declaration. afaik, that does not cause any issues as it gets automatically converted via a widening conversion to a `long`. The compiler takes care of not allowing numbers that may get truncated from `int` to `long` widening. Thanks for catching it, I will make it consistent by adding it.  
"
665259927,10579,satishd,2021-07-07T10:50:18Z,"@junrao The reason why we need to initialize in a different thread here is that RLMM will be created and `configure()` will be called before the broker starts accepting the requests. So, we can not call topic creation requests in the same thread as the brokers are not yet up. 

Another way to do this is to have this topic as an internal topic and it will be auto created whenever it is accessed. afaik, creating producer and consumer instances can be done without the brokers up and running and they will not trigger a request to auto creation of remote log metadata topic. Consumer assignment will send a metadata request which will trigger auto creation of topics. This assignment on consumer is done only when RLMM receives callback thorough the broker about leader and isr updates from the controller. This is what we had in pre-2.7 implementation but we saw an intermittent deadlock issue but I do not see that happening on trunk.  

In the current PR, I will make the existing producer manager and consumer manager in configure() and have the topic creation done in the tests. 

I will have a quick follow-up PR with internal topic changes and remove the topic creation code from tests. 
"
665299398,10579,satishd,2021-07-07T11:54:00Z,"`RemoteLogMetadataManager.configure(Map<String, ?> configs)` is always invoked with stripping the RLMM  prefix. ""bootstrap.servers"" property is sent as part of the configs here and any registered RLMM plugin will receive this property. 
"
665531236,10579,satishd,2021-07-07T16:29:41Z,I made the mentioned changes for this PR in the latest commit. 
665544995,10579,kamalcph,2021-07-07T16:48:33Z,This condition should be inverted.
665555878,10579,satishd,2021-07-07T17:04:26Z,"Good catch, we do not really need this check here as it is already guarded in `TopicBasedRemoteLogMetadataManager`."
666372073,10579,junrao,2021-07-08T17:03:35Z,"@satishd : To create a topic, you don't need a particular broker to be up. You just need to be able to access the bootstrap brokers. Auto creating this topic on access is a bit weird since it's not truly an internal topic and it is just an implementation detail of RLMM. So, it makes more sense for topic-based RLMM to create it."
666697373,10579,satishd,2021-07-09T06:15:00Z,"@junrao 
As I mentioned earlier, RLMM is created before broker starts accepting the requests. So, when RLMM is getting initialized and tries to create a topic in the same thread, then none of the brokers(including the brokers related to the bootstrap-servers config) will be available for taking any admin client requests for topic creation. That is why I was doing this in a different thread as it allows the broker/controller to comeup and accept the admin client request for topic creation.
"
667102624,10579,junrao,2021-07-09T17:21:12Z,"@satishd : My understanding is that when we enable remote storage, we will do that through a rolling upgrade. So, at any given time, there is at most a single broker being down. Therefore, as long as the bootstrap broker list contains more than one broker, operations like creating topics can still be done while a single broker is starting."
667427888,10579,satishd,2021-07-11T06:04:21Z,"@junrao: That is a good point. But that is valid only for upgrade scenario.

There are two scenarios here.
1) Upgrade path
2) Fresh install/deploy with the release containing this feature.

What you said makes sense for upgrade path. But ""bootstrap.servers"" list is configured with the local broker endpoint only in which RLMM is getting initialized. So, if we try to initialize RLMM in the same thread, it wont be able to connect to the local broker as it is not yet started to accept the broker API requests. One way to address this is to give `bootstrap.servers` with more than one broker's endpoint. This will also put a limitation to create a cluster with one broker instance for test/demo environments. 

When users install/deploy a fresh environment with the release containing this feature, there should not be a restriction to do rolling restarts with the configuration enabled. Please let me know if I am missing anything here."
670800796,10579,junrao,2021-07-15T20:58:17Z,Should halt the jvm in this case? Ditto below when the partition count doesn't match.
670820515,10579,junrao,2021-07-15T21:34:47Z,"Since bootstrap_servers is used for the internal producer/consumer, should bootstrap_servers be defined with a prefix of  REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX?"
670823373,10579,junrao,2021-07-15T21:40:10Z,Does pendingAssignPartitions need to be synchronizedSet since it's accessed under a lock?
670947570,10579,satishd,2021-07-16T03:59:51Z,"Yes, we are already doing that [here](https://github.com/apache/kafka/pull/10579/files#diff-50bf08ff4e92c16dfbea9239c89467a3ebf5fa38e5f4ea44ed4acff80013136eR452) whenever this class is accessed with/for remote log metadata operations."
671094510,10579,satishd,2021-07-16T09:13:18Z,It is needed because `pendingAssignPartitions` is updated in both `onPartitionLeadershipChanges` and `onStopPartitions` methods which can happen concurrently as both of them take read lock.
671367565,10579,satishd,2021-07-16T16:04:32Z,"No, it is not needed to be sent with any prefix(like common_client, producer or consumer) because ""bootstrap.servers"" property is sent for any registered RLMM plugin but not only limited to the default RLMM. I will update the KIP with these details. "
671381765,10579,junrao,2021-07-16T16:27:07Z,"Since producerManager is initialized asynchronously, how do we deal with the case when the producerManager is not ready when an event needs to be published?"
671383087,10579,junrao,2021-07-16T16:29:09Z,"Hmm, ""bootstrap.servers"" makes sense for a topic based RLMM since it depends on Kafka. Why do we require ""bootstrap.servers"" in other RLMM implementations?"
671415369,10579,satishd,2021-07-16T17:25:57Z,"That is a good point. We plan to improve the semantics here. 

Earlier, we plan to introduce RetriableException for RLMM and RSM so that callers can have an option to know whether they can retry or not. In the case of initialization is not yet complete, RetriableException can be thrown caller can retry based on backoff. RLMM can send non retriable exception if it is in closing state and there will not be any retries.  

Another way to handle this is to take these events and store them in in-memory queue and return Future. These Futures will be considered successful if initialization is successful and the events are published to the topic.

I plan to address these in a followup PR while these APIs are integrated with RLM, filed https://issues.apache.org/jira/browse/KAFKA-13097. "
671419080,10579,satishd,2021-07-16T17:32:24Z,I am not sure about usecases with other RLMM but it allows them to connect to the broker. 
671516460,10579,junrao,2021-07-16T20:46:16Z,"What about client security related properties? It's weird that we pick up ""bootstrap.servers"" from one prefix, but the corresponding security properties under a different prefix. If we do provide the security related properties in the same prefix, they seem to be duplicated from those under prefix REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX, REMOTE_LOG_METADATA_COMMON_CLIENT_PREFIX or REMOTE_LOG_METADATA_CONSUMER_PREFIX."
125019733,3325,dguy,2017-06-30T11:38:49Z,this is unused
125019734,3325,dguy,2017-06-30T11:38:50Z,This is unused too. We should have a test for this.
125019810,3325,dguy,2017-06-30T11:39:29Z,This is in a public package so we should provide some javadoc
125020051,3325,dguy,2017-06-30T11:41:18Z,"Same as above - needs javadoc. I guess it is intended for users? 
We should at least have some tests that use it."
125020092,3325,dguy,2017-06-30T11:41:40Z,nit: extra space between `abstract` and `class`
125020150,3325,dguy,2017-06-30T11:42:07Z,javadoc
125020184,3325,dguy,2017-06-30T11:42:20Z,javadoc
125020664,3325,dguy,2017-06-30T11:45:39Z,This should probably default to `NoOpStateRestoreListener` otherwise i think it is going `NullPointerException` if the user doesn't add a listener
125020841,3325,dguy,2017-06-30T11:46:45Z,I think we should probably do a null check here and throw. Setting the listener to null doesn't seem valid to me
125021380,3325,dguy,2017-06-30T11:49:56Z,Rather than setting this to `null` if it isn't an instance of `BatchingStateRestoreCallback` perhaps you could set it to an instance of an internal class that implements `BatchingStateRestoreCallback`. The benefit being that the `null` check is then only done once here and not also in `restoreAll` 
125021406,3325,dguy,2017-06-30T11:50:09Z,See comment above in ctor
125021525,3325,dguy,2017-06-30T11:50:57Z,Unit tests for this class?
125022092,3325,dguy,2017-06-30T11:54:42Z,Again i think we could use an internal implementation (probably the same one) for `BatchingStateRestoreCallback`. So here we always have a `BatchingStateRestoreCallback` and we can get rid of the `if(...){...}else{...}` and the extra `if(!restoreRecords.isEmpty)` I think that would make the code easier to follow.
125022610,3325,dguy,2017-06-30T11:58:16Z,Pass this in as a ctor param rather than constructing it? The `stateRestoreCallback` is only used to create the `CompositeRestoreListener`
125036770,3325,dguy,2017-06-30T13:20:23Z,Do we need to synchronize access to `stateRestoreListener`? It can be set by a user thread and used by the `StreamThread`
125037238,3325,dguy,2017-06-30T13:22:33Z,nit: `private`
125037487,3325,dguy,2017-06-30T13:23:46Z,nit: keep fields with the same access level together
125040681,3325,dguy,2017-06-30T13:38:20Z,nit: `Collections.singletonList(...)` or `Utils.mkList(..)`
128017007,3325,bbejeck,2017-07-18T15:51:41Z,"Ack, removed not needed as it's passed through to `StreamThread`"
128018122,3325,bbejeck,2017-07-18T15:55:41Z,"Don't think so, the `stateRestoreListener` set by user is passed through to the `StreamThread`.  Javadoc in `StateRestoreListener` states that it expects operations to be stateless."
128018683,3325,bbejeck,2017-07-18T15:57:34Z,"Ack, added integration test"
128018746,3325,bbejeck,2017-07-18T15:57:47Z,Ack
128018769,3325,bbejeck,2017-07-18T15:57:51Z,Ack
128236093,3325,dguy,2017-07-19T12:53:08Z,nit: my preference is to mark all method params as `final`
128250446,3325,dguy,2017-07-19T13:47:18Z,`final` and there is an extra space
128251187,3325,dguy,2017-07-19T13:50:01Z,nit: extra line
128251819,3325,dguy,2017-07-19T13:52:27Z,I think we should probably add a unit test in `RocksDBStoreTest` to prove that this works.
128253384,3325,dguy,2017-07-19T13:58:04Z,"I'd prefer to see these broken down into multiple smaller tests, i.e, you are effectively testing 4 different methods in each test. Ideally a unit test is only testing a single method."
128254391,3325,dguy,2017-07-19T14:01:35Z,nit: `final` + next line and might as well do the previous while you are at it ;-)
128256200,3325,dguy,2017-07-19T14:07:39Z,"could we extract these 3 lines into a method, say `verifyCallbackStatesCalled` or something better! the same block of code is repeated 3 times in the test so would make it easier to grok"
128303773,3325,bbejeck,2017-07-19T16:51:49Z,Ack - agreed
128303812,3325,bbejeck,2017-07-19T16:51:55Z,Ack
128414835,3325,guozhangwang,2017-07-20T02:48:34Z,"The javadoc may read a bit hard for end users since 1) `internal threads assignment` is not known to them at all, 2) `conclusion of restoring a statestore` is also a mess up.

From their pov (not familiar with concept of task, etc) we can just state that `... set the listener which is triggered whenever a state is being restored in order to resume processing..`."
128415004,3325,guozhangwang,2017-07-20T02:50:14Z,"Same as above, do not need to mention ""all internal threads"". Just emphasize it is triggered whenever a state is being restored is fine."
128415203,3325,guozhangwang,2017-07-20T02:52:04Z,"nit: `not supported, please use...`"
128415747,3325,guozhangwang,2017-07-20T02:58:16Z,"I think we can use the following as part of Javadoc:

```
    /**
     * See {@link StateRestoreCallback#restore(..)}
     * 
     * more javadocs if needed.
     */
```

As a reference see `KafkaProducer` and `Producer` in clients."
128415932,3325,guozhangwang,2017-07-20T03:00:12Z,"I did not catch it in the KIP wiki but.. the class names are a bit inconsistent here:

```
AbstractBatchingRestoreCallback
AbstractNotifyingRestoreCallback
```

Better be either

```
AbstractBatchingRestoreCallback
AbstractRestoreCallback
```

or 

```
AbstractNotifyingBatchingRestoreCallback
AbstractNotifyingRestoreCallback
```

WDYT?"
128415948,3325,guozhangwang,2017-07-20T03:00:30Z,"Ditto as above, we can refer to the javadocs of the base interface here."
128416068,3325,guozhangwang,2017-07-20T03:01:46Z,"Seems in the library, if it is determined a `BatchingStateRestoreCallback` at runtime we then would never call the `restore` function ever. Is this true and will be future forever? If yes we should state it in the java doc."
128416128,3325,guozhangwang,2017-07-20T03:02:26Z,Does this need to be in `o.a.k.streams.state` or this package? I'm just wondering..
128416252,3325,guozhangwang,2017-07-20T03:03:55Z,"Better state ""when calling `setState...` in \@code KafkaStreams, the passed instance is expected to be stateless since..""

Because not everyone understand what does ""... for reporting all state store recovery.."" means, stating from the API point of view would be easier to understand. Ditto elsewhere."
128416789,3325,guozhangwang,2017-07-20T03:10:02Z,`In this case the size of the batch is whatever the value of the MAX_POLL_RECORDS is set to.` Is this really the case?? It is an upper bound but not necessary the exact value right?
128603596,3325,bbejeck,2017-07-20T19:00:07Z,Ack
128603630,3325,bbejeck,2017-07-20T19:00:15Z,Ack
128607545,3325,bbejeck,2017-07-20T19:18:41Z,"Agreed, I think the second option is best."
128607578,3325,bbejeck,2017-07-20T19:18:51Z,Ack
128607619,3325,bbejeck,2017-07-20T19:19:00Z,Ack
128607645,3325,bbejeck,2017-07-20T19:19:09Z,Ack
128607671,3325,bbejeck,2017-07-20T19:19:16Z,Ack
128607701,3325,bbejeck,2017-07-20T19:19:25Z,Ack
128607714,3325,bbejeck,2017-07-20T19:19:30Z,Ack
128611709,3325,bbejeck,2017-07-20T19:37:21Z,"Maybe, we could also move `StateStore`, `StateStoreSupplier`, `StateStoreCallback` as well.  Let's see what others think."
128612612,3325,bbejeck,2017-07-20T19:41:59Z,Ack
128614536,3325,bbejeck,2017-07-20T19:51:06Z,Ack
128642317,3325,guozhangwang,2017-07-20T22:05:53Z,"If we start from scratch then maybe these would be better be in `state`, but they have been added to `processor` and moving them would be incompatible changes. So I'm more concerning about the newly added classes."
128642627,3325,guozhangwang,2017-07-20T22:07:36Z,Maybe the parameter descriptions are not needed as well? Ditto elsewhere.
128643629,3325,guozhangwang,2017-07-20T22:13:58Z,We can define two static variables of `NoOpStateRestoreListener` and `NoOpStateRestoreCallback` instead of creating a new instance multiple times.
128646844,3325,guozhangwang,2017-07-20T22:34:54Z,"For `reportingStoreListener`, better rename it to `globalStoreListener` as it is the instance-level listener, but it is not necessarily used for reporting only."
128647685,3325,guozhangwang,2017-07-20T22:40:22Z,"Also code structure wise I'm wondering if it is easier to keep the per-store callback/listener and the global listener in two separate classes than keeping them in this `composite` class? Then we can do:

```
perStoreListener.onRestoreStart();
if (globalStoreListener != null) {
    synchronized (globalStoreListener) {   // since it can be accessed by concurrent threads
        globalStoreListener.onRestoreStart();
    }
}
```

I feel this null-check would be more performant than the no-op function call?"
128648024,3325,guozhangwang,2017-07-20T22:42:40Z,"As mentioned above, I'm wondering if it is better to not use the `set` function of global listener in this composite class but keep it in a separate class? Also since the global listener could be accessed by concurrent threads of the instance, does it need to be synchronized?"
128648103,3325,bbejeck,2017-07-20T22:43:09Z,Ok fair enough I can move it over.  My only concern is that it could be slightly confusing.
128648481,3325,guozhangwang,2017-07-20T22:45:19Z,Hmm... should we ever expect the passed in callback to ever be `null`? If it is really null then no data will ever be restored right?
128649545,3325,guozhangwang,2017-07-20T22:52:52Z,"Instead of using this separate class and do the `instanceof` check on each call (which maybe expensive), maybe we could just have a `WrappedBatchingStateRestoreCallback` which only takes the non-batching `StateRestoreCallback` in constructor and then in `restoreAll` always do the for-loop, and in places that we need it (seems we only have two callers) we can do sth. like

```
internalBatchingRestoreCallback = stateRestoreCallback instanceof BatchingStateRestoreCallback ? stateRestoreCallback : WrappedBatchingStateRestoreCallback(stateRestoreCallback);
```

Just once."
128651168,3325,guozhangwang,2017-07-20T23:04:14Z,"I'm thinking that we can simply this logic a bit:

1) in line 124 above, when `needsRestoring.put(topicPartition, restorer);` call `restorer.restoreStarted`.
2) then we can remove the `restoreStarted` boolean in `StoreRestorer` and also the line here.
"
128652257,3325,guozhangwang,2017-07-20T23:12:47Z,"This logic seems a bit complex to me, and also if we return at line 229 `restoreBatchCompleted` is not called as well. Is this correct? How about:

```
restoreRecords = new list..
nextPosition = -1;
for (...) {
    if (restorer.hasCompleted) {
        nextPosition = record.offset();
        break;
    } else {
        restoreRecords.add(...);
    }
}

if (nextPosition == -1)
    nextPosition = consumer.position(restorer.partition());

if (!restoreRecords.isEmpty()){
    restorer.restore(restoreRecords);
    restorer.restoreBatchCompleted(currentPosition, records.size());
}

return nextPosition;
```"
128652963,3325,guozhangwang,2017-07-20T23:17:34Z,"Following my comments above, we can rename to `setGlobalStateRestoreListener` to make it clear."
128653176,3325,guozhangwang,2017-07-20T23:19:29Z,"These two functions can be merged into one?

```
restore(..) {
    listener.restoreAll();
    listener.onBatchRestored();
}
```

See my other comments on the `StoreChangelogReader.java` class."
128653276,3325,guozhangwang,2017-07-20T23:20:19Z,Ditto above: `setGlobalStateRestoreListener`
128653708,3325,guozhangwang,2017-07-20T23:23:42Z,"One caveat of letting users to set it via code is that, we cannot forbid users to call this function after calling `streams.start()`, in which case the behavior would be bad. Also people can set it multiple times which is also not suggested.

I'm now thinking maybe we should enforce users to set this global listener via configs? cc @dguy @bbejeck @mjsax WDYT."
128654031,3325,guozhangwang,2017-07-20T23:26:02Z,"`prepareForBulkload` will always be true here, right?"
128654227,3325,guozhangwang,2017-07-20T23:27:35Z,"I do not think we need this variable at all, since as mentioned above when initiating it will always be true, and that is the only place this variable is ever read."
128654443,3325,guozhangwang,2017-07-20T23:29:00Z,we do not need to set `open = true` here again.
128655290,3325,guozhangwang,2017-07-20T23:36:26Z,"Personally I'm not a big fan of this integration test, since I felt that the its coverage has already been subsumed by unit tests. We should only consider integration tests for some end-to-end behavior that involves multiple modules to interact with each other, otherwise unit tests should be used per-module."
128656227,3325,guozhangwang,2017-07-20T23:44:14Z,Actually I was really just asking for people's opinions :) the cons are that these classes will be in different packages which may looks a bit weird.
128700639,3325,dguy,2017-07-21T07:33:05Z,"If we put it in config then they lose the ability to use capture any objects/state etc in their application.
We could always only allow the listener to be set when KafkaStreams is in the `CREATED` state and throw an exception if it isn't."
128779376,3325,bbejeck,2017-07-21T14:44:10Z, 
128800005,3325,bbejeck,2017-07-21T16:05:22Z,Ack
128800078,3325,mjsax,2017-07-21T16:05:43Z,"In JavaDoc this markup is not needed
 1. between the text and the parameter list, it will insert some space automatically
 2. `<p>` is just use to start new paragraphs (but JavaDoc is not HTML, there is no closing `</p>`)"
128800519,3325,mjsax,2017-07-21T16:08:08Z,"We could do a small KIP and move the classes (preserving the old ones as deprecated). Overall, I don't have a strong opinion."
128800688,3325,mjsax,2017-07-21T16:09:03Z,Nit: why `code` but not `link` ?
128800950,3325,mjsax,2017-07-21T16:10:23Z,Nit: `.` missing at end of sentence
128800998,3325,mjsax,2017-07-21T16:10:41Z,Nit: remove unnecessary blank.
128802000,3325,bbejeck,2017-07-21T16:16:03Z,"Ack on the name.

I think we should keep `composite` class as it keeps implementation details out of the `StateRestorer` class which doesn't need to know the details of restoring notification.  

As for synchronizing we can do that from within the `composite` class as well.  Although we specify in the javadoc it's expected the `globbalStoreListener` is stateless and implementors will need to provide synchronization if needed.  

If you insist I can remove the composite class and/or synchronize the calls on the `globalStoreListener`"
128802185,3325,bbejeck,2017-07-21T16:17:00Z,Ack
128807922,3325,bbejeck,2017-07-21T16:46:35Z," Ack, but I thinking some more we should replace the default value with  `Objects.requireNonNull`  "
128809089,3325,bbejeck,2017-07-21T16:52:30Z,"Updated the set method to reflect the new name.  

Comments on the `composite` class and synchronization same as above."
128815139,3325,bbejeck,2017-07-21T17:22:11Z,Ack
128829007,3325,guozhangwang,2017-07-21T18:22:36Z,"Re synchronization: enforcing users to do sync themselves inside the function is fine, I did not see `implementors will need to provide synchronization if needed` so there may be a mis-understanding. Could you make that statement more clear in javadocs?

Re separating classes: One motivation I had is that, currently we did one ""instanceof"" for each call, and if the global listener is not set we still call a `no-op` listener; this does not seem optimized for me. Instead we can do a `null` check or even a boolean flag indicating if a global listener is ever set already. That would be more performant? If you can do that inside this composite class I think we could keep it centralized."
128843230,3325,bbejeck,2017-07-21T19:27:35Z,"Ack, good catch."
128847495,3325,bbejeck,2017-07-21T19:49:37Z,Ack
128849307,3325,bbejeck,2017-07-21T19:59:39Z,Ack
129029991,3325,bbejeck,2017-07-24T13:10:02Z,leaving  as is based on offline-conversation
129030050,3325,bbejeck,2017-07-24T13:10:17Z,Ditto from above
129030086,3325,bbejeck,2017-07-24T13:10:26Z,Ditto from above
129030265,3325,bbejeck,2017-07-24T13:11:13Z,"Ok, I'll take it out."
129090134,3325,bbejeck,2017-07-24T16:45:27Z,Ack
129090501,3325,bbejeck,2017-07-24T16:46:54Z,"Oversight on my part, changing."
129091268,3325,bbejeck,2017-07-24T16:50:16Z,Ack
129606674,3325,mjsax,2017-07-26T15:27:05Z,Nit `{@code StateRestorerListener}`
129608628,3325,mjsax,2017-07-26T15:33:46Z,Can't this `extend AbstractNotifyingRestoreCallback` to save all the boiler plate from below?
129617878,3325,mjsax,2017-07-26T16:05:20Z,"Can we introduce a global ""one parameter per line"" code style? I think it would help to make diffs cleaner. We can do this incrementally. If yes, please do for all newly introduced code of this PR.

Also, should be add `final` all over the place?"
129618333,3325,mjsax,2017-07-26T16:07:06Z,"nit: parameter descriptions are no sentences, thus no `.` at the end (on many other places, too). If we say they are sentences, they it should start with upper case `[T]he TopicPartition`"
129620426,3325,mjsax,2017-07-26T16:13:55Z,As above?
129623215,3325,mjsax,2017-07-26T16:24:35Z,Can you elaborate? 
129624489,3325,mjsax,2017-07-26T16:28:29Z,We should not `expect` here and use `fail` within try-catch
129626342,3325,mjsax,2017-07-26T16:35:25Z,As above.
129629040,3325,bbejeck,2017-07-26T16:44:40Z,Ack
129664724,3325,bbejeck,2017-07-26T18:54:03Z,Ack
129667712,3325,bbejeck,2017-07-26T19:05:08Z,"Actually this class won't be used anymore, so removed."
129670240,3325,bbejeck,2017-07-26T19:15:34Z,"Sure thing, can you add to the Steams guidelines?"
129677466,3325,bbejeck,2017-07-26T19:46:19Z,"Will still have one no-op method, but I guess it's worth it as it does reduce the boilerplate some."
129692869,3325,bbejeck,2017-07-26T20:52:12Z,"There was some confusion over the number of times we  open and close the `RocksDBStateStore` for handling optimized bulk loads, once that was clarified the comments pertaining to setting `prepareForBulkload` and `open` didn't need to be addressed."
129963670,3325,guozhangwang,2017-07-27T21:26:05Z,nit: rename this function to `restoreStarted` to be consistent with other names. Such will help other code readers to understand these functions are for the same code granularity and semantics.
129964281,3325,guozhangwang,2017-07-27T21:28:46Z,Is this comment missed somehow? I think line 42 above could be `storeRestoreListener = NO_OP_STATE_RESTORE_LISTENER`.
129964947,3325,guozhangwang,2017-07-27T21:32:21Z,nit: space after `//` and we do not need capitalize the in-function comments.
129965165,3325,guozhangwang,2017-07-27T21:33:28Z,Ditto for in-function and simple top function comments.
129965459,3325,guozhangwang,2017-07-27T21:34:51Z,nit: new lines are generally not recommended to break object type declaration with object name. For this specific line I think we can still make them in one line.
129965596,3325,guozhangwang,2017-07-27T21:35:38Z,Ditto: newline after keywords are generally not recommended.
129965850,3325,guozhangwang,2017-07-27T21:37:04Z,Ditto for new line rules. Could you make a pass over all the newlines and see if they can be improved?
129966240,3325,guozhangwang,2017-07-27T21:39:04Z,We can use the `WrappedBatchingStateRestoreCallback` here?
129966522,3325,guozhangwang,2017-07-27T21:40:35Z,`org.apache.kafka.streams.processor.internals.NoOpStateRestoreListener` can be used here?
129970463,3325,bbejeck,2017-07-27T22:01:09Z,Ack
129970523,3325,bbejeck,2017-07-27T22:01:27Z,"Ack, must have overlooked"
129970837,3325,bbejeck,2017-07-27T22:03:18Z,Ack
129970870,3325,bbejeck,2017-07-27T22:03:26Z,Ack
129970957,3325,bbejeck,2017-07-27T22:03:53Z,"Ack, need to adjust Intellij settings"
129971207,3325,bbejeck,2017-07-27T22:05:25Z,"Ack, same as above"
129971290,3325,bbejeck,2017-07-27T22:05:57Z,Ack
129974506,3325,bbejeck,2017-07-27T22:26:26Z,Ack
129974595,3325,bbejeck,2017-07-27T22:26:54Z,Ack
589867244,10218,junrao,2021-03-09T01:16:23Z,"It's a bit weird to include this in the client module. Since this is implemented in java, we could potentially create a new java module for it (like the Raft module). This reduces the size of the client jar and also avoids the inter-dependencies between java and scala.

Also, is this for testing? If so, it needs to be in the test directory."
589867914,10218,junrao,2021-03-09T01:17:56Z,"Is this for testing? If so, it needs to be in the test directory."
590723978,10218,junrao,2021-03-09T21:14:18Z,typo imemory
590739158,10218,junrao,2021-03-09T21:40:00Z,"Hmm, it's possible for a segment to transition to DELETE_SEGMENT_STARTED here. Should those segments still be added?"
590740461,10218,junrao,2021-03-09T21:42:19Z,"Once a segment is in DELETE_SEGMENT_STARTED state, the corresponding segment could be gone any time after that. So, it seems that we should remove the segment from leaderEpochToOffsetToId once it's in DELETE_SEGMENT_STARTED?"
590742564,10218,junrao,2021-03-09T21:46:15Z,Could we just get the segment list from `idToSegmentMetadata.values()`?
590744034,10218,junrao,2021-03-09T21:48:28Z,"highestLogOffset => highestSegmentStartOffset ?
"
590745670,10218,junrao,2021-03-09T21:50:53Z,Could we add a comment to this class?
590748558,10218,junrao,2021-03-09T21:55:55Z,We are not returning null here.
591585302,10218,satishd,2021-03-10T14:49:20Z,"The plan was to use the related classes in the default RLMM implementation and move any class which is relevant only for tests to test dir later. I am +1 to have this as a separate module. I will update with those changes. 
"
591588474,10218,satishd,2021-03-10T14:52:49Z,"This behavior was kept to be the same as local log cleanup behavior, in which leader epoch is truncated only after local log is moved/deleted. Ideally, it is good not to consider the segments available that are being deleted as you said.  "
591591507,10218,satishd,2021-03-10T14:56:17Z,">This behavior was kept to be the same as local log cleanup behavior, in which leader epoch state is truncated only after local log is moved/deleted. Ideally, it is good not to consider the segments available that are being deleted as you said. "
591595247,10218,satishd,2021-03-10T15:00:08Z,There may be few segments with state as `COPY_SEGMENT_STARTED` and they will be part of `remoteLogSegmentIdInProgress` only but not `idToSegmentMetadata`. That is why we need to add them to the list. 
591603442,10218,satishd,2021-03-10T15:09:34Z,"No, it is not highestSegmentStartOffset but it is the highest log offset for the given leader epoch.
Nice catch! We need to give the max endoffset of all the segments for the given leader epoch. "
591905906,10218,kowshik,2021-03-10T22:05:03Z,This c'tor can be removed in exchange for the default generated c'tor.
591912017,10218,kowshik,2021-03-10T22:15:55Z,It seems like we allow for an entry already existing with the same ID to be replaced with a different entry. Would that happen in practice?
591915104,10218,kowshik,2021-03-10T22:21:41Z,"Can `offsetToId` be empty if it is not null?
I understand it is right to check for emptiness here, but I was just curious to learn if it could happen in practice."
591919624,10218,kowshik,2021-03-10T22:30:10Z,"Looking at the implementation, it appears we maintain some rules on when a `RemoteLogSegmentId` exists in one of these data structures versus all of them. It would be useful to briefly document those rules, and mention invariants (if any). For example, when an upload is in progress it is not (yet) added to this map."
591921511,10218,kowshik,2021-03-10T22:33:57Z,can `metadata` be a better variable name over `rlsm`?
592012860,10218,kowshik,2021-03-11T02:02:41Z,Should we move this log message before L51? so that the message conveying the intent is logged first before any possible  errors.
592015659,10218,kowshik,2021-03-11T02:11:02Z,Hmm... it seems like the only allowed state in `rlsmUpdate` is `COPY_SEGMENT_FINISHED`. Should we validate for that instead?
592016707,10218,kowshik,2021-03-11T02:13:35Z,s/resource/entry ?
592022166,10218,kowshik,2021-03-11T02:28:08Z,"It seems like we want to add more protections here.

1. If `remotePartitionDeleteMetadata.state()` is `DELETE_PARTITION_FINISHED`, then should there have been a prior entry with `DELETE_PARTITION_STARTED` or  `DELETE_PARTITION_MARKED`?
2. Imagine there exists an entry in `partitionToRemoteLogMetadataCache` while the partition is also being deleted. Is that a valid state, or if not should we assert against it?"
592022431,10218,kowshik,2021-03-11T02:28:56Z,Sorry I don't understand what does this comment refer to?
592024533,10218,kowshik,2021-03-11T02:35:18Z,"Hmm, any reason to not implement these methods? Is it that they don't serve any purpose in the in-memory implementation?"
592070871,10218,satishd,2021-03-11T05:10:50Z,This can happen in race condition when this method is queried while it was getting added in `addRemoteLogSegmentMetadata`. It may not happen in practice but it is good to have these checks.
592071729,10218,satishd,2021-03-11T05:13:59Z,"Right, it is not applicable for inmemory implementation."
592074962,10218,satishd,2021-03-11T05:24:46Z,"""No <s>resource</s> metadata found for partition: ""?"
592075662,10218,satishd,2021-03-11T05:26:51Z,It may not occur in practice. 
592096637,10218,satishd,2021-03-11T06:28:40Z,"It allows any state other than `COPY_SEGMENT_STARTED`, that is why we are checking only for this state."
592123658,10218,kowshik,2021-03-11T07:32:25Z,Is it useful to add a check against it?
592124191,10218,kowshik,2021-03-11T07:33:38Z,Sure
592173915,10218,satishd,2021-03-11T08:56:03Z,"Other states include `COPY_SEGMENT_FINISHED`, `DELETE_SEGMENT_STARTED`, and `DELETE_SEGMENT_FINISHED`."
592248245,10218,satishd,2021-03-11T10:37:36Z,"I meant there will be an external trigger based on delete partition marker, that is responsible for deleting the segments in a partition and updating the metadata. I will remove it as it looks to create confusion.  "
592437366,10218,satishd,2021-03-11T15:05:20Z,"1 -> Added more assertions. 
2 -> is a valid case.
"
592577341,10218,satishd,2021-03-11T17:55:01Z,"Sure, will add comments. "
594509836,10218,junrao,2021-03-15T16:51:52Z,DELETE_PARTITION_MARKED is not part of RemoteLogSegmentState.
594510134,10218,junrao,2021-03-15T16:52:11Z,This seems to be an internal implementation and is not part of the public API? Ditto for the same method in RemotePartitionDeleteState.
594550095,10218,junrao,2021-03-15T17:41:48Z,"This method updates idToSegmentMetadata, which seems redundant since it's done in line 107 already."
594554053,10218,junrao,2021-03-15T17:47:01Z,"It's possible that after this, there is no segment associated with a leader epoch. Should we remove the entry with that leader epoch from leaderEpochToOffsetToId?"
594560602,10218,junrao,2021-03-15T17:55:35Z,It's kind of inefficient to have to iterate through the whole segment list. Could we make leaderEpochToOffsetToId an ordered map and then do highEntry on that?
594562408,10218,junrao,2021-03-15T17:57:34Z,highestLogOffset => highestOffsetForEpoch?
594564204,10218,junrao,2021-03-15T17:59:44Z,"It seems that there is a semantic difference between this method and the next one. While this one exposes all segments (including in progress ones), the latter only exposes segments that are completed. It would be useful to document this clearly in the public API."
594567815,10218,junrao,2021-03-15T18:04:22Z,"> There may be few segments with state as COPY_SEGMENT_STARTED and they will be part of remoteLogSegmentIdInProgress only but not idToSegmentMetadata. That is why we need to add them to the list.

Hmm, it seems that we add the in-progress segment to idToSegmentMetadata in addToInProgress? It would be useful to add a comment for idToSegmentMetadata."
594577274,10218,junrao,2021-03-15T18:17:25Z,"> This behavior was kept to be the same as local log cleanup behavior, in which leader epoch state is truncated only after local log is moved/deleted. Ideally, it is good not to consider the segments available that are being deleted as you said.

For the local log, we first schedule the segment for async deletion and then take it out of leaderEpochCache. So, the equivalent of that for remote storage seems to require taking the segment out of leaderEpochCache once the segment deletion is initiated.
"
594578101,10218,junrao,2021-03-15T18:18:32Z,inmemory => in-memory
595492577,10218,kowshik,2021-03-16T19:49:18Z,"1. Will it be useful to place the implementation of this validation in a separate module, so that it can be reused with `RLMMWithTopicStorage` in the future?
2. Suggestion from the standpoint of code readability/efficiency: Would it make sense to replace the `if-else` logic by looking up from a `Map< RemoteLogSegmentState, Set< RemoteLogSegmentState>>` where key is the source state and value is a set of allowed target states?

"
595493561,10218,kowshik,2021-03-16T19:50:55Z,I have the same suggestions from `RemoteLogSegmentState` for this as well. Please refer to this comment: https://github.com/apache/kafka/pull/10218#discussion_r595492577
595500039,10218,kowshik,2021-03-16T20:00:45Z,Really minor comment/discussion: Any reason to call this prefixed with `add` as `addRemoteLogSegmentMetadata` vs calling the deletion one prefixed with `put` as `putRemotePartitionDeleteMetadata` i.e. instead can these 2 methods both start with the same prefix either `add` or `put`?
595505146,10218,kowshik,2021-03-16T20:09:07Z,"We may want to think more about the locking semantics for this class and `RemoteLogMetadataCache`. 
Are we sure there would _not_ be use cases where we need to serialize mutations across the individually thread-safe attributes? If the answer is no, then using a fine-grained `Object` lock makes more sense because we can use it to guard critical sections.

Should we evaluate this upfront?

cc @junrao "
595506748,10218,kowshik,2021-03-16T20:11:49Z,"In the comment:
s/putRemoteLogSegmentMetadata/addRemoteLogSegmentMetadata

"
595954157,10218,satishd,2021-03-17T12:05:42Z,This is not really an internal implementation but it validates the state transition and it is the same for any implementation.
596032415,10218,satishd,2021-03-17T13:46:26Z,I will update the javadoc of the APIs to make this clear.
596055570,10218,satishd,2021-03-17T14:12:17Z,Good point! It will clear the values which are empty maps. 
596203582,10218,satishd,2021-03-17T16:48:25Z,"`add` -> adding a new entry.
`put` -> add or update. 
`putRemotePartitionDeleteMetadata` is used for both add or update the `RemotePartitionDeleteMetadata`."
596204864,10218,satishd,2021-03-17T16:49:52Z,It was deliberate not to add locking semantics for now. We will add them once we have the respective changes using these classes. 
596247752,10218,satishd,2021-03-17T17:41:19Z,"1 -> imho, this validation method should be part of the state enum and it can be used by any implementation including default RLMM. 

2 -> I would have preferred the suggested approach if there are many complex transitions but the transitions here are few and simple. "
598964030,10218,kowshik,2021-03-22T18:13:16Z,You can drop `It` and start with `Indicates the state...`.
598971940,10218,kowshik,2021-03-22T18:24:49Z,"IMHO, we can simplify this to say:

```
// If the source state is null, check the target state as the initial state viz DELETE_PARTITION_MARKED.
// This ensures simplicity as we don't have to define an additional state type to represent the initial state.
```"
598974632,10218,kowshik,2021-03-22T18:28:26Z,Could we call this `idToRemoteLogMetadataCache` to align with the naming of the other attribute thats called `idToPartitionDeleteMetadata` ?
598981046,10218,kowshik,2021-03-22T18:37:34Z,Can this be checked inside `RemoteLogMetadataCache.addToInProgress()` instead of here?
598982742,10218,kowshik,2021-03-22T18:40:00Z,It seems to me that `srcState` is never null in practice. Where does this check come into play in practice?
598984916,10218,kowshik,2021-03-22T18:43:16Z,This is defined to be not thread safe unlike the other maps. Is there any reason?
598990605,10218,kowshik,2021-03-22T18:51:29Z,Can we add a 1-line doc for this similar to other attributes below?
599000060,10218,kowshik,2021-03-22T19:05:29Z,"Before we insert into the map/set, we should check if the provided `remoteLogSegmentMetadata.state()` is `COPY_SEGMENT_STARTED`."
599001851,10218,kowshik,2021-03-22T19:08:20Z,"In this method, we allow for existing entries in `idToSegmentMetadata` to be replaced, even if the state of the existing and new entries are the same. Is that intentional?"
599003846,10218,kowshik,2021-03-22T19:11:32Z,"Hmm, the entry for `existingMetadata` gets overwritten in the call to `addRemoteLogSegmentMetadata` in L110. Should we be accounting for the same here?"
599006691,10218,kowshik,2021-03-22T19:16:01Z,"Similar to above comment, why not check this inside `remoteLogMetadataCache.updateRemoteLogSegmentMetadata()`?"
599008337,10218,kowshik,2021-03-22T19:18:33Z,"Typos:
1. s/wWe/We
2. s/gettign/getting"
599009374,10218,kowshik,2021-03-22T19:20:12Z,"Can we improve the local variable names? for example `segIdFooTp0s0e100`, `segMetFooTp0s0e100` etc. is not easy to read. We can use simpler names."
599013040,10218,kowshik,2021-03-22T19:26:18Z,"The implementation compromises on the ordering, since it converts the iterator to a set. Is that intentional?"
599014824,10218,kowshik,2021-03-22T19:29:04Z,"This particular test checks a number of things together in one test. Instead, could sections (1) to (4) from below each be defined as a separate unit test? Especially since each section seems to operate on a different segment, so it seems logically independent."
599016901,10218,kowshik,2021-03-22T19:32:49Z,Could we add test(s) for `highestLogOffset` API?
599018475,10218,kowshik,2021-03-22T19:35:22Z,"Could we assert just before this line that `seg3S350` is not empty? this will simplify the `seg3S350.orElse(null)` argument to `seg3S350.get()`.

(same comment applies for other places in this test)"
599021005,10218,kowshik,2021-03-22T19:39:17Z,"Should we alter the other arguments too, for example `BROKER_ID` and `eventTimestamp`? It appears that we expect `RemoteLogMetadataCache` to [apply all of the provided updates](https://github.com/apache/kafka/blob/0d9a95a7d0ab06aecc4480901707e29dd2a3147e/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteLogSegmentMetadata.java#L240-L242), and this may include the other fields as well."
599021666,10218,kowshik,2021-03-22T19:40:23Z,Can we remove this c'tor in exchange for the default generated c'tor?
599024909,10218,kowshik,2021-03-22T19:45:33Z,"As per the interface we [mandate](https://github.com/apache/kafka/blob/0d9a95a7d0ab06aecc4480901707e29dd2a3147e/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageManager.java#L76) the caller to ensure unique ID, but is it useful to add a guard that disallows replacing existing values?)"
599026440,10218,kowshik,2021-03-22T19:47:57Z,"We could add a c'tor overload to [RemoteStorageException](https://github.com/apache/kafka/blob/0d9a95a7d0ab06aecc4480901707e29dd2a3147e/clients/src/main/java/org/apache/kafka/server/log/remote/storage/RemoteStorageException.java) that takes a `Throwable` as argument, it would the need to pass 2 args here."
599027282,10218,kowshik,2021-03-22T19:49:15Z,Probably better to say `...must be greater than or equal to...` ?
599029284,10218,kowshik,2021-03-22T19:52:27Z,"Hmm, do we need to explicitly check if `endPosition` < `segment.length`? "
599029805,10218,kowshik,2021-03-22T19:53:16Z,Is this intentionally left empty?
606249948,10218,satishd,2021-04-02T13:59:30Z,"It takes `Math.min(endPosition, segment.length)`. So, no need to have that check. "
606249998,10218,satishd,2021-04-02T13:59:36Z,Right.
606250883,10218,satishd,2021-04-02T14:01:49Z,"Yes, it can happen to generate an event with the same state incase of retries.  "
606251255,10218,satishd,2021-04-02T14:02:48Z,Good catch. This is addressed in the latest commit.
606253220,10218,satishd,2021-04-02T14:07:45Z,"I thought earlier about having different methods, but it checks `listAllSegments/listSegment(leaderEpoch)` APIs that return earlier segments. But I will have a separate test for that and extract as suggested. "
606253804,10218,satishd,2021-04-02T14:09:07Z,We can add that. 
606257469,10218,satishd,2021-04-02T14:17:46Z,"Sure, I will add that."
606343187,10218,junrao,2021-04-02T17:40:10Z,indicate => indicates Ditto in a few other places.
606348092,10218,junrao,2021-04-02T17:52:27Z,"The following table is a bit hard to read for developers. Since this is not meant for a public interface, could we make it more readable for developers?"
606348477,10218,junrao,2021-04-02T17:53:24Z,I guess this is an internal class. Will this be exposed in javadoc since currently it includes **/org/apache/kafka/server/log/remote/storage/* ?
607191843,10218,junrao,2021-04-05T16:41:07Z,Could we include remoteLogSegmentMetadata in the exception message?
607196509,10218,junrao,2021-04-05T16:49:33Z,Should we include metadataUpdate in the message of the exception?
607205642,10218,junrao,2021-04-05T17:05:49Z,This comment is confusing since there is no update here.
607211862,10218,junrao,2021-04-05T17:17:23Z,"Hmm, during unclean leader election, some of the old segments may need to be added to unreferenced segment id list but may not have the exact offset of the new segment. How are those segments handled here?"
607225040,10218,junrao,2021-04-05T17:40:58Z,It's weird to reference offsetToId here since it's in a separate class.
607226551,10218,junrao,2021-04-05T17:43:59Z,Is the test for RemoteLogSegmentState.COPY_SEGMENT_FINISHED necessary since it seems that only segments with RemoteLogSegmentState.COPY_SEGMENT_FINISHED exist in offsetToId.
607241348,10218,junrao,2021-04-05T18:10:59Z,updateHighestLogOffset => maybeUpdateHighestLogOffset ?
607247458,10218,junrao,2021-04-05T18:22:28Z,When are entries in leaderEpochEntries removed?
607249504,10218,junrao,2021-04-05T18:26:13Z,"Since existingState can be null, we want to handle it properly."
607250466,10218,junrao,2021-04-05T18:27:51Z,Should we requireNonNull for topicIdPartition here too?
607255605,10218,junrao,2021-04-05T18:37:31Z,This is an existing issue. But is `&gt;` in line 37 expected?
607256604,10218,junrao,2021-04-05T18:39:31Z,Is this constructor needed?
607546803,10218,satishd,2021-04-06T06:19:42Z,No. javadoc is generated for clients module with the package `/org/apache/kafka/server/log/remote/storage/ `. But this class is in `remote-storage` module.
607546852,10218,satishd,2021-04-06T06:19:45Z,"Sure, I will make it as simple ascii text. "
607551710,10218,satishd,2021-04-06T06:27:06Z,It prints null. I may be missing something here. What needs to be handled here?
607556778,10218,satishd,2021-04-06T06:33:21Z,Looks like autoformatter changed it. 
607559905,10218,satishd,2021-04-06T06:38:20Z,"good point, this check is no more needed. "
607561788,10218,satishd,2021-04-06T06:40:35Z,Not really needed for now. 
607562925,10218,satishd,2021-04-06T06:41:59Z,Updated the comment.
607580027,10218,satishd,2021-04-06T07:07:24Z,"Yes, in the case of unclean leader election, the leader will remove the old segments for the respective leader epochs. The removal process involves removing the actual segment and updating the respective metadata of the segments. "
607597875,10218,satishd,2021-04-06T07:30:13Z,"One way to do that is to clear the entry when the respective `RemoteLogLeaderEpochState` is empty. That means all the segments reached `DELETE_SEGMENT_FINISHED` state. 
This is not currently addressed. I plan to look into it when we integrate these APIs with RemoteLogManager by exploring other options too. 
"
607959239,10218,kowshik,2021-04-06T15:34:20Z,nit: remove empty `@return`
607959314,10218,kowshik,2021-04-06T15:34:24Z,nit: remove empty `@return`
607972558,10218,kowshik,2021-04-06T15:50:20Z,Should we call this map as `idToLeaderEpochState` or `idToEpochState` similar to the naming for the other map?
608025473,10218,kowshik,2021-04-06T16:58:53Z,"Same comment as before: https://github.com/apache/kafka/pull/10218/files#r598982742.
Can srcState be null in practice? If not, this can be defined as an instance method."
608820288,10218,satishd,2021-04-07T16:40:30Z,The key is not really `id` but `epoch num`. What about `remoteLogLeaderEpochStateEntries` or `leaderEpochToState` or any other better name? 
608822419,10218,satishd,2021-04-07T16:43:31Z,"Yes, it can be null. It is called from [here](https://github.com/apache/kafka/pull/10218/files#diff-3724bb53d7ab4bc5a6ec4e1ab4c91c47bf90e4166d881f7706e2adc1848a5d16R293)"
609073411,10218,junrao,2021-04-07T21:13:57Z,It would be useful to add a comment on whether the methods in this class are thread-safe or not.
609083684,10218,junrao,2021-04-07T21:32:55Z,"It seems that it's inconsistent that we update highest log offset here but not in handleSegmentWithCopySegmentStartedState(). 

Could we comment on whether highestLogOffset reflects the segments that have reached  COPY_SEGMENT_FINISHED or not?"
609089991,10218,junrao,2021-04-07T21:45:33Z,It would be useful to document the meaning of the following table.
609093303,10218,junrao,2021-04-07T21:52:28Z,Could we make it clear this is for offset range?
609095034,10218,junrao,2021-04-07T21:56:11Z,"At this point, RLMM hasn't cleared all its internal state yet."
609097289,10218,junrao,2021-04-07T22:00:24Z,Is this logging needed? Does it need to be in info level?
609100380,10218,junrao,2021-04-07T22:05:53Z,Could we make it clear this is for offset range?
609103486,10218,junrao,2021-04-07T22:11:25Z,Is this logging needed? Ditto below.
609104878,10218,junrao,2021-04-07T22:14:31Z,It's kind of weird that the segment with epoch 0 is already deleted and yet we still expect the highest offset for epoch 0 to be returned.
609107657,10218,junrao,2021-04-07T22:21:00Z,listRemoteLogSegments(0) => listRemoteLogSegments(1)
609351020,10218,satishd,2021-04-08T06:32:43Z,We may have this as debug level by default. It will be helpful to see for which entry the test is failed. 
609352593,10218,satishd,2021-04-08T06:35:08Z,We may have this as debug level by default. It will be helpful to see for which `EpochOffset` the test is failed. 
609369185,10218,satishd,2021-04-08T06:57:19Z,"Sure, I will add the doc. They are currently not thread safe. But we want to address them when we integrate these APIs. "
609369253,10218,satishd,2021-04-08T06:57:24Z,"After thinking through this more, we need to update this only when the segment reaches COPY_SEGMENT_FINISHED. This is effectively used to find out up to which offset the segments are already copied. I will remove the call here and keep the call only in handleSegmentWithCopySegmentFinishedState. WDYT?"
609369434,10218,satishd,2021-04-08T06:57:37Z,Added a note.
609369536,10218,satishd,2021-04-08T06:57:42Z,Done
609369903,10218,satishd,2021-04-08T06:58:06Z,Updated.
609372137,10218,satishd,2021-04-08T07:01:06Z,`highestLogOffset` can contain the deleted segments. `highestLogOffset` means the highest offset up to which the segments have been copied. Pl take a look at the [comment](https://github.com/apache/kafka/pull/10218#discussion_r609369253).
609372257,10218,satishd,2021-04-08T07:01:14Z,Done
609387061,10218,satishd,2021-04-08T07:20:04Z,Updated.
609882620,10218,junrao,2021-04-08T16:29:27Z,This is redundant.
609891200,10218,junrao,2021-04-08T16:38:55Z,Could we move this to debug level then?
609895193,10218,junrao,2021-04-08T16:44:18Z,"> One way to do that is to clear the entry when the respective RemoteLogLeaderEpochState is empty. That means all the segments reached DELETE_SEGMENT_FINISHED state.
> This is not currently addressed. I plan to look into it when we integrate these APIs with RemoteLogManager by exploring other options too.

Could we add a TODO comment here so that we don't forget about it?"
609896011,10218,junrao,2021-04-08T16:45:25Z,Sounds good. Could you make the change in the PR?
609998576,10218,kowshik,2021-04-08T18:45:30Z,`leaderEpochToState` sounds good.
610286497,10218,kowshik,2021-04-09T02:14:20Z,"Here is a slightly simpler version:
```
 while (iterator1.hasNext() && iterator2.hasNext()) {
     if (!Objects.equals(iterator1.next(), iterator2.next())) {
            return false;
    }
}

return !iterator1.hasNext() && !iterator2.hasNext();
```"
610293058,10218,kowshik,2021-04-09T02:26:22Z,"Hmm here we assume that `id` should be present in the provided `idToSegmentMetadata`. Due to programming error, or other reasons, the caller may not be able to ensure this. Would it be safer if we instead threw whenever `id` is absent in `idToSegmentMetadata`  to catch that case?"
610298531,10218,kowshik,2021-04-09T02:37:36Z,"The add call won't replace an existing element with the same `remoteLogSegmentId`. Is that expected?
For example, what happens if `addCopyInProgressSegment` is called twice but this line doesn't replace the existing entry?"
610302012,10218,kowshik,2021-04-09T02:44:27Z,"nit: add one whitespace at the end after ""...state"" "
610305248,10218,kowshik,2021-04-09T02:50:25Z,"Is this method expected to be idempotent?
Note: this comment is related to my other comment: https://github.com/apache/kafka/pull/10218#discussion_r610298531."
610454559,10218,satishd,2021-04-09T08:42:52Z,"IMHO, existing code looks easy to read/comprehend, and no multiple calls to hasNext().
How about the below code after removing inline variables in the existing code?

```
while (iterator1.hasNext()) {
    if (!iterator2.hasNext()) {
        return false;
    }

    if (!Objects.equals(iterator1.next(), iterator2.next())) {
        return false;
    }
}

return !iterator2.hasNext();
```"
610462859,10218,satishd,2021-04-09T08:55:44Z,It already replaces the existing entry [here](https://github.com/apache/kafka/pull/10218/files#diff-3724bb53d7ab4bc5a6ec4e1ab4c91c47bf90e4166d881f7706e2adc1848a5d16R299). 
610464252,10218,satishd,2021-04-09T08:57:54Z,Addressed in the above [comment](https://github.com/apache/kafka/pull/10218#discussion_r610462859). 
610491754,10218,kowshik,2021-04-09T09:40:59Z,Sounds good
610494457,10218,kowshik,2021-04-09T09:45:34Z,"Ok, I think this is fine then."
610494694,10218,kowshik,2021-04-09T09:45:53Z,Sounds good
610517070,10218,satishd,2021-04-09T10:23:25Z,"Good point, I will add a check for that. "
610809710,10218,junrao,2021-04-09T17:53:19Z,Typo epty
610940157,10218,satishd,2021-04-09T22:50:50Z,"Yes, it is done. "
610940434,10218,satishd,2021-04-09T22:51:56Z,"Yes, I updated the PR."
610944855,10218,satishd,2021-04-09T23:07:59Z,Fixed. 
611267145,10218,kowshik,2021-04-12T00:13:25Z,typo: The title of the last column should be `DELETE_SEGMENT_FINISHED`.
611305646,10218,satishd,2021-04-12T03:25:13Z,"thanks, addressed it in the latest commit."
1335136107,14432,vamossagar12,2023-09-24T07:16:06Z,I have taken the liberty and updated the log line to use an argument based loggers instead of the string concatenation based pattern that existed before.
1338285360,14432,vamossagar12,2023-09-27T08:53:33Z,This is not necessarily needed but added to avoid situations when a non static member tries to send a member epoch -2.
1338844884,14432,kirktrue,2023-09-27T16:04:11Z,"This is for the case where the static member is leaving temporarily, right?

Would it be possible to add more detail to these error messages to aid in troubleshooting/debugging on the client when this condition is hit?"
1338849515,14432,kirktrue,2023-09-27T16:07:34Z,Nice! Can we add the Group ID to the 'static member' log message?
1338895795,14432,vamossagar12,2023-09-27T16:34:12Z,"> This is for the case where the static member is leaving temporarily, right

Yes, that's correct. I have added some debugging information (like groupId etc). Let me know if that makes sense."
1338895972,14432,vamossagar12,2023-09-27T16:34:19Z,Done.
1338900198,14432,vamossagar12,2023-09-27T16:37:16Z,"Also, `MemberId can't be empty.` string is used in other places as well (when member epoch is > 0  or equal to -1. Should we look to change those as well?"
1344754813,14432,kirktrue,2023-10-03T21:19:41Z,"It's just my preference, so if there's precedent for how you have it, I wouldn't hold up this PR in an effort to change the other places. Thanks!"
1345450086,14432,vamossagar12,2023-10-04T08:57:24Z,"Makes sense. I think I updated that one comment. But yeah the rest of the loggers, I won't be touching them, as that would be noisy to review."
1348694911,14432,dajac,2023-10-06T13:02:39Z,nit: Indentation should be 4 spaces.
1348699907,14432,dajac,2023-10-06T13:07:25Z,I suppose that this must be a `TimelineHashMap`.
1348700266,14432,dajac,2023-10-06T13:07:46Z,nit: Indentation should be four spaces.
1348701917,14432,dajac,2023-10-06T13:09:15Z,The state should not be updated like this. All the updates are handled in the `replay()` methods.
1348845154,14432,dajac,2023-10-06T15:03:46Z,Could we add custom message to all the exceptions raise in this method?
1348855163,14432,dajac,2023-10-06T15:11:38Z,I wonder if we really need the second part of the condition here. What was your thinking about it?
1348858332,14432,dajac,2023-10-06T15:14:12Z,"This is a bit weird because you pass `existingMember` to the builder and then you still have to override other fields. Would it be better to do `new ConsumerGroupMember.Builder(existingMember)` and then override the fields? I think that we only need to set the new member id.

nit: The indentation should be four spaces."
1348858959,14432,dajac,2023-10-06T15:14:33Z,"As said previously, the state should not be updated here but in replay."
1348859279,14432,dajac,2023-10-06T15:14:48Z,nit: indentation.
1348861003,14432,dajac,2023-10-06T15:15:53Z,I wonder if we could log something here as well when a static member is replaced.
1348864154,14432,dajac,2023-10-06T15:18:25Z,This does not seem correct because we will write a record whenever the member is not updated and we have an instance id. I think that it would be better to capture the fact that we have a new static member in the condition at L801.
1348866036,14432,dajac,2023-10-06T15:19:51Z,"I just thought about something else. When a static member is replaced, we need to write records to erase the state of the previous member."
1348872341,14432,dajac,2023-10-06T15:24:47Z,I would rather prefer to have a separate method for the static leave group.
1348872557,14432,dajac,2023-10-06T15:24:57Z,nit: indentation.
1348873017,14432,dajac,2023-10-06T15:25:14Z,nit: Should we introduce a constant for -2 as well?
1348874306,14432,dajac,2023-10-06T15:26:16Z,We don't need to use getOrMaybeCreateStaticMember here as we only want to look up the member by its id.
1348875180,14432,dajac,2023-10-06T15:26:59Z,"In there, we need to update the static id mapping in updateMember and removeMember, I think."
1354486658,14432,vamossagar12,2023-10-11T09:00:09Z,done.
1354487016,14432,vamossagar12,2023-10-11T09:00:16Z,done.
1354488025,14432,vamossagar12,2023-10-11T09:00:44Z,"Makes sense, I have removed this direct update of states and moved it to `replay()`"
1354491971,14432,vamossagar12,2023-10-11T09:02:44Z,Actually this is no longer required. Have removed it.
1354492275,14432,vamossagar12,2023-10-11T09:02:53Z,Ack.
1354505609,14432,vamossagar12,2023-10-11T09:06:55Z,"> I just thought about something else. When a static member is replaced, we need to write records to erase the state of the previous member.

YEs that was a miss. I have added relevant tombstone records for the replaced static member and also cancelled it's timers."
1354506320,14432,vamossagar12,2023-10-11T09:07:07Z,Added.
1354506847,14432,vamossagar12,2023-10-11T09:07:17Z,done
1354507878,14432,vamossagar12,2023-10-11T09:07:33Z,Done 
1354516913,14432,vamossagar12,2023-10-11T09:10:20Z,This method is no longer being used. I added custom messages in the other method which is now being called.
1354518973,14432,vamossagar12,2023-10-11T09:10:51Z,"YEs, that didn't quite make sense. It's not needed anymore. Fixed indent as well."
1354520295,14432,vamossagar12,2023-10-11T09:11:12Z,This bit of code has changed now.
1362101057,14432,dajac,2023-10-17T13:19:21Z,nit: We tend to use a single line for getters. eg. `@return The member id corresponding to the given instance id or null if it does not exist`.
1362102003,14432,dajac,2023-10-17T13:20:03Z,nit: We don't prefix getters with `get`. Let's add javadoc as well.
1362105275,14432,dajac,2023-10-17T13:22:21Z,nit: Would it make sense to have a method like the others? I would also do this before calling `maybeUpdateGroupState`.
1362105499,14432,dajac,2023-10-17T13:22:30Z,ditto.
1362106636,14432,dajac,2023-10-17T13:23:12Z,Let's add unit tests for the new or changed methods to the corresponding file.
1362197123,14432,dajac,2023-10-17T14:13:31Z,I think that the old member will be in `members` so the computed target assignment is incorrect. We need to remove it with `removeMember` and we also need to set the target assignment of the new member from the old one.
1362197956,14432,dajac,2023-10-17T14:14:04Z,We have similar code somewhere else. Could we add a method for this and reuse it?
1362198218,14432,dajac,2023-10-17T14:14:15Z,Same for this one. It would be great to have a method.
1362200665,14432,dajac,2023-10-17T14:15:44Z,"I am not sold on this. Is it too difficult to reuse the main logic? There are a few issues with this approach. For instance, the member's assignment is not reconciled like we do in the main logic. Another one is that the subscription metadata must be updated as well if the subscriptions have changed."
1362215540,14432,dajac,2023-10-17T14:23:59Z,I don't fully get this one. Could you please elaborate?
1363301832,14432,vamossagar12,2023-10-18T06:22:35Z,"The main reason for extracting this out was that while using the main logic, there always always a group epoch bump even when a new static member replaces an older one. When I debugged it further, it seems to be because of this logic [here](https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java#L885-L889)
. More specifically, the issue at hand is that `subscriptionMetadata` has partition racks info while the currently stored metadata doesn't have it. This is with regards to the test `testStaticMemberGetsBackAssignmentUponRejoin`. I  wasn't totally sure if this is an issue with the test itself but since this led to a group epoch bump, I thought we shouldn't do it. 

Actually when I think about it now, maybe it makes sense to have a group epoch bump in this case as well. While it might go against no rebalance during static member rejoin but the reason for rejoin is a change in subscription metadata and not a static member re-join. The latter seemed harder to replicate via tests though because it always bumped up the group epoch due to the above mentioned issue. Please let me know your thoughts.
"
1363322519,14432,vamossagar12,2023-10-18T06:35:12Z,"I missed the `removeMembers` part. Thanks for pointing it out. Regarding 

> we also need to set the target assignment of the new member from the old one.

I assumed, this [block of code](https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/TargetAssignmentBuilder.java#L262-L288)  should take care of it. Was my assumption wrong?"
1363327084,14432,vamossagar12,2023-10-18T06:38:42Z,"This is mainly needed for the static member rejoin case. Let's say a static member with instance id `id` departed. When it departs, we would write a member epoch value of -2 against it. Now, if a new static member joins with the same instance id `id` and a member epoch value of 0, then without this condition, the rejoin would always fail with `FencedMemberEpochException`. This condition was added to avoid the same. "
1363408245,14432,dajac,2023-10-18T07:51:33Z,"Well, at the moment, a new target assignment will be computed for the new static member and that block of code will indeed create the record for it. What I meant is that the new static member should actually reuse the target assignment of the previous member (vs computing a new one). Then we only need to recompute the assignment if there is a group epoch bump."
1363414003,14432,dajac,2023-10-18T07:55:20Z,Hum... I am not sure to fully follow. The subscription metadata should not be different if the subscriptions and the metadata image have not changed. Does the new static member has the same subs as the previous one in your case?
1363419939,14432,dajac,2023-10-18T07:58:37Z,"I see. Would it make sense to put this condition first in the method, before `if (receivedMemberEpoch > member.memberEpoch())`? I got confused by the fact that it is within the if branch."
1370523697,14432,vamossagar12,2023-10-24T16:47:34Z,"Yeah, that makes sense as well. I placed it inside the if condition because this condition at hand shows up only when received epoch > current member epoch. But it should be ok to have it outside as you said. I have made the change."
1370523927,14432,vamossagar12,2023-10-24T16:47:47Z,Done.
1370524066,14432,vamossagar12,2023-10-24T16:47:54Z,Done.
1370531600,14432,vamossagar12,2023-10-24T16:52:34Z,"I got around the issue by explicitly setting the rack info in the subscription metadata like [here](https://github.com/apache/kafka/pull/14432/files#diff-1396a400e5c9f5ccaa44cb20194066eaa919a7e02c558db02005a4a2b67a93b9R2117-R2122). I guess so far this wasn't apparent because all the tests expected a group epoch bump happening. In this case, we didn't want a group epoch bump and hence I could notice the discrepancy."
1370532536,14432,vamossagar12,2023-10-24T16:53:25Z,That makes sense and thanks for the explanation. I have now changed the code to remove the existing static member and add the new static member. Rest of the state would remain as is. 
1370532706,14432,vamossagar12,2023-10-24T16:53:34Z,Done.
1370533114,14432,vamossagar12,2023-10-24T16:54:00Z,Done.
1370533226,14432,vamossagar12,2023-10-24T16:54:07Z,Done.
1370538866,14432,vamossagar12,2023-10-24T16:55:35Z,"Also, regarding 

> Then we only need to recompute the assignment if there is a group epoch bump.

Given that now we will have a group epoch bump whenever a static member re-joins with a different subscription, should this be mentioned in the KIP? As we noticed, this is a deviation from how the static member rejoining with a different subscription case as of today. WDYT?"
1370539621,14432,vamossagar12,2023-10-24T16:55:51Z,Done.
1370539793,14432,vamossagar12,2023-10-24T16:55:59Z,Done.
1370542261,14432,vamossagar12,2023-10-24T16:58:01Z,"@dajac , I realised that even in the static member re-joining case, while the group epoch doesn't bump, the partitions would be in pending assignment state. I believe, eventually the member would get it's assignments. In that case, this state seems correct to me. WDYT? "
1377556564,14432,dajac,2023-10-31T13:05:28Z,"I wonder if we could simplify it even more. For instance, would it be possible to have something like the following:

```
ConsumerGroupMember member;
ConsumerGroupMember updatedMember;

if (instanceId == null) {
  member = group.getOrMaybeCreateMember(memberId, createIfNotExists);
  throwIfMemberEpochIsInvalid(member, memberEpoch, ownedTopicPartitions);
  log.info(...);
  updatedMember = new ConsumerGroupMember.Builder(member)
     ....
} else {
  // the new logic.
  // member is the current static member.
  // updatedMember is the updated current member or the new one.
}
```

"
1377557816,14432,dajac,2023-10-31T13:06:15Z,If we rely on `member` and `updatedMember` then we don't need this because `!updatedMember.equals(member)` will catch the new member.
1377567890,14432,dajac,2023-10-31T13:14:15Z,I don't fully understand how this would work because the members and the target assignment are not set.
1377571172,14432,dajac,2023-10-31T13:16:47Z,"When the `TargetAssignmentBuilder` builds the spec for the assignor, it must use the target assignment of the previous static member for the new static member. How do we ensure this? We may have to update the `TargetAssignmentBuilder` to understand that a static member is replaced."
1377572235,14432,dajac,2023-10-31T13:17:39Z,"Don't we need to also force the step 3.? If we don't do it, we don't write the current assignment record for the new member and we don't reconcile him."
1377575023,14432,dajac,2023-10-31T13:19:44Z,nit: removeMemberAndCancelTimers? The logic is not tight to static members. I would also directly pass the groupId and the memberId as this is all it needs.
1377614473,14432,dajac,2023-10-31T13:46:48Z,"This is actually executed twice. Once here and once in `consumerGroupStaticMemberGroupLeave`. I also wonder if we need to full validation here. I suppose that ensuring that the member id is correct would be enough, no?"
1377615301,14432,dajac,2023-10-31T13:47:21Z,nit: We check this twice. Once here and once earlier to lookup the member. Could we combine them?
1377616732,14432,dajac,2023-10-31T13:48:07Z,"nit: `""[GroupId {}] Static member {} with member id {} left the consumer group.""`? I would also use a similar logging structure for the other log messages."
1377617256,14432,dajac,2023-10-31T13:48:25Z,nit: Let's align the description of the params.
1377617520,14432,dajac,2023-10-31T13:48:35Z,nit: `member`?
1377619412,14432,dajac,2023-10-31T13:49:40Z,nit: The `addAll` does not seem necessary here. Could we avoid it?
1377621712,14432,dajac,2023-10-31T13:50:48Z,nit: We can reduce the space between the name and the desc.
1377623260,14432,dajac,2023-10-31T13:51:46Z,"nit: Let's use `""MemberId can't be empty.""` to be consistent with the previous errors."
1377624326,14432,dajac,2023-10-31T13:52:27Z,I think that the instance id cannot be null and cannot be empty as well. Then let's use `InstanceId can't be null or empty.`
1377625954,14432,dajac,2023-10-31T13:53:36Z,Could we add something to the exception?
1383610866,14432,vamossagar12,2023-11-06T16:27:59Z,I have updated the logic in line with your suggestion. 
1383611138,14432,vamossagar12,2023-11-06T16:28:11Z,"Yes, that makes sense."
1383612141,14432,vamossagar12,2023-11-06T16:28:57Z,I have added members and target assignment.
1383612935,14432,vamossagar12,2023-11-06T16:29:33Z,I think I got it now. I added some state tracking to the `TargetAssignmentBuilder` so that it doesn't compute assignments for a replacing static member.
1383615616,14432,vamossagar12,2023-11-06T16:30:58Z,"Yes, I am now building the replacing static member with the same set of assignments (target/pending). This forces it directly to have the current assignment record for the new member."
1383616531,14432,vamossagar12,2023-11-06T16:31:09Z,done.
1383617107,14432,vamossagar12,2023-11-06T16:31:17Z,Removed.
1383617522,14432,vamossagar12,2023-11-06T16:31:25Z,done.
1383617966,14432,vamossagar12,2023-11-06T16:31:31Z,done.
1383618637,14432,vamossagar12,2023-11-06T16:31:41Z,done.
1383620879,14432,vamossagar12,2023-11-06T16:32:19Z,Added.
1397018204,14432,dajac,2023-11-17T10:00:25Z,I would just use `InstanceId can't be null.` here. I think that we should also verify that the instance id is not empty.
1397269098,14432,dajac,2023-11-17T13:01:00Z,nit: It seems that we could declare this one only when we use it at L948.
1397287511,14432,dajac,2023-11-17T13:14:27Z,"I still find this logic quite complex to follow. I wonder if we could be a little more explicit. I think that the complexity comes from `throwIfStaticMemberValidationFails` which hide quite a lot of the logic.

I wonder if something as follow would be better. I am not sure... What do you think?

```
ConsumerGroupMember existingMember = group.staticMember(instanceId);
if (memberEpoch == 0) {
   // A new static member joins or the existing static member rejoins.
   if (existingMember == null) {
      // New static member.
      member = group.getOrMaybeCreateMember(memberId, true);
      updatedMemberBuilder = new ConsumerGroupMember.Builder(member);
   } else {
      // Static member rejoins with a different instance id so it should replace
      // the previous instance iff the previous instance has -2.
      if (existingMember.memberEpoch() != LEAVE_GROUP_STATIC_MEMBER_EPOCH) {
         // The new member can't join.
         throw Errors.UNRELEASED_INSTANCE_ID.exception(...);
      } else {
         // Replace the current member.
        staticMemberReplaced = true;
        member = existingMember;
        updatedMemberBuilder = new ConsumerGroupMember.Builder(group.getOrMaybeCreateMember(memberId, true));
        removeMemberAndCancelTimers(records, group.groupId(), member.memberId());
      }
} else {
   // Check member id or throw FENCED_INSTANCE_ID
   // Check epoch with throwIfMemberEpochIsInvalid
}
```

Note that I just wrote this without testing it so the code is likely not 100% correct :)."
1397290332,14432,dajac,2023-11-17T13:16:47Z,I would say `[GroupId {}] Static member {} with instance id {} joins the consumer group.`
1397291443,14432,dajac,2023-11-17T13:17:55Z,"I was thinking about this a little more and I actually wonder if we need this after all. If we don't see it, I think that the reconciliation will kick in and compute the current assignment. What do you think?"
1397321543,14432,dajac,2023-11-17T13:35:23Z,"Is the condition really correct? It seems to me that we must remove the previous member whenever staticMemberReplaced is true.

How about the following?

```
TargetAssignmentBuilder assignmentResultBuilder =
    new TargetAssignmentBuilder(groupId, groupEpoch, assignors.get(preferredServerAssignor))
         .withMembers(group.members())
         .withSubscriptionMetadata(subscriptionMetadata)
         .withTargetAssignment(group.targetAssignment());

if (staticMemberReplaced) {
    assignmentResultBuilder
         .removeMember(member.memberId())
         .addOrUpdateMember(memberId, updatedMember);
} else {
    assignmentResultBuilder
         .addOrUpdateMember(memberId, updatedMember);
}

TargetAssignmentBuilder.TargetAssignmentResult assignmentResult = 
    assignmentResultBuilder.build();
```"
1397322222,14432,dajac,2023-11-17T13:35:48Z,nit: Let's keep this log message where it was.
1397350631,14432,dajac,2023-11-17T13:54:43Z,"How about the following?

```
List<Record> records;
if (instanceId == null) {
  ConsumerGroupMember member = group.getOrMaybeCreateMember(memberId, false);
  log.info(""[GroupId {}] Member {} left the consumer group."", groupId, memberId);
  records = consumerGroupFenceMember(group, member);
} else {
  // Get static member
  ...
  // Validate the member id matches the expect id or throw fenced member id
  ...
  if (memberEpoch == LEAVE_GROUP_STATIC_MEMBER_EPOCH) {
      log.info(""[GroupId {}] Static Member {} with instance id {} temporarily left the consumer group"",
           group.groupId(), memberId, instanceId);
      records = consumerGroupStaticMemberGroupLeave(group, instanceId, member, memberId);
  } else {
      log.info(""[GroupId {}] Static Member {} with instance id {} left the consumer group"",
           group.groupId(), memberId, instanceId);
      records = consumerGroupFenceMember(group, member);
  }
}
```"
1397353174,14432,dajac,2023-11-17T13:56:52Z,"Instead of doing this, could we just pass the mapping from the ConsumerGroup?"
1397356292,14432,dajac,2023-11-17T13:59:21Z,"It is a tad annoying that we have to build this mapping here because we usually only need it for one member. Instead of doing this, I wonder if we could lookup the member id from the instance id mapping and then get the target assignment of the member id. The mapping will contain the previous member or nothing."
1397360473,14432,dajac,2023-11-17T14:02:39Z,Should we remove this as the member is not there anymore?
1397363828,14432,dajac,2023-11-17T14:05:27Z,nit: Let's put this comment before `ConsumerGroupMember member2UpdatedEpoch = ...`.
1397366414,14432,dajac,2023-11-17T14:06:46Z,"If I recall correctly, we automatically do this in `consumerGroupHeartbeat()`."
1397366873,14432,dajac,2023-11-17T14:06:58Z,nit: `Member...`.
1397367189,14432,dajac,2023-11-17T14:07:11Z,nit: `Member...` and `.` at the end.
1397368692,14432,dajac,2023-11-17T14:08:31Z,I suppose that this is not required as it don't be called.
1397369747,14432,dajac,2023-11-17T14:09:29Z,nit: Let's remove this empty line.
1397373074,14432,dajac,2023-11-17T14:11:28Z,nit: We can remove this empty line.
1397376607,14432,dajac,2023-11-17T14:14:25Z,Should we also verify that the timers are cancelled correctly?
1397378393,14432,dajac,2023-11-17T14:15:52Z,"Do we test all possible error cases? Thinking of fenced instance id, unknown member id, fenced member epoch, etc."
1397379496,14432,dajac,2023-11-17T14:16:50Z,"This is true if the member leaves with -2. If it leaves with -1, it should be removed immediately. Should we test this as well?"
1397382111,14432,dajac,2023-11-17T14:18:37Z,"I wonder if we could add a few more tests. Thinking about the following ones:
* The leaving static member should disappear if the new one does not rejoin with the session timeout.
* The new data structures should be updated correctly on replay."
1397383047,14432,dajac,2023-11-17T14:19:27Z,"Should we simplify a bit this test? We only need on member in the group to verify what we want here. We could also use one topic only, etc."
1397383490,14432,dajac,2023-11-17T14:19:50Z,We could also simplify this one.
1397383760,14432,dajac,2023-11-17T14:20:04Z,This one as well.
1399522392,14432,vamossagar12,2023-11-20T17:22:47Z,sure. `instance id should not be empty` check is already happening [here](https://github.com/apache/kafka/blob/trunk/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/GroupMetadataManager.java#L658)
1402215662,14432,vamossagar12,2023-11-22T15:12:10Z,"Actually, `groupEpoch == targetAssignmentEpoch` is not needed. I was just trying to ensure that the group epoch and target member epoch are the same which is what will happen when static member is replaced. So, in a way it's redundant. I will remove it."
1402217667,14432,vamossagar12,2023-11-22T15:13:39Z,"If I don't set `setAssignedPartitions`, then the replacing static member doesn't get it's assignments back. The pending assignments bit, I added just to ensure all assignments from previous member are assigned to the new member. The reason that happens is that it lands up [here](https://github.com/apache/kafka/blob/6b26c0428a5cf1f64e458e5a33df3bbfb0e33c6b/group-coordinator/src/main/java/org/apache/kafka/coordinator/group/consumer/CurrentAssignmentBuilder.java#L175) and if there are no partitions assigned, it gets back empty assignments."
1404037483,14432,dajac,2023-11-24T07:50:21Z,I would rather use `InstanceId can't be null.` here in order to be consistent with the other error messages.
1404037631,14432,dajac,2023-11-24T07:50:33Z,I suppose that we could remove this one now. Could we?
1404037934,14432,dajac,2023-11-24T07:50:54Z,Is it still necessary with the last implementation?
1404038516,14432,dajac,2023-11-24T07:51:40Z,nit: `Leave` -> `leave`.
1404039007,14432,dajac,2023-11-24T07:52:16Z,Should we also log something in this case?
1404039730,14432,dajac,2023-11-24T07:53:14Z,nit: I would put `member` as the first argument to be consistent with the other helpers.
1404042879,14432,dajac,2023-11-24T07:57:03Z,"* Could we move those helpers next to `throwIfMemberEpochIsInvalid`? Could we also add some javadoc to each of them?
* I wonder if we could also find better names for the params because it is not clear whether `memberId` and `instanceId` are the ones of the existing member or the ones received in the request. We could perhaps use `receivedMemberId`, etc. What do you think? This also applies to the other helpers."
1404047713,14432,dajac,2023-11-24T08:02:56Z,I wonder if we could follow the structure of the other log messages here: `[GroupId {}] Static Member {} with instance id {}....`.
1404047826,14432,dajac,2023-11-24T08:03:06Z,Should we also log something here?
1404050011,14432,dajac,2023-11-24T08:05:46Z,nit: `.` at the end.
1404050141,14432,dajac,2023-11-24T08:05:56Z,nit: `.` at the end.
1404051525,14432,dajac,2023-11-24T08:07:38Z,"nit: Could we also use `records = ` here? With this, we could remove `new ArrayList<>()` when `records` is declared, I think."
1404058043,14432,dajac,2023-11-24T08:15:26Z,nit: Let's revert this change as it is not necessary.
1404062478,14432,dajac,2023-11-24T08:20:30Z,"I am not sure to follow this one. My understanding is that we populate `staticMembers` only when `addOrUpdateMember` is called. In the main flow, we basically call this only once with the new or updated member.

Let's imagine that a new static member joins. We will add its static id with its member id to `staticMembers`. Therefore here, we basically get back its member id and end up with no assignment. Did I get this right?

I think that this could work but we would need to pass the `staticMembers` mapping from the `ConsumerGroup` to the builder, like we pass the members. If we have this, we could use it here to find the previous member with the static id if the member is new and has a static id."
1404070683,14432,dajac,2023-11-24T08:29:42Z,nit: `Instance id {} is unknown.`?
1404071502,14432,dajac,2023-11-24T08:30:30Z,nit: `Static member {} with instance id {} cannot join the group because the instance id is owned by member {}.`?
1404072143,14432,dajac,2023-11-24T08:31:17Z,nit: `Static member {} with instance id {} was fenced by member {}.`?
1404124114,14432,dajac,2023-11-24T09:22:48Z,"To close on this one, it is indeed correct to set the assigned partitions here. Without it, the reconciler checks if the partitions in the target assignment are still owned and they are effectively still owned until the previous member is removed. This only happens when the records are processed."
1404125908,14432,dajac,2023-11-24T09:24:32Z,I think that we could also `setPartitionsPendingRevocation` to empty because we know that the member has revoked all its partitions when it leaves.
1404157968,14432,dajac,2023-11-24T09:52:34Z,"This should not be here. I think that you mix in two different things. `addGroupMember` is basically what is used to build what will be passed to `withMembers` and `withTargetAssignment` whereas `updateMemberSubscription` is for `addOrUpdateMember`. Therefore, the test does not reproduce how we use it."
1404158564,14432,dajac,2023-11-24T09:53:07Z,This one is incorrect as well because the newly added member is not added via `withMembers`.
1404163349,14432,dajac,2023-11-24T09:57:41Z,nit: Indentation seems off here. I think that it should be 4 spaces earlier.
1404164525,14432,dajac,2023-11-24T09:58:45Z,nit: Indentation is incorrect. 
1404165368,14432,dajac,2023-11-24T09:59:30Z,nit: Indentation.
1404167570,14432,dajac,2023-11-24T10:01:35Z,Let's replace `-2` with the relevant constant. There are other cases in this file.
1404168570,14432,dajac,2023-11-24T10:02:31Z,nit: Temporarily leave?
1404172073,14432,dajac,2023-11-24T10:05:48Z,We already have `testConsumerHeartbeatRequestValidation` so I wonder if we could just add the new case there. What do you think?
1404172650,14432,dajac,2023-11-24T10:06:25Z,nit: Indentation.
1405732004,14432,vamossagar12,2023-11-27T07:18:31Z,No it is not. Removed it.
1405732141,14432,vamossagar12,2023-11-27T07:18:43Z,Added a log line
1405732647,14432,vamossagar12,2023-11-27T07:19:24Z,"Done, moved the methods next to `throwIfMemberEpochIsInvalid`, added Javadocs and updated the argument names."
1405733434,14432,vamossagar12,2023-11-27T07:20:28Z,Done.
1405737202,14432,vamossagar12,2023-11-27T07:25:42Z,"Yes, your understanding is correct. I see what you are saying about how this won't work when a new static member joins. I have updated the group to expose the current set of static members in the group."
1405737404,14432,vamossagar12,2023-11-27T07:26:02Z,Thank you for the confirmation.
1405738152,14432,vamossagar12,2023-11-27T07:27:02Z,"I see. Thanks for the explanation, I hadn't understood the usage of these methods correctly. I have removed these unwanted calls to `updateMemberSubscription`"
1405739030,14432,vamossagar12,2023-11-27T07:28:09Z,Ok.. I am slightly confused by this comment. The new member is being added using `addGroupMember` which internally invokes `withMembers`. I had an unwanted call to `updateMemberSubscription` which I have removed. Probably I am missing something here.
1405785516,14432,dajac,2023-11-27T08:13:45Z,I think that we should call `updateMemberSubscription` instead of calling `addGroupMember` here because `updateMemberSubscription` is what is used to eventually call `addOrUpdateMember`.
1331125886,14408,dajac,2023-09-20T07:24:58Z,nit: You could use `foreach` which is a bit more concise. 
1331127964,14408,dajac,2023-09-20T07:26:29Z,"nit: Let's put `delete-group` on a new line as well. Could you also ensure that the format conforms to the existing code? e.g. where the closing parenthesis is, the indentation (4 spaces), etc."
1331129777,14408,dajac,2023-09-20T07:27:57Z,It may be better to use `join` instead of `get`. I think that you would be able to remove the try..catch if you use `join`.
1331132885,14408,dajac,2023-09-20T07:30:11Z,"Let's assume that one of the write operation fails with `COORDINATOR_LOAD_IN_PROGRESS`, this would result in failing `allFutures` even though some write operations may have been successful. It seems to me that we should handle exceptions for each write operation future before we combine them, no?"
1331133058,14408,dajac,2023-09-20T07:30:21Z,nit: Indentation. 
1331134867,14408,dajac,2023-09-20T07:31:13Z,nit: Indentation. There are other cases in this PR. I won't mention them all.
1331141079,14408,dajac,2023-09-20T07:36:09Z,"I have a few comments regarding this piece of code:
1. I think that we should write the tombstones for the offsets before the ones for the group.
2. It is a bit strange to return a CoordinatorResult from `deleteAllOffsets` and to ignore it. It would be better to pass the list of records to the method and to let the method populate it if the deletion is accepted. I would also remove the response as we don't need it.
3. The `validGroupIds` is a bit weird. How about iterating over the group ids here? Then, you can call the various methods from the manages to validate, delete offsets and finally delete the group. If there is an error, you can directly populate the response with it."
1331143758,14408,dajac,2023-09-20T07:37:23Z,"`newGroupMetadataTombstoneRecord` only works for generic groups. For consumer groups, we need to write other tombstones."
1331145296,14408,dajac,2023-09-20T07:38:32Z,We should not write the record if subscribedToTopic is true because it will effectively delete the offset.
1331146404,14408,dajac,2023-09-20T07:39:23Z,"As I said earlier, I think that returning CoordinatorResult is not appropriate here because we don't need a response in this case. We basically build for the response to ignore it right after."
1331958841,14408,dongnuo123,2023-09-20T17:19:03Z,"Yeah, it makes sense. I'm not sure what to return if there's an exception in the write operation, since we can't set an error code for a `DeletableGroupResultCollection`."
1331960774,14408,dongnuo123,2023-09-20T17:20:56Z,done
1331961151,14408,dongnuo123,2023-09-20T17:21:20Z,done
1331961609,14408,dongnuo123,2023-09-20T17:21:49Z,done
1331969154,14408,dongnuo123,2023-09-20T17:29:14Z,"I rearranged this part. Now we loop over the group ids and process them one by one -- validate, populate record list with offset tombstones, add group tombstone, and add response."
1331969312,14408,dongnuo123,2023-09-20T17:29:25Z,done
1332009903,14408,dongnuo123,2023-09-20T18:08:40Z,Fixed.
1332010169,14408,dongnuo123,2023-09-20T18:08:59Z,Fixed
1332010328,14408,dongnuo123,2023-09-20T18:09:11Z,Fixed
1332057781,14408,dajac,2023-09-20T18:59:25Z,`DeletableGroupResultCollection` contains `DeletableGroupResult` which has an error code. Therefore I think that we should create a `DeletableGroupResult` per group id in the `groupList` when there is an exception.
1332069169,14408,dajac,2023-09-20T19:11:34Z,"I think that we need to generate the above records here.
* newTargetAssignmentEpochTombstoneRecord
* newGroupSubscriptionMetadataTombstoneRecord
* newGroupEpochTombstoneRecord"
1332072625,14408,dajac,2023-09-20T19:15:21Z,Do we need this CoordinatorResult?
1332073736,14408,dajac,2023-09-20T19:16:28Z,"This is the same as newGroupEpochTombstoneRecord, no?"
1332632377,14408,dajac,2023-09-21T07:53:22Z,"nit: I wonder if we should use `topicPartitionFor` here. With this, we could directly have the TopicPartition as the key in the Map and we would not need to create `new TopicPartition(Topic.GROUP_METADATA_TOPIC_NAME, partition)` later on. What do you think?"
1332633082,14408,dajac,2023-09-21T07:53:58Z,nit: We could specify the size of the array when we allocate it.
1332637192,14408,dajac,2023-09-21T07:56:43Z,"nit: You could do the following to avoid having to put the list again into the map.

```
groupsByPartition
    .computeIdAbsent(partition, __ -> new ArrayList())
    .put(groupId);
```"
1332637739,14408,dajac,2023-09-21T07:57:13Z,nit: `res.addAll(future.join())` to reduce the code?
1332656181,14408,dajac,2023-09-21T08:12:15Z,"It is interesting to point out that, in the current implementation, all these errors are swallowed. This is definitely not ideal because it tells to the user that the deletion is successful even if was not. Should we apply the same error handling to the deleteGroups?"
1332661430,14408,dajac,2023-09-21T08:16:25Z,nit: We can remove this empty line.
1332661944,14408,dajac,2023-09-21T08:16:51Z,"nit: `deleteAllOffsets`? I also wonder if the context is required. If not, we could remove it."
1332664716,14408,dajac,2023-09-21T08:18:59Z,"The CoordinatorResult is a bit annoying here. How about passing `records` to the method as well? Then we could construct the response here. We could also remove the context if it is not needed.

How about naming it `deleteGroup` to be consistent with `deleteOffsets`?"
1332665059,14408,dajac,2023-09-21T08:19:14Z,nit: We could remove this empty line?
1332667219,14408,dajac,2023-09-21T08:20:58Z,"I have a question regarding the error handling. Could `groupDelete` thrown an exception? If it can, we would need to handle records a bit differently because we don't want to delete the offsets if the group cannot be delete. The operation should be atomic."
1332667737,14408,dajac,2023-09-21T08:21:23Z,nit: The indentation is incorrect.
1332670356,14408,dajac,2023-09-21T08:22:52Z,nit: Let's remove this empty line and add javadoc to the method.
1332672724,14408,dajac,2023-09-21T08:23:41Z,nit: Indentation is incorrect. 
1332679503,14408,dajac,2023-09-21T08:28:21Z,"Let's do the validation before allocating response, records, etc. We don't have to allocate them if the request is invalid. `group` could also be `final`."
1332679648,14408,dajac,2023-09-21T08:28:28Z,nit: final?
1332680359,14408,dajac,2023-09-21T08:29:01Z,"I think that the try..catch is not needed here because we handle the exceptions in the group coordinator service, no?"
1332682319,14408,dajac,2023-09-21T08:30:30Z,nit: `response = ` is not needed here as `setTopics` mutates the response directly.
1332689518,14408,dajac,2023-09-21T08:35:32Z,I wonder if we need to verify if there is actually an offset for the topic/partition. We don't need to write a tombstone if there is not. What do you think?
1332692288,14408,dajac,2023-09-21T08:37:43Z,I think that a consumer group will actually never transition to Dead. We could actually remove this state.
1332692487,14408,dajac,2023-09-21T08:37:52Z,ditto.
1332693494,14408,dajac,2023-09-21T08:38:40Z,I wonder if using a switch would be better here. What do you think?
1332694686,14408,dajac,2023-09-21T08:39:31Z,This does not seem correct to me because this exception does not apply to consumer groups.
1332695102,14408,dajac,2023-09-21T08:39:47Z,"As mentioned earlier, we have to generate other tombstones."
1332697684,14408,dajac,2023-09-21T08:41:20Z,Throwing an exception does not seem to be the right approach to me because we still want to delete the group and the exception will stop the process. My understanding is that we could just skip generating the tombstone if the generation <= 0.
1333162337,14408,dajac,2023-09-21T14:32:57Z,"Actually, what I said is wrong here. I think that we should generate the tombstone in any cases to ensure that the group is removed from the timeline hashmap."
1333326880,14408,rreddy-22,2023-09-21T16:24:11Z,"nit: can we add a tab space and capitalize ""The""  -> topic      The topic name."
1333327696,14408,rreddy-22,2023-09-21T16:24:52Z,"Also I thought we had decided to use topicIds instead of topic names throughout the new protocol, are we using topic names for this API?"
1333329945,14408,rreddy-22,2023-09-21T16:26:53Z,"nit: same with this, tab spaces to align both param descriptions"
1333331492,14408,rreddy-22,2023-09-21T16:28:22Z,nit: extra line
1333348886,14408,rreddy-22,2023-09-21T16:42:49Z,nit: period is missing
1333554676,14408,jeffkbkim,2023-09-21T20:11:58Z,"let's say that for some of the topic partitions, the deleteGroups write operations were successful. For others, let's say that there was a timeout. This would return a request timeout to the clients indicating that the request failed. I think this is fine, but it could be confusing to the user. @dajac what are your thoughts?"
1333558630,14408,jeffkbkim,2023-09-21T20:16:28Z,we can use Collections.singletonList()
1333583218,14408,jeffkbkim,2023-09-21T20:45:18Z,this can be null right? if there are no offsets for the given group id
1333600423,14408,jeffkbkim,2023-09-21T21:01:38Z,"nit: Handles ""a""

maybe we can reword this to ""Deletes offsets as part of a DeleteGroups request."""
1333601100,14408,jeffkbkim,2023-09-21T21:02:28Z,"nit: `(partition, __) ->`"
1333615622,14408,jeffkbkim,2023-09-21T21:20:16Z,should it be `offsetsByTopic.get(topic.name())`?
1333615893,14408,jeffkbkim,2023-09-21T21:20:40Z,should this be `containsKey(partition.partitionIndex())`?
1334471084,14408,dajac,2023-09-22T14:37:24Z,@jeffkbkim I made the same comment earlier and @dongnuo123 updated the code to handle exceptions for each write operation.
1334519906,14408,dajac,2023-09-22T15:15:50Z,Offsets APIs still use topic names...
1336162536,14408,rreddy-22,2023-09-25T16:56:06Z,got it okie!
1336165006,14408,rreddy-22,2023-09-25T16:58:32Z,nit: can we add new lines between the tests
1336165306,14408,rreddy-22,2023-09-25T16:58:52Z,nit: line
1336168777,14408,rreddy-22,2023-09-25T17:02:31Z,nit: line
1336348269,14408,jeffkbkim,2023-09-25T20:18:12Z,i wonder if createGroupTombstoneRecords() makes more sense
1336363313,14408,jeffkbkim,2023-09-25T20:36:10Z,"nit: ""DeleteGroups"" request.

This should reflect the actual ApiKeys#DELETE_GROUPS name"
1336441437,14408,jeffkbkim,2023-09-25T22:24:33Z,"can we add a test with three __consumer_offsets topic partitions where one finishes immediately, another takes a while, and the last coordinator throws an exception?"
1336446270,14408,jeffkbkim,2023-09-25T22:33:21Z,"nit: testDeleteGroups

also, can we verify the number of method invocations and also test that we append records correctly for multiple groups?"
1336446500,14408,jeffkbkim,2023-09-25T22:33:44Z,"nit: testDeleteGroupsInvalidGroupId

can we also add a valid group id and verify the first stores invalid group id error and the second stores NONE?"
1336449678,14408,jeffkbkim,2023-09-25T22:38:58Z,should this be a static method?
1336451305,14408,jeffkbkim,2023-09-25T22:42:01Z,we can inline this to L380
1336461743,14408,jeffkbkim,2023-09-25T23:01:52Z,this can be removed
1336461886,14408,jeffkbkim,2023-09-25T23:02:11Z,we can do `consumerGroup::validateGroupDelete` for this along with the other invocations in the test
1336465543,14408,jeffkbkim,2023-09-25T23:10:02Z,should we add EMPTY test case? also for testValidateGroupDelete
1339019021,14408,jeffkbkim,2023-09-27T18:13:35Z,should these be DeleteGroup?
1339020329,14408,jeffkbkim,2023-09-27T18:14:44Z,Whether
1339025948,14408,jeffkbkim,2023-09-27T18:19:28Z,"should this be ""delete-groups""?"
1339176820,14408,jeffkbkim,2023-09-27T20:29:44Z,"Validations are done in `{@link GroupCoordinatorShard#deleteGroups(RequestContext, List)}`"
1339182547,14408,jeffkbkim,2023-09-27T20:34:35Z,"ditto on link

can we add a comment on why we don't expect an exception to be thrown here?"
1339194498,14408,jeffkbkim,2023-09-27T20:45:50Z,The id of the group to be deleted.
1339198942,14408,jeffkbkim,2023-09-27T20:50:06Z,"in the existing implementation, we transition to DEAD if the group is empty so that even if the write operation fails we delete the group in the next purge cycle.

we don't need to do this here since if the write operation fails we revert to the previous state and return an error so the client knows that the operation failed. is this correct?"
1339201386,14408,jeffkbkim,2023-09-27T20:52:23Z,"the current implementation logs how many groups and offsets were removed. should we add something similar?

```
      info(s""The following groups were deleted: ${groupsEligibleForDeletion.map(_.groupId).mkString("", "")}. "" +
        s""A total of $offsetsRemoved offsets were removed."")
```"
1339216413,14408,jeffkbkim,2023-09-27T21:06:07Z,can we move this check outside of the forEach block? we perform this check for every partition of the topic
1339228792,14408,jeffkbkim,2023-09-27T21:17:29Z,"don't we also need to check whether the stable group is using the ConsumerProtocol.PROTOCOL_TYPE?
from
```
                case PreparingRebalance | CompletingRebalance | Stable if group.isConsumerGroup =>
```
"
1339232259,14408,jeffkbkim,2023-09-27T21:21:07Z,"should this be ""delete-offsets""?"
1339234027,14408,jeffkbkim,2023-09-27T21:22:59Z,do we need this?
1339235261,14408,jeffkbkim,2023-09-27T21:24:20Z,can we follow the same line break as in L1101-1102? same for result2
1339255544,14408,jeffkbkim,2023-09-27T21:46:00Z,"in general, it's not a good practice to use Thread.sleep in tests. also, i don't think this tests what we actually want to test.

We want to confirm that the final future is not completed until this operation completes. So i propose:
1. have this thread wait
2. confirm future did not complete
3. unblock this thread
4. confirm future completes

something like the following:
```
        CountDownLatch latch = new CountDownLatch(1);
        ...

        when(runtime.scheduleWriteOperation(
            ArgumentMatchers.eq(""delete-group""),
            ArgumentMatchers.eq(new TopicPartition(""__consumer_offsets"", 0)),
            ArgumentMatchers.any()
        )).thenAnswer(invocation -> CompletableFuture.supplyAsync(() -> {
            try {
                assertTrue(latch.await(5, TimeUnit.SECONDS));
            } catch (InterruptedException ignored) {}
            return resultCollection2;
        }));

        when(runtime.scheduleWriteOperation(
            ArgumentMatchers.eq(""delete-group""),
            ArgumentMatchers.eq(new TopicPartition(""__consumer_offsets"", 1)),
            ArgumentMatchers.any()
        )).thenReturn(FutureUtils.failedFuture(new CoordinatorLoadInProgressException(null)));

        List<String> groupIds = Arrays.asList(""group-id-1"", ""group-id-2"", ""group-id-3"", null);
        CompletableFuture<DeleteGroupsResponseData.DeletableGroupResultCollection> future =
            service.deleteGroups(requestContext(ApiKeys.DELETE_GROUPS), groupIds, BufferSupplier.NO_CACHING);

        assertFalse(future.isDone());
        latch.countDown();

        TestUtils.waitForCondition(future::isDone, ""The future did not complete."");
        assertTrue(expectedResultCollection.containsAll(future.get()));
        assertTrue(future.get().containsAll(expectedResultCollection));
```"
1340712345,14408,dajac,2023-09-28T22:14:06Z,nit: Could we refactor `getErrorResponse` to use this new method as well? Should we also add a unit test for this one?
1340712743,14408,dajac,2023-09-28T22:14:53Z,I wonder if we should rather pass the list of records as an argument in order to avoid having to copy the records afterwards. Have you considered this?
1340716345,14408,dajac,2023-09-28T22:22:06Z,nit: Should we set the expected size here?
1340716744,14408,dajac,2023-09-28T22:22:57Z,"Good question. In my opinion, this log line is useful for the expiration case. I am not sure if it really is in this one."
1340717283,14408,dajac,2023-09-28T22:24:01Z,nit: Let's remove this empty line.
1340717524,14408,dajac,2023-09-28T22:24:31Z,nit: Should we just add this to the document of the groupId field?
1340720235,14408,dajac,2023-09-28T22:29:11Z,"@jeffkbkim Do we ever transition to Dead? If not, I wonder if we should just remove this and remove the Dead state. What do you think?"
1340720555,14408,dajac,2023-09-28T22:29:51Z,nit: I wonder if using a switch would be better here. Have you considered it?
1340720632,14408,dajac,2023-09-28T22:30:01Z,nit: ditto for the switch.
1340744802,14408,dajac,2023-09-28T23:20:18Z,nit: Could we put `setGroupId` on a new line as well?
1340745968,14408,dajac,2023-09-28T23:23:13Z,nit: The `null` here is not ideal. Could we put a string instead? Or you could also use COORDINATOR_LOAD_IN_PROGRESS.exception().
1340747829,14408,dajac,2023-09-28T23:27:23Z,"I am not sure to understand what you are trying to achieve here. Could you elaborate?

If you want to delay the completion of the future, the best would be to create a CompletableFuture, use thenReturn(future), and then complete the future at L1149."
1340749921,14408,dajac,2023-09-28T23:33:00Z,Is there a reason why you don't use when().thenAnswer(...)?
1340750286,14408,dajac,2023-09-28T23:33:55Z,Could we also use `groupIds` to generate the list here?
1340750616,14408,dajac,2023-09-28T23:34:52Z,ditto for those two.
1340750923,14408,dajac,2023-09-28T23:35:44Z,nit: Could we put an empty line before this one? I find the code a bit hard to read because all the lines are all together.
1340752025,14408,dajac,2023-09-28T23:38:51Z,"small nit: It may be a bit easier to read if we create the expected response as follow? What do you think? If you find it better, we could also update the other test cases.

```
new DeleteGroupsResponseData.DeletableGroupResultCollection(Arrays.asList(
   new DeleteGroupsResponseData.DeletableGroupResult()
       .setGroupId(""group-id-1""),
  ....
).iterator());


```"
1340752304,14408,dajac,2023-09-28T23:39:37Z,Should we at minimum verify the group ids here?
1340753404,14408,dajac,2023-09-28T23:42:35Z,"In the GroupMetadataManagerTestContext, we actually moved this to the replay method. See [here](https://github.com/apache/kafka/blob/ad7956170bcaf093ea8b2f725126d42cf7fb522b/group-coordinator/src/test/java/org/apache/kafka/coordinator/group/GroupMetadataManagerTest.java#L1144). It may be better to do the same here. What do you think?"
1340753611,14408,dajac,2023-09-28T23:43:06Z,nit: Indentation should be 4 spaces.
1340753971,14408,dajac,2023-09-28T23:44:05Z,Should we move this method to the test context?
1340754157,14408,dajac,2023-09-28T23:44:38Z,We could also apply my formatting suggestion here.
1340754965,14408,dajac,2023-09-28T23:46:50Z,"This block is really hard to parse. 

```
final List<Record> expectedRecords = 
     error == Errors.NONE && context.offsetMetadataManager.offset(groupId, topic, partition) != null ?
            Collections.singletonList(RecordHelpers.newOffsetCommitTombstoneRecord(groupId, topic, partition)) :
            Collections.emptyList();
```

Would it be better like this? Otherwise, I would use a regular if statement."
1340756205,14408,dajac,2023-09-28T23:50:13Z,What does `InvalidOffset` mean here?
1340756359,14408,dajac,2023-09-28T23:50:43Z,ditto.
1340757038,14408,dajac,2023-09-28T23:52:20Z,Could we use `GroupType` instead? Then you could use a switch based on the enum.
1340757258,14408,dajac,2023-09-28T23:52:59Z,nit: Could we put an empty line here?
1340783686,14408,jeffkbkim,2023-09-29T01:11:23Z,"@dajac the current [draft PR for group/offset expiration](https://github.com/apache/kafka/pull/14467) transitions groups to dead once a group is empty && offsets are gone.

the current behavior for generic groups is the above, and i copied the same behavior for consumer groups. then once the group is Dead, it will be considered for expiration in the next cycle."
1340783926,14408,jeffkbkim,2023-09-29T01:12:09Z,should we at least log the number of groups that were deleted from the DeleteGroups request?
1340788752,14408,jeffkbkim,2023-09-29T01:27:40Z,"this was my suggestion. your suggestion is much simpler, thanks!"
1341653044,14408,dajac,2023-09-29T17:49:06Z,"I am not sure to understand why we need to do this. Couldn't we just delete the group when it is empty and offsets are gone instead of transitioning to Dead and then deleting it?

My understanding is that we use Dead in the old code because we can't remove the group from the map before the change is committed to the log. During this time, the group is in the Dead state. In our world, the group is remove from the map immediately and the change is reverted if the write fails."
1341917129,14408,yangy0000,2023-09-30T06:16:35Z,"Error handling codes in lines 555-586 and 797-816 are very similar. An alternative implementation is 

create a util method:

```
private Errors transformExceptionCode(Exception e)
   if (exception instanceof UnknownTopicOrPartitionException ||
                exception instanceof NotEnoughReplicasException) {
                return Errors.COORDINATOR_NOT_AVAILABLE
     }
...
)
```
within deleteGroups : 
```
exception-> {
return DeleteGroupsRequest.getErrorResultCollection(
                            groupList,
                            transformExceptionCode(e))
}
```

within deleteOffsets: 
```
exception-> {
return new OffsetDeleteResponseData()
                    .setErrorCode(
                            transformExceptionCode(e).code());
}
```"
1341918630,14408,yangy0000,2023-09-30T06:34:17Z,"nit: the else braces can be simplified to 
```
final TimelineHashMap<Integer, OffsetAndMetadata> offsetsByPartition = offsetsByTopic == null ?
                null : offsetsByTopic.get(topic.name());
if(offsetsByPartition  ==null) {continue};              
for(Integer paritionIndex : offsetsByPartition.keySet()) {
   responsePartitionCollection.add(new OffsetDeleteResponseData.OffsetDeleteResponsePartition().setPartitionIndex(paritionIndex)
   );
     records.add(RecordHelpers.newOffsetCommitTombstoneRecord(
     request.groupId(),
     topic.name(),
      paritionIndex
  ));
}
```"
1341919233,14408,yangy0000,2023-09-30T06:42:12Z,"No check on ""isSubscribedToTopic"" in here? is this expected? "
1342599402,14408,dajac,2023-10-02T12:02:33Z,nit: Given that there is only one test. I would rather move everything into that test.
1342610382,14408,dajac,2023-10-02T12:13:55Z,"I am +1 for bringing the definition of `offsetsByPartition` within the `else` clause. However, we have to keep using `topic.partitions().forEach(` to iterate over the partitions. However, I don't like `if(offsetsByPartition ==null) {continue};`. How about using `offsetsByPartition != null`?"
1342611067,14408,dajac,2023-10-02T12:14:42Z,"No, we don't need it here because the group is completely removed in this case."
1342627317,14408,dajac,2023-10-02T12:31:57Z,I wonder if we should rather log this within the shard in order to have it logged per shard (with the shard context). What do you think?
1342628078,14408,dajac,2023-10-02T12:32:46Z,nit: How about `return allFutures.thenApply...`?
1342632075,14408,dajac,2023-10-02T12:36:58Z,"nit: Would it make sense to use? We don't really need `response` except here.

```
return new CoordinatorResult<>(
    records,
    new OffsetDeleteResponseData().setTopics(responseTopicCollection)
);"
1342635373,14408,dajac,2023-10-02T12:40:21Z,nit: Could we use `state() != Empty`? This would be more robust.
1342640395,14408,dajac,2023-10-02T12:45:34Z,I wonder if we should test all the exceptions that we re-map. I did something similar in `testConsumerGroupHeartbeatWithException`. What do you think?
1342641976,14408,dajac,2023-10-02T12:47:11Z,ditto about the error mapping verification.
1342642294,14408,dajac,2023-10-02T12:47:29Z,Can't we use `assertEquals`?
1342643914,14408,dajac,2023-10-02T12:48:59Z,It would be better to use the other way in order to remain consistent with the other tests. Is this possible?
1342645279,14408,dajac,2023-10-02T12:50:17Z,nit: expectedError?
1342647294,14408,dajac,2023-10-02T12:51:42Z,"nit: Should we define an helper method in the context (e.g. hasOffset(groupId, topic, partition))? I would also bring `error == Errors.NONE` back on the previous line because it fits there."
1342851207,14408,yangy0000,2023-10-02T15:33:32Z,Any chance deleteAllOffsets will get invoked before the group is completely removed?
1343253093,14408,jeffkbkim,2023-10-02T23:24:28Z,was this addressed?
1343255509,14408,jeffkbkim,2023-10-02T23:30:33Z,"also, we can remove the `v`"
1343257434,14408,jeffkbkim,2023-10-02T23:34:56Z,nit: an
1343258721,14408,jeffkbkim,2023-10-02T23:38:09Z,"""At this point, we have already validated the group id so we know that the group exists and that no exception will be thrown.""

how's this?"
1343258959,14408,jeffkbkim,2023-10-02T23:38:35Z,"nit: can we change all usages of ""ID"" to ""id""?"
1343259306,14408,jeffkbkim,2023-10-02T23:39:31Z,we can move these into the test as well
1343259404,14408,jeffkbkim,2023-10-02T23:39:45Z,"we can remove the ""v"""
1343260319,14408,jeffkbkim,2023-10-02T23:41:35Z,what's the benefit of using final variables here?
1343262352,14408,jeffkbkim,2023-10-02T23:46:19Z,"we can use
```
                offsetsByPartition.keySet().forEach(partition ->
```"
1343263220,14408,jeffkbkim,2023-10-02T23:48:18Z,nit: newline
1343263632,14408,jeffkbkim,2023-10-02T23:49:12Z,@Override?
1343266197,14408,jeffkbkim,2023-10-02T23:54:55Z,"can we use
```
    @MethodSource(""testConsumerGroupHeartbeatWithExceptionSource"")
```
and remove the helper method?"
1343267277,14408,jeffkbkim,2023-10-02T23:57:24Z,can we assert true that the future is done?
1343267559,14408,jeffkbkim,2023-10-02T23:57:57Z,"can we use
```
    @MethodSource(""testConsumerGroupHeartbeatWithExceptionSource"")
```

and remove the helper?"
1343269222,14408,jeffkbkim,2023-10-03T00:01:47Z,can we change all of the `doSomething...when...` to `when().doSomething`?
1344182771,14408,dajac,2023-10-03T14:14:31Z,"I think that we should be careful with this. The change is not 100% equivalent to the previous implementation here because the error message is not set for all errors whereas it was only set of a sub set before. While I agree that we could do better, I would suggest to tackle this in a separate PR."
1344185641,14408,dajac,2023-10-03T14:16:09Z,"nit: If we keep it, the method could be static and we usually don't prefix methods with `get`. `normalizeException` maybe an alternative name."
1344186369,14408,dajac,2023-10-03T14:16:40Z,Why do we need an AtomicInteger here?
1344187568,14408,dajac,2023-10-03T14:17:30Z,nit: `... removed.`.
1344188289,14408,dajac,2023-10-03T14:17:57Z,"I guess that they don't hurt, isn't it?"
1344189933,14408,dajac,2023-10-03T14:19:04Z,"If you look at the usage in `[GroupCoordinatorShard.java](https://github.com/apache/kafka/pull/14408/files#diff-d6369ef583dce1f7570cf396d7a4762c679fd2af323e1e1f93c9b665258373a0)`, all offsets are removed before deleting the group."
1344192203,14408,dajac,2023-10-03T14:20:23Z,I agree. I mentioned this a few times a well.
1344192583,14408,dajac,2023-10-03T14:20:34Z,ditto.
1344192793,14408,dajac,2023-10-03T14:20:41Z,ditto.
1344192915,14408,dajac,2023-10-03T14:20:45Z,ditto.
1344193032,14408,dajac,2023-10-03T14:20:50Z,ditto.
1344498860,14408,dongnuo123,2023-10-03T17:44:07Z,"If we use int, we'll get error `Variable used in lambda expression should be final or effectively final`. Lambda expressions do not allow any external variable operation within itself. I can add a small comment here."
1344502310,14408,dongnuo123,2023-10-03T17:47:11Z,when(method).doSomething requires method to return a non-void value. I can add a comment here for explanation.
1344523685,14408,dongnuo123,2023-10-03T18:06:47Z,rolled back
1344524665,14408,dongnuo123,2023-10-03T18:07:42Z,comment added
1344524782,14408,dongnuo123,2023-10-03T18:07:49Z,comment added
1344524966,14408,dongnuo123,2023-10-03T18:07:59Z,comment added
1344525094,14408,dongnuo123,2023-10-03T18:08:08Z,comment added
1344554094,14408,dajac,2023-10-03T18:34:48Z,Ack. I wonder if we should use `for (String groupId : groupIds) ....` then. What do you think?
464705276,9100,hachikuji,2020-08-03T23:00:19Z,Probably reasonable to handle it the same way other inter-broker RPCs are handled.
464708303,9100,hachikuji,2020-08-03T23:10:04Z,"Good question. Might be fair to assume the controller is correct and use STALE_BROKER_EPOCH. Once kip-500 is all done, it would be totally fair since the controller will be guaranteed to have the latest state. The other question is what the broker should do if it sees STALE_BROKER_EPOCH..."
464709341,9100,hachikuji,2020-08-03T23:13:24Z,"Hmm.. This adds a delay of 2.5s to every ISR change, which is a bit annoying. I guess the point is to allow batching? I think a better approach might be to send requests immediately on arrival, but set a limit on the maximum number of in-flight requests (maybe just 1) and let the changes accumulate when there is a request in-flight. Then we can still get a big batching benefit when there are a large number of ISR changes that need to be sent in a hurry."
464710258,9100,hachikuji,2020-08-03T23:16:19Z,Hmm.. Not sure it's worth doing these validations up-front. These checks could fail between the time that the event is enqueued and the time it is processed.
464716040,9100,mumrah,2020-08-03T23:35:25Z,That makes sense. I'll change that (this was pulled in from the previous ISR notification code in ReplicaManager)
464717171,9100,mumrah,2020-08-03T23:39:05Z,"The main rationale for validating in the request handler is so we can return meaningful partition-level errors to the broker (fenced leader, not leader or follower, etc). Although, I'm not sure the broker could do anything useful with these errors since it probably has stale metadata in these cases.

The KIP calls out four partition-level errors. Do we actually need them?"
464718819,9100,hachikuji,2020-08-03T23:44:49Z,"To be clear, I'm not questioning the need for the validation, just the fact that it is done before enqueueing the event instead of when the event is processed."
465029352,9100,mumrah,2020-08-04T12:56:33Z,"Ah, I see what you mean. Initially, I was concerned about blocking for too long while waiting for a response, but it looks like there is precedent for this pattern for some requests (reassignment, leader election, controlled shutdown). I'll move this validation and the callback down into the event processor method"
465708837,9100,cmccabe,2020-08-05T13:00:48Z,"It seems like we need to set the `inSyncReplicaIds` here, since we don't do it in `shrinkIsr`."
465711962,9100,cmccabe,2020-08-05T13:06:08Z,This is also a concurrency bug since you can't access stuff like the controllerContext except from the controller thread itself (it would be multi-threaded access without a lock)
465712943,9100,cmccabe,2020-08-05T13:07:48Z,This also needs to call `NetworkClient#wake` in case we are blocking inside `NetworkClient#poll`
465715770,9100,cmccabe,2020-08-05T13:12:19Z,"It would be good to find a better name for this.  When I read ""AlterIsrChannelManager"" I assumed it had its own separate channel, rather than using the ControllerChannelManager."
465743922,9100,mumrah,2020-08-05T13:54:54Z,@cmccabe good to know about `controllerContext`  
465746162,9100,mumrah,2020-08-05T13:58:00Z,"It might be simpler just to use AlterIsrRequestData and AlterIsrResponseData throughout this code (rather than converting to `Map[TopicPartition, LeaderAndIsr]` and `Map[TopicPartition, Errors]`)"
465748120,9100,mumrah,2020-08-05T14:00:36Z,"I think we need to send LeaderAndIsr for all the given partitions whether we updated the ISR or not. In cases where we failed due, the leaders likely have stale metadata. This way we can proactively send them the latest state."
465748504,9100,mumrah,2020-08-05T14:01:11Z,"Yea, maybe just ""AlterIsrManager""? "
465749546,9100,mumrah,2020-08-05T14:02:38Z,"Should probably get rid of this and change the method to 

```enqueueIsrUpdate(TopicPartition, LeaderAndIsr)```"
465750445,9100,mumrah,2020-08-05T14:03:55Z,Remove this
465750632,9100,mumrah,2020-08-05T14:04:10Z,newline
465888182,9100,abbccdda,2020-08-05T17:28:23Z,nit: could move these operations to the `AlterIsrRequest` as helpers.
469372344,9100,hachikuji,2020-08-12T16:03:52Z,"With KIP-500, I imagine we could end up with other cases where we end up using optimistic concurrency control. Does it make sense to make this error a little more generic? Maybe `INVALID_UPDATE_VERSION` or something like that.."
469373339,9100,hachikuji,2020-08-12T16:05:24Z,nit: maybe drop the parameters if they do not need to be documented
469379488,9100,hachikuji,2020-08-12T16:14:57Z,nit: info feels a bit high for a message like this
469382729,9100,hachikuji,2020-08-12T16:20:13Z,We might need to be careful about performance here since this would get called on every follower fetch.
469383572,9100,hachikuji,2020-08-12T16:21:34Z,"The usage is a bit surprising given the ""pending"" name. I wonder if it would be clearer if we used a type of `Option[Set[Int]]` so that we could use `None` when there is no pending ISR change.

One more thing. It's worth double-checking the threading assumptions here. It looks like `updateAssignmentAndIsr` is only called while holding the write side of `leaderIsrUpdateLock`. On the other hand, I don't see any lock held in `updateFollowerFetchState`. It's worth stepping through that logic to make sure that we do not depend on `inSyncReplicaIds` and `pendingInSyncReplicaIds` getting set atomically."
469385374,9100,hachikuji,2020-08-12T16:24:32Z,"I think the answer is no. The pending ISR set is not guaranteed, so we cannot depend on it to enforce min.isr."
469388095,9100,hachikuji,2020-08-12T16:29:06Z,"Related to the other comment, but we need to be careful with the min.isr check below. I think it is correct to wait for `effectiveInSyncReplicaIds` before acknowledging the produce request, but we should probably use the size of `inSyncReplicaIds` in the min.isr check since that is the only set we can guarantee."
469399037,9100,hachikuji,2020-08-12T16:47:34Z,"There is a ""classic"" edge case in Kafka which goes as follows:

1. Leader is 1, ISR is [1, 2, 3]
2. Broker 3 begins controlled shutdown. While awaiting shutdown, it continues fetching.
3. Controller bumps epoch and shrinks ISR to [1, 2] and notifies replicas
4. Before controlled shutdown completes and 3 stops fetching, the leader adds it back to the ISR.

This bug was fixed by KIP-320 which added epoch validation to the Fetch API. After shrinking the ISR in step 3, the controller will send `LeaderAndIsr` with the updated epoch to [1, 2] and `StopReplica` to [3]. So 3 will not send any fetches with the updated epoch, which means it's impossible for the leader to add 3 back after observing the shrink to [1, 2]. 

I just want to make sure whether above is correct and whether `AlterIsr` changes it in any way. I think the answer is no as long as ISR expansion is _only_ done in response to a fetch request, but it's worth double-checking."
469406972,9100,hachikuji,2020-08-12T17:00:32Z,I think it's worth adding a comment in the cases we rely on `effectiveInSyncReplicaIds` to explain why.
469412587,9100,hachikuji,2020-08-12T17:10:15Z,"I think the implementation here is actually different than what was in the model. Consider the following case:

1) Initial state: isr=[1, 2], pendingIsr=[1, 2]
2) Leader expands ISR. isr=[1, 2], pendingIsr=[1, 2, 3]
3) Leader shrinks ISR. isr=[1, 2], pendingIsr=[1, 2]

We don't know which of the updates in 2) or 3) will be accepted, but after 3), we will not assume that broker 3 could be in the ISR, which could lead to a correctness violation if the update in 2) is accepted by the controller.

In the model, we always assumed the maximal ISR across _any_ potential update to protect from this edge case. Maybe in the end it is simpler to not allow multiple in-flight updates.
"
469413205,9100,hachikuji,2020-08-12T17:11:21Z,nit: remove parenthesis for simpler getters like `code`. A few more of these
469413455,9100,hachikuji,2020-08-12T17:11:47Z,Missing license header in this file.
469414718,9100,hachikuji,2020-08-12T17:13:57Z,"nit: avoid loaded terminology like ""blackout"" (see https://cwiki.apache.org/confluence/display/KAFKA/KIP-629%3A+Use+racially+neutral+terms+in+our+codebase). Do we actually need this or `IsrChangePropagationInterval` below?"
469415543,9100,hachikuji,2020-08-12T17:15:14Z,We should use `Time`
469417613,9100,hachikuji,2020-08-12T17:18:30Z,Probably need to reduce the log level here and below.
469419452,9100,hachikuji,2020-08-12T17:21:31Z,"I think the basic approach here is to ignore successful responses and wait for the `LeaderAndIsr` update. I am wondering how we should handle the case when the update failed. Say for example that our update fails with the INVALID_VERSION error. Inside `Partition`, we will still have the pendingIsr set. Do we need to clear it? How about other errors?"
469421031,9100,hachikuji,2020-08-12T17:24:17Z,nit: more useful as a debug if we add request details to the message
469421800,9100,hachikuji,2020-08-12T17:25:29Z,The broker epoch is not a constant. It gets reinitialized whenever the broker has to create a new session.
469423834,9100,hachikuji,2020-08-12T17:28:57Z,"The term ""pending"" again is a little unclear. Perhaps ""unsentIsrUpdates"" would make the usage clearer."
469425898,9100,hachikuji,2020-08-12T17:32:22Z,Removal from this set won't prevent `BrokerToControllerRequestThread` from retrying in-flight requests. I'm considering whether we should have a way to cancel requests that we are still awaiting.
469460462,9100,mumrah,2020-08-12T18:33:11Z,"We don't, these were copied over from the ReplicaManager's ISR propagation logic. I'll clean this up"
469467881,9100,mumrah,2020-08-12T18:46:27Z,"I'm currently looking at the effective ISR to find new out of sync replicas. This can include new ISR members which haven't made it into the ""true"" ISR via LeaderAndIsr yet (like broker=3 in your example). Maybe we should only consider removing ISR members iff they are in the true ISR. IOW changing from

```scala
val candidateReplicaIds = effectiveInSyncReplicaIds - localBrokerId
```
to
```scala
val candidateReplicaIds = inSyncReplicaIds - localBrokerId
```

Also, I wonder if the batching that's happening in AlterIsrChannelManager violates the model. It sends the request asynchronously with a small delay, so multiple ISR changes can be batched into one AlterIsr."
469516375,9100,abbccdda,2020-08-12T20:20:11Z,nit: new line
470053173,9100,mumrah,2020-08-13T15:51:39Z,"yea, lots of these will be lowered, was just doing this during development"
470063833,9100,mumrah,2020-08-13T16:08:16Z,"I don't think AlterIsr changes anything since we're now just sending the async ISR update where we were previously directly updating ZK. 

Looking at the usages, `updateFollowerFetchState` is only called following a read (`Partition#readRecords`). These reads only happen on fetch requests and from the alter log dirs fetcher. I'm not sure about the alter log dirs flow, but as long as it sends the leader epoch, it should be safe.

"
470673412,9100,mumrah,2020-08-14T14:51:00Z,Fixed this by adding getBrokerEpoch to KafkaZkClient
470675300,9100,mumrah,2020-08-14T14:54:10Z,"With the latest changes to prevent multiple in-flight requests, I don't think this should happen for a given partition. Even if it did, the retried in-flight request from BrokerToControllerRequestThread would fail on the controller with an old version. 

I'm wondering if we even need this clearPending behavior. Since I changed the AlterIsr request to fire at most after 50ms, it's a narrow window between enqueueing an ISR update and receiving a LeaderAndIsr. "
470678123,9100,mumrah,2020-08-14T14:58:58Z,"Since we are now only allowing one in-flight AlterIsr, I changed the semantics of pendingInSyncReplicaIds to be the maximal ""effective"" ISR. This way we don't need to compute it each time.
"
473515635,9100,abbccdda,2020-08-20T01:36:09Z,"Should we move the startup logic to `KafkaServer`? Note the channel is shared between different modules, so it makes sense to start and close inside the server."
474106591,9100,mumrah,2020-08-20T16:17:31Z,"I found a race during the system tests when a follower is shutting down. The controller handles the shut down before it handles an AlterIsr. If the proposed ISR includes the now-offline replica, the controller refuses to update that ISR change and returns an error for that partition. It then sends out the current LeaderAndIsr.

The problem is that the broker ignores this LeaderAndIsr since it has the same leader epoch. This is easy enough to fix, we can bump the leader epoch in the controller (and ZK) before sending it out.

However, there's still the case of failing to update ZK. I think we should probably treat this the same way as an offline replica. If we simply return an error in AlterIsr response and let the leader reset the pending ISR state, the leader will just retry with stale metadata and the update will fail again. 

I think in all these error cases we must bump the leader epoch to force the leader to accept the new LeaderAndIsr. Thoughts?"
474930329,9100,mumrah,2020-08-21T20:15:17Z,It's a little tricky since LeaderAndIsr isn't visible to AlterIsrRequest. 
474934833,9100,mumrah,2020-08-21T20:20:56Z,"Update: after some discussion and looking over failed system tests, we ended up with the following error handling:

* REPLICA_NOT_AVAILABLE and INVALID_REPLICA_ASSIGNMENT will clear the pending ISR to let the leader retry. This covers a case where a leader tries to add a replica to the ISR which is offline because it (the follower) just finished shutdown.
* INVALID_UPDATE_VERSION will not clear the pending ISR since the broker has stale metadata.
* FENCED_LEADER_EPOCH, NOT_LEADER_OR_FOLLOWER, UNKNOWN_TOPIC_OR_PARTITION will _not_ clear the pending state and therefor will not retry. We presume here that the controller is correct and the leader has old metadata. By not clearing the pending ISR, the leader will await LeaderAndIsr before attempting any further ISR changes
* Other unspecified errors: clear the pending state and let the leader retry. Not sure what cases could cause other errors, but it is probably better to be in a retry loop than to be completely stuck"
474959176,9100,mumrah,2020-08-21T20:50:34Z,Continued in https://github.com/apache/kafka/pull/9100#discussion_r474934833
475625111,9100,mumrah,2020-08-24T13:53:27Z,This error message should be less specific
475626573,9100,mumrah,2020-08-24T13:54:36Z,"Need to revert this stuff, didn't mean to commit"
475627290,9100,mumrah,2020-08-24T13:55:09Z,Rename to alterIsrManager
475629593,9100,mumrah,2020-08-24T13:57:02Z,Expand on this comment to discuss the maximal ISR
475630695,9100,mumrah,2020-08-24T13:57:55Z,Fix comment to refer to correct variable
475632719,9100,mumrah,2020-08-24T13:59:34Z,newline
477616910,9100,hachikuji,2020-08-26T22:08:38Z,We may as well add flexible version support for the request and response.
477618083,9100,hachikuji,2020-08-26T22:11:44Z,"nit: I think `AlterIsrResponseTopics` should be singular (similarly for other arrays in both of these schemas). 

Also, I wonder if it's reasonable to leave off the `AlterIsr` prefix. We could access it as `AlterIsrResponse.TopicData` or something like that."
477618646,9100,hachikuji,2020-08-26T22:13:13Z,nit: unneeded newline
477620548,9100,hachikuji,2020-08-26T22:18:02Z,Do we need this message? It seems the one on line 537 below has more detail already. It would be useful to include the zkVersion in the message on 537 as well.
477625772,9100,hachikuji,2020-08-26T22:32:18Z,"I am wondering if we can split this into two separate methods:
- `effectiveIsr`: takes into account any pending changes which may or may not have happened (I could probably also be convinced to call this `maximalIsr`)
- `confirmedIsr`: the latest known value from Zookeeper (or the Controller)

That makes the code easier to follow since we wouldn't have to interpret this flag. Some high-level comments might be helpful as well. For example, it's useful to mention somewhere that the high watermark is always treated with respect to the effective ISR."
477629764,9100,hachikuji,2020-08-26T22:41:43Z,Can we move this check earlier in the flow so that we can skip acquiring the write lock if there is an inflight AlterIsr? Maybe it can be part of `needsExpandIsr` and `needsShrinkIsr` for example.
477748433,9100,hachikuji,2020-08-27T00:16:03Z,"I have been thinking a little bit about the semantics of min.isr. Basically I am wondering if should be treated as a guarantee on the state of the ISR at the time the high watermark is reached (i.e. how many replicas are in the ISR), or instead should it be a guarantee on the state of progress of replication (i.e. some number of replicas have reached a given offset)? We may not have ever formally decided this, but here we are taking a stance that it is the latter because we are using the effective (uncommitted) ISR.

One of the consequences of this view is that a leader may continue to accept appends satisfying min.isr even if the true ISR never reaches min.isr. For example, imagine we have the following state:

replicas: (1, 2, 3)
isr: (1)
leader: 1

Suppose that replica 2 has caught up to the leader, but the leader is unable to expand the ISR because the controller is unavailable or unreachable. With the logic here, we will nevertheless continue to satisfy acks=all requests with a min.isr of 2. 

I am not sure there is much choice about it to be honest. If instead we used only the ""confirmed"" ISR, then we would have sort of an opposite problem. For example, consider this state:

replicas: (1, 2, 3)
isr: (1, 2)
leader: 1

Suppose the leader wants to remove 2 from the ISR. The AlterIsr is received by the controller and the state is updated, but the controller fails to send the corresponding LeaderAndIsr. Then committing on the basis of the confirmed ISR would lead to a similar problem.

Here is the current documentation for the config:
```
  val MinInSyncReplicasDoc = ""When a producer sets acks to \""all\"" (or \""-1\""), "" +
    ""min.insync.replicas specifies the minimum number of replicas that must acknowledge "" +
    ""a write for the write to be considered successful. If this minimum cannot be met, "" +
    ""then the producer will raise an exception (either NotEnoughReplicas or "" +
    ""NotEnoughReplicasAfterAppend).<br>When used together, min.insync.replicas and acks "" +
    ""allow you to enforce greater durability guarantees. A typical scenario would be to "" +
    ""create a topic with a replication factor of 3, set min.insync.replicas to 2, and "" +
    ""produce with acks of \""all\"". This will ensure that the producer raises an exception "" +
    ""if a majority of replicas do not receive a write.""
```
Even though it is named in terms of the ISR, the documentation only discusses acks from other replicas, so it seems like the implementation here is consistent even if potentially surprising in some cases."
477785759,9100,hachikuji,2020-08-27T00:35:59Z,Maybe trace would be better? This could get verbose while we have an inflight AlterIsr.
477786630,9100,hachikuji,2020-08-27T00:36:26Z,I still think we need a better name for `pendingInSyncReplicaIds` since it is misleading in this case. Maybe we could call it `overrideInSyncReplicaIds` or something like that?
477787350,9100,hachikuji,2020-08-27T00:36:50Z,nit: maybe `sendAlterIsrRequest`?
477811336,9100,hachikuji,2020-08-27T00:49:30Z,"Hmm.. I am not sure it is safe to reset `pendingInSyncReplicaIds` in any case except `INVALID_UPDATE_VERSION`. For example, imagine the following sequence:

1. Broker sends AlterIsr
2. Controller writes new ISR and crashes before sending response
3. Broker hits session expiration 
4. Broker retries AlterIsr on new controller with old broker epoch
5. Controller responds with STALE_BROKER_EPOCH

In this case, the ISR was updated, but the broker is going to revert to the old state. I think the _only_ time we can reset `pendingInSyncReplicaIds` is when we know the change could not have been applied."
477817624,9100,hachikuji,2020-08-27T00:52:47Z,"The conversion logic is a tad annoying, but it makes the rest of the code nicer. I'm ok with it. That said, could we use scala conventions, e.g.:

```scala
    alterIsrRequest.topics.forEach { topicReq => 
      topicReq.partitions.forEach { partitionReq =>
```"
477832161,9100,hachikuji,2020-08-27T01:00:46Z,Don't forget the TODO!
477876231,9100,hachikuji,2020-08-27T01:32:23Z,"Should we try to make `AlterIsr` an idempotent operation? I think currently if we retry an update that was successfully applied, then we will see INVALID_VERSION.

In general, I'm a bit concerned about the number of errors that are possible through this API and how the leader is supposed to handle them. I am thinking it might make our lives easier if we return some additional information in the response about what the current state really is. Let's say that we always try to add the full state tuple to the response: (leaderId, epoch, ISR, zkVersion). Then we can go through a simple process of reconciliation?

- Am I still the leader?
- Do I have the latest epoch?
- Has the zkVersion been bumped?
- Did the ISR change get applied?

Basically I'm looking for a reliable way to determine whether we should continue retrying the request and whether it is safe to clear the pending replica set. At the same time, I'm feeling a bit on-the-fence about relying exclusively on LeaderAndIsr for state changes. If we need to return the current state in the response anyway to properly handle errors, then perhaps we may as well allow the state to be updated as well? This would actually be closer to the flow that we have today, which is the following:

1. Leader changes state in Zookeeper and updates current ISR directly.
2. After some delay, it posts the ISR update to isr_change_notifications.
3. Controller picks up the notification and sends UpdateMetadata to all the brokers.

Notice that the controller does not send LeaderAndIsr to the followers in this flow. What we could do is something more like the following:

1. Leader sends AlterIsr to controller.
2. Controller applies the change and returns the updated state.
3. Leader receives the response and applies the state change.
4. After some delay, the controller sends UpdateMetadata to the brokers with the change.

If we did this, then we wouldn't need to have the controller bump the epoch when handling AlterIsr. Just as we do today, we can reserve epoch bumps for controller-initiated changes.

Then we might be able to simplify the error handling to the following:

- If the epoch is the same and we are still the leader, then apply the update
- If the epoch is higher, leave pendingIsr set and do not bother retrying
- Otherwise just keep retrying

What do you think?
"
477889557,9100,hachikuji,2020-08-27T01:43:00Z,Probably not a useful log message
477892802,9100,hachikuji,2020-08-27T01:45:16Z,Not sure about this. Do we really want to put zk in the path to sending to the controller?
477902972,9100,hachikuji,2020-08-27T01:52:41Z,This should be fixed
477904479,9100,hachikuji,2020-08-27T01:53:43Z,"As far as I can tell, we don't have any logic which tells us whether there is an inflight request. I am considering whether we should as a mechanism for batching/flow control. It might be simpler if we just allow one inflight request. While we are waiting for it to return, we can collect additional pending updates. In case we need to retry the request, we could coalesce the new updates into the request.

Note that currently `BrokerToControllerChannelManagerImpl` currently sets max inflight requests to 1 anyway."
477911757,9100,hachikuji,2020-08-27T01:59:01Z,"We seem to be losing some of the value of having a top-level error code here. As far as I can tell, the following top-level errors should be possible:

1. NOT_CONTROLLER: should be retried (handled in `BrokerToControllerChannelManagerImpl`)
2. STALE_BROKER_EPOCH: should be retried (could we do that here?)
3. CLUSTER_AUTHORIZATION_FAILED: probably should be fatal (can we handle that here?)

Seems like it might simplify the error handling if we can handle them at a corresponding granularity."
477912314,9100,hachikuji,2020-08-27T01:59:22Z,nit: misaligned
477916564,9100,hachikuji,2020-08-27T02:02:29Z,Any particular reason to change the order here?
477918557,9100,hachikuji,2020-08-27T02:03:54Z,We should probably have a try/catch in here somewhere for the unhandled errors to make sure that the callback always gets applied.
477920314,9100,hachikuji,2020-08-27T02:05:17Z,Maybe debug is more suitable?
477921396,9100,hachikuji,2020-08-27T02:06:04Z,Could be debug perhaps?
478067221,9100,mumrah,2020-08-27T03:55:53Z,"I think that sounds pretty reasonable. Would we need any kind of timeout at this layer, or just rely on the underlying channel to provide timeouts?"
478069145,9100,mumrah,2020-08-27T03:57:52Z,"I wasn't too happy about this. Is there another way to get the current broker epoch? As I understand it, the broker epoch can change during the lifecycle of a broker. "
486521810,9100,mumrah,2020-09-10T17:43:05Z,"I think this sounds good, explict over implicit and all that. If we have two methods like this, should we then make `inSyncReplicaIds` a private member? "
486535166,9100,mumrah,2020-09-10T18:05:57Z,"Yea, good idea. I'll leave the check here since we actually acquire and release the lock when checking if we should expand/shrink. It's possible that pendingInSyncReplicaIds is cleared by a LeaderAndIsr before we acquire the write lock to do the update"
486535868,9100,mumrah,2020-09-10T18:07:08Z,How about `uncommittedInSyncReplicaIds`?
486535964,9100,mumrah,2020-09-10T18:07:20Z,"nope, will revert"
489569400,9100,hachikuji,2020-09-16T16:29:34Z,"I know we've gone back and forth on including some of these fields. This is one I'm inclined to get rid of since we already include ""BrokerId"" at the top level and `AlterIsr` can only be sent by leaders."
489571330,9100,hachikuji,2020-09-16T16:32:40Z,nit: shall we call this `LeaderId` in line with `BrokerId` in the request?
489572418,9100,hachikuji,2020-09-16T16:34:32Z,"Can we revert this change? I think the trace logging is intended, if a bit odd."
489572745,9100,hachikuji,2020-09-16T16:35:06Z,We probably need another version since we bumped the Fetch protocol version yesterday.
489577394,9100,hachikuji,2020-09-16T16:42:48Z,"nit: we use ""maximal"" and ""effective"" interchangeably in this PR. Maybe we can choose one term and use it consistently. I do sort of like ""maximal"" since it is more suggestive of the semantics."
489579590,9100,hachikuji,2020-09-16T16:46:31Z,nit: maybe `hasInFlightAlterIsr` so that it's clearer what the return value indicates?
489580384,9100,hachikuji,2020-09-16T16:47:54Z,nit: maybe we could rename `curInSyncReplicaIds` to `curEffectiveIsr` 
489582879,9100,hachikuji,2020-09-16T16:52:06Z,nit: probably need to reword mention of `LeaderAndIsr` since the `AlterIsr` response is now used.
489593301,9100,hachikuji,2020-09-16T17:09:49Z,nit: maybe we can check in-flight requests first (same in `needsExpandIsr`). Otherwise it's a little odd that `getOutOfSyncReplicas` may be based on the maximal ISR while we have an in-flight.
489593650,9100,hachikuji,2020-09-16T17:10:29Z,nit: redundant comment
489594684,9100,hachikuji,2020-09-16T17:12:13Z,nit: you can take the topic partition out of this message since it is already included in `logIdent`. Same on line 1262 below.
489604587,9100,hachikuji,2020-09-16T17:29:38Z,"The problem is that it is a sort of worst-case ISR and not the intended ISR update itself. Tough to come up with a good name to describe that. Just for the sake of having an alternative, what if we used case classes to represent the current ISR state and pending update? For example:

```scala
sealed trait IsrStatus {
  def isr: Set[Int]
  def maximalIsr: Set[Int]
}
case class PendingExpand(isr: Set[Int], newInSyncReplicaId: Int) extends IsrStatus {
  val maximalIsr = isr + newInSyncReplicaId
}
case class PendingShrink(isr: Set[Int], outOfSync: Set[Int]) extends IsrStatus  {
  val maximalIsr = isr
}
case class Stable(isr: Set[Int]) extends IsrStatus {
  val maximalIsr = isr
}
```

Then we can get rid of `effectiveIsr`, `inSyncReplicaIds`, and `pendingInSyncReplicaIds`."
489606127,9100,hachikuji,2020-09-16T17:32:22Z,"nit: ""... doesn't know about this **topic** or partition""?"
489607165,9100,hachikuji,2020-09-16T17:34:25Z,"Hmm... Why do we reset `pendingInSyncReplicaIds` if we are retrying? Unless we are guaranteed that the update failed, then I think we need to continue assuming the worst-case ISR. Maybe we could just could call `enqueueIsrUpdate` again to explicitly retry?"
489608511,9100,hachikuji,2020-09-16T17:36:44Z,"nit: We don't need topic partition here, but it would be nice if we could include the intended update."
489608740,9100,hachikuji,2020-09-16T17:37:10Z,"nit: as long as we're updating this, can we use `$` substitutions?  Also can we mention that this update came from `AlterIsr`?"
489609866,9100,hachikuji,2020-09-16T17:39:09Z,"Maybe helpful if these messages indicate that this `leaderAndIsr` can from an `AlterIsr` response. Also, it may be useful to include the current (stale) leader epoch."
489610509,9100,hachikuji,2020-09-16T17:40:19Z,"nit: similarly, we can include current `zkVersion`"
489612385,9100,hachikuji,2020-09-16T17:43:38Z,nit: maybe we could shorten this name to just `enqueue` since the fact that it is an ISR update is already implied by the argument and the name of the trait itself.
489614105,9100,hachikuji,2020-09-16T17:46:39Z,nit: usually we write this as `forEach { topic =>`. Avoids the extra parenthesis.
489614732,9100,hachikuji,2020-09-16T17:47:43Z,nit: maybe split this into two separate methods?
489617709,9100,hachikuji,2020-09-16T17:52:57Z,The use of a queue is a tad odd here. We could use `ListBuffer`? Also nit: use type inference.
489619376,9100,hachikuji,2020-09-16T17:55:53Z,Still not super keen on this propagation delay. At least it would be nice if we did not have to wakeup the thread every 50ms when there's nothing to do. This is potentially something we can save for a follow-up since coming up with a good solution might require some experimentation and analysis.
489619767,9100,hachikuji,2020-09-16T17:56:37Z,nit: use type inference
489621121,9100,hachikuji,2020-09-16T17:59:00Z,nit: can we include the broker epoch that was sent in this message?
489621168,9100,hachikuji,2020-09-16T17:59:06Z,"Hmm.. Where does this exception get caught? Since it is in the response handler, I guess that `NetworkClient` just eats it. Perhaps we should just continue retrying so that the problem remains visible in the logs."
489621815,9100,hachikuji,2020-09-16T18:00:08Z,nit: this message would be more useful if we include the response. Perhaps it would be better to log each partition update separately?
489622001,9100,hachikuji,2020-09-16T18:00:29Z,nit: unneeded parenthesis
489623549,9100,hachikuji,2020-09-16T18:03:18Z,Maybe we could log a warning and let the partition remain in `unsentIsrUpdates` so that it is retried until we get a response?
489623929,9100,hachikuji,2020-09-16T18:04:01Z,nit: may as well convert to `Errors` since we do so below anyway
489624854,9100,hachikuji,2020-09-16T18:05:34Z,I think authorization should probably be the first thing we do.
489625184,9100,hachikuji,2020-09-16T18:06:09Z,nit: remove these lines
489625611,9100,hachikuji,2020-09-16T18:07:02Z,Do we still need this change? I think we are trying to keep the current approach where the controller bumps the leader epoch for any controller-initiated change.
489625836,9100,hachikuji,2020-09-16T18:07:25Z,I guess we don't need this anymore.
489703810,9100,mumrah,2020-09-16T19:27:49Z,"I think this has been a long-standing bad assumption on my part in this PR. I've been (mis)treating `pendingInSyncReplicaIds` as a mechanism for initiating a retry along with its other semantics. You're right though, explicitly re-sending the ISR is definitely better."
489720971,9100,mumrah,2020-09-16T20:00:38Z,"Yup, my mistake, shouldn't have been committed"
489721316,9100,mumrah,2020-09-16T20:01:18Z,"""maximal"" works for me  "
489730265,9100,mumrah,2020-09-16T20:19:09Z,"ah, missed one ;) "
489740479,9100,mumrah,2020-09-16T20:39:05Z,"Currently we impose a 2.5s delay for the old ZK based ISR propagation method. We could probably increase this 50ms up to a few hundred without any ill-effects. We still benefit from fact that we assume the maximal ISR immediately. How about 200ms?

Longer term we can look into having a single thread invocation that sits in a while loop trying to consume from a LinkedBlockingQueue or maybe even a SynchronousQueue. But agreed we should leave this for later."
489744561,9100,mumrah,2020-09-16T20:47:19Z, How about we raise this to an error log with the exception?
489745539,9100,mumrah,2020-09-16T20:49:17Z,Should we drop it to trace in that case?
489747101,9100,mumrah,2020-09-16T20:52:28Z,Good idea. Another case not covered is if partitions are included in the response but weren't sent out. These will be ignored as things currently stand -- maybe that's ok
489753946,9100,mumrah,2020-09-16T21:05:59Z,This is actually a really good point. I filed a JIRA to fix this in other places in KafkaApis https://issues.apache.org/jira/browse/KAFKA-10491
489756083,9100,mumrah,2020-09-16T21:10:09Z,"No, we don't need this anymore. This was added so a LeaderAndIsr could update the Partition state without a leader epoch bump, but we don't have that flow anymore so we can revert this."
490464756,9100,hachikuji,2020-09-17T18:20:10Z,nit: `hasInflight`?
490465180,9100,hachikuji,2020-09-17T18:20:54Z,nit: It's surprising to have a side effect like this in a function like this. I think it would be better to include this logging at the caller when we are considering a specific change. That way we can also include in the log message information about the change that we were intending to make.
490469252,9100,hachikuji,2020-09-17T18:28:13Z,"Another possibility is that the replica is pending removal in which case another `AlterIsr` will be needed. I think it might be more intuitive to make this check:

```scala
if (!isrState.inflight && !isrState.isr.contains(followerId))
```"
490488626,9100,hachikuji,2020-09-17T19:03:35Z,"I think we can refactor this a little bit to avoid some duplication and inconsistency. We have the following logic above when updating follower state:
```scala
        if (!isrState.maximalIsr.contains(followerId))
          maybeExpandIsr(followerReplica, followerFetchTimeMs)
```
This is a little inconsistent because here we are checking `isrState.isr`. I'd suggest splitting this method into something like the following:

```scala
def hasReachedHighWatermark(follower: Replica): Boolean = {
  leaderLogIfLocal.exists { leaderLog =>
    val leaderHighwatermark = leaderLog.highWatermark
    isFollowerInSync(follower, leaderHighwatermark)
  }
}

def canAddToIsr(followerId: Int): Boolean = {
  val current = isrState
  !current.inflight && !current.isr.contains(followerId)
}

def needsExpandIsr(follower: Replica): Boolean = {
  canAddToIsr(follower.brokerId) && hasReachedHighWatermark(follower)
}
```

Then we can change the logic in `maybeExpandIsr` to the following:
```scala
    val needsIsrUpdate = canAddToIsr(followerReplica) && inReadLock(leaderIsrUpdateLock) {
...
```

"
490492340,9100,hachikuji,2020-09-17T19:10:10Z,Seems like we do not have a check for inflight AlterIsr after the write lock has been acquired.
490492780,9100,hachikuji,2020-09-17T19:10:50Z,"This is related to my comment above for the ISR expansion case, but it is a bit confusing to use maximal ISR when the expectation is that we will not shrink as long as we have a pending update inflight. Would it be better to check for inflights and document that this method will return an empty set as long as there is a pending AlterIsr request?"
490494777,9100,hachikuji,2020-09-17T19:14:38Z,"It might be a little more intuitive to change the order here. Something like this:
```scala
      val upatedIsrState = PendingShrinkIsr(isrState.isr, outOfSyncReplicas)
      if (sendAlterIsrRequest(updatedIsrState)) {
        isrState = updatedIsrState
...
```
 "
490494975,9100,hachikuji,2020-09-17T19:14:57Z,"nit: can probably rework this as `exists`
```scala
isrToSendOpt.exists { isrToSend =>
...
}
```"
490527627,9100,hachikuji,2020-09-17T20:00:06Z,"Since `INVALID_UPDATE_VERSION` is one of the expected errors at this level, can we add a separate case for it? For unexpected errors, we might want to log at warn level."
490528339,9100,hachikuji,2020-09-17T20:01:27Z,Shall we include some details about the failed request?
490528744,9100,hachikuji,2020-09-17T20:02:13Z,nit: rewrite with `$`
490529253,9100,hachikuji,2020-09-17T20:03:19Z,I think `warn` might be too high here. We should expect to see some of these even if the cluster is working properly. How about debug?
490530496,9100,hachikuji,2020-09-17T20:05:35Z,nit: can we avoid using `_1` and `_2`? It's a lot easier to follow if they are named.
490531814,9100,hachikuji,2020-09-17T20:08:12Z,"nit: use type inference. It's conventional to write this as 
```scala
val partitionResponses = mutable.Map[TopicPartition, Either[Errors, LeaderAndIsr]].empty()
```"
490532481,9100,hachikuji,2020-09-17T20:09:31Z,nit: I think we can get rid of this. The logging in `ControllerChannelManager.sendUpdateMetadataRequests` is probably good enough.
490534060,9100,hachikuji,2020-09-17T20:12:36Z,"nit: it's subjective, so feel free to ignore, but I find this a little easier to read if we handle the error cases first. So..
```scala
if (!authorize(request.context, CLUSTER_ACTION, CLUSTER, CLUSTER_NAME)) {
      sendResponseMaybeThrottle(request, requestThrottleMs =>
        alterIsrRequest.getErrorResponse(requestThrottleMs, Errors.CLUSTER_AUTHORIZATION_FAILED.exception))Errors.CLUSTER_AUTHORIZATION_FAILED.exception))  
} else if (!controller.isActive) {
     sendResponseMaybeThrottle(request, requestThrottleMs =>
        alterIsrRequest.getErrorResponse(requestThrottleMs, Errors.NOT_CONTROLLER.exception()))
} else {
...
}
```
Basically we're discarding the error cases so that the successful path continues flowing downward and we're avoiding extra nesting. Like I said, it's subjective."
490534274,9100,hachikuji,2020-09-17T20:13:00Z,nit: unneeded newline
490534954,9100,hachikuji,2020-09-17T20:14:26Z,nit: not sure it makes sense to include this change any longer
490536839,9100,hachikuji,2020-09-17T20:17:56Z,"I wonder if we should be exposing this. Would it be enough to have a `def inSyncReplicaIds = isrState.isr`? One thing we need to be a little careful of is the fact that we now have a volatile variable with multiple fields. So if you try to access two fields through the `isrState` reference, you could see inconsistent data."
490539522,9100,hachikuji,2020-09-17T20:23:10Z,Need to address the TODOs in this class.
490540789,9100,hachikuji,2020-09-17T20:25:23Z,"I may have missed it, but do we have tests which verify error handling? I see tests which verify requests get sent, but at a quick glance I didn't see tests of responses."
490541557,9100,hachikuji,2020-09-17T20:26:45Z,nit: sort of conventional to use a name like `MockAlterIsrManager`
490542749,9100,mumrah,2020-09-17T20:29:02Z,Yea i was thinking we should move the ISR to a separate public accessor. I'll change this
490561028,9100,mumrah,2020-09-17T21:04:05Z,"Makes sense, that will also satisfy your other comment about not checking for inflight requests within the write lock"
490562630,9100,mumrah,2020-09-17T21:07:24Z,"Also, yes it's confusing to refer to `maximalIsr` here even though it should always equal the committed ISR at this point (assuming we check for inflight first). "
490594335,9100,mumrah,2020-09-17T22:21:46Z,Yea checking the maximal set isn't needed anymore since adding the sealed trait. I'll just update this to simply call `maybeExpandIsr` which will do the check you propose here
494480847,9100,hachikuji,2020-09-24T17:13:35Z,"Consider the following scenario:

1) broker sends AlterIsr
2) the update succeeds but the response is lost
3) broker retries AlterIsr

Currently the leader will be stuck after 3) because it has no way to get the latest LeaderAndIsr state if the first attempt fails. To handle this, I think we need to add an idempotence check here. After we have validated the leader epoch, if the intended state matches the current state, then we can just return the current state. "
494483808,9100,hachikuji,2020-09-24T17:18:40Z,"It might make more sense to handle this case similarly to FENCED_LEADER_EPOCH. Retrying won't help since we know our version will be rejected Come to think of it, this would be kind of a strange error to hit in the current implementation which only allows one request inflight at a time. For controller-initiated changes, we'd expect to hit FENCED_LEADER_EPOCH. Anyway, I think it's still worth keeping the error."
494486578,9100,hachikuji,2020-09-24T17:23:03Z,"Is there any way that we could end up retrying after the pending ISR state had already been reset? I know we have `AlterIsrManager.clearPending`, but that only removes the partition from the unsent queue. How do we handle inflight `AlterIsr` requests after the state has been reset. Seems like it might be worth adding a check here to validate whether the request is still needed."
494520245,9100,mumrah,2020-09-24T18:21:33Z,"I was trying to think some kind of race with a zombie leader trying to update the ISR, however this would get fenced by the leader epoch. This should be pretty easy to add"
494523773,9100,mumrah,2020-09-24T18:26:13Z,"True, we could see a new ISR from controller initiated changes via LeaderAndIsr while our request is in-flight. We have a check for this on successful responses, but we should also check here. Since our request failed, we don't have a leaderEpoch to check against so I think the best we can do is see if `isrState` is still pending before re-sending the request"
494660084,9100,hachikuji,2020-09-24T23:13:45Z,"nit (for follow-up): fix grammar ""since due"""
494660983,9100,hachikuji,2020-09-24T23:16:38Z,"nit: conventionally we prefer ""retriable"""
494661344,9100,hachikuji,2020-09-24T23:17:43Z,It might be worth mentioning that this could happen in the case of a retry after a successful update.
494661583,9100,hachikuji,2020-09-24T23:18:19Z,nit: leave off parenthesis after `exception`
99288458,2472,norwood,2017-02-03T07:44:06Z,should probably make this `<? extends NewTopic>`
99288467,2472,norwood,2017-02-03T07:44:07Z,if you are going to do this check then why not make `newTopics` a `Set`? if we'd rather do this check here then we can use `Collection` instead
99288478,2472,norwood,2017-02-03T07:44:16Z,should include a `listTopics` that takes a `Collection<String>` of topics to query
99288485,2472,norwood,2017-02-03T07:44:20Z,is it possible that this masks errors in `partitionMetadata`? e.g. if we have a topic level error and an error on a specific partition.
99406097,2472,cmccabe,2017-02-03T19:30:13Z,agree
99406308,2472,cmccabe,2017-02-03T19:31:20Z,I didn't make it a set because I didn't want to have to worry about hashCode and equals.  I think you're right; it should just be a Collection.
99406744,2472,cmccabe,2017-02-03T19:33:33Z,"Hmm, I'm not sure I follow.  All the errors in the RPC response are faithfully reproduced in the return values... what else can we do to improve it?"
99407725,2472,norwood,2017-02-03T19:38:50Z,"you have a `continue` in this block. my question is related to if you end up with a `TopicMetadata` that looks like:
```
{
  topic: 'mytopic',
  error: Error.SOMETHING_BAD,
  partitionMetadata: [
    {
      partition: 0
      error: SOMETHING_ELSE_THAT_IS_BAD
    } 
  ]
}
```

also, in writing this up, i realize that in line 487 you are checking `topic.error()` instead of `partition.error()`"
99410119,2472,cmccabe,2017-02-03T19:51:06Z,I added a function which allows you to get information for one topic
99687358,2472,norwood,2017-02-06T21:23:13Z,"i still really think that we should have a `listTopics(Set<String> topics, EnumSet<ListTopicsFlags> flags)`, e.g. one thing i've had to do is write code that is basically waits until i have seen our topics replicated/etc. it would be nice to not have to get all if i dont have to, and also nice to batch for me :)

sample code

```
Set<String> topicsIWant = {..important topics};
checkErrors(createTopics(topicsIWant))
while(topicsIWant.size() > 0) {
  for (Try<TopicInfo> ti : listTopics(topicsIWant)) {
    if (!ti.isFailure) topicsIWant.remove(ti.get().name());
  }
}
```"
112326031,2472,ijuma,2017-04-19T22:10:04Z,"This package doesn't currently contain public API classes. If we want `clients` to be a public package, we need to move all the classes that are there at the moment to `clients.internals`."
112326124,2472,ijuma,2017-04-19T22:10:41Z,"As per our conversation today, maybe we should say that this will implement `CompletionStage` once we require Java 8. Also, I think we should just expose an interface to users and have the implementation under an internal package. We can then only expose methods that users should use (`complete` and `completeExceptionally` should never be used by users, for example)."
112326754,2472,ijuma,2017-04-19T22:14:38Z,We need to complete this.
112326863,2472,ijuma,2017-04-19T22:15:23Z,Is it intentional that we mention `KafkaAdminClient` here? The idea behind a `create` method like this is to avoid mentioning an implementation.
112327164,2472,ijuma,2017-04-19T22:17:26Z,We should flesh out the documentation for these methods. Important details about what success means should be mentioned (in the same way we did for `deleteTopics`).
112327310,2472,ijuma,2017-04-19T22:18:30Z,I think this is unused.
112327572,2472,ijuma,2017-04-19T22:20:22Z,This default doesn't seem to make sense for the `AdminClient`.
112327616,2472,ijuma,2017-04-19T22:20:39Z,I wonder if we should be sharing these values in `CommonClientConfigs`.
112327840,2472,ijuma,2017-04-19T22:22:20Z,Seems like we sometimes have a constant field for `DOC` and sometimes we don't. We should choose a pattern and stick to it.
112328054,2472,ijuma,2017-04-19T22:23:49Z,I wonder if we should be using such magic values for public classes. It that using `null` to indicate an unset value is a bit safer and there's some indication by the type system.
112328352,2472,ijuma,2017-04-19T22:24:42Z,We can use a Rule to avoid repeating this in every test? Same for other tests.
112328439,2472,ijuma,2017-04-19T22:25:29Z,We typically use `*IntegrationTest` to indicate integration tests and simply `*Test` for unit tests.
112328490,2472,ijuma,2017-04-19T22:25:50Z,Nit: this method seems unnecessary.
112328573,2472,ijuma,2017-04-19T22:26:23Z,We can use `TestUtils.waitUntilTrue` to simplify this a little?
112328632,2472,ijuma,2017-04-19T22:26:50Z,This is not a JUnit assert method.
112328873,2472,ijuma,2017-04-19T22:28:16Z,I was thinking we should put this class in an `internals` package.
112329026,2472,ijuma,2017-04-19T22:29:20Z,This block formatting is a bit weird and we don't usually use it in Kafka.
112329873,2472,ijuma,2017-04-19T22:35:14Z,We close the selector in `NetworkClient` and `ChannelBuilder` in the `Selector`. Do we really need to close them separately?
112336265,2472,cmccabe,2017-04-19T23:22:51Z,"Yeah, implementing `CompletionStage` is probably a good idea, once we have jdk8 support.

I don't see what's so bad about users calling `complete` or `completeExceptionally`.  It just means that if the `KafkaAdminClient` attempts to call those methods, the future will already be completed and it will have no effect.  That seems pretty harmless?"
112336354,2472,cmccabe,2017-04-19T23:23:32Z,"Oops, let me replace that with something descriptive."
112336940,2472,cmccabe,2017-04-19T23:28:12Z,"OK, I will remove the reference to `KafkaAdminClient`."
112337328,2472,cmccabe,2017-04-19T23:31:36Z,It's used by the Metadata object.  Although the rate-limiting is not implemented by the AdminClient for normal requests yet.
112337741,2472,cmccabe,2017-04-19T23:35:00Z,"OK.  I suppose I'll just have the constant, then, even if it just maps back to `CommonClientConfigs`."
112338241,2472,ijuma,2017-04-19T23:38:55Z,"Those methods are racy and that's why they are not present in CompletionStage or Scala Futures. Is there a use case where it's useful for users to call these methods? 

The scenario you outlined does not seem harmless since it seems like a clear user error to cause the AdminClient values to be ignored."
112338335,2472,cmccabe,2017-04-19T23:39:46Z,"I agree that the text doesn't make sense, since we don't care about `max.poll.ms` here.  I'll set 2 minutes (a high default is good here to avoid test failures and so forth.)"
112338407,2472,ijuma,2017-04-19T23:40:18Z,I'm referring to the DOC field...
112338643,2472,cmccabe,2017-04-19T23:41:58Z,"I'm not sure there's a huge amount of utility in sharing this particular config value with the producer and consumer-- it is a different application, after all.  We should probably shorten the idle time a bit"
112339193,2472,cmccabe,2017-04-19T23:47:10Z,Fair enough.  I'll change it to use Integer.
112340310,2472,cmccabe,2017-04-19T23:57:41Z,"That seems like a lot of churn.  Does it make sense to do it in a separate JIRA, either beforehand or after this one?"
112340400,2472,cmccabe,2017-04-19T23:58:33Z,"Hmm, you mean the JavaDoc indentation?"
112340956,2472,cmccabe,2017-04-20T00:03:32Z,"I didn't realize that NetworkClient closed the Selector... but I looked at the code again, and indeed it does.  So we don't need to worry about that in `KafkaAdminClient#close`.  However, we do need to handle the case where the various constructors throw exceptions (pretty much all of them can.)  For example, the Selector constructor could fail and leave our ChannelBuilder dangling, and etc."
112358759,2472,cmccabe,2017-04-20T03:26:35Z,I removed the tracking of the Selector in KafkaAdminClient.  I kept the close in the factory function so that error handling is bulletproof...
112358902,2472,cmccabe,2017-04-20T03:28:22Z,"Hmm, RETRY_BACKOFF_MS_DOC is used later down... unless I'm looking at the wrong thing..."
112359050,2472,cmccabe,2017-04-20T03:30:11Z,"So, the unfortunate thing about moving this into a separate namespace is that it means a bunch of other methods need to be public. The API surface and potential for API breaks increases a lot because stuff like constructors for all the public helper classes KafkaAdminClient uses needs to be public.  Java doesn't yet have any concept of package-private like Scala does.  I wonder if it might be better to just put an annotation or comment saying that KafkaAdminClient is public on the class."
112362257,2472,cmccabe,2017-04-20T04:10:56Z,"One use-case I can think of is cancelling a future.  In `CompletableFuture` this is essentially equivalent to a call to `CompletableFuture#completeExceptionally(new CancellationException(...))`  Cancellation isn't an optional operation for us since it's part of Java's original `java.util.concurrent.Future` API, which we need to implement.

I agree that it's a bit nicer conceptually to separate the ""listen for stuff happening"" API from the ""signal that stuff has happened"" API.  It makes sense that Scala did this with `scala.concurrent.Future`.  Java's `CompletableFuture` muddies the waters a bit by combining them into one concrete class.  I don't know if this is something that is really likely to trip users up, though..."
112444553,2472,ijuma,2017-04-20T12:54:01Z,"Yes, we should do the move separately if we think making `clients` public is the way forward (instead of having a separate package for shared classes). This class doesn't seem specific to `clients` though so maybe we should move it to `common`?"
112444774,2472,ijuma,2017-04-20T12:55:05Z,"`cancel` is exposed via the `Future` interface, so it doesn't seem like we need to expose `completeExceptionally` even if we use it internally for that?"
112445364,2472,ijuma,2017-04-20T12:57:48Z,I wonder if there's a better name for this than `KafkaFuture`. Can't think of anything that's not already taken. But maybe you have some ideas. :)
112445599,2472,ijuma,2017-04-20T12:59:03Z,We should probably make these interfaces and move them to top level in their own package. Maybe `clients.function` or `common.function` (Java uses `java.util.functions`).
112446037,2472,ijuma,2017-04-20T13:01:08Z,"Nit: make this final. Also, it seems like not all tests have been updated to use Rule."
112446276,2472,ijuma,2017-04-20T13:02:18Z,"If we want `vals` to always have two values, then we should just make the method take two parameters?"
112446649,2472,ijuma,2017-04-20T13:04:07Z,"Hmm, not sure about this. Can we not avoid using reflection for this?"
112447470,2472,ijuma,2017-04-20T13:07:59Z,"I think that was changed in a subsequent commit. Before the define was doing `CommonClientConfigs.RETRY_BACKOFF_MS_DOC`. On that point, it makes it harder to review if history is rewritten (i.e. additional changes are squashed into the existing commit). I have no way to tell what changed. :) Can you please just add separate commits instead?"
112449184,2472,ijuma,2017-04-20T13:15:17Z,Yes. There is huge amounts of whitespace. I suspect it's that way because there's an attempt at having alignment across all methods. But the value of that doesn't seem worth it.
112449330,2472,ijuma,2017-04-20T13:15:50Z,Nit: remove `get` from this and other similar methods.
112449528,2472,ijuma,2017-04-20T13:16:44Z,"Why not use `Utils.closeQuietly`? We can change the signature to take an `AutoCloseable` instead of `Closeable`, if that's the issue."
112449930,2472,ijuma,2017-04-20T13:18:35Z,"Hmm, we should probably have a config for metric reporters and add the JmxReporter by default like other clients."
112450736,2472,ijuma,2017-04-20T13:22:13Z,Should this be `admin-client`? Or is the idea that the prefix should be whatever comes before the first `-`?
112451618,2472,ijuma,2017-04-20T13:24:26Z,"We should probably use `KafkaThread` here. Naming convention should be consistent with other client threads. For the producer we do:

```java
            String ioThreadName = ""kafka-producer-network-thread"" + (clientId.length() > 0 ? "" | "" + clientId : """");
            this.ioThread = new KafkaThread(ioThreadName, this.sender, true);
```"
112452024,2472,ijuma,2017-04-20T13:26:06Z,"Generally, it's good to have consistent terminology to avoid confusion. The Kafka code typically just says `Requests` instead of `Rpcs`."
112453789,2472,ijuma,2017-04-20T13:33:24Z,"Starting a new comment thread about the closing we do here since GitHub is not allowing me to add a comment to the existing one. Since this is an issue for all clients, maybe we should ensure that the constructors close things in case of exceptions in the constructor?"
112491693,2472,cmccabe,2017-04-20T15:57:23Z,"Yeah, I'll just add separate commits"
112514535,2472,cmccabe,2017-04-20T17:36:00Z,I moved it to `org.apache.kafka.common`.
112514652,2472,cmccabe,2017-04-20T17:36:22Z,"OK, I have split them.  We'll see how this looks."
112516162,2472,cmccabe,2017-04-20T17:41:01Z,"Hmm, I'm not sure if there's anything better.  I think `KafkaFuture` is short and clearly expresses the concept that it's our `Future` class."
112516461,2472,cmccabe,2017-04-20T17:42:30Z,"Well, they are abstract classes so that we can add more methods without breaking compatibility in Java 7.  Remember that user code has to implement these.

As to making them top-level... I'm not sure.  They are stopgap classes until we can use the real classes from Java 8..."
112517345,2472,cmccabe,2017-04-20T17:46:35Z,"It's not that it always has two parameters, it is that it always has an even number of parameters.  It's used to create a map from key1, value1, key2, value2, key3, value3, ..."
112520218,2472,cmccabe,2017-04-20T17:58:35Z,"I don't feel strongly about it either way.  It gets used in Selector.java to build the metric group name as follows: `String metricGrpName = metricGrpPrefix + ""-metrics"";`

I'm not aware of any convention that says we can't have a hyphen here, so I guess I'll put it in..."
112523477,2472,cmccabe,2017-04-20T18:12:15Z,"OK, I'll reduce the indentation where I can."
112527847,2472,cmccabe,2017-04-20T18:29:54Z,"The constructor is private and can't be called from outside this class.  Even the create methods are package-private and only used by the public interface in AdminClient and by unit tests.  Putting a lot of logic in the constructor doesn't work very well because the logic needs to be different for the unit tests (which don't use NetworkClient, Selector, or ChannelBuilder, for example)"
112655250,2472,ijuma,2017-04-21T10:02:07Z,Seems better to update the `import-control.xml` file.
112972103,2472,ijuma,2017-04-24T15:07:55Z,"Will we eventually retry the request, return an error to the user or something else?"
113934868,2472,ijuma,2017-04-28T13:52:13Z,"We should add a class comment stating that this tests the internal Scala AdminClient that will be replaced
by the Java one eventually."
113935275,2472,ijuma,2017-04-28T13:53:58Z,This should probably use `messageWithFallback`.
113943073,2472,ijuma,2017-04-28T14:29:05Z,Nit: `public` not needed.
113943764,2472,ijuma,2017-04-28T14:31:56Z,"Hmm, should we not just throw an error here? Something would have to be very wrong for this to happen."
113944479,2472,ijuma,2017-04-28T14:35:12Z,This seems more informative than what's in the abstract class.
113944854,2472,ijuma,2017-04-28T14:36:56Z,Nit: `!isEmpty` instead of `length() > 0`.
113945142,2472,ijuma,2017-04-28T14:38:14Z,This comment doesn't seem accurate.
113948024,2472,ijuma,2017-04-28T14:49:27Z,`CommonClientConfigs.METRICS_NUM_SAMPLES_DOC` should just be `METRICS_NUM_SAMPLES_DOC`.
113948175,2472,ijuma,2017-04-28T14:50:05Z,We should either do this for all of the configs or none.
113948212,2472,ijuma,2017-04-28T14:50:15Z,The DOC configs should be private.
113948629,2472,ijuma,2017-04-28T14:52:01Z,Seems like we're missing `METRICS_RECORDING_LEVEL_CONFIG`.
113950116,2472,ijuma,2017-04-28T14:58:27Z,The consumer default is 64k while the producer default is 32k. Was it intentional that you picked the same default as the producer?
113950557,2472,ijuma,2017-04-28T15:00:27Z,This comment needs to be updated since you reduced the MAX_IDLE value to 5 minutes.
113965338,2472,ijuma,2017-04-28T16:08:22Z,Do we need something like the producer's `MAX_REQUEST_SIZE_CONFIG` or are we OK to just rely on the broker if someone creates a batch that is too large somehow?
113967006,2472,ijuma,2017-04-28T16:16:37Z,Nit: maybe this should simply be `testClose()`?
113967166,2472,ijuma,2017-04-28T16:17:33Z,Seems like we always create an AdminClient and close it at the end. Maybe we can do that in `setUp` and `tearDown`? That way we don't leak resources if the test fails.
113967578,2472,ijuma,2017-04-28T16:19:43Z,"It would be good to run these tests with security enabled. Maybe a variant with SASL_SSL would do the job. It's reasonably straightforward, see `SaslSslConsumerTest`."
113970289,2472,ijuma,2017-04-28T16:34:24Z,"Oh, the result type of `createTopic` and `createTopics` is the same? That makes the non batch method less useful, no? May as well let the user use `Collections.singleton` (with a static import)`."
114015243,2472,cmccabe,2017-04-28T20:39:23Z,"Since `KafkaFuture#completedFuture` returns a completed future, we need access to the `KafkaFutureImpl#complete` method.  Normally, `org.apache.kafka.common.KafkaFuture` accessing `org.apache.kafka.common.internal.KafkaFuture` would be blocked.  This exception allows it to be called."
114015511,2472,cmccabe,2017-04-28T20:41:00Z,"Hmm.  I was following the example of `Producer.java`, whose JavaDoc has a link to `KafkaProducer.java`.  I will add a little more JavaDoc to the `AdminClient.java` class, though."
114016199,2472,cmccabe,2017-04-28T20:45:37Z,"We don't know what request the response corresponds to, because its correlation ID is invalid.  Of course, requests that don't receive a response will time out after enough time."
114016438,2472,cmccabe,2017-04-28T20:47:06Z,"It's kind of awkward because other futures might have been completed successfully at this point.  Assuming that there is a bug where the server is responding with an incorrect topic name, this will be caught by means of the sanity check at the bottom that fails futures which haven't been completed by the response."
114022797,2472,cmccabe,2017-04-28T21:26:19Z,"I didn't think about it that much.  I suppose 64k might be a better default, though, since some messages have large responses (larger than those the producer normally gets, at least)."
114023074,2472,cmccabe,2017-04-28T21:28:06Z,I don't think that we need a config like that here.  The size of messages should come naturally out of the batch size which the client chooses.  The RPC system will catch it if it gets too big (although there are probably other performance problems when admin requests get that big).
114024595,2472,cmccabe,2017-04-28T21:38:18Z,Good idea.  I will add a close to an @After cleanup function.
114024764,2472,cmccabe,2017-04-28T21:39:29Z,Let's revisit this later once we have more experience with use-cases
114024818,2472,cmccabe,2017-04-28T21:39:54Z,I updated this to 64k.
114025089,2472,cmccabe,2017-04-28T21:41:54Z,Good idea.  I added some JavaDoc here and also on the Scala AdminClient with that info
114025911,2472,cmccabe,2017-04-28T21:47:28Z,"Hmm, good catch.  I think it makes more sense to add the fallback logic into `Errors#exception`, though.  If null is passed to that method, it should return an exception with the default message for that error code."
114026050,2472,cmccabe,2017-04-28T21:48:31Z,"Note: I did add an entry to the `import-control.xml` for the `org.apache.kafka.clients.admin` namespace, which was previously missing.  That entry is useful for allowing imports between classes in the same package.  But it doesn't affect this issue in `org.apache.kafka.common`."
114029206,2472,ijuma,2017-04-28T22:13:01Z,"When is later though, the feature freeze is pretty soon. As you know, in API design, adding methods is easy, removing them is hard. Our experience so far in the consumer is that the batch version is enough so it seems to make sense to start that way. Also, are we adding the Unstable annotation to this class?"
114029207,2472,ijuma,2017-04-28T22:13:02Z,"When is later though, the feature freeze is pretty soon. As you know, in API design, adding methods is easy, removing them is hard. Our experience so far in the consumer is that the batch version is enough so it seems to make sense to start that way. Also, are we adding the Unstable annotation to this class?"
114037965,2472,cmccabe,2017-04-28T23:48:49Z,"I suppose it is easier to add new methods than to remove them.  I will remove the singular methods for now, and add the \@Unstable annotation to the API classes."
114044708,2472,ijuma,2017-04-29T03:06:43Z,"We should just allow `common` to access `common.internals`, there's no good reason not to allow that. It's a simple addition:

<allow pkg=""org.apache.kafka.common.internals"" exact-match=""true"" />

In general, `import-control.xml` is the right place to make these changes, I don't think we should be using suppressions for this."
114044716,2472,ijuma,2017-04-29T03:08:18Z,"`Producer` and `Consumer` are a bit weird because the implementation classes are what people typically use (due to the constructor). For the AdminClient, the abstract class is what people will be exposed to most probably."
114044732,2472,ijuma,2017-04-29T03:09:21Z,I would have thought that we would fail all the requests for that connection.
387427500,8218,mjsax,2020-03-04T03:11:27Z,"This is not really relevant for this PR, but we need to add it for KIP-447 eventually, thus I just include it in this PR."
387427638,8218,mjsax,2020-03-04T03:12:04Z,We moved this to `TaskManager`
387427823,8218,mjsax,2020-03-04T03:12:59Z,"On `suspend()` and `prepareCommit()` we don't commit yet, but return the offsets that need to be committed"
387427911,8218,mjsax,2020-03-04T03:13:24Z,We don't commit and thus don't throw any longer
387428416,8218,mjsax,2020-03-04T03:15:41Z,"Frankly, not sure if this is correct any longer. What do we want to record with this sensor exactly? Flushing can be expensive and we might want to record it as part of committing -- but I am not sure."
387428786,8218,mjsax,2020-03-04T03:17:11Z,"I am not happy with this rewrite (but as I know that John did some changes in this class in another PR, I just did this hack her for now -- needs some cleanup after a rebase)"
387429067,8218,mjsax,2020-03-04T03:18:27Z,"We could also do a second loop over all tasks, _after_ calling `commit(..)` below -- not sure if this is ok as-is?"
387430556,8218,mjsax,2020-03-04T03:25:43Z,Moved both tests to `TaskManagerTest`
387430633,8218,mjsax,2020-03-04T03:26:02Z,move all 4 tests to `TaskManagerTest`
387983590,8218,abbccdda,2020-03-04T22:54:42Z,Did you already update the KIP for the new config?
387984270,8218,abbccdda,2020-03-04T22:56:19Z,What's the benefit of building this as a static helper?
387994001,8218,mjsax,2020-03-04T23:23:13Z,We need to allow committing in `SUSPENDED` state now as we first suspend all tasks and than commit. Cf. `TaskManager#handleRevocation()`
387994269,8218,mjsax,2020-03-04T23:23:59Z,MInor improvement: we include writing the checkpoint and the caller can indicate if it should be written or not.
387994697,8218,mjsax,2020-03-04T23:25:15Z,This issues was introduced in the PR that introduced `StreamsProducer` -- we forgot to close them. Fixing this on the side.
387995044,8218,mjsax,2020-03-04T23:26:10Z,"We call `closeClean` below -- just fixing the comment here for now (\cc @guozhangwang)

Note that we don't commit offsets for this case any longer -- previously, committing offsets ""might"" have been done with `closeClean()` (even if I believe that the task would be marked as ""commitNeeded == false""). We don't let the TaskManager commit offsets here, as it should not be required."
387995875,8218,mjsax,2020-03-04T23:28:26Z,Similar to above: this issue was introduced in the `StreamsProducer` PR. We nee to close the producer when we remove it.
387995961,8218,mjsax,2020-03-04T23:28:44Z,as above
387996132,8218,mjsax,2020-03-04T23:29:16Z,Not sure why we use an iterator here. Simplifying the code with a `for`-loop
387996797,8218,mjsax,2020-03-04T23:31:16Z,"We need to commit explicitly in TTD now to mimic the TaskManger. Hence, we need access to the `consumer` and `streamsProducer` "
388015857,8218,abbccdda,2020-03-05T00:33:39Z,Why do we start to suppress warnings?
388016523,8218,abbccdda,2020-03-05T00:35:52Z,"Sounds good, just mark that depending on John's fix, we probably don't need to handle this."
388018015,8218,abbccdda,2020-03-05T00:41:10Z,Add a comment describing the new return statement.
388019423,8218,abbccdda,2020-03-05T00:45:58Z,I would prefer a second loop to guarantee a consistent reflection on the task committed state.
388019820,8218,abbccdda,2020-03-05T00:47:32Z,"In EOS beta, we should be able to send out a batch commit instead of individual ones?"
388022132,8218,abbccdda,2020-03-05T00:55:25Z,I don't think we need to test `assertFalse(task.commitNeeded()` as its outcome is controlled by `task.markCommitted`. So we only need to do it once.
388022419,8218,abbccdda,2020-03-05T00:56:24Z,Why do we no longer have the mock verification?
388026690,8218,abbccdda,2020-03-05T01:11:22Z,Should we do `expectLastCall` here?
388028665,8218,abbccdda,2020-03-05T01:18:20Z,We should also verify the thrown cause
388028820,8218,abbccdda,2020-03-05T01:18:46Z,"Same here, for verifying the thrown cause"
388029164,8218,abbccdda,2020-03-05T01:20:06Z,"What's the reasoning her for only wrapping the consumer offset commit case here, not for EOS case?"
388029471,8218,abbccdda,2020-03-05T01:21:10Z,Always feels better for one less parameter :)
388029958,8218,abbccdda,2020-03-05T01:22:58Z,Makes sense to me.
388030250,8218,abbccdda,2020-03-05T01:23:57Z,Checkmark for proving the 6 tests are all migrated.
388030837,8218,abbccdda,2020-03-05T01:25:58Z,Probably need to change after rebase
389051551,8218,mjsax,2020-03-06T17:50:27Z,"We should have done this from the beginning on... (it's just a ""side fix"")"
389052130,8218,mjsax,2020-03-06T17:51:29Z,We will need this later (ie follow up PR) and it reduced code duplication
389052956,8218,mjsax,2020-03-06T17:53:26Z,Correct. Unifying the commit logic as done is this PR allows us to do this in a follow up PR that actually enable producer per thread -- the whole purpose of this PR is to prepare/refactor for this.
389053599,8218,mjsax,2020-03-06T17:54:52Z,"You mean exception handling? For the producer all exception handling is done within `StreamsProducer` (note that `threadProducer` above is a `StreamsProducer`, not a `KafkaProducer`)"
389054695,8218,mjsax,2020-03-06T17:57:11Z,"We remove `RecordCollector#commit()` method in this PR and thus we remove the expected call to commit at the beginning of this test -- thus, there is nothing to be verified any longer and we don't call `commit()` with `prepareCommit()` any longer."
390712211,8218,mjsax,2020-03-11T02:20:42Z,"This is an open question: we don't want to remove this sensor however it was unclear to me how to handle this metric after we split ""task committing"" into three steps (prepareCommit; taskManager#commit; postCommit)."
390712571,8218,mjsax,2020-03-11T02:22:13Z,Simplification to avoid passing in `eosEnabled` and reducing constructor parameter list -- we just piggy back on the `application.id` that shall be `null` for non-eos.
390712665,8218,mjsax,2020-03-11T02:22:35Z,Avoid redundant logging.
390712770,8218,mjsax,2020-03-11T02:23:06Z,Side cleanup: All those method can actually be package private.
390712971,8218,mjsax,2020-03-11T02:23:55Z,Removing this state -- this is an open question if I did this correctly. \cc @vvcephei 
390714027,8218,mjsax,2020-03-11T02:27:59Z,"After we addressed the question how we want to do metrics, we can update this tests"
390714488,8218,mjsax,2020-03-11T02:29:59Z,Because we make app method in `StreamsProducer` package private but need access to `commit()` we add `TestDriverProducer` to get access.
390714741,8218,mjsax,2020-03-11T02:31:09Z,Just added to give public access to `commitTransaction()` to TTD (it's more elegant than making `StreamsProducer#commitTransaction` public IMHO)
390727409,8218,abbccdda,2020-03-11T03:27:07Z,nit: we could log thread-id here for easier log search.
391162808,8218,vvcephei,2020-03-11T18:01:56Z,"It seems a bit roundabout to have to remember we should send a `null` `application.id` as the constructor argument to indicate that eos is enabled. What's wrong with saying ""eos is enabled"" when you want eos to be enabled?"
391233773,8218,abbccdda,2020-03-11T20:05:06Z,Could we internalize this state check inside the task to simplify the logic here?
391234248,8218,abbccdda,2020-03-11T20:05:36Z,"Similarly here, this state check could be internalized."
391236509,8218,abbccdda,2020-03-11T20:07:57Z,"nit: let's order the functions as 
```
prepareCloseClean
closeClean
prepareCloseDirty
closeDirty
```
"
391237078,8218,abbccdda,2020-03-11T20:08:36Z,Prepare to uncleanly close a task that we may not own.
391241528,8218,abbccdda,2020-03-11T20:13:52Z,"Having a `prepareCloseDirty` makes the calling of `closeDirty` a bit cumbersome as we always need to call `prepareCloseDirty` first. To simplify or just do a reminder, I have two suggestions:

1. Internally create a task state called PREPARE_CLOSE or just a boolean like `closeDirtyPrepared` as the state check, so that closeDirty will throw illegal state if the flag is false
2. Following #1, instead of throw, if we don't see the prepareClose is being called, the `closeDirty` will invoke `prepareCloseDirty` first internally."
391243910,8218,abbccdda,2020-03-11T20:17:03Z,"@guozhangwang For my own education, why we do `and` here instead of just checking `commitRequested`?"
391244729,8218,abbccdda,2020-03-11T20:18:43Z, 
391260243,8218,abbccdda,2020-03-11T20:50:44Z,"Comment here for no better place: Standby task always returns an empty `committableOffsetsAndMetadata`, then why do we still need to check `commitNeeded` for it? Shouldn't it always set to false?"
391261158,8218,abbccdda,2020-03-11T20:52:34Z,`logContext ` is not used.
391261929,8218,abbccdda,2020-03-11T20:54:06Z,Should we attempt to add more fine-grained metrics for 3 stages then?
391265711,8218,abbccdda,2020-03-11T21:01:37Z,Could we add a `@return` for this method? Also we should comment about the different indications when we return an empty map vs null.
391267109,8218,abbccdda,2020-03-11T21:04:39Z,Remove `if`
391286988,8218,abbccdda,2020-03-11T21:47:16Z,I feel a bit weird here as we don't need `prepareCloseClean` anymore. This API usage is a little complicated upon when we should do it and we don't.
391287132,8218,abbccdda,2020-03-11T21:47:37Z,Similarly for `closeDirty` and `prepareCloseDirty`
391289786,8218,abbccdda,2020-03-11T21:54:04Z,nit; 228 - 229 could be merged.
391291102,8218,abbccdda,2020-03-11T21:57:16Z,"Also the above step #4 is no longer correct, the commit is done on TaskManager now."
391295221,8218,abbccdda,2020-03-11T22:08:20Z,Do we need to keep a task once it is failed to clean close? Why couldn't we just close it dirty immediately after we see the exception?
391296895,8218,abbccdda,2020-03-11T22:11:16Z,Why do we need to move these tests?
391298785,8218,abbccdda,2020-03-11T22:13:39Z,Looks like we lack test coverage for TimeoutException and KafkaException cases
391301678,8218,abbccdda,2020-03-11T22:17:34Z,We don't have unit test coverage for this exception case
391302227,8218,abbccdda,2020-03-11T22:18:14Z,We lack unit test coverage for this case
391303712,8218,abbccdda,2020-03-11T22:20:40Z,"Could we verify the assignment stack and lost stack separately, by doing `handleAssignment` verify first before calling `handleLost`"
391337786,8218,mjsax,2020-03-12T00:07:25Z,The `threadId` is already added to the log prefix when the `log` object is created in `StreamsThread`
391338238,8218,mjsax,2020-03-12T00:09:08Z,Not sure if I can follow. We don't check `commitNeeded` in `postCommit()`? Can you elaborate?
391338530,8218,mjsax,2020-03-12T00:10:18Z,"Frankly, I have no good idea atm... Also, if we change metrics, we need to update the KIP and it's getting more complicated. If possible, I would prefer to not change any metric, but not sure if it is possible..."
391338712,8218,mjsax,2020-03-12T00:10:57Z,"Yeah, this PR does not yet add all required test..."
391339213,8218,mjsax,2020-03-12T00:13:04Z,"Why do we need to document this in the method JavaDoc? It's an internal method? Internal comment outdate quickly if code is changed and comments are not updated accordingly (what happens 99% of the time). Hence, I would prefer to limit comments if possible. In doubt, we should document at `Task` level anyway."
391339463,8218,mjsax,2020-03-12T00:13:57Z,You see -- that is may point from above... The code should be written in a way that explains itself... Updating comments always slips...
391340439,8218,mjsax,2020-03-12T00:17:53Z,@guozhangwang I am actually wondering about point (5) -- why do we need to checkpoint the state manager if we wipe out the store later anyway for the unclean EOS case?
391342962,8218,mjsax,2020-03-12T00:28:08Z,"I actually had a similar though, but was not sure if it's worth it. Would like to hear from @guozhangwang @vvcephei what they think?

If we do this, we might want to do it for ""commit"" and ""suspend"", too. For suspend() adding a state SUSPEND_PREPARED is not helpful as suspend() does different things depending on the previous state. (For commit and close an additional state would work). For consistency reasons, an internal flag might be better though.

Not sure ate if calling ""prepare"" automatically would actually be correct for all cases?"
391344437,8218,mjsax,2020-03-12T00:33:24Z,"`prepareCloseClean()` already does a state check and returns `emptyMap` if state is `CREATED`.

The point of this check is, that we don't add anything to the `consumedOffsetsAndMetadataPerTask` map -- this is important for the corner case for which all tasks are in state CREATED and thus no transaction was initialized. For this case we cannot call `producer.addOffsetsToTranscation()` and must skip this step entirely. Note, that we have a corresponding check below to not call `commitOffsetsOrTransaction` if the map is empty."
391344863,8218,mjsax,2020-03-12T00:35:23Z,We need to call `prepareCloseClean` (as done in L196 above) _before_ we call `commitOffsetsOrTransaction` (L215 above).
391344936,8218,mjsax,2020-03-12T00:35:38Z,Some comment as above.
391344967,8218,mjsax,2020-03-12T00:35:44Z,I know...
391345982,8218,mjsax,2020-03-12T00:40:13Z,"To avoid the overhead to commit offset that are already committed, ie, the previous commit committed offset 5 and now we would commit offset 5 again."
391346285,8218,mjsax,2020-03-12T00:41:22Z,I think it's easier to read if it's split.
391347114,8218,mjsax,2020-03-12T00:44:22Z,"I try to keep ""order"" and group test methods to keep an overview if test coverage is complete.
```
// generic tests
    // functional
    // exception handling
// non-EOS tests
    // functional
    // exception handling
// EOS tests
    // functional
    // exception handling
```"
391348100,8218,mjsax,2020-03-12T00:48:34Z,"Not sure if I can follow? The comments just mark which setup calls belongs to which test call, nothing more. All setup is done upfront before we call the actually methods under test."
391348586,8218,mjsax,2020-03-12T00:50:19Z,Good catch.
391361243,8218,mjsax,2020-03-12T01:47:39Z,Good idea!
391809790,8218,guozhangwang,2020-03-12T18:22:32Z,nit: add a check that taskId exists in `taskProducers` to make sure we do not return null.
391812242,8218,guozhangwang,2020-03-12T18:27:10Z,Subjectively I'd +1 that adding one more parameter to avoid piggy-backing on the applicationId is better.
391829473,8218,guozhangwang,2020-03-12T18:59:20Z,"Actually on a second thought, I'm wondering if the following inside TaskManager is cleaner:

```
for (task <- taskManager.activeTasks)
    task.recordCollector().commit(taskToCommit.getTask(task.id);
```

Instead of:

```
activeTaskCreator.streamsProducerForTask(taskToCommit.getKey()).commitTransaction(taskToCommit.getValue());
```

My gut feeling is that it is cleaner to not access the task creator for its created stream-producers (and hence here we need to change the task-producer map to streamsProducers), but just access each task's record collector and call its `commit` --- today we already have a `StreamTask#recordCollector` method."
391829854,8218,guozhangwang,2020-03-12T19:00:10Z,"Please see my other comment above --- I think it is cleaner to just call `foreach(active-task) task.recordCollector.commit` inside the task-manager; and inside RecordCollectorImpl we check that eosEnabled is always true, otherwise illegal-state thrown.

In the next PR where we have the thread-producer, we could then only create a single `recordCollector` object that is shared among all active tasks and wraps the thread-producer, and then the caller `taskManager` code then can just get one active task and call its record-collector's commit function knowing that is sufficient to actually commit for all tasks since everyone is using the same record-collector.

WDYT?"
391838078,8218,guozhangwang,2020-03-12T19:16:48Z,"This is a meta comment: I think we can consolidate `prepareCommit` and `prepareClose` and `prepareSuspend` here by introducing the clean parameters to the function, since their logic are very similar (for the part that they diffs a bit, it can be pushed to post logic), and on task-manager during commit:

1) for each task -> task.prepareCommit(true)
2) commit
3) for each task -> task.postCommit(true)

During close:

if (clean)
    1) for each task -> task.prepareCommit(true)
    2) commit()
    3) for each task -> task.postCommit(true)
else
    1) for each task -> task.prepareCommit(false)
     // do not commit
    3) for each task -> task.postCommit(false)
4) tasks.close(flag)

And the same for suspension."
391838707,8218,guozhangwang,2020-03-12T19:18:06Z,"I actually think that we can remove this DEBUG-level per-task commit metrics, since we already have the INFO-level per-thread commit metric and this one does not provide much more additional information?"
391974508,8218,mjsax,2020-03-13T00:39:06Z,"I think it's unclean to let the RecordCollector commit (note that this PR removes `RecordCollector` not at side refactoring but on purpose) -- to me the RecordCollector has the responsibility to bridge the gap between the runtime code (that is typed), and the Producer that uses `<byte[],byte[]>` (ie, it serialized the data and manages errors from `send`) -- why would a **_collector_** know anything about committing (for which it also needs a handle to the consumer)?

About accessing the `ActiveTaskCreator`: we could also expose the `StreamsProducer` via the `RecordCollector` though (or directly via the task)? That would be cleaner I guess."
391993410,8218,mjsax,2020-03-13T02:06:16Z,It's a personal preference I guess. But seems you don't like it. Will revert it.
392017955,8218,guozhangwang,2020-03-13T03:50:05Z,"This is not introduced in this PR, but: while thinking about it, I realized for RESTORING state we do not need to rely on eosDisabled to checkpoint, in fact we can always checkpoint during RESTORING here."
392783249,8218,mjsax,2020-03-16T04:59:42Z,Will do this in a follow up PR.
392783415,8218,mjsax,2020-03-16T05:00:28Z,Ack
392783859,8218,mjsax,2020-03-16T05:02:44Z,Covered via `shouldCommitNextOffsetFromQueueIfAvailable` and `shouldCommitConsumerPositionIfRecordQueueIsEmpty`
392784308,8218,mjsax,2020-03-16T05:05:18Z,We can address this in a follow up PR.
392794910,8218,mjsax,2020-03-16T05:56:23Z,Added test `shouldThrowWhenHandlingClosingTasksOnProducerCloseError`
393132558,8218,abbccdda,2020-03-16T16:00:20Z,We need a unit test for this function.
393136595,8218,abbccdda,2020-03-16T16:06:09Z,"we could just do one log in front: `log.info(""Prepare suspending {}"", state());`"
393139575,8218,abbccdda,2020-03-16T16:10:32Z,Do we have unit test to check the checkpoint status after `postCommit()`?
393180380,8218,abbccdda,2020-03-16T17:06:41Z,"I couldn't fully follow this idea, just playing devil advocates here, if we think meta code comments actually hinder the readability of internal class, why not just remove all the internal function meta comments, as they would get outdated anyway? For me the return type comment is still valuable for understandability. If the comment gets outdated, we should just update it. cc @guozhangwang if the idea here makes sense, or we could get a consensus on what needs to be done in internal class comments, and what's not."
393183310,8218,abbccdda,2020-03-16T17:11:13Z,nit: { could be reduced.
393195455,8218,abbccdda,2020-03-16T17:31:51Z,"Yea, a TODO is also ok."
393336160,8218,guozhangwang,2020-03-16T22:07:16Z,"`For the next PR` (all other comments with this tag means no changes required for this PR): my understanding is that we would make the thread-producer also a `StreamsProducer` instead of a `KafkaProducer` which would be used to `commitTransaction` under `eosBeta`, is that right?"
393336618,8218,guozhangwang,2020-03-16T22:08:31Z,"nit: we can have a wrapped StreamsProducer#close / metrics, and then #kafkaProducer would be for testing-only."
393336934,8218,guozhangwang,2020-03-16T22:09:16Z,"After syncing offline about this, I think I'm convinced now that moving this logic into TaskManager is better."
393339067,8218,guozhangwang,2020-03-16T22:14:56Z,"I think we should just let the prepareXX function to return the map of partitions -> partition-timestamp to indicate if it should be included in the map of committing offsets, so that we do not need to leak the state into task-manager here. Also we only need to call `mainConsumer.position` once for all tasks -- please see my other comment above.


Also: we should not try to commit state if we are in RESTORING but only flushing store and writing checkpoints (I think this is already the behavior in trunk), since the partitions are paused from the main-consumer before restoration is done --- maybe it is partly causing some unit test flakiness."
393339213,8218,guozhangwang,2020-03-16T22:15:23Z,"SG.

I think in this PR we still can do the change to let `prepareXX` to return the map of partitions -> partition-timestamp to indicate whether this task should be included in committing."
393341972,8218,guozhangwang,2020-03-16T22:23:14Z,"In either eos-alpha or eos-beta or non-eos, we can just loop over all the ""committable partitions"" and call `mainConsumer#position` once, so this function can be extracted out of the task as a per-task call.

More specifically, in the prepareXX calls, we know based on the state of the task and clean flag whether or not we should commit the source topic offsets for this task, so we can let the prepareXX function to return `Map<TopicPartition, Long> partitionTimes` encoding the extracted timestamps for each partition instead of void --- when we decided not to commit we return an empty map. And then inside TaskManager we just use the `mainConsumer` to call position once and then pass that to the `commitOffsetsOrTransaction` call."
393344611,8218,guozhangwang,2020-03-16T22:28:40Z,nit: we can do `if / else if / else` here still and move the `closeTaskSensor.record(); / transitionTo(State.CLOSED);` to avoid duplication.
393345037,8218,guozhangwang,2020-03-16T22:29:26Z,"Ditto here, I think if / else if / else is more readable."
393350516,8218,mjsax,2020-03-16T22:43:50Z,"Well, we log ""skip"" for state CREATED and we throw for invalid states. Note sure how to do this?"
393350827,8218,mjsax,2020-03-16T22:44:47Z,"Yes, `shouldRespectCommitNeeded()` check this already."
393371781,8218,guozhangwang,2020-03-16T23:47:56Z,"`For the next PR`: as I mentioned in the last commit I feel `prepareSuspend` and `prepareClose` can be consolidated with `prepareCommit` but in the next PR these logic would be changed again for eos-beta so maybe we cannot do that any more, so I'm fine with keeping as-is and we can revisit to see if we can really do this refactoring or not in the next PR when we did the eos-beta."
393373023,8218,guozhangwang,2020-03-16T23:52:01Z,Yes we are unnecessarily checkpointing here --- the reason is that EOS flag was original striped out of task and only processor-state-manager knows about it; now since we get this EOS flag back to task (sigh.. :) we can add this additional check.
393374466,8218,guozhangwang,2020-03-16T23:57:18Z,"I would suggest not restricting ourselves to some specific rules about comments :) Personally I tried to avoid the `one line comment explaining one line code` type of comments inside a function since it should be obvious, rather I'd add some comments for a block or several blocks if I fear it maybe hard to read by itself. I think you guys should just make your best judgement here.

And for internal functions, I agree that we do not necessarily need to write java-docs, and this one, for example, I wrote the java-doc as part of the tech debt cleanup just to remind what operations MUST be considered here inside closing / suspending etc so that later on when we change the function itself by other contributors, they would use it as a reference to check if they mistakenly missed some steps or re-ordered some steps. However if we are going to split this function into multiple, instead of just re-structuring the function as a whole, then although I have my preference I'd leave to you guys if you want to add the javadoc for both pre/post of you feel now it is too obvious to bother :) "
393377256,8218,guozhangwang,2020-03-17T00:07:51Z,"`For the next PR`: I see the reason I return the checkpoint is that we are now extracting the committing out of the task and I need to remember if we need to checkpoint and if yes which offsets after we've flushed and before we checkpoint, but since the state of the task would not change before / after the commit during close. 

More specifically we only have three cases: 1) to not write checkpoint, 2) write checkpoints for written offsets (changelogs) only, 3) write checkpoint for written and consumed offsets. And no matter which case it is during the `preClose`, it would always be the same in the `post`, so why do we need to return it to task-manager, book-keep there, and then after commit to pass it back to tasks?"
393378779,8218,guozhangwang,2020-03-17T00:14:27Z,nit: we should emphasize that PrepareClose and close calls should be implemented idempotent since we may call it multiple times if a task close clean first and then fail and then close dirty. 
393384266,8218,guozhangwang,2020-03-17T00:37:48Z,"`For the next PR`: I think we can save prepareClose (or more accurately, merge prepareClose and close together again) if we make a state diagram change that only suspended state can transit to closed state, i.e. at task-manager level whenever we want to close a task we call its `suspend` function first, which would, depending on its state, be a no-op, or flushing, or closing topology etc, and then after that the task is always in SUSPENDED state, and then we call ""commit"" if necessary, and then we call close (a minor thing is that today when the state is in SUSPENDED we would omit committing inside task, and we need to lift this restriction; and also the transition actions to transit to SUSPENDED need to rely on the clean flag, hence we need `suspend(clean-flag)`).

AND we can further merge prepareSuspend and suspend as well by just making the checkpointing logic as part of post-commit instead of post-suspend, since as I mentioned above you only have three cases:

1) do not need to checkpoint: if you are in CREATED.
2) checkpoint written and consumed offsets: if you are in RUNNING, in which you need to commit offsets as well.
3) checkpoint only store offsets: if you are in RESTORING, and in which case you do not need to commit offsets.

In fact, if we are not in the RUNNING state yet, the `consumedOffsets` as well as `recordCollector#offsets()` are always going to be empty, so it is always safe to call `stateMgr.checkpoint(checkpointableOffsets())` and not condition on the state and call `stateMgr.checkpoint(emptySet())`.

And if we now allow committing in SUSPENDED state as part of closing (i.e. suspend -> commit -> close), similar rules apply: if we are suspending from a RESTORING state, then in `postCommit` while we ``stateMgr.checkpoint(checkpointableOffsets())` the `checkpointableOffsets` would always be empty; if we are suspending from a RUNNING state it would contain some offsets."
393385438,8218,guozhangwang,2020-03-17T00:41:49Z,"See my other comments: we should not commit in CREATED, RESTORING and SUSPENDED state, and it's better just to let the prepareXX function to indicate if there's anything to commit based on its state internally than letting task-manager to branch on the task state -- more specifically, here the prepareClose call should not return the map of checkpoints but the map of partition -> partition-timestamps (if empty it means nothing to commit), since the checkpoint map are not needed at task-manager at all and post commit, if the offsets should be empty it would still be empty."
393387491,8218,guozhangwang,2020-03-17T00:50:23Z,"Same here: not only CREATED, but also RESTORING and SUSPENDED tasks should not be included in `consumedOffsetsAndMetadataPerTask` and we should not let the task-manager to peek its state."
393387840,8218,guozhangwang,2020-03-17T00:51:31Z,"""as above"" :)"
393389583,8218,mjsax,2020-03-17T00:58:53Z,Correct. For eos-beta there will be one `StreamsProducer` that is shared over all tasks.
393389903,8218,mjsax,2020-03-17T01:00:30Z,"We could, but the idea was that `ActiveTaskCreator` creates the producer via `new KafkaProducer()` and thus it should call `KafkaProducer#close()`, too, and not delegate it to `StreamsProducer`. Thoughts?"
393392898,8218,mjsax,2020-03-17T01:12:55Z,"Well, we can, but we get an empty ""than block"" what is weird:
```
if (state() == State.CREATED) {
  // empty
} else if {
...
}
```"
393418010,8218,mjsax,2020-03-17T02:58:33Z,"Within `maybeCommitActiveTasksPerUserRequested` we know that we are in state `RUNNING` and thus, no need to check what `committableOffsetsAndMetadata()` returns but we can ""blindly"" commit."
393792511,8218,abbccdda,2020-03-17T16:05:29Z,"By `next PR`, you mean the one after we finish the EOS-beta commit feature right?"
393884343,8218,guozhangwang,2020-03-17T18:25:59Z,I mean the next PR when we add the EOS-beta feature --- please see the first comment I have with this tag.
393885815,8218,guozhangwang,2020-03-17T18:28:21Z,"Hmm, I think moving forward we would create and maintain both the single thread-producer and task-producers as StreamsProducer objects right?"
393887170,8218,guozhangwang,2020-03-17T18:30:28Z,"I'd say we can always log a debug there saying ""doing nothing in this function since we are in this state"" :) The main concern I had, is that if in the future we want to add more steps in addition to `recording sensor` etc, we may forget adding it in one place or the other. Removing duplicated code helps us to be less vulnerable."
393898999,8218,mjsax,2020-03-17T18:50:49Z,SGTM
393905732,8218,guozhangwang,2020-03-17T19:03:16Z,Thanks for the cleanup!
393906971,8218,guozhangwang,2020-03-17T19:05:38Z,Why we have to transit to SUSPENDED before prepare-closing? Originally we want to check that CREATED state can still trigger close.
393907754,8218,guozhangwang,2020-03-17T19:07:06Z,"Not introduced in this PR: could we add test checking CLOSED state should not commit as well?

Also checking SUSPENDED state close-call is no-op."
393911532,8218,guozhangwang,2020-03-17T19:14:37Z,"Why making `commitTransaction` is less elegant? I thought that was fine since `StreamsProducer` is inside the internals package anyways? In fact, in TTD we have access to InternalTopologyBuilder accessing it functions (we used to also have a wrapper of InternalTopologyBuilder which we removed later) so I thought that was the agreed pattern."
393917156,8218,guozhangwang,2020-03-17T19:25:38Z,"This is a meta comment: since we moved the commit logic out of the tasks into task-manager already, we should add the check that:

1) inside the task manager, if the commit failed with fatal errors, the corresponding follow-up steps (postCommit, suspend, closeClean) should be skipped, and the exception is thrown out of the task-manager to thread
2) if commit failed with fenced errors, follow-up steps are also skipped (tasks state should be un-changed) and the task-migration exception is thrown out of the task-manager."
393963708,8218,mjsax,2020-03-17T20:53:23Z,Ack. We can remove this.
393964723,8218,mjsax,2020-03-17T20:55:19Z,Ack
393966087,8218,mjsax,2020-03-17T20:57:46Z,"> Also checking SUSPENDED state close-call is no-op.

But in `close()` if state is suspended we might still wipe out the state store -- it's not a no-op"
393968843,8218,mjsax,2020-03-17T21:03:00Z,"It's obviously subjective -- personally, even if something is internal, we should not just declare stuff as `public` but try to keep it to a minimum to follow the idea of encapsulation (not always possible). If you want me to remove this class and make the method `public` I can do it in a follow up PR. Not sure if we have an agreed pattern, though."
393979041,8218,guozhangwang,2020-03-17T21:23:47Z,"Cool, in that sense let's just keep it then -- do not add it in one PR and remove it immediately in the next."
56714551,1095,granthenke,2016-03-18T20:05:32Z,"Not sure if this is the best way to do this. I need a different constructor for requesting a `null` list because `null` matches both the list and struct constructors with the same ""specificity"".

I am open to ideas for a better way to support requesting no topic metadata. 
"
56716409,1095,gwenshap,2016-03-18T20:20:17Z,"you mean other.rack?

Also, I'm wondering whether we want to consider a node to be a different node when the rack changed. I guess it depends on how it is used - except it isn't used...
"
56716786,1095,granthenke,2016-03-18T20:23:02Z,"Good catch. I just maintained equals meaning all fields match. I suspect thats the safest path to go right now. 
"
56717383,1095,gwenshap,2016-03-18T20:27:12Z,"I noticed you keep using BROKER_V0 anywhere except METADATA, and I think thats the plan going forward as well.
If my understanding is correct, BROKER_V1 is not really a ""newer"" broker definition (which implies that eventually we'll move everything to use that) but rather a broker definition specific for METADATA request or just a more detailed broker definition. Maybe rename to something less misleading?
"
56717758,1095,gwenshap,2016-03-18T20:29:50Z,"And on similar topic (but should be separate JIRA) - the protocol has few different places with BROKER definitions - maybe more consolidation is possible?
"
56717859,1095,granthenke,2016-03-18T20:30:38Z,"Yes, my understanding is these ""sub-schemas"" are supposed to tie directly to single api/protocol to allow them to change independently. I think the fact that a BROKER_V0 was shared was a mistake. I had left the old one being shared just to minimize change. I can break-out and rename a bit to prevent an accidental sharing in the future. 
"
56718714,1095,granthenke,2016-03-18T20:37:26Z,"I am of the impression that we are not consolidating so that the wire protocols can change independently. However, we want to represent that in Java or Scala object is up to the parser. This decouples protocol from implementation. Though even in implementation we have chosen to duplicate in the past too.
"
56768466,1095,ijuma,2016-03-20T17:47:15Z,"Would it make sense to use a bitset for these booleans?
"
56892437,1095,granthenke,2016-03-21T20:38:10Z,"Yeah, that could save a byte for each topic. I will mention it during the wire protocol discussion on the next kip call, since this needs to be reviewed/voted. 
"
57952993,1095,hachikuji,2016-03-30T19:57:34Z,"I've been wondering if it would be better to use static factory methods instead of relying on constructors. In that case, you could use something like `MetadataRequest.allTopics()` or something like that.
"
57953330,1095,granthenke,2016-03-30T19:59:43Z,"Good suggestion. I like the builder pattern as well. The implementation can be a bit verbose in java, but it can do validation at build time and prevent the telescoping constructor problem.  It may also be able to provide simplified api compatibility. 
"
57967577,1095,hachikuji,2016-03-30T21:31:43Z,"I also like the builder pattern. It reads nicely and you don't have to care about argument order. On the other hand, it's also easier to forget necessary arguments and it feels kind of silly when the number of arguments is small. One nice thing about factory methods is that you can give them convenient names (e.g. MetadataRequest.allTopicsV1()). Both options are probably better than using the constructors, but I don't think it would be too bad to stick with the current convention for this patch. 

Another option would be to include an explicit flag in the constructor. For example:

``` java
public MetadataRequest(boolean includeAllTopics);
```

We could then have the other constructor which accepts the topic list use the ""empty means empty"" semantics, and users would have to call this method to get all topics.
"
58056988,1095,granthenke,2016-03-31T13:55:48Z,"@hachikuji I do like the idea of a flag to make it explicit. The challenge that poses is compatibility. I think any existing constructor needs to continue to function the way it used to. So empty list would need to continue to signify ""all topics"", at least for existing constructors.
"
58058004,1095,ijuma,2016-03-31T14:02:16Z,"@granthenke request classes are not API so I am not sure why it has to be like that? The official position is that we generate javadoc for classes that are API: https://kafka.apache.org/090/javadoc/index.html (yes, I know it's confusing, I hope to change that so internal classes are all in internal packages).
"
58060325,1095,granthenke,2016-03-31T14:15:24Z,"@ijuma I didn't realize compatibility wasn't a concern on these classes. Then we can do whatever we would like here. 
"
58221673,1095,granthenke,2016-04-01T15:24:57Z,"@hachikuji @ijuma I updated the constructor to take an `allTopics` boolean. I think the changes required to do so may also solve [KAFKA-3358](https://issues.apache.org/jira/browse/KAFKA-3358). It worked nicely with `metadata.needMetadataForAllTopics` in the `NetworkClient`.
"
58239805,1095,SinghAsDev,2016-04-01T17:36:14Z,"In current proposal, we are saying a null indicates all topics. Would it be possible to use that here as well?
"
58240259,1095,granthenke,2016-04-01T17:39:41Z,"@SinghAsDev The proposal is to use null on the wire protocol, not in the API. In the discussions it was mentioned that being more explicit in the API was favored. 

Beyond that, if you specify `MetadataRequest(null)` java wont know which constructor to use between `MetadataRequest(List<String> topics)` and `MetadataRequest(Struct struct)`. 
"
58243837,1095,SinghAsDev,2016-04-01T18:04:58Z,"Got it, thanks for the explanation @granthenke.
"
58245304,1095,granthenke,2016-04-01T18:14:51Z,"Thanks for the review @SinghAsDev!
"
58536600,1095,ijuma,2016-04-05T13:26:17Z,"Just noting that this is no longer relevant given the current implementation (for other people reading the PR).
"
60636130,1095,gwenshap,2016-04-21T18:55:09Z,"Don't we usually add new arguments at the end?
"
60638297,1095,gwenshap,2016-04-21T19:08:35Z,"Do we need to validate if this is nullable before writing?
"
60638460,1095,gwenshap,2016-04-21T19:09:40Z,"Do we want to throw an exception with appropriate message if item if null but shouldn't be?
"
60638661,1095,gwenshap,2016-04-21T19:11:02Z,"Not sure if it matters, but we will get the exact same hash for empty and null arrays?
"
60641282,1095,granthenke,2016-04-21T19:28:51Z,"Since this was in internals and I needed to update all usages regardless, I located ""related"" parameters close to each other. I can move to the  end if you prefer. 
"
60642147,1095,granthenke,2016-04-21T19:35:12Z,"I think all of the other types assume validate will be called before calling write and that the written object would be the one returned by the validate call.

Here is a code snippet from `Schema.write(...)`:

``` scala
Object value = f.type().validate(r.get(f));
f.type.write(buffer, value);
```
"
60642638,1095,granthenke,2016-04-21T19:39:10Z,"It fails further down during the cast the same way it would before nulls were allowed. That exception is caught and re-thrown as a SchemaException. This behavior is validated by `ProtocolSerializationTest.testNulls`.
"
60645983,1095,granthenke,2016-04-21T20:02:39Z,"Good point. We don't have `hashcode` or `equals` defined for `Schema`, `Field` or `Type`...perhaps we should.
"
60660381,1095,gwenshap,2016-04-21T21:45:10Z,"Yeah, I saw that. I'm suggesting a separate SchemaException with message specific for nulls, to make troubleshooting/debugging a bit easier. Not a big deal though.
"
60660437,1095,gwenshap,2016-04-21T21:45:34Z,"Want to create a followup JIRA?
"
60660863,1095,granthenke,2016-04-21T21:48:48Z,"Created [KAFKA-3603](https://issues.apache.org/jira/browse/KAFKA-3603) to track that. 
"
60661081,1095,ijuma,2016-04-21T21:50:27Z,"Personally, I'd introduce a `ArrayOf.nullable` static factory method as I think that would be more readable than the `true` here (since we don't have named arguments in Java).
"
60661303,1095,gwenshap,2016-04-21T21:52:16Z,"I don't think we can do a fall-through here? Since v1 error response is not a valid v0 response?
"
60661511,1095,granthenke,2016-04-21T21:53:47Z,"I can do that. It should probably be done for all Types that don't accept null. I opened [KAFKA-3603](https://issues.apache.org/jira/browse/KAFKA-3603) to track that.
"
60662604,1095,gwenshap,2016-04-21T22:03:00Z,"I am bit confused about the use of controller node vs controllerId. 

The protocol requires sending controllerId, which is what we have in most places. But you also find the actual node and provide an API (which isn't used anywhere? not even in tests?) to get the node.

Wondering what was the the plan here.
"
60672248,1095,gwenshap,2016-04-21T23:41:16Z,"Thanks!
"
60672977,1095,gwenshap,2016-04-21T23:49:55Z,"I'm wondering if we can push down the work of figuring out if a topic is internal to TopicMetadata constructor. It will clean up the API and TopicMetadata has all the information it needs to find out. 
"
60673350,1095,gwenshap,2016-04-21T23:54:39Z,"Mind adding a comment here, something along the lines of:
""In version 0, we returned an error when brokers with replicas were unavailable, while in higher versions we simply didn't include the broker in the list we returned""
(or is it vice-versa? anyway, its non-obvious and need a comment IMO)
"
60673675,1095,SinghAsDev,2016-04-21T23:58:50Z,"NIT: I think BROKER or BROKER_INFO or BROKER_METADATA would be better.
"
60673775,1095,gwenshap,2016-04-22T00:00:03Z,"Not 100% sure about changes to this test. Random cleanup or related to KIP-4?
"
60673904,1095,SinghAsDev,2016-04-22T00:01:23Z,"Should we mention effect of a null string as rack here?
"
60673918,1095,gwenshap,2016-04-22T00:01:35Z,"Aren't we adding a test for the results with errorUnavailableEndpoints = false?
"
60673941,1095,gwenshap,2016-04-22T00:01:52Z,"Love it!
"
60674080,1095,gwenshap,2016-04-22T00:03:44Z,"ok, I saw we are testing both request versions below. I still find it a bit weird it is missing from the cache tests, since it is added functionality for the cache - but up to you.
"
60674146,1095,gwenshap,2016-04-22T00:04:31Z,"Do you want to also validate that V0 request with null still returns all topics?
"
60674190,1095,gwenshap,2016-04-22T00:05:11Z,"I also don't see us checking for V1 all-topics explicitly. Do you want to add something?
"
60674223,1095,gwenshap,2016-04-22T00:05:38Z,"Not sure how this is related.
"
60674299,1095,gwenshap,2016-04-22T00:06:13Z,"Its fine. Just checking :)
"
60674310,1095,gwenshap,2016-04-22T00:06:26Z,"Got it. Thanks.
"
60674321,1095,gwenshap,2016-04-22T00:06:34Z,"Thanks!
"
60674337,1095,gwenshap,2016-04-22T00:06:52Z,"+1
"
60674619,1095,SinghAsDev,2016-04-22T00:10:09Z,"Just thinking it loud here, it seems we follow getter convention here, we really do not have a common convention across the codebase. However, it is not too bad, as this is not end-user. I think user facing classes/interfaces do not follow getter convention though.
"
60675033,1095,SinghAsDev,2016-04-22T00:15:12Z,"shouldn't we check something like `topics != null && !topics.isEmpty()` or maybe we just want to rename the method to indicate we are indeed checking for topics to be non-null.
"
60675649,1095,SinghAsDev,2016-04-22T00:23:11Z,"Would it make sense to be consistent with other key names? Have something like ""controller_id"".
"
60676104,1095,gwenshap,2016-04-22T00:29:20Z,"KAFKA-3603 seems to be for something else?
"
60676663,1095,SinghAsDev,2016-04-22T00:36:36Z,"+1
"
60676892,1095,SinghAsDev,2016-04-22T00:40:19Z,"NIT: Maybe we can leave this file out if there are no other changes?
"
60755927,1095,ijuma,2016-04-22T15:19:50Z,"This should be `byte` instead of `Byte`.
"
60756062,1095,ijuma,2016-04-22T15:20:48Z,"What is the reasoning for accepting any non-zero value as `true`? It seems more error-prone IMO. If any value outside of `0` or `1` are used, it's probably a mistake.
"
60756385,1095,ijuma,2016-04-22T15:22:59Z,"Do we want to ignore or it would it be better to validate it (ie `topics` must be empty or null for that case).
"
60757857,1095,ijuma,2016-04-22T15:31:46Z,"You can just return the controller here and then you don't need the `controller` variable or break (in Scala, I don't like to use `return`, but if one is using `break` already, then it's at a similar level IMO).
"
60758124,1095,ijuma,2016-04-22T15:33:37Z,"If possible, Jun generally suggests that we have one constructor per version with older version constructors deprecated. Seems like we could do that here right? The older version would not have `controllerId`.
"
60758473,1095,ijuma,2016-04-22T15:35:40Z,"I think it would be useful to add a comment here and when we set other new fields (`RACK_KEY_NAME`, `IS_INTERNAL_KEY_NAME`, etc.) saying stating the version they were added in (we do that in other request/response classes).
"
60758863,1095,ijuma,2016-04-22T15:38:05Z,"Hmm, if we don't have that field, then we don't know if it's internal or not, right?
"
60761238,1095,granthenke,2016-04-22T15:53:41Z,"@ijuma I had maintained the old constructor for compatibility in an older version of the patch. But in previous reviews I was told I did not need to maintain it: https://github.com/apache/kafka/pull/1095#discussion_r58058004
"
60761858,1095,granthenke,2016-04-22T15:57:30Z,"I am okay with that. 
"
60762586,1095,granthenke,2016-04-22T16:01:17Z,"This is left over from changes required when leveraging zookeeper to track deletes. I left it because it looks like its closer to what was actually trying to be mocked based on the comment. I can keep it or leave it. 
"
60762636,1095,granthenke,2016-04-22T16:01:35Z,"Sure I will add some more test cases.
"
60762667,1095,granthenke,2016-04-22T16:01:52Z,"I can add those tests here too.
"
60762756,1095,granthenke,2016-04-22T16:02:28Z,"This is left over from changes required when leveraging zookeeper to track deletes. I left it because it looks like its closer to what was actually trying to be mocked based on the comment. I can keep it or leave it.
"
60762885,1095,granthenke,2016-04-22T16:03:00Z,"Will add the comment. 
"
60762972,1095,granthenke,2016-04-22T16:03:33Z,"Thats an interesting idea. Let me take a look and report back. 
"
60763255,1095,granthenke,2016-04-22T16:05:26Z,"This is challenging because we use the same class for both V0 and V1 responses. In the common case if you have the new version of the class you would be using V1 protocol, but thats not always true. In the case of new fields I tried to pick a safe default. Since V0 is not aware of internal topics I defaulted to false. Do you have thoughts on a better approach? 
"
60763550,1095,ijuma,2016-04-22T16:07:18Z,"Sorry for the confusion. If we don't need to use the older version, we can remove it. However, if we still need to use it, then it's better to keep a constructor for the older version instead of having a version parameter. That's based on Jun's previous reviews, I actually prefer the current approach as we don't need to call deprecated constructors. Anyway, if Gwen is fine with this, maybe we can leave as is.
"
60763729,1095,ijuma,2016-04-22T16:08:38Z,"Do we need to check `false` twice?
"
60763932,1095,ijuma,2016-04-22T16:10:07Z,"Shall we add a topic where the boolean is `true`?
"
60764088,1095,ijuma,2016-04-22T16:11:07Z,"Should we just pass `null` instead of empty list (like in `RequestResponseTest`)?
"
60764126,1095,granthenke,2016-04-22T16:11:23Z,"Whenever a broker id is used on the wire protocol, logically this class looks up the broker by id in the brokers list and represents it to the user as a node.  This is very similar logic to how replicas ids are handled in this class. I am not saying its my favorite, but I chose to maintain the existing pattern. 

The controller node is not used yet, but in KIP-4 it may be used to route messages.
"
60764164,1095,ijuma,2016-04-22T16:11:39Z,"Nitpick: no need for braces.
"
60764416,1095,ijuma,2016-04-22T16:13:41Z,"Not sure about that. The request classes shouldn't really have any logic IMO. They should just be a way to serialize/deserialize to the Kafka protocol. We could have a util method somewhere that does this. Or at least a static factory method (instead of constructor) in the request classes as a pragmatic compromise.
"
60764750,1095,granthenke,2016-04-22T16:16:09Z,"Its a bit of a gray area. We introduced the boolean to be explicit and be the single deciding factor for all topics. If we have this validation, its actually the topics list that really matters again and we essentially have 2 toggles to enable all topics.

When validating would we require null & true to get all topics and fail in all other cases in order to match closely to the wire protocols behavior?
"
60764789,1095,granthenke,2016-04-22T16:16:25Z,"Will fix.
"
60764921,1095,ijuma,2016-04-22T16:17:34Z,"We are using this `toString` in a number of log statement, not sure the rack information is that relevant (the fact that `toString` is used for both debug and more user-facing string is a pain). @hachikuji, you cleaned up the logs, what do you think?
"
60765137,1095,ijuma,2016-04-22T16:19:07Z,"OK, maybe add a comment in this case as it's not obvious.
"
60765466,1095,granthenke,2016-04-22T16:21:18Z,"I implemented it this way to match the existing behavior of StopReplica requests delete_partitions boolean. That way this type could be used there as well without a version bump, and all the ""booleans"" would behave the same. I planned to update StopReplicaRequest after this patch based on the decisions here. 

Here is the relevant code: https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/requests/StopReplicaRequest.java#L83
"
60765571,1095,granthenke,2016-04-22T16:22:00Z,"This class uses get for all other methods so I followed that. 
"
60765672,1095,granthenke,2016-04-22T16:22:56Z,"Apologies, copy and paste error. Its [KAFKA-3604](https://issues.apache.org/jira/browse/KAFKA-3604).
"
60765811,1095,granthenke,2016-04-22T16:24:02Z,"What message would you suggest? 
"
60766018,1095,granthenke,2016-04-22T16:25:38Z,"The idea is that this is the broker used with the metadata response. All objects tied to the metadata response are prefixed with METADATA. There are other broker objects in this file that I would not want to be confused.  
"
60766035,1095,granthenke,2016-04-22T16:25:43Z,"Will do.
"
60766571,1095,granthenke,2016-04-22T16:29:49Z,"These constructors are generally only used in the broker. I like passing in the version because then one constructor can handle all versions and the broker is guaranteed to send the correct version back without a bunch of if/switch logic.

There could be a case in the future where a separate constructor is required to maintain compatibility. But if I can avoid many constructors I would prefer it.
"
60766711,1095,granthenke,2016-04-22T16:31:05Z,"Sure. Will remove.
"
60766886,1095,ijuma,2016-04-22T16:32:03Z,"`boolean` is a keyword, that's why.
"
60766905,1095,granthenke,2016-04-22T16:32:18Z,"No. Will fix.
"
60767008,1095,granthenke,2016-04-22T16:33:08Z,"Sure we can.
"
60767033,1095,ijuma,2016-04-22T16:33:23Z,"I know what you mean, I did the same in a previous PR, but Jun made me change it. ;) As I said, let's see what Gwen thinks.
"
60767100,1095,granthenke,2016-04-22T16:33:56Z,"I can do that. I will include explanation in the version comments suggested earlier. 
"
60767154,1095,granthenke,2016-04-22T16:34:22Z,"I think It depends on the choice in earlier comments here: https://github.com/apache/kafka/pull/1095#discussion_r60764750
"
60767558,1095,granthenke,2016-04-22T16:37:27Z,"This is related to this discussion too: https://github.com/apache/kafka/pull/1095#discussion_r60764126

Right now some these classes have quite a bit of logic and don't expose the ""raw"" information. I agree that it would be nice to have some layering here where the logic is handled/exposed elsewhere. I don't want to impose to much change on existing logic in this patch though either. 
"
60767635,1095,granthenke,2016-04-22T16:38:04Z,"That too. 
"
60769396,1095,ijuma,2016-04-22T16:52:26Z,"This is a bit different. There are 3 levels, in a sense:
1. Wire protocol
2. Wrapping the wire protocol into domain model classes
3. Having business logic like what topics are internal in the request classes (or calling helper methods for that from within the request classes)

As we discussed previous @granthenke, I think we should really do `1`, but we do `2` in a number of places. This suggestion is `3`. In any case, I'm OK if we do it in a static factory method as a starting point (and we can move it elsewhere in the future in a separate PR maybe).
"
60770343,1095,SinghAsDev,2016-04-22T16:59:21Z,"Ok, thanks for the explanation.
"
60770562,1095,SinghAsDev,2016-04-22T17:00:59Z,"Something that would capture the following, need not be this verbose though.

> If one or more brokers does NOT have rack information
> For auto topic creation, AdminUtils.assignReplicasToBrokers will create the same assignment as the current implementation (as if no broker has the rack information) and continue with topic creation. This allows auto topic creation to work when doing rolling upgrade.
> For command line tools (TopicCommand and ReassignPartitionsCommand), an exception will be thrown. This will alert the user that a broker may be misconfigured. An additional command line argument --ignore-racks can be supplied to suppress such error and continue with topic creation ignoring all rack information.
"
60770887,1095,ijuma,2016-04-22T17:03:39Z,"Generally, I'm against adding unused methods until we actually need to use them. I agree that this may make sense here from a consistency perspective, but even then I would lean towards not having it. If we do have it, then we should use it from a test, at least.
"
60771090,1095,SinghAsDev,2016-04-22T17:05:24Z,"Yea, I was just curious about reasoning, not specifically for the method.
"
60771140,1095,ijuma,2016-04-22T17:05:55Z,"Legacy code is annoying. ;)
"
60776015,1095,ijuma,2016-04-22T17:42:03Z,"The idea is to avoid mistakes. So, if someone gets the boolean logic inverted and passes some topics, then the validation would find that.
"
60776076,1095,ijuma,2016-04-22T17:42:34Z,"By the way, I agree that this is a bit subjective.
"
60776472,1095,granthenke,2016-04-22T17:45:18Z,"I will play around with some options and run it by you. 
"
60776680,1095,granthenke,2016-04-22T17:46:41Z,"That information is useful for rack configuration on the broker. I am not sure if it needs to be in the protocol documentation for the metadata response. 
"
60776722,1095,ijuma,2016-04-22T17:46:58Z,"This method is the main hotspot when it comes to the performance of metadata requests, so we need to be careful about adding additional logic here. I should have added a comment saying that, maybe you could do that.

Under the assumption that unavailable nodes are rare, it seems like this change is safe.
"
60776919,1095,ijuma,2016-04-22T17:48:21Z,"Why don't we return `Option[Int]` here?
"
60776992,1095,ijuma,2016-04-22T17:48:51Z,"Is there a reason why we don't check that the value is the same as `NO_CONTROLLER_ID`?
"
60777018,1095,granthenke,2016-04-22T17:49:00Z,"This is used in `MetadataRequestTest.testControllerId`. I will look at keeping it around or not. If I do, I will make sure hasController is tested too.
"
60777028,1095,ijuma,2016-04-22T17:49:05Z,"Formatting nit: this could be on the previous line.
"
60777369,1095,ijuma,2016-04-22T17:51:32Z,"I agree that these changes (and also in the other file) are weird to include here.
"
60777637,1095,ijuma,2016-04-22T17:53:39Z,"Seems unnecessary since the superclass already defines it?
"
60778171,1095,gwenshap,2016-04-22T17:57:17Z,"I actually like the deprecated constructors because we get explicit compile warnings when we bump versions and don't accidentally forget to update some of the places where they are called. 

If you are not convinced that having warnings to help with bumps is useful, I won't make you change it ;)
"
60786052,1095,gwenshap,2016-04-22T18:51:22Z,"@granthenke has the full scope of KIP-4 though, if he knows it will be used, it is best to add it now (since the whole point was to add the lower-level APIs now)
"
60787278,1095,ijuma,2016-04-22T18:59:32Z,"This is an internal class, so methods can be added at any time though. The point was to update the wire protocol now as far as I know.
"
60788788,1095,gwenshap,2016-04-22T19:09:55Z,"I don't think it is internal? It isn't in an ""internals"" package... it is a public class in ""common"". AFAIK, this makes it public? Obviously it isn't as widely used as KafkaProducer, but my understanding was that anything not in a package called ""internals"" is public?
"
60789535,1095,ijuma,2016-04-22T19:14:58Z,"Today the only public classes are the ones we generate javadoc for (this was confirmed by Jay and Neha). We don't generate the javadoc for the request classes. As you know, I think this is very confusing and I want to change it so that we use  internals packages across the board.
"
60801036,1095,ijuma,2016-04-22T20:49:31Z,"This is doing the same thing as the `controllerId` line. I think you meant to get the controller id from `metadataResponse`.
"
60801175,1095,ijuma,2016-04-22T20:50:37Z,"Don't you mean to use `controllerServer2` and `metadataResponse2`?
"
60801224,1095,ijuma,2016-04-22T20:51:00Z,"Typo, `what's`.
"
60801286,1095,ijuma,2016-04-22T20:51:25Z,"Nitpicks: `()` not needed in `brokers` and `rack`.
"
60801527,1095,ijuma,2016-04-22T20:53:21Z,"There's a few other examples in this file.
"
60801878,1095,ijuma,2016-04-22T20:55:59Z,"Can we rely on the ordering of the metadata in the response and of the partition metadata?
"
60802099,1095,ijuma,2016-04-22T20:57:50Z,"Nitpick: for cases like this, I think it's more readable to have the opening brace after `=>`. For cases where the opening brace can replace the opening parenthesis, it makes sense to position the brace like this case. A bit subjective though.
"
60802287,1095,ijuma,2016-04-22T20:58:51Z,"Same question for a couple of other cases like this below.
"
60802721,1095,ijuma,2016-04-22T21:02:08Z,"How do we know that there are always 3 brokers? I guess the intent is that subclasses will use this in `generateConfigs` but that seems easy to miss. We could define this as a def and let the subclasses define it. Or we could implement `generateConfigs` here and call an abstract method `generateConfig(brokerId)` or something.
"
60802854,1095,ijuma,2016-04-22T21:03:19Z,"The 3 methods above seem to be used in many tests. Would it make sense to have a utility class or trait that people can mix-in?
"
60802895,1095,ijuma,2016-04-22T21:03:40Z,"Nitpick: `correlationId += 1`
"
60802936,1095,ijuma,2016-04-22T21:04:09Z,"This seems to be unused?
"
60809216,1095,gwenshap,2016-04-22T22:03:27Z,"Got it. Thanks for clarifying. If we can modify it at any time, there's really no point in including ""may need"" features.
"
60939173,1095,granthenke,2016-04-25T16:01:04Z,"It is unused, I just need to call parse to ensure it parses correctly and moves the buffer forward. I can remove the val assignment though. 
"
60940329,1095,granthenke,2016-04-25T16:08:16Z,"There is definitely a better way to share this test code across other tests. I would like to do this cleanup, but would you mind if I open a separate jira to track that and do it shortly after this patch?
"
60940596,1095,ijuma,2016-04-25T16:09:48Z,"Sounds good to me. Also, please add a comment as side-effects like that are not obvious.
"
60940760,1095,ijuma,2016-04-25T16:10:53Z,"Sure.
"
60943982,1095,granthenke,2016-04-25T16:30:16Z,"I moved the configuration to the BaseRequestTest and provided a way to override the properties in the subclass.
"
60944350,1095,granthenke,2016-04-25T16:32:36Z,"Since I am asking for only 1 topic and that topic has only 1 partition it shouldn't matter here. 
"
60946083,1095,granthenke,2016-04-25T16:44:19Z,"Since any negative value is invalid I map it to None. 
"
60946121,1095,granthenke,2016-04-25T16:44:37Z,"Yeah I should return the option here and use `MetadataResponse.NO_CONTROLLER_ID` KafkaApis.
"
60946477,1095,granthenke,2016-04-25T16:46:54Z,"I will add the comment. 
"
60948003,1095,granthenke,2016-04-25T16:55:56Z,"I have added comments that they only exist in v1+
"
60948570,1095,granthenke,2016-04-25T16:59:26Z,"@gwenshap I think it actually ends up being a bit of ""crying wolf"" since the brokers would be required to use the old constructors to support the old versions. We would throw, and likely suppress, the deprecation warnings in that case. I think unit tests are the best way to verify this behavior. I will work on increasing the version coverage on this patch and consider updating other apis at a later time. 
"
60951090,1095,granthenke,2016-04-25T17:15:52Z,"I would like to leave it for use in unit tests and also with the expectation that it will be used shortly. I will remove any method that is not being used or tested.
"
60963008,1095,granthenke,2016-04-25T18:26:59Z,"This can fall through because the constructor supports version 0 and 1. The version is passed as the last parameter. 

This is related to the discussion here: https://github.com/apache/kafka/pull/1095#discussion_r60758124
"
60963844,1095,gwenshap,2016-04-25T18:31:45Z,"Sounds good!
"
60963946,1095,ijuma,2016-04-25T18:32:19Z,"The issue with this approach is that we accept instead of failing when we receive invalid values. In some cases, this can lead to bugs being missed.
"
61162466,1095,ijuma,2016-04-26T20:54:58Z,"We can call `isAllTopics` on `request` and remove the comment about `null`
"
61163654,1095,ijuma,2016-04-26T21:01:33Z,"I think it's important to be clear about the expected usage of methods like this. Either we return a new instance or we mutate the passed instance. Given how `Properties` are generally used, isn't it better to just not return `Properties`?
"
61163744,1095,ijuma,2016-04-26T21:02:09Z,"Shall we add a helpful message via `getOrElse`?
"
61164103,1095,ijuma,2016-04-26T21:04:35Z,"the second argument to `assertEquals` could be `controllerId` right?
"
61164205,1095,ijuma,2016-04-26T21:05:17Z,"This is kind of weird, we should probably get the broker id from the config (ie `controllerServer.config.brokerId`) or add a method to `KafkaServer` that returns the broker id.
"
61164585,1095,ijuma,2016-04-26T21:07:39Z,"It's a bit weird that we assert that failover happened and then we wait until failover happens. Shouldn't the `waitUntilTrue` be before anything else?
"
61165412,1095,ijuma,2016-04-26T21:13:00Z,"We should also include an assert for the field we can use to know that a replica is down.
"
61178157,1095,granthenke,2016-04-26T22:50:00Z,"This is because the failover is basically immediate when looking up the servers directly via:

``` scala
servers.find(_.kafkaController.isActive()).get
```

But propagating that state to the MetadataCache and therefore the MetadataResponse could take a bit of time and requires the `TestUtils.waitUntilTrue`
"
61178707,1095,granthenke,2016-04-26T22:55:27Z,"Will add a check to confirm the downed broker is not in the brokers list
"
61178731,1095,granthenke,2016-04-26T22:55:41Z,"Will change to config
"
61182686,1095,ijuma,2016-04-26T23:36:29Z,"Thanks for the explanation.
"
341684130,7629,mjsax,2019-11-01T17:53:06Z,"`application`? The operator are named, not the application?"
341686787,7629,mjsax,2019-11-01T17:59:30Z,"Should this be a paragraph, ie, wrapped with `<p>...</p>` tags? Same below."
341687086,7629,mjsax,2019-11-01T18:00:12Z,"This is not the upgrade section, hence, I would remove the reference to `2.4` and the word `now`."
341687746,7629,mjsax,2019-11-01T18:01:54Z,"`prefixed` -> the ""suffix"" is ""prefixed"" -- quite hard to read. Also, are the details of the `number-suffix` format relevant?"
341689152,7629,mjsax,2019-11-01T18:05:23Z,"We should be a little bit more clear. At PAPI level, there are `Processors` and `StateStores` and people need to name the explicitly.

At the DSL, we have operators, and an operator may compile down to multiple `Processors, `StateStores`, and `repartition-topic`, and all those are name automatically, and there is a relationship between the processor-name and the store/changelog topic name and repartition topic names."
341694524,7629,JimGalasyn,2019-11-01T18:18:42Z,"```suggestion
             Kafka Streams allows you to <a href=""dsl-topology-naming.html"">name processors</a> created via the Streams DSL
```"
341694902,7629,JimGalasyn,2019-11-01T18:19:41Z,"```suggestion
		The number is prefixed with a varying number of ""0""s to create a suffix that is consistently 10 
```"
341695194,7629,JimGalasyn,2019-11-01T18:20:26Z,"```suggestion
		method, you can see what the processor is, but you don't have any context for its business purpose.  
```"
341695265,7629,mjsax,2019-11-01T18:20:38Z,nit: remove version reference 
341695340,7629,JimGalasyn,2019-11-01T18:20:50Z,"```suggestion
		Running <code>Topology#describe()</code> yields this string:
```"
341695374,7629,mjsax,2019-11-01T18:20:56Z,nit: remove `now`
341695619,7629,JimGalasyn,2019-11-01T18:21:36Z,"```suggestion
		 From this report, you can see what the different operators are, but what is the broader context here?  
```"
341695757,7629,JimGalasyn,2019-11-01T18:21:59Z,"```suggestion
		 For example, consider <code>KSTREAM-FILTER-0000000001</code>: you can see that it's a 
```"
341695907,7629,JimGalasyn,2019-11-01T18:22:23Z,"```suggestion
		 filter operation, which means that records are dropped that don't match the given predicate.  But what is 
```"
341696007,7629,JimGalasyn,2019-11-01T18:22:37Z,"```suggestion
		 the meaning of the predicate?  Additionally, you can see the topic names of the source and sink nodes, 
```"
341696110,7629,JimGalasyn,2019-11-01T18:22:50Z,"```suggestion
		 but what if the topics aren't named in a meaningful way?  Then you're left to guess the 
```"
341696288,7629,JimGalasyn,2019-11-01T18:23:16Z,"```suggestion
		 Also, notice the numbering here: the source node is suffixed with <code>0000000000</code>,
```"
341696372,7629,mjsax,2019-11-01T18:23:29Z,"Should this be formatted with ""single line per operator"" to make it easier to read (and to align with the formatting of the other code snippets?"
341696380,7629,JimGalasyn,2019-11-01T18:23:30Z,"```suggestion
		 The filter is suffixed with <code>0000000001</code>, indicating it's the second processor in 
```"
341696493,7629,JimGalasyn,2019-11-01T18:23:45Z,"```suggestion
		 the topology. In the 2.4 release of Kafka Streams, there are now overloaded methods for 
```"
341696657,7629,JimGalasyn,2019-11-01T18:24:10Z,"```suggestion
		 both <code>KStream</code> and <code>KTable</code> that accept
```"
341696731,7629,mjsax,2019-11-01T18:24:23Z,"Does this render correctly? I think, `<code>` does not remove ""indenting whitespaces"" and thus this would render the indention..."
341696750,7629,JimGalasyn,2019-11-01T18:24:25Z,"```suggestion
		 a new parameter <code>Named</code>. By using the <code>Named</code> class DSL users can 
```"
341696958,7629,JimGalasyn,2019-11-01T18:24:55Z,"```suggestion
		Now you can look at the topology description and easily understand what role each processor 
```"
341697057,7629,JimGalasyn,2019-11-01T18:25:09Z,"```suggestion
		plays in the topology. But there's another reason for naming your processor nodes when you 
```"
341697249,7629,JimGalasyn,2019-11-01T18:25:36Z,"```suggestion
		 Generated names are numbered where they are built in the topology.  
```"
341697439,7629,JimGalasyn,2019-11-01T18:26:04Z,"```suggestion
		 processor shifts, which shifts the name of the processor. Since <strong>most</strong> processors exist 
```"
341697558,7629,JimGalasyn,2019-11-01T18:26:22Z,"```suggestion
		 in memory only, this name shifting on topology changes presents no issue. But the name 
```"
341697674,7629,JimGalasyn,2019-11-01T18:26:38Z,"```suggestion
		 Here's a different topology with some state:
```"
341697905,7629,JimGalasyn,2019-11-01T18:27:10Z,"```suggestion
		  You can see from the topology description above that the state store is named 
```"
341698092,7629,JimGalasyn,2019-11-01T18:27:37Z,"```suggestion
		  <code>KSTREAM-AGGREGATE-STATE-STORE-0000000002</code>.  Here's what happens when you 
```"
341698239,7629,JimGalasyn,2019-11-01T18:27:57Z,"```suggestion
		  Notice that since you've added an operation before the <code>count</code> operation, the state 
```"
341698410,7629,JimGalasyn,2019-11-01T18:28:27Z,"```suggestion
		  store (and the changelog topic) names have changed. This name change means you can't 
```"
341698519,7629,JimGalasyn,2019-11-01T18:28:40Z,"```suggestion
		  do a rolling re-deployment of your updated topology. Also, you must use the  
```"
341698857,7629,JimGalasyn,2019-11-01T18:29:32Z,"```suggestion
		  to re-calculate the aggregations, because the changelog topic has changed on start-up, and the 
```"
341699061,7629,JimGalasyn,2019-11-01T18:30:02Z,"```suggestion
		  Fortunately, there's an easy solution to remedy this situation. Give a user-defined name 
```"
341699175,7629,JimGalasyn,2019-11-01T18:30:22Z,"```suggestion
		  to the state store, instead of relying on the generated one, so you don't have to worry about topology 
```"
341699433,7629,JimGalasyn,2019-11-01T18:30:59Z,"```suggestion
		  Now, even though you've added processors before your state store, the store name and its changelog 
```"
341699529,7629,JimGalasyn,2019-11-01T18:31:13Z,"```suggestion
		  topic names don't change. This makes your topology more robust and resilient to changes made by 
```"
341699688,7629,JimGalasyn,2019-11-01T18:31:34Z,"```suggestion
		 It's a good practice to name your processing nodes when using the DSL, and it's even
```"
341700141,7629,JimGalasyn,2019-11-01T18:32:37Z,"```suggestion
		 more important to do this when you have processing that remains when you restart your application - repartition
```"
341700566,7629,JimGalasyn,2019-11-01T18:33:41Z,"```suggestion
		 Here are a couple of points to remember when naming your DSL topology:
```"
341700677,7629,JimGalasyn,2019-11-01T18:33:58Z,"```suggestion
				If you have an <em>existing topology</em> and you <em>haven't</em> named your
```"
341700882,7629,JimGalasyn,2019-11-01T18:34:31Z,"```suggestion
				state stores (and changelog topics) and repartition topics, we recommended that you
```"
341701072,7629,JimGalasyn,2019-11-01T18:35:05Z,"```suggestion
				application instances, make the changes, and run the
```"
341701171,7629,JimGalasyn,2019-11-01T18:35:22Z,"```suggestion
				If you have a <em>new topology</em>, make sure you name the persistent parts of your topology
```"
341701333,7629,JimGalasyn,2019-11-01T18:35:46Z,"```suggestion
				: state stores (changelog topics) and repartition topics.  This way, when you deploy your
```"
341701524,7629,JimGalasyn,2019-11-01T18:36:15Z,"```suggestion
				application, you're protected from topology changes that break your Kafka Streams application.
```"
341701679,7629,JimGalasyn,2019-11-01T18:36:45Z,"```suggestion
				If you don't want to add names to transient processors at first, that's fine, because you can
```"
341701897,7629,JimGalasyn,2019-11-01T18:37:19Z,"```suggestion
		  Here's a quick reference on naming the critical parts of 
```"
341702007,7629,JimGalasyn,2019-11-01T18:37:35Z,"```suggestion
		  your Kafka Streams application to prevent topology name changes from breaking your application:
```"
341707445,7629,mjsax,2019-11-01T18:52:05Z,nit: `<emph>before</emph>` ?
341707858,7629,mjsax,2019-11-01T18:53:20Z,"`with the Joined, StreamJoined, or Grouped classed`"
341708595,7629,mjsax,2019-11-01T18:55:01Z,`even though you've added processors before your state store` -> `with or without the <code>filter<code> operation`
341709276,7629,mjsax,2019-11-01T18:56:52Z,"nit: `:` should be on line above `topolog:` -- otherwise it will be rendered as `topolog :` (ie, with a ws in between)"
341709757,7629,mjsax,2019-11-01T18:58:08Z,"For `KStream-KTable` joins, it's still `Joined`, right? So we need to add one line to the column?"
341710210,7629,mjsax,2019-11-01T18:59:16Z,For aggregations and KTables-KTable joins right? To distinguish the `KStream-KStream` join case?
341881771,7629,ableegoldman,2019-11-04T00:02:03Z,"How about ""Naming Operators in a Streams DSL application""?"
341881852,7629,ableegoldman,2019-11-04T00:03:23Z,nit: remove comma after 'DSL'
341882038,7629,ableegoldman,2019-11-04T00:07:16Z,"I agree it's not necessary to mention the details here, but I do think it's appropriate to briefly explain where the compatibility or ""name shifting"" issue comes from in the `Changing Names` section below"
341882125,7629,ableegoldman,2019-11-04T00:08:57Z,"Have you considered any alternative names for this section/trade-off? ""Cognitive Issues"" seems to have a weird connotation, what about just `Readability` or `Readability Problems`?"
341882260,7629,ableegoldman,2019-11-04T00:11:22Z,"> aren't named in a meaningful way

Are you referring to repartition topics, or users failing to name their topics meaningfully? We should definitive make it clear they are encouraged to name things meaningfully, and it seems weird to expect users to choose meaningful names for their operators but not for their topics"
341882322,7629,ableegoldman,2019-11-04T00:12:26Z,"```suggestion
		 Now let's take a look at your topology with all the processors named:
```"
341882442,7629,ableegoldman,2019-11-04T00:15:10Z,"I personally find it more readable when everything is in its own section, eg the section beginning ""But there's another reason..."" was moved to the `Changing Names` section whose topic it's actually referring to.  But feel free to leave as-is if you or anyone else disagrees"
341882738,7629,ableegoldman,2019-11-04T00:20:47Z,"I'm finding the comment about **most** processors existing in memory only a bit confusing -- maybe just use the term ""stateless"" instead? Also, I think it's a bit misleading to say that ""because most are stateless, this shifting presents no issue"" -- maybe rephrase to something like ""Many processors are stateless and therefore exist in memory only, so this name shifting on topology change presents no issue to applications built entirely of such operators."" ?"
341883192,7629,ableegoldman,2019-11-04T00:29:55Z,"""Give the state store a constant user-defined name instead of relying..."""
341883257,7629,ableegoldman,2019-11-04T00:31:12Z,nit: replace `--` with `such as` 
341883357,7629,ableegoldman,2019-11-04T00:33:07Z,"nit: `transient` --> `stateless` ? I think it's good to be consistent in our terminology throughout the docs -- it might be obvious to us that ""stateless"", ""in-memory"", and ""transient"" all refer to the same thing, but I think users will find this confusing."
342141293,7629,bbejeck,2019-11-04T16:24:57Z,ack
342150101,7629,bbejeck,2019-11-04T16:41:15Z,ack
342151782,7629,bbejeck,2019-11-04T16:44:25Z,"I'll clean this up some, but I'd prefer to leave this here to set the stage for the `Changing Names` section."
342186088,7629,bbejeck,2019-11-04T17:54:38Z,ack
342269064,7629,bbejeck,2019-11-04T21:04:28Z,ack
342270140,7629,bbejeck,2019-11-04T21:07:09Z,ack
342270257,7629,bbejeck,2019-11-04T21:07:28Z,ack
342271483,7629,bbejeck,2019-11-04T21:10:33Z,ack
342271670,7629,bbejeck,2019-11-04T21:11:03Z,"That's what I wanted, but is the indentation not correct?"
342271794,7629,bbejeck,2019-11-04T21:11:24Z,ack
342271936,7629,bbejeck,2019-11-04T21:11:45Z,ack
342272114,7629,bbejeck,2019-11-04T21:12:11Z,ack
342272560,7629,bbejeck,2019-11-04T21:13:14Z,ack
342272791,7629,bbejeck,2019-11-04T21:13:51Z,ack
342273114,7629,bbejeck,2019-11-04T21:14:45Z,ack
342273794,7629,bbejeck,2019-11-04T21:16:27Z,My point here is that most users won't have control over topic names.  They'll build a streams application to work with existing topics.
342274069,7629,bbejeck,2019-11-04T21:17:07Z,ack
342274140,7629,bbejeck,2019-11-04T21:17:17Z,ack
342274425,7629,bbejeck,2019-11-04T21:18:03Z,ack
342274497,7629,bbejeck,2019-11-04T21:18:14Z,ack
342274660,7629,bbejeck,2019-11-04T21:18:39Z,ack
342275213,7629,bbejeck,2019-11-04T21:19:53Z,ack
342275977,7629,bbejeck,2019-11-04T21:21:43Z,ack
342276210,7629,bbejeck,2019-11-04T21:22:18Z,ack
342276356,7629,bbejeck,2019-11-04T21:22:41Z,ack
342276577,7629,bbejeck,2019-11-04T21:23:13Z,ack
342658799,7629,bbejeck,2019-11-05T16:24:07Z,"I think it fits where I had it originally, but you are correct about it being in the same section as well.   If I kept it in the original location, I should at least repeat the information in the `Changing Names` section.  But I opted to take your suggestion and just move it there altogether"
342659633,7629,bbejeck,2019-11-05T16:25:33Z,ack
342659990,7629,bbejeck,2019-11-05T16:26:08Z,ack
342662764,7629,bbejeck,2019-11-05T16:30:36Z,ack
342664122,7629,bbejeck,2019-11-05T16:32:41Z,ack
342664196,7629,bbejeck,2019-11-05T16:32:50Z,ack
342665851,7629,bbejeck,2019-11-05T16:35:39Z,ack
342667608,7629,bbejeck,2019-11-05T16:38:49Z,ack
342668807,7629,bbejeck,2019-11-05T16:41:02Z,ack
342669710,7629,bbejeck,2019-11-05T16:42:28Z,ack
342670496,7629,bbejeck,2019-11-05T16:43:43Z,ack
342676589,7629,bbejeck,2019-11-05T16:54:24Z,"ack, I used something similar"
342676930,7629,bbejeck,2019-11-05T16:55:01Z,ack
342678646,7629,bbejeck,2019-11-05T16:58:12Z,ack
342680311,7629,bbejeck,2019-11-05T17:01:20Z,ack
342682536,7629,bbejeck,2019-11-05T17:05:33Z,ack
342683837,7629,bbejeck,2019-11-05T17:08:03Z,ack
342684647,7629,bbejeck,2019-11-05T17:09:43Z,ack
342685100,7629,bbejeck,2019-11-05T17:10:34Z,ack
342686001,7629,bbejeck,2019-11-05T17:12:14Z,ack
342686440,7629,bbejeck,2019-11-05T17:13:06Z,ack
342687361,7629,bbejeck,2019-11-05T17:14:50Z,ack
342687629,7629,bbejeck,2019-11-05T17:15:23Z,ack
342688024,7629,bbejeck,2019-11-05T17:16:06Z,ack
342688356,7629,bbejeck,2019-11-05T17:16:40Z,ack
342689620,7629,bbejeck,2019-11-05T17:19:10Z,ack with a slight tweek
342690110,7629,bbejeck,2019-11-05T17:20:11Z,good point
342690641,7629,bbejeck,2019-11-05T17:21:20Z,ack
342690865,7629,bbejeck,2019-11-05T17:21:42Z,ack
342691755,7629,bbejeck,2019-11-05T17:23:21Z,good catch!
342693339,7629,bbejeck,2019-11-05T17:26:23Z,ack
342694351,7629,bbejeck,2019-11-05T17:28:30Z,"I'd prefer to keep this as is, but if you have a strong opinion on this, I'll make the change."
342696400,7629,bbejeck,2019-11-05T17:32:41Z,ack
342783983,7629,ableegoldman,2019-11-05T20:34:33Z,"```suggestion
             <a class=""headerlink"" href=""#naming-a-streams-app"" title=""Permalink to this headline""><h2>Naming Operators in a Streams DSL application</h2></a>
```"
342784132,7629,ableegoldman,2019-11-05T20:34:56Z,"```suggestion
        <h1>Naming Operators in a Kafka Streams DSL Application<a class=""headerlink"" href=""#naming"" title=""Permalink to this headline""></a></h1>
```"
342784689,7629,ableegoldman,2019-11-05T20:36:23Z,"```suggestion
		   compile down to multiple <code>Processors</code> and <code>StateStores</code>, and
```"
342787455,7629,ableegoldman,2019-11-05T20:43:20Z,"Q: What do you mean by `the generated processor-name state store`? And which topic names, changelog or input/output? I'm not sure I follow this sentence"
342819399,7629,ableegoldman,2019-11-05T21:58:57Z,"prop: I still find this sentence confusing but if others disagree I'll hold my peace...but how about something like ""...this name shifting presents no issue for many topologies"" or ""for any stateless topologies"" ?"
342852018,7629,ableegoldman,2019-11-05T23:34:27Z,"prop: sentence reads a bit awkward, how about `...the state store (and changelog topic) names...` or `...the state store names (and changelog topics as well) have changed` ?"
342852119,7629,ableegoldman,2019-11-05T23:34:45Z,"```suggestion
		  do a rolling re-deployment of your updated topology.  Also, you must use the
```"
342852282,7629,ableegoldman,2019-11-05T23:35:20Z,"```suggestion
		  to re-calculate the aggregations, because the changelog topic has changed on start-up and the
```"
342852426,7629,ableegoldman,2019-11-05T23:35:56Z,"```suggestion
		  <code>StreamJoined</code>, and<code>Grouped</code> classes, and
```"
342852525,7629,ableegoldman,2019-11-05T23:36:18Z,"```suggestion
		  name state store and changelog topics with <code>Materialized</code>.
```"
342853431,7629,ableegoldman,2019-11-05T23:39:42Z,"```suggestion
		  Here's a quick reference on naming the critical parts of
```"
342854854,7629,ableegoldman,2019-11-05T23:44:54Z,"req: does it make sense for this section to go after the ""Testing a Streams Application"" section? I would keep it together with the other DSL operators
prop: do you think we should include a quick line about why you'd want to name things here? Just wondering if we should protect against people who won't want to read the full manual, and won't realize that naming is essential for compatibility"
344388199,7629,bbejeck,2019-11-08T22:16:06Z,"I think it's a bit subjective.  IMHO it should come after all the other operations are described as this is a ""meta-operation"".  At any rate,  I've moved it up closer, so it's the first item in the list after describing the other operations.   

I'm inclined to leave this as-is, it's just one click, and users can read the first paragraph. "
344393729,7629,bbejeck,2019-11-08T22:36:56Z,clarified some (I think). If it's still unclear I'll just remove it altogether.
344401859,7629,bbejeck,2019-11-08T23:12:02Z,updated
344403668,7629,bbejeck,2019-11-08T23:20:50Z,ack
344404824,7629,ableegoldman,2019-11-08T23:27:03Z,Fair enough
344405137,7629,ableegoldman,2019-11-08T23:28:37Z, This is a good point to drive home 
344406892,7629,ableegoldman,2019-11-08T23:38:02Z,"Hm. Is this roughly what you mean here: ""the processor name generated for a state store (and hence changelog topic name) will also be used in generating the repartition topic name"" or something to that affect? 
Or even replacing `generated processor-name state store` with `state store's generated processor name` or `generated processor-name of a state store`?"
344474564,7629,mjsax,2019-11-10T06:22:20Z,"This example has a similar issue: https://kafka.apache.org/23/documentation/streams/developer-guide/config-streams#rocksdb-config-setter

Note that the first block of the code example has a larger indention as the last two lines. If you compare with the docs files, the first block has whitespace: https://kafka.apache.org/23/documentation/streams/developer-guide/config-streams#rocksdb-config-setter

The last two lines don't have and render correctly: https://kafka.apache.org/23/documentation/streams/developer-guide/config-streams#rocksdb-config-setter

The example I picked uses `<pre>` tag but I think `<code>` work the same way.

"
344474629,7629,mjsax,2019-11-10T06:25:06Z,nit: remove whitespace in `<code> repartition...`
344474691,7629,mjsax,2019-11-10T06:27:05Z,"-> `processor names, state store names (and hence changelog topics names), and repartition topic names`

If you don't repeat `names` (note plural) it's hard to read -- and not `-` in `processor names`."
344474755,7629,mjsax,2019-11-10T06:29:58Z,"Why `But`? Maybe better, `Note, that the names of state stores and changelog/repartition topics are ""stateful"" while processor names are ""stateless"".`

I would not use `<em>` but put into quote -- a name is not really stateful, but I understand what you want to say. Again, no `-` in `processor names`"
344474790,7629,mjsax,2019-11-10T06:31:45Z,"I think, `<code>` tag has similar indentation issue as `<pre>` tag -- compare my other comment. (There are more `<code>` tags below -- won't comment on them but please fix all)"
344474827,7629,mjsax,2019-11-10T06:33:58Z,Missing `<p>` tag
344474843,7629,mjsax,2019-11-10T06:34:20Z,missing `</p>` tag
344474850,7629,mjsax,2019-11-10T06:34:31Z,missing `<p>` tag
344474854,7629,mjsax,2019-11-10T06:34:36Z,missing `</p>` tag
344474856,7629,mjsax,2019-11-10T06:34:44Z,missing `<p>` tag
344474861,7629,mjsax,2019-11-10T06:35:07Z,"missing `</p>` tag

(there seems to be more below; please fix)"
345833885,7629,bbejeck,2019-11-13T15:43:14Z,ack
345833978,7629,bbejeck,2019-11-13T15:43:23Z,ack
345834082,7629,bbejeck,2019-11-13T15:43:33Z,ack
345834190,7629,bbejeck,2019-11-13T15:43:43Z,ack
345834273,7629,bbejeck,2019-11-13T15:43:51Z,ack
345834384,7629,bbejeck,2019-11-13T15:44:01Z,ack
418108095,8589,abbccdda,2020-04-30T15:45:14Z,could we pass the members into the context?
420339593,8589,abbccdda,2020-05-05T19:03:29Z,Remove print statements
420340211,8589,abbccdda,2020-05-05T19:04:30Z,"Curious why we are still continuing in this case, as the member lookup already fails."
420341894,8589,abbccdda,2020-05-05T19:07:17Z,Could we just make members to be `Optional<Set<MemberToRemove>>` so that we don't need a separate removeAll parameter?
420344946,8589,abbccdda,2020-05-05T19:12:50Z,"style error here.

I would recommend doing a self style check like:
`./gradlew checkstyleMain checkstyleTest spotbugsMain spotbugsTest spotbugsScoverage compileTestJava` otherwise we still need to fix those failures after we do jenkins build."
420568190,8589,feyman2016,2020-05-06T06:23:45Z,"Thanks for the advice, will fix it in the next commit."
420806232,8589,feyman2016,2020-05-06T13:50:14Z,"Sure. Taking a step further, can we just keep the the type `Set<MemberToRemove>` for `members` unchanged and treat it as `removeAll` if the `members` is empty set?"
420824271,8589,feyman2016,2020-05-06T14:13:08Z,"My initial thought was to put the `members` in the context, but hesitated to do so because the `ConsumerGroupOperationContext` seems to be for generic usage. So I just refer to `KafkaAdminClient#getAlterConsumerGroupOffsetsCall` and make the members as a separate input param. Anyway, I'm glad to make the change if we think it's preferred to put the `members` in context."
420824572,8589,feyman2016,2020-05-06T14:13:32Z,Fixed~
420848223,8589,feyman2016,2020-05-06T14:43:57Z,"Thanks, will fix this ."
421216617,8589,feyman2016,2020-05-07T03:24:17Z,Fixed
421216746,8589,feyman2016,2020-05-07T03:24:43Z,Updated~
421216977,8589,feyman2016,2020-05-07T03:25:42Z,"I reran the self style check, but didn't capture any error. I assume the error would be the missed `final` in for loop, updated."
424198050,8589,abbccdda,2020-05-13T06:23:08Z,nit: space before `:`
424510483,8589,abbccdda,2020-05-13T15:04:03Z,"Yes, I feel this is more consistent for internal calls not to do a second round of interpretation for which `members` set to use."
424512993,8589,abbccdda,2020-05-13T15:07:20Z,nit: remove extra line
424516058,8589,abbccdda,2020-05-13T15:11:24Z,Should be Collection.emptyList()
424517564,8589,abbccdda,2020-05-13T15:13:24Z,"Why do we blindly put  `allMembers`? I believe we base on context to interpret, but like discussed earlier, this is easy to make mistake, we should rely on one source for members."
424518540,8589,abbccdda,2020-05-13T15:14:40Z,"And to be clear, I'm not suggesting we have to put stuff into the context, just always passing in the intended removal list and do not depend on `context.removeAll` again inside internal function."
424522056,8589,abbccdda,2020-05-13T15:19:18Z,Not necessary change
424522792,8589,abbccdda,2020-05-13T15:20:13Z,nit: space after `EMPTY_GROUP_INSTANCE_ID`
424524146,8589,abbccdda,2020-05-13T15:22:05Z,Could we specify the return type?
424525171,8589,abbccdda,2020-05-13T15:23:22Z,"I don't think we really need this struct, could we just put `null` in `groupInstanceSet`?"
424525436,8589,abbccdda,2020-05-13T15:23:44Z,nit: space
424525798,8589,abbccdda,2020-05-13T15:24:16Z,Why do we suppress here?
424527021,8589,abbccdda,2020-05-13T15:25:51Z,remained -> remaining
424529767,8589,abbccdda,2020-05-13T15:29:16Z,Do we also want to edit the `usage` info on top to mention the force delete option?
428579361,8589,feyman2016,2020-05-21T10:47:11Z,"I think so, updated"
428580805,8589,feyman2016,2020-05-21T10:50:37Z,Fixed~
428581576,8589,feyman2016,2020-05-21T10:52:32Z,Fixed
428583883,8589,feyman2016,2020-05-21T10:58:19Z,"Fixed, now we explicitly pass in the members to be deleted to the private `getRemoveMembersFromGroupCall`"
428585120,8589,feyman2016,2020-05-21T11:01:17Z,Reverted
428585373,8589,feyman2016,2020-05-21T11:01:53Z,Fixed
428585617,8589,feyman2016,2020-05-21T11:02:29Z,Refactored
428586730,8589,feyman2016,2020-05-21T11:05:08Z,"I feel like this is more informative, so didn't update it, but yeah, I can update if we really not prefer this~"
428587018,8589,feyman2016,2020-05-21T11:05:49Z,Fixed
428588172,8589,feyman2016,2020-05-21T11:08:27Z,Fixed
428589350,8589,feyman2016,2020-05-21T11:11:16Z,"Didn't change the exception handling logic here, just extract the Thread creation logic to reuse~ "
428745517,8589,abbccdda,2020-05-21T15:51:55Z,"I think we should catch `Exception` here:
https://stackoverflow.com/questions/2274102/difference-between-using-throwable-and-exception-in-a-try-catch
"
428748938,8589,abbccdda,2020-05-21T15:56:31Z,"This indentation is a bit weird, let's just merge L3625-3626"
428751049,8589,abbccdda,2020-05-21T16:00:08Z,Let's get back the original indentation.
428752600,8589,abbccdda,2020-05-21T16:02:44Z,nit: we could merge L3666-3667
428753012,8589,abbccdda,2020-05-21T16:03:26Z,nit: we could name it `members` now
428756738,8589,abbccdda,2020-05-21T16:09:52Z,I could see this doesn't hold true for a plain static member removal. Let's discuss why skipping the individual member check in `RemoveMembersFromConsumerGroupResult` makes sense over there.
428757676,8589,abbccdda,2020-05-21T16:11:25Z,Collections.emptySet() makes more sense since it is immutable.
428758681,8589,abbccdda,2020-05-21T16:13:16Z,"In `removeAll()` mode, why could we skip the individual member removal results? I guess although we don't need to verify against the original member list (because they don't exist for `removeAll`), going throw the sub error list is still valuable to make sure there is no unexpected failure."
428758798,8589,abbccdda,2020-05-21T16:13:27Z,Remove print statement.
428762047,8589,abbccdda,2020-05-21T16:19:03Z,"This test looks good, but it seems that we didn't test the case where some members get deleted successfully while some are not?"
428763784,8589,abbccdda,2020-05-21T16:22:10Z,"Should we check the member removal result here before proceeding? If that call failed, the whole operation should fail with error message containing the result IMHO."
428764977,8589,abbccdda,2020-05-21T16:24:15Z,Fair enough
428767771,8589,abbccdda,2020-05-21T16:28:56Z,Does this check duplicate L1103? Also I think it makes sense to check all the members' clientId as they should all equal to `testClientId`
428769255,8589,abbccdda,2020-05-21T16:31:25Z,"I prefer `testInstanceIdOne = ""test_instance_id_1""` and `testInstanceIdTwo = ""test_instance_id_2""`"
428770028,8589,abbccdda,2020-05-21T16:32:48Z,size - 1
428770176,8589,abbccdda,2020-05-21T16:33:09Z,We could remove this comment for now
428770601,8589,abbccdda,2020-05-21T16:33:50Z,"nit: format
I'm pretty surprised this wasn't caught in my previous template. Let me check how to cover this in style test as well."
428771346,8589,abbccdda,2020-05-21T16:35:08Z,"What does `"""" + ` mean?"
428772089,8589,abbccdda,2020-05-21T16:36:25Z,nit: parameters are not aligned.
428772936,8589,abbccdda,2020-05-21T16:37:52Z,"Like said earlier, I think we could just return
`return new StreamsResetter().run(parameters, cleanUpConfig) == 0`"
428773173,8589,abbccdda,2020-05-21T16:38:16Z,"We could add meta comment for the return value here, and instead of returning an exit code, I feel a boolean is suffice to indicate whether the clean operation was successful or not."
429070319,8589,feyman2016,2020-05-22T06:43:25Z,"Indeed, updated as suggested"
429070411,8589,feyman2016,2020-05-22T06:43:39Z,Updated
429071232,8589,feyman2016,2020-05-22T06:46:07Z,Fixed
429082038,8589,feyman2016,2020-05-22T07:16:49Z,"Without the `"""" +` to convert the value to String, we will get exception like: it is because `STREAMS_CONSUMER_TIMEOUT = 2000L`, `""""+` is widely used in this test, just follow it here without any change to not enlarge the scope of this PR, I can help to create a Jira to enhance it if we think this workaround is not quite intuitive~

```
org.apache.kafka.common.config.ConfigException: Invalid value 200000 for configuration session.timeout.ms: Expected value to be a 32-bit integer, but it was a java.lang.Long
        at org.apache.kafka.common.config.ConfigDef.parseType(ConfigDef.java:672)
        at org.apache.kafka.common.config.ConfigDef.parseValue(ConfigDef.java:474)
        at org.apache.kafka.common.config.ConfigDef.parse(ConfigDef.java:467)
        at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:108)
        at org.apache.kafka.common.config.AbstractConfig.<init>(AbstractConfig.java:129)
        at org.apache.kafka.clients.consumer.ConsumerConfig.<init>(ConsumerConfig.java:606)
        at org.apache.kafka.clients.consumer.KafkaConsumer.<init>(KafkaConsumer.java:630)
        at org.apache.kafka.streams.processor.internals.DefaultKafkaClientSupplier.getRestoreConsumer(DefaultKafkaClientSupplier.java:56)
        at org.apache.kafka.streams.processor.internals.StreamThread.create(StreamThread.java:313)
        at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:766)
        at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:652)
        at org.apache.kafka.streams.KafkaStreams.<init>(KafkaStreams.java:562)
        at org.apache.kafka.streams.integration.AbstractResetIntegrationTest.testResetWhenLongSessionTimeoutConfiguredWithForceOption(AbstractResetIntegrationTest.java:270)
        at org.apache.kafka.streams.integration.ResetIntegrationTest.testResetWhenLongSessionTimeoutConfiguredWithForceOption(ResetIntegrationTest.java:77)

``` "
429082487,8589,feyman2016,2020-05-22T07:18:01Z,"Thanks, but I wonder what does the **template** refer to here?"
429083564,8589,feyman2016,2020-05-22T07:20:46Z,"Removed, but curious about the reason :)"
429083611,8589,feyman2016,2020-05-22T07:20:54Z,Fixed
429083674,8589,feyman2016,2020-05-22T07:21:04Z,Fixed
429083960,8589,feyman2016,2020-05-22T07:21:45Z,"Thanks, all members' clientId are checked now"
429084219,8589,feyman2016,2020-05-22T07:22:22Z,"Agreed, fixed "
429084559,8589,feyman2016,2020-05-22T07:23:04Z,Good catch! Added the test for partial failure 
429084786,8589,feyman2016,2020-05-22T07:23:40Z,"Make sense, fixed"
429086933,8589,feyman2016,2020-05-22T07:28:59Z,"Yeah, just as you surmised, but you are right, we should scan the removal results as well. Slightly updated, followed the convention of `non-removeAll` scenario, just return with the first exception"
429087051,8589,feyman2016,2020-05-22T07:29:17Z,"Yeah, fixed"
429087133,8589,feyman2016,2020-05-22T07:29:27Z,Updated
429087216,8589,feyman2016,2020-05-22T07:29:38Z,Fixed
429087282,8589,feyman2016,2020-05-22T07:29:48Z,Fixed
429087492,8589,feyman2016,2020-05-22T07:30:17Z,"Make sense, fixed~"
429133442,8589,feyman2016,2020-05-22T09:11:18Z,Fixed
429145185,8589,feyman2016,2020-05-22T09:35:24Z,"For removing static members, this still true because we put memberId as `""""` in the request, and the server will also response with the same request field. (Verified `GroupCoordinator#handleLeaveGroup`)
For removing dynamic members, we need this change to know the memberId for the caller.
I suppose the `individual check` here is just to check the response against the members to be removed(for `removeAll` scenario)? Previously I thought of putting all members got from `KafkaAdminClient#getMembersFromGroup` in the RemoveMembersFromConsumerGroupResult for checking, but in `removeAll` scenario, we get members as  `MemberIdentity` which cannot be converted back to `MemberToRemove`, so I'm hesitate to do in this way"
429276600,8589,feyman2016,2020-05-22T14:22:23Z,Wrap to let the failed member info available for caller like `StreamsResetter`. Only capture the first found member error like in the non `removeAll` scenario.
429329313,8589,abbccdda,2020-05-22T15:57:04Z,nit: extra semi-colon
429333199,8589,abbccdda,2020-05-22T16:11:06Z,"Let's put the exception in the cause so that we could verify the cause in `KafkaAdminClientTest`, as:
```
if (exception != null) {
  result.completeExceptionally(new KafkaException(
 ""Encounter exception when trying to remove: "" + entry.getKey(), exception));
  return;
}
```"
429333355,8589,abbccdda,2020-05-22T16:11:21Z,"nit: we could set `""0""` to `JoinGroupRequest.UNKNOWN_MEMBER_ID` if we don't want to test it out. Having all members use the same member.id is a bit weird."
429333999,8589,abbccdda,2020-05-22T16:12:30Z,"nit: space after `*`. Also I feel we could make the context more concrete by:
```
When long session timeout has been configured, active members could take longer to get expired on the broker thus blocking the reset job to complete. Use the \""--force\"" option could remove those left-over members immediately. Make sure to stop all stream applications when this option is specified to avoid unexpected disruptions.
```"
429338025,8589,abbccdda,2020-05-22T16:20:11Z,"I see, this is indeed weird, please file a JIRA so that we could clean in a follow-up PR if others feel the same way."
429404004,8589,abbccdda,2020-05-22T18:50:06Z,This is no longer used.
429504184,8589,feyman2016,2020-05-23T01:57:09Z,"Cool, updated"
429504532,8589,feyman2016,2020-05-23T02:01:24Z,No existing help method to assert the cause of exception throw by `all()`. Also I think it's more straight forward in this way.
429504600,8589,feyman2016,2020-05-23T02:02:25Z,Removed
429504705,8589,feyman2016,2020-05-23T02:04:05Z,"Indeed, updated"
429504725,8589,feyman2016,2020-05-23T02:04:26Z,"Yeah, updated"
429507189,8589,feyman2016,2020-05-23T02:44:53Z,"Created https://issues.apache.org/jira/browse/KAFKA-10035 for tracking, thanks!"
430826643,8589,mjsax,2020-05-27T02:43:18Z,"```suggestion
                        + "" You can use option '--force' to remove active members from the group."");
```"
430826763,8589,mjsax,2020-05-27T02:43:48Z,"```suggestion
                        + ""Make sure to stop all running application instances before running the reset tool.""
```"
430827065,8589,mjsax,2020-05-27T02:45:05Z,"```suggestion
        forceOption = optionParser.accepts(""force"", ""Force the removal of members of the consumer group (intended to remove stopped members if a long session timeout was used). "" +
```"
430827165,8589,mjsax,2020-05-27T02:45:32Z,"```suggestion
                ""Make sure to shut down all stream applications when this option is specified to avoid unexpected rebalances."");
```"
430828662,8589,mjsax,2020-05-27T02:51:46Z,Why do we need this part? Seems sufficient to end the test here?
430828933,8589,mjsax,2020-05-27T02:52:49Z,"With `cleanGlobal` and `--force` the consumer group could be empty when `cleanGlobal` returns, right? Hence, we should do this assertion without timeout or retries?"
430830840,8589,mjsax,2020-05-27T03:00:33Z,"If `option.members()` is empty, it implies that we do a `removeAll()` -- hence, should we pass in `members` into the `RemoveMembersFromConsumerGroupResult` instead of `options.members()` ?"
430831033,8589,mjsax,2020-05-27T03:01:24Z,"nit: fix formatting:
```
private Call getRemoveMembersFromGroupCall(ConsumerGroupOperationContext<Map<MemberIdentity, Errors>, RemoveMembersFromConsumerGroupOptions> context,
                                           List<MemberIdentity> members) {
```"
430832010,8589,mjsax,2020-05-27T03:05:21Z,Not sure if I understand the change. Also not sure if I can follow the comments. Can you elaborate?
430832423,8589,mjsax,2020-05-27T03:07:09Z,"As we have different semantics for an empty collection (it was ""remove nothing"" originally, and we change it to ""remove all""), I am wondering if we should do a check if `members` is empty or not and throw an exception if empty? Or at least log a WARNING that empty implies ""remove all"" now?"
430833520,8589,mjsax,2020-05-27T03:12:09Z,Not sure why the `removeAll()` case needs to be handled differently? Can you elaborate?
430833928,8589,mjsax,2020-05-27T03:13:55Z,"Why that? I understand that we expect that users don't know the memberId if the so a ""remove all""; however, I don't see why we need to disallow this call? Can you elaborate?"
430834203,8589,mjsax,2020-05-27T03:15:16Z,"Nit: formatting
```
private static DescribeGroupsResponseData prepareDescribeGroupsResponseData(String groupId,
                                                                            List<String> groupInstances,
                                                                            List<TopicPartition> topicPartitions) {

```"
430835656,8589,mjsax,2020-05-27T03:21:36Z,nit: formatting: move `new NewTopic(...)` to next line
431247169,8589,feyman2016,2020-05-27T15:48:10Z,"This is to verify that after the `successfully force removal of active members`, the stream application re-run can send exactly the same records again to the output topics"
431266629,8589,feyman2016,2020-05-27T16:11:11Z,"--- If option.members() is empty, it implies that we do a removeAll()
=> Yes, that is correct.

--- hence, should we pass in members into the RemoveMembersFromConsumerGroupResult instead of options.members()
=> The members is of type `List<MemberIdentity>` and `MemberIdentity` contains field: `memberId` which supports the removal of dynamic members, while `options.members()` is of type: `Set<MemberToRemove>`, MemberToRemove only supports static member removal specification, in RemoveMembersFromConsumerGroupResult we treat similarly like in `RemoveMembersFromConsumerGroupOptions`, empty `members` implies `removeAll`,
we handle it in this way because we think in `non removeAll` scenario we would only remove static members, while in `removeAll` scenario we may remove both static and dynamic members."
431302352,8589,feyman2016,2020-05-27T17:02:53Z,"Because in non `removeAll` scenario, we have put the members to be deleted in the `RemoveMembersFromConsumerGroupResult#memberInfos`, while in the `removeAll` scenario, we don't do so(members to be deleted are decided in the private method: `KafkaAdminClient#getMembersFromGroup` of `KafkaAdminClient`). "
431305795,8589,feyman2016,2020-05-27T17:08:53Z,"Since in the `removeAll` scenario, we don't save the members to be deleted in `RemoveMembersFromConsumerGroupResult`,  so I think calling `memberResult` doesn't seem applicative."
431312462,8589,feyman2016,2020-05-27T17:20:23Z,Fixed
431313074,8589,feyman2016,2020-05-27T17:21:25Z,Make sense. It will throw exception if empty members provided now.
431313236,8589,feyman2016,2020-05-27T17:21:43Z,Fixed
431313242,8589,feyman2016,2020-05-27T17:21:44Z,Fixed
431322560,8589,mjsax,2020-05-27T17:37:06Z,"Seems redundant as tested somewhere else. And the purpose of the test is to verify `--force` itself. This additional checks have nothing to do with `--force` IMHO. It seems best to keep test to a ""minimum"". "
431323350,8589,mjsax,2020-05-27T17:38:31Z,Thanks for clarifying.
431327264,8589,mjsax,2020-05-27T17:45:07Z,"Well, while `memberInfo` is empty for the `removeAll` case, I am still wondering if the code for `removeAll` would not work for the other case, too?"
431329158,8589,mjsax,2020-05-27T17:48:14Z,I see. Makes sense.
431341731,8589,feyman2016,2020-05-27T18:04:07Z,"Yes, updated"
431345466,8589,feyman2016,2020-05-27T18:10:45Z,"I'm not sure I understand the question, could you elaborate more?"
431352788,8589,feyman2016,2020-05-27T18:23:50Z,"Yeah, I totally agree with: `It seems best to keep test to a ""minimum"".`
Not sure if my understanding is correct, but I still think the tests for `resetter` should compare the first run and re-run results, from the test's perspective, it cannot assume that `--force` option won't do something underneath that make the re-run produce different results. 
But I'm ok to remove the RE-RUN part if we do think it's redundant."
431354284,8589,mjsax,2020-05-27T18:26:19Z,"Can we just do for both cases?
```
for (Map.Entry<MemberIdentity, Errors> entry: memberErrors.entrySet()) {
    Exception exception = entry.getValue().exception();
    if (exception != null) {
        Throwable ex = new KafkaException(""Encounter exception when trying to remove: ""
             + entry.getKey(), exception);
        result.completeExceptionally(ex);
        return;
    }
}
```

The ""issue"" with using `memberInfos` is, that for the removeAll() case it's empty and we cannot use it. However, `memberErrors` should have an entry for all members for both cases?"
431355573,8589,mjsax,2020-05-27T18:28:43Z,Fair enough. Let's leave it as-is.
431372024,8589,feyman2016,2020-05-27T18:58:10Z,"I'm afraid not because, in the non `removeAll` scenario, caller specify the members(`memberInfos`) to be deleted, and according to `maybeCompleteExceptionally`, the `memberInfos` is used because it might sometimes happen that certain member in `memberInfos` cannot be found in `memberErrors `, that's the reason I didn't use the `removeAll` logic for all cases."
431373621,8589,mjsax,2020-05-27T19:00:58Z,Thanks for explaining!
199933294,5322,vvcephei,2018-07-03T19:59:28Z,"I think we did it this way on purpose, so we wouldn't automatically assume that later versions would have this data. But now that I'm looking at it again, it seems like this boolean expression will become silly. Also, the risk of breakage is low. If we choose not to include this stuff in later versions, it'll be pretty obvious that we have to put an upper bound on this condition.

So I think this change is good."
199934416,5322,vvcephei,2018-07-03T20:04:10Z,"I don't think we need to move this field. When you use it, you return immediately after assigning it, so you could just make it a local variable in that block for return.

Then the uninitialized field won't be in scope for everything else in this method."
199935554,5322,vvcephei,2018-07-03T20:08:46Z,"Maybe we can introduce a method to build this assignment and save some vertical space in this super-long method.

Then you could just return it, such as `return errorAssignment(clientsMetadata, errorCode)`.

In fact, we could ditch this variable entirely, and just return directly in all three spots we currently set it.
"
199938505,5322,vvcephei,2018-07-03T20:20:24Z,"It seems like it would be nice also to have a constant for the ""no error"" value (0).

I'm wondering if namespacing the error codes would be beneficial.

Minimally, we could prefix the constant like ""ERR_UNKNOWN_PARTITION"".

Or we could use an enum:
```
public enum Error {
        NONE(0),
        UNKNOWN_PARTITION(1);

        private final int code;

        Error(final int code) {
            this.code = code;
        }

        public int getCode() {
            return code;
        }
        
        public static Error fromCode(final int code) {
            switch (code) {
                case 0:
                    return NONE;
                case 1:
                    return UNKNOWN_PARTITION;
                default:
                    throw new IllegalArgumentException(""Unknown error code: "" + code);
            }
        }
    }
```

Then, we could encode with `out.writeInt(errCode.getCode());`
and decode with `assignmentInfo.errCode = Error.fromCode(in.readInt());`

Just an idea... what do you think?"
199939219,5322,vvcephei,2018-07-03T20:23:17Z,I guess we'll need one of these for version 4.
199940084,5322,vvcephei,2018-07-03T20:26:38Z,"It seems like this new constructor only supports the ""error assignment"" code path. Can we just inline it?

I admittedly didn't quite follow why we need this version check now."
199952165,5322,guozhangwang,2018-07-03T21:15:58Z,"This is a meta comment: I'd suggest having a separate check at the very beginning of `assign()`, after `Step Zero`, that for each entry value in `topicGroups = taskManager.builder().topicGroups()`, if each of its `TopicsInfo#sourceTopics` are either in `TopicsInfo#repartitionSourceTopics` or can be found in `metadata`. If the check fails we immediately falls back into the error case of 1) log an ERROR, and 2) set dummy assignment to all the clients with error code. 

Then in line 419 here we do not need this `do-while` loop, instead we should follow the sub-topology id ordering to assign num.partitions for repartition topics, and if it cannot be decided we will throw an runtime exception since it is not expected any more. In addition we can remove `NOT_AVAILABLE` as well."
199953001,5322,guozhangwang,2018-07-03T21:19:16Z,"+1, we can add an enum inside StreamsPartitionAssignor which can be extended in the future.

Also for this error case the name `UNKNOWN_PARTITION` is a bit confusing, I'd suggest we name it `INCOMPLETE_SOURCE_TOPIC_METADATA`.

And upon receiving this error code we should log an error that `some of the source topics ( + source topic lists) are not known yet during rebalance, please make sure they have been pre-created before starting the Streams application.`"
199953116,5322,guozhangwang,2018-07-03T21:19:44Z,"As mentioned in the JIRA ticket, we should log an ERROR that `some of the source topics ( + source topic lists) are not known yet during rebalance, please make sure they have been pre-created before starting the Streams application.`"
199953424,5322,guozhangwang,2018-07-03T21:20:57Z,nit: add empty line.
199955030,5322,guozhangwang,2018-07-03T21:27:16Z,"This is another meta comment, not related to this line: in `CopartitionedTopicsValidator` we should also update the logic accordingly, first of the

```
if (partitions == null) {
                        throw new org.apache.kafka.streams.errors.TopologyException(String.format(""%sTopic not found: %s"", logPrefix, topic));
                    }
```

Should never happen, since we would already fail before if the metadata is not complete, and the `NOT_AVAILABLE` case should not happen either (see my other comment).

Also note that there is a related bug fix PR for this JIRA long time ago about when the ensureCopartitioning should be called: https://github.com/apache/kafka/pull/2815/files, with this general change:

```
if (numPartitions == UNKNOWN) {
                for (final Map.Entry<String, InternalTopicMetadata> entry: allRepartitionTopicsNumPartitions.entrySet()) {
                    if (copartitionGroup.contains(entry.getKey())) {
                        final int partitions = entry.getValue().numPartitions;
                        if (partitions > numPartitions) {
                            numPartitions = partitions;
                        }
                    }
                }
            }
```

Should not happen either since all topic's num.partitions should be determined by then.

"
199955216,5322,guozhangwang,2018-07-03T21:28:03Z,"nit: add empty line between functions, ditto below."
199956438,5322,guozhangwang,2018-07-03T21:33:15Z,"Again this is another meta comment: the member decoding and handling leader's propagated assignment is in `onAssignment`, in which the we decode `AssignmentInfo` from `Assignment#userData`. In that function we should check the returned error code in `AssignmentInfo`, and if it is not NONE we should ""gracefully"" shutdown than just throwing a runtime exception: for example, we can set a flag indicating we need to error out, and then in `onPartitionsAssigned` callback we can check this flag and then decide to shutdown if necessary."
199971943,5322,tedyu,2018-07-03T23:01:43Z,"For onPartitionsAssigned, did you mean the method in StreamThread ?
The method takes Collection<TopicPartition>. Does this mean the flag should be added to TopicPartition ?"
199972239,5322,tedyu,2018-07-03T23:04:07Z,"Still need to figure out how to follow the sub-topology id ordering.
For now, I keep the do-while loop."
200792196,5322,guozhangwang,2018-07-06T23:32:18Z,"@vvcephei @tedyu The way `StreamThread` class and `StreamsPartitionAssginor` communicates today is bit weird: in order to break mutual dependency and keep the code cleaner, what we did is to pass in mutually needed modules as internal configs via the `StreamsConfig`, which `StreamsPartitionAssignor` will call `configure` on. The process works the following:

1. When StreamThread creates the consumer, it adds more objects into the properties map to the consumer.

2. Consumer client would create the instantiated `StreamsPartitionAssignor`, which will then call `configure` with the passed in properties. The reason is because of the way consumers instantiate their coordinator's `assignor` object today.

2.a) for example, StreamThread passed in the `TaskManager` object as `StreamsConfig.InternalConfig.TASK_MANAGER_FOR_PARTITION_ASSIGNOR`. The StreamsPartitionAssignor would then call its update functions to update the assigned tasks, which stream thread would then try to access in its own class. Another example is we pass in the version prob flag as an `AtomicBoolean` to be set / reset between these two classes as `StreamsConfig.InternalConfig.VERSION_PROBING_FLAG`.

So what I meant for a `flag` is to suggest doing the similar thing like the `AtomicBoolean prob-flag`, in which `StreamsPartitionAssignor` can set in its `onAssignment` function. The `onPartitionAssigned` function called within `StreamThread` can then check this flag.

For more details you can reference the current implementation pattern of the version probing flag"
200792492,5322,guozhangwang,2018-07-06T23:35:44Z,"@tedyu @vvcephei one way to break the while loop, is to rely on the 

```
Map<Integer, InternalTopologyBuilder.TopicsInfo> topicGroups
```

Note the `Integer` key is indeed the sub-topology id here, and since we sort the sub-topology by their ids, starting with `1`, the first sub-topology 1 should have no internal topics as its source topic. 

So we can start by this key ordering, to first determine any of the repartition topic's num.partitions as their sink topics of sub-topology 1, and then based on them as for the source topics of sub-topology 2, we can determine sub-topology 2's sink repartition topic's numb.partitions, and so on."
201814927,5322,guozhangwang,2018-07-11T19:36:10Z,It is simpler to just have an `encodeVersionFour` which does not change anything than `encodeVersionThree` but just put the different version. Note that within this release cycle we may introduce other format changes as well (so eventually the `encodeVersionFour` may be implemented differently anyways).
201815805,5322,guozhangwang,2018-07-11T19:39:33Z,"nit: rename to `__assignment.error.code__` and `ASSIGNMENT_ERROR_CODE`?

Another comment: thinking about this a bit more, maybe we can subsume the `VERSION_PROBING_FLAG` with the `ASSIGNMENT_ERROR_CODE`, as upon receiving the error code the member should be handling it separately on the error code, sometimes re-join the group with a down-graded encoding version, some time to shutdown, etc. With that we can generalize the handling logic."
201816222,5322,guozhangwang,2018-07-11T19:41:19Z,"For trouble shooting only: maybe we can still check that `!partitions.isEmpty()`, and if yes log an FATAL and throw runtime exception? If we had a bug that still causes `partitions.size() == 0`, then the line 83 below would silently skip assigning the maxNumPartitions update, which would be very hard to capture during debugging."
201818695,5322,guozhangwang,2018-07-11T19:50:35Z,"I think it is to not call `return` after that since line 272 below will return null and hence return anyways, plus it will log the final debug entry as well.

Also note that in the only other caller we do

```
for (final StreamThread thread : threads) {
                        thread.setStateListener(null);
                        thread.shutdown();
                    }
```

I.e. we need to set `thread.setStateListener(null);` since otherwise there may be deadlock issues. We need to do the same here."
201819967,5322,guozhangwang,2018-07-11T19:55:16Z,cc @mjsax wdyt?
201820822,5322,guozhangwang,2018-07-11T19:58:04Z,nit: comment line misaligned 
201821078,5322,guozhangwang,2018-07-11T19:59:02Z,"See my other comment: it's better just duplicate the logic of handling version 3 and version 4 information for now, as we may add new info for version 4 soon which would make the handling logic different."
201824357,5322,guozhangwang,2018-07-11T20:12:12Z,"Same as above, let's add a FATAL error and throw a runtime exception like IllegalStateException."
201824624,5322,guozhangwang,2018-07-11T20:13:10Z,This seems not used.
201825636,5322,guozhangwang,2018-07-11T20:16:53Z,nit: empty line.
201825651,5322,guozhangwang,2018-07-11T20:16:55Z,nit: align parameters.
201825785,5322,guozhangwang,2018-07-11T20:17:27Z,We can remove the code block line 82-85 above since it will be called here.
201826312,5322,guozhangwang,2018-07-11T20:19:19Z,"Ditto, I'd suggest just duplicating the code since we may add more logic for version 4 anyways."
201826496,5322,guozhangwang,2018-07-11T20:19:57Z,nit: latestSupportedVersion
201826570,5322,guozhangwang,2018-07-11T20:20:13Z,nit: align parameters.
201826760,5322,guozhangwang,2018-07-11T20:20:55Z,Ditto. Let's just add an `encodeVersionFour` with duplicated logic except the version.
201843313,5322,tedyu,2018-07-11T21:21:47Z,"Version probing is a boolean flag.
Assignment error code is int.
If we unify these two, we need to encode version probing.

BTW version probing doesn't imply assignment error."
201850915,5322,tedyu,2018-07-11T21:49:45Z,"Unfortunately no.
The `this` call goes to line 100 where this is no such check."
201851272,5322,tedyu,2018-07-11T21:51:16Z,Addition to version 4 can be added at the end of `encodeVersionFour`
201851485,5322,tedyu,2018-07-11T21:52:12Z,I did that first - resulting in duplicate local variable.
202841909,5322,guozhangwang,2018-07-16T22:15:37Z,"Yes, my intention is to use another error code value (seems you've already done it as `VERSION_PROBING`) to replace the flag, and then after onAssignment is called we would check the error code and if it is not `NONE` react accordingly, it may be simply shutdown and stop-the-world, or other actions like down-grade."
202842440,5322,guozhangwang,2018-07-16T22:18:13Z,I see --- we can define latestSupportedVersion before the switch not as a `final` int then?
202855945,5322,guozhangwang,2018-07-16T23:29:29Z,"If we are adding this function in `KafkaStreams` then we do need a KIP.. but I was thinking if we can just add this in `StreamThread`, which is an internal class and hence doing so does not need a KIP. We can, instead, add a `KafkaStreamsWrapper` (it is similar to `TopologyWrapper` and `TopologyTestDriverWrapper` to allow unit test code to access their internal private fields) which can access the threads and globalThread, and then adds this function in this class to manipulate them.

Also note that StreamThread already has a state change listener `final class StreamStateListener implements StreamThread.StateListener` so our `StateListenerStub` should not completely replace its logic. Instead we can extend that listener to `StateListenerStub` which does the state change tracing in additional to the necessary logic."
202856322,5322,guozhangwang,2018-07-16T23:31:58Z,"This condition seems not right: we should not shutdown if the error was `VERSION_PROBING`, right? I.e we should firstly check if the error was `NONE`, and if not, switch branch on the actual error code to handle them accordingly."
202856380,5322,guozhangwang,2018-07-16T23:32:17Z,"Similarly, here we would only create tasks if the error was `NONE`."
202856437,5322,guozhangwang,2018-07-16T23:32:39Z,case 2 is missing.
202856560,5322,guozhangwang,2018-07-16T23:33:24Z,ping @mjsax again.
202857024,5322,guozhangwang,2018-07-16T23:36:20Z,"As we are merging the two scenarios to use the error code, we should let the leader to set the error code in the version probing case as well as setting the `receivedAssignmentMetadataVersion` in the assignment. And then we only need to do the logic in line 796 above, and do not need to set it in line 827 any more."
202857302,5322,guozhangwang,2018-07-16T23:38:05Z,We should not use a `TopologyException` here any more since `TopologyException` should be used for DSL statement parsing only. Instead we could just throw an IllegalStateException since it should never be expected (i.e. if the metadata is indeed not known we should error out with the error assignment earlier and never reach this line).
202857594,5322,guozhangwang,2018-07-16T23:39:56Z,Adding a parameter `version` for `encodeVersionThree` is very confusing to other readers. I'd suggest completely duplicate the code in `encodeVersionFour` and remove this parameter in `encodeVersionThree`.
202857927,5322,guozhangwang,2018-07-16T23:42:01Z,"Thanks for adding this integration test! It looks reasonable, but we'd generally adding integration Java test under `org.apache.kafka.streams.integration`, not `org.apache.kafka.streams.scala` (the latter is only for the scala API only)."
202858371,5322,guozhangwang,2018-07-16T23:44:49Z,"And also the title `testShouldCountClicksPerRegionWithMissingTopic` is confusing, it should be `shouldShutdownWithMissingTopic` right? And note this class is for `StreamToTableJoinScalaIntegrationTestImplicitSerdes`, so not the right class to add this test. I'd suggest adding a new test class under the above mentioned package for this test case, like `AssignmentErrorHandlingIntegrationTest`, and this test case be `shouldAutoShutdownOnIncompleteMetadata`."
202864203,5322,tedyu,2018-07-17T00:23:59Z,"Pardon. Can you explain in a bit more detail ?
I am not sure how leader sets the error code for version probing if not done on line 827."
202867018,5322,tedyu,2018-07-17T00:45:35Z,"I took a look at streams/src/test/java/org/apache/kafka/streams/integration/StreamTableJoinIntegrationTest.java where cluster is not involved.
If I move the new test there, a lot of Scala code for setting up the testing environment would be repeated.

It seems more intuitive if the new integration test is added in this test class in terms of code reuse."
202868232,5322,tedyu,2018-07-17T00:55:42Z,"Please confirm: threads and state fields of KafkaStreams can be changed to protected.
Otherwise the new method in KafkaStreamsWrapper still cannot access them."
202901382,5322,guozhangwang,2018-07-17T05:56:17Z,"Yes, they can."
202901958,5322,guozhangwang,2018-07-17T06:00:28Z,"Currently the version probing works as the following:

1. when leader receives the subscription info encoded with a higher version that it can understand (e.g. the leader is on version 3, while one of the subscription received is encode with version 4), it will send back an empty assignment with the assignment encoded with version 3, and also `latestSupportedVersion` set to 3.

2. when the member receives the assignment, it checks if `latestSupportedVersion` is smaller than the version it used for encoding the sent subscription (i.e. the above logic). If it is smaller, then it means that leader cannot understand, in this case, version 4. It will then set the flag and then re-subscribe but with a down-graded encoding format of version 3.

NOW with your PR, we can let leader to clearly communicate this error via the error code, and upon receiving the assignment, if the error code is `VERSION_PROBING`, then the member can immediately know what happens, and hence can simplify the above logic. Does that make sense? also cc @mjsax "
202902176,5322,guozhangwang,2018-07-17T06:01:51Z,"I understand the code duplication, but still adding a test case that has nothing to do with `StreamToTableJoinScalaIntegrationTestImplicitSerdes` is not recommended. I'd still suggest making a new class and duplicate the setup code a bit."
202932846,5322,tedyu,2018-07-17T08:22:31Z,"In the example given above, the gap in subscription info versions between leader and the member is 1.
Is the expectation that when the gap is > 1, at least one round trip is reduced for version probing compared to the existing implementation ?

The version probing error code currently is hard coded and not correlated with the actual gap.

I wonder if the optimization can be done in another JIRA."
203204145,5322,mjsax,2018-07-17T22:41:10Z,"IIRC, the idea was to be as explicit as possible and list out the versions, in case a future version does not encode `partitionsByHostState` any longer. The risk of the change is small though. I am ok with it."
203204413,5322,mjsax,2018-07-17T22:42:43Z,"nit: code formatting and missing `final`:
```
Map<String, Assignment> errorAssignment(final Map<UUID, ClientMetadata> clientsMetadata,
                                        final String topic,
                                        final int errorCode) {
```"
203204472,5322,mjsax,2018-07-17T22:42:57Z,nit: add `final`
203204612,5322,mjsax,2018-07-17T22:43:41Z,nit: use `{}` instead of string concatenation
203204975,5322,mjsax,2018-07-17T22:45:43Z,Not sure if calling `shutdown()` directly is the best way? Shouldn't we just `return` and break the loop in `StreamThread#runLoop()` ?
203205168,5322,mjsax,2018-07-17T22:46:38Z,nit: use `{}` instead of string concatenation
203205249,5322,mjsax,2018-07-17T22:47:08Z,nit: add `final`
203205508,5322,mjsax,2018-07-17T22:48:23Z,why remove `final` ?
203205842,5322,mjsax,2018-07-17T22:49:59Z,"`processVersionThreeAssignment` -> `processVersionFourAssignment`

If both are identical, it's ok to call `processVersionThreeAssignment()` from within `processVersionFourAssignment()` IMHO, but adding a `processVersionFourAssignment()` seems to be cleaner to me."
203206140,5322,mjsax,2018-07-17T22:51:46Z,`final` ?
203206482,5322,mjsax,2018-07-17T22:53:30Z,Do we need this new constructor? Can the existing one not just be extended with `errorCode`?
203206896,5322,mjsax,2018-07-17T22:55:50Z,nit: add `final`
203206912,5322,mjsax,2018-07-17T22:55:55Z,nit: add `final`
203206951,5322,mjsax,2018-07-17T22:56:07Z,nit: indention
203207194,5322,mjsax,2018-07-17T22:57:30Z,why do you remove this test?
203207828,5322,guozhangwang,2018-07-17T23:01:17Z,"Hmm.. I'm not sure if this is the right fix, but maybe upgrading the test when we bump up version is also out side the scope of this PR itself. I'll let @mjsax to take a look and decide how can we fix forward the upgrade-test."
203208753,5322,mjsax,2018-07-17T23:06:40Z,"If we bump the version, we need to update this to 5 and 4 as already done.

We should also make this more generic and test upgrades from 3 -> 4, 3 -> 5 and 4 -> 5. The current code does only go from latest version to future version. However, generalizing the test should be out of scope of this PR and just changing the expected numbers should be fine for this PR."
203209970,5322,tedyu,2018-07-17T23:13:46Z,"The type of the version probing flag has changed from boolean to integer.
There is another subtest for checking the error code."
203210372,5322,tedyu,2018-07-17T23:16:16Z,This was suggested by Guozhang and I tend to agree with calling shutdown()
203216673,5322,mjsax,2018-07-17T23:55:19Z,"I understand that. Thus, this test should be updated to `shouldThrowKafkaExceptionIfVersionProbingFlagConfigIsNotAtomicInteger` -- it tests the data type, ie, the cast operation."
203216813,5322,mjsax,2018-07-17T23:56:01Z,@guozhangwang What is the reasoning behind this?
203248963,5322,guozhangwang,2018-07-18T04:35:43Z,"Copying my response from the email thread:

```
I wanted the integration test in the same PR since I'm concerned about the call trace of `streamThread.shutdown()` in the onPartitionAssigned callback, because today we only call that in the main loop of the thread, I'm concerned if there are any race condition that may cause deadlocks.. The integration test should not actually be hard to add, you can take a look at the integration test folder and let me know if you have any questions.
```

Currently I cannot think of a race condition that calling `shutdown` in the callback would introduce than calling shutdown in the main loop, but I'm not 100% sure, so I insisted on triggering a system test."
203542445,5322,mjsax,2018-07-18T21:54:19Z,"This should never happen, right? Thus, I am wondering if we should throw an `IllegalStateException` instead?"
203542534,5322,mjsax,2018-07-18T21:54:38Z,nit: add `{ }` to then-block
203542781,5322,mjsax,2018-07-18T21:55:50Z,Seems this slipped in the last update.
203543587,5322,mjsax,2018-07-18T21:58:57Z,"Ack.

We might still want to add a `return` to make clear it's an early exit. Of course, the `if` below evaluate to `false` anyway, however, it makes the code more readable IMHO."
203544388,5322,mjsax,2018-07-18T22:02:10Z,nit: we usually omit `get` prefix for all getter method. Please update to `code()` to align with common naming conventions.
203544506,5322,mjsax,2018-07-18T22:02:35Z,comment can be omitted
203547299,5322,mjsax,2018-07-18T22:15:20Z,"@guozhangwang For future version this might work. However, if we upgrade from 2.0 to 2.1 with version bump from 3 -> 4, the old leader is on version 3 and cannot encode the version probing via the error flag.

As we are stuck with older version 3 metadata, I am not sure if we gain a lot if we change the logic, as we still need the current code anyway."
203547399,5322,mjsax,2018-07-18T22:15:50Z,nit: fix indention
203548013,5322,mjsax,2018-07-18T22:18:58Z,why do we move this up here? `topics` is only used when an exception is thrown (or did I miss anything)?
203548143,5322,mjsax,2018-07-18T22:19:35Z,did this slip?
203548277,5322,mjsax,2018-07-18T22:20:13Z,Any comments? I would like to keep the number of constructors small if possible.
203548421,5322,mjsax,2018-07-18T22:20:52Z,Nit: rename to `errorCode` ? We try to avoid abbriviations
203548476,5322,mjsax,2018-07-18T22:21:06Z,nit: rename `errorCode()`
203548586,5322,mjsax,2018-07-18T22:21:40Z,nit: add empty line
203548969,5322,mjsax,2018-07-18T22:23:18Z,"add `getVersionFourByteLength()` ?

Or rename method to `getVersionThreeAndFourByteLength()` ?"
203549210,5322,mjsax,2018-07-18T22:24:16Z,nit: fix indention
203549314,5322,mjsax,2018-07-18T22:24:47Z,nit: add empty line
203549359,5322,mjsax,2018-07-18T22:25:01Z,seems this slipped
203549395,5322,mjsax,2018-07-18T22:25:11Z,seems this slipped
203549533,5322,mjsax,2018-07-18T22:25:49Z,seems this slipped
203549904,5322,mjsax,2018-07-18T22:27:14Z,Should we change the test to expect an exception instead of removing it?
203550235,5322,mjsax,2018-07-18T22:28:20Z,"as above: should we check that an exception is thrown? (we had ""infinite loop"" bugs in the past -- those tests seems to be valuable)"
203550652,5322,mjsax,2018-07-18T22:29:45Z,seems this can be removed?
203550712,5322,mjsax,2018-07-18T22:29:59Z,"seems, this can be removed?"
203555179,5322,mjsax,2018-07-18T22:49:12Z,Why is this change required? It seems we forgot to update `AssignmentInfo#equals()` and `AssignmentInfo#hashCode()`...
203555598,5322,mjsax,2018-07-18T22:51:27Z,How does this test relate to the change?
203555659,5322,mjsax,2018-07-18T22:51:52Z,As above? Why do we change this test?
203556910,5322,tedyu,2018-07-18T22:58:08Z,"Right, this code shouldn't be reached."
203557121,5322,tedyu,2018-07-18T22:59:13Z,There is no then-block - I guess you mean the if-block.
203559036,5322,tedyu,2018-07-18T23:09:57Z,I want to mention versionProbingFlag just in case some developer who knew the flag comes wondering what happened to the flag :-)
203559379,5322,mjsax,2018-07-18T23:11:30Z,"Terminology is fun... it's and if-then-else statement -- there is no `then` keyword, but still and then-block -- interesting that you call it if-block :) "
203559593,5322,mjsax,2018-07-18T23:12:51Z,`git blame` is their friend :) -- that's why there is a commit history. Allows us to keep the code base clean :)
203563364,5322,tedyu,2018-07-18T23:34:18Z,Right.
203564178,5322,tedyu,2018-07-18T23:39:04Z,StreamToTableJoinScalaIntegrationTestBase is created for reusing cluster setup code between existing test and new integration test.
203564273,5322,tedyu,2018-07-18T23:39:37Z,StreamToTableJoinScalaIntegrationTestImplicitSerdes is changed to preserve non-cluster setup test code.
203564445,5322,tedyu,2018-07-18T23:40:44Z,I see - yeah I call it if block.
203807596,5322,mjsax,2018-07-19T17:21:41Z,"As mentioned in a previous comment, we need to update `hashCode()` and `equals()`."
203829239,5322,mjsax,2018-07-19T18:31:54Z,"I understand your other test changes now. However, I am wondering why we add this test to the scala module in the first place? Kafka Streams is written in Java, and we should write all tests in Java, too.

The scala module is just a thin language wrapper on top, and integration tests in the scala module should only test the scala/java integration, but not core functionality. This test belongs to `stream/src/test/java/org/apache/kafka/stream/integration`

Thus, my argument is not really about java vs scala, but putting test into the scala wrapper module scatters our test code across two modules and we should not do this."
1137856849,13391,jolshan,2023-03-15T23:01:21Z,bug here -- we don't want to clear non-inflight nodes.
1137901882,13391,jolshan,2023-03-16T00:00:27Z,I have a fix I will push with the rest of the tests
1142707455,13391,artemlivshits,2023-03-20T21:44:34Z,"Looks like this could be called from multiple threads, do we need to add synchronization?"
1142722842,13391,artemlivshits,2023-03-20T22:07:29Z," Would it be possible to have a retry (say first request timed out, and then we send another one) and have more than one request?"
1142729789,13391,artemlivshits,2023-03-20T22:17:58Z,We could use getOrElseUpdate.
1142747856,13391,artemlivshits,2023-03-20T22:48:21Z,"If the request is already in flight, looks like we wouldn't be able to detect and reject a stale request here.  Is it needed for correctness?  If yes, we need to fix that, if not, I'd propose to remove this logic and just properly handle stale epoch when it gets to transaction coordinator."
1142753293,13391,artemlivshits,2023-03-20T22:58:00Z,inflightNodes seem to be accessed only by the inter-broker send thread so synchronization is not actually needed.
1142758001,13391,artemlivshits,2023-03-20T23:06:56Z,"Shouldn't it get cleared automatically once it gets out of scope?  If there is a tricky consideration, let's add a comment."
1142762452,13391,artemlivshits,2023-03-20T23:15:37Z,"Is this client going to be used only for AddPartitionsToTxnManager or some other inter-broker communication (in the future) as well?  If it the former, we should make name more specific."
1142786061,13391,jolshan,2023-03-21T00:08:15Z,"I thought about that, but I was concerned about blocking on a single produce request too long. I though maybe the producer's retry mechanism would be enough to handle this."
1142787086,13391,jolshan,2023-03-21T00:10:10Z,"I think it's ok to have new data when a request is inflight. 
The issue is that I have an invariant here that we can only have one queued item for a given txn ID at a time. This is due to how the information is stored in the map. 

The only time we can receive two requests from the same txn id is when the producer restarts and the epoch is bumped. That is why I have this logic here. "
1142787356,13391,jolshan,2023-03-21T00:10:48Z,I guess I was considering more than one send thread  I guess we don't have that now.
1142788144,13391,jolshan,2023-03-21T00:12:38Z,We are reusing this for each transactional ID and it remains in scope at this time. 
1142788313,13391,jolshan,2023-03-21T00:13:00Z,we can make it more specific.
1144090231,13391,junrao,2023-03-21T23:38:45Z,Does the TODO still need to be addressed?
1144092204,13391,junrao,2023-03-21T23:41:57Z,Could we add a new line after?
1144106075,13391,junrao,2023-03-22T00:13:25Z,Could this be private?
1145103445,13391,junrao,2023-03-22T16:26:10Z,"I am wondering why we need to do this in a request thread. For example, TransactionMarkerRequestCompletionHandler already appends to the log in a separate thread.
"
1145118797,13391,junrao,2023-03-22T16:36:44Z,We already have a TransactionMarkerChannelManager for TXN coordinator to send requests to brokers. Could we reuse that for sending requests from brokers to TXN coordinators? We probably don't want too many separate threads for exchanging requests among brokers.
1145376271,13391,jolshan,2023-03-22T20:30:39Z,We are sending from the partition leader to the txn coordinator. Can we still use that same thread? I think the usage is different right?
1145376651,13391,jolshan,2023-03-22T20:31:03Z,@artemlivshits thought this should be done. Perhaps he can explain better than me.
1145453181,13391,junrao,2023-03-22T22:01:40Z,"That's true. However, if you look at TransactionMarkerChannelManager, the main API is `addMarkersForBroker`. So at that level, it's just sending some requests to another broker. In AddPartitionsToTxnManager, its main API is `addTxnData`. Again, it's just sending some requests to another broker. Instead of creating more and more of those specialized broker-to-broker communication channels, it may be better to consolidate them into a single general framework."
1145463265,13391,jolshan,2023-03-22T22:15:52Z,"I thought about this, but the trouble is how each request is built slightly differs. Some of them pass the node they are sending to when sending adding the request (like this one) and some do the calculation right before (ie, controller channel manager)

I can take a look at the transaction channel marker channel manager and see how it handles it. But I think the other benefit for this class is keeping all the add partitions logic together. Perhaps there is a way to consolidate this, but also use the same thread/channel?"
1145487679,13391,junrao,2023-03-22T22:55:39Z,"Yes, I was wondering if there is a way to reuse the thread and the channel for broker to broker communication for all low volume requests."
1145533272,13391,jolshan,2023-03-22T23:54:53Z,I can look into this. Do we also want to handle the callback on that same thread still?
1145545834,13391,junrao,2023-03-23T00:21:10Z,"Yes, that's what I was wondering. TransactionMarkerChannelManager writes the final commit to the log in the callback of sending the marker to the broker. I am wondering if we could just do the same here."
1145549423,13391,jolshan,2023-03-23T00:30:05Z,"I think @artemlivshits is the best to answer this. He was the one who said we should do this and although I vaguely remember the explanation, he will do a better job explaining."
1146789680,13391,YiDing-Duke,2023-03-23T20:24:15Z,"if 1st request timeout, the second one cannot hit this stage unless 1st one is done due to connection muted?"
1146821982,13391,jolshan,2023-03-23T20:46:16Z,"Yi is correct. I also think if we hit timeout, it is the epoch bump case I mentioned before."
1146944915,13391,junrao,2023-03-23T22:36:19Z,"Chatted with Artem offline. His reasoning is for performance. It's better to do any IO related operations in the request thread pool to prevent blocking the callback thread. This could be a bit better. If we do this, maybe we should also change TransactionMarkerRequestCompletionHandler so that it writes the complete marker in the request thread instead of the callback thread. That could be done in a followup jira."
1146951880,13391,jolshan,2023-03-23T22:49:09Z,(answer is above)
1147152053,13391,artemlivshits,2023-03-24T06:01:39Z,"This should be .offer -- we don't need to block if the request queue is full, and it's ok if we don't have a wakeup request in a full queue -- the queue would would contain a request (due to the fact that it's full) to wake up the poll."
1147156380,13391,artemlivshits,2023-03-24T06:10:23Z,"We could probably handle WakeupRequest in this function, so that the wakeup mechanism is encapsulated in RequestChannel (i.e. check if we got a wakeup request from the requestQueue and poll the callbackQueue again in that case)."
1147689005,13391,junrao,2023-03-24T14:50:15Z,Could we just reuse the ActionQueue instead of introducing a new queue? ActionQueue is drained by the request thread whenever it finishes processing the current event.
1147787658,13391,jolshan,2023-03-24T16:05:36Z,Will we still have the behavior of putting the callback at the front of the queue? It would go to the end of the action queue right?
1147789029,13391,jolshan,2023-03-24T16:06:54Z,Offer was throwing spotbugs errors since I didn't check the response. I can do that but it will be a little uglier.
1147794328,13391,junrao,2023-03-24T16:11:55Z,"It would go to the end of the action queue. However, not every request adds entries to the action queue. So, the action queue is typically smaller than the request queue. Also, request threads prioritize the action queue over the request queue."
1147804661,13391,jolshan,2023-03-24T16:22:05Z,So it is the case that all the action entries are removed before the next request? For some reason I thought it just cleared one for each request. I can take a look.
1147813111,13391,junrao,2023-03-24T16:28:59Z,"It clears all entries it sees at the beginning.

```
  def tryCompleteActions(): Unit = {
    val maxToComplete = queue.size()
    var count = 0
    var done = false
    while (!done && count < maxToComplete) {
      try {
        val action = queue.poll()
        if (action == null) done = true
        else action()
      } catch {
        case e: Throwable =>
          error(""failed to complete delayed actions"", e)
      } finally count += 1
    }
  }

```"
1147857120,13391,jolshan,2023-03-24T17:09:05Z,thanks!
1147991721,13391,artemlivshits,2023-03-24T19:43:30Z,"If a request hits a timeout the new request will come in the new connection, so the fact the old connection is muted wouldn't prevent the new request to come.  The timeout processing on the client is just a timer, once it expires, it'll kill old connection, create a new one and re-send the batch.  We won't bump the epoch on retry -- it'll override duplicate checking logic, so we'd have duplicates if we did, so the exact same batch (same epoch, same sequence) will come again on the new connection."
1147999817,13391,artemlivshits,2023-03-24T19:53:33Z,I'd expect the synchronization would be added as part of making the class properly multithreaded.
1148002291,13391,artemlivshits,2023-03-24T19:56:51Z,"Should we move it to the proper scope then?  (the indentation is hard to follow in the PR view, I assumed it's already in the proper scope)"
1148025113,13391,artemlivshits,2023-03-24T20:29:51Z,"Oh, I see, we have only one request for a trasnactional.id, so we need to complete previous one in order to replace.

>The only time we can receive two requests from the same txn id is when the producer restarts and the epoch is bumped

See my other comments about retries.
"
1148044044,13391,artemlivshits,2023-03-24T20:52:43Z,"We could try to re-purpose the action queue for this, but it currently has different semantics -- the error processing is different, for example (it's executed in the finally clause).  What we want here is the semantics that we'd get if we just blocked the thread while we're contacting TC, but without blocking the thread.  Same error handling, same context, same metrics, etc.

The basic implementation of the desired semantics is just a queue, but we could also preserve request context (see TODOs in KafkaRequestHandler.scala) so that this functionality that would work for any requests that need to do non-blocking async calls.
"
1148057123,13391,artemlivshits,2023-03-24T21:08:17Z,"I guess, if the request queue is full then it means that the request threads are all overloaded, so if we block the inter-broker channel thread it probably wouldn't matter much as the broker is already in pain.  But on the other hand, it's kind of strange to not implement technically more correct behavior because we've got some spurious warnings."
1148060963,13391,artemlivshits,2023-03-24T21:14:41Z,"We should measure the time it took to process the request to TC and report it (either as ""remote time"" that we already have or as a new metric), otherwise it'll just roll into ""local time"" (time spent by this broker to handle the request)."
1148081688,13391,jolshan,2023-03-24T21:39:48Z,Yeah. I planned to look at the request timing stuff next. Thanks for the reminder :) 
1148082243,13391,jolshan,2023-03-24T21:40:36Z,The build will fail with the warnings. But I can add the extra code so the build passes.
1148084290,13391,jolshan,2023-03-24T21:42:44Z,"Ah. I guess I was thinking of the client restarting. So you are saying in the case where the request times out before we send the request here, we can hit the error?"
1149586417,13391,jolshan,2023-03-27T17:34:15Z,I think we want to update in both cases -- and we want to distinguish the two.
1149838206,13391,artemlivshits,2023-03-27T22:12:27Z,"Yes, that's my understanding based on my reading of the code -- once a connection to a broker has at least one timed-out in-flight request, it's disconnected and all in-flight requests get a timeout error (which can be then retried by the producer on a new connection).  https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/clients/NetworkClient.java#L831

It's also not guaranteed that the request that comes later here is the retry (i.e. the assumption that we can just fail the previous request because it must've timed out anyway is not true).  Should be unlikely with default settings, so we can probably just return some retriable error and document the caveat.  Alternatively (I would actually prefer that), we could just support multiple requests per transactional.id which would eliminate the need to handle this case here altogether: just add all new requests to the pending ""batch"" and let the TC handle different cases."
1149847782,13391,jolshan,2023-03-27T22:28:23Z,I don't think we can add multiple to the batch because of how the callbacks are currently implemented. We would need to know which response goes to which callback. I thought about this a while and one callback is the the simplest way to do it.
1149873099,13391,artemlivshits,2023-03-27T23:14:12Z,"Ok, looks like the protocol is not designed to support multiple requests for one transactional.id.  For same-epoch case, though, we'd get the same answer because we'd be asking the same question, so we could just keep multiple callbacks per trasnactional.id and call them all with the response, this way we don't have to guess which message is first and which is the retry."
1149903116,13391,jolshan,2023-03-28T00:17:49Z,Hmm -- would it be better to just replace the old one? I don't think we'd expect two responses. And we definitely don't want to write twice.
1151026334,13391,artemlivshits,2023-03-28T18:44:14Z,"We don't know which one is the old one -- the one that's there or the one that just arrived.  We won't write twice -- at some point we'll check for duplicates and one of the requests would succeed and the other would get 'duplicate' error (which is effectively success).

We had this situation anyway without verifying the transactions: 2 requests will just continue processing and eventually one would succeed and another would get a duplicate.  We also have this situation when one request is already in-flight to TC."
1151046857,13391,artemlivshits,2023-03-28T19:06:16Z,"(could have syntax errors as I didn't compile it)
```
  val nodeAndTransactionData = nodesToTransactions.getOrElseUpdate(node,
    new TransactionDataAndCallbacks(
      new AddPartitionsToTxnTransactionCollection(1),
      mutable.Map.Empty))

 val currentTransactionData = nodeAndTransactionData.transactionData.find(transactionData.transactionalId))
"
1151269827,13391,YiDing-Duke,2023-03-29T00:07:47Z,"I assume the duplicate and out of order check happens in producer id cache and before this stage. If that's true, in general, the next batches with the same epoch won't hit this stage and should be fenced with out of order retry error to client.

There can be a special case where this is the very first batch write from new epoch so the producer id cache cannot block any batches into this stage. "
1152479391,13391,artemlivshits,2023-03-29T20:59:25Z,"Duplicate check + store has to be atomic (cannot really discard a new request as a duplicate until the previous request succeeds, not can let it go through until the previous request fails), so it needs to happen under a lock.  The purpose of this stage is to not let a request go into log if the transaction is not there, so it got to be either between the check and the store or before the check and the store, hopefully, it's the latter, because otherwise we'd have a long lock around inter-broker RPC.

BTW, there should be no out-of-order errors or fenced errors during normal retry processing -- the first try should go through and others would be bounced with ""duplicate"" error which is effectively a success.  This way all tries would be effectively successful and the intermittent error would be transparently handled by Kafka without bubbling up to the application."
1152485888,13391,jolshan,2023-03-29T21:07:05Z,"> I assume the duplicate and out of order check happens in producer id cache and before this stage.

It does not happen before this stage.

But Artem is right. If we do process the callback twice, I suppose the second one will see it as a duplicate and return fine. I think my concern is that we return a timeout response + the callback response, but I guess this sort of thing can happen normally? 

I guess my question is whether we still want to return both callbacks with the response to addpartitionstotxn, or if it is just sufficient to ignore the first one. "
1152509597,13391,jolshan,2023-03-29T21:37:29Z,will do in https://github.com/apache/kafka/pull/13463 
1152543409,13391,jolshan,2023-03-29T22:21:23Z,"Actually -- I misread the original comment -- I thought this was about callback time.
I'm wondering if we want to report this in a different way after all. It would be a bit confusing to report this as remote time since this is technically still on the broker.

I'm also not sure how we would parse the separate metric.

Is the idea that we would somehow get the time to send and receive the response and subtract it from the local time? 

I don't even see the local time metric getting incremented at this level. One idea is to take the request time and subtract the time to do the callback. I'd have to see if this is even possible though."
1152603124,13391,artemlivshits,2023-03-30T00:03:28Z,"Actually, this is a higher level discussion that we should probably have on the KIP thread, as it affects public metric semantics.  I think it would be useful to have a separate metric, because it'll help with diagnosing extra latency in transactions that we're introducing with this RPC call.  Especially if we provide a feature flag to turn off this check for perf reasons -- we'd need to have some data to help the admin make a decision."
1152615315,13391,artemlivshits,2023-03-30T00:26:37Z,Synchronization?
1152617531,13391,artemlivshits,2023-03-30T00:32:15Z,"If we'll have a thread pool of network threads, we'd need to synchronize this too.  So we probably need to add a comment here."
1152624055,13391,artemlivshits,2023-03-30T00:48:02Z,"The KIP also mentions a race condition where the transaction may be aborted just after we've verified it, but before we got the reply, it doesn't seem like this PR addresses that, do we plan to do it in a different PR?"
1153539479,13391,jolshan,2023-03-30T16:56:29Z,thanks for the reminder -- I need to remember the details here.
1153540952,13391,jolshan,2023-03-30T16:57:51Z,I don't recall having a pool of network threads. this would just be the one send thread for now i think.
1154744670,13391,artemlivshits,2023-03-31T17:49:25Z,"Currently we only have one thread and it might be the case forever, but from this code the threading model is not obvious and it would be useful to have a comment that we don't need synchronization for inflightNodes because inflightNodes is only accessed from methods that are called on the sender's thread."
1157599255,13391,jolshan,2023-04-04T18:07:21Z,"I've changed the internal details a bit so I no longer have an identifier that the first check succeeded. We append to the log on callback. Originally, I was going to update the PSM entry to contain the first offset and then we would remove the entry on the end marker. However, now we don't write the offset until the first append. (We can check for the next ones, but not the first append). The later changes with the epochs will make this not necessary since we bump the epoch on the marker write. I'm trying to think how we can handle this in the old clients case on the first append -- if there are any indicators we can check."
1157781323,13391,jolshan,2023-04-04T21:20:40Z,"One option here is to check the timestamp of the last time the entry was updated if there is no ""last txn"" information. On the first append, the timestamp should not change on the entry. (I believe it only changes on data appends and txn markers)"
1158766750,13391,jolshan,2023-04-05T16:40:42Z,I'm going to keep what I have for now -- we can revisit the action queue if we want later.
1160343683,13391,artemlivshits,2023-04-07T00:02:30Z,"If we don't want to put too many things in one change, we could implement the race condition checks in a separate change -- even though we didn't fully fix the problem we didn't regress (in fact improved quite a bit).  On the other hand, fixing localTime metric should be done in this change, because it worked before this change so if we don't fix it, it would be a regression.

Another approach (if it makes things simpler in any way) could be to split out the framework to run callbacks on request threads and add metrics there, then rebase this change on top of framework, so this change focuses on the transaction-specific stuff."
1160355023,13391,artemlivshits,2023-04-07T00:38:32Z,"Update comment to say ""requestChannel and request""."
1160356274,13391,artemlivshits,2023-04-07T00:42:51Z,We should set it to null once we're done with the request (in a finally clause).  This would avoid issues of a request living longer than needed because it's referenced in a thread local of some request thread.
1160357216,13391,artemlivshits,2023-04-07T00:46:09Z,"This change uses only one callback, but as a generic framework, a request could run multiple RPCs during its processing, so we should do `currentRequest.set(request.originalRequest` (and clean it afterwards)."
1160357426,13391,artemlivshits,2023-04-07T00:46:50Z,Maybe use `callback` instead of `request`.
1160363167,13391,artemlivshits,2023-04-07T01:04:07Z,"I think the metric could be updated when the response is sent in request.fun(), so it may actually see some value in request.originalRequest.callbackRequestDequeTimeNanos and no value in callbackRequestCompleteTimeNanos, in which case the processing time could be negative.

Also, if multiple callbacks are used during the request processing, we'd lose callback processing time for all but last callback (for this we could add a comment, to add this logic if we have multiple callbacks to handle)."
1160803600,13391,jolshan,2023-04-07T16:06:04Z,"Are you saying network client sends the response + updates the metrics before we reach line 115, so we should put the complete message as part of the response? I'm not sure the best way to put that in the callback. I'll have to think about it."
1160807017,13391,jolshan,2023-04-07T16:12:00Z,I wasn't sure if we would have a new request here though. We could be handling this callback after the original request returned right?
1160817791,13391,jolshan,2023-04-07T16:30:14Z,We've opted to return a retriable error for the case where the second response is returned first.
1160819448,13391,jolshan,2023-04-07T16:32:51Z,Let's do the check before append in a followup -- here is the JIRA -- https://issues.apache.org/jira/browse/KAFKA-14561
1160830029,13391,jolshan,2023-04-07T16:51:33Z,Ah I think I figured it out.
1160831069,13391,jolshan,2023-04-07T16:53:25Z,Ah I misunderstood your comment. I guess I'm just not sure what setting current request here does for us. Why would we use currentRequest instead of original request? Unless we wanted to schedule another callback  
1160849589,13391,artemlivshits,2023-04-07T17:27:55Z,"> Unless we wanted to schedule another callback

We might in the future.  From the implementation perspective it may seem like we have ""the request"" and ""the callback"", but conceptually we should think in terms of a request that may run a few asynchronous operations and then continue processing, in which case we can have multiple callbacks in the context of the same request.  So logically it's like this:

-- request starts processing
1. Do some processing on a broker request thread
2. Do async operation (RPC, or maybe even async storage access in some distant future)
3. Continue processing on a broker request thread.
4. Do some async operation
5. Continue processing on a broker request thread
6. Do some async operation
7. Finish processing on a broker request thread

-- request is done

In this case the steps 1, 3, 5, 7 would be accounted as local time and logically we just keep processing the same request, only instead of blocking the thread in steps 2, 4, 6, we wait without blocking the thread."
1160872423,13391,jolshan,2023-04-07T18:10:20Z,"I think the issue is that the way that I implemented the metric is that we only have callback start and end. If we wanted to store results of multiple callbacks, we would need to change this. I'm tempted to tackle this sort of thing in a followup change to avoid overcomplicating this one."
1160875392,13391,artemlivshits,2023-04-07T18:15:07Z,"I think it's still good to keep the request.originalRequest.callbackRequestCompleteTimeNanos = Some(time.nanoseconds()) here as well.  Looking at the code, request could end in many different ways, some could happen before callback completion, some may happen after callback completion.  There are some error cases, like connection disconnects that may not go through the success path."
1160879089,13391,artemlivshits,2023-04-07T18:22:06Z,"Instead of doing this, I would put the request.callbackRequestCompleteTimeNanos = Some(time.nanoseconds()) back after the callback.fun() call and just update the place where we update metrics to take the current time if callbackRequestCompleteTimeNanos is empty.  Then we don't need chase all the code paths that we could go through before updating the metrics."
1160883532,13391,artemlivshits,2023-04-07T18:30:54Z,"1. Isn't it backwards?  The complete time should be larger than deque time.
2.  If we do the following logic, then it should work regardless of whether we arrive here before setting callbackRequestCompleteTimeNanos or after setting callbackRequestCompleteTimeNanos.

```
val callbackRequestTimeNanos = callbackRequestCompleteTimeNanos.getOrElse(endTimeNanos) - callbackRequestDequeTimeNanos.getOrElse(endTimeNanos)
````"
1160892559,13391,artemlivshits,2023-04-07T18:49:23Z,"Yeah, if we need to make larger change to complete the framework, then we should definitely do in a follow-up change, but I think in this case all we need to do is this:
```
  val prevCallbacksTimeNanos = originalRequest.callbackRequestCompleteTimeNanos.getOrElse(0) - originalRequest.callbackRequestDequeTimeNanos.getOrElse(0)
  originalRequest.callbackRequestCompleteTimeNanos = None

  originalRequest.callbackRequestDequeTimeNanos = Some(time.nanoseconds() - prevCallbacksTimeNanos)
  currentRequest.set(callback.originalRequest)
```
and then we've got ourselves a fully functional framework for supporting arbitrary number of async calls in a request processing :-)."
1160895598,13391,jolshan,2023-04-07T18:55:52Z,ooops -- good call.
1160896272,13391,jolshan,2023-04-07T18:57:08Z,"Hmm -- so we should only update if callback is also defined. 
I'm also not sure about setting it twice, so we should only set after fun() if it is not already set."
1160975587,13391,artemlivshits,2023-04-07T21:27:29Z,Need to clear as well in the finally clause.
1160976550,13391,artemlivshits,2023-04-07T21:30:02Z,"Not sure if the 'if' clause is required, prevCallbacksTimeNanos would be 0 if there was no prev callback."
1160980316,13391,jolshan,2023-04-07T21:40:29Z,"I just wanted to avoid doing all this extra code if not needed. In my head, it made more sense to someone reading it."
1161050689,13391,artemlivshits,2023-04-08T02:40:25Z,"We don't need to pass the current request, this could be completely encapsulated within wrap.  In fact, having a request argument here makes it look like that we could pass some arbitrary request, while here we need exactly the one that is currently processed on the thread."
1161050986,13391,artemlivshits,2023-04-08T02:43:44Z,or currentRequest == null
1161051019,13391,artemlivshits,2023-04-08T02:44:17Z,ok
1161051461,13391,artemlivshits,2023-04-08T02:48:44Z,Do we have a test that actually tests that we'd get a failure if we try to produce without adding the partition to transaction first?
1161058187,13391,artemlivshits,2023-04-08T04:07:10Z,producerStateManager.activeProducers.get(producerId).exists(entry => entry.currentTxnFirstOffset.isPresent)
1161058423,13391,artemlivshits,2023-04-08T04:09:33Z,leaderLogIfLocal.exists(leaderLog => leaderLog.hasOngoingTransaction(producerId))
1161846085,13391,jolshan,2023-04-10T15:59:45Z,"My original concern was that if we just used the thread local, we would access it when the inner method is called. I guess I can just save a local variable when wrap is called and pass that value into the inner method."
1161847053,13391,jolshan,2023-04-10T16:00:34Z,"Depends what you mean here. If you mean a unit test -- yes. If you mean a integration test, no because the correct behavior is built into the producer."
1161854813,13391,jolshan,2023-04-10T16:06:53Z,Actually hmm -- I suppose this test is not present if you mean the exact path of returning the error and not producing to the log. I really did think I added such a test to replica manger test. I can try to add this path.
1161874495,13391,jolshan,2023-04-10T16:31:12Z,"This is a Java map, so that doesn't work. I can convert to scala, but not sure that is much better."
1161896585,13391,jolshan,2023-04-10T16:58:16Z,We do have tests from the previous PR that return errors if the partition is not added to the txn. See https://github.com/apache/kafka/commit/29a1a16668d76a1cc04ec9e39ea13026f2dce1de
1161909242,13391,junrao,2023-04-10T17:14:34Z,Our long term goal is to replace the scala code with java. Could we write this new class and the corresponding test in java?
1161915489,13391,junrao,2023-04-10T17:22:40Z,"The above comment still says ""is still under developement"". Is the latest version indeed stable? Or should we change the comment accordingly?"
1161918638,13391,junrao,2023-04-10T17:26:46Z,The params of this method is getting a bit large. Could we add the javadoc explaining each of the param?
1161932999,13391,junrao,2023-04-10T17:45:39Z,CoordinatorNotAvailable error is not listed in ProduceResponse. Should we add it there and verify that it's handled as expected by existing clients?
1161943463,13391,junrao,2023-04-10T17:58:16Z,Does this need to be volatile?
1161946467,13391,jolshan,2023-04-10T18:01:40Z,Added a line to the ReplicaManager test to see that we return early on the error in the callback.
1161971721,13391,junrao,2023-04-10T18:33:41Z,Do we have a use case where the same callback needs to be handled multiple times by the request thread? How do we prevent that the callback is added an infinite number of time to the callback queue?
1161978192,13391,junrao,2023-04-10T18:41:52Z,"So, `originalRequest.callbackRequestCompleteTimeNanos.isEmpty` is expected to be true? Should we add a warning log if it's false?"
1161979607,13391,junrao,2023-04-10T18:43:46Z,"Since we don't expect to see  WakeupRequest here, should we add a warning log?"
1161982117,13391,junrao,2023-04-10T18:46:51Z,This seems to be a more general mechanism than ActionQueue. Could we move all existing ActionQueue usage to callback queue and get rid of ActionQueue? This could be done in a separate PR.
1161984615,13391,junrao,2023-04-10T18:50:06Z,"Incomplete sentence ""Check if we have already""."
1161987786,13391,junrao,2023-04-10T18:54:20Z,NETWORK_EXCEPTION is not listed in ProduceResponse. Should we add it there and verify that it's handled as expected by existing clients?
1161996676,13391,junrao,2023-04-10T19:05:43Z,addPartitionsToTxnCollection seems unused?
1162000865,13391,junrao,2023-04-10T19:11:11Z,"Since this can be skipped, should this be warn instead of error?"
1162001708,13391,junrao,2023-04-10T19:12:20Z,extra space before returned
1162003823,13391,junrao,2023-04-10T19:15:09Z,"Hmm, not quite sure that I follow. Do you mean that we return INVALID_RECORD to be compatible for old clients?"
1162043968,13391,jolshan,2023-04-10T20:06:24Z,I will update the comment.
1162044614,13391,jolshan,2023-04-10T20:07:21Z,it is retriable -- currently if the error is retriable we just retry. It seems like most retriable errors are not enumerated specifically.
1162045206,13391,jolshan,2023-04-10T20:08:02Z,^ ditto comment about retriable errors.
1162045551,13391,jolshan,2023-04-10T20:08:23Z,"We can make it volatile, but this is only really used in tests."
1162046736,13391,jolshan,2023-04-10T20:09:36Z,"Artem requested this. See comment here. https://github.com/apache/kafka/pull/13391#discussion_r1160849589

There is currently not a way to prevent infinite callbacks. "
1162047387,13391,jolshan,2023-04-10T20:10:12Z,It is not true if we returned a response. We also update the value there.
1162047588,13391,jolshan,2023-04-10T20:10:25Z,We can add a warning log.
1162049144,13391,jolshan,2023-04-10T20:12:25Z,"Yes. Sorry for the confusion. In the KIP we mention INVALID_TXN_STATE for the new clients, but old clients use INVALID_RECORD for compatibility. I will update the comment."
1162109668,13391,jolshan,2023-04-10T21:38:30Z,Let's do in a separate PR.
1163283951,13391,junrao,2023-04-11T20:21:19Z,"Hmm, not sure that I follow. In the false case, it seems that we just don't set `callbackRequestCompleteTimeNanos`? This is a bit weird since the callback is completed."
1163284515,13391,junrao,2023-04-11T20:21:57Z,Could we file a jira to track this in a followup?
1163287233,13391,junrao,2023-04-11T20:23:51Z,Could this be private? Ditto for a few other helper methods in this file.
1163292661,13391,junrao,2023-04-11T20:28:22Z,"""The error map should remain empty."" What does this mean?"
1163294534,13391,junrao,2023-04-11T20:29:59Z,Get rid of the new line since the following validation is related to this action?
1163323491,13391,junrao,2023-04-11T20:55:26Z,"baseSequence is redefined as val. So, this is unused."
1163335886,13391,junrao,2023-04-11T21:08:11Z,Do we need this? It doesn't seem the request handler thread is involved in the test.
1163418845,13391,jolshan,2023-04-11T23:22:00Z,"It means that we didn't complete the request. If we returned an error response, we would populate the map. But if it just replaced the old data, we don't send a response."
1163420192,13391,jolshan,2023-04-11T23:24:31Z,https://issues.apache.org/jira/browse/KAFKA-14895
1163421308,13391,jolshan,2023-04-11T23:26:54Z,"There are two cases according to artem --
one where we send a response via network client and one where we don't (and we have more callbacks)

in case a, we set in the network client and that is the end of handling of the request. This is also when the metric is updated. In case b, we don't set it there so we must do it here. I have the check to match the same sort of protocol we see in kafka apis where we check if the value is -1. Maybe it's fine to set it twice in case a as we won't update the metrics again, but I did it for consistency."
1163421979,13391,jolshan,2023-04-11T23:28:30Z,Oops. I see we changed this logic. I will fix the comment.
1163429337,13391,jolshan,2023-04-11T23:44:47Z,"We still call the wrap method which checks if we are on the request handler thread.
Since we are not it will fail via illegal state exception.

We we set the bypass, it just runs the callback. "
1163446164,13391,junrao,2023-04-12T00:23:43Z,"I am still a bit confused on this.

case a - only 1 callback. In this case, callbackRequestCompleteTimeNanos is never set and is expected to be empty.

case b - multiple callbacks. In this case, after the first callback, callbackRequestDequeueTimeNanos is set. So, when the 2nd callback is processed, we set callbackRequestCompleteTimeNanos to empty before executing the callback. When we get here, callbackRequestCompleteTimeNanos is expected to be empty.

So, it seems that in both case a and b, we expect callbackRequestCompleteTimeNanos to be empty?"
1163659843,13391,artemlivshits,2023-04-12T06:02:22Z,"Basically, we should search for `apiLocalCompleteTimeNanos` and update the callbackRequestCompleteTimeNanos in similar places.  My understanding is that we can send the reply during request / callback and it may complete (concurrently) before we end the request / callback or after we end request / callback.  In which case we want the end time to be the earliest of the two.
Usually, the way it's done is the start time is set at the start of the request / callback processing and the end time is set after, but if the end time is not set then the metric is reported, then we just report the current time."
1163676408,13391,artemlivshits,2023-04-12T06:26:09Z,"Right now, this framework just accounts for a single logical request that does a few non-blocking calls and continues serially after each call.  We could probably add a check that prohibits adding a second .wrap during request / callback execution."
1164335669,13391,jolshan,2023-04-12T15:55:15Z,"In case a, we set it in the network client -- it will not be empty.
in case b, it is empty and we set it. Then, when executing the next callback, we take this time and subtract it from the start of the new callback. This gives us total time of all. "
1164392731,13391,junrao,2023-04-12T16:45:31Z,"Thanks for the explanation. I understand it now. callbackRequestCompleteTimeNanos can be non-empty if the response is sent before the callback completes. Currently, we update the metric when the sending of the response completes. So, a more accurate place to set callbackRequestCompleteTimeNanos in RequestChannel is when the response send completes. 

An alternative approach is to delay the updating of the request time metrics until the callback completes. This simplifies the setting of callbackRequestCompleteTimeNanos since it only needs to be done in KafkaRequestHandler and is more accurate since we don't need to cut off callbackRequestCompleteTimeNanos by response send complete time.

Both of these could be addressed in a followup jira."
1164399008,13391,jolshan,2023-04-12T16:51:49Z,"I'm not sure I follow the first part since right now, we need to account for all the callback local times. Unless we change to a cumulative callback total time where we periodically add to it, we will need to update either on the send OR when the callback completes (if we don't send yet)

I'm also not sure about waiting until the callback completes because that is after we send via the network client and that is no longer considered ""local time"""
1164424805,13391,junrao,2023-04-12T17:17:36Z,"For simplicity, let's just assume there is only one callback for now. Currently, the code sets callbackRequestCompleteTimeNanos when the response is being sent. If the callback has been started, but not completed at that point, the measurement of the callback portion time is not accurate. If we delay the cut off of callbackRequestCompleteTimeNanos at the time when request time metric is updated (which currently is when the response send completes), we allow for more accurate measurement of the callback portion time.

For your second question, yes, it's a bit weird to report the callback time as 'local' after the response is being sent. However, 'local' really reflects the amount of time a request handler thread is tied up for processing a request. From that perspective, it could also make sense. If we cut off the callback portion of the time by response send time, we could leave a portion of the request thread time unaccounted for?"
1164431328,13391,jolshan,2023-04-12T17:24:15Z,"I asked to just assume one callback earlier but I was told we should just implement this now  (thread here https://github.com/apache/kafka/pull/13391#discussion_r1160872423)

I'm not sure I follow 
> If the callback has been started, but not completed at that point, the measurement of the callback portion time is not accurate.

which scenario is inaccurate?

"
1164445859,13391,junrao,2023-04-12T17:33:50Z,"In the more general case, the callback is not guaranteed to be completed when the response is sent, right? In that case, the code in RequestChannel cuts off callbackRequestCompleteTimeNanos at response send time. When the callback completes, KafkaRequestHandler doesn't set callbackRequestCompleteTimeNanos since it's not empty. This means the measurement of callback time is not accurate, right?"
1164460056,13391,jolshan,2023-04-12T17:48:36Z,"Given that we have only one case right now I guess I'm not sure we have a general case -- but in the current case, we complete the callback by sending the response.

I guess I saw this protocol working as follows:

a) we have one callback and send the response in which local time is correct
b) we have multiple callbacks and the first few do not send the response. We updated after calling the callback. Then the final one sends the response (case a)

We always must return in the final callback because we only use this callback if we've already returned from handle. We must return via the callback in my understanding."
1164476174,13391,junrao,2023-04-12T18:04:26Z,"Yes, in the current usage where the last callback sends the response as the last thing, it doesn't make much difference.

I was thinking that the callbacks in ActionQueue are executed after the response is generated. If want to replace ActionQueue with the callback queue, we need to handle the more general case."
1164479606,13391,jolshan,2023-04-12T18:08:12Z,"Yeah -- I guess the previous decisions were made without considering the action queue. 
I'm wondering if it is better to leave as is and change via a followup or simplify back to ""base case""  (ie, case a which is currently what we do)"
1164488222,13391,junrao,2023-04-12T18:16:22Z,"Yes, we can leave the PR as it is. Could we file a jira to revisit action queue? We can make any needed changes there."
1164492470,13391,jolshan,2023-04-12T18:20:43Z,https://issues.apache.org/jira/browse/KAFKA-14899
1164633122,13391,junrao,2023-04-12T20:41:15Z,It's probably used for to log the destination broker on which the request fails. Ditto in other logging.
1164638992,13391,jolshan,2023-04-12T20:48:13Z,I will switch to the destination node.
61458168,1251,gwenshap,2016-04-28T16:23:35Z,"Nit: Oracle's javadoc guide specifically says: 

> When writing a phrase, do not capitalize and do not end with a period:
> 
> @param x  the x-coordinate, measured in pixels

http://www.oracle.com/technetwork/java/javase/documentation/index-137868.html
"
61458366,1251,gwenshap,2016-04-28T16:25:03Z,"""iff"" means ""if and only if"" and is correct in this context. I don't think it was a typo.

If you find ""iff"" unclear, feel free to replace with ""if and only if""
"
61458701,1251,gwenshap,2016-04-28T16:27:09Z,"These are new public APIs right? (@ijuma - correct me if I'm wrong).
"
61459284,1251,gwenshap,2016-04-28T16:30:38Z,"It is a bit hard to tell from the diff, but did you add a new argument as the first in the list? Is there a reason? We usually add new arguments at the end.
"
61459399,1251,gwenshap,2016-04-28T16:31:16Z,"the nulls are getting a bit hard to read. Do we want constants for NO_APIS and NO_METADATA?
"
61459449,1251,ijuma,2016-04-28T16:31:33Z,"Looks like only the consumer and producer packages are considered API: https://kafka.apache.org/090/javadoc/index.html
"
61460110,1251,gwenshap,2016-04-28T16:36:22Z,"Thanks :)
"
61460735,1251,gwenshap,2016-04-28T16:40:12Z,"""trace"" maybe? This seems incredibly chatty even for debug.
"
68186750,1251,ijuma,2016-06-23T07:24:20Z,"Why was this removed?
"
68190038,1251,ijuma,2016-06-23T07:52:43Z,"We probably want a `KafkaConsumer` and `KafkaProducer` test that expects a newer than supported broker. Sounds like it needs to be a system test. It probably needs a request to be bumped up too before it works. Hmm, actually, we should have system tests for broker versions that don't support `ApiVersionsRequest` (e.g. 0.9.0.1 and 0.8.2.1).

In addition, it would be great if we could detect if something is missing from this list at unit/integration test time. Any ideas on how to do that?
"
68190553,1251,ijuma,2016-06-23T07:56:40Z,"I think it would be better for `READY` to be the state where one sends requests. This means that if the api version requests are not needed, we can automatically transition from `CONNECTED` to `READY`. Thoughts? cc @hachikuji.
"
71025620,1251,SinghAsDev,2016-07-15T19:06:20Z,"It should not be, fixed in next version.
"
71026255,1251,SinghAsDev,2016-07-15T19:11:00Z,"Added system test against trunk and older broker versions. The interesting tests like testing against unsupported broker versions, will probably have to wait till we have such versions.

The list of APIs used by client is similar to the practice of specifying used errors for a request. As you said, it will be nice to have this checked dynamically at test time, however nothing comes to my mind with which we can do that. Will be happy to accommodate any suggestion on this.
"
71026376,1251,SinghAsDev,2016-07-15T19:11:49Z,"I do not have a strong bias on this, will be happy to make the change if you really think it is a must.
"
74093108,1251,dpkp,2016-08-09T16:25:49Z,"Apologies for the late review, but I feel like it would be much clearer if the state transition were:
`DISCONNECTED -> CONNECTING -> CHECKING_API_VERSIONS -> CONNECTED`

It strikes me that the main reason you have the READY state is to manage `canSendMore`, but I can't actually find where `canSendMore` is called in the api version request flow. NetworkClient `doSend` skips the `canSendMore` check, I believe, so do we really need that extra layer of complexity?

And so would it be possible to check for `this.requiredApiVersions == null` directly here and set the next connectionState to CONNECTED if null, or to CHECKING_API_VERSIONS if not? Then in the api version response handler the state could be updated to CONNECTED.
"
74093303,1251,dpkp,2016-08-09T16:26:46Z,"why not use a future + callback handler? The inline check in `handleCompletedReceives` is a bit confusing / buried
"
91202952,1251,cmccabe,2016-12-07T00:18:58Z,"Hmm.  ""java.util.Collection"" could be something unsorted like a List.  How about making this a map from ApiKey to ApiVersion so that we can compare it with the server response more efficiently?"
91203250,1251,cmccabe,2016-12-07T00:21:40Z,"Hmm.  I was initially confused by what the ""apis"" parameter did.  Can we call this ""requiredApiVersions""?

Same comment for the other constructor overloads."
91203437,1251,cmccabe,2016-12-07T00:23:19Z,"Can we have ""if"" statements instead of the ternary operator?"
92007667,1251,hachikuji,2016-12-12T18:25:33Z,nit: Feels like overkill to have constants for `null` which are only used in the constructors.
92007933,1251,hachikuji,2016-12-12T18:26:55Z,Might be helpful to add a brief comment here explaining the behavior if `requiredApiVersions` is null.
92008244,1251,hachikuji,2016-12-12T18:28:25Z,+1. This is a little hard to read.
92009315,1251,hachikuji,2016-12-12T18:33:31Z,"Might be helpful to add a short comment explaining the new states and transitions. For example, if the version check fails, do we go to `DISCONNECTED`?"
92036463,1251,hachikuji,2016-12-12T20:48:34Z,"nit: this could be `else if`, right? Also, I think we could do an `instanceof` check on the response body, and pass the cast result to `handleApiVersionsResponse` instead of going through the `Struct` instance. "
92036719,1251,hachikuji,2016-12-12T20:49:55Z,nit: `else if`.
92038835,1251,hachikuji,2016-12-12T21:00:39Z,"I'm in favor of this suggestion, but maybe overloading `CONNECTED` would be misleading. Perhaps the states could be `DISCONNECTED -> CONNECTING -> CHECKING_API_VERSIONS -> READY` instead? "
92042824,1251,hachikuji,2016-12-12T21:21:12Z,"Does this need to be concurrent? `NetworkClient` itself is not thread-safe. Also, I wonder if queue is the right data structure. Would a `Set` make more sense? Are there any cases we'd add the same node more than once? 

Now that I'm thinking about it... Do we need this collection at all? It seems like it is identical to the set of nodes which are in the `CONNECTED` state. Maybe we could just get that directly from `ClusterConnectionStates`?"
92043237,1251,hachikuji,2016-12-12T21:23:30Z,nit: this change seems unneeded
92043284,1251,hachikuji,2016-12-12T21:23:50Z,nit: could be static.
92043619,1251,hachikuji,2016-12-12T21:25:26Z,nit: Why not let this be `List<ApiKeys>`? It's usually preferable to preserve type information as long as possible. Same in `KafkaProducer` and the test cases.
92044055,1251,hachikuji,2016-12-12T21:27:45Z,This looks like same code as in `KafkaConsumer`. Perhaps we should move it to `Utils`?
92047117,1251,hachikuji,2016-12-12T21:43:40Z,"nit: seems we could use a singleton, maybe `ApiVersionsRequest.INSTANCE` or something like that?"
92048027,1251,hachikuji,2016-12-12T21:48:13Z,Should we try to use `canSendRequest`?
92049126,1251,hachikuji,2016-12-12T21:53:42Z,Might this be simpler if we just have two methods: `canSendRequest(String node)` and `canSendApiVersionRequest(String node)`?
92049847,1251,hachikuji,2016-12-12T21:57:36Z,"These tests are a little obscure. I wonder if it would be a little clearer to 1) use only one API instead of a list, and 2) reference the versions directly using `ProtoUtils`?"
92050099,1251,hachikuji,2016-12-12T21:58:38Z,Kudos for adding the system test!
92050177,1251,hachikuji,2016-12-12T21:59:03Z,nit: why remove the newline?
92050279,1251,hachikuji,2016-12-12T21:59:39Z,nit: why remove the newline?
92050477,1251,hachikuji,2016-12-12T22:00:40Z,"Also, maybe we could call this `CONSUMER_APIS` and get rid of the comment, which doesn't add much value."
92074335,1251,ijuma,2016-12-13T00:36:53Z,I think the ternary operator is probably OK if the connection state checks were extracted into a variable.
92074732,1251,ijuma,2016-12-13T00:40:05Z,`else if` is not required here since the `if` always throws right? Or do you just mean as a style thing it's clearer?
92079250,1251,ijuma,2016-12-13T01:18:29Z,"Yeah, that would be similar to `API_VERSIONS_RESPONSE` in `ApiVersionsResponse`."
92154634,1251,ijuma,2016-12-13T12:01:02Z,It seems a bit inconsistent that `connected` clears the internal list while other methods like `disconnected` don't. Maybe we should call `clear()` from `poll`?
92154840,1251,ijuma,2016-12-13T12:02:18Z,"We should add `LATEST_0_10_1` and `LATEST_0_10_0` too. Since they support `ApiVersionsRequest`, I take it that the behaviour is more graceful?"
92155097,1251,ijuma,2016-12-13T12:04:08Z,Nit: maybe there should be no default for `should_fail` since we always pass a value.
92155443,1251,ijuma,2016-12-13T12:06:18Z,Seems like they can be final.
92155793,1251,ijuma,2016-12-13T12:09:05Z,Nit: missing space before `:`.
92156071,1251,ijuma,2016-12-13T12:11:06Z,It would be good to include more information about the failure in the message.
92156138,1251,ijuma,2016-12-13T12:11:28Z,Do we want to validate that it failed for the right reason?
92156687,1251,ijuma,2016-12-13T12:15:26Z,Why are we duplicating all of this logic here? Can we not reuse the existing serialization logic?
92243074,1251,SinghAsDev,2016-12-13T19:11:41Z,Sounds good.
92243960,1251,SinghAsDev,2016-12-13T19:16:03Z,"I don't think it has to be sorted, though having a consistent order will make it easier to read in logs. Also, I failed to see why changing this to a map is going to be more efficient. The checking is happening in `handleApiVersionsResponse` and response already has a map of keys to supported versions."
92244101,1251,SinghAsDev,2016-12-13T19:16:50Z,"After taking out `CONNECTED` state, this is no longer an issue."
92259621,1251,SinghAsDev,2016-12-13T20:31:40Z,"We would need some kind of ds to keep track of api versions request that need to be sent during poll. If we were to use `ClusterConnectionStates` to do this, we will either have to add a state between `CheckingApiVersions` and `Ready` to indicate that api versions check is already in progress for a connection, or we will have to move trigger for sending api versions from poll to handleConnections. I am more in favor of having it the way it is right now, and have the api version requests be sent during polls. However, I do agree that the ds for this can be a set, and I will make that change."
92265829,1251,SinghAsDev,2016-12-13T21:05:22Z,"`canSendRequest` also checks if connection state is `Ready`, which won't be true here."
92266604,1251,SinghAsDev,2016-12-13T21:09:25Z,Good point.
92270976,1251,SinghAsDev,2016-12-13T21:32:08Z,"If the version check fails, connection is closed. No reason to go in `DISCONNECTED` state, as their is no way broker would start supporting the version without having to terminate the connection."
92279721,1251,SinghAsDev,2016-12-13T22:16:58Z,"Not sure if those changes help, but making those changes anyway. Let me know if you meant something else."
92285072,1251,SinghAsDev,2016-12-13T22:49:22Z,"Not sure, if you wanted me to add checks for exception messages, as it makes these tests a little brittle. I anyway, went ahead and added those checks. Let me know if you think they don't add much."
92286131,1251,SinghAsDev,2016-12-13T22:55:28Z,Ahh.. left over from some debugging.
92294978,1251,hachikuji,2016-12-13T23:56:13Z,"Yeah, it's personal preference, so feel free to ignore. The `else if` makes the relation between the two cases stand out a bit more."
92295136,1251,hachikuji,2016-12-13T23:57:39Z,I think this check and the one above are redundant. Maybe we can remove the check for the api key?
92296138,1251,hachikuji,2016-12-14T00:05:20Z,Would be nice to have `node` somewhere in this name. Maybe `nodesRequestingApiVersions`?
92296968,1251,hachikuji,2016-12-14T00:13:05Z,Or maybe `nodesNeedingApiVersionsFetch`? Might be closer to the usage.
92297033,1251,hachikuji,2016-12-14T00:13:38Z,"If the node is in `apiVersionsRequests`, then wouldn't the state already be `CHECKING_API_VERSIONS`?"
92299342,1251,hachikuji,2016-12-14T00:33:07Z,nit: the check seems unnecessary.
92303136,1251,hachikuji,2016-12-14T01:02:50Z,"I'm wondering if we can simplify this a little bit. Once the connection has completed, it seems that both `Selector.isChannelReady()` and `InFlightRequests.canSendMore()` should both be true. So instead of going through this intermediate collection before sending the `ApiVersionRequest`, perhaps we can just immediately send the request when we transition to `CHECKING_API_VERSIONS`? I think that would let us get rid of the `apiVersionsRequests` set.

@ijuma Does that seem right?"
92303744,1251,SinghAsDev,2016-12-14T01:08:18Z,"Yea that is one option that I was pointing to earlier. It has been so long that I do not remember why I chose this over sending, but one reason that comes to my mind is that handling all sends receives from poll seems like a good idea. However, I am open to change that. "
92305344,1251,hachikuji,2016-12-14T01:22:43Z,"Never mind. I talked with Jun and Ismael about this and ""connected"" currently does not imply ""ready."""
92316606,1251,hachikuji,2016-12-14T03:24:29Z,nit: alignment
92316756,1251,hachikuji,2016-12-14T03:27:07Z,nit: doesn't seem like this method adds much.
92317190,1251,hachikuji,2016-12-14T03:33:42Z,I don't think we need this one.
92318309,1251,hachikuji,2016-12-14T03:47:33Z,"Not sure you saw my previous comment, but these two constants seem unneeded."
92320200,1251,SinghAsDev,2016-12-14T04:18:28Z,"@hachikuji this was explicitly asked in one of the previous review comments, having bunch of nulls in a method call, makes it hard to read. If this is not a big concern, maybe we can leave it in as it makes code more readable? LMK."
92433034,1251,hachikuji,2016-12-14T16:34:34Z,"Sure, that's fair."
92452791,1251,hachikuji,2016-12-14T18:07:07Z,Seems this is unused. Do we need it?
92456223,1251,hachikuji,2016-12-14T18:23:38Z,"The second argument here is used to indicate when the request is a metadata request initiated by the `NetworkClient`. This lets us send metadata requests externally without having the `NetworkClient` intercept them (I think we were the ones who added this feature initially  ). I think it would probably make sense to do the same for the `ApiVersionsRequest`. It seems a little safer and I can imagine using this API from the admin client in the future, for example."
92496174,1251,SinghAsDev,2016-12-14T21:45:07Z,"I initially thought to do that, but it is very likely that someone would have raised that as not required in review. However, now that is is raised, happy to add it. If there are no more reviews, I will shortly update the PR here."
92498039,1251,hachikuji,2016-12-14T21:53:51Z,Could we use a single `isInternalRequest` flag combined with an `instanceof` check to distinguish the two cases?
92501345,1251,SinghAsDev,2016-12-14T22:10:43Z,"One can argue against it, as now the same check has to be done at multiple places. If number of such internal requests grow beyond two, then probably it won't make sense to keep a flag for each of them. At that point it would be better to add a method like `boolean isInternalRequestOfType(ApiKeys apiKey)`. Makes sense?"
92503655,1251,hachikuji,2016-12-14T22:23:26Z,"We should probably have the `instanceof` checks anyway prior to casting the response object, so this just removes the need for an extra field. Is there any downside?"
92505757,1251,SinghAsDev,2016-12-14T22:35:00Z,"Not sure I am following you here. Below is one of the places where `isInternalMetadataRequest` flag is used.
```
if (req.isInternalMetadataRequest)
                metadataUpdater.handleCompletedMetadataResponse(req.header, now, body);
```
Are you suggesting that I change it to something like.
```
if (req.isInternalRequest && body instanceOf MetadataResponse)
                metadataUpdater.handleCompletedMetadataResponse(req.header, now, (MetadataResponse) body);
```
If so, then we are talking about changing `MetadataUpdater` interface. Do we really want to do that?

Do you see any other type of request being initiated by network client any time soon?"
92510635,1251,hachikuji,2016-12-14T23:03:03Z,"Yeah, that's exactly what I was thinking. It seems like a nice improvement to let the `MetadataUpdater` interface accept an instance of `MetadataResponse` directly so that it doesn't have to deal with casting itself."
92513323,1251,hachikuji,2016-12-14T23:20:59Z,nit: seems we no longer have much use for `handleMetadataResponse`. Maybe we just move its body here?
92515767,1251,ijuma,2016-12-14T23:39:14Z,"Nit: `oldest` and `latest` are not symmetric. Also, it's a bit odd that we use different names for the field names (basically `current` and `minimum`). Finally, why do we use `int` instead of `ApiKeys`?"
92515994,1251,ijuma,2016-12-14T23:40:55Z,"If you look at `ProducerRecord`, we use a slightly different convention for `toString` (I know we are not consistent everywhere, but I'm trying to improve that)."
92516990,1251,ijuma,2016-12-14T23:48:30Z,`Collections.singletonList` can be used to make this more concise.
92517073,1251,ijuma,2016-12-14T23:49:08Z,"I don't understand this name, why do we call it `USED_API_KEY`?"
92517275,1251,ijuma,2016-12-14T23:50:41Z,I don't understand why we add +2 to `Short.MAX_VALUE` and then cast to `short` causing it to overflow?
92517541,1251,ijuma,2016-12-14T23:52:41Z,Don't we have some code that we can reuse for this? Why are we manually serialising `ApiVersionResponse`?
92517616,1251,ijuma,2016-12-14T23:53:18Z,We should have a timeout here so that we don't loop forever in case of a bug.
92517670,1251,ijuma,2016-12-14T23:53:47Z,"Are some of these fields supposed to be `final` (existing code, I know)?"
92517741,1251,ijuma,2016-12-14T23:54:17Z,Nit: maybe this should be `createNetworkClient` and similar for the other method.
92517776,1251,ijuma,2016-12-14T23:54:36Z,Nit: should probably just be `expectedApiVersions`.
92517927,1251,ijuma,2016-12-14T23:55:49Z,We should have some shared code for serialising the response and response header instead of duplicating it in multiple places.
92517976,1251,ijuma,2016-12-14T23:56:12Z,Did you see this comment?
92518023,1251,ijuma,2016-12-14T23:56:37Z,Nit: extra empty line.
92518167,1251,ijuma,2016-12-14T23:57:35Z,"Hmm, I thought we'd have `LATEST_0_10_1` and `LATEST_0_10_0` instead of `LATEST_0_10` (which doesn't make sense because we have multiple feature releases in the `0_10` line."
92518242,1251,ijuma,2016-12-14T23:58:15Z,Should we not check that we get a better error for the case where `ApiVersions` is available (i.e. `0.10.0.x`)?
92528123,1251,hachikuji,2016-12-15T01:32:33Z,"Yeah, maybe just remove the constant and use `ApiKeys.METADATA` directly?"
92866560,1251,ijuma,2016-12-16T19:01:30Z,"On second thought, I'm OK to leave this as it is and we can leave this improvement as future work."
92897920,1251,SinghAsDev,2016-12-16T22:35:34Z,"Yea, this will require a bit of refactoring. Current changes are inline with the way `ProtoUtils` is written. Leaving it as it is for now."
92897966,1251,SinghAsDev,2016-12-16T22:35:51Z,Good point!
37427316,151,ijuma,2015-08-19T15:21:47Z,"What is the right way to handle this? The random suffix is there because an exception will be thrown in `registerMetric` if a broker is removed and then added again. The `Selector` doesn't allow the caller to control that in its current form.
"
37427419,151,ijuma,2015-08-19T15:22:27Z,"Is it OK to use the same config as `SocketServer` for this?
"
37427623,151,ijuma,2015-08-19T15:23:51Z,"`config.controllerSocketTimeoutMs` was previously being used for `BlockingChannel.readTimeoutMs`, is it correct to use it here in this way?
"
37427682,151,ijuma,2015-08-19T15:24:19Z,"Is it OK to reuse the `SocketServer` settings here?
"
37427714,151,ijuma,2015-08-19T15:24:36Z,"`config.controllerSocketTimeoutMs` was previously being used for `BlockingChannel.readTimeoutMs`, is it correct to use it here in this way?
"
37427818,151,ijuma,2015-08-19T15:25:18Z,"Is it OK to use the `SocketServer` config parameters here?
"
37470292,151,gwenshap,2015-08-19T21:25:21Z,"I don't think you need maxBytes here... this was for limiting the message size from clients. We have NetworkReceive.UNLIMITED if we want to leave this open.
"
37470393,151,gwenshap,2015-08-19T21:26:05Z,"Can to expend on why would the broker get removed and re-added from here?
"
37472395,151,ijuma,2015-08-19T21:44:00Z,"`ControllerChannelManager.removeBroker` is called in `BrokerChangeListener.handleChildChange`, the actual line is:

`deadBrokerIds.foreach(controllerContext.controllerChannelManager.removeBroker(_))`
"
37474431,151,gwenshap,2015-08-19T22:02:49Z,"So, it looks like you have a selector for each controller->broker connection?
I'd expect one Selector for the controller and open and close connections for brokers.
This way the selector string will include the broker ID for the controller, but not for the brokers themselves. And we'll probably want to maintain metricsPerConnection so we can track those separately.

Does that make sense?
"
37476236,151,gwenshap,2015-08-19T22:23:50Z,"I think these utils are very generic, but also make the code a bit harder to read.

It looks like the ""find"" we are waiting for is basically always a reply from a specific broker? In this case, maybe make the generic method private and provide a more specific wrapper?
"
37476467,151,ijuma,2015-08-19T22:26:34Z,"Yes, I understand that one selector per controller would be better. I was trying to avoid a complete redesign of `ControllerChannelManager` though. It currently uses one `RequestSendThread` per broker with a `BlockingQueue` and `BlockingChannel`. `ControllerChannelManager` has a public `sendRequest` method that adds to the relevant queue. `RequestSendThread` takes from this queue. My understanding is that a given Selector should be used from just one thread and hence the resulting code. Please let me know if this is not correct.

In an ideal world, we would redesign things to use one Selector per Controller, but it seemed riskier and I thought it would be better to do that in a separate change after the next release (since our main goal at this point is just to use `Selector` for the TLS/SSL support). What are your thoughts?
"
37476513,151,gwenshap,2015-08-19T22:27:11Z,"yeah, I think it does exactly the same thing we used to do: block until we get a reply or get to the timeout time.

IIRC, the NetworkReceive code has some deprecated methods specifically to support the blocking channel timeouts. Maybe this can be cleaned up now.
"
37476689,151,gwenshap,2015-08-19T22:29:20Z,"BlockingChannel used to get the buffer sizes as parameters... shouldn't we keep the same behavior?
"
37476724,151,ijuma,2015-08-19T22:29:48Z,"`BlockingChannel` is still used by the old consumer, so I am not sure we can remove anything just yet. I'll double-check though.
"
37476769,151,gwenshap,2015-08-19T22:30:30Z,"I think it is, since it keeps the same behavior.
"
37476900,151,ijuma,2015-08-19T22:32:17Z,"We were passing `BlockingChannel.UseDefaultBufferSize` (which is `-1`) everywhere and the code in `BlockingChannel` does:

```
        if(readBufferSize > 0)
          channel.socket.setReceiveBufferSize(readBufferSize)
        if(writeBufferSize > 0)
          channel.socket.setSendBufferSize(writeBufferSize)
```

So, we were basically not setting anything.
"
37477305,151,gwenshap,2015-08-19T22:36:56Z,"Lets see if I got it.

We have a communication thread per broker, and a selector per thread.
If a broker dies, we close the thread and the selector.
If the broker comes back, we can't create a new selector with the same name due to metrics.

Since the number of possible brokers is limited, can we just store the selector for each broker in a Map and reuse / reconnect if the dead broker is revived?  This should keep the metrics sane too.
"
37477570,151,gwenshap,2015-08-19T22:39:45Z,"LOL, we are cute :)

I'd leave it at -1 then. 
"
37477835,151,gwenshap,2015-08-19T22:42:36Z,"I'd also document that these are for creating blocking behavior on top of our async network classes, and to use only where blocking makes sense...
"
37478175,151,ijuma,2015-08-19T22:47:28Z,"I can try to add some wrappers and see how it looks. To help me understand, what makes it hard to read? Is it because of the long line with many parameters? This could be simplified quite a bit by using implicit value classes and passing the time implicitly as well. Example follows:

Now:

```
receive = SelectorUtils.pollUntilFound(selector, config.controllerSocketTimeoutMs)(findResponse)(time).getOrElse {
```

After the proposed change:

```
receive = selector.pollUntilFound(config.controllerSocketTimeoutMs)(findResponse).getOrElse {
```
"
37478292,151,ijuma,2015-08-19T22:49:29Z,"Yes, that seems doable. I'll try that tomorrow and update the PR.
"
37478390,151,ijuma,2015-08-19T22:50:46Z,"Good point about documenting the blocking aspect. Will do.
"
37478724,151,ijuma,2015-08-19T22:54:31Z,"We can't pass -1 to `Selector.connect` though as it will fail when it calls `Socket.sendBufferSize`. Maybe you are suggesting that I update `Selector.connect` to behave as `BlockingChannel` did when a negative value is passed in (basically not call `set*BufferSize`)?
"
37479192,151,gwenshap,2015-08-19T23:00:29Z,"yep. Basically move the constants and their behavior to the new network classes.
"
37479252,151,ijuma,2015-08-19T23:01:11Z,"Sounds good.
"
37479424,151,gwenshap,2015-08-19T23:03:33Z,"I actually find the very generic ""any predicate"" or ""any find on a collection"" challenging. I'm concerned that future contributors won't quite know what to do with those.

I'm thinking that baking the specific ""find"" we actually have into a wrapper, will help.
So:
pollUntilRecievingResponseFromConnection(selector, timeout, connection_id)
"
37567314,151,gwenshap,2015-08-20T18:53:17Z,"nice :)
"
37606457,151,junrao,2015-08-21T04:49:32Z,"We need to block until the connection is established, right?
"
37606471,151,junrao,2015-08-21T04:50:02Z,"We will need to handle disconnects (like what NetworkClient does) by checking selector.disconnected(). If a socket is disconnected, there is no need to wait for the timeout. We should just throw an IOException back. Ditto to the selector.poll() below.
"
37606475,151,junrao,2015-08-21T04:50:16Z,"We haven't been using the s notation so far. To be consistent, perhaps it's better to use the string format thing for now and do a global replacement at some point if we feel that's better?
"
37606486,151,junrao,2015-08-21T04:50:28Z,"Since there is a selector per broker, we need to add a broker id tag for the metric tag.
"
37614464,151,ijuma,2015-08-21T08:12:40Z,"I thought we did not as we will eventually block after the `send`, which happens a few lines below. Have I misunderstood how the `Selector` should be used in this case?
"
37614569,151,ijuma,2015-08-21T08:14:11Z,"Makes sense, will do.
"
37614852,151,ijuma,2015-08-21T08:19:05Z,"I considered that, but we have over 800 instances of `format` in our codebase, so it would be quite a painful global replacement (with potential for introducing bugs). String interpolation is safer and performs better than `.format`, so I thought we could use it for new code where it made sense. We use it in a small number of places already. Please let me know if you still think I should change it.
"
37614934,151,ijuma,2015-08-21T08:20:33Z,"Will do. I'm not really sure how these tags work to be honest, but I'll see if I can find an existing example. :)
"
37626125,151,ijuma,2015-08-21T11:28:49Z,"@junrao, I added the `broker-id` tag below. Does it still make sense to include `broker.id` in the `metricGrpPrefix` above? 
"
37651956,151,junrao,2015-08-21T16:34:42Z,"If you look at NetworkClient, the way that it uses Selector is the following. Before NetworkClient can send any request, it has to make sure the connector is connected and the channel is ready. If you don't follow this protocol, things can get weird. For example, in SSL, after we finish the handshake, we will turn off the interest bit for write. If the interest bit for write has already been turned on by a request sent before the handshake completes, the send may never complete.

Thinking a bit more about this. It seems to implement a blocking channel on the Selector, we will need most of the connection state management logic in NetworkClient. The problem with using NetworkClient is that it's tied to Metadata refresh, which is not needed. We can either add an option to turn off the metadata part in NetworkClient and use it to implement the blocking channel. Alternatively, we can implement a BlockingSelector on top of Selector, but has to copy some of the state management logic from NetworkClient over. Not sure which one is better.
"
37651974,151,junrao,2015-08-21T16:34:54Z,"metricGrpPrefix is just a prefix of the metrics group, which is a string. Tags give metadata in key/value pairs, which is more informative.
"
37653240,151,ijuma,2015-08-21T16:48:38Z,"OK, I see. I'll check if turning off metadata in `NetworkClient` can be done without too much complexity.
"
38272677,151,ijuma,2015-08-30T15:23:09Z,"Will add javadoc.
"
38272682,151,ijuma,2015-08-30T15:23:15Z,"Will add javadoc.
"
38272693,151,ijuma,2015-08-30T15:23:58Z,"Will add `and version` at the end of the sentence.
"
38272702,151,ijuma,2015-08-30T15:25:30Z,"V1 was added during the development of 0.8.3 so we could change this to be like V0 if we think it's important.
"
38272718,151,ijuma,2015-08-30T15:27:19Z,"Will add `scaladoc` to this class and methods.
"
38279205,151,junrao,2015-08-31T01:03:19Z,"Do we need to wait for the next poll() call? disconnect() will cancel the key, after which the key will never be selected again.
"
38279208,151,junrao,2015-08-31T01:03:29Z,"It seems that we also need to remove nodeId from ClusterConnectionStates?
"
38279209,151,junrao,2015-08-31T01:03:35Z,"Need to add @override.
"
38279211,151,junrao,2015-08-31T01:03:42Z,"Need to add @override.
"
38279215,151,junrao,2015-08-31T01:03:47Z,"Need to add @override.
"
38279216,151,junrao,2015-08-31T01:03:54Z,"Do we need to pass mode in? Could we just get it from configs?
"
38279217,151,junrao,2015-08-31T01:04:00Z,"This field should be named live_leaders.
"
38279219,151,junrao,2015-08-31T01:04:04Z,"This can just be referencing LEADER_AND_ISR_REQUEST_PARTITION_STATE_V0.
"
38279226,151,junrao,2015-08-31T01:04:16Z,"Good catch. It's probably too late to change that though.
"
38279229,151,junrao,2015-08-31T01:04:22Z,"leaders is better named as liveLeaders.
"
38279232,151,junrao,2015-08-31T01:04:33Z,"It's probably better to create two constructors, one for each version. We can then mark the v0 constructor as deprecated and can remove it in the future.
"
38279234,151,junrao,2015-08-31T01:04:38Z,"Both v0 and v1have HOST_KEY_NAME. Perhaps we should check field SECURITY_PROTOCOL_TYPE_KEY_NAME?
"
38279239,151,junrao,2015-08-31T01:04:44Z,"Should we mock the other new method disconnect() too?
"
38279241,151,junrao,2015-08-31T01:04:49Z,"response version should be 1.
"
38279242,151,junrao,2015-08-31T01:04:51Z,"response version should be 1.
"
38279278,151,junrao,2015-08-31T01:07:21Z,"We probably want maxInflightRequests to be 1 so that it's consistent with the current behavior in BlockingChannel. 
"
38279285,151,junrao,2015-08-31T01:07:47Z,"There are a few unused imports.
"
38279287,151,junrao,2015-08-31T01:07:55Z,"selector should now be NetworkClient. There are a few other mentions of selector below.
"
38279289,151,junrao,2015-08-31T01:08:00Z,"$socketTimeoutException should be $socketTimeoutMs, right?
"
38279291,151,junrao,2015-08-31T01:08:06Z,"We probably want maxInflightRequests to be 1 so that it's consistent with the current behavior in BlockingChannel. 
"
38279296,151,junrao,2015-08-31T01:08:14Z,"Do we need to do this? Nodes are only used for sending MetadataRequest and we are not doing that here.
"
38279300,151,junrao,2015-08-31T01:08:36Z,"We probably want to move the blocking support to the client side. In the future, we likely will be writing the admin tools in java and it potentially will need to make blocking request calls too.
"
38279310,151,junrao,2015-08-31T01:09:04Z,"If the connection didn't fail, is there any reason to back off? It seems that we should be calling networkClient.poll() with the timeout immediately.
"
38279316,151,junrao,2015-08-31T01:09:31Z,"Perhaps it's more convenient to include to logic to wait for the connection being ready here. This way, we are guaranteed that every time we send a request, the socket is ready. Currently, in ControllerChannelManager, if we can't establish a socket connection, it seems that we can send a request before the channel is ready.
"
38292654,151,ijuma,2015-08-31T08:21:53Z,"I copied that documentation from `Selector.disconnect`. There is the following code in `Selector.poll`:

```
                    if (!key.isValid()) {
                        close(channel);
                        this.disconnected.add(channel.id());
                    }
```

And there is the following test in `NetworkClientTest`:

```
        assertTrue(""Now the client is ready"", client.ready(node, time.milliseconds()));
        selector.disconnect(node.idString());
        client.poll(1, time.milliseconds());
        selector.clear();
        assertFalse(""After we forced the disconnection the client is no longer ready."", client.ready(node, time.milliseconds()));
        assertTrue(""Metadata should get updated."", metadata.timeToNextUpdate(time.milliseconds()) == 0);
```

So, that lead me to believe that a poll was needed to update the internal state of the `Selector` and `NetworkClient`. The test above fails without the `poll` line (although it does use a `MockSelector`, so the bug could be there).

What are your thoughts based on this additional information?
"
38292972,151,ijuma,2015-08-31T08:27:34Z,"I did think about setting the state to DISCONNECTED (which has a similar effect), but then I thought that this would be updated automatically via `poll` as per the comment added in the discussion around the javadoc for `KafkaClient.poll`. But maybe the right thing is to do update the state here as you suggest. Do you think we should remove the id or set the state to `DISCONNECTED`?
"
38296748,151,ijuma,2015-08-31T09:30:40Z,"The 4 places where this is used all pass this parameter explicitly (as `SSLFactory.Mode.{Client,Server}`). We could put a key into the configs map, but I think that would make things more opaque with no clear benefit. What do you think?
"
38296826,151,ijuma,2015-08-31T09:32:11Z,"Will fix. We also have `alive_brokers` elsewhere, should we be calling that `live_brokers` instead for consistency?
"
38296887,151,ijuma,2015-08-31T09:33:21Z,"OK, will do. I wasn't sure what was our policy when it came to referencing schemas from other APIs in the same class. Good to know.
"
38296946,151,ijuma,2015-08-31T09:34:04Z,"Will do.
"
38296965,151,ijuma,2015-08-31T09:34:20Z,"Will do.
"
38297159,151,ijuma,2015-08-31T09:37:48Z,"In V1, `HOST_KEY_NAME` is in the `end_points` struct though, so it should be fine either way as far as I can see. I'll change it to `SECURITY_PROTOCOL_TYPE_KEY_NAME` as it's clearer though.
"
38297285,151,ijuma,2015-08-31T09:39:41Z,"It was already there previously.
"
38297303,151,ijuma,2015-08-31T09:39:56Z,"Good catch, will fix.
"
38297314,151,ijuma,2015-08-31T09:40:10Z,"Good catch, will fix.
"
38297556,151,ijuma,2015-08-31T09:44:08Z,"I think the behaviour should be consistent anyway as we block for each request. Setting this to `1` will enforce it, so I'll change it.
"
38297677,151,ijuma,2015-08-31T09:46:00Z,"Will fix.
"
38297917,151,ijuma,2015-08-31T09:49:38Z,"I'll change this to say `NetworkClient`/`Selector`. I think the other `Selector` mentions are fine.
"
38297929,151,ijuma,2015-08-31T09:49:53Z,"Good catch, will fix.
"
38297986,151,ijuma,2015-08-31T09:50:55Z,"Similar to the other instance of this, I think the behaviour is still correct, but I'll change it to be clearer/safer.
"
38298281,151,ijuma,2015-08-31T09:56:12Z,"We don't strictly need it, but `KafkaClient.leastLoadedNode` would return `null` if we don't do this. As you say, we don't use that method so it would have no effect right now if we remove it. I did that because I thought it may help avoid surprises in the future, but I don't have a strong opinion on it. Do you still think it should be removed?
"
38299301,151,ijuma,2015-08-31T10:12:13Z,"I checked this and I think 0 is correct for `LeaderAndIsrResponse`. Or am I missing something?
"
38299668,151,ijuma,2015-08-31T10:19:05Z,"I think you meant `ControlledShutdownResponse`, I fixed that.
"
38300557,151,ijuma,2015-08-31T10:33:47Z,"Is this something we want to do now or when the need arises? I didn't want to add this to the clients jar because I thought it was just useful to help us transition the broker code without bigger changes. It seems like there may be more use cases, but isn't it better to wait until they are concrete before moving the code to the clients jar?
"
38303535,151,ijuma,2015-08-31T11:31:47Z,"Actually I can't make this change because `SECURITY_PROTOCOL_TYPE_KEY_NAME` will never exist at this level. I could check for `ENDPOINTS_KEY_NAME`, but I think the current way is less error-prone (as endpoints could be empty due to a bug perhaps). So I'll leave it as is unless you disagree.
"
38303909,151,ijuma,2015-08-31T11:38:38Z,"Will fix.
"
38303913,151,ijuma,2015-08-31T11:38:45Z,"Will fix.
"
38303918,151,ijuma,2015-08-31T11:38:53Z,"Will fix.
"
38304564,151,ijuma,2015-08-31T11:50:35Z,"Perhaps. I'm not sure though. It's easy enough for the caller to call `blockingReady` if that is desired, right? I didn't do that in `ControllerChannelManager` because the existing code already handles the exception that would be thrown if the initial connection fails:

```
case e: Throwable => // if the send was not successful, reconnect to broker and resend the message
  warn((""Controller %d epoch %d fails to send request %s to broker %s. "" +
    ""Reconnecting to broker."").format(controllerId, controllerContext.epoch,
      request.toString, toBroker.toString()), e)
  networkClient.disconnectAndPoll(brokerNode.idString, 0)(time)
  connectToBroker()
  isSendSuccessful = false
  // backoff before retrying the connection and send
  CoreUtils.swallowTrace(Thread.sleep(300))
```

The existing code is not the prettiest, but I tried not to make too many changes at this point. I think we should consider redesigning the controller/broker communication to make better use of `NetworkClient` in the future.
"
38306320,151,ijuma,2015-08-31T12:21:10Z,"I did this, but it's worth being aware of the downsides (and why I had avoided that in the first place):
- We now have a number of warnings due to the fact that we call the deprecated constructor
- The calling code is more complex as it needs to pass slightly different data to each constructor (we still support both versions from the calling code and previously it just had to pass a version)
- A new deprecated inner class `BrokerEndPoint` had to be introduced for the V0 constructor
"
38313413,151,ijuma,2015-08-31T13:54:32Z,"I'll remove `retryBackoff`. It's how I had it at first, but I was seeing some issues where `poll` was returning immediately which was causing the thread to spin and seemed to be causing more connection timeouts than usual. I now think that this was caused by a bug in the handling of the connection failures (which I fixed before reopening the PR).
"
38332668,151,junrao,2015-08-31T16:58:29Z,"Yes, that's how MockSelector is implemented. In the real selector, my understanding is that if you disconnect a client, we cancel the key. The cancelled key will never be selected by the selector again. So, you would have to set the connection state in the same disconnect call. If that's the case, we probably need to change the behavior of MockSelector.
"
38332742,151,junrao,2015-08-31T16:59:17Z,"This is related to the comment above. If a client calls disconnect, it may not be interested in using this connection any more. So, we probably want to fresh the memory by removing the id from the connection state.
"
38332766,151,junrao,2015-08-31T16:59:27Z,"What you have is fine. I didn't realize that mode is implicit and not an explicit property.
"
38332782,151,junrao,2015-08-31T16:59:34Z,"What you had is fine.
"
38332818,151,junrao,2015-08-31T16:59:55Z,"It's fine if we set the nodes explicitly. If so, we should do the same for the ManualMetadataUpdater used in ControllerChannelManager.
"
38332943,151,junrao,2015-08-31T17:01:04Z,"Yes, the logic in ControllerChannelManager is not pretty. The issue that I see is that if connectToBroker() fails to establish a connection in time, we just disconnect. So, by the time you send a request, there is not guarantee that the connection is ready.
"
38335942,151,junrao,2015-08-31T17:31:06Z,"We can keep the blocking logic in scala for now.
"
38336063,151,ijuma,2015-08-31T17:32:10Z,"We do indeed, but at construction time because we use a `NetworkClient` for each broker connection, I paste the line below:

`new ManualMetadataUpdater(Seq(brokerNode).asJava)`
"
38336694,151,ijuma,2015-08-31T17:37:51Z,"Yes, I understand. So, if the connection is not ready, an `IllegalStateException` will be thrown which will cause a disconnect (no-op) followed by another connect and then we will retry again after a 300ms sleep since `isSendSuccessful === false`. This behaviour is very similar to what would happen with `BlockingChannel` where a ClosedChannelException would be thrown if `send` was called and the channel was not connected due to a failure.

I can add a `blockingReady` call before the `send` if you think that is better though.
"
38337234,151,ijuma,2015-08-31T17:42:47Z,"I see. I'll test the behaviour of the real selector just to be sure. In the likely case that it matches your description, I will do the following:
- Update `MockSelector`
- Update `NetworkClientTest`
- Update `Selector.disconnect` and `NetworkClient.disconnect` documentation
- Update implementation of `NetworkClient.disconnect` to update connection states
- Remove `NetworkClientBlockingOps.disconnectAndPoll` as `disconnect` will be sufficient

Have I covered everything?
"
38338561,151,junrao,2015-08-31T17:53:43Z,"Yes, relying on IllegalStateException probably works, but it's kind of hacky. IllegalStateException is meant to capture all programming errors, and using it for error handling feels wrong.
"
38338732,151,junrao,2015-08-31T17:55:07Z,"Got it. I missed that you can pass in the nodes from the constructor.
"
38338901,151,junrao,2015-08-31T17:56:31Z,"That sounds good.
"
38340333,151,ijuma,2015-08-31T18:08:22Z,"OK, I will add a `blockingReady` before the send then.
"
38400639,151,ijuma,2015-09-01T09:30:33Z,"We have `liveLeaders` in `LeaderAndIsrRequest`. Do we want to keep this as `aliveBrokers` or would it be better to call it `liveBorkers`?
"
38423086,151,ijuma,2015-09-01T14:16:52Z,"I did this here instead of changing `blockingSendAndReceive` to make it easier to maintain the previous behaviour of logging once a connection is established.
"
38423154,151,ijuma,2015-09-01T14:17:33Z,"This method already existed in `NetworkClient`, I just added it to the interface.
"
38423354,151,ijuma,2015-09-01T14:19:08Z,"I named this `close` instead of `disconnect` to match the relevant method in `Selector`, but I'd be interested in feedback regarding the naming. Also, it's not clear to me when `Selector.disconnect` is actually useful.
"
38473289,151,junrao,2015-09-01T21:16:58Z,"Is that added?
"
38473306,151,junrao,2015-09-01T21:17:04Z,"This is not really an instance of InvalidMetadataException.
"
38473310,151,junrao,2015-09-01T21:17:09Z,"This is not really an instance of InvalidMetadataException.
"
38473361,151,junrao,2015-09-01T21:17:30Z,"Even though UpdateMetadataRequest_v0 has identical structure as LeaderAndIsrRequest_v0, the set of brokers used are slightly different. In UpdateMetadataRequest_v0, we pass in all live brokers. In LeaderAndIsrRequest_v0, we pass in all live brokers that are the leaders. So, we can keep the names as they are.
"
38473426,151,junrao,2015-09-01T21:17:59Z,"In version 0, we should only allow passing in a single BrokerEndPoint.
"
38473454,151,junrao,2015-09-01T21:18:06Z,"Now that we have the broker tag, the group prefix can just be ""controller-channel"" or sth like that.
"
38473470,151,junrao,2015-09-01T21:18:13Z,"Is the if statement needed since the test is already done in networkClient.blockingReady()?
"
38475260,151,ijuma,2015-09-01T21:35:08Z,"Is it a `RetriableException` or `ApiException` then?
"
38475273,151,ijuma,2015-09-01T21:35:16Z,"Is it a `RetriableException` or `ApiException` then?
"
38475371,151,ijuma,2015-09-01T21:36:00Z,"Will fix.
"
38475773,151,ijuma,2015-09-01T21:40:14Z,"OK. We don't need to include the controller id in the group prefix or as a tag?
"
38476438,151,ijuma,2015-09-01T21:46:57Z,"It's done this way to only log the ""Controller %d connected"" message if the connection is not already ready.
"
38477988,151,junrao,2015-09-01T22:02:59Z,"ApiException for both.
"
38478139,151,junrao,2015-09-01T22:04:55Z,"We don't need the broker id in group prefix. Still need the broker id in the metrics tag.
"
38478285,151,junrao,2015-09-01T22:07:00Z,"Ok. We can leave it as it is. Thanks for the explanation.
"
38478450,151,ijuma,2015-09-01T22:09:06Z,"Sorry, my question was about the controller id (I had understood the point about the broker id).
"
38478876,151,ijuma,2015-09-01T22:14:31Z,"Are you sure about this? Looking at the Scala code, it does:

Read:
`case 0 => for(i <- 0 until numAliveBrokers) yield new Broker(BrokerEndPoint.readFrom(buffer),SecurityProtocol.PLAINTEXT)`

Write:
``case 0 => aliveBrokers.foreach(_.getBrokerEndPoint(SecurityProtocol.PLAINTEXT).writeTo(buffer))`
"
38483970,151,ijuma,2015-09-01T23:18:34Z,"Not yet, I was focusing on getting the behaviour right first. I'll add all the missing javadoc/scaladoc in my next commit.
"
38498073,151,junrao,2015-09-02T04:29:17Z,"Actually, my comment wasn't right. What you had is right since in v0, we pass in a set of BrokerEndPoint and v1, we pass in a set of Broker.
"
38521906,151,ijuma,2015-09-02T11:30:39Z,"Yes, I understand the difference between brokers and leaders. The bit I was asking about is the `live` prefix (in `liveLeaders`) versus the `alive` prefix (in `aliveBrokers`). It seemed to me that `liveBrokers` means the same and the prefix would be consistent. Anyway, not a big deal, so fine to keep as is if you prefer.
"
38559331,151,junrao,2015-09-02T17:21:34Z,"Yes, I agree that it's better to make the names consistent. So we can use live_brokers and live_leaders here and in protocols as well.
"
114734810,2967,ijuma,2017-05-04T09:43:29Z,Are the `private` -> `protected` changes still needed?
114739200,2967,ijuma,2017-05-04T10:09:24Z,"For consistency, it may make sense for `wrapForOutput` to take a `ByteBuffer` too."
114740176,2967,ijuma,2017-05-04T10:15:34Z,"We could change `DefaultRecord.readFrom` to take a `DataInput` (instead of `DataInputStream`), change `KafkaLZ4BlockInputStream` to implement `DataInput` and then only wrap if the returned value is not already a `DataInput`. That would remove a layer of indirection and if it's possible to implement `readFully` more efficiently in `KafkaLZ4BlockInputStream`, then it could be a win. If you have time, it may be worth a try."
114742214,2967,ijuma,2017-05-04T10:27:46Z,"It wasn't clear to me why we needed both `theBuffer` and `decompressionBuffer`. It seems that we rely on the fact that `theBuffer` is only populated after the first `readBlock` to implement `available` correctly. Is that the only difference? If so, would it not be simpler to have a single buffer that is allocated for the first time in `readBlock`?"
114742293,2967,ijuma,2017-05-04T10:28:16Z,`mark()` and `reset()` are synchronized and probably should not be.
114742552,2967,ijuma,2017-05-04T10:29:51Z,I think the default should be `CURRENT_MAGIC_VALUE`.
114742615,2967,ijuma,2017-05-04T10:30:16Z,Why did you remove `NONE` from this list? It seems useful to have the uncompressed baseline.
114742753,2967,ijuma,2017-05-04T10:31:07Z,Seems like more fields could be final in this class.
114743094,2967,ijuma,2017-05-04T10:33:53Z,You should probably use `AbstractRecords.sizeInBytesUpperBound`.
114744092,2967,ijuma,2017-05-04T10:41:09Z,"To make the benchmark less dependent on implementation details, it would be better if it used `MemoryRecords.readableRecords(...).batches()`. Same for the other benchmark."
114744933,2967,ijuma,2017-05-04T10:46:15Z,Maybe this should be `RecordBatchIterationBenchmark`?
114746255,2967,ijuma,2017-05-04T10:55:31Z,"We followed the same approach for Snappy and LZ4 to deal with the possibility that the libraries are not in the classpath because it's not supported in a given platform. However, we are using Kafka classes for the block input stream and block output stream. As such, we can probably reference the constructor directly. We won't invoke the constructor unless LZ4 is configured and it's OK to fail in that case.

Does that make sense?"
114746515,2967,ijuma,2017-05-04T10:57:30Z,Why do we need this?
114807909,2967,xvrl,2017-05-04T15:14:02Z,"we don't need them anymore, indeed"
114808828,2967,xvrl,2017-05-04T15:17:27Z,sure
114808856,2967,xvrl,2017-05-04T15:17:33Z,"the benchmark is in the `org.apache.kafka.common.record` package. Now that we don't rely on package protected classes anymore, we can move it to `org.apache.kafka.jmh` and remove this."
114810971,2967,xvrl,2017-05-04T15:24:57Z,`theBuffer` is also pointed directly to the input buffer when a block is not compressed to avoid copying bytes.
114811084,2967,xvrl,2017-05-04T15:25:21Z,"hmm, not sure why they were in the first place"
114812768,2967,ijuma,2017-05-04T15:31:22Z,"Copy and paste, I bet."
114812862,2967,ijuma,2017-05-04T15:31:42Z,Sounds good.
114814192,2967,ijuma,2017-05-04T15:36:30Z,"I had noticed that, but didn't think through how that could work if we had a single buffer variable. OK, seems simplest to have two buffers. Can we add a comment explaining this?"
114824952,2967,xvrl,2017-05-04T16:19:55Z,"seems like this is not as straightforward as it seems. MemoryRecordsBuilder relies on having access to the underlying ByteBuffer held by the ByteBufferOutputStream, which would break if the buffer is expanded in ByteBufferOutputStream"
114826480,2967,xvrl,2017-05-04T16:26:38Z,"makes sense, as far as I know lz4 falls back to a pure-java version anyway, so it should be safe regardless (unless there are other platform issues that are unrelated to native code)"
114826699,2967,xvrl,2017-05-04T16:27:43Z,"I'd prefer to do that in a follow-up PR in trunk, since I'd also like to backport this to 0.10.2.x"
114826741,2967,xvrl,2017-05-04T16:27:56Z,will do
114828336,2967,xvrl,2017-05-04T16:35:40Z,"note however that we store the LZ4SafeDecompressor as a static field, so loading the Kafka class would also trigger loading the lz4 classes."
114873529,2967,xvrl,2017-05-04T20:00:12Z,fixed
114873560,2967,xvrl,2017-05-04T20:00:19Z,fixed
114874156,2967,xvrl,2017-05-04T20:03:09Z,"When using `MemoryRecords.readableRecords(...)` we don't technically test the Compression.NONE ""compression"" method, but test a different code path, so it's not really comparable if you care about an upper bound on how fast decompression could be."
114874228,2967,xvrl,2017-05-04T20:03:31Z,I added it back so we could at least see what it means in practice though
114874277,2967,xvrl,2017-05-04T20:03:40Z,fixed
114874313,2967,xvrl,2017-05-04T20:03:52Z,good to know
114874345,2967,xvrl,2017-05-04T20:04:02Z,changed.
114876003,2967,xvrl,2017-05-04T20:12:03Z,"I was also thinking, if we avoided readFully altogether, we could avoid the EOFException penalty when reaching the end of the batch on legacy records and get a further 3x improvement for small legacy batches."
114877226,2967,ijuma,2017-05-04T20:18:00Z,"It's not the same code path, but that's OK. The comparison I'm looking for is a bit like the one in the LZ4 GitHub page where it compares memcpy with lz4 decompression (https://github.com/lz4/lz4)."
114879573,2967,ijuma,2017-05-04T20:28:49Z,"Hmm, `DataInputStream.readFully` only throws an exception if we ask it to read past the end of the InputStream. So supposedly, if we fix the underlying InputStream, it's enough either way. The following PR does that:

https://github.com/apache/kafka/pull/2025/files#diff-eaa7e4414f285da2ff8e4508456078d2L192"
115125280,2967,hachikuji,2017-05-06T17:42:08Z,"nit: this looks a little weird. could we just do
```java
private static final int BUFFER_SIZE = 12; // offset + size
```
Also, the name is a bit vague. Maybe `RECORD_HEADER_SIZE`?"
115125399,2967,hachikuji,2017-05-06T17:47:46Z,This block seems to be the same code as below. Perhaps we could move it into a `readFully` function in `Utils` (we have a couple of these for working with `FileChannel` already)?
117260682,2967,ijuma,2017-05-18T14:20:03Z,"Good point, can we please add a comment explaining the reason for the inconsistency?"
117292625,2967,ijuma,2017-05-18T16:19:57Z,"Yeah, but `KafkaLZ4BlockInputStream` itself should not be initialised until it's actually used. I made this change, used CompressionType.SNAPPY.wrap* methods while LZ4 was not in the classpath and it worked fine. The same exercise did not work for Snappy because `SnappyInputStream` was not in the classpath. Anyway, we can keep it as it is if you prefer as the performance impact is low when compared to other things (8ns instead of 4ns per construction)."
117300016,2967,xvrl,2017-05-18T16:54:17Z,done
117300539,2967,xvrl,2017-05-18T16:56:50Z,"ok, I've change it to construct the lz4 streams directly, and updated the comments accordingly."
118957598,2967,ijuma,2017-05-29T15:38:55Z,Is it safe to do this on the received buffer? `DeepRecordsIterator` doesn't seem to duplicate the buffer before calling `CompressionType.wrapForOutput`.
118962982,2967,ijuma,2017-05-29T16:25:02Z,"I'm not totally sure why we were previously using a `readUnsigned` method and now we're not. For many of the other cases where we do this, it's easy to verify that the values we care about work either way. The ones that weren't totally clear to me were this and the block checksum cases. Can you please elaborate why the change is safe in those two cases?"
118966334,2967,ijuma,2017-05-29T17:01:00Z,"Hmm, so this bug meant that the `blockSize > maxBlockSize` check was not working, right? Also, did we just get lucky that the `blockSize == 0` check worked?"
118970809,2967,ijuma,2017-05-29T17:52:18Z,Nit: is there a reason why we are modifying the existing block size instead of just setting the bad block size directly?
119135013,2967,xvrl,2017-05-30T15:35:05Z,"from reading the code, `ByteUtils.readUnsignedIntLE` is a misnomer and should probably be just be called `readIntLE`. Unlike `readUnsignedInt`, which returns a long and applies the proper bit mask, `readUnsignedIntLE` returns an integer and doesn't do anything other than shifting the bytes."
119140839,2967,xvrl,2017-05-30T15:55:42Z,"`DeepRecordsIterator` technically duplicates the buffer when calling [`wrapperRecord.value()`](https://github.com/apache/kafka/blob/trunk/clients/src/main/java/org/apache/kafka/common/record/AbstractLegacyRecordBatch.java#L310) and never reuses it, but I agree that could easily be missed or introduce odd bugs if that code were to be rewritten. I can add a `duplicate()` here just for safety."
119141976,2967,ijuma,2017-05-30T15:59:58Z,Sounds good.
119146762,2967,xvrl,2017-05-30T16:18:08Z,"Indeed, the size check wasn't working, but at the end of the day, we would just thrown a different exception in case the block size exceeded the max.

Regarding the zero size block, we were just lucky that we never set the `LZ4_FRAME_INCOMPRESSIBLE_MASK` flag in `KafkaLZ4BlockOutputStream.writeEndMark`. I checked the spec and it didn't seem to specify whether the flag should be set or not in that case, so better be safe."
119157414,2967,xvrl,2017-05-30T17:02:15Z,"yes, to make sure it works in both cases when the incompressible flag is set and when it is not."
119160775,2967,hachikuji,2017-05-30T17:15:48Z,"Should we rewind the buffer in case the position wasn't reset after the last use?

(Also nit: can we just call it `buffer`?)"
119161965,2967,hachikuji,2017-05-30T17:20:29Z,It's a little odd for this to live here given the interface is in `KafkaLZ4BlockInputStream`. I'd suggest either moving it there or pulling the `BufferSupplier` interface out of `KafkaLZ4BlockInputStream`.
119174351,2967,ijuma,2017-05-30T18:10:08Z,"Yeah, my PR removes all of these in favour of existing constants: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42"
119174470,2967,ijuma,2017-05-30T18:10:32Z,You and I had the exact same thought: https://github.com/apache/kafka/pull/3164/commits/a7bc3e6e98ad3f983f5619601b216e83142afe42
119174697,2967,ijuma,2017-05-30T18:11:25Z,3 for 3: https://github.com/apache/kafka/pull/3164/commits/8403cbb616dd88cd7b06dfe8910aa0d0cded8b6f#diff-ee888ddca9dfb15863eeddcd379f65c6R1
119174941,2967,ijuma,2017-05-30T18:12:24Z,That's one issue. The other is that if there are two `KafkaLZ4BlockInputStream` instances in the same thread: https://github.com/apache/kafka/pull/3164/commits/8403cbb616dd88cd7b06dfe8910aa0d0cded8b6f#diff-ee888ddca9dfb15863eeddcd379f65c6R36
119183125,2967,xvrl,2017-05-30T18:47:29Z,"@hachikuji technically we don't have to, KafkaLZ4BlockInputStream resets position and limit every time anyway when preparing the buffer for consumption.

@ijuma agree that would be a concern if someone were to consume from two separate consumers in the same thread. Happy to replace the buffer supplier implementation with the one you pointed to."
119185468,2967,hachikuji,2017-05-30T18:57:10Z,"@xvrl Yeah, figured that was the case, but it makes the code a bit brittle to depend on that."
107401674,2719,dguy,2017-03-22T12:36:06Z,I wonder if we should set this higher? is there any harm in setting it to `Integer.MAX_VALUE`?
107402332,2719,dguy,2017-03-22T12:39:55Z,Do you think we should maybe kill some more brokers? Or is that going to be too non-deterministic in terms of test failures?
107427012,2719,enothereska,2017-03-22T14:26:45Z,We probably can. The main problem I have right now is how to sidestep all the Kafka corner cases when it comes to failures while still showing that streams is resilient. Stay tuned.
107454080,2719,enothereska,2017-03-22T15:57:22Z,Ok
107481625,2719,enothereska,2017-03-22T17:36:40Z,@ijuma do these parameters make sense for a config that should not lose data? We are ok to have duplicates. Do I need to do anything with `unclean.leader.election`?
108192236,2719,dguy,2017-03-27T14:58:35Z,would be better to pass in an implementation of `Time` and use `time.sleep(1000L)` here.
108193858,2719,dguy,2017-03-27T15:04:12Z,We currently don't do anything about `min.insync.replicas` - should we?
108194089,2719,dguy,2017-03-27T15:05:03Z,"nit: ""active tasks {}, standby tasks {}, suspended tasks {}, and suspended standby tasks {}"""
108194869,2719,dguy,2017-03-27T15:07:41Z,"`new StreamsException(""Could not poll."", e)`"
108195049,2719,dguy,2017-03-27T15:08:16Z,"Apart from a `StreamsException` i think the only other exception that `poll` is going to throw is `IllegalStateException` - should we just handle this in `sendRequest` and leave this as it was.
Even if there are more exceptions, i think it would be better to handle it in `sendRequest` and throw a `StreamsException` from there"
108196783,2719,dguy,2017-03-27T15:15:24Z,i guess you can remove this now?
108197021,2719,dguy,2017-03-27T15:16:24Z,You can remove this now as you've set it as the default in `StreamsConfig`
108197175,2719,dguy,2017-03-27T15:16:59Z,Same with this one
108202868,2719,enothereska,2017-03-27T15:37:34Z,Yes we should. @norwood any insights on what you have found useful for that? Thanks.
108203239,2719,enothereska,2017-03-27T15:38:57Z,"Not quite, since in StreamsConfig it is the internal streams producer. Here it's another producer."
108262374,2719,norwood,2017-03-27T19:48:07Z,"something like `min(2, replicationFactor)` should be a good default.

i'd also be concerned about this override for `brokers.size() < replicationFactor`. i think i'd prefer we fail here, rather than getting in to a misconfigured state. we have run in to issues where a user brings up a cluster and as kafka is doing its thing also brings up their streams app. so during startup we see 1 broker, then sometime down the line broker 2...N. this was causing us to precreate a bunch of our streams topics incorrectly (when we only saw one broker), and then on restart we would try to verify topics against actual configs and fail. "
108262836,2719,norwood,2017-03-27T19:50:09Z,i think `validateTopicPartitions` should also validate `replicationFactor`
108263977,2719,mjsax,2017-03-27T19:55:21Z,we need a KIP if we change any default values...
108264259,2719,mjsax,2017-03-27T19:56:42Z,Do we really want to do this? I would strongly prefer to throw an exception to the user!
108264430,2719,mjsax,2017-03-27T19:57:23Z,I would not change this default value -- it's a hassle of anyone want to run a demo with local single broker setup.
108264962,2719,mjsax,2017-03-27T19:59:40Z,Ups. We really missed to close suspended tasks. Really bad :( Great catch Eno!
108265404,2719,mjsax,2017-03-27T20:01:29Z,nit: add `final`
108265542,2719,mjsax,2017-03-27T20:02:06Z,nit: add `final`
108265707,2719,mjsax,2017-03-27T20:02:53Z,nit: add `final`
108265721,2719,mjsax,2017-03-27T20:02:56Z,nit: add `final`
108268924,2719,norwood,2017-03-27T20:16:25Z,"maybe a more dynamic default here, like with `acks=all=-1`? 

e.g. set `replicationFactor=-1` => `actualReplicationFactor=min(3, brokers.size())`"
108276754,2719,enothereska,2017-03-27T20:51:12Z,"The code actually uses min(#brokers, REPLICATION_FACTOR), and prints a warning, but it still runs with, say, 1 broker."
108276951,2719,enothereska,2017-03-27T20:51:55Z,"This is consistent with how things like schema registry, proactive support etc handle cases when the number of brokers is less than replication factor. "
108277094,2719,enothereska,2017-03-27T20:52:35Z,"Do we? @guozhangwang ? I thought we needed a KIP to add new config values, not each time we tune them.  "
108277327,2719,enothereska,2017-03-27T20:53:28Z,"The goal here is to do the right thing when there are enough brokers, not to provide magic when there just aren't enough brokers (e.g., in a test environment). Currently we do the wrong thing when there are enough brokers."
108278624,2719,enothereska,2017-03-27T20:59:15Z,Yeah this was fun :)
108281740,2719,guozhangwang,2017-03-27T21:14:49Z,"I'd suggest throwing an exception here with the motivation similar to @norwood above. We have seen similar issues with offset topic num.partitions which we do this ""min(broker.size, required num.broekrs)"" trick and it introduces much more confusions than user-friendly benefits. For unit tests we should just always override these configs."
108282040,2719,guozhangwang,2017-03-27T21:16:24Z,Let's write a quick KIP for this (also including the default replication factor to 3 above)? I think they are mostly fixing a bug but would better making them well known in the community as well.
108282772,2719,guozhangwang,2017-03-27T21:20:17Z,"Also I feel 1 second maybe too long in production? In practice brokers should be up / running much earlier than streams apps. For @norwood cases I still think it's better to fail fast and educate users retry creating their apps after the broker is fully up than trying to wait for, say 5 seconds and hopefully it will succeed."
108283433,2719,guozhangwang,2017-03-27T21:23:29Z,These two functions are very similar: could we collapse them into one function `performOnTasks` and pass in a `List<AbstractTask>` as an additional parameters?
108283588,2719,guozhangwang,2017-03-27T21:24:15Z,Ditto above.
108283829,2719,guozhangwang,2017-03-27T21:25:22Z,Why we want to wrap even a RTE as a StreamsException here?
108283871,2719,guozhangwang,2017-03-27T21:25:38Z,Same here.
108288736,2719,mjsax,2017-03-27T21:49:35Z,"I think this dynamic change is quite dangerous -- if I specify replication 3 and cannot get it, I want an exception... Thus, I would leave replication factor to 1 for demoing purpose -- if anyone goes to production she can set to whatever value is suitable -- or we make the parameter non-optional.

I think it would be a hassle to have default value 3, and overwrite it with 1 in each example we do..."
108289042,2719,mjsax,2017-03-27T21:51:10Z,We really need a bug fix release for this! \cc @guozhangwang 
108292755,2719,enothereska,2017-03-27T22:09:39Z,"@guozhangwang @mjsax if I understand you right, you want the default 3, with the option for a user to set it to 1, right?
Or you want no change at all to what we currently have (default 1, user can set higher). I don't like the current option since it leads to trouble in production. I'm ok with the first option."
108292984,2719,enothereska,2017-03-27T22:11:04Z,I think a KIP unnecessarily slows things down. Why do we need to do a KIP to correct an internal flaw? Users are already expecting internal topics to be robust. I'd argue we're fixing a bug here.
108293564,2719,norwood,2017-03-27T22:14:25Z,"yeah, my suggestion was to make the dynamism configurable. 

REPLICATION_FACTOR_CONFIG= N where N >0 => i know what i want. streams should fail if it can't meet this contract
REPLICATION_FACTOR_CONFIG = -1 => streams is smart and can figure it out for me.

this allows *me* to be as anal retentive as i want, but allows the defaults to work for demos/tests/etc."
108301670,2719,mjsax,2017-03-27T23:06:09Z,"> REPLICATION_FACTOR_CONFIG = -1 => streams is smart and can figure it out for me.

What should this be than? Would we need another parameter ""default_replacation_factor"" and Streams can choose between 1 and this value? Not sure if this would not be too confusing."
108301836,2719,mjsax,2017-03-27T23:07:18Z,I am happy without a KIP :) Makes live easier for us. It's @guozhangwang call. Or any other committer.
108302114,2719,norwood,2017-03-27T23:09:18Z,"my suggestion above was `min(3, brokers.size())` 

i dont like this cause it seems like magic, but it also addresses most peoples issues. "
108302312,2719,mjsax,2017-03-27T23:10:37Z,"I would prefer to keep the current default 1 and mark the parameter importance as ""high"", indicating that one most likely wants to change the default if going to production. Default values must not be ""production ready"" settings IMHO (cf. `state.dir`). Default values should give the best ""out-of-the-box"" experience when getting started with you first ""word count"" -- ie., local single broker setting."
108341596,2719,enothereska,2017-03-28T06:20:32Z,"Ok, I cannot keep the default to 1. This is what led to several bugs. It's not great to expect users to set this parameter which streams should be maintaining correctly. "
108359843,2719,ijuma,2017-03-28T08:19:41Z,"@enothereska, as you know, we have changed the behaviour for the offsets topic so that we default to the safe production setting and fail otherwise. That is based on experience, as @guozhangwang said, and seems more relevant than some of the other examples given.

The question then is how to make it easy for development. For the offsets topic, we set the value to 1 in the `server.properties` that is used in the quickstarts, etc. That won't work here. There are a few possible solutions that will be helpful for this and many other configs:

1. Have a config where users can define whether the environment is prod or dev and change the defaults based on that.
2. Provide methods so that a user can get a prod or dev config. For example, `StreamsConfigs.production()` or `StreamConfigs.development()`.
3. Add an enum to the constructor of `StreamConfigs` where users can define if the environment is production or development.

I think I like `3` best. In any case, we don't need to block this PR on the long-term solution. Still, it may be worth figuring out the end state and then a plan on how to get there."
108364658,2719,enothereska,2017-03-28T08:44:11Z,cc @ijuma 
108368495,2719,ijuma,2017-03-28T09:01:55Z,"We typically do KIPs for config changes that impact users. KIP-106 is one such example. If you can make the case that this is an internal bug fix and has no compatibility impact, then no KIP is needed. The replication factor one would seemingly have a compatibility impact."
108471613,2719,enothereska,2017-03-28T16:35:58Z,This will now be done in a KIP and different PR.
108472487,2719,enothereska,2017-03-28T16:39:37Z,"I'm reducing the time, but passing in `Time` is a bit of a pain here and other calls also use Thread.sleep. Would prefer to do a cleanup pass later."
108472921,2719,enothereska,2017-03-28T16:41:17Z,"@mjsax I didn't get this, what should be `final`?"
108473345,2719,enothereska,2017-03-28T16:43:10Z,This class should hide all underlying network exceptions and wrap them with stream exception IMO. This is consistent with other examples in this class. Otherwise the upper layers would need to know all the details of the underlying classes.
108473773,2719,enothereska,2017-03-28T16:44:56Z,What exactly? I don't get this.
108479165,2719,enothereska,2017-03-28T17:07:45Z,This will now require a KIP. Will do in separate PR.
108479321,2719,enothereska,2017-03-28T17:08:29Z,This will now require a KIP and will be done in separate PR.
108479431,2719,enothereska,2017-03-28T17:09:01Z,This will now require a KIP and will be done in a separate PR. 
108481016,2719,mjsax,2017-03-28T17:15:57Z,-> `} catch (final Exception e) {`
108656204,2719,dguy,2017-03-29T11:52:48Z,Can we pass in `Time` and use `time.sleep(100L)` here?
108656295,2719,dguy,2017-03-29T11:53:14Z,Same as above
108892220,2719,dguy,2017-03-30T10:31:20Z,I'm thinking it might be better to have a field in `StreamPartitionAssignor` for time. It would default to `Time.System` and then have a package private method for overriding it in tests. This way we wouldn't need to make the `time` field on `StreamThread` public
108892245,2719,dguy,2017-03-30T10:31:29Z,See comment above
108894798,2719,enothereska,2017-03-30T10:46:16Z,But couldn't that lead to cases when `StreamThread` is given one type of time and `StreamPartitionAssignor` another type of time? I'm not sure what that would mean. I think they should use the same time type.
108895304,2719,dguy,2017-03-30T10:49:18Z,"In tests that could happen. I guess there are other `public final` fields (which i don't agree with), so whatever"
109059440,2719,guozhangwang,2017-03-30T23:13:34Z,I'd agree with Damian to have this Time object pass long the hierarchy than passing it from the thread directly to the internal topic manager. I would not worry too much about passing in different objects since both of them are internal topics so the only place we may directly pass the object is in unit tests.
109060051,2719,guozhangwang,2017-03-30T23:18:48Z,"For suspended tasks, could the closure process be simpler? For example we have already closed the topology as well as committing the states, etc. Ditto below."
109073610,2719,mjsax,2017-03-31T01:39:01Z,"Nit: I think the error message does not read well:
`Not enough brokers 2 for replication factor 3`

Better: `Found only 2 brokers, but replication factor is 3. Decrease replication factor for internal topics via StreamsConfig parameter ""replication.factor"" or add brokers to your cluster.`
"
109073704,2719,mjsax,2017-03-31T01:40:35Z,Should we increase backup time if we keep retrying?
109073754,2719,mjsax,2017-03-31T01:41:19Z,As above?
109074329,2719,mjsax,2017-03-31T01:49:23Z,Why do we need a `node` now but not before?
109074528,2719,mjsax,2017-03-31T01:52:08Z,Didn't we change the default values for both already?
109074584,2719,mjsax,2017-03-31T01:52:51Z,add `final` to `Properties props = new Properties();` and method parameters. Also all variables used farther down.
109074646,2719,mjsax,2017-03-31T01:53:37Z,Rename `props` to `producerProps` ;)
109074740,2719,mjsax,2017-03-31T01:54:38Z,Not sure what's happening here. But `6` does still not seem to be large. Why change it in the first place?
109075186,2719,mjsax,2017-03-31T02:01:04Z,"We have replication factor 3 and min.in.sync.replicas 2 -- it might happen that if 2 brokers fail, a topic does not have enough in-sync-replicates anymore?"
109114808,2719,enothereska,2017-03-31T08:47:50Z,"I could add an extra method in StreamTask.java to close the rest, not the topology. However, this only happens at shutdown, not sure it's worth it. So it could be simpler, but with more lines of code. "
109115463,2719,enothereska,2017-03-31T08:51:36Z,"Traditionally we don't do anything clever with backoff times throughout the code. It can get complicated, e.g., by how much to increase in each step."
109115631,2719,enothereska,2017-03-31T08:52:28Z,"Because now in `makeReady` we check the number of brokers in metadata, whereas before we didn't. "
109117043,2719,enothereska,2017-03-31T09:01:02Z,We decided that changing the internal default values required a KIP. For now just changing the test values in this PR.
109117552,2719,enothereska,2017-03-31T09:03:53Z,"This is effectively the timeout of the test. With higher replication factor for the internal topics, and with acks to ""all"" I noticed that the test takes slightly longer to run, so upped this value."
109117773,2719,enothereska,2017-03-31T09:04:50Z,"Yeah that's fine. This just means that when there is no failure, keep 2 replicas in sync. When there are 2 failures, the system should still be up and running."
109203917,2719,mjsax,2017-03-31T16:54:28Z,"Depends on the use case: either add the same value each time, or double up. We this when waiting for locks to get released on rebalance already:
https://github.com/apache/kafka/blob/trunk/streams/src/main/java/org/apache/kafka/streams/processor/internals/StreamThread.java#L1228"
109208623,2719,guozhangwang,2017-03-31T17:19:31Z,Makes sense.
109279169,2719,enothereska,2017-04-01T06:40:45Z,I'm not ready for this yet. We can revisit backoffs in all the streams code and see how the networking client code has done them. Also we should tie these numbers to some sort of config users can set. 
649771532,10822,erdody,2021-06-11T07:56:17Z,Nit: or empty **if** this worker ....
649777408,10822,erdody,2021-06-11T08:05:50Z,"Nit: This actually collects the state of all tasks, so it's only empty if there are no tasks, right?"
649778786,10822,erdody,2021-06-11T08:08:00Z,Nit: can use a method reference
649791058,10822,erdody,2021-06-11T08:26:54Z,"There are a few comments in different places explaining the special equality implementation in RestartRequest.
Have we considered making this a Map<String, RestartRequest> to make it explicit that we keep the latest per connector, have a more typical equals/hashcode and avoid all the warnings?"
649792398,10822,erdody,2021-06-11T08:28:48Z,"Just out of curiosity, any particular reason why we want to process these in connectorName order? (instead of FIFO)"
649797930,10822,erdody,2021-06-11T08:36:52Z,Nit: Use a message similar to the one you corrected in line 1030?
649800302,10822,erdody,2021-06-11T08:40:27Z,"Just wondering: could blocking the addition of new entries be a problem, considering that this method can take some time? Would it be worth creating a copy of the collection to minimize the synchronized time?"
649803403,10822,erdody,2021-06-11T08:44:48Z,"Nit: quotes around connectorName, like in the line below"
649806839,10822,erdody,2021-06-11T08:49:53Z,The return value is only used by tests. Can we assert based on other methods calls instead?
651253521,10822,erdody,2021-06-14T20:26:32Z,Nit: Move up so you can share with 270?
651262244,10822,erdody,2021-06-14T20:40:49Z,Nit: `Boolean.parseBoolean()`
651291606,10822,kpatelatwork,2021-06-14T21:30:52Z,done
651291754,10822,kpatelatwork,2021-06-14T21:31:09Z,Fixed
651291878,10822,kpatelatwork,2021-06-14T21:31:22Z,Fixed
651292365,10822,kpatelatwork,2021-06-14T21:32:16Z,Fixed
651292417,10822,kpatelatwork,2021-06-14T21:32:22Z,Fixed
651293447,10822,kpatelatwork,2021-06-14T21:34:20Z,"I agree it is used by only tests but it made the tests more clear rather than relying on mock verify.  I added a comment to the code to make this clear, Please let me know if the new changes looks good."
651296666,10822,kpatelatwork,2021-06-14T21:40:35Z,"We can't make a copy because we are doing pollFirst and its removing it out of set
`        while ((request = pendingRestartRequests.pollFirst()) != null) {
`

The whole method synchronization was done deliberately to keep the code simple. One more point is that this just triggers the start of connector/tasks and real start happens in another thread so this should be pretty fast."
651298044,10822,kpatelatwork,2021-06-14T21:43:12Z,"no particular reason, TreeSet was a NavigableSet implementation that came to my mind.  But I like your above idea of using a map and simplifying the code, Let me work on it. "
651300080,10822,kpatelatwork,2021-06-14T21:46:49Z,"Good idea, Fixed"
651323170,10822,kpatelatwork,2021-06-14T22:38:07Z,"Fixed, Could you please check to see if it looks good now?"
651323500,10822,kpatelatwork,2021-06-14T22:38:54Z,new implementation with iterator makes this explicit. Could you please check to see if it looks good now?
651508714,10822,erdody,2021-06-15T07:09:53Z,"The main problem is that you're testing the code you added for tests, not that the actual actions are executed.
Unless you're also have coverage to verify that there's correspondence between the different results and the actions that need to happen in each case.
"
651509086,10822,erdody,2021-06-15T07:10:23Z,"AFAICS, since all accesses are synchronized, this doesn't need to be concurrent."
651852657,10822,kpatelatwork,2021-06-15T14:32:29Z,Fixed
651854001,10822,kpatelatwork,2021-06-15T14:33:51Z,I removed the return and fixed the tests just to power mock verify. Anyway I have Integration tests so I partially agree that the return values were just me being paranoid in testing.
652845546,10822,rhauch,2021-06-16T16:16:53Z,"Can we replace these two methods with the following?
```
    private boolean isRestarting(ConnectorStateInfo.AbstractState state) {
        return isRestarting(state.state());
    }
```"
652846118,10822,rhauch,2021-06-16T16:17:35Z,"Nit:
```suggestion
```"
652848315,10822,rhauch,2021-06-16T16:20:31Z,"This method name is very similar to the `includeTasks()` getter method. WDYT about changing to `isTaskIncluded(TaskStatus status)` instead, to more clearly differentiate that it's not merely a getter?

If so, then `includeConnector(...)` could be renamed to `isConnectorIncluded(ConnectorStatus status)`, too."
652850147,10822,rhauch,2021-06-16T16:22:43Z,"Nit:
```suggestion
    // The latest pending restart request for each named connector
```"
652852357,10822,rhauch,2021-06-16T16:25:39Z,"If the `doRestartConnectorAndTasks(..)` method fails, then we won't remove the restart request and the herder will never make progress. Should we protect this call a bit more with a try-catch-finally block?"
652854145,10822,rhauch,2021-06-16T16:28:00Z,"Another option might be to dequeue (from the map) the restart requests in a synchronized block, and then perform those restarts outside of the synchronized block. This would at least minimize the work being done within the synchronized block to be just iteration and removal, lessoning the likelihood of blocking new requests. WDYT?"
652858133,10822,rhauch,2021-06-16T16:33:15Z,"Of course, another option is to use a concurrent queue rather than a map, and deduplicate the requests only upon processing. For example, this method could dequeue all of them to a collection, replacing any earlier request with a more recent one, and then restart all of the deduplicated requests (ideally in the same order in which they were received). I'm not sure this is actually simpler.

Or, did you consider using a ConcurrentMap to avoid synchronization? That may be the least complicated way to remove the synchronization and yet require very few changes to the existing logic."
652858998,10822,rhauch,2021-06-16T16:34:22Z,"Let's avoid more than 1 adjacent blank line:
```suggestion

```"
652860987,10822,rhauch,2021-06-16T16:37:08Z,"I assume the choice of INFO here was intentional. If not, at what log level do we really want this log message? If so or if we do want this at INFO level, then maybe we should modify the `RestartRequest.toString()` method to be a little more user friendly (e.g., `restart request for connector %s (...)`).

And would it help to mention here that it's being enqueued? Maybe:
```suggestion
            log.info(""Received and enqueuing {}"", request);
```"
652872036,10822,rhauch,2021-06-16T16:52:13Z,"Nice! I didn't recall that `@DefaultValue` exists. But we should not have a space between that annotation and the open parenthesis:
```suggestion
                                 final @DefaultValue(""false"") @QueryParam(""includeTasks"") Boolean includeTasks,
                                 final @DefaultValue(""false"") @QueryParam(""onlyFailed"") Boolean onlyFailed,
```"
652875494,10822,rhauch,2021-06-16T16:55:46Z,"Won't `String.valueOf(boolean)` be used here, and thus we're unboxing the `Boolean` instance here? If there is a default value, then will `includeTasks` and `onlyFailed` both always be non-null, and if so then couldn't we just use `Boolean.toString()` here?"
652877090,10822,rhauch,2021-06-16T16:57:28Z,"Maybe add a comment here:
```suggestion
        // In all other cases, submit the async restart request and return connector state
        FutureCallback<ConnectorStateInfo> cb = new FutureCallback<>();
```"
652881239,10822,rhauch,2021-06-16T17:03:05Z,"Nit: I wonder if these log messages would be a bit easier to search and filter if they were reworded slightly:
```suggestion
                    log.debug(""Connector '{}' restart successful"", connectorName);
                } else {
                    log.debug(""Connector '{}' restart failed"", connectorName, error);
```
If you agree, then maybe also make similar changes to the corresponding log messages in the `DistributedHerder`."
652883217,10822,rhauch,2021-06-16T17:05:57Z,"Should we use a constant for this? I see that it's not really a pattern in the existing code, but maybe we should still do that here."
652884464,10822,rhauch,2021-06-16T17:07:48Z,"WDYT about being a bit more tolerant of missing keys? For example, we could use defaults here."
652885234,10822,rhauch,2021-06-16T17:08:52Z,"Let's avoid more than one adjacent blank lines.
```suggestion
```"
652886935,10822,rhauch,2021-06-16T17:10:25Z,"""both"" what?"
652887340,10822,rhauch,2021-06-16T17:10:45Z,"Again, ""both"" what?"
652888065,10822,rhauch,2021-06-16T17:11:27Z,Can we be more explicit in the test method names about what this test does?
652888765,10822,rhauch,2021-06-16T17:12:32Z,"What does this mean:
 > true is the default"
652890167,10822,rhauch,2021-06-16T17:14:33Z,"IIUC, we are not actually testing that the connector and task instances are distributed across different worker nodes, though it is likely to happen given the light connector load on the Connect cluster."
652891026,10822,rhauch,2021-06-16T17:15:44Z,"How long does it take for all of the tests in this IT to run, say locally? IIUC, we're setting up a new Kafka cluster and Connect cluster for every test. How difficult would it be to share the same Kafka and Connect cluster across most of the tests?"
652921473,10822,kpatelatwork,2021-06-16T17:55:54Z,"Could you guide me if there a predictable way to spread the tasks then I can change the test?

I found that when I gave numWorkers>1 most of the time they were distributed that's how I found an NPE bug and had to use @DefaultValue in the API.

I am using numWorkers=1 in other test to have a deterministic number of task Restarts, whenever I had numWorkers>1 the tasks would get restarted more than what I expect due to rebalance on worker nodes coming up."
652923058,10822,kpatelatwork,2021-06-16T17:57:09Z,"its on average 7-11sec per test (the ones with 35 sec are due to waiting in NOOP cases)
![image](https://user-images.githubusercontent.com/29556518/122268900-42763900-cea2-11eb-8647-e417265a234d.png)

Let me see if I can find a way to share and if it reduces the time."
653046083,10822,kpatelatwork,2021-06-16T20:53:16Z,"The name was bothering me also :), the method documentation says  ""Determine whether the connector with the given status is to be restarted."" WDYT about changing it to one of the below?

1. isConnectorRestartable
2. isTaskRestartable 

or 

1. shouldRestartConnector
2. shouldRestartTask
"
653053764,10822,kpatelatwork,2021-06-16T21:06:14Z,"Good point @rhauch, I missed this behavior when I moved from NavigableMap to a normal HashMap. Let me fix it."
653088343,10822,kpatelatwork,2021-06-16T22:07:20Z,Fixed
653089179,10822,kpatelatwork,2021-06-16T22:09:03Z,Fixed and changed all test names to be explicit
653089598,10822,kpatelatwork,2021-06-16T22:09:58Z,"Thanks for noticing this, its my bad the IDE method extract left the comment and I didn't notice."
653089924,10822,kpatelatwork,2021-06-16T22:10:42Z,fixed
653115877,10822,rhauch,2021-06-16T23:17:29Z,Either work for me.
653116810,10822,rhauch,2021-06-16T23:20:07Z,"Two things. First, these multi-worker ITs are not necessarily testing the requests getting forwarded to the leader, but that's really hard to do. Second, it might be good to try to send the request to a worker that is not running any instances to be restarted. One way to do that is to run 1 more worker than the total number of `Connector` and `Task` instances for the connector being restarted. Though I'm not sure this adds a lot of value given the others unit tests."
653117252,10822,rhauch,2021-06-16T23:21:18Z,"Yeah, that's already a good portion of the total time to run the Connect unit and integration tests. It'd be great if we could avoid increasing the build times by that much."
653117875,10822,kpatelatwork,2021-06-16T23:23:03Z,"Good idea, I fixed it"
653118611,10822,kpatelatwork,2021-06-16T23:25:12Z,"Good idea @rhauch, I fixed it. I added a default constant in KafkaConfigBackingStore class, the only thing I was unsure of if I should use the same default constant in ConnectorsResource class API or not. They seem to be 2 different world and I didn't wanted to introduce a dependency between the classes as it would expose unwanted details so right now default=false is in 2 places. Please Let me know if you have a better idea to solve this or if it's ok to have it copied the default value in 2 places."
653119839,10822,kpatelatwork,2021-06-16T23:28:46Z,"Thanks, I went with shouldXXX and it's committed, the code looks much better now."
653174069,10822,kpatelatwork,2021-06-17T02:10:54Z,"@rhauch  this was a good find, I just refactored the code, and this saved 2 min

![image](https://user-images.githubusercontent.com/29556518/122320107-52fcd280-cee7-11eb-9e14-3c6f9af3a917.png)
"
653681672,10822,rhauch,2021-06-17T15:32:38Z,"Nit: would a delimiter here help?
```suggestion
    private static final String CONNECTOR_NAME_PREFIX = ""simple-source-"";
```"
653687374,10822,rhauch,2021-06-17T15:39:33Z,"This method is where the worker restarts connector and task instances assigned to it. Lines 1153 and 1155, along with the `assignedIdsToRestart` are really the only evidence that this method does that. Can we modify the comments and log messages in this method to make that more evident, and maybe add JavaDoc?"
653698313,10822,rhauch,2021-06-17T15:52:11Z,"Can you confirm that the methods called in this block of logic handle failures? For example, the `worker.stopAndAwaitConnector(...)` method waits only up to 5 seconds, and we have no visibility into whether the connector was actually stopped in that time or we timed out. If we timed out, should we restart the connector, or should we abort this method and try again later? If we retry again later, it seems like it's okay to try stopping an already stopped connector, but it'd be good to double check that.

Same questions about stopping the tasks.

We may need to modify the `stopAndAwait...(...)` methods to add a callback that will allow us to better track what was actually stopped. And where this method is called we may need to handle the case where this restart was not completed and re-enqueue it for restart."
653701944,10822,rhauch,2021-06-17T15:56:20Z,"One thing about using a map is that we're always saving the last restart request, rather than the restart request that will (potentially) have the greatest impact. 

Consider two restart requests are submitted at about the same time:
1. RestartRequest 1 requests that everything is restarted (onlyFailed=false) for connector Foo
2. RestartRequest 2 requests that only failed instances are restarted (onlyFailed=true) for connector Foo

If this method saw RestartRequest 2 before RestartRequest 1, then everything would be restarted as expected. But if this method saw RestartRequest 1 first, then only the failed tasks would be restarted.

Seems like we should keep the most impactful one: RestartRequest 1 in this example. WDYT?"
653705258,10822,rhauch,2021-06-17T16:00:11Z,"Excellent! IIUC, the tests are only using 2 differently-sized clusters (1 node and 4 nodes), which is good because it minimizes the startup time. Can you confirm that statement is true? It seems like there are three total methods that take more than 30 seconds -- any reduction in duration on any of those three will have a very direct impact."
653745712,10822,kpatelatwork,2021-06-17T16:49:20Z,"@erdody and @rhauch  I changed the implementation to dequeue from map and perform the restart outside the synchronized block, could you please review to see if it looks ok now?"
653749526,10822,kpatelatwork,2021-06-17T16:52:53Z,"@rhauch this is an excellent suggestion, I just added this test based on the above discussion https://github.com/apache/kafka/pull/10822/commits/936e9da9f38ce6b87c52ff6ef95558c75bf56982 and now feel more confident that we are testing the distributed behavior of the feature.

Here is a sample response I copied from the above test output and we can see the call from the test was made on a worker not running the task or connector instance.

> [2021-06-17 11:41:22,306] INFO POST response for URL=http://localhost:49544/connectors/conn-testMultiWorkerRestartBothConnectorAndTasks/restart?onlyFailed=false&includeTasks=true is {""name"":""conn-testMultiWorkerRestartBothConnectorAndTasks"",""connector"":{""state"":""RESTARTING"",""worker_id"":""localhost:49424""},""tasks"":[{""id"":0,""state"":""RESTARTING"",""worker_id"":""localhost:49424""},{""id"":1,""state"":""RESTARTING"",""worker_id"":""localhost:49452""},{""id"":2,""state"":""RESTARTING"",""worker_id"":""localhost:49484""},{""id"":3,""state"":""RESTARTING"",""worker_id"":""localhost:49512""}],""type"":""source""} (org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster:735)
"
653753526,10822,kpatelatwork,2021-06-17T16:56:42Z,"I agree and I fixed it as part of recent tests changes,
`    private static final String CONNECTOR_NAME_PREFIX = ""conn-"";
`"
653815012,10822,kkonstantine,2021-06-17T18:02:00Z,"Seeing above, seems that this might as well be called `onRestart`. It's only naming but still would be nice to maintain consistency
"
653819933,10822,kpatelatwork,2021-06-17T18:09:43Z,"I went with a different sleep duration and that cut the times by 1minute 15 sec 
![image](https://user-images.githubusercontent.com/29556518/122449965-132bfe80-cf6c-11eb-8658-e2a389163455.png)

https://github.com/apache/kafka/pull/10822/commits/b60fb53a139dbd44c247ac1a4e2c927ef42ddc99.
I chose 5sec because locally the test that sets up the cluster takes around 5 sec to do it and  most test after they setup the cluster finishes in <300ms, request propagation and restart isn't taking long but thats my local machine and I chose to do 15 times the wait in case cloud or other developer machines are slow.  @rhauch Could you please review to see if you agree with the fix as another idea is to reduce it to 1 sec?"
653854373,10822,kpatelatwork,2021-06-17T18:57:12Z,"@rhauch  I agree and implemented it, Could you please review the below commit and see if it suffices? https://github.com/apache/kafka/pull/10822/commits/cb5afec3af37d60abae988c58e8180440ac97a58"
654043932,10822,kpatelatwork,2021-06-17T23:19:44Z,+1 I just fixed them in both classes
654060352,10822,kpatelatwork,2021-06-18T00:10:56Z,"@rhauch I did confirm that both stop and await for task and connector does connector.cancel() and task.cancel() if it timed out. I added try/catch in start connector/tasks https://github.com/apache/kafka/pull/10822/commits/5b5debe2d42a8d67e08cdb4998ef267fb87f9385

However, I am not sure about re-enqueue as it can lead to infinite tries, and adding retries may complicate things so need some guidance. Could you please review to see if it looks a bit better now?

"
654741212,10822,kpatelatwork,2021-06-19T03:30:22Z,"The new times after checking that API response didn't return any RESTARTING state is 13 sec and 10 sec out of it is in bringing up kafka cluster.

![image](https://user-images.githubusercontent.com/29556518/122629722-ac4e3880-d084-11eb-8742-8cdaf703ba99.png)
"
654741381,10822,kpatelatwork,2021-06-19T03:32:23Z,added javadoc and cleaned some logs and there are logs in the called methods so didn't added new logs
655408049,10822,kpatelatwork,2021-06-21T14:06:30Z,"@rhauch we can add a retryCount in the RestartRequest and start with a default of 5 and decrement it every time we re-enqueue. If the tick() happens every second then we can exhaust the retries pretty fast so we may need to add backoff with a delay before processing the request. Adding retry logic to this PR can make the PR complex, I would vote for adding it in a separate PR."
655658286,10822,rhauch,2021-06-21T19:45:48Z,"> Adding retry logic to this PR can make the PR complex, I would vote for adding it in a separate PR.

I would agree. We're not sure whether we need this behavior yet, so I'm +1 for keeping it simple in this PR and adding it in the future only if we discover a need for it."
655660680,10822,rhauch,2021-06-21T19:49:52Z,Let's mention the natural sort order.
655678526,10822,rhauch,2021-06-21T20:21:36Z,"The `startConnector(...)` method _should_ handle most of the errors by calling the callback, but there are still a few potential errors that could happen before the worker actually tries starting the connector.

Do you think this try-catch is needed, though, since the try-catch where this `doRestartConnectorAndTasks(...)` is called will catch and log any unexpected change? I guess keeping this will ensure we proceed with starting the tasks, rather than failing quickly."
655683692,10822,rhauch,2021-06-21T20:30:35Z,"If we were to change `pendingRestartRequests` to a `ConcurrentMap`, then we could use the `compute(...)` functionality that actually would work even if we removed the synchronization. I wonder if this is a bit more readable (even though strictly speaking we don't need concurrent access). WDYT?
```
                pendingRestartRequests.compute(connectorName, (k, existing) -> {
                    if (existing == null || request.compareTo(existing) > 0) {
                        log.debug(""Overwriting existing {} and enqueuing the higher impact {}"", existing, request);
                        return request;
                    }
                    log.debug(""Preserving existing higher impact {} and ignoring incoming {}"", existing, request);
                    return existing;
                });
```"
655688374,10822,rhauch,2021-06-21T20:38:42Z,What do you think about returning `Optional<String>` here (if every worker has at least one instance for the given connector) rather than throwing an exception? 
655749684,10822,kpatelatwork,2021-06-21T22:40:23Z,"@rhauch  I kinda agree with you but I had seen this code in below two links and it had try/catch around both startTask and startConnector .  I added try/catch around startTask so if one tasks fails we can see if others atleasst succeed. The try/catch around startConnector was because I saw the pattern.  But I am torn on this, if you think we should remove the try/catch on connector, I am open to removing the try/catch given its already caught at the higher layer, but IMHO we can keep the startTask try/catch. Wdyt?

https://github.com/apache/kafka/blob/1bd99c809b6546cd6711cc4f2d40785393ca584a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1437
https://github.com/apache/kafka/blob/1bd99c809b6546cd6711cc4f2d40785393ca584a/connect/runtime/src/main/java/org/apache/kafka/connect/runtime/distributed/DistributedHerder.java#L1490"
655755572,10822,kpatelatwork,2021-06-21T22:55:44Z,turns out a normal hashmap also has compute so I just used it.
655755798,10822,kpatelatwork,2021-06-21T22:56:17Z,"@rhauch I added some Javadoc, could you please review and see if it looks good?"
655757673,10822,kpatelatwork,2021-06-21T23:01:30Z,@rhauch  I think it's a good idea but I saw that org.apache.kafka.connect.util.clusters.EmbeddedConnectCluster#endpointForResource method is using the same pattern and if I change it then I will have to change a lot of classes not related to this PR and ultimately in the test I will have to check for Optional.present and do an assert.  The situation for not finding a worker is rare and doing empty check and assert in all places will be a lot of copy/paste whereas throwing an exception here is centralizing this rare situation. Do you think it's ok to leave it as is?
655776106,10822,rhauch,2021-06-21T23:42:01Z,"Yeah, I think it's actually fine the way it is: with the try-catrch around `startConnector(...)`, because that helps us catch any problem _at that point_ and allows us to continue starting the tasks.

If we were to remove this try-catch around `startConnector(...)`, then any problem would cause us to return immediately from the method."
655778416,10822,rhauch,2021-06-21T23:48:33Z,"Well, the `EmbeddedConnectCluster` is technically not an official public API for Connect, but lots of connector projects use it (as intended), meaning we have to be very clear about backward compatibility. It's probably fine to keep the same pattern. "
655781412,10822,rhauch,2021-06-21T23:57:06Z,"It might be good to clean up the method name a bit. Since it's similar to the existing `endpointForResource(...)`, maybe just add a suffix ... maybe something like `endpointForResourceNotRunningConnector()`?"
655781662,10822,rhauch,2021-06-21T23:57:46Z,Please add descriptions of the other parameters and the return value.
655885518,10822,kkonstantine,2021-06-22T05:25:50Z,"I think we can avoid this alignment style. It leaves us with significantly less space to write lambdas, etc. (another indicator is that this style is not applied elsewhere in the file). Two tab stops in the line below should be fine, even if the declaration above is where it is now. "
655887809,10822,kkonstantine,2021-06-22T05:31:37Z,"I don't think we have examples in Connect where we refer to an argument in the name of a method. 
Maybe we don't want to change this just yet with the opportunity of the changes introduced by this feature. 

Another observation is that we don't use `get`, `set` and possibly `build`. But since it wouldn't be obvious if it's an action or an object maybe leaving `buildRestartPlan` might be fine here. (`restartPlan` would be the alternative)"
655891458,10822,kkonstantine,2021-06-22T05:41:30Z,maybe a good idea to say that this is a plan per connector (and not a global plan). I know the javadoc of the constructor says it already but good to be at the top level too. 
655892160,10822,kkonstantine,2021-06-22T05:43:08Z,"nit: in the constructor we refer to member fields with `this.` during initialization (even if they are not shadowed)
```suggestion
        this.idsToRestart = Collections.unmodifiableList(
```"
655893949,10822,kkonstantine,2021-06-22T05:47:38Z,`shouldRestartTasks` or `shouldRestartAnyTasks` (same recommendation as above)
655893977,10822,kkonstantine,2021-06-22T05:47:42Z,since this is not an action but a recommendation I think it'd be better to call `shouldRestartConnector` 
655894996,10822,kkonstantine,2021-06-22T05:50:00Z,Not sure we should say `restart` in the method name. The object is already a `RestartPlan` type. So that's a bit redundant I think. `connectorStateInfo` ?
655896479,10822,kkonstantine,2021-06-22T05:53:29Z,below we have a method name called `restartConnectorAndTasks` implying _any_ tasks. So maybe we can skip `Any` in the name. 
655898225,10822,kkonstantine,2021-06-22T05:57:38Z,"```suggestion
     * Builds and executes a restart plan for the connector and its tasks from <code>request</code>.
```"
655900253,10822,kkonstantine,2021-06-22T06:02:13Z,use of `final` doesn't seem to be consistent in this method. I'd suggest skipping its addition to local variables unless we need it in lambdas. 
655902961,10822,kkonstantine,2021-06-22T06:07:49Z,"any reason not to use primitive types?
```suggestion
    public static final boolean ONLY_FAILED_DEFAULT = false;
    public static final boolean INCLUDE_TASKS_DEFAULT = false;
```"
655904820,10822,kkonstantine,2021-06-22T06:11:50Z,"Do we have to call it snapshot?
Is `StartsAndStops` enough?
```suggestion
public class StartAndStopCounterSnapshot {
```"
655905352,10822,kkonstantine,2021-06-22T06:13:02Z,I found the name a bit overloaded and added a suggestion below. You think we could make it a bit simpler?
656348082,10822,kpatelatwork,2021-06-22T15:41:56Z,"The defaults are used in code like below and using primitive would lead to an extra boxing

>         Object failed = valueAsMap.get(ONLY_FAILED_FIELD_NAME);
>         if (failed == null) {
>             log.warn(""Invalid data for restart request '{}' field was missing, defaulting to {}"", ONLY_FAILED_FIELD_NAME, ONLY_FAILED_DEFAULT);
>             failed = ONLY_FAILED_DEFAULT;
>         }
>         if (!(failed instanceof Boolean)) {
>             log.warn(""Invalid data for restart request '{}' field should be a Boolean but is {}, defaulting to {}"", ONLY_FAILED_FIELD_NAME, failed.getClass(), ONLY_FAILED_DEFAULT);
>             failed = ONLY_FAILED_DEFAULT;
>         }
>         boolean onlyFailed = (Boolean) failed;"
656356700,10822,kpatelatwork,2021-06-22T15:50:11Z,"@kkonstantine  excellent suggestion, naming is indeed hard problem and I had the same feeling this is overloaded.  I applied your suggestion."
656356999,10822,kpatelatwork,2021-06-22T15:50:29Z,fixed
656357145,10822,kpatelatwork,2021-06-22T15:50:38Z,Thanks for catching this
656357635,10822,kpatelatwork,2021-06-22T15:51:09Z,Thanks for catching this.
656358121,10822,kpatelatwork,2021-06-22T15:51:45Z,renamed to buildRestartPlan as per your suggestion. 
656358424,10822,kpatelatwork,2021-06-22T15:52:04Z,"Good observation, I fixed it."
656633158,10822,kpatelatwork,2021-06-22T22:49:51Z,Fixed documentation and renamed the method.
656633775,10822,kpatelatwork,2021-06-22T22:51:29Z,"Good idea, I renamed the method. "
656652231,10822,kpatelatwork,2021-06-22T23:39:56Z,Fixed
656653569,10822,kpatelatwork,2021-06-22T23:42:54Z,"I think we should keep it because in the caller code plan.connectorStateInfo() conveys its the current state but plan.restartConnectorInfo() conveys the intent that its the stateInfo with restart state.  I know its not the best name :( 

CC: @rhauch if he has any better suggestions for the name."
656654293,10822,kpatelatwork,2021-06-22T23:44:45Z,"@kkonstantine  and @rhauch  I renamed the methods for shouldXXX suggestion.

For removing Any in the name, I think we should keep it in the name because plan.shouldRestartTasks() in caller code conveys the intent it would restart all tasks but plan.shouldRestartAnyTask conveys it would restart at least one task. I know its again not the best name.

wdyt if we rename it to shouldRestartAtleastOneTask, it was too verbose that's why we had picked Any?   "
657118416,10822,rhauch,2021-06-23T13:46:50Z,"This is subtly different than a normal `ConnectorStateInfo`, so keeping the current name may be a bit more clear in the context where this method is used, even if it's less conventional."
657148983,10822,kpatelatwork,2021-06-23T14:18:16Z,Renamed to shouldRestartTasks
660274546,10822,kkonstantine,2021-06-29T04:38:50Z,"Is there any reason we are not adding these new methods in the `TaskStatus.Listener` interface?
In any case we should add javadoc to the base declaration (interface or class methods). "
660275673,10822,kkonstantine,2021-06-29T04:42:29Z,"nit: the ternary operator can be used (`?:`) as below, unless you're not a fan. 
```suggestion
        AbstractStatus.State connectorState = request.shouldRestartConnector(connectorStatus)
                ? AbstractStatus.State.RESTARTING 
                : connectorStatus.state();
```"
660276986,10822,kkonstantine,2021-06-29T04:46:34Z,"A bit confusing that a second assignment follows if the `if` statement is true. 
I'd also call the variable `taskState` (as opposed to `connectorState` above)

Ternary can be used here as well: 
```suggestion
                    AbstractStatus.State state = request.shouldRestartTask(taskStatus)
                            ? AbstractStatus.State.RESTARTING
                            : taskStatus.state();
```
(as with any suggestion from github, please check it compiles and conforms to the style)"
660280566,10822,kkonstantine,2021-06-29T04:57:05Z,No problem.
660281788,10822,kkonstantine,2021-06-29T05:00:50Z,"something like? 
```suggestion
return shouldRestartConnector()
               ? String.format(""plan to restart connector and %d of %d tasks for %s"", restartTaskCount(), totalTaskCount(), request)
               : String.format(""plan to restart %d of %d tasks for %s"", restartTaskCount(), totalTaskCount(), request);
```
breaking this long statement doesn't make it much less long I guess. "
660283562,10822,kkonstantine,2021-06-29T05:06:17Z,"```suggestion
    public boolean forceRestartConnectorOnly() {
```
? (seems a bit more common way to say it, but I don't have a strong opinion)"
660284464,10822,kkonstantine,2021-06-29T05:08:43Z,"```suggestion
        return result == 0 ? impactRank() - o.impactRank() : result;
```"
660290194,10822,kkonstantine,2021-06-29T05:25:47Z,"I see we've been verbose in similar logic above before, but maybe we can improve on that a bit, at least in new code. Here's another suggestion: 
```suggestion
        if (!(value.value() instanceof Map)) {
            log.error(""Ignoring restart request because the value is not a Map but is {}"", value.value() == null ? ""null"" : value.value().getClass());
            return null;
        }
```"
660291164,10822,kkonstantine,2021-06-29T05:28:23Z,"Following up on the comment above, here's how we could write this and avoid autoboxing/unboxing while using the more readable primitives. 
```suggestion
        if (!(failed instanceof Boolean)) {
            log.warn(""Invalid data for restart request '{}' field should be a Boolean but is {}, defaulting to {}"",
                    ONLY_FAILED_FIELD_NAME, failed == null ? ""null"" : failed.getClass(), ONLY_FAILED_DEFAULT);
            onlyFailed = ONLY_FAILED_DEFAULT;
        } else {
            onlyFailed = (Boolean) failed;
        }
```"
660291287,10822,kkonstantine,2021-06-29T05:28:45Z,Similar suggestion as above
660291760,10822,kkonstantine,2021-06-29T05:29:56Z,I agree the conversion is on a frequently used path. But maybe it's the code below that can be re-written to avoid both autoboxing and unboxing (when it's not required)
660293584,10822,kkonstantine,2021-06-29T05:34:43Z,"This misuse might have originated because above, specifically for tasks, `null` is actually a valid value (tombstone/delete), which is not the case here. "
660295568,10822,kkonstantine,2021-06-29T05:40:09Z,"Since we print the warning inside `recordToRestartRequest` we can probably avoid the early return with:
```suggestion
                // Only notify the listener if this backing store is already successfully started (having caught up the first time)
                if (request != null && started) {
```"
660296712,10822,kkonstantine,2021-06-29T05:43:05Z,"btw, all other log statements are `log.error` elsewhere. Should we remain consistent with that, instead of using `log.warn` just here? The issues seem similar above. "
660301592,10822,kkonstantine,2021-06-29T05:55:31Z,"dependents in the implementation of `StartAndStopLatch` seems to overload the meaning of `null` with a check. But it actually doesn't seem to matter. The call is equivalent to passing an empty list. 

Should we simplify that with something like: 
```
List<StartAndStopLatch> taskLatches = includeTasks
                ? taskHandles.values().stream()
                        .map(task -> task.expectedStarts(expectedStarts))
                        .collect(Collectors.toList())
                : Collections.emptyList();
                ```
?
"
660301819,10822,kkonstantine,2021-06-29T05:56:03Z,see comment above on whether `null` matters. 
660303274,10822,kkonstantine,2021-06-29T05:59:36Z,"does it make sense to skip, given that it's called below? (I hope I'm not missing something)"
660309326,10822,kkonstantine,2021-06-29T06:13:53Z,"nit: similar style as elsewhere in this file
```suggestion
    public void restartConnectorAndTasks(RestartRequest request, Callback<ConnectorStateInfo> callback) {
```"
660311654,10822,kkonstantine,2021-06-29T06:18:53Z,"nit: type inference is nice ... 
```suggestion
restartRequests.forEach(restartRequest -> {
```"
660315005,10822,kkonstantine,2021-06-29T06:25:58Z,I'd call it `plan` here. 
660315335,10822,kkonstantine,2021-06-29T06:26:38Z,"safe to skip the declaration here. 
```suggestion
                            callback.onCompletion(null, plan.get().restartConnectorStateInfo());
```"
660315588,10822,kkonstantine,2021-06-29T06:26:59Z,"nit: extra
```suggestion
```"
660320316,10822,kkonstantine,2021-06-29T06:36:25Z,"same as the connector call above
```suggestion
                    if (startTask(taskId)) {
```"
660781818,10822,kpatelatwork,2021-06-29T16:29:13Z,fixed
660782579,10822,kpatelatwork,2021-06-29T16:30:10Z,I Agree and I fixed it like you suggested.
660834568,10822,kpatelatwork,2021-06-29T17:42:17Z,"The plan may or may not be present, that's why I was calling it maybePlan"
660836387,10822,kpatelatwork,2021-06-29T17:45:00Z,"we are logging an error if we are rejecting the request due to an invalid type, but in this particular case as we are defaulting the missing fields, that's why the warn."
660864346,10822,kpatelatwork,2021-06-29T18:26:17Z,"I added the missing documentation.

I didn't add the onRestart method to  Listener because it didn't felt it should be a part of the task lifecycle controlled by the worker. Earlier the method was called recordRestart but based on a review comment, it was renamed it to onRestart, Do you think we should rename it back to recordRestart. The original intent of this method was to just record the state change."
660866556,10822,kpatelatwork,2021-06-29T18:29:36Z,fixed as per your suggestions.
660867874,10822,kkonstantine,2021-06-29T18:31:31Z,Did you compare it with other methods? My understanding is that `onDelete` is the same in that respect and exists on the `Listener` as well. But I might have skimmed too quickly through the code
660868474,10822,kkonstantine,2021-06-29T18:32:28Z,Didn't notice. Thanks. Makes sense then
660870565,10822,kkonstantine,2021-06-29T18:35:40Z,"We might be too precise here. As we discussed elsewhere, baking the meaning of the type in every variable name might be too verbose. 
It's a plan, and the fact that is optional means that there might be one or it might not. That's how I'd read it. 
The other use of `maybePlan` below is harder to avoid. Feel free to keep it consistent here with what you have below. 
Again, my point is not to bake type meaning in the variable names. I feel this keeps things simpler. "
660925249,10822,kpatelatwork,2021-06-29T20:02:52Z,@kkonstantine after your latest explanation it makes sense to rename it to keep it simple. Thanks for guiding me. I have renamed it. 
660953271,10822,kpatelatwork,2021-06-29T20:48:53Z,@kkonstantine you are right. Do you mind if we do this work in a follow-up PR?  The reason I ask is because if we add it to the listener then it becomes part of the interface and this would require me to retrofit the listener event into the old restartTask and restartConnector API for backward compatibility reasons and it can be big change to this already big PR.
661028655,10822,kpatelatwork,2021-06-29T23:28:38Z,@kkonstantine I moved it to the Lisetner interface in https://github.com/apache/kafka/pull/10822/commits/7f8a588aaba9f1f80889e0429b2b9f495b1c666b.  Could you please check to see if it looks good.
1579221285,15640,cadonna,2024-04-25T10:08:01Z,"Why do you use a timer here? The `asyncCommit()` does not throw any timeout exception, does it? If you need to pass the timer to the `CommitEvent` or further up the class hierarchy then you can create the timer in the constructor of `AsyncCommitEvent` or even further up the class hierarchy. "
1579239807,15640,cadonna,2024-04-25T10:24:10Z,"Why should those two fields ever be `null`?
They seem necessary for the consumer to function correctly. If my statement is correct, the constructors should ensure that those fields are never `null`."
1579264527,15640,cadonna,2024-04-25T10:45:03Z,"I have two questions here:

1. Why is this code not in `CompletableEvent`?
2. Why do you not keep the timer in a object field? 

Regarding 2, I have the feeling this PR contains code that already exist in the timer. You could have a method on `CompletableEvent` that checks for the expiration without exposing the timer. Something like `isExpired()` or `isTimedOut`."
1579269984,15640,cadonna,2024-04-25T10:49:30Z,Could you export this lines into a method `processApplicationEvents()` or similar. I think it makes the code more readable.
1579275627,15640,cadonna,2024-04-25T10:54:36Z,Do we need this comment? I am not a big fan of inline comments in general. All the information about the behavior in the comment should be clear from the corresponding unit tests. I do not think we need the references to the legacy consumer. Once the legacy consumer is gone we need to remove these references which is work that we can avoid by just not writing those comments. 
1579283124,15640,cadonna,2024-04-25T11:00:50Z,I think that is clear from the code. We do not need the comment.
1579292380,15640,cadonna,2024-04-25T11:09:42Z,"Here I have a similar comment as with `AsyncCommitEvent`, I would move the creation of the timer into the constructor of `ConsumerRebalanceListenerCallbackNeededEvent`."
1579314894,15640,cadonna,2024-04-25T11:30:24Z,Same questions as above.
1579322300,15640,cadonna,2024-04-25T11:36:39Z,"nit:
IMO, this is more readable, but feel free to leave it if you do not share my taste.
```suggestion
        tracked.stream()
            .filter(e -> !e.future().isDone())
            .filter(e -> currentTimeMs > e.deadlineMs())
            .forEach(timeoutEvent);
```"
1579324613,15640,cadonna,2024-04-25T11:38:47Z,Wouldn't it be safer to not have a default here as a reminder for closing.
1579733831,15640,kirktrue,2024-04-25T15:45:14Z,"Yeah, I went back and forth on this a few times 

Ultimately I wanted to force the caller to be explicit about its timeout intention, vs. having it implicitly ""hidden"" away in the event hierarchy.

Also, to create a `Timer` in the event constructor, we'd have to pass in a `Time` object (`time.timer(Long.MAX_VALUE)`), which seemed a bit obtuse, so  "
1579735209,15640,kirktrue,2024-04-25T15:46:22Z,"They're only `null` if there was an error in the constructor. The constructor's `finally` block calls `close()`, so we need to handle the case where the consumer wasn't fully constructed before it's closed."
1579756071,15640,kirktrue,2024-04-25T16:01:51Z,"I'm happy to reword the comment and clean it up, but the lines that follow that comment are the raison d'tre of this change. It's very subtle and easy to miss, hence the call-out."
1579760563,15640,kirktrue,2024-04-25T16:05:28Z,I'll look into how to do this in a way that I don't find too ugly  
1579864626,15640,kirktrue,2024-04-25T17:22:11Z,"`CompletableEvent` is an interface, but I could see if I can put a static method in there to keep the logic in one place."
1579865601,15640,kirktrue,2024-04-25T17:23:08Z,"Yeah, I can see your point. TBH, I'm not sure if that's even needed still "
1579884608,15640,kirktrue,2024-04-25T17:40:17Z,Added a brief comment. PTAL.
1579884758,15640,kirktrue,2024-04-25T17:40:25Z,Done.
1579885684,15640,kirktrue,2024-04-25T17:41:16Z,Split into a separate method to accommodate the reworded comment.
1579885847,15640,kirktrue,2024-04-25T17:41:24Z,Removed.
1579897029,15640,kirktrue,2024-04-25T17:51:50Z,"For point 1, I created a new method named `calculateDeadlineMs` that moves this code into one place.

For point 2, there is no `Timer` in the event because `Timer` is not thread safe, and events cross the application/background thread boundary. I did not want to expose the `Timer` in the event to avoid its possible usage from the background thread."
1579899408,15640,kirktrue,2024-04-25T17:54:02Z,"It turns out `close()` wasn't needed any more, so I just removed it "
1579914824,15640,kirktrue,2024-04-25T18:04:46Z,I was able to refactor the code to eliminate the need for passing in a `Timer`.
1579915561,15640,kirktrue,2024-04-25T18:05:12Z,I was able to refactor the code and this is now sans `Timer` again.
1580785461,15640,cadonna,2024-04-26T09:53:28Z,"nit: 
Could you please move this parameter to the previous line? "
1580790248,15640,cadonna,2024-04-26T09:57:24Z,I think you do actually not need the timer in this method at all. You could pass a deadline to the event.
1580879040,15640,cadonna,2024-04-26T11:13:20Z,The timer is not used anywhere else. Maybe a deadline for this event would be simpler.
1580883395,15640,cadonna,2024-04-26T11:18:18Z,Also here the timer is only used in the event. Using a deadline would be simpler.
1580884538,15640,cadonna,2024-04-26T11:19:34Z,The timer is only used by the event. Maybe a deadline is simpler.
1580885372,15640,cadonna,2024-04-26T11:20:30Z,The timer is only used in the event.
1583451774,15640,kirktrue,2024-04-29T17:28:47Z,"Per the above, I added `CompletableEvent.calculateDeadlineMs()` to keep the code in a shared location."
1583452334,15640,kirktrue,2024-04-29T17:29:14Z,"Yep, I made the change to use the deadline directly (vs. a `Timer`)."
1583452722,15640,kirktrue,2024-04-29T17:29:36Z,Agreed. Using the deadline directly instead of a `Timer`.
1583452937,15640,kirktrue,2024-04-29T17:29:46Z,Changed.
1583520696,15640,kirktrue,2024-04-29T18:20:21Z,Removed use of `Timer` in favor of calculating the deadline from the `time` and `timeout` directly.
1583520759,15640,kirktrue,2024-04-29T18:20:26Z,Removed use of `Timer` in favor of calculating the deadline from the `time` and `timeout` directly.
1585003046,15640,lianetm,2024-04-30T15:10:56Z,"Couldn't we make that the same add operation removes the event whenComplete? Seems tighter that the same operation that adds the event ensures that it's removed (if completes), and then it's simpler here, when we only need to care about maintaining the uncompleted (which seems like the core responsibility of the reaper). Also that would mean that we don't rely on calls to reap to remove events that complete in time."
1585004533,15640,lianetm,2024-04-30T15:11:39Z,"responsible for events that ""are being processed"" right? "
1585005659,15640,lianetm,2024-04-30T15:12:14Z,"typo ""we are"""
1585035057,15640,lianetm,2024-04-30T15:28:16Z,"what about renaming this to be explicit about what we process here? It gets confusing given that at this consumer level we're dealing with app events and background events. 

`processBackgroundEvents` feels pretty clear, and I know there is already another one called liked that, but the other one is more about `awaitFutureProcessingBackgroundEvents` , because it actually blocks for a time, only used from the unsubscribe, so maybe rename here and there?"
1585083395,15640,lianetm,2024-04-30T15:44:39Z,"Regarding the func doc, typo and clarification:

> When the application thread sees ..., it is processed, and **then a ...is then** enqueued by the application thread **on the background event queue**

The app thread enqueues the event in an **application event queue** (that the background thread consumes), right? In the doc we ended up mentioning the background and app thread both adding to the background event queue. "
1585091606,15640,lianetm,2024-04-30T15:49:20Z,"Also, regarding:

> as part of this rebalancing work, the {@link ConsumerRebalanceListener#onPartitionsRevoked(Collection)} callback needs to be invoked

it does not need to if the consumer unsubscribing does not own any partition, so just for accuracy in the example I would suggest to extend it with ""...needs to be invoked for the partitions the consumer owns"""
1585152310,15640,lianetm,2024-04-30T16:17:07Z,"Not introduced by this PR, but reviewing this processing I don't quite see the value in all [these lines](https://github.com/apache/kafka/blob/097522abd6b51bca2407ea0de7009ed6a2d970b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1910-L1915) , that are even repeated further down, just for a log, when in practice this are both the happy path that will have [this](https://github.com/apache/kafka/blob/097522abd6b51bca2407ea0de7009ed6a2d970b4/clients/src/main/java/org/apache/kafka/clients/consumer/internals/AsyncKafkaConsumer.java#L1506) log from the unsubscribe. A one liner with `return ConsumerUtils.getResult(future);` would achieve the same and make the func much simpler. 
(even if we end up using this from a func other than the unsubscribe, seems an overkill to have all this code for something we don't need now, or know if we we'll need some day)"
1585179214,15640,philipnee,2024-04-30T16:35:22Z,Any specific reason for using linkedlist implementation? 
1585390985,15640,lianetm,2024-04-30T19:17:17Z,"This logic is always needed whenever we `reapIncomplete`, and is currently repeated when we call it from the AsyncConsumer or here, so what about we move it into the `reapIncomplete`, make it receive a list of all events and internally filter the ones that are `CompletableEvent`?"
1585435927,15640,lianetm,2024-04-30T19:34:00Z,Is there a reason for loosing the final on the offsets map? 
1585466853,15640,lianetm,2024-04-30T20:05:10Z,"The reaper actually calls `completeExceptionally` with a `CancellationException` instead of calling `CompletableFuture#cancel(boolean)`. Unless I'm missing a subtle semantic diff they should achieve the same, but still, adding a link to `cancel` here would not be accurate I would say."
1585531596,15640,philipnee,2024-04-30T21:13:12Z,can we remove this? I think the test works without it.
1585537917,15640,philipnee,2024-04-30T21:20:55Z,the test should work without setting the current time to 0.  so I think new MockTime(0) should be fine.
1585558712,15640,philipnee,2024-04-30T21:46:34Z,ditto: we probably don't need the final but it would be good to be consistent.
1586499310,15640,kirktrue,2024-05-01T16:46:20Z,"My first attempt at this resulted in a `ConcurrentModificationException`, since we're removing each entry from the very same list we're iterating over  "
1586524076,15640,kirktrue,2024-05-01T17:11:43Z,"Unfortunately, the term ""processed"" is sufficiently ambiguous  So we're _both_ right 

Here, I'm referring to events that had been passed to the `EventProcessor`'s [process()](https://github.com/apache/kafka/blob/74a7ed78cc69f0d28bd18139b90c28468058e111/clients/src/main/java/org/apache/kafka/clients/consumer/internals/events/EventProcessor.java#L40) method.

Which, sadly, isn't even correct, because they're being `add()`-ed to the reaper _before_ they're passed to `EventProcessor.process()` "
1586537358,15640,kirktrue,2024-05-01T17:24:31Z,I reworked the comments/documentation to avoid that altogether. PTAL. Thanks.
1586539952,15640,kirktrue,2024-05-01T17:27:19Z,Fixed.
1586542049,15640,kirktrue,2024-05-01T17:29:36Z,I renamed `process()` as `processBackgroundEvents()`. Is that OK?
1586557142,15640,kirktrue,2024-05-01T17:43:16Z,"I made the documentation changes you requested and removed the logging to make the logic simpler.

When you state that it seems like ""overkill to have all this code for something we don't need now, or know if we we'll need some day,"" I'm a bit confused  Because unsubscribing may require invoking `ConsumerRebalanceListener` callbacks, we need a way to check and run those events that are coming from the background thread, right?

I do agree that it's overkill to have this broken out as a separate method since it's only used for the `unsubscribe()` case. IIRC, there was some talk of another use case for this, and it does make unit testing it easier."
1586559943,15640,kirktrue,2024-05-01T17:46:08Z," Yes, I went back and forth on this at least three times during development. I'll look at switching back to the approach you suggest."
1586603709,15640,kirktrue,2024-05-01T18:20:32Z,Pulled the logic to `reapIncomplete()` as suggested.
1586607598,15640,kirktrue,2024-05-01T18:23:21Z,Added back `final` and changed back to `protected`. Not sure how/why I changed those  
1586620323,15640,kirktrue,2024-05-01T18:32:47Z,"Good catch!

I had been using `cancel()`, but noticed that the message in the exception the caller of `Future.get()` later received was unhelpful. Yes, `cancel()` calls `completeExceptionally(new CancellationException())`, but I wanted the exception to include a (hopefully) meaningful message.

Anyhoo... I've updated the documentation to reflect that change."
1586622390,15640,kirktrue,2024-05-01T18:34:25Z,"Yes, because it has the `drainTo()` method. However, this code is now gone, so it's moot "
1586626516,15640,kirktrue,2024-05-01T18:37:25Z,Done.
1586626597,15640,kirktrue,2024-05-01T18:37:29Z,Done.
1586632167,15640,kirktrue,2024-05-01T18:40:25Z,"Moved the `listOffsetsEvent` up to the previous line. Missed it on first read, sorry "
1587519244,15640,cadonna,2024-05-02T11:59:47Z,"@lianetm Do you propose to remove completed events in `CompletableEventReaper#add`?
If yes, to avoid the `ConcurrentModifictionException` @kirktrue, you could iterate over `tracked` with an iterator and remove completed events through the iterator:
```java
final Iterator<T> iterator = tracked.iterator();
while (iterator.hasNext()) {
    final T event = iterator.next();
    if (event.future().isDone()) {
        iterator.remove();
    }
}
``` 
"
1587734052,15640,cadonna,2024-05-02T14:26:17Z,Is this enough error handling?
1587741997,15640,cadonna,2024-05-02T14:31:18Z,Is this really unclear from the name `deadlineMs`?
1587841410,15640,cadonna,2024-05-02T15:30:00Z,"On a second thought, what is the issue with moving 
```java
tracked.removeIf(e -> e.future().isDone());
```
to `add()`?

I guess, I am misunderstanding your comments."
1588986665,15640,cadonna,2024-05-03T09:49:48Z,You actually do not need a timer here. The event takes the deadline and also the reaper does not use the timer.
1589007012,15640,cadonna,2024-05-03T10:07:31Z,"Also here, I do not think you need this timer."
1589007924,15640,cadonna,2024-05-03T10:08:31Z,Same here.
1589011280,15640,cadonna,2024-05-03T10:12:05Z,Not needed.
1589011422,15640,cadonna,2024-05-03T10:12:15Z,Not needed.
1589016749,15640,cadonna,2024-05-03T10:17:44Z,Couldn't you use a simple `Collection` with a simple `ArrayList` here?
1589016846,15640,cadonna,2024-05-03T10:17:50Z,Couldn't you use a simple `Collection` with a simple `ArrayList` here?
1589061761,15640,cadonna,2024-05-03T11:05:52Z,Tests for the new behavior added to `ConsumerNetworkThread` are missing.
1589073577,15640,cadonna,2024-05-03T11:19:32Z,Do you not need to add some tests to verify the newly added reaper?
1599232056,15640,kirktrue,2024-05-14T00:16:37Z,"There's some debate over that, for sure. However, this is the same error processing we've used before this change, so hopefully we can continue that debate separately? "
1599232355,15640,kirktrue,2024-05-14T00:17:17Z,"We've had some confusion in the past between timeouts and expiration, but I can remove it if you firmly want it removed."
1599233659,15640,kirktrue,2024-05-14T00:19:47Z,The use of the `Timer` is only for the test method itself. I'd argue the use of the `Timer` makes the code a little more obvious than a plain `long` variable as the code switches between expiration timestamps and timeouts.
1599235694,15640,kirktrue,2024-05-14T00:24:21Z,"Yes.

Previous versions of `reapExpiredAndCompleted()` required a `BlockingQueue`. When the logic was changed to use a more general `Collection`, I failed to update the test code.

I will make the change as suggested."
1599236609,15640,kirktrue,2024-05-14T00:25:57Z,Done.
1599236661,15640,kirktrue,2024-05-14T00:26:04Z,Done.
1599243250,15640,kirktrue,2024-05-14T00:40:27Z,I went ahead and made the change you suggested 
1599243369,15640,kirktrue,2024-05-14T00:40:36Z,Removed.
1599243426,15640,kirktrue,2024-05-14T00:40:42Z,Also removed.
1599243498,15640,kirktrue,2024-05-14T00:40:50Z,"Removed here, too."
1599243592,15640,kirktrue,2024-05-14T00:41:04Z,And from here as well 
1599244057,15640,kirktrue,2024-05-14T00:42:02Z,"I will add tests for the `ConsumerNetworkThread`, but it will probably take some contortion "
1599244816,15640,kirktrue,2024-05-14T00:43:28Z,I will try my best to add tests to `AsyncKafkaConsumerTest`. I'm hoping it's not quite as bad as I'm imagining it will be 
1599266953,15640,kirktrue,2024-05-14T01:27:07Z,"Keep in mind that `add()` is called _for each event_ in the queue. If we remove completed events in `add()`, we're updating `tracked` each time, too.

In the current approach, `tracked` is only updated once inside `reapExpiredAndCompleted()`.

I think I'm missing the benefit of moving it to `add()` "
1599698758,15640,cadonna,2024-05-14T09:31:32Z,"Usually if it takes some contortion to write unit tests, then there is usually a code smell somewhere.  Unit tests do not only test code in execution, but often also help to design components that have loose coupling and high cohesion.  "
1599699017,15640,cadonna,2024-05-14T09:31:43Z,See my comment above. I am sure you will find a good solution!
1600781636,15640,kirktrue,2024-05-14T23:38:33Z,"Oh, the code definitely has smells! 

I added a test to `ConsumerNetworkThread`."
1600781735,15640,kirktrue,2024-05-14T23:38:45Z,I added a test to `AsyncKafkaConsumerTest`.
1601414540,15640,cadonna,2024-05-15T10:56:53Z,"Why is this not a mock?
You do not need to test the actual reaper here. You just need to verify that the reaper is called correctly in the correct situations. 

Please do not use a spy. "
1601422068,15640,cadonna,2024-05-15T11:02:46Z,"The reaper is not only called in `close()`. It is also called in `unsubscribe()` and `poll()`. I do not know how important it is that the reaper is called in `unsubscribe()` or if it is a collateral that we do not need to test. You know best. However, verifying the calls to the reaper in `poll()` seems important to me, doesn't it?"
1601428828,15640,cadonna,2024-05-15T11:07:52Z,"If you used a mock for the reaper, something like this would be enough to verify the correct use of the reaper in `close()`.

```suggestion
    void testReaperIsCalledInClose() {
        consumer = newConsumer();

        consumer.close();

        verify(backgroundEventReaper).reap(backgroundEventQueue)
    }
```"
1601430406,15640,cadonna,2024-05-15T11:09:01Z,"Also here, why not a mock?"
1601431832,15640,cadonna,2024-05-15T11:10:14Z,With a mock also this test should get simpler.
1601434014,15640,cadonna,2024-05-15T11:12:16Z,"I would write two distinct tests for `runOnce()` and `cleanup()`. By using a mock, it should get simpler to separate the tests."
1602381867,15640,kirktrue,2024-05-15T23:56:21Z,Done
1602381940,15640,kirktrue,2024-05-15T23:56:30Z,Agreed. PTAL.
1602382027,15640,kirktrue,2024-05-15T23:56:39Z,"Yes, PTAL."
1603682545,15640,lianetm,2024-05-16T16:22:52Z,the benefit I see is self-cleaning which reduces the scope/responsibilities of `tracked`. We don't really care about events that are created and complete successfully (so nice that they are just temporarily tracked and automatically disappear when they complete). Then `reapExpiredAndCompleted` it's simply about `reapExpired`
1603740393,15640,lianetm,2024-05-16T17:05:16Z,just a suggestion but I would leave it to you as they may be impl details I'm missing ;)
1603994190,15640,kirktrue,2024-05-16T20:30:44Z,Changed to mock.
1603994499,15640,kirktrue,2024-05-16T20:31:07Z,"Added tests for reaper invocation for `close()`, `poll()`, and `unsubscribe()`."
1603994732,15640,kirktrue,2024-05-16T20:31:20Z,Done.
1604239000,15640,kirktrue,2024-05-17T01:39:30Z,"@lianetm & @cadonnaplease let me know if the following makes sense. I am trying to convince myself of this design as much as anyone else ...

-------

The `ConsumerNetworkThread.run()` method sits in a tight loop that calls `runOnce()` on each pass. The ordering of operations inside `runOnce()` is as follows:

1. Call `processApplicationEvents()`, for each event... 
  a. Call `CompletableEventReaper.add()` to add the event to `tracked` list
  b. Call `ApplicationEventProcessor.process()` to call relevant request manager APIs based on the event. Many of the calls to the request manager APIs will create and return `CompletableFuture`s
2. Call `RequestManager.poll()` for each request manager
3. Call `NetworkClientDelegate.add()` for each `UnsentRequest` returned from step 2
4. Call `NetworkClientDelegate.poll()` to process all the requests added in step 3. This will invoke handlers for received network responses, which will call `complete()`/`completeExceptionally()` to be called on any `CompletableFuture`s created in step 1b
5. Call `reapExpiredApplicationEvents()` (which calls `CompletableEventReaper.reap()`) to remove expired events and any `CompletableFuture`s completed in step 4

The design of `ConsumerNetworkThread` is such that an event's `Future` will _only_ `complete()` (success or failure) in step 4. Any events eligible to be removed from `tracked` in step 1 would have already been removed in the previous loop's step 5. So it makes the most sense to me to leave the removal in `reap()` vs. `add()`."
1608020071,15640,cadonna,2024-05-21T10:02:24Z,Why did you remove this test without replacement?
1608024280,15640,cadonna,2024-05-21T10:05:39Z,You control the time here. Why do you not verify that `reap()` is called with the correct time?
1608024737,15640,cadonna,2024-05-21T10:06:00Z,You control the time here. Why do you not verify that `reap()` is called with the correct time?
1608418633,15640,lianetm,2024-05-21T14:17:52Z,"Do we expect the close to throw? If so, we should verify that (at the moment our test will just complete successfully if the close does not throw). If that's the expectation, maybe this simpler snippet would cover it all:

```suggestion
        Throwable e = assertThrows(KafkaException.class, () -> consumer.close());
        assertInstanceOf(FencedInstanceIdException.class, e.getCause());
        consumer = null;
```"
1608453186,15640,lianetm,2024-05-21T14:38:42Z,"This is not a ""maybe"" anymore, so what about `autoCommitSyncAllConsumed`?"
1608487528,15640,lianetm,2024-05-21T14:59:16Z,Don't we want >= here when identifying expired events? I would expect so (that's the semantic applied in the `Timer` class [isExpired](https://github.com/apache/kafka/blob/9fe3932e5c110443f7fa545fcf0b8f78574f2f73/clients/src/main/java/org/apache/kafka/common/utils/Timer.java#L71) for instance)
1608516652,15640,lianetm,2024-05-21T15:18:31Z,"This `processor` passed as argument is in the end always a reference to the `backgroundEventProcessor`, so  could we simplify this, remove the arg and directly reference the var? It caught my attention when seeing how this is used, which seems a bit redundant with all calls having to provide the same `processBackgroundEvents(backgroundEventProcessor, ...` which feels like an internal that the `processBackgroundEvents` could know about."
1608534184,15640,kirktrue,2024-05-21T15:30:00Z,Good call. Done!
1608534408,15640,kirktrue,2024-05-21T15:30:11Z,"And done here, too."
1608539565,15640,kirktrue,2024-05-21T15:33:44Z,Resolving this as there has been further discussion for some time.
1608540876,15640,kirktrue,2024-05-21T15:34:43Z,Resolving this thread as there have been no further comments for some time. Please un-resolve if there is further discussion needed.
1608541319,15640,kirktrue,2024-05-21T15:35:03Z,Resolving this thread as there have been no further comments for some time. Please un-resolve if there is further discussion needed.
1608541494,15640,kirktrue,2024-05-21T15:35:12Z,Resolving this thread as there have been no further comments for some time. Please un-resolve if there is further discussion needed.
1608541856,15640,kirktrue,2024-05-21T15:35:25Z,Resolving this thread as there have been no further comments for some time. Please un-resolve if there is further discussion needed.
1608542017,15640,kirktrue,2024-05-21T15:35:32Z,Resolving this thread as there have been no further comments for some time. Please un-resolve if there is further discussion needed.
1608542270,15640,kirktrue,2024-05-21T15:35:43Z,Resolving this thread as there have been no further comments for some time. Please un-resolve if there is further discussion needed.
1608542683,15640,kirktrue,2024-05-21T15:36:00Z,Resolving this thread as there have been no further comments for some time. Please un-resolve if there is further discussion needed.
1608584519,15640,kirktrue,2024-05-21T16:00:40Z,Reinstated.
1608595900,15640,kirktrue,2024-05-21T16:06:50Z,"This is an interesting point 

If a user provides a timeout of 1000 milliseconds, is it expired at 1000 milliseconds or at 1001 milliseconds?

Regardless, I will change it to `>=` to be consistent."
1608607062,15640,kirktrue,2024-05-21T16:15:26Z,There is a unit test that passes in a mocked event processor. Let me look at refactoring this.
1608607604,15640,kirktrue,2024-05-21T16:15:53Z,Changed to just `autoCommitSync()`. Is that OK?
1608614638,15640,lianetm,2024-05-21T16:21:36Z,"Yes, the flow makes sense to me. Also fine with me to keep the reap handling completed and expired."
1608730407,15640,kirktrue,2024-05-21T18:01:36Z,Done.
1608732398,15640,kirktrue,2024-05-21T18:03:30Z,Done. That's much better 
1608870767,15640,lianetm,2024-05-21T19:43:33Z,"how did we resolve this? I see the section got completely removed, verification not needed?"
1608875582,15640,kirktrue,2024-05-21T19:48:33Z,"Yes, it turns out that changes made elsewhere have obviated the need for this check."
1608897877,15640,lianetm,2024-05-21T20:10:37Z,"Actually seems to me that we shouldn't have this test here (and maybe this is why @kirktrue removed it before?). As I see it, this unit test is testing something that is not the `ConsumerNetworkThread`'s responsibility (and that's why it ends up being complicated, having to mimic the reaper behaviour and spying). It is testing that events are completed, and that's the reaper.reap responsibility, so seems to me we need to:

1. test that the `ConsumerNetworkThread` calls the reaper with the full list of events -> done already in the [testCleanupInvokesReaper](https://github.com/apache/kafka/blob/91af164415b8b950c70d3a61bb0837c34ae4ed69/clients/src/test/java/org/apache/kafka/clients/consumer/internals/ConsumerNetworkThreadTest.java#L331)
2. test that the `CompletableEventReaper.reap(Collection<?> events)` completes the events -> done in CompletableEventReaperTest ([testIncompleteQueue](https://github.com/apache/kafka/blob/91af164415b8b950c70d3a61bb0837c34ae4ed69/clients/src/test/java/org/apache/kafka/clients/consumer/internals/events/CompletableEventReaperTest.java#L135) and [testIncompleteTracked](https://github.com/apache/kafka/blob/91af164415b8b950c70d3a61bb0837c34ae4ed69/clients/src/test/java/org/apache/kafka/clients/consumer/internals/events/CompletableEventReaperTest.java#L170)) 

In the end, as it is, we end up asserting a behaviour we're mocking ourselves in the `doAnswer`, so not much value I would say? Agree with @cadonna that we need coverage, but I would say that we have it, on my points 1 and 2, and this should be removed. Makes sense? 

"
1608934073,15640,kirktrue,2024-05-21T20:48:04Z,"Yes, the test was a little suspect in terms of its value-add, so I'd removed it.

I was planning to file a Jira to move several of the tests (including this one) from `ConsumerNetworkThreadTest` to `ApplicationEventProcessorTest`. Then we could fix up some of the funkiness in this test as a separate task."
1609312616,15640,cadonna,2024-05-22T06:09:52Z,"That is all fine! I was not arguing that we need to keep the test, but if I see a test removed without replacement, I suspect a mistake. Which did apparently not happen in this case. Next time comment on the PR why you removed the test.    "
1609353060,15640,cadonna,2024-05-22T06:28:44Z,"Do you still have the change locally, because here it does still not verify the correct time?"
