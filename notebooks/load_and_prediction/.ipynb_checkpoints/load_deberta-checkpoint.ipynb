{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a847ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import necessary libraries\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoConfig, DebertaForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73f81a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete!\n"
     ]
    }
   ],
   "source": [
    "project_root = \"/Users/tayebekavousi/Desktop/github_sa\"\n",
    "# Save the original directory to go back to it later if needed\n",
    "original_dir = os.getcwd()\n",
    "# Change to the project root directory\n",
    "os.chdir(project_root)\n",
    "# Ensure the project root is in the Python path\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "sys.path.insert(0, '')  # Add current directory (empty string) to path\n",
    "\n",
    "\n",
    "print(\"Environment setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6cc3e2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at microsoft/deberta-base were not used when initializing DebertaForSequenceClassification: ['lm_predictions.lm_head.bias', 'lm_predictions.lm_head.dense.bias', 'lm_predictions.lm_head.dense.weight', 'lm_predictions.lm_head.LayerNorm.bias', 'lm_predictions.lm_head.LayerNorm.weight']\n",
      "- This IS expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DebertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-base and are newly initialized: ['classifier.weight', 'classifier.bias', 'pooler.dense.weight', 'pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DeBERTa model + tokenizer loaded from .pt checkpoint.\n"
     ]
    }
   ],
   "source": [
    "from transformers import DebertaForSequenceClassification, DebertaTokenizerFast\n",
    "import torch\n",
    "\n",
    "# Step 1: Init model with same architecture\n",
    "model = DebertaForSequenceClassification.from_pretrained(\"microsoft/deberta-base\", num_labels=3)\n",
    "\n",
    "# Step 2: Load saved checkpoint (just the state_dict, not full model)\n",
    "checkpoint_path = \"/Users/tayebekavousi/Desktop/github_sa/saved_models/deberta_3class/deberta_5epochs/model_best_f1_0.8954.pt\"\n",
    "checkpoint = torch.load(checkpoint_path, map_location=\"cpu\")\n",
    "model.load_state_dict(checkpoint[\"model_state_dict\"], strict=False)  # ðŸ‘ˆ add strict=False to avoid minor key mismatch\n",
    "\n",
    "# Step 3: Ready for inference\n",
    "model.eval()\n",
    "\n",
    "# Optional: load tokenizer\n",
    "tokenizer = DebertaTokenizerFast.from_pretrained(\"microsoft/deberta-base\")\n",
    "\n",
    "print(\"âœ… DeBERTa model + tokenizer loaded from .pt checkpoint.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b5e81fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§  Text: \"well , as long as you ok with it lgtm!\"\n",
      "âœ… Predicted Class: Positive (2)\n",
      "ðŸ“Š Confidence: 87.18%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define your class names in the correct order (based on training config)\n",
    "class_names = [\"Negative\", \"Neutral\", \"Positive\"]  # ðŸ‘ˆ update if your order differs\n",
    "\n",
    "# Sample input\n",
    "sample_text = \"well , as long as you ok with it lgtm!\"\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(sample_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Predict\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "\n",
    "# Extract prediction\n",
    "predicted_class_idx = torch.argmax(probs, dim=1).item()\n",
    "predicted_class_name = class_names[predicted_class_idx]\n",
    "confidence = probs[0][predicted_class_idx].item() * 100\n",
    "\n",
    "# Output\n",
    "print(f\"ðŸ§  Text: \\\"{sample_text}\\\"\")\n",
    "print(f\"âœ… Predicted Class: {predicted_class_name} ({predicted_class_idx})\")\n",
    "print(f\"ðŸ“Š Confidence: {confidence:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
