id,pr_number,user,created_at,body,deberta_sentiment_label,deberta_confidence
297865351,2929,lindong28,2017-04-27T23:19:46Z,can you review this patch? the patch has been rebased onto trunk. both integration test and the newly-added system test has passed. thanks.,1,0.9610828757286072
298077449,2929,lindong28,2017-04-28T18:47:47Z,i have gone through the code again after rebasing the patch and it should be ready for review.,0,0.9924506545066833
306559832,2929,junrao,2017-06-06T17:31:21Z,": thanks for the patch. haven't looked at it in details. just a couple of quick comments. (1) once a disk is marked offline, it might be useful for the admin to be able to fix the bad disk (e.g., remounting) while the broker is online. this reduces the time that a broker has to be down while waiting for a disk to be fixed. if we do support this, it would be useful to test this out a bit. (2) according to [a link] it seems that an i/o exception in mmap buffer crashes the jvm. this will affect the failure detection in jbod. not sure how much we can do to improve it in the short term. at the minimum, it would be useful to do some testing to see how much this impacts us and document the impact.",1,0.96886146068573
306989136,2929,lindong28,2017-06-08T03:35:19Z,"thanks for helping with the review! i wanted to rebase earlier but got delayed due to the busy oncall week. i should be able to rebase/test the patch and review it myself again by this friday. hope we can get this reviewed and committed soon instead of another major rebase :) thanks for the suggestion. it is definitely useful for the admin to be able to fix the bad disk while the broker is running. i think it requires a new kip in addition to kip-112 and kip-113 because it takes new tool script and notification event which is not currently included in those two kips. is it ok for me work on this as a new kip after kip-112 and kip-113 is completed? the handling of i/o exception in mmap seems similar to the sixth future work mentioned in kip-112 wiki, i.e. handling various failure scenario case-by-case. kip-112 currently only handles disk failure that can be caught in the form of ioexception instead of jvm crash. i will think about how to handle this i/o exception and let you know my answer.",1,0.9964962601661682
307003408,2929,lindong28,2017-06-08T05:35:28Z,"the patch is ready for review. i have rebased it onto the latest trunk, addressed comments, passed integration tests and gone over the patch myself. thanks!",1,0.986565887928009
309918401,2929,junrao,2017-06-20T23:17:51Z,"""t is definitely useful for the admin to be able to fix the bad disk while the broker is running. i think it requires a new kip in addition to kip-112 and kip-113 because it takes new tool script and notification event which is not currently included in those two kips. is it ok for me work on this as a new kip after kip-112 and kip-113 is completed?"" : for the above, i wasn't referring to the case when the admin fixes the disk and then notifies the broker about the fix online. the case that i was referring to is that when a disk is marked offline, if the admin can fix the disk while the broker is still online, then the admin can just restart the broker quickly after the disk is fixed. the downtime window for that broker is just the restart time. if the admin has to first shut down the broker and then fix the disk, the time for fixing the disk is part of the downtime window. so, my question is that is there any issue that prevents the admin from fixing the offline disk while the broker is up (e.g., the mmap-ed files on the bad disk).",0,0.9180817008018494
309958368,2929,lindong28,2017-06-21T04:16:53Z,"thanks for the explanation. i think i understand your point now. my understanding is that you are concerned with the possibility of broker crash with single disk failure due to mapped files issue. because i don't have multiple disk devices on my desktop to test mount/unmount of log directories while broker is running, i will test it in dev test cluster and i am waiting for sre to mount/unmount log directory for me since they have the sudo access. i will let you know the result tomorrow. i have also gone through the blog of mapped files you provided. most of the issue (e.g. limited number of mmap handlers) are the same between raid-10 broker and jbod broker. since we are not having them now, i assume we won't have them with jbod in the near future. the only issue that is more likely to happen with jbod is that jvm may crash when disk gets full. it is more likely to happen with jbod because the single jbod log directory will have smaller size than the raid-10 log directory. we currently use mmap files for reading/writing to index files. the solution suggested by the blog is to write to index files using `filechannel` instead of mmap files. it is mentioned that performance is minimal if large blocks are used. however, it is not clear whether there will be significant performance hit for index file write operation since the block size for index file write is small. suppose we can not use `filechannel` to write to index files due to performance reason, then i think the solution is for kafka administrator to properly monitor the usage of log directories on brokers and rebalance usage across log directories before any log directory gets full. for example rebalance can be triggered if usage exceeds 60%. in the rare case that there is extreme traffic spike that exhausts the space of a log directory, it seems ok to just let this broker fail. it will reduce availability of the broker in any unnecessary manner but it seems ok if this happens rarely. to summarize, i will test how broker handles umount of log directory while it is running. and kip-113 provides a reasonable solution to deal with vm crash due to full disk. does this sound reasonable?",1,0.9672363996505737
310195927,2929,lindong28,2017-06-21T20:30:00Z,i have updated the code (see [a link] and verified that the all files handlers in the offline log directory will be removed and we can unmount the disk while the broker is still running after the corresponding log directory goes offline.,0,0.9945582151412964
310785873,2929,becketqin,2017-06-23T22:22:44Z,"talked to offline. i think there might be a cleaner way to handle the disk exceptions. it looks that what we want to do when disk exception happens is the following: 1. notify controller there is a disk failure and mark the disk as failed. 2. return an error to whoever sent the request if the failure was triggered by a request. i think we can have a util method to do (1) immediately when the disk io occurs, and bubble up a kafkastorageexception which is a retriable exception. when the clients received this exception, it will act accordingly. the benefit of doing this is that we don't need to care about the what exact operation was in progress when the disk io exception occurs. the leader will always be moved to somewhere else and the related operation will be retried if needed.",0,0.9661317467689514
310809552,2929,lindong28,2017-06-24T03:01:45Z,"after further discussion with , we agree not to handle ioexception in asyc read for fetchrequest. i will add `logmanager` to the constructor of `logcleaner` and `logcleanermanager` so that they can call `logmanager.handlelogdirectoryfailure(...)` when there is `ioexception`. in addition, `partition.getorcreatereplica(...)` will check whether the log of an existing local replica is actually offline and throw `kafkastorageexception` if it is offline. we need this extra check because with the change in logcleaner, it is possible for replicas to be offline in `logmanager` while they are still in the cache `replicamanager.allpartitions`. this guarantees that if `logcleaner` encounters `ioexception`, controller will be notified via zookeeper path and `leaderandisrresponse` will tell controller the offline replicas.",0,0.9889425039291382
310817415,2929,lindong28,2017-06-24T06:08:40Z,"regarding your suggestion to go through usages of all methods in `filerecords`. there are currently many io related operations in kafka whose ioexception is either explicitly swallowed or not explicitly handled. i am not sure we should find out all of them and mark the corresponding log directory and replicas as offline in this patch. one reason is that this maybe over engineering and even reduce the availability of kafka in an unnecessary manner. given that we are not explicitly shutting down broker after catching those ioexception, either most log directory failure will already be caught and trigger `exit.halt(1)`, or kafka broker may be working just fine with its current way of handling those ioexception. thus it may not do us much benefit to handle them in a different way. it is also about the efficiency and the priority of work. i agree with you that there exists ioexception that we can handle in a better way to improve the quality of jbod support. but this task seems independent to the features added in this patch. the main goal of this patch is to change the way we handle log directory failure (i.e. those that currently trigger `halt()`) and it has covered most log directory failure (i.e. those that trigger ioexception during producerequest, fetchrequest, checkpoint read/write and logcleaner read/write). on the other hand, what you asked for is what other ioexception should be treated as log directory failure. i am wondering if we can do that separately in a follow-up patch after kip-112 and kip-113 are implemented. this may help speedup the overall implementation efficiency and reduce the need for rebase by splitting a large patch into smaller ones. also, by finishing kip-112 and kip-113 first, we can deploy the jbod feature sooner and the experience from its deployment can help us determine whether we need to handle some exception differently.",0,0.9051088094711304
311121325,2929,junrao,2017-06-26T17:07:00Z,": i will try to make a pass of the patch later this week as well. about the disk failure detection, we can probably start with something simple. currently, we only fail a broker on ioexception during writes. we can probably just treat that as disk failure to start with. it would be useful to make the failure detection part a bit more general so that we can incorporate more sophisticated failure detection in the future (e.g., consistent long i/os, on-disk crc failure, etc).",0,0.9376205801963806
311122631,2929,lindong28,2017-06-26T17:12:03Z,"it would be great to have your review! thanks! yeah i also would like to start with something simple. becket and i have agreed to limit the scope of disk failure in this patch to mainly those that currently cause `exit.halt(1)` as of the current kafka implementation. and yes, we need to expand it later to make the failure detection more general. for example, detection of long i/os is included as the sixth future work in the kip-112.",1,0.9963911175727844
311594553,2929,lindong28,2017-06-28T08:36:18Z,"thanks much for your review! i have updated the patch to address all the comments. i have added one integration test to simulate the log directory failure and verify that producer will receive notleaderforpartitionexception. note that producer request does not necessarily trigger ioexception immediately after log directory failure because log append operation uses mmap and its write operation to disk is delayed. on the other hand, our write operation to checkpoint file can see ioexception immediately because it uses filechannel. and if the ioexception is triggered when broker writes to checkpoint file, produceresponse will show notleaderforpartitionexception instead of kafkastorageexception. i will try to add another test to verify that producer can send message after retry.",1,0.9919021725654602
311597230,2929,lindong28,2017-06-28T08:48:08Z,"it seems that has finished reviewing the patch and i think the kip-112 implementation is in good shape. on the other hand, i am in the process of writing kip-113 and it should be ready for review this week. i am not sure if you have time to review both kip-112 and kip-113. kip-113 is like going to be more complicated than kip-112 and needs closer look by senior committers. i am wondering if you can help review kip-113 if you have time for only one kip. thanks!",1,0.9912461638450623
311820470,2929,lindong28,2017-06-28T23:31:46Z,"i have updated the newly-added test and it verifies that the producer will see notleaderforpartitionexception when there is log directory failure and succeed after it retries. i have also rebased the patch onto trunk. all tests have passed except for the one test in kafka connector, which i don't think is caused by this patch.",0,0.9402201175689697
311823840,2929,junrao,2017-06-28T23:53:38Z,: i am taking a look a the patch now. should be done by tomorrow.,0,0.9298561215400696
311824795,2929,lindong28,2017-06-29T00:00:41Z,thanks much!,1,0.9186553955078125
312411970,2929,lindong28,2017-07-01T05:28:57Z,"thanks so much for detailed review. i have addressed most of the comments in the updated patch. the main remaining issues are 1) whether we should invoke `replicamanager.handlelogdirfailure()` in `logmanager.handlelogdirfailure()` and 2) whether/how we can handle `ioexception` in the methods of `transactionstatemanager`. i have provided explanation under the corresponding comments. btw, there is one comment regarding a line in `partition.scala` that can only be replied in [a link] but not in this page.",1,0.9784339070320129
313317734,2929,lindong28,2017-07-06T07:24:55Z,thanks much for the comment .,1,0.5755707025527954
313796522,2929,lindong28,2017-07-07T21:23:32Z,"finally i was able to fix the test failure by closing the newly added logdirfailurehandler thread, removing newly-added when the replicamanager shutdown, and optimizing the `logmanager.livelogdirs` not to instantiate new array when there is no offline log dir. these are only causing test failure in github (not on my machines) probably because the virtual machine used in github is slower with less memory. the patch is fully ready for review again :)",1,0.9911670088768005
314612960,2929,lindong28,2017-07-12T00:51:40Z,"excuse me.. i am wondering if there is any further issue to be addressed with the patch. or if there is no major issue and if you are busy, is it ok for becket to give it a final pass and merge it?",0,0.8365232348442078
314808010,2929,junrao,2017-07-12T15:35:26Z,: i started looking at your latest patch. i will finish my review today.,0,0.9797266721725464
314838404,2929,lindong28,2017-07-12T17:22:20Z,: thanks much for your time! i also reviewed the entire patch again and found 3 minor issues. i will address all of them after your review.,1,0.9931339025497437
315308226,2929,lindong28,2017-07-14T08:54:46Z,"thanks so much for taking so much time to review this patch! i thought the patch is ready but clearly there was space for improvement. i have updated the patch to address all comments. i have gone over the patch carefully with the hope that i don't miss anything this time. besides minor changes such as the method comment and removal of unused import, here are the bigger changes that were made: - simplified the way we handle kafkastorageexception such that we no longer need to invoke `maybeaddlogfailureevent()` if `kafkastorageexception` is caught. - added a final static variable `replicamanager.offlinepartition` and use it to make sure that kafkastorageexception is consistently returned when client attempts to access an offline replica. this is verified with the updated test code. - replaced all usage of `kafka.common.kafkastorageexception` with `org.apache.kafka.common.errors.kafkastorageexception` so that we can remove `kafka.common.kafkastorageexception` going forward. - added metric `offlinereplicacount` which counts the number of offline local replicas on a broker. - added logdirfailurechannel to log and logsegment and invoke `maybeaddlogfailureevent()` right after the ioexception is caught. - renamed the new exception to unknownerrorcodeexception",1,0.992957353591919
315517505,2929,lindong28,2017-07-15T07:52:13Z,"this afternoon i reviewed the patch myself by going over the entire patch line by line. i improved the comments in a few places, removed unnecessary imports and reverted a few changes that have become unnecessary now. i sincerely think that i should have fixed these before your previous review.. for ease of review, i have squashed the commits before your previous review into one commit ([a link]. the second commit ([a link] includes the change that were made to address your previous comments from the previous review. the summary of this change can be found in the earlier comment i posted yesterday. the 3rd commit ([a link] ) improved comment and reverted unnecessary changes. some major changes where made based on that previous two rounds of review from you, e.g. introduction of the logdirfailurechannel and the consistency of kafkastorageexception in response. i am 95% confident now that the patch won't need similar major change anymore in its current state. i will give this patch another two rounds of end-to-end review, make sure that there is indeed no further improvement i can figure out by myself, before giving it to you for further review. thanks again for your time!",1,0.721525251865387
315590692,2929,lindong28,2017-07-16T07:14:33Z,"i gave the patch another round of review, made some minor improvements (e.g. comment) and fixed system test failure. the only notable change is that `kafkastorageexception` and `unknowncodeexception` now extends `invalidmetadataexception`. this is needed so that producer can update metadata if the leader replica is now offline. these changes are in [a link] commit. i will give the patch the third review tomorrow.",0,0.7333003282546997
315707331,2929,lindong28,2017-07-17T09:30:01Z,i have carefully reviewed the patch three times and made all the improvements i can think of. all system tests and integration tests have passed. would you have time to review the patch sometime this week? thank you! there is only one improvement that i haven't made in this patch. currently the producer will still send producerequest with version 3 if the message magic version is 2. this is because the newly-added produce version 4 is incompatible with 0.11 broker. the downside of this is that server will translate kafkastorageexception to notleaderforpartitionexception. but this should not cause any problem to the functionality of the producer. we can fix this in a followup patch if needed.,1,0.9904698133468628
315982647,2929,lindong28,2017-07-18T07:34:12Z,thanks so much for the quick and detailed review! i have addressed most of the comments. please see my reply above.,1,0.9951570630073547
316247280,2929,lindong28,2017-07-19T01:40:30Z,i have updated the patch and addressed all the comments. all tests have passed. i have reviewed the latest commit and it looks ok to me. i am reviewing the entire patch with all the commits.,1,0.6937696933746338
316276230,2929,lindong28,2017-07-19T05:23:52Z,thanks much for all the help! i have rebased the patch onto trunk head and finished reviewing the patch. the following changes were made in the latest commit. - catch ioexception before it is propagated to replicamanager - removed a few other ioexception handling now that the ioexception will be caught and thrown as kafkastorageexception in log's methods. - halt broker on log directory failure if inter.broker.protocol < 0.11.1 - removed unknownerrorcodeexception - updated upgrade.html,1,0.9936338663101196
316926503,2929,lindong28,2017-07-21T07:27:48Z,"thanks much for the quick help! i have rebased the patch onto trunk head and addressed all the comments. in particular, i have further moved ioexception from logsegment to those methods in log and updated the comments as suggested. i have reviewed the entire patch again end-to-end and fixed everything to my best knowledge. it should be ready for your review again. thanks!",1,0.9954198598861694
317064344,2929,lindong28,2017-07-21T17:35:06Z,"thank you so much for taking so much time to review the patch. i definitely think the current patch is much more consistent in style and cleaner than it was before. initially i was focusing only on the functionality. i will focus more on the code style and consistency next time, e.g. in kip-113. i have addressed jun's comments and rebased patch onto trunk head. all tests have passed. i will also review the patch again myself. can you take a look at the patch? thanks!",1,0.9951692223548889
317103017,2929,lindong28,2017-07-21T20:20:20Z,i have finished reviewing the patch and made minor changes in the latest commit. i verified that the patch has passed all ducktape system tests.,0,0.9291926622390747
317157179,2929,lindong28,2017-07-22T05:32:34Z,thanks much for your help! i have updated the patch to address all comments except those that can be done in the followup patch. and the patch has been rebased onto trunk head.,1,0.99184650182724
656589433,9001,dajac,2020-07-10T09:48:19Z,i just noticed that you haven't updated the code which creates the `apiversionsresponse` in `saslserverauthenticator`. is it intentional or something left to be done?,0,0.9914728403091431
656902905,9001,kowshik,2020-07-10T21:37:40Z,"thank you for taking a look! iiuc you are referring to these lines: [a link] my requirement is that under the hood of the newly added api: `org.apache.kafka.clients.admin#describefeatures`, the `apiversionsresponse` returned to the `adminclient` needs to contain the features information. note that this new api issues an explicit `apiversionsrequest` under the hood. in such a case do you think i should populate the features information in the above lines in `saslserverauthenticator` too? i'm trying to understand where would this come into play (sorry i know little to nothing about `saslserverauthenticator`).",1,0.9884457588195801
659752993,9001,abbccdda,2020-07-17T00:27:00Z,retest this,0,0.9793913960456848
659793027,9001,abbccdda,2020-07-17T02:19:38Z,retest this,0,0.9793913960456848
700585584,9001,kowshik,2020-09-29T09:31:43Z,"thanks a lot for the review! i've addressed the comments in the recent commit: 06d8b47131f168db88e4f7d5bda3dd025ba9a2a2. i've provided a response to all of your comments. there are few i couldn't address, and 1-2 comments i'll address in the near future.",1,0.9936659932136536
702023940,9001,kowshik,2020-10-01T09:52:21Z,"thanks for the review! i've addressed the comments from your most recent pass in a7f4860f5f8bb87cfb01452e208ff8f4e45bcd8b. to answer your question, the deployment will fail if the feature was finalized at say `[minversionlevel=1, maxversionlevel=6]` previously, but the new broker only supports version range: `[minversion=4, maxversion=6]`. this is where `firstactiveversion` becomes useful. by bumping it up during a release (instead of the supported feature's `minversion`), we are able to get past this situation. when `firstactiveversion` is advanced in the code, and the cluster is deployed, the controller (and all brokers) know that the advancement acts a request to the controller to act upon the feature deprecation (by writing the advanced value to `featureznode`). so, in this case we would release the broker with the supported feature version range: `[minversion=1, firstactiveversion=4, maxversion=6]`, and the deployment wouldn't fail.",1,0.9873286485671997
702026788,9001,kowshik,2020-10-01T09:57:55Z,thanks for the review! i've addressed the comments in c31d6b5245c635e659ff0f203bd08bc015a15ffb.,1,0.9889233708381653
702621705,9001,kowshik,2020-10-02T09:21:02Z,thanks for the review comments! i have done the change proposed in [a link] in the most recent commit: 4218f95904989028a469930d0c266362bf173ece . please have a look.,1,0.9928688406944275
703048608,9001,kowshik,2020-10-03T05:14:21Z,"thanks for the review! i've addressed the comments, the pr is ready for another pass. i've also fixed the compilation errors.",1,0.9922240972518921
703873486,9001,kowshik,2020-10-05T20:34:42Z,thanks for the review! i've addressed the latest comments in e55358fd1a00f12ef98fc4d2d649a297ddf146da . the pr is ready for another pass.,1,0.9878206849098206
704393633,9001,junrao,2020-10-06T16:21:21Z,: any more comments from you?,0,0.9886167645454407
704596581,9001,kowshik,2020-10-06T22:59:16Z,"the test failure in `mirrorconnectorsintegrationtest.testreplication` does not seem related. i have rebased the pr now against latest ak trunk, i'd like to see if the failure happens again.",0,0.989267885684967
704786154,9001,kowshik,2020-10-07T08:40:13Z,the test failures in the latest ci runs do not seem related to this pr: * jdk 8: `org.apache.kafka.connect.integration.exampleconnectintegrationtest.testsourceconnector` * jdk 11: `org.apache.kafka.streams.integration.eosbetaupgradeintegrationtest.shouldupgradefromeosalphatoeosbeta` the test that failed previously under jdk 15 has passed in the latest ci run: `mirrorconnectorsintegrationtest.testreplication`.,0,0.9920592308044434
705068372,9001,junrao,2020-10-07T16:59:34Z,: there are 27 system test failures with this pr. [a link] are they existing test failures compared with [a link] ?,0,0.9910210967063904
705073059,9001,kowshik,2020-10-07T17:07:49Z,"thanks for the links! i had a look at the links. i found similar stats in both links, with exactly 27 test failures in both links. i compared the individual test failures and found that they have all failed on the same tests. would that mean we are ok to merge this pr (since it doesn't seem to introduce a new failure)?",1,0.9917942881584167
705079102,9001,junrao,2020-10-07T17:18:59Z,": thanks for following up. i will merged this pr as it is since the system test failures are not new. also, in scala, we prefer sealed traits over enumeration since the former gives you exhaustiveness checking. with scala enums, you don't get a warning if you add a new value that is not handled in a given pattern match. maybe you can address that in your followup pr.",1,0.9788358807563782
705378300,9001,chia7712,2020-10-08T07:10:05Z,sorry for bringing trivial comments after this is merged. i just noticed those nits in testing new apis in 2.7.0.,-1,0.9929087162017822
705419934,9001,kowshik,2020-10-08T08:35:51Z,"no worries, thanks for the suggestions! i have opened a separate pr addressing your comments. would you be able to please review it? [a link]",1,0.9947340488433838
276871714,2476,lindong28,2017-02-02T05:26:00Z,-rosenblatt i have manually tested it successfully using the `./bin/kafka-purge-data.sh` in the patch. i am going to add unit test and probably ducktape integration test as well. but the core code should be ready for review. would you have time to review the patch?,0,0.9362202286720276
277101595,2476,ijuma,2017-02-02T22:13:55Z,", thanks for the pr. i probably won't have time to review before next week. cc as well since he reviewed the kip.",1,0.978408694267273
279289445,2476,lindong28,2017-02-13T03:58:27Z,-rosenblatt i have added tests and the patch is fully ready for review. would you have time to review this patch?,0,0.9757077097892761
279343352,2476,becketqin,2017-02-13T10:06:50Z,thanks for the patch. it seems the patch has conflicts. could you rebase?,1,0.9223902225494385
279471864,2476,lindong28,2017-02-13T18:04:15Z,i thought it will take 1+ week for the patch to be reviewed and there will be conflict again anyway. thus i was going to rebase it after first round of review. what is our general guideline for rebasing big patches? i can certainly rebase it now if you think it is useful.,0,0.9850539565086365
279524819,2476,lindong28,2017-02-13T21:16:17Z,all conflicts have been resolved and all tests are passed. thanks!,1,0.9877304434776306
279567997,2476,becketqin,2017-02-14T00:24:45Z,"thanks for updating the patch. i'll take a look. usually if there are multiple big patches in parallel, the committers who are reviewing the code would hold back some of the patches to avoid unnecessary rebase.",1,0.96309494972229
281270870,2476,lindong28,2017-02-21T07:59:37Z,thanks so much for taking time to review the patch! can you check if the updated patch has addressed your comments?,1,0.9882556200027466
281594223,2476,lindong28,2017-02-22T07:48:30Z,i have addressed all your comments and all tests have passed. could you take another look? thanks!,1,0.9734085202217102
282936279,2476,becketqin,2017-02-28T04:05:51Z,do you have time to take a look? given that we probably have a few big patches from kip-98 (and potentially kip-82 and kip-112). we probably need to synchronize on them to avoid unnecessary rebase. it might be better to get this patch in first. what do you think? thanks.,1,0.8646334409713745
283340571,2476,ijuma,2017-03-01T13:33:25Z,"also, it would be good to merge trunk into this branch. i know this can be annoying, but the request/response code has changed enough that some parts of the review can't be done properly without that.",0,0.9755402207374573
283426390,2476,lindong28,2017-03-01T18:28:24Z,"thanks for reviewing the patch. this kip depends on the kafka-4820 and i will rebase the patch after kafka-4820 is committed. and i will also explain the purgedatacommand in the mailing thread and the kip wiki. initially we considered to use purge, truncate or delete in the method name. i don't have a strong preference among them. i chose purge because joel/becket prefers it and no one else raises concerns with it in the open source discussion. i think one reason is to distinguish it from `deletetopic` method. now that we have `data` in the name, i agree that it may be better to use `deletedatabefore`. let me propose it in the open source mailing list. i think the purpose of including `data` in the method name is to distinguish it from `deletetopic` method name. the reason of not having `data` in the api name purgerequest is because `producerequest` or `fetchrequest` doesn't include `data` in the name. the reason of including `before` in the method name is because we may want to have a method to delete data after a given offset in the future.",1,0.949891984462738
283438149,2476,ijuma,2017-03-01T19:09:53Z,thanks for explaining the reasoning for the name choices. let me think about it and see how it fits with the other proposed adminclient methods.,1,0.9493967890739441
284215633,2476,lindong28,2017-03-05T09:17:15Z,i have addressed all the comments and rebased the patch onto trunk. do you have time to review the latest patch?,0,0.9948177933692932
284963025,2476,lindong28,2017-03-08T06:56:21Z,thanks much for your review. i have updated the patch to address most of the comments. can you see if my reply to your comments make sense?,1,0.973227858543396
285518268,2476,lindong28,2017-03-09T23:29:55Z,do you have time to give the patch another pass? i have gong through the patch and all the reviews and the patch seems good now.,0,0.6417263746261597
285582265,2476,junrao,2017-03-10T05:31:15Z,: have you run system tests on this pr?,0,0.9941755533218384
285972512,2476,lindong28,2017-03-12T20:13:07Z,sorry for late reply. i didn't run ducktape-based system tests previously. i have been trying to run `tests/kafkatest` since your comment. but it seems that some tests will fail even on kafka trunk without my patch. i am doing more detailed analysis to see if my patch is the cause of any test failure.,-1,0.9874558448791504
286312386,2476,lindong28,2017-03-14T03:34:02Z,some tests fail consistently even without my patch if i run ducktape-based system test on my desktop. i am not sure if it is because the setup on my desktop. becket just told me that it may be more reliable to run system test via [a link] but all recent system tests have failed due to error `importerror: no module named setuptools` since last friday. do you know who can look into this problem and fix the jenkins setup?,0,0.639737606048584
286471347,2476,junrao,2017-03-14T16:09:04Z,: sorry for the inconvenience. we just fixed an issue in the branch builder. could you try it again?,-1,0.9933745265007019
286545382,2476,lindong28,2017-03-14T20:13:53Z,thanks much for the quick help! please ignore my previous comment.. i thought my test has succeeded but i was looking at the wrong test result. it should take another 3 hours for my test to finish.,1,0.9934127926826477
286560968,2476,lindong28,2017-03-14T21:07:54Z,"i noticed that in a test i started yesterday using `system-test-kafka-branch-builder-2` (instead of `system-test-kafka-branch-builder`), my branch has indeed passed all tests. see [a link] the patch has been rebased onto trunk without any conflict and `nextoffsetfromlog()` has been made private. do we need further review or test? is there anything else i can do? thanks for help!",1,0.9891610741615295
286928771,2476,lindong28,2017-03-16T01:15:34Z,"i have renamed `purgerequest` to `deleterecordsrequest`, `purgeresonse` to `deleterecordsresponse`, and `purgedatabefore()` to `deleterecordsbefore()`. for simplicity and consistency of internal code, i am still using `purge` to refer to `deleterecords` when `records` is not explicitly specified as the operation target. for example, i use `logpurgeresult` instead of `logdeleteresult` or `logdeleterecordsresult`. and `delayedpurgepurgatory` instead of `delayeddeletepurgatory` or `delayeddeleterecordspurgatory`. this should address jun's goal of differentiating between removing whole log vs removing portion of the log since we only use `delete` when `records` is specified. can you review this patch again? thanks!",0,0.9905312657356262
287506966,2476,becketqin,2017-03-18T01:28:42Z,thanks for the patch. lgtm except one pending comments. do you have time to take another look? thanks.,1,0.9823870658874512
287695596,2476,lindong28,2017-03-20T08:02:37Z,thanks for the comment! i have addressed most of them. can you check if my reply make sense?,1,0.9927390217781067
288525644,2476,lindong28,2017-03-22T20:17:33Z,test with jdk 8 and scala 2.11 failed because `kafkastreamstest.testcannotstartonceclosed` took more 2+ hours before it is killed. `./gradlew -pscalaversion=2.11 streams:test --tests org.apache.kafka.streams.kafkastreamstest` works on my machine. thus the failure is probably not caused by this patch. all system tests have passed. see [a link],0,0.9877618551254272
288563254,2476,lindong28,2017-03-22T22:46:29Z,": all comments are addressed. all integration tests and system tests have passed. and has reviewed the latest patch. since we are so close to finish review for this patch, would you have time to review the patch again so that we can merge it before other major change in trunk?",0,0.5548630952835083
288931216,2476,lindong28,2017-03-24T04:34:15Z,"all comments have been addressed as suggested. all tests have passed except `org.apache.kafka.streams.integration.kstreamaggregationdedupintegrationtest.shouldreduce` with scala 2.12, which i don't think is caused by this patch. thanks for all your time to review the patch!",1,0.989155113697052
289188244,2476,lindong28,2017-03-25T04:32:33Z,thanks for the review. yeah it took me quite a while to rebase the patch. i just finished rebase and the integration tests have passed. i will run system tests as well. because it is very time consuming to rebase each single commit. i end up squash the all commits into one commit onto trunk. can you see if my reply to your most recent comment make sense?,1,0.9532040953636169
289336913,2476,lindong28,2017-03-27T02:05:10Z,the patch has been rebased again on ismael's most recent commit. all integration and system tests have passed. can you review the latest patch? thanks!,1,0.9612260460853577
289613754,2476,lindong28,2017-03-27T23:13:06Z,"as of current patch the logstartoffset of compacted topic is always 0. it seems reasonable because if consumer seek to offset 0 of a compacted topic, they will still be able to consume (instead of receiving offsetoutofrangeexception) even if the offset of the first message is larger than 0. and it is not straightforward to retrieve the offset of the first message in the segment. according to java doc, `recordbatch.baseoffset()` will `return the first offset of the original record batch for magic version prior to 2`, which means the baseoffset of the first batch of the first log segment doesn't necessarily tell us the offset of the first message in the log. does this sound reasonable?",0,0.9908258318901062
289655962,2476,junrao,2017-03-28T03:56:11Z,"it seems that right now, for a compacted topic, the base offset of the first segment is always 0. so, the patch is fine.",0,0.9736289381980896
289656024,2476,junrao,2017-03-28T03:56:48Z,: thanks for the patch. lgtm. : do you want to make another pass and then merge?,1,0.9825415015220642
289827754,2476,lindong28,2017-03-28T16:32:15Z,all integration tests have passed except `org.apache.kafka.connect.runtime.workertest.testaddremovetask` with scala 2.11. i don't think this is caused by this patch because this test passed all other recent tests. thanks so much for taking time to review this patch!,1,0.9932540059089661
289838515,2476,becketqin,2017-03-28T17:08:36Z,thanks for updating the patch. merged to trunk. thanks for the review!,1,0.9931790828704834
283204521,2614,hachikuji,2017-03-01T00:21:50Z,"cc a couple initial points of discussion: 1. message class hierarchy. the `logentry` interface now represents the record set and `logrecord` represents the record. i have left `records` with its current name which is a bit weird when you need to write things like `records.entries.records`. i have also been thinking of changing `logentry` to `logrecordset`, which would make the `records` name a bit more intuitive. previously i considered changing `records` to simply `log`, though that's a little annoying because of the server class with the same name. 2. the new magic version is implemented in `eoslogentry` and `eoslogrecord`. this name should be changed of course. any suggestions? the old magic version is implemented in `oldlogentry`, so we could call it `newlogentry`?",0,0.772894561290741
283401129,2614,hachikuji,2017-03-01T17:00:01Z,"fyi: system tests run here: [a link] the one failure looks like a known problem, but i haven't investigated.",0,0.9884845614433289
283982801,2614,ijuma,2017-03-03T15:24:57Z,", good questions! 1. i don't like the `set` suffix because order is relevant, so if we wanted to go that way, i think it should be `logrecordslist`. which is kind of just `logrecords`, which is confusing given that it's part of `records`. is record set just a record batch? 2. instead of `newlogentry`, should it be `defaultlogentry`? and `oldlogentry` could be `legacylogentry` or something like that.",1,0.9930047392845154
284025203,2614,hachikuji,2017-03-03T18:00:04Z,"thanks for the suggestions. 1. it is a batch. the producer already uses `recordbatch`, but maybe we could commandeer that name? i'm not too fond of `logrecordslist`. it's a bit tough to come up with something reasonable given the name of `records`. i'd rename that personally to something which conveyed the fact that it was a sequence of bytes of the log (but of course i already tried that and failed). 2. `defaultlogentry` sounds good to me. no strong preference between `oldlogentry` and `legacylogentry`. but perhaps ""legacy"" is a bit more suggestive of its use. the other thing i wanted to mention is that i've left around the old `record` class more or less as it currently is. this is why i needed to introduce the `logrecord` interface. this seemed fine given use of the ""log"" prefix in `logentry`, but i've considered several times moving the current `record` into `oldlogentry`, and then using `record` in place of `logrecord`. what do you think?",1,0.9847307205200195
284029702,2614,ijuma,2017-03-03T18:18:21Z,"i'm not fond of `logrecordslist` either, in case it was not clear from my message. :) i was thinking that maybe we could rename the existing `recordbatch` to `producerbatch` or something. yes, i prefer `legacy` a bit for the reason you state. i think it makes sense to rename `logrecord` to `record`. the current record could then either be moved to `legacylogentry` (already using this name ;)) or it could just be `legacyrecord`.",1,0.9931454658508301
284031945,2614,hachikuji,2017-03-03T18:27:27Z,"so if we follow those suggestions, then we have the following hierarchy: [code block] that seems reasonable to me. the `recordbatch` -> `producerbatch` renaming will probably cause some confusion, but people will get over it.",0,0.9852132201194763
284033990,2614,ijuma,2017-03-03T18:35:46Z,", what do you think?",0,0.9417104721069336
284537436,2614,junrao,2017-03-06T21:24:23Z,"a few other high level comments. 1. in the eos message format, it's probably worth considering including the message count in the message format. this has the benefit that the consumer can allocate the right size of the array to store those messages and is also consistent with how we represent an array in other places. 2. about eoslogentry and oldlogentry. perhaps we can name them based on the magic. so, it would be v2logentry, v1logentry and v0logentry.",0,0.9094181060791016
284557074,2614,ijuma,2017-03-06T22:36:50Z,", about including the message format version as a prefix or suffix. i considered that, but the issue is that more than one version is supported by each class. unless we want to move away from that, it seems a bit awkward.",-1,0.5151100754737854
285983415,2614,junrao,2017-03-12T22:44:20Z,": currently, the broker supports a debuggingconsumerid mode for the fetch request. should we extend that so that the consumer can read the control message as well? should we also have some kind of debuggingmessageformatter so that consoleconsumer can show all the newly introduced fields in the new message format (e.g., pid, epoch, etc) for debugging purpose? both can be done in a followup patch if needed.",0,0.9901124835014343
286815104,2614,junrao,2017-03-15T17:19:08Z,": also, it would be great if you could do some basic perf test to make sure there is no noticeable performance degradation.",0,0.7095149755477905
289112773,2614,hachikuji,2017-03-24T18:50:07Z,ran system core and client system tests here: [a link] the failing zookeeper upgrade test seems to be transient. here's a passing run: [a link] i investigated the failure in one case and it seems to be unrelated (apparently caused the overridden min isr setting of 2 and the fact that the test does not actually ensure that 2 brokers are always up).,0,0.9918631315231323
289124423,2614,apurvam,2017-03-24T19:39:54Z,lgtm!,1,0.8055883049964905
308997035,3131,ijuma,2017-06-16T11:00:47Z,please close this pr and use the mailing list for questions.,0,0.9933521747589111
330710008,3874,junrao,2017-09-20T00:22:38Z,"also, in the request protocol and objects, we have alter_replica_dir_request and describe_log_dirs_request. could we make them consistent?",0,0.9943600296974182
330768947,3874,lindong28,2017-09-20T07:29:33Z,"thanks much for reviewing the patch! i have answered all comments and addressed most of them. i think it is reasonable to rename alter_replica_dir_request to alter_replica_dirs_request. given that it is merely a refactor and it touches many files, should we do this in a follow up patch before or after this patch is merged? i am not sure if you prefer to see this refactor in this pull request because it may make it harder to review this patch.",1,0.9891554713249207
330771247,3874,lindong28,2017-09-20T07:40:34Z,i have update the patch to help discussion. i will fix test failures tomorrow.,0,0.9888213872909546
330891633,3874,ijuma,2017-09-20T15:37:45Z,we can probably do the request naming fixes in a follow-up. i also raised some issues regarding the naming of the adminclient methods. maybe we can consider fixing those in the same follow-up.,0,0.9904946088790894
330891767,3874,ijuma,2017-09-20T15:38:10Z,", there is a merge conflict and 6 test failures.",0,0.896422803401947
331022962,3874,ijuma,2017-09-21T01:13:16Z,", let me know when is a good time to start a system tests run.",0,0.9708704352378845
331024201,3874,lindong28,2017-09-21T01:22:36Z,"sure. i am trying my best to address commands and add test. can you tell me the deadline for getting this patch committed and pass system tests if we were to include it in 1.0 release? btw, i am flying to shanghai and will be on vacation 9/21 - 10/16. my response maybe a bit delayed due to the flight and various errand. admittedly it makes it harder for me to focus on the patch and the test. i will do best effort.",0,0.5818274617195129
331086912,3874,lindong28,2017-09-21T08:20:48Z,"i have rebased the path onto trunk head and fixed most tests. currently i disabled 5 tests in replicafetcherthreadtest because these tests replied on the previous internal implementation of replicafetcherthread.maybetruncate() which is changed in my patch. i tried to fix the test for 1 hour but couldn't probably due to my lack of experience with easymock library. although all tests can pass, it seems that there is file descriptor leak because my mac explains about `too many open files in system`. i will try to fix it tomorrow. i don't have a good idea where it comes from.. i think you can run the system test with my patch. it is probably a good idea to also run the system test without this patch.",0,0.7527862191200256
331087122,3874,lindong28,2017-09-21T08:21:46Z,"fyi, there is one remaining issue related to how we rename directories to replace current log with the future log. this may also take some time to discuss and implement.",0,0.9880759716033936
331212055,3874,ijuma,2017-09-21T16:34:18Z,i started the system tests a few hours ago. i will check and post the status soon.,0,0.9882254600524902
331243188,3874,lindong28,2017-09-21T18:32:08Z,"i have fixed all tests in replicafetcherthreadtest and rebased patch onto latest trunk. ""too many open files in system"" seems to stop showing up when i run tests on mac laptop after rebasing the patch. the only remaining issue is how to promote future replica to be the current replica. i am awaiting jun's reply.",0,0.8426180481910706
333150135,3874,ijuma,2017-09-29T14:58:44Z,"there was a discussion about potentially tweaking the name of the adminclient and request/response for `alterreplicadir` and `describelogdirs`. the adminclient api would have to be changed before the code freeze next wednesday. what is the current thinking? two points from me: 1. as i raised previously, `alterreplicadir` doesn't follow the naming convention of other adminclient methods that are `plural`. i think we should definitely fix this one. 2. the lack of symmetry between `alterreplicadir` and `describelogdirs` is not ideal. could we not simply call the latter `describereplicadirs` as well? this would have to be a separate pr, but i just raised it here since there was an ongoing discussion about it. happy to take it to a separate pr or jira.",0,0.898744523525238
333152696,3874,lindong28,2017-09-29T15:08:09Z,"regarding 1), i think it is a good idea to rename `alterreplicadir` to `alterreplicadirs`, and similarly rename the corresponding request and response. i will submit a patch to do it. regarding 2) , describelogdirs has this name because it returns the information per log directory. there is already an existing adminclient api `describereplicalogdir` which returns the log directory information per replica. so i am not sure we need to have symmetry between alterreplicadir and describelogdirs.",0,0.8706763386726379
333157758,3874,ijuma,2017-09-29T15:26:26Z,"sounds good , i had missed the other method.",1,0.771960973739624
333158559,3874,lindong28,2017-09-29T15:29:11Z,"talking about describereplicalogdir, maybe we should rename this to describereplicalogdirs as well?",0,0.9907926917076111
333159096,3874,ijuma,2017-09-29T15:30:59Z,"yes. also, shouldn't `describereplicalogdir` be `describereplicadirs`? i thought that this one was supposed to be the dual of `alterreplicadirs`?",0,0.9882263541221619
333160232,3874,lindong28,2017-09-29T15:35:21Z,"it is describereplicalogdir because we return the log directory of the partition in the response, rather than the directory of the partition. the directory of the partition is essentially path_to_log_dir/topi-partition. previously i used alterreplicadirs because alterreplicalogdirs seems a bit tedious. technically speaking we are changing both the log directory and the directory of the partition. so it seems fine to say alterreplicadirs. do you think we should rename it to alterreplicalogdirs?",0,0.9799531698226929
333161547,3874,ijuma,2017-09-29T15:40:19Z,"oh, i see. yes, it does seem to me that it would be clearer if they both talked about `logdirs`. it may make sense to hear if has any thoughts before doing the renaming as it's a bit tedious to change it a second time.",0,0.965218186378479
333276886,3874,lindong28,2017-09-30T02:42:50Z,all issues have been addressed as discussed. i have made a pass through the patch and made minor improvements. i will pass through the patch another 2 times and let you know.,0,0.7899742126464844
333287139,3874,lindong28,2017-09-30T06:15:50Z,"also, thanks so much for taking time to review the patch! your comments are really helpful.",1,0.9958027005195618
333711087,3874,lindong28,2017-10-03T01:17:58Z,yes. i think we should commit [a link] first. i am not sure about the details of the 1.0 release plan. my understanding is that we should focus first on the stability for 1.0 release at this time. thus we should commit bug fixes such as [a link] before 1.0 code freeze. this patch can be committed later after we have finished all necessary works needed for 1.0 release. i will work on your comments. thanks!,1,0.991275429725647
333935484,3874,lindong28,2017-10-03T18:27:16Z,"thanks for your detailed comments! i have rebased patch onto trunk, renamed methods following kafka-5995, reviewed patch myself end-to-end, and replied to all comments. i think the only remaining issue is whether future replica should be truncated using leader epoch after the current replica is truncated.",1,0.9917930960655212
335013622,3874,lindong28,2017-10-08T15:19:12Z,i have rebased patch onto trunk and replied or addressed the latest comments. i will review the patch end-to-end after we have agreed on the solution to the latest comments. thanks!,1,0.9851509928703308
335988493,3874,lindong28,2017-10-12T00:47:48Z,let me review the patch one more time before committing this patch. thanks!,1,0.9487960934638977
336215355,3874,junrao,2017-10-12T17:52:21Z,": there are a few system test failures on your branch. [a link] most of them seem to be related to the old consumer, which is potentially affected by the abstractfetcherthread change in this patch.",0,0.9908773303031921
336464525,3874,lindong28,2017-10-13T14:13:31Z,"there are 6 system test failure in the link you provided. i run system test locally for 3 of the 6 tests and all of these tests succeeded. then i downloaded the log for these three tests (e.g. `kafkatest.sanity_checks.test_verifiable_producer`). for all these three tests, verifiable_producer.stdout shows the following exception: [code block] i am not sure how this exception could be related to this patch since this patch does not touch the constructor of producerrecord. could you re-run the test, maybe locally, and see if the failure can be reproduced? could you tell me which git hash was used in the system test? btw, i made one more commit to further improve the patch after i reviewed the patch end-to-end.",0,0.9898743629455566
336473772,3874,ijuma,2017-10-13T14:45:04Z,you should rebase against trunk to fix the `nosuchmethoderror` issue.,0,0.9943254590034485
336475395,3874,lindong28,2017-10-13T14:50:56Z,sure. i have rebased the patch onto trunk. is this `nosuchmethoderror` a known issue which has been recently fixed in the trunk?,0,0.9899978041648865
336476902,3874,ijuma,2017-10-13T14:56:09Z,yes [a link],0,0.9936238527297974
336480112,3874,lindong28,2017-10-13T15:06:53Z,thanks for the information. this explains the test failure and it is very likely that system test can succeed now that i rebased the patch onto trunk. could you re-run the system test for this patch? i have finished reviewing the patch myself and made all the improvements i can think of. can you see if this patch is good to go? thanks!!,1,0.9952021837234497
337089038,3874,lindong28,2017-10-17T01:19:22Z,thanks for all the review! i have addressed all comments and rebased the patch onto trunk. i will go over the patch again. do we need to re-run system test for this patch?,1,0.992100179195404
337093789,3874,junrao,2017-10-17T01:52:57Z,": thanks for the latest patch. lgtm. i kicked off another run of system tests. once the tests finish, i can merge the patch.",1,0.9826656579971313
337138220,3874,lindong28,2017-10-17T07:06:46Z,thanks much! i have reviewed the patch again and slightly improved the error message.,1,0.9889305233955383
337272700,3874,junrao,2017-10-17T15:36:14Z,: thanks a lot for working on the pr. merged to trunk.,1,0.9777013063430786
337406756,3874,lindong28,2017-10-17T23:23:12Z,this is great. thank you so much for all your time and review!!,1,0.9973377585411072
208608858,1215,becketqin,2016-04-11T23:14:18Z,it seems this conflicts with kafka-3510. i'll do the rebase.,0,0.9930700659751892
208640666,1215,becketqin,2016-04-12T00:34:13Z,cc . will you be able to help review this patch? thanks!,1,0.9822123050689697
214130026,1215,junrao,2016-04-25T04:45:24Z,"also, could you patch the dumplogsegment tool to support the time index?",0,0.9955015778541565
214479103,1215,ijuma,2016-04-25T18:55:55Z,", it would be nice to have some numbers for how long it takes to do search by timestamp before and after the change on a cluster with a reasonable amount of data.",0,0.96073979139328
214572966,1215,becketqin,2016-04-26T00:38:21Z,"is there a specific use case where the performance of searching by timestamp matters much? supposedly searching by timestamp should be a relatively rare operation. with logappendtime, the time will be similar to offset search. with createtime, the time cost largely depends on how the data look like. i.e. if we have to scan tons of logs, it is going to be slow, otherwise it is fast. one thing for sure is that it is going to be slower than before which did not do much except for looking at the last modification time of segment files.",0,0.9799071550369263
215312071,1215,ijuma,2016-04-28T05:13:32Z,"as a general rule, we should be very thorough with regards to performance testing whenever we introduce a new disk structure (like an index). i gave one example, but we should really also verify the impact on normal operations as well. even if we believe that the impact should be minimal, we cannot be sure until we have tested it.",0,0.9814745187759399
215997683,1215,becketqin,2016-04-30T21:56:11Z,"thanks for the review. i see, yes, i agree that we should understand the performance impact of the changes we make in general. i was just curious if you care about the search by timestamp specifically. i updated the patch to let the index point to the offsets of shallow messages instead of the next offsets. would you be able to take another look? i am not sure if we are still able to make it into 0.10.0 given 0.10.0 should have cut off on yesterday.",1,0.9553864002227783
223086486,1215,becketqin,2016-06-01T18:41:18Z,i addressed your previous comments. would you take another look?,0,0.9911053776741028
224696267,1215,junrao,2016-06-08T19:09:38Z,": thanks for updating the patch. as the patch is getting closer to be ready, could you run your branch on the system tests? also, could you do some performance testing like you did in kip-31/32 to see if there is any noticeable performance degradation?",1,0.9690921306610107
225242928,1215,becketqin,2016-06-10T17:20:46Z,thanks a lot for the review. i have updated the patch. the only pending thing is whether we want to allow the empty time index or not. i replied to your comment. please let me know if you think the benefit is good to have or not. i have sent email to the mailing list regarding the time based log rolling behavior change to see if people have any concern. i will run the performance test as we did for kip-31/32 and update the result.,1,0.9868773221969604
227025455,1215,becketqin,2016-06-19T22:54:48Z,thanks a lot for the review. i just updated the patch to address your comments. i will create another kip to discuss the new search for timestamp semantic.,1,0.9845693707466125
227599503,1215,junrao,2016-06-21T23:11:20Z,": thanks for the latest patch. i made another pass and left some comments. most of them are minor though. once you have addressed those, it would be good if you could run through our system tests and post some performance testing results on the jira. ideally, we probably want to load some sizable amount of data on at least 2k - 4k partitions in a broker and see if there is any performance degradation.",1,0.970784604549408
228078724,1215,becketqin,2016-06-23T15:02:22Z,thanks a lot for the patient and careful review. i am on vacation in china from jun 22 to jul 11. the network becomes a little flaky. could you help kick off a system test? i will see if i can go to our newly opened shanghai office and use the vpn there to run the performance test on our test cluster. thanks again.,1,0.9923263788223267
231480822,1215,junrao,2016-07-08T21:46:21Z,"also, in dumplogsegments, could we dump the timestamp in the .log file too?",0,0.9950193166732788
231481580,1215,junrao,2016-07-08T21:50:36Z,": i made another pass and left a few more comments. it would be good if you can run some performance tests to see if there is any degradation. for instance, with this pr, we will need to read the last entry of each time index during broker startup. not sure how much impact this will have on starting time. so, it will be good to test this out.",0,0.8442391753196716
232181730,1215,becketqin,2016-07-12T21:07:42Z,"thanks for the review and sorry for the late response. i just returned from china yesterday. i have updated the patch to address your comments. because we are in the middle of data center cutting over, it may take some time to setup the new dev test environment. i will do the performance test this week.",-1,0.9554991722106934
232449796,1215,junrao,2016-07-13T18:45:59Z,"also, i left a comment in the jira about making the output of dumplogsegments clearer. could you take a look?",0,0.9943304061889648
232879331,1215,becketqin,2016-07-15T07:07:47Z,"i just updated the patch to address the latest comments. one thing worth mentioning is that dumping the active time index may be slow because time index requires looking up the offset index. the offset index of the active segment is always the max index size. this will cause the dump log segment tool to create an offset index with a lot of empty index entries in the end so the binary search in this case does not quite work. also because the active segment is still growing, dumping it may have some race condition and result in false alarm. so i suppose it is fine to assume that people won't dump the active log segment.",0,0.9541313052177429
233226827,1215,junrao,2016-07-18T04:28:00Z,: thanks for the latest patch. it looks good to me. do you have any updates on performance tests and the system tests?,1,0.9923071265220642
233382135,1215,becketqin,2016-07-18T16:29:54Z,"thanks for the review. i have tested the produce and consume performance with the same script we used for kip-31/32. the producer and consumer performance are the same. our new dev clusters are ready for use now, but i am still waiting for the mirror maker instances, which should be setup by today. i will run the performance test on the real data and test the segment loading and recovery time with more partitions after that.",1,0.9694300293922424
233383485,1215,becketqin,2016-07-18T16:34:48Z,"for system test, i was trying to use the confluent jenkins server before. but it seems that it does not support building against an arbitrary branch anymore. i am setting up a hudson job to run the system test internally at this point. but that may take some time as i am not familiar with that.",0,0.9356231093406677
233386240,1215,ijuma,2016-07-18T16:44:46Z,", you can use [a link]",0,0.9937390089035034
233400013,1215,becketqin,2016-07-18T17:34:12Z,got it. thanks.,1,0.9930570125579834
234819292,1215,becketqin,2016-07-25T01:49:03Z,"sorry for the late response. i have run the performance test. on a broker with ~15500 log segment files. the log segments loading time is about 6 seconds without this patch. on another broker with 17700 log segments files, the log loading time was 35 seconds with this patch. the results varied a little from run to run, but it is 4x - 6x slower when we need to load the timestamp from the log segment. for a large kafka cluster, this might be an issue. one optimization may be load the log maximum timestamp lazily. i'll update the new patch after testing that. i did not see any throughput issue with this patch. so the impact is only at startup time.",-1,0.9850639700889587
235023979,1215,junrao,2016-07-25T17:32:07Z,: thanks for the results. so the 4x-6x overhead mostly comes from reading the last entry of the time index? you also did producer/consumer perf test on a large number of topic/partitions and saw no degradation in throughput?,1,0.9637874960899353
235178030,1215,becketqin,2016-07-26T06:50:43Z,"i made some further test. the previous 4x - 6x slow down may not be very accurate. i noticed a bug in the code that may cause unnecessary rebuild of the indexes if a time index is empty. after fixing the bug, on a broker with ~20000 log segment files, i got the following results: 1. log loading time: ~13 seconds without time index. 27-28 seconds with time index.: 2. i tested a patch that skips sanity check and loads the max timestamp lazily so we don't read the last entry at starting time. the time reduced by 1-2 seconds but not much. so it looks that the slow down was mainly caused by the system calls that create the memory mapped files. because we are creating 2x many memory mapped files, it takes about 2x of the time. i tested the throughput with part of the production data. the brokers handled fine. and the produce request total time and local time looks similar. i will try to push more traffic and see if that makes any difference.",0,0.9526224136352539
235442848,1215,junrao,2016-07-27T00:01:35Z,": thanks for the update. is memory-mapping the index really that expense? could you do some micro-benchmark to test this out? also, have you run the system tests?",1,0.9669370055198669
235443110,1215,becketqin,2016-07-27T00:03:12Z,"yes, i have run system test and it passed. sure, i can run the micro benchmark on mmap.",0,0.9577687382698059
235443205,1215,becketqin,2016-07-27T00:03:48Z,the link to system test run. [a link],0,0.9947516322135925
235774517,1215,becketqin,2016-07-28T01:49:29Z,"i did some investigation on the segment loading and here are the findings: 1. mmap() and resize() takes much time during reloading. 2. mmap() speed dramatically slows down after 20000 files. 3. resize() is also much slower when timeindex files are added. the summary can be found here: [a link] in the graph each bucket contains 250 mmap() or resize() call. the time is the summary of the time taken by all the calls in that bucket. for example, the first 250 mmap() calls during log loading goes to the first bucket, the next 250 mmap() calls goes to the second bucket, and so on. i am not sure what is the right solution here. maybe there are some os level tweaks to avoid the deterioration of the mmap() time and resize() time, but i haven't looked into that yet.",0,0.9395155310630798
236021027,1215,junrao,2016-07-28T20:50:27Z,": thanks for the results. the results in ""log loading time distributing"" shows that loading the log with timestamp index takes 15 times longer than without (399 secs vs 21 secs). is that correct? if the overhead is in mmap(), with timestamp index should be at most twice as slow. does this suggest there is sth in the timestamp index that is much more expensive than the offset index?",1,0.9715718626976013
236038932,1215,becketqin,2016-07-28T22:02:43Z,"sorry i did not explain it clearly. in the google sheet there are also two sheets showing that the time taken by mmap() is related to how many files has already been memory mapped. after we have mapped ~20000 files, the time taken for further mmap() calls increased dramatically. the reason we see a 15 times larger loading time is the following. currently i have about 22000 log segments in a broker. so that results in 22000 offset index files. it took 21 seconds to load. when we do mmap for those files, it is roughly at the tipping point where the mmap() becomes taking much longer. now we added the time index file so the number of mmap() call is doubled. even though the first 22000 mmap() calls took the same time as before, which is 21 seconds, the next 22000 mmap() calls took much longer. it does not matter whether the mmap() calls are for offset index file or time index file. because after adding the time index, presumably the first 22000 mmap() calls are half/half for each type of index files. i am not sure if this is os specific and only an issue for our environment or it is a general issue.",-1,0.9617859721183777
236065457,1215,junrao,2016-07-29T00:40:50Z,: what happens if you do an experiment with 44k segments and turn off the time index? does that also take 15 times longer? were the mmap results based on a micro benchmark or did you extract that from the segment loading test?,0,0.9903541803359985
236270191,1215,becketqin,2016-07-29T19:19:09Z,"i ran test with 8500 log segments with time index files. they are loaded in less than 3.5 seconds. and it took 1.5 second without time index files. not sure if that is equivalent to testing files with 44k, but apparently the time does not grow linearly with the number of log segments. the mmap results are extracted from the segment loading test. micro benchmark which repeatedly maps a single file does not reveal this scalability issue.",0,0.979872465133667
236301416,1215,junrao,2016-07-29T21:43:44Z,": thanks for the new results. it feels a bit weird that the turning point is at exactly 22k segments. also, in the ""time to resize"" sheet, the results seem to suggest that resizing at 10 batches with time index is 6 times more expensive than resizing at 10 batches w/o time index. this seems to be at the pointe where the number of loaded segments is still well below 22k? is it possible to do an experiment with 44k segments and turn off the time index? this will tell us more about whether the overhead comes from scalability related to mmap() or something else in the time index.",1,0.6566397547721863
236312009,1215,becketqin,2016-07-29T22:44:13Z,"i refined the profiling code a little, i can stably reproduce the mmap() time issue. but the resize time seems to be consistent now. i'll do an experiment on a broker with ~50k segments without time index and see what happens there.",0,0.9771077632904053
237590174,1215,junrao,2016-08-04T15:31:50Z,: any new updates on the experiment?,0,0.9873065948486328
239509491,1215,junrao,2016-08-12T17:31:27Z,": are you still working on this? there are other kips such as kip-58 that will be depending on this patch. so, we probably don't want this patch to sit for too long. we are pretty close to committing this patch.",0,0.9409867525100708
239598105,1215,becketqin,2016-08-13T02:56:55Z,"sorry, i just saw your ping... yes, i am still working on this but was distracted by something else before. i will work on this this weekend and get it done. i tried with 50k segments, and it took about 11 seconds to load all the segments without time index, and about 30 seconds with the time index. i ran a bunch of tests on different machines. it seems that the machine i originally ran the tests on has some disk issue. a stably reproduceable result was that loading the segments took 3-4x longer with the time index. and what interesting is that for some of the logs with small number of log segments (e,g 100) i saw the loading time is significantly longer than other logs with much more segments (e.g. 3000). this happened both w/ and w/o time index. i am trying to see what caused this. i'll update the findings this weekend.",-1,0.9773195385932922
239683336,1215,junrao,2016-08-14T16:41:56Z,": thanks for the update. is the loading time affected by # of log segments, but not by the size of log segments? it seems that loading the time index is still more expensive than the offset index? looking at the code, we could make a couple of simple improvements. 1. in the constructor of logsegment, we call timeindex.lastentry twice unnecessarily. /\* the maximum timestamp we see so far */ private var maxtimestampsofar = timeindex.lastentry.timestamp private var offsetofmaxtimestamp = timeindex.lastentry.offset 2. in timeindex.sanitycheck(): we also call lastentry twice unnecessarily. not sure how much difference they will make. does profiling reveal which part of loading time index is expensive? if it's in calculating maxtimestamp and offsetofmaxtimestamp, it may be possible to do the calculation lazily. this probably will make the code more complicated. so, probably only worth doing if there is significant performance gain.",1,0.9745358824729919
239713049,1215,becketqin,2016-08-15T01:34:37Z,"i have been running tests this afternoon. actually the previous results i got have included the changes you mentioned above. in addition, in the sanity check one difference for time index is that we also check that the last indexed timestamp is greater than the first indexed timestamp. the requires reading both the first and last entry. i also commented out that check. the broker that used to have > 40000 segments has only 12000 segments now due to the retention. so i was not able to run the tests on the 40000 partitions. i started to populate data to the cluster again. hopefully i will have >40000 segments by tomorrow. one promising experiment i did this afternoon was changing the number of log recovery threads. this seems reduce the mmap time effectively. but since i do not have a 40000-segments broker this afternoon, it is hard to say if that is going to help in a large broker. i'll try this out tomorrow. back to the tests i ran earlier on a 40000 segments broker and this afternoon, the profiling i did was the following: 1. the time used to load each partition 2. the time used for mmap function specifically to see if there is long mmap (>100 ms) 3. the time used in sanity check. the profiling shows time spent on (1) is not completely related to the # of segments in the partition. for example, a partition with 1 segment may takes 35 ms to load while another partition with 12 segments only takes 4 ms to load. even for the two partitions of the same topic that have the exact same number of segments, the time can differ by 10x from 5 ms to 50 ms. **this happens both with and without time index.** when i look at the partitions that takes longer to load (e.g. 2 seconds w/o time index and 6 seconds w/ time index), those segments tend to be loaded later in the segment loading process. as for time spent on (2), i did not see any long mmap when the number of segments is less than 12000, whether with or without time index. but after the # of segments is greater than 20000, some long mmap() are witnessed. again, those long mmap call only occur on the log segments that are loaded later in the segment loading process. the loading time was pretty linear when number of segments is 12000. it took about 2 seconds to load the segments without time index, and took about 4 seconds with time index. regarding the time spent on (3), when we have to read both the first timestamp and last timestamp in the time index, the sanity check time significantly grew, but still short compared with the time spent on loading the files. based on the test i have run, it seems clear that 1. the biggest part of segment loading time is mmap(), no matter w/ or w/o time index. 2. it does not seem that loading time index is more expensive than loading offset index.",0,0.5507188439369202
239843860,1215,junrao,2016-08-15T15:59:22Z,": thanks for the updates. 1. what file system were you using in your testing? 2. earlier you mentioned that loading 50k segments w/o time index took 11 seconds. how does that compare with say loading 25k segments with time index? if loading time index is no more expensive than loading the offset index, you would expect the latter to take no longer than the former.",1,0.961982011795044
239872879,1215,becketqin,2016-08-15T17:45:07Z,"the file system type is xfs. currently i have ~19000 segments on the broker. using 10 log recovery threads, it takes about 3 seconds to load without time index and about 10 seconds with time index. if i reduce the log recovery threads number to 5, the loading time becomes 6 seconds with time index. so it looks that the number of log recovery threads was causing the slow log segments loading. i will test this on a 25k segments cluster with time index as well after the broker reaches there.",0,0.986409068107605
239929404,1215,junrao,2016-08-15T21:07:01Z,": thanks for the info. so using more log recovery threads slows down the loading with time index? also, about mmap() becoming more expensive with more log segments. mentioned to me that metadata lookups in file systems get more expensive with more open file handles since the lookups are based on an implementation of red-black tree. that's mostly cpu overhead. in your tests, do you notice any i/o activity when mmap() becomes more expensive?",1,0.9664903879165649
240002314,1215,becketqin,2016-08-16T05:07:29Z,"i did not check i/o activities when mmap() becomes more expensive, it is kind of hard to check because the segments loading only takes a few seconds. i tested log loading on 25k segments with time index using 5 loading threads. it takes about 13 seconds, which is comparable to the 11 seconds on a 50k segments cluster without time index. i will try to test it on a 40k cluster tomorrow. if the test results look good, i will update the patch with a few bug fixes and rebase on trunk as well.",0,0.7823153734207153
240139087,1215,junrao,2016-08-16T15:29:29Z,": thanks for the update. in your new patch, could you include a summary of the performance impact (and perhaps the ideal number of log recovery threads) in the upgrade doc?",1,0.9612851738929749
240276231,1215,junrao,2016-08-17T00:01:54Z,: i am trying to hold off [a link] and [a link] to avoid the rebasing overhead of this patch. do you think you could provide your latest patch in the next day or two?,0,0.9819313287734985
240319219,1215,becketqin,2016-08-17T05:48:16Z,"i have all the code ready now. i am still trying to find the reasonable number of recover threads. it seems there is system cache effect, i.e. if i do a start-shutdown-start sequence, the second start would be much faster than the first one because some of the disk read may come from the system cache. to avoid this effect, i need to wait for a while between each broker startup. i will upload the patch latest by thursday. is that ok?",0,0.7846601605415344
240436736,1215,junrao,2016-08-17T14:51:07Z,": thursday is fine. not sure if this helps, but you can empty dentries/inodes through a command ([a link]",0,0.9065018892288208
240630032,1215,becketqin,2016-08-18T05:50:28Z,"thanks for the dentries/inode cleaning command. i just saw them and haven't tried that yet. currently i have a script reboot a broker every hour and it seems stable enough to give some results. some updates on the number of log recovery thread tests. for a cluster with about 40k segments, with one log recovery thread, it takes 14 - 17 seconds to load the logs, and about 20 seconds with 2 threads, > 30 seconds with 5 threads. so it seems that having a single thread loading segments is the best setting. i have updated the patch after rebasing on trunk. i also squashed some previous commits.",1,0.9514629244804382
240802214,1215,junrao,2016-08-18T17:50:47Z,": thanks for the update. are your latest numbers with time index? what about the numbers w/o time index? also, how many log dirs are there since the number of recovery thread is per dir?",1,0.9722766876220703
240815948,1215,becketqin,2016-08-18T18:38:20Z,the latest numbers are with time index. i see log loading time to be 21 - 23 seconds on a 46k segments with time index. it takes about 10 seconds to load the segments without time index. so the loading time with time index is about 2x of without time index. this is also comparable to the 11 seconds loading time without time index on a 50k segments broker. i have ~8500 directories (partitions) on the broker. some of the partitions are pretty big (> 2500 segments).,0,0.9751175045967102
240838240,1215,junrao,2016-08-18T19:58:31Z,": thanks. could you confirm the number of log dirs you have and the number of disks per dir? intuitively, if there are multiple disks per dir, it seems using more than one 1 recovery thread should lead to better performance.",1,0.9583418965339661
240859249,1215,becketqin,2016-08-18T21:15:27Z,"we only have one log directory on a raid 10 containing 10 disk. so in our case, multiple thread might not help. in a jbod environment, i agree that multiple threads would probably help.",0,0.9855234622955322
240870837,1215,junrao,2016-08-18T22:01:14Z,": interesting, in theory, using more than 1 thread should still help since those threads can drive the i/os on different disks in parallel.",0,0.8892533779144287
240883073,1215,becketqin,2016-08-18T23:04:45Z,"i am not sure, one thing i noticed is that when there is one thread, the log loading time of each partition is pretty linear to the number of log segments. but if multiple threads are used, the linearity goes away. for example, the following logs are from running the code using 10 log recovery threads without time index (`old_mark` indicates no time index, and the same random old_mark means they are from the same test run). for topic1, it only took ~400 ms to load about 2000 log segments, at about the same time (300 ms earlier) topic2 takes 22 ms to load ~70 log segments. however, later on the same run (3 seconds later), for some other partitions of topic2 that has about the same number of segments, it takes much longer (~400 ms) to load. this was the reason that i suspected that the later mmap() calls are more expensive, which seems also proven from profiling by measuring the time on mmap calls in different buckets. i am not sure why this happens, but i didn't see such issue when there is one log loading thread. [code block]",0,0.8871511220932007
240895715,1215,junrao,2016-08-19T00:22:28Z,": thanks for the latest patch. i made another pass and only had a few minor comments. once those are addressed, i can merge in the patch.",1,0.9799196720123291
240931044,1215,becketqin,2016-08-19T05:25:29Z,thanks a lot for all the patient reviews. i updated the patch to address you latest comments and added a few unit tests. thanks again.,1,0.9922089576721191
241075286,1215,junrao,2016-08-19T17:05:05Z,thanks for the patch. lgtm,1,0.9300410747528076
241077444,1215,junrao,2016-08-19T17:13:31Z,: thanks a lot for working on this diligently. a couple of followups. 1. i left a couple of minor comments after merging. could you address them in a followup jira? 2. do you plan to work on a followup kip to add a new seektotimestamp() api to the consumer and a new request for getting offsets based on timestamp?,1,0.9907622933387756
241471040,1215,becketqin,2016-08-22T16:33:50Z,"thanks a lot for the patient review. i will submit a follow up pr soon. and yes, i will start working on the kip to add seektotimestamp() api now and post the wiki this week.",1,0.9847107529640198
241557337,1215,ijuma,2016-08-22T21:31:19Z,", it would be good to update the kafka documentation to mention the time index. maybe an easy way is to search it for offset index and see if it makes sense to mention the time index in those cases as well. would you be willing to file a jira and take this on as well?",0,0.9857960939407349
418101464,5527,Kaiserchen,2018-09-03T12:30:21Z,"we should just remove all the `final` keywords, i don't think they add any benefit?",0,0.983805239200592
418140255,5527,bellemare,2018-09-03T15:04:25Z,"i had to add all the final keywords to pass the linting check - iirc, my first run had dozens of linting errors preventing compilation.",0,0.9756084084510803
470747449,5527,mjsax,2019-03-07T23:47:51Z,what is your jira id? would like to assign the ticket to you.,0,0.9949643611907959
470940088,5527,bellemare,2019-03-08T14:07:49Z,jira id is abellemare,0,0.9774512648582458
481338093,5527,sachabest,2019-04-09T17:05:52Z,"hi, sorry to intrude on a potentially stale pr, but is this functionality still in development? would be extraordinarily useful for joining two changelog-like entities.",-1,0.9823617339134216
481350147,5527,pgwhalen,2019-04-09T17:23:26Z,"i sure hope so, my team is looking forward to it as well! given that the kip was accepted a few weeks ago, i think it's safe to say it will make it in fairly soon. i would definitely pick up development if can't continue.",1,0.9963841438293457
481367621,5527,bellemare,2019-04-09T18:06:47Z,hey folks - i'm still trying to get the code put together and finalize some of the changes that were outlined in the kip. stay tuned!,1,0.9377217888832092
482606869,5527,bellemare,2019-04-12T14:59:37Z,"hi all - i'm at a point where i need some feedback on a couple of things: 1) the organization of the code 2) the organization of the graph nodes in ktableimpl 3) insights into why i am getting nullpointerexception in ktablektableforeignkeyinnerjoinmultiintegrationtest (though not consistently). i believe this is a misunderstanding on my part as to how partitions are co-partitioned, but there may be more to it that i am missing. basically, it seems that depending on how the tasks and partitions are assigned, we either get a `java.lang.nullpointerexception: task was unexpectedly missing for partition table1-1` or it we don't. **this must be resolved if we wish to have flexible partition counts for joining, ie: fk join a topic with 3 partitions and a topic with 7 partitions** 4) anything else. feedback is very much appreciated, as this is the first pr i've put up against kafka and i'm sure i've violated a number of things.",1,0.6899272799491882
483831611,5527,vvcephei,2019-04-16T20:31:09Z,"hi , thanks for your pr! i'll review this as soon as i get the chance, and pay particular attention to the points you called out. -john",1,0.9954627156257629
491508007,5527,bellemare,2019-05-11T12:43:54Z,"hi john - thanks for the feedback so far! i haven't had time to attend to this due to some recent personal matters, but i should be able to take a crack at it this upcoming week. i think that (in my mind) the discussion about the topic names has been resolved, so i don't think there are any impediments other than me getting this cleaned up and then rebased to trunk.",1,0.993263304233551
492267940,5527,bellemare,2019-05-14T14:43:41Z,"- completed all your feedback so far john. thanks so much. i also updated the wiki page to more clearly show the underlying implementation in the diagram. i renamed some of the processors and hopefully added more clarity with the comments and other work. currently, i do not know enough about the underlying mechanisms to get variable-partition counts working (ie: join a ktable with 7 partitions with that of 11 partitions). this is explained above in the april 12th post on ktablektableforeignkeyinnerjoinmultiintegrationtest. multi-partition support could be added in a later revision if we wish to get this in for 2.3. i will rebase this to trunk and commit that too shortly.",1,0.9929961562156677
492710161,5527,bellemare,2019-05-15T15:44:35Z,"rebasing to trunk has been considerably longer than i planned. dealing with the new timestamped data stores has been a bit of a nightmare. additionally, data which used to be present in the ktableimpl class is no longer available. in the ktableimpl constructor,storebuilder and isqueryable have been replaced by materialized.queryablestorename(), which means that i do not have the ability to attach my resolver to the original, ""this"" materialized instance in the case where a queryable name is not set. i will look at ways to resolve this, but i do not anticipate being done before 2.3. i have spent considerable time on it in the past day and it's looking like much more is required.",-1,0.8827419877052307
492729592,5527,vvcephei,2019-05-15T16:36:19Z,"hey , thanks for the update, and for the rebase work. yes, the new timestamped stores changed a *lot* of implementation classes. it's a bummer that it happened to get merged after you forked. i agree, it's unlikely that this be able to get merged by friday (the feature freeze for 2.3). but no worries, it just means that we'll have more time to review it, write lots of tests, system tests, work on docs and blogs, etc., before it does get released, which decreases the overall risk of such a big feature. once you finish up the rebase, i'll take another pass. i didn't make it all the way through last time, and wound up just commenting on the little things i noticed along the way. -john",1,0.9463163614273071
493129698,5527,bellemare,2019-05-16T16:02:58Z,"there's an issue with the changing of the ktableimpl api 2.3-trunk: [code block] 2.0-trunk (what i had rebased it to last) [code block] `isqueryable` and `queryablestorename` in 2.0 have been replaced by just `queryablestorename` in 2.3, with a null value intending to mean that it is not queryable. the problem is that the onetomany joiner needs to query the underlying statestore previously represented by `this.queryablestorename` during resolution of the hash code values. previously, queryablestorename was populated even if the underlying state store was anonymous (ie: not passed in as a materialized parameter). now, however, if the user does not materialize a state store, the anonymous one is not passed in as `this.queryablestorename`. i think that it was only my code which would be affected by this unfortunately. i think i will need to change it back to how it was in 2.0 unless someone has any other ideas about how to get the underlying queryablestorename when it is anonymous.",0,0.9847385883331299
495390035,5527,mjsax,2019-05-23T21:22:34Z,"with kip-258 being merged, i think this pr should switch to use `timestampedkeyvaluestores`, too. i try to do a full review soon, now that feature freeze deadline is over i have a little more head room :)",1,0.9850693345069885
496669680,5527,adaniline-traderev,2019-05-28T20:08:47Z,"hi, i was using this pr before successfully, but i get an exception now: [code block] this happens on tableprocessornode(""ktable-source-output0000000046""), which is a child of ktablektableforeignkeyjoinresulutionnode(""ktable-source-resolver0000000045"") i think what broke it is that this time the join is done after filter (to emulate a left join): [code block] - i see ""is this right?"" comment in the code (org.apache.kafka.streams.kstream.internals.ktableimpl#dojoinonforeignkey) - this.queryablestorename is null, but it is asserted not to be null later. [code block]",0,0.9872575402259827
496750446,5527,bellemare,2019-05-29T01:36:58Z,"-traderev this is a consequence of the changes made in [a link] by (of which i need his guidance on for how best to remedy this issue). the workaround is to manually materialize the ktables you are going to use this join function on. for instance, instead of: [code block] use: [code block] and then this should work (because you provided a queryablestorename): [code block] prior to this change, the queryablestorename was either the user-provided name materialized.as(""storename"") or the internal name, if no materialized name was provided. from the change notes itself: `ktableimpl's queryablestorename and isqueryable are merged into queryablestorename only, and if it is null it means not queryable. as long as it is not null, it should be queryable (i.e. internally generated names will not be used any more).` the consequence of this is that there is now no way (as far as i can tell) for the ktableimpl to access internally generated state stores, which effectively kills this ticket. previously i was able to access both user-materialized ktable state stores as well as internal ones. now i can only access user-materialized ones, which is a non-starter for getting this committed. the only way forward that i can see at the moment (due to my own limited understanding of the code) is to revert the ktableimpl changes that implemented. to do so would be non-trivial, and i would like his guidance to do so. until we get past this detail, this jira is blocked.",0,0.9724637269973755
497091665,5527,adaniline-traderev,2019-05-29T20:13:01Z,", thank you, with your workaround i was able to progress further. i have another problem now - fk join calls serializers for both tablea and tableb records with the same topic name, causing compatibility error from the schema registry. the first call is for tableb value, triggered from [a link] the second call is for tablea value, triggered from [a link] the topic name is ""applicationid-steam-ktable-joinother-ktable-repartition-0000000041-value"" is it right to pass topic name in the wrapper serde here? [a link]",1,0.9442560076713562
497104592,5527,bellemare,2019-05-29T20:52:23Z,"-traderev this is where it gets tricky. there was a discussion previously in this review (and spilling over into the confluent schema registry git repo) over whether considerations for the schema registry need to be taken into account or not. my intention was to have this resolved so that it would be favourable for the schema registry serdes (since i too use them) but i have not gotten far enough with testing to hit the error you encountered. i believe a quick and dirty workaround, if you don't care about the registering dummy schemas, is to change the following: [a link] i believe if you create a randomly generated integer during the creation of this class, you can add it to the end of the topic name in l89. it should remedy the issue of registration. if it does not... i suggest just mucking around with the context.topic() names in the various processors and serdes until it works. it's not great, but since i don't have this running in front of me it's the best i can offer right now (and is still definitely a hack). the problem with the confluent schema registry is that it registers even null-topics, so unless you pass in dummy topics or are absolutely sure your topology isn't registering two schemas on the same internal topic (which it shouldn't even be doing in the first place...), you're kinda screwed. the next step in ktables should be separating the consumed.with() serdes from the materialized.with() serdes, so that we can use the confluent ones only for consuming the topic but not at all within the internalized topology.",0,0.9218870401382446
497406670,5527,guozhangwang,2019-05-30T17:09:48Z,"i left a comment under your question above, but since i've not done a full pass over it i'm actually not sure if my proposal would work for you (briefly looked into `subscriptionresolverjoinprocessorsupplier` but still not clear the exact line where it would npe)",0,0.9822589159011841
497731250,5527,bellemare,2019-05-31T14:35:06Z,"-traderev i threw in a fix for the schema registry. the main issue is that the valueserializer here does not have access to the correct topic name. [a link] i have made a dummy topic name, since i do not think we can actually access the true ""tablea"" topic name without a bunch of hacky work. if anyone strongly objects to this they can come up with a better suggestion.",0,0.9238206148147583
497766138,5527,adaniline-traderev,2019-05-31T16:05:01Z,"thanks, i had to solve it myself by creating a wrapper serde that always passes the correct topic name",0,0.8163527846336365
497778195,5527,bellemare,2019-05-31T16:39:43Z,"-traderev can you elaborate? my impression here is that the serializers i have created in this ticket are always receiving the correct topic, since they're simply using the existing streams framework when they get forwarded. if i am doing something wrong in this pr, please point it out.",0,0.9647179841995239
497794730,5527,adaniline-traderev,2019-05-31T17:29:12Z,", sorry, i meant that i did this before your change as a workaround - have not had time to test your change yet. so, to clarify, with the latest changes i can: - remove materialized from tables if i don't need them (except for fk join method) - use avro serdes without a need for a fixed topic name wrapper i will do this changes and will let you know thank you",-1,0.9689699411392212
497802451,5527,bellemare,2019-05-31T17:52:16Z,"okay sounds good. i appreciate all the feedback you have given, so if you think i'm doing something incorrectly i definitely want to hear about it :)",1,0.9966455101966858
498871200,5527,bellemare,2019-06-04T22:48:09Z,"hi all, to those still paying attention to this pr. i have done some major work on this over the past couple of days. i suspect i am about 95% complete on it now, but again, i still need feedback. still need to incorporate kip-307, but currently blocked on waiting for [a link] to be approved. - added a large number of tests in line with ""statestore.range(...)"" tests (and found a few bugs while doing so...) - unit tests for the various serdes - prefixscan appears to be working with all necessary stores, including timestamped. - works with timetamped tables things to consider: 1) the murmur3 code copied from hive has a number of `fallthrough` warnings. we either need to suppress the warnings or alter the code. currently i have the warnings suppressed. 2) currently needs the same partition count for both `this` and `other` table, as i have not been able to dig into the task/partition assignment logic to get it working with variable partitions. this could be done at a later date. 3) folder structure - all in foreignkeyjoin - i suspect i should not do that.",1,0.934637725353241
499291988,5527,guozhangwang,2019-06-05T23:25:04Z,"thanks for the great summary about the progress, i think the rest of kip-307 should be done soon and pinging and to continue reviewing on this pr.",1,0.9941416382789612
499586049,5527,bellemare,2019-06-06T17:14:03Z,"thanks -traderev and i have been working on a specific bug (introduced by this pr, not existing) that he discovered around some incorrect casting between timestamped and nontimestamped stores. i hope to resolve that today or tomorrow, but it shouldn't affect 99% of the pr.",1,0.9584203958511353
499632932,5527,bellemare,2019-06-06T19:23:24Z,"okay i think i isolated the issue and submitted a fix. the tests that were failing should now be passing. in the meantime i'm running tests locally to validate. -traderev, let me know if this works for you, your feedback has been very helpful.",1,0.9897748231887817
499928589,5527,bellemare,2019-06-07T15:24:16Z,anyone know how to handle licensing for tests? [code block],0,0.9951536655426025
499935136,5527,guozhangwang,2019-06-07T15:40:58Z,"for any added files (both src and test) you'd need to add apache license as the javadoc at the top, you can take a look at any existing files. basically it looks like: [code block]",0,0.9945970773696899
499996967,5527,bellemare,2019-06-07T18:45:17Z,- of course! obvious in hindsight. thank you.,1,0.9941642880439758
500947033,5527,bellemare,2019-06-11T17:35:43Z,"alright, well there's not much more for me to do here without more feedback from reviewers and the completion of [a link]",0,0.9858654737472534
505013615,5527,bellemare,2019-06-24T13:38:23Z,thanks john! i'll address them all over the next few days.,1,0.9933184385299683
505137962,5527,joeslice,2019-06-24T19:02:23Z,"thanks for your hard work on this feature. it's going to be super valuable to our team!! in my integration of your branch i noticed something that seems like a bug, or at least warrants some further discussion: i have a topology with two ""input"" `ktables` (`a` and `b`) and one output (`c`). simply put: * this topology expects stream `a` to be messages like (`key`, `x1`), (`key`, `x2`), etc... * it expects stream `b` to be messages like (`x1`, `v1`), (`x2`, `v2`), etc. * on `c` it emits the key from `a` and the value from `b` (where the key on `b` is the value referenced in the message value from `a`). some behavior that works as expected: * when a message in stream `a` references a known (or later known) message key in stream `b`, the resulting stream emits the key in stream `a` and the value from stream `b`. success! * when a message (on either side) has its value updated, the join's new result is emitted. success! what seems worth discussing / incorrect: * if a message value from stream `a` is updated to reference a non-existent key on stream `b`, nothing is emitted. this means that ""old"" results are still considered correct in downstream consumers. for example if i send a message on topic `a`: (`k`, `1`) and a message on topic `b`: (`1`, `one`), it correctly emits on `c`: (`k`, `one`). if i then send an update on `b`: (`1`, `one`) then a message on `c` is emitted: (`k`, `one`). if i then send an update on `a`: (`k`, `2`), but no message with key `2` has ever been seen on `b`, nothing is emitted. on one hand this seems correct: the results of an inner join should never have a `null` value for either side of the join. on the other hand this seems incorrect: the resulting stream is not updated to reflect the latest value `null` value/tombstone for key `k`. i suggest that the output stream should contain tombstones to indicate that the key extracted from `keyextractor` no longer matches a value on the ""right"" side of the join. what do you think? /cc for what it's worth, the topology i am using for testing: [code block]",1,0.9964402318000793
505147263,5527,bellemare,2019-06-24T19:29:29Z,"thanks for the excellent discovery! i believe that this is indeed a bug, and i think it's because of the decisions made around propagating the `null` events on a foreign key change. i will take a look at this more closely, because what you have written about expecting a tombstone output makes sense to me. let me consider this.",1,0.9945816397666931
505517987,5527,bellemare,2019-06-25T16:20:59Z,"hi john i pushed a fix to the prefixscan, along with some tests illustrating how they work. things seem to be working correctly now, but any feedback on this specific set of changes is well welcomed. i have a solution in mind for the bug you discovered, i'll try to get to that in the next day or two.",1,0.9869565963745117
505859127,5527,bellemare,2019-06-26T12:45:40Z,"hi i have updated the kip to reflect what i think can be done, and it is working in my dev branch. the main issue is that we do not maintain sufficient state to know that a delete was propagated out to downstream consumers on a previous change. a chain of changes with no available fk on the rhs would emit a (k, null) for each change. this is much more akin to a left join, but the big difference is that we wouldn't execute the join logic if inner is selected. anyways, check out the table i made in [a link] i'm going to continue working on the changes i have in mind, and more importantly, build out a number of test scenarios so i can get better coverage.",0,0.5903060436248779
505936671,5527,vvcephei,2019-06-26T15:55:20Z,"hey and , i started a new follow-up discussion thread in the mailing list to evaluate the proposed design modification. thanks, -john",1,0.9792653322219849
506104616,5527,bellemare,2019-06-27T01:39:50Z,"- i just pushed a commit that should fix your issue. it also adds left joins since we were 99% of the way there anyways. please let me know if it's meeting your expectations, because feedback like yours has been extremely valuable in working out the kinks, and i thank you for that.",1,0.9889848232269287
506493558,5527,joeslice,2019-06-27T20:13:52Z,"i'm pleased to report that this is working perfectly in my project's test cases. i have replaced a significant portion of my internals to prove this works in our test cases. so that's great! i did notice some `closing n open iterators` upon shutdown after processing ~1/2 million records, and can't confirm whether it's expected or even troubling. i thought i would pass it on in case we are ""leaking"" iterators in an unexpected way. thanks again for your hard work on this feature! [code block]",1,0.9964801669120789
506747200,5527,bellemare,2019-06-28T14:07:41Z,- a classic programming blunder - i definitely forgot to close the iterator after the prefix scan. i am adding that now.,-1,0.8309895992279053
506795729,5527,bellemare,2019-06-28T16:28:24Z,did a rebase to trunk and force pushed. looking to add named and incorporate more of john's feedback.,0,0.9926698207855225
506810499,5527,bellemare,2019-06-28T17:13:50Z,"one issue with using named... there are 5 nodes to name, 2 topics and 1 internal materialized store. i don't think it's appropriate to use 8 named elements, but i could see about using it as a common prefix for each of those.",0,0.9383765459060669
506820978,5527,bellemare,2019-06-28T17:47:03Z,"sorry... correction - 5 processing nodes, 2 sink nodes, 2 source nodes, 2 topics and 1 materialized internal state store.",-1,0.9922450184822083
507096788,5527,bellemare,2019-07-01T02:44:33Z,"- added a bunch of updates. and - perhaps you, along with john, can recommend where ""prefixscan"" should live in the web of keyvaluestore interfaces. all i really need it is: 1) support in rocksdb, caching and logging 2) must be able to access both `prefixscan` and `get` i tried previously to find a clean way to include it, but could not find a way without effectively adding it to the base keyvaluestore interface. i ended up settling on trying to get it to parity with `range` since they're very similar. thanks.",1,0.986346423625946
507786579,5527,bellemare,2019-07-02T18:08:22Z,"hi folks i'm going to try to keep this moving along, as i've just hit the 1-year birthday of working on this. we have it working internally and it would be good to get it in to 2.4, but i need more eyes on it to get it over the barrier, otherwise i fear it will languish in pr-purgatory forever. outstanding issues (as raised by john): 1) where in the interface definitions should prefixscankeyvaluestore.prefixscan() live? - not every type will have a prefix / won't make sense. - many state stores need the data definition (caching, logging, rocksdb, timestampedrocksdb) but many do not `need` it per se, for the purpose of this pr. => i don't have a good take on this. i want to avoid completely rehauling the keyvaluestore interfaces, but i also want to be able to use the caching and logging stores, as well as rocksdb to do the scan. everything else is optional to me. 2) the implicit dependencies between combinedkey, its serialization format, and prefixscan operation. prefixscan operates on the byte serialization of combinedkey, and the byte serialization is important for the cases where we just have a prefix (foreign/rhs update). => i don't see this one as much of a problem, as it seems obvious that byte-level prefix scanning should rely on the serialized format. while it could possibly be simplified, i don't have any immediate ideas. thanks again for any help... adam",-1,0.713373064994812
513779794,5527,bellemare,2019-07-22T12:56:52Z,", , - do any of you have any thoughts on point #1 and #2 above? currently everything works in the pr, but i suspect the api for #1 needs some more thought, as does the inter-dependencies of the serialization for #2. pinging you because it's been 20 days with inactivity.",0,0.8771730661392212
514062726,5527,tedchangtr,2019-07-23T05:45:02Z,i also wanted to say that along with and we are eagerly looking forward to this functionality as it solves many common uses cases we have.,1,0.9774187207221985
514693865,5527,vvcephei,2019-07-24T16:00:26Z,"hey , sorry (again) for the delay (again). i'm going to check out your branch and see what i can come up with in response to the issues. -john",-1,0.9920712113380432
516523887,5527,vvcephei,2019-07-30T17:50:58Z,"hey , like i mentioned before, i needed to check out the code and hack on it to address your questions... it was simpler to send my thoughts in pr form, so i created [a link] . it looks like we made some concurrent modifications, but the point was just to express the ideas anyway. wdyt? by the way, after getting my hands dirty, i have to give you mad props on this pr. you had to work though a bunch of the most complex inner workings of streams to pull this off. nice work! -john",1,0.9829795360565186
517427553,5527,bellemare,2019-08-01T19:36:07Z,"i put your changes into a single commit and merged them in. currently the build + tests seem to pass fine! thanks so much for your help, it is extremely appreciated. i think the major stumbling blocks i had were succinctly resolved by john's changes, so i believe the next step is to ensure that proper test coverage exists. i will take a look at that in short order, but just wanted to get his work in so that others can look at it. the bulk of it is summarized in his own descriptions here ( [a link] ), but boils down to: 1) removed the need for prefixscan 2) use a basic byte store with range scan to access elements (this is part of where we need to ensure it's fully working as expected) 3) centralize the specificity of prefixes and byte ordering into a single combinedkeyschema class. thanks again john!",1,0.9955427050590515
519149386,5527,bellemare,2019-08-07T15:31:23Z,"so there is (theoretically) a bit of a problem. currently our combinedkeyschema is represented as follows: `{integer.bytes foreignkeylength}{foreignkeyserialized}{optional-primarykeyserialized}` `store.range(start,end)` will not work when the prefix is the maximum value of a particular byte array. for example: [code block] namedcache currently still has a fragment of code to remove from the previous prefixscan implementation, but it illustrates the issue perfectly: ([a link] **the range iterator:** [code block] **the prefix iterator.** note that we use tailmap when the prefix increment would cause a wrap-around: [code block] options to fix this: 1) ignore it - we likely never hit integer.bytes for foreignkey length (that is, after all, 2,147,483,647 bytes for a signed integer... a bit bigger than we would expect to see in a kafka event). it is also worth pointing out that this would require the event coming through a repartition topic, and i don't think 2gb+ keys are reasonably likely to be used by anyone. 2) custom implementation of bytes specifying a custom comparator. i am leery about this one because technically the current byte comparison is correct, and we would be making it incorrect simply to work around the range wrap-around corner case. 3) add an extra byte to the start of combinedkeyschema just for the wrap-around corner-case: //{empty byte preventing wrap-around issues}{integer.bytes foreignkeylength}{foreignkeyserialized}{optional-primarykeyserialized} in this way, our previous example will now return the correct results, because end > start according to the default bytes comparator. [code block] my inclination is to somehow use option 1 while acknowledging that it could possibly be an issue. after all, the current key size limit is arbitrary as is (why not use long?). i suspect that adding the byte would be technically correct but simply a waste of space. i do not think anyone would ever use a 2gb key, but i think we still need to guard against it.",0,0.9821565747261047
519198629,5527,vvcephei,2019-08-07T17:40:34Z,"nice catch, , i also think option 1 is ok. actually, i _think_ this is not a problem. the key length is actually not arbitrary, since all keys must fit into byte arrays, and arrays in java cannot be larger than ""max int"", aka `0x7fff ffff` thus, we already have room to wrap around, with the first byte becoming `0x8f`. does that add up, or is it just wishful thinking on my part? -john",1,0.9917311072349548
519198864,5527,vvcephei,2019-08-07T17:41:18Z,"retest this, please.",0,0.788542628288269
519223034,5527,vvcephei,2019-08-07T18:47:46Z,the builds are failing because of some style violation. you can debug it with `./gradlew :streams:test-utils:checkstylemain`.,0,0.9769777059555054
519223220,5527,vvcephei,2019-08-07T18:48:20Z,"oh, found it: [code block]",0,0.9414973855018616
519512086,5527,bellemare,2019-08-08T13:13:10Z,"ah, correct! 7fff ffff should indeed be the maximum. i have just been testing at the byte level and had erroneously thought that ffff ffff was the max. since that's the case, i don't think that there's any issue with the usage of range as we have it now.",0,0.6183099746704102
520025675,5527,vvcephei,2019-08-09T18:48:55Z,"thanks for double-checking, . maybe we can get some other reviews at this point? also, the test failures were unrelated. retest this, please.",1,0.9509329199790955
520162968,5527,bellemare,2019-08-10T16:38:31Z,"by retest, do you mean rerun the scala 2.13 tests? i can't figure out how to do that without submitting a dummy commit...",0,0.9832478761672974
520532972,5527,vvcephei,2019-08-12T18:06:34Z,"oh, sorry, . if you say that sentence in a comment, it triggers the build to re-run. in other words, that last sentence was directed at jenkins, no you ;) after that re-test, two of the builds are passing, so i think we don't need to run them again until after the next cycle of reviews. (just guessing there'll be more commits later). i just wanted to see some green checkmarks.",1,0.6464702486991882
520847928,5527,bellemare,2019-08-13T14:02:39Z,"thanks! i was a bit confused... i think it's the politeness of which the restesting was asked for that made me think it was directed at me... anyways, til!",1,0.9896017909049988
524442847,5527,vvcephei,2019-08-23T19:57:32Z,"hey , i was just looking over the interface in ktable. can you correct the javadoc so that the `return` field is filled out? also, one of the `named` params is missing a description, and two of the overloads have no javadoc at all. thanks! -john",1,0.9937698245048523
524500288,5527,vvcephei,2019-08-24T00:39:31Z,"hey again, , i was talking to about reviewing this pr, and he had a good suggestion... can you send a separate pr just adding the murmur3 hash class and test (and guava dependency)? this would let us discuss the new depenency in isolation, as well as consider adopting the new hash algorithm elsewhere sooner. and as soon as it gets merged, this pr will automatically get almost 1,000 lines of code shorter. wdyt? -john",1,0.6367982625961304
525309148,5527,bellemare,2019-08-27T13:47:05Z,"okay, will do.",0,0.9375463724136353
525315925,5527,bellemare,2019-08-27T14:03:15Z,i assume you mean i should make a new pr and then rebase this one off of the new one? i think i may have to butcher this one up a bit since i don't think i committed the murmur3 hash code neatly in one commit.,0,0.9774430990219116
526226069,5527,bellemare,2019-08-29T15:01:16Z,- the murmur3 pr - [a link],0,0.9914286732673645
530440677,5527,bbejeck,2019-09-11T15:44:24Z,"one additional thought came to mind. we should include a test case where we have the `streamsconfig` set to `optimize` (and use overloaded build method `streambuilder.build(props)` ) to make sure everything still works as designed. i think it should, but it will good to confirm.",0,0.9001902937889099
530527737,5527,bellemare,2019-09-11T19:22:27Z,- would it be sufficient to put it in one of the integration tests? it seems to all work when i include the following: [code block],0,0.9936291575431824
530592010,5527,bbejeck,2019-09-11T22:33:44Z,"yeah, that sounds fine. maybe the `ktablektableforeignkeyjoinintegrationtest` since it has 8 tests in it?",0,0.9513373970985413
532466041,5527,vvcephei,2019-09-18T00:28:24Z,"the job output was lost to the sands of time. retest this, please.",-1,0.8184787034988403
533170339,5527,bellemare,2019-09-19T14:56:49Z,"okay, i added code to the streamsresetter, but i can't find a good place to test for the correct deletion of the topics during a full app reset. streamsresettertest doesn't seem to provide for that. any suggestions?",0,0.9644367098808289
533212559,5527,bbejeck,2019-09-19T16:38:00Z,"off the top of my head maybe add the `streamsresetter` to one of the integration tests? i'm thinking something like 1. process some records, confirm the output 2. reset the app confirm topics deleted 3. re-run the same records and confirm all processed again wdyt? \cc",0,0.98729008436203
533217402,5527,vvcephei,2019-09-19T16:51:03Z,"i think it's worth adding the test right now, though, given our recent experience with repeated the multiple operational regressions after implementing suppress. see the `org.apache.kafka.streams.integration.abstractresetintegrationtest` for testing resets.",0,0.9836380481719971
533355567,5527,vvcephei,2019-09-20T00:21:31Z,", now that #7271 has been merged, you're going to have to rebase (since the location of murmur3 is different now). we should see all the murmur3 code and tests disappear from this diff.",0,0.9879816174507141
533356431,5527,bbejeck,2019-09-20T00:26:13Z,java 11 builds timed out. java 8 failed with all tests from `org.apache.kafka.streams.integration.ktablektableforeignkeyinnerjoinmultiintegrationtest` and `org.apache.kafka.streams.integration.ktablektableforeignkeyjoinintegrationtest` but running locally all test pass for me. can you rebase this pr? nm just saw the comment from,0,0.9899391531944275
536039458,5527,thebearmayor,2019-09-27T17:58:48Z,"hi, what is the expected output with a leftjoin when an event is published to the lhs with null foreign key (assume no other events have been published). i am observing no output, which was unexpected for a leftjoin. my apologies if this is described in the kip, i couldn't find this particular situation called out.",-1,0.9397686719894409
536145694,5527,vvcephei,2019-09-28T02:38:54Z,"hey , you mean the key itself is `null`? normally, this would be considered invalid data (in any join), and streams would just drop the record with no output (which you've observed). although, there should be a warning log, if memory serves. i don't think this was discussed in the kip discussion, so we might need to amend the kip to handle this case. as i understand it, in many relational dbs, foreign key references are allowed to be null because the reference itself is understood to be an implication. the presence of a foreign key reference implies that the key is a primary key on the foreign table, but a null reference is the _absence_ of a foreign key, so it doesn't imply anything. in this situation, the db implementation is free to define the result however it likes. so, i guess that we can go for consistency and say that null foreign key references work the same way as null equi-join (primary) keys and just drop the record with a warning. on the other hand, the situation is slightly different, and if it's useful to emit a `(lhs, null)` result, we could try to make it work. however, the implementation would be quite difficult, maybe impossible. the reason for this is that to do the join, we need to send a ""subscription"" to the rhs primary key (ie the foreign key). when the foreign key is `null`, there is no where we can send the subscription, and we can't even pick a sentinel value, since we don't control the type of the foreign key. some quick details: the (caller-provided) serdes would have to handle null keys, which they normally do not, probably resulting in npes, and they (maybe caller-provided) topic partitioner would have to handle null keys also, which it probably does not. in light of this reflection, i guess i'm leaning toward just documenting the behavior you've observed. although it might be a bit of dissonance if you're used to working with rdbs that allow it, at the end of the day, it would be less headache for you overall not to have to cope with `null` in all these unexpected places. hopefully, you could accomplish the same objective by mapping the `null` reference to some non-null, but out-of-domain sentinel value that's guaranteed not to be the primary key of an actual rhs record. how does this all sound to you? thanks for bringing it up, -john",0,0.9811366200447083
536186044,5527,thebearmayor,2019-09-28T12:40:47Z,"thanks, john. specifically, i meant the result of foreignkeyextractor is `null`. in a normal case that could be because the foreign key reference is missing or the value is null, or maybe you're doing something weird in the extractor which is giving a null. i think a sentinel value will work fine. my goal is something like ""enhance the left side with data from the right side"", but the left side is the important part. so i don't want to drop records that have invalid or missing references. thanks for your help! i do think it would be good to document this, because it surprised me in comparison to a left equi join.",1,0.9922620058059692
537581324,5527,bellemare,2019-10-02T16:47:47Z,"yep, it looks like the issue is as you described. the thing is that it's hard to do, as outlined by john. i think you would have to use your own sentinel value to get an adequate output, and i think i will have to document this into the code.",0,0.9746538400650024
537582199,5527,bellemare,2019-10-02T16:49:48Z,"i'm going to merge all of the current commits into a single patch and apply it before rebasing to trunk. there are so many commits right now that rebasing takes a significantly long time and is error prone - so i will force push a new, single commit, rebase to trunk, fix up the murmur3 hash and add the comments regarding left-hand joins with null foreign keys. if anyone objects or has a better idea as to how to handle this, let me know, as this is my first big multi-month(/year) commit where rebasing is painful.",-1,0.9757999777793884
537599188,5527,vvcephei,2019-10-02T17:31:55Z,"hey , i started working on this just now, since we didn't hear back from you for a while. i wound up just merging trunk into your branch again, it wasn't too bad. i'd recommend not rebasing or squashing. since there's a merge commit in the history of this branch, it's just going to make you sad. once alternative (which i'd also not recommend) is checking out a clean branch from trunk and using diff/patch to just apply the diff between this branch and trunk. this is risky, as you could easily lose some important recent change in trunk, and it would be hard to notice or track down later. on the other hand, if you just merge trunk into this branch, you can resolve the merge conflicts (there are only two), and then the github (squash+merge) button should take care of the rest.",-1,0.8913500905036926
537601076,5527,bellemare,2019-10-02T17:36:46Z,"- ah thanks! sorry, i was away on vacation for ~9 days and had some other things come up - did you want to carry it across the line or did you want me to keep at it?",-1,0.9897291660308838
537605591,5527,bellemare,2019-10-02T17:48:11Z,"ah i updated it all anyways. let me know if that comment is sufficient, or if there needs to be more clarity around it.",0,0.9806560277938843
537606172,5527,vvcephei,2019-10-02T17:49:36Z,sounds good! can you also add this to the `build.gradle` (in the `:streams` project): [code block] i noticed it when i was running the tests locally,1,0.987558901309967
537608972,5527,bellemare,2019-10-02T17:56:40Z,how's that?,0,0.9798794388771057
537610839,5527,vvcephei,2019-10-02T18:01:20Z,"since jenkins has been a bit twitchy recently, i'll just make a note that `./gradlew clean :streams:test` has just passed for me.",0,0.971504271030426
537694233,5527,vvcephei,2019-10-02T21:43:16Z,"hmm. i just checked the jenkins queues, and i don't see this pr in there. retest this, please.",0,0.9014365673065186
537732545,5527,mjsax,2019-10-03T00:15:35Z,checkstyle error: [code block],0,0.9889177083969116
537754256,5527,vvcephei,2019-10-03T02:04:15Z,"huh. the streamspartitionassignor isn't part of this pr, and that import isn't there in this pr's branch. maybe someone merged a broken commit to trunk, and then the jenkins pr builder sucked it in here? retest this, please.",0,0.9052226543426514
537754397,5527,vvcephei,2019-10-03T02:04:56Z,"oh, yep, it was trunk: [code block]",0,0.9852561950683594
537974203,5527,bellemare,2019-10-03T14:37:49Z,"retest this, please.",0,0.788542628288269
537983099,5527,bellemare,2019-10-03T14:58:00Z,"the prior builds had all failed on one test or another. one had failed on 5 integration tests, the other on 1 other integration test, and i forget about the third. it looks like it is flakiness and unrelated to the code changes, but best to run it again.",0,0.9673274159431458
537995486,5527,vvcephei,2019-10-03T15:27:10Z,"thanks, , yeah, it would be nice to see jenkins give us at least 1 green build.",1,0.9323140978813171
538084227,5527,vvcephei,2019-10-03T19:10:47Z,"the java 8 build had just one failure, which i think is ok: [code block] likewise with the java 11+scala 2.13 build: [code block] and the java 11+scala 2.12 build: [code block]",0,0.8492560982704163
538085084,5527,bellemare,2019-10-03T19:13:17Z,"(jdk 11 & scala 2.13) and (jdk 8, scala 2.11) both failed on the same test, though it appears totally unrelated: [code block]",0,0.9897199273109436
538086489,5527,vvcephei,2019-10-03T19:17:25Z,"hmm, it's almost like we're all just sitting around watching these builds... i agree, i don't think that failure is related, and i also don't think it's worthwhile to keep running the tests to see if that broken test passes next time.",-1,0.9411638975143433
538108501,5527,vvcephei,2019-10-03T20:10:41Z,"note, the failing test is known to be broken: [a link]",0,0.9940688610076904
538160544,5527,bbejeck,2019-10-03T22:57:13Z,"test failures unrelated and local build passes, so merging this.",0,0.9788020849227905
538161279,5527,bbejeck,2019-10-03T23:00:14Z,merged #5527 into trunk.,0,0.9928476810455322
538161381,5527,bbejeck,2019-10-03T23:00:36Z,thanks for the hard work and perseverance to get this done!,1,0.9875538349151611
538182923,5527,vvcephei,2019-10-04T00:43:45Z,"yeah, congratulations, for this awesome contribution!",1,0.9964874982833862
538184423,5527,pgwhalen,2019-10-04T00:51:56Z,kudos! ive been following this feature for a year and am extremely excited to see it make it in.,1,0.9967214465141296
538189810,5527,guozhangwang,2019-10-04T01:18:23Z,"congratulations , it's been a long kip and journey :)",1,0.9956284761428833
538226162,5527,mjsax,2019-10-04T04:26:02Z,congratulations ! and thanks for all the hard work! also thanks to for his initial proposal and the many hours you invested in this kip! check out [a link],1,0.9962596893310547
538396002,5527,bellemare,2019-10-04T13:26:53Z,"thanks for all the help everyone, especially from . i couldn't have done it without you all. and thanks to jan too for getting it started so long ago.",1,0.9927465319633484
538471438,5527,mjsax,2019-10-04T16:35:51Z,"do we need to update the docs for the new feature? we should at least mention in the upgrade guide. would you like to update the wiki, too: [a link]",0,0.9953097701072693
538481205,5527,guozhangwang,2019-10-04T17:04:56Z,"besides the upgrade guide, i think we can also update the developer-guide on dsl section: [a link]",0,0.9938622713088989
539648633,5527,bellemare,2019-10-08T18:43:37Z,i'll take a look at updating them. what/where is the upgrade guide?,0,0.9929236173629761
539843057,5527,mjsax,2019-10-09T05:46:24Z,"i would add maybe one bullet point to [a link] (as ""notable changes"") and also list it in [a link] not sure if both files have already a section for `2.4` release -- if not, just add one :)",1,0.8002471923828125
541166957,5527,thebearmayor,2019-10-11T18:07:20Z,"do you know of any cases that would lead to duplicate updates being emitted by these joins? we first noticed this because we saw ""detected out-of-order ktable update"" messages for topics emitted from some of these left joins. looking at the updates, they were identical except for the timestamps. but it happens intermittently -- and not consistently even with the same input data. we switched to inner joins, and still saw them. sometimes they updates had the same timestamp, but when they had different timestamps, one would have the timestamp from the corresponding left-table update, and the other would have the timestamp from the right-table update. so far i don't think this is causing bad data or any real problems. just wanted to check if you had seen it before i keep digging. thanks!",0,0.9796828627586365
541769834,5527,guozhangwang,2019-10-14T16:08:30Z,i think this is fine: as we've discussed in kip duplicates may happen for the same join-results but they will happen intermittently as you've observed. since it is for a ktable changelog streams overwriting multiple times would not cause correctness issues. and could chime in with more insights.,0,0.5364359617233276
541770417,5527,guozhangwang,2019-10-14T16:09:34Z,"that also makes me thinking, should we make [code block] an info level entry than warn to not cause unnecessary panic since it would not cause correctness issues anyways?",0,0.9684370160102844
541961886,5527,mjsax,2019-10-14T22:39:38Z,"interesting though -- mid to long term, i think we need to allow better handling of out-of-order data for `ktables` anyway. the main purpose to the log message was for the `builder.table()` operator -- it required data be ordered because it applied updates in offset-order, not timestamp order. hence, upstream the single-writer principle should be applied -- the warning makes sense for that case imho as it indicated a potential upstream problem. maybe we should make the warning more flexible and only turn it on for `builder.table()` operator but disable it if we use the same processor elsewhere?",0,0.7941574454307556
541973602,5527,thebearmayor,2019-10-14T23:33:02Z,"hi, , just to be clear i am getting the warning from a `builder.table()` operator, which is reading from a topic which is written to by one of these joins. the join operator itself does not issue this warning.",0,0.9707072973251343
542099166,5527,mjsax,2019-10-15T08:25:49Z,"thanks for the clarification. maybe we introduce more out-of-order records due to the round-trip via two repartition topics than we anticipated... but i am not 100% sure why -- each update to the left-hand side would send one message to the right hand side and should receive zero or one respond messages. an update to the right hand side could send multiple message to the left hand side, however maximum one per key. -- if we compute the join result timestamp as maximum of both input timestamps, i don't see atm why we would introduce much out-of-order data. \cc thoughts?",1,0.904107928276062
542302479,5527,bellemare,2019-10-15T16:41:13Z,"do you have any other information on how common it is, or steps to reproduce? i don't have any ideas off the top of my head, but i will take a look at the code again with this in mind...",0,0.9815250635147095
542481324,5527,thebearmayor,2019-10-16T02:22:26Z,"i'll tell you everything i can think of, most of which won't be relevant. i didn't mean to take up more of your time. i'll be offline until next week, and i mean to dig into it more then. i don't have any way yet to reproduce this in development. i've only seen it running in ec2 with 3 instances against large topics, with 16 partitions. i assume there is some timing component to the issue. - we're not using the absolute latest version of this code. i think we built your branch when it was ""feature complete"" sometime around mid-august. - we're joining two topics with hundreds of millions of messages each. they're fairly old and compacted, so i don't think there are any records with duplicate keys. - the join itself is simple -- we just keep the right-side value and discard the left-side. - it's not very common -- i would say duplicate records occur a few thousand times in a few million input records. of those, far fewer have out-of-order timestamps.",-1,0.5646754503250122
629872893,8680,kowshik,2020-05-17T22:49:00Z,"hi and , this pr is ready for code review. please have a look and do let me know your thoughts. cc and thank you!",1,0.9917691946029663
641549841,8680,junrao,2020-06-09T20:25:00Z,ok to test,0,0.992953360080719
641561083,8680,junrao,2020-06-09T20:48:13Z,it seems there are some checkstyle failures. [code block],0,0.9852927327156067
641765964,8680,kowshik,2020-06-10T06:47:29Z,thanks for the review! i have fixed the checkstyle issue now in 74ff66f.,1,0.9908533692359924
641766088,8680,kowshik,2020-06-10T06:47:40Z,ok to test,0,0.992953360080719
642155141,8680,hachikuji,2020-06-10T17:34:20Z,ok to test,0,0.992953360080719
642180658,8680,junrao,2020-06-10T18:22:59Z,test this please,0,0.9870431423187256
642751664,8680,junrao,2020-06-11T15:42:55Z,test this please,0,0.9870431423187256
642855293,8680,junrao,2020-06-11T18:26:49Z,the test failures seem unrelated. merging this to trunk.,0,0.9857468605041504
1728441122,14406,kirktrue,2023-09-20T21:19:06Z,"can you add the `ctr` label, please :pleading_face:",0,0.9824740290641785
1773483995,14406,kirktrue,2023-10-20T22:53:03Z,fyi: i rebased against `trunk` to remove the conflicts.,0,0.9943593144416809
1776108652,14406,kirktrue,2023-10-23T22:16:56Z,"no, i do not believe they are. there are 8 ""new"" failures, of which two are the same test with different parameters. here are the tests and any jiras for flakiness: * `kafka.server.describeclusterrequesttest.testdescribeclusterrequestexcludingclusterauthorizedoperations`: kafka-15419 * `kafka.server.dynamicbrokerreconfigurationtest.testlogcleanerconfig`: kafka-7966, though the jira is resolved :man_shrugging: * `o.a.k.common.network.sslversionstransportlayertest.testtlsdefaults`: kafka-9714, marked as ""critical"" but open for 3 1/2 years * `o.a.k.connect.integration.connectorrestartapiintegrationtest.testmultiworkerrestartonlyconnector`: kafka-15675, just filed (by me) with some pointers to a recent flakiness rate of 9% * `o.a.k.streams.integration.standbytaskeosmultirebalanceintegrationtest.shouldhonoreoswhenusingcachingandstandbyreplicas`: no jira. only around 2% flaky * `o.a.k.tiered.storage.integration.transactionswithtieredstoretest.testsendoffsetswithgroupid`: kafka-8003, though the jira is filed against a different test, but test failures in `testsendoffsetswithgroupid` are mentioned as being related * `o.a.k.trogdor.coordinator.coordinatortest.testtaskrequestwitholdstartmsgetsupdated`: kafka-8115, which is another old, 4.5 year old ""critical"" issue there are 22 ""existing"" failures, which are all related to the `verifynounexpectedthreads` check. `kafka.server.dynamicbrokerreconfigurationtest` is reporting that there are threads when it attempts to tear down the test harness. the remaining 21 failures all report the same unexpected threads in their check during test harness setup. there are other recent, unrelated pull requests that have experienced similar issues. in the other cases, i haven't seen that it's the `dynamicbrokerreconfigurationtest` that is the cause, though.",0,0.8970609307289124
1776110492,14406,kirktrue,2023-10-23T22:18:20Z,closing and reopening to trigger another jenkins test run.,0,0.9910257458686829
1776377391,14406,kirktrue,2023-10-24T02:05:31Z,"ugh. one of the builds failed with: [code block] no integration test failures due to threads, though :smirking_face:",-1,0.5952684283256531
1776378053,14406,kirktrue,2023-10-24T02:05:48Z,closing and reopening to restart test.,0,0.9908498525619507
1777454027,14406,kirktrue,2023-10-24T15:13:43Z,closing and reopening to restart test. jenkins doesn't seem happy lately.,-1,0.7206256985664368
1777855712,14406,junrao,2023-10-24T19:04:42Z,: the build for jdk 21 and scala 2.13 failed this time. did it succeed before?,0,0.9887804985046387
1777892441,14406,kirktrue,2023-10-24T19:28:41Z,"yes. here's a brief history for jdk 21, starting with the most recent build (build 74): * [a link]: couldn't clone repo * [a link]: tests ran, but jenkins timed out * [a link]: tests ran * [a link]: tests ran * [a link]: tests ran, but jenkins shut down in the middle :thinking_face: these same intra-jenkins communication, `git` cloning, unexpected threads, and flaky tests affect all of the jdks at random times :crying_face:",-1,0.9592645168304443
956456320,11390,kowshik,2021-11-01T18:01:21Z,it'd be helpful if you could please update the pr description explaining the scope of the draft pr (in its current form) and what's remaining to be done.,0,0.976818323135376
971736892,11390,satishd,2021-11-17T16:18:40Z,this pr is ready for review. please take a look.,0,0.82227623462677
992617350,11390,satishd,2021-12-13T15:55:04Z,"thanks for the review. please find inline replies, addressed most of them with latest commits.",1,0.9278669953346252
998818025,11390,satishd,2021-12-21T14:20:51Z,": thanks for the review. please find inline replies, updated the pr addressing them with the latest commit.",1,0.9521385431289673
1015392509,11390,satishd,2022-01-18T13:04:39Z,"thanks for the review. please find inline replies, updated the pr addressing them with the latest commits.",1,0.9140903353691101
1019966726,11390,satishd,2022-01-24T10:53:11Z,: thanks for the review. addressed your comments with the latest commits. rebased with the trunk to resolve the conflicts.,1,0.9471571445465088
1330143560,11390,satishd,2022-11-29T06:21:51Z,"thanks for your latest comments. addressed them inline, let me know if you have any comments.",1,0.9504775404930115
1330147549,11390,satishd,2022-11-29T06:28:15Z,"thanks for your comments, addressed them with inline comments.",1,0.832221508026123
1330147705,11390,satishd,2022-11-29T06:28:27Z,"thanks for your comments. addressed most of them, working on couple of the remaining comments to update the javadoc.",1,0.9546918272972107
1340522323,11390,showuon,2022-12-07T07:38:07Z,", do you want to have another review? since branch 3.4 has created, and this pr blocks some following tiered storage feature development (ex: copying segments to tiered storage, retention checks to clean local and remote log segments), we might need to consider to merge it first and have follow-up prs for complicated changes. wdyt? thanks.",1,0.7732605934143066
1341457933,11390,junrao,2022-12-07T19:09:21Z,: i plan to take another look at the pr in the next few days.,0,0.9828199148178101
1348935454,11390,satishd,2022-12-13T16:14:51Z,"thanks for your review, addressed them with inline replies. i updated the pr with latest commits addressing the review comments.",1,0.9577106833457947
1348940728,11390,satishd,2022-12-13T16:15:58Z,: filed [a link] as you suggested.,0,0.9924654960632324
1354541220,11390,satishd,2022-12-16T10:45:06Z,thanks for your updated review. addressed them with inline comments and updated with the latest commits.,1,0.9414440393447876
1356256797,11390,satishd,2022-12-17T13:12:53Z,"thanks for the review, addressed it with the latest commit. there are a few tests that are failed but they do not seem to be related to this pr.",1,0.923693835735321
1356390240,11390,ijuma,2022-12-17T19:00:58Z,"hey , i wanted to let you know about kafka-14470 as i think it affects some of the future kip-405 prs. can we align these efforts so that we can get to the desired end state faster? for example, once the prs that have been submitted are merged, we can move `remoteindexcache` to the `storage` module too.",0,0.9672260880470276
471557966,6363,kkonstantine,2019-03-11T14:20:39Z,"the pr is ready for review. the following items are expected to be addressed along with your comments: * the code on task assignment is currently written to be readable and evaluate correctness. i'd like to add micro-benchmarks and i expect it'll be optimized. * unit tests for `distributedherder` and `workercoordinatorincrementaltest` will be expanded to guard against changes in the new protocol more extensively. * more logging will be added appropriately and some integration with metrics will be considered. * a few more integration tests will be added. some files contain changes that are introduced by other outstanding prs. if still present here, please skip or review the changes in their respective prs. really looking forward to your comments! thanks!",0,0.6922513842582703
484717843,6363,rayokota,2019-04-18T23:04:57Z,", thanks for responding to my feedback! i just had one remaining comment. looking great!",1,0.9967017769813538
493287766,6363,kkonstantine,2019-05-17T01:41:14Z,"thanks and for all the insightful and useful comments! i believe i've addressed everything, except a few cleanup/refactoring suggestions that deemed high risk at this point and will be addressed in a follow up pr after this feature is merged. soak testing has been also performed and has confirmed correct execution for several days. more extensive testing and performance benchmarking will follow up in the next few days. i'll be glad if we can get this in. thanks!",1,0.9961559176445007
554337624,6363,findnanda,2019-11-15T12:15:43Z,"hi can you please suggest in which kafka version is this issue fixed as i am still seeing this problem every time i add a new connector, all the connector gets restarted?",0,0.960752546787262
554407084,6363,rhauch,2019-11-15T15:35:16Z,", the jira issue ([a link] shows that this was merged and completed in ak 2.3.0. if you're using ak 2.3.0 or later and still having problems, please create a new jira issue and provide the connect worker configs and a lot more detail about a procedure to replicate the problem. thanks!",1,0.9736402630805969
417397352,5582,rondagostino,2018-08-30T17:15:54Z,"beginning to add tests, and i need to rebase to get `kafka-7324: npe due to lack of saslextensions in sasl/oauthbearer (#5552)`",0,0.9931446313858032
417676910,5582,rondagostino,2018-08-31T14:12:36Z,rebasing again to resolve conflicts with `kafka-6950: delay response to failed client authentication to prevent potential dos issues (kip-306)`,0,0.9924699664115906
423861956,5582,rondagostino,2018-09-24T01:13:41Z,i rebased and pushed a squashed commit that implements your approach except it triggers a re-auth upon a write as described by the kip rather than upon a read (which is what your prototype used). i think triggering upon a write is correct. i'll add metrics and tests soon. hopefully we will get the 3rd binding up-vote in time for the 2.1.0 deadline today. thanks again for that prototype -- i could not have gotten there myself in a reasonable amount of time because the low-level network code is pretty tough to pick up.,1,0.9785102605819702
424499373,5582,rondagostino,2018-09-25T20:58:44Z,"this pr now has everything except: - implementation and tests for client-side latency metrics - implementation and tests for server-side connection kill metrics - a test to see if server-side kill works i added re-authentication to the `nioechoserver` and i test for it with bearer tokens and delegation tokens within `saslauthenticatortest`. i added re-authentication every 50 ms for all end-to-end scala authorization integration tests as well. i will work on the remaining 3 items on wednesday. can you review what is here already, or would you prefer to wait? ron",0,0.8425325751304626
424644619,5582,rajinisivaram,2018-09-26T09:14:38Z,i will review the pr tonight or tomorrow. there is a failing test iin the pr builds (`org.apache.kafka.common.security.authenticator.saslauthenticatortest.testtokenreauthenticationoversaslscram[reauthenticate=true]`) - can you check if that is a test issue or a code issue? i will start a system test run once you get a chance to look at that.,0,0.989467203617096
424707930,5582,rondagostino,2018-09-26T13:06:39Z,it is a test issue. i cannot reproduce the error locally -- it passes on my laptop. i pushed a commit to sleep a bit longer in the hope that it will fix the issue during the build.,0,0.9746735692024231
424718026,5582,rajinisivaram,2018-09-26T13:37:30Z,"thanks, i have started a system test run on your branch: [a link] will review the pr later tonight.",1,0.8625325560569763
424802468,5582,rondagostino,2018-09-26T17:28:41Z,test still fails here but cannot reproduce the issue locally. pushed another commit to try to fix it...,0,0.9716787934303284
424950015,5582,rondagostino,2018-09-27T03:52:19Z,"pushed a commit that i think fixes/resolves most of the issues you raised. it's getting late here so i couldn't get to all of them, but i think we are in a better place at this point.",1,0.571796715259552
425078873,5582,rondagostino,2018-09-27T12:52:14Z,"the test failures are due to the tests being flaky rather than a coding problem in the feature implementation. i may have been able to solve the delegation token-related test that was failing (it didn't fail in these runs) by increasing the session lifetime in that test (from 50 ms to 500 ms). i will try to fix these flaky tests by increasing the associated session lifetimes as well. i think what may be going on is the tests are taking a long time to run and by the time they send data the session expiration time has arrived and the connection is being killed. it has also occurred to me that setting the session lifetime to something less than 1000 ms is probably not a good idea in general -- it may invite this type of flakiness due to unexpected delays, for example -- and maybe we should change the units of the configuration value to seconds instead of milliseconds to make sure it is never less than 1000 ms. thoughts? commit pushed with larger lifetimes, it was clean on my laptop.",0,0.9649165272712708
425097942,5582,rondagostino,2018-09-27T13:44:25Z,i just pushed a commit to add an expiredsessionskilledcount metric. the kip called for expired-connections-killed-total as the metric name but expiredsessionskilledcount is consistent with the existing metric requesthandleravgidlepercent. thoughts?,0,0.9717733263969421
425170620,5582,rajinisivaram,2018-09-27T17:10:04Z,"sorry, i haven't had any time today to look into this pr. given the complexity of the pr and how close it is to feature freeze, it would be very helpful if we could get another reviewer. i will try and take another look tomorrow, but not sure if we will have enough time to get this merged without help from one more reviewer. if it didn't meet feature freeze deadline, would it work for you if it was committed only to trunk and not the 2.1.0 branch?",-1,0.9769267439842224
425176688,5582,rondagostino,2018-09-27T17:28:45Z,"we would really like this in 2.1.0 if at all possible. is there anything i can do to help? maybe find another reviewer, perhaps someone else who has voted for the kip?",1,0.8558603525161743
425179089,5582,rajinisivaram,2018-09-27T17:36:17Z,"yes, getting another reviewer would definitely speed up the process.",0,0.9735808372497559
425180086,5582,rondagostino,2018-09-27T17:39:23Z,"there are a couple of changes that are not necessary as part of this pr -- more like cleanup of things i discovered while working on the alternate implementations that we considered and rejected. i could remove those changes and submit them as minor tickets post-2.1.0 if it helps to minimize the surface area requiring review. `expiringcredentialrefreshinglogin` is one, for example, and `oauthbearersaslclienttest` is another.",0,0.9908899664878845
425261409,5582,rondagostino,2018-09-27T22:21:55Z,i discovered that many of the failing tests were due to the fact that gssapi was not re-authenticating because it uses the client_complete state on the client and i wasn't handling that possibility correctly. this meant its connections were continually being killed by the server. this is now fixed. i also moved the expiration check from kafkarequesthandler to socketserver so that the check is done more upstream and therefore with as little delay as possible; i think that delay sometimes has an impact when we run with really tight expiration times on the order of milliseconds. i'll be curious to see how the tests look now.,1,0.6503489017486572
425285024,5582,rondagostino,2018-09-28T00:43:35Z,"both builds passed. i'm going to work on adding the last metrics, and if i get that done tonight then i believe this will be complete.",0,0.8692435622215271
425312612,5582,rondagostino,2018-09-28T03:48:56Z,"i was able to add the additional metrics. this pr now has all functionality, but i could not figure out how to test server-side expired connection kill or its metric since i have no way of running an older client. i know it works because it was the cause of the test failures in `saslgssapisslendtoendauthorizationtest` -- i just don't have a way to test it in an automated fashion because there is no way to disable re-authentication at the client side. i probably have to add metric documentation as well. but i think that's it. i asked someone to review, but this section of the product is pretty difficult to understand without experience, so i'm not optimistic about it. any chance you can review this on friday? al tests passed last run, and i hope the same will be true of this one.",0,0.8663526177406311
425365864,5582,rajinisivaram,2018-09-28T08:41:45Z,"in terms of testing, i think system test may be the way to go for testing with older clients since we already have other system tests for testing with older clients. if this goes in by monday, you have until code freeze two weeks later to add the system test to 2.1.0.",0,0.9758005738258362
425408404,5582,rondagostino,2018-09-28T11:37:13Z,ok. last set of builds was clean again. i'm confident this is working as expected and is able to be merged assuming a review cycle can be completed.,0,0.6457082033157349
425438808,5582,rondagostino,2018-09-28T13:40:12Z,"metric documentation is added, this is now done except for any additional pr review required. all conversations from above are resolved/fixed. i've contacted 5 people (4 of whom up-voted the kip) in the hope that at least one will be able to review.",0,0.9653214812278748
425588102,5582,rondagostino,2018-09-28T22:53:47Z,i reverted 9 files and eliminated 500 changed lines from this pr. i will submit minor tickets for these changes separately. these changes perform cleanup of code that was discovered while implementing rejected alternatives. these changes are minor and can be submitted separately at a later date. they have no impact on functionality or performance and are simply reducing technical debt. keeping these separate reduces the surface area of this pr.,0,0.9344301223754883
426148561,5582,rondagostino,2018-10-02T04:53:16Z,"i added functionality to prevent changing the principal or the sasl mechanism during re-authentication and added tests for both cases. i defined a 1-second minimum before you can re-authenticate a second time (see comment above) to prevent the rogue/buggy client from re-authenticating over and over again (the connection will be closed if the 1-second timeframe is violated, and then there will be the new ddos delay as well, so i think this covers it). i will address client interoperability with system tests (i assume it will be easier as you stated, though i am unfamiliar with the system test suite at the moment). assuming everything looks good, any chance of this being merged and included in 2.1.0 even though it is the morning after?",0,0.9720940589904785
426184322,5582,rajinisivaram,2018-10-02T08:00:59Z,i can do another review today to make sure it is ready to merge. can you check with the 2.1.0 release manager on whether it can still go into 2.1.0? the alternative is to merge to trunk only after the branch is cut.,0,0.9944557547569275
426244843,5582,rondagostino,2018-10-02T11:51:38Z,see comment from above -- any chance this can get into the 2.1.0 release if it is merged today?,0,0.9946426153182983
426246111,5582,lindong28,2018-10-02T11:56:32Z,"do you think we can merge this pr by end of wednesday? since this pr is almost ready to be merged, it is reasonable to wait for a day or two to include it in 2.1.0 release.",0,0.9923128485679626
426253681,5582,rondagostino,2018-10-02T12:24:35Z,"the test failures seem to be timing-based. i can't reproduce any of the issues locally, and the jdk 8 and 10 results are widely divergent as well. i'm going to revert the part of the checks for re-authentications in the endtoend tests -- i'll only check to make sure nothing has been killed and no re-authentications have failed and allow zero re-authentications instead of demanding that at least one occurs. i'll also set the max session re-auth to something greater than 1 second since that is the cutoff for repeated re-authentication rejection and i do see a re-authentication failure in the jdk 8 build (probably in the jdk 10 build as well since there are so many more failures there, but i haven't looked hard enough to find one). let's see if the next builds are clean again.",0,0.9833557605743408
426260661,5582,rajinisivaram,2018-10-02T12:49:14Z,"i will do another review today and see where we are. we also need a good system test run on this branch. unfortunately the last one i started didn't work, so started another one: [a link] since we don't have any interop tests yet, it will be good if you can run some manual tests with different versions of broker/clients.",0,0.9157339930534363
426294584,5582,rondagostino,2018-10-02T14:27:00Z,i am unable to see the system test at the link [a link] do i need some kind of permission in the jenkins instance that i do not have? i get http 404 not found. i will work on manually running some interop tests.,0,0.9834716320037842
426304117,5582,rajinisivaram,2018-10-02T14:52:16Z,"sorry, not sure about access since it is a confluent jenkins. i will update with the system test results when the test run completes.",-1,0.9119098782539368
426329033,5582,rajinisivaram,2018-10-02T15:56:48Z,we need to get to the bottom of these test failures today if we want to merge tomorrow.,0,0.9798857569694519
426358785,5582,rondagostino,2018-10-02T17:22:10Z,"agreed. the tests failures in the jdk 8 build appeared when i ""fixed"" the expiredconnectionskilledcount metric. i'm going to comment out that metric to see if the failures go away. unfortunately this is a long turnaround cycle -- a couple of hours -- but it's the best i can do given that i can't reproduce the issue locally under jdk 1.8.",0,0.9400529861450195
426396730,5582,rondagostino,2018-10-02T19:15:54Z,"still seeing this error on jdk 8 build: this is the same type of problem that occurred before. it isn't happening as frequently without the metric there, but it is still happening. i pushed a commit to add more debug information to the error message. will have to wait for these builds to finish and then wait another 1.5 hours for a failure to occur in the next build.",0,0.7391797304153442
426462405,5582,rondagostino,2018-10-02T23:20:50Z,"i agree it would be unwise to include this in the 2.1.0 release. thank you both for your willingness to entertain the possibility over the past days. the point that convinces me is this one: i am actively working on the authenticationexception issue, and while it is difficult to run experiments where the turnaround time is on the order of hours, i am hopeful that i can figure out what is going on there. let's shoot for merging this to trunk when the above two issues are resolved. i don't expect to be able to write exhaustive tests to guarantee that authentication is not impacted by re-authentication, but i will try to re-organize the code so that the diff makes it much easier to confirm by inspection that there is no impact. i did not keep this in mind as i worked, so i think i can accomplish the same things with more clearly fenced-in changes. i think that -- at a minimum -- will be helpful. thanks again for your willingness to give the pr a chance for 2.1.0, and rajini thank you especially for the technical advice on the implementation and the reviews. hopefully i can get this into a trunk-mergeable state soon.",1,0.8073725700378418
426643419,5582,rajinisivaram,2018-10-03T13:46:38Z,the system test run from this branch from yesterday passed all tests except the nine variations of quotatest which are also failing from trunk.,0,0.9859477281570435
426653511,5582,rondagostino,2018-10-03T14:14:46Z,"i am now unable to reproduce the errors. i've run 3 separate builds -- some were clean, and while others had errors they seem to be unrelated. i can't fix something that doesn't show up!",-1,0.9809107184410095
426656500,5582,rondagostino,2018-10-03T14:23:19Z,"i will respond to review comments incrementally so that i kick off a bunch of builds. one of them is bound to exhibit the problem at some point, and then maybe i'll get useful information from the additional detail i put in the error message.",0,0.9783269762992859
426986850,5582,rondagostino,2018-10-04T11:39:56Z,"still unable to reproduce the problem, both jdk 8 and 10 builds are totally clean at this point. today i'm going to try to re-work the `saslclientauthenticator` and `saslserverauthenticator` code so that we can more easily say via inspection that the existing authentication flow is unchanged. hopefully that will be useful and then the builds will either remain clean or the `authenticationexception` problem will arise (and i will get additional information via the exception message, which i augmented). then we can decide whether to commit this to trunk or not.",0,0.914858877658844
426999888,5582,rajinisivaram,2018-10-04T12:28:41Z,"thanks for making the updates. fyi if you want to rerun tests without a commit, you can just add a comment `retest this please`.",1,0.9746401906013489
427260765,5582,rondagostino,2018-10-05T06:33:47Z,"i reorganized `saslclientauthenticator` and `saslserverauthenticator` to make it much easier to tell by inspection that authentication is unaffected by the re-authentication code. i am unable to reproduce the `authenticationexception` problem we were seeing the other day -- the issue has not reappeared across several builds. the reorganized code and a successful system test should give us high confidence that authentication is unaffected and the issue is likely something that happens rarely -- maybe due to some kind of race condition that is possible only during specific re-authentication states. that's conjecture on my part, but i don't know what else i can do if the issue doesn't reappear. i spent the entire day staring at the code and reorganizing it, and nothing jumped out at me. please review the latest code and let me know your thoughts on how you wish to proceed with respect to potentially merging this to trunk.",0,0.9510104060173035
427291312,5582,rajinisivaram,2018-10-05T08:46:16Z,"thanks for the updates. i will do another round of review early next week. since there have been no more failures, i think we can commit to trunk. since we are early in the release cycle for the next release, we will have plenty of builds before that and if we do run into that exception again, hopefully the additional debug you added will pinpoint the cause.",1,0.9774093627929688
427367528,5582,rondagostino,2018-10-05T13:33:36Z,"thanks. i'll look forward to addressing any review comments. fyi, i just updated the kip to refer to 2.2.0 instead of 2.1.0 and the current metric names (some of which changed over the course of this pr). i also just pushed a commit updating the `ops.html` document to refer to 2.2.0 instead of 2.1.0.",1,0.9841519594192505
429698198,5582,rondagostino,2018-10-15T03:32:48Z,"everything is finished except re-doing `saslauthenticatortest` to make it less wasteful (i.e. don't just blindly re-run all tests in ""re-authenticate"" mode) and performing a rebase. i found rebase without squash to be difficult given that i had a bunch of oauth technical debt cleanup changes in here along the way that i backed out. i have a squash rebase ready to push that runs the test suite cleanly; i'll push that sometime monday edt, then assuming that runs well on the build farm all that will remain will be `saslauthenticatortest`.",0,0.9194850325584412
430094679,5582,rondagostino,2018-10-16T04:12:22Z,"i believe this is all set, all review comments are addressed. note that there were lots of errors on the previous jdk 11 build ([a link] but i looked at some of the other pr builds and the same thing happens elsewhere at times (e.g. [a link] the jdk 8 build was clean. so i suspect this is fine. **i have yet to see a recurrence of the `authenticationexception` problem**.",0,0.722006618976593
430175809,5582,rajinisivaram,2018-10-16T09:53:25Z,thanks for the updates. agree that the test failures are unrelated to this pr. i will do another review tomorrow and hopefully we can merge this week.,1,0.9755502343177795
430468029,5582,rondagostino,2018-10-17T02:32:35Z,rebasing to resolve conflict with `clients/src/test/java/org/apache/kafka/common/network/networktestutils.java`,0,0.9926794767379761
430472303,5582,rondagostino,2018-10-17T02:58:54Z,looks like `trunk` build is broken due to [a link] when i rebased onto trunk for this pr i am getting this compile error locally and i assume it will also happen here: [code block] i think `org.apache.kafka.streams.kstream.windowstest` has a similar compile error.,0,0.9754037857055664
430474995,5582,rondagostino,2018-10-17T03:15:29Z,"ok, i see [a link] is where this will be fixed.",0,0.9924232959747314
431457065,5582,rondagostino,2018-10-19T18:32:52Z,any progress on another review? would like to merge asap so it gets as many builds and test runs as possible prior to the next release.,0,0.9860473871231079
431789440,5582,rajinisivaram,2018-10-22T09:49:32Z,"sorry, haven't had time to review yet, will try and do it today or tomorrow.",-1,0.9892997741699219
432309421,5582,rondagostino,2018-10-23T16:00:31Z,latest comments are addressed/pushed. the one issue that might need more attention is the `reauth_bad_mechanism` state in `saslserverauthenticator`. see my comment above.,0,0.9893777966499329
433043426,5582,omkreddy,2018-10-25T13:08:05Z,my recent merge #5684 created some conflicts with this pr. please rebase the pr.,0,0.9744774699211121
433181272,5582,rondagostino,2018-10-25T19:46:16Z,"made two suggested changes, both builds were clean. rebasing onto latest trunk now to resolve conflicts.",0,0.9943471550941467
433383564,5582,rondagostino,2018-10-26T11:56:58Z,"after rebase jdk8 build was clean, jdk 11 build has 2 failures but they seem unrelated/transient issues.",0,0.9806134700775146
433390920,5582,rajinisivaram,2018-10-26T12:28:08Z,"thanks for the updates. i think we are good to go. i will take a look later today and merge to trunk (if there are any other changes required, we can do them in follow-on prs).",1,0.9819298386573792
627487350,8657,chia7712,2020-05-12T17:34:11Z,please take a look :),0,0.9434741735458374
637668386,8657,chia7712,2020-06-02T16:34:08Z,could you take a look?,0,0.9896900653839111
643890644,8657,chia7712,2020-06-15T04:06:29Z,so... could we keep it simpler?,0,0.9809629917144775
643933638,8657,chia7712,2020-06-15T06:37:38Z,[code block] the flaky is traced by #8853,0,0.9941592216491699
644521032,8657,chia7712,2020-06-16T04:16:09Z,my bad :( i'll keep that in mind,-1,0.9961332082748413
646487562,8657,chia7712,2020-06-19T07:34:56Z,thanks for all your reviews. how to see the result from this url?,1,0.9422212243080139
646685855,8657,junrao,2020-06-19T15:03:35Z,": the system test results can be found in [a link] if you click on report.html, it shows there were 76 failures. could you take a look and see if it's related to this pr? i will trigger another run just to see if any of those failures were transient.",0,0.9906392097473145
646718970,8657,chia7712,2020-06-19T16:08:47Z,i compare the failed tests to [a link] (the link is attached to [a link] there are two failed tests happens only on this pr. 1. consumebenchtest [code block] 2. streamsoptimizedtest [code block] wait for the new run.,0,0.9937731623649597
647020292,8657,junrao,2020-06-20T16:55:57Z,this is a second run of the system tests. [a link],0,0.991803765296936
647233460,8657,chia7712,2020-06-22T02:32:10Z,[code block] pass on second run. [code block] still fail and the error message is [code block] need to dig in it :( could you trigger a system test for trunk branch ? the related code have been updated recently (0f68dc7a640b26a8edea154ea4ea2b6d93b5104b) i run system test [code block] on my local for trunk branch and it always fails :(,-1,0.9959922432899475
647352301,8657,chia7712,2020-06-22T07:57:28Z,i have filed [a link] to trace [code block],0,0.9955959916114807
647688705,8657,junrao,2020-06-22T18:07:36Z,it seems we just fixed a bunch of client compatibility related failures in [a link] another 18 test failures were due to tlsv1.3 and are tracked in [a link] i started another run of system tests.,0,0.9892340898513794
648249808,8657,junrao,2020-06-23T15:47:28Z,there were still lots of client compatibility related failures [a link] . not sure why since those tests were passing in [a link],0,0.9434743523597717
648254407,8657,junrao,2020-06-23T15:54:48Z,: i think the client compatibility test failures are probably because you haven't rebased the pr. #8841 was fixed 6 days ago. could you rebase your pr?,0,0.9852021336555481
648254987,8657,chia7712,2020-06-23T15:55:44Z,done,0,0.8682363629341125
648259565,8657,junrao,2020-06-23T16:03:31Z,thanks. triggering another round of system tests.,1,0.8500022292137146
649615869,8657,junrao,2020-06-25T15:17:27Z,latest test results with 31 failures. [a link] i will trigger a system tests for trunk.,0,0.993065595626831
649635872,8657,chia7712,2020-06-25T15:46:00Z,big thanks !,1,0.9871113896369934
650240109,8657,junrao,2020-06-26T15:25:37Z,"for comparison, 29 test failures in trunk [a link]",0,0.9928989410400391
650503360,8657,chia7712,2020-06-27T06:13:16Z,it seems to me there are many flaky in system tests and i need more time to dig in them (tlsv1.3 is traced by [a link] **following failed tests happens with this pr** [code block] [code block] [code block] [code block] [code block] [code block] [code block] [code block] --- **following failed tests happens without this pr** [code block] [code block] [code block] [code block] [code block] [code block] [code block],0,0.9693294167518616
650583070,8657,junrao,2020-06-27T16:31:50Z,i just merged the pr for kafka-10180. perhaps you can rebase again.,0,0.9948709011077881
650586275,8657,chia7712,2020-06-27T17:01:31Z,done,0,0.8682363629341125
650786185,8657,junrao,2020-06-28T16:02:44Z,latest system test results. down to 15 failures. [a link] will do another trunk run for comparison.,0,0.9889189600944519
650923843,8657,chia7712,2020-06-29T05:50:05Z,1. connect_rest_test.py is traced by #8944 2. zookeeper_tls_test.py is traced by #8949,0,0.9917069673538208
651902726,8657,junrao,2020-06-30T16:24:40Z,14 system test failures in trunk. [a link],0,0.9781166315078735
651941352,8657,chia7712,2020-06-30T17:38:11Z,there are three failed tests which take place with this pr. 1. core/transactions_test 1. core/downgrade_test 1. core/upgrade_test will dig in them :),1,0.6992456316947937
653224351,8657,chia7712,2020-07-02T21:17:33Z,core/downgrade_test core/upgrade_test those are flaky on my local as well. should i fix all flaky before merging this pr?,0,0.9786493182182312
653227128,8657,junrao,2020-07-02T21:24:53Z,": thanks for the investigation. you don't need to fix all those flaky tests. it would be helpful if you could file separate jiras to track them, if not already. overall, do you think there is no new system test failure introduced by this pr?",1,0.9630935192108154
653232232,8657,chia7712,2020-07-02T21:41:37Z,i did not observe obvious relation between flaky and this pr. i have filed 4 prs to fix flaky system tests. maybe we can run system tests again after those prs are merged. hope there is good luck to me :),1,0.9939467906951904
653953935,8657,junrao,2020-07-05T23:39:49Z,: thanks. are the unit test failures also due to flaky tests?,1,0.9424300789833069
653956055,8657,chia7712,2020-07-05T23:58:41Z,"i think so. however, im trying to resolve all of them so as to make this pr more safe :) #8981 #8974 #8913 are pending for reviewing. the remaining failed tests on this pr are downgrade_test and upgrade_test. i will check them carefully tomorrow.",1,0.9928015470504761
656365594,8657,junrao,2020-07-09T21:43:11Z,: all 3 prs you fixed above have been merged. do you want to rebase again so that i can run system tests one more time?,0,0.9927747249603271
656382338,8657,chia7712,2020-07-09T22:31:33Z,sure!,0,0.972205400466919
657739902,8657,chia7712,2020-07-13T19:11:12Z,could you trigger system tests again?,0,0.9933815002441406
658027392,8657,chia7712,2020-07-14T07:47:55Z,[code block] it is traced by [a link],0,0.9958465695381165
658281873,8657,junrao,2020-07-14T16:31:22Z,: here is the latest system test result. [a link] the number of failures went up to 17.,0,0.9878489971160889
658294532,8657,chia7712,2020-07-14T16:55:42Z,ok. let me dig in them again :) there are 3 failed tests related to [code block] and i have filed #9021 to fix them. [code block] is traced by #9026 and the approach of #9026 works for [code block] i think. [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link] [code block] -> [a link],1,0.9925474524497986
661862318,8657,chia7712,2020-07-21T13:29:35Z,"could you run system tests on both trunk and this pr? the tests which fail frequently on my local are shown below. 1. transactions_test.py #9026 1. group_mode_transactions_test.py #9026 1. security_rolling_upgrade_test.py #9021 1. streams_standby_replica_test.py (i'm digging it) also, i have filed tickets for other failed system tests.",1,0.5139414668083191
662642708,8657,junrao,2020-07-22T19:15:47Z,: i just merged #9026. could you rebase again? i will run the system tests after that. thanks.,1,0.9718896746635437
662810186,8657,chia7712,2020-07-23T04:14:53Z,done. the known flaky [code block] is traced by #9059,0,0.9913592338562012
663109443,8657,junrao,2020-07-23T16:37:46Z,: only 6 test failures in the latest run with your pr. [a link] i will do another run on trunk for comparison.,0,0.9831991791725159
663597341,8657,junrao,2020-07-24T15:32:27Z,3 system test failures with trunk. [a link],0,0.9866788983345032
663618045,8657,chia7712,2020-07-24T16:17:50Z,"thanks for the reports. unfortunately, the failed tests are totally different in both results :( this pr has been rebased so the fix for [code block] is included. [code block] will get fixed by #9066. i will test the others on my local later.",-1,0.9925179481506348
663818439,8657,chia7712,2020-07-25T06:42:16Z,"i have rebased this pr to include fix of [code block]. could you run system tests again? except for [code block], [code block] and transaction tests, other tests work well on my local.",0,0.9933072924613953
664652669,8657,junrao,2020-07-27T21:40:27Z,": thanks for the diligence. running the system tests again. : there are two proposed ways of solving this issue. 1. async completion of delayed operations in a separate thread pool ([a link] 2. this pr where we let the caller of replicamanager.appendrecords() to complete returned delayed operations without holding a lock. approach 1 makes the api a bit cleaner. however, it introduces yet another thread pool used in the common path.this thread pool needs to be configured and monitored properly, which adds complexity. since the deadlock issue is isolated in groupcoordinator, i am not sure if it's worthwhile to add another thread pool just to avoid the deadlock. so, even though approach 2 adds a bit complexity in the api, it's probably still the better solution at this stage. please let me know your thoughts.",1,0.9684828519821167
665146517,8657,junrao,2020-07-28T16:38:05Z,2 system test failures in the latest pr. [a link],0,0.9888650178909302
665149860,8657,chia7712,2020-07-28T16:44:19Z,[code block] -> [a link] i will take a look at [code block] ([a link],0,0.9943631887435913
666398632,8657,chia7712,2020-07-30T14:26:32Z,those 2 failed tests are flaky on my local and there are issue/pr related to them. please take a look.,-1,0.5209233164787292
666403581,8657,ijuma,2020-07-30T14:33:18Z,"thanks for the summary. with regards to the thread pool option, would this be used for completion of delayed operations for the group coordinator only? when it comes to tuning and monitoring, you're thinking we'd have to introduce a config for the number of threads and an `idle` metric?",1,0.9546399116516113
666753554,8657,junrao,2020-07-30T22:45:13Z,": i think the proposal is to complete all delayed operations in a separate thread pool. my concerns for that approach are the following: (1) configuration: how many threads should be used? should that be dynamically configurable? should the queue to this thread pool be unbounded or fixed size? if it's the latter, how do we configure the queue size? (2) monitoring: how do we monitor the utilization of the new thread pool and the potential back pressure it creates (if the queue is fixed size)? (3) quota: do we need to account for the resource utilization of this new thread pool per client/user for quota purpose? (4) debugging: if there is a latency issue, how do we know whether this is due to delays in the new thread pool? so, overall, i feel that we should be careful with adding another thread pool used in the common code path since it adds other complexity. given that this issue is isolated in groupcoordinator, i am not sure adding a thread pool is an overall simpler solution. : i saw that you consolidated the commit history. were there significant changes since the last review?",0,0.9293897747993469
666882466,8657,chia7712,2020-07-31T02:46:07Z,i rebased code base to include [a link] which fixes flaky [code block]. retest this please,0,0.9904909133911133
667233872,8657,junrao,2020-07-31T17:09:09Z,": so, it's just a rebase and there is no change in your pr?",0,0.9883735775947571
667442912,8657,chia7712,2020-08-01T01:01:39Z,the last change to this pr is to rename a class (according to s comment),0,0.9944671392440796
669330984,8657,junrao,2020-08-05T17:39:14Z,": the following is my thought after thinking about this a bit more. the changes that we made in delayedjoin is complicated and it still doesn't completely solve the deadlock issue. adding more complexity there to solve the issue is probably not ideal. as a compromise, i was thinking another approach. we could pass down a flag to replicamanager.appendrecords() to complete the delayed requests in a separate thread there. only groupcoordinator will set this flag. so, the background completeness check is limited to the offset_commit topic. since this topic is low volume, a single thread is likely enough to handle the load. so, we don't have to make the number of thread configurable and don't have to worry about this thread being overwhelmed. the benefit of this approach is that the code is probably easier to understand since we could keep most existing logic in groupcoordinator unchanged. what do you think?",0,0.7758811116218567
669647781,8657,chia7712,2020-08-06T02:35:20Z,"i also don't want to include more mechanisms to complicate this story. it seems to me we can do a litter factor for [code block] to resolve this issue. [code block] is thread-safe already so it does not need lock. it means [code block] can be decoupled from [code block]. with this change, [code block] is involved by [code block] and it is called after releasing lock. for example: [code block] also, [code block] can be renamed to [code block]. the method name is consistent to its actual behavior. the benefit from this change are shown below. 1. simplify behavior of [code block]. the subclasses don't need to call [code block] always 1. lower possibility of conflicting lock",0,0.9662795662879944
670113688,8657,junrao,2020-08-06T18:38:05Z,": if we could solve the issue by simplifying delayedoperation, it would be ideal. i am not sure how your proposal avoids the above potential deadlock. could you provide a bit more detail? thanks.",1,0.8692460060119629
670293057,8657,chia7712,2020-08-07T02:55:59Z,"the new deadlock you mentioned is caused by pr. this pr introduces extra lock ([code block]) to [code block] (by contrast, previous [code block] has a group lock only). the thread has to get two locks (group lock and reentrantlock) to complete [code block] and so deadlock happens when two locks are held by different thread. hence, my approach is to remove the extra lock introduced by this pr and so deadlock will be gone. the changes to [code block] in this pr is to resolve deadlock happening on [code block]. [code block] has to get other group lock to resolve delayed request but it is executed within a group lock. in order to resolve deadlock, this pr tries to move [code block] out of group lock. however, after thinking about this a bit more, why [code block] is executed within a lock? it is thread-safe already and it is not always executed within a lock (if it is executed on timeout). hence, the implementation of my approach is to refactor [code block] to move [code block] out of unnecessary lock. the refactor includes following changes. 1. [code block] should be executed by [code block] other than subclasss 1. [code block] is executed after releasing lock [code block] 3. [code block] is renamed to [code block]",0,0.9863210320472717
671520491,8657,junrao,2020-08-10T18:38:10Z,": i agree mostly with your assessment. for most delayed operations, the checking for the completeness of the operation and the calling of oncomplete() don't have to be protected under the same lock. the only one that i am not quite sure is delayedjoin. currently, delayedjoin.trycompletejoin() checks if all members have joined and delayedjoin.oncomplete() modifies the state of the group. both operations are done under the same group lock. if we relax the lock, it seems that the condition ""all members have joined"" may no longer be true when we get to delayedjoin.oncomplete() even though that condition was true during the delayedjoin.trycompletejoin() check. it's not clear what we should do in that case.",0,0.7727771997451782
671749755,8657,chia7712,2020-08-11T06:13:06Z,"your feedback always makes sense. :thumbs_up: it seems to me the approach has to address following issue. 1. avoid conflicting (multiples) locks 1. small change (don't introduce complicated mechanism) 1. keep behavior compatibility ([code block] and [code block] should be included in same lock) i'd like to add an new method (default implementation is empty body) to [code block]. the new method is almost same to [code block] except for that it is executed without locking. currently, only [code block] has to use it. [code block]",1,0.9955980181694031
679256967,8657,junrao,2020-08-24T17:16:42Z,": sorry for the late response. i just realized there seems to be another issue in addition to the above one that i mentioned. the second issue is that we hold a group lock while calling `joinpurgatory.trycompleteelsewatch`. in this call, it's possible that delayedjoin.oncomplete() will be called. in that case, since the caller holds the group lock, we won't be completing partitionstocomplete in completedelayedrequests().",-1,0.9828102588653564
679855792,8657,chia7712,2020-08-25T07:30:26Z,"i go through group lock again and it is almost used anywhere :( i'm worry about deadlock caused by current approach so i'd like to address refactor and your approach (separate thread). the following changes are included. 1. [code block] does not complete delayed operations. instead, it adds those delayed operations to a queue. the callers ought to call the new method [code block] to complete those delayed operations in proper place (to avoid conflicting locking). 1. apple above new method to all callers who need to complete delay operations. 1. [code block] is called by [code block] only but it always held a group lock. to resolve it, we complete delayed requests in a separate thread. 1. keep using [code block]. the known conflicting locks should be resolved by above changes. using [code block] can protect us from deadlock which we have not noticed :) wdyt?",-1,0.9952425956726074
681109748,8657,junrao,2020-08-26T20:35:03Z,": thanks for the reply. i like your overall idea and i think it can be used to solve the problem completely in a simpler way. 1. instead of at `partition`, we collect all pending delayed check operations in a queue in replicamanager. all callers to replicamanager.appendrecords() are expected to take up to 1 item from that queue and check the completeness for all affected partitions, without holding any conflicting locks. 2. most callers to replicamanager.appendrecords() are from kafkaapis. we can just add the logic to check the replicamanager queue at the end of kafkaapis.handle(), at which point, no conflicting locks will be held. 3. another potentially caller is the expiration thread in a purgatory. systemtimer always runs the expiration logic in a separate thread and delayedoperation.onexpiration() is always called without holding any conflicting lock. so, for those delayed operations using replicamanager.appendrecords(), we can pass down a flag to delayedoperation so that at the end of onexpiration, we check the replicamanager queue if the flag is set. 4. we keep `delayedjoin` as it is, still passing in the group lock to delayedoperation to avoid deadlocks due to two levels of locking. 5. we can still get rid of the `trylock` logic in delayedoperation for simplification since there is no opportunity for deadlock. what do you think?",1,0.989725649356842
681318790,8657,chia7712,2020-08-27T03:16:04Z,"just double check. we use a separate thread to handle [code block] and [code block] in [code block], right? [code block] is called by [code block] only and it is possible to hold a group lock already.",0,0.9930041432380676
681361030,8657,junrao,2020-08-27T04:04:57Z,"the following is my understand. the current pr introduces a new deadlock through the following path. path 1 hold group lock -> joinpurgatory.trycompleteelsewatch(delayedjoin) -> watchforoperation (now delayedjoin visible through other threads) -> operation.maybetrycomplete() -> hold delayedjoin.lock path 2 delayedjoin.maybetrycomplete -> hold hold delayedjoin.lock -> trycomplete() -> hold group lock the existing code doesn't have this deadlock since (1) delayedjoin.lock is the same as the group lock held in the caller and (2) a delayed join operation is registered under the group key (so each time we check completeness for a group key, only one delayed join operation will be affected). by switching back to this code, we avoid the new deadlock. the existing code has a different deadlock issue that groupmanager.storegroup() in groupcoordinator.oncompletejoin may need to complete to other delayed operations and potentially hold a different group lock while already holding a group lock. this issue will be resolved if we complete those delayed operations due to groupmanager.storegroup() elsewhere without holding any locks.",0,0.9932700991630554
681375395,8657,chia7712,2020-08-27T04:14:55Z,so [code block] does not complete any delayed requests in this path anymore and we expect that someone who don't hold lock should complete them?,0,0.9919548630714417
681708424,8657,chia7712,2020-08-27T07:41:55Z,"i have submitted a draft patch. as this new approach is totally different from previous code, i delete previous commits in order to clean git history.",0,0.9798231720924377
682879578,8657,chia7712,2020-08-28T16:50:30Z,thanks for all suggestions! is jenkins on vacation? could you trigger a system test?,1,0.9867004752159119
682991870,8657,ijuma,2020-08-28T17:54:39Z,see kafka-10444 with regards to jenkins.,0,0.9950594902038574
683052000,8657,junrao,2020-08-28T18:30:49Z,: do you want to take another look at the latest solution from chia-ping? it (1) solves the known issues completely; (2) doesn't require new threads; (3) adds minimal changes to existing code; (4) simplifies existing code by removing the trylock logic.,0,0.978694498538971
683075891,8657,ijuma,2020-08-28T18:53:25Z,"this looks promising. one question, do we want every request to drain this replicamanager queue or only the callers of `appendrecords`? i think this answer affects the design a bit. if it's meant to be called by every request, then maybe we should have the delayed actions in a separate class instead of replicamanager. other classes could, in theory, add their own delayed actions to this queue too. if it's meant to be called after calling `appendrecords`, then it may be cleaner to add the call within the method that calls `appendrecords` (with maybe a helper method in `kafkaapis` to make it less error prone).",0,0.5108675956726074
683284825,8657,chia7712,2020-08-29T12:36:45Z,i preferred this idea as it prevent us from missing any action.,1,0.5676546692848206
683400677,8657,chia7712,2020-08-30T09:59:37Z,are there any suggested official benchmark tools?,0,0.9931226372718811
683434648,8657,chia7712,2020-08-30T15:33:32Z,the result of [code block] is attached below. it seems the patch gets better throughput :) i will run more performance tests tomorrow. **before** [code block] **after** [code block],1,0.995964527130127
683602732,8657,chia7712,2020-08-31T07:03:55Z,"the result of [code block] is attached (see description) the main regression ({""records_per_sec"": 3653635.3672, ""mb_per_sec"": 348.4378} -> {""records_per_sec"": 2992220.2274, ""mb_per_sec"": 285.3604}) happens in case [code block]. i re-run the case 5 times and it seems the throughput of that case is not stable. **before** 1. {""records_per_sec"": 3653635.3672, ""mb_per_sec"": 348.4378} 1. {""records_per_sec"": 3812428.517, ""mb_per_sec"": 363.5815} 1. {""records_per_sec"": 3012048.1928, ""mb_per_sec"": 287.2513} 1. {""records_per_sec"": 3182686.1871, ""mb_per_sec"": 303.5246} 1. {""records_per_sec"": 2997601.9185, ""mb_per_sec"": 285.8736} **after** 1. {""records_per_sec"": 2992220.2274, ""mb_per_sec"": 285.3604} 1. {""records_per_sec"": 3698224.8521, ""mb_per_sec"": 352.6902} 1. {""records_per_sec"": 2977076.5109, ""mb_per_sec"": 283.9161} 1. {""records_per_sec"": 3676470.5882, ""mb_per_sec"": 350.6156} 1. {""records_per_sec"": 3681885.1252, ""mb_per_sec"": 351.1319}",0,0.9891403913497925
683859366,8657,junrao,2020-08-31T15:41:57Z,": thanks for the performance results. it seems that the average across multiple runs doesn't change much? also, 1 failure in the latest system test run. [a link]",1,0.9777941107749939
683863356,8657,chia7712,2020-08-31T15:48:18Z,yep. i didn't observe obvious regression caused by this patch. [code block] was flaky (see [a link],0,0.9404476881027222
685831526,8657,junrao,2020-09-02T15:58:01Z,it seems there are some compilation errors in jenkins? [a link] ` 00:31:51 [error] /home/jenkins/jenkins-agent/workspace/kafka_kafka-pr_pr-8657/streams/streams-scala/src/test/scala/org/apache/kafka/streams/scala/streamtotablejoinscalaintegrationtestimplicitserdes.scala:136: value stringserde is not a member of object org.apache.kafka.streams.scala.serdes `,0,0.9935306906700134
685833800,8657,chia7712,2020-09-02T16:01:44Z,this error is caused by #8955 and #9245 is going to fix it,0,0.9822537302970886
685837334,8657,chia7712,2020-09-02T16:07:31Z,[code block] pass on my local. retest this please,0,0.9951626062393188
688481474,8657,chia7712,2020-09-07T19:19:30Z,"thanks for all reviews again :thumbs_up: my bad. i forgot this request :( expect for [code block], the other unused methods (in production scope) are removed by this pr. it seems we can remove [code block] from[code block]. that is similar to this pr and [code block] should not complete delayed request anymore. i can take over this in separate pr :)",1,0.9952220320701599
689241869,8657,junrao,2020-09-09T01:29:16Z,: i think this pr is ready to be merged. any further comments from you?,0,0.9454900622367859
689256048,8657,chia7712,2020-09-09T02:17:30Z,"[code block] [code block] on my local, they are flaky on trunk branch.",-1,0.9371920228004456
689838971,8657,junrao,2020-09-09T21:46:20Z,: thanks a lot for staying on this tricky issue and finding a simpler solution!,1,0.9881393909454346
689921735,8657,chia7712,2020-09-10T01:46:16Z,thanks for all suggestions. i benefit a lot from it.,1,0.9895251989364624
267731925,2264,cmccabe,2016-12-17T01:06:34Z,"previously, there were many places in the code that were creating abstractrequest objects directly and passing them to the networkclient. unfortunately, abstractrequest objects are version-specific. for example, a listoffsetrequest object of version 0 will have different fields inside its struct than a listoffsetrequest object of version 1. as soon as the message constructor is invoked, the contained structs are formatted in the version-specific format. this change adds the abstractrequest#builder types. client code in the fetcher and producer, and other places, can create these objects and pass them to the networkclient. the networkclient can then decide what version of each request to use based on the apiversionsrequest data. when the version that needs to be used is too old to support a necessary feature, an error is returned to the upper layer, which can then decide what to do.",0,0.9773896336555481
270150362,2264,ijuma,2017-01-03T16:08:53Z,", can you please rebase this branch? thanks!",1,0.9786832332611084
270250551,2264,cmccabe,2017-01-03T23:01:26Z,rebased on trunk and fixed some unit tests,0,0.9860310554504395
270380394,2264,ijuma,2017-01-04T14:18:19Z,"sorry, can you please rebase again?",-1,0.9932543635368347
270446437,2264,cmccabe,2017-01-04T18:26:06Z,rebased,0,0.9767982959747314
270447481,2264,cmccabe,2017-01-04T18:30:09Z,"also removed a few places where this change duplicated kafka-4548, and reverted a change to the test log4j.properties file",0,0.9941602945327759
270510560,2264,cmccabe,2017-01-04T22:51:34Z,"fix authorizerintegrationtest, fetchrequesttest, saslapiversionsrequesttest, plaintextconsumertest. fixed a bug where fetcher#beginningoffsets and fetcher#endoffsets incorrectly required a broker with the new listoffsetsrequest rpc. will add this to compatibilitytest.",0,0.9584379196166992
271399925,2264,cmccabe,2017-01-09T20:35:42Z,"good point about kip-74 compatibility handling. it sounds like we will have to reproduce the pre-kip-74 behavior of ""getting stuck"" when messages are too big rather than the new behavior of returning at least one message. i'll add back in that code shortly and work on a compatibility test for it. the other comments should be addressed now in the latest patch.",1,0.8813934922218323
271674875,2264,cmccabe,2017-01-10T19:35:27Z,i posted an update with a lot of fixes. i still need to address: * brokers should not send apiversionsrequest * implement pre-kip-74 message-too-big behavior * more centralized handling of versionmismatch exception * some other stuff,0,0.52281254529953
271680561,2264,hachikuji,2017-01-10T19:56:30Z,"i think your builder suggestion seems worth investigating.. if we continue with the model of delayed version determination, i was considering whether we could decouple the `struct` representation from the request objects. haven't really thought this through, but i was thinking something like this: [code block] that way the request objects remain immutable. i think you also had a suggestion which allowed us to construct the right version up-front.",0,0.5613734126091003
271720119,2264,cmccabe,2017-01-10T22:35:31Z,"* brokers no longer send apiversionsrequest, to allow inter-broker communication with versions before 0.10 * addressed some style comments",0,0.9802339673042297
271724878,2264,cmccabe,2017-01-10T22:56:48Z,", : unfortunately github doesn't let me post comments on the thread you have going about refactoring abstractrequest. so i'll comment here. there are really two questions here: 1. should the abstractrequest objects represent serialized data or unserialized data? 2. can we know the version of the request that we need before we send it to networkclient for #1, currently, abstractrequest objects represent serialized data, so they inherently have a version and a binary format. a lot of the methods in abstractrequest focus on serialization.. for example, abstractrequest#getrequest deserializes, abstractrequestresponse#sizeof gets the size of the serialized data, etc. you have to change the whole class hierarchy to modify this assumption. probably if you do that you would want to use a different name, such as message, to represent the fact that the new class hierarchy you are creating has nothing to do with serialized data. this might be worth doing. it would be a very big change. for #2, there are methods in networkclient such as leastloadednode that send the request to a node of the networkclient's own choosing. this doesn't really work unless the networkclient is in charge of picking a version. there also seems to be a bit of a layering violation here if the users of nc need to start concerning themselves with questions like has the peer they're talking to restarted or dropped its tcp session since we last learned about its version? #1 might make sense as part of a general refactor of the rpc class hierarchies. getting rid of the request class altogether might be a good idea since it invites confusion with clientrequest. i'm not sure i see a big win from #2 or a way to do it (but perhaps i am missing something).",-1,0.9308547377586365
271765728,2264,ijuma,2017-01-11T03:06:31Z,i submitted a pr with some clean-ups: [a link],0,0.9938919544219971
271775275,2264,cmccabe,2017-01-11T04:30:18Z,: thanks for the cleanup pull request. looks good... i merged it into this branch.,1,0.990856409072876
271867738,2264,ijuma,2017-01-11T13:26:33Z,"there are a number of failures in the system tests run. i started another one to double-check that it's not an environmental issue. in the meantime, i submitted another pr with clean-ups: [a link] i think we can merge after that (assuming the system tests pass) and do additional work in follow-up prs.",0,0.9897095561027527
1728555687,14182,rreddy-22,2023-09-20T23:34:54Z,created new pr [a link],0,0.9958928823471069
173692160,764,apovzner,2016-01-21T20:02:06Z,would you add the remaining kip-31 and kip-32 work to this patch (client side work and timestamp in produce response)? or that would be a different patch?,0,0.9946382641792297
173710219,764,becketqin,2016-01-21T21:16:32Z,this patch contains all the features in kip-31 and kip-32. the rest of the work is probably adding integration test. i have already added some unit tests but we can also add more if needed.,0,0.9820079207420349
173749672,764,apovzner,2016-01-21T23:31:50Z,"my question about remaining kip-31 and kip-32 work was based on outdated info -- i did not refresh my window and did not see that you added client-side implementation + returning timestamp in produce response. i see now that you also updated the pr description, thanks!",1,0.9759012460708618
174748282,764,guozhangwang,2016-01-26T00:39:19Z,"here are some high-level thoughts about the protocol: - basically we want the consumer to return the timestamp of the type specified by that topic even for compressed message set, but without the additional information the consumer would not know if logappendtime or logcreationtime is used. and as mentioned by just setting the wrapper message as the max value of all inner message timestamps and letting consumer check if wrapper timestamp is the max value does not perfectly work since 1) it requires the consumers to always decompress the whole message before returning any to the user, hence restricting buffer memory management we wanted to add in the future, 2) there is a corner case that if logappendtime is used and broker overrides the wrapper timestamp, it happens to be the same as the max of inner timestamps. i think we can add this information into the attribute field of the message, which currently only used 2 bits for four different compression types; instead we can make it a mask manner where the first 3 (or if we want to be safer, use 4) bits are preserved for indicating compression codec, leaves us a total of 8 (or 16) supported compression types, and use the forth (fifth) bit for indicating if the wrapper timestamp (for logappendtime, hence it is overridden) or the inner timestamp (for logcreationtime) should be used to set the consumer record's timestamp. and with this neither producer nor consumer needs to learn about this per-topic config from metadata responses, which makes the client change simpler, and other languages' adoption easier. - i am curious if the ducktape integration tests will be added in another pr?",0,0.975982666015625
174759962,764,becketqin,2016-01-26T01:32:12Z,"using attribute field is a good approach. it also lets consumers know the timestamp type. to make sure i understand your suggestion correctly: 1. the producer simply send message assuming the broker is using createtime. i.e. both attributes and timestamp will be using createtime. 2. if log append time is used, the broker only overrides the outer message's attribute field and timestmap field to use logappendtime 3. when consumer sees the message, it checks both magic as well as attribute field to see which timestamp is used (if magic > 0), and then decide whether it will override the inner message's timestamp or not. another thing is that we still need to decompress the entire compressed message, because of the reason i mentioned in one of the comments. given the stream compression used by producer, we will not have a actual ""relative offset compared with last message"" until we close the batch. instead, we only have the ""relative offset compared with the first message"" when we write a message into a batch. because the outer message only has the absolute offset of the last message, in order to have the absolute offset of an inner message, we have to decompress the entire compressed message to find out the ""relative offset compared with the last message"", then compute the absolute offset. i feel this is fine for new consumer because we are delivering messages in batch to use anyways.",1,0.6228975057601929
174762898,764,becketqin,2016-01-26T01:41:50Z,"btw, currently the compressioncodecmask is set to 0x7, so it is 3 bits. changing that to 4 bits is backward compatible so that should be fine.",0,0.9871783256530762
174773567,764,guozhangwang,2016-01-26T01:59:22Z,"yes your understanding is correct. i was initially thinking about possibilities of not decompressing the whole message when we add the memory management feature in the future so that we can choose to buffer less decompressed messages. but it seems not possible now, which maybe still fine for us so let's forget about it.",0,0.9373942613601685
175792263,764,becketqin,2016-01-27T18:55:42Z,i updated the patch with guozhang's proposal. i will add integration test in a separate pr. the intended tests are: 1. change timestamp type on the fly. 2. test message format version upgrade i actually want to do end to end test using different version of producers and consumers. but not sure if it is possible the current integration test because that requires different clients jars.,0,0.976334810256958
175972676,764,apovzner,2016-01-28T04:47:00Z,you can do end-to-end compatibility testing with system tests. take a look at compatibility_test.py. it currently tests 0.9.x java producer against 0.8.x brokers and 0.8.x consumer against an 0.9.x brokers. they both succeed on expected failure. you can add couple of more system tests to that to test newer brokers with older producers and/or consumers. note that you would need to update vagrant/base.sh to get kafka release 0.9.0.0.,0,0.9699802994728088
175982253,764,apovzner,2016-01-28T05:08:59Z,"maybe i missed it, but i don't see where producer assigns timestamps if the user does not specify the timestamp in producerrecord. the code was there before, but maybe it got accidentally removed with recent changes?",0,0.960070788860321
175992639,764,becketqin,2016-01-28T05:49:37Z,"thanks for the direction on compatibility test. extracting timestamp type out makes sense, given we already did that for compressiontype. i will change server side as well.",1,0.9443004727363586
176356903,764,apovzner,2016-01-28T19:27:13Z,i reviewed the kip-32 part of the patch (did not go in detail about kip-31 related changes). using timestamp type in attributes made producer/consumer code much cleaner! i made minor comments. otherwise looks good to me.,1,0.9887391328811646
176481781,764,becketqin,2016-01-28T23:37:32Z,thanks for the review. will you help take a look at the patch? thanks.,1,0.9628108739852905
176513163,764,becketqin,2016-01-29T01:39:13Z,the test failure is intermittent and is not related to this change.,0,0.9852110147476196
176885686,764,becketqin,2016-01-29T17:52:43Z,. thanks for the review. i addressed your previous comments. would you take another look? thanks.,1,0.9908349514007568
178268615,764,apovzner,2016-02-02T00:10:48Z,"correct me if i am wrong, but it looks like we don't have integration test for the case when the broker rejects messages because the timestamp is outside of configured time difference. i think it would be easier to track the additional tests we need to do (beyond this pr) if we created separate kafka jiras for them. could you please create jiras?",0,0.9200668931007385
178321814,764,becketqin,2016-02-02T02:32:25Z,good catch. i am going to add the rejected message unit integration test. this actually exposed a separate bug on the server side that causing it not returning the correct error code. i create kafka-3189 for that bug. will update unit integration test after that bug is fixed.,1,0.9835026860237122
178911759,764,becketqin,2016-02-03T00:31:04Z,thanks a lot for the review .i updated the patch to address your comments. some explanation on the changes: 1. the broker now return invalidmessageexception when producer does not set the timestamp type to createtime. previously it just overrides it. 2. it seems difficult to completely avoid unnecessary message magic value check when send fetch response back. i chose the way which i think has the least impact. but it might still break if people roll back message.format.version. 3. i haven't add the configuration sanity check for message.format.version yet. i will do that after we decide on whether it should be a topic level configuration and whether we want to use magic value or apiversions.,1,0.985576331615448
178974585,764,junrao,2016-02-03T03:00:56Z,: thanks for the patch. do you plan add some integration tests to cover backward compatibility? we can do that in a separate jira.,1,0.9704362154006958
179291438,764,junrao,2016-02-03T15:24:29Z,"also, could you update the doc on upgrade?",0,0.9954195022583008
180561754,764,becketqin,2016-02-05T21:27:35Z,thanks for the review. your comments are addressed. i have created separate tickets for integration test. i am working on the upgrade doc now and will update it later.,1,0.9680095314979553
181683922,764,becketqin,2016-02-09T03:02:02Z,i will start a poll on the mailing list as jun suggested. i updated the patch with upgrade doc. would you take a look? thanks. the test failure is irrelevant. i will open a ticket for that as it fails from time to time.,1,0.9148574471473694
183538336,764,junrao,2016-02-13T00:10:08Z,"hi, jiangjie, thanks for addressing the review comments. do you expect to submit a new patch soon? thanks,",1,0.9807784557342529
183561045,764,becketqin,2016-02-13T02:16:23Z,hi thanks a lot for the review. i just submitted a new patch. thanks!,1,0.9949592351913452
185176868,764,ijuma,2016-02-17T12:24:32Z,", thanks for the pr and for the quick response in addressing review comments. i filed a pr with your branch as the target with some additional minor improvements: [a link] please integrate into your pr (merge, cherry-pick or any other way you prefer) if you agree with the proposed changes. i expect to finish reviewing tomorrow. if jun merges in the meantime, we can use follow-up prs (if needed).",1,0.9709619283676147
185363481,764,becketqin,2016-02-17T19:25:24Z,thanks for the review. i just merged your pr.,1,0.9349986910820007
185510642,764,junrao,2016-02-18T02:16:08Z,"thanks for the patch. looks good overall. just left a few minor comments. also, in topiccommand, when listing the available config options, could we add a description that messageformat will be ignored if it's not consistent with the inter broker protocol setting?",1,0.9931025505065918
185992097,764,becketqin,2016-02-19T00:27:41Z,thanks for the patient review. i think i have addressed previous comments. could you take another look?,1,0.961575448513031
186009747,764,junrao,2016-02-19T01:54:55Z,": thanks for the latest patch. it looks good to me. once you address the last few minor comments, i can merge this in.",1,0.988359808921814
186274027,764,junrao,2016-02-19T15:53:53Z,: thanks a lot for working on the patch! lgtm,1,0.9955869913101196
186277555,764,ijuma,2016-02-19T16:04:01Z,nice work . and the reviewers too. :),1,0.9968433380126953
186463160,764,becketqin,2016-02-20T00:15:15Z,thank and so much for the great help on review!,1,0.9943318367004395
189037289,764,guozhangwang,2016-02-25T23:44:22Z,"some of the streams tests were incorrect when adding the timestamp. for example in processorstatemanagertest: `new consumerrecord<>(persistentstoretopicname, 2, 0l, offset, timestamptype.create_time, key, 0)` should be `new consumerrecord<>(persistentstoretopicname, 2, offset, 0l, timestamptype.create_time, key, 0)` actually i'm thinking if it harms to keep the old constructor for consumerrecord and make default values of 0l and timestamptype.create_time, and revert all the changes in stream tests? that way we can be free of incorporating the metadata timestamp until it is supported.",0,0.9802842736244202
189040692,764,ijuma,2016-02-25T23:56:32Z,"well-spotted. i actually wanted to suggest moving `timestamptype` before the timestamp to make this kind of error harder, but i only noticed this potential problem late in the process and then wasn't sure if it was worth the effort. having real bugs instead of theoretical ones adds motivation. i would prefer if we don't add the old `consumerrecord` constructor personally as `consumerrecord` is used outside of streams too. maybe we could add a utility method in streams in the meantime?",0,0.9337953925132751
189047528,764,guozhangwang,2016-02-26T00:13:40Z,"makes sense. the only place streams use `consumerrecord` directly is in `timestampextractor`, what kind of utility method do you have in mind?",0,0.9911581873893738
189048926,764,ijuma,2016-02-26T00:19:09Z,i just mean a method like `newconsumerrecord` that behaves exactly like the old constructor. then you could revert the changes in the streams tests and then do a search and replace in the streams folder.,0,0.993520200252533
189049558,764,guozhangwang,2016-02-26T00:22:43Z,sounds good.,1,0.857205867767334
1625488949,13870,dajac,2023-07-07T14:21:16Z,there are issues with the build as well. could you look into this?,0,0.9693862199783325
1625762579,13870,jeffkbkim,2023-07-07T17:57:38Z,"thanks for the review, i have addressed your comments.",1,0.8530596494674683
1631769383,13870,jeffkbkim,2023-07-12T02:46:42Z,thanks for the review. i have addressed your comments.,1,0.903476357460022
1632143234,13870,dajac,2023-07-12T09:13:11Z,"now that [a link] is merged, could you update your pr?",0,0.9961806535720825
1633127990,13870,jeffkbkim,2023-07-12T19:58:50Z,i have updated with latest and unified the mockcoordinatortimer.,0,0.995233952999115
1641304041,13870,jeffkbkim,2023-07-19T02:47:59Z,the test failures are unrelated,0,0.9829645752906799
767804738,9944,jolshan,2021-01-26T20:23:03Z,"added fetch session components. will add some versioning tests and final cleanups, then open for review",0,0.9920756816864014
767987726,9944,jolshan,2021-01-27T03:16:34Z,i'm aware that the latest changes to ensure the correct fetch version is sent seem to be causing more test timeouts. will need to investigate further and hopefully decrease the time needed for fetch requests. i may need to check some other flaky tests related to my changes.,0,0.5053834915161133
776097476,9944,jolshan,2021-02-09T17:14:49Z,"just to clarify this, the top is the fetch branch, so i think it is better than trunk i do want to take another look at the fetcherthread and fetchsession benchmarks which are slightly worse.",0,0.9709986448287964
790133100,9944,jolshan,2021-03-03T22:59:01Z,waiting on [a link] before proceeding since this pr touches a lot of the same files.,0,0.9899511933326721
794755556,9944,jolshan,2021-03-10T02:24:37Z,"currently working on merge conflicts. should have a first pass out in the next day or so. there are a few changes that don't work with the previous refactor. , can you take a look when i have the commit ready?",0,0.9150750041007996
794897185,9944,chia7712,2021-03-10T05:17:42Z,sure :),0,0.9888246655464172
796317046,9944,jolshan,2021-03-11T00:23:38Z,here's the commit. as mentioned in the commit message: i also acknowledge that this code creates a ton of data structures in fetchrequest and fetchresponse. i hope to clean those up soon.,0,0.7284329533576965
796886007,9944,jolshan,2021-03-11T16:56:59Z,there was a lot of back and forth about whether we should simply include the topic id and the name in the protocol (to uniquely identify the topic--which is something we may want to do to make sure we are consuming the right topic) versus use the topic id to replace the topic name. the main arguments were: 1. topic ids are in most cases shorter than topic names so we can shorten an already somewhat large protocol 2. we will eventually want to move over to using topic ids for everything so we might as well make this change now. i do agree that the topicpartitions used make this task much harder and i tried to find a clean way to accomplish this. the main idea is that we keep track of topics that did not have topic ids server-side (receiving-side) and send an error response back. the idea is that the sender will always have the topic id -> name conversion for topic ids it sends.,0,0.9336165189743042
796902838,9944,chia7712,2021-03-11T17:19:57Z,do you mean the client (consumer) must have a mapping (name -> id) in local cache before fetching data?,0,0.9930074214935303
796906853,9944,jolshan,2021-03-11T17:26:03Z,"yes. the consumer already gets periodic metadata updates, so i use that to get the mapping.",0,0.9941562414169312
798608479,9944,jolshan,2021-03-13T16:41:18Z,i've run jmh benchmarks before but not since the merge. i plan to do so again once i figure out how to improve the efficiency regarding unconvertedfetchresponse and fix up some the one test that is failing. i also plan to rerun system tests to make sure those are passing as well.,0,0.960055410861969
804157913,9944,jolshan,2021-03-22T15:35:26Z,"new benchmark results: these are pretty similar to the previous results, but thought i should include to compare to updated code (both this branch and trunk). note: i have modified `fetchresponsebenchmark.testconstructfetchresponse` on trunk to match the benchmark in this branch (so it doesn't count the time to build the map) and added `fetchresponsebenchmark.testpartitionmapfromdata` to the trunk code to capture that benchmark. [code block] [code block]",0,0.9013230204582214
814408153,9944,jolshan,2021-04-06T20:11:09Z,~currently blocked on [a link] (need to add topic ids to the metadata topic for fetching)~ no longer blocked,0,0.9734651446342468
828663698,9944,jolshan,2021-04-28T18:02:11Z,"thanks for taking another look i agree. this has been in the back of my mind for a while, specifically whether all the changes in fetchsession are necessary for such cases. so thanks for bringing this up. i think the only thing i was really concerned about was during a roll to upgrade/the new topic case you mentioned. but even using the current approach, i wasn't sure if the fetch session stuff was really helping. i think my biggest confusion comes from when the client will refresh metadata. will returning a top level error guarantee a refresh? (vs given an unknown topic id response?) i think that the top level approach will likely be better.",1,0.9826093912124634
828795518,9944,junrao,2021-04-28T21:35:32Z,"i think the upgrade case is similar---it's rare and transient. so, we could choose to have a simpler and less optimized way for handling it. i am not sure if we trigger metadata refresh for top level error right now. if not, we could probably just add the logic to refresh metadata for all topics on topicid error at the top level.",0,0.9805263876914978
830445376,9944,jolshan,2021-04-30T23:00:36Z,"ok, updated the code. one thing i assumed here is that we don't switch from not using topic ids in the session (requests versions 12 or below) to using 13+. i ensure this in the fetcher + abstractfetcherthread code, but maybe we can't assume this for all clients. if we can update from not using ids in the session to using them, i'll update the code.",0,0.9559665322303772
1540289311,13443,dajac,2023-05-09T14:52:18Z,-22 the build failed due to compilation errors.,0,0.7407544255256653
484241405,6592,yeralin,2019-04-17T20:07:29Z,usage: `new listserde<>(serdes.string())` to create a serde for `list ` values,0,0.9944778680801392
485460971,6592,mjsax,2019-04-22T16:07:06Z,"thanks a lot for the pr ! i like the idea and think it's a good addition. however, adding those classes is a public api change and not a `minor` pr. thus, it should be backed by a jira ticket and it also requires a kip: [a link] let me know if you have any question about the kip process (should actually be well document in the wiki), or if you need any other assistance. btw: you should also add unit tests :)",1,0.9970953464508057
489700008,6592,mjsax,2019-05-06T17:16:45Z,"great to see some progress on this pr, but we still need a kip... i would recommend to work on the kip first (or in parallel),",1,0.9886572957038879
489709013,6592,yeralin,2019-05-06T17:42:39Z,"kip, jira, and discuss thread are started: [a link]",0,0.9842737913131714
492021215,6592,yeralin,2019-05-13T23:20:57Z,"hmmm, strange i added a test case, but it still says that test results were not found",-1,0.5445331335067749
516673508,6592,yeralin,2019-07-31T02:57:09Z,any updates on this?,0,0.9843987226486206
516952229,6592,yeralin,2019-07-31T17:48:54Z,"sorry, i thought to finish all the discussions first, that's why i did not push. new commit is in place! :grinning_face_with_smiling_eyes: upd: merged with apache:trunk",-1,0.7565692663192749
518247929,6592,yeralin,2019-08-05T14:04:32Z,"hey, i'm sorry i thought you wanted me to merge with `trunk` that's why so many files popped up. i think i know how to resolve this. i'll force push **before** the merge, and then keep pushing my commits only related to `introduce_list_serde` work. and at the end, once all reviews are completed, i'll do the final merge and we can close this pr. upd: ok it should be much cleaner now :)",1,0.9787847399711609
519362418,6592,mjsax,2019-08-08T04:51:25Z,"seems there is a merge conflict again. and yes, i usually rebase to `trunk` (not merge `trunk`) and then push force to update a pr.",0,0.9926851987838745
519700266,6592,yeralin,2019-08-08T21:49:15Z,"hmmm, i think i did it! i haven't worked with git on a project of this scale, so bear with me :) i think i performed a rebase with `apache/kafka:trunk`, resolved couple conflicts along the way, and now everything should be fine.",1,0.9938141107559204
520019651,6592,mjsax,2019-08-09T18:29:35Z,"build failed with spotbug issue. can you address this? to test locally before pushing, i recommend to run `./gradlew streams:clean streams:test` -- this will run checkstyle and spotbug, too.",0,0.9749099612236023
520549718,6592,yeralin,2019-08-12T18:51:10Z,"i fixed all checkstyle warnings, except for: [code block] under `listserializer.java` and `listdeserializer.java` the build is failing bc of that. --- also, lots of tests are failing because for some reason they are expecting `default_list_key/value_serde_inner_class`, `default_list_key/value_serde_type_class` and other properties to have default values: [code block] i understand that i can set those default values during `.define(...` call. i guess i should set them to `null`?",0,0.8955865502357483
520550127,6592,yeralin,2019-08-12T18:52:20Z,"as per unchecked warnings, i suppressed them at the variable or method levels.",0,0.9932277798652649
526295380,6592,yeralin,2019-08-29T17:58:05Z,would you have time to take a look? :),1,0.9394195675849915
530406657,6592,yeralin,2019-09-11T14:28:55Z,rebased the branch with `trunk`. still seeing unrelated `spotbugs` errors in failed jenkins builds.,0,0.9632943272590637
531210633,6592,cadonna,2019-09-13T12:04:17Z,"retest this, please",0,0.9489298462867737
531377737,6592,yeralin,2019-09-13T20:20:26Z,"those were actually _related_ spotbug errors, my fault :grinning_face_with_sweat:",1,0.9678316116333008
531892218,6592,yeralin,2019-09-16T18:09:20Z,i think it is ready for the next round of review :) thank you in advance!,1,0.9913188815116882
535305283,6592,yeralin,2019-09-26T02:35:32Z,bump :),0,0.9189615249633789
537524269,6592,yeralin,2019-10-02T14:38:51Z,ready for another round :),0,0.9094529151916504
539604139,6592,yeralin,2019-10-08T16:50:58Z,bump :1st_place_medal:,1,0.7508653402328491
541655992,6592,yeralin,2019-10-14T12:44:40Z,bump :input_numbers: also updated kip and jira ticket,0,0.9933880567550659
545630718,6592,yeralin,2019-10-23T20:51:46Z,bump :1st_place_medal:,1,0.7508653402328491
547412636,6592,yeralin,2019-10-29T13:13:42Z,would you be so kind to take another look? :),1,0.9825631380081177
548114057,6592,yeralin,2019-10-30T21:11:15Z,rrrrready for another round :) thank you :1st_place_medal:,1,0.9965241551399231
549672088,6592,vvcephei,2019-11-05T05:46:39Z,"thanks for looking into that, , i also tried it out just now, and i think you're right. the only thing i think we could do to be able to spit out a ""fully typed"" result is to introduce a ""dummy interface"" like jackson's `typereference`. this kind of interface can be used to capture the full generic argument list at run time and use it to construct the collection. short of that, i believe we'd be doomed to be producing `rawtypes` warnings in user code. which is probably not a good choice for the api. note, this is all in retrospect. it's not obvious at all that this would crop up until you try it. in light of that, i think we probably need to change up the interfaces a little. something like: `class listdeserializer extends deserializer >` with a constructor like `public listdeserializer(class listclass, deserializer innerdeserializer)`. when you call `deserialize`, you'd get a `list ` (e.g., `list `) out, but this is probably fine, since best practice is to declare the variable type as the interface, not the implementation anyway. and the advantage is that the calling code (aka ""user code"") wouldn't have any ""rawtypes"" or ""unsafe"" warnings at all. again, thanks a million for catching this!",1,0.9512585401535034
550966966,6592,mjsax,2019-11-07T07:59:18Z,"thanks for your input! i agree that that it would be better to preserve the ""inner type"" instead of the ""list type"", ie, `deserialize()` should return `list ` instead of raw `arraylist`. -- do you agree this this assessment? if yes, please update the pr accordingly.",1,0.9840412139892578
553067625,6592,yeralin,2019-11-12T19:08:58Z,just pushed a commit with changes according to your description. not entirely sure if i got everything right.,0,0.7183414697647095
555213976,6592,yeralin,2019-11-18T21:19:36Z,updated kip and jira ticket as well,0,0.9905374646186829
555233480,6592,mjsax,2019-11-18T22:13:00Z,thanks -- i am currently fully loaded with `2.4` release issues... will try to cycle back to your pr but might take some time.,1,0.8734429478645325
571205357,6592,yeralin,2020-01-06T16:24:39Z,happy new year! :) i rebased the branch according to `upstream/trunk`. i noticed that branches aren't getting built anymore. let me know if there is anything else i can do here!,1,0.9971743822097778
585361037,6592,mjsax,2020-02-12T18:52:35Z,any progress on this pr?,0,0.9906713962554932
585364165,6592,yeralin,2020-02-12T18:59:49Z,"sorry, have been busy at work and personal stuff. should start making progress soon!",-1,0.9940091371536255
585969307,6592,yeralin,2020-02-13T21:00:38Z,introduced [code block] they test non-arg-constructors and use `configure` methods. i had to also update `configure` logic in both classes. take a look :) thank you!,1,0.9946417808532715
587819699,6592,yeralin,2020-02-18T20:46:35Z,"hmmm i was thinking about this magic flag, and there are some corner cases that i want to discuss. my initial idea was to create an enum: [code block] then it becomes counterintuitive: case 1: encoding primitives `arr={1, 2, null, 3}` user chooses `serializationstrategy.negative_size`, i guess we'd have to throw an exception saying that this serialization strategy is forbidden? case 2: encoding non-primitives `arr={'a', 'b', null, 'c'}` user chooses `serializationstrategy.null_index_list`, but then what do we store for size of `arr[2]`? zero? negative one? we are still wasting 4 bytes that way. i think our best solution is to simply use ""null index list"" for all primitives, and `-1` strategy for all non-primitives avoiding any magical, serialization strategy flags. what do you think?",0,0.9300262331962585
606241093,6592,mjsax,2020-03-30T20:47:32Z,"sorry for the late reply... adding an enum sounds like a good idea. i don't think we would need to throw, however, we would need to choose the non-optimized encoding and encode the length for each list item (even if it is the same length) over and over again. we would store zero bytes, because we can skip the entire entry. during deserialization we can just do a `list.add(null)` when we hit the corresponding index as encoded in the null-index-list header, ie, during deserialization we maintain a counter for the index of the element we deserialize. does this make sense? that would also be possible as an initial non-configurable default implementation, but i would still encode a magic header byte in case we want to change it later (just to be future prove). my suggestion to only implement one strategy was aiming to limit the scope of the pr. but if you want to implement both strategies, i have no objection either. however, if we actually do implement both strategies, than we could make it configurable from the very beginning on?",-1,0.9849586486816406
606252503,6592,yeralin,2020-03-30T21:12:22Z,i will implement both strategies. my problem with making it configurable is user might choose inefficient strategy and shoot him/herself in the foot. that's why i was thinking just but i do agree on leaving future prove 1 byte magic flag in the header.,0,0.9636251926422119
606277356,6592,mjsax,2020-03-30T22:08:55Z,"i guess it's called freedom of choice :) if we feel strong about it, we could of course disallow the ""negative size"" strategy for primitive types. however, it would have the disadvantage that we have a config that, depending on the data type you are using, would either be ignored or even throw an error if set incorrectly. from a usability point of view, this would be a disadvantage. it's always a mental burden to users if they have to think about if-then-else cases. i guess, in the end it's subjective which approach is better. but we could discuss pros/cons on the mailing list. feel free to propose whatever you prefer personally (and list all pros/cons for different approaches) and people can just chime in. personally, i have a slight preference to allow both strategies for all types as i think easy of use is more important, but i am also fine otherwise.",1,0.9776477813720703
657756747,6592,yeralin,2020-07-13T19:49:04Z,"aaaaaan i am back! :grinning_face_with_smiling_eyes: sorry for such a long break, was dealing with some personal stuff and work. pushed a few commits with `null-index-list` and `negative-size` serialization strategy functionality. rrrready for another round! the code syntactically is kinda raw, made it so to facilitate the reviewing process. the following set of commits is recommended for reviewing: [a link]",1,0.9507694840431213
665078937,6592,yeralin,2020-07-28T14:37:11Z,any updates?,0,0.9840564727783203
665304810,6592,mjsax,2020-07-28T21:58:28Z,"-- your pr is in my review queue. not sure how quickly i will find time to have a look though atm -- maybe next week, but i can't promise.",0,0.8477714657783508
698414958,6592,yeralin,2020-09-24T15:23:04Z,any updates :)?,1,0.9895402789115906
701300767,6592,helpermethod,2020-09-30T10:19:00Z,"this would be an awesome addition, as i wouldn't have to maintain my own impl :grinning_face_with_smiling_eyes:.",1,0.9945775270462036
702774321,6592,yeralin,2020-10-02T14:42:16Z,let me know when i can jump back to it :),0,0.9320210814476013
730815895,6592,yeralin,2020-11-20T03:06:24Z,"hi, any updates on this?",0,0.9787452220916748
731626798,6592,mjsax,2020-11-21T19:41:30Z,"-- sorry for the delay. happy to hear that you are still on top of your kip! i did not find time yet to review the pr after your latest updates. (not sure if it makes you feel any better, i did also not review any other kip related pr for a while, and yours is actually at the top of my list...) -- i hope to find time soon and hope we can get this merged before the end of the year... cannot guarantee it though. at least, i would like to get it into 2.8 release.",-1,0.9404681324958801
763952940,6592,yeralin,2021-01-20T21:18:34Z,"hey, just rebased my branch with the trunk, updated my tests to use junit 5. let me know when you guys will have time to review it.",0,0.9145374894142151
776422927,6592,yeralin,2021-02-10T04:01:44Z,any updates on this? :),1,0.7287705540657043
776934773,6592,mjsax,2021-02-10T18:56:01Z,-- sorry that i did not get to review your updates yet... i need to finish #9744 first that is very close to get merged. your pr is next in the list... (just trying to work through my kip pr backlog one-by-one...) btw: seems there is a merge conflict. could you rebase the pr in the mean time?,-1,0.9917210340499878
778674834,6592,yeralin,2021-02-13T20:37:53Z,"ok, resolved all conflicts. thank you for dedicating your time into this :)",1,0.99528968334198
808566124,6592,yeralin,2021-03-26T22:41:48Z,bump,0,0.9793093204498291
819183273,6592,ableegoldman,2021-04-14T02:49:38Z,"hey , are you still working on this/looking for reviews? i know everyone is often busy but if you can just respond to let us know you're still active, we can try to pitch in to get this across the finish line. cc for help reviewing",1,0.9222532510757446
819226355,6592,yeralin,2021-04-14T04:53:39Z,"i am! :) routinely checking this pr. i totally understand, we all get busy sometimes :thumbs_up:",1,0.9973691701889038
821515617,6592,yeralin,2021-04-16T19:29:23Z,"replied under each review comment, waiting for response before pushing the review changes.",0,0.9928795695304871
823384191,6592,yeralin,2021-04-20T15:47:46Z,did you have time to look at it? let me know if you need more context on my comments.,0,0.9805068969726562
829779733,6592,ableegoldman,2021-04-30T03:26:34Z,"hey one other thing, can you give the kip document a quick pass and make sure everything in there is up to date with what we've discussed and anything else that's evolved during the pr review? for example we might want to point out the `null` handling, though it's technically an implementation detail it would be good to clarify what is and isn't possible with this new serde",0,0.9814899563789368
840015831,6592,yeralin,2021-05-12T18:46:11Z,"ok, updated the kip with serializing nulls for different strategies.",0,0.9935286641120911
840856470,6592,yeralin,2021-05-13T21:52:33Z,seems like all checks passed :),0,0.952567458152771
840878936,6592,ableegoldman,2021-05-13T22:50:10Z,build has only some unrelated test failures in known flaky `ktablektableforeignkeyinnerjoinmultiintegrationtest#shouldinnerjoinmultipartitionqueryable` and `raftclustertest`,0,0.9900352358818054
840880983,6592,ableegoldman,2021-05-13T22:56:16Z,"merged to trunk! thanks for all the work and patience it took to get this pr in, and to all the reviewers who helped get it here along the way. , can you update the kip to note that it's completed, and then move this kip to the ""adopted"" section on both the kip main page and the streams kips subpage? :folded_hands:",1,0.9951220154762268
840925233,6592,mjsax,2021-05-14T01:04:32Z,thanks for pushing it through and thanks for helping out reviewing! really nice addition!,1,0.9957321286201477
1603267651,6592,venkatesh010,2023-06-22T20:23:12Z,hey getting serializationexception in this serde serializationexception: invalid serialization strategy flag value flag value is derived from bytes size (which is coming >2) which is amount of enum variables hence its breaking seems to be a bug please check,0,0.9732995629310608
1603284146,6592,venkatesh010,2023-06-22T20:37:10Z,this is while using list where e is class which is inner serde used for inner is jsonserde of type e,0,0.9945007562637329
1603491428,6592,yeralin,2023-06-23T00:59:45Z,could you please provide a code snippet?,0,0.9942482709884644
416341868,5567,vvcephei,2018-08-27T19:34:03Z,"note: both builds passed, but jenkins got rate-limited trying to report it to github. and i'm just now wondering... if jenkins failed to report the job status to github... why do we see the status here on github? o_o",1,0.9686179161071777
416457836,5567,guozhangwang,2018-08-28T05:40:43Z,could you take a look?,0,0.9896900653839111
417199290,5567,guozhangwang,2018-08-30T05:58:54Z,"i have not reviewed the latest changes on this pr yet, but here are two meta-level thought i'd like to share: 1. serdes: here's my reasoning on whether we need to enforce serdes. we have the following scenarios: a) the ktable-to-be-suppressed (i will just call it ktable from now on) has user-specified serdes. in this case we do not need to require serdes again for suppression. b) the ktable is materialized and users do not specify serdes during materialization. in this case we will try to use the default ones from config (or we can use the inherited ones in the future, but that is not guaranteed to be always correct anyways), so if the default serde to use is incorrect, we will get the exception even before suppression at all. so we do not need to require serdes either. c) the ktable is not materialized and users do not specify serdes. today this case is not possible but in the future it may be the case due to optimizations, e.g. `ktable#filter / mapvalues` generated ktable. in this case if we do not require users to specify serdes and default ones are not correct, it will indeed have unexpected exceptions. but i think this case can still be walk-around by users to provide the `materialized` object in those operators; plus in the future we can have further optimization to ""push the suppression ahead"" which i will talk about later in this comment. so in sum, i think it is not necessary to always enforce users to provide serdes in the buffer config. 2. changelogs: about whether or not we should add new changelog topics for the suppression buffer itself, i think it depends on how we will implement the commit behavior. my suggestion is the following: a) for size / interval based intermediate suppression, we will still honor the commit operation to always ""flush"" the suppression buffer, i.e. the intermediate suppression is still best-effort which yields to commits. in this case, we do not need to bookkeep ""what records have been buffered and not emitted"" in the changelog either but can simply assume none have been emitted until commit. b) for final result suppression of window stores, we cannot yield to commit because that will violate the intended guarantees, but since we will not emit any records before the grace deadline anyways, the ""state"" of the buffer does not need to be book-kept anyways: if it is beyond the grace deadline, then every records should be flushed, otherwise, none should be flushed. note that the above approach works for both eos and non-eos: for non-eos, if there is a crash in between commits, we may emit a single record multiple times but that is fine for non-eos; for eos, if there is a crash in between commits we need to restore the whole state from the beginning anyways as of now (until we have consistent checkpoints someday), so this is also fine. one caveat though is that 2.b) above relies on the current stream time to determine upon re-start whether or not the window store data have been emitted to downstream; but stream time today is not deterministically defined even with kip-353 merged in so if we re-process, it may generate different behavior. i think this is acceptable as of now and can be fixed in the future: for example, we can include the ""current stream time"" in the commit message when adding consistent checkpoints to make the stream time deterministic on those checkpoints. so in sum, i think we can accept to not have changelogs for the buffer itself as long as we still respect commits except for final result suppression, which we can implement as a special case. in addition, the ktable-before-suppression's changelog can be suppressed along with the buffer as well: we can only write to the change logger when the buffer emits. more details in the next section. 3. future roadmap: we have discussed about how to re-design kip-63 after this is done, and one thing is to consider how to maintain the suppression effect on the state store's changelog topics as well. together it has some implications on our memory management story as well. here's my thinking following the above proposal on changelogs: a) say if we remove the buffer on top of the state stores, the saved memory can be given to 1) the added buffer ""behind"" the state stores, and 2) to enlarge the state store's only write buffer (e.g. rocksdb). b) we can optimize the topology to ""implicitly"" add suppression when necessary in addition to user-requested suppressions to reduce the traffic to the original ktable's store changlog topics. more concretely, think about the following examples: [code block] * with logical materialization, only table1 will be materialized. * with logical materialization plus implicit suppression, the above will become: [code block] in which case table1's store changelog will be suppressed as well: we only write to the change logger when we emit. now if users explicitly calls suppression: [code block] it will also be re-written to [code block] as well, in which case table1's changelog will be suppressed still based on the `suppress()` config, and then the suppressed changelog stream will be filtered to table2, which can be logically materialized still. c) finally about memory management: we can deprecate the current `max.cache.bytes` and replace with the `total.memory.bytes` which controls the total amount of memory that streams will use, similarly to what we described in [a link] note this total bytes will cover both user-requested and implicitly added suppression buffers. in other words, each buffer's own buffer config's semantics will be a soft-limit which is only best-effort full-filled, since it is yield to commit interval, and the total bytes usable. this is just a sketchy thought and may need more detailed implementation discussions. let me know wdyt.",0,0.8413693308830261
417352990,5567,vvcephei,2018-08-30T15:05:21Z,"hi , thanks for the thoughtful feedback. 1. i think your argument about the serdes is sound. * for any non-k/v-changing operation that produces a ktable, we will be able to forward serdes from upstream, through the operator, and to suppress * the rest of the operators may change keys or values, but in all cases, it's possible to provide serdes at the key/value-changing operator, and then forward to suppress. so in all cases, we don't need to ask for serdes in suppress, which i vastly prefer. thanks! 2. for changelogs, i think it would be much better if we offered tight semantics in all cases. forcing people to reason about how the commit interval interplays with the suppression is needlessly complicated. but i do think that we can still optimize it to avoid the extra changelog. the good news is that at this stage, realizing that we can get serdes without asking for them in the suppression config means that we don't have to worry about the changelog or commit behavior. so this pr is not blocked on that conversation. i'll take it as a design goal to avoid an extra changelog and spend some time to see what i can come up with. at the least, you've offered a way to do it by relaxing the suppression semantics. 3. yes, i think that's a good long-term vision. and it would be all the more important to avoid an extra changelog if we wind up tacking a suppression on to every ktable. 3c. thanks for that reference. it would be nice to have a simple control bounding the memory usage. however, i'm not sure i agree that that config should be allowed to alter the program we've been asked to execute. if we were to add a `streams.memory.bytes`, we will also have to consider what to do if it's overconstrained. clearly, we cannot execute a streams program within 7 bytes, so we would have some validation on startup that says ""hey, you asked for no more than 7 bytes, but we need at least 800mb for this program"". rather than relaxing the suppression semantics, i'd advocate for explicit user-specified buffer sizes to be included in this arithmetic. but that is again a problem for the future. so in conclusion: i'll drop the serdes from the api, and forward from the source ktable instead. we should then be able to resume the review of this pr, right?",1,0.9942818880081177
417429229,5567,guozhangwang,2018-08-30T18:54:59Z,"yup, thanks! yeah i'm not married to my approach, so i'm all ears if you do already have some concrete ideas :)",1,0.9958305954933167
417804756,5567,bbejeck,2018-08-31T22:26:44Z,i also agree with the approach for serdes management approach.,0,0.957353413105011
418515105,5567,vvcephei,2018-09-04T20:56:44Z,", i've added the missing tests. i think this is the remainder of the comments. i'll update the kip and send out a notice now. thanks, -john",1,0.9759418964385986
418892024,5567,vvcephei,2018-09-05T21:45:55Z,"i think i've addressed all your concerns. about the system tests / performance concerns: yes, i plan to follow up with that once the features themselves are merged: 1. this pr (api only) 2. independent names for suppression nodes 3. in-memory buffering (not resilient to node shutdown/crash; spill-to-disk not implemented) 4. resilient buffering (wrap the buffer in a changelog) 5. spill-to-disk buffering 6. performance testing & optimization this was the set of smallest diffs i could think of that made sense to review. please let me know if some other subdivision makes sense. /cc",1,0.7241166830062866
419145301,5567,vvcephei,2018-09-06T15:49:38Z,thanks for the review ! i know this one was a lot of work.,1,0.9901604652404785
419982389,5567,guozhangwang,2018-09-10T16:50:25Z,i do not have further comments. lgtm.,0,0.9830526113510132
422444318,5567,vvcephei,2018-09-18T15:39:50Z,", i believe this pr is ready for a final pass. in response to your comments, i have revised the `suppressed` interface: * renaming the static/instance methods to follow our conventions * flattened the intermediatesuppression config into the top-level suppressed interface * renamed the suppressed methods to be clearer (like `untilwindowcloses` instead of `finalresults`) and also match each other (`untilwindowcloses` and `untiltimeelapses`). * added javadocs",0,0.917597234249115
422910901,5567,vvcephei,2018-09-19T18:31:56Z,"thanks so much for the detailed review. i realized that your concern about ambiguity between the configured suppression time and the grace period was actually due to a regression i introduced earlier in refactoring. apologies for not realizing this sooner. i have made a couple of changes to resolve this: * `suppressed` now only declares static factory methods (as it did initially). in particular, this ensures that we can rely on the config for `untilwindowclose` to be just as we create it in that method. it's not possible to mutate it afterwards. * i replaced the `isfinalresults` boolean in `suppressedimpl` with a `finalresultssuppressionbuilder`. previously, the code just assumed that some fields, like the suppress duration, would be null for final results `suppressed` configs. now, there is no ambiguity about that. i'm going to make a final pass over this diff myself, but otherwise, i think it's ready for (another) final review. thanks!",1,0.9030577540397644
422992786,5567,vvcephei,2018-09-19T23:39:50Z,"hey , the tests have (finally) passed. did you have any other feedback, or are we good to go?",0,0.5136205554008484
423587312,5567,vvcephei,2018-09-21T16:05:44Z,"ok , i've addressed your concerns. i'll second the request for a final review from and .",0,0.9818511009216309
423589986,5567,guozhangwang,2018-09-21T16:13:35Z,i've made a pass over the updated kip wiki and the api lgtm. one clarification question: what's the semantics of [code block] compared with [code block] the javadoc of these two functions are exactly the same?,0,0.9921970367431641
423594358,5567,vvcephei,2018-09-21T16:26:53Z,"thanks for that catch. i failed to add the extra parameter to the javadoc for the second method. the semantics are basically the same. the first one defaults to an unbounded buffer. in retrospect, this seems like an unnecessary shortcut. i think i'll just ditch the first method and document the buffer config on the second.",1,0.9487636089324951
423614660,5567,vvcephei,2018-09-21T17:32:42Z,"hmm. that java 10 build is failing for something in core, but i can't figure out what (see also [a link]",0,0.8559154272079468
423625474,5567,vvcephei,2018-09-21T18:10:15Z,update: i've submitted [a link] to fix trunk.,0,0.9821078181266785
423994933,5567,vvcephei,2018-09-24T14:29:44Z,"this pr was incompatible with the rename of `internalprocessorcontext.initialized`, so i rebased and fixed it.",0,0.9950050711631775
423995090,5567,vvcephei,2018-09-24T14:30:10Z,"i think this is ready to merge now, once the tests pass.",0,0.9731190204620361
424120339,5567,vvcephei,2018-09-24T20:50:58Z,"thanks so much for all your time reviewing, and",1,0.9792992472648621
932750960,11331,ijuma,2021-10-02T13:17:52Z,"thanks for the pr. a high-level question, what are we trying to optimize for here? 1. requests that don't include topic ids 2. requests that include topic ids 3. both 4. some kind of balance of both where we compromise a bit to keep the code maintainable",1,0.952087938785553
933821031,11331,jolshan,2021-10-04T20:15:01Z,"requests that include topic ids both some kind of balance of both where we compromise a bit to keep the code maintainable the goal of this pr is to gracefully handle the new topic case. currently in kafka, when we create a new topic, the leader and isr request is sent first, then the update metadata request. this means that we will often encounter transient ""unknown_topic_id"" errors. in the new world of topic ids, we will see this as ""unknown topic id"" errors. the current logic returns a top level error and delays all partitions. this is a regression from previous behavior, and so this pr's goal is to return to the behavior where we store the unknown partition in the session until it can be resolved. see [a link] for more information.",0,0.9672999382019043
949274575,11331,jolshan,2021-10-22T04:20:15Z,"todos: 1. ~change inconsistent topic id so it is no longer a top level error~ 2. ~maybe refactor some of the receiving side code, we have a map with topicidpartition, partitiondata and both contain topic id~ decided to hold off on this as there are still usages for topicidpartition in the receiving side. 3. maybe change fetchsession to update newly unresolved partitions to no longer include topic name.",0,0.9905299544334412
961259072,11331,dajac,2021-11-04T17:24:58Z,"it seems that there are a few compilation errors, at least for `jdk 8 and scala 2.12`. could you check?",0,0.9917089343070984
965553054,11331,jolshan,2021-11-10T17:07:04Z,"system test results: [a link] a previous run was all green, so will need to confirm the 3 failed tests are unrelated to this change.",0,0.9939980506896973
965677394,11331,jolshan,2021-11-10T19:38:05Z,looks like the topic id partition changes broke the build. i'll probably need to pull the latest version.,0,0.9776909351348877
966339841,11331,dajac,2021-11-11T14:19:24Z,have you been able to triage these failures?,0,0.9817626476287842
968676970,11331,dajac,2021-11-15T09:06:16Z,system test failures are not related. merged to trunk and to 3.1.,0,0.9893192648887634
375409892,4756,mjsax,2018-03-22T18:22:51Z,\cc,0,0.984728217124939
375974891,4756,debasishg,2018-03-25T14:32:28Z,added commit with changes for code review feedback. waiting for more feedback and some of the still unresolved questions. should i rebase now or wait till all questions resolved ?,0,0.9459773898124695
376668126,4756,mjsax,2018-03-27T20:42:00Z,"meta comment: for javadocs, so we need to set up some pipeline to get javadocs published? or will this happen automatically? additionally, this pr should include updates to the web docs in `docs/streams/...` and in ""notable changes"" in `docs/upgrade.html` ?",0,0.9947510361671448
377176774,4756,debasishg,2018-03-29T09:23:03Z,"meta comment - besides updating documentation and javadoc, is there any outstanding item in this pr that needs to be addressed ?",0,0.994281530380249
377706175,4756,seglo,2018-03-31T16:43:04Z,"for user documentation my plan is to update the following: * [a link] - update the scala example to use scala dsl * developer guide * [a link] - add a reference to the streams-scala artifact and add scala examples * [a link] - note that default key/value serdes not required for scala dsl. * [a link] - add a new section for the scala dsl. packages, examples, implicit serdes and other differences from java dsl. the bulk of the content would go here. * [a link] - reference implicit serdes from streams dsl page which includes implicit serdes explanation and usage, or vice versa. i could use some advice on how to actually go about updating the docs efficiently. the [a link] doesn't go into detail about a workflow i can use while developing docs (make a change, preview, etc). i noticed the code samples for the streams docs are formatted as html, but i couldn't find the source for these examples in the project, are they generated somehow? edit: i also added to the [a link] section, to explain how to include the dependency in maven build file, e.g. to use the scala wrapper. edit 2: i also updated the section 2.3 (streams api) to reference the new client library. edit 3: i also updated the streams upgrade guide.",0,0.9854341745376587
377755547,4756,mjsax,2018-04-01T05:29:57Z,sounds great. you are right that it is html -- just edit it directly. i have no concrete workflow suggestion. what code examples do you mean? it should all be in the html files. there is nothing in streams docs thats generated.,1,0.979144275188446
377786142,4756,seglo,2018-04-01T13:16:09Z,i'm referring to the code snippets found in pages like the dsl api. they look like they've been generated with syntax highlightling. [code block] [a link],0,0.9912830591201782
377829608,4756,guozhangwang,2018-04-02T00:41:39Z,"for the doc changes, as i mentioned before we should also add a new section in the `[a link] section, to explain how to include the dependency in maven build file, e.g. to use the scala wrapper.",0,0.9946650266647339
377863998,4756,mjsax,2018-04-02T06:14:56Z,"i see -- i guess, somebody used some special html editor... \cc -hamill might be able to shed some light... most people just use plain text editors. you can just add plain html without any syntax highlighting. cf [a link] (i was just not sure what you mean by ""generated"" because the configs html is generated from the source code directly -- cf. [a link] and [a link]",0,0.9666992425918579
378364212,4756,seglo,2018-04-03T19:14:01Z,i've implemented the user docs. however i don't know how to view the documentation in my browser. none of the html files in `docs` subdir render when i open them locally with chrome (even before i made any edits). there are likely some formatting mistakes since they're difficult to spot in the markup alone. can someone familiar with editing these docs please describe a workflow i can use to view rendered documentation locally? in the meantime i encourage people to review the content. /cc -hamill,0,0.6478700041770935
378411984,4756,guozhangwang,2018-04-03T21:54:42Z,"you can read this wiki page for render the pages locally: [a link] to do that you'd need to first copy-paste the docs to `kafka-site` repo, and start the apache server locally in order to see the difference.",0,0.9954975843429565
378682617,4756,seglo,2018-04-04T17:34:10Z,"i was able to setup a local apache2 webserver (wow, i haven't done that in 10 years!) i fixed formatting issues, typos, added the blurb to the streams upgrade guide, and other misc. feedback you provided.",1,0.9527606964111328
378730925,4756,guozhangwang,2018-04-04T20:16:22Z,thanks! could you also rebase your pr against trunk to resolve the conflicts as well?,1,0.9778314828872681
378796513,4756,seglo,2018-04-05T01:44:33Z,done!,1,0.6799882054328918
379273985,4756,debasishg,2018-04-06T14:39:48Z,"- one of the reasons we wrote the tests was to demonstrate the idiomatic usage of the scala apis. if you think we should write additional tests to verify if the correct topology is built, what kind of tests can do this verification ? is there any example test in the java apis that does this verification of building the correct topology ?",0,0.985039472579956
379284118,4756,dguy,2018-04-06T15:11:56Z,- good enough reason. for additional tests i can think of a few of ways of doing it. 1. use the `topologytestdriver` - so you can build the topology for say `mapvalues` and then run that thought the `topologytestdriver` to verify the output. 2. call `describe` on the built `topology` and verify it has the correct nodes (see `topologytest`) 3. mock/stub the the java interfaces and verify the correct interactions.,0,0.8908059000968933
379284887,4756,debasishg,2018-04-06T15:14:25Z,- thanks! we will take a look and have additional tests.,1,0.9937654733657837
379378808,4756,guozhangwang,2018-04-06T21:06:03Z,the jenkins failures seems relevant: [code block] we should exclude `logs/kafka-streams-scala.log` from rat check.,0,0.992946445941925
379628965,4756,debasishg,2018-04-09T04:38:58Z,- the kafka streams scala api is a dsl level api rather than a `processor` level one. so are u suggesting that for a specific scala api (say `kstream#mapvalues`) we write the equivalent `processor` level code and verify if the results match with the scala dsl api ? another option could be to use both the scala and the java api for `kstream#mapvalues` and verify if we get the same results. isn't the latter option better in the sense that we are comparing apis at the same level ?,0,0.9920327663421631
379758448,4756,dguy,2018-04-09T13:49:01Z,- yes comparing them would be fine.,0,0.9697664976119995
380139596,4756,debasishg,2018-04-10T15:18:56Z,- in the last commit i wrote the original test cases using the java apis in scala. this verifies if the 2 apis deliver the same result. is this what u meant ?,0,0.9923771619796753
380143884,4756,dguy,2018-04-10T15:31:01Z,"- i was thinking more along the lines of using the scala api to build simple streams topologies, i.e, `topology topology = streams.mapvalues(..).build()` and then using the `topologytestdriver` to pipe some input through the topology and verify the output is as expected. on tue, 10 apr 2018 at 16:19 debasish ghosh wrote:",0,0.9903188943862915
380151904,4756,debasishg,2018-04-10T15:52:56Z,"- may be i am missing something .. in your example `topology topology = streams.mapvalues(..).build()`, what is `streams` ? `mapvalues` is a method on `kstream` - right ? is there any example in the code base that i can refer to for similar stuff ?",0,0.9823545217514038
380154612,4756,dguy,2018-04-10T16:00:32Z,"- yes that is correct. sorry, my bad. yes you would do something like: [code block] you can have a look at [a link] for how to pipe input and verify the output etc. hth on tue, 10 apr 2018 at 16:53 debasish ghosh wrote:",-1,0.9908532500267029
380381008,4756,debasishg,2018-04-11T09:03:05Z,- i added a suite `topologytest` which verifies that the topology generated by the java and scala apis are identical. please let me know if this works. i did not use `topologytestdriver` as i did not want to execute any topology. the earlier tests verify that the results we get from the java and the scala api are identical. these tests verify that the topologies are identical as well.,0,0.6533818244934082
380382707,4756,dguy,2018-04-11T09:08:53Z,thanks - that works,1,0.8631601333618164
380383532,4756,dguy,2018-04-11T09:11:43Z,btw - did you start a voting thread for the kip yet?,0,0.9924807548522949
380384424,4756,debasishg,2018-04-11T09:14:43Z,"thanks .. regarding voting thread, no we haven't done yet. also we are not sure how to do it :-) .. how do we start a voting thread ?",1,0.9961056113243103
380392395,4756,dguy,2018-04-11T09:42:50Z,"on the dev.apache.org list you start a thread, similar to the [discuss] thread you already started, but use [vote] instead of [discuss]. the vote needs to run for at least 72 hours and needs at least 3 binding votes. on wed, 11 apr 2018 at 10:15 debasish ghosh wrote:",0,0.9939190149307251
380400070,4756,debasishg,2018-04-11T10:09:36Z,done ..,0,0.992914080619812
380672649,4756,debasishg,2018-04-12T04:14:44Z,", , - sent out the [vote] email on `kafka-dev`",0,0.9917393922805786
382386413,4756,ijuma,2018-04-18T13:29:10Z,", for some reason github doesn't let me reply inline. your ""more efficient version"" is still allocating a tuple, right? to avoid it, mapper would have to return a keyvalue.",0,0.5657455325126648
382389088,4756,debasishg,2018-04-18T13:37:43Z,"- the `map` function doesn't allocate the tuple. it takes a function of the form `(k, v) => (kr, vr)` and constructs the `keyvalue` from the tuple that `mapper` returns. wanted to abstract the construction of `keyvalue` to avoid noise. [code block]",0,0.9933839440345764
382391983,4756,ijuma,2018-04-18T13:46:42Z,"my point is that there's still a throwaway tuple for each mapped item which is the issue raised. the gc is generally good at dealing with such short lived objects, so not sure if it matters in this case, but it's clearly adding some overhead when compared to the java implementation.",0,0.932015061378479
382401057,4756,debasishg,2018-04-18T14:13:16Z,"yeah .. the point is this tuple is generated within the `map` function through the call of the `mapper`. to avoid this, the way out will be to change the `mapper` to `(k, v) => keyvalue[kr, vr]`. which can be done .. but makes the api contract a bit noisy. otoh we can rely on the jit and the ability of gc to deal with such short lived objects. suggestions welcome :-)",1,0.9734383821487427
382462315,4756,guozhangwang,2018-04-18T17:16:39Z,"maybe i was a bit paranoid on the gc pressure before, just raised it as a concern. since you mentioned `we did run some tests in bulk to check the diff in performance between the 2 versions. couldn't find much of a difference though.` i'm fine with keeping the api as elegant as of now and assume jit doing the right thing.",0,0.8731497526168823
383113457,4756,ijuma,2018-04-20T14:26:31Z,"out of curiosity, how long does it take to build (compile and run the tests) kafka-streams-scala.",0,0.976814329624176
385124698,4756,ijuma,2018-04-28T00:26:12Z,"i have been unable to reply to this pr since it has continuously failed to load for me for 1 week. so i am resorting to ""reply by email"". :) "" how are multiple versions of kafka core published at part of the release process? is the build script called twice with appropriate scalaversion parameter?"" we currently invoke it twice with -pscalaversion=2.12 for the second invocation because we haven't enabled scala 2.12 builds by default. the reason is that scala 2.12 requires java 8. once we switch our build to require java 8 (should happen soon), we will enable 2.12 by default and then a single command with the *all suffix will do it for all scala versions supported. to give an example, `test` will run the tests with the default scala version while `testall` will run the tests with all supported scala versions. for what it's worth, i strongly agree that we should do the same we do for core, i.e. publish for both scala 2.11 and scala 2.12 and include the scala version in the artifact id, as it's standard practice for scala libraries. ismael on wed, apr 25, 2018 at 1:21 pm, sean glover wrote:",1,0.9943653345108032
222823006,1446,aartigupta,2016-05-31T21:14:52Z,", what do you think? i was able to run the examples and see the metrics per node in a jmx console.",0,0.9675722122192383
222835496,1446,guozhangwang,2016-05-31T22:05:17Z,"thanks , could you take a look first at this ticket? i have assigned you as the reviewer on the ticket, and please feel free to re-assign to me otherwise.",1,0.8117787837982178
223304691,1446,enothereska,2016-06-02T14:14:46Z,"perhaps the pr name should be ""kafka-3715: add granular metrics per node""? the jira number is usually part of the pr name. minor thing but just for consistency.",0,0.985332190990448
224061436,1446,enothereska,2016-06-06T19:25:36Z,thanks . two higher level questions: does it make sense to add a unit test or two for the new metrics? and do we have any overhead measurements in the sense of how much to the new recordings add to the end to end latency?,1,0.9133504629135132
225488213,1446,aartigupta,2016-06-13T04:43:05Z,"ran org.apache.kafka.streams.perf.simplebenchmark with the following configuration (i.e. without state store backed streams and simple print statements indicating which part of the benchmark is being run) [code block] // benchmark.processstreamwithstatestore(); then attached a yourkit profiler and saw the following differences (see attached screenshots) without any changes to the code and using cpu sampling in yourkit saw 61% cpu contention with the per node metrics and using cpu sampling in yourkit, saw 70% cpu contention, without any changes org.apache.kafka.streams.perf.simplebenchmark producer producer performance [mb/sec write]: 8.853212193170378 consumer [yourkit java profiler 2016.02-b38] log file: /users/aartikumargupta/.yjp/log/simplebenchmark-1754.log consumer performance [mb/sec read]: 4.596191726854892 simple stream performance source->process streams performance [mb/sec read]: 14.361679855964493 simple stream performance source->sink streams performance [mb/sec read+write]: 4.535097423059803 with node metrics producer performance [mb/sec write]: 5.035256582778346 consumer [yourkit java profiler 2016.02-b38] log file: /users/aartikumargupta/.yjp/log/simplebenchmark-1549.log consumer performance [mb/sec read]: 2.751484036579496 simple stream performance source->process streams performance [mb/sec read]: 8.014018691588785 simple stream performance source->sink streams performance [mb/sec read+write]: 6.562667414985077 ran this multiple times and the results varied between 63%(no changes) and 72%(with per node metrics) the difference seems to be around the point at which yourkit profiler is attached that said, not sure if this is a valid load simulating scenario mentions in [a link] that is the simplebenchmark a good scenario to be profiling ? if not any suggestions on another scenario, maybe we can add (check in) such a scenario under examples, which can be used for all similar future profiling exercises still working on the unit tests for per node metrics.",0,0.9922550916671753
227377187,1446,gfodor,2016-06-21T08:41:38Z,hey it's kind of hard to tell based on your screenshots where the time is going since i don't see any drilldown into the call stacks of the streamthread run loops. it's probably necessary for you to flip things on in the yourkit profiler so you can get the full call stacks and determine if `sensor.record` is the source of most of the time.,0,0.953683078289032
227602467,1446,guozhangwang,2016-06-21T23:29:08Z,"thanks , some general comments: 1. for naming consistency as with other metrics objects, for finer grained metrics we tend to name the sensors as ""level-name.level-id.metrics-name"", for example in `sendermetrics` we used `topic.[topic-name].records-per-batch` etc for per topic-level metrics and in `selectormetrics` we used `node-[node-id].bytes-sent` etc for per node-level metrics, and in my latest pr #1530 i was doing similar naming. you may already notice that this is for creating different sensors as we synchronize at the per-sensor basis, and since in producer / consumer we always has single-thread, today we do not have any contentions for the lock yet, and in streams we are trying to add per-thread metrics and consider adding global metrics only after the syncrhonization is removed in kafka-3155 since as we have discussed in other prs with multiple threads contention overhead can be large. 2. different metrics reporter has the freedom of constructing their reporting metrics name from the hierarchy of ""metrics-prefix, group-name, metrics-name, metrics-tags"" where metrics-prefix are ""kafka.producer"" / ""kafka.consumer"" / ""kafka.streams"" depending on which client library you are using. and in this case the sensor names are actually ignored as they are used internally of the metrics object for grouping different metrics only. for example in `jmxreporter` we create the mbeanname / attributename as [code block] so we need to make sure that the hierarchy is sufficient for different reporters to differentiate these metrics in their own space.",1,0.9570897221565247
227604083,1446,guozhangwang,2016-06-21T23:39:13Z,"btw the `simplebenchmark` numbers are pretty low compared to my laptop (4gb memory, and low-end cpus). what environment did you run the profiler?",0,0.9744608402252197
228123517,1446,aartigupta,2016-06-23T17:33:13Z,"mackbook 12 inch 2015 early edition, 1.3ghz dual-core intel core m processor (turbo boost up to 2.9ghz) with 4mb shared l3 cache. 8gb of 1600mhz lpddr3 onboard memory i think that it has to do with attaching yourkit profiler. without the profiler i get the following producer producer performance [mb/sec write]: 22.247686586525987 consumer consumer performance [mb/sec read]: 56.39283169836138 simple stream performance source->process streams performance [mb/sec read]: 40.33237957119899 simple stream performance source->sink streams performance [mb/sec read+write]: 18.71113212350438 process finished with exit code 0",0,0.9798070192337036
229234020,1446,theduderog,2016-06-29T01:43:32Z,is there a way to register user-defined metrics?,0,0.9939716458320618
263255635,1446,enothereska,2016-11-28T12:10:00Z,would you still have time for this pr or should i have a look? thanks.,1,0.8498115539550781
264912720,1446,enothereska,2016-12-05T17:06:54Z,"![a link] added two levels of logging metrics, as described in kafka-3811. showing performance of pr when logging all metrics, vs just some higher level metrics in streams (see last two bars of each benchmark, the rest is details). average of 3 runs for each test shown. all data at: [a link]",1,0.8987430334091187
264913402,1446,enothereska,2016-12-05T17:09:14Z,"as part of this pr we are introducing different levels of logging for sensors, as initially described in kafka-3811 [a link] could you see if you are satisfied with the way these levels are introduced? in particular, i'd focus on everything outside of streams, while can focus on the streams part too. thanks. cc too if they have time for looking at the sensor changes. thanks.",1,0.9899613857269287
264927773,1446,enothereska,2016-12-05T18:01:45Z,(pr failures due to known reset tests),0,0.9617502689361572
264964781,1446,enothereska,2016-12-05T20:17:54Z,registering user-defined metrics will be in another jira. thanks.,1,0.9138438701629639
265074848,1446,aartiguptaa,2016-12-06T06:50:30Z,"looks good! thanks for cleaning this up and adding the missing metrics. when i took this on earlier, jay mentioned on a related jira ([a link] that the usage of metrics in streams incurs an overhead because we walk through a list of sensors inside a synchronized method. when i ran the code in the pr and the simple benchmark tests, and looked at the hrof counter recordings using yourkit profiler, i observed a 5 percent cpu overhead with my fix as compared to without it. wondering now if that was a measurement error/ test setup error? out of curiosity, can you describe your perf testing experiment. did you use simple benchmark for the perf counters? , can you share the example use case /automated test that you ran? how many nodes? how many tasks, topology , and which profiler tool, flight recorder/yourkit profiler?",1,0.9939238429069519
265104382,1446,enothereska,2016-12-06T09:39:37Z,"thanks i did use yourkit, but the final numbers i put on the pr are using simplebenchmark.java that is included with streams. that runs locally on my macbook pro 16gb, ssd, dual core. so on one node we have kafka and the simplebenchmark application. i don't think yours was a measurement error, but some metrics were missing back then.",1,0.8860141038894653
265452922,1446,enothereska,2016-12-07T13:56:27Z,test passed. weird error on jenkins: `hudson.remoting.channel:ubuntu-4: java.io.ioexception: remote call on ubuntu-4 failed`,-1,0.5058077573776245
265559198,1446,enothereska,2016-12-07T20:09:22Z,if you have time.,0,0.9097360372543335
265830060,1446,guozhangwang,2016-12-08T19:23:26Z,"about the synchronization overhead, i think jay was referring to kafka-3769 and i have fixed in some time ago (you can see the graphs before and after the fix): [a link]",0,0.9922803640365601
266758773,1446,guozhangwang,2016-12-13T14:53:47Z,"thinking about it a bit more, i now understand your arguments before: tags were not used to distinguish sensors in the sensor registry (i.e. `concurrentmap sensors`), so without the processor-node as part of the sensor name they will collapse into the same sensor, that can be potentially accessed by multiple threads. in that case, you are right that we do need to have both the processor node name as well as the task id string as part of the `nodemetrics` sensors, and have the task id string as the `taskmetrics` sensors. your current naming as the format of [code block] where `thread.thread-id` comes from the prefix in `streamsmetricsimpl`. and there is no separators between `taskid` and `processor name`, and we use the `-` separator between `processor name` and the actually sensor name. i am wondering if we could remove this prefix and refactor the rest prefixes as [code block] the reason is that tasks can migrated between threads from time to time, and when that happens some of the existing sensor will not have any data any more while some more sensors need to be created if we keep the thread id as the prefix, and for using the `.` separator it is mainly for consistency with other sensors. ditto for task metrics as well.",0,0.9735569953918457
266827021,1446,enothereska,2016-12-13T18:52:35Z,"about your latest comment, when a task migrates, when we close the topology, i remove all previous sensors. so when the task migrates it will only have new sensors. the old sensors will be gone. let me know what you think. thanks.",1,0.972105085849762
267309387,1446,enothereska,2016-12-15T11:49:09Z,will be adding unit tests shortly.,0,0.9928575754165649
267466910,1446,guozhangwang,2016-12-15T22:45:59Z,"what i meant is mainly around user experience: by having task / processor-node level metrics to be distinguished by the thread name (e.g. define sensor name as `thread.streams-thread-xxx.task.1-1.processor-xxx-processor-throughput`), users then need to enumerate all possibly created sensors in their monitoring system and / or web ui, while only one of them will be valid / having non-blank graphs at a given point of time; on the other hand, since at any given time only one thread will be owning the task / processor node, it is safe to collapse them into a single sensor that could be accessed and updated by different threads at different times, and users then can simply polling / monitoring this single sensor (e.g. `task.1-1.processor-xxx-processor-throughput`).",0,0.9887380599975586
268056393,1446,enothereska,2016-12-19T19:32:58Z,i agree with you on removing the thread prefix. will do so with next commit. thanks.,1,0.9775130152702332
268075855,1446,guozhangwang,2016-12-19T20:55:46Z,is this relevant? [code block],0,0.9954581260681152
268085496,1446,enothereska,2016-12-19T21:35:25Z,"i don't think so, looks like an environment one.",0,0.9577443599700928
268106260,1446,enothereska,2016-12-19T23:13:24Z,"![a link] updated performance graph (avg of 3 runs). a couple of takeaways: - trunk and pr have comparable performance in the ""no metrics recorded"" case. worse case difference is around 2% better for trunk in one case and 5% better for pr in another case. - both trunk and pr see a similar drop in performance when collecting some metrics (info) - notice how the pr's info metrics does better than trunk's metrics. that is because in the pr the state store metrics are now debug, so we're collecting fewer metrics by default. i'll get another graph where the pr temporarily assigns info to the store metrics, for a better apple-to-apple comparison. - worst case scenario drop in perf for debug: 60%. best case drop: 20% - worst case drop in perf for info: 11%. best case drop: 3% (compare with current trunk below). - worst case drop in perf for trunk metrics: 17%. best case drop: 9%. - not relevant for this pr, but looks like all numbers have moved up from the last graph. that's good.",1,0.8862244486808777
268108598,1446,guozhangwang,2016-12-19T23:26:24Z,"thanks for the newly updated stats . a few quick question: 0. for the comparison you mentioned above ""worst case .. best case"", they are all compared with trunk with metrics turned on right? 1. regarding `worst case drop in perf for info: 11%. best case drop: 3% (compare with current trunk below).` since you mention this pr's info metrics perf is better than trunk's metrics, i was assuming that the best case perf drop should be a negative ratio? 2. regarding `worst case drop in perf for trunk metrics: 17%. best case drop: 9%.` what does this mean? i thought for `trunk metrics` it means info + state store metrics, but it seems you have not got the results for this yet.",1,0.9785269498825073
268219056,1446,enothereska,2016-12-20T11:20:13Z,"not quite: 0. no, they are not compared with trunk, they are compared with the pr with metrics turned off. 1. again, not compared with trunk, just with no metrics. sure, if you compare with trunk you'd get the negative ratio. 2. this is trunk today, so yes, it is info + state store metrics. i'm just pointing out that their cost is between 9-17% today. compared with no metrics.",0,0.9506675601005554
268227910,1446,enothereska,2016-12-20T12:09:56Z,"![a link] updated with an additional run where we collect both info + state store metrics (just like trunk does now). overall, the main takeaway is that we're not introducing any particular overhead with the pr with an apples-to-apples comparison with trunk. at this point i'm personally happy with the numbers. thanks.",1,0.9947906732559204
268304113,1446,enothereska,2016-12-20T17:26:23Z,thanks i think i addressed your comments.,1,0.7320516109466553
270527621,1446,guozhangwang,2017-01-05T00:28:31Z,"some of the comments seem not addressed yet, and could you rebase as well?",0,0.9894989728927612
270619470,1446,enothereska,2017-01-05T11:04:34Z,"i removed the extra sensor registration apis as you suggested, and i also agree with the rest of your comments and have addressed them. thank you.",1,0.9682478308677673
270620026,1446,enothereska,2017-01-05T11:07:40Z,"i removed the extra sensor registration apis as you suggested, and i also agree with the rest of your comments and have addressed them. thank you.",1,0.9682478308677673
271103036,1446,enothereska,2017-01-07T19:04:31Z,"failures unrelated to pr, e.g., kafka.api.sslproducersendtest.testsendcompressedmessagewithcreatetime",0,0.9884053468704224
271436295,1446,guozhangwang,2017-01-09T23:04:19Z,"made another pass over the latest patch. lgtm overall. just one minor comment about testing coverage: would you consider adding some test cases for `streammetricsimpl` inside `streamthreadtest` as well, for testing is naming conventions, etc?",0,0.9706405997276306
271662859,1446,guozhangwang,2017-01-10T18:52:10Z,test this please,0,0.9870431423187256
271671814,1446,enothereska,2017-01-10T19:24:11Z,what do you mean by `test this please`? which part? thanks.,0,0.796534538269043
271675184,1446,guozhangwang,2017-01-10T19:36:40Z,[a link] :),1,0.801796555519104
271679307,1446,enothereska,2017-01-10T19:51:48Z,wow!,1,0.9457429051399231
271681953,1446,enothereska,2017-01-10T20:02:04Z,unrelated error: kafka.integration.uncleanleaderelectiontest.testuncleanleaderelectiondisabled,0,0.9886488318443298
271951164,1446,enothereska,2017-01-11T18:22:42Z,unrelated: kafka.api.sslproducersendtest.testclosewithzerotimeoutfromsenderthread,0,0.9934808015823364
271969606,1446,enothereska,2017-01-11T19:29:48Z,the old org.apache.kafka.streams.integration.resetintegrationtest failure is back but shouldn't be related to pr.,0,0.9670741558074951
271977388,1446,guozhangwang,2017-01-11T19:59:40Z,merged to trunk. many thanks to and !!,1,0.9955037236213684
395236826,5101,hzxa21,2018-06-06T22:42:10Z,"dong, i have updated the pr to address the comments. could you take a second look? thanks!",1,0.9739013910293579
406153652,5101,lindong28,2018-07-19T04:48:45Z,"hey , is this patch ready for review?",0,0.99151211977005
406507085,5101,hzxa21,2018-07-20T06:57:20Z,yes. can you help take a look?,0,0.985859215259552
406659794,5101,lindong28,2018-07-20T16:50:12Z,sure. can you rebase the patch to resolve the conflict?,0,0.9933445453643799
406738020,5101,hzxa21,2018-07-20T22:01:50Z,rebased onto trunk. thanks dong!,1,0.9956788420677185
412941942,5101,hzxa21,2018-08-14T16:55:52Z,i have updated the pr to add more logging and address the naming issues. could you take a look again? thanks!,1,0.9236109852790833
414773731,5101,hzxa21,2018-08-21T18:22:43Z,thanks for your comments. i have rebased the patch. can you take a look?,1,0.9369244575500488
415120454,5101,hzxa21,2018-08-22T17:51:15Z,thanks dong for the review. i have addressed the comments and will add more tests for the controller behavior.,1,0.973045289516449
415132003,5101,hzxa21,2018-08-22T18:26:44Z,rebased.,0,0.9845768809318542
415160017,5101,lindong28,2018-08-22T19:59:14Z,"hey , i have finished reviewing this patch except tests. i will wait for to add tests. would you like to review this patch as well?",0,0.7306746244430542
415241065,5101,junrao,2018-08-23T01:18:44Z,": another thing is that i am wondering if you have done any perf testing. we probably don't expect any slowdown with the additional controller epoch check. however, it would be useful to verify that common operations such as controlled shutdown, broker startup, leader balancing are not slower after this patch.",0,0.9758070111274719
415954168,5101,hzxa21,2018-08-25T08:48:10Z,thanks for the comments. i have added some controller integration tests for the patch and address the comments. i will do some perf testing and post the results once i get the numbers.,1,0.9682808518409729
415955473,5101,hzxa21,2018-08-25T09:12:44Z,"also, we didn't have the protection against a stale controller deleting `\controller` znode when `controllermovedexception` is thrown in `oncontrollerfailover()` during controller initialization. this can cause controller switch storm. consider the following events: 1. broker a creates /controller znode 2. broker a reads the controller epoch zkversion 3. broker a lose its zk session -> /controller node gets deleted 4. broker b creates /controller znode 5. broker b reads the controller epoch zkversion 6. broker b increments the controller epoch and bumps the zkversion of /controller_epoch znode 7. broker a tries to update zookeeper (e.g. update `/admin/reassign_partitions`) but fails because /controller_epoch zkversion mismatch. a controllermovedexception is thrown. 8. broker a removes the /controller znode (currently owned by broker b) because we didn't explicitly catch `controllermovedexception` in `elect()` and trigger a controller move in the following code block: [code block] 9. broker c creates /controller znode ... this loop may never end and the controller role will keep switching across brokers without making any progress. i have also updated the pr to include the fixes: 1. catch `controllermovedexception` in `elect()` without triggering the controller move. this makes sense because when the `controllermovedexception` is thrown, the new controller has been elected and there is no need to trigger another round of controller election. 2. guard against the `\controller` deletion by controller epoch zkversion. this ensures stale controller cannot delete `\controller` znode.",0,0.9884428381919861
416999642,5101,omkreddy,2018-08-29T15:39:20Z,looks like tests are getting stuck while shutting down the controller. observed this while running tests using [code block]. not able to reproduce if i run individual tests . looks like some race condition. [code block],0,0.889979362487793
417161498,5101,hzxa21,2018-08-30T01:40:34Z,"thanks for the comments. there is indeed a race condition between clearing the queue when handling `controllermovedexception` and closing controllereventmanager. because we use a special event to interrupt and shutdown the controller event thread, if `controllermovedexception` happens after we put the shutdown event into the queue and before event thread picks it up, the shutdown event get cleared when handling `controllermovedexception`. to resolve it, i added the `isshuttingdown` flag to ensure that the event queue will not be cleared if the shutdown event has been put into the queue.",1,0.9556483030319214
417161614,5101,hzxa21,2018-08-30T01:41:27Z,thanks a lot for the review and i really appreciated the comments. i have updated the pr to address the issues. could you take a look again?,1,0.9932750463485718
417195949,5101,lindong28,2018-08-30T05:38:21Z,"it seems that if controller receives controllermovedexception, the controller should not simply remove all events from the controller queue. events such as `reelect` and `controlledshutdown` probably should stay in the queue as these events need to be processed even if the broker is not controller. if we do this, we can solve the race condition without having the additional `isshuttingdown`. and we can also make the change that jun suggested previously without worrying about `reelect` event being removed from the controller event queue. what do you think?",0,0.975830614566803
417217971,5101,hzxa21,2018-08-30T07:27:55Z,"thanks for the comment. you brought up a very good point. we need to differentiate between events that need to be processed by the active controller and events that need to be processed by every broker. because `controllermovedexception` only indicates active controller role switched, we shouldn't try to stop processing events in the latter category. from the implementation point of view, we can just mark the controller as inactive and resign without touching the event queue at all instead of doing a conditional clear operation on the queue. the reason why we want to clear the queue before is that if controller has moved, we don't want the event thread to waste cycles on unnecessary events and want it to get to `reelect` as soon as possible by clearing the potential backlog. since we already have logic like `if (!isactive) return` to guard against events related to active controller, we just need to preempt the ""mark inactive and resign"" operation when `controllermovedexception` happens. this will ensure correctness as well as simplicity and resolve the event queue backlog with small overhead (most cases we just dequeue controller event and do a boolean check)",1,0.9875262379646301
417919248,5101,hzxa21,2018-09-02T10:17:22Z,thanks for the review. i have updated the pr to address your comments. appreciated if you can take a look again when available.,1,0.9813151955604553
418243421,5101,hzxa21,2018-09-04T05:14:40Z,"perf testing has finished. overall, there is no significant overhead after fencing zookeeper updates for common controller events (controllerfailover, controlledshutdown, brokerstartup, preferredreplicaleaderelection). the environment: - 5 node zookeeper and 5 broker kafka cluster with brokers on different racks - 2,000 topics each with 50 partitions and rf = 1 - 10k single partition topics with rf=1 + 10k 3 partitions topics with rf=2 here are the results: **1. controller fails over** trunk (`bf0675`) - run 1 [code block] trunk (`bf0675`) - run 2 [code block] kafka-6082 - run 1 [code block] kafka-6082 - run 2 [code block] trunk avg = ~11s kafka-6082 avg = ~9s **2. preferred replica leader election** trunk (`bf0675`) - ~2.4k leadership movements [code block] kafka-6082 - ~2.4k leadership movements [code block] trunk = 491ms kafka-6082 = 544ms trunk (`bf0675`) - ~4.8k leadership movements [code block] kafka-6082 - broker 1497 (~4.8k leader) [code block] trunk = 1.104s kafka-6082 = 1.084s trunk (`bf0675`) - ~6k leadership movements [code block] kafka-6082 - broker 1494 (~6k leadership movements) [code block] trunk = 1.339s kafka-6082 = 1.152s **3. controlled shutdown** trunk (`bf0675`) - run 1 [code block] trunk (`bf0675`) - run 2 [code block] kafka-6082 - run 1 [code block] kafka-6082 - run 2 [code block] trunk avg = 3.63s kafka-6082 avg = 3.65s **4. broker start** trunk (`bf0675`) - run 1 [code block] trunk (`bf0675`) - run 2 [code block] kafka-6082 - run 1 [code block] kafka-6082 - run 2 [code block] trunk avg = 4.67s kafka-6082 avg = 2.21s",0,0.980774462223053
418570274,5101,junrao,2018-09-05T01:34:03Z,: thanks for the perf results. they look good.,1,0.992179274559021
419242869,5101,hzxa21,2018-09-06T21:12:07Z,thanks so much for all of your comments and suggestions.,1,0.9661015868186951
419568907,5101,lindong28,2018-09-07T21:20:50Z,thanks much for the patch . lgtm. merged to trunk.,1,0.9799053072929382
419572127,5101,hzxa21,2018-09-07T21:35:37Z,thanks a lot for the reviews.,1,0.6895106434822083
419750329,5101,ijuma,2018-09-09T22:41:16Z,"did we check the performance impact of this change? if so, it would be great to include the details in the pr description.",1,0.8407052159309387
420325390,5101,junrao,2018-09-11T15:58:44Z,: there were some perf results. see the above comment on sep. 4.,0,0.9891608357429504
420347877,5101,ijuma,2018-09-11T17:07:40Z,thanks . that's great. i suggest adding a summary of the results to the pr description.,1,0.9931208491325378
420371333,5101,hzxa21,2018-09-11T18:21:21Z,thanks for the suggestion. i have updated the description to include the perf test.,1,0.9460740685462952
472781135,6295,enothereska,2019-03-14T09:57:32Z,- checked new topic creation in remote cluster - checked acl syncing the code looks good and ideally we get some tests for the above as well. as part of the above check i'm wondering if it makes sense to add an integration test plan to the kip (apologies if i missed) since some of this functionality needs full e2e testing.,1,0.6832236051559448
493280895,6295,ryannedolan,2019-05-17T00:58:35Z,thanks for taking a look. i could see min.insync.replicas causing problems if sync'd. i'll add min.insync.replicas to config.properties.blacklist by default to avoid that problem.,1,0.9730926156044006
498269358,6295,dvirgiln,2019-06-03T14:01:23Z,"any idea when this pr will be merged? we are thinking to use mirrormaker, but depending when this pr is merged, we could wait for it instead of using the current mirrormaker v1.0. thanks",1,0.9805296659469604
498439837,6295,ryannedolan,2019-06-03T21:50:50Z,thanks for your interest . current plan is with the 2.4 release. the best way to make sure this happens is to contribute reviews!,1,0.9840540289878845
500217111,6295,williamhammond,2019-06-09T14:40:34Z,does it make sense to mention this in the kip or document it elsewhere? this seems important enough that it ought to jump out at people the first time they deploy mm2,0,0.9885311126708984
503503275,6295,enothereska,2019-06-19T10:28:15Z,did some more testing and happy to see the progress from the last comments. there are some usability issues still but we can probably address them in a separate round. at this point it would be good if committers had a look and reviewed. thanks.,1,0.9939897060394287
505123732,6295,jeremy-l-ford,2019-06-24T18:22:13Z,"i have been testing with this for a while and messages are being copied. however, i noticed that record headers are not copying correctly. it appears that the record header data received by the mirrored broker data is base64 encoded. by default, connect uses simpleheaderconverter. i noticed that the mirrormakerconfig is not setting a header converter. i think it should default the header converter to the byte array converter just like the key and value converters. clients: kafka 0.11 based clients. brokers: 2.1 mm2.0 - latest branch manually defaulting the convert via configuration resolves the issue.",0,0.8931597471237183
506736259,6295,vpernin,2019-06-28T13:35:28Z,"what would be the proper way to monitor each replication lag with this new architecture, the offsets of upstreams topics being stored in the kafka backing store topic of kafka connect ?",0,0.9930040240287781
506793879,6295,ryannedolan,2019-06-28T16:22:43Z,"this is an interesting question. the kip does not emit offset lag metrics -- only ""replication latency"", which is a measure of time. offset lag is measured from the perspective of a particular consumer, and it would be possible to supply an interceptor to mm2's consumers in order to get this measure, if you like. looking at the connect offsets is an interesting idea as well. that said, i think latency is a better metric to monitor wrt replication, as there is no good way to aggregate offset lag over a bunch of unrelated topic-partitions. offset lag is more meaningful when looking at a particular consumer and a particular topic.",1,0.6588729023933411
517308971,6295,realradical,2019-08-01T14:21:17Z,"hey, i use kubernettes to spin up 2 kafka clusters locally (3 brokers each). and then i run mm2 locally as well to sync topic messages. when i send a message to source topic, source.topic is created in the sink cluster, but the message is not delivered. an exception is thrown in the console (see below). when i restart mm2, the message arrives in the source.topic. does anyone recognize this error? moreover, when i move one of the kafka cluster to a different machine, everything works again. i tried to increase network/io threads in the local setup, it still doesn't solve the issue. [2019-08-01 10:18:29,033] info workersourcetask{id=mirrorsourceconnector-0} flushing 21 outstanding messages for offset commit (org.apache.kafka.connect.runtime.workersourcetask:418) [2019-08-01 10:18:29,072] info workersourcetask{id=mirrorsourceconnector-0} finished commitoffsets successfully in 39 ms (org.apache.kafka.connect.runtime.workersourcetask:500) [2019-08-01 10:18:29,072] error workersourcetask{id=mirrorsourceconnector-0} task threw an uncaught and unrecoverable exception (org.apache.kafka.connect.runtime.workertask:179) java.lang.nullpointerexception at org.apache.kafka.connect.mirror.mirrorsourcetask.poll(mirrorsourcetask.java:140) at org.apache.kafka.connect.runtime.workersourcetask.poll(workersourcetask.java:245) at org.apache.kafka.connect.runtime.workersourcetask.execute(workersourcetask.java:221) at org.apache.kafka.connect.runtime.workertask.dorun(workertask.java:177) at org.apache.kafka.connect.runtime.workertask.run(workertask.java:227) at java.base/java.util.concurrent.executors$runnableadapter.call(executors.java:515) at java.base/java.util.concurrent.futuretask.run(futuretask.java:264) at java.base/java.util.concurrent.threadpoolexecutor.runworker(threadpoolexecutor.java:1128) at java.base/java.util.concurrent.threadpoolexecutor$worker.run(threadpoolexecutor.java:628) at java.base/java.lang.thread.run(thread.java:834) [2019-08-01 10:18:29,073] error workersourcetask{id=mirrorsourceconnector-0} task is being killed and will not recover until manually restarted (org.apache.kafka.connect.runtime.workertask:180) [2019-08-01 10:18:29,073] info [producer clientid=connector-producer-mirrorsourceconnector-0] closing the kafka producer with timeoutmillis = 30000 ms. (org.apache.kafka.clients.producer.kafkaproducer:1153) [2019-08-01 10:18:29,080] info [producer clientid=producer-7] closing the kafka producer with timeoutmillis = 9223372036854775807 ms. (org.apache.kafka.clients.producer.kafkaproducer:1153) [2019-08-01 10:18:44,602] info [worker clientid=connect-2, groupid=sec-mm2] attempt to heartbeat failed since coordinator localhost:31000 (id: 2147483647 rack: null) is either not started or not valid. (org.apache.kafka.clients.consumer.internals.abstractcoordinator:931) [2019-08-01 10:18:44,602] info [worker clientid=connect-2, groupid=sec-mm2] group coordinator localhost:31000 (id: 2147483647 rack: null) is unavailable or invalid, will attempt rediscovery (org.apache.kafka.clients.consumer.internals.abstractcoordinator:780) [2019-08-01 10:18:44,612] info [worker clientid=connect-2, groupid=sec-mm2] discovered group coordinator localhost:31002 (id: 2147483645 rack: null) (org.apache.kafka.clients.consumer.internals.abstractcoordinator:728)",0,0.9416871070861816
519080763,6295,mimaison,2019-08-07T12:46:51Z,"upon startup, i'm also getting this exception: [code block]",0,0.9788455963134766
520463091,6295,mimaison,2019-08-12T15:03:37Z,i can replicate clusters when running as dedicated mirrormaker cluster but i'm having issues running on top of kafka connect (no errors but topics are not replicated). is that supposed to work at the moment? imho kafka connect support is key. a rest api to manage connectors is much better than having to start processes on remote hosts,1,0.49623584747314453
520863798,6295,mimaison,2019-08-13T14:40:51Z,"there also seem to be an issue with metrics. after starting mm2 in dedicated mirrormaker cluster mode, i only have the following metrics registered: according to [a link], there should also be `mirrorcheckpointconnector`. looking at different values, i can strange behaviours to like `record-count` going up and down even with no traffic!",-1,0.8784656524658203
522074048,6295,ryannedolan,2019-08-16T16:42:12Z,"the connectors will work with connect just fine, but the configuration properties differ slightly from the mm2.properties file. try something like: [code block] n.b. properties like ""primary->backup.topics"" are understood by the mm2 driver but not the underlying connectors. here's a working configuration as returned from the connect rest api during mirrorconnectorsintegrationtest (new!): [code block] most of those properties are redundant, but `mirrormakerconfig.connectorbaseconfig()` fills them in anyway.",0,0.9868060350418091
522078873,6295,ryannedolan,2019-08-16T16:57:37Z,"mirrorcheckpointconnector won't emit any metrics unless it is actively producing checkpoints, which won't happen until there is a whitelisted consumer group subscribed to a whitelisted topic. by default kafka-console-consumer groups are blacklisted, so you need to set a specific --group-id to see this happen. record-count is windowed, so it resets after a short while. might make sense to change that. also, even if there is no data to replicate, mirrorsourceconnector always replicates heartbeats, so there is always _some_ replication happening.",0,0.9905614256858826
525802843,6295,mimaison,2019-08-28T15:41:44Z,"with 2.4.0 approching, do you think the pr is ready to be considered for that release? or do you still have work planned? if it's ready, let's grab the attention of committers asap",0,0.9462067484855652
526404615,6295,ryannedolan,2019-08-30T00:05:26Z,"yes, we should get some committers to take a look.",0,0.9868423342704773
531890077,6295,harshach,2019-09-16T18:03:47Z,"thanks, for the contribution and patience in getting reviewed and thanks to everyone for reviewing the code. i went through the code a few times to get a better understanding, overall lgtm. i don't have any further comments on the code itself. i am running a few tests on our clusters and will report back my findings.",1,0.9885450005531311
533214612,6295,harshach,2019-09-19T16:43:35Z,thanks for your patience and following upon the pr do you want to take a look. we have 4 reviewers approved already. so let us know your thoughts otherwise i would like to merge this in.,1,0.9767003655433655
533322760,6295,junrao,2019-09-19T21:51:55Z,: i will make another pass of the pr by early next week.,0,0.9856007695198059
533557824,6295,omkreddy,2019-09-20T13:39:31Z,"since this is a major feature, we need to add few system tests. it is good have system tests before code freeze. for tracking purpose, can you please create jiras for system tests and docs?",1,0.9002697467803955
533628482,6295,ryannedolan,2019-09-20T16:45:12Z,"i've created kafka-8929 and kafka-8930 to track tests and docs. i will update the javadocs as part of this pr, but i'll create a separate pr for the system tests. should be done next week.",0,0.945153534412384
535781371,6295,junrao,2019-09-27T04:33:06Z,": also, the kip includes both a mirrorsourceconnector and a mirrorsinkconnector, but the pr only has the former?",0,0.9880004525184631
538051233,6295,ryannedolan,2019-10-03T17:44:33Z,"i've got ducktape tests implemented and will create a second pr for those, which we can merge before 2.4 code freeze. please note pr 6295 (this one) includes mirrorconnectorsintegrationtest, which uses connect's integration test framework. in particular, these cover replication, topic detection, task rebalances, offset translation, and consumer migration. the ducktape tests cover basic replication and bouncing the cluster. basically they are the same ducktape tests from legacy mirrormaker, just with mm2 dropped in. mirrorsinkconnector (the missing fourth connector) will come in a subsequent pr. the sink connector is not necessary for the driver to function so i've deprioritized it for this pr. there will be very little new code for the sink connector, as almost everything will be extracted from mirrorsourceconnector. also missing from the kip is the ""legacy mode"" script, which will eventually replace the existing mirror-maker.sh. both should land in 2.5, with legacy mm being deprecated as soon as 2.6. looks like some churn on master has caused a few failed builds, but once green this is ready to merge.",0,0.9677455425262451
538107371,6295,harshach,2019-10-03T20:07:30Z,looks like the deadline to make the 2.4 release is on friday. can you please take a look at the pr and see if there is anything else missing in merging it in for 2.4. cc,0,0.9811291098594666
538144648,6295,junrao,2019-10-03T21:55:41Z,: there are still a couple of comments that haven't been addressed. the biggest one is on dealing with compacted topic for offset translation.,0,0.9816992282867432
538159605,6295,ryannedolan,2019-10-03T22:53:17Z,"sorry, i didn't see a few comments that were folded/hidden for some reason. hopefully i've addressed everything that would otherwise delay the merge.",-1,0.9784316420555115
538609224,6295,omkreddy,2019-10-05T02:36:22Z,any update on test failures? please merge the pr once the jun's comments are addressed and we have green builds.,0,0.9949167966842651
538614075,6295,ryannedolan,2019-10-05T04:04:16Z,i traced the build failures to an npe from kip-507 committed yesterday. it is breaking mm2's and other connect integration tests. i'll fix here i guess.,0,0.9759320020675659
538614836,6295,ryannedolan,2019-10-05T04:18:13Z,i fixed the npe from kip-507 -- let's see if we can get a green build now.,0,0.9857544898986816
538645845,6295,ryannedolan,2019-10-05T12:32:24Z,good to go!,1,0.9939682483673096
538650932,6295,rhauch,2019-10-05T13:42:25Z,", , : as mentioned above in one of the comments on `workersourcetask`, this pr also implements [a link]. i've edited the description of the pr to mention this. according to the [a link], kip-415 has not yet been approved, and per the [a link] the kip deadline was on sept 25. , what's required here?",0,0.9753021597862244
538669326,6295,junrao,2019-10-05T17:10:40Z,": we can fold the changes in workersourcetask into this kip. ryan, could you update your kip wiki and send an email to the voting thread about the additional changes to make sure that no one objects to this? if kip-416 is completely subsumed by this, we can just cancel it. otherwise, kip-416 can cover the remaining part.",0,0.9905253648757935
538671064,6295,rhauch,2019-10-05T17:30:37Z,"thanks, . kip-416 is straightforward, limited to the one new method change we discussed (and agreed to), and required for kip-382, so pulling those changes into kip-382 makes sense to me.",1,0.9638603925704956
538830899,6295,harshach,2019-10-07T03:58:35Z,"some of the comments mentioned here can be handled in a follow-up patch and can be part of minor release that will follow-up. given the no.of users that are interested and the number of reviews, we had it in the pr and also maintaining this big of patch as the trunk continues to evolve will be challenging. if you don't have any major concerns lets merge this in for 2.4 release and address any new comments in follow-up patch. cc",0,0.9614855051040649
538890813,6295,omkreddy,2019-10-07T08:21:47Z,"thanks for the pr. merging the pr. lets address any issues in follow-up prs. pls raise jiras for any pending work (mirrorsinkconnector, legacy mode etc.) for next releases.",1,0.9676076769828796
603995709,6295,bpux,2020-03-25T18:02:27Z,try to find jira of mirrorsinkconnector but search return nothing. can you please point me where i can find status of this work? thanks!,1,0.931495726108551
603998987,6295,ewencp,2020-03-25T18:08:29Z,[a link] the primary jira for kips are linked from the kip document itself: [a link],0,0.994972825050354
604003001,6295,bpux,2020-03-25T18:15:44Z,thanks! will comment in there,1,0.9908864498138428
620318837,6295,michael-trelinski,2020-04-28T01:13:39Z,"hi - i just got done setting up mirror maker v2 where i work. it was successful, but the documentation doesn't match the code in various places. a couple of things that ""got me"" - - the jmx metrics in the documentation: it should actually be something like (pardon me, i'm using the jmx-prometheus exporter): `kafka.connect.mirror `. note how it's actually **kafka.connect.mirror** not **kafka.mirror.connect**, also it's mirrorsourceconnector, not mirrorsourceconnect. - in the documentation, there is a part about blacklisting groups: in the code, it is `public static final string groups_blacklist_default = ""console-consumer-.*, connect-.*, __.*"";`; here: [a link] - this got me because we're actually using some kafka connect stuff for other projects, but we're running mirror maker as it's own cluster. maybe you could blacklist only mirror maker's kafka connect connect- consumer group? - this one is kind of a nit-pick, but in remoteclusterutils.translateoffsets, the timeout parameter isn't really a timeout in the conventional sense. stepping further into the call that's made using timeout, `return client.remoteconsumeroffsets(consumergroupid, remoteclusteralias, timeout);` we can see that it's being used as a deadline, not a timeout. in mirrorclient's remoteconsumeroffsets method, it's being used as a timeout, and as a deadline. setting this too low caused me to miss some offsets because i didn't understand how it was being used. setting this absurdly high got me the results that i wanted. all in all a great product and vastly superior to mirror maker v1. thanks!",1,0.6921836137771606
1004211004,6295,Migueljfs,2022-01-03T16:33:16Z,"i can't seem to find the new metrics mentioned in the kip. like -trelinski said above, i've tried both kafka.connect.mirror / kafka.mirror.connect and mirrorsourceconnector / mirrorsourceconnect and all combinations of them but still i don't see the mirror domain the respective mbeans. i installed jmxterm on my mirror-maker pod and this is what i got: domains: [code block] beans for connect: [code block] as you can see, no mentions of ""mirror"" of any kind anywhere. anyone able to help me understand what's going on?",0,0.8413029313087463
736700777,9485,ctan888,2020-12-01T17:25:49Z,"build successful in 18s 122 actionable tasks: 3 executed, 119 up-to-date docker exec ducker01 bash -c ""cd /opt/kafka-dev && ducktape --cluster-file /opt/kafka-dev/tests/docker/build/cluster.json ./tests/kafkatest/ --subset 0 --subsets 15"" traceback (most recent call last): file ""/usr/local/bin/ducktape"", line 8, in sys.exit(main()) file ""/usr/local/lib/python3.7/dist-packages/ducktape/command_line/main.py"", line 118, in main os.makedirs(consoledefaults.metadata_dir) file ""/usr/lib/python3.7/os.py"", line 211, in makedirs makedirs(head, exist_ok=exist_ok) file ""/usr/lib/python3.7/os.py"", line 221, in makedirs mkdir(name, mode) permissionerror: [errno 13] permission denied: '.ducktape' ducker-ak test failed thanks for the review . i've addressed all the previous comments. can you take another look at this pr? also, any idea how to resolve this ducktape permissionerror?",0,0.98056960105896
736701573,9485,ctan888,2020-12-01T17:27:09Z,6206tests1failures61ignored6m45.08sduration | 6206tests | 1failures | 61ignored | 6m45.08sduration | 99%successful -- | -- | -- | -- | -- | -- 6206tests | 1failures | 61ignored | 6m45.08sduration failed tests ignored tests packages classes kafkaproducertest. testinittransactiontimeout which is flaky,0,0.7977332472801208
738942757,9485,ctan888,2020-12-04T18:24:17Z,"make a new aggregated index called resourceindex, which is a combination of ace, resourcetype, and patterntype. new benchmark looks much better: [a link] when `aclcount=100` and `resourcecount=200000`, in the worst case where every ""allow resource"" of a given ace has a dominant ""deny resource"" and the ""deny resource"" name is allow_string.substring(0, len-1), the time cost is 45.897  4.748 ms/op",0,0.9902742505073547
740036778,9485,ctan888,2020-12-07T16:43:09Z,"[a link] benchmark with aclauthorizer::updatecache with aclcount=50 and resourcecount=200000, 200000 call to updatecache took ~59 seconds the cpu overhead is mainly on re-hashing (everytime when the resourcecache hits 75% of the capacity). but we can overlook it. the memory overhead is m = (aclcount * 8) * (resourcecount * 8) + unique(resourcecount) * (average_resource_name_length) bytes, since strings are cached in the constant string pool. when aclcount=50 and resourcecount=200000, m < 20mb.",0,0.9904513359069824
740056125,9485,ctan888,2020-12-07T17:14:45Z,"hi . thanks for the detailed review! ive addressed all comments you left. also, after some optimization on the `resourcecache`, the benchmark looks much better now. please take another look and let me know if there's anything we need to change. thanks.",1,0.9954738020896912
740199487,9485,ctan888,2020-12-07T21:47:08Z,benchmark results run against trunk: [a link],0,0.9955599308013916
740501882,9485,ctan888,2020-12-08T09:32:43Z,"![a link] chart description (from left to right) 1. the performance comparison btw the trunk's and my branch's of `aclauthorizer#updatecache`. 2. the performance comparison btw the trunk's and my branch's of `aclauthorizer#authorize`. 3. the performance comparison btw my branch's `aclauthorizer#authorize` and `aclauthorizer#authorizebyresourcetype` 1 and 2 indicate that the newly added `resourcecache` doesn't add too much overhead to the time complexity. the time complexity increasing is constant (my branch doubled the hashmap add/remove operation). in chart 3, the horizontal axis ""percentage of dominant deny"" means the percentage of ""allow resource"" which has a ""deny resource"" added to the authorizer. the benchmark used in chart 3 has the ""dominant deny"" distributed evenly. define the average of resource name length is l, the expected ""allow resource"" in the ""allow resource set"" to iterate is e_a, the time complexity of my implementation of `authorizebyresourcetype` is l * e_a. how to compute e_a? define the ""percentage of dominant deny"" as p. every resource in the ""allow resource"" set has the probability (100-p)/100 to be allowed by the authorizer. so expected_elements_iterated_in_the_allow_resource_set. 1. the time complexity of `aclauthorizer#authorize` is irrelevant to the ""percentage of dominant deny"" 2. if we treat l as a constant, the time complexity of `aclauthorizer#authorizebyresourcetype` is only relevant to e_a, where e_a = 1/[(100-p)/100] = 100/(100-p), which is a hyperbola. chart 3 can confirm this. ![a link] from the benchmark, when p = 99.99, if ""dominant deny"" distributed evenly, authorizebyresourcetype only took 2.5ms. in the worst case, if ""dominant deny"" distributed unevenly or p = 100, the api will take ~40ms, but this is very unlikely.",0,0.8607653975486755
743884225,9485,ctan888,2020-12-12T21:17:45Z,"thanks for another round of detailed review. i've 1. add the corner case checks in aclauthorizer when there's no deny acl binding in aclauthorizer. 2. re-write the aclwrapper logic to use the `baseauthorizer` to give the information about if we should ""allow everyone if no acl found"" in the configure() method. 3. fix the ""candenyall"" logic in aclwrapper. 4. revert the unnecessary changes in the tests and benchmark. there're a few outstanding comments that need further discussion as i replied, mostly the nit changes. please let me know what you think about those. thanks.",1,0.9768021702766418
744493191,9485,ctan888,2020-12-14T14:53:25Z,"shouldupgradefromeosalphatoeosbeta is flaky. other than this, all tests passed.",0,0.9707648158073425
745712214,9485,ctan888,2020-12-16T01:55:24Z,"thanks, for the review. according to the comments, i've 1. support super user in authorizer, aclauthorizer, and authorizerwrapper. add tests to the three corresponding testing classes. 2. rename resourceindex to resourcetypekey, and make it an inner class of aclauthorizer. 3. cleaned the tear-off logic in the unit tests since zk starts in setup() indeed. 4. addressed other nit suggestions. please let me know if anything else needs to be changed, especially if the super user part looks good to you. thank you.",1,0.9897631406784058
747116007,9485,ctan888,2020-12-17T00:10:03Z,thanks for the nit and test structure suggestions. i've adopted those and re-struct the test classes. please let me know if we are good to merge now.,1,0.963813304901123
747881257,9485,ctan888,2020-12-18T05:43:47Z,thanks for going through the pr once again. i really appreciate your time spent on this work. i've addressed all the new comments. please let me know if we're good to go after the pr built.,1,0.9911417961120605
748190567,9485,rajinisivaram,2020-12-18T16:30:50Z,"thanks for your patience with this pr, builds look good, merging to trunk.",1,0.9889973402023315
748242438,9485,ctan888,2020-12-18T18:19:53Z,thank you,1,0.9420937299728394
756982595,9485,ijuma,2021-01-08T20:26:48Z,"i don't see any updates to the release notes. unless i missed it, we should add one since authorizers should implement the new method we introduced.",0,0.9729089736938477
756985555,9485,ctan888,2021-01-08T20:34:05Z,sure. could someone point me to the release note page so i can edit it?,0,0.9853901863098145
756987814,9485,ijuma,2021-01-08T20:40:01Z,there you go: [a link],0,0.9638202786445618
247191884,1776,junrao,2016-09-14T23:51:01Z,"i left the following comment on simplerate earlier. having two different rates is going to make it harder for developers to decide which one to use. if this is strictly better than rate, perhaps we should just change rate.windowsize(). if this is just for testing, perhaps we can create simplerate in test?",0,0.9833690524101257
247520427,1776,junrao,2016-09-16T05:20:06Z,thanks for the patch. lgtm. we can address the remaining minor issues in a followup jira.,1,0.9660866856575012
1789830091,14690,kirktrue,2023-11-01T23:29:51Z,"i am a bear of very little brain, so the chained `future` mechanism is simultaneously interesting, hard to follow, but ultimately useful. my one concern is that we don't allow those `future`s to be `complete`d by the application thread, since we don't want it to run all the chained logic. that would be bad :grinning_squinting_face:",1,0.6222511529922485
1793014796,14690,philipnee,2023-11-03T19:52:20Z,"hi and - thank you for putting this pr out, much appreciate for the effort. i have concern about the extensive use of callback here because it is kind of against the original design goal, i.e. making background thread operate in a linear fashion. with the main thread involved, i'm also seeing the risk of multithreading access to the background thread. here are my questions: do we need to use the completable future to facilitate the rebalance callback completion cycle? my original thought was to use queues to relay the callback invocation and completion, because i see that in some cases, it is unclear to me which thread is completing which callback. to avoid the confusion all together, i think we should try to use the queue as much as possible. also, it make the program to operate more linearly. if we don't use completable future for the callback, we just need to do two things 1. if the listen ispresent, we just need to enqueue an event to the backgroundeventqueue and wait for the consumer to poll. once the main thread completes the callback, it enqueues an ack event for background thread to consume, to transition to the next state. 2. if the listener is not presented. then we directly invoke the next state transition method. also, it seems like `reconciliationresult.whencomplete((result, error) -> {` this is completed by the application thread no? i think we really want the background thread to perform in a single-threading fashion, i'm afraid using callback might result in a some sort of race condition.",1,0.9814314842224121
1793496177,14690,AndrewJSchofield,2023-11-04T17:01:19Z,"hi , thanks for you comment. it does raise an interesting point. my view is that completablefuture provides a really nice abstraction for asynchronous processing. using it in this pr is entirely appropriate. however, i also take your overall point. the threading for completablefuture depends upon the details of how it is used. if a future is completed on the ""wrong"" thread, then processing which is dependent upon completion of that future will also execute on the same ""wrong"" thread. if one of the more complex methods such as `supplyasync` is used, it runs on a thread from a worker pool. i don't think we're using that pattern here. if we actually need to achieve signalling back to a specific thread, we could use the cf to enqueue an event. that compromise seems pretty sensible to me, but i'm not sure whether we need this with the pr as written. i need to re-read the code before i'll be certain.",1,0.9876530170440674
1793538245,14690,philipnee,2023-11-04T19:41:57Z,"hi - thank you for your inputs here, your point is absolutely valid in terms of the abstraction it provides. my main concern is the rebalance callback invocation. maybe i misunderstood the pr, but i think the state transition in the whencomplete can be completed by the main thread if the user supplies a callback, unless the callback is not supplied. i will double-check the syntax. could you be more specific about `using cf to enqueue an event`? do you mean by enqueuing a callback completion signal to some background thread queue, to notify the thread to perform the subsequent action?",1,0.9730520844459534
1793877944,14690,AndrewJSchofield,2023-11-05T23:15:02Z,i meant that the code which runs in the `completablefuture.whencomplete()` could enqueue an action for the background thread to ensure that the main logic all runs in the background thread.,0,0.9918556213378906
1793892733,14690,philipnee,2023-11-06T00:08:09Z,"- thanks, i think that's what i have in mind as well.",1,0.9250828623771667
1794951522,14690,lianetm,2023-11-06T14:26:47Z,"comment regarding the async reconciliation process and callback execution. i get your concerns , but this pr does not include callback execution for now, the membership manager triggers the callbacks and needs to know when they complete. i expect that will follow in a separate pr based on events as mentioned. without having gotten into the implementation details yet, i expect that the membershipmanager (background thread) will enqueue an app event to execute the callbacks in the app thread, and when that app thread completes the callbacks, it will enqueue a background event to notify the manager about the completion (btw, nice here again that according to the protocol there will be only one assignment being reconciled at a time ). the async nature of the membership manager in this pr it is not only due to the callbacks. the reconciliation process handles 3 main async operations: metadata, commit, callbacks (triggering and notification when completed, no execution). by basing the them on completable futures we ensure that the background thread continues it operations while there is a reconciliation in process. note on the reconciliation not being time bounded on the client. the reasoning is that the time boundaries are set by the broker, that keeps a rebalance timer. if the reconciliation takes longer than allowed (ex. stuck in any of the 3 async operations), the expectation is that the broker will re-assign the partitions and kick the member out of the group. so from the client side we just care about triggering the async reconciliation, making sure we keep sending hb and processing responses (will let us know if kicked-out), and make sure that when reconciliation completes we check if it is still relevant.",1,0.7602457404136658
1818883304,14690,dajac,2023-11-20T11:35:43Z,i just merged trunk to fix conflicts. we can merge it when the build completes.,0,0.9939727187156677
1830640924,14690,lianetm,2023-11-28T20:07:52Z,follow-up pr [a link],0,0.9959075450897217
276160087,2466,eliaslevy,2017-01-30T19:14:01Z,:thumbs_up:,1,0.9533231854438782
276214966,2466,mjsax,2017-01-30T22:39:04Z,one more thing. please add some tests!,0,0.9919323325157166
276293619,2466,mjsax,2017-01-31T07:29:54Z,"i don't see a good reason why you ""wanted to get rid of"" this line... just leave it, as i suggested. furthermore, semantically the timestamp extractor does not belong to the context, and imho it would not be a good design/abstraction adding it their.",0,0.8960798978805542
276335874,2466,jeyhunkarimov,2017-01-31T11:08:39Z,sorry it is my bad. i realized this short after writing comment and removed comment immediately.,-1,0.9933009147644043
276491890,2466,jeyhunkarimov,2017-01-31T21:05:49Z,please review,0,0.9914347529411316
277006630,2466,jeyhunkarimov,2017-02-02T16:28:00Z,"- i added extra test for `kstreambuilder.addsource` with `timestampextract` arguments. not sure it is much different from `topologybuilder.addsource` test. - in `sourcenode` class, not all variables can be final as they are initialized in `init()` method. - in `streamtask` 121, the `sourcenode` was getting `null` if the source is defined with `pattern`. i deleted ""pattern [ ]"" string from `sourcenode`name (it was initialized like ""pattern [ "" + regex + ""]"" ) so that i can compare with `topic` names. i could not get the reason to insert new overloaded methods further below. the next method below is the most generic one (with most arguments) which is [code block]",0,0.9852460026741028
277353548,2466,jeyhunkarimov,2017-02-03T20:27:01Z,would you mind to review again please?,0,0.9905209541320801
277392859,2466,mjsax,2017-02-03T23:39:32Z,"would you mind fixing this typo, too: [a link]",0,0.9913964867591858
277395085,2466,mjsax,2017-02-03T23:53:52Z,there is a build error. please fix before we can do a review.,0,0.946325421333313
277395661,2466,jeyhunkarimov,2017-02-03T23:57:46Z,"yes, some pr changed the (test) classes related with this pr so. i will fix them.",0,0.9917848110198975
277395841,2466,mjsax,2017-02-03T23:58:54Z,"one more general comment. i would be better if you would not squash your commits on every update -- this can simplify reviewing the pr. and when the pr get's merged, all commits get squashed automatically anyway.",0,0.9858353734016418
277471760,2466,jeyhunkarimov,2017-02-04T19:56:02Z,i fixed the errors,0,0.9886513948440552
278685807,2466,jeyhunkarimov,2017-02-09T16:02:07Z,i had to rebase because of conflicts in different commits,0,0.676482081413269
278729643,2466,mjsax,2017-02-09T18:28:34Z,anyof call for second review,0,0.9906274080276489
278970132,2466,jeyhunkarimov,2017-02-10T15:11:44Z,i did changes,0,0.9839634299278259
279079856,2466,mjsax,2017-02-10T22:08:46Z,"i just realized, that we do have a kip for this. i am very sorry that i missed to mention this from the beginning on. are you familiar with the kip process? would you like to do the kip? see [a link]",-1,0.9923419952392578
279081152,2466,jeyhunkarimov,2017-02-10T22:14:52Z,"np. in general, i am familiar with kip process so is there some existing kip related with this pr (if yes, can you provide the number) or should i create kip for this pr and discuss in mailing list?",0,0.9929748773574829
279091209,2466,mjsax,2017-02-10T23:07:24Z,"we don't have a kip. please, use the template to create a new kip page, add it to the table ""kips under discussion"" (and to kafka streams page in the wiki) and start a discuss thread on the dev mailing list. thanks a lot!",1,0.9920143485069275
279102611,2466,jeyhunkarimov,2017-02-11T00:25:30Z,i don't have `create` button on my confluence page ([a link] do i have to ask somebody to give me authorization to create kip?,0,0.9840264320373535
279119214,2466,mjsax,2017-02-11T04:00:55Z,can you give write access to to the kafka wiki so he can prepare the kip ?,0,0.9953492283821106
279120938,2466,guozhangwang,2017-02-11T04:42:24Z,what is your apache id?,0,0.9943608641624451
279188872,2466,mjsax,2017-02-12T01:19:44Z,jeyhun.karimov see link in jeyhun's comment :),1,0.945999801158905
279241592,2466,guozhangwang,2017-02-12T19:25:56Z,oh right. done.,0,0.923977792263031
281548260,2466,mjsax,2017-02-22T02:22:39Z,"ignore my last comment (deleted it already)... i did update the jira, but it does not require a change of this pr. you might want to sent another reminder to dev list about the kip. ask for further feedback and if nodoby response, you might want to start the vote thread.",0,0.9811756014823914
281740050,2466,mjsax,2017-02-22T17:27:17Z,retest this please.,0,0.9739659428596497
281839301,2466,mjsax,2017-02-22T23:27:26Z,the test errors are a little weird. the test that fails is not part of you current pr branch but was added to `trunk` later on. can you please rebase to `trunk` -- i hope this fixed the issue. not sure why it occurred in the first place.,-1,0.9696875810623169
282328998,2466,jeyhunkarimov,2017-02-24T16:03:52Z,"the tests will fail when there are no source nodes defined in the test topology. also, if the defined source nodes are not related with partitions, the tests will also fail due to the nullpointerexception in `streamtask` 125: [code block] maybe we should add extra check specifically for that issue, when validating the topology.",0,0.9934937357902527
282346689,2466,mjsax,2017-02-24T17:09:16Z,"i think that's fine -- for tests it would be a testing error to not add a source. not sure if we have a proper test for real topologies to guard against it (only required for papi) -- for kstreambuilder it cannot happen. maybe `topologybuilder` should contain a test that ensures that the dag is connected and ""well-formed"" ? but i actually think, we would not even generate the `processortopology` in the first place -- we start building it from the sources, and if no source was added, the whole `processortopology` would be empty. this would also be a testing error only. for real topologies, if there are not partitions, we would not create a task for the `sourcenode` in the first place and thus, this code should never be executed. \cc (does this make sense?)",0,0.9502052068710327
282424079,2466,jeyhunkarimov,2017-02-24T22:33:57Z,"sorry i had to rebase again, because of conflicts.",-1,0.9917635917663574
282426549,2466,mjsax,2017-02-24T22:46:47Z,"no worries for rebasing, you actually don't need to squash previous commit (but i know that rebasing is simple if you have a single commit). however, you could squash and rebase before you do the pr update and put a new commit with the change on top :)",1,0.9870484471321106
288674025,2466,jeyhunkarimov,2017-03-23T10:14:55Z,"sorry, i had to rebase again as there were a lot of conflicts among my commits and among mine and other commits. i deprecated [code block] for test classes, i replaced the deprecated variables with new ones. for non-test classes, i put an if condition to check both of the variables.",-1,0.9783592820167542
289434341,2466,jeyhunkarimov,2017-03-27T12:08:54Z,can you please check?,0,0.9944165945053101
297880899,2466,mjsax,2017-04-28T01:12:47Z,can you rebase once again -- we merged a few conflicting pr today. sorry for that. call for review,-1,0.9941530823707581
298576227,2466,enothereska,2017-05-02T09:30:30Z,could we get this in please? thanks.,1,0.9366081357002258
490292130,6694,ConcurrencyPractitioner,2019-05-07T23:36:19Z,ping for review,0,0.987627387046814
490298156,6694,ableegoldman,2019-05-08T00:07:02Z,thanks for the pr! i agree this seems like a straightforward patch but i'm wondering if we shouldn't try and think through the eos case a bit more? or is there really no way to safely cover it as well?,1,0.9901638627052307
490311761,6694,ConcurrencyPractitioner,2019-05-08T01:25:09Z,"hi thanks for reviewing! i was planning on attacking the eos case. but as you could guess from the code, trying to retrieve the metadata committed is not as simple as in the non eos case. i was hoping for some input on that. so some small amount of advice is greatly appreciated. :)",1,0.9969078898429871
490746569,6694,ConcurrencyPractitioner,2019-05-09T05:14:43Z,"oh, just found out something. regardless if it is eos case or not, calling [code block] should be sufficient. so there shouldn't be any big problems with this. i will try to add a test case. :)",1,0.9956921935081482
490750108,6694,ableegoldman,2019-05-09T05:34:30Z,"that sounds reasonable. looks like the build failed on checkstyle, can you try running it? +1 on adding a test case(s)",0,0.5416786670684814
491130846,6694,ConcurrencyPractitioner,2019-05-10T02:23:09Z,"alright, done. added a test case as well. would be good if you could take a look. :)",1,0.9962604641914368
493146367,6694,ConcurrencyPractitioner,2019-05-16T16:49:44Z,pinging and for review,0,0.9850538372993469
493695650,6694,ConcurrencyPractitioner,2019-05-18T18:01:32Z,"oh sorry, my bad. underestimated the scope of the pr. sorry for pinging you guys. will dig some more.",-1,0.9949913620948792
497908544,6694,ConcurrencyPractitioner,2019-06-01T03:31:40Z,cc i have rebased the pr. want to take a look?,0,0.9939981698989868
498844777,6694,ConcurrencyPractitioner,2019-06-04T21:09:44Z,you mind taking a look?,0,0.9808496832847595
509822799,6694,ConcurrencyPractitioner,2019-07-09T21:49:15Z,"ok, so i added a test to try and see if we can handle the eos enabled scenario and it doesn't appear to be that straightforward (since the test fails). we either have to mess around with the [code block] config or think of something else. you could see the test failure in jenkins, and the nullpointerexception indicates that we cannot retrieve the timestamp from consumer when eos is enabled (probably since we used producer's commit api instead of consumer's).",0,0.9811025261878967
509877987,6694,ConcurrencyPractitioner,2019-07-10T02:05:31Z,"there is a question relating to timestampextractor and its usage that has been bugging me a little. i have added a [code block] instance field to recordqueue, and then we use that as the [code block] input argument. is that the right way to use it? after all, from what i could understand of timestampextractor, the previous timestamp should be the previous _record_'s timestamp rather than the max timestamp seen so far.",-1,0.8655877113342285
510693180,6694,ConcurrencyPractitioner,2019-07-11T23:47:07Z,you want to check this out?,0,0.9346608519554138
511595872,6694,ConcurrencyPractitioner,2019-07-15T22:38:48Z,might want some of your input.,0,0.970781147480011
512005573,6694,mjsax,2019-07-16T22:03:58Z,with regard to `previoustime` it's actually a miss leading name. there is already a pr to clean this up: [a link] -- it should be called `partitiontime`.,0,0.9916630983352661
512015515,6694,mjsax,2019-07-16T22:21:59Z,"the test result are not available any longer. however, looking into the test code, i assume that the issue is, that it is a unit test. for this case, the commit of a producer does not make it to the consumer (because there is no broker, and the consumer is mocked). hence, we need to mock that the consumer is returning to correct metadata -- i guess, the test setup code will need to call `mockconsumer#commitsync(map offsets)` to do this. however, we also need to have an integration test to verify that the producer committed offsets make it to the consumer, too. i think we need a completely new test (maybe called `partitiontimeintegrationtest`) that uses an `embeddedkafkacluster`. compare the existing integration tests for this. the integration test should run twice, once with eos enabled and once with eos disabled. does this make sense?",0,0.9875001311302185
512031374,6694,ConcurrencyPractitioner,2019-07-16T22:49:57Z,"oh, so that was the reason why the eos case wasn't working. it had never occurred to me that there was no broker, causing the failure. thanks for the heads up on that one. the integration test i will add. we will definitely need one.",1,0.9662644267082214
512606577,6694,ConcurrencyPractitioner,2019-07-17T23:32:22Z,retest this please.,0,0.9739659428596497
512892534,6694,ConcurrencyPractitioner,2019-07-18T16:36:07Z,"alright, so after some checks, it appears that if we move the logic for restoring the correct partitiontime to newtasks, it will cause several tests relating to resetting streams to break, particularly in resetintegrationtest. i've traced the root cause to be possibly both the committed() call / the setpartitiontime() call. the why of what happened is still unknown, but we need to dig deeper.",0,0.9873108863830566
513067124,6694,ConcurrencyPractitioner,2019-07-19T02:36:29Z,"what do you think about this? when moving this logic into newtasks(), several tests seem to break due to it. your thoughts?",0,0.9381290078163147
513328157,6694,mjsax,2019-07-19T18:20:32Z,test results are not available any longer. checked out the pr and ran it locally. - `assignedstreamstaskstest` -> just needs some updates to the mocks bunch of integration test fail all with the same error message: [code block] or [code block] this is definitely your new code that does not yet work properly. you will need to dig into those issues and fix your code.,0,0.9764966368675232
513479272,6694,ConcurrencyPractitioner,2019-07-20T16:03:46Z,"there is some system.out.println statements i added for debugging purposes. later, i will remove them.",0,0.9922027587890625
513562958,6694,ConcurrencyPractitioner,2019-07-21T15:15:57Z,retest this please.,0,0.9739659428596497
513981439,6694,ConcurrencyPractitioner,2019-07-22T22:38:42Z,"alright, added an integration test for both eos and non eos case. you think we need anything else?",0,0.9934715032577515
514459621,6694,ConcurrencyPractitioner,2019-07-24T02:58:26Z,call for review from .,0,0.9919052124023438
516211153,6694,ConcurrencyPractitioner,2019-07-30T00:26:28Z,i guess we want to try to wrap this pr up?,0,0.9793537855148315
517098808,6694,ConcurrencyPractitioner,2019-08-01T02:59:49Z,"ok, test modifications are a little more complicated since when i converted the test i added to a more simple [code block] test, it failed. still working to identify the cause. (some debugging statements has been added, they will be removed shortly).",0,0.9815475940704346
517497668,6694,ConcurrencyPractitioner,2019-08-01T23:55:09Z,"alright, i have proof that initializetasktime is called at the wrong location. it appears that when we switch to a simple builder.stream().to() program, it is _never called_. i've found that if you put it in front of the addrecords() method, it will be called regardless. so i thought it was a pretty safe bet.",0,0.7825230360031128
517778823,6694,ConcurrencyPractitioner,2019-08-02T17:13:49Z,okay addressed most of your comments.,0,0.9887827038764954
517871590,6694,ConcurrencyPractitioner,2019-08-02T23:30:56Z,"about why initializetasktime() and the reasons it was moved. in a comment above, i actually stated the reason: ""alright, i have proof that initializetasktime is called at the wrong location. it appears that when we switch to a simple builder.stream().to() program, it is never called [where it is right now in initializenewtasks()]. i've found that if you put it in front of the addrecords() method, it will be called regardless. so i thought it was a pretty safe bet."" strange, i know. integration test however seems to indicate this should be the better approach.",0,0.8816178441047668
517874623,6694,mjsax,2019-08-02T23:53:49Z,"thinking about it again, maybe you added it to the wrong line in `initializenewtasks()` [code block] however, we call `transitiontorunning()` multiple times and need to call `initializetasktime()` each time before we. hence, it would be simpler to call initializetasktime() within `transitiontorunning()`",0,0.9833127856254578
517876334,6694,ConcurrencyPractitioner,2019-08-03T00:06:56Z,"oh, nice catch! tried it out and it worked. i didn't think that we would have to move it to the else branch of the method.",1,0.9950394034385681
517888203,6694,ConcurrencyPractitioner,2019-08-03T02:45:34Z,"oh looks like i can get rid of the sleep now. weird, could no longer replicate the behavior any longer. might have been a fluke.",-1,0.9712517261505127
518775221,6694,ConcurrencyPractitioner,2019-08-06T17:51:41Z,do you think that this pr as it is could be merged? feel like its pretty close to completion.,0,0.6397898197174072
519186670,6694,ConcurrencyPractitioner,2019-08-07T17:08:13Z,retest this please.,0,0.9739659428596497
519339761,6694,ConcurrencyPractitioner,2019-08-08T02:37:49Z,"actually, bouncycastle dependency has been added. there shouldn't be any problems from here on out. so everything should be fine from here on out.",0,0.8336514234542847
519598495,6694,ConcurrencyPractitioner,2019-08-08T16:46:28Z,"all comments has been addressed. a thought did occur to me though while doing this pr. if a streams has been restarted by the user, shouldn't the user expect the timestamp of a streamtask to be -1 before it starts processing (meaning that there is no retrieval of committed timestamps) ? i don't really know if a streams restart is a good simulation of a crash because it might perform contrary to user expectations.",0,0.9904943704605103
519618724,6694,mjsax,2019-08-08T17:42:22Z,"not sure. stopping and restarting an application is the same as fail-over. if there are committed offsets you want to continue where you left off, and the application should be in the exact some state as when you stopped it. hence, why would you expect -1 as start timestamp? of course, if you reset the application and loose the offsets, you trigger auto-offset-reset and you would start with -1 (similar if you use the reset tool to seek to a specific start offset). hence, i don't think this would be a problem.",0,0.9351387619972229
519659608,6694,ConcurrencyPractitioner,2019-08-08T19:40:20Z,think we need anything else?,0,0.9860851764678955
519725293,6694,ConcurrencyPractitioner,2019-08-08T23:40:36Z,retest this please.,0,0.9739659428596497
519753100,6694,ConcurrencyPractitioner,2019-08-09T02:24:25Z,"ok, comments should be addressed.",0,0.9879103899002075
520000977,6694,mjsax,2019-08-09T17:30:01Z,java 8: [code block] java 11 / 2.13: [code block] java 11 / 2.12: [code block],0,0.994533896446228
520058640,6694,ConcurrencyPractitioner,2019-08-09T20:48:03Z,"looks like we still need to manually commit offsetandmetadata. mock producer doesn't do that for us, interestingly enough.",0,0.978661298751831
520096521,6694,mjsax,2019-08-09T23:42:17Z,"well. i guess i missed to explain something: the producer adds and commits the offsets as part of the transaction: [a link] the `mockproducer` first adds the `offsetandmetadata` into its ""uncommitted cache"": [a link] on commit, it moves the offsets to ""committed offsets"": [a link] thus, `task.initializetasktime();` will not find them because the mock consumer and mock producer are not connected to each other. hence, your observation is correct. however, committing the offsets using the mock consumer in the test code, dose not verify if the producer did commit the correct offsets and metadata. therefore, instead of calling `task.initializetasktime()`, the test should check if the producer did the correct commit. if we really want to keep `initializetasktime()`, the test code should extract the offsets from the producer and commit the on the consumer to ""connect"" both mocks. (this is still different to the current code, that provide the offset as part of the test what does not help to test what we want to verify). does this make sense?",0,0.9444565176963806
520113404,6694,ConcurrencyPractitioner,2019-08-10T03:17:01Z,"thanks for the tip! got everything done and updated the streamtask test. we hopefully should be pretty close, so one last push. :)",1,0.997038722038269
520463985,6694,ConcurrencyPractitioner,2019-08-12T15:05:47Z,any more comments?,0,0.9907190203666687
520910285,6694,ConcurrencyPractitioner,2019-08-13T16:34:59Z,retest this please.,0,0.9739659428596497
521360908,6694,ConcurrencyPractitioner,2019-08-14T18:22:08Z,pinging for final review.,0,0.9908936023712158
521716728,6694,ConcurrencyPractitioner,2019-08-15T17:00:32Z,"alright, done.",0,0.9857764840126038
522779481,6694,ConcurrencyPractitioner,2019-08-19T22:33:24Z,another round of comments would be good or approval of pr if everything looks right. :),1,0.9957224130630493
524449143,6694,marcospassos,2019-08-23T20:20:27Z,this issue is the cause of critical bugs we recently faced up in our applications that rely on the `sessionstore` for processing retroactive events. do you think this fix can be included as part of 2.3.1?,0,0.9851258993148804
525998259,6694,ConcurrencyPractitioner,2019-08-29T02:45:41Z,any last comments?,0,0.9919871091842651
528057138,6694,ConcurrencyPractitioner,2019-09-04T19:48:23Z,pinging.,0,0.9158015251159668
528148998,6694,mjsax,2019-09-05T00:50:38Z,i don't think that we will include it in 2.3.1 -- it's not really a bug fix but an improvement. i try to review again in the next days.,0,0.9516434669494629
529068486,6694,ConcurrencyPractitioner,2019-09-07T03:39:25Z,retest this please.,0,0.9739659428596497
530728164,6694,cadonna,2019-09-12T08:43:57Z,all builds reported spotbug issues.,0,0.8876434564590454
531017385,6694,ConcurrencyPractitioner,2019-09-12T21:40:26Z,"yeah, got it fixed.",0,0.9042342305183411
531208299,6694,cadonna,2019-09-13T11:55:33Z,the following test failures seem related: [code block],0,0.9865230917930603
531255894,6694,ConcurrencyPractitioner,2019-09-13T14:20:46Z,"oh, just realized that 's comment caused a regression. if you would look earlier in the conversation, you would find a segment where a call to close() resets all partition times to negative one. therefore, we need to store the partition times in a map _before_ they are reset, and then they are passed into the commit() method. the extra parameter is needed after all due to the order of operations in close(). we will need to rollback some changes.",0,0.9673534035682678
533262618,6694,mjsax,2019-09-19T18:53:29Z,thanks for the hard work !,1,0.9708004593849182
385120644,4931,mageshn,2018-04-27T23:47:53Z,thanks for the comments. i will certainly be adding more javadocs when ready for the final review. i still didn't do the java serviceprovider api changes because i just wanted to finalize the public changes here.,1,0.9658650159835815
391870227,4931,mageshn,2018-05-24T21:33:56Z,any final thoughts here before i request a committer to take a look?,0,0.9917258024215698
391898959,4931,rhauch,2018-05-24T23:44:49Z,there are a couple of unaddressed comments.,0,0.9836286306381226
392278971,4931,mageshn,2018-05-26T18:22:33Z,i have addressed all the comments from your last pass.,0,0.9936957955360413
392637083,4931,mageshn,2018-05-29T02:40:02Z,what are you thoughts about enforcing a version to be non null & nonempty? i think we can enforce this for rest extension only when one tries to use it.,0,0.9911755323410034
392879147,4931,mageshn,2018-05-29T18:08:32Z,i have pushed a change where i check for the version when the plugin gets instantiated. this should be effective for all plugins that use the newplugin() method in future.,0,0.9855371117591858
393006763,4931,mageshn,2018-05-30T02:06:28Z,thanks . lots of great detailed comments that helped this pr evolve.,1,0.9931105375289917
393030354,4931,mageshn,2018-05-30T04:56:00Z,"thanks . regarding the connectclusterstate being mutable, i think that's the real use of it. however in the implementation the 2 methods that it exposes are currently not atomic. also, the connectclusterstate doesn't expose the connector configs at all, so i'm not how kip-297 would affect this.",1,0.9622601866722107
456933267,6177,abbccdda,2019-01-23T19:24:55Z,call for review folks!,0,0.7743203639984131
458241606,6177,abbccdda,2019-01-28T18:15:08Z,thanks stanis for the great suggestions. could i get some further review? ?,1,0.9953510761260986
459831480,6177,abbccdda,2019-02-01T19:02:24Z,pinging for more reviews thanks a lot!,1,0.9851675629615784
460243061,6177,stanislavkozlovski,2019-02-04T13:05:05Z,"for future reference, could we not force-push changes after review comments? we will squash everything in the end anyway. it makes subsequent reviews much easier",0,0.975593090057373
460350699,6177,abbccdda,2019-02-04T18:07:10Z,"sounds good, i will try to see if i could work around that when i rebase to trunk.",1,0.6381371021270752
468834722,6177,abbccdda,2019-03-01T22:33:31Z,do you have time to take a closer look at the group coordinator logic?,0,0.9894557595252991
470385891,6177,abbccdda,2019-03-07T05:03:16Z,"i have addressed most comments except two things: 1. the assertion vs exception 2. use option[string] instead of option i have written my thoughts on both questions. for #2, my question is primarily on whether we should still use option when upstream is confirming to pass in a valid string? let me hear your feedback, thank you!",1,0.9849758744239807
470752834,6177,hachikuji,2019-03-08T00:12:05Z,"thanks for the updates. there is one additional point that i wanted to discuss. let's use the following example. suppose we have three consumers in the group with static instance ids: a, b, and c. assume we have a stable group and the respective memberids are 1, 2, and 3. so inside the group coordinator, we have the following state: [code block] in fact, the consumer leader of the group is not aware of the instance ids of the members. so it sees the membership as: [code block] now suppose that a does a rolling restart. after restarting, the coordinator will assign a new memberid to a and let it continue using the previous assignment. so we now have the following state: [code block] the leader on the other hand still sees the members in the group as {1, 2, 3} because it does not know that member a restarted and was given a new memberid. suppose that eventually something causes the group to rebalance (e.g. maybe a new topic was created). when the leader attempts its assignment, it will see the members {2, 3, 4}. my basic question is how can it know that 4 is the same member as 1? i think the proposal essentially relies on some auxiliary information provided by streams in order to determine this, but i cannot think of a good reason why we should not just use the instance id itself. in other words, we can change the joingroup response to include both the instance id and the member id. [code block] this approach provides some benefit even for the simple partition assignors. consider, the default range assignor, for example. basically it works by sorting the members in the group and then assigning partition ranges to achieve balance. suppose we have a partition with 9 partitions. if the membership were {1, 2, 3}, then the assignment would be the following: [code block] now when the membership changes to {2, 3, 4}, then all the assignments change as well: [code block] so basically all of the assignments change even though it's the same static members. however, if we could consider the instanceid as the first sort key, then we can compute the assignment consistently even across restarts: [code block] and after the restart: [code block] to summarize, i think basically what i'm saying is that the full benefit of static assignment can only be realized if the assignor knows the instance ids of the members in the group. it shouldn't be necessary to do anything fancy with additional metadata. if that makes sense, i'd suggest that we alter the joingroup response here since we are bumping the protocol anyway. we can leave any changes to the `partitionassignor` interface for another pr.",1,0.9540450572967529
471014920,6177,guozhangwang,2019-03-08T17:45:30Z,"i like 's idea, it looks a good thing to have with very small cost on protocol changes.",1,0.9869044423103333
471020583,6177,abbccdda,2019-03-08T18:03:41Z,"thanks for the great proposal jason! i think the benefit of this change is more valuable for range or round-robin assignors because they rely on member id to do the sharding. for sticky assignors, my understanding is that as long as the application is not rolling restart, we should preserve the same subscription info when rebalance happens. the leader should be able to see the current subscription info for all members iiuc? i would definitely take some time to read through the code and make sure i understand the sticky assignment better.",1,0.9935560822486877
472058830,6177,hachikuji,2019-03-12T15:51:31Z,"yeah, my point is that it is generally useful for assignors to have access to the static ids of the current group members. the static id allows us to reliably detect the same instance across restarts which frees users from having to either embed the static id in the user data or implement some kind of heuristic to try and guess the same member. anyway, i think it would be a missed opportunity to introduce this fancy new notion of static id and not allow the assignors to leverage it. do you see any downsides?",0,0.9476442933082581
472079669,6177,abbccdda,2019-03-12T16:37:25Z,"nope, after reading the code i think the idea is brilliant! could you first take a look at [a link] since i need to automate join group protocol first, thank you!",1,0.9965092539787292
472462552,6177,stanislavkozlovski,2019-03-13T15:07:43Z,fwiw i also think that idea is great. could we make sure to update the kip if we'll be following this approach?,1,0.9889047741889954
472539911,6177,abbccdda,2019-03-13T18:04:28Z,"yep, i will definitely do that!",1,0.9041525721549988
478372433,6177,guozhangwang,2019-03-31T19:25:53Z,"we should also update kip-345 itself for both the protocol change, as well as the susbscription class (as it's a public class) which is passed via `partitionassignor#assign`.",0,0.9950181245803833
478382417,6177,abbccdda,2019-03-31T20:53:04Z,thanks guozhang for the great summary! i will address the agreed changes and update the kip for subscription class change.,1,0.9960013031959534
480142674,6177,abbccdda,2019-04-05T04:15:04Z,"ah, i checked the `subscription` class and it doesn't contain any id field [code block] what are we trying to add here?",0,0.9923436045646667
480145428,6177,guozhangwang,2019-04-05T04:36:14Z,"as suggested, we want to encode the instance.id into the subscription so that assignor can behave on sticky assignment.",0,0.9939751029014587
480400795,6177,hachikuji,2019-04-05T19:48:42Z,"i wanted to suggest an alternative to the proposal above for handling the ""dueling consumers"" problem. i would say the underlying issue is that the coordinator cannot distinguish the two cases when a memberid has been replaced and when it has been removed. if it could, then we would be able to use member_id_mismatch consistently and the consumer could treat this error as fatal. here are two options: 1. include instanceid in the heartbeat and offsetcommit apis. then the coordinator can return the proper error code. 2. we can can use a convention to embed the instanceid into the generated memberid. at the moment, the current format is `{clientid}-{random uuid}`. for static members, i think instanceid is more useful than clientid and we could probably use timestamp as a more concise alternative to uuid. so we could have `{instanceid}-{timestamp}` as the memberid for static members. then we would be able to extract this from any request and the coordinator could use the proper error code. my preference is probably for option 1 because it is explicit, but i know guozhang expressed some unwillingness to change additional apis. a nice benefit of option 2 is that it will be easy to see the instanceid of existing members using tools like `kafka-consumer-groups.sh`. this will make debugging easier.",0,0.9675717353820801
480439917,6177,guozhangwang,2019-04-05T22:19:20Z,"for option 1), today we call resetgeneration on five places upon `unknown_producer_id`: leave, join, sync, commit, heartbeat. i think for leave-group we can probably not adding instance.id, but for other three excluding join, we will need to add instance.id to make it work. my personal feeling is it is a bit too intrusive to add on all these request protocols, but honestly i do not have a very objective rationale either. option 2) looks better to me, although it requires to make such string construction rules as part of the public apis since other client implementations would need to rely on that as well.",0,0.8590254187583923
480445141,6177,hachikuji,2019-04-05T22:46:33Z,"i think we should probably use something like the member id generation scheme proposed in 2) in any case, whether or not we rely on it internally. i think it will be useful for debugging to always be able to derive the instance id from the member id. i'm a bit torn on whether or not to include instance id in apis that you mentioned. it seems like a cleaner solution to me because it allows the coordinator to be unambiguous about the error. i think the approach which requires retrying the joingroup in order to disambiguate the error is a little hard to explain. i also feel a little annoyed about modifying more protocols, but i'm not really sure we should be trying to optimize for the number of protocol bumps. by the way, if we cannot rely on the generated memberid, then it seems inevitable that we will also need to modify the describegroup api so that it is easy to find the instance ids associated with current members.",0,0.8952459692955017
480647194,6177,abbccdda,2019-04-08T00:30:34Z,"thanks for the discussion here! in high level, i think both approaches make sense here. the issue with 1) is that we are altering many protocols all at once just for the purpose of solving a client mis-configured scenario, which seems like an over-kill to me. the attractive side of 2) is extra benefit in terms of debugging, however it still involves a lot of logic changes to double check static member's identity in all consumer protocols and client needs to handle `member_id_mismatch` exception everywhere, not mentioning corresponding unit tests. no matter which approach we eventually take, it seems better to have the change go in with another pr to reduce the review burden for the current one, which is already high. the severity of instance id conflict should be tolerable for now, and i think we still have enough time to get the fix diff ready before 2.3 ddl. so if you both agree, i could revert the generation reset change and leave the id conflict fix for next diff if possible. wdyt?",1,0.9879563450813293
480672408,6177,guozhangwang,2019-04-08T03:38:44Z,yeah i think we should keep this pr small as it already grows from 600 to about 1200 loc anyways. i'm fine with having a follow-up pr with whatever approach we've agreed upon to handle the instance id conflict issue.,0,0.9577500224113464
480693499,6177,abbccdda,2019-04-08T05:55:07Z,"sounds good, will revert the current changes of `resetgeneration()`.",0,0.6129031181335449
481373001,6177,hachikuji,2019-04-09T18:22:06Z,sounds good to me to resolve this problem separately. i'll do another pass on the pr today and hopefully we can merge this week.,1,0.5970245599746704
481796564,6177,abbccdda,2019-04-10T17:56:37Z,"`let me try one more time. how about fenced_instance_id?` lol, you got it.",1,0.9709837436676025
482215766,6177,abbccdda,2019-04-11T17:26:35Z,"added most comments except ones that we haven't cleared, do you mind taking another look when possible? thank you!",1,0.9488797187805176
482400070,6177,guozhangwang,2019-04-12T01:24:51Z,"i've made another pass on it. one meta comment is that we've not yet updated the web docs for this -- if you want to do it in another pr that's fine, just pointing it out so we do not forget about it. otherwise, i'm +1 modulo 's comments.",1,0.5663701295852661
482429127,6177,abbccdda,2019-04-12T04:02:49Z,that sounds good guozhang! will do it in another pr.,1,0.993363618850708
483362126,6177,abbccdda,2019-04-15T18:17:43Z,could you take another when you got time? thank you!,1,0.9453867673873901
484348012,6177,abbccdda,2019-04-18T04:04:25Z,"i refactored the usage of `group.instance.id` on client side to use `option ` so that we could avoid string comparison. let me know if this works, thank you!",1,0.9667983055114746
484955097,6177,abbccdda,2019-04-19T16:52:41Z,do you think we have luck to get this done today?,0,0.8996146321296692
485040283,6177,abbccdda,2019-04-19T23:55:03Z,i think the failure is due to a flakey test fyi,0,0.9625639319419861
485060306,6177,guozhangwang,2019-04-20T05:32:15Z,some of the consumer / upgrade tests have failed: [a link],0,0.9887908697128296
485294349,6177,abbccdda,2019-04-22T00:39:09Z,"fyi, in `verifiableconsumer.java`, i realized that i couldn't do `consumerprops.put(consumerconfig.group_instance_id_config, null);` because hashtable rejects null value. so i did a workaround to use empty string as default value for `group.instance.id` but still keeps all the optional classes in the consumer/broker code paths.",0,0.9883657693862915
485474245,6177,hachikuji,2019-04-22T16:52:54Z,"shouldn't really be necessary. for `properties` objects, you can just leave out the key if the value is null.",0,0.9908079504966736
485553178,6177,abbccdda,2019-04-22T21:07:49Z,that's a bit tricky since non-existing key get will through exception: [code block] should we do a try-catch in `kafkaconsumer` for instance id retrieval then?,0,0.9625741839408875
485599691,6177,guozhangwang,2019-04-23T00:38:26Z,"hmm that's weird, if we have a default value defined already, then upon the config object construction if there's no user-provided value the default value will be used to fill in the `values` map (note only the `originals` map contains user-overridden values).",-1,0.9798561334609985
486845915,6177,guozhangwang,2019-04-25T21:33:15Z,"i've taken another look at the added commits, and besides this minor [a link] it lgtm.",0,0.992344319820404
486848243,6177,abbccdda,2019-04-25T21:41:50Z,[a link] this shows that the system tests are fixed now.,0,0.9938485026359558
486848349,6177,guozhangwang,2019-04-25T21:42:09Z,triggered [a link] (`345_system_test` is on top of `345_merge`) and passed.,0,0.9953778982162476
486867096,6177,abbccdda,2019-04-25T22:58:37Z,[a link] triggered and passed exactly on top of current `345_merge` branch.,0,0.995797872543335
487161856,6177,guozhangwang,2019-04-26T18:45:23Z,"thanks for the great patience and great work on the first step towards kip-345, .",1,0.9940080642700195
290643473,2772,michaelandrepearce,2017-03-31T07:56:22Z,"need to rebase, dd71e4a8d830c9de40b5ec3f987f60a1d2f26b39 changed test class's breaking fetchertest on ci build",0,0.9936109185218811
290944811,2772,michaelandrepearce,2017-04-01T20:21:41Z,"can someone check trunk or the pr build plan, the build failure is due to: pure virtual method called terminate called without an active exception :streams:unittest failed i note other preceding builds for other pr's also have this issues, e.g. [a link] [a link]",0,0.990532636642456
290948772,2772,michaelandrepearce,2017-04-01T21:37:04Z,"-rosenblatt thanks for the time and the review feedback :), i've committed some changes based on it, if you wish to re-review. p.s. it seems the pr builds in jenkins ci plans are not stable atm. i assume someone is looking into as its not just my pr affected.",1,0.9931572079658508
291421176,2772,ijuma,2017-04-04T07:46:10Z,", would you like to review this one?",0,0.9052082896232605
291990765,2772,becketqin,2017-04-05T20:43:16Z,thanks for the patch michael. i'll take a look either today or tomorrow.,1,0.9686777591705322
292451094,2772,michaelandrepearce,2017-04-07T06:17:40Z,"thanks for the review, i have left response on all, i hopefully have marked clearly the ones we don't need any discussion on and will just implement, will aim to do today. if you could read and comment on those with responses.",1,0.9707677364349365
296411045,2772,michaelandrepearce,2017-04-23T00:37:43Z,"thanks for the time and the review feedback :), i've committed some changes based on it. also have added the additional integration tests. plus yet another rebase",1,0.9946082830429077
297570869,2772,becketqin,2017-04-26T23:37:40Z,"thanks for the patch. lgtm. we need to update `upgrade.html` as well, but that can also be done in the follow up patch that removes the serde hack.",1,0.9621647000312805
297874309,2772,hachikuji,2017-04-28T00:22:05Z,thanks for the patch! i'm planning to merge this tomorrow. it would probably be a good idea to send an e-mail to the dev list with the changes to the kip to make sure there are no concerns.,1,0.9936601519584656
297883375,2772,michaelandrepearce,2017-04-28T01:31:36Z,battery on laptop just died at 2am here. managed to quickly commit some javadoc changes. so i don't have to go through yet another annoying rebase tomorrow could we merge as is and i commit to raise a new pr with the extra tests you mentioned.,-1,0.9423081278800964
297933355,2772,michaelandrepearce,2017-04-28T07:55:25Z,rebased .... again .... [a link],0,0.9941114783287048
298133490,2772,hachikuji,2017-04-29T00:14:16Z,"merge to trunk is imminent. the record header saga is finally drawing to a close. what battles await tomorrow?! (in all seriousness, thanks for the patch and the persistence with this kip)",1,0.9345569014549255
298144836,2772,simplesteph,2017-04-29T03:48:17Z,"sorry i'm late to the party and i'm going to be a pain, but if the headers are making it to the producerrecord, they should also make it to the sourcerecord, so that the connect framework directly benefits from it. also, imo this would have been a perfection opportunity of adding such headers as a method or something... right now, many constructors are missing the headers parameter, and the only way to get to add headers is the very dangerous [a link] (because of the partition parameter). just saw this got merged an hour ago... cf [a link] cc",-1,0.9944573640823364
298147071,2772,michaelandrepearce,2017-04-29T04:49:12Z,"hi steph, you actually add a header in normal usage by calling record.headers.add(key,value), this constructor was added in the record really only aimed at mirror making solutions. also adding headers is mostly aimed to be added by interceptors (where record is given) thus mutable at that stage. this is why on produce once sent the headers are made read only. yes connect framework isn't done as wasn't covered in this kip, once this kip was done i was actially going to raise another kip immediately to add them (a bit like timestamp was first added to core and then a second kip was raised to add them to connect), but first step it was important to get headers added at the core level. we actually have the code for that ready so hold on :)",0,0.8855746984481812
298147275,2772,simplesteph,2017-04-29T04:55:22Z,that helps thanks !,1,0.9688069820404053
298152782,2772,michaelandrepearce,2017-04-29T07:23:05Z,"was going to raise all this in a week or so time, but as we had the code ready and obviously you have a need thus raised the point re connect, i have raised it all now. here's kip [a link] here's pr (which we had the code for) [a link] obviously first needs kip discussion and vote. so please feel free to get active in that space supporting that :)",1,0.9947799444198608
298181606,2772,mjsax,2017-04-29T17:11:57Z,see [a link] for details about this work. you can just update your own kip accordingly.,0,0.9939771890640259
271833277,2330,ijuma,2017-01-11T10:27:51Z,", you know the networking code pretty well, so maybe you could help with this review. also , but his time is very limited this week.",0,0.9240135550498962
271888557,2330,mimaison,2017-01-11T14:53:42Z,one concern that was raised also in kip-81 (reusing the same memory pool logic on the consumer side) is whether the muting might impact group coordination. one suggestion is to only allocate in the memory pool requests that are larger than a small threshold (~256bytes for example) and let smaller requests be allocated on the heap as usual. this should not add too much load/memory pressure on the broker (since we also have the queued.max.requests setting) and would allow heartbeats and other group requests to be processed uninterrupted.,0,0.9717338681221008
273309380,2330,radai-rosenblatt,2017-01-17T21:43:10Z,"- if you want to bypass the memory pool for small allocations (which is perfectly valid) you could implement a compositememorypool that delegates requests under a certain size to the none pool and requests over that size to a ""real"" pool. that would give you the behaviour you want.",0,0.9916553497314453
273972279,2330,radai-rosenblatt,2017-01-20T04:02:37Z,- i've added a basic test of oom functionality to selectortest. i didnt write an ssl equivalent yet,0,0.9837682843208313
274049159,2330,rajinisivaram,2017-01-20T11:18:33Z,"-rosenblatt the implementation looks good to me. for testing ssl, it may be easier to add a memory pool to `nioechoserver` and add a test to `ssltransportlayertest`.",1,0.7374776601791382
274122627,2330,radai-rosenblatt,2017-01-20T16:57:05Z,"for some reason i cant respond directly to your comment above on networkreceive.java, but some of the ""no-pool"" ctr calls come from sasl, for example. reasons for not using a pool would be: 1. cost-benefit - sasl for example probably doesnt allocate large buffers and allowing small allocations to bypass the central pool is probably a good idea 2. focus of this change set. my immediate concern is broker being overwhelmed by large produce requests, as that is a common (relatively) scenario for me. future work could broaden the scope to include: 2.1 (de)compression - you'd want to use the pool for decompression target buffers. doing this efficiently might require a wire format change (would be nice to know decompressed size up front) 2.2 maybe send buffers? without ssl they get zero-copied from disk out to socket, but under ssl theres a byte buffer allocated. havent fully investigated so dont know how much of a potential oom risk that is. 2.3 usage on the client-side - both producer and consumer, also interacts with (de)compression.",0,0.9135515093803406
274196525,2330,radai-rosenblatt,2017-01-20T22:25:48Z,"- i've implemented the ssl version of testmuteonoom(). its not pretty, but it directly tests the core functionality. also, thank you very much for your time spent reviewing this.",1,0.9757768511772156
274617983,2330,radai-rosenblatt,2017-01-23T21:10:51Z,"- i've changed the tests to use the non-strict mode, like actual code. note that the scenario you've described of constantly muting and unmuting is also possible with a non strict pool: 1. server has n incoming connections 2. at every iteration pool only has enough memory to accommodate a single request 3. at every call to poll, n connections are unmuted, 1 is services, n-1 are muted again, progress indication is set to true 4. next iteration either makes no further progress (in which case the next after that will actually wait for a while), or again repeats this. this isn't as tight a loop as with a strict pool (as the 2nd invocation will not have memory and will not make progress), but its still sub-optimal. a more optimal solution would be to somehow remember the smallest unfulfilled request, have some pool.canallocate(long bytes) - which will account for strict vs non-struct - and only unmute is the result is true. since there's one pool and multiple selectors this isn't guaranteed to not unmute for nothing. to me this is a corner case though, as under these conditions currently the broker just explodes. i can implement the canallocate(bytes) api if you think its required, but i think this is optimizing a corner case.",0,0.9833396673202515
282928080,2330,radai-rosenblatt,2017-02-28T03:03:42Z,- to the best of my understanding i've addressed/answered everything? anything still pending from you guys?,0,0.9348159432411194
290571351,2330,radai-rosenblatt,2017-03-30T23:14:20Z,"- rebased again. slight modifications to comply with method complexity checks that were introduced to master since i last rebased. also kicked off an integration test, will report the results (everything passes locally)",0,0.9058315753936768
290735579,2330,radai-rosenblatt,2017-03-31T14:57:55Z,successful integration test run - [a link],0,0.9113845229148865
299710401,2330,radai-rosenblatt,2017-05-07T14:37:00Z,just rebased this again on trunk. the following tests are being flaky (on trunk as well as on my other pr): [code block],0,0.9882704019546509
314771034,2330,mimaison,2017-07-12T13:36:28Z,what's the status of this pr ? i'd like to start working on kip-81 but it has a dependency on this.,0,0.9885507225990295
314807274,2330,junrao,2017-07-12T15:32:54Z,: i dropped the ball of finishing the review. i will make another pass in the next few days.,0,0.6764193773269653
314819925,2330,radai-rosenblatt,2017-07-12T16:15:25Z,i've rebased it just yesterday. passes tests both locally and on the build server.,0,0.9911147356033325
317422141,2330,rajinisivaram,2017-07-24T13:28:04Z,"-rosenblatt i had a look through the code and it is looking good. it is a bit harder to review because the commits were squashed. the last time i had reviewed this, `selector#pollselectionkeys()` was returning a boolean which indicated if progress was made. some of those changes seem to have been reverted - perhaps those changes are still required to update `selector#madeprogresslastpoll`?",0,0.6402234435081482
317587446,2330,radai-rosenblatt,2017-07-24T23:49:40Z,i've fixed all the issues youve pointed out above,0,0.9137632250785828
317880700,2330,radai-rosenblatt,2017-07-25T21:39:39Z,- removed setting the progress flag where it wasnt strictly required.,0,0.9909517765045166
317958785,2330,junrao,2017-07-26T06:16:49Z,-rosenblatt : thanks for the latest patch. lgtm.,1,0.9827212691307068
317960830,2330,ijuma,2017-07-26T06:29:02Z,thanks for sticking with us -rosenblatt. and thanks for merging this during your holiday!,1,0.9912473559379578
518840417,7170,mjsax,2019-08-06T20:55:25Z,thanks for the pr. there are some checkstyle errors. can you please fix them before we review your pr?,1,0.9363015294075012
518955481,7170,lkokhreidze,2019-08-07T06:12:46Z,done,0,0.8682363629341125
519222258,7170,lkokhreidze,2019-08-07T18:45:30Z,"this is the first pr (written in pr description) `kstream#groupby` changes will be part of next pr if this one passes. i didn't want to create big pr without validating underline machinery and logic in the first place. i'm okay doing all the changes here, i was just thinking it will make review harder.",0,0.769711434841156
519225997,7170,lkokhreidze,2019-08-07T18:56:12Z,"agreed, i'll do that. i wanted to tag as seems like he's the main author behind optimization logic. i'll add tests for optimization logic to make sure nothing breaks.",0,0.9338890314102173
519303125,7170,mjsax,2019-08-07T23:19:50Z,"ah. i guess, i skipped the pr description... sorry for that. i discussed the proposal with in person, and thinking about the semantics once more, i am actually wondering if it is wise to change `groupby` at all (this thought also affects my previous comment to include `join()` -- i would like to retract this idea :) ) we have basically two dimensions which 2 cases each to consider for `groupby` (and also `join()`: 1) repartition not required -- no `repartitioned` object provided 2) repartition not required -- `repartitioned` object is provided 3) repartition required -- no `repartitioned` object provided 4) repartition required --`repartitioned` object is provided case (1), (2), and (4) are straight forward. however, case (2) is somewhat awkward because we actually want to treat `repartitioned` as a configuration object but specifying it in `groupby` should not enforce a repartitioning (if one want to enforce a repartitioning, they should use the new `repartition()` operator). hence, for case (2) the `repartitioned` configuration would be ignored. therefore, only case (4) is left in which passing in `repartitioned` would have an effect. for this case, it would be possible to change the number of partitions or to specify a different partitioner. (i ignore the serde and naming because this can be achieved via `grouped` anyway). however, if one wants to change the number of partitions or wants to set a specific partitioner, it seems they want to to apply (ie enforce) this configuration independent of the upstream topic configuration; ie, it is actually a case for which the user wants to _enforce_ a repartitioning. hence, it seems perfectly fine (and actually better, because it's semantically more explicit) that a user should call `repartition()` instead. therefore, i don't see a good use case for which it make sense to pass in `repartitioned` into `groupby`. would be great if you could share your thoughts about it? a second point i discussed with is about the optimization. we both have the impression that `repartition()` should not be subject to the repartition topic optimization. instead, an enforced repartitioning step, should be added to the topology in a hard coded way similar to a call to `through()`. maybe we can be some advanced optimization rules later, but it seems difficult (and potentially unsafe/incorrect) to apply the repartition topic optimization for this case. hence, we would suggest to skip this optimization in this pr.",1,0.583954930305481
519307224,7170,ableegoldman,2019-08-07T23:40:44Z,"not sure i agree -- maybe you just want to control the parallelism *in case* a repartition is required? you could enforce users to step through their whole topology, figure out when/where repartitioning is needed, and use `repartition` to set the parallelism. or, you could let streams do this for you -- as it currently does, way more conveniently and probably less error-prone -- and supply a configuration to be used if streams figures out you need to repartition.",0,0.8355790376663208
519309839,7170,mjsax,2019-08-07T23:54:42Z,"i don't see this as a use case in practice. why would one want to change the parallelism? because, the aggregation operation is over or under provisions and thus one wants to decrease or increase the parallelism. if i am ok with the ""default"" parallelism in case there is no repartitioning, why would i not be ok with it if data is repartitioned? this is less an issue imho, because if i want to scale up for example, it's sufficient to insert `repartition()` once upstream and all downstream auto-created topics would inherit the parallelism implicitly (as they inherit it now from source topics). hence, i don't need to insert `repartition` all over the place.",0,0.9034265279769897
519315591,7170,ableegoldman,2019-08-08T00:24:40Z,"maybe you're processing data from a topic on your company's cluster, which has a huge number of partitions to begin with. maybe your workload needs nowhere near this many partitions (you're filtering out most records, it's overpartitioned to begin, your just testing). you run your streams app which creates some n topics all of which have this huge number of partitions. your brokers struggle and your boss gets mad? why does anyone want to change this ever? (ie with `repartition`) that's a good point though. so the `repartition` operator is really a ""auto-create topic"" + ""set parallelism"" operator -- should be sure to document this well. now, what if someone wants to configure the parallelism of a certain repartition topic(s) but would like to continue using the source topic's parallelism as the default? not saying we should necessarily support that, but we should at least make it very clear to users how using this will affect downstream topics.",1,0.5829803347587585
519319181,7170,mjsax,2019-08-08T00:44:35Z,"my question is, why do you need `groupbykey(repartitioned.with())`? if you want to scale down, it seems better to explicitly call `repartition(repartitioned.with()).groupbykey()` -- otherwise, you might not scale down if not auto-repartition topic is created and it seems rather error prone that we allow to specify the number of partitions and than ignore the config entirely. similar to your second comment, if you want to ""scale up"" again later, you call `repartition()` again. i agree that we need to document this explicitly if we follow this route. however, it's similar behavior as we have as-of now. if you insert a `through()` all downstream operators inherit the parallelism from it.",0,0.9694854021072388
519328010,7170,ableegoldman,2019-08-08T01:34:05Z,"ok, well i am fine with this framing it as a ""set parallelism"" operation...i don't want to stall this kip/pr further, but what if this was split into a new set of `setparallelism` and `repartition` operators, where the `repartition` just auto-creates the topic while upstream `setparallelism` is responsible for setting the number of partitions? just wondering if it's worth making this more explicit, since there's really two new functionalities being introduced here. doesn't hurt to bundle them into one operator, as long as users know what two things it actually does...",0,0.9145365953445435
519385569,7170,lkokhreidze,2019-08-08T06:30:43Z,"hello thanks a lot for valuable insights and interesting discussion. - your arguments make sense, but i'm leaning more towards points. in addition, in my mind, one other important point that we need to take into account is not only parallelism but general configuration of repartition topics. in my mind, this kip can be the foundation of actually giving users control over each individual repartition topic configuration. to be honest, i was tempted to propose deprecating `kstream#groupby(grouped)` operation altogether. let me explain my reasoning a bit. after this kip, i don't see any actual benefit nor need of actually using `kstream#groupby(grouped)`. with `kstream#groupby(repartitioned)` user can do exact same things, plus more. right now, in kstream there're `grouped` and `joined` configuration classes (and maybe some others that i'm missing) that are used for specifying a) topology name (which translates to repartition topic naming) b) producer serdes c) partitioner all those configurations can be encapsulated under `repartitioned`, in addition with all other topic level configurations that user may want to pass to internal topic creation. maybe this was discussed before, and there's a good reason why there're separate configuration classes for each operation (besides giving api the nice look, of course :) ). one argument that comes to mind why we may actually want to have `repartitioned` for `groupby` is simple syntax sugar. for example, there isn't fundamental different between this two topologies: 1) [code block] 2) [code block] while, for me, as a user, 2nd option looks much more appealing, similarly how key selector for `kstream#groupby` merges together two operations (`selectkey().groupby()`). again, your arguments are totally valid, and all can be achieved just by having `repartition(repartitioned)` operation. but on the other hand, i don't see anything bad with adding `repartitioned` option to groupby. it won't break api semantics (at least i think it won't) and will give the user extra flexibility around controlling repartition topics.",1,0.989418089389801
519387441,7170,lkokhreidze,2019-08-08T06:37:26Z,"it's harder to comment on this. first, i would like to see how optimization logic behaves with this changes. based on the first glance, it should be fine, but would need to verify this by adding more tests. if necessary, optimization can be easily removed i guess. would love to hear your thoughts/suggestions as well on this.",1,0.9630510210990906
520079726,7170,vvcephei,2019-08-09T22:08:46Z,"hey, all, just to wade in (hopefully not stalling this pr too long): we should exclude this change from optimizations. the optimization logic around repartitions is already quite complex. it pushes repartition nodes around the processor graph and merges them together when possible (and the merge results in picking some repartition topic names over others). adding a new class of repartition nodes (manually specified ones via this new operator) would only complicate the algorithm further. it's always safe to skip an optimization, so we can just skip it for now and consider it in a separate scope of follow-on work. further arguments against optimization: it's not clear that, if i put a `repartition()` node at a specific point in my topology, i would _want_ streams to move it somewhere else. also, if i have two different `repartition()` calls with _different_ parallelisms, and the optimizer wants to merge them, what can it do? what about if it wants to merge a groupby repartition (which implicitly has a parallelism already) and an explicit `repartition()` with a different parallelism? we can reason through all these cases, but it will still: * drag out the kip discussion (since we have to revisit all these cases) * increase code complexity in the optimizer * increase system complexity for the people using streams (who will have a hard time understanding the results of the optimization) we can avoid all this by just excluding the manual repartitions from optimization for now. regarding the other question about overlap with grouped (and joined): we should keep all these config objects separate. we have had a lot of trouble in the past trying to use one operation's configuration objects on other operations. it may seem like a good match, but it inevitably puts us in an awkward bind later on. when we have two different operations, we should have two different config classes as well. `grouped` is for configuring `groupby`, `joined` is for configuring `join`, and `repartitioned` is for configuring `repartition`. this may result in code duplication, but it also results in an api that is very consistent, clear, easy to learn, and easy to use. likewise, nesting one operation's configuration inside another operation's configuration is again sacrificing a clean api in favor of code de-duplication. it's not a good tradeoff. far better to configure `groupby` with `grouped` using setters for the direct properties we want to set, and likewise configure `repartition` with `repartitioned` using setters for the direct properties we want to set. then, we don't have to worry about the implications for `groupby` every time we consider changes for `repartition`. as long as they are separate operations, they should have separate configuration classes. if we want to give people control over the parallelism in `groupby`, we should just add `grouped#numberofpartitions(final int numberofpartitions)`. anyway, that's my 2 cents ;)",0,0.9016559720039368
520097324,7170,mjsax,2019-08-09T23:48:22Z,"this does not resolve my main concern, that one passed in `numberofpartitions` and we just ignore the configuration...",0,0.9479556083679199
520181014,7170,vvcephei,2019-08-10T21:30:26Z,"thanks, , i'm not saying that we should (you have a good point). i was just saying that we can add/deprecate methods in different config objects independently, but if we switch to use the same config object in two operations, it ties our hands. on the optimization front, what we could do is make a small change to the optimization algorithm that, when searching upstream to find out if a repartition is necessary, if it finds a repartition operation, it can decide that one is not necessary. then, if you do `ktable...map(...).repartition(repartitioned.numberofpartitions(1234)).groupby(...)`, we won't get a double-repartition, just the one we specifically requested to scale out parallelism. thanks, -john",1,0.9609622955322266
520222537,7170,lkokhreidze,2019-08-11T12:00:59Z,"thank you for the thorough review, much appreciated! i think i've addressed all of the comments mentioned in this pr. to summarize: - repartitioning is now always performed when calling `kstream#repartition` operations. - i've added separate, `unoptimizablerepartitionnode` which is **not** subject of optimization algorithm. `kstream#repartition` operations create `unoptimizablerepartitionnode` when they're called. we can create followup ticket to investigate optimization possibilities for `kstream#repartition`. thank you for the clarification. motivation behind having separate configuration classes per operation does make a lot of sense after your explanation. actually, it won't do double re-partitioning even now (there's even test for this `kstreamrepartitionintegrationtest#shouldcreateonlyonerepartitiontopicwhenrepartitionisfollowedbygroupbykey`). thing is, `kstream#repartition` creates `kstreamimpl` with `repartitionrequred` as `false`. regarding changes for `groupby` operations - those changes are not part of this pr either way, so i think we can leave that discussion for now. i propose to resurrect discussion in mailing list when this pr is merged and have followup discussion on that, wdyt? . even if we go through with `groupby` changes, after reading arguments on why it's not good idea to have same configuration class for multiple operations, it makes more sense to have `numberofpartitions` in `grouped` class for `groupby` operations. personally, i still feel that control over repartition topic configurations is necessary in the long run (sometimes i really want to configure retention per specific repartition topics), and if we gonna have configurations duplicated in each operation class, it may create a lot of pain as well. anyway, since there isn't other use-case yet besides specifying number of partitions per repartition topic, we can cross that bridge when we get there.",1,0.9930242300033569
520973806,7170,lkokhreidze,2019-08-13T19:26:35Z,would appreciate second review.,0,0.7746174335479736
522163447,7170,ableegoldman,2019-08-16T21:54:50Z,"hey, not sure if you looked into this or not but there may be some changes needed in streamspartitionassignor. it tries to validate that repartition topics have been created with the correct number of partitions (defined as the max number of partitions of any source topics) this might not affect this pr so much as the `groupby(repartitioned)` one, but it might be good to verify with an integration test that goes through a rebalance",0,0.988382875919342
522426073,7170,lkokhreidze,2019-08-19T06:05:09Z,"hi , thanks for the info. i've added integration test where rebalancing is triggered.",1,0.9710260033607483
522645678,7170,vvcephei,2019-08-19T16:09:29Z,"hey , thanks for the update! i'll take a look as soon as i can. -john",1,0.9941262006759644
523701860,7170,mjsax,2019-08-22T00:45:27Z,sgtm. why would you need that? repartition topics are configured with infinite retention anyway and kafka streams does explicit purge data calls after records are fully processed.,0,0.9823070168495178
523808665,7170,lkokhreidze,2019-08-22T08:38:05Z,"fair, i was thinking more about older version when retention wasn't set as -1. missed the part that it's long-time fixed now :) anyway, internal topic configuration options is subject for whole new discussion (if there will be ever need for that).",1,0.9884139895439148
524659120,7170,lkokhreidze,2019-08-25T19:59:07Z,"hi , thank you for thorough review, much appreciated. i've addressed all of your comments, please have a look when you got time. i've run `./gradlew streams:reportcoverage` and can verify that main code that i've changed has over ~97% test coverage. i've also added more tests related to `repartitioned#streampartitioner` (both integration and unit tests)",1,0.9888550639152527
526316345,7170,lkokhreidze,2019-08-29T18:54:52Z,"hello , can you please have a look at this pr one more time :) (pinging you just in case so that it won't get lost) regards, levani",1,0.996618390083313
527985897,7170,vvcephei,2019-09-04T16:43:43Z,"thanks, , fwiw, i've left a few replies on threads that are marked as ""outdated"" or ""resolved"". i think the only one that's really important is: [a link]",1,0.9738851189613342
527991663,7170,lkokhreidze,2019-09-04T16:59:22Z,"hi thanks for the review. i've left reply as well. btw, i tried my best to make tests pass on `jdk 11 and scala 2.13` but they seem to fail, right now because of `kafka.api.plaintextconsumertest.testlowmaxfetchsizeforrequestandpartition` is there anything i can do about it?",1,0.9634015560150146
528126203,7170,mjsax,2019-09-04T23:07:16Z,there is [a link] -- feel free to comment on the ticket.,0,0.9751819968223572
528230308,7170,lkokhreidze,2019-09-05T07:07:19Z,"thanks , left the comment. please let me know if you have any more concerns/comments about this pr. thanks, levani",1,0.9861337542533875
529540016,7170,lkokhreidze,2019-09-09T15:43:05Z,"hello just small update - there were conflicts with trunk around `internaltopicconfig` class. i've synced the feature branch with trunk and resolved the conflicts. regards, levani",1,0.7735110521316528
534176196,7170,lkokhreidze,2019-09-23T16:25:49Z,"hi , bumping this thread so it won't get lost. any updates around this pr? would love to finalize this pr and move on to other kips :) regards, levani",1,0.997041642665863
536229726,7170,lkokhreidze,2019-09-28T22:18:56Z,"hello , thanks for the review and thanks for pointing out challenges around `join`. i've added two integration test to address your concerns. basically, in both cases `copartitionedtopicsenforcer` now chooses max partition number from the repartition topics and updates the number of partitions config accordingly. initially, there was a bug in my implementation, basically co-partitioning wasn't working properly when using `repartition` operation. it's fixed by this commit: [a link] problem was that in the `repartition` implementation, when creating new `kstreamimpl` i wasn't passing new set of source nodes, therefore `abstractstream#ensurejoinablewith` was choosing wrong `sourcenodes` for ensuring co-partitioning. i guess this wasn't problem with other internal topics, since they were inheriting number of partitions from the source topic. but since `repartition` operation may change number of partition, it was necessary to set new `sourcenodes` when creating `kstreamimpl`. new integration tests verify that max number of partitions is chosen when doing join operation. as you pointed out, it may be different what user specified, but not sure if it's a bad thing... in my mind it kinda makes sense if internal implementation of kstream chooses proper number of partitions based on the operations. not sure what's the other way around it... also, tested it when topology optimization is specified and added integration test for it: `shouldchoosemaxpartitionnumberfromsourcetopicsforjoinoperationwhentopologyoptimizationisspecified`. seems like optimization algorithm is removing/updating graph nodes without modifying `internaltopologybuilder#copartitionsourcegroups`. this was breaking `copartitionedtopicsenforcer` because nodes that are added in `copartitionsourcegroups` maybe completely removed by the optimizer. therefore, when calling `internaltopologybuilder#copartitiongroups`, it would try to get topic with old node name which results in null. i added `internaltopologybuilder#maybeupdatecopartitionsourcegroups` and it's called during optimization when node is replaced. this was quite fun debugging :) now i know much more how all co-partitioning works around repartition topics. regards, levani",1,0.9688828587532043
538802836,7170,ableegoldman,2019-10-07T00:10:59Z,"hey levani, just a heads up in case you're struggling to get a 3/3 green build with all tests passing: given the number of flaky tests it can be difficult to get a clean pass, so it's useful to record which tests failed on a given run before retesting. that way we can tell if it's the same tests failing every time, or any potentially related streams test, vs flaky tests in connect or core (ideally also create a ticket for each flaky test or if a ticket already exists, comment with the failed link, to draw attention to the flakiest and hopefully get them addressed/fixed)",1,0.630705714225769
538903669,7170,lkokhreidze,2019-10-07T08:55:58Z,"hey , thanks for the suggestion. i've added comments on relevant jira tickets.",1,0.9670630097389221
554428862,7170,lkokhreidze,2019-11-15T16:24:42Z,latest commit has the code that verifies number of partitions when `repartiton()` operation is used next to `join()` notable changes are: 1) introduced new `immutablerepartitiontopicconfig extends repartitiontopicconfig` which has `setnumberofpartitions` as no-op in order to make sure that number of partitions specified by the user won't be altered. 2) `copartitionedtopicsenforcer` has updated logic which throws exception whenever number of partitions do not pass the validation all is covered with unit/integration tests.,0,0.9604477286338806
573815440,7170,lkokhreidze,2020-01-13T18:55:03Z,"hello , sorry for pinging, but would love to get some estimate when this pr will be reviewed. it's getting harder and harder to keep this branch in sync with the trunk. would like to finalize this while i'm able to actively support this kip. also, seems like branch builds aren't triggered, any ideas how to trigger the build?",-1,0.9888665080070496
574055141,7170,cadonna,2020-01-14T08:11:05Z,"retest this, please",0,0.9489298462867737
576140932,7170,lkokhreidze,2020-01-20T07:24:42Z,"totally understandable and sorry for pushing, i'm just myself excited to move this kip forward. thanks for the review, i'll address your comments shortly.",-1,0.836976170539856
581141756,7170,lkokhreidze,2020-02-02T14:39:02Z,ran `streams` test suite locally. the only test that failed was `kstreamimpltest#shouldsupporttriggermaterializedwithktablefromkstream` [code block] seems like it's not connected to this pr. haven't created jira ticket / pr to fix it because as per [a link] seems like people are on it.,0,0.9921315312385559
585833956,7170,lkokhreidze,2020-02-13T16:04:44Z,ran `streams` module tests locally. all the tests passed. can we somehow trigger the jenkins build? cc,0,0.9936769604682922
593048414,7170,vvcephei,2020-03-01T03:31:45Z,test this please,0,0.9870431423187256
596474986,7170,lkokhreidze,2020-03-09T11:35:12Z,"hi any chance we could push this changes by 2.6 release? it's been a while :) regards, levani",1,0.9928820133209229
598798702,7170,lkokhreidze,2020-03-13T16:09:01Z,ran `streams` module tests locally. all the tests passed.,0,0.9907034635543823
599112529,7170,vvcephei,2020-03-14T18:07:44Z,"hey , yes! i'm reviewing it _now_ :) thanks so much for your patience. if it makes you feel better, i've been walking around with this sense of guilt over this pr hanging around.",1,0.9970641732215881
599235445,7170,lkokhreidze,2020-03-15T16:48:04Z,thanks for the update and no worries :),1,0.951590895652771
605279103,7170,vvcephei,2020-03-27T19:35:17Z,test this please,0,0.9870431423187256
608775493,7170,lkokhreidze,2020-04-03T23:03:02Z,"hi , i've addressed your comments, would appreciate another review.",1,0.7158279418945312
610434777,7170,lkokhreidze,2020-04-07T14:54:41Z,"hi small update: [a link] in this commit i've added topology optimization option as test parameter. this pr touches topology optimization (indirectly). in order to make sure that everything works as expected, i though it would beneficial in the integration tests verifying both, `topology.optimization: all` and `topology.optimization: none` configurations. hope this makes sense. regards, levani",1,0.982903778553009
610521827,7170,vvcephei,2020-04-07T17:33:24Z,"wow, that's great. thanks, !",1,0.9976577758789062
611807387,7170,mjsax,2020-04-09T23:53:25Z,merged to `trunk`. congrats ! and thanks a lot for your hard work and patience!,1,0.9963584542274475
612049574,7170,vvcephei,2020-04-10T14:20:21Z,"yes, thank you for seeing this through!",1,0.9752649664878845
572334805,7884,ConcurrencyPractitioner,2020-01-09T01:14:53Z,this pr is ready for review. :),1,0.9931542873382568
572855268,7884,ConcurrencyPractitioner,2020-01-10T03:19:18Z,retest this please.,0,0.9739659428596497
574909313,7884,ConcurrencyPractitioner,2020-01-15T23:41:50Z,"hi thanks for the comments you left! overall, i managed to simplify the code somewhat and removed a couple of methods that was probably not necessary. notably, there is not as many calls involving [code block] as there was previously. hope this was what you wanted. :)",1,0.9965274930000305
579021231,7884,ConcurrencyPractitioner,2020-01-28T00:24:28Z,"i've mostly resolved your comments. i'm working on how we could trigger a call for a clean when the latest delete horizon had been passed. other than that, feel free to add anything else. :)",1,0.9945275187492371
581047464,7884,ConcurrencyPractitioner,2020-02-01T16:47:52Z,do you want to take another look?,0,0.9910902976989746
582220845,7884,ConcurrencyPractitioner,2020-02-05T03:11:56Z,pinging.,0,0.9158015251159668
582709395,7884,junrao,2020-02-06T02:40:14Z,: thanks for the updated pr. will take another look.,1,0.9380503296852112
585015794,7884,ConcurrencyPractitioner,2020-02-12T03:57:23Z,"alright, addressed most of the major comments. i thought that some of the comments might need some discussion before tackling. :)",1,0.9932869076728821
587968002,7884,ConcurrencyPractitioner,2020-02-19T00:09:56Z,"alright, cool. all done. i decided to keep retrievedeletehorizon since the [code block] flag is set in there, and afterwards, it is also used by [code block] as well. since both of them use the same flag (which is located in logcleaner), i think thats the best we can do.",1,0.940751314163208
588366375,7884,ConcurrencyPractitioner,2020-02-19T18:15:57Z,"alright, i think this pr is pretty close. we might need to poll for review if need be.",0,0.9311253428459167
589856382,7884,ConcurrencyPractitioner,2020-02-21T22:04:03Z,"cool, got these comments resolved.",1,0.9867953062057495
591514108,7884,ConcurrencyPractitioner,2020-02-26T16:22:46Z,alright thanks for your patience! got everything done.,1,0.9947326183319092
592296237,7884,ConcurrencyPractitioner,2020-02-28T03:36:06Z,thanks for the comprehensive review of my test code changes! it looks like i was a little to aggressive with my find / replace all usage. (that lead to a lot of long.maxvalues being replaced sometimes unnecessarily by largedeletehorizon),1,0.9927757978439331
592297453,7884,ConcurrencyPractitioner,2020-02-28T03:40:43Z,got these comments addressed.,0,0.9809868931770325
592984991,7884,ConcurrencyPractitioner,2020-02-29T19:20:59Z,"thanks for these comments! about the comments that i left unaddressed. turns out the tests failed if we only do two passes over the batch when we in fact need three. i added some log statements, and this was the following behavior (this is for testabortmarkerremoval): 1. on this pass, containstombstonesortxnmarker is false. i.e. we cannot remove the transaction marker yet (since oncontrolbatchread() returned false). 2. on the second pass, containstombstonesortxnmarker is now true. we can remove the transaction marker now. in this case, we have set the delete horizon, but have not _removed_ the delete horizon marker. 3. it is finally on the third pass that we can remove the transaction marker. i found this was the resulting behavior (which lead to the need for three passes). i think the situation is similar for the other control batch tests as well.",1,0.9880820512771606
596021569,7884,ConcurrencyPractitioner,2020-03-07T00:40:52Z,"cool, we are good.",1,0.9939941763877869
596026016,7884,junrao,2020-03-07T01:09:27Z,ok to test,0,0.992953360080719
596026481,7884,junrao,2020-03-07T01:12:35Z,i also kick off system tests on this pr. [a link],0,0.9955059289932251
596050270,7884,junrao,2020-03-07T05:45:07Z,ok to test,0,0.992953360080719
596149722,7884,junrao,2020-03-07T23:55:49Z,add to whitelist,0,0.992867112159729
596151455,7884,ConcurrencyPractitioner,2020-03-08T00:18:54Z,test this please,0,0.9870431423187256
596226854,7884,ConcurrencyPractitioner,2020-03-08T16:59:34Z,ok to test,0,0.992953360080719
596226994,7884,ConcurrencyPractitioner,2020-03-08T17:00:49Z,"found the spotbugs violation. turns out when i look through the console output, i can't find the error because its never logged. (instead, its stored in some xml file which i cannot access)",0,0.9754653573036194
596231999,7884,junrao,2020-03-08T17:50:26Z,ok to test,0,0.992953360080719
596667016,7884,junrao,2020-03-09T17:24:10Z,both tests seem to be failing at the following test. [code block],0,0.9557912349700928
596825345,7884,ConcurrencyPractitioner,2020-03-09T23:21:53Z,can you retrigger tests? i was not able to replicate the issue in local.,0,0.9896768927574158
596841889,7884,junrao,2020-03-10T00:27:32Z,ok to test,0,0.992953360080719
596896786,7884,junrao,2020-03-10T04:29:38Z,ok to test,0,0.992953360080719
597320638,7884,ConcurrencyPractitioner,2020-03-10T21:13:25Z,"i did a little bit of research. it appears that my pr is _not_ responsible for this failing test. perhaps, some prior pr broke it. see test result for this pr: [a link]",0,0.968323290348053
597348217,7884,junrao,2020-03-10T22:17:30Z,it seems that test failure was just fixed by kafka-9682. triggering another test.,0,0.9867435693740845
597348253,7884,junrao,2020-03-10T22:17:37Z,ok to test,0,0.992953360080719
597348559,7884,junrao,2020-03-10T22:18:35Z,ok to test,0,0.992953360080719
597419512,7884,ConcurrencyPractitioner,2020-03-11T03:00:14Z,all tests are green.,0,0.987215518951416
599785168,7884,ConcurrencyPractitioner,2020-03-16T22:25:13Z,any comments on logcleaner?,0,0.9903662204742432
600281660,7884,ConcurrencyPractitioner,2020-03-17T20:29:25Z,"i think i might know why the ""add to whitelist"" command doesn't work. do you have to be a contributor in order to trigger a test? i checked the contributors list, but from what i could tell, my account handle isn't listed (which is kind of strange, since i've submitted a handful of prs in the past).",0,0.9488081336021423
600348752,7884,junrao,2020-03-17T23:28:52Z,: filed [a link] to figure out the whitelist issue.,0,0.9937870502471924
600876622,7884,ConcurrencyPractitioner,2020-03-18T21:50:36Z,ok to test,0,0.992953360080719
601433830,7884,ConcurrencyPractitioner,2020-03-19T21:51:04Z,all comments addressed. see if there is anything else that we might need to account for.,0,0.9936394095420837
605705828,7884,ConcurrencyPractitioner,2020-03-29T21:38:08Z,pinging .,0,0.813514232635498
613061861,7884,ConcurrencyPractitioner,2020-04-13T19:42:34Z,pinging for review,0,0.9886258244514465
616751692,7884,junrao,2020-04-20T19:08:59Z,: we now have [a link] you can add yourself to jenkins's whitelist by following [a link] .,0,0.9800145626068115
616773372,7884,ConcurrencyPractitioner,2020-04-20T19:52:32Z,ok to test,0,0.992953360080719
616776257,7884,ConcurrencyPractitioner,2020-04-20T19:58:32Z,ok to test,0,0.992953360080719
616776614,7884,ConcurrencyPractitioner,2020-04-20T19:59:20Z,cool. it's just that should i edit the [code block] as part of this pr? or will i need to do it some other way?,1,0.9660069346427917
616787194,7884,junrao,2020-04-20T20:21:52Z,: you can just submit a separate pr to add yourself in .asf.yml.,0,0.9944809675216675
616819297,7884,ConcurrencyPractitioner,2020-04-20T21:28:01Z,"alright, got it done.",0,0.936964213848114
617537213,7884,ConcurrencyPractitioner,2020-04-22T04:09:02Z,ok to test,0,0.992953360080719
617537261,7884,ConcurrencyPractitioner,2020-04-22T04:09:16Z,test this please,0,0.9870431423187256
618659683,7884,ConcurrencyPractitioner,2020-04-23T20:46:08Z,"i don't think the .asf.yaml worked. tried to trigger a few test rounds, but jenkins didn't respond.",0,0.9506465792655945
618669412,7884,junrao,2020-04-23T21:03:35Z,"could you try ""retest this please""? if it still doesn't work, you can file an apache infra jira for help.",0,0.9930931329727173
618766295,7884,ConcurrencyPractitioner,2020-04-24T02:25:40Z,did try on another pr. looks like it didn't work. i will fire a jira.,0,0.9643859267234802
618768078,7884,ConcurrencyPractitioner,2020-04-24T02:32:37Z,reported in jira here: [a link],0,0.9956045150756836
622490343,7884,ConcurrencyPractitioner,2020-05-01T17:48:29Z,do you have time to review? just give me a heads-up if there are some comments left unaddressed.,0,0.9845871925354004
737674537,7884,wushujames,2020-12-03T05:26:37Z,hi . what is the status of this pr? we are also experiencing [a link] . thanks!,1,0.98988276720047
738119273,7884,junrao,2020-12-03T16:28:11Z,: this pr is mostly ready. it's just waiting for another committer more familiar with the transactional logic to take another look. : would you be able to rebase this pr? thanks.,1,0.9752163290977478
760814414,7884,akamensky,2021-01-15T10:33:44Z,this pr has been stale since april 2020. when would it be ready to merge? we are hitting this issue and it causes insanely long startup times in our applications as they need to read all the tombstones that are not being removed.,-1,0.9475837349891663
761223136,7884,ConcurrencyPractitioner,2021-01-15T22:06:10Z,migrating to a new pr. you could find it here #9915.,0,0.9926861524581909
289147056,2735,apurvam,2017-03-24T21:27:04Z,cc,0,0.9699252247810364
289222942,2735,apurvam,2017-03-25T16:34:23Z,"the newly introduced integration test seems to be flaky on jenkins. there are produce exceptions when bouncing brokers. i didn't see this locally, and i have run it 100's of times. will look into it.",0,0.9412326812744141
289626999,2735,apurvam,2017-03-28T00:35:32Z,"i found the reason why the new integration test was failing. with a recent refactor of `sender.completebatch`, we only requested a metadata update on non-retriable errors. this was a regression, and as a result, when the leader for a partition changed, the metadata would not get updated and all the retries would get exhausted. i moved the metadata update code to the correct place, and the tests are passing now. i also addressed other pr comments. cc",0,0.7979668974876404
289627070,2735,apurvam,2017-03-28T00:36:08Z,"somehow the new integration test only failed with jdk7 or scala 2.12, neither of which i run locally. so i never caught it until this pr.",0,0.9762826561927795
289952136,2735,apurvam,2017-03-29T01:12:07Z,"current todos from discussing with jun offline: 1. filed [a link] as a future improvement. 2. need to remove the `log.updateidmap` which removes entries before the current dirty offset from the pid map. this is a bug which would result in real entries being lost. 3. need to name the snapshot files appropriately. currently they are in the partition directory, but have the topic partition in their name, which is redundant. 4. need to ensure that `produceridmapping.maybetakesnapshot` is called periodically.",0,0.9872446656227112
290309068,2735,apurvam,2017-03-30T05:50:04Z,"i addressed most of your comments, except two. especially for `sendandawaitinitpidrequest` comment, i would like to sync face to face tomorrow.",0,0.8514869809150696
290622112,2735,apurvam,2017-03-31T05:47:44Z,latest run of the system tests on this branch succeeded: [a link] kicked off a muckrake run against this branch too : [a link],0,0.9636521339416504
290625051,2735,apurvam,2017-03-31T06:04:45Z,"thanks for the review , , and .. i think i have addressed all the comments. the system tests are running as well (links above).",1,0.9740505218505859
290880753,2735,apurvam,2017-04-01T00:49:22Z,"the jenkins builds are puzzling. mostly different streams tests fail in each run. there hasn't been a failure in core or clients for a while. the perf numbers are being collected here: [a link] so far, so good. actually enabling the idempotent producer costs 20% throughput. but using the new code with idempotent producer turned off seems to have no effect. a muckrake test is finally running properly and seems to be passing: [a link] i also addressed most of the major comments from ismael, ie. those which had to do with adding test cases or improving error messages or improving documentation. the remaining ones are more to with minor code style.",1,0.47131699323654175
290935845,2735,apurvam,2017-04-01T17:47:53Z,muckrake test passed: [a link] system test against this branch also passed: [a link] investigating the jenkins branch builder failures.,0,0.9886499643325806
290937397,2735,apurvam,2017-04-01T18:13:52Z,latest failure (jdk8 and scala 2.11): [code block] somehow my local runs are super stable. i don't know what would cause this kind of thing.,0,0.8628448843955994
291037102,2735,junrao,2017-04-03T02:40:36Z,: thanks for the patch. lgtm,1,0.9780473113059998
129892398,130,lazyval,2015-08-11T14:06:46Z,it's quite awkward to see commits like this,-1,0.9930412769317627
129935567,130,guozhangwang,2015-08-11T15:42:11Z,"apologies for the commits history, i was fighting with git merge history back then from two branches and hence the commits was not well organized. i will create another pr with squashed commits after addressing the collected comments from this pr.",-1,0.7519853711128235
131564609,130,rhauch,2015-08-16T14:44:27Z,"this is an excellent proposal, and after a quick pass this pr looks good. more detailed comments and questions to follow.",1,0.9945967197418213
132316453,130,rhauch,2015-08-18T18:50:23Z,"is there a way to make the sources and processors created by the topology aware of or dependent upon the supplied configuration? for example, let's say my job's configuration has a parameter that, if set one way, will result in a slight variation of the topology (e.g., an extra filtering processor between the source and my primary processor). is that possible?",0,0.9898374676704407
132731196,130,guozhangwang,2015-08-19T18:20:11Z,"thanks for the comments. just to be clear we are actively working on addressing them and will respond them individually once the next version of this patch is finished, with a squashed commit history.",1,0.9499384164810181
133943310,130,rhauch,2015-08-23T21:54:11Z,", i'm willing to help resolve issues, add test cases, make suggestions via patches, and even add javadoc. but i suspect that'd be easier after after you squash commit history. please let me know what you think.",0,0.5706316828727722
135907602,130,guozhangwang,2015-08-28T23:05:51Z,"for ""is there a way to make the sources and processors created by the topology aware of or dependent upon the supplied configuration? for example, let's say my job's configuration has a parameter that, if set one way, will result in a slight variation of the topology (e.g., an extra filtering processor between the source and my primary processor). is that possible?"" for this case, could it be written by adding a filter with a config that depending on its value, do no-op or the real filtering logic? since all these operations will be in-memory, the only overhead will be checking the configured flag for each record, which should be negligible.",0,0.9882863163948059
135908565,130,guozhangwang,2015-08-28T23:15:20Z,"thanks for being interested to contribute! we are currently shooting for a second patch next monday / tuesday, it changes the processor apis and threading models quite a bit from the original patch and hence i will also update the kip wiki accordingly. it will be great if you can then take that patch and help adding more unit tests / java docs / reviews / anything.",1,0.9941537976264954
138945194,130,rhauch,2015-09-09T15:22:45Z,", in case you missed it, here's a pr for the npe in meteredkeyvaluestore mentioned [a link]: [a link]",0,0.9610273838043213
142681892,130,rhauch,2015-09-23T18:03:50Z,", i'm not sure what happened, but `build.gradle` is still missing this code: [code block] and without it the build does not upload/publish the test jar to maven (at least locally). my original pr (#34) to add these and a few other lines was merged into this branch, but it looks like auto-merge in 74455f29f2 put them in the 'tools' area rather than 'streams'. again, without it clients cannot reuse any of the test classes (like `kstreamtestdriver`) in their own tests.",0,0.9417872428894043
142687808,130,guozhangwang,2015-09-23T18:23:47Z,"you are right, the tools package was added at roughly the same time and hence probably messed up with the auto-merge. we can add that again.",0,0.9907338619232178
142719926,130,rhauch,2015-09-23T20:31:04Z,", i also have a `keyvaluestore` implementation that maintains an in-memory, limited-size lru cache (e.g., `inmemorylrucachestore`) of recently used entries. sometimes a processor wants to track up to _n_ recently-used items, and this is a convenient way to do this. i'm willing to contribute it via a new pull request if you are interested. it does need a few enhancements to `meteredkeyvaluestore` to better support entries being ""automatically"" removed by the 'inner' store. but a larger problem is that `meteredkeyvaluestore` currently assumes it can get the key and value serializers and deserializers from the context. iiuc this is not always valid since those are merely the _default_ key and value serializers and deserializers. for example, i might have a topology that uses multiple processors with different key and value types, in which case the context's default (de)serializers will be valid for only one of the processors. or, i might have a single processor that uses a key value store with different key and value types than the processor. so this seems like an invalid assumption that needs to be fixed. if you agree that the latter is a problem, i'd be happy to create a pr to address it. if you're interested in the `inmemorylrucachestore` class, that could be a separate pr.",0,0.9519295692443848
142749083,130,guozhangwang,2015-09-23T22:31:58Z,"i agree about ser-de, maybe we can allow users to pass in their (de)serializers upon construction of the key-value store. about `inmemorylrucachestore`, feel free to go ahead with another pr.",0,0.9827390909194946
142762261,130,rhauch,2015-09-23T23:46:51Z,", see my [a link] for the serdes changes; should be up-to-date with the `streaming` branch. feedback appreciated.",1,0.900597095489502
143052732,130,rhauch,2015-09-24T21:14:15Z,", i created a pr for the [a link] and a mini test framework for key-value stores, with new unit tests for all current `keyvaluestore` implementations. it does depends on my proposed [a link] changes in key-value stores.",0,0.9880563020706177
143380723,130,guozhangwang,2015-09-26T00:10:32Z,"update the pr body before merging, and the original message is here: --- some open questions collected so far on the first patch. thanks . - topology api: requiring users to instantiate their own topology class with the overridden build() function is a little awkward. instead it would be great to let users explicitly build the topology in main and pass it in as a class: [code block] so the implementation of kstream.filter look instead like this: [code block] the advantage is that the user code can now get rid of the whole topology class with the builder. i think the order of execution for that api is quite unintuitive. - we can probably move the forward() function from processor to processorcontext, and split processorcontext into two classes, one with all the function calls as commit / send / schedule / forward, and another with the metadata function calls as topic / partition / offset / timestamp. - can we hide the chooser interface from users? in other words, if users can specify the ""time"" on each fetched messages from kafka, would a hard-coded mintimestampmessagechooser be sufficient so that we can move timestamptracker / recordqueue / chooser / recordcollector / etc all to the internal folders? - shall we split the o.a.k.clients into two folders, with o.a.k.clients.processor in stream? or should we just remove o.a.k.clients.processor and make everything under o.a.k.stream? in addition, currently there is a cyclic dependency between that two, would better to break it in the end state. - consider moving the external dependencies such as rocksdb into a separate jar? for example we can just include a kafka-stream-rocksdb.jar which includes the rocksdbkeyvaluestore only, and later on when we deprecate / remove such implementations we can simply remove the jar itself. - the way the keyvaluestore is created seems a bit weird. since this is part of the internal state managed by kafkaprocessorcontext, it seems there should be an api to create the keyvaluestore from kafkaprocessorcontext, instead of passing context to the constructor of keyvaluestore. - merge processorconfigs with processorproperties. - we can potentially remove the processor argument in processorcontext.schedule().",1,0.9732827544212341
320162288,3621,lindong28,2017-08-04T05:48:45Z,"can you help review this patch if you have time? thank you! this patch has implemented all features in kip-113 except the ability to move replica between log directories on the same broker. the implementation of that feature is more complicated because it requires the creation of a temporary replica on the broker. thus i choose to separate the implementation of kip-113 into two patches. note that this patch allows user to specify the destination log directory of a replica if the replica has not been created on the broker yet. more specifically, user can use adminclient.changereplicadir() before creating the reassignment znode so that the replica will be created in the destination log directory when broker receives leaderandisrrequest later. this allows user to balance load across log directories using the `kafka-reassign-partitions.sh` as long as the replica is always moved to a given log directory on another broker. the caveat is that the replica may be created in the wrong log directory if the broker restarts after it received changereplicadirrequest but before it receives leaderandisrrequest.",1,0.9898087978363037
321054272,3621,lindong28,2017-08-08T19:15:43Z,tests have been added to cover every new request/response/api and i have reviewed the patch end-to-end. it is fully ready for review now. thanks!,1,0.9894271492958069
322349033,3621,lindong28,2017-08-15T00:56:46Z,thanks much for your review. i have addressed all comments and reviewed the patch end-to-end. can you take another look? thanks!,1,0.9918391108512878
323557883,3621,lindong28,2017-08-20T01:44:34Z,"thanks much for taking time to review the patch! i have addressed most of the comments and rebased the patch onto trunk head. the only one that i am not sure is whether we should rename `alterreplicadirrequest` to `alterreplicalogdirrequest` (and similarly for other related classes). after this one is addressed, i will review the patch myself end-to-end and let you know.",1,0.9891352653503418
323606600,3621,lindong28,2017-08-20T19:33:42Z,"could you please provide some comment as to whether we should rename `alterreplicadirrequest` to `alterreplicalogdirrequest`? prefer `alterreplicalogdirrequest` so that it is more explicit that we are referring to log directory instead of arbitrary directory. on the other hand, i prefer `alterreplicadirrequest` because i think `alterreplicalogdirrequest` is a bit verbose. i also think it should be clear enough to user/developer that the dir means log dir as long as the field name in `alterreplicalogdirrequest` is `log_dir`.",0,0.9878121018409729
323897171,3621,junrao,2017-08-22T01:52:31Z,i can take a look at the patch in the next day or two.,0,0.9774935245513916
324254692,3621,lindong28,2017-08-23T08:09:46Z,thanks much for taking time to review the patch! i have addressed all the comment. can you take another look when you get time?,1,0.9920433163642883
324383296,3621,cmccabe,2017-08-23T16:04:46Z,"thanks for the patch. it looks good overall. i think we should split the changes to the options class, to create a base class, into a separate jira. it's not related to kip-113. i'm also not completely convinced we should do it, but perhaps we can discuss that in another jira or on the mailing list.",1,0.9902515411376953
324399622,3621,lindong28,2017-08-23T17:04:07Z,thanks much for the comment! i personally don't mind whether we refactor the options class in this patch or in a separate patch. i just talked to and he prefers to do it in this patch if there is no concern. the argument is that this patch adds three more option classes to apache kafka and it is reasonable to try to reduce the extra code in this patch. can you explain a bit more what is your concern with the change to the option class? we can move it to another jira if there is anything worth further discussion about this change.,1,0.9908223152160645
325093325,3621,lindong28,2017-08-26T06:28:00Z,"thanks much for your review! i have addressed all comments except for the last one which we are discussing. and i have gone over the patch end-to-end, removed unused imports i can find and improved comments. the patch has been rebased onto trunk head.",1,0.9891772866249084
326144106,3621,lindong28,2017-08-30T23:09:09Z,thanks for taking time to review the patch! i have rebased patch onto trunk head and addressed all comments from jun. do you have time to take a look and commit the patch? thanks!,1,0.9946194887161255
326774010,3621,lindong28,2017-09-02T23:05:01Z,"thanks much for taking time to review the patch again in detail! i have updated the patch to add comments and throw exception if an expected error is found. regarding the error code in describelogdirresponse, it seems that there is currently no realistic issue with assuming cluster_authorization_failed when the response is empty -- it doesn't affect correctness or performance. then the reason for adding the response level error is purely ideological, which applies to other requests as well (e.g. describe_groups_response or describe_configs_response). we have discussed this offline previously. it probably makes sense to have a response level error for every response instead of only describelogdirresponse. can i open a separate ticket and make a separate pull request for this? i would like to avoid making non-trivial change to this patch at this moment so that it can be finished soon. i feel that the use of this extra field in describelogdirresponse alone is kind of unnecessary and less useful. it probably makes more sense to add the response level error in `abstractresponse` to solve a bigger problem. or we can add this response level error to `describelogdirresponse` in the future when it is needed. doe this make sense?",1,0.9889410138130188
326786682,3621,becketqin,2017-09-03T06:14:19Z,"yeah, it is probably worth adding a response level error for all the requests. we can do it in separate kip.",0,0.9871929883956909
326787087,3621,becketqin,2017-09-03T06:26:51Z,"thanks for the patch. merged to trunk. thanks a lot for the review, and",1,0.9934816360473633
341874911,3621,ijuma,2017-11-04T06:04:11Z,"i think this pr broke binary compatibility for the adminclient, code compiled against 0.11.0 fails like: [code block]",0,0.9540842175483704
341942416,3621,lindong28,2017-11-05T01:30:42Z,"this patch added new methods in the interface adminclient. according to [a link] this should not cause binary incompatibility. not sure why you see that error. i also tried the following steps to hopefully reproduce the error: - git clone -b upgrade-to-11.0.0 [a link] - compile with ./gradlew jar - replace ./build/dependant-libs/kafka-clients-0.11.0.0.jar with kafka-clients-1.0.0-snapshot.jar, where kafka-clients-1.0.0-snapshot.jar is generated by apache kafka adefc8ea076354e. - run ./bin/kafka-monitor-start.sh config/kafka-monitor.properties it appears that the project can still be compiled and run after i replaced the jar. can you tell me how i can reproduce this error? also, do you know which change in this patch can break binary incompatibility, e.g. addition of new methods?",0,0.9787068367004395
341957639,3621,ijuma,2017-11-05T09:43:27Z,"sorry, i thought it was obvious from the error message, but i noticed now that the person that ran into it only pasted the method affected and not the actual error. the binary compatibility issue is due to the removal of the `timeoutms` method from various classes. the method now exists in the abstract class, so code that is recompiled works (source compatible), but code that is not recompiled breaks with a `nosuchmethoderror`. we should reintroduce the removed methods in 1.0.1 and trunk to fix the issue. would you mind filing a jira, please?",-1,0.9768127202987671
342006765,3621,lindong28,2017-11-05T21:22:01Z,sure. i created [a link] we can continue discussion there.,0,0.9813448190689087
571192193,7898,dongjinleekr,2020-01-06T15:51:36Z,note: - [a link] - [a link],0,0.9897171258926392
571371789,7898,ijuma,2020-01-07T00:11:21Z,this requires a kip since the log4j2 config is not compatible with log4j.,0,0.9916455149650574
571505061,7898,dongjinleekr,2020-01-07T09:23:37Z,no problem. thank you for your guidance. :smile: stay tuned!,1,0.9973828196525574
574137873,7898,rgoers,2020-01-14T11:49:39Z,log4j 2.13.0 contains experimental support for some log4j 1 configuration flies. see [a link],0,0.9917035102844238
574187380,7898,ijuma,2020-01-14T14:00:43Z,"that's awesome, thanks for sharing.",1,0.9937467575073242
589484883,7898,akamensky,2020-02-21T03:47:49Z,"i just had to deal with configuring filtered logs in kafka and was shocked to find it uses log4j 1.2.17. log4j v1 has been dead since 2015. 5 years ago it was known that it should not be used for any new designs and applications should migrate to v2, yet still apache kafka stuck with that? wanted to raise a ticket, but there is this pr, which does not seem to be getting merged anywhere... dear lord...",-1,0.9947354197502136
589655512,7898,dongjinleekr,2020-02-21T13:36:01Z,"i am sorry to hear that. i am almost done the kip and will start the discussion next week. if you are interested in this issue, please join in. have a nice weekend.",-1,0.9738194346427917
603000463,7898,OneCricketeer,2020-03-24T03:59:39Z,"man, i really wish more apache projects started this ""upgrade""",-1,0.9710990190505981
603269642,7898,dongjinleekr,2020-03-24T14:25:34Z,"sorry for the delay. while working on this issue, i found that this upgrade is much more complicated than i first expected; it is related to lots of module dependencies, api changes, test code modification, and providing backward-compatibility for the logging configuration. anyway, it is almost done. i successfully upgraded the whole project and now working with some race conditions on test suites. i hope i can complete it in a couple of days. :winking_face:",-1,0.9734202027320862
603402312,7898,OneCricketeer,2020-03-24T17:45:47Z,"no worries. i didn't realize the backwards compatible issues either. i was under the impression that slf4j bridges handled that. personally, i've been using logback successfully for years",1,0.929606556892395
604911816,7898,OneCricketeer,2020-03-27T09:53:05Z,"so, i'm actually working on a project that i just started... can verify (part of) this log4j-slf4j + slf4j-log4j12 bridging definitely works without issue [a link] notice too [code block]",0,0.9715738892555237
605995589,7898,dongjinleekr,2020-03-30T13:22:25Z,"here is the wip update. i almost completed the migration into log4j2 `2.13.1` but here are some issues: 1. i migrated all test methods into logj42 api, using the way log4j2 itself does; however, i failed to migrate `statemanagerutiltest`. it seems like this class is tightly coupled with kafka streams' slf4j logging mechanism, but i don't understand it yet. 2. the updated test suites working correctly when running individually, but if run at once (e.g., `./gradlew :streams:test`), some test suites go so flaky. i am also tracking down the reason. if you can give me some advice, it will be a great help! :smiley:",0,0.947777271270752
607126286,7898,tombentley,2020-04-01T08:59:42Z,"i hadn't realised you were working on this via kafka-9366. i was looking at it via kafka-1368. you've made more progress than me, so happy for you to take it forward. but there are a couple of issues i noticed in the course of my effort: 1. the `log4jcontrollermbean.getloggers` returns a scala wrapper implementation of `java.util.list`, which means that jmx tools (e.g. `jvisualvm`) can't deserialize the list unless they have scala library on their classpath. that's easily fixed by returning a `java.util.arraylist` copy of the list. 2. i suspect you already know this, but with log4j there were loggers for things like `kafka.controller` because they appeared in the config file, even though a logger with that name was never created in the code. because log4j2 separates loggers and logger configurations the call `logcontext.getloggers` only returns the loggers created in code. so afaics you won't be able to change the log level for `kafka.controller`, and would have to change the level of all descendant loggers individually. i guess this is a regression, since `log4jcontroller` is used for `alterconfigs` rpc.",1,0.9915661811828613
607328633,7898,dongjinleekr,2020-04-01T15:45:53Z,"here is the update, with rebasing onto the latest trunk. now all tests run properly. however, it still has a problem: when i ran the test suites individually (e.g., `./gradlew :streams:test --tests org.apache.kafka.streams.kafkastreamstest`) all tests passes clearly. however, if i tried to run them in package-wide (e.g., `./gradlew :streams:test --tests org.apache.kafka.streams.internal.*`) or all-in-once (e.g., `./gradlew :streams:test`) the tests go so flaky and some tests fail, with one of the following error messages: [code block] [code block] to understand these errors, here is a background: to validate the logging messages, kafka has been used a test appender (i.e., `listappender` here) attached to the root logger. although this pr updates log4j 1.x to 2.x, the overall approach has not changed. this approach works well when running individually, but when running in batch, the test suite randomly fails to initialize the `listappender` (type 1 error) or forward the log message to `listappender`(type 2 error); i ran the tests more than a hundred of times but what i found was it is totally random and be affected by gradle's `maxparallelforks` parameter - it seems like there is a problem deep inside of log4j here. i have been working on this issue for the last week could not find any perfect solution. if any of you have some ideas on this, it would be a great help. thanks in advance. :smiley:",0,0.8649480938911438
607331030,7898,dongjinleekr,2020-04-01T15:49:50Z,thanks for your valuable comments - absoultely it will be a great help! i am now applying the comments from so as soon as it is completed i will review your comments again and leave a feedback. :smile:,1,0.9974173307418823
607512708,7898,OneCricketeer,2020-04-01T22:04:13Z,"i've written test appenders in other projects just to verify contents of log messages, but not really sure the implications of race conditions on that.",0,0.9004678130149841
612980927,7898,dongjinleekr,2020-04-13T16:42:38Z,"it seems like we need to consult to log4j mailing list. okay, i will have a try.",0,0.982988178730011
613082686,7898,OneCricketeer,2020-04-13T20:30:09Z,e.g. [a link],0,0.9937499165534973
628732206,7898,jeffhuang26,2020-05-14T16:04:06Z,what is timeline for merging this pr?,0,0.9949210286140442
636188471,7898,ijuma,2020-05-29T20:56:07Z,"can please submit a kip for this? we should have a better good idea of the compatibility implications by now, right?",0,0.9923316240310669
636191680,7898,ijuma,2020-05-29T21:04:48Z,"oh, i had missed the comment about the errors we are seeing when running the tests. it may be worth upgrading to the latest release in case it has been fixed.",0,0.9867563247680664
637684403,7898,dongjinleekr,2020-06-02T17:04:48Z,"all // sorry for being late, i just got out from my last project; i will have a look at this pr this weekend.",-1,0.9863424897193909
648795743,7898,dongjinleekr,2020-06-24T12:41:51Z,"here is the fix. i completed to implement all the features, migrating tests to follow log4j2 api, and rebasing onto the latest trunk, but there is a problem in logging message validation. ![a link] when i run `:stream:test` task in my dev environment, the following 10 tests fail: - `streamsconfigtest`: `shouldlogwarningwhenpartitiongrouperisused` - `kstreamktablejointest`: `shouldlogandmeterwhenskippingnullleft[key,value]withbuiltinmetricsversion[latest,0100to24]` - `inmemorysessionstoretest`: `shouldlogandmeasureexpiredrecordswithbuiltinmetricsversion[latest,0100to24]`, `shouldnotthrowinvalidrangeexceptionwithnegativefromkey` - `rocksdbtimestampedstoretest`: `shouldmigratedatafromdefaulttotimestampcolumnfamily`, `shouldopennewstoreinregularmode` however, if i run the test suites `streamsconfigtest`, `kstreamktablejointest`, and `inmemorysessionstoretest` individually, they work fine. and if i run `rocksdbtimestampedstoretest` test suite, it fails; the expected log message does not forwarded to the appender. ![a link] in contrast, if i run the test methods individually, they also work fine: ![a link] it seems like there is a problem with log4j in forwarding the log message to the appender. (or is the appender closed before the log message arrives?) but i can't certain; i followed the way log4j2 test suites do, but could not find similar cases in their codebase. i tried to fix this problem for several days but not succeeded. if you have some spare time, could you check out this pr and run the tests on your machine? i am working with ubuntu 20.04 + openjdk 8. i am curious the same tests also fail in the other environments. cc/",0,0.6385635137557983
649282794,7898,dongjinleekr,2020-06-25T07:02:08Z,retest this please.,0,0.9739659428596497
668034540,7898,dongjinleekr,2020-08-03T13:51:17Z,"finally, it is finished! :congratulations: all features and tests are now successfully migrated into log4j2 api and passes clearly! i changed the pr title and preparing kip. stay tuned! :smiley: cc/",1,0.9961482286453247
669229785,7898,dongjinleekr,2020-08-05T14:34:36Z,here is the kip - [a link] could you have a look? i have thought you must be the perfect reviewer [a link]. :smile:,1,0.9960343241691589
673145549,7898,svudutala,2020-08-12T22:41:54Z,what is timeline for merging this pr and making this upgrade available?,0,0.9950928688049316
673305906,7898,dongjinleekr,2020-08-13T07:14:37Z,it is what exactly i hope to ask the pmc members and committers. it seems like they are too busy right now - let us wait for a while. +1. rebased onto the lastest trunk.,0,0.9763384461402893
691668375,7898,dongjinleekr,2020-09-13T12:56:45Z,"rebased onto the lastest trunk, with including sliding window support. could you have a look?",0,0.9915838241577148
693414587,7898,dongjinleekr,2020-09-16T13:43:50Z,could you have a look? :pray:,1,0.9827302694320679
693453914,7898,ijuma,2020-09-16T14:44:37Z,has the kip been approved?,0,0.9951505661010742
693552944,7898,dongjinleekr,2020-09-16T17:32:51Z,of course not. but i hoped to reboot the discussion. (thanks for the kind reponse! :+1:),1,0.9965319633483887
701008405,7898,dongjinleekr,2020-09-29T21:45:36Z,"hi all, here is the update. all compatibility breaks caused by the root logger name change between log4j and log4j2 (`""root""`  `""""`) is now resolved. plus, i also migrated raft module into log4j2. cc/",1,0.8396101593971252
733591265,7898,dongjinleekr,2020-11-25T09:41:01Z,"here is the update. i rebased the pr onto the latest trunk, with: 1. upgrade log4j to log4j2: this is the main commit. 2. trivial improvements (typos, etc.) 3. wip: enable ignored tests in `plaintextadminintegrationtest` ([a link], cc/ ) it now includes 's [a link]. cc/",0,0.9745101928710938
754531924,7898,GandophSmida,2021-01-05T09:55:08Z,the newest kafka version is 3.7.0.could i know the reseason that change the log4j 1.x to log4j 2.x isn't merged.,0,0.9946868419647217
776772272,7898,dongjinleekr,2021-02-10T15:08:37Z,"rebased onto the latest trunk, with including [a link] and [a link] by could you have a look? :pray: as you already know, [a link] is approved. sorry for being late. i am planning to provide a customized preview version for this feature.",-1,0.9603076577186584
777453160,7898,ch4rl353y,2021-02-11T13:22:57Z,"regarding the cve-2019-17571 from [a link] is there another way to mitigite the risk? we're looking for a temporary solution until this pr finally gets approved, but are not sure if and how the vulnerability could even get exploited. any thoughts?",0,0.968582272529602
777454845,7898,dongjinleekr,2021-02-11T13:25:18Z,"which version are you using? i am now working for a customized patch for 2.6.0 and 2.7.0, with docker image.",0,0.9936129450798035
777461164,7898,ch4rl353y,2021-02-11T13:34:46Z,"we're using strimzi/kafka / 0.21.0-kafka-2.7.0 our sca scanning tool (jfrog xray) found this cve among many others (speaking of third party lib cves only). we're just wondering if there's a way (e.g. via message sanitizing or logging config adjustments, etc.) to be sure the mentioned cve cannot be exploited.",0,0.9914571642875671
777469267,7898,dongjinleekr,2021-02-11T13:47:08Z,"great. :+1: as soon as i complete the custom release, i will have a look at strimzi docker image. i guess it will take at least one week.",1,0.9970223307609558
777482442,7898,ch4rl353y,2021-02-11T14:05:37Z,alright thx for your fast response! we'll keep an eye on this pr :),1,0.9923762679100037
783322141,7898,priyavj08,2021-02-22T11:55:54Z,"i have similar question, can this security vulnerability cve-2019-17571 get exploited. i use kafka operator from banzaicloud 0.12.3/ kafka:2.13-2.6.0 when will the custom release be available? thanks",1,0.9573230147361755
784202224,7898,dongjinleekr,2021-02-23T13:27:28Z,"sorry for being late. finally, here it is! i succeeded to backport this feature to the latest 2.7.0 release. (commit 466b798412) - working branch: [a link] - custom build distribution: [a link] - 2.7.0 patch: [a link] now i will consult to strimzi, banzaicloud team for a custom release. i am also preparing a graalvm based docker image distribuion [a link]. stay tuned! :smile: cc/",1,0.890070915222168
784901850,7898,dongjinleekr,2021-02-24T08:29:43Z,"rebased onto the latest trunk, with minor log4j2 configuration corrections. if you already downloaded the patch & tarball above, please re-download it. +1. i built a [a link] and validated it in my minikube cluster. as you can see here, it now works like a charm with `log4j-slf4j-impl-2.14.0`! :smile: ![a link]",1,0.9942375421524048
786531132,7898,priyavj08,2021-02-26T09:40:20Z,thanks have you consulted banzaicloud about this patch?,0,0.9296131730079651
786645278,7898,dongjinleekr,2021-02-26T13:23:19Z,"sure. here is a guide from the banzaicloud team. (see: [a link] by replacing the docker image to [a link], you can make use of this feature. (don't forget to specify `kafka_log4j_opts` to `""-dlog4j.configurationfile=file:{kafka.home}/bin/../config/log4j2.properties""`) [code block]",0,0.9556120038032532
788957476,7898,fouadsemaan,2021-03-02T14:42:25Z,"to 's question, is the vulnerability invoked by kafka or does it lie dormant?",0,0.9851598739624023
789662902,7898,priyavj08,2021-03-03T11:58:11Z,"really appreciate your guidance here. thanks for the patch. if i chose to not to move to this patch right away, can you please confirm that this vulnerability in log4j (cve-2019-17571) doesn't affect kafka? thanks",1,0.9956637024879456
789735966,7898,dongjinleekr,2021-03-03T14:03:00Z,"i'm sorry that i can't be certain. but as far as i know, any project with log4j 1.2.7 is not safe. (it is why i have been working on this issue.) +1. i also released 2.6.1 backport. - working branch: [a link] - custom build distribution: [a link] - 2.6.1 patch: [a link]",-1,0.9850987195968628
790427075,7898,priyavj08,2021-03-04T08:30:32Z,when will this fix make it in to one of kafka upstream release? thanks,1,0.762908935546875
790429327,7898,sofarsoghood,2021-03-04T08:34:15Z,we now checked kafka's source code for any appearances of the socketserver class or corresponding config files but were not able to find any. furthermore we took a closer look at the listening ports inside the running containers. conclusion: it looks like the affected socketserver class is not used by kafka.,0,0.9878722429275513
790473929,7898,dongjinleekr,2021-03-04T09:35:20Z,"since this kip is already passed, it will be included in 2.8.0 release.",0,0.9947497248649597
800184688,7898,dongjinleekr,2021-03-16T11:37:16Z,"rebased onto the latest trunk, with migrating the tests module into log4j2 and windows support.",0,0.9928258657455444
829970019,7898,dongjinleekr,2021-04-30T09:32:55Z,rebased onto the latest trunk.,0,0.9934989213943481
868564730,7898,svudutala,2021-06-25T15:04:54Z,thanks for all the effort on this feature. do you have a target eta on this to be merged?,1,0.9589995741844177
869010612,7898,dongjinleekr,2021-06-26T14:23:00Z,"thanks for the interest in this feature. this feature will be included in the 3.0.0 release. if you need this feature urgently, i am now backporting it to 2.7.1 and 2.8.0. please refer [a link].",1,0.9691062569618225
869010720,7898,dongjinleekr,2021-06-26T14:23:43Z,"if you need the log4j2 appender, see [a link].",0,0.994956910610199
876149123,7898,kkonstantine,2021-07-08T05:52:24Z,it's been a week since feature freeze for ak 3.0 and this pr is open (with several conflicts). please let me know asap if this pr is not ready and kip-653 needs to be removed from the 3.0 plan,0,0.962074875831604
876265899,7898,dongjinleekr,2021-07-08T09:02:58Z,rebased onto the latest trunk. could anyone review this pr? :bow: cc/,1,0.9617976546287537
876773034,7898,emveee,2021-07-08T21:59:30Z,do you have a patch that will work with 2.8.0?,0,0.9942715167999268
879046873,7898,dongjinleekr,2021-07-13T12:31:46Z,thank you so much for being so interested. 2.8.0 backport preview will be released this week.,1,0.986135721206665
911500667,7898,ashishpatil09,2021-09-02T10:18:36Z,hi guys is there any plan to release this fix soon? thanks ashish,1,0.9885714650154114
915363359,7898,dongjinleekr,2021-09-08T15:56:50Z,"many thanks for your interest in this feature. i think it will be released with ak 3.1, but i can't be certain yet. please refer [a link] if you need a preview or a custom patch for [2.6.1, 2.8.0]. cc/",1,0.9755988717079163
950328049,7898,dongjinleekr,2021-10-24T13:44:53Z,rebased onto the latest trunk. could you have a look when you are free?,0,0.9912431240081787
974801081,7898,naanagon,2021-11-21T11:44:52Z,the reason for migration isn't cve-2019-17571. this doesn't impact us (kafka don't use socketserver). upgrading from log4j 1.2.17 to log4j 2 can be done because of log4j 2 features but not for this(cve-2019-17571) can you clarify this..?,0,0.9940127730369568
976480514,7898,dongjinleekr,2021-11-23T12:47:02Z,"hi , agree. after reconsidering the issue, i concluded that [a link] is rather a minor issue; it is only problematic only when the user tries to use the `socketserver` appender. i think the primary reason for migration is log4j2 features like syntax. (for example, using log4j 1.x syntax is confusing since it is already obsolete several years ago.)",0,0.9127137660980225
991150391,7898,soumiksamanta,2021-12-10T17:17:15Z,will this pr solve [a link]?,0,0.9951272010803223
991209328,7898,svudutala-vmware,2021-12-10T18:44:16Z,[a link] should be upgraded to 2.15.0. log4j <= 2.14.0 all have this issue. initially i thought log4j 1.x is not impacted but as per [a link] it is.,0,0.9813018441200256
991705948,7898,unverified-user,2021-12-11T16:40:16Z,thank you for sharing the comment. isn't that comment for log4j v1 in general. kafka by default does not use jms appender. do you think it is impacted under the default configuration. also refer to this post: [a link],1,0.9463356733322144
991894590,7898,vinayakshukre,2021-12-12T13:02:16Z,"you must be already aware about the new log4j zero day vulnerability with log4j 2.14.1 versions and below that. hope when you will be done with this merge, you will take it to 2.15.0 and then make it available in the final release. thanks for your efforts.",1,0.9790661931037903
992556542,7898,svudutala-vmware,2021-12-13T14:50:50Z,yeah -user . my understanding is same too. this should not impact unless there is use of jms.,0,0.9917056560516357
994740102,7898,dongjinleekr,2021-12-15T12:21:55Z,"hi all, sorry for being late. here is the update! this pr is rebased onto the latest trunk and now uses log4j2 2.16.0 to address [a link] and [a link]. according to [a link], log4j 1.x is [a link] but, only when it uses jms appender. so, unless you are using the jms appender, you are safe from this vulnerability.",-1,0.9581974148750305
995250657,7898,dhruvp-8,2021-12-15T22:06:09Z,it would be great if you can provide this change as part of your preview releases for 2.7.1 and 2.8.0,1,0.5590577721595764
997166170,7898,dongjinleekr,2021-12-18T08:01:14Z,"all // you can find a preview of apache kafka 3.0.0 w/ log4j2 2.16.0 [a link]. -8 i have no plan for 2.7.0 or 2.8.0 but, currently working on 2.8.1 now.",0,0.9851706624031067
1005938463,7898,gabrieljones,2022-01-05T17:43:33Z,how hard is it to backport this all the way back to kafka v2.3.x?,0,0.9873917698860168
1006282789,7898,dongjinleekr,2022-01-06T04:50:11Z,almost impossible. there are too many conflicts.,-1,0.6337986588478088
1006913721,7898,vbsiegem,2022-01-06T20:30:16Z,"are there any firm plans at this time for when log4j v2 support in kafka will be released, and in what release(s)?",0,0.9922130703926086
1006920498,7898,diegoazevedo,2022-01-06T20:42:01Z,"just for information, the use of an obsolete version of log4j is now seen as a critical vulnerability for some security scanners. we received this ""critical warning"" from tenable nessus on our kafka cluster: [code block]",0,0.9889744520187378
1007242660,7898,dongjinleekr,2022-01-07T09:00:52Z,"i hope ak 3.2.0, but can't certain yet.",0,0.9341417551040649
1007353055,7898,vbsiegem,2022-01-07T12:00:37Z,"when is ak 3.2.0 targeted to be released? as indicated, lack of support for log4j2 is increasingly seen as a critical vulnerability, and some corporations using ak are urgently looking for a resolution.",0,0.989855170249939
1007366645,7898,dongjinleekr,2022-01-07T12:23:29Z,"i can't certain since i am not a committer. but if someone urgently needs a log4j2 based version, there is [a link]. a 3.1 based one will be also released as soon as the official ak 3.1 is released.",0,0.9755849242210388
1009197651,7898,vlsi,2022-01-10T18:16:17Z,"i might be slightly late for the party, however, have you explored the possibility of resurrecting and releasing 1.x? i do think it is worth resurrecting 1.x and fix the critical issues. here are the relevant threads: * dev (my initial proposal): [a link] * general (after logging pmc suggested re-incubating 1.x): [a link] (<-- there are people who expressed interest in resurrecting 1.x) if you are interested in log4j 1.x releases (e.g. review, test changes), it would be great if you could comment on general thread.",1,0.9141404628753662
1009624788,7898,dongjinleekr,2022-01-11T06:01:14Z,not yet. kip-653 has been started long before the log4j crisis of 2020.,0,0.9733843803405762
1009629904,7898,vlsi,2022-01-11T06:11:59Z,"if the only motivation for moving 1.x -> 2.x is security, then it might be better for both kafka and kafka users to stick with 1.x and ask/wait/help with releasing a hardened 1.x. then the users won't need to learn new configuration/debugging/maintenance approaches of 2.x.",0,0.9897270798683167
1010167705,7898,diegoazevedo,2022-01-11T16:58:42Z,"the problem is that version 1.* is deprecated since 2015. details: [a link] the vulnerability reported by security scanners (like tenable nessus) is exactly that: the use of an ""unsupported version"". so i think the movement for 2.* is correct.",0,0.9821089506149292
1010208088,7898,vlsi,2022-01-11T17:44:20Z,", let me explain: if there are people willing to support log4j 1.x, then it could be supported and maintained just fine. [a link] is a fork by (the one who created log4j in the first place!) as i highlighted above, there are individuals (including asf committers like myself or even asf members) who are willing to volunteer on supporting log4j 1.x. currently, the question has not yet been decided by the asf, however, i do not see how they can ""forbid"" maintaining 1.x provided the version is wildly used in the industry, there's a high demand on the fixes, and there are individuals to work on that. of course, if the asf allows the volunteers to maintain 1.x, then reload4j will not be required. however, if the asf blocks 1.x (for any reason), then reload4j might be way better for the consumers than migrating to 2.x or something else.",0,0.9043865203857422
1011241406,7898,gabrieljones,2022-01-12T16:42:13Z,confluent also has a fork of log4j 1 [a link] the github repo seems to have disappeared though.,0,0.9915221929550171
1014401948,7898,viktorsomogyi,2022-01-17T11:10:12Z,"you can use other log libraries today as well. the other day i tried out logback runtime (on the broker side) and it works well. all you need is to include logback (or i guess reload4j for that matter) jars runtime and configure it up. i think what we should concentrate on is to be independent from logging frameworks. kafka has two problems: kafkalog4jappender and log4jcontroller/loggingresource. the other stuff is mostly testing related to which i say it doesn't really matter, those aren't exposed to production environment anyway. the appender can be replaced by a similar implementation in log4j2 and the controller could be transformed to a pluggable implementation so users won't lose functionality (right know if you switch to logback or something else you won't be able to manipulate log levels dynamically). as the strongly dependent functionality i think isn't critical and users are mostly free to choose implementation today, i also think it's a good idea to upgrade to log4j2 as a new default. however i think this discussion should be had on the kip discussion thread, please let us concentrate here on the code review itself.",0,0.9322901368141174
1021440975,7898,mimaison,2022-01-25T17:36:14Z,thanks for your work. i hope to start reviewing the log4j2 prs later this week. do you recommend starting with this one or with [a link],1,0.9748117327690125
1023168840,7898,dongjinleekr,2022-01-27T12:44:29Z,"many thanks for your effort. sure, here it is; i rebased it against the latest trunk, also updating `logcapturecontext` with 's comments. #10244 is also ready. resolving the conflicts, i also found a little inconsistency problem with `admin` api (see `plaintextadminintegrationtest#testincrementalalterconfigsforlog4jloglevelsdoesnotworkwithinvalidconfigs`) i will file a minor kip addressing it.",1,0.9724770188331604
1026731878,7898,Indupa,2022-02-01T11:14:20Z,"hi , can you please help me to build this latest patch you have released on top of kafka_2.8,1. i tried using this following command to compile kafka source code always, but iam facing an issue to build this latest source code of log4j2 patch . the command i used is : gradle -pscalaversion=2.13 releasetargz -x signarchives 1 . the error iam getting is : build file 'c:\kafka-2.8.1-log4j2\build.gradle' line: 1339 * what went wrong: a problem occurred evaluating root project 'kafka-2.8.1-log4j2(patch by github)'. the following types/formats are supported: - instances of dependency. - string or charsequence values, for example 'org.gradle:gradle-core:1.0'. - maps, for example [group: 'org.gradle', name: 'gradle-core', version: '1.0']. - filecollections, for example files('some.jar', 'someother.jar'). - projects, for example project(':some:project:path'). - classpathnotation, for example gradleapi(). comprehensive documentation on dependency notations is available in dsl reference for dependencyhandler type. * try: run with --stacktrace option to get the stack trace. run with --info or --debug option to get more log output. run with --scan to get full insights. * get more help at [a link] deprecated gradle features were used in this build, making it incompatible with gradle 7.0. use '--warning-mode all' to show the individual deprecation warnings. see [a link] 2 . when i tried commenting a line ""testcompile libs.mockitojunitjupiter // supports mockitoextension "" at line 1339 , iam getting following error. c:\checkouts\kafka_2.8.1>gradle jar building project 'core' with scala version 2.13.5 failure: build failed with an exception. * where: build file 'c:\checkouts\kafka_2.8.1\build.gradle' line: 1362 * what went wrong: a problem occurred evaluating root project 'kafka_2.8.1'.",0,0.6842502355575562
1027702595,7898,dongjinleekr,2022-02-02T08:40:33Z,i'm sorry. there was a mistake rebasing onto 2.8.1. you can see the updated patch with built tarball [a link].,-1,0.9899497032165527
1027741356,7898,Indupa,2022-02-02T09:28:16Z,thank you so much . let me try to apply a patch and build.will update you,1,0.9873366951942444
1028142767,7898,Indupa,2022-02-02T16:52:41Z,"hi ,i could able to build latest patch and also need one input from you. is all dependencies of log4j 1.x is completely removed in this patch............?, i could see,still dependency on log4j_1.2.17 in build.gradle and dependency.gradle.also there are dependency on log4j.properties and tools-log4j.properties instead of log4j2.properties and tools-log4j2.properties in some of the files.is it still require or we can remove those dependencies as well.............?. the things i tried from my end is as follows, 1. i tried updating build.gradle and dependency.gradle by removing the dependency of log4j. 2. also,i tried updating some of the files,where you have added echo statement to update log4j.properties into log4j2.properties in those places where u have mentioned in that patch file by removing log4j.properties and connect-log4j.properties and tools-log4j.properties file. 3. after that,i compiled the code and extracted folder under ""c:\kafka_2.8.1\core\build\distributions\kafka_2.13-2.8.1\kafka_2.13-2.8.1"" and named it as kafka.zip file and using in our component by installing and run it as kafka service. 4.but when i tried running kafka,iam getting following exception. 2022-02-02 05:57:17.158 [inf] [kafka] connecting to localhost:2181 2022-02-02 05:57:27.571 [inf] [kafka] watcher:: 2022-02-02 05:57:27.571 [inf] [kafka] watchedevent state:syncconnected type:none path:null 2022-02-02 05:57:27.574 [inf] [kafka] [] 2022-02-02 05:58:17.227 [err] [kafka] error statuslogger reconfiguration failed: no configuration found for '764c12b6' at 'null' in 'null' 2022-02-02 05:58:17.684 [inf] [kafka] deprecated: using log4j 1.x configuration. to use log4j 2.x configuration, run with: 'set kafka_log4j_opts=-dlog4j.configurationfile=file:c:\kafka/config/tools-log4j2.properties' to brief about my requirement is , currently the kafka package we using,contains some of the patches which we have added on top of kafka_2.8.1 source code.in which one the custom change we have made is,we are using apache-log4j-extras 1.2.17 with timebased triggering policy for rolling log files as it is not available in log4j.1.2.17. since this version has vulnerability ,we wanted to use that log4j2 api for this rolling policy logic which is working in your patch. can you please help me on this...............?",0,0.5913744568824768
1028778134,7898,dongjinleekr,2022-02-03T09:26:52Z,"hi , 1. sure, log4j 1.x is removed entirely. it is still defined in `build.gradle` for `log4j-appender`, deprecated with kip-719, but not included in the classpath. (see below) ![a link] 2. as you can see here, this patch includes both of log4j 1.x and 2.x properties files. but it runs with 1.x properties file by default for backward compatibility. to use log4j2 properties, set `kafka_log4j_opts` like the logging message: [code block] you can use included `config/log4j2.properties` for `timebasedtriggeringpolicy`. please update it following your use case.",0,0.9575468301773071
1028865528,7898,Indupa,2022-02-03T10:58:23Z,"hi , yeah i already have made changes to use log4j2 properties, set kafka_log4j_opts. but after that as well, as i sent in my previous comment, iam getting this following exception. 2022-02-02 05:57:17.158 [inf] [kafka] connecting to localhost:2181 2022-02-02 05:57:27.571 [inf] [kafka] watcher:: 2022-02-02 05:57:27.571 [inf] [kafka] watchedevent state:syncconnected type:none path:null 2022-02-02 05:57:27.574 [inf] [kafka] [] 2022-02-02 05:58:17.227 [err] [kafka] error statuslogger reconfiguration failed: no configuration found for '764c12b6' at 'null' in 'null'. but still iam getting this exception,kafka is not running.can you help me with what is the cause of this exception......................?",0,0.9699323177337646
1028870380,7898,dongjinleekr,2022-02-03T11:04:26Z,"hi , please refer [a link].",0,0.992222011089325
1028876857,7898,Indupa,2022-02-03T11:12:32Z,"hi , yeah i refered this article earlier,but didn't get to know the soluton what they are suggesting like dlog4j.configuration in vm arguments.i have not seen where we are using this vm arguments. how i can overcome from this issue................?",0,0.8583658933639526
1028885839,7898,dongjinleekr,2022-02-03T11:23:50Z,which vm paramater are you using? -dlog4j.configuration or -dlog4j.configuration**file**?,0,0.9958716034889221
1028965316,7898,dongjinleekr,2022-02-03T12:59:05Z,please refer [a link] and the log messages.,0,0.9936407208442688
1028979162,7898,Indupa,2022-02-03T13:14:15Z,"sure .thank you so much for sharing documentation ,sorry i was not aware that -dlog4j.configuration is vm parameter its pointing to in this ,so was confused with that comment. i got it now.let me through the changes what i have made to use log4j2 properties by setting kafka_log4j_opts in detail,whether i have missed to update it any of the file and will update you.thank you",1,0.9743178486824036
1047472293,7898,Indupa,2022-02-22T06:38:46Z,"hi , can you please provide me some information on the vulenerabiities regarding log4j features , is this following vulnerable features of log4j are using in kafka 2.8.1 for any of the kafka related things...................? cve-2019-17571 is a high severity issue targeting the socketserver. log4j includes a socketserver that accepts serialized log events and deserializes them without verifying whether the objects are allowed or not. this can provide an attack vector that can be expoited. => **is log4j's socketserver used in kafka.......?** cve-2020-9488 is a moderate severity issue with the smtpappender. improper validation of certificate with host mismatch in apache log4j smtp appender. this could allow an smtps connection to be intercepted by a man-in-the-middle attack which could leak any log messages sent through that appender. **=> is log4j's smtpappender is used in kafka..........?** cve-2022-23302 is a high severity deserialization vulnerability in jmssink. jmssink uses jndi in an unprotected manner allowing any application using the jmssink to be vulnerable if it is configured to reference an untrusted site or if the site referenced can be accesseed by the attacker. for example, the attacker can cause remote code execution by manipulating the data in the ldap store. **=> is is log4j's jmssink is used in kafka..............?** cve-2022-23305 is a high serverity sql injection flaw in jdbcappender that allows the data being logged to modify the behavior of the component. by design, the jdbcappender in log4j 1.2.x accepts an sql statement as a configuration parameter where the values to be inserted are converters from patternlayout. the message converter, %m, is likely to always be included. this allows attackers to manipulate the sql by entering crafted strings into input fields or headers of an application that are logged allowing unintended sql queries to be executed. **=> is is log4j's jdbcappender is used in kafka.....................?** can you please help me and provide me the info on this....................?",0,0.787513792514801
1047473190,7898,dongjinleekr,2022-02-22T06:40:51Z,"as long as the user explicitly configure the loggers to use them, they are not used yet.",0,0.9918273687362671
1047496080,7898,Indupa,2022-02-22T07:22:42Z,"ohhh okk, in that case none of the 4 vulnerable things ,used in kafka as of now right.....................?",0,0.8969193696975708
1047501444,7898,Indupa,2022-02-22T07:30:40Z,"currently in kafka we have not used socketserver,smtpappender,jdbcappender and jmssink features from log4j,unless user explicitly use it in their custom changes.................? can you please confirm on this...................?",0,0.9724149107933044
1047522217,7898,rgoers,2022-02-22T08:03:15Z,"please note that the apache logging services project continues to receive security vulnerability reports against log4j 1.x. it is not typical to file cve's against an eol'd project. we recently did, however as we were made aware that a fork of log4j 1 claimed to have fixed all the security issues. we may file more but log4j 1 is not a high priority so i cannot say when more might be forthcoming. in addition to the security issues there are several serious bugs that will never be fixed.",0,0.9577581286430359
1047536617,7898,ceki,2022-02-22T08:22:42Z,"if you receive a cve against log4j 1.x, how difficult is it to forward to the reload4j project? more specifically, are you aware of a single cve against log4j 1.x that was not fixed in reload4j? as for the ""other serious bugs that will never be fixed"" that is a bold claim which will not stand the test of time. it is unbecoming for an oss project leader to engage in this sort of fud. frankly, it is embarrassing to watch. for the sake of your own credibility, please stop.",-1,0.985528290271759
1062401482,7898,infa-rbliznet,2022-03-08T23:47:42Z,"is there any progress on it? according to this: [a link] , kip should be completed by march 2 and feature freeze is on march 16.",0,0.9927843809127808
1072452078,7898,dongjinleekr,2022-03-18T14:14:10Z,"here is the update, rebasing onto the latest trunk. :bow:",1,0.9888699054718018
1073801293,7898,edoardocomar,2022-03-21T11:51:09Z,hi would you consider this patch to fix the compilation error [code block],0,0.9934559464454651
1076403226,7898,dongjinleekr,2022-03-23T13:53:53Z,rebased onto the latest trunk. cc/,0,0.9937849044799805
1105464108,7898,dhruvp-8,2022-04-21T16:50:44Z,thanks for working on this pr. is there a timeline on when will this be merged? as per this doc [a link] is it pushed to kafka 3.3?,1,0.9486026763916016
1113974839,7898,dongjinleekr,2022-04-30T11:50:22Z,"rebased onto the latest trunk. -8 sorry for being late. for compatibility reasons, the adoption of this pr is postponed to 4.0. in 3.x, the reloadlog4j will be used instead. you can find out the custom build, patch with log4j2 [a link] - i released the ak 3.1.0 based one this week and working on [a link] now.",-1,0.9673873782157898
1321549707,7898,Indupa,2022-11-21T07:07:08Z,"hi , we ran into the issue to run zookeepr and kafka , when we tried to use reload4j instead of log4j in kafka-2.8.1 package. please find below for more details and could you please help us on how to resolve this issue............? issue : currently we are using kafka-2.8.1 in which it has log4j vulnerabilities reported . fix : so we tried to use reload4j-1.2.22 in kafka-2.8.1 to overcome all the vulnerabilities reported by log4j-1.2.1 and made the changes to point to reload4j instead of log4j as its done kafka 3.2.1 latest version below are the 2 files in kafka which we made changes to replace log4j with reload4j. 1. build.gradle 2. dependencies.gradle after pointing to reload4j, its failing to run kafka and zookeeper with below errors. **zookeeper.log** log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.configurerootcategory(propertyconfigurator.java:630) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:516) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""kafkaappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""authorizerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""controllerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""cleanerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:388) at org.apache.zookeeper.server.quorum.quorumpeermain. (quorumpeermain.java:68) log4j:error could not instantiate appender named ""statechangeappender"". **kafka.log** log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.configurerootcategory(propertyconfigurator.java:630) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:516) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""kafkaappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""authorizerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""controllerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""requestappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""cleanerappender"". log4j:error could not instantiate class [org.apache.log4j.rolling.rollingfileappender]. java.lang.classnotfoundexception: org.apache.log4j.rolling.rollingfileappender at java.net.urlclassloader.findclass(urlclassloader.java:381) at java.lang.classloader.loadclass(classloader.java:424) at sun.misc.launcher$appclassloader.loadclass(launcher.java:349) at java.lang.classloader.loadclass(classloader.java:357) at java.lang.class.forname0(native method) at java.lang.class.forname(class.java:264) at org.apache.log4j.helpers.loader.loadclass(loader.java:190) at org.apache.log4j.helpers.optionconverter.instantiatebyclassname(optionconverter.java:304) at org.apache.log4j.helpers.optionconverter.instantiatebykey(optionconverter.java:123) at org.apache.log4j.propertyconfigurator.parseappender(propertyconfigurator.java:755) at org.apache.log4j.propertyconfigurator.parsecategory(propertyconfigurator.java:738) at org.apache.log4j.propertyconfigurator.parsecatsandrenderers(propertyconfigurator.java:652) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:518) at org.apache.log4j.propertyconfigurator.doconfigure(propertyconfigurator.java:577) at org.apache.log4j.helpers.optionconverter.selectandconfigure(optionconverter.java:504) at org.apache.log4j.logmanager. (logmanager.java:119) at org.slf4j.impl.reload4jloggerfactory. (reload4jloggerfactory.java:67) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:72) at org.slf4j.impl.staticloggerbinder. (staticloggerbinder.java:45) at org.slf4j.loggerfactory.bind(loggerfactory.java:150) at org.slf4j.loggerfactory.performinitialization(loggerfactory.java:124) at org.slf4j.loggerfactory.getiloggerfactory(loggerfactory.java:417) at org.slf4j.loggerfactory.getlogger(loggerfactory.java:362) at com.typesafe.scalalogging.logger$.apply(logger.scala:48) at kafka.utils.log4jcontrollerregistration$. (logging.scala:25) at kafka.utils.log4jcontrollerregistration$. (logging.scala) at kafka.utils.logging.$init$(logging.scala:47) at kafka.kafka$. (kafka.scala:30) at kafka.kafka$. (kafka.scala) at kafka.kafka.main(kafka.scala) log4j:error could not instantiate appender named ""statechangeappender"". could you please help us if we made the changes to those 2 gradle files , is it enought to make kafka-2.8.1 to work using reload4j............?",0,0.37423568964004517
1321685023,7898,ceki,2022-11-21T08:29:11Z,the correct fully qualified class name for `rollingfileappender` is `org.apache.log4j.rollingfileappender` and not `org.apache.log4j.rolling.rollingfileappender`.,0,0.9939171671867371
2386057956,7898,mimaison,2024-10-01T14:00:49Z,"we're now accepting changes for 4.0 in trunk. do you think you can rebase this pr? if not, let us know so someone can complete this work. thanks.",1,0.9725180268287659
2441591375,7898,ppkarwasz,2024-10-28T13:27:46Z,", what is the eta for 4.0? i might be able to look at it mid-november.",0,0.9888766407966614
2442079763,7898,mimaison,2024-10-28T16:33:49Z,we're actively working on it in [a link] hopefully that will be complete for 4.0.0.,0,0.9715600609779358
2480964449,7898,dongjinleekr,2024-11-17T06:51:55Z,sorry for being late. i have been very busy nowadays and just got some free time by this end of the year. thanks for taking the issue. i will join the pr and follow up for the review & improvements. let's close this pr and continue on #17373! :smiley:,1,0.890559732913971
668325203,9039,mjsax,2020-08-04T01:32:00Z,retest this please.,0,0.9739659428596497
679394512,9039,ableegoldman,2020-08-24T22:17:06Z,test this please,0,0.9870431423187256
683916587,9039,ableegoldman,2020-08-31T17:22:20Z,test this please,0,0.9870431423187256
684074223,9039,vvcephei,2020-08-31T22:24:03Z,"since jenkins pr builds are still not functioning, i've merged in trunk and verified this pull request locally before merging it.",0,0.993463933467865
2262311490,16456,chirag-wadhwa5,2024-08-01T08:05:38Z,"hi . thanks a lot for the review. i have made the required changes in the last commit. i have also left some replies to your comments, it will be really helpful if you could leave your suggestions there.",1,0.9937127232551575
2265855133,16456,chirag-wadhwa5,2024-08-02T17:38:18Z,"hi , thanks a lot for the review. i have made the changes suggested by you. also, i have replied to some of your comments and have left those conversations unresolved above. could you pls take a look at those and let me know if anything else is required. thanks a lot ! p.s. - i am looking into the test failure",1,0.9955600500106812
2269577431,16456,chirag-wadhwa5,2024-08-05T17:37:58Z,"hi , looks like the sharepartitionmanager wasn't being closed on the broker shutdown. i have pushed in a new commit with the changes, it should work now. is there any other remaining gap in the pr ? if not can i get an approval on it ? thanks a lot for all the suggestions !",1,0.9949454665184021
2270479029,16456,chirag-wadhwa5,2024-08-06T06:25:17Z,": looks like the tests are passing now, a couple of them fail probably because they are flaky",0,0.7048301696777344
1726880993,14364,philipnee,2023-09-20T03:50:17Z,"- made some changes based on the comments, but obviously broke some existing tests.",0,0.9570354223251343
1728333915,14364,lianetm,2023-09-20T19:48:17Z,"hey , thanks for the changes! i completed another pass to all the non-test files. agree with the move of the error handling to the hb manager but left a few other comments. i will wait for the fixes to the failing tests and then i will go over the test files. thanks!",1,0.9962033629417419
1728878443,14364,philipnee,2023-09-21T05:29:39Z,"- much thanks for spending time reviewing the pr, i tried to address most of the comments in the pr. i'll check back with the unit test results - as i've only run them locally. let me know if you have more comments to follow up.",1,0.977102518081665
1731476330,14364,lianetm,2023-09-22T14:01:17Z,"hey , i see several updates here, thanks! let me know when you want to me give it another pass (there are still some test failures)",1,0.9898890852928162
1734529330,14364,philipnee,2023-09-25T22:05:51Z,hey - i made some updates based on your last comments. let me know your thoughts!,1,0.8149181008338928
1735694640,14364,lianetm,2023-09-26T14:46:05Z,"thanks for the changes , left a few other minor comments and questions but lgtm. as i see it, the main areas requiring follow-up in other prs would be: - fully integrate with the state defined in the membershipmanager (getting rid of all the parallel `groupstate` defined here) - integrate with the assignment processing component, driving the logic to delegate callback execution and send hb on completion as required. - extend hb manager test to cover successful path and timeout scenarios. it would be helpful if you can take another look at it now, as it has evolved quite a bit. thanks!",1,0.9899701476097107
1743351780,14364,philipnee,2023-10-02T16:38:04Z,hello - thanks for the review. i hope i've addressed most of your concerns in the recent reviews. thanks!,1,0.9943796992301941
1745567545,14364,philipnee,2023-10-03T19:11:46Z,"thanks for the feedback. i addressed more of your comments. i wanted to point out that i filed 3 follow-up tickets to close some of the gaps. these are: - propagate time during failure to avoid time.milliseconds(): [a link] - ensure some of the fields are only sent once: [a link] - ensure coordinator node is removed on disconnection: [a link] i believe all of the open comments are addressed/replied, so let me know if there's anything else.",1,0.9742851853370667
1746844528,14364,dajac,2023-10-04T13:07:47Z,thanks. we also need to handle the consumer close case and send the final heartbeat.,1,0.905838668346405
1747674288,14364,philipnee,2023-10-04T21:34:07Z,thanks - i refactored some tests based on your comments. thanks a lot for putting time into it. here i've got a list of jira for the follow-ups: - propagate time during failure to avoid time.milliseconds(): [a link] - ensure some of the fields are only sent once: [a link] - ensure coordinator node is removed on disconnection: [a link] - send heartbeat on closing the consumer as part of handling close(): [a link],1,0.9912988543510437
1751532067,14364,philipnee,2023-10-07T00:44:01Z,"jdk11 build failed with `command ""git reset --hard"" returned status code 128:` , for the rest here is the list of failing tests: [code block]",0,0.9899745583534241
1751592366,14364,philipnee,2023-10-07T04:09:25Z,there's some issue with the jdk11 build - retriggering the tests don't seem to work. so i opened a [a link] to run the test. it seems like that's the only way to pass the build.,0,0.950433611869812
1752310508,14364,philipnee,2023-10-09T03:51:32Z,"- not entirely sure what is the best way to fix the jdk11 build. the rest of the builds seem to be fine with the following failures: [code block] however, i did open a draft pr from this branch and jdk11 was able to complete.",0,0.578615128993988
1753238153,14364,dajac,2023-10-09T15:35:04Z,"if we combine the last two builds, i am confident that the changes are good so i will merge it to trunk.",0,0.8767015337944031
2394094049,17373,frankvicky,2024-10-04T16:43:26Z,this is the initial version. i'd like to run it on ci first.,0,0.9902395009994507
2395277491,17373,frankvicky,2024-10-06T03:32:10Z,"hello thanks for your feedback. unfortunately, i barely missed the kip for some reason, but i'll take a look and adjust the pr accordingly. :grinning_cat:",1,0.9934625625610352
2430811748,17373,showuon,2024-10-23T03:53:53Z,"sounds good to me. but if we decided to remove them later, please open a jira ticket for them. thanks.",1,0.994046688079834
2430832465,17373,frankvicky,2024-10-23T04:02:51Z,[a link] i have filed a jira for it. :grinning_face_with_smiling_eyes:,1,0.9918169379234314
2434890030,17373,chia7712,2024-10-24T10:24:44Z,"please fix the conflicts, thanks!",0,0.6830363273620605
2453429889,17373,frankvicky,2024-11-03T13:33:14Z,there are lots of tests fail come out after merging `trunk`. :disappointed_face: i will take a look......,-1,0.9954753518104553
2454941038,17373,frankvicky,2024-11-04T14:59:20Z,"hello everyone, i am having some trouble debugging the new failures. :crying_face: the root cause seems to be that these failing tests are unable to capture logs correctly (resulting in empty content), which leads to assertion failures. it appears that these issues are caused by #17615, although these test cases worked fine when using log4j1. any feedback or information would be greatly appreciated. thank you!",1,0.9161723852157593
2462639102,17373,frankvicky,2024-11-07T16:12:20Z,update: it seems that the root cause its because the log event could not be captured correctly. ![a link] ![a link],0,0.8317343592643738
2472887781,17373,frankvicky,2024-11-13T08:57:45Z,"hello ,, since this pr modifies a large number of files, particularly `build.gradle`, its highly susceptible to conflicts with other prs, making it rather exhausting to resolve these conflicts frequently. it would be helpful if we could merge this pr into trunk sooner, as there arent any outstanding issues or points of contention with it. this would also allow us to begin addressing any follow-up issues. many thanks.",1,0.9732868075370789
2478290150,17373,showuon,2024-11-15T09:06:17Z,", do you have any other comments? i'll merge it tomorrow if no other comments. thanks.",1,0.9582186341285706
2480966246,17373,dongjinleekr,2024-11-17T06:55:17Z,fyi: i just moved from #7898 and reviewing the differences between our 3.6.x in-house fork. it was already applied to our implementation and is actively running now :) let me have a look!,1,0.9946976900100708
2485719308,17373,dongjinleekr,2024-11-19T13:29:51Z,"all // to clearly state the reasoning for root logger's name, i just updated the [a link]. please have a look on [a link]. fei chang gan xie :pray: (""extremely thankful for your great help."") to keep a record of your effort taking over this huge issue, i added a new section mentioning [a link]. when you merge this pr, please don't omit mentioning my credit. :bowing_man:",1,0.9743430018424988
2485801878,17373,chia7712,2024-11-19T14:00:39Z,will roger that :),0,0.9742090106010437
2486208765,17373,mimaison,2024-11-19T16:36:59Z,yes you totally deserve being marked as a co-author. thanks!,1,0.9947124719619751
2488600707,17373,mimaison,2024-11-20T13:32:52Z,"i'm having issues with the latest code (f3a68e1b9b). [code block] running `trunk` the `logs` folder is automatically created. if i create the folder manually, then it seems to log at warn level by default and i only get a single log line printed while my broker is running: [code block]",0,0.5577926635742188
2503451783,17373,mimaison,2024-11-27T10:05:11Z,can you rebase to fix the conflicts? thanks,1,0.8015167713165283
2504196849,17373,frankvicky,2024-11-27T15:42:21Z,"hi , i have filed a jira for the issue with the connect api: [a link] btw, im aware that this pr is expected to break some e2e tests, but so far, ive only confirmed that `connect_test.py` is affected. should we fix it in this pr, or should we file a separate jira to track it?",0,0.9828275442123413
2504201143,17373,mimaison,2024-11-27T15:44:01Z,the connect issue should be fixed in this pr. adding new tests to verify the connect rest api /admin/loggers endpoint can be done in another pr but not the bug fix.,0,0.9936707019805908
2504207669,17373,mimaison,2024-11-27T15:46:45Z,"apart if the changes to fix the system tests are very large, i'd rather do them in this pr as well. we typically don't merge prs that we know will break tests.",0,0.9829141497612
2504369138,17373,mimaison,2024-11-27T16:59:32Z,i just tested the connect rest api with [a link] and it worked fine. you mentioned issues with the system tests. have you run the full suite? are there only issues with `connect_test.py`?,0,0.9863459467887878
2505169162,17373,frankvicky,2024-11-28T02:56:23Z,"hi , unfortunately, i don't have the resources to perform the full test suite as it would take a very long time :grimacing_face: instead, i selected a test related to this pr. the first one i picked is `connect_test.py`, but it failed. here is the assertion failure for `test_file_source_and_sink.converter`: [code block] as we can see, there are 2 exception logs at the end of the array that breaks the test. i am investigating but currently stuck. the rest of the test failures of `connect_test.py` also seem to be related to this `timeoutexception`.",-1,0.9903004169464111
2506901731,17373,showuon,2024-11-29T00:58:36Z,", we ran system tests on connect related tests (tests/kafkatest/tests/connect) based on this pr, it failed quite a lot. [a link] is the result. compared with the latest trunk, it only has 1 failure. so please help fix them. thanks.",1,0.9526357054710388
2507753807,17373,frankvicky,2024-11-29T12:48:03Z,"since this issue is quite tricky for me, i went through a process of elimination to locate the bug. i found this line might be the root cause: [a link] if we use `-dlog4j2.configurationfile=file:`, it will lead to a `timeoutexception` in the console consumer. however, if we use `-dlog4j.configuration=file:`, all tests in `connect_test.py` will pass. i'm still trying to figure it out.",0,0.8872857689857483
2507855608,17373,mimaison,2024-11-29T13:47:16Z,"thanks for investigating. i ran the system tests in our ci and confirm most the connect tests are currently failing with your branch (i've not tried running other tests yet). i'll try to debug next week, if you can't find the issue. i'll also be able to run the tests in our environment once we have a fix.",1,0.9722263216972351
2508123859,17373,chia7712,2024-11-29T16:34:13Z,"in the end-to-end tests, you must use the correct file extension when passing the log4j2 yaml configuration. if you use an incorrect extension - for example - the output of `log.error` called by the console consumer gets redirected to `console_consumer.stdout`. this is the root cause of the connection error as it see output which should not be existent. as a straightforward solution, we can render the configuration file based on the specific node version. [code block] noted that `log4j_config` can be removed as we don't use it anymore. additionally, you can apply this approach to all py files - such as `verifiable_producer.py`, `kafka.py`, `transactional_message_copier.py`, `consumer_performance.py`, `end_to_end_latency.py`, and `producer_performance.py`.",0,0.9909155964851379
2508957854,17373,frankvicky,2024-11-30T13:10:23Z,"[a link] `metadataschemacheckertooltest#testverifyevolutiongit` keep failing in ci, i think it's not related to this pr.",0,0.9605814814567566
2508981008,17373,chia7712,2024-11-30T14:34:23Z,"the root cause is the test assumes the repo has ref ""refs/heads/trunk"" but it is not existent in pr, since our pr does not fetch that ref. i have filed [a link] to fix it",0,0.9925553202629089
2510443802,17373,frankvicky,2024-12-02T02:38:30Z,failed ci is handled by #17996,0,0.9926829934120178
2510857877,17373,chia7712,2024-12-02T08:19:41Z,could you please rebase code?,0,0.9917995929718018
2511535886,17373,chia7712,2024-12-02T13:23:33Z,"connect e2e has another issue, and i have filed [a link] to fix it",0,0.9932059049606323
2515161122,17373,chia7712,2024-12-03T17:23:48Z,this is another issue ([a link] used to fix connect e2e,0,0.9951364398002625
2518956077,17373,chia7712,2024-12-05T02:32:04Z,the last (maybe) connect end-to-end issue is [a link]. we can revisit this pr after the existing connect end-to-end issues are resolved.,0,0.9938164353370667
2540936116,17373,mimaison,2024-12-13T09:07:33Z,should we consider merging this and disabling the broken system test for now? wdyt?,0,0.9888784289360046
2541679409,17373,chia7712,2024-12-13T15:17:20Z,"let me perform a final review later, and then i will merge it!",0,0.9834470748901367
2541895357,17373,chia7712,2024-12-13T17:15:18Z,i add you to the co-author. see [a link],0,0.9910793900489807
2542607479,17373,frankvicky,2024-12-14T01:07:41Z,thank you all for your patience in reviewing. i truly appreciate all your help. :person_bowing_medium-light_skin_tone:,1,0.9970299005508423
2543435727,17373,chia7712,2024-12-15T03:52:04Z,"i am currently testing this patch, and all related issues are tracked under [a link]. please feel free to report any issues if i overlook any broken changes. :( the known bugs caused by this pr are listed below. 1. `logger` does not handle the compatibility correctly (see [a link] 2. incorrect log4j2 config in e2e (see [a link]",-1,0.9962177872657776
2556096026,17373,jolshan,2024-12-20T01:44:08Z,"hey -- not sure if there is something up with my setup, but after this pr, i see many tests failing due to kafka server not starting up (in the logs i see ` port already in use: 9192; nested exception is: java.net.bindexception: address already in use`or because there are no process ids (see [a link] in both cases, it seems like something changed in startup -- perhaps some change in kafka.py here triggered the issue. did we run system tests after this change? i see some tests mentioned in kafka-18161, but not all. i'm seeing about 500 failures on trunk now (compared to 50 or so last week). [a link]",0,0.9071375727653503
2556229381,17373,chia7712,2024-12-20T03:47:02Z,"thanks for your report. the known issue caused by this pr is the ""logger"" of connect. and i run the one of test (`transactions_upgrade_test.py`) on my local: [code block] there is no `bindexception`, and i will check other tests later.",1,0.9557839632034302
2557032193,17373,ijuma,2024-12-20T13:35:45Z,i don't see any upgrade notes - did i miss something or did we forget to add that?,0,0.9477962255477905
2557302557,17373,frankvicky,2024-12-20T16:16:09Z,i will open a pr adding upgrade notes.,0,0.9950079917907715
2557311234,17373,chia7712,2024-12-20T16:21:41Z,could you please open minor to address following items? 1. move the deprecation warnings to `kafka-run-class` [a link] 2. remove unnecessary reference of `jacksondatabindyaml` [a link] 3. add upgrade notes [a link],0,0.9947081804275513
2557355489,17373,jolshan,2024-12-20T16:37:36Z,interesting. this one fails consistently on our infra (and succeeds without this change). i wonder if there is some different test configuration that causes the issue.,0,0.8495808839797974
2557479743,17373,chia7712,2024-12-20T18:03:46Z,there are many tests which are failed quickly. [code block] it can pass the check of `grep 'kafka\s*server.*started'` (kraft controller) but the check of process id fails. i open a minor to add more log ([a link] could you please run the patch on your infra?,0,0.9913539290428162
2557576698,17373,jolshan,2024-12-20T19:10:54Z,sure. i will run it and post the results,0,0.9700381755828857
2557623349,17373,jolshan,2024-12-20T19:44:36Z,here's the result. [a link],0,0.9908163547515869
2577813317,17373,trnguyencflt,2025-01-08T14:30:16Z,"this pr breaks downstream project that depends on kafka_2.13 and reload4j because there is clashing in class loggingevent, which exists in `reload4j` and `log4j-1.2-api` jars. the application will crash with this exception [code block]",0,0.9705801606178284
2577875259,17373,ppkarwasz,2025-01-08T14:56:16Z,"you can not have both `reload4j` and `log4j-1.2-api` on your classpath, since they are both **replacements** of `log4j:log4j`.",0,0.9929277300834656
2577897980,17373,ijuma,2025-01-08T15:05:14Z,"the point is that kafka was including `reload4j` before and hence many other projects aligned with that. it's one thing to cause logging to change, it's another to cause projects not to start anymore with a `incompatibleclasschangeerror` - that's a much bigger deal.",0,0.9765560030937195
2578032374,17373,chia7712,2025-01-08T15:58:11Z,"i assume the scenario you describe is to add ""kafka_2.13"" as dependency for the downstream project. the log4j2 are declared as ""implementation"" in core module, so downstream project should not ""include"" log4j2 in the dependencies automatically. not sure why `log4j-1.2-api` is included in the downstream project. do you add `log4j-1.2-api` to your project manually?",0,0.9886844754219055
2578035232,17373,chia7712,2025-01-08T15:59:22Z,"thanks for your report. i'd like to reduce the gap of upgrading to kafka 4.0 as much as possible, so please share the details to me",1,0.9657531380653381
2580893706,17373,trnguyencflt,2025-01-09T17:35:30Z,we include kafka_2.13 as dependency via maven [a link] and it brings log4j-1.2-api as a runtime dependency (below is from mvn dependency:tree) [code block],0,0.9954832792282104
2581051461,17373,chia7712,2025-01-09T19:03:24Z,"thanks for your response. `log4j-1.2-api` is used to convert log4j.properties at runtime, and hence maybe we can remove it from gradle runtime scope and then add it into distribution directly. with that change, `log4j-1.2-api` gets removed from published pom file but it remains in the distribution. this is the pom file with above approach and there is not `log4j-1.2-api` [code block] by contrast, `log4j-1.2-api` is still included by distribution. [code block]",1,0.9310716390609741
2581610985,17373,frankvicky,2025-01-10T02:21:07Z,hi thanks for report. i have filed kafka-18466 to track this issue. i will filed a pr for this issue soon.,1,0.9656265377998352
2586347959,17373,chia7712,2025-01-13T07:07:40Z,"we have resolved the issue in #18472. if you have some free time, could you please test it on your project? please don't hesitate to provide any feedback if you think something has been overlooked. thank you!",1,0.9850934743881226
1508030768,13561,showuon,2023-04-14T07:07:32Z,"`kafka-14888: remotelogmanager - deleting expired/size breached log segments to remote storage implementation` was created for this task. i've updated the pr title, fyi",0,0.990672767162323
1555964010,13561,satishd,2023-05-20T18:08:18Z,the proposed approach with the changes introduced in this pr avoids inconsistency issues as mentioned [a link] and also avoids remote segment leaks in unclean leader election scenarios.,0,0.992983341217041
1571615955,13561,kamalcph,2023-06-01T08:44:40Z,"will you open a separate pr to delete the active segment once it breaches the retention time? or, will handle it in this patch.",0,0.9939649701118469
1571973707,13561,satishd,2023-06-01T12:36:11Z,"planned to have it in a followup pr, filed [a link] just to clarify , we need to roll the active segment incase remote storage is enabled and eligible for retention cleanup so that this segment can be copied by the remote storage subsystem and eventually picked up for retention cleanup.",0,0.9943535327911377
1573234538,13561,satishd,2023-06-02T06:43:46Z,thanks for your review comments. addressed them inline and/or with the latest commits.,1,0.9006824493408203
1573968832,13561,divijvaidya,2023-06-02T16:02:20Z,hey are you planning to address the open comments such as [a link] before i do another pass of code review?,0,0.9943002462387085
1575514622,13561,satishd,2023-06-04T10:44:20Z,i will let you know once i address the remaining few comments in the next couple of days.,0,0.9885154366493225
1576564925,13561,satishd,2023-06-05T10:51:48Z,"pulled the latest trunk, resolved the conflicts, and pushed the changes.",0,0.9943627119064331
1596907836,13561,showuon,2023-06-19T10:11:58Z,", any update for this pr? if you don't have time on it, just let me know. :)",1,0.99037104845047
1597034800,13561,satishd,2023-06-19T11:43:17Z,"addressing the review comments in progress, needs minor refactoring which is going on. will have those changes pushed in the next couple of days.",0,0.9917460680007935
1597103576,13561,satishd,2023-06-19T12:30:50Z,rebased with the trunk as this pr had conflicts because of other introduced changes in the trunk.,0,0.9785128831863403
1659937748,13561,showuon,2023-08-01T09:32:37Z,", is this pr ready for another round of review?",0,0.9909219145774841
1665486770,13561,satishd,2023-08-04T11:52:22Z,thanks for the review. addressed them with the latest commits.,1,0.8359997868537903
1665489584,13561,satishd,2023-08-04T11:53:48Z,thanks for the review. addressed your comments inline or with the latest commits.,1,0.8380887508392334
1670963444,13561,satishd,2023-08-09T09:14:37Z,thanks for the review. addressed your comments inline or with the latest commits.,1,0.8380887508392334
1672759994,13561,satishd,2023-08-10T08:08:10Z,thanks for the review. addressed the review comments inline and/or with the latest commits.,1,0.9020833373069763
1680370332,13561,satishd,2023-08-16T10:41:13Z,thanks for the review. addressed them with inline comments and/or with the latest commits. will add more uts. we will have integration tests in a followup pr once [a link] is merged.,1,0.9460247755050659
1682651711,13561,satishd,2023-08-17T17:01:16Z,those failures(except one) are not related to this pr. updated with a few minor changes and tests. the latest run had a few failures which seem to be unrelated to this change.,0,0.9878860712051392
1685049750,13561,junrao,2023-08-19T16:59:54Z,: the latest build still has 120 test failures.,0,0.9846424460411072
1685264422,13561,satishd,2023-08-20T11:52:57Z,those tests are not related to the changes in the pr. the next [a link] had one related test failure(plaintextadminintegrationtest.testoffsetsfortimesafterdeleterecords) which is fixed with the latest commit.,0,0.9945809245109558
1692562057,13561,satishd,2023-08-24T23:56:20Z,"there are a few failures unrelated to this pr, merging it to trunk.",0,0.9822607636451721
1692563258,13561,satishd,2023-08-24T23:58:13Z,merging it to 3.6 branch,0,0.9900640845298767
396640388,5201,bbejeck,2018-06-12T15:52:00Z,"apologies for the massive pr but it seems unavoidable at this point. more updates to come to this pr soon, but it's worth reviewing the graph object types and the overall physical plan generation",-1,0.8391216993331909
396673545,5201,bbejeck,2018-06-12T17:38:59Z,failure unrelated. retest this please,0,0.7219146490097046
397827935,5201,bbejeck,2018-06-16T17:35:22Z,updated this,0,0.9438372254371643
397829288,5201,bbejeck,2018-06-16T18:01:37Z,rebased from trunk,0,0.9920662641525269
398142054,5201,guozhangwang,2018-06-18T17:59:18Z,could you rebase this pr?,0,0.9941056966781616
399225979,5201,bbejeck,2018-06-21T20:05:24Z,failure unrelated retest this please,0,0.5913138389587402
399778525,5201,bbejeck,2018-06-24T18:49:39Z,"fixed issue with optimizing joins, updated integration test to include comparing results between optimized and non-optimized results and included a join in the test case.",0,0.9857892394065857
401196303,5201,bbejeck,2018-06-28T22:50:13Z,"in this pr we're still relying on the fix put into 2.0, in 4th pr we'll revert that change and include it along with the repartition optimization.",0,0.9927462935447693
401196999,5201,bbejeck,2018-06-28T22:54:00Z,required to keep find-bugs happy,0,0.9789729714393616
401198070,5201,bbejeck,2018-06-28T23:00:17Z,updated this,0,0.9438372254371643
401200256,5201,guozhangwang,2018-06-28T23:12:35Z,"i made another pass over the updated pr, and left two follow-up comments as in [a link] and [a link]",0,0.9939449429512024
401493756,5201,guozhangwang,2018-06-29T22:57:21Z,"regarding the meta comment about `abstractstream#addgraphnode`, now i understand the duplicate logic is because `kstreamaggregateprocessorbuilder` is not `abstractstream` and hence cannot use this function. my personal preference would then be, moving the `addgraphnode` from `abstractstream` to `internalstreamsbuilder`, with a small change on api to require passing in both the child and the parent node reference. in this case 1) beyond `abstractstream` we can still call it, and 2) moving forward if we need to have more then one streamgraphnode within an abstractstream it would be more natural to do so.",0,0.9838948845863342
405652620,5201,guozhangwang,2018-07-17T16:55:49Z,please let us when this pr is ready for review again.,0,0.9899017810821533
408222437,5201,bbejeck,2018-07-26T20:18:53Z,updated this. moved `addgraphnode` from `abstractstream` to `internalstreamsbuilder`. also removed the restriction of only building topology only once and added a unit test for it. also rebased with trunk. ready for reviews again,0,0.7320307493209839
408230707,5201,bbejeck,2018-07-26T20:49:59Z,"just remembered, need to update `tostring` for individual graph nodes",0,0.9923306703567505
408895387,5201,bbejeck,2018-07-30T15:03:38Z,"updated this address last comments, rebased from trunk",0,0.9945255517959595
408991717,5201,bbejeck,2018-07-30T20:03:05Z,updated to include clean-up from on minor cleanup prs,0,0.9811943173408508
409269000,5201,bbejeck,2018-07-31T15:46:08Z,kicked off system tests [a link] also kicked off streams smoke tests [a link],0,0.993061363697052
409429334,5201,bbejeck,2018-08-01T02:32:20Z,i like `processorgraphnode` wdyt?,0,0.9908526539802551
409433491,5201,bbejeck,2018-08-01T02:58:42Z,updated this and rebased from trunk,0,0.9939814209938049
409443610,5201,guozhangwang,2018-08-01T04:14:04Z,sounds good to me.,1,0.9217013120651245
409580226,5201,bbejeck,2018-08-01T13:46:48Z,failure unrelated. retest this please,0,0.7219146490097046
409619948,5201,bbejeck,2018-08-01T15:40:41Z,updated to rename `statelessprocessornode` to `processorgraphnode` from comments.,0,0.9940392971038818
409639309,5201,guozhangwang,2018-08-01T16:38:57Z,"one more meta comment: we had a bunch of warnings due to lack of types for processorparameters. this can be resolved by adding the following, e.g. in `kstreamimpl#filter()`: [code block] i.e. we can still maintain the typing information here when constructing the logical node.",0,0.9908039569854736
409676911,5201,bbejeck,2018-08-01T18:32:42Z,updated this,0,0.9438372254371643
409739132,5201,guozhangwang,2018-08-01T22:01:33Z,lgtm. merging to trunk.,0,0.9873740077018738
1557159854,13639,dajac,2023-05-22T12:47:55Z,thanks for your reviews. i think that i have addressed all your comments.,1,0.9661481976509094
1562650336,13639,dajac,2023-05-25T10:16:58Z,"thanks for your comments. i have addressed them, i think.",1,0.9577818512916565
378068897,4812,vvcephei,2018-04-02T22:39:17Z,just starting on this; still need to add the missing measurements and actually run the tests.,0,0.9696855545043945
378777667,4812,guozhangwang,2018-04-04T23:35:56Z,jenkins failures are relevant to `unused import - ..`.,0,0.9882325530052185
378780973,4812,vvcephei,2018-04-04T23:56:20Z,"ah, thanks, by the time i went to look at the last failure, the logs were already gone.",1,0.9346045851707458
378791813,4812,vvcephei,2018-04-05T01:10:36Z,"i took the liberty of resolving all my ide warnings for the test files. let me know if i went too far, and i can revert them.",0,0.9765814542770386
378792028,4812,vvcephei,2018-04-05T01:12:00Z,"checkstyle complained because i explicitly imported log4j, which is appropriate in this case. i isolated the usage to a ""testutils"" package, so i could allow the usage without allowing it for all of ""stream.processor.internals"".",0,0.9586765766143799
379272773,4812,vvcephei,2018-04-06T14:35:55Z,"i think you're right. for streams, the thread level is the most global scope we have atm. i think what you're pointing out is that i've conflated the global scope with the thread scope. ideally, these would be two separate scopes. let me refactor a bit more, and see what you think.",0,0.8604787588119507
379318433,4812,guozhangwang,2018-04-06T17:16:04Z,"actually i'm arguing that `streamsmetrics` should only be some sort of a util class, that 1) wraps the actual `metrics` object as the metrics registry, 2) provides util functions like `addxxsensor`, `addsensor` etc. and then `threadmetrics`, `taskmetrics` etc exposes the api of their defined sensors, and in their corresponding impl class they keep a reference of the `streamsmetrics` to register new metrics. we still need to note that, since we allow users to register their custom metrics via the `streamsmetrics`, we need to expose `streamsmetrics` in user-facing apis, while for other built-in `xxmetrics` they are only used internally, while we can still expose their `xxsensor` functions for other internal classes to use.",0,0.9869567155838013
379376338,4812,vvcephei,2018-04-06T20:58:28Z,"and about that experimental commit, i've decided to ditch it and implement the kip with minimal changes to the structure of the metrics. i think i'd like to submit a kip to alter the metric registration strategy we're employing later on, but i don't want to pollute kip-274.",0,0.9819522500038147
379401092,4812,vvcephei,2018-04-06T22:27:00Z,"i don't understand why these tests are failing. the message says: [code block] but line 804 in streamtasktest is: [code block] retest this, please.",-1,0.8687370419502258
379406708,4812,mjsax,2018-04-06T23:04:36Z,"i see [code block] (line 804) -- this makes sense, it should be `task = createstatelesstask(createconfig(true));`",0,0.9938209652900696
379412401,4812,vvcephei,2018-04-06T23:43:20Z,"ah, it was because jenkins (surprisingly) merges with trunk before testing. also, there was an undetected merge conflict, resulting in the broken code. i've rebased and corrected it. once the tests pass for me, i'll push again.",0,0.9746527075767517
379905976,4812,vvcephei,2018-04-09T21:54:21Z,"i'm currently working on adding metrics there. i'm also adding warning logs, as it's totally silent right now.",0,0.9844282269477844
379907479,4812,vvcephei,2018-04-09T22:00:57Z,and regarding this: ack. i might do that in a follow-up pr (under the same jira/kip) to keep the loc in this pr lower.,0,0.9910752773284912
379914936,4812,vvcephei,2018-04-09T22:35:54Z,"i've rebased and pushed the latest changes. i still need to add tests for the processors' metrics, but this change is otherwise pretty much where i want this pr to be. note that i rebased and put the change to `sensor` at the beginning so that i can send that one change as a separate pr, if needed, for review by the client library people. this change is necessary, since the existing `maybeaddmetric` implementation in streams is unsafe. to be done properly, it has to be synchronized, which my change does. my change also allows us to differentiate between registering a metric twice for a particular sensor (ok) and registering the same metric name on two different sensors (not ok). also, reminder that we should agree on a log style. i left `[]` in the pr as a placeholder. the discussion for this is above.",0,0.7852702736854553
380241162,4812,vvcephei,2018-04-10T20:46:46Z,"ok, , this pr is ready for another pass. i have completed all code and tests. docs will follow in another pr. please comment on: * my strategy for sharing the skipped metric around the code base * my changes to sensor (and whether i need to send a separate pr for that change) * the aforementioned log enclosing delimiter discussion thanks, all.",1,0.9595636129379272
380859168,4812,vvcephei,2018-04-12T16:07:55Z,"huh, that's a new one. it looks like (aside from the kafkaadmintest continuing to flake out), the tests failed because the jenkins worker ran out of disk! i'll wait until the last job completes before starting them again. i've rebased this pr on trunk now that #4853 is merged. i still have a few nits to clean up. i'll notify again when i'm ready for final reviews.",0,0.7195051312446594
380971785,4812,mjsax,2018-04-12T23:11:48Z,meta comment: please update the pr title with the jira number,0,0.9958103895187378
380976453,4812,vvcephei,2018-04-12T23:40:43Z,sorry about that,-1,0.9929924011230469
381155079,4812,vvcephei,2018-04-13T14:34:50Z,the tests passed. the failure was a rate-limit exception publishing coverage to github: [code block],0,0.9924875497817993
381174729,4812,vvcephei,2018-04-13T15:36:51Z,"hey , i hear you on the pollution of this pr, and the unsatisfactory state of metrics refactoring here. maybe i'll take a couple of hours and extract all the code cleanup (making variables final, etc.) into a separate pr and then rebase this one on that. then, this pr will be smaller, and i'll feel more comfortable proceeding with some more refactoring of the metrics code.",0,0.7168054580688477
381276277,4812,vvcephei,2018-04-13T22:33:15Z,i have pulled out the trivial changes into pr #4872 and rebased this pr on that one. please focus reviews on #4872 until it is merged.,0,0.9908231496810913
382128932,4812,vvcephei,2018-04-17T20:20:42Z,rebased on trunk now that dependee pr is merged.,0,0.9942936301231384
382191401,4812,guozhangwang,2018-04-17T23:34:47Z,jenkins failure: [code block],0,0.975396990776062
382848909,4812,vvcephei,2018-04-19T19:11:18Z,"ok, , i believe this is ready for final review. i've made a pass over it to make sure the diff is clean and to comment on the rationale of some of the choices. the diff is still quite long, but it's mostly because of all the processors that now record skipped metrics and the corresponding tests.",0,0.8329704403877258
383137615,4812,vvcephei,2018-04-20T15:43:47Z,thanks for the review! i added [a link] and [a link] in response to your comments. i also have a next pr queued up for after this one is merged (in response to our concerns about namedcachemetrics). please let me know what you think! -john,1,0.9931491017341614
383609305,4812,vvcephei,2018-04-23T15:07:23Z,addressed bill's comments and rebased.,0,0.9904154539108276
534847170,7378,jukkakarvanen,2019-09-25T04:34:27Z,there is still a lot of old topologytestdriver test using deprecated methods needed to migrate to use new ones. so let me know if someone have possibility to help with those.,0,0.9808654189109802
535173884,7378,jukkakarvanen,2019-09-25T19:24:32Z,example class and develper guide updated.,0,0.9915653467178345
536172291,7378,jukkakarvanen,2019-09-28T10:05:38Z,deprecated method migrated in streams and streams-scala packages,0,0.9937534928321838
537780874,7378,mjsax,2019-10-03T04:29:19Z,"seems there are some conflicts -- can you rebase this pr? the feature freeze deadline was pushed to friday, so we still have 2 days to get this into 2.4 :)",1,0.8934893608093262
538072445,7378,vvcephei,2019-10-03T18:38:22Z,"hey , i just had another thought while reading over 's comments... for each of the ""deprecated"" javadoc lines, can you also mention the version it was deprecated in? like, ` since 2.4. please use xxxxx instead...` it just helps everyone understand when exactly the method should be removed.",0,0.8158661127090454
538112784,7378,jukkakarvanen,2019-10-03T20:23:00Z,", conflicts resolved changes made based on review also added deprecated since 2.4 based on suggestion",0,0.9810535311698914
538365070,7378,bbejeck,2019-10-04T11:50:12Z,java 11/2.12 failed with [code block] java 8 timed out retest this please,0,0.991784930229187
538501838,7378,jukkakarvanen,2019-10-04T18:01:52Z,i tried to cover all your comments,0,0.9773548245429993
538528904,7378,mjsax,2019-10-04T19:22:09Z,lgtm.,0,0.9637624621391296
538614484,7378,jukkakarvanen,2019-10-05T04:11:03Z,unrelated integration test failures. retest this please,0,0.9417821168899536
538702218,7378,bbejeck,2019-10-06T01:02:43Z,java 11/2.12 failed with [code block] java 11/2.13 failed with [code block] java 8 failed with [code block] retest this please,0,0.9931513071060181
538832801,7378,mjsax,2019-10-07T04:12:38Z,"jdk 11 / 2.12 [code block] jdk 11 / 2.13 [code block] java8 no test failures, but timed out. retest this please.",0,0.9916927814483643
538883648,7378,mjsax,2019-10-07T08:00:54Z,`org.apache.kafka.connect.integration.exampleconnectintegrationtest.testsourceconnector` failed. unrelated.,0,0.9875375628471375
425418058,5709,bbejeck,2018-09-28T12:20:56Z,failure unrelated retest this please,0,0.5913138389587402
425575819,5709,bbejeck,2018-09-28T21:49:47Z,updated per comments,0,0.9704824686050415
425780847,5709,bbejeck,2018-10-01T03:32:20Z,as we are re-using the repartition topic name when optimizing checking for the `-repartition` suffix was done during those cases when re-using the topic name which is already formatted with `appid-basename-repartition` i have since cleaned up the code and re-use the topic name in the `internalstreamsbuilder` when performing the optimization.,0,0.994170606136322
425788398,5709,guozhangwang,2018-10-01T04:47:37Z,"hmm. i still cannot fully understand it since from the source code, the existing `repartitionforjoin` function seems not reused elsewhere in this pr.",0,0.9371737837791443
425791386,5709,bbejeck,2018-10-01T05:13:33Z,updated this,0,0.9438372254371643
425924162,5709,bbejeck,2018-10-01T14:16:25Z,failures unrelated retest this please,0,0.8057448267936707
426015601,5709,bbejeck,2018-10-01T18:35:52Z,"call for final review - i believe i've addressed all of your comments, and this is ready for merging",0,0.8639419674873352
426085849,5709,bbejeck,2018-10-01T22:30:35Z,restest this please,0,0.9829925894737244
426094341,5709,lindong28,2018-10-01T23:12:56Z,this pr seems very close to be merged. do you think we can merge it in a day or two for 2.1.0 release?,0,0.9774976372718811
426096811,5709,mjsax,2018-10-01T23:26:41Z,the goal is to merge it today.,0,0.9934985637664795
426135610,5709,bbejeck,2018-10-02T03:19:56Z,rebased this,0,0.9921702742576599
426159558,5709,mjsax,2018-10-02T06:08:01Z,merging this. please address comments if follow up pr.,0,0.9946006536483765
200954587,812,rajinisivaram,2016-03-24T18:12:33Z,do you by any chance have time to review this pr? thank you...,1,0.9057470560073853
205629220,812,junrao,2016-04-05T03:47:46Z,could you also run the system tests on this patch?,0,0.9948371648788452
205806557,812,ijuma,2016-04-05T13:32:21Z,quick note to mention that the pr does not merge cleanly to trunk at the moment.,0,0.9855645298957825
206484021,812,rajinisivaram,2016-04-06T17:43:45Z,thank you for the review. i have done the rebase and started a system test run.,1,0.9358679056167603
206877257,812,rajinisivaram,2016-04-07T12:50:49Z,system test results are here: [a link] there was one failure (offsetvalidationtest) which is an existing transient failure already being addressed by kafka-3513.,0,0.9952241778373718
209425539,812,rajinisivaram,2016-04-13T13:03:32Z,"the latest commit replaces the request-response to configure sasl mechanisms with asaslhandshakerequest/saslhandshakeresponse pair that conforms to the standard kafka protocol, as discussed in the kip-35/kip-43 threads in the mailing list. the overall code structure for authentication/handshake has not been changed.",0,0.9927229285240173
212379418,812,rajinisivaram,2016-04-20T10:55:43Z,will you be able to review the latest changes so that this can be committed for 0.10.0? i have rebased and started another system test run. thank you.,1,0.9546960592269897
212644270,812,rajinisivaram,2016-04-20T22:56:23Z,thank you for the review. i have made most of the updates and left a couple of responses for you to review. many thanks.,1,0.9886041879653931
213326258,812,rajinisivaram,2016-04-22T08:37:37Z,the latest system test results from this branch are here: [a link],0,0.9941171407699585
213821646,812,ijuma,2016-04-23T20:07:14Z,"thanks for running the system tests. are you going to update some of the system tests to test the mechanism functionality? also, weren't there some unit (as opposed to integration) tests for sasl in a previous iteration of the pr?",1,0.8919422626495361
214000262,812,junrao,2016-04-24T16:51:46Z,": thanks for the patch. looks good to me. just had a minor comment. also, once the vote for kip-35 passes, would you be up for adding the support of apiversionrequest in the sasl authenticator layer since you are more familiar with the logic there?",1,0.9915338158607483
214094390,812,ijuma,2016-04-25T02:15:19Z,thanks for the pr. i have completed my review now and left some comments.,1,0.9285855889320374
214276979,812,rajinisivaram,2016-04-25T11:39:15Z,"thank you for the reviews. i have made most of the updates suggested and left a few comments. i had moved the sasl unit tests out of this pr to make it easier to keep it updated (since the tests have quite a bit of refactoring that makes it hard to rebase). once this is committed, i will rebase and submit the additional tests in another jira. i will also submit the pr for system tests of the changes - i think it should be sufficient to run only a few tests with plain, plain+gssapi. i will be happy to submit the pr for handling apiversionrequests during sasl handshake as well, once the new request is added for kip-35.",1,0.986957311630249
214356654,812,ijuma,2016-04-25T14:20:32Z,", thanks for updating the pr. i reviewed your updates and left some very minor comments. the main item left on this pr in my opinion is how we name the inter-broker sasl mechanism property. i think it would be better if the config name is consistent with the property for inter-broker security protocol (ie there is an `inter.broker` in the name). let's see what thinks. before 0.10.0.0, we need to: 1. add unit, negative and system tests for this pr 2. add handling of apiversionrequests during sasl handshake would you mind please filing jiras with 0.10.0.0 as the ""fix version""?",1,0.969459593296051
214464394,812,rajinisivaram,2016-04-25T18:03:19Z,thank you for reviewing the updates. have filed new jiras for the remaining work: - kafka-3617 : add unit tests for sasl authenticator - kafka-2693 : ducktape tests - kafka-3618: handle apiversionrequest before sasl handshake,1,0.9353963136672974
214563135,812,ijuma,2016-04-25T23:35:03Z,"thanks , lgtm.",1,0.8876838684082031
214868708,812,ijuma,2016-04-26T20:01:20Z,", i ran the unit/integration tests with this branch twice and both times i got some failures in sasl tests. do the tests pass reliably for you? is it possible that a recent change is causing issues?",0,0.9857207536697388
214900437,812,rajinisivaram,2016-04-26T21:57:36Z,"the tests have been passing consistently for me. i reran them again a few times after rebasing on the latest level and they still pass. if you let me know which tests have been failing for you, i can rerun those in a loop to see if i can recreate the failure. will also see how the automated pr build does after the rebase. thanks.",1,0.984345018863678
214906615,812,ijuma,2016-04-26T22:27:01Z,", thanks. i am running the tests again in case it was a mistake on my end. if they pass in jenkins and locally, i'll merge the pr (i checked with jun and he's happy with it). if not, i'll provide information about the failing tests.",1,0.969260573387146
214924091,812,ijuma,2016-04-26T23:57:31Z,"the failing tests were related to the flaky wireless network in the kafka summit, i got the same failure with ssl tests (where sasl wasn't involved). after i stabilised the network, the tests passed twice and they passed in jenkins too. merged to trunk.",0,0.9876675605773926
137300283,165,ewencp,2015-09-03T01:54:36Z,"i left a few comments, but mostly they were things that could potentially be clarified. only one question about correctness. after those issues are followed up on, this looks good to me.",1,0.9074286222457886
146709577,165,hachikuji,2015-10-08T22:47:58Z,"i've updated this patch based on the latest proposal ([a link] which uses a single client to do the group assignment. i'm still doing a bit of polishing and adding tests, but since most existing tests are passing, it seems ready to start reviewing. there is one failing kstreams test that i have yet to investigate. a few general notes on the patch: - one significant difference from the proposal is that i had to push the assignment strategies out of the consumer's embedded protocol and into the joingroup request as a ""subprotocols"" field. this allows the coordinator to do assignment strategy selection similar to the initial patch. it also lets the coordinator reject incompatible members without forcing a group rebalance. otherwise, we can only detect inconsistencies after the group has rebalanced and the leader sees the metadata from all members. the problem then becomes how the group should handle the inconsistency? we could let the leader kick certain members out of the group, but that complicates the protocol. we could also just fail the entire group, but that seemed extreme. in the end, it seemed simplest to give the coordinator a mechanism to validate compatibility of group members. - this brings up the issue of whether each ""subprotocol"" (or assignment strategy) needs to have its own metadata and embedded as in the initial patch. without it, any assignment-strategy-specific information must itself be embedded in the member metadata included in the joingroup request. this is easy if the consumer is using a metadata format like json, but trickier with packed arrays. the advantage of keeping everything in the same metadata, however, is that it simplifies the protocol and avoids redundancies such as including subscriptions multiple times. - if the leader has an unexpected error during synchronization, the patch does not provide a way to propagate that error to the rest of the group as the proposal called for. instead, the leader will propagate the exception to the user and the leader's session timeout will expire on the coordinator which will allow for a new leader to be elected. the problem i found when trying to implement this feature is that it requires a new state on the coordinator which is basically like stable except contains an error to be propagated when the rest of the group syncs, which adds considerable complexity. it also creates a race condition where some members might see the error and immediately rejoin which forces the group out of that state. this means that some members wouldn't generally see the error. in the end, it felt like the cost of propagating the error through the coordinator was too high to justify capability it was exposing. since unexpected leader failures should be rare, the simpler approach is just to let it timeout. - the versioning in the consumer's embedded protocol (as contained in consumerprotocol.java) is clearly inadequate and will require further consideration. i'm inclined to push that to a separate jira so that we can consider it without the rest of the noise around this patch. - the protocol implemented here provides no special support for regex handling. as before, each member watches metadata and adds a normal topic subscription when it finds a new matching topic. if we want to have special handling, that can also be pushed to another jira.",0,0.5209600925445557
147134061,165,ewencp,2015-10-10T23:19:31Z,"not critical for this patch, but do we want to reconsider the packages for some of the classes? for example, `groupcoordinator` is currently in `org.apache.kafka.clients.consumer.internals`. keeping it in _some_ `internals` package makes sense at the moment to indicate it shouldn't be relied on publicly, but since it's generalized functionality, housing it under `consumer` might not be ideal. not sure if we want a new package (e.g. `org.apache.kafka.clients.group`), move it under `common`, or just leave it where it is and preserve some indication of its heritage.",0,0.9655458331108093
147185767,165,ijuma,2015-10-11T11:47:38Z,just a heads-up: this branch doesn't merge cleanly against trunk anymore.,0,0.9776907563209534
147262916,165,hachikuji,2015-10-12T00:16:20Z,"yeah, i think it makes sense to move it to a new package. to make that possible, i think we might have to merge consumernetworkclient into networkclient, but that's the only complication i can think of.",0,0.9875232577323914
147550028,165,guozhangwang,2015-10-12T23:40:58Z,"a general comment: i feel adding this to the join-group to allow coordinator-side pre-checking would be ok, since although it adds an extra field in the join group request, it is not very hard to understand. but now the protocol field seems less useful to me given the group-id and sub-protocols, since we only use it to check all members use the same group-protocol string. but in practice how possible that members joining the same group name happen to be different types (e.g. one normal consumer and one copycat connector)? having this field will not be so useful in excluding human-mistakes but even likely increase false-positives that two members that do belong to the same group got kicked out because they happen to specify different protocol names by mistake. so i would suggest we remove the protocol string and rename sub-protocols to protocols, doing this we may end up having group ids like ""kafka-consumer-xxx"", ""kafka-stream-yyy"" and ""kafka-copycat-zzz"", which is already sufficient to differentiate different types of members.",0,0.9218728542327881
147768297,165,hachikuji,2015-10-13T16:27:51Z,"that suggestion sounds fine to me, especially if it makes the protocol easier to understand. it leads to the question of how we are going to implement protocol-specific metadata. there's basically two options: **option 1:** each protocol has its own metadata: this is probably the simplest to implement for client extensions. this would change the joingroup request to look something like this: [code block] the downside is that this leads to redundant data in the request. for example, to support both the round-robin and range assignors in the consumer (at the same time), you'd have to pass the subscription list twice. perhaps this is a rare enough case that we aren't too worried about the overhead? it also leads to somewhat more complex parsing, though i don't think this is a major concern. **option 2:** the second option, which is currently implemented, is that we have only one metadata field and we assume that any protocol-specific attributes are embedded in it. it would look like this: [code block] this is generally more difficult for clients to implement, but that depends largely on the format of the metadata. for example, if json is chosen, then protocol-specific attributes come naturally. however, it's a little tough to see how this would work for kafka's packed structures, which have no field identifiers. as an advantage, since it allows protocols to share metadata, it avoids the duplication in the first approach. the groupprotocol field in the current patch was basically intended to declare compatibility of the metadata as used in option 2. if we get rid of it, then i would tend to favor the first option. what do you think?",0,0.8609796166419983
147772825,165,ewencp,2015-10-13T16:44:34Z,"it might be worth making multiple assignors work to figure out how best to handle this. right now the code only handles one assignor and i think that makes option 2 look like it can work, but the code is probably going to get messy or confusing if you try to actually implement it. since you only have one metadata field, you'll need to somehow combine all the metadata required by different assignors. this means you'll have to assume some format like json and merge them. i think the duplication in option 1 isn't a big deal. it should definitely be rare. it also keeps things simpler -- the joingroup request is a bit more complex, but the whole implementation is easier and we don't end up with completely different protocols having to coordinate in order to make sure their metadata is compatible with each other.",0,0.7813045382499695
147814103,165,guozhangwang,2015-10-13T18:58:33Z,"my understanding is that multiple protocols will only be used for upgrades, and most of time the protocols size should be 1. if that is the case: 1. 98% we will have 1 protocol. 2. 1.9999% we will have 2 protocols. 3. 0.0001% we will have 3 protocols. so i think option 1) would be find, but could clarify if there are no scenarios that multiple protocols are needed life-time.",0,0.9900482892990112
147815469,165,ewencp,2015-10-13T19:04:04Z,i think 3 protocols should never happen aside from misconfiguration. the only use case for 2 is to upgrade/switch assignors.,0,0.9692364931106567
147816246,165,guozhangwang,2015-10-13T19:07:26Z,"yeah, for upgrade / switch 2 protocols are temporary since we will usually do two rolling bounce to remove the old one in the second round. anyways, i think option 1) should be fine.",0,0.9628530740737915
148478770,165,hachikuji,2015-10-15T18:19:44Z,i've reworked the protocol as discussed to allow protocol-specific metadata with coordinator negotiation similar to the initial patch. basically this resulted in pushing serialization concerns into abstractcoordinator extensions. also have a look at the new partitionassignor interface. it should be much clearer how assignor implementations can leverage custom metadata.,0,0.9855552911758423
148802867,165,hachikuji,2015-10-16T18:43:34Z,made the changes discussed. i think this does simplify the assignor internals quite a bit.,0,0.8944786787033081
148823145,165,guozhangwang,2015-10-16T20:17:10Z,the client-side protocol and implementations lgtm overall. thanks! do you want to refactor the server-side fsm a bit now? also some response error codes like syncgroupresponse need to be updated as well.,1,0.9812397956848145
149285168,165,hachikuji,2015-10-19T17:14:01Z,anything else to clear up before this can be merged?,0,0.9928701519966125
149287269,165,guozhangwang,2015-10-19T17:21:07Z,"i'm still reviewing the changes made on the server-side coordinator, i thought you did the fsm refactoring right?",0,0.9889937043190002
149288054,165,hachikuji,2015-10-19T17:23:52Z,"i updated the fsm documentation that was in groupmetadata.scala. was there anything else you wanted me to fix? by the way, i'm rebasing off of flavio's commit and should be able to update shortly.",0,0.9645292162895203
149288862,165,guozhangwang,2015-10-19T17:27:18Z,the problem is that i cannot get the full diff now as it become too big. right now i can only review it per-commits which is not optimal..,-1,0.8930931687355042
149308251,165,guozhangwang,2015-10-19T18:39:44Z,"this is what i think about the state machine (if we agree we can also update the diagram in the wiki), with the assumption that for all request handling we first check coordinator availability and group ownership is correct. all the events except consumer failure is triggered by requests. state down: group is either not created or has no members. -> onjoin: create group and add member if necessary (start hb in purgatory), transit to preparerebalance. -> onsync: return rebalance_in_progress (could be from as -> pr -> down). -> onheartbeat: return illegal_generation. -> onoffsetcommit: blindly accepts and return ok. -> onoffsetfetch: blindly accepts and return ok. -> onconsumerfailure: should not happen, throw exception. state preparerebalance: waiting for all members to join -> onjoin: check if all members joined, if yes transit to awaitingsync -> onsync: return rebalance_in_progress (could be from as -> pr). -> onheartbeat: return rebalance_in_progress. -> onoffsetcommit: return rebalance_in_progress. -> onoffsetfetch: return rebalance_in_progress. -> onconsumerfailure: remove consumer (stop hb in purgatory), check if all rest members joined, if yes transit to awaitingsync. state awaitingsync: waiting for leader to assign -> onjoin: update / create consumer if necessary, transit to preparerebalance -> onsync: if from follower, treat it as hb and park it (unmute further hbs); otherwise treat it as hb for the leader and transit to stable. -> onheartbeat: return rebalance_in_progress. -> onoffsetcommit: return rebalance_in_progress. -> onoffsetfetch: return rebalance_in_progress. -> onconsumerfailure: remove consumer (stop hb in purgatory), if that is the leader transit to preparerebalance state stable: group formed with resource assigned and remembered by coordinator -> onjoin: update / create consumer if necessary, transit to preparerebalance -> onsync: handle normally with remembered assignment -> onheartbeat: handle normally. -> onoffsetcommit: check on member-id / generation-id and return ok or corresponding errors. -> onoffsetfetch: check on member-id / generation-id and return ok or corresponding errors. -> onconsumerfailure: remove consumer (stop hb in purgatory), transit to preparerebalance. --- transit action: from down to preparerebalance: 1) start delayed rebalance in purgatory if there are still members in the group 2) mute delayed hb for all members in the group transit action: from preparerebalance to awaitingsync: 1) clear dalyed rebalance in purgatory, send back join response to followers and leader 2) unmute delayed hb for all members in the group transit action: from awaitingsync to preparerebalance: 1) send back sync response to all parked requests with rebalance_in_progress 2) start delayed rebalance in purgatory if there are still members in the group 3) mute delayed hb for all members in the group transit action: from stable to preparerebalance: 1) start delayed rebalance in purgatory if there are still members in the group 2) mute delayed hb for all members in the group --- i think we are already doing this in this patch, but just want to make the comments in groupmetadata to be more clear. and not need to include in this jira, but moving forward we will possibly add a background scheduler that periodically remove groups with zero members or not belonged to coordinator any more and transit them to dead.",0,0.9861970543861389
149316362,165,hachikuji,2015-10-19T19:12:04Z,"thanks for writing that up! i think there's a couple slight differences in the current patch from what you wrote: state down: group is either not created or has no members. -> onheartbeat: return unknown_member_id -> onoffsetcommit: return illegal_generation if generation is positive, otherwise accept commit -> onoffsetfetch: return unknown_member_id state preparerebalance: waiting for all members to join -> onoffsetcommit: accept commits from previous generation, otherwise illegal_generation -> onoffsetfetch: return offsets blindly state awaitingsync: waiting for leader to assign -> onoffsetcommit: allow commit from the joined generation -> onoffsetfetch: return offsets blindly for the awaitingsync state, i think your suggestion to return rebalance_in_progress for offset commits is better than the current behavior since there should be no need to commit offsets before receiving the assignment and the member still has the preparingrebalance state to commit offsets before rebalances. for fetching committed offsets, i actually wonder if we should skip all group checks and return the offsets blindly. do we really need to impose the restriction that committed offsets can only be queried by members of the group?",1,0.9935277104377747
149355804,165,guozhangwang,2015-10-19T21:42:12Z,could we make partitioner also configurable as we discussed offline? besides the above comments lgtm.,0,0.9954125285148621
149362802,165,hachikuji,2015-10-19T22:13:13Z,have a look at the updated state transitions. i wonder if we should have a `ready` state for the initial state when no group members have joined. currently the group starts up in `stable` even though it has no members. what do you think?,0,0.9754980206489563
149372398,165,guozhangwang,2015-10-19T23:07:40Z,"i agree to the `ready` state, since if we are going to clean up groups moving forward we will no longer need the `dead` state any more.",0,0.9786460995674133
149996276,165,guozhangwang,2015-10-21T19:07:29Z,"lgtm, the jenkins failures seem irrelevant and transient. also made a passing ducktape system test build on this branch. please remember to address 's comments in the later follow-up path. merging this large patch for now as it is getting very large.",0,0.9893798232078552
2427455305,17539,adixitconfluent,2024-10-21T18:41:04Z,"hi , please review my pr when you get a chance. thanks!",1,0.9875962734222412
2433136716,17539,adixitconfluent,2024-10-23T18:35:35Z,"hi , thanks for being kind enough to reviewing this pr. i have addressed both your comments. please take a look when you can!",1,0.9909900426864624
2450647031,17539,adixitconfluent,2024-10-31T19:18:26Z,"hi , i am still in the middle of the refactor. i will raise a re-review once i am done. i was just resolving the comments for which i pushed the fixes in my recent commits, probably that gave a false idea that i was done addressing your changes, sorry about that.",-1,0.9853336811065674
2452271960,17539,adixitconfluent,2024-11-01T17:21:19Z,"hi , i have addressed all your comments. please re-review my pr when you get a chance. thanks!",1,0.9883302450180054
2455446039,17539,adixitconfluent,2024-11-04T18:42:13Z,"hi , i have addressed your comments. please re-review my pr when you get a chance. thanks!",1,0.9865790009498596
2461751459,17539,adixitconfluent,2024-11-07T09:38:03Z,"hi , i've addressed the comments from the latest review. please re-review my pr when you get a chance. thanks!",1,0.9876536726951599
293814064,2849,dguy,2017-04-13T07:27:46Z,"for reviews please. note: this is not the complete transactioncoordinator, i.e., transaction expiration, transactionalid -> pid mapping expiration, and recovery in handling initpidrequest all haven't been done yet",0,0.9858230352401733
296940620,2849,dguy,2017-04-25T07:27:43Z,thanks for taking the time to review again. regarding: correct they have not been done yet.,1,0.961566150188446
297542444,2849,guozhangwang,2017-04-26T21:11:18Z,collapsed all commits and merged to trunk.,0,0.9930075407028198
297552932,2849,ijuma,2017-04-26T21:55:47Z,"for large prs like this, we should run the system tests before we merge. can we post a link to a successful run for future record (assuming we've done that)?",0,0.9941540360450745
297553624,2849,hachikuji,2017-04-26T21:58:55Z,i've kicked off a build here: [a link] let's cross our fingers since it's already merged!,1,0.9323981404304504
297651197,2849,ijuma,2017-04-27T08:40:52Z,"thanks , build passed. :)",1,0.9966729879379272
298811252,2964,guozhangwang,2017-05-03T02:38:23Z,i am still working to fixing all the unit tests but the non-testing code is ready for reviews.,0,0.9651191234588623
298820761,2964,apurvam,2017-05-03T04:26:29Z,looks like the build fails with compilation errors..,0,0.8754266500473022
298828273,2964,guozhangwang,2017-05-03T05:51:20Z,those are unit test compilations. i'm still working on those but non-testing code does build.,0,0.9882748126983643
298833334,2964,apurvam,2017-05-03T06:37:02Z,ah.. ok.. thanks!,1,0.976762592792511
300638549,2964,guozhangwang,2017-05-10T23:14:48Z,i have addressed your comments. two major modifications: 1. `transactionmarkerrequestcompletionhandler`: exhaust all possible error codes. 2. merge `transactionmarkerchannel` into `transactionmarkerchannelmanager`.,0,0.9870672821998596
300989188,2964,guozhangwang,2017-05-12T05:45:28Z,"addressed your comments, and fixed unit tests. also another observation is that we are unnecessarily sending multiple txn marker requests to a single broker for txnids from different txn topic partitions, i have grouped them into a single request as well in this pr. cc",0,0.9874890446662903
301195878,2964,guozhangwang,2017-05-12T22:01:31Z,merged to trunk.,0,0.9864094257354736
301222344,2964,guozhangwang,2017-05-13T03:31:54Z,"thanks for the comments, i will try to incorporate them in a follow-up pr.",1,0.9164412021636963
248246823,1884,dguy,2016-09-20T09:10:47Z,"one general comment. `internaltopicmanager` isn't really doing anything anymore. i left this comment on the previous pr: it seems to me that they should be merged into a single class? , thoughts?",0,0.9174911975860596
248379483,1884,hjafarpour,2016-09-20T17:53:45Z,made changes according to your feedback.,0,0.9862051606178284
248967534,1884,hjafarpour,2016-09-22T17:14:23Z,made changes according to your feedback and pushed the changes.,0,0.9918994903564453
249255156,1884,hjafarpour,2016-09-23T17:36:00Z,applied the feedback and pushed the changes.,0,0.9936460256576538
249611052,1884,dguy,2016-09-26T15:50:50Z,"oic - the problem with making it public on kafkastreams is that it would then be part of the public api. we probably should move the field from kafkastreams to an internal class and have kafkastreams and this class reference that. problem is i'm not entirely sure where to put it right now. [a link] might have an idea. otherwise, just leave it here and we can fix it later on mon, 26 sep 2016 at 16:42 hjafarpour notifications.com wrote:",0,0.9256038665771484
249612400,1884,hjafarpour,2016-09-26T15:55:27Z,"alright, leaving it as it until we decide where we can move the field.",0,0.9902634620666504
250317873,1884,hjafarpour,2016-09-28T22:16:18Z,pushed new changes.,0,0.9884201288223267
250342391,1884,guozhangwang,2016-09-29T00:49:50Z,"i ran the unit test locally and the jenkins failures seems to be pretty consistent, were your local unit test run passed? [code block]",0,0.9911386370658875
250561766,1884,nitantbhartia,2016-09-29T19:07:45Z,do you have a timeline for this fix? we need this fix as part of one of our client apps that we are building that uses spring cloud stream. anything my team or i can do to help?,0,0.9739964604377747
255222098,1884,guozhangwang,2016-10-20T20:41:37Z,"sorry for the late reply, we are currently merging this pr as a post-0.10.1.0 pr, which means that it will likely to be included in the next minor release or in a bug-fix release (0.10.1.1).",-1,0.9757694602012634
256424024,1884,guozhangwang,2016-10-26T17:43:38Z,could you file a kip for this api change as well?,0,0.995051920413971
270991824,1884,guozhangwang,2017-01-06T20:00:09Z,test this please,0,0.9870431423187256
271060729,1884,guozhangwang,2017-01-07T03:55:46Z,test this please,0,0.9870431423187256
271096444,1884,guozhangwang,2017-01-07T17:16:05Z,jenkins failure seems related: [code block] [code block],0,0.9897010326385498
271452298,1884,hjafarpour,2017-01-10T00:35:42Z,test this please,0,0.9870431423187256
271714172,1884,hjafarpour,2017-01-10T22:10:31Z,test this please,0,0.9870431423187256
271930762,1884,guozhangwang,2017-01-11T17:16:25Z,"lgtm. merged to trunk, thanks !",1,0.9885109663009644
406449812,5379,stanislavkozlovski,2018-07-20T00:05:09Z,could you give this another look?,0,0.9840785264968872
406665448,5379,stanislavkozlovski,2018-07-20T17:10:08Z,i think the final commit addresses all of the comments. can you confirm?,0,0.9913039207458496
406723141,5379,stanislavkozlovski,2018-07-20T20:50:22Z,made `saslextensionscallback` return `saslextensions`. please take a look,0,0.9940153956413269
406733607,5379,stanislavkozlovski,2018-07-20T21:38:54Z,seems like some related tests are failing. currently investigating them but it's not obvious what caused them to fail at all,0,0.7453646063804626
406750227,5379,stanislavkozlovski,2018-07-20T23:25:15Z,"i had some issues with failing tests due to inappropriate handling of `scramextensionscallback`. i've now separated it from `saslextensionscallback` entirely - it doesn't return `saslextensions` but a `map<string, string`. hopefully this shouldn't matter too much as we deprecated it. should be all clear now. can you review?",0,0.7354593276977539
407474576,5379,stanislavkozlovski,2018-07-24T16:47:41Z,"this should address all comments , . thanks for being patient with me",1,0.9845788478851318
407884676,5379,rondagostino,2018-07-25T20:28:34Z,lgtm,0,0.9618706703186035
410763090,5379,rajinisivaram,2018-08-06T16:13:35Z,merging to trunk. thanks for the reviews.,1,0.9693832993507385
137326320,191,harshach,2015-09-03T04:15:32Z,"anyone interested running this on vagrant , build kafka with this patch in and checkout [a link] . drop the .tgz under kafka-vagrant. vagrant up.",0,0.994122326374054
137876636,191,ijuma,2015-09-04T23:16:27Z,"thanks harsha, good to see this. in order to review this, it would be good to have a summary of the status of the pr, what areas (if any) you are still working on?",1,0.990365207195282
137877918,191,harshach,2015-09-04T23:28:55Z,"cleaning up the code and adding sasl configs part, more unit tests.",0,0.9931341409683228
137878143,191,ijuma,2015-09-04T23:31:49Z,thanks.,0,0.5270382761955261
146770873,191,harshach,2015-10-09T06:44:43Z,please review the updated patch. thanks.,0,0.633817732334137
146864969,191,ijuma,2015-10-09T13:03:05Z,thanks . i submitted a pr with your branch as the target: [a link] it merges trunks (and resolves the conflicts) and it includes some fixes and improvements. please review and integrate if you agree with the changes. i am still going through the code and i will submit another pr later that will include a saslconsumertest.,1,0.9735930562019348
146999246,191,harshach,2015-10-09T22:22:28Z,updated the pr with consumer test . i'll be updating with docs and other fixes mentioned in the above comments tonight.,0,0.9805617928504944
147425105,191,ijuma,2015-10-12T15:00:44Z,here's another pr [a link] which: - merges and resolves conflicts from trunk - address some of jun's and parth's comments - make fields final - reduce scope of variables where possible - remove unused fields and methods - fix javadoc - fix formatting and naming issues - return non-anonymous `kafkaprincipal` in `saslclientauthenticator.principal` please review and integrate as appropriate . there is still feedback to be addressed as well as more tests (particularly for inter-broker communication and sasl_ssl). i will continue working on these.,0,0.9552488923072815
147777026,191,ijuma,2015-10-13T16:57:10Z,thanks for regularly merging my prs harsha. here's another one: [a link],1,0.9815628528594971
147903851,191,junrao,2015-10-14T02:06:33Z,"also, a general question, if the tgt ticket can't be renewed, do we get exceptions when reading/writing through the sasl port?",0,0.9918882250785828
148224075,191,harshach,2015-10-14T22:41:49Z,yes. it will throw a kafkaexception,0,0.9854483008384705
148396199,191,ijuma,2015-10-15T14:04:28Z,"pr 4 is ready: [a link] it: - fixes issues with the loginmanager singleton (we need two instances, one for mode.server and one for mode.client and closing it is not as simple as how it's done in the existing code) - a number of logging clean-ups (avoid string concat so that we don't pay the cost if that particular level is disabled) - remove setconfiguration call as suggested by next on my list: - [x] removeinterestops issues on server and client authenticator mentioned by jun - [x] refactor tests to reduce duplication, test sasl_ssl, test sasl for inter-broker communication and - fix test failure when all the tests are run via gradle (i know what the issue is) - [x] replace kerberosname system property with config - [x] investigate if there's a way to avoid using proprietary classes in `jaasutils.defaultrealm` i think we're getting closer.",0,0.957286536693573
148564929,191,harshach,2015-10-16T01:08:09Z,i see you changed loginmanager to do client and server. but in sasl case we use kafkaserver section for both kafkaserver and any inter broker calls. the reason for this is we want keep kafkaclient section for only the clients and use the keytab in kafkaserver section. hence the reason we want to initialize the loginmanager eitehr with server or client but not with both. so in broker side we do loginmanager(server) and inside the controller or replica we use inter.broker.security.protocol and use client as mode initiate salsclientauthenticator with loginmanager.subject.,0,0.9783813953399658
148567215,191,ijuma,2015-10-16T01:18:18Z,"regarding `loginmanager`, thanks for the explanation, good to know that we want to use `kafkaserver` section for the broker, whether it's a client or a server. however, if `loginmanager` doesn't support client and server simultaneously, how will it work in situations where you have a client and a server in the same jvm (like in tests, for example)? i think `loginmanager` should support both modes, but we should change how we call it from `saslchannelbuilder` perhaps. what do you think?",1,0.9783263206481934
148568777,191,harshach,2015-10-16T01:21:49Z,yes makes sense.,0,0.980026364326477
148571060,191,junrao,2015-10-16T01:34:25Z,"is that right? in controllerchannelmanager, replicafetcherthread, and kafkaserver (for controlled shutdown), we create the channel in client mode (instead of server). this seems correct since they initiate client connections. the socketserver is always created in server mode. so, it seems that the broker will always need to support two modes for loginmanager.",0,0.9878266453742981
148572207,191,ijuma,2015-10-16T01:45:02Z,"i think the confusion is due to the fact that `mode` is being misused in `loginmanager`. it actually means `logincontexttype` in that context and it is only used to derive the logincontextname. i intend to change it along these lines so that it's clearer. , please correct me if i misunderstood.",0,0.9218617081642151
148574665,191,junrao,2015-10-16T01:57:09Z,"hmm, so all channels in controllerchannelmanager, replicafetcherthread, and kafkaserver should be created in server mode to pick up the kafkaserver section?",0,0.9847829937934875
148575986,191,ijuma,2015-10-16T02:06:16Z,"not exactly, i think it's something like this: [a link] (i just did it quickly to show it in code, still need to double-check it)",0,0.9860209822654724
148593273,191,harshach,2015-10-16T03:49:14Z,your code looks good to me. if you open a pr i can run some tests. thanks.,1,0.9953396320343018
148683145,191,rajinisivaram,2015-10-16T10:58:18Z,"i am not sure _""servicename has always been used in jaas config""_. i can run zookeeper 3.4.6 with sasl either with the default name `zookeeper` or by specifying the system property `zookeeper.sasl.client.username` to override the name. at the moment, using `servicename` in jaas.conf as the only way to configure the name prevents sasl from being used with ibm jdk. at the very least, we need to set a default. the exceptions with ibm jdk when `servicename` is set as well as the exception when `servicename` is not set are below. if `servicename` is specified, the exception is: [code block] if `servicename` is not specified, the exception is: [code block]",0,0.9491899013519287
148689700,191,ijuma,2015-10-16T11:27:12Z,i agree that we should do something about this. my preference is to move it to the kafka config.,0,0.9711860418319702
148702719,191,ijuma,2015-10-16T12:22:14Z,"pr 5: [a link] - introduces `logintype` and simplifies caching code in `loginmanager` - make `auth_to_local` configurable via kafkaconfig instead of a system property. i went with the same name as the one hadoop uses, but it's not the most intuitive so we may want to revise that (it should be a simple change though).",0,0.9903755784034729
148716439,191,ijuma,2015-10-16T13:28:46Z,i started on the inter-broker tests and things are looking good. sasl_plaintext and sasl_ssl work fine (the latter required a small fix).,1,0.8577885031700134
148738345,191,harshach,2015-10-16T14:53:37Z,ok for servicename i want to pick default via jaas config and if its not set we will look for it in the sasl configs. will that work for you?,0,0.9941728711128235
148739210,191,rajinisivaram,2015-10-16T14:57:13Z,"yes, thank you. that will work for us,",1,0.9728626012802124
148740906,191,ijuma,2015-10-16T15:04:07Z,i pushed the inter-broker tests for sasl_plaintext and sasl_ssl to [a link] working on sasl_ssl tests for producer and consumer now.,0,0.9941698312759399
149203336,191,ijuma,2015-10-19T12:45:55Z,", pr 6 is ready [a link] - merged trunk and fixed conflicts - refactored producer and consumer tests to reduce duplication and to also test sasl_ssl (fixed an important, but small bug with sasl_ssl in the process) - fixed issue where interestops was not being turned off when it should be - document `authenticate` in `saslclientauthenticator` and `saslserverauthenticator` - make it possible to configure servicename via kafkaconfg `./gradlew test` passed on my laptop. we need to move fast in order to get this into 0.9.0.0, so i'd appreciate it if you could merge this into your branch so that jun can review it.",0,0.5801668167114258
149315813,191,junrao,2015-10-19T19:09:49Z,": pr #6 from looks good to me. do you want to merge that into your branch? once you do that, i think we can probably just commit the patch as it is and address other issues, if any, in followup jiras.",1,0.7125228047370911
149746290,191,harshach,2015-10-21T00:52:02Z,can you open a pr against this branch so that i can merge it in,0,0.9943174719810486
149750466,191,ijuma,2015-10-21T01:12:57Z,"thanks for merging the pr to your branch harsha. since jun merged the sasl pr via #334 (which is exactly the same as this one now), would you mind closing this one please? thanks for your work on this. we'll probably need a follow-up to tweak some things and some more tests (particularly ducktape ones) but we're most of the way there.",1,0.9867231249809265
781745956,10070,cmccabe,2021-02-19T01:16:41Z,"hmm, wouldn't we want to switch the leader to 2 in that case, since 2 is more preferred?",0,0.96719890832901
782521288,10070,cmccabe,2021-02-20T02:01:21Z,"thanks, ! :)",1,0.9964231848716736
326210780,3765,onurkaraman,2017-08-31T07:13:02Z,note that this pr is wip and only replicastatemachinev2 currently attempts to do retries and error handling. retries and error handling will be added to the rest of the controller in later rounds of review.,0,0.992423415184021
326461993,3765,onurkaraman,2017-09-01T01:20:46Z,can you take a look?,0,0.992240309715271
332270742,3765,onurkaraman,2017-09-26T17:17:49Z,"i rebased, resolved merge conflicts, and force pushed the change.",0,0.9819074273109436
335763940,3765,onurkaraman,2017-10-11T10:15:50Z,"i just ran an experiment that measures controller failover time before and after this pr. the environment: * 5 broker kafka cluster with brokers on different racks * 5 node zookeeper ensemble with nodes on different racks * 100,000 single-partition single-replica topics trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585): [code block] kafka-5642: [code block] trunk average controller failover time: ~60 seconds kafka-5642 average controller failover time: ~24 seconds",0,0.9830641150474548
336032158,3765,onurkaraman,2017-10-12T06:20:36Z,"i compared the breakdowns of where time was getting spent in both the trunk and kafka-5642 runs of the experiment. i don't see any performance regressions in the parts that don't interact with zookeeper. so the zookeeper parts got better with kafka-5642 and the rest stayed the same, which is what we expect.",0,0.9792137742042542
336194567,3765,onurkaraman,2017-10-12T16:40:18Z,"i just realized that controlled shutdowns will not improve at all with the current pr because controlled shutdown still calls the state machines one partition at a time, so there won't be any zk pipelining. we can just do a bunch of filters upfront in controlledshutdown.docontrolledshutdown that categorize partitions into the actions to take and then take the actions on the collections we've gathered. i'll try to update the pr within the next few hours.",0,0.9717742800712585
336318966,3765,onurkaraman,2017-10-13T00:41:12Z,"i just ran an experiment measuring controlled shutdown time before and after this pr. the environment: * 5 broker kafka cluster with brokers on different racks * 5 node zookeeper ensemble with nodes on different racks * 25,000 single-partition topics with rf = 2 the replicas were made with the following python script: [code block] special configs: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controlled shutdown time: ~6.5 minutes kafka-5642 average controlled shutdown time: ~3 seconds",0,0.9831175804138184
336486274,3765,ijuma,2017-10-13T15:29:55Z,can we update the pr description to include a summary of the changes done in this pr?,0,0.995636522769928
336520938,3765,onurkaraman,2017-10-13T17:43:53Z,"today, zookeeperclient and zkclient couples the data/child change handlers with the actual watcher registration, which means handler registrations actually send out a request to the zookeeper ensemble to do the actual watcher registration. in `kafkacontroller.oncontrollerfailover`, we register partition modification handlers (thereby registering watchers) and additionally lookup the partition assignments for every topic in the cluster. in an offline discussion, and i realized that we can shave a bit of time off failover if we merge these two operations. there are two ways we can do this: 1. change zookeeperclient to register data change watchers with getdatarequests instead of existrequests. this means the handler and watcher registration will also return the znode data. 2. decouple handler registration from watcher registration in zookeeperclient. handler registrations and unregistrations would be a purely local operation. watcher registrations would be delegated to the user in a following lookup. option 1 made us realize a bug in the existing pr: we don't check the return codes of watcher registrations and attempt retries on connectionloss. option 2 would just reuse the retry logic already in kafkacontrollerzkutils. i prefer option 2, especially since it makes the zookeeperclient registration/unregistration apis more consistent in that they would both now be purely in-memory operations.",0,0.9674969911575317
336531088,3765,junrao,2017-10-13T18:23:42Z,": yes, i agree that option 2 would be better. it would be useful to add a comment to describe the semantic of zookeeperclient.registerznodechangehandler(). if we do this, we will want to do this consistently with registerznodechangehandler(). also, could you remove wip from the title of the pr?",0,0.9692524671554565
336737463,3765,onurkaraman,2017-10-15T20:03:19Z,"something that was not factored into the above two experiments was logging inefficiencies. one major issue with kafka today is its use of log4j synchronous logging instead of log4j2's asynchronous logging. this heavily distorts the experiment results. if we want to more purely measure the performance change from this patch, i think it'll make sense to rerun the experiments with controller logs set to something like error and above or just disable it entirely.",0,0.9702954888343811
336750646,3765,onurkaraman,2017-10-15T23:41:42Z,"in an offline discussion, suggested i look into determining whether the long controlled shutdown time was actually from the logger writing to disk or simply from creating the log message itself, in which case async logging wouldn't have helped. to test this, i ran the same controlled shutdown experiment as above for trunk but with: `log4j.appender.statechangeappender=org.apache.log4j.varia.nullappender` `log4j.appender.controllerappender=org.apache.log4j.varia.nullappender` nullappender simply doesn't output the message to a device. controlled shutdown still took 6+ minutes. i later reverted the two appenders back to `org.apache.log4j.dailyrollingfileappender` but this time set the controller and state change log levels to fatal so that they effectively never log. controlled shutdown finished in 11 seconds. this proves that the vast majority of time was getting spent in log message creation.",0,0.9887682795524597
336792142,3765,onurkaraman,2017-10-16T06:28:22Z,"alright i reran the controller failover and controlled shutdown experiments but with some logging tweaks to more purely compare trunk's controller using zkclient with kafka-5642's controller using zookeeperclient. the logging tweaks are the following: - the state machine loggers set to error - the state change logger set to debug - the rest of the controller logger set to info the full log4j.properties is here: [code block] **controller failover** the environment: - 5 broker kafka cluster with brokers on different racks - 5 node zookeeper ensemble with nodes on different racks - 100,000 single-partition single-replica topics trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controller failover time: ~63 seconds kafka-5642 average controller failover time: ~17 seconds **controlled shutdown** the environment: - 5 broker kafka cluster with brokers on different racks - 5 node zookeeper ensemble with nodes on different racks - 25,000 single-partition topics with rf = 2 the replicas were made with the following python script: [code block] special configs: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controlled shutdown time: ~22 seconds kafka-5642 average controlled shutdown time: ~3 seconds",0,0.9899773597717285
336801412,3765,onurkaraman,2017-10-16T07:22:17Z,"i went back and looked into why logging resulted in distorted results in the controlled shutdown experiment for trunk: 6.5 minutes with logging vs 22 seconds without logging. it's due to this line in partitionstatemachine.electleaderforpartition: [code block] i reran the experiment with the original log levels (controller and state change loggers at trace, no special state machine loggers) but removed the above debug log statement. this brings the controlled shutdown times back down to below 20 seconds: [code block] the reason why this one log line is so problematic is because for each partition undergoing election, it logs every partition state in the cluster. so you end up getting o(n^2) log behavior where n is the total number of partitions in the cluster. rather than log every partition's state per partition undergoing election, it would be sufficient to just log that single partition's state: [code block]",0,0.9817349314689636
337302275,3765,onurkaraman,2017-10-17T17:15:15Z,"by the way, i reran the controlled shutdown test with the latest changes that adds state change logs for successful state machine transitions and still got around 3 seconds for controlled shutdown: [code block]",0,0.9924877882003784
337373454,3765,junrao,2017-10-17T21:13:18Z,: thanks for the latest patch. lgtm. running all system tests now.,1,0.9768933057785034
337373595,3765,onurkaraman,2017-10-17T21:13:41Z,rebased against trunk and squashed all commits.,0,0.9890866279602051
337513329,3765,onurkaraman,2017-10-18T09:00:25Z,"looks like the system tests passed: [a link] i also went ahead and ran a modified version of the controller failover test with the same overall partition counts, but with more partitions-per-topic and fewer topics. the environment: - 5 broker kafka cluster with brokers on different racks - 5 node zookeeper ensemble with nodes on different racks - 2,000 topics each with 50 partitions and rf = 1 trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 1: [code block] trunk (91517e8fbd7767ba6d7f43b517f5a26b6f870585) run 2: [code block] kafka-5642 run 1: [code block] kafka-5642 run 2: [code block] trunk average controller failover time: ~28 seconds kafka-5642 average controller failover time: ~14 seconds",0,0.9651967287063599
337644430,3765,junrao,2017-10-18T16:13:01Z,: thanks a lot for working on this patiently! lgtm,1,0.996483564376831
341092491,3765,astubbs,2017-11-01T12:26:22Z,"hi , great work! any chance you could give us some insight into the machines you used for the tests? how were they spec'd and the order of magnitude network latency?",1,0.9963856935501099
1428304503,13240,Hangleton,2023-02-13T17:01:57Z,"hello david (), still working on this but opening a draft if you wish to start reviewing at your convenience.",0,0.697869062423706
1428396257,13240,dajac,2023-02-13T17:50:49Z,thanks. i will take a look later this week.,1,0.913498044013977
1429942230,13240,Hangleton,2023-02-14T15:34:01Z,"thanks for the review, david. i am working on adding unit tests for `offsetcommitresponse` and the server-side handling of the request/response, and fix the bugs you have identified.",1,0.960873007774353
1437297790,13240,Hangleton,2023-02-20T16:42:13Z,"hello david, i updated the pr to take into account your comments and have been adding tests.",0,0.8937339186668396
1442380719,13240,Hangleton,2023-02-23T20:21:29Z,"many thanks david for the review. i updated the pr to cover all the points you raised. i still have more unit tests to add for the broker-side code paths, need to check if any further change/test is required on the admin client side. will update the pr with those. thanks!",1,0.9935755133628845
1443590758,13240,Hangleton,2023-02-24T12:03:59Z,- added a test for `invalid_request` in case of definition of topic id and name for a topic in the `offsetcommit` request. - added a test exercising a valid `offsetcommit` request across all schema version. we can see that the topic id and name is propagated to the group coordinator in all cases.,0,0.9883692264556885
1446539881,13240,dajac,2023-02-27T15:28:10Z,"thanks for the update. i will take a look shortly. in the meantime, could you already add a few integration test for the offsetcommitrequest like we have for the offsetfetchrequest in kafka.server.offsetfetchrequesttest? i believe that topic ids are actually lost when they are passed to the group coordinator. hence, i request with topic ids will very likely return a response with topic names instead of topic ids. the issue is that we don't catch those kind of issues with the existing tests because the logic in the consumer supports both ways.",1,0.9447174072265625
1447864999,13240,Hangleton,2023-02-28T09:41:19Z,thanks for the review david. updating the pr to enforce the exclusive use of topic ids from version 9 and adding the integration test you mentioned. thanks for the guidance!,1,0.989953875541687
1449920535,13240,Hangleton,2023-03-01T11:16:38Z,"hello david, thanks for the fast review. apologies for being slow, i hadn't finished the previous revision. will include your comments. working on it right now. thanks!",1,0.9950633645057678
1449935049,13240,dajac,2023-03-01T11:23:38Z,no worries. you are not slow. i noticed a few new commits so i had a quick look at them.,1,0.9767133593559265
1449936916,13240,dajac,2023-03-01T11:24:33Z,"sorry, used the wrong button...",-1,0.991915225982666
1452495333,13240,Hangleton,2023-03-02T20:21:17Z,"thanks david for the review, have a few more tests to add but this should be eligible to another pass.",1,0.9349654316902161
1453452130,13240,Hangleton,2023-03-03T12:20:21Z,the integration test `mirrorconnectorsintegrationbasetest` is failing due to unknown topic id detected when altering committed offsets with the admin client. this is because the `offsetcommitrequest` generated by the admin client is using version 9 but should be 8 as discussed. this should have been captured by the unit tests on the admin client. this exhibits a gap in the test coverage of this pr.,0,0.9791406989097595
1453619279,13240,Hangleton,2023-03-03T14:32:23Z,"used v8 for alterconsumergroupoffsets in the admin client and added corresponding unit and integration tests. `mirrorconnectorsintegrationbasetest` is now successful. also just realized i need to update the `authorizerintegrationtest`. the reason is the tests are committing offsets from a consumer without having the topic subscribed to, and without the topic id in the client metadata cache.",0,0.9465871453285217
1454676488,13240,Hangleton,2023-03-04T09:27:52Z,"hello david, found a case where the use of offsetcommit requests version 9 from the consumer resulted in an error in the acl authorization tests. the reason is that when the commit offsets are performed by the tests to exercise acls, the topic ids are not yet present in the consumer metadata cache, in this case because the consumer hasn't subscribed to the topics or be assigned any of their partitions. as a result, the offsetcommitrequest version 9 was sent with zero topic ids. in order to avoid this, we could: 1. enforce a metadata update for any topic in the offsets to commit, using the transient topics list exposed by the `consumermetadata` and used for instance by the `offsetfetcher` to fetch offsets for out-of-band topic-partitions which aren't part of any subscription. this approach however adds complexity to the offset commit implementation in the consumer and only address this specific use case. there could be other cases where metadata hasn't converged yet and the topic id would not be available to the consumer. 2. adds a condition on the support of topic ids when constructing the `offsetcommitrequest`. this is the approach used when constructing the `fetch` request. the advantage of the approach is that it provides the invariant that any offsetcommitrequest version >= 9 have valid (non-zero) topic ids in it. one downside is that if a bug in the consumer makes a topic id unavailable, the consumer will keep using version <= 8 permanently and silently, while we would want to know about it as implementors to address any potential gap. the second approach is currently implemented in the pr. happy to discuss more about it.",0,0.9679002165794373
1454803355,13240,dajac,2023-03-04T16:57:01Z,i think that we have to do 2) anyway because topic ids may not be enabled on the server side.,0,0.9811576008796692
1455667030,13240,Hangleton,2023-03-06T08:09:12Z,took a look at the authorizer tests (`authorizerintegrationtests`). it seems that authorization with topic and group `read` permissions and unknown topic name is not currently being tested. we could add this use case and extend it for topic ids in a separate pr? (ref: [a link],0,0.992077648639679
1459774076,13240,Hangleton,2023-03-08T08:53:47Z,"many thanks, david, for the review. working on fixing the pr now. i will send the corrective commits by eod. thanks.",1,0.9851391911506653
1460209520,13240,Hangleton,2023-03-08T14:07:47Z,"hi david, thanks for the review. i addressed all your comments and updated the pr. ~i am just adding a test to add coverage on addition of topic ids in the response generated by the coordinator.~ _edit: added said test._",1,0.9816053509712219
1469825143,13240,dajac,2023-03-15T11:21:23Z,i just merge [a link] we can update this pr now.,0,0.9956763386726379
1469956438,13240,Hangleton,2023-03-15T12:54:44Z,"many thanks david. i will try to get to this in the next couple of days. apologies for the delay, i wish i could get to this sooner.",1,0.9339079856872559
1485037330,13240,Hangleton,2023-03-27T12:12:56Z,"hello david (), this pr has been updated and is ready for review. thanks!",1,0.9908202290534973
1505120343,13240,dajac,2023-04-12T11:38:40Z,there are a few conflicts. could you please rebase the pr? i plan to make another pass on it afterwards.,0,0.978766679763794
1505154651,13240,Hangleton,2023-04-12T12:06:35Z,"sure, done. thanks!",1,0.9897446036338806
1511110416,13240,Hangleton,2023-04-17T10:44:05Z,"hi david, thanks for the review. understand about refactoring, i will try to see if i can revert some of them if possible.",1,0.9813665151596069
1553270438,13240,clolov,2023-05-18T15:57:56Z,"heya ! i hope i have addressed your comments on all files except `consumercoordinatortest` and the `offsetcommitrequesttest`, could you review everything except those two and confirm whether this is the case? as far as i understand your general concerns with `consumercoordinatortest` and `offsetcommitrequesttest` is that those tests are either not parameterised in a simple way or they are not parameterised at all - am i correct? for the ones which are not parameterised simply i cannot think of an easier approach - the setup is just long-winded, but the main idea behind the arguments is [a link] i guess we can split them into separate tests were we vary just one of the arguments rather than all of them if that's what you mean? the ones which are not parameterised at all i believe are not parameterised because the same functions are used for tests which only test behaviour in versions >= 9. i am happy to implement any suggestions you might have to improve on what's already there.",1,0.9504090547561646
1553290938,13240,Hangleton,2023-05-18T16:13:11Z,"thanks christo () for your help on the pr, i will take a look at the changes tomorrow. thanks!",1,0.9936042428016663
1562458362,13240,Hangleton,2023-05-25T08:00:45Z,"hi david (), thanks for the review and apologies for the delayed reply. thanks to christo's help, i believe most of your comments have been addressed. i have one question regarding the behaviour of the offset commit consumer api that you identified [a link]. thanks!",1,0.9938855767250061
1571989946,13240,Hangleton,2023-06-01T12:47:18Z,"hello david (), i was discussing this with christo today as part of his work on the offsetfetch api. would you like this pr on offsetcommit to be split to make the review easier and reduce risks?",0,0.9403562545776367
1741511578,13240,dajac,2023-09-29T21:32:07Z,closing this pr for now as the topic id work will be done later. we can re-open it when we resume the work.,0,0.991280734539032
260852086,2140,hachikuji,2016-11-16T04:36:09Z,"ping this patch modifies the server implementation to use the client-side `record` objects for all internal processing. as you can see, this was a hefty bit of work, but fortunately most of the transformations are straightforward. the main thing to focus on is the implementation of `logvalidator`, which contains the offset assignment and record validation logic that was previously contained in `bytebuffermessageset`. i've been pretty careful to preserve the optimizations that were present previously (e.g. in-place assignment where possible), but don't take my word for it. one quick note on naming. i've renamed the `records` object and subclasses to `logbuffer`. so `memoryrecords` is now `memorylogbuffer`. the reason for this change was that it felt unintuitive for an instance of `records` to be an `iterable `, with the `logentry` instances being the actual container for the records. a `logbuffer` instead represents a range of the log and provides access to the log entries contained in it. that seemed more intuitive to me, but let me know if you agree or if you have other suggestions.",0,0.9347435235977173
260854848,2140,onurkaraman,2016-11-16T05:02:37Z,"slightly related, slightly tangential: is there a specific reason why we put the new broker-specific java classes under clients/ ? i'm talking about stuff like: filerecords leaderandisrrequest / leaderandisrresponse stopreplicarequest / stopreplicaresponse updatemetadatarequest / updatemetadataresponse",0,0.975955069065094
260856355,2140,hachikuji,2016-11-16T05:15:56Z,"yeah, i've wondered a bit about that also. i'd be ok moving `filerecords` to the server if people prefer that. i was thinking one potential benefit is that it opens the door to adding persistence to the client, which some users have requested (we have an ancient jira for this, but the use case might not be too compelling). in the end, i decided it wasn't that much code, so having it in clients didn't hurt too much and it kept all record-related stuff close together, which may make it easier to share common pieces.",0,0.916954517364502
260979285,2140,ijuma,2016-11-16T15:43:02Z,", thanks for tackling this. about the naming question, i also found it a bit confusing how we sometimes have an offset and sometimes don't when talking about records. that is, `memoryrecords` includes the offset (and record size in the underlying buffer) for each record while `record` does not. it all becomes clearer when one realises that `memoryrecords` (renamed to `memorylogbuffer` in the pr) actually contains `logentry` instances, each being a pair of `offset` and `record`. one thing to think about is whether this fits with the other `record` classes we have and whether that matters (maybe it doesn't). for example, `consumerrecord` contains the `offset` while `producerrecord` does not. also, it would have been a bit easier to review if the rename had been done in a separate pr, but probably too late for that. :) about having the classes in `clients`, i think that's ok as they are in an internal `common` package.",1,0.9675544500350952
261073297,2140,hachikuji,2016-11-16T21:11:59Z,would be nice to get your feedback also. put on the coffee and lose yourself in code review!,1,0.9729846119880676
261693933,2140,becketqin,2016-11-19T05:01:19Z,"wow, a 5000 line change... i'll take a look this weekend...",0,0.6666823625564575
261860811,2140,becketqin,2016-11-21T07:13:06Z,"i ran into some other stuff today and didn't finish reading the entire patch. just some thoughts when i was reading the code. i think ""message"" (and ""record"" in the new clients) are a well established concept for kafka. it is indeed a little weird that `records` is an `iterable `, but i felt changing all the `reocrds` to `logbuffer` seems introducing a new concept (btw `filelogbuffer` sounds a little weird given it actually does not have a buffer). i would like to see if we can avoid solving the confusion by adding a new concept. not sure if there was any thinking on changing `logentry` to something like `logrecord` which indicate it's something resides in the log? then `records` would contain a few `logrecord` which contains (`offset` + `record`). we also have a pretty symmetric naming for `producerrecord`, `consumerrecord` and `logrecord` clearly indicating where they are used. we can also consider renaming `records` to `logrecords` to make it clear. i am not sure if `records` counts as a public interface or not, though. i know we have a page stating which packages are public and which are not, but i doubt if people follows that given we have an explicit `internals` package... i'll save my other comments until i go through a full pass of the code in case some of them are not valid at all (i already found some...)",0,0.6085002422332764
261887880,2140,ijuma,2016-11-21T09:32:06Z,", that's a fair point about the rename and introducing a new concept. i have similar concerns and was wondering how we could make things clearer without that. your suggestion looks promising.",1,0.9614731669425964
262029927,2140,onurkaraman,2016-11-21T18:48:03Z,does it make sense to separate the renaming from the actual task of this patch?,0,0.9912315011024475
262033620,2140,hachikuji,2016-11-21T19:01:08Z,"thanks for taking a look. i'm not sure i follow why you consider the renaming a conceptual change. the object works the same as before, but i felt the name fit closer to what the object actually represents, which is a range of bytes from the log. the name `records` to me just suggests a container for `record` objects. the suggestion about `logrecord` makes sense to me. i have actually done something similar in work building off of this patch. at the same time, i would like to preserve a concept of `logentry` as a container for records which sits between `logbuffer` (or `records`) and `logrecord` (or `record`). the basic idea is to treat the shallow messages as log entries, and the deep messages as log records (an uncompressed message is treated as a log entry containing just a single log record). to give a bit more background, we're trying to generalize the concept of a message set so that 1) it uses a separate schema from individual messages, and 2) it's extended to uncompressed data. this allows us to amortize the cost of additional metadata which is invariant for the messages contained in a message set. i'm happy to provide some additional detail if you're interested (there will be a kip on the way some time in the next few weeks). yeah, we can do that if it makes this patch easier to get in. let's see what others think. sigh, any suggestion to reduce lines of code is likely to be popular to all except me.",1,0.9569987654685974
262306541,2140,hachikuji,2016-11-22T17:25:40Z,"i ran system tests on the latest patch and everything looks good: [a link] i will probably continue to add some additional test cases, but i'll leave the rest as is pending further review comments.",1,0.8431157469749451
262421264,2140,junrao,2016-11-23T02:21:40Z,: will also take a look at the patch. just a quick note. could you do some performance test to make sure there is no regression?,0,0.9709427952766418
262422183,2140,hachikuji,2016-11-23T02:30:55Z,thanks for taking a look. performance testing is next on my plate after i fill in some of the gaps in test coverage.,1,0.9679991602897644
263670626,2140,hachikuji,2016-11-29T19:22:00Z,update: i've begun performance testing. i'm seeing a substantial performance degradation on the consumer side. i'll update this pr when i know more.,0,0.9465256333351135
263687962,2140,hachikuji,2016-11-29T20:24:27Z,"i found the cause of the performance regression. when handling a fetch, we must read through the log to find the starting position of a given offset (starting from the position given by the index). to do so, we only need to read the offset and size, but one of my recent commits accidentally changed this behavior to unnecessarily read the full record. i've fixed this in the last commit and now it looks like performance is within 5% of trunk for the producer and consumer. perhaps still on the slower side though, so i'll continue investigating.",0,0.8600564002990723
264081910,2140,hachikuji,2016-12-01T05:24:09Z,"i really appreciate the thorough review. i've addressed the easier items and left a few replies. i'll get to the rest tomorrow. by the way, in the near future, i'd like to squash commits to make rebasing a bit easier. it hasn't been too much of a problem yet, but it will get harder with more iterations.",1,0.9898862838745117
264770009,2140,guozhangwang,2016-12-05T05:17:02Z,"about the naming of `records` to `logbuffer`, i share the same concern with and . my proposal would be to rename `logentry` to `recordentry` or simply `recordandoffset` (seems more scala-ish), and to me it is ok to have `records.iterator()` return `iterator `. as for `consumerrecord` and `producerrecord`, it would be best if them both contain a `record` field, and then `consumerrecord` to contain a separate `offset` field, but since it is public apis we have to leave it as is, which is not too bad to me. also, i'd like to suggest we separate the renaming out of this pr for the ease of reviewing, if it is still possible to revert it back.",0,0.9824556112289429
264923518,2140,hachikuji,2016-12-05T17:46:28Z,"`recordentry` would work for me. keep in mind that if kip-98 is approved, it will do more than just track the offset, so i'd rather not use something specific like `recordandoffset`. since my `logbuffer` suggestion is not too popular, i'll go ahead and revert that change. then the hierarchy will be `records` -> `recordentry` -> `record`. does that sound reasonable?",0,0.9856778979301453
264983825,2140,hachikuji,2016-12-05T21:32:29Z,i've gone ahead and squashed commits. you can still find the old commit history here: [a link],0,0.993929386138916
265818436,2140,hachikuji,2016-12-08T18:38:55Z,fyi: here's a system test run from last night: [a link] there was one failure. i'll investigate and report back what i find.,0,0.9871565699577332
266538764,2140,hachikuji,2016-12-12T20:10:32Z,"latest round of comments addressed. please take a look. on the question of `iterator` vs `iterable`, i'm pretty open. i used the former for consistency with the old code, but i agree it would be nice to be able to use the ""foreach"" syntax.",1,0.8038420677185059
266684166,2140,hachikuji,2016-12-13T09:06:53Z,"thanks for the reviews. i did some testing this evening. i thought i was seeing some performance difference initially in the producer, but it seems within the variance of the test runs. if i were guessing from the results, i'd say the non-compressed path is a tad slower while the compressed path might be a tad faster, but don't put much weight behind either conclusion. in any case, the results seem close enough that i'd recommend merging now. note that i did add one commit to address a couple minor cleanups and tighten up the iteration code a little.",1,0.9609140753746033
384162821,4830,lindong28,2018-04-25T05:13:22Z,the patch lgtm. would you have time to review this patch? thanks much!,1,0.9871282577514648
384163802,4830,jonlee2,2018-04-25T05:20:30Z,"thank you very much for the detailed review, . i talked to both and and they are not available for review at the moment. i've asked if she can review.",1,0.9886009097099304
386722748,4830,jonlee2,2018-05-04T20:22:38Z,"the lastest commit includes the following fixes: 1. bump up all request and response protocol versions 2. add a method called shouldclientthrottling() to abstractresponse to tell the client whether or not it should throttle based on the received response version. 3. fix networkclient to do throttling based on the response version using the new method. 4. add the throttle_time_ms field to the following cluster actions that may be throttled: leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse, saslhandshakeresponse 5. update kafkaapis to pass throttletimems to above responses , can you take a look? thank you.",0,0.9849646687507629
386752308,4830,rajinisivaram,2018-05-04T22:37:05Z,"hi jon, thanks for the updates. i will take a look at the pr early next week. we were deliberately avoiding throttling leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse and saslhandshakeresponse. and i don't think we discussed throttling these as part of the kip. can you explain why the change was required? if we are changing request format, it ought to be added to the discuss thread and the kip. on fri, may 4, 2018 at 9:23 pm, jon lee wrote:",1,0.9811304211616516
386759605,4830,jonlee2,2018-05-04T23:29:10Z,"hi rajini, thanks for the comment. the reason why i added throttle_time_ms to leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse and saslhandshakeresponse was that these responses are actually throttled on errors (for request quotas). and when they are throttled, i want throttle time to be passed back so that client can refrain from sending more requests to the broker until the throttle time is over. for non-error cases, they won't be throttled, so there are no behavior changes there (except that the response will have zero throttle time). i'll mention this change to the discussion thread. meanwhile, please go ahead and review the changes. the response format changes are well contained and shouldn't block you from reviewing other changes. if we decide to take it back, it shouldn't be much work. thanks, jon ________________________________ from: rajini sivaram sent: friday, may 4, 2018 3:37:25 pm to: apache/kafka cc: jon lee; mention subject: re: [apache/kafka] kafka-6028: improve the quota throttle communication (kip-219) (#4830) hi jon, thanks for the updates. i will take a look at the pr early next week. we were deliberately avoiding throttling leaderandisrresponse, updatemetadataresponse, saslauthenticateresponse and saslhandshakeresponse. and i don't think we discussed throttling these as part of the kip. can you explain why the change was required? if we are changing request format, it ought to be added to the discuss thread and the kip. on fri, may 4, 2018 at 9:23 pm, jon lee wrote:  you are receiving this because you were mentioned. reply to this email directly, view it on github , or mute the thread .",1,0.9549439549446106
389022296,4830,jonlee2,2018-05-15T02:24:44Z,"thanks for the review, . i added some questions to your first comment. please let me know what you think.",1,0.984508752822876
389401521,4830,jonlee2,2018-05-16T05:45:31Z,"btw, let me know if you want me to squash multiple commits into one. i didn't do that to keep the entire history, but i think i'll need to do that anyway at some point before submission.",0,0.9802128672599792
389444225,4830,rajinisivaram,2018-05-16T08:48:44Z,"thanks for the updates. it is looking good. you don't need to squash the commits, they get squashed when the pr is merged. so the pr can keep the commits to maintain history. since it is a large pr, it will be good to do one full review before committing it. i was expecting that you would do the final review and commit this pr. let me know if that is not the case. thanks.",1,0.9925498962402344
389447800,4830,lindong28,2018-05-16T09:00:48Z,"thanks for the review ! yes, i will do one full review before committing this.",1,0.9867309331893921
390454844,4830,lindong28,2018-05-20T03:25:06Z,is there a public jenkins service that we can use to run system tests for this patch?,0,0.9947332143783569
390694484,4830,jonlee2,2018-05-21T15:46:23Z,"thanks for running the tests. for some reason, however, i cannot access the link. i got an 404 error, saying ""problem accessing /job/system-test-kafka-branch-builder/1754. reason: not found"". do i need a permission or anything else to access it? also, what are these system tests and where can i find them? i may need to change these tests as well since the throttling behavior has now changed.",1,0.894942581653595
390722724,4830,lindong28,2018-05-21T17:22:54Z,that is not a public url. here are the failed tests: [code block],0,0.9825594425201416
390754366,4830,lindong28,2018-05-21T19:17:22Z,thanks much for helping running the test!,1,0.8584735989570618
390902167,4830,jonlee2,2018-05-22T08:12:39Z,"i found a bug in selector.java, which caused the test failures. for socketserver, selector.unmute() unmutes the channel only if the mute state is muted. however, it removes the channel from explicitlymutedchannels regardless of the outcome of unmute. this allows a new client request to be received by socketserver when throttling for the previous request is still in progress and thus breaks the state machine. i ran the system tests locally after fixing this and all previously failed quota tests succeeded. this was not really caught by the existing quota integration tests because they produce/consume till throttled and stop there. i also noticed that passing invalid_session_id in an empty fetch response (when throttled) will close the incremental fetch session. i added a special session id to keep the session open. please review and also start another round of system tests. thank you.",0,0.9570713639259338
393244326,4830,lindong28,2018-05-30T17:17:03Z,this pr has been merged in the trunk with [a link],0,0.9962463974952698
670693346,9103,cmccabe,2020-08-07T20:10:07Z,"i think it would be good to split this pr up a little bit. it seems like we could have a split like this: pr 1. `add flag to the requestcontext` and `add initial principal name` pr 2. authorization changes for alterconfigs / incrementalalterconfigs, forwarding if required, ibp check, bump rpc versions of alterconfigs / incrementalalterconfigs pr 3. adminclient changes in behavior based on versions of alterconfigs / incrementalalterconfigs, alterconfigsutil, etc.",0,0.9055256843566895
696225206,9103,cmccabe,2020-09-21T16:25:39Z,"hi , thanks for the pr! it looks good. i like the idea behind `forwardrequesthandler`. since this class doesn't have any internal state, i wonder if it would be more scala-like to just have a function which just takes some callbacks as arguments. can we get rid of the need for the `customizedauthorizationerror` callback by having `resourcesplitbyauthorization` return a `map[rk, apierror]` instead of `map[rk, rv]`? when would rv be needed for keys where authorization failed?",1,0.9957972764968872
696281629,9103,abbccdda,2020-09-21T18:11:18Z,sounds good to me to remove the customized error helper.,0,0.5531272292137146
713244123,9103,abbccdda,2020-10-21T02:03:46Z,test this please,0,0.9870431423187256
719681493,9103,hachikuji,2020-10-30T17:12:47Z,"one more thing to verify. when the metadata quorum feature flag is not defined, i think we should not expose the envelope api in the `apiversion` request.",0,0.9925582408905029
722005317,9103,abbccdda,2020-11-04T22:17:54Z,"mvn failure is not related, merging",0,0.9804398417472839
133280679,132,allenxwang,2015-08-21T04:31:38Z,the failed test in the last pr build passed on my laptop locally. it seems to be flaky as it also failed on an earlier build without my changes.,0,0.9235954880714417
170421487,132,joestein,2016-01-11T03:00:15Z,"one overall general comment on the implementation is that the brokers properties themselves could cary this information making it so the topic creator doesn't have to know this. the fact is that still a lot of humans run the topic command but in many cases it is some software system operationally doing it. in either case if the broker had a property rack=0 or whatever it could then just be the way you have the topic distribute that information it should already be able to gather. granted, this implementation saves a lot of having to store it in zookeeper so rationally speaking this is better than putting any code in to that. sorry if i missed the entire discussion thread on this just seeing it for first time. i like it, would love to see this get into trunk and start to be used and also in the next release. nice work so far!",-1,0.9234356880187988
170429303,132,allenxwang,2016-01-11T04:38:16Z,"please see kip-36 for the latest proposal. ([a link] the biggest difference is that the rack information is added as a broker meta data in zookeeper. consequently the inter-broker (updatemetadatarequest) and client to broker meta data query protocol (topicmetadataresponse) will be changed to have rack information. once the kip is accepted, i will update this pr to incorporate these new ideas.",0,0.981685221195221
173002403,132,sslavic,2016-01-19T22:02:42Z,"wouldn't it be nice to have metric per topic partition, in how many different racks do isrs live?",0,0.9797862768173218
190237447,132,ijuma,2016-02-29T14:42:34Z,", can you please fix the merge conflicts?",0,0.9921332597732544
190387743,132,allenxwang,2016-02-29T21:00:01Z,done with resolving conflicts. would appreciate if this can be reviewed and merged with a higher priority as it touches multiple areas and easy to get conflicts if sitting there for a bit longer.,0,0.6778489351272583
190404703,132,hachikuji,2016-02-29T21:28:46Z,"i was looking for the changes to topicmetadatarequest, but didn't see them. were you planning to do them here?",0,0.9899333119392395
190409503,132,granthenke,2016-02-29T21:45:16Z,i need to make changes to topicmetadata response too for kip-4. we also need to migrate to the new topicmetadata request/responses on the server side [a link]. we should probably migrate before making changes. otherwise we need to make the changes in both the java and scala classes.,0,0.9908884763717651
190410867,132,hachikuji,2016-02-29T21:47:43Z,makes sense to me. i can pick up 2073 if you haven't already started.,0,0.9792054295539856
190413309,132,granthenke,2016-02-29T21:52:23Z,feel free to pick it up. i have some intermediate changes i can share with you later tonight. the hardest part about migrating was metadatacache. you more or less need to re-implement it because its tied very heavily to the old requests/responses. after that i had some test failures that i haven't had a chance to dig into. i was thinking adding some additional tests may help flush out the issues more clearly too.,1,0.6723057627677917
190415650,132,allenxwang,2016-02-29T21:57:09Z,i am not going to include changes to topicmetadatarequest/response in this pr which was indicated in the kip discussion thread. you are free to do it in later on after this is merged.,0,0.9896155595779419
190418313,132,hachikuji,2016-02-29T22:06:49Z,have you opened another jira for the topic metadata changes? we should probably make sure we don't lose track of them.,0,0.9914162755012512
190422237,132,granthenke,2016-02-29T22:13:44Z,i can include them in my changes to the response for kip-4. i will open a jira to track that. it would be nice to make all the changes at once anyway. i will ask for your feedback when posting to proposed changes.,0,0.8704010248184204
190431869,132,granthenke,2016-02-29T22:39:59Z,i have created [a link] to track updating the metadata response. i should have a preliminary patch this week.,0,0.9908300638198853
190547078,132,allenxwang,2016-03-01T05:00:06Z,flaky test? passed on my laptop and on jenkins before the last commit. test result (2 failures / +1) kafka.api.sslconsumertest.testautocommitonrebalance kafka.network.socketservertest.testsocketscloseonshutdown,0,0.8266588449478149
190795970,132,granthenke,2016-03-01T16:28:19Z,thanks for the work on this. i did a quick review. will have to come back for a more in depth look.,1,0.981205940246582
191917048,132,junrao,2016-03-03T19:05:10Z,"thanks for the patch. now that we are changing the broker registration in zk, could we update the upgrade doc to warn users that they need to upgrade from 0.9.0.1 (instead of 0.9.0.0) to 0.10.0? otherwise, the old scala consumer in 0.9.0.0 will break once the broker is upgraded to 0.10.0.",1,0.9604799151420593
192326728,132,ijuma,2016-03-04T15:39:33Z,"thanks for the pr . i did an initial pass and left a few comments/questions and i submitted a pr with some fixes and improvements: [a link] please review and integrate in any way you please (merge, cherry-pick, manually, etc.). i will take a second and final pass by the end of monday.",1,0.9653459191322327
193290563,132,ijuma,2016-03-07T15:10:20Z,", thanks for addressing the review comments and merging my pr. as promised, i completed my review today and i left one comment and created another pr with more code style improvements: [a link] i think we're pretty close (but we'll be sure once jun takes another look). as far as i can see, outside of the pr i filed, the following is potentially outstanding: - [ ] a couple of deprecated constructors in `updatemetadatarequest.java` are only used in tests. i'd like to to confirm that we actually need them. - [x] i think we need to tweak the adminutils.assignreplicastobrokers comment a little to integrate it better with what we had there previously. - [ ] jun asked if some tests are actually needed or if they can be simplified. explained why they are needed, so if jun is happy with the explanation, then this is resolved. - [x] finally, the only non-minor change (although mechanical) is whether we should use a `brokermetadata` class instead of broker ids and a separate `rackinfo` map (suggested by jun). allen wasn't sure if this was better so it's still being discussed.",1,0.9743825197219849
193465023,132,ijuma,2016-03-07T21:36:34Z,a couple more things that we need: - [x] update `upgrade.html` to mention that old clients need to upgrade to 0.9.0.1 before the brokers are upgraded to 0.10.0.x. - [x] system tests must pass.,0,0.9947932362556458
193921244,132,allenxwang,2016-03-08T19:10:52Z,can you please help look into test failures? it seems that the failures occur after merging your pull request #2.,0,0.9932534098625183
193945557,132,ijuma,2016-03-08T20:04:33Z,investigating now.,0,0.9790982007980347
193978242,132,ijuma,2016-03-08T21:34:52Z,"sorry about that, the following should fix it: [a link] . looks like i managed to push code to my pr that was slightly different to what i had tested.",-1,0.9869415163993835
194106914,132,junrao,2016-03-09T04:14:34Z,": could you also run the system tests with your patch? if needed, can help you run your branch in confluent's jenkins.",0,0.9928566217422485
194129518,132,ijuma,2016-03-09T06:11:49Z,"i actually started a system tests build yesterday: [a link] there are 7 failures, but some are unrelated. i am going to analyse it in more detail today.",0,0.9692363739013672
194365328,132,ijuma,2016-03-09T16:05:52Z,"with regards to system failures, they are: [code block] i think these were all (transiently) failing in trunk based on the last time trunk was merged to this branch. , can you please merge from trunk again? the streams tests have since been disabled and `test_upgrade` has been fixed. it's particularly important that we know that it passes even after the changes in this branch. i will re-run the system tests once you do this.",0,0.9842895865440369
194432464,132,allenxwang,2016-03-09T18:14:50Z,merged with trunk and run systemtest task on my laptop. no test failures.,0,0.9875079989433289
194576557,132,ijuma,2016-03-10T00:02:29Z,"thanks , i started another system tests run: [a link] are you sure you ran the system tests? setting them up is a bit involved: [a link]",1,0.9234657883644104
194595633,132,allenxwang,2016-03-10T00:55:30Z,i got it wrong. all i did is `gradlew systemtest`.,-1,0.6547091007232666
194641381,132,ijuma,2016-03-10T03:17:42Z,"to settle the `brokermetadata` versus `brokerlist` and `rackinfo` question, i made the change in the following commit so people can check how it could look: [a link] let me know what you think. is it better, similar or worse?",0,0.9677984714508057
194766656,132,ijuma,2016-03-10T10:02:42Z,"there were 4 failures in the last system tests run, none of them seem to be specific to this branch: [code block]",0,0.991266667842865
194975802,132,allenxwang,2016-03-10T17:52:49Z,your approach looks good to me. it has the advantage that the data structure is clear and allows future addition if more metadata is required to do replica assignment. a further variant would be making `rackawaremode` an argument to `adminutils.assignreplicatobrokers`. but i like the way it is now because it keeps `adminutils.assignreplicatobrokers` simple and free of user input and context.,1,0.9506558179855347
195082363,132,ijuma,2016-03-10T22:38:02Z,"ok, great. i cleaned up the commit a little and created a pr: [a link] i will let you decide whether you want to merge it or if you would prefer to wait for 's opinion.",0,0.5329395532608032
195110017,132,allenxwang,2016-03-11T00:10:32Z,i will wait for 's comments on this.,0,0.952373206615448
195245185,132,ijuma,2016-03-11T08:05:08Z,i ran the system tests on a branch with [a link] and master merged into it and there are no new failures when compared to trunk: [a link],0,0.9908798933029175
195570860,132,junrao,2016-03-11T22:03:13Z,": yes, i agree that using brokermetadata in 's patch is better.",0,0.9772676825523376
195573879,132,ijuma,2016-03-11T22:15:45Z,"also, some conflicts need to be fixed with trunk. , let me know if you'd like me to merge master into [a link] in order to fix the conflicts.",0,0.9935837388038635
195574433,132,allenxwang,2016-03-11T22:17:59Z,"yes, please merge with the trunk in your pull request. thanks.",1,0.7945923209190369
195596529,132,ijuma,2016-03-11T23:06:07Z,merged master into [a link] fixed conflicts and verified that tests pass.,0,0.9914404153823853
195605604,132,ijuma,2016-03-11T23:48:05Z,"allen merged my pr, so this is ready for review.",0,0.9932492971420288
195644348,132,junrao,2016-03-12T02:37:48Z,": for your previous question on the json version for zk registration, my preference is still to do the version change now. this way, our hands are not tied for potential future changes and it's also easier to document this. as for compatibility, most people will probably be on 0.9.0.1 before they upgrade to 0.10.0. so the impact should be limited.",0,0.9124611020088196
196364260,132,granthenke,2016-03-14T15:18:07Z,thanks for the confirmation. i understand json version will change.,1,0.9541590809822083
196844027,132,junrao,2016-03-15T14:24:37Z,thanks for the patch. lgtm. could you rebase?,1,0.9293829798698425
196845140,132,ijuma,2016-03-15T14:28:14Z,", the following merges master into your branch: [a link]",0,0.9940483570098877
424496003,5693,vvcephei,2018-09-25T20:47:48Z,"when you get a chance, please review this code. i have done my best locally to produce a nice, clean implementation, but now that the diff is published, i'll make another pass over it looking for sharp edges.",0,0.7633945345878601
424781723,5693,vvcephei,2018-09-26T16:26:11Z,"i've partially addressed the comments so far. notably, i've dropped the punctuator and now handle both time and size constraints during `process`.",0,0.9900847673416138
424843948,5693,vvcephei,2018-09-26T19:40:20Z,"hi , , and , i believe i've addressed all the comments thus far, with the exception of whether to store the values serialized. i did notice a low-effort optimization in the current implementation to skip serializing if the buffer isn't size-constrained, which would carry over even when we add the changelog, if the buffer delays serializing until flush, so for high-turnover buffers, many records may never be serialized at all. but that's all a little beside the point... i'm still happy to change the whole implementation to store serialized data if that's the reviewers' preference.",1,0.9197161793708801
425098149,5693,bbejeck,2018-09-27T13:44:53Z,test failure unrelated retest this please,0,0.5868735313415527
425156616,5693,vvcephei,2018-09-27T16:25:32Z,"hey all, fyi, i've just updated this pr to store serialized data instead. it was a bit more work than i anticipated because i ran into some snags related to preserving the window end information for timewindows (and also discovered i was using the wrong serde for session windows). these issues are both fixed now.",1,0.7133944034576416
425168281,5693,vvcephei,2018-09-27T17:02:30Z,"ok, , i have added some unit tests that verify that the processor throws the exception, and also some integration tests that verifies that streams shuts down for the same conditions. thanks for that catch.",1,0.9744318723678589
425171800,5693,vvcephei,2018-09-27T17:13:51Z,"also, i just had a better idea for maintaining the mintimestamp value that cleans up the implementation quite a bit.",0,0.991303563117981
425724425,5693,vvcephei,2018-09-30T14:25:23Z,"just to document the current state... i think the latest set of commits should resolve all the open comments, so this pr should be ready to merge, pending a rebase once #5521 is merged (or we could merge this first, if #5521 needs more reviews).",0,0.9885488152503967
426046990,5693,vvcephei,2018-10-01T20:14:55Z,java 8 failure unrelated: [code block],0,0.9586083292961121
426059684,5693,vvcephei,2018-10-01T20:55:22Z,"/ fyi, the tests have passed.",0,0.9772896766662598
426086754,5693,bbejeck,2018-10-01T22:34:41Z,retest this please,0,0.980658769607544
426103858,5693,vvcephei,2018-10-02T00:08:51Z,i had an unused import. the tests pass for me locally now.,0,0.9941115975379944
435276167,5821,hzxa21,2018-11-02T05:34:56Z,can you help to review this pr when you have time? thanks!,1,0.9677262306213379
438036817,5821,lindong28,2018-11-12T21:36:29Z,"lgtm. thanks for the update. could you rebase the patch? hey , would you like to review the patch or it is ok for me to merge it?",1,0.9780253171920776
438039462,5821,junrao,2018-11-12T21:46:12Z,", : yes, i will make another pass in the next couple of days.",0,0.8445127606391907
442712153,5821,hzxa21,2018-11-29T05:33:35Z,i have updated the pr to address all the comments. could you take a second look?,0,0.9947673082351685
443155235,5821,hzxa21,2018-11-30T10:08:28Z,thanks for the prompt reply. i have updated the pr to address your comments and rebase.,1,0.9526363611221313
443357329,5821,hzxa21,2018-11-30T22:21:20Z,thanks for the comments. i have addressed your comments in the updated commit.,1,0.9355854392051697
443358641,5821,junrao,2018-11-30T22:27:28Z,: do you want to take another look of this pr before merging?,0,0.9918147921562195
443404840,5821,lindong36,2018-12-01T06:48:08Z,lgtm. thanks for the in-depth review ! i am currently not able to access my original github account due to loss of my phone number and can not merge this pr myself.,1,0.9913835525512695
443437923,5821,junrao,2018-12-01T16:23:00Z,: could you address the last comment on line 415 in kafkazkclient.scala?,0,0.9940541386604309
443440384,5821,hzxa21,2018-12-01T16:58:56Z,my bad. updated the pr to fix that.,-1,0.994317352771759
443601653,5821,hzxa21,2018-12-03T06:24:02Z,"thanks a lot for the review. there are several follow-ups (kafka-6029 & kafka-7283) we can do after getting the broker epoch. i will address them accordingly and submit prs. if you are aware of anything that can also benefit from broker epoch, do let me know and i am open to more discussion. thanks again!",1,0.9950287938117981
410306248,5428,guozhangwang,2018-08-03T16:27:37Z,"for reviews. note the logic of processing enforcement is open for discussion, and currently i'm thinking to change it to time based than iteration based. i will include a kip for this change for open discussions.",0,0.990368127822876
411805845,5428,guozhangwang,2018-08-09T15:51:11Z,system test passed at [a link] need to wait for kip-345 to be discussed and voted.,0,0.9952173233032227
412221182,5428,guozhangwang,2018-08-10T22:18:53Z,"i've done some simple benchmark comparing * trunk [a link] * this branch (default max.idle = 0ms) [a link] * this branch (default max.idle = 500ms) [a link] * this branch (default max.idle = 5000ms) [a link] my observations: 1. branch-0ms and branch-500ms have similar perf compared to trunk with one single input partition, with two partitions (i.e. joins) branch-0ms has degraded 25% compared to trunk, while branch-500ms outperforms slightly over trunk. still need to investigate why branch-0ms is worse with trunk. 2. branch-5000ms performs worse on single partitions with branch-0ms and branch-500ms, while its performance is comparable on two partitions (joins).",0,0.9376220107078552
413359579,5428,guozhangwang,2018-08-15T22:31:16Z,"i've found out the actual perf difference is that for joins, we are using leftjoin actually. and we are populating the data before starting the run the tests, hence it is actually a ""catching up"" mode where the first fetch request may just return one partition of the data. with no synchronization (ms = 0), it means that there are less records being joined and output to sink topics. i've changing leftjoin to join and the results are following: trunk: [a link] max.idle = 0ms: [a link] max.idle = 500ms: [a link] max.idle = 5000ms: [a link] some observations: 1. from the stream-stream join results we can see that the old manner (max.waits = 5 iterations) is actually very close to max.idle = 0ms (i guess it was maybe a few millis idle time). 2. from the table-table join results, the new code actually processes more data than the old code.",0,0.9596478343009949
413940230,5428,mjsax,2018-08-17T17:47:21Z,can you rebase?,0,0.9934529066085815
414789168,5428,guozhangwang,2018-08-21T19:12:11Z,the benchmark results looks good to me: [a link] [a link],1,0.9776875376701355
415487809,5428,guozhangwang,2018-08-23T16:48:48Z,"the tests i added in unit suite checks that 1) `idle time is respected, and timer is reset correctly once enforce has happened`, 2) optimization on skipping commit indeed happens. i think i can add more on 3) punctuate / commit happens as expected, 4) break loop and calls poll as expected. anything else comes to your mind?",0,0.975376546382904
415540813,5428,guozhangwang,2018-08-23T19:24:58Z,i'm now working on add more tests ( let me know if the above plan is sufficient). i tried about separating commit for active and standby tasks but after some investigation i realized it is related to [a link] and hence i'd leave it to this jira / pr (also left a comment in the ticket for some clarification questions).,0,0.9575143456459045
415849669,5428,guozhangwang,2018-08-24T18:50:35Z,the latest perf numbers can be found here: [a link],0,0.9928492307662964
416423200,5428,guozhangwang,2018-08-28T01:43:45Z,"comments replied / addressed, please take another look.",0,0.9870442748069763
417518708,5428,guozhangwang,2018-08-31T01:19:34Z,"just to clarify, the reason i did this change is that i think recording the sensor every time we enforce processing actually makes more sense than only recording it the first time we start enforcing, since each enforce-processing period may differ and hence the number of records it processed may differ, recording only at the first time may not give us right amount of ""risk assessment"" when it happens.",0,0.9895926713943481
417686888,5428,vvcephei,2018-08-31T14:46:22Z,"lgtm! thanks,",1,0.9929248094558716
417743966,5428,bbejeck,2018-08-31T17:58:18Z,failures unrelated retest this please,0,0.8057448267936707
417778438,5428,guozhangwang,2018-08-31T20:18:50Z,"that's a good question. personally i think we should make strict guarantees that if `processorcontext.commit()` is called, we make sure the state is committed before the next record for this task is going to be processed, because some application logic may rely on that (think: i did some external changes, and hence i do want to make sure the state is committed right aware before processing the next record). and hence the implementation of `maybecommitperuserrequested` in stream thread actually always check for `commitrequest` and `commitneeded` while ignoring the commit interval. could you point out which part of the doc indicates that `processorcontext.commit()` is more of a suggestion? maybe we do need to update it.",1,0.8393359184265137
419097804,5428,bbejeck,2018-09-06T13:42:20Z,"i can't seem to find it, i must be mistaken so ignore my previous comment.",0,0.8959269523620605
419674466,5428,mjsax,2018-09-08T21:36:45Z,test failures seem to be related. [code block],0,0.9817584753036499
420112814,5428,guozhangwang,2018-09-11T01:15:42Z,could you give it another look?,0,0.9866482615470886
420474848,5428,mjsax,2018-09-12T00:54:03Z,nice! super happy we got this finally in! great work !,1,0.9976009726524353
289730122,2743,ijuma,2017-03-28T10:35:38Z,"once the jenkins build is passing, it would be great to run the system tests via the branch builder.",0,0.8101420998573303
289955048,2743,ijuma,2017-03-29T01:31:52Z,", i merged a pr that disables the test_zk_security_upgrade test until this pr lands. when you rebase, can you please remove the ignore annotation from that test? thanks.",1,0.733881950378418
290489990,2743,benstopford,2017-03-30T17:53:14Z,system tests running here: [a link],0,0.9956315755844116
291013072,2743,benstopford,2017-04-02T20:35:25Z,i've addressed your second round of comments. i still need to add some code to protect earlier versions of the message format. i'll dig into that tomorrow.,0,0.9671790599822998
291670976,2743,benstopford,2017-04-04T23:32:48Z,this pr was re-raised from the confluent repo: [a link],0,0.99628746509552
2565744411,18240,ahuang98,2024-12-30T17:29:24Z,"yes, i decided not to list that as an option because i felt it was equal to if not worse than the option of having resigned transition to prospective in epoch x + 1. personally i felt it was nicer to have less edge cases to the invariant that only prospective should transition to candidate",0,0.9825359582901001
266410201,2244,dguy,2016-12-12T11:47:49Z,i've not added any metrics to this at the moment as i would rather wait until the metrics prs from make it in (so i can avoid more merge conflicts),0,0.9830290079116821
270882471,2244,enothereska,2017-01-06T11:06:28Z,so far reviewed only top level apis and lgtm. need to review rest.,0,0.9894722104072571
270945528,2244,enothereska,2017-01-06T16:43:36Z,i've reviewed next level down up to tests. overall lgtm. there are opportunities for doing a code refactor later on (post-pr) because the separate thread and state manager are similar to the existing code.,0,0.9920469522476196
271264571,2244,enothereska,2017-01-09T11:39:43Z,"had a look at tests, they lgtm. overall i personally feel this optimisation adds a lot of complexity but i suppose that ship has sailed.",0,0.8863776326179504
271393703,2244,guozhangwang,2017-01-09T20:10:03Z,"about the `old value computation` issue, i cannot think of a quick solution on top of my head and you may not like my ""long"" solution, but here it goes: 1. materialize the ktable-ktable join results as well as the joining ktales. as we discussed in another thread of ktable api confusions this is the last place where we may not materialize a ktable. 2. after all ktables are materialized, we then do not need to pass the "" "" pair before the join operations at all, but only before the aggregation operations, since we can always read the old joined value from the materialized store. 3. for global ktables, if we add its self joins with global ktables as well in the future, it will no be `virtual` either but also materialized. 4. one potential issue is, say if we have a multi-way table joins, then it may not be really worthwhile to materialize the intermediate tables as well. this can possibly be resolved when we extend the dsl parsing to global context to optimize some of the topologies.",0,0.9196482300758362
271529418,2244,dguy,2017-01-10T09:39:26Z,"- your points above - that is the same solution i had in mind. i didn't really want to go down that path due to the overhead of introducing another store, changelog etc, but maybe that is the only way. there is also an issue i discovered on ktable/ktable join which means we might need to do something like this anyway - [a link]",0,0.9687039256095886
271560509,2244,dguy,2017-01-10T12:13:50Z,", - i've fixed the issue by materializing the join as g mentioned above. this does mean i'll need to update the kip to add 2 overloaded methods as we need the serdes to write to the store. at the moment i think we either go this way or we remove the ktable/globalktable joins for now. my preference is probably to go with it",0,0.9421876072883606
272077407,2244,guozhangwang,2017-01-12T05:08:25Z,lgtm. could you rebase?,0,0.9903671741485596
272127373,2244,dguy,2017-01-12T10:24:03Z,- done.,0,0.8567222356796265
272216338,2244,dguy,2017-01-12T16:49:11Z,test this please,0,0.9870431423187256
272262967,2244,guozhangwang,2017-01-12T19:46:40Z,merged to trunk. thanks !! please feel free to close the corresponding jira when asf is back online.,1,0.9957996010780334
257023225,2074,vahidhashemian,2016-10-28T20:42:10Z,this pr also fixes another issue that was introduced by the fix for kafka-3144. i point to it inline. let me know if you think that issue should be fixed and merged (more quickly) in a separate pr. thanks.,1,0.9571873545646667
266855542,2074,vahidhashemian,2016-12-13T20:41:13Z,i've tried to address both kip-88 and kafka-3853 in the same pr here. i'd appreciate your feedback when you get a chance to review this. thanks.,1,0.9868582487106323
271678517,2074,vahidhashemian,2017-01-10T19:48:44Z,"thanks for your thorough review of the pr and your feedback. i updated the pr based on your comments (except for the part that's related to kip-97, as discussed).",1,0.9795788526535034
271742743,2074,hachikuji,2017-01-11T00:35:18Z,took a very quick look at this and it seems like it's not too far. i'm thinking we might want to let the client compatibility kip go in first since it is already quite large and it will be easier to make the changes here to continue supporting the old version of the offset fetch request. does that seem reasonable?,0,0.9236370921134949
271751793,2074,vahidhashemian,2017-01-11T01:28:59Z,"yes, that sounds good to me. thanks for reviewing the patch. i'll work on the rest of your review items in the meantime.",1,0.9868767261505127
271970642,2074,hachikuji,2017-01-11T19:33:55Z,looks like kip-97 (client compatibility) was merged.,0,0.9896457195281982
272019686,2074,vahidhashemian,2017-01-11T22:44:25Z,thanks again for your feedback. i've tried to address them in the latest update. feel free to take a look while i work on rebasing the pr to cover client compatibility (kip-97).,1,0.9687585234642029
272361345,2074,hachikuji,2017-01-13T04:57:23Z,"saw your question about why we are only checking 3 of the 5 error codes. haha, i noticed the same thing when reviewing and opened kafka-4622.",1,0.9871833324432373
272362673,2074,vahidhashemian,2017-01-13T05:10:48Z,thanks for clarifying. then i can leave that to be fixed as part of that jira.,1,0.9743737578392029
272507729,2074,hachikuji,2017-01-13T18:15:21Z,"i merged kip-103, which looks like it causes some conflicts in `apiversion`. hopefully shouldn't be too much trouble to resolve.",0,0.9694468379020691
329325194,3849,apurvam,2017-09-13T23:27:51Z,"thanks for the pr . looking over the changes, it seems that there are two cases from the kip which don't seem to be covered: 1. setting the `polltimeout` to be the expirytime of the oldest batch being sent in the produce request. i think we need this to make sure that we expire batches in a timely manner. 2. related to the previous point, the current patch doesn't seem to expire inflight requests, which is another feature that the kip seems to promise. have i missed something? or are you planning on adding the functionality above?",1,0.9510508179664612
329635100,3849,apurvam,2017-09-14T23:22:39Z,"heads up , the following pr just got merged, and may have some conflicts with the current patch : [a link] there shouldn't be any impact on the logic, however.",0,0.9924845099449158
330281613,3849,ijuma,2017-09-18T16:41:36Z,friendly reminder that the feature freeze is this wednesday.,0,0.9896777868270874
330318612,3849,becketqin,2017-09-18T18:43:40Z,"just want to check. do you think this feature is a ""minor"" feature?",0,0.9825752973556519
330426320,3849,ijuma,2017-09-19T04:31:01Z,", it is possible to classify this as a minor feature, but the fact that it affects a core part of the producer puts it in a bit of a grey area. if the pr is almost ready and we miss the feature freeze, my take is that it would be ok to merge it by the end of this week. later than that and it seems a bit risky. it's a bit worrying that the merge conflicts haven't been resolved since last week.",-1,0.6340653896331787
330584441,3849,sutambe,2017-09-19T15:52:00Z,i've an new pr but after a rebase i've to fix one more test. working on that now.,0,0.9607422947883606
330586776,3849,ijuma,2017-09-19T15:58:56Z,thanks !,1,0.8865044713020325
330600198,3849,sutambe,2017-09-19T16:44:02Z,it's not clear to me why `testexpiryoffirstbatchshouldcauseresetiffuturebatchesfail` and `testexpiryoffirstbatchshouldnotcauseunresolvedsequencesiffuturebatchessucceed` are failing. it looks like a batch that should be in incompletebatches isn't there. any thoughts?,0,0.7000368237495422
330674030,3849,tedyu,2017-09-19T21:08:10Z,i added the following to testexpiryoffirstbatchshouldcauseresetiffuturebatchesfail before the first sender.run() call [code block] the test still fails.,0,0.9941613078117371
330699245,3849,apurvam,2017-09-19T23:10:16Z,where are those tests failing? the latest pr builder suggests that the clients and core tests all passed.,0,0.9841361045837402
330700192,3849,sutambe,2017-09-19T23:16:08Z,the sender and recordaccumulator are passing now. the failing tests are connect tests that are irrelevant.,0,0.9913700819015503
330701042,3849,apurvam,2017-09-19T23:21:58Z,"i don't think the test failures are irrelevant since the same 3 tests failed in all the runs. further, the cause of the failure is: [code block] i think their mocks may need to be updated to take account of the new configs and attendant `configexceptions`",0,0.9838132858276367
331024421,3849,apurvam,2017-09-21T01:24:08Z,"i had a look at the failing sender expiry tests. what is happening is that the tests are not modified to account for the fact that the inflight batches can be expired. in the tests, we used to expire a batch sitting in the accumulator but _not_ the inflight batch. when the inflight batch returned, it would be re queued. but now, the test sends the response for the inflight batch, but when it goes to requeue, it discovers that there shouldn't be an inflight request an raises an exception. the tests should be updated to account for the new behavior and make sure that the inflight batch is _not_ expired.",0,0.9333487153053284
331024720,3849,apurvam,2017-09-21T01:26:25Z,"actually, the test reveals a bug in the current patch: the response for the inflight batch which expired is not being handled correctly. we should not be trying to requeue it to start with. so we need two tests: one where the inflight batch is not expired, and the current case. the reenqueue logic in the sender needs to be updated to not reenqueue the expired batches.",0,0.98628169298172
331324500,3849,sutambe,2017-09-22T01:22:37Z,added a couple of tests. test a batch is correctly inserted into the in.flight.batches if needed. and not inserted if not needed. `recordaccumulatortest.testsoontoexpirebatchesarepickedup` test the callback of an expired batch is fired in time when it is in-flight/not in-flight `sendertest.testinflightbatchesexpireondeliverytimeout`,0,0.9781977534294128
331497207,3849,hachikuji,2017-09-22T16:36:14Z,i don't see any changes since my comments. are you sure you have pushed them?,0,0.9819961786270142
331581751,3849,sutambe,2017-09-22T23:21:09Z,"added more tests test when expire an in-flight batch, we still wait for the request to finish before sending the next batch = `sendertest.testwhenfirstbatchexpirenosendsecondbatchifguranteeorder` test we are not going to retry an already expired batch = `sendertest.testexpiredbatchdoesnotretry` regarding the last test ""test when batch is expired prematurely, the buffer pool is only deallocated after the response is returned. (because we are still holding the batch until the response is returned)"". i'm not sure if it's really needed because `recordaccumulator.abortbatches` deallocates all incomplete batches (whether they have been sent or not).",0,0.9908449053764343
331584625,3849,becketqin,2017-09-22T23:49:16Z,i am looking at the update patch. `recordaccumulator.abortbatches()` is only used when the producer is force closed. we do not need to worry about the memory consumption at that point anymore because the producer has stopped accepting new messages.,0,0.9885225296020508
331585074,3849,becketqin,2017-09-22T23:53:45Z,it looks there are 3 test failures from connect. can you check?,0,0.9904306530952454
331613171,3849,becketqin,2017-09-23T05:59:15Z,"btw, i did not see the tests to ensure we are not double deallocating memory. will you add those tests?",0,0.9832709431648254
331983684,3849,apurvam,2017-09-25T19:15:38Z,"fyi, the connect tests are still failing with: [code block] it would be good to get some green builds before this can be considered safe for merge.",0,0.9731734395027161
331987993,3849,sutambe,2017-09-25T19:33:20Z,fixed connect tests.,0,0.9935925602912903
332034321,3849,apurvam,2017-09-25T22:40:11Z,"i have a high level comment about the inflight batch handling. i was wondering what the advantages/disadvantages of the current approach (where batches are grouped by creation time) would be against adding a `map >` to the recordaccumulator. this would complement the existing `map >`. the priority queue would be ordered by the batch creation time (similar to the current `navigablemap`). we would add to the queue on drain. and we would remove from the queue in `sender.failbatch` and `sender.completebatch`, just like we do for the `transactionmanager` (see: [a link] so we don't have any special logic for the `recordaccumulator.expiredbatches` would now iterate over both maps (`topicpartition -> deque` and `topicpartition -> priorityqueue`), with the priority queue being iterated on first. this doesn't change the time complexity of the algorithm, since we are iterating over all partitions in `expiredbatches` once anyway. the advantage of this approach is that managing inflight batches is much simpler (add to the queue on drain, remove on completion). it doesn't complicate the expiry in any way. the main disadvantage that i see is in computing the `earliestdeliverytimeout`. for this you would need to peek at the head of the queue for each partition which has an inflight batch. but this should be fairly cheap, since we would be iterating over 1000's of objects even in the largest installations. what do you think?",0,0.9089749455451965
332039241,3849,apurvam,2017-09-25T23:08:58Z,"i made this comment in a thread, but that may be buried because github collapses comments. there is a bug in the current implementation which means that we will retain references to `producerbatches` in the producer long after they have been successfully completed. the following sequence of assertions in any `sendertest` shows the presence of this bug: [code block]",0,0.9909768104553223
332043954,3849,becketqin,2017-09-25T23:38:36Z,"initially i was under the same impression. one concern of the priority queue is that whenever we need to remove things out of the queue, we will have to iterate over it. since a producer can poll thousands of times per seconds, and maybe remove tens of thousands of batches. that said, in most cases, the queue would be empty because most of the batches are sent when remaining delivery.timeout.ms is greater than request.timeout.ms. however, it may still slowdown things when retries occur. my take on this is that if the implementation complexity does not increase much, we may want to follow the robust way instead. regarding the memory issue. we could not release the memory of an expired batch right away when the batch is still in-flight. this is because the in-flight batch is still referred by the requestcompletionhandler of the request. we could remove it from the requestcompletionhandler, but this is a little tricky.",0,0.9490423202514648
332044968,3849,apurvam,2017-09-25T23:45:40Z,"i am not sure i follow this: is this a response to my comment above where i shared a snippet from `sendertest` to show that we are retaining batches after they are completed? if so, i didn't mean that we need to release an inflight batch right away. what i was pointing out is that the _only_ way the current patch is deleting batches is by expiring them. this may not be a big deal, since the deallocation is done when the batch is completed, but it still seems like an unnecessary anomaly.",-1,0.6121717095375061
332045109,3849,apurvam,2017-09-25T23:46:29Z,"also, it looks like recent commits are causing threads to leak, .",0,0.9382616281509399
332046231,3849,apurvam,2017-09-25T23:54:07Z,not sure i follow. it would be no worse that what we do today with the `deque` except it would be limited only to partitions which had in flight batches. so it is not making things much worse.,0,0.5246639847755432
332058975,3849,becketqin,2017-09-26T01:26:11Z,i misunderstood your proposal. i though you were suggesting to use a priority queue to store all the in-flight batches. a `map >` sounds reasonable.,0,0.919684112071991
332080828,3849,apurvam,2017-09-26T04:06:38Z,"yes, i meant a separate `map >`, to replace the current `navigablemap`.",0,0.9944570064544678
332662977,3849,hachikuji,2017-09-27T21:36:49Z,any updates on this? do you need help addressing the test failures?,0,0.9914734363555908
332973054,3849,sutambe,2017-09-28T21:46:31Z,sorry for disappearing on you guys. i got sucked into an oncall issue. so i had to context switch. i plan to resume work on this pr next week. i've not had a chance to respond to the comments so far. thank for you the review so far. it's been very useful.,-1,0.9438754320144653
344707856,3849,sutambe,2017-11-15T19:49:41Z,i've updated the pr. please take a look,0,0.978773295879364
344734652,3849,apurvam,2017-11-15T21:30:07Z,"please fix the failing tests, there is a config exception due to the new delivery timeout config.",0,0.9851130247116089
345031109,3849,sutambe,2017-11-16T19:11:24Z,all checks passed,0,0.9897468090057373
353216067,3849,sutambe,2017-12-20T23:47:57Z,i updated the implementation to use `concurrentmap >`. please take a look. i don't see the above test failures on my machine.,0,0.9943063855171204
358480286,3849,apurvam,2018-01-17T23:08:17Z,"so the `org.apache.kafka.clients.producer.internals.sendertest.testmetadatatopicexpiry` test has failed twice in a row with: [code block] given that these changes are on the same code, and given the consistent failure of this test, it is probably a regression. can you reproduce the failure locally?",0,0.9924452900886536
358480445,3849,apurvam,2018-01-17T23:08:58Z,"just looking at the stack trace and the test, it may be that an expired batch is being closed twice in some cases.",0,0.987454891204834
368266171,3849,hachikuji,2018-02-24T22:43:34Z,it would be nice to unblock this. can someone else pick up the work?,0,0.84945148229599
370507706,3849,becketqin,2018-03-05T18:02:09Z,"yeah, this has been pending for too long. i have spoken to and he said he still wants to finish the patch. he will figure out the eta and see if that works.",0,0.748426616191864
385707559,3849,bbejeck,2018-05-01T15:56:47Z,is there any update on the status of this pr? it would be great if we could get this in the next release.,1,0.6715326309204102
387590454,3849,Ishiihara,2018-05-09T01:18:58Z,we also hit this issue when running kafka streams library with some high volume output topics. it would be nice to get this moving and push it to the next release.,0,0.9666382074356079
387592858,3849,radai-rosenblatt,2018-05-09T01:35:41Z,becket cant load this page for some reason (some weird issue with his github profile?). we are ok with you taking over this patch.,0,0.8979479074478149
387609246,3849,sutambe,2018-05-09T03:28:23Z,"i don't have any update since dec last year. sorry, the work has stalled and it has been very hard to find cycles for this effort. i don't mind if confluent wants to take this effort forward. better later than never.",-1,0.9887852668762207
387636464,3849,Ishiihara,2018-05-09T06:41:26Z,cc,0,0.9699252247810364
393617255,3849,yuyang08,2018-05-31T17:49:33Z,i made some change based on your pull request to fix style check and test failure. do yo mind i amend the change to this pull request? cc [a link],0,0.9939084053039551
393700104,3849,guozhangwang,2018-05-31T22:22:49Z,i'd suggest you create your own pr against apache kafka trunk and let other reviewers to continue reviewing that one.,0,0.986215353012085
393760744,3849,yuyang08,2018-06-01T05:18:09Z,sure. will create a separate pull request,0,0.9913539290428162
399271744,3849,yuyang08,2018-06-21T23:12:09Z,created new pr [a link] for kafka-5886,0,0.9951144456863403
464845906,3849,ijuma,2019-02-18T19:07:09Z,"this has been merged via a different pr, closing.",0,0.9953041076660156
217925465,1336,vahidhashemian,2016-05-09T17:05:54Z,would appreciate your input on this pull request.,0,0.5240486860275269
226875059,1336,vahidhashemian,2016-06-17T20:35:45Z,thank you for your feedback. the pr was updated to address your comments.,1,0.9276379942893982
227269465,1336,hachikuji,2016-06-20T21:06:06Z,i'm wondering about approaches for testing this patch. one possibility is to parse the output in the test case and build assertions off from that. another option might be to decouple the command logic from the print logic so that you can build test cases using a convenient object representation. what do you think?,0,0.9457823634147644
227306696,1336,vahidhashemian,2016-06-21T00:12:42Z,"thank you very much for your careful review and feedback. i made some changes in the new patch based on the feedback received. the only part that is outstanding in addressing your feedback is [a link]. i responded to your comment and will wait for your clarification as i may be missing something. both approaches you suggested for designing unit tests for this patch should work. parsing the output and writing assertions based on that would be faster but probably not the cleaner option, in my opinion. separating the logic from reporting seems more reasonable to me but it would require more time and thinking. i can take either direction once i hear your (and others') feedback about it.",1,0.9733587503433228
227503027,1336,hachikuji,2016-06-21T16:56:47Z,"yeah, i agree that separating the logic would make testing easier. it's more painful now, but making the code more testable would probably pay off in the long run. i wonder if we could introduce a simple trait to encapsulate the output writing. for example: [code block] then we could test the command using a mock implementation. anyway, if the level of effort to get some basic coverage using this or another pattern is not too high, i'd suggest we do it here. otherwise, we could create a followup jira.",0,0.9270712733268738
227780324,1336,vahidhashemian,2016-06-22T15:26:15Z,thanks for your suggestion on how to go about separating the logic. i'll give it a try and let you know how it goes.,1,0.9699283838272095
228908052,1336,vahidhashemian,2016-06-27T23:39:00Z,i've spent some time on this and it seems to involve restructuring of the code to some extent (assuming i haven't over-complicated things). so i'm wondering whether we should keep the changes simpler and do this work and unit tests in separate work items.,0,0.9205561280250549
228910259,1336,hachikuji,2016-06-27T23:52:18Z,"either way works for me. i don't think there's any particular hurry to get this patch in since it will probably go in 0.10.1, so if restructuring the code seems like a good idea, i'm probably slightly in favor of doing it here so we don't forget about it later. that said, i know it can be painful to fill in some of these testing gaps (they're gaps because testing wasn't easy!), so if is ok with it, then doing it separately seems fine to me.",0,0.6328725814819336
229165709,1336,vahidhashemian,2016-06-28T20:03:32Z,"sounds good. i'm okay both ways and will wait for 's feedback. regarding the unit tests, i'm trying to wrap my head around how to write them with proper mocking. you can see [a link] how i've made changes to the last patch so the `outputwriter` trait that you suggested is used. the issue now is i don't see clearly how to write the unit tests with mocking (for example, for `--describe` the main functionality is inside the protected `describegroup()` method). this tells me perhaps further restructuring is required to make the command properly mockable, but i wanted to ask your opinion before heading down that road. thanks.",1,0.9536570906639099
232168291,1336,vahidhashemian,2016-07-12T20:22:52Z,i restructured `consumergroupcommand.scala` a little bit for testing purposes according to your suggestion and added a number of unit tests. i would appreciate your feedback.,1,0.9527617692947388
233806677,1336,vahidhashemian,2016-07-20T00:32:44Z,"this in another pending pr that conflicts with changes required for [a link], and would be great if we can finalize it soon. thanks.",1,0.9914383888244629
234130456,1336,vahidhashemian,2016-07-21T01:16:08Z,thanks for the thorough feedback. i hope i addressed all of them in the recent update.,1,0.9740380048751831
235795010,1336,vahidhashemian,2016-07-28T04:01:06Z,thanks for another round of reviews. the pr is updated to address your comments.,1,0.933872640132904
236029039,1336,vahidhashemian,2016-07-28T21:21:22Z,thanks . made those changes.,1,0.6993794441223145
245448879,1336,vahidhashemian,2016-09-07T23:19:52Z,the patch is updated to include coordinator info when `--new-consumer` is used. since the group column and the new coordinator column report redundant information i modified the output of `--new-consumer` option to [code block] the zookeeper based output will not print the coordinator line above. your feedback is appreciated.,1,0.9429442286491394
254966900,1336,vahidhashemian,2016-10-19T23:09:19Z,thanks for the thorough feedback. i hope i was able to address your comments in the recent patch.,1,0.9716061949729919
255177309,1336,vahidhashemian,2016-10-20T17:46:36Z,i submitted another update taking into account your comments. thanks again for the thorough reviews and constructive feedback.,1,0.9836836457252502
255270819,1336,vahidhashemian,2016-10-21T01:25:25Z,"i had one more crack at this. if you spot any more potential improvements please let me know. thanks. btw, the junkins error seems to be unrelated to this.",1,0.9880913496017456
255469685,1336,vahidhashemian,2016-10-21T21:28:39Z,just did some cleanup of variable/method/field names based on the recent `member id` to `consumer id` renaming.,0,0.9943391680717468
255487773,1336,hachikuji,2016-10-21T23:22:08Z,"lgtm. really appreciate the effort on this pr. it's a huge contribution when you can not only add functionality, but improve the testability of the code.",1,0.9938043355941772
255492874,1336,vahidhashemian,2016-10-22T00:08:19Z,thanks for your feedback along the way. very much appreciated.,1,0.9848625063896179
290404058,2744,rajinisivaram,2017-03-30T13:03:31Z,"thank you for the review, i have addressed the comments and left one question. system tests passed here: [a link]",1,0.9501821398735046
292580163,2744,rajinisivaram,2017-04-07T16:12:14Z,"thank you for the review. i haven't moved throttle-time to the response header as explained in the previous comment. one alternative to making the code in `kafkaapis` neater would be to make `throttletimems` a non-final field. that way the response could be created as it is now, and throttle time can just be updated in the response. i didn't want to do that initially because it makes the code inconsistent, but perhaps that is ok for this case?",1,0.9299830794334412
296140304,2744,ijuma,2017-04-21T09:22:51Z,"when it comes to performance tests, it would also be good to run them on aws as vms like xen often have an impact of time-related methods. for example: [a link]",0,0.9918033480644226
296242715,2744,rajinisivaram,2017-04-21T16:44:49Z,"thank you for the reviews. i have rebased. i have performance tests on my laptop with 8 threads and see no noticeable difference. - producer (100 byte messages): 161.92 mb/s (before) and 164.4 mb/s(after) - consumer (100 byte messages) : 168.27 mb/se (before) and 173.5 mb/s (after) i don't have an account to run the tests on aws, but i will start a system test run to compare.",1,0.9804973602294922
296455215,2744,ijuma,2017-04-23T16:32:17Z,"by the way, since the additional `system.nanotime` call per selection key only happens if the new config is enabled, i think it's ok to merge this and do additional performance testing later. i think this is one of the cases where issues only show themselves when the number of threads and cpus is larger than one can test on a laptop.",0,0.9793459177017212
296608816,2744,rajinisivaram,2017-04-24T10:24:47Z,"yes, agree that it is difficult to assess the impact of the change on a laptop. i have addressed most of the issues. left comments for the others. many thanks for the reviews.",1,0.9708483219146729
296814266,2744,rajinisivaram,2017-04-24T20:35:12Z,"thank you for the review. i have run the performance tests on my laptop with high request quota configured. again, there was no noticeable difference. producer (100 bytes): trunk: 151.33 mb/s requestquota: 155.78 mb/s consumer (100 bytes): trunk: 332.32 mb/s requestquota: 339.64 mb/s also started a system test run with the latest level of code: [a link]",1,0.9613811373710632
297013271,2744,rajinisivaram,2017-04-25T12:24:17Z,system tests have passed: [a link],0,0.9885116815567017
298118490,2744,rajinisivaram,2017-04-28T22:11:50Z,thank you for the reviews. have rebased and addressed the comments.,1,0.9301319718360901
298363968,2744,junrao,2017-05-01T16:11:47Z,: thanks for the latest patch. lgtm.,1,0.9688839912414551
729396557,9487,ableegoldman,2020-11-18T04:23:42Z,system test run (still running but so far it's all pass) -- [a link],0,0.9864120483398438
729474257,9487,ableegoldman,2020-11-18T06:53:32Z,"system tests passed, all three java builds passed. merging now",0,0.9345157742500305
729475252,9487,ableegoldman,2020-11-18T06:55:22Z,merged to trunk :partying_face:,1,0.7666565775871277
764961250,9487,mjsax,2021-01-21T21:45:25Z,seems we forgot the update the docs for this kip. can you do a follow up pr? this follow up pr should also cover the changes of [a link] that is part if the same kip.,0,0.9907991290092468
857564039,10851,lkokhreidze,2021-06-09T10:04:17Z,call for review,0,0.990225613117218
862166415,10851,lkokhreidze,2021-06-16T08:33:17Z,gentle nudge,0,0.9650260806083679
879032262,10851,cadonna,2021-07-13T12:08:48Z,sorry for the long silence! some other tasks got in my way. i plan to review this pr tomorrow.,-1,0.993237316608429
880127165,10851,lkokhreidze,2021-07-14T18:46:39Z,thanks for the feedback. i've replied/addressed all of your comments.,1,0.9715455770492554
882283484,10851,lkokhreidze,2021-07-19T06:41:50Z,thanks for the valuable feedback :person_bowing: i've replied/addressed your comments.,1,0.9950624108314514
885645624,10851,cadonna,2021-07-23T13:38:45Z,fyi: i will be offline the next two weeks. i am sorry that i haven't had time to review this pr this week.,-1,0.9911136031150818
899245208,10851,lkokhreidze,2021-08-16T06:01:13Z,hi ! thanks for the feedback. i will address your comments this week.,1,0.9876165986061096
902900779,10851,lkokhreidze,2021-08-20T19:11:41Z,hi thank you for valuable feedback. i've addressed your comments. please have a look once you got time.,1,0.9811106324195862
905490438,10851,lkokhreidze,2021-08-25T13:13:26Z,"thanks for the feedback , i've pushed the new changes.",1,0.9180072546005249
905514160,10851,lkokhreidze,2021-08-25T13:44:11Z,"the general thought on the implementation. as of now, we choose concrete `standbytaskassignor` implementation based on passed `assignmentconfigs` value. instead, an alternative approach would be to have a chained standby task assignment based on multiple implementations. for instance, when required client tag configuration is present, we can try to assign the standby tasks based on `list.of(new clienttagawarestandbytaskassignor(), new leastloadedclientstandbytaskassignor())`. this way, `clienttagawarestandbytaskassignor` can try to distribute the standby tasks based on tags dimensions. if there are no more client tag dimensions available, and we still have `any(taskstoremainingstandbys) > 0`, the next implementation in the chain (in this case `leastloadedclientstandbytaskassignor`) will be called. with this, `leastloadedclientstandbytaskassignor` can default to assigning the remaining standbys to the least loaded clients. right now, `clienttagawarestandbytaskassignor` does both assignments based on available dimensions and fallback to the least loaded if there are no enough tag dimensions. current implementation leads to more complex code. while with the approach above, we can clearly separate the implementations without duplication (there's no code duplication, rather ""logic"" duplication). for now, i've chosen to go with the simplest approach - having two independent implementations and selecting an appropriate one based on passed `assignmentconfigs.` still, i wanted to share this idea here, just in case.",0,0.9841895699501038
911752624,10851,lkokhreidze,2021-09-02T14:33:30Z,"hi , i've addressed/replied to your comments. thanks for the feedback. fyi - i'll be offline from next week for 2 weeks.",1,0.9871395230293274
925659295,10851,lkokhreidze,2021-09-23T09:45:41Z,hi is it possible to continue pushing this pr forward? i'm back from my holidays.,0,0.9723306894302368
927659618,10851,cadonna,2021-09-27T08:47:34Z,welcome back ! sorry for the silence around this pr. i got side-tracked by other tasks. i will try to review this pr this week.,-1,0.9941108822822571
934149747,10851,lkokhreidze,2021-10-05T07:40:28Z,thanks for the feedback i will review and address your comments this week.,1,0.9125552177429199
944314838,10851,lkokhreidze,2021-10-15T13:45:39Z,"hi , small update on my side - wasn't able to find time to work on this pr this week. will try to prioritise this for the next week.",0,0.8423479795455933
959050366,10851,lkokhreidze,2021-11-03T13:16:55Z,"hi very sorry for disappearing, didn't find time to deal with this pr. i've addressed your comments. please have another look when you got time.",-1,0.9907481074333191
983959899,10851,lkokhreidze,2021-12-01T18:55:18Z,"hi , will you have time to look at this pr again?",0,0.947087287902832
984453564,10851,cadonna,2021-12-02T09:36:10Z,i am really sorry that i haven't found the time to look at your updated pr. i will put it on my list for next week.,-1,0.9893847107887268
984477740,10851,lkokhreidze,2021-12-02T10:06:16Z,no worries and thank you.,1,0.9535226821899414
993399413,10851,lkokhreidze,2021-12-14T10:30:39Z,hi thanks for the feedback. i'll address your comments shortly.,1,0.92336106300354
1017438641,10851,lkokhreidze,2022-01-20T12:17:45Z,hi small update from my side. i came back from holidays a week ago so will continue working on this pr this week. sorry for the delay.,-1,0.9937844276428223
1017815908,10851,lkokhreidze,2022-01-20T18:51:38Z,"hi , i've address your comments with the latest commit. please have a look when you got time. thanks.",1,0.9760830402374268
1018535614,10851,lkokhreidze,2022-01-21T14:08:34Z,"the latest commit (be3dcff4dc463fd8d23998537e36f852b99ec083) has a few changes. 1. there's explicit fallback to fallback to `defaultstandbytaskassignor` logic if rack aware standby task assignment fails to assign standby tasks. 2. rack aware standby task assignment logic takes into account client capacity when assigning the standby task. if client has reached the capacity, algorithm won't assign standby task to it. when that happens, we will fallback to the least loaded clients. i believe this is better approach, as we will avoid overloading the clients. there will be a warning log, to inform the user. **edit:** instead of just checking if capacity is reached on the client, we also now check if load can be balanced [a link]. looking forward hearing your thoughts,",0,0.9092721939086914
1025503003,10851,lkokhreidze,2022-01-31T08:48:28Z,"hi , sorry for the ping. any chance we could review pr this week? thanks",-1,0.992679238319397
1030773435,10851,showuon,2022-02-06T08:06:28Z,", thanks for the pr. i'll take a look next week. thanks.",1,0.9893261194229126
1032412988,10851,cadonna,2022-02-08T09:48:50Z,i will try to review this friday. sorry for the delay but i was sick for the last two weeks.,-1,0.9915675520896912
1041380775,10851,lkokhreidze,2022-02-16T11:16:09Z,"hi , thanks for the valuable feedback. i've addressed your comments and pushed the changes. i also resolved conversations feel free to unresolve them if you think i haven't addressed your comments. looking forward hearing your thoughts.",1,0.9950057864189148
1044574529,10851,lkokhreidze,2022-02-18T14:02:00Z,hi thanks for the feedback. i've addressed your comments.,1,0.9243869185447693
1046758399,10851,lkokhreidze,2022-02-21T11:08:33Z,"thanks for the valuable feedback. i've addressed your comments, please have a look when you got time.",1,0.986671507358551
1048612254,10851,lkokhreidze,2022-02-23T09:56:15Z,hi i've addressed your comments. please have a look. thank you!,1,0.9840270280838013
1050043629,10851,lkokhreidze,2022-02-24T16:38:30Z,hi mind having another look? hoping to finalise this pr as soon as possible :) thanks,1,0.9860947132110596
1055442027,10851,cadonna,2022-03-01T13:26:38Z,"i will try to have a look this week. i would also like to get this pr merged as soon as possible. since has already approved it. if i will not manage to have a look, can merge it.",0,0.8904854655265808
1055489018,10851,lkokhreidze,2022-03-01T14:14:22Z,"thanks , appreciate it. please also note that next pr, the protocol change, is also ready to be reviewed. [a link] whenever it's possible, please have a look at that too.",1,0.9895750880241394
1056919176,10851,cadonna,2022-03-02T13:15:29Z,let me know if you want to address my minor comments in this pr. after that or i can merge this pr. nice work!,1,0.9902796149253845
1056940382,10851,lkokhreidze,2022-03-02T13:37:19Z,"hi , thanks for the review. i agree with what you said and made a note to myself to address your comments in the follow-up prs. so if it's okay, i think we can merge this one. thank you!",1,0.9957320094108582
1057630852,10851,showuon,2022-03-03T03:30:46Z,failed tests are unrelated and also failed in `trunk` build. [code block],0,0.9905552268028259
1057900673,10851,cadonna,2022-03-03T10:28:49Z,:partying_face:,0,0.9041956067085266
137872274,195,Parth-Brahmbhatt,2015-09-04T22:37:57Z,the only test failure reported by the bot was kafka.producer.producertest > testsendwithdeadbroker failed java.lang.assertionerror: message set should have 1 message at org.junit.assert.fail(assert.java:88) at org.junit.assert.asserttrue(assert.java:41) at kafka.producer.producertest.testsendwithdeadbroker(producertest.scala:260) which passes locally and seems very unrelated to the changes i made. i will try and run the test in a loop to see if i can reproduce.,0,0.9914699196815491
139683279,195,Parth-Brahmbhatt,2015-09-11T23:04:29Z,added a generic zknodechangelistener and removed the scheduler thread. couldn't think of a better name for the class so if you have a better name let me know. addressed all other comments in this pr.,0,0.9558595418930054
141352763,195,Parth-Brahmbhatt,2015-09-18T05:53:26Z,all comments addressed. there is no return statement in the code now.,0,0.9955198764801025
142060970,195,ijuma,2015-09-21T17:56:29Z,-brahmbhatt i did a pr to your branch ([a link] with some suggested code readability improvements. if you agree you can merge it to your branch.,0,0.9653148055076599
142071503,195,Parth-Brahmbhatt,2015-09-21T18:41:03Z,changes to zknodechangenotificationlistener can not be accepted as wants the import to be local. other than that have incorporated your proposed changes.,0,0.9953698515892029
142071956,195,ijuma,2015-09-21T18:43:00Z,"-brahmbhatt, thanks for incorporating the changes. my understanding is that wanted the import to be local if `javaconversions` was used. with `javaconverters`, this is not needed because you have the `asscala` and `asjava` to show you that a conversion is happening (unlike the `javaconversions` case where everything happens silently).",1,0.9452248811721802
142098566,195,junrao,2015-09-21T20:24:01Z,"also, for javaconverters, it's ok to import it globally since we are using asscala explicitly.",0,0.993063747882843
142102372,195,Parth-Brahmbhatt,2015-09-21T20:40:46Z,all comments addressed.,0,0.9876030087471008
142148112,195,junrao,2015-09-22T00:45:15Z,thanks a lot for the patch. lgtm.,1,0.967147171497345
142148924,195,ijuma,2015-09-22T00:54:08Z,thanks parth!,1,0.9744465351104736
297201690,2910,hachikuji,2017-04-26T00:17:09Z,"i realized that the relationship between the lso and the high watermark is trickier than i originally thought. i had assumed that it was sufficient to take the minimum of the two as the maximum offset that a client could fetch in read_committed mode. the issue is when the commit or abort marker itself is larger than the high watermark. we need to ensure that none of the records from the associated transaction becomes readable until the marker itself is replicated (even if the records have offsets lower than the high watermark). this requires a little more bookkeeping as we need to keep track of which transactions have been completed, but are not replicated. i'll update the patch as soon as i have a solution ready.",0,0.9659914374351501
297508059,2910,hachikuji,2017-04-26T18:56:31Z,"note to self: once the transactional client patch lands, we should update `fetcher` to use the control sequence number when finding the commit/abort markers.",0,0.9911346435546875
298828817,2910,hachikuji,2017-05-03T05:56:40Z,"i've added a commit to do transaction index recovery. the approach i've taken is the following: 1. i've removed the periodic pid snapshot. 2. instead, when the log rolls, i take a new snapshot using the offset of the new segment. 3. when a segment has been flushed to disk, i remove the corresponding snapshot. the snapshot corresponding the start of the active segment is never removed. 4. when recovering a segment, i have to read the log from the starting segment in the worst case in order to rebuild the transaction index. once finished rebuilding an individual segment, i take another snapshot so that we don't have to repeat the same work if we have to recover another segment. to make this efficient, i ensure that logs are loaded/recovered in ascending order. 5. when the log is cleanly closed, i take another snapshot. i was a bit unsure of the benefit of this. if we can always or usually expect to truncate upon startup, then perhaps the benefit is low and we need to revisit periodic pid expiration to reduce the chance that we have to scan the last segment upon initialization.",0,0.9499669671058655
299591718,2910,junrao,2017-05-05T22:40:11Z,: thanks for the patch. lgtm.,1,0.9669569730758667
299591871,2910,junrao,2017-05-05T22:41:13Z,i will let you merge this after running the system tests.,0,0.9948325157165527
299648323,2910,hachikuji,2017-05-06T15:49:45Z,thanks for the review cycles. system tests are looking good: [a link] i am going to look at a couple of the build failures above before merging to see if they could be related.,1,0.9901898503303528
299653658,2910,hachikuji,2017-05-06T17:14:56Z,"it looks like the recent transient failures are known issues: [a link] [a link] [a link] in one of the previous builds, i saw a failure in kafka.server.logrecoverytest.testhwcheckpointwithfailuressinglelogsegment. this could be related to this patch and there is no active jira, so i'm trying to reproduce it locally.",0,0.98723304271698
299658852,2910,hachikuji,2017-05-06T18:48:02Z,i wasn't able to reproduce the failure in `logrecoverytest` locally and it didn't occur in the last few builds. i went ahead and increased the timeout that failed just in case. i'm going to go ahead and merge this to trunk.,0,0.9884838461875916
843289802,10579,satishd,2021-05-18T15:54:18Z,this pr is ready for review. pl let me know your comments.,0,0.8898937106132507
851023730,10579,satishd,2021-05-30T16:12:48Z,thanks for your comment. addressed most of them with the latest commit.,1,0.8898277282714844
859400725,10579,satishd,2021-06-11T08:53:38Z,thanks for the review comments. addressed them with the latest commits.,1,0.8917972445487976
868083433,10579,satishd,2021-06-25T00:01:17Z,thanks for the review. addressed them with the latest commit [a link],1,0.8696068525314331
874000725,10579,satishd,2021-07-05T10:27:20Z,thanks for your comments. addressed them with the latest commit.,1,0.8340033292770386
875750234,10579,satishd,2021-07-07T16:30:24Z,thanks for the comments. pl take a look at my inline replies and the latest commit.,1,0.907971978187561
881556895,10579,satishd,2021-07-16T16:05:44Z,"thanks for your latest comments, replied to them inline.",1,0.7202057242393494
881805037,10579,satishd,2021-07-17T02:51:25Z,thanks for the comment. i addressed it with the latest commit.,1,0.8819029331207275
308242425,3325,bbejeck,2017-06-13T20:42:43Z,"ping for initial review this still need more test coverage, but i wanted to submit the pr to move the discussion forward. edit: the batch restoration benchmark will undergo some refactoring as well. edit round 2: batch restoration benchmark will be in a follow in pr",0,0.9819427132606506
308246447,3325,bbejeck,2017-06-13T20:57:40Z,fixing checkstyle errors,0,0.9920754432678223
308258160,3325,bbejeck,2017-06-13T21:46:00Z,addressing errors,-1,0.9250874519348145
308445355,3325,bbejeck,2017-06-14T14:17:19Z,error seems unrelated to this pr `execution failed for task ':core:compilescala'.` can't reproduce locally either.,0,0.9750738143920898
308464132,3325,bbejeck,2017-06-14T15:16:21Z,retest this please.,0,0.9739659428596497
308476305,3325,enothereska,2017-06-14T15:55:50Z,"i think the restore callbacks etc look good. my main question will be around the bulk loading part, i.e., if we go with batch loading, will we still be able to look into kafka-4868 or are they mutually exclusive? thanks .",1,0.9930642247200012
308483354,3325,bbejeck,2017-06-14T16:20:25Z,current plan is to have kafka-4868 be part of the bulk/batch loading in this pr,0,0.9919405579566956
308511600,3325,bbejeck,2017-06-14T18:02:37Z,retest this please.,0,0.9739659428596497
309618705,3325,bbejeck,2017-06-20T01:31:15Z,- updated code to handle kafka-4868 edit: still owe tests on this pr,0,0.9864383935928345
312245974,3325,bbejeck,2017-06-30T11:41:16Z,still owe tests,0,0.5402585864067078
316110967,3325,bbejeck,2017-07-18T15:58:34Z,updated per comments - added javadoc and tests. cc\,0,0.9830831289291382
316818632,3325,bbejeck,2017-07-20T20:20:50Z,updated per comments \cc,0,0.9902410507202148
317077171,3325,guozhangwang,2017-07-21T18:26:16Z,good point. let's add the check that users can only call this before calling start() then (and hence calling it whatever times are fine then). similarly for `kafkastreams#setstatelistener` and `#setuncaughtexceptionhandler` let's do the same enforcement.,1,0.8370659351348877
317484949,3325,bbejeck,2017-07-24T16:54:05Z,"agreed, but as this pr is large enough i'll make those changes in a follow-up pr.",0,0.9823049306869507
317509633,3325,bbejeck,2017-07-24T18:17:09Z,updates per comments - i believe i've addressed all issues.,0,0.9287193417549133
317530363,3325,guozhangwang,2017-07-24T19:33:12Z,"that is what i meant :) not needed necessarily for this pr, just to make sure we do not forget.",1,0.9933854937553406
318182744,3325,bbejeck,2017-07-26T21:06:23Z,rebasing now,0,0.982344925403595
318187014,3325,bbejeck,2017-07-26T21:24:04Z,rebased and comments addressed,0,0.9792408347129822
318470546,3325,bbejeck,2017-07-27T20:02:09Z,ping for final review and merge,0,0.9664567708969116
318514748,3325,bbejeck,2017-07-27T23:29:43Z,updates per comments,0,0.9529802799224854
318729754,3325,guozhangwang,2017-07-28T18:31:26Z,merged to trunk. thanks !,1,0.9930251836776733
797234253,10218,satishd,2021-03-12T04:56:35Z,"thanks for the review comments, addressed with the latest commits/comments.",1,0.8744174838066101
806359943,10218,satishd,2021-03-25T04:45:09Z,"i discussed the proposed changes in the call on 23rd, and i mentioned that pr is not updated with those changes. i will let you know once those changes are pushed into this pr.",0,0.9916529059410095
812572844,10218,satishd,2021-04-02T15:14:03Z,"thanks for your review and comments. in the latest commit, addressed the review comments. i have also refactored the code with better abstractions.",1,0.9711416363716125
813897020,10218,satishd,2021-04-06T07:32:23Z,thanks for the review. addressed them with the latest [a link].,1,0.8612717986106873
814275837,10218,satishd,2021-04-06T16:54:41Z,i renamed `remote-storage` module to `storage` module as you suggested in the commit [a link].,0,0.9949954748153687
815062947,10218,satishd,2021-04-07T16:44:27Z,"this pr is on top of [a link] . so, [a link] be merged before merging this pr.",0,0.9946130514144897
815513359,10218,satishd,2021-04-08T07:14:14Z,thanks for your comments. replied on comments and addressed them with the [a link] commit.,1,0.9298732876777649
816736436,10218,satishd,2021-04-09T14:47:16Z,thanks for the comments. addressed them with the [a link].,1,0.8492908477783203
817018916,10218,satishd,2021-04-09T23:09:05Z,"thanks for the comment, addressed with the commit [a link].",1,0.7369822859764099
817452410,10218,satishd,2021-04-12T03:24:39Z,it looks like failures are not related to this pr.,0,0.9847990870475769
1732507180,14432,vamossagar12,2023-09-24T07:14:12Z,", do you think it is ok to have this small pr focussed only on static member departure? i plan to create such small prs for other cases when a static member joins etc. plz let me know if you prefer this approach or a single pr with all changes. thanks!",1,0.9891833066940308
1732788357,14432,vamossagar12,2023-09-25T01:58:46Z,test failures seem unrelated.,0,0.8597891926765442
1736982966,14432,vamossagar12,2023-09-27T08:52:46Z,"thanks for the review david! responses inline ack. thanks for the confirmation. sorry about the confusion. i have corrected it. got it. i have reverted to using member epoch -2 for static member leave request. i have updated the member epoch to -2 if the departing worker is a static member. i am not sure if that will be enough though because i am not returning any corresponding results to be written to the topic. all that's going to happen is that we will complete this current request with a result having member epoch id equal to -2. also, the meaning of `member releasing the instance id` is not clear to me.",1,0.9832764863967896
1737732772,14432,vamossagar12,2023-09-27T16:35:14Z,thanks for the review ! i have addressed the comments.,1,0.9794411659240723
1739714081,14432,vamossagar12,2023-09-28T17:09:25Z,"i realised that a logic similar to [a link] is missing in this pr. i think that's important. to get this working though, i would also need to add the logic to add a different map for staticmembers and also support adding a static member to the group. in that case, i might need to enhance this pr to support both leave request and join request.",0,0.8638477921485901
1747026910,14432,vamossagar12,2023-10-04T14:45:31Z,i updated this pr to handle this case and to also handle (re)-join and leaving the group using consumergroupheartbeat api.,0,0.9936886429786682
1747117455,14432,dajac,2023-10-04T15:23:34Z,thanks ! i will get to this pr soon.,1,0.9917961359024048
1747157933,14432,vamossagar12,2023-10-04T15:39:37Z,thank you. i am still coming to terms with the new group coordinator so there might be rough edges in this pr (upfront apologies for that ),1,0.8911274671554565
1757221757,14432,vamossagar12,2023-10-11T09:11:35Z,"thanks , i have addressed the comments.",1,0.7056897878646851
1767770461,14432,vamossagar12,2023-10-18T06:40:07Z,"thanks for the review , most of my changes are on my local, but before i push those, wanted your thoughts on [a link] and [a link] comment. i can proceed with the rest of the changes post that.",1,0.9395232200622559
1767897337,14432,dajac,2023-10-18T07:59:11Z,thanks. i just replied to your questions.,1,0.9658553004264832
1777654947,14432,vamossagar12,2023-10-24T16:59:21Z,"thanks , i have addressed all the comments. have added a couple of more questions. thanks for all the pointers, the changes look cleaner than the previous iteration (to me atleast).",1,0.987245500087738
1795371595,14432,vamossagar12,2023-11-06T16:32:52Z,"thanks , i have addressed the review comments. please let me know how the changes are looking now.",1,0.9180700182914734
1824047001,14432,vamossagar12,2023-11-23T09:19:45Z,"thanks . i have addressed all the comments. the second part of [a link] comment, i am not sure which new data structures being referred here. the test `teststaticmembergetsbackassignmentuponrejoin` asserts all the records returned after replay. is that what you are referring to?",1,0.9410712122917175
1827266684,14432,vamossagar12,2023-11-27T07:29:18Z,", thank you for another round of review. i have handled all review comments.",1,0.9675496220588684
692859976,9100,mumrah,2020-09-15T17:25:32Z,"recent changes include: * controller no longer sends leaderandisr, only updatemetadata (more more leader epoch bump) * update partition's isr from alterisr response * periodic scheduled thread for calling ""propagateisrchanges"" in alterisrmanager (we could miss updates otherwise) * top-level error handling in alterisrmanager * additional checks in alterisrmanager to prevent multiple in-flight requests and also prevent multiple isr changes for a given partition * changes to the callback logic to ensure we don't leave any partitions stuck",0,0.9869606494903564
697953307,9100,mumrah,2020-09-23T20:25:08Z,"yea, good catch. this works today using a zk watch on the partition ""/state"" znode which is still getting triggered with this pr. we can modify the new isr update path to explicitly call `onpartitionreassignment` after writing out the isr. how about we save this as a follow-on?",1,0.791850209236145
698458937,9100,hachikuji,2020-09-24T16:42:08Z,"yeah, i'm ok leaving that for [a link]",0,0.934104859828949
698637507,9100,hachikuji,2020-09-24T23:24:24Z,i think the failing tests are known flakes and should be fixed by [a link],0,0.9864540696144104
699438281,9100,jacky1193610322,2020-09-26T06:27:27Z,"i got a thought, there is a scenario that a leader can see its followers, but cannot see zookeeper, and then the leader will be fenced when it attempts to shink isr or expand isr because it holds the leaderisrupdatelock, but now the leader can't be fenced because it just sends the message and will process message normally. when the ack=1, it will continue processing the msg, and will be lost. should we reject process the msg when the alterisr find the broker is not in livebrokeridandepochs? [a link]",0,0.9461112022399902
729316632,9100,hachikuji,2020-11-18T01:27:41Z,"i missed this comment before. it's a good question. in general, the leader will continue in its current state as long as possible. as you say, as soon as it needs to shrink/expand the isr, it grabs the leaderandisr update and attempts to synchronously update the state. if zookeeper can't be reached, then the thread gets stuck. eventually this causes the broker to effectively deadlock, which has the side effect of preventing any produce requests (and any other requests) from getting through. i think it's a fair point that this affords some protection for acks=1 requests, but i think we tend to view the side effect of deadlocking the broker as worse than any benefit. in kip-500, we have an alternative approach for self-fencing. the analogous case is when the leader cannot reach the controller. we use a heartbeating mechanism to maintain liveness in the cluster. unlike with zookeeper, we do not rely on the session expiration event in order to tell that a broker has been declared dead. instead if we do not get a heartbeat response from the controller before some timeout, then we will stop accepting produce requests. i have been thinking a little bit about your suggestion to self-fence after getting an invalid version error from alterisr. it might help in the interim before kip-500 is complete. i think our expectation here was that if we get an invalid version error, then the leaderandisr with the updated state should soon be on the way. i suppose we could come up with reasons why that assumption might fail, so it might make sense to be a little more defensive. i will file a jira about this and we can see what others think. thanks for the suggestion!",1,0.8809946179389954
729349575,9100,jacky1193610322,2020-11-18T03:15:10Z,"thanks for your reply, i also have read kip-500 and other kip-631, it's good about the fence. but it will be released in a few months, before that, i think we also need to try the best to fence the broker when the controller already think the broker has died. in other words, we should fence 2-way. ` self-fence after getting an invalid version error from alterisr ` yes, i think we need self-fence when the session is lost, we can't rely on receiving the other machines response because we can't receive the response when the broker2controllerchannel is broken. please let me know if you create a jira.",1,0.9697707295417786
276844638,2472,twbecker,2017-02-02T02:00:34Z,good to see some progress on kip-4! one question - does this allow new topics to be created with the cluster defaults for number of partitions and replication factor? having to have a priori knowledge of those configs on the client is a big pain point for us using adminutils and i thought the elimination of that requirement was a goal.,1,0.9876130223274231
277339758,2472,cmccabe,2017-02-03T19:30:02Z,"hi , thanks for taking a look! be sure to also check out the discussion thread here: [a link] unfortunately, as far as i know, there is no way to allow the server to use a default replication factor when making a createtopicsrequest. the rpc must have either a manual partitioning assignment, or a number of replicas and number of partitions specified. i think supporting server-side defaults would be a nice improvement, but it would require a separate kip to implement.",1,0.9906099438667297
298144387,2472,ijuma,2017-04-29T03:37:04Z,3 test failures: [code block],0,0.9847436547279358
298358441,2472,ijuma,2017-05-01T15:46:14Z,"i fixed the test failure and some trivial clean-ups: [a link] system tests passed: [a link] i think we can merge this after the pr above is merged. i will file a jira with additional clean-ups that i noticed, but those can be done later.",0,0.9789156317710876
298366874,2472,cmccabe,2017-05-01T16:25:50Z,"thanks, . the cleanups look good to me.",1,0.9859752058982849
594313472,8218,abbccdda,2020-03-04T03:58:30Z,could we have a summary for this pr?,0,0.9945647716522217
594314018,8218,mjsax,2020-03-04T04:00:38Z,"the summary is not longer than the pr title. we make `taskmanager` the class that is responsible for committing (to allow to commit all tasks at once, instead of each task individually)",0,0.9950429201126099
594706982,8218,abbccdda,2020-03-04T17:56:26Z,"the title sounds pretty vague to me. the description could at least include what the committing behavior look like under `taskmanager`, what's the motivation, etc as we are already overloading jira 9441. in general we could try to be more specific about the changes in the pr description as i could see you are also adding upgrade flags inside this pr. major side effects should be better to document at first imho.",0,0.7723603844642639
598504502,8218,guozhangwang,2020-03-13T01:28:09Z,thanks for the rebasing ! will take another look soon.,1,0.9816675186157227
599368368,8218,mjsax,2020-03-16T06:36:14Z,"rebased to resolve merge conflicts. addressed more review comments and added more tests. also added a fix (from [a link] as `trunk` is broken atm, to make jenkins pass. triggered systems tests: [a link]",0,0.9920929074287415
599779177,8218,ableegoldman,2020-03-16T22:06:19Z,"i linked the system test failures i saw on the meeting notes for this week, but just to clarify: 1) two failing consistently: `streamseostest.test_failure_and_recovery{_complex}` 2) one flaky: `streamsstandbytask.test_standby_tasks_rebalance` anything outside of that is a new failure",0,0.9891629219055176
599794804,8218,mjsax,2020-03-16T22:57:28Z,"thanks for pointing out. in the run i triggered, the following failed: - `streamscooperativerebalanceupgradetest.test_upgrade_to_cooperative_rebalance` - `streamsupgradetest.test_metadata_upgrade` - `streamseostest.test_failure_and_recovery` - `streamseostest.test_failure_and_recovery_complex` - `streamseostest.test_rebalance_complex` - `streamseostest.test_rebalance_simple` not sure if this is any helpful to have a safety net for this pr...",1,0.9627500772476196
599816765,8218,mjsax,2020-03-17T00:23:00Z,address review comments and rebased to pick up fixed from `trunk`.,0,0.9957412481307983
600345056,8218,mjsax,2020-03-17T23:14:32Z,addressed review comments and fix bug exposed by system tests. also added more tests. new system test run: [a link],0,0.9681816101074219
600402163,8218,guozhangwang,2020-03-18T03:10:24Z,lgtm. please feel free to merge after green builds.,0,0.9878312945365906
600417507,8218,abbccdda,2020-03-18T04:22:27Z,the failed test seems to be just flaky org.apache.kafka.streams.processor.internals.statedirectorytest.shouldreturnemptyarrayiflistfilesreturnsnull,0,0.9670639038085938
600422344,8218,mjsax,2020-03-18T04:45:14Z,"actually the same test failed in both previous runs -- but it passes locally for me. as this pr has nothing to do with the failing test, i might just want go ahead and merge. the last pr that changed the failing test was merged recently: [a link] there is one concerning comment on that pr: `merged to trunk and 2.5 after tests green locally.` ? seems the nightly jenkins job also fails on this test: - - `trunk` -> [a link] - `2.5` -> [a link] did this pr break the jenkins job? \cc",0,0.9749294519424438
600424167,8218,abbccdda,2020-03-18T04:53:45Z,"my understanding is that this pr will fix the problem [a link] you could choose to merge it first if looks good, and rebase the current one",0,0.9904235005378723
600432582,8218,guozhangwang,2020-03-18T05:30:17Z,i just double checked and the failure in trunk is fixed as [a link] while the failure (a different one) in 2.5 is also fixed by 's pr. we should be fine now.,0,0.8313895463943481
600432616,8218,guozhangwang,2020-03-18T05:30:25Z,test this please,0,0.9870431423187256
600799765,8218,mjsax,2020-03-18T18:45:38Z,system test failure `streamseostest` (4 different test as mentioned by sophie above). given that some more fixed got pushed to `trunk` i rebased. this should give us a green jenkins and we should be able to merge.,0,0.9804850816726685
600815165,8218,ableegoldman,2020-03-18T19:21:58Z,only two `streamseostest` system tests were failing when i ran them. `test_rebalance_complex` and `test_rebalance_simple` are either newly failing or due to this pr,0,0.9904283285140991
600815624,8218,ableegoldman,2020-03-18T19:23:03Z,"also, the two already-failing tests (`test_failure_and_recovery`) failed with a different error than they were trunk: [code block]",0,0.9927856922149658
601059160,8218,mjsax,2020-03-19T08:49:47Z,thanks for pointing it out! pushed a fix and updated the unit test accordingly. \cc new system test run: [a link],1,0.985331654548645
601318837,8218,mjsax,2020-03-19T17:38:40Z,`streamseostest.test_failure_and_recovery` and `streamseostest.test_failure_and_recovery_complex` failed. i only see: [code block] followed by [code block] seems to be unrelated to this pr. are we good to merge?,0,0.9876420497894287
601320091,8218,abbccdda,2020-03-19T17:41:17Z,we should be good. i have another pr to fix those [a link],0,0.5943370461463928
198455269,1095,granthenke,2016-03-18T17:03:11Z,"it would be nice to get this into 0.10. it should be the only required changes for existing protocol messages for kip-4, which would simplify a backport into 0.10.1 when the rest is agreed upon and ready. it also adds rack data to the metadata response which goes along with kip-36 that was added in this release.",1,0.5197836756706238
198461416,1095,ijuma,2016-03-18T17:23:11Z,", i agree that it would be great to get this in if there is agreement on the protocol changes. i haven't reviewed it in detail yet, but 2 things: - the pr needs to be rebased - i think it would be great to make it possible to ask for an update with `0` topics in cases where we just want to get the updated cluster data. people have run into performance issues when clients have 0 topics, but they ask for _all_ topics because the protocol only supports that.",1,0.7347226738929749
198463871,1095,granthenke,2016-03-18T17:32:10Z,"i will rebase and look at supporting the idea of `null` (versus empty list) indicating that you don't want to request topic metadata at all. its a bit nuanced, but its the most compatible and has fewer edge cases. worst case scenario if someone passes an empty list is performance. this would help solve [a link].",0,0.9857416152954102
198519711,1095,granthenke,2016-03-18T20:03:16Z,"i rebased and added the ""no topics"" functionality.",0,0.9928472638130188
198557490,1095,gwenshap,2016-03-18T21:49:35Z,"sorry, noticed a bit late that this pr includes protocol changes that didn't go through a community discussion. lets make sure there are 3 committers who are ok with the protocol modification?",-1,0.9792112708091736
208938334,1095,granthenke,2016-04-12T14:38:04Z,this patch represents the current vote thread for the kip-4 metadata changes. please feel free to review at your convenience.,0,0.9857418537139893
212985925,1095,granthenke,2016-04-21T15:57:33Z,pinging for review. the kip-4 metadata changes vote passed and i rebased on trunk.,0,0.9913777112960815
213592590,1095,ijuma,2016-04-22T21:21:30Z,"thanks for the pr. i left a few minor comments, looks good otherwise. i think one thing worth paying special attention to is the `errorunavailableendpoints` change, but it seems like gwen has already done that.",1,0.9868033528327942
214567207,1095,granthenke,2016-04-25T23:59:32Z,"i updated the patch based on the reviews. i added more tests, and some static factory methods. most notably i changed from using and alltopics boolean to using null and a static method. it required a little bit of re-work on internal apis but looks reasonable to me. if you think the alltopics boolean is better don't hesitate to weigh in. the specific commit with that change is here: [a link]",1,0.656021773815155
214889420,1095,ijuma,2016-04-26T21:16:56Z,"thanks . aside from a few minor comments, lgtm. i started a system tests build here: [a link]",1,0.9753129482269287
214915320,1095,granthenke,2016-04-26T23:11:13Z,i updated the patch based on your feedback,0,0.9927446842193604
214920024,1095,ijuma,2016-04-26T23:36:47Z,"thanks , lgtm. , do you want to take a look as well?",1,0.7982397079467773
214924898,1095,gwenshap,2016-04-27T00:02:35Z,looks perfect. i was waiting for you to finish nitpicking tests ;),1,0.9957881569862366
548594725,7629,bbejeck,2019-10-31T22:30:27Z,ping and for review,0,0.9892039895057678
548594856,7629,bbejeck,2019-10-31T22:31:01Z,"i still need to test this out with a local deployment of the docs, but i wanted to get feedback on the content sooner.",0,0.9796810150146484
548596297,7629,bbejeck,2019-10-31T22:36:54Z,\cc and,0,0.9893174767494202
548596456,7629,bbejeck,2019-10-31T22:37:35Z,i'll also do a follow-up pr to `kafka-site` for the `2.4` folder if necessary.,0,0.9945275187492371
549929236,7629,bbejeck,2019-11-05T17:34:21Z,rebased and updated per comments,0,0.9893052577972412
552029806,7629,bbejeck,2019-11-08T23:24:27Z,updated per comments,0,0.9704824686050415
553465499,7629,bbejeck,2019-11-13T15:55:13Z,merged #7629 into trunk.,0,0.993033766746521
622095405,8589,mjsax,2020-04-30T20:34:34Z,retest this please. call for review,0,0.9901093244552612
622372958,8589,feyman2016,2020-05-01T12:43:42Z,"oh, sorry i didn't run the checkstyle and spotbugs quality checks locally, i will update shortly with these fixed",-1,0.9911350011825562
622417550,8589,feyman2016,2020-05-01T14:47:11Z,retest this please.,0,0.9739659428596497
622614438,8589,mjsax,2020-05-01T23:52:46Z,only committers can trigger jenkins retesting... retest this please.,0,0.9503278732299805
622661261,8589,feyman2016,2020-05-02T03:20:44Z,"i see, thanks! and call for review :)",1,0.9913812279701233
624403133,8589,feyman2016,2020-05-06T01:55:23Z,"thanks a lot for the review, will update soon.",1,0.9374046921730042
625008508,8589,feyman2016,2020-05-07T03:29:40Z,"hey, updated based on comments, and also left some comments there, thanks.",1,0.9301014542579651
626141315,8589,feyman2016,2020-05-09T10:03:20Z,"call for retest and review, thanks!",1,0.9352578520774841
627772018,8589,abbccdda,2020-05-13T06:21:20Z,sure thing!,0,0.8992865681648254
629055884,8589,feyman2016,2020-05-15T06:32:14Z,sorry for causing too much trouble about style....will make sure style check correctly executed before requesting for review next time.,-1,0.9913649559020996
629591570,8589,abbccdda,2020-05-16T05:30:05Z,no worry! it's just for first time :),1,0.9946228265762329
630714942,8589,feyman2016,2020-05-19T09:51:54Z,"i'm fixing the style error, and i found that it showed success if i run the below cmd locally : ` ./gradlew checkstylemain checkstyletest spotbugsmain spotbugstest spotbugsscoverage compiletestjava ` i'm didn't find the reason after some investigating, but the style check cmdline can capture style error if i cherry-pick the first commit of this pr to another branch, guessing that it might be the second commit: `merge trunk` somehow made the style check doesn't work... so would it break our convention if i revert to the first commit:`add option to force delete active members in streamsresetter` and fix style error from there ? this may change the commit history of this pr. i'm asking because i noticed :""please address feedback via additional commits instead of amending existing commits."" on [a link] thanks!",0,0.7735111713409424
630918452,8589,abbccdda,2020-05-19T16:03:34Z,"thanks for the context. i don't worry too much for comment vanish if you change the commit history, as they would not be gone but just show as `outdated` on github. just do whatever you feel makes sense. also just a reminder that we are one week away from the feature freeze, so let's try to ramp up and get this into 2.6.",1,0.9639580845832825
631299671,8589,feyman2016,2020-05-20T07:44:46Z,"thanks, will try my best to get this into 2.6",1,0.9094132781028748
632027791,8589,feyman2016,2020-05-21T11:12:04Z,"updated, call for review, thanks!",1,0.9656422734260559
632246270,8589,feyman2016,2020-05-21T17:41:00Z,"thanks so much for the timely and detailed comments, i will update soon.",1,0.9858705401420593
632722266,8589,feyman2016,2020-05-22T14:30:00Z,"updated and req for review agian.. really appreciated your help to pick out so much style violations, also wondering if we can use some format tool like `scalafmt` to automatic format~",1,0.992735743522644
632974316,8589,feyman2016,2020-05-23T02:48:20Z,"updated, thanks!",1,0.9716637134552002
632987873,8589,feyman2016,2020-05-23T05:21:22Z,"ahh, sorry, checking, will fix soon",-1,0.9942724704742432
632991430,8589,feyman2016,2020-05-23T06:00:50Z,"just fixed, thanks!",1,0.9858607053756714
633084014,8589,feyman2016,2020-05-23T16:07:34Z,"thanks very much for the review and comments! wonder if it is still possible to get this into 2.6~ also call for retest, thanks!",1,0.9943863153457642
633667548,8589,guozhangwang,2020-05-25T17:51:38Z,test this,0,0.988132655620575
633733992,8589,abbccdda,2020-05-25T22:39:22Z,only known flaky eos tests are failing: [code block],0,0.9704392552375793
634846209,8589,mjsax,2020-05-27T18:09:55Z,retest this please.,0,0.9739659428596497
634846336,8589,mjsax,2020-05-27T18:10:10Z,retest this please.,0,0.9739659428596497
634855359,8589,feyman2016,2020-05-27T18:26:17Z,thanks a lot for the review and tests triggering!,1,0.9831578135490417
634941215,8589,mjsax,2020-05-27T21:09:07Z,java 8: [code block] java 11: [code block] java 14: [code block],0,0.9937356114387512
634941290,8589,mjsax,2020-05-27T21:09:15Z,retest this please.,0,0.9739659428596497
634941557,8589,mjsax,2020-05-27T21:09:41Z,retest this please.,0,0.9739659428596497
634943565,8589,mjsax,2020-05-27T21:13:31Z,retest this please.,0,0.9739659428596497
634944109,8589,mjsax,2020-05-27T21:14:33Z,retest this please.,0,0.9739659428596497
634944577,8589,mjsax,2020-05-27T21:15:23Z,jenkins does not cooperate... will try again later,0,0.9486453533172607
634954721,8589,mjsax,2020-05-27T21:35:42Z,retest this please.,0,0.9739659428596497
635025385,8589,feyman2016,2020-05-28T00:53:44Z,"finally, jenkins cooperated :)",1,0.9612001180648804
635041171,8589,mjsax,2020-05-28T01:41:15Z,thanks for the kip and pr !,1,0.9605314135551453
635041609,8589,abbccdda,2020-05-28T01:42:36Z,"great work, !",1,0.9964998960494995
635293293,8589,feyman2016,2020-05-28T11:55:17Z,"thanks a lot for your kindly review and help! are you guys using some formatting tool, i asked this because this pr had too many formatting issues, would be good if next time i could effectively avoid them, thanks!",1,0.9950072765350342
635335273,8589,feyman2016,2020-05-28T13:01:08Z,"fyi, since we took a slightly different implementation(leveraging the empty members rather than introducing a new field to imply the `removeall` scenario), i updated the kip-571 accordingly to keep them consistent.",0,0.9925056099891663
635616335,8589,mjsax,2020-05-28T21:24:49Z,"i use intellij that does have some auto-formatting. use it basically with default settings. in doubt, disable auto-formatting to avoid unnecessary reformatting. -- some things are also a little bit of ""personal taste""... thanks! can you send a follow up email to the voting thread explaining the change? to make sure nobody has concerns about it.",1,0.9705939888954163
636274254,8589,feyman2016,2020-05-30T04:30:43Z,"sure, updated in the voting thread of kip-571",0,0.9918389916419983
402306389,5322,guozhangwang,2018-07-03T22:08:58Z,"another class that have some part of the logic is `defaultpartitiongrouper`, and in that impl we should not expect any [code block] any more. sorry the whole task assignment logic was scattered across multiple classes / functions, we just need to make sure we would clean up all the corner cases and do not introduce any regressions. for that we'd better add some integration test for this case as well.",0,0.8988016843795776
402316622,5322,tedyu,2018-07-03T23:06:59Z,thanks for the comments. i am trying to accommodate meta comment first. please take a look at current form to see if i removed code which wouldn't be executed with the early check in place.,1,0.951912522315979
402335251,5322,tedyu,2018-07-04T01:20:23Z,"currently i am looking at the following code (discovered thru shouldupdateclustermetadataandhostinfoonassignment) in onassignment(): [code block] to make the above check pass, receivedassignmentmetadataversion is downgraded to 3 if receivedassignmentmetadataversion is 4 and there is no error in the assignmentinfo.",0,0.9940394163131714
402379446,5322,tedyu,2018-07-04T06:42:21Z,defaultpartitiongroupertest#shouldnotcreateanytasksbecauseonetopichasunknownpartitions still fails. it tests against partitiongrouper where there is no relevant logic after the pr.,0,0.9912531971931458
403171070,5322,guozhangwang,2018-07-06T23:40:48Z,"about the version control semantics, please read kip-268 ([a link] for the detailed explanation. as for this scenario, if `i` am using the latest kafka version (2.1.0-snapshot) then `usedsubscriptionmetadataversion = subscriptioninfo.latest_supported_version` should be 4. in your pr you've only bumped up the latest_supported_version for assignmentinfo to 4, but not `subscriptioninfo.latest_supported_version`, which is why it's failing.",0,0.9919107556343079
403172175,5322,tedyu,2018-07-06T23:51:04Z,"when calling completeshutdown in streamsthread, what should be the value for cleanrun ? [code block] i assume it should be false.",0,0.9934685230255127
403173358,5322,guozhangwang,2018-07-07T00:01:55Z,"we can just call `shutdown()` which will simply change the state to `pending_shutdown`, then in the streamthread's main loop `while (isrunning()) {` will exit and continue to `completeshutdown`.",0,0.9932692646980286
403279407,5322,tedyu,2018-07-08T10:55:51Z,when i used the following command line: [code block] i was told: [code block] also tried the following command: [code block] [code block],0,0.9936332702636719
403366866,5322,tedyu,2018-07-09T05:49:22Z,"in the test output, e.g. [a link] , i don't see detailed log. how can i enable -i for the integration tests ?",0,0.9889700412750244
403634460,5322,guozhangwang,2018-07-09T22:01:28Z,i think this test should be modified as we should not expect this case any more.,0,0.9755034446716309
403634927,5322,guozhangwang,2018-07-09T22:03:27Z,"try replace `streams:test` with `streams:streams-scala:test`, note that the package hierarchy is defined as sub-scope in `build.gradle`.",0,0.9942702054977417
403635159,5322,guozhangwang,2018-07-09T22:04:23Z,"you need to run your branch with log4j.properties updated, but i would not recommend that. i think running locally on a single test case that was failed is more effective.",0,0.9888513684272766
403742335,5322,tedyu,2018-07-10T08:27:14Z,i observed the following pattern for the timed out test testshouldcountclicksperregion : [code block] the above repeats till the timeout. here is how the additional log is added around line 495: [code block],0,0.9936022162437439
403753271,5322,tedyu,2018-07-10T09:04:28Z,the timeout was caused by rebalancing (streams/streams-scala/logs/kafka-streams-scala.log): {code} 59150 [stream-table-join-scala-integration-test-0544d201-4cfa-41d9-846c-bd8efb7e94d6-streamthread-1] info org.apache.kafka.streams.processor.internals.streamthread - stream- thread [stream-table-join-scala-integration-test-0544d201-4cfa-41d9-846c-bd8efb7e94d6-streamthread-1] version probing detected. triggering new rebalance. {code} which is not observed in the output of successful run.,0,0.994971752166748
403757169,5322,tedyu,2018-07-10T09:18:07Z,"from streamthread: [code block] i added the following in streamspartitionassignor : [code block] in streams/streams-scala/logs/kafka-streams-scala.log , receivedassignmentmetadataversion was 3 while usedsubscriptionmetadataversion was 4.",0,0.9937082529067993
403768210,5322,tedyu,2018-07-10T09:57:09Z,"i looked at related code in streamspartitionassignor and assignmentinfo - it seems receivedassignmentmetadataversion was 3 because [code block] usedversion was 3, decoded from subscriptioninfo",0,0.9926334023475647
403807709,5322,tedyu,2018-07-10T12:41:00Z,reassignpartitionsclustertest.shouldexecutethrottledreassignment was not related to the pr. i ran it locally with my changes and it passed.,0,0.9937173128128052
405027255,5322,tedyu,2018-07-14T14:32:02Z,"if i make the following change in streamtotablejoinscalaintegrationtestimplicitserdes.testshouldcountclicksperregion : [code block] in kafka-streams-scala.log , i observe: [code block] the test fails with: [code block]",0,0.9917863011360168
405285036,5322,tedyu,2018-07-16T15:24:26Z,"by registering kafkastreams.statelistener, i was able to see the following transitions: [code block]",0,0.9934566617012024
405289444,5322,tedyu,2018-07-16T15:37:27Z,"by registering streamthread.statelistener, i was able to see the following transitions: [code block]",0,0.9943234920501709
405577056,5322,tedyu,2018-07-17T13:13:50Z,i ran the two failed tests locally with my pr - they passed.,0,0.9913015961647034
405640617,5322,guozhangwang,2018-07-17T16:16:59Z,re-triggered system test [a link] for the newest changes.,0,0.9953586459159851
405725260,5322,guozhangwang,2018-07-17T20:57:05Z,"system test seems relevant, you can go to for more detailed logs: [a link] [a link] the failed one is this: [code block]",0,0.9942492246627808
405734184,5322,tedyu,2018-07-17T21:29:06Z,[a link] gave me 404. there is no clickable link on report.html related to the failed test. the test failure probably is related to the new way of passing version probing parameter. should i restore the existing approach ?,0,0.9792527556419373
405737220,5322,guozhangwang,2018-07-17T21:40:35Z,"if you click on `detail` link (the last column) on the `streamsupgradetest` row (it is the only row in orange, meaning failed case), you'll be able to download the logs",0,0.9928112626075745
405740804,5322,tedyu,2018-07-17T21:54:16Z,the following timed out: [code block] trying to see why.,0,0.9808357954025269
405743077,5322,tedyu,2018-07-17T22:02:43Z,"this log, from output of streamsupgradetestjobrunnerservice-0-140702392667856/worker4, is probably the reason: [code block] meaning the actual version numbers are bumped from existing values.",0,0.9921769499778748
405743406,5322,tedyu,2018-07-17T22:04:09Z,i can bump the hardcoded version numbers in streams_upgrade_test.py is there better way of modifying streams_upgrade_test.py ?,0,0.9942646622657776
405749066,5322,guozhangwang,2018-07-17T22:29:37Z,"could you share some thoughts here? my understanding is that `test_version_probing_upgrade` is testing a ""future"" version which is relying on `streamsupgradetest.java` code, but am not 100% percent sure why bumping up the version to 4 now would break this test.",0,0.9790542125701904
405750218,5322,tedyu,2018-07-17T22:35:01Z,"there're many streamsupgradetest.java in the codebase e.g. [code block] from [a link] , it was not immediately obvious which class led to the failure.",0,0.9883949756622314
405752440,5322,tedyu,2018-07-17T22:46:22Z,from 46/streamsupgradetestjobrunnerservice-2-140702392665808/worker6 [code block],0,0.9942574501037598
405753197,5322,tedyu,2018-07-17T22:50:04Z,mind triggering another system test run ? thanks,1,0.5143125057220459
405758276,5322,mjsax,2018-07-17T23:18:29Z,"please make also sure, that we extend all existing test with regard to version probing, and supported assignment, subscription numbers (ie, `assignmentinfotest`, `subscriptioninfotest`, `streamspartitionassignortest`)",0,0.9942323565483093
405759633,5322,tedyu,2018-07-17T23:26:11Z,i have gone over the listed test classes above and seen new tests added.,0,0.9934096932411194
405794452,5322,tedyu,2018-07-18T03:06:33Z,"from guozhang 11 days ago: we can just call shutdown() which will simply change the state to pending_shutdown, then in the streamthread's main loop [code block] will exit and continue to completeshutdown.",0,0.9927181005477905
405999388,5322,tedyu,2018-07-18T16:48:20Z,can you trigger another system test run ? thanks,1,0.8160760998725891
406054511,5322,guozhangwang,2018-07-18T19:56:23Z,re-triggered [a link],0,0.9951795339584351
406090972,5322,guozhangwang,2018-07-18T22:13:34Z,it still fails: [a link] logs to download: [a link],0,0.9881382584571838
406095151,5322,tedyu,2018-07-18T22:34:06Z,from 46/streamsupgradetestjobrunnerservice-0-139790769699792/worker4 [code block] i will update streams_upgrade_test.py covering missed version numbers.,0,0.9952954649925232
406096393,5322,tedyu,2018-07-18T22:40:33Z,please trigger one more system test run. thanks,1,0.6903010010719299
406098237,5322,guozhangwang,2018-07-18T22:50:14Z,done: [a link],0,0.9949890971183777
406123254,5322,guozhangwang,2018-07-19T01:15:55Z,[a link] has passed.,0,0.9953981041908264
406155515,5322,tedyu,2018-07-19T05:01:34Z,from the scala 2.12 test run console log: [code block],0,0.9951077699661255
406403161,5322,tedyu,2018-07-19T20:25:19Z,"w.r.t. integration test, is it okay to leave that to a follow-on pr (the functionality has been verified in the current integration test) ? i will add hashcode() and equals() in the next update.",0,0.9952608942985535
406433536,5322,guozhangwang,2018-07-19T22:27:52Z,"merged to trunk. while working on fixing forward the test migration, i realized i would need more time, and during which new commits may got it to veto this one, so i've decided to work on it in a separate pr.",0,0.9704682230949402
406448483,5322,mjsax,2018-07-19T23:56:23Z,thanks for the pr !,1,0.9210616946220398
406449043,5322,tedyu,2018-07-20T00:00:17Z,thanks for your detailed review.,1,0.6751462817192078
1474405568,13391,jolshan,2023-03-17T21:09:38Z,todo: ~* config for the feature~ * request handling optimizations + bookeeping ~* confirm compatibility --> we may need ibp~,0,0.9903676509857178
1504291314,13391,jolshan,2023-04-11T23:58:12Z,fyi i found this: [a link],0,0.992289125919342
1504315306,13391,jolshan,2023-04-12T00:16:02Z,^ seems like this still may be initially caused by my change so i'm investigating.,0,0.639552652835846
1505528988,13391,jolshan,2023-04-12T15:56:31Z,i realized i didn't push sorry :( just pushed now. i will also look at that failure some more today.,-1,0.9960036873817444
1505555154,13391,jolshan,2023-04-12T16:12:13Z,i found the issue. i will fix. [code block],0,0.9698599576950073
1505622263,13391,jolshan,2023-04-12T17:04:03Z,found the issue -- we didn't correctly handle disconnects which would cause npes and force close the producer. pushed the code to handle disconnects -- locally i did not see the issue after running 40 times (typically would see it in the first two runs before the fix),0,0.989267885684967
1506123475,13391,jolshan,2023-04-13T00:07:39Z,here are errors on the latest build on trunk i could find: [a link] seems to roughly correlate with the failures i see. i think the only suspicious one is [a link] which was failing before without my fix. this error is slightly different and related to endtxn. i did see this flake one time when i was repeatedly testing on my branch. i can look on trunk as well.,0,0.9649534821510315
213117738,1251,SinghAsDev,2016-04-21T21:17:19Z,mind reviewing this?,0,0.9670141339302063
215311447,1251,SinghAsDev,2016-04-28T05:09:48Z,rebased on trunk.,0,0.9928845167160034
215454261,1251,granthenke,2016-04-28T15:00:37Z,i think we should have some tests for this functionality.,0,0.9833779335021973
215487593,1251,gwenshap,2016-04-28T16:34:56Z,: mind explaining why you chose to plug in the validation where you did? both why do you think a new state is needed and why do you think handleconnection() (which until now just did some metrics) is the correct place for the validation?,0,0.9910596013069153
215489877,1251,gwenshap,2016-04-28T16:43:01Z,": another design level question: it looks like you are validation every response. but - brokers can't respond in anything other than the request version, which the client provides. i thought we discussed only validating the broker apis when connecting. what made you change your mind?",0,0.9455106258392334
215490039,1251,gwenshap,2016-04-28T16:43:32Z,i'm pausing the line-level review until i have more clarity on the design decisions / scope.,0,0.9723310470581055
220498557,1251,SinghAsDev,2016-05-20T01:55:18Z,"thanks for the review. below are the goals. - allow clients to not utilize kip-35. - allow clients interested in kip-35 to be able to check if all api versions it needs is supported by a broker. - api version check has to be done for each new connection, even for re-connects. below is the design overview. - for each new connection established, if client, i.e., `kafkaproducer`/ `kafkaconsumer`, has specified api versions it will be using, send `apiversions` request. - when `apiversions` response is received, client validates that client's requested api versions are supported by broker. if not, throw `kafkaexception`. - whether a connection is ready to send requests is determined as below. - if client does not want to check api versions, i.e, client has not specified required api versions, the connection must be in `connected` state. - if client wants to check api versions, i.e., client has specified required api versions, the connection must be in `ready` state. valid connection states transitions. - client does not want to perform api versions check. note that this is what it is right now. `disconnected` -> `connecting` -> `connected` - client wants to perform api versions check. added with this patch. `disconnected` -> `connecting` -> `connected` -> `checking_api_versions` -> `ready` a new state is needed as a connection can be ready to send requests on either in `connected` or in `ready` state, based on client's need to check api versions. as the api versions check is done for each connection, it is best to do it when we have a connection established. if check passes transition it to `ready` state that indicates successful api versions check. if the check fails, the connection is closed and an `kafkaexception` is thrown. i don't think every response is getting validated, only `apiversion` response will be validated. `apiversion` request is only sent while connecting.",1,0.9468713402748108
225725972,1251,SinghAsDev,2016-06-13T22:19:15Z,does the above explanation answer your question?,0,0.9925386309623718
227901779,1251,SinghAsDev,2016-06-22T22:54:43Z,would you mind reviewing this?,0,0.989207923412323
227978150,1251,ijuma,2016-06-23T08:02:55Z,", i left some initial comments, i didn't do a detailed review. personally, i think it's really important to have system tests for this feature to make sure it works as expected. some of the system tests can be written now (for brokers 0.8.2.x and 0.9.0.x), but we probably can only write the ones for 0.10.0.0 once trunk has bumped the version of request types used by the producer and consumer. testing at the `networkclient` level is not really enough as we want to make sure the errors are propagated all the way to the user.",0,0.8869979381561279
228110393,1251,SinghAsDev,2016-06-23T16:45:11Z,"i am planning on adding system tests for it. however, i want to get some feedback on overall approach. though i had outlined the same approach as a response to jay's question on kip-35 discuss list, it wasn't discussed in great details. gwen too had some questions on overall approach, to which i have posted answers earlier. i have discussed the approach with though, but review from a committer who can eventually help to commit this will help. meanwhile, i can work on system tests.",0,0.6567157506942749
228185438,1251,ijuma,2016-06-23T21:12:40Z,"yes, it makes sense to get agreement on the overall approach before spending more time on this. i am a bit undecided on this one. it seems a bit wasteful to make an additional request to check the versions to simply fail if they don't match. the broker should really be returning an error when an unsupported request arrives. what would be really cool is making the java client support multiple broker versions. but that would require a discussion in the mailing list and it would be a bigger change.",0,0.8586824536323547
228204101,1251,SinghAsDev,2016-06-23T22:25:03Z,"this was discussed and agreed upon as part of kip-35. below is an excerpt. i think it has value to know that clients are not compatible with brokers it is trying to talk to, rather than getting connection dropped or similar not so sure response. this was proposed in kip-35 and was shot down. we should definitely give it a try again, but as you said this will be a larger and lengthier discussion. getting everyone on same page when it comes to compatibility definition, has not been so easy :).",1,0.9857527613639832
228292359,1251,ijuma,2016-06-24T08:51:47Z,"yes, i am aware of kip-35 discussions and outcome. all those discussions happened with 0.10.0 in mind, which has now been released and this part of the kip didn't make it into that release. part of the motivation for this part was to ensure that apiversions request worked properly. that's a weaker argument now since we have already released it and we relied on other clients to test it for us. i agree that there is value in knowing that clients are not compatible with brokers (which was the main motivation). however, we are only doing it this way because we don't have a way to return a generic error on any request, which is a bit of a shame. on the topic of supporting multiple broker versions, yes, kip-35 wasn't the right place for that. however, now that kip-35 is in, it could be considered again. it's weird that the java clients don't support this when third-party clients do. if we did this, there would be less of a need to backport client fixes to older versions (which people often can't upgrade because of the brokers). anyway, it would be interesting to think if the approach we are doing now could be extended in that direction. it just occurred to me that we are not handling the sasl case correctly. for that case, we need to send the `apiversionsrequest` before the `saslhandshakerequest` (see `saslclientauthenticator`). right? finally, i don't mean to block this from going in, i was just sharing my thoughts on the subject. since there is value and it was approved as part of kip-35, as long as we can ensure that we don't break existing behaviour and that the new functionality works correctly, it seems like it can go in.",0,0.9418096542358398
228835703,1251,SinghAsDev,2016-06-27T18:38:15Z,"i agree with all you said. however, one of the issues i keep running into is that the longer a kip or a pr remains open the more times its intent changes. could we agree on adhering to what we decided on kip-35 scope and let this go in, sure i will work on fixing any issues with it. this will make sure that we will have at least this basic check in java clients in the next release. i have plans on using kip-35's support to make rolling upgrade easier and along that we can initiate thoughts around adding support for multiple brokers. i am sure that it is going to take some time, however we definitely will have more insight as third party clients would have some working examples for us to point at. if you agree, then i can start looking into sasl issues you pointed out and adding tests. will wait for your response.",1,0.5962759256362915
230643285,1251,ijuma,2016-07-06T00:43:50Z,", i checked with jun and he's happy for this to go in as long as it's not too complicated. perhaps the best thing is to first look at the sasl case to see how we handle that before spending effort on anything else?",1,0.8830363750457764
232095144,1251,SinghAsDev,2016-07-12T16:03:12Z,"sounds good . i looked into sasl scenario and it looks like sasl is a bit off than other scenarios where we handle `apiversionrequest` in `saslserverauthenticator` itself, which means if a client wants to get api versions before sasl authentication, it is allowed. i think in kip-43 we wanted to add this support to let clients know supported versions of `saslhandshakerequest`. do we currently have a reason to do so? maybe will have some thought here. the current changes will work even for sasl scenario, however if we really want to handle multiple versions of `saslhandshakerequest`, we need to get api versions before sasl authentication, and we have following options for that. 1. move down whole api version checking from `networkclient` to `kafkachannel`. have `kafkachannel` maintain supported api versions. clients will access api versions info from the channel. 2. as sasl authentication has a special handling for `apiversionrequest`, only have api versions fetched for the sasl scenario in `authenticator` and put a check in `networkclient` to go to `ready` state from `connected`, skipping `checking_api_versions`. thoughts?",1,0.8867927193641663
232279950,1251,rajinisivaram,2016-07-13T07:40:48Z,"at the moment, there is only one version of `saslhandshakerequest`, but it will still be good to consider how this can be validated as well while you are doing the others. perhaps a separate request from `saslclientauthenticator`to fetch only the supported versions of `saslhandshakerequest` would be neater? that way, you don't need to propagate the list of used apis to the authenticator or the versions back to the networkclient. the rest of the code can stay as is. it would mean an additional `apiversionsrequest` for sasl, but perhaps that is ok. if that is the agreed approach, you could add a comment in `saslclientauthenticator` and perhaps defer implementation until a new version of `saslhandshakerequest` is required. what do you think?",0,0.986850380897522
232622378,1251,ijuma,2016-07-14T10:04:26Z,", your suggestion makes sense to me. since the current saslhandshakerequest will continue to be supported, it is fair to only add the check once the client needs to use a hypothetical new version of saslhandshakerequest (hopefully never).",0,0.7769016027450562
232727718,1251,SinghAsDev,2016-07-14T17:03:52Z,"sounds like we do not have a strong reason to add special handling for sasl case as of now, and as such the current design for api version checking does not have to change. does this take care of your concern for sasl scenario, if it does i will start looking at other review comments.",0,0.9533957839012146
232730951,1251,ijuma,2016-07-14T17:15:37Z,"yes, worth adding a comment as rajini suggested.",0,0.9888924956321716
234559193,1251,SinghAsDev,2016-07-22T14:29:33Z,"this should be good for you to take a look. the two test failures are not reproducible, tried locally multiple times with java 7.",0,0.9433176517486572
236014652,1251,SinghAsDev,2016-07-28T20:26:15Z,one more ping :). let me know if there is still something blocking this from getting merged.,1,0.9826722145080566
266899476,1251,SinghAsDev,2016-12-13T23:51:19Z,"thanks for the review, updated pr.",1,0.8649253249168396
266912696,1251,SinghAsDev,2016-12-14T01:10:38Z,"so i added `latest_0_10` to `apiversionschecktest` and apparently due to kafka-4093, trunk's client won't be able to talk to `0.10.1.0`. not sure if this is known. if not, let me know and i can file a jira.",0,0.9846315383911133
266917584,1251,ijuma,2016-12-14T01:40:33Z,not sure i understand. kafka-4093 was included in 0.10.1.0. why would that prevent trunk clients from talking to 0.10.1.0 brokers?,0,0.7840507626533508
266919305,1251,SinghAsDev,2016-12-14T01:50:53Z,"i meant `0.10.0.1`, that is what `latest_0_10_0` is pointing to.",0,0.9904229640960693
266921743,1251,ijuma,2016-12-14T02:06:31Z,"it's true that trunk can't talk to `latest_0_10_0`, supposedly we detect this via `apiversionsresponse`? and for the `latest_0_10_1` case, it should work atm.",0,0.9931564927101135
266926910,1251,SinghAsDev,2016-12-14T02:42:20Z,"well, as this was supposed to be a step before client compatibility, in current pr api version checks are based on if there is any version overlap between client and broker for an api. however, as the client always sends the latest api version, to be able to catch that i will have to set expected minimum api version to latest version of the api. with that the version mismatch will be caught. however, soon client compatibility work will have to revert it back to what it is right now. i am changing the min expected version to be the latest api version and adding a comment indicating that it needs to be changed to min version when backwards client compatibility is added. makes sense?",0,0.9529194235801697
266927464,1251,ijuma,2016-12-14T02:46:14Z,"if it's a simple change, that sounds good to me. if not, then we can leave as is.",0,0.8605582118034363
267188425,1251,hachikuji,2016-12-14T23:22:29Z,"thanks. left one nitpick, but the latest changes are looking good.",1,0.9814679026603699
267339807,1251,ijuma,2016-12-15T14:26:35Z,started system tests build here: [a link],0,0.9938919544219971
267714157,1251,ijuma,2016-12-16T22:43:59Z,", i created a pr to your branch that addresses the feedback that i think we need to address before this is merged: [a link] the rest can be done as part of the client compatibility kip. if you are happy with the changes, can you please merge them to your branch and then merge trunk to your branch?",0,0.8808004856109619
267714603,1251,SinghAsDev,2016-12-16T22:46:57Z,"thanks for the changes, i was working on addressing them myself. but, glad you commented here, merging your changes.",1,0.9913882613182068
267714875,1251,ijuma,2016-12-16T22:48:36Z,", sorry, i was afraid you may have been busy and wanted to take advantage of the momentum to get this merged. :)",1,0.8098663091659546
267715212,1251,SinghAsDev,2016-12-16T22:50:22Z,"no worries, rather thanks!",1,0.9868183135986328
267716319,1251,ijuma,2016-12-16T22:57:12Z,"looks like i didn't include a local change in my pr to get one of the tests to compile. it should be trivial to fix. also, can you please address the overflow issue? i wasn't sure what the intent was on that one.",0,0.9594684839248657
267725348,1251,ijuma,2016-12-17T00:02:13Z,system tests run: [a link],0,0.9946409463882446
267759785,1251,ijuma,2016-12-17T12:19:04Z,"the system tests failures are also happening in trunk with the exception of the new system test that seems to be failing. because the newly introduced system test will change as part of the client compatibility work, i will remove it and merge this pr.",0,0.9879900813102722
267760134,1251,ijuma,2016-12-17T12:27:35Z,"lgtm, merged to trunk (without the system test as explained in the previous comment).",0,0.9919651746749878
132635211,151,ijuma,2015-08-19T15:16:22Z,tests passed locally.,0,0.9927990436553955
133041845,151,ijuma,2015-08-20T15:01:22Z,", i believe i have addressed everything that was discussed. i also merged trunk into this branch to fix a minor import conflict in a test. tests pass locally. please have a look and if you're happy, this may be good to go.",1,0.9662330150604248
133056252,151,ijuma,2015-08-20T15:44:27Z,closed/reopened to trigger another ci build.,0,0.9932264685630798
133058687,151,ijuma,2015-08-20T15:54:02Z,the output doesn't actually say what (if anything) failed in the last ci build.,0,0.9466656446456909
133070221,151,ijuma,2015-08-20T16:37:59Z,ran the tests locally two more times and they passed both times.,0,0.9922332763671875
133213237,151,gwenshap,2015-08-20T23:48:49Z,"i think this needs a rebase, but otherwise lgtm. , do you want to take a look too?",0,0.9804049134254456
133217955,151,gwenshap,2015-08-20T23:58:37Z,"actually, not good - maybe rebase related - but cloning the pr doesn't pass unit tests on my machine: uncaught exception during compilation: java.lang.stackoverflowerror (scala 2.10.5, java 8)",0,0.7433866262435913
133224996,151,gwenshap,2015-08-21T00:18:07Z,"ok, the stack size issue is intermittent and seem unrelated to the patch (happened to me once or twice on trunk too). my lgtm is back. wanted to take a look, so i'll hold off committing to give him a chance to comment.",0,0.9769655466079712
133226071,151,ijuma,2015-08-21T00:23:27Z,i fixed the conflict (just an import),0,0.9909718036651611
133226348,151,gwenshap,2015-08-21T00:26:29Z,cool. the test failure seems like an unrelated flaky. opened kafka-2455 for it.,1,0.9640233516693115
133227314,151,ijuma,2015-08-21T00:34:13Z,thanks!,1,0.8631753921508789
133377428,151,ijuma,2015-08-21T11:27:10Z,", thanks for the review. learned a couple of new things about the `selector` and metrics. :) i pushed two commits to address two of your comments. the other two i was not sure about so i asked some questions.",1,0.9968026876449585
133491198,151,ijuma,2015-08-21T16:49:03Z,closing this for now.,0,0.9799432158470154
135916524,151,ijuma,2015-08-28T23:52:09Z,tests passed locally.,0,0.9927990436553955
136154088,151,ijuma,2015-08-30T15:31:26Z,tests passed locally and in jenkins once. the second time failed due to an unrelated failure that has been seen in other prs (testmetricsleak).,0,0.992330014705658
136378962,151,ijuma,2015-08-31T13:58:50Z,thanks for the review . i have addressed a number of the issues raised and left comments/questions for the rest.,1,0.9632253050804138
136400954,151,ijuma,2015-08-31T15:09:02Z,"`./gradlew test` passed locally (i ran it 3 times). however, there is one system test failure: test_id: 2015-08-31--001.kafkatest.tests.replication_test.replicationtest.test_hard_bounce status: fail run time: 7 minutes 32.756 seconds [a link] i will investigate this tomorrow (pointers are welcome though).",0,0.9829990267753601
136737500,151,ijuma,2015-09-01T14:20:58Z,", pushed two additional commits to address the two outstanding issues. i implemented things in a slightly different way than what was discussed. i added some comments to the pr explaining. let me know what you think. `gradlew test` passed locally and system tests passed too: [a link]",0,0.8174282908439636
136881820,151,ijuma,2015-09-01T22:30:26Z,pushed a couple of commits addressing review comments.,0,0.9923385381698608
137114214,151,ijuma,2015-09-02T14:58:55Z,", merged trunk and added missed scaladoc/javadoc. `gradlew test` passed locally (as expected since there hasn't been any change that would affect behaviour)",0,0.9833020567893982
137174188,151,ijuma,2015-09-02T17:09:52Z,started a kafka 0.8.2.1 cluster with 3 brokers manually and did a rolling upgrade (with kill -9) to to this branch. used console producer and console consumer as well as describe-topic and things seemed to work as expected.,0,0.950524091720581
137178231,151,junrao,2015-09-02T17:23:22Z,thanks for the latest patch. just a minor comment on the names of alive_brokers. otherwise lgtm.,1,0.9659399390220642
137185985,151,ijuma,2015-09-02T17:47:34Z,renamed to live_brokers as agreed. tests passed locally.,0,0.9863452315330505
299043716,2967,xvrl,2017-05-03T21:40:22Z,"sure, i can add the benchmarks, wasn't aware of the module",0,0.9919677376747131
299595224,2967,xvrl,2017-05-05T23:06:28Z,"adressed all the comments i believe, now just waiting on getting the build to pass. you might be interested in reviewing this part [a link]",1,0.626905083656311
299654668,2967,hachikuji,2017-05-06T17:33:03Z,"probably not necessary to get a full green build if the failures are known issues. on the other hand, if you're just trying to shake out some more transient failures so that we can add jiras, have at it!",0,0.9132388830184937
299658313,2967,xvrl,2017-05-06T18:39:08Z,i give up :grinning_face_with_big_eyes: i think i've have each java/scala combination succeed at least once. also haven't seen any new failures in the last few runs.,1,0.8424261212348938
302507386,2967,xvrl,2017-05-18T18:47:27Z,i have addressed all your comments,0,0.9696050882339478
305035542,2967,ijuma,2017-05-30T23:15:39Z,client system tests build will appear in the following link when it starts: [a link],0,0.995991051197052
305054123,2967,ijuma,2017-05-31T01:16:43Z,clients system tests passed: [a link],0,0.9900038838386536
305054234,2967,ijuma,2017-05-31T01:17:29Z,"lgtm, merging to trunk and 0.11.0.",0,0.987253725528717
288180972,2719,enothereska,2017-03-21T18:48:55Z,"already getting an error from this basic test, that needs to be fixed before this pr goes in. org.apache.kafka.streams.errors.streamsexception: could not create internal topics. at org.apache.kafka.streams.processor.internals.internaltopicmanager.makeready(internaltopicmanager.java:69). looks like this might not be a real error though, since the replication factor of the internal topics is not maintained. adding another broker and re-testing.",0,0.9799235463142395
288384076,2719,enothereska,2017-03-22T12:30:36Z,want to have a look? thanks.,1,0.6913896203041077
288461109,2719,enothereska,2017-03-22T16:41:54Z,"making as wip again since 2 tests are failing and i need to investigate. [2017-03-22 16:25:05,575] error stream-thread [smoketest-b25363c8-5bce-4d72-8c51-8fe0b49715f6-streamthread-1] streams application error during processing: (org.apache.kafka.streams.processor.internals.streamthread) java.lang.illegalstateexception: attempt to send a request to node 1 which is not ready. at org.apache.kafka.clients.networkclient.dosend(networkclient.java:284) at org.apache.kafka.clients.networkclient.send(networkclient.java:265) at org.apache.kafka.streams.processor.internals.streamskafkaclient.sendrequest(streamskafkaclient.java:238) at org.apache.kafka.streams.processor.internals.streamskafkaclient.createtopics(streamskafkaclient.java:170) at org.apache.kafka.streams.processor.internals.internaltopicmanager.makeready(internaltopicmanager.java:63) at org.apache.kafka.streams.processor.internals.streampartitionassignor.preparetopic(streampartitionassignor.java:615) at org.apache.kafka.streams.processor.internals.streampartitionassignor.assign(streampartitionassignor.java:445) at org.apache.kafka.clients.consumer.internals.consumercoordinator.performassignment(consumercoordinator.java:347) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.onjoinleader(abstractcoordinator.java:505) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.access$1100(abstractcoordinator.java:93) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$joingroupresponsehandler.handle(abstractcoordinator.java:455) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$joingroupresponsehandler.handle(abstractcoordinator.java:437) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$coordinatorresponsehandler.onsuccess(abstractcoordinator.java:788) at org.apache.kafka.clients.consumer.internals.abstractcoordinator$coordinatorresponsehandler.onsuccess(abstractcoordinator.java:769) at org.apache.kafka.clients.consumer.internals.requestfuture$1.onsuccess(requestfuture.java:190) at org.apache.kafka.clients.consumer.internals.requestfuture.firesuccess(requestfuture.java:153) at org.apache.kafka.clients.consumer.internals.requestfuture.complete(requestfuture.java:120) at org.apache.kafka.clients.consumer.internals.consumernetworkclient$requestfuturecompletionhandler.firecompletion(consumernetworkclient.java:498) at org.apache.kafka.clients.consumer.internals.consumernetworkclient.firependingcompletedrequests(consumernetworkclient.java:342) at org.apache.kafka.clients.consumer.internals.consumernetworkclient.poll(consumernetworkclient.java:251) at org.apache.kafka.clients.consumer.internals.consumernetworkclient.poll(consumernetworkclient.java:167) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.joingroupifneeded(abstractcoordinator.java:351) at org.apache.kafka.clients.consumer.internals.abstractcoordinator.ensureactivegroup(abstractcoordinator.java:307) at org.apache.kafka.clients.consumer.internals.consumercoordinator.poll(consumercoordinator.java:294) at org.apache.kafka.clients.consumer.kafkaconsumer.pollonce(kafkaconsumer.java:1033) at org.apache.kafka.clients.consumer.kafkaconsumer.poll(kafkaconsumer.java:999) at org.apache.kafka.streams.processor.internals.streamthread.runloop(streamthread.java:542) at org.apache.kafka.streams.processor.internals.streamthread.run(streamthread.java:326)",0,0.9641441106796265
289475648,2719,enothereska,2017-03-27T14:44:56Z,your review is appreciated. this should eventually got to 0.10.2 bug release as well i believe.,1,0.9739348292350769
289514114,2719,enothereska,2017-03-27T16:51:53Z,system tests pass on jenkins too: [a link],0,0.9897437691688538
289601989,2719,enothereska,2017-03-27T22:13:06Z,we wrap exceptions as streams exceptions since the code that calls these functions and retries only understands streams exceptions. i'd argue that's a good thing since that code cannot possibly know all other exceptions in the lower layers.,0,0.9884999394416809
289602348,2719,enothereska,2017-03-27T22:14:48Z,"about `i still think it's better to fail fast and educate users retry creating their apps after the broker is fully up than trying to wait for, say 5 seconds and hopefully it will succeed.` remember that the broker can fail anytime and the user cannot have any guarantee that once a broker is up it won't fail again. in a subsequent jira we can consider unifying the backoff times to match other standard backoff times that clients use.",0,0.9777555465698242
289720444,2719,enothereska,2017-03-28T09:52:58Z,"note: i'll split this pr into multiple prs, one that requires a kip and another that doesn't and is needed for bug fix. cc",0,0.9901536107063293
289919770,2719,enothereska,2017-03-28T22:06:54Z,i've adjusted this pr so it doesn't require a kip. needs re-reviewing though. thanks.,1,0.9534632563591003
290362875,2719,enothereska,2017-03-30T10:01:45Z,one last look please? thanks.,1,0.6824686527252197
290928787,2719,enothereska,2017-04-01T15:55:28Z,[a link] is green,0,0.9941233992576599
291110511,2719,enothereska,2017-04-03T10:54:47Z,can i get a final approve so that or others can check in? also just opened 0.10.2 cherry picked version of this pr. thanks.,1,0.9477980732917786
291204138,2719,enothereska,2017-04-03T16:55:06Z,"env failure, unrelated to pr",0,0.9375841021537781
291277682,2719,enothereska,2017-04-03T21:21:08Z,"this can go in, thanks. also the 0.10.2 version of this: [a link] thanks.",1,0.9795006513595581
857875970,10822,kpatelatwork,2021-06-09T17:07:38Z,"below are some logs when both connector and tasks are restarted. [2021-06-09 12:02:26,049] info [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] received restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1709) [2021-06-09 12:02:26,051] info [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] executing plan to restart connector and 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1137) [2021-06-09 12:02:26,063] debug [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] restarting 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1169) [2021-06-09 12:02:26,114] debug [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] restarted 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} as requested (org.apache.kafka.connect.runtime.distributed.distributedherder:1171) [2021-06-09 12:02:26,114] info [worker clientid=connect-1, groupid=connect-integration-test-connect-cluster] completed plan to restart connector and 4 of 4 tasks for restartrequest{connectorname='simple-source', onlyfailed=false, includetasks=true} (org.apache.kafka.connect.runtime.distributed.distributedherder:1173)",0,0.9920762777328491
859064545,10822,kpatelatwork,2021-06-10T21:02:17Z,passing system tests results ![a link],1,0.9580123424530029
863639444,10822,kpatelatwork,2021-06-18T00:12:45Z,i have taken care of most of the review comments and resolved them. i left review comments with follow up questions as unresolved. if you get some time could you please review and guide me?,1,0.5455381274223328
864493282,10822,kpatelatwork,2021-06-20T03:15:09Z,"fired up local kafka server and kafka connect and was able to curl the api locally. {""name"":""local-file-source"",""connector"":{""state"":""running"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""running"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%  kafka git:(kafka-4793)  curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""restarting"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""restarting"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%  kafka git:(kafka-4793)  curl -xpost [a link]  kafka git:(kafka-4793)  curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""running"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""running"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%  kafka git:(kafka-4793)  curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""running"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""running"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%  kafka git:(kafka-4793)  curl -xpost [a link] {""name"":""local-file-source"",""connector"":{""state"":""restarting"",""worker_id"":""192.168.1.220:8083""},""tasks"":[{""id"":0,""state"":""restarting"",""worker_id"":""192.168.1.220:8083""}],""type"":""source""}%",0,0.9907341003417969
870820269,10822,kpatelatwork,2021-06-29T18:29:01Z,thanks a lot for taking the time and posting the detailed review comments. i was able to resolve all of them except for a few where i left some additional comments. please let me know how you want to proceed with them.,1,0.9853571057319641
870979782,10822,kkonstantine,2021-06-29T23:16:56Z,"fyi, a test failure seems relevant: `testcorsenabled  org.apache.kafka.connect.runtime.rest.restservertest` i don't remember this test being flaky and it failed in both builders.",0,0.9793940782546997
871880686,10822,kpatelatwork,2021-07-01T03:14:34Z,after rebase i do not see the failures,0,0.9767913818359375
2050761892,15640,kirktrue,2024-04-12T00:29:10Z,please review this pr if you have some spare time. thanks!,1,0.9708532691001892
2077870915,15640,kirktrue,2024-04-25T18:09:51Z,"thanks for your review. i have made the requested changes, so please take another pass. thanks!",1,0.9940410256385803
2083370524,15640,kirktrue,2024-04-29T18:19:47Z,i agree. what about the timed wait in `awaitpendingasynccommitsandexecutecommitcallbacks()`?,0,0.9876424074172974
2083394774,15640,kirktrue,2024-04-29T18:32:54Z,"in most places i removed use of a `timer` to calculate the deadline. event classes no longer require a `timer`, it is the caller who must call `completableevent.calculatedeadlinems()` when creating the event.",0,0.9902706146240234
2084906500,15640,cadonna,2024-04-30T10:11:21Z,do you have something in mind? like using the `commitfuture` as a timeout for the awaiting of the execution of the callback?,0,0.9945165514945984
2087355371,15640,lianetm,2024-04-30T21:17:39Z,"agree we should not wait on the `commitfuture` with a timer because the deadline is contained in the event we submitted, and already enforced by the reaper, and not clear about what the proposed relationship with `awaitpendingasynccommitsandexecutecommitcallbacks` is?? i would expect we only need to call `consumerutils.getresult(commitfuture);`, and that is consistent with how we get results for all other completable events now: - we create an event with a deadline - we call `applicationeventhandler.addandget(event)` for the commit case that flow has a different shape just because we use `applicationeventhandler.add(event)` [a link], to cater for commit sync and async, but we should still apply the same approach and just call get without any time boundary i would say.",0,0.9888021945953369
2087359722,15640,lianetm,2024-04-30T21:19:03Z,"hey , thanks a lot for the pr, this is a big piece! i completed a pass of all the non-test files, left some comments.",1,0.9965783953666687
2089079055,15640,kirktrue,2024-05-01T20:26:23Z,"here's my reasoning on the need for a `timer`-based `get()` in `awaitpendingasynccommitsandexecutecommitcallbacks()`... the `future` that's referenced in `lastpendingasynccommit` comes from an `asynccommitevent` and has a hard-coded deadline of `long.max_value`. as such, the `competableeventreaper` in the network thread will never prune that event. without a timeout when calling `get()` on the `lastpendingasynccommit`, the caller could hang for up to `request.timeout.ms` while we wait for the network i/o request to complete (or timeout). does that make sense? cmiiw, please :folded_hands:",0,0.9896765351295471
2090517636,15640,cadonna,2024-05-02T13:34:30Z,"i think, we are talking about two separate things here: 1. change `consumerutils.getresult(commitfuture, requesttimer);` to `consumerutils.getresult(commitfuture);` 2. do we need a timer for `awaitpendingasynccommitsandexecutecommitcallbacks(requesttimer, true);` as far as i understand, we agree on 1 but do not know if there is a better solution for 2. is this correct? currently, i do not see a better way than using a timer on awaiting the execution of the async commit callback. as pointed out, since the async commit does basically not have a timeout, we cannot wait on the deadline of the `asynccommitevent`. we can also not wait for the deadline of the `synccommitevent` since if that event completes before the timeout, we would not wait enough for the completion of the async commit callback.",0,0.9735116958618164
2090837554,15640,cadonna,2024-05-02T15:40:18Z,"thinking about it, i guess having a thread-safe timer and using it in the background thread and the application thread would be the cleanest solution. however, i do not think it is worth blocking this pr on that.",0,0.96234530210495
2092616162,15640,lucasbru,2024-05-03T09:13:49Z,is there any problem if we leave `awaitpendingasynccommitsandexecutecommitcallbacks` as is? it clearly needs the timer,0,0.9910348057746887
2092808466,15640,cadonna,2024-05-03T11:22:05Z,"yeah, i think that we should leave this as it is for now.",0,0.9658971428871155
2115938603,15640,lianetm,2024-05-16T18:31:18Z,"high level comment, just to clarify and make sure it's something we are considering and will cover with the follow-up prs for timeouts. here we're introducing a component to ensures that app events are expired only after having one chance, but that's only at the app thread level, and not for all events, but only for unsubscribe, and poll. thing is that events can also be expired indirectly when a request is expired (so playing against this changes). so even if the `processbackgroundevents` introduced here gives an expired event a change to run one, that may actually not happen (because the underlying request expires). i expect that side needed to make this whole intention work in practice will be a follow-up pr, am i right? also note, almost none of our integration test cover the poll(zero) case (helper funcs [a link], used by most of the test, poll with timeout > 0). that's probably why we did not find the expiration issues we have with poll(0) before. i guess that after addressing the timeout/expiration (this pr and follow-up), we should be able to add some.",0,0.9813696146011353
2117012544,15640,cadonna,2024-05-17T08:23:24Z,"could you elaborate on this a bit. i cannot completely follow. i followed the `syncevent`. the event is processed in the `applicationeventhandler` and a request is added to the commit request manager. then the commit request manager is polled, the requests are added to the network client and the the network client is polled. as far as i can see the `syncevent` is given a chance to complete. i did also not understand the connection to `processbackgroundevents`. that is for events that originate from the background thread. how does that influence if an application event was given a chance to complete? what do you mean with the ""underlying requests expires""? do you mean it exceeds the request timeout? does the legacy consumer ensure that a request completed when timeout is set to `0`? sorry for all these questions, but i would like to understand.",0,0.9684089422225952
2117763853,15640,lianetm,2024-05-17T14:43:23Z,"hey , the tricky bit is that, for some events, the request managers do expire requests too, so in this flow you described: when the manager is polled, if the event had timeout 0, it will be expired/cancelled before making it to the network thread. currently we have 2 managers that do this (that i can remember): [a link] and [a link]. so for those events, even with this pr, if they have timeout 0, they won't have a chance to complete. my point is not to bring more changes into this pr, only to have the whole situation in mind so we can address it properly (with multiple prs). this other [a link] attempts to address this situation i described, but only in the `commitrequestmanager` for instance. we still have to align on the approach there, and also handle it in the `topicmetadatamanager` i would say. i would expect that a combination of this pr and those others would allow us to get to a better point (now, even with this pr, we cannot make basic progress with a consumer being continuously polled with timeout 0 because `fetchcommittedoffsets` is always expired by the manager, for instance). i can easily repro it with the following integration test + poll(zero) (that i was surprised we have not covered, because testutils always polls with a non-zero timeout) [code block] makes sense?",0,0.9347489476203918
2118578191,15640,kirktrue,2024-05-18T01:56:19Z,"yes, the network layer changes are captured in kafka-16200 and build on top of this pr.",0,0.9926161766052246
2118579935,15640,kirktrue,2024-05-18T01:57:33Z,i believe i have addressed all the actionable feedback. are there additional concerns about this pr that prevent it from being merged? thanks.,1,0.96502685546875
2122575967,15640,cadonna,2024-05-21T12:56:15Z,thanks for the explanation!,1,0.8257373571395874
2123163240,15640,kirktrue,2024-05-21T18:04:40Z,the latest batch of feedback has been addressed. thanks!,1,0.9743289947509766
2125302873,15640,kirktrue,2024-05-22T16:52:58Z,i added kafka-16818 to cover the cases to refactor/migrate/remove tests. thanks & for your reviews!,1,0.9830489158630371
